{"cell_type":{"168d2e6f":"code","2411756d":"code","0e301b19":"code","dd42f63a":"code","d0ebd1be":"code","657ebb9f":"code","598cc61f":"code","6d95db30":"code","458eab92":"code","ed8e1088":"code","ac95915e":"code","9263930c":"code","b7a0601a":"code","2de6a57e":"code","05a39b1d":"code","51122e14":"code","9f208563":"code","c52533fa":"code","3d6d684d":"code","7af2670a":"code","b43634a8":"code","693c7248":"code","43d6fd18":"code","dfb8a2c0":"markdown","c73e74d2":"markdown","71aa01aa":"markdown","6913d03e":"markdown","b7b2dc67":"markdown","c7151097":"markdown","23f7ee83":"markdown","3dfa231f":"markdown","da845598":"markdown","f48b845a":"markdown","26651af3":"markdown","2a9d9f88":"markdown","0dd2550b":"markdown","be2bbe35":"markdown","ec86def2":"markdown","03e8df6e":"markdown","fbc5e72c":"markdown","29c27b00":"markdown","9600fcfc":"markdown"},"source":{"168d2e6f":"# Standard library\nimport copy\nimport random\nimport gc\nimport json\n\n# Specific imports from the standard library\nfrom functools import partial\n\n# Basic imports\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport scipy\n\n# Graphs\n%matplotlib notebook\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scikit-learn\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# LightGBM\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nimport lightgbm\n\n# Keras\nimport keras\nimport keras.backend as K\nfrom keras import models\nfrom keras import layers\nfrom keras import callbacks\nfrom keras import optimizers\nfrom tensorflow.python.ops import confusion_matrix\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.python.keras.callbacks import Callback\n\n# Other things just to try\nfrom numba import jit \nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix as scikit_confusion_matrix","2411756d":"# I just needed to be sure, that this is really read in case there is an error\nsample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","0e301b19":"# Funtions modifying full DataFrame\n\ndef get_useful_train_samples(df):\n    \"\"\"\n    Get only installation ids which did at least one assessment.\n    \n    Without an assessment an installation id is useless as we can not use it for training.\n    \"\"\"\n    usefull_installation_ids = list(df.loc[df[\"type\"]==\"Assessment\", \"installation_id\"].unique())\n    df = df.loc[df[\"installation_id\"].isin(usefull_installation_ids), :]\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    return df\n\ndef add_time_of_learning(df):\n    \"\"\"\n    Get better estimation of time spend on every action.\n    \"\"\"\n    df[\"game_time\"] = df[\"game_time\"] \/ 1000   # Convert game time to seconds\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])   # Convert timestamp to datetime\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    # Compute time of action, i.e. time spent on current action OR time until next action\n    # Note that when IID changes we are subtracting values for different IIDs\n    # and for last action there is no reasonable time, but we fix this later and set it to zero\n    df[\"action_time\"] = (df[\"timestamp\"].shift(-1) - df[\"timestamp\"])\n    # Get positions of indices, i.e. where a group for IID starts\/ends, where a group for IID and GID starts\/ends\n    first_I_indices = df.reset_index().groupby(\"installation_id\")[\"index\"].first().values\n    last_I_indices = df.reset_index().groupby(\"installation_id\")[\"index\"].last().values\n    first_IG_indices = df.reset_index().groupby([\"installation_id\", \"game_session\"])[\"index\"].first().values\n    last_IG_indices = df.reset_index().groupby([\"installation_id\", \"game_session\"])[\"index\"].last().values\n    # For last action there is no time until next action, so set it to zero.\n    df.loc[df.index.isin(last_I_indices), \"action_time\"] = pd.Timedelta(0)\n    # If timedelta is higher than X hours, then a child probably kept the game running but went away, do not count this time\n    df.loc[df[\"action_time\"] > pd.Timedelta(hours=3), \"action_time\"] = pd.Timedelta(0)\n    df[\"action_time\"] = df[\"action_time\"].dt.total_seconds()   # Convert timedelta to seconds\n    # Now time for everything is clipped at X time units, but for clips it is easy to get their max time.\n    # So, watch all videos, time them and store those times.\n    # For other things, you need to create histogram of times between actions and clip them\n    # where you think it makes sense\n    max_clip_times = {\"Tree Top City - Level 1\":15, \"Ordering Spheres\":60, \"Costume Box\":60, \"12 Monkeys\":110, \"Tree Top City - Level 2\":22,\n                      \"Pirate's Tale\":80, \"Treasure Map\":155, \"Tree Top City - Level 3\":24, \"Rulers\":125,\n                      \"Crystal Caves - Level 1\":16, \"Balancing Act\":70, \"Crystal Caves - Level 2\":22, \"Crystal Caves - Level 3\":17,\n                      \"Lifting Heavy Things\":115, \"Honey Cake\":140, \"Heavy, Heavier, Heaviest\":53, \n                      \"Magma Peak - Level 1\":18, \"Magma Peak - Level 2\":20, \"Slop Problem\":55,\n                     }\n    max_game_times = {\"Scrub-A-Dub\":6.5, \"All Star Sorting\":5.1, \"Air Show\":7.3, \"Dino Drink\":6.9, \n                      \"Bubble Bath\":12.8, \"Dino Dive\":9.9, \"Chow Time\":12.2, \"Pan Balance\":7.7,\n                      \"Happy Camel\":9.0, \"Leaf Leader\":8.9, \"Crystals Rule\":4.5}\n    max_activity_times = {\"Sandcastle Builder (Activity)\":12.2, \"Fireworks (Activity)\":8.4, \"Flower Waterer (Activity)\":10.3,\n                          \"Bug Measurer (Activity)\":5.8, \"Watering Hole (Activity)\":12.2, \"Bottle Filler (Activity)\":11.2,\n                          \"Chicken Balancer (Activity)\":7.8, \"Egg Dropper (Activity)\":11.2}\n    max_assessment_times = {\"Mushroom Sorter (Assessment)\":7.8, \"Bird Measurer (Assessment)\":7.6,\n                            \"Cauldron Filler (Assessment)\":8.4, \"Cart Balancer (Assessment)\":8.4,\n                            \"Chest Sorter (Assessment)\":10}    \n    max_times = {**max_clip_times, **max_game_times, **max_activity_times, **max_assessment_times}\n    # Now clip everything at its max timelength.\n    for title, max_time in max_times.items():\n        df.loc[df[\"title\"]==title, \"action_time\"] = df.loc[df[\"title\"]==title, \"action_time\"].clip(upper=max_time)  \n    return df\n\ndef distinguish_train_and_test_asessments(dataframe):\n    \"\"\"\n    Test data has event_count == 1 and event_code == 2000,\n    other assessment should probably have more event counts\n    \n    New columns\n    ===========\n    is_train - True if it is training assessment,\n        False if it is test assessment,\n        None if it is not an assessment\n    \"\"\"\n    agg_df = dataframe.loc[dataframe[\"type\"]==\"Assessment\"].sort_values(by=\"timestamp\")\\\n                      .groupby(by=[\"installation_id\", \"game_session\"])\\\n                      .agg(N_events=(\"game_session\", \"count\"),\n                           sum_event_codes=(\"event_code\", \"sum\")).reset_index()\n    mask = (agg_df[\"N_events\"] == 1) & (agg_df[\"sum_event_codes\"] == 2000)\n    test_game_ids = agg_df.loc[mask, \"game_session\"].to_list()\n    dataframe.loc[dataframe[\"type\"]==\"Assessment\", \"is_train\"] = True\n    dataframe.loc[dataframe[\"game_session\"].isin(test_game_ids), \"is_train\"] = False\n    return dataframe\n\ndef one_hot_assessments(df):\n    one_hot = pd.get_dummies(df.loc[df[\"type\"]==\"Assessment\", \"title\"]).add_prefix(\"OH - \")\n    df = pd.merge(df, one_hot, how=\"left\", left_index=True, right_index=True)\n    return df\n\ndef label_encode_assessments(df):\n    label_dict = {\"Chest Sorter (Assessment)\":[0, 0], \"Bird Measurer (Assessment)\":[1, 1],\n                  \"Mushroom Sorter (Assessment)\":[2, 1], \"Cauldron Filler (Assessment)\":[3, 0],\n                  \"Cart Balancer (Assessment)\":[4, 2]}\n    for name, labels in label_dict.items():\n        df.loc[df[\"title\"]==name, \"AssessmentLabel\"] = labels[0]\n        df.loc[df[\"title\"]==name, \"WorldLabel\"] = labels[1]\n    return df\n\ndef fix_event_data(df):\n    df.loc[:, \"event_data\"] = df[\"event_data\"].apply(json.loads)\n    return df","dd42f63a":"# Helper functions for event aggregations\n\ndef get_hole_size(row):\n    \"\"\"\n    Helper function for find_dino_drinks.\n    \"\"\"\n    return row[\"holes\"][row[\"hole_position\"]-1][\"size\"]\n\ndef get_measurement_type(row):\n    \"\"\"\n    Helper function for find_camel_bowls.\n    \"\"\"\n    if row[\"TotalCamelBowls\"] == 2:\n        if row[\"BowlsFromResources\"] == 2:\n            return \"Good\"\n        else:\n            return \"Bad\"\n    elif row[\"TotalCamelBowls\"] == 3:\n        if row[\"BowlsFromResources\"] == 2:\n            if row[\"ToyMeasuredNTimes\"] == 0:\n                return \"Great\"\n        if row[\"ToyMeasuredNTimes\"] == 1:\n            return \"Good\"\n        else:\n            return \"Bad\"        \n    elif row[\"TotalCamelBowls\"] == 4:\n        if row[\"BowlsFromResources\"] == 2:\n            if row[\"ToyMeasuredNTimes\"] == 1:\n                return \"Good\"\n            else:\n                return \"Bad\"\n        if row[\"BowlsFromResources\"] == 3:\n            if row[\"ToyMeasuredNTimes\"] == 0:\n                return \"Great\"\n        if row[\"ToyMeasuredNTimes\"] == 1:\n            return \"Good\"\n        else:\n            return \"Bad\"\n        ","d0ebd1be":"# IID = installation ID\n# GID = game session ID\n\n# Functions aggregating full DataFrame and returning data in IID-GID DataFrame\n\ndef get_aggregated_base(df):\n    return df.sort_values(by=[\"installation_id\", \"timestamp\"]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n\ndef get_time_spent_on_actions(dataframe):\n    df = dataframe[[\"installation_id\", \"game_session\", \"title\", \"type\", \"world\", \"timestamp\"]].drop_duplicates()\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    time_spent_on_actions = dataframe.sort_values(by=[\"installation_id\", \"timestamp\"]).groupby([\"installation_id\", \"game_session\"]).agg(single_activity_time=('action_time', sum)).reset_index()\n    df = pd.merge(df, time_spent_on_actions, how=\"left\",\n                    left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df.drop_duplicates(subset=[\"installation_id\", \"game_session\", \"title\", \"timestamp\"])\n    df = df.pivot_table(index=[\"installation_id\", \"game_session\", \"timestamp\"], columns=\"title\", values=\"single_activity_time\").fillna(0).reset_index()\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n    cols_for_cumsum = list(df.columns)\n    cols_for_cumsum.remove('installation_id')\n    cols_for_cumsum.remove('game_session')\n    cols_for_cumsum.remove('timestamp')\n    for col in cols_for_cumsum:\n        df.loc[:, col] = df.groupby([\"installation_id\"])[col].cumsum()\n    renaming = {}\n    for col in cols_for_cumsum:\n        renaming[col] = f\"total_time_on-{col}\"\n    df = df.rename(renaming, axis=1)\n    df = df.drop(\"timestamp\", axis=1)\n    return df\n\ndef get_views_of_clip_checkpoints(dataframe):\n    df = dataframe[[\"installation_id\", \"game_session\", \"title\", \"type\", \"world\", \"timestamp\"]].drop_duplicates()\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    time_spent_on_actions = dataframe.sort_values(by=[\"installation_id\", \"timestamp\"]).groupby([\"installation_id\", \"game_session\"]).agg(single_activity_time=('action_time', sum)).reset_index()\n    df = pd.merge(df, time_spent_on_actions, how=\"left\",\n                    left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df.drop_duplicates(subset=[\"installation_id\", \"game_session\", \"title\"])\n    df = df.pivot_table(index=[\"installation_id\", \"game_session\", \"timestamp\"], columns=\"title\", values=\"single_activity_time\").fillna(0).reset_index()\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n    useful_clip_times = {\"Ordering Spheres\":[29, 37], \"Costume Box\":[16, 27], \"12 Monkeys\":[17, 70], \n                         \"Tree Top City - Level 2\":[16], \"Pirate's Tale\":[30, 38, 60], \"Treasure Map\":[15, 39, 105], \n                         \"Tree Top City - Level 3\":[17], \"Rulers\":[39, 95],\n                         \"Balancing Act\":[30, 50], \"Crystal Caves - Level 2\":[8, 12, 18], \n                         \"Crystal Caves - Level 3\":[12],\n                         \"Lifting Heavy Things\":[35, 55, 85, 105], \"Honey Cake\":[80, 130], \"Heavy, Heavier, Heaviest\":[37], \n                         \"Magma Peak - Level 2\":[15], \"Slop Problem\":[15, 27, 44],\n                         }\n    list_of_needed_columns = [\"installation_id\", \"game_session\"]\n    for clip, list_of_checkpoints in useful_clip_times.items():\n        for checkpoint_number, checkpoint_value in enumerate(list_of_checkpoints):\n            column_name = f\"{clip}-Checkpoint_{checkpoint_number+1}\"\n            df[column_name] = df[f\"{clip}\"] >= checkpoint_value\n            df.loc[:, column_name] = df.groupby([\"installation_id\"])[column_name].cumsum()\n            list_of_needed_columns.append(column_name)\n    df = df.loc[:, list_of_needed_columns]\n    return df\n\ndef get_assessment_info(df):\n    \"\"\"\n    Get info about assessments, how many times children tried to finish the assessment and how many times did they answered in\/correctly.\n    \n    Every assessment has its own end code, so we can filter using these event end codes.\n    Using event data we find whether the answer was in\/correct.\n    \n    Returns\n    -------\n    Dataframe with installation_id, game_session, number of in\/correct tries and how many times the assessment was ran.\n    \"\"\"\n    assessment_end_codes = {\"Bird Measurer (Assessment)\":4110, \"Cart Balancer (Assessment)\":4100,\n                            \"Cauldron Filler (Assessment)\":4100, \"Chest Sorter (Assessment)\":4100,\n                            \"Mushroom Sorter (Assessment)\":4100}\n    a = copy.deepcopy(df[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    for assessment_name, end_code in assessment_end_codes.items():\n        basic_mask = (df[\"title\"] == assessment_name) & (df[\"event_code\"]==end_code)\n        c = df.loc[basic_mask, :].sort_values(by=[\"installation_id\", \"timestamp\"])\n        incorrect_mask = (c[\"event_data\"].str.contains('\"correct\":false'))\n        c.loc[incorrect_mask, f\"{assessment_name}-CumIncorrectNumber\"] = c.loc[incorrect_mask, :].groupby([\"installation_id\"])[\"title\"].cumcount() + 1\n        correct_mask = (c[\"event_data\"].str.contains('\"correct\":true'))\n        c.loc[correct_mask, f\"{assessment_name}-CumCorrectNumber\"] = c.loc[correct_mask, :].groupby([\"installation_id\"])[\"title\"].cumcount() + 1\n        cols = [f\"{assessment_name}-CumIncorrectNumber\", f\"{assessment_name}-CumCorrectNumber\"]\n        b = c.sort_values(by=[\"installation_id\", \"game_session\", \"timestamp\"]).drop_duplicates([\"installation_id\"], keep=\"first\")\n        b = b.fillna(0)\n        c = c.combine_first(b)\n        c[cols] = c[cols].fillna(method='ffill')\n        c[cols] = c[cols].fillna(0)\n        c[cols] = c[cols].astype(int)    \n        q = copy.deepcopy(c)\n        d = c.groupby([\"installation_id\", \"game_session\"]).agg(N_correct_number=(f\"{assessment_name}-CumCorrectNumber\", \"max\"), \n                                                           N_incorrect_number=(f\"{assessment_name}-CumIncorrectNumber\", \"max\")).reset_index()\n        g = copy.deepcopy(c.loc[:, [\"installation_id\", \"game_session\"]].drop_duplicates(keep=\"first\"))\n        g[f\"{assessment_name}-N_assessment\"] = g.groupby([\"installation_id\"]).cumcount() + 1  \n        d = d.rename({\"N_correct_number\":f\"{assessment_name}-N_correct_tries\", \"N_incorrect_number\":f\"{assessment_name}-N_incorrect_tries\"}, axis=1)#, \"N_assessment\":f\"{assessment_name}-N_assessment\"}, axis=1)\n        a = pd.merge(a, d, how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n        a = pd.merge(a, g, how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    q = copy.deepcopy(g)\n    cols = list(a.columns)\n    cols.remove(\"installation_id\")\n    cols.remove(\"game_session\")\n    e = a.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    f = a.combine_first(e)\n    f[cols] = f[cols].fillna(method='ffill')\n    f[cols] = f[cols].astype(int)\n    return f\n\ndef compute_accuracy_group(df):\n    \"\"\"\n    Accuracy group can be computed only for train assessments.\n    \"\"\"\n    assessment_end_codes = {\"Bird Measurer (Assessment)\":4110, \"Cart Balancer (Assessment)\":4100,\n                            \"Cauldron Filler (Assessment)\":4100, \"Chest Sorter (Assessment)\":4100,\n                            \"Mushroom Sorter (Assessment)\":4100}\n    a = copy.deepcopy(df[[\"installation_id\", \"game_session\", \"type\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\", \"type\"])\n    a[\"accuracy_group\"] = np.NaN\n    for assessment_name, end_code in assessment_end_codes.items():\n        basic_mask = (df[\"title\"] == assessment_name) & (df[\"event_code\"]==end_code)\n        c = df.loc[basic_mask, :].sort_values(by=[\"installation_id\", \"timestamp\"])\n        incorrect_mask = (c[\"event_data\"].str.contains('\"correct\":false'))\n        c.loc[incorrect_mask, f\"{assessment_name}-IncorrectNumber\"] = c.loc[incorrect_mask, :].groupby([\"installation_id\", \"game_session\"])[\"title\"].cumcount() + 1\n        correct_mask = (c[\"event_data\"].str.contains('\"correct\":true'))\n        c.loc[correct_mask, f\"{assessment_name}-CorrectNumber\"] = c.loc[correct_mask, :].groupby([\"installation_id\", \"game_session\"])[\"title\"].cumcount() + 1\n        cols = [f\"{assessment_name}-IncorrectNumber\", f\"{assessment_name}-CorrectNumber\"]\n        b = c.sort_values(by=[\"installation_id\", \"game_session\", \"timestamp\"]).drop_duplicates([\"installation_id\", \"game_session\"], keep=\"first\")\n        b = b.fillna(0)\n        c = c.combine_first(b)\n        c[cols] = c[cols].fillna(method='ffill')\n        c[cols] = c[cols].fillna(0)\n        c[cols] = c[cols].astype(int)\n        d = c.sort_values(by=[\"installation_id\", \"game_session\", \"timestamp\"]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n        d[\"new_accuracy_group\"] = 0\n        d.loc[(d[f\"{assessment_name}-CorrectNumber\"]==1) & (d[f\"{assessment_name}-IncorrectNumber\"]==0), \"new_accuracy_group\"] = 3\n        d.loc[(d[f\"{assessment_name}-CorrectNumber\"]==1) & (d[f\"{assessment_name}-IncorrectNumber\"]==1), \"new_accuracy_group\"] = 2\n        d.loc[(d[f\"{assessment_name}-CorrectNumber\"]==1) & (d[f\"{assessment_name}-IncorrectNumber\"]>1), \"new_accuracy_group\"] = 1\n        d = d.loc[:, [\"installation_id\", \"game_session\", \"new_accuracy_group\"]]\n        a = pd.merge(a, d, how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n        a[\"accuracy_group\"] = a[\"accuracy_group\"].combine_first(a[\"new_accuracy_group\"])\n        a = a.drop([\"new_accuracy_group\"], axis=1)\n    a.loc[a[\"type\"]==\"Assessment\", \"accuracy_group\"] = a.loc[a[\"type\"]==\"Assessment\", \"accuracy_group\"].fillna(0)\n    a = a.drop([\"type\"], axis=1)\n    return a\n\ndef compute_assessment_streak(dataframe):\n    \"\"\"\n    It is certainly very important to know how many successive\n    tries were in\/correct. Because if a child likes this assessment\n    and finished it correctly multiple times in a row, it will\n    certainly finish it correctly even for the next try.\n    \"\"\"\n    pass\n\ndef find_sandcastles_built(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"84538528\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    sizes = json_normalize(df[\"event_data\"], max_level=0)[[\"size\"]]\n    df[\"SandcastleSizes\"] = sizes[\"size\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"SandcastleSizes\"]]\n    df = pd.concat((df,pd.get_dummies(df[\"SandcastleSizes\"]).add_prefix(\"SandcastleOfHeight-\")), axis=1) \n    g = df.groupby(by=\"installation_id\").agg(TotalHeightOfSandcastles=(\"SandcastleSizes\", \"cumsum\"),\n                                         SandcastlesBuilt=(\"SandcastleSizes\", \"cumcount\"),\n                                         SmallSandcastlesBuilt=(\"SandcastleOfHeight-1\", \"cumsum\"),\n                                         MediumSandcastlesBuilt=(\"SandcastleOfHeight-2\", \"cumsum\"),\n                                         BigSandcastlesBuilt=(\"SandcastleOfHeight-3\", \"cumsum\"))\n    cols = [\"TotalHeightOfSandcastles\", \"SandcastlesBuilt\", \"SmallSandcastlesBuilt\", \"MediumSandcastlesBuilt\", \"BigSandcastlesBuilt\"]\n    g[\"SandcastlesBuilt\"] = g[\"SandcastlesBuilt\"] + 1\n    g[\"installation_id\"] = df[\"installation_id\"]\n    g[\"game_session\"] = df[\"game_session\"]\n    a = g.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_clouds_rained(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"2fb91ec1\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    sizes = json_normalize(df[\"event_data\"], max_level=0)[[\"cloud_size\"]]\n    df[\"CloudSizes\"] = sizes[\"cloud_size\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"CloudSizes\"]]\n    df = pd.concat((df,pd.get_dummies(df[\"CloudSizes\"]).add_prefix(\"CloudOdSize-\")), axis=1) \n    g = df.groupby(by=\"installation_id\").agg(\n                                         CloudsRained=(\"CloudSizes\", \"cumcount\"),\n                                         SmallCloudSizes=(\"CloudOdSize-1\", \"cumsum\"),\n                                         MediumCloudSizes=(\"CloudOdSize-2\", \"cumsum\"),\n                                         BigCloudSizes=(\"CloudOdSize-3\", \"cumsum\"))\n    cols = [\"CloudsRained\", \"SmallCloudSizes\", \"MediumCloudSizes\", \"BigCloudSizes\"]\n    g[\"CloudsRained\"] = g[\"CloudsRained\"] + 1\n    g[\"installation_id\"] = df[\"installation_id\"]\n    g[\"game_session\"] = df[\"game_session\"]\n    a = g.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_fireworks_launched(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"e694a35b\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"FireworksLaunched\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"FireworksLaunched\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_flowers_collected(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"fcfdffb6\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"FlowersCollected\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"FlowersCollected\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_bottles_filled(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"e9c52111\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"BottlesFilled\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"BottlesFilled\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_eggs_dropped(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"4c2ec19f\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"EggsDropped\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"EggsDropped\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_dinoscale_changes(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"461eace6\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"DinoScaleChanges\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"DinoScaleChanges\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_bugs_measured(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"022b4259\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    sizes = json_normalize(df[\"event_data\"], max_level=0)[[\"buglength\"]]\n    df[\"MeasuredBuglength\"] = sizes[\"buglength\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"MeasuredBuglength\"]]\n    g = df.groupby(by=\"installation_id\").agg(TotalBuglengthMeasured=(\"MeasuredBuglength\", \"cumsum\"),\n                                         TotalBugMeasurements=(\"MeasuredBuglength\", \"cumcount\"),\n                                         MaximumBuglengthMeasured=(\"MeasuredBuglength\", \"cummax\"))\n    cols = [\"TotalBuglengthMeasured\", \"TotalBugMeasurements\", \"MaximumBuglengthMeasured\"]\n    g[\"TotalBugMeasurements\"] = g[\"TotalBugMeasurements\"] + 1\n    g[\"installation_id\"] = df[\"installation_id\"]\n    g[\"game_session\"] = df[\"game_session\"]\n    a = g.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_bath_estimates(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"3bb91dda\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)[[\"containers\", \"correct\", \"target_containers\"]]\n    df[\"ContainersEstimate\"] = estimate_info[\"containers\"].values\n    df[\"IsBathEstimateCorrect\"] = estimate_info[\"correct\"].values\n    df[\"ContainersCorrectTarget\"] = estimate_info[\"target_containers\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsBathEstimateCorrect\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"ContainersEstimate\", \"IsBathEstimateCorrect\", \"ContainersCorrectTarget\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsBathEstimateCorrect\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"ContainersEstimate\", \"IsBathEstimateCorrect\", \"ContainersCorrectTarget\"]]\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(PointsPerCorrectContainers=(\"ContainersCorrectTarget\", \"cumsum\"),\n                                                             MaximumCorrectNumberOfContainers=(\"ContainersCorrectTarget\", \"cummax\"),\n                                                             NCorrectlyEstimatedContainers=(\"ContainersCorrectTarget\", \"cumcount\"))\n    g_correct[\"NCorrectlyEstimatedContainers\"] = g_correct[\"NCorrectlyEstimatedContainers\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(NIncorrectlyEstimatedContainers=(\"ContainersCorrectTarget\", \"cumcount\"))\n    g_incorrect[\"NIncorrectlyEstimatedContainers\"] = g_incorrect[\"NIncorrectlyEstimatedContainers\"] + 1\n    cols = [\"PointsPerCorrectContainers\", \"MaximumCorrectNumberOfContainers\", \"NCorrectlyEstimatedContainers\", \"NIncorrectlyEstimatedContainers\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalBathEstimates\"] = (final[\"NCorrectlyEstimatedContainers\"] + final[\"NIncorrectlyEstimatedContainers\"]).astype(int)\n    final[\"RatioOfCorrectBathEstimates\"] = final[\"NCorrectlyEstimatedContainers\"] \/ final[\"TotalBathEstimates\"]\n    final[\"RatioOfCorrectBathEstimates\"] = final[\"RatioOfCorrectBathEstimates\"].fillna(0)\n    return final\n\ndef find_chickens_on_seesaw(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"499edb7c\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"ChickensOnSeesaw\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"ChickensOnSeesaw\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_pigs_on_seesaw(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"46cd75b4\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"PigsOnSeesaw\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"PigsOnSeesaw\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_correct_air_shows(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"9b4001e4\"]\n    base = copy.deepcopy(dataframe.sort_values(by=[\"installation_id\", \"timestamp\"])[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\"]]\n    df[\"CorrectAirShows\"] = df.groupby(by=\"installation_id\")[\"installation_id\"].cumcount() + 1 \n    cols = [\"CorrectAirShows\"]\n    a = df.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    return final\n\ndef find_incorrect_air_shows(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"7423acbc\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    event_info = json_normalize(df[\"event_data\"], max_level=0)[[\"identifier\"]]\n    df[\"ErrorType\"] = event_info[\"identifier\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"ErrorType\"]]\n    df = pd.concat((df,pd.get_dummies(df[\"ErrorType\"])), axis=1) \n    g = df.groupby(by=\"installation_id\").agg(\n                                         BadAirShowBetweenCliff=(\"WrongBetweenCliff\", \"cumsum\"),\n                                         BadAirShowBetweenTree=(\"WrongBetweenTree\", \"cumsum\"),\n                                         BadAirShowOver=(\"WrongOver\", \"cumsum\"),\n                                         BadAirShowUnder=(\"WrongUnder\", \"cumsum\"))\n    cols = [\"BadAirShowBetweenCliff\", \"BadAirShowBetweenTree\", \"BadAirShowOver\", \"BadAirShowUnder\"]\n    g[\"installation_id\"] = df[\"installation_id\"]\n    g[\"game_session\"] = df[\"game_session\"]\n    a = g.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"IncorrectAirShows\"] = (final[\"BadAirShowBetweenCliff\"] + final[\"BadAirShowBetweenTree\"] +\n                                  final[\"BadAirShowOver\"] + final[\"BadAirShowUnder\"]).astype(int)\n    return final\n\ndef find_crystal_rulers(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"86c924c4\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)[[\"correct\", \"round\"]]\n    df[\"IfCrystalSizeCorrect\"] = estimate_info[\"correct\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IfCrystalSizeCorrect\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IfCrystalSizeCorrect\", \"RoundNumber\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IfCrystalSizeCorrect\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IfCrystalSizeCorrect\", \"RoundNumber\"]]\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxCrystalFinishedRound=(\"RoundNumber\", \"cummax\"),\n                                                             CrystalsMeasuredCorrectly=(\"RoundNumber\", \"cumcount\"))\n    g_correct[\"CrystalsMeasuredCorrectly\"] = g_correct[\"CrystalsMeasuredCorrectly\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(CrystalsMeasuredIncorrectly=(\"RoundNumber\", \"cumcount\"))\n    g_incorrect[\"CrystalsMeasuredIncorrectly\"] = g_incorrect[\"CrystalsMeasuredIncorrectly\"] + 1\n    cols = [\"MaxCrystalFinishedRound\", \"CrystalsMeasuredCorrectly\", \"CrystalsMeasuredIncorrectly\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalCrystalMeasurements\"] = (final[\"CrystalsMeasuredCorrectly\"] + final[\"CrystalsMeasuredIncorrectly\"]).astype(int)\n    final[\"RatioOfCorrectCrystalMeasurements\"] = final[\"CrystalsMeasuredCorrectly\"] \/ final[\"TotalCrystalMeasurements\"]\n    final[\"RatioOfCorrectCrystalMeasurements\"] = final[\"RatioOfCorrectCrystalMeasurements\"].fillna(0)\n    return final\n\ndef find_clean_animals(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"5c3d2b2f\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)[[\"size\", \"animal\", \"correct\", \"round\", \"level\"]]\n    animal_sizes = {\"chicken\":0, \"sheep\":1, \"pig\":2, \"hog\":3, \"cow\":4}\n    estimate_info = estimate_info.replace({\"animal\": animal_sizes})\n    df[\"IsSizeForAnimalCorrect\"] = estimate_info[\"correct\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df[\"LevelNumber\"] = estimate_info[\"level\"].values\n    df[\"AnimalSize\"] = estimate_info[\"animal\"].values\n    df[\"SelectedSize\"] = estimate_info[\"size\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsSizeForAnimalCorrect\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IsSizeForAnimalCorrect\", \"RoundNumber\", \"LevelNumber\", \"AnimalSize\", \"SelectedSize\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsSizeForAnimalCorrect\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IsSizeForAnimalCorrect\", \"RoundNumber\", \"LevelNumber\", \"AnimalSize\", \"SelectedSize\"]]\n    df_incorrect[\"AE\"] = np.abs(df[\"AnimalSize\"]-df[\"SelectedSize\"])\n    df_incorrect[\"SE\"] = (df[\"AnimalSize\"]-df[\"SelectedSize\"])**2\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxScrubADubRound=(\"RoundNumber\", \"cummax\"),\n                                                             MaxScrubADubLevel=(\"LevelNumber\", \"cummax\"),\n                                                             CorrectScrubADubChoices=(\"SelectedSize\", \"cumcount\"))\n    g_correct[\"CorrectScrubADubChoices\"] = g_correct[\"CorrectScrubADubChoices\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(IncorrectScrubADubChoices=(\"SelectedSize\", \"cumcount\"),\n                                                                 CumulativeAbsoluteScrubADubError=(\"AE\", \"cumsum\"),\n                                                                 CumulativeSquaredScrubADubError=(\"SE\", \"cumsum\"))\n    g_incorrect[\"IncorrectScrubADubChoices\"] = g_incorrect[\"IncorrectScrubADubChoices\"] + 1\n    cols = [\"MaxScrubADubRound\", \"MaxScrubADubLevel\", \"CorrectScrubADubChoices\",\n            \"IncorrectScrubADubChoices\", \"CumulativeAbsoluteScrubADubError\", \"CumulativeSquaredScrubADubError\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalScrubADubChoices\"] = (final[\"CorrectScrubADubChoices\"] + final[\"IncorrectScrubADubChoices\"]).astype(int)\n    final[\"ScrubADubMAE\"] = final[\"CumulativeAbsoluteScrubADubError\"] \/ final[\"TotalScrubADubChoices\"]\n    final[\"ScrubADubMSE\"] = final[\"CumulativeSquaredScrubADubError\"] \/ final[\"TotalScrubADubChoices\"]\n    final[\"RatioOfCorrectScrubADubChoices\"] = final[\"CorrectScrubADubChoices\"] \/ final[\"TotalScrubADubChoices\"]\n    final[\"ScrubADubMAE\"] = final[\"ScrubADubMAE\"].fillna(4)\n    final[\"ScrubADubMSE\"] = final[\"ScrubADubMSE\"].fillna(16)\n    final[\"RatioOfCorrectScrubADubChoices\"] = final[\"RatioOfCorrectScrubADubChoices\"].fillna(0)\n    return final\n\ndef find_dino_drinks(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"74e5f8a7\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)\n    estimate_info[\"hole_size\"] = estimate_info.apply(lambda row: get_hole_size(row), axis=1)\n    estimate_info = estimate_info[[\"shell_size\", \"hole_size\", \"correct\", \"round\"]]\n    df[\"IsSizeOfDrinkCorrect\"] = estimate_info[\"correct\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df[\"ShellSize\"] = estimate_info[\"shell_size\"].values\n    df[\"DrinkSize\"] = estimate_info[\"hole_size\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsSizeOfDrinkCorrect\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IsSizeOfDrinkCorrect\", \"RoundNumber\", \"ShellSize\", \"DrinkSize\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsSizeOfDrinkCorrect\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IsSizeOfDrinkCorrect\", \"RoundNumber\", \"ShellSize\", \"DrinkSize\"]]\n    df_incorrect[\"AE\"] = np.abs(df[\"DrinkSize\"]-df[\"ShellSize\"])\n    df_incorrect[\"SE\"] = (df[\"DrinkSize\"]-df[\"ShellSize\"])**2\n    df_incorrect[\"TooMuchError\"] = df[\"ShellSize\"] > df[\"DrinkSize\"]\n    df_incorrect[\"TooLittleError\"] = df[\"ShellSize\"] < df[\"DrinkSize\"]\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxDinoDrinkRound=(\"RoundNumber\", \"cummax\"),\n                                                             CorrectDinoDrinks=(\"ShellSize\", \"cumcount\"))\n    g_correct[\"CorrectDinoDrinks\"] = g_correct[\"CorrectDinoDrinks\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(IncorrectDinoDrinks=(\"ShellSize\", \"cumcount\"),\n                                                                 CumulativeAbsoluteDinoDrinkError=(\"AE\", \"cumsum\"),\n                                                                 CumulativeSquaredDinoDrinkError=(\"SE\", \"cumsum\"),\n                                                                 TooBigDinoDrinkErrors=(\"TooMuchError\", \"cumsum\"),\n                                                                 TooLittleDinoDrinkErrors=(\"TooLittleError\", \"cumsum\"))\n    g_incorrect[\"IncorrectDinoDrinks\"] = g_incorrect[\"IncorrectDinoDrinks\"] + 1\n    cols = [\"MaxDinoDrinkRound\", \"CorrectDinoDrinks\", \"IncorrectDinoDrinks\",\n            \"CumulativeAbsoluteDinoDrinkError\", \"CumulativeSquaredDinoDrinkError\",\n            \"TooBigDinoDrinkErrors\", \"TooLittleDinoDrinkErrors\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalDinoDrinks\"] = (final[\"CorrectDinoDrinks\"] + final[\"IncorrectDinoDrinks\"]).astype(int)\n    final[\"DinoDrinksMAE\"] = final[\"CumulativeAbsoluteDinoDrinkError\"] \/ final[\"TotalDinoDrinks\"]\n    final[\"DinoDrinksMSE\"] = final[\"CumulativeSquaredDinoDrinkError\"] \/ final[\"TotalDinoDrinks\"]\n    final[\"RatioOfCorrectDinoDrinks\"] = final[\"CorrectDinoDrinks\"] \/ final[\"TotalDinoDrinks\"]\n    final[\"DinoDrinksMAE\"] = final[\"DinoDrinksMAE\"].fillna(3)\n    final[\"DinoDrinksMSE\"] = final[\"DinoDrinksMSE\"].fillna(9)\n    final[\"RatioOfCorrectDinoDrinks\"] = final[\"RatioOfCorrectDinoDrinks\"].fillna(0)\n    return final\n\ndef find_balanced_pans(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"a5e9da97\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)[[\"scale_weights\", \"target_weight\", \"correct\", \"round\"]]\n    df[\"IsPanBalanced\"] = estimate_info[\"correct\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df[\"RockWeight\"] = estimate_info[\"scale_weights\"].values\n    df[\"WeightsWeight\"] = estimate_info[\"target_weight\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsPanBalanced\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IsPanBalanced\", \"RoundNumber\", \"RockWeight\", \"WeightsWeight\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsPanBalanced\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IsPanBalanced\", \"RoundNumber\", \"RockWeight\", \"WeightsWeight\"]]\n    df_incorrect[\"AE\"] = np.abs(df[\"RockWeight\"]-df[\"WeightsWeight\"])\n    df_incorrect[\"SE\"] = (df[\"RockWeight\"]-df[\"WeightsWeight\"])**2\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxPanBalanceRound=(\"RoundNumber\", \"cummax\"),\n                                                             CorrectPanBalances=(\"WeightsWeight\", \"cumcount\"))\n    g_correct[\"CorrectPanBalances\"] = g_correct[\"CorrectPanBalances\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(IncorrectPanBalances=(\"WeightsWeight\", \"cumcount\"),\n                                                                 CumulativeAbsolutePanBalanceError=(\"AE\", \"cumsum\"),\n                                                                 CumulativeSquaredPanBalanceError=(\"SE\", \"cumsum\"))\n    g_incorrect[\"IncorrectPanBalances\"] = g_incorrect[\"IncorrectPanBalances\"] + 1\n    cols = [\"MaxPanBalanceRound\", \"CorrectPanBalances\", \"IncorrectPanBalances\",\n            \"CumulativeAbsolutePanBalanceError\", \"CumulativeSquaredPanBalanceError\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalPanBalances\"] = (final[\"CorrectPanBalances\"] + final[\"IncorrectPanBalances\"]).astype(int)\n    final[\"PanBalanceMAE\"] = final[\"CumulativeAbsolutePanBalanceError\"] \/ final[\"TotalPanBalances\"]\n    final[\"PanBalanceMSE\"] = final[\"CumulativeSquaredPanBalanceError\"] \/ final[\"TotalPanBalances\"]\n    final[\"RatioOfCorrectPanBalances\"] = final[\"CorrectPanBalances\"] \/ final[\"TotalPanBalances\"]\n    final[\"PanBalanceMAE\"] = final[\"PanBalanceMAE\"].fillna(5)\n    final[\"PanBalanceMSE\"] = final[\"PanBalanceMSE\"].fillna(25)\n    final[\"RatioOfCorrectPanBalances\"] = final[\"RatioOfCorrectPanBalances\"].fillna(0)\n    return final\n\ndef find_camel_bowls(dataframe):\n    df1 = dataframe.loc[dataframe[\"event_id\"]==\"c2baf0bd\"]\n    event_info = json_normalize(df1[\"event_data\"], max_level=0)\n    df1[\"TotalCamelBowls\"] = event_info[\"total_bowls\"].values\n    df1[\"RoundNumber\"] = event_info[\"round\"].values\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\", \"timestamp\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n    infobase = copy.deepcopy(base)\n    infobase = pd.merge(infobase, df1[[\"installation_id\", \"game_session\", \"RoundNumber\", \"TotalCamelBowls\"]], how=\"inner\", on=[\"installation_id\", \"game_session\"])\n    df2 = dataframe.loc[dataframe[\"event_id\"]==\"8af75982\"]\n    event_info = json_normalize(df2[\"event_data\"], max_level=0)\n    df2[\"IsToyInBowl\"] = event_info[\"correct\"].values\n    df2[\"IsToyInBowl\"] = df2[\"IsToyInBowl\"].astype(int)\n    df2[\"RoundNumber\"] = event_info[\"round\"].values\n    df2 = df2.groupby(by=[\"installation_id\", \"game_session\", \"RoundNumber\"]).agg(CamelGuesses=(\"IsToyInBowl\", \"count\"),\n                                                                                 CorrectCamelGuesses=(\"IsToyInBowl\", \"sum\")).reset_index()\n    df2[\"IncorrectCamelGuesses\"] = df2[\"CamelGuesses\"] - df2[\"CorrectCamelGuesses\"]\n    infobase = pd.merge(infobase, df2, how=\"inner\", on=[\"installation_id\", \"game_session\", \"RoundNumber\"])\n    df3 = dataframe.loc[dataframe[\"event_id\"]==\"6bf9e3e1\"]\n    event_info = json_normalize(df3[\"event_data\"], max_level=0)\n    df3[\"ToyMeasured\"] = event_info[\"has_toy\"].values.astype(int)\n    df3[\"RoundNumber\"] = event_info[\"round\"].values\n    df3[\"BowlSource\"] = event_info[\"source\"].values    \n    df3 = pd.concat((df3,pd.get_dummies(df3[\"BowlSource\"]).add_prefix(\"BowlSource-\")), axis=1)\n    df3 = df3.groupby(by=[\"installation_id\", \"game_session\", \"RoundNumber\"]).agg(BowlMeasurements=(\"ToyMeasured\", \"count\"),\n                                                                                 ToyMeasuredNTimes=(\"ToyMeasured\", \"sum\"),\n                                                                                 BowlsFromResources=(\"BowlSource-resources\", \"sum\"),\n                                                                                 MovesOnCamelScale=(\"BowlSource-scale\", \"sum\")).reset_index()\n    infobase = pd.merge(infobase, df3, how=\"inner\", on=[\"installation_id\", \"game_session\", \"RoundNumber\"])\n    infobase[\"MeasurementType\"] = infobase.apply(lambda row: get_measurement_type(row), axis=1)\n    infobase = pd.concat((infobase,pd.get_dummies(infobase[\"MeasurementType\"]).add_prefix(\"MeasurementType-\")), axis=1)\n    x = infobase.groupby([\"installation_id\", \"game_session\"]).agg(MaxRoundNumber=(\"RoundNumber\", \"max\"),\n                                                                  TotalRounds=(\"RoundNumber\", \"count\"),\n                                                                  TotalGuesses=(\"CamelGuesses\", \"sum\"),\n                                                                  CorrectGuesses=(\"CorrectCamelGuesses\", \"sum\"),\n                                                                  IncorrectGuesses=(\"IncorrectCamelGuesses\", \"sum\"),\n                                                                  TotalMove=(\"BowlMeasurements\", \"sum\"),\n                                                                  TotalMeasurements=(\"BowlsFromResources\", \"sum\"),\n                                                                  TotalNoInfoMoves=(\"MovesOnCamelScale\", \"sum\"),\n                                                                  BadMeasurements=(\"MeasurementType-Bad\", \"sum\"),\n                                                                  GoodMeasurements=(\"MeasurementType-Good\", \"sum\"),\n                                                                  PerfectMeasurements=(\"MeasurementType-Great\", \"sum\")).reset_index()\n    x = pd.merge(x, base, how=\"inner\", on=[\"installation_id\", \"game_session\"])\n    x = x.sort_values(by=[\"installation_id\", \"timestamp\"])\n    g = x.groupby(by=\"installation_id\").agg(\n                                            MaxCamelRound=(\"MaxRoundNumber\", \"cummax\"),\n    #                                         TotalCamelRounds=(\"TotalRounds\", \"cumsum\"),\n    #                                         TotalCamelGuesses=(\"TotalGuesses\", \"cumsum\"),\n                                            TotalCorrectCamelGuesses=(\"CorrectGuesses\", \"cumsum\"),\n                                            TotalIncorrectCamelGuesses=(\"IncorrectGuesses\", \"cumsum\"),\n                                            TotalCamelBowlMoves=(\"TotalMove\", \"cumsum\"),\n                                            TotalCamelBowlsMovedToScale=(\"TotalMeasurements\", \"cumsum\"),\n                                            TotalCamelNoInfoMoves=(\"TotalNoInfoMoves\", \"cumsum\"),\n                                            BadCamelMeasurements=(\"BadMeasurements\", \"cumsum\"),\n                                            GoodCamelMeasurements=(\"GoodMeasurements\", \"cumsum\"),\n                                            PerfectCamelMeasurements=(\"PerfectMeasurements\", \"cumsum\"))\n    cols = [\"MaxCamelRound\", \"TotalCorrectCamelGuesses\",\n            \"TotalIncorrectCamelGuesses\", \"TotalCamelBowlMoves\", \"TotalCamelBowlsMovedToScale\", \"TotalCamelNoInfoMoves\",\n            \"BadCamelMeasurements\", \"GoodCamelMeasurements\", \"PerfectCamelMeasurements\"]\n    g[\"installation_id\"] = x[\"installation_id\"]\n    g[\"game_session\"] = x[\"game_session\"]\n    a = g.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final = final.drop([\"timestamp\"], axis=1)\n    final[\"RatioOfBadCamelMeasurements\"] = final[\"BadCamelMeasurements\"] \/ final[\"TotalCorrectCamelGuesses\"]\n    final[\"RatioOfGoodCamelMeasurements\"] = final[\"GoodCamelMeasurements\"] \/ final[\"TotalCorrectCamelGuesses\"]\n    final[\"RatioOfPerfectCamelMeasurements\"] = final[\"PerfectCamelMeasurements\"] \/ final[\"TotalCorrectCamelGuesses\"]\n    final[\"RatioOfBadCamelMeasurements\"] = final[\"RatioOfBadCamelMeasurements\"].fillna(0)\n    final[\"RatioOfGoodCamelMeasurements\"] = final[\"RatioOfGoodCamelMeasurements\"].fillna(0)\n    final[\"RatioOfPerfectCamelMeasurements\"] = final[\"RatioOfPerfectCamelMeasurements\"].fillna(0)\n    return final\n\ndef find_sorted_stars(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"2dc29e21\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=1)\n    estimate_info = estimate_info[[\"size\", \"house.size\", \"correct\", \"round\"]]\n    df[\"IsSizeOfHouseCorrect\"] = estimate_info[\"correct\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df[\"DinoSize\"] = estimate_info[\"size\"].values\n    df[\"HouseSize\"] = estimate_info[\"house.size\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsSizeOfHouseCorrect\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IsSizeOfHouseCorrect\", \"RoundNumber\", \"DinoSize\", \"HouseSize\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsSizeOfHouseCorrect\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IsSizeOfHouseCorrect\", \"RoundNumber\", \"DinoSize\", \"HouseSize\"]]\n    df_incorrect[\"AE\"] = np.abs(df[\"HouseSize\"]-df[\"DinoSize\"])\n    df_incorrect[\"SE\"] = (df[\"HouseSize\"]-df[\"DinoSize\"])**2\n    df_incorrect[\"TooMuchError\"] = df[\"DinoSize\"] > df[\"HouseSize\"]\n    df_incorrect[\"TooLittleError\"] = df[\"DinoSize\"] < df[\"HouseSize\"]\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxAllStarsRound=(\"RoundNumber\", \"cummax\"),\n                                                             CorrectDinoHouses=(\"DinoSize\", \"cumcount\"))\n    g_correct[\"CorrectDinoHouses\"] = g_correct[\"CorrectDinoHouses\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(IncorrectDinoHouses=(\"DinoSize\", \"cumcount\"),\n                                                                 CumulativeAbsoluteAllStarsError=(\"AE\", \"cumsum\"),\n                                                                 CumulativeSquaredAllStarsError=(\"SE\", \"cumsum\"),\n                                                                 TooBigAllStarsErrors=(\"TooMuchError\", \"cumsum\"),\n                                                                 TooLittleAllStarsErrors=(\"TooLittleError\", \"cumsum\"))\n    g_incorrect[\"IncorrectDinoHouses\"] = g_incorrect[\"IncorrectDinoHouses\"] + 1\n    cols = [\"MaxAllStarsRound\", \"CorrectDinoHouses\", \"IncorrectDinoHouses\",\n            \"CumulativeAbsoluteAllStarsError\", \"CumulativeSquaredAllStarsError\",\n            \"TooBigAllStarsErrors\", \"TooLittleAllStarsErrors\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalDinoHouses\"] = (final[\"CorrectDinoHouses\"] + final[\"IncorrectDinoHouses\"]).astype(int)\n    final[\"AllStarsMAE\"] = final[\"CumulativeAbsoluteAllStarsError\"] \/ final[\"TotalDinoHouses\"]\n    final[\"AllStarsMSE\"] = final[\"CumulativeSquaredAllStarsError\"] \/ final[\"TotalDinoHouses\"]\n    final[\"RatioOfCorrectDinoHouses\"] = final[\"CorrectDinoHouses\"] \/ final[\"TotalDinoHouses\"]\n    final[\"AllStarsMAE\"] = final[\"AllStarsMAE\"].fillna(3)\n    final[\"AllStarsMSE\"] = final[\"AllStarsMSE\"].fillna(9)\n    final[\"RatioOfCorrectDinoHouses\"] = final[\"RatioOfCorrectDinoHouses\"].fillna(0)\n    return final\n\ndef find_dived_dinosaurs(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"c0415e5c\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)\n    estimate_info = estimate_info[[\"size\", \"correct\", \"round\"]]\n    df[\"IsDinoCorrectSize\"] = estimate_info[\"correct\"].values\n    df[\"DinoSize\"] = estimate_info[\"size\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = copy.deepcopy(df.loc[df[\"IsDinoCorrectSize\"]==True]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_correct = df_correct[[\"installation_id\", \"game_session\", \"IsDinoCorrectSize\", \"RoundNumber\", \"DinoSize\"]]\n    df_incorrect = copy.deepcopy(df.loc[df[\"IsDinoCorrectSize\"]==False]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    df_incorrect = df_incorrect[[\"installation_id\", \"game_session\", \"IsDinoCorrectSize\", \"RoundNumber\", \"DinoSize\"]]\n    g_correct = df_correct.groupby(by=\"installation_id\").agg(MaxDinoDiveRound=(\"RoundNumber\", \"cummax\"),\n                                                             CorrectDinoDives=(\"DinoSize\", \"cumcount\"),\n                                                             CorrectCumulativeDinoDiveSizes=(\"DinoSize\", \"cumsum\"))\n    g_correct[\"CorrectDinoDives\"] = g_correct[\"CorrectDinoDives\"] + 1\n    g_incorrect = df_incorrect.groupby(by=\"installation_id\").agg(IncorrectDinoDives=(\"DinoSize\", \"cumcount\"),\n                                                                 IncorrectCumulativeDinoDiveSizes=(\"DinoSize\", \"cumsum\"))\n    g_incorrect[\"IncorrectDinoDives\"] = g_incorrect[\"IncorrectDinoDives\"] + 1\n    cols = [\"MaxDinoDiveRound\", \"CorrectDinoDives\", \"CorrectCumulativeDinoDiveSizes\",\n            \"IncorrectDinoDives\", \"IncorrectCumulativeDinoDiveSizes\"]\n    g_correct[\"installation_id\"] = df_correct[\"installation_id\"]\n    g_correct[\"game_session\"] = df_correct[\"game_session\"]\n    g_incorrect[\"installation_id\"] = df_incorrect[\"installation_id\"]\n    g_incorrect[\"game_session\"] = df_incorrect[\"game_session\"]\n    a = g_correct.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    b = g_incorrect.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, b, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[\"TotalDinoDives\"] = (final[\"CorrectDinoDives\"] + final[\"IncorrectDinoDives\"]).astype(int)\n    final[\"RatioOfCorrectDinoDivesAgainstAll\"] = final[\"CorrectDinoDives\"] \/ final[\"TotalDinoDives\"]\n    final[\"RatioOfIncorrectDinoDivesAgainstCorrect\"] = final[\"IncorrectDinoDives\"] \/ final[\"CorrectDinoDives\"]\n    final[\"RatioOfIncorrectDinoDivesAgainstCorrect\"] = final[\"RatioOfIncorrectDinoDivesAgainstCorrect\"].fillna(0)\n    final[\"RatioOfCorrectDinoDivesAgainstAll\"] = final[\"RatioOfCorrectDinoDivesAgainstAll\"].fillna(0)\n    return final\n\ndef find_leaf_leaders(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"262136f4\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    infobase = copy.deepcopy(base)\n    timebase = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\", \"timestamp\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n    event_info = json_normalize(df[\"event_data\"], max_level=0)\n    event_info = event_info[[\"mode\", \"dinosaur_count\", \"round\"]]\n    df[\"Mode\"] = event_info[\"mode\"].values\n    df[\"DinoCount\"] = event_info[\"dinosaur_count\"].values\n    df[\"RoundNumber\"] = event_info[\"round\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = pd.concat((df,pd.get_dummies(df[\"Mode\"]).add_prefix(\"Mode-\")), axis=1)\n    df11 = df.drop_duplicates(subset=[\"installation_id\", \"game_session\", \"RoundNumber\"], keep=\"last\")[[\"installation_id\", \"game_session\", \"DinoCount\", \"RoundNumber\"]]\n    df12 = df.groupby(by=[\"installation_id\", \"game_session\", \"RoundNumber\"]).agg(Additions=(\"Mode-add\", \"sum\"),\n                                                                                  Removals=(\"Mode-remove\", \"sum\")).reset_index()\n    df = pd.merge(df11, df12, on=[\"installation_id\", \"game_session\", \"RoundNumber\"])\n    df[\"TotalModes\"] = df[\"Additions\"] + df[\"Removals\"]\n    g = df.groupby(by=[\"installation_id\", \"game_session\"]).agg(TotalLeafLeaderRounds=(\"RoundNumber\", \"count\"),\n                                                               MaxLeafLeaderRound=(\"RoundNumber\", \"max\"),\n                                                               SumLeafLeaderDinoCount=(\"DinoCount\", \"sum\"),\n                                                               SumLeafLeaderAdditions=(\"Additions\", \"sum\"),\n                                                               SumLeafLeaderRemovals=(\"Removals\", \"sum\"),\n                                                               SumLeafLeaderModes=(\"TotalModes\", \"sum\"),\n                                                               MaxLeafLeaderModes=(\"TotalModes\", \"max\")).reset_index()\n    g = pd.merge(g, timebase, how=\"left\", on=[\"installation_id\", \"game_session\"]).sort_values(by=[\"installation_id\", \"timestamp\"])\n    h = g.groupby(by=\"installation_id\", as_index=True).agg(TotalLeafLeaderRounds=(\"TotalLeafLeaderRounds\", \"cumsum\"),\n                                            MaxLeafLeaderRound=(\"MaxLeafLeaderRound\", \"cummax\"),\n                                            SumLeafLeaderDinoCount=(\"SumLeafLeaderDinoCount\", \"cumsum\"),\n                                            SumLeafLeaderAdditions=(\"SumLeafLeaderAdditions\", \"cumsum\"),\n                                            SumLeafLeaderRemovals=(\"SumLeafLeaderRemovals\", \"cumsum\"),\n                                            SumLeafLeaderModes=(\"SumLeafLeaderModes\", \"cumsum\"),\n                                            MaxLeafLeaderModes=(\"MaxLeafLeaderModes\", \"cummax\"))\n    q = pd.merge(h, g[[\"installation_id\", \"game_session\"]], how=\"left\", left_index=True, right_index=True)\n    q[\"LeafLeaderAdditionsPerRound\"] = q[\"SumLeafLeaderAdditions\"] \/ q[\"TotalLeafLeaderRounds\"]\n    q[\"LeafLeaderRemovalsPerRound\"] = q[\"SumLeafLeaderRemovals\"] \/ q[\"TotalLeafLeaderRounds\"]\n    q[\"LeafLeaderModesPerRound\"] = q[\"SumLeafLeaderModes\"] \/ q[\"TotalLeafLeaderRounds\"]\n    q[\"LeafLeaderAvgDinoCount\"] = q[\"SumLeafLeaderDinoCount\"] \/ q[\"TotalLeafLeaderRounds\"]\n    cols = [\"TotalLeafLeaderRounds\", \"MaxLeafLeaderRound\", \"LeafLeaderAdditionsPerRound\", \"LeafLeaderRemovalsPerRound\",\n            \"LeafLeaderModesPerRound\", \"LeafLeaderAvgDinoCount\"]\n    q = q[[\"installation_id\", \"game_session\"]+cols]\n    a = q.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e[[\"TotalLeafLeaderRounds\", \"MaxLeafLeaderRound\"]] = e[[\"TotalLeafLeaderRounds\", \"MaxLeafLeaderRound\"]].fillna(0)\n    e[[\"LeafLeaderAdditionsPerRound\", \"LeafLeaderRemovalsPerRound\"]] = e[[\"LeafLeaderAdditionsPerRound\", \"LeafLeaderRemovalsPerRound\"]].fillna(20)\n    e[[\"LeafLeaderModesPerRound\"]] = e[[\"LeafLeaderModesPerRound\"]].fillna(40)\n    e[[\"LeafLeaderAvgDinoCount\"]] = e[[\"LeafLeaderAvgDinoCount\"]].fillna(4)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[[\"TotalLeafLeaderRounds\", \"MaxLeafLeaderRound\"]] = final[[\"TotalLeafLeaderRounds\", \"MaxLeafLeaderRound\"]].astype(int)\n    return final\n\ndef find_chewing_dinos(dataframe):\n    df = dataframe.loc[dataframe[\"event_id\"]==\"cb6010f8\"]\n    base = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"])\n    infobase = copy.deepcopy(base)\n    timebase = copy.deepcopy(dataframe[[\"installation_id\", \"game_session\", \"timestamp\"]]).drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"first\")\n    event_info = json_normalize(df[\"event_data\"], max_level=0)\n    event_info = event_info[[\"identifier\", \"round\"]]\n    df[\"Identifier\"] = event_info[\"identifier\"].values\n    df[\"RoundNumber\"] = event_info[\"round\"].values\n    df = df.loc[df[\"Identifier\"]==\"1408\", :]\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"RoundNumber\"]]\n    g1 = df.groupby(by=[\"installation_id\"]).agg(MaxRoundNumber=(\"RoundNumber\", \"cummax\"),\n                                                N_rounds=(\"RoundNumber\", \"cumcount\"))\n    g1[\"N_rounds\"] = g1[\"N_rounds\"] + 1\n    g1[\"installation_id\"] = df[\"installation_id\"]\n    g1[\"game_session\"] = df[\"game_session\"]\n    a = g1.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])\n    df = dataframe.loc[dataframe[\"event_id\"]==\"2230fab4\"]\n    estimate_info = json_normalize(df[\"event_data\"], max_level=0)\n    event_info = event_info[[\"identifier\", \"round\"]]\n    df[\"ErrorType\"] = estimate_info[\"identifier\"].values\n    df[\"RoundNumber\"] = estimate_info[\"round\"].values\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    df = df[[\"installation_id\", \"game_session\", \"ErrorType\", \"RoundNumber\"]]\n    df = pd.concat((df,pd.get_dummies(df[\"ErrorType\"]).add_prefix(\"ErrorType-\")), axis=1)\n    g2 = df.groupby(by=[\"installation_id\"]).agg(TooLowErrors=(\"ErrorType-1438\", \"cumsum\"),\n                                                TooHighErrors=(\"ErrorType-1437\", \"cumsum\"))\n    g2[\"installation_id\"] = df[\"installation_id\"]\n    g2[\"game_session\"] = df[\"game_session\"]\n    a = g2.drop_duplicates(subset=[\"installation_id\", \"game_session\"], keep=\"last\")\n    base = pd.merge(base, a, how=\"left\", on=[\"installation_id\", \"game_session\"])    \n    cols = [\"MaxRoundNumber\", \"N_rounds\", \"TooLowErrors\", \"TooHighErrors\"]\n    e = base.drop_duplicates(subset=[\"installation_id\"], keep=\"first\")\n    e = e.fillna(0)\n    final = base.combine_first(e)\n    final[cols] = final[cols].fillna(method='ffill')\n    final[cols] = final[cols].astype(int)\n    final[f\"TooMuchChewingRatio\"] = final[\"TooHighErrors\"] \/ final[\"N_rounds\"]\n    final[f\"TooFewChewingRatio\"] = final[\"TooLowErrors\"] \/ final[\"N_rounds\"]\n    final[\"TooMuchChewingRatio\"] = final[\"TooMuchChewingRatio\"].fillna(0)\n    final[\"TooFewChewingRatio\"] = final[\"TooFewChewingRatio\"].fillna(0)\n    return final","657ebb9f":"# Functions modifying aggregated DataFrame\n\ndef get_time_spent_on_grouped_actions(df, game_types, activity_types, assessment_types, clip_types):\n    game_types = [f\"total_time_on-{col}\" for col in game_types]\n    activity_types = [f\"total_time_on-{col}\" for col in activity_types]\n    assessment_types = [f\"total_time_on-{col}\" for col in assessment_types]\n    clip_types = [f\"total_time_on-{col}\" for col in clip_types]\n    df[\"TimeSpentOnGames\"] = df[game_types].sum(axis=1)\n    df[\"TimeSpentOnActivities\"] = df[activity_types].sum(axis=1)\n    df[\"TimeSpentOnAssessments\"] = df[assessment_types].sum(axis=1)\n    df[\"TimeSpentOnClips\"] = df[clip_types].sum(axis=1)\n    return df\n\ndef get_time_spent_in_worlds(df, actions_per_worlds):\n    treetopcity_actions = [f\"total_time_on-{col}\" for col in actions_per_worlds[\"TREETOPCITY\"]]\n    magmapeak_actions = [f\"total_time_on-{col}\" for col in actions_per_worlds[\"MAGMAPEAK\"]]\n    crystalcaves_actions = [f\"total_time_on-{col}\" for col in actions_per_worlds[\"CRYSTALCAVES\"]]\n    df[\"TimeSpentInTreetopCity\"] = df[treetopcity_actions].sum(axis=1)\n    df[\"TimeSpentInMagmaPeak\"] = df[magmapeak_actions].sum(axis=1)\n    df[\"TimeSpentInCrystalCaves\"] = df[crystalcaves_actions].sum(axis=1)\n    return df\n\ndef compute_averages_on_activities(dataframe):\n    # I mean what is the average time spent on \n    pass\n\ndef compute_previous_accuracy_group(dataframe, assessment_names):\n    df = copy.deepcopy(dataframe)\n    for assessment_name in assessment_names:\n        mask = (df[\"title\"] == assessment_name)\n        df.loc[mask, f\"{assessment_name}-PreviousAccuracyGroup\"] = df.loc[mask, \"accuracy_group\"]\n        df.loc[:, f\"{assessment_name}-PreviousAccuracyGroup\"] = df.loc[:, f\"{assessment_name}-PreviousAccuracyGroup\"].shift(1)\n        first_I_indices = df.sort_values(by=[\"installation_id\", \"timestamp\"]).reset_index().groupby(\"installation_id\")[\"index\"].first().values\n        mask = df.index.isin(first_I_indices)\n        df.loc[mask, f\"{assessment_name}-PreviousAccuracyGroup\"] = 0\n        df[f\"{assessment_name}-PreviousAccuracyGroup\"] = df[f\"{assessment_name}-PreviousAccuracyGroup\"].fillna(method='ffill')\n        df[f\"{assessment_name}-PreviousAccuracyGroup\"] = df[f\"{assessment_name}-PreviousAccuracyGroup\"].astype(int)\n    return df\n\ndef compute_best_accuracy_group(dataframe, assessment_names):\n    df = copy.deepcopy(dataframe)\n    for assessment_name in assessment_names:\n        mask = (df[\"title\"] == assessment_name)\n        df.loc[mask, f\"{assessment_name}-BestAccuracyGroup\"] = df.loc[mask, :].sort_values(by=[\"installation_id\", \"timestamp\"]).groupby(by=[\"installation_id\"])[\"accuracy_group\"].cummax()\n        first_I_indices = df.sort_values(by=[\"installation_id\", \"timestamp\"]).reset_index().groupby(\"installation_id\")[\"index\"].first().values\n        mask = df.index.isin(first_I_indices)\n        df.loc[mask, f\"{assessment_name}-BestAccuracyGroup\"] = 0\n        df[f\"{assessment_name}-BestAccuracyGroup\"] = df[f\"{assessment_name}-BestAccuracyGroup\"].fillna(method='ffill')\n        df[f\"{assessment_name}-BestAccuracyGroup\"] = df[f\"{assessment_name}-BestAccuracyGroup\"].astype(int)\n    return df\n\ndef compute_avgstd_accuracy_group(dataframe, assessment_names):\n    df = copy.deepcopy(dataframe)\n    for assessment_name in assessment_names:\n        mask = (df[\"title\"] == assessment_name)\n        df.loc[mask, f\"{assessment_name}-AvgAccuracyGroup\"] = df.loc[mask, :].sort_values(by=[\"installation_id\", \"timestamp\"]).groupby(by=[\"installation_id\"])[\"accuracy_group\"].expanding().mean().reset_index(level=0, drop=True)\n        df.loc[mask, f\"{assessment_name}-StdAccuracyGroup\"] = df.loc[mask, :].sort_values(by=[\"installation_id\", \"timestamp\"]).groupby(by=[\"installation_id\"])[\"accuracy_group\"].expanding().std().reset_index(level=0, drop=True)\n        first_I_indices = df.sort_values(by=[\"installation_id\", \"timestamp\"]).reset_index().groupby(\"installation_id\")[\"index\"].first().values\n        mask = df.index.isin(first_I_indices)\n        df.loc[mask, f\"{assessment_name}-AvgAccuracyGroup\"] = 0\n        df.loc[mask, f\"{assessment_name}-StdAccuracyGroup\"] = 0\n        df[f\"{assessment_name}-AvgAccuracyGroup\"] = df[f\"{assessment_name}-AvgAccuracyGroup\"].fillna(method='ffill')\n        df[f\"{assessment_name}-StdAccuracyGroup\"] = df[f\"{assessment_name}-StdAccuracyGroup\"].fillna(method='ffill')\n    return df\n\ndef find_total_air_shows(dataframe):\n    dataframe[\"TotalAirShows\"] = (dataframe[\"IncorrectAirShows\"] + dataframe[\"CorrectAirShows\"]).astype(int)\n    dataframe[\"RatioOfCorrectAirShows\"] = dataframe[\"CorrectAirShows\"] \/ dataframe[\"TotalAirShows\"]\n    dataframe[\"RatioOfCorrectAirShows\"] = dataframe[\"RatioOfCorrectAirShows\"].fillna(0)\n    return dataframe\n\ndef add_ordinal_classification_groups(dataframe):\n    dataframe[\"AG01\"] = 0\n    dataframe[\"AG12\"] = 0\n    dataframe[\"AG23\"] = 0\n    dataframe.loc[dataframe[\"accuracy_group\"]>0, \"AG01\"] = 1\n    dataframe.loc[dataframe[\"accuracy_group\"]>1, \"AG12\"] = 1\n    dataframe.loc[dataframe[\"accuracy_group\"]>2, \"AG23\"] = 1\n    return dataframe","598cc61f":"def shift_dataframe(df):\n#     skip_columns = ['event_id', 'game_session', 'timestamp', 'event_data', 'installation_id', 'event_count', \n#                     'event_code', 'game_time', 'title', 'type', 'world', 'action_time', 'is_train', \"accuracy_group\",\n#                     'Mushroom Sorter (Assessment)-PreviousAccuracyGroup', 'Bird Measurer (Assessment)-PreviousAccuracyGroup', \n#                     'Cauldron Filler (Assessment)-PreviousAccuracyGroup', \n#                     'Cart Balancer (Assessment)-PreviousAccuracyGroup', 'Chest Sorter (Assessment)-PreviousAccuracyGroup', \n#                     'OH - Bird Measurer (Assessment)', 'OH - Cart Balancer (Assessment)', \n#                     'OH - Cauldron Filler (Assessment)', 'OH - Chest Sorter (Assessment)', 'OH - Mushroom Sorter (Assessment)',\n#                     \"AssessmentLabel\", \"WorldLabel\"]    \n    cols_to_shift = ['Bird Measurer (Assessment)-N_correct_tries', 'total_time_on-Bird Measurer (Assessment)',\n                 'total_time_on-Cart Balancer (Assessment)', 'total_time_on-Cauldron Filler (Assessment)', 'total_time_on-Chest Sorter (Assessment)',\n                 'total_time_on-Mushroom Sorter (Assessment)',\n                 'Bird Measurer (Assessment)-N_incorrect_tries', 'Bird Measurer (Assessment)-N_assessment', \n                 'Cart Balancer (Assessment)-N_correct_tries', 'Cart Balancer (Assessment)-N_incorrect_tries', \n                 'Cart Balancer (Assessment)-N_assessment', 'Cauldron Filler (Assessment)-N_correct_tries', \n                 'Cauldron Filler (Assessment)-N_incorrect_tries', 'Cauldron Filler (Assessment)-N_assessment', \n                 'Chest Sorter (Assessment)-N_correct_tries', 'Chest Sorter (Assessment)-N_incorrect_tries', 'Chest Sorter (Assessment)-N_assessment', \n                 'Mushroom Sorter (Assessment)-N_correct_tries', 'Mushroom Sorter (Assessment)-N_incorrect_tries', \n                 'Mushroom Sorter (Assessment)-N_assessment', 'TimeSpentOnAssessments', \n                 'TimeSpentInTreetopCity', 'TimeSpentInMagmaPeak', 'TimeSpentInCrystalCaves', \n                 'Mushroom Sorter (Assessment)-BestAccuracyGroup', 'Bird Measurer (Assessment)-BestAccuracyGroup', \n                 'Cauldron Filler (Assessment)-BestAccuracyGroup', 'Cart Balancer (Assessment)-BestAccuracyGroup', \n                 'Chest Sorter (Assessment)-BestAccuracyGroup', 'Mushroom Sorter (Assessment)-AvgAccuracyGroup', \n                 'Mushroom Sorter (Assessment)-StdAccuracyGroup', 'Bird Measurer (Assessment)-AvgAccuracyGroup', \n                 'Bird Measurer (Assessment)-StdAccuracyGroup', 'Cauldron Filler (Assessment)-AvgAccuracyGroup', \n                 'Cauldron Filler (Assessment)-StdAccuracyGroup', 'Cart Balancer (Assessment)-AvgAccuracyGroup', \n                 'Cart Balancer (Assessment)-StdAccuracyGroup', \n                 'Chest Sorter (Assessment)-AvgAccuracyGroup', 'Chest Sorter (Assessment)-StdAccuracyGroup']\n    dataframe = copy.deepcopy(df)\n    dataframe.loc[:, dataframe.columns.isin(cols_to_shift)] = dataframe.loc[:, dataframe.columns.isin(cols_to_shift)].shift(1) \n    return dataframe\n\n# Function for selecting assessments from aggregated DataFrame\ndef get_assessments_for_training(dataframe):\n    dataframe = shift_dataframe(dataframe)\n    mask = (dataframe[\"type\"] == \"Assessment\") & (dataframe[\"is_train\"]==True)\n    return dataframe.loc[mask, :]\n\ndef get_assessments_for_testing(dataframe):\n    dataframe = shift_dataframe(dataframe)\n    mask = (dataframe[\"type\"] == \"Assessment\") & (dataframe[\"is_train\"]==False)\n    return dataframe.loc[mask, :]","6d95db30":"def apply_preprocessing_functions(dataframe):\n    # Define constants\n    world_types = ['NONE', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']\n    game_types = ['Scrub-A-Dub', 'All Star Sorting', 'Air Show', 'Crystals Rule',\n                  'Dino Drink', 'Bubble Bath', 'Dino Dive', 'Chow Time',\n                  'Pan Balance', 'Happy Camel', 'Leaf Leader']\n    activity_types = ['Sandcastle Builder (Activity)', 'Fireworks (Activity)',\n                      'Flower Waterer (Activity)', 'Bug Measurer (Activity)',\n                      'Watering Hole (Activity)', 'Bottle Filler (Activity)',\n                      'Chicken Balancer (Activity)', 'Egg Dropper (Activity)']\n    assessment_types = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n                        'Cauldron Filler (Assessment)', 'Cart Balancer (Assessment)',\n                        'Chest Sorter (Assessment)']\n    clip_types = ['Welcome to Lost Lagoon!', 'Magma Peak - Level 1',\n                  'Magma Peak - Level 2', 'Tree Top City - Level 1',\n                  'Ordering Spheres', 'Slop Problem', 'Costume Box', '12 Monkeys',\n                  'Tree Top City - Level 2', \"Pirate's Tale\", 'Treasure Map',\n                  'Tree Top City - Level 3', 'Rulers', 'Crystal Caves - Level 1',\n                  'Balancing Act', 'Crystal Caves - Level 2',\n                  'Crystal Caves - Level 3', 'Lifting Heavy Things', 'Honey Cake',\n                  'Heavy, Heavier, Heaviest']\n    actions_per_world = {\"NONE\":['Welcome to Lost Lagoon!'],\n                         \"MAGMAPEAK\":['Magma Peak - Level 1', 'Sandcastle Builder (Activity)',\n                                 'Slop Problem', 'Scrub-A-Dub', 'Watering Hole (Activity)',\n                                 'Magma Peak - Level 2', 'Dino Drink', 'Bubble Bath',\n                                 'Bottle Filler (Activity)', 'Dino Dive',\n                                 'Cauldron Filler (Assessment)'],\n                         \"TREETOPCITY\":['Tree Top City - Level 1', 'Ordering Spheres', 'All Star Sorting',\n                                 'Costume Box', 'Fireworks (Activity)', '12 Monkeys',\n                                 'Tree Top City - Level 2', 'Flower Waterer (Activity)',\n                                 \"Pirate's Tale\", 'Mushroom Sorter (Assessment)', 'Air Show',\n                                 'Treasure Map', 'Tree Top City - Level 3', 'Crystals Rule',\n                                 'Rulers', 'Bug Measurer (Activity)', 'Bird Measurer (Assessment)'],\n                         \"CRYSTALCAVES\":['Crystal Caves - Level 1', 'Chow Time', 'Balancing Act',\n                                 'Crystal Caves - Level 2', 'Crystal Caves - Level 3',\n                                 'Chicken Balancer (Activity)', 'Lifting Heavy Things',\n                                 'Pan Balance', 'Honey Cake', 'Happy Camel',\n                                 'Cart Balancer (Assessment)', 'Heavy, Heavier, Heaviest',\n                                 'Egg Dropper (Activity)', 'Chest Sorter (Assessment)',\n                                 'Leaf Leader']}\n    print(\"  PREPROCESSING\")\n    # Funtions modifying full DataFrame\n    dataframe = get_useful_train_samples(dataframe)\n    print(\"  - filtering useful train samples\", dataframe.shape)\n    dataframe = add_time_of_learning(dataframe)\n    print(\"  - adding time of learning\", dataframe.shape)\n    dataframe = distinguish_train_and_test_asessments(dataframe)\n    print(\"  - distinguishing train and test\", dataframe.shape)\n    dataframe = one_hot_assessments(dataframe)\n    print(\"  - one-hotting assessments\", dataframe.shape)\n    dataframe = label_encode_assessments(dataframe)\n    print(\"  - label-encoding assessments\", dataframe.shape)\n    # Functions aggregating full DataFrame and returning data in IID & GID DataFrame\n    # These should be always merged on IID and GID\n    df = get_aggregated_base(dataframe)\n    print(\"  - composing aggregated base\", df.shape)\n    df = pd.merge(df, get_time_spent_on_actions(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])   # This is lower bound\n    print(\"  - computing time spent on actions\", df.shape)\n    df = pd.merge(df, get_views_of_clip_checkpoints(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])   # This is lower bound\n    print(\"  - computing number of achieving clip checkpoints\", df.shape)\n    df = pd.merge(df, get_assessment_info(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - computing in\/correct assessment tries\", df.shape)\n    df = pd.merge(df, compute_accuracy_group(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - computing accuracy group\", df.shape)\n    dataframe = fix_event_data(dataframe)\n    print(\"  - fixing event data\", dataframe.shape)\n    df = pd.merge(df, find_sandcastles_built(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Sandcastle Builder\", df.shape)\n    df = pd.merge(df, find_clouds_rained(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Watering Hole\", df.shape)\n    df = pd.merge(df, find_fireworks_launched(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Fireworks\", df.shape)\n    df = pd.merge(df, find_flowers_collected(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Flower Waterer\", df.shape)\n    df = pd.merge(df, find_bottles_filled(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Bottle Filler\", df.shape)\n    df = pd.merge(df, find_eggs_dropped(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Egg Dropper 1\", df.shape)\n    df = pd.merge(df, find_dinoscale_changes(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Egg Dropper 2\", df.shape)\n    df = pd.merge(df, find_bugs_measured(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Bug Measurer\", df.shape)\n    df = pd.merge(df, find_bath_estimates(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Bubble Bath\", df.shape)\n    df = pd.merge(df, find_chickens_on_seesaw(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Chicken Balance 1\", df.shape)\n    df = pd.merge(df, find_pigs_on_seesaw(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Chicken Balance 2\", df.shape)\n    df = pd.merge(df, find_correct_air_shows(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Air Show 1\", df.shape)\n    df = pd.merge(df, find_incorrect_air_shows(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Air Show 2\", df.shape)\n    df = pd.merge(df, find_crystal_rulers(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Crystals Rule\", df.shape)\n    df = pd.merge(df, find_clean_animals(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Scrub A Dub\", df.shape)\n    df = pd.merge(df, find_dino_drinks(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Dino Drink\", df.shape)\n    df = pd.merge(df, find_balanced_pans(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Pan Balance\", df.shape)\n    df = pd.merge(df, find_camel_bowls(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Happy Camel\", df.shape)\n    df = pd.merge(df, find_sorted_stars(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - All Star Sorting\", df.shape)\n    df = pd.merge(df, find_dived_dinosaurs(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Dino Dive\", df.shape)\n    df = pd.merge(df, find_leaf_leaders(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Leaf Leader\", df.shape)\n    df = pd.merge(df, find_chewing_dinos(dataframe), how=\"left\", left_on=[\"installation_id\", \"game_session\"], right_on=[\"installation_id\", \"game_session\"])\n    print(\"  - events - Chow Time\", df.shape)\n    df = df.sort_values(by=[\"installation_id\", \"timestamp\"])\n    print(\"  - sorting by id and timestamp\", df.shape)\n    # Functions modifying aggregated DataFrame\n    df = get_time_spent_on_grouped_actions(df, game_types, activity_types, assessment_types, clip_types)\n    print(\"  - grouping time on actions\", df.shape)\n    df = get_time_spent_in_worlds(df, actions_per_world)\n    print(\"  - grouping time in worlds\", df.shape)\n    df = compute_previous_accuracy_group(df, assessment_names=assessment_types)\n    print(\"  - previous accuracy group:\", df.shape)\n    df = compute_best_accuracy_group(df, assessment_names=assessment_types)\n    print(\"  - computing best accuracy group so far\", df.shape)\n    df = compute_avgstd_accuracy_group(df, assessment_names=assessment_types)\n    print(\"  - computing mean and std of accuracy group\", df.shape)\n    df = find_total_air_shows(df)\n    print(\"  - events - Air Show 3\", df.shape)\n    df = add_ordinal_classification_groups(df)\n    print(\"  - events - Adding ordinal classification groups\", df.shape)\n    # Function for selecting assessments from aggregated DataFrame\n    final_train = get_assessments_for_training(df)\n    final_test = get_assessments_for_testing(df)\n    print(\"  - final shapes are\", final_train.shape, final_test.shape)\n    return final_train, final_test","458eab92":"def get_number_of_rows(filename):\n    with open(filename) as f:\n        return sum(1 for line in f)","ed8e1088":"def preprocess(path_to_dataframe='\/kaggle\/input\/data-science-bowl-2019\/train.csv', read_rows=int(1e5)):\n    max_number_of_rows = get_number_of_rows(path_to_dataframe)\n    proceed = True\n    start_from = 0\n    first_round = True\n    while proceed:\n        df = pd.read_csv(path_to_dataframe, nrows=read_rows, skiprows=range(1,start_from+1), header=0)\n        first_I_indices = df.reset_index().groupby(\"installation_id\")[\"index\"].first().values\n        #print(start_from, df.shape, first_I_indices[-1])      \n        if start_from + df.shape[0] + 1 == max_number_of_rows:   # No cut, read everything, it is the final chunk; +1 because of header\n            proceed = False\n        else:   # Not final chunk, cut last ID away, so that it is complete in next chunk\n            cut_to = first_I_indices[-1]\n            df = df.iloc[:cut_to, :]\n            start_from += cut_to\n        print(\"DF shape:\", df.shape)\n        if first_round:   # If it is first round, final DataFrame has to be created\n            preprocessed_train, preprocessed_test = apply_preprocessing_functions(df)\n            print(\"Output shapes:\", preprocessed_train.shape, preprocessed_test.shape)\n            first_round = False\n        else:   # If it is not the first round, then we concatenate to final DataFrame\n            if df.shape[0] > 0:   # Only if current DataFrame is not empty\n                additional_train, additional_test = apply_preprocessing_functions(df)\n                preprocessed_train = pd.concat((preprocessed_train, additional_train), ignore_index=True)\n                preprocessed_test = pd.concat((preprocessed_test, additional_test), ignore_index=True)\n                print(\"Output shapes:\", preprocessed_train.shape, preprocessed_test.shape)\n        print()\n    return preprocessed_train, preprocessed_test","ac95915e":"train_train, train_test = preprocess(read_rows=500000, path_to_dataframe='\/kaggle\/input\/data-science-bowl-2019\/train.csv')\ntrain_test[\"is_train\"] = True\ntrain_train = pd.concat((train_train, train_test), ignore_index=True)\ntest_train, test_test = preprocess(read_rows=500000, path_to_dataframe='\/kaggle\/input\/data-science-bowl-2019\/test.csv')\ntest_test = test_test.drop_duplicates(subset=\"installation_id\", keep=\"last\") \ntrain_train = train_train.replace([np.inf, -np.inf],0)\ntrain_test = train_test.replace([np.inf, -np.inf],0)\ntest_test = test_test.replace([np.inf, -np.inf],0)\nprint(train_train.shape)\nprint(test_train.shape)\nprint(test_test.shape)","9263930c":"train_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train_train.columns]\ntest_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in test_train.columns]\ntest_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in test_test.columns]","b7a0601a":"train_ids = train_train.groupby(by=[\"installation_id\"]).agg(id_count=(\"installation_id\", \"count\")).sort_values(by=[\"id_count\"])\ntest_ids = copy.deepcopy(test_test[[\"installation_id\"]]).set_index(\"installation_id\")\ntest_ids[\"id_count\"] = 0\na = test_train.groupby(by=[\"installation_id\"]).agg(id_count=(\"installation_id\", \"count\")).sort_values(by=[\"id_count\"])\ntest_ids = a.combine_first(test_ids).sort_values(by=[\"id_count\"])\nall_test_ids = copy.deepcopy(test_test[[\"installation_id\"]])","2de6a57e":"class ShuffledGroupKFold(object):\n    \n    def __init__(self, groupby, n_splits=2, single_select=1):\n        self.groupby = groupby\n        self.n_splits = n_splits\n        self.single_select = 1\n        self.vicinity = self.n_splits * self.single_select\n        \n    def generate_splits(self, ids_to_split):\n        self.splits = [list() for i in range(self.n_splits)]\n        for i in range(0, len(ids_to_split), self.vicinity):\n            current_choice = ids_to_split[i:i+self.vicinity]\n            for split_number in range(self.n_splits):\n                current_length = len(current_choice)\n                if current_length >= self.single_select:\n                    selected_indices = random.sample(range(current_length), self.single_select)\n                    for indice in sorted(selected_indices, reverse=True):\n                        selected_index = current_choice.pop(indice)\n                        self.splits[split_number].append(selected_index)\n                else:\n                    self.splits[split_number] = self.splits[split_number] + current_choice\n                    break\n    \n    def get_splits(self):\n        for a in range(self.n_splits):\n            train = list()\n            for b in range(self.n_splits):\n                if a != b:\n                    train = train + self.splits[b]\n            test = self.splits[a]\n            yield train, test","05a39b1d":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        return -cohen_kappa_score(y, preds, weights = 'quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [1.0, 1.5, 2.0]\n        self.coef_ = scipy.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        return preds\n    \n    def coefficients(self):\n        return list(np.sort(self.coef_['x']))","51122e14":"def get_one_hot_encodings(df):\n    feature_list = sorted(['OH___Bird_Measurer__Assessment_', 'OH___Cart_Balancer__Assessment_', 'OH___Cauldron_Filler__Assessment_', \n                'OH___Chest_Sorter__Assessment_', 'OH___Mushroom_Sorter__Assessment_'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N    \n\ndef get_big_labels(df):\n    feature_list = sorted(['AssessmentLabel', 'WorldLabel'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N    \n                           \ndef get_small_times(df):\n    feature_list = sorted(['total_time_on_12_Monkeys', 'total_time_on_Air_Show', 'total_time_on_All_Star_Sorting', \n                'total_time_on_Balancing_Act', 'total_time_on_Bird_Measurer__Assessment_', 'total_time_on_Bottle_Filler__Activity_', \n                'total_time_on_Bubble_Bath', 'total_time_on_Bug_Measurer__Activity_', 'total_time_on_Cart_Balancer__Assessment_', \n                'total_time_on_Cauldron_Filler__Assessment_', 'total_time_on_Chest_Sorter__Assessment_', \n                'total_time_on_Chicken_Balancer__Activity_', 'total_time_on_Chow_Time', 'total_time_on_Costume_Box', \n                'total_time_on_Crystal_Caves___Level_1', 'total_time_on_Crystal_Caves___Level_2', 'total_time_on_Crystal_Caves___Level_3',\n                'total_time_on_Crystals_Rule', 'total_time_on_Dino_Dive', 'total_time_on_Dino_Drink', \n                'total_time_on_Egg_Dropper__Activity_', 'total_time_on_Fireworks__Activity_', 'total_time_on_Flower_Waterer__Activity_',\n                'total_time_on_Happy_Camel', 'total_time_on_Heavy__Heavier__Heaviest', 'total_time_on_Honey_Cake',\n                'total_time_on_Leaf_Leader', 'total_time_on_Lifting_Heavy_Things', 'total_time_on_Magma_Peak___Level_1',\n                'total_time_on_Magma_Peak___Level_2', 'total_time_on_Mushroom_Sorter__Assessment_', 'total_time_on_Ordering_Spheres',\n                'total_time_on_Pan_Balance', 'total_time_on_Pirate_s_Tale', 'total_time_on_Rulers',\n                'total_time_on_Sandcastle_Builder__Activity_', 'total_time_on_Scrub_A_Dub', 'total_time_on_Slop_Problem', \n                'total_time_on_Treasure_Map', 'total_time_on_Tree_Top_City___Level_1', 'total_time_on_Tree_Top_City___Level_2', \n                'total_time_on_Tree_Top_City___Level_3', 'total_time_on_Watering_Hole__Activity_'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N   \n\ndef get_small_times_magmapeak(df):\n    feature_list = sorted(['total_time_on_Bottle_Filler__Activity_', \n                'total_time_on_Bubble_Bath','total_time_on_Dino_Dive', 'total_time_on_Dino_Drink', \n                'total_time_on_Cauldron_Filler__Assessment_', 'total_time_on_Magma_Peak___Level_1',\n                'total_time_on_Magma_Peak___Level_2', \n                'total_time_on_Sandcastle_Builder__Activity_', 'total_time_on_Scrub_A_Dub', 'total_time_on_Slop_Problem', \n                'total_time_on_Watering_Hole__Activity_'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N   \n\ndef get_small_times_treetopcity(df):\n    feature_list = sorted(['total_time_on_12_Monkeys', 'total_time_on_Air_Show', 'total_time_on_All_Star_Sorting', \n                'total_time_on_Bird_Measurer__Assessment_','total_time_on_Bug_Measurer__Activity_', \n                'total_time_on_Costume_Box',\n                'total_time_on_Crystals_Rule',\n                'total_time_on_Fireworks__Activity_', 'total_time_on_Flower_Waterer__Activity_',\n                'total_time_on_Mushroom_Sorter__Assessment_', 'total_time_on_Ordering_Spheres',\n                'total_time_on_Pirate_s_Tale', 'total_time_on_Rulers',\n                'total_time_on_Treasure_Map', 'total_time_on_Tree_Top_City___Level_1', 'total_time_on_Tree_Top_City___Level_2', \n                'total_time_on_Tree_Top_City___Level_3',])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N   \n\ndef get_small_times_crystalcaves(df):\n    feature_list = sorted([\n                'total_time_on_Balancing_Act',\n                'total_time_on_Cart_Balancer__Assessment_', \n                'total_time_on_Chest_Sorter__Assessment_', \n                'total_time_on_Chicken_Balancer__Activity_', 'total_time_on_Chow_Time',\n                'total_time_on_Crystal_Caves___Level_1', 'total_time_on_Crystal_Caves___Level_2', 'total_time_on_Crystal_Caves___Level_3',\n                'total_time_on_Egg_Dropper__Activity_',\n                'total_time_on_Happy_Camel', 'total_time_on_Heavy__Heavier__Heaviest', 'total_time_on_Honey_Cake',\n                'total_time_on_Leaf_Leader', 'total_time_on_Lifting_Heavy_Things',\n                'total_time_on_Pan_Balance'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N   \n\ndef get_checkpoints(df):\n    feature_list = sorted(['Ordering_Spheres_Checkpoint_1', 'Ordering_Spheres_Checkpoint_2', 'Costume_Box_Checkpoint_1',\n                'Costume_Box_Checkpoint_2', '12_Monkeys_Checkpoint_1', '12_Monkeys_Checkpoint_2', 'Tree_Top_City___Level_2_Checkpoint_1',\n                'Pirate_s_Tale_Checkpoint_1', 'Pirate_s_Tale_Checkpoint_2', 'Pirate_s_Tale_Checkpoint_3',\n                'Treasure_Map_Checkpoint_1', 'Treasure_Map_Checkpoint_2', 'Treasure_Map_Checkpoint_3',\n                'Tree_Top_City___Level_3_Checkpoint_1', 'Rulers_Checkpoint_1', 'Rulers_Checkpoint_2', 'Balancing_Act_Checkpoint_1',\n                'Balancing_Act_Checkpoint_2', 'Crystal_Caves___Level_2_Checkpoint_1', 'Crystal_Caves___Level_2_Checkpoint_2',\n                'Crystal_Caves___Level_2_Checkpoint_3', 'Crystal_Caves___Level_3_Checkpoint_1', 'Lifting_Heavy_Things_Checkpoint_1',\n                'Lifting_Heavy_Things_Checkpoint_2', 'Lifting_Heavy_Things_Checkpoint_3', 'Lifting_Heavy_Things_Checkpoint_4',\n                'Honey_Cake_Checkpoint_1', 'Honey_Cake_Checkpoint_2', 'Heavy__Heavier__Heaviest_Checkpoint_1',\n                'Magma_Peak___Level_2_Checkpoint_1', 'Slop_Problem_Checkpoint_1', 'Slop_Problem_Checkpoint_2', \n                'Slop_Problem_Checkpoint_3'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N     \n\ndef get_checkpoints_magmapeak(df):\n    feature_list = sorted(['Magma_Peak___Level_2_Checkpoint_1', 'Slop_Problem_Checkpoint_1', 'Slop_Problem_Checkpoint_2', \n                'Slop_Problem_Checkpoint_3'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N     \n\ndef get_checkpoints_treetopcity(df):\n    feature_list = sorted(['Ordering_Spheres_Checkpoint_1', 'Ordering_Spheres_Checkpoint_2', 'Costume_Box_Checkpoint_1',\n                'Costume_Box_Checkpoint_2', '12_Monkeys_Checkpoint_1', '12_Monkeys_Checkpoint_2', 'Tree_Top_City___Level_2_Checkpoint_1',\n                'Pirate_s_Tale_Checkpoint_1', 'Pirate_s_Tale_Checkpoint_2', 'Pirate_s_Tale_Checkpoint_3',\n                'Treasure_Map_Checkpoint_1', 'Treasure_Map_Checkpoint_2', 'Treasure_Map_Checkpoint_3',\n                'Tree_Top_City___Level_3_Checkpoint_1', 'Rulers_Checkpoint_1', 'Rulers_Checkpoint_2'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N     \n\ndef get_checkpoints_crystalcaves(df):\n    feature_list = sorted(['Balancing_Act_Checkpoint_1',\n                'Balancing_Act_Checkpoint_2', 'Crystal_Caves___Level_2_Checkpoint_1', 'Crystal_Caves___Level_2_Checkpoint_2',\n                'Crystal_Caves___Level_2_Checkpoint_3', 'Crystal_Caves___Level_3_Checkpoint_1', 'Lifting_Heavy_Things_Checkpoint_1',\n                'Lifting_Heavy_Things_Checkpoint_2', 'Lifting_Heavy_Things_Checkpoint_3', 'Lifting_Heavy_Things_Checkpoint_4',\n                'Honey_Cake_Checkpoint_1', 'Honey_Cake_Checkpoint_2', 'Heavy__Heavier__Heaviest_Checkpoint_1'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N     \n\ndef get_assessment_info(df):\n    feature_list = sorted(['Bird_Measurer__Assessment__N_correct_tries', 'Bird_Measurer__Assessment__N_incorrect_tries',\n                'Bird_Measurer__Assessment__N_assessment', 'Cart_Balancer__Assessment__N_correct_tries', \n                'Cart_Balancer__Assessment__N_incorrect_tries', 'Cart_Balancer__Assessment__N_assessment', \n                'Cauldron_Filler__Assessment__N_correct_tries', 'Cauldron_Filler__Assessment__N_incorrect_tries', \n                'Cauldron_Filler__Assessment__N_assessment', 'Chest_Sorter__Assessment__N_correct_tries', \n                'Chest_Sorter__Assessment__N_incorrect_tries', 'Chest_Sorter__Assessment__N_assessment', \n                'Mushroom_Sorter__Assessment__N_correct_tries', 'Mushroom_Sorter__Assessment__N_incorrect_tries',\n                'Mushroom_Sorter__Assessment__N_assessment'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n\ndef get_assessment_info_magmapeak(df):\n    feature_list = sorted([\n                'Cauldron_Filler__Assessment__N_correct_tries', 'Cauldron_Filler__Assessment__N_incorrect_tries', \n                'Cauldron_Filler__Assessment__N_assessment', \"Cauldron_Filler__Assessment__PreviousAccuracyGroup\",\n                \"Cauldron_Filler__Assessment__BestAccuracyGroup\", \"Cauldron_Filler__Assessment__AvgAccuracyGroup\",\n                \"Cauldron_Filler__Assessment__StdAccuracyGroup\"])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n\ndef get_assessment_info_treetopcity(df):\n    feature_list = sorted(['Bird_Measurer__Assessment__N_correct_tries', 'Bird_Measurer__Assessment__N_incorrect_tries',\n                'Bird_Measurer__Assessment__N_assessment', \"Mushroom_Sorter__Assessment__PreviousAccuracyGroup\",\n                'Mushroom_Sorter__Assessment__N_correct_tries', 'Mushroom_Sorter__Assessment__N_incorrect_tries',\n                'Mushroom_Sorter__Assessment__N_assessment', \"Mushroom_Sorter__Assessment__StdAccuracyGroup\", \n                \"Bird_Measurer__Assessment__StdAccuracyGroup\", \"Bird_Measurer__Assessment__AvgAccuracyGroup\",\n                \"Mushroom_Sorter__Assessment__AvgAccuracyGroup\", \"Bird_Measurer__Assessment__BestAccuracyGroup\",\n                \"Mushroom_Sorter__Assessment__BestAccuracyGroup\", \"Bird_Measurer__Assessment__PreviousAccuracyGroup\"])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n\ndef get_assessment_info_crystalcaves(df):\n    feature_list = sorted(['Cart_Balancer__Assessment__N_correct_tries', \n                'Cart_Balancer__Assessment__N_incorrect_tries', 'Cart_Balancer__Assessment__N_assessment', \n                'Chest_Sorter__Assessment__N_correct_tries', \n                'Chest_Sorter__Assessment__N_incorrect_tries', 'Chest_Sorter__Assessment__N_assessment',\n                'Cart_Balancer__Assessment__PreviousAccuracyGroup', 'Chest_Sorter__Assessment__PreviousAccuracyGroup',\n                'Cart_Balancer__Assessment__BestAccuracyGroup', \n                'Chest_Sorter__Assessment__BestAccuracyGroup', \"Cart_Balancer__Assessment__AvgAccuracyGroup\",\n                'Cart_Balancer__Assessment__StdAccuracyGroup', 'Chest_Sorter__Assessment__AvgAccuracyGroup', \n                'Chest_Sorter__Assessment__StdAccuracyGroup'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n\ndef get_event_features(df):\n    feature_list = sorted(['TotalHeightOfSandcastles',\n                'SandcastlesBuilt', 'SmallSandcastlesBuilt', 'MediumSandcastlesBuilt', 'BigSandcastlesBuilt',\n                'CloudsRained', 'SmallCloudSizes', 'MediumCloudSizes', 'BigCloudSizes', \n                'FireworksLaunched', 'FlowersCollected', 'BottlesFilled', \n                'EggsDropped', 'DinoScaleChanges', 'TotalBuglengthMeasured',\n                'TotalBugMeasurements', 'MaximumBuglengthMeasured', 'PointsPerCorrectContainers', 'MaximumCorrectNumberOfContainers',\n                'NCorrectlyEstimatedContainers', 'NIncorrectlyEstimatedContainers', 'TotalBathEstimates', \n                'RatioOfCorrectBathEstimates', 'ChickensOnSeesaw', 'PigsOnSeesaw', 'CorrectAirShows', 'BadAirShowBetweenCliff',\n                'BadAirShowBetweenTree', 'BadAirShowOver', 'BadAirShowUnder', 'IncorrectAirShows', 'MaxCrystalFinishedRound',\n                'CrystalsMeasuredCorrectly', 'CrystalsMeasuredIncorrectly', 'TotalCrystalMeasurements', \n                'RatioOfCorrectCrystalMeasurements', 'MaxScrubADubRound', 'MaxScrubADubLevel', 'CorrectScrubADubChoices', \n                'IncorrectScrubADubChoices', 'CumulativeAbsoluteScrubADubError', 'CumulativeSquaredScrubADubError',\n                'TotalScrubADubChoices', 'ScrubADubMAE', 'ScrubADubMSE', 'RatioOfCorrectScrubADubChoices', 'MaxDinoDrinkRound',\n                'CorrectDinoDrinks', 'IncorrectDinoDrinks', 'CumulativeAbsoluteDinoDrinkError', 'CumulativeSquaredDinoDrinkError',\n                'TooBigDinoDrinkErrors', 'TooLittleDinoDrinkErrors', 'TotalDinoDrinks', 'DinoDrinksMAE', 'DinoDrinksMSE',\n                'RatioOfCorrectDinoDrinks', 'MaxPanBalanceRound', 'CorrectPanBalances', 'IncorrectPanBalances', \n                'CumulativeAbsolutePanBalanceError', 'CumulativeSquaredPanBalanceError', 'TotalPanBalances', 'PanBalanceMAE', \n                'PanBalanceMSE', 'RatioOfCorrectPanBalances', 'MaxCamelRound', 'TotalCorrectCamelGuesses', 'TotalIncorrectCamelGuesses',\n                'TotalCamelBowlMoves', 'TotalCamelBowlsMovedToScale', 'TotalCamelNoInfoMoves', 'BadCamelMeasurements',\n                'GoodCamelMeasurements', 'PerfectCamelMeasurements', 'RatioOfBadCamelMeasurements', \n                'RatioOfGoodCamelMeasurements', 'RatioOfPerfectCamelMeasurements', 'MaxAllStarsRound', 'CorrectDinoHouses', \n                'IncorrectDinoHouses', 'CumulativeAbsoluteAllStarsError', 'CumulativeSquaredAllStarsError', 'TooBigAllStarsErrors',\n                'TooLittleAllStarsErrors', 'TotalDinoHouses', 'AllStarsMAE', 'AllStarsMSE', 'RatioOfCorrectDinoHouses', \n                'MaxDinoDiveRound', 'CorrectDinoDives', 'CorrectCumulativeDinoDiveSizes', 'IncorrectDinoDives',\n                'IncorrectCumulativeDinoDiveSizes', 'TotalDinoDives', 'RatioOfCorrectDinoDivesAgainstAll', \n                'RatioOfIncorrectDinoDivesAgainstCorrect', 'TotalLeafLeaderRounds', 'MaxLeafLeaderRound',\n                'LeafLeaderAdditionsPerRound', 'LeafLeaderRemovalsPerRound', 'LeafLeaderModesPerRound',\n                'LeafLeaderAvgDinoCount', 'MaxRoundNumber', 'N_rounds', 'TooLowErrors', 'TooHighErrors',\n                'TooMuchChewingRatio', 'TooFewChewingRatio', 'TotalAirShows', \"RatioOfCorrectAirShows\"])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n                           \ndef get_event_info_magmapeak(df):\n    feature_list = sorted(['TotalHeightOfSandcastles',\n                'SandcastlesBuilt', 'SmallSandcastlesBuilt', 'MediumSandcastlesBuilt', 'BigSandcastlesBuilt',\n                'CloudsRained', 'SmallCloudSizes', 'MediumCloudSizes', 'BigCloudSizes', \n                'BottlesFilled', \n                'PointsPerCorrectContainers', 'MaximumCorrectNumberOfContainers',\n                'NCorrectlyEstimatedContainers', 'NIncorrectlyEstimatedContainers', 'TotalBathEstimates', \n                'RatioOfCorrectBathEstimates', 'MaxScrubADubRound', 'MaxScrubADubLevel', 'CorrectScrubADubChoices', \n                'IncorrectScrubADubChoices', 'CumulativeAbsoluteScrubADubError', 'CumulativeSquaredScrubADubError',\n                'TotalScrubADubChoices', 'ScrubADubMAE', 'ScrubADubMSE', 'RatioOfCorrectScrubADubChoices', \n                'MaxDinoDiveRound', 'CorrectDinoDives', 'CorrectCumulativeDinoDiveSizes', 'IncorrectDinoDives',\n                'IncorrectCumulativeDinoDiveSizes', 'TotalDinoDives', 'RatioOfCorrectDinoDivesAgainstAll', \n                'RatioOfIncorrectDinoDivesAgainstCorrect', \n                'MaxDinoDrinkRound',\n                'CorrectDinoDrinks', 'IncorrectDinoDrinks', 'CumulativeAbsoluteDinoDrinkError', 'CumulativeSquaredDinoDrinkError',\n                'TooBigDinoDrinkErrors', 'TooLittleDinoDrinkErrors', 'TotalDinoDrinks', 'DinoDrinksMAE', 'DinoDrinksMSE',\n                'RatioOfCorrectDinoDrinks'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N                            \n                           \ndef get_event_info_treetopcity(df):\n    feature_list = sorted([\n                'FireworksLaunched', 'FlowersCollected',\n                'TotalBuglengthMeasured',\n                'TotalBugMeasurements', 'MaximumBuglengthMeasured','CorrectAirShows', 'BadAirShowBetweenCliff',\n                'BadAirShowBetweenTree', 'BadAirShowOver', 'BadAirShowUnder', 'IncorrectAirShows', 'MaxCrystalFinishedRound',\n                'CrystalsMeasuredCorrectly', 'CrystalsMeasuredIncorrectly', 'TotalCrystalMeasurements', \n                'RatioOfCorrectCrystalMeasurements', 'MaxAllStarsRound', 'CorrectDinoHouses', \n                'IncorrectDinoHouses', 'CumulativeAbsoluteAllStarsError', 'CumulativeSquaredAllStarsError', 'TooBigAllStarsErrors',\n                'TooLittleAllStarsErrors', 'TotalDinoHouses', 'AllStarsMAE', 'AllStarsMSE', 'TotalAirShows', \"RatioOfCorrectAirShows\"])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N                            \n                           \ndef get_event_info_crystalcaves(df):\n    feature_list = sorted([\n                'EggsDropped', 'DinoScaleChanges', 'ChickensOnSeesaw', 'PigsOnSeesaw', \n                'MaxPanBalanceRound', 'CorrectPanBalances', 'IncorrectPanBalances', \n                'CumulativeAbsolutePanBalanceError', 'CumulativeSquaredPanBalanceError', 'TotalPanBalances', 'PanBalanceMAE', \n                'PanBalanceMSE', 'RatioOfCorrectPanBalances', 'MaxCamelRound', 'TotalCorrectCamelGuesses', 'TotalIncorrectCamelGuesses',\n                'TotalCamelBowlMoves', 'TotalCamelBowlsMovedToScale', 'TotalCamelNoInfoMoves', 'BadCamelMeasurements',\n                'GoodCamelMeasurements', 'PerfectCamelMeasurements', 'RatioOfBadCamelMeasurements', \n                'RatioOfGoodCamelMeasurements', 'RatioOfPerfectCamelMeasurements',\n                'TotalLeafLeaderRounds', 'MaxLeafLeaderRound',\n                'LeafLeaderAdditionsPerRound', 'LeafLeaderRemovalsPerRound', 'LeafLeaderModesPerRound',\n                'LeafLeaderAvgDinoCount', 'MaxRoundNumber', 'N_rounds', 'TooLowErrors', 'TooHighErrors',\n                'TooMuchChewingRatio', 'TooFewChewingRatio'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N                            \n\ndef get_big_times(df):\n    feature_list = sorted(['TimeSpentOnGames', 'TimeSpentOnActivities', 'TimeSpentInCrystalCaves', \n                'TimeSpentOnAssessments', 'TimeSpentOnClips', 'TimeSpentInTreetopCity', 'TimeSpentInMagmaPeak', ])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N \n\ndef get_previous_accuracies(df):\n    feature_list = sorted(['Mushroom_Sorter__Assessment__PreviousAccuracyGroup', \n                'Bird_Measurer__Assessment__PreviousAccuracyGroup', 'Cauldron_Filler__Assessment__PreviousAccuracyGroup',\n                'Cart_Balancer__Assessment__PreviousAccuracyGroup', 'Chest_Sorter__Assessment__PreviousAccuracyGroup',\n                'Mushroom_Sorter__Assessment__BestAccuracyGroup', 'Bird_Measurer__Assessment__BestAccuracyGroup', \n                'Cauldron_Filler__Assessment__BestAccuracyGroup', 'Cart_Balancer__Assessment__BestAccuracyGroup', \n                'Chest_Sorter__Assessment__BestAccuracyGroup', 'Mushroom_Sorter__Assessment__AvgAccuracyGroup', \n                'Mushroom_Sorter__Assessment__StdAccuracyGroup', 'Bird_Measurer__Assessment__AvgAccuracyGroup', \n                'Bird_Measurer__Assessment__StdAccuracyGroup', 'Cauldron_Filler__Assessment__AvgAccuracyGroup', \n                'Cauldron_Filler__Assessment__StdAccuracyGroup', 'Cart_Balancer__Assessment__AvgAccuracyGroup',\n                'Cart_Balancer__Assessment__StdAccuracyGroup', 'Chest_Sorter__Assessment__AvgAccuracyGroup', \n                'Chest_Sorter__Assessment__StdAccuracyGroup'])\n    N = len(feature_list)\n    feature_values = df[feature_list].values\n    return feature_values, N ","9f208563":"def prepare_for_predictions(df, distros=None, scaler=None):\n    \"\"\"\n    Transform DataFrame with engineered features to \n    input needed by ML model.\n    \n    Parameters\n    ==========\n    - df\n        DataFrame with engineered features.\n    - scaler\n        If None, new StandardScaler will be created, else\n        provided scaler will be used for scalling features.\n    \"\"\"\n    # Extract features from the DataFrame as not all are needed\n    one_hot_encoding_features, N_one_hot_encoding_features = get_one_hot_encodings(df)\n    big_labels_features, N_big_labels_features = get_big_labels(df)\n    small_times_features_magmapeak, N_small_times_features_magmapeak = get_small_times_magmapeak(df)\n    small_times_features_treetopcity, N_small_times_features_treetopcity = get_small_times_treetopcity(df)\n    small_times_features_crystalcaves, N_small_times_features_crystalcaves = get_small_times_crystalcaves(df)\n    checkpoints_features_magmapeak, N_checkpoints_features_magmapeak = get_checkpoints_magmapeak(df)\n    checkpoints_features_treetopcity, N_checkpoints_features_treetopcity = get_checkpoints_treetopcity(df)\n    checkpoints_features_crystalcaves, N_checkpoints_features_crystalcaves = get_checkpoints_crystalcaves(df)\n    assessment_info_features_magmapeak, N_assessment_info_features_magmapeak = get_assessment_info_magmapeak(df)\n    assessment_info_features_treetopcity, N_assessment_info_features_treetopcity = get_assessment_info_treetopcity(df)\n    assessment_info_features_crystalcaves, N_assessment_info_features_crystalcaves = get_assessment_info_crystalcaves(df)\n    event_features_magmapeak, N_event_features_magmapeak = get_event_info_magmapeak(df)\n    event_features_treetopcity, N_event_features_treetopcity = get_event_info_treetopcity(df)\n    event_features_crystalcaves, N_event_features_crystalcaves = get_event_info_crystalcaves(df)\n    big_times_features, N_big_times_features = get_big_times(df)\n    # Save input lenghts in a list and in a dict\n    input_lengths = [N_one_hot_encoding_features, N_big_labels_features, \n                     N_small_times_features_magmapeak, N_small_times_features_treetopcity, N_small_times_features_crystalcaves,\n                     N_checkpoints_features_magmapeak, N_checkpoints_features_treetopcity, N_checkpoints_features_crystalcaves,\n                     N_assessment_info_features_magmapeak, N_assessment_info_features_treetopcity, N_assessment_info_features_crystalcaves,\n                     N_event_features_magmapeak, N_event_features_treetopcity, N_event_features_crystalcaves,\n                     N_big_times_features, \n                    ]  \n    input_lengths_dict = {\"OneHots\":N_one_hot_encoding_features, \"BigLabels\":N_big_labels_features,  \n                          \"SmallTimes-MP\":N_small_times_features_magmapeak, \n                          \"SmallTimes-TTC\":N_small_times_features_treetopcity, \"SmallTimes-CC\":N_small_times_features_crystalcaves,\n                          \"Checkpoints-MP\":N_checkpoints_features_magmapeak,\n                          \"Checkpoints-TTC\":N_checkpoints_features_treetopcity, \"Checkpoints-CC\":N_checkpoints_features_crystalcaves,\n                          \"Assessments-MP\":N_assessment_info_features_magmapeak,\n                          \"Assessments-TTC\":N_assessment_info_features_treetopcity, \"Assessments-CC\":N_assessment_info_features_crystalcaves,\n                          \"Events-MP\":N_event_features_magmapeak,\n                          \"Events-TTC\":N_event_features_treetopcity, \"Events-CC\":N_event_features_crystalcaves,\n                          \"BigTimes\":N_big_times_features\n                         }\n    # Concatenate extracted features\n    X = np.concatenate((\n                        small_times_features_magmapeak, small_times_features_treetopcity, small_times_features_crystalcaves,\n                        checkpoints_features_magmapeak, checkpoints_features_treetopcity, checkpoints_features_crystalcaves, \n                        assessment_info_features_magmapeak, assessment_info_features_treetopcity, assessment_info_features_crystalcaves,\n                        event_features_magmapeak, event_features_treetopcity, event_features_crystalcaves,\n                        big_times_features, \n                       ), axis=1)    \n    # Scale data\n    if scaler is not None:   # If scaler is provided use it\n        Xscaler = scaler\n    else:   # If scaler is not provided, create a new one and fit data with it\n        Xscaler = StandardScaler()\n        Xscaler.fit(X)\n    X = Xscaler.transform(X)\n    X = np.concatenate((one_hot_encoding_features, big_labels_features, X), axis=1)\n    return X, input_lengths, Xscaler, input_lengths_dict\n\ndef split_to_inputs(X, feature_lengths):\n    \"\"\"\n    Split input for model to input layers.\n    \n    The NN takes more Input groups,\n    this function takes care of providing each Input layer\n    with its corresponding input in array of all input values called X.\n    \"\"\"\n    arrays = list()\n    start = 0\n    for i in feature_lengths:\n        end = start + i \n        arrays.append(X[:, start:end])\n        start = end\n    return arrays","c52533fa":"def define_advanced_model():\n    \"\"\"\n    Define and return neural network.\n    \n    Returns\n    =======\n    keras.NN\n    \"\"\"\n    # Inputs\n    input_OneHots = Input(shape=(feature_lengths[\"OneHots\"],), name=f'Input-OneHots')\n    input_BigLabels = Input(shape=(feature_lengths[\"BigLabels\"],), name=f'Input-BigLabels')\n    input_SmallTimes_MP = Input(shape=(feature_lengths[\"SmallTimes-MP\"],), name=f'Input-SmallTimes-MP')                   \n    input_SmallTimes_TTC = Input(shape=(feature_lengths[\"SmallTimes-TTC\"],), name=f'Input-SmallTimes-TTC')    \n    input_SmallTimes_CC = Input(shape=(feature_lengths[\"SmallTimes-CC\"],), name=f'Input-SmallTimes-CC')       \n    input_Checkpoints_MP = Input(shape=(feature_lengths[\"Checkpoints-MP\"],), name=f'Input-Checkpoints-MP')    \n    input_Checkpoints_TTC = Input(shape=(feature_lengths[\"Checkpoints-TTC\"],), name=f'Input-Checkpoints-TTC')    \n    input_Checkpoints_CC = Input(shape=(feature_lengths[\"Checkpoints-CC\"],), name=f'Input-Checkpoints-CC')    \n    input_Assessments_MP = Input(shape=(feature_lengths[\"Assessments-MP\"],), name=f'Input-Assessments-MP')               \n    input_Assessments_TTC = Input(shape=(feature_lengths[\"Assessments-TTC\"],), name=f'Input-Assessments-TTC')\n    input_Assessments_CC = Input(shape=(feature_lengths[\"Assessments-CC\"],), name=f'Input-Assessments-CC')                     \n    input_Events_MP = Input(shape=(feature_lengths[\"Events-MP\"],), name=f'Input-Events-MP')                     \n    input_Events_TTC = Input(shape=(feature_lengths[\"Events-TTC\"],), name=f'Input-Events-TTC')\n    input_Events_CC = Input(shape=(feature_lengths[\"Events-CC\"],), name=f'Input-Events-CC')                     \n    input_BigTimes = Input(shape=(feature_lengths[\"BigTimes\"],), name=f'Input-BigTimes')                                    \n    # Type of assessment part\n    OneHots_embedding = Embedding(feature_lengths[\"OneHots\"], 5, name=\"OneHots-Embedding\")\n    OneHots_flatten = Flatten(name=\"OneHots-Flatten\")\n    OneHots_outputs = OneHots_flatten(OneHots_embedding(input_OneHots))  \n    BigDense_1 = Dense(2, activation='relu', name=\"BigLabels-Dense\")(input_BigLabels)                                 \n    AssessmentTypesConcat = Concatenate(name=\"AssessmentTypes-Concat\")([OneHots_outputs, BigDense_1])\n    TypeOfAssessment = Dense(3, activation='relu', name=\"TypeOfAssessments-Dense\")(AssessmentTypesConcat)                       \n    # Big times\n    big_times = Dense(4, activation='relu', name=\"BigTimes\")(input_BigTimes)   \n    # MagmaPeak\n    mp_times = Dense(4, activation='relu', name=\"MP-Times\")(input_SmallTimes_MP) \n    mp_checkpoints = Dense(2, activation='relu', name=\"MP-Checkpoints\")(input_Checkpoints_MP) \n    mp_assessments = Dense(6, activation='relu', name=\"MP-Assessments-1\")(input_Assessments_MP) \n    mp_assessments = Dense(5, activation='relu', name=\"MP-Assessments-2\")(mp_assessments) \n    mp_events = Dense(20, activation='relu', name=\"MP-Events-1\")(input_Events_MP) \n    mp_events = Dense(15, activation='relu', name=\"MP-Events-2\")(mp_events) \n    mp_events = Dense(10, activation='relu', name=\"MP-Events-3\")(mp_events) \n    mp_concat = Concatenate(name=\"MP-Concatenate\")([mp_times, mp_checkpoints, mp_assessments, mp_events])\n    out_MP = Dense(20, activation='relu', name=\"MP-Dense-1\")(mp_concat) \n    out_MP = Dense(20, activation='relu', name=\"MP-Dense-2\")(out_MP)                                  \n    # TreetopCity\n    ttc_times = Dense(8, activation='relu', name=\"TTC-Times\")(input_SmallTimes_TTC) \n    ttc_checkpoints = Dense(6, activation='relu', name=\"TTC-Checkpoints\")(input_Checkpoints_TTC) \n    ttc_assessments = Dense(12, activation='relu', name=\"TTC-Assessments-1\")(input_Assessments_TTC) \n    ttc_assessments = Dense(10, activation='relu', name=\"TTC-Assessments-2\")(ttc_assessments) \n    ttc_events = Dense(16, activation='relu', name=\"TTC-Events-1\")(input_Events_TTC) \n    ttc_events = Dense(13, activation='relu', name=\"TTC-Events-2\")(ttc_events) \n    ttc_events = Dense(10, activation='relu', name=\"TTC-Events-3\")(ttc_events) \n    ttc_concat = Concatenate(name=\"TTC-Concatenate\")([ttc_times, ttc_checkpoints, ttc_assessments, ttc_events])\n    out_TTC = Dense(20, activation='relu', name=\"TTC-Dense-1\")(ttc_concat) \n    out_TTC = Dense(20, activation='relu', name=\"TTC-Dense-2\")(out_TTC)       \n    # CrystalCaves\n    cc_times = Dense(7, activation='relu', name=\"CC-Times\")(input_SmallTimes_CC) \n    cc_checkpoints = Dense(5, activation='relu', name=\"CC-Checkpoints\")(input_Checkpoints_CC) \n    cc_assessments = Dense(12, activation='relu', name=\"CC-Assessments-1\")(input_Assessments_CC) \n    cc_assessments = Dense(10, activation='relu', name=\"CC-Assessments-2\")(cc_assessments) \n    cc_events = Dense(24, activation='relu', name=\"CC-Events-1\")(input_Events_CC) \n    cc_events = Dense(18, activation='relu', name=\"CC-Events-2\")(cc_events) \n    cc_events = Dense(12, activation='relu', name=\"CC-Events-3\")(cc_events) \n    cc_concat = Concatenate(name=\"CC-Concatenate\")([cc_times, cc_checkpoints, cc_assessments, cc_events])\n    out_CC = Dense(20, activation='relu', name=\"CC-Dense-1\")(cc_concat) \n    out_CC = Dense(20, activation='relu', name=\"CC-Dense-2\")(out_CC)                                       \n    # Head\n    concated_parts = Concatenate(name=\"ConcatenatedParts\")([out_MP, out_TTC, out_CC, big_times, TypeOfAssessment])\n    x1 = Dense(100, activation='relu', name=\"Dense-1\")(concated_parts)\n    x1 = BatchNormalization(name=\"BatchNorm-1\")(x1)\n    x1 = Dropout(0.1, name=\"Dropout-1\")(x1)\n    x1 = Concatenate(name=\"Concatenate-1\")([x1, TypeOfAssessment])\n    x2 = Dense(100, activation='relu', name=\"Dense-2\")(x1)\n    x2 = BatchNormalization(name=\"BatchNorm-2\")(x2)\n    x2 = Dropout(0.1, name=\"Dropout-2\")(x2)\n    x2 = Concatenate(name=\"Concatenate-2\")([x1, x2, TypeOfAssessment])\n    x3 = Dense(80, activation='relu', name=\"All-Dense-3\")(x2)\n    x3 = BatchNormalization(name=\"BatchNorm-3\")(x3)\n    x3 = Dropout(0.1, name=\"Dropout-3\")(x3)\n    x3 = Concatenate(name=\"Concatenate-3\")([x2, x3, TypeOfAssessment])\n    x4 = Dense(50, activation='relu', name=\"Dense-4\")(x3)\n    x4 = BatchNormalization(name=\"BatchNorm-4\")(x4)\n    x4 = Dropout(0.1, name=\"Dropout-4\")(x4)\n    x4 = Concatenate(name=\"Concatenate-4\")([x3, x4, TypeOfAssessment])\n    x5 = Dense(50, activation='relu', name=\"Dense-5\")(x4)\n    x5 = BatchNormalization(name=\"BatchNorm-5\")(x5)\n    x5 = Dropout(0.1, name=\"Dropout-5\")(x5)\n    final_concatenation_output = Concatenate(name=\"Concatenate-Final\")([x1, x2, x3, x4, x5, TypeOfAssessment])\n    # Basic regression\n    basic_regression = Dense(1, name=\"QWK\")(final_concatenation_output)\n    # Ordinal classification\n    p1 = Dense(1, activation=\"sigmoid\", name=\"OC-0\")(final_concatenation_output)\n    p2 = Dense(1, activation=\"sigmoid\", name=\"OC-1\")(final_concatenation_output)\n    p3 = Dense(1, activation=\"sigmoid\", name=\"OC-2\")(final_concatenation_output)\n    ordinal_classification = Concatenate(name=\"Concatenate-OC\")([p1, p2, p3])\n    model = Model(inputs=[\n                        input_OneHots, input_BigLabels,\n                        input_SmallTimes_MP, input_SmallTimes_TTC, input_SmallTimes_CC,\n                        input_Checkpoints_MP, input_Checkpoints_TTC, input_Checkpoints_CC,\n                        input_Assessments_MP, input_Assessments_TTC, input_Assessments_CC,\n                        input_Events_MP, input_Events_TTC, input_Events_CC,\n                        input_BigTimes,\n                        ],\n                  outputs=[basic_regression, ordinal_classification]) \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005, decay=1e-4),\n                  loss=[keras.losses.mse, keras.losses.mse],\n                  loss_weights=[1, 10])\n    return model\n\ndef define_model():\n    \"\"\"\n    Define and return neural network.\n    \n    Returns\n    =======\n    keras.NN\n    \"\"\"\n    inputs = Input(shape=(236,), name=f'Inputs')\n    x1 = Dense(200, activation='relu', name=\"Dense-1\")(inputs)\n    x1 = BatchNormalization(name=\"BatchNorm-1\")(x1)\n    x1 = Dropout(0.1, name=\"Dropout-1\")(x1)\n    x2 = Dense(150, activation='relu', name=\"Dense-2\")(x1)\n    x2 = BatchNormalization(name=\"BatchNorm-2\")(x2)\n    x2 = Dropout(0.1, name=\"Dropout-2\")(x2)\n    x2 = Concatenate(name=\"Concatenate-2\")([x1, x2])\n    x3 = Dense(100, activation='relu', name=\"All-Dense-3\")(x2)\n    x3 = BatchNormalization(name=\"BatchNorm-3\")(x3)\n    x3 = Dropout(0.1, name=\"Dropout-3\")(x3)\n    x3 = Concatenate(name=\"Concatenate-3\")([x2, x3])\n    x4 = Dense(100, activation='relu', name=\"Dense-4\")(x3)\n    x4 = BatchNormalization(name=\"BatchNorm-4\")(x4)\n    x4 = Dropout(0.1, name=\"Dropout-4\")(x4)\n    x4 = Concatenate(name=\"Concatenate-4\")([x3, x4])\n    x5 = Dense(50, activation='relu', name=\"Dense-5\")(x4)\n    x5 = BatchNormalization(name=\"BatchNorm-5\")(x5)\n    x5 = Dropout(0.1, name=\"Dropout-5\")(x5)\n    x5 = Concatenate(name=\"Concatenate-5\")([x1, x2, x3, x4, x5])\n    predictions = Dense(1, name=\"QWK\")(x5)\n    model = Model(inputs=[inputs], \n                  outputs=[predictions])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005, decay=1e-4),\n                  loss=[keras.losses.mse],\n                  loss_weights=[1])\n    return model","3d6d684d":"data_columns = ['OH___Bird_Measurer__Assessment_', 'OH___Cart_Balancer__Assessment_', 'OH___Cauldron_Filler__Assessment_', \n                'OH___Chest_Sorter__Assessment_', 'OH___Mushroom_Sorter__Assessment_', 'AssessmentLabel', 'WorldLabel', \n                'total_time_on_12_Monkeys', 'total_time_on_Air_Show', 'total_time_on_All_Star_Sorting', \n                'total_time_on_Balancing_Act', 'total_time_on_Bird_Measurer__Assessment_', 'total_time_on_Bottle_Filler__Activity_', \n                'total_time_on_Bubble_Bath', 'total_time_on_Bug_Measurer__Activity_', 'total_time_on_Cart_Balancer__Assessment_', \n                'total_time_on_Cauldron_Filler__Assessment_', 'total_time_on_Chest_Sorter__Assessment_', \n                'total_time_on_Chicken_Balancer__Activity_', 'total_time_on_Chow_Time', 'total_time_on_Costume_Box', \n                'total_time_on_Crystal_Caves___Level_1', 'total_time_on_Crystal_Caves___Level_2', 'total_time_on_Crystal_Caves___Level_3',\n                'total_time_on_Crystals_Rule', 'total_time_on_Dino_Dive', 'total_time_on_Dino_Drink', \n                'total_time_on_Egg_Dropper__Activity_', 'total_time_on_Fireworks__Activity_', 'total_time_on_Flower_Waterer__Activity_',\n                'total_time_on_Happy_Camel', 'total_time_on_Heavy__Heavier__Heaviest', 'total_time_on_Honey_Cake',\n                'total_time_on_Leaf_Leader', 'total_time_on_Lifting_Heavy_Things', 'total_time_on_Magma_Peak___Level_1',\n                'total_time_on_Magma_Peak___Level_2', 'total_time_on_Mushroom_Sorter__Assessment_', 'total_time_on_Ordering_Spheres',\n                'total_time_on_Pan_Balance', 'total_time_on_Pirate_s_Tale', 'total_time_on_Rulers',\n                'total_time_on_Sandcastle_Builder__Activity_', 'total_time_on_Scrub_A_Dub', 'total_time_on_Slop_Problem', \n                'total_time_on_Treasure_Map', 'total_time_on_Tree_Top_City___Level_1', 'total_time_on_Tree_Top_City___Level_2', \n                'total_time_on_Tree_Top_City___Level_3', 'total_time_on_Watering_Hole__Activity_',\n                'Ordering_Spheres_Checkpoint_1', 'Ordering_Spheres_Checkpoint_2', 'Costume_Box_Checkpoint_1',\n                'Costume_Box_Checkpoint_2', '12_Monkeys_Checkpoint_1', '12_Monkeys_Checkpoint_2', 'Tree_Top_City___Level_2_Checkpoint_1',\n                'Pirate_s_Tale_Checkpoint_1', 'Pirate_s_Tale_Checkpoint_2', 'Pirate_s_Tale_Checkpoint_3',\n                'Treasure_Map_Checkpoint_1', 'Treasure_Map_Checkpoint_2', 'Treasure_Map_Checkpoint_3',\n                'Tree_Top_City___Level_3_Checkpoint_1', 'Rulers_Checkpoint_1', 'Rulers_Checkpoint_2', 'Balancing_Act_Checkpoint_1',\n                'Balancing_Act_Checkpoint_2', 'Crystal_Caves___Level_2_Checkpoint_1', 'Crystal_Caves___Level_2_Checkpoint_2',\n                'Crystal_Caves___Level_2_Checkpoint_3', 'Crystal_Caves___Level_3_Checkpoint_1', 'Lifting_Heavy_Things_Checkpoint_1',\n                'Lifting_Heavy_Things_Checkpoint_2', 'Lifting_Heavy_Things_Checkpoint_3', 'Lifting_Heavy_Things_Checkpoint_4',\n                'Honey_Cake_Checkpoint_1', 'Honey_Cake_Checkpoint_2', 'Heavy__Heavier__Heaviest_Checkpoint_1',\n                'Magma_Peak___Level_2_Checkpoint_1', 'Slop_Problem_Checkpoint_1', 'Slop_Problem_Checkpoint_2', \n                'Slop_Problem_Checkpoint_3', 'Bird_Measurer__Assessment__N_correct_tries', 'Bird_Measurer__Assessment__N_incorrect_tries',\n                'Bird_Measurer__Assessment__N_assessment', 'Cart_Balancer__Assessment__N_correct_tries', \n                'Cart_Balancer__Assessment__N_incorrect_tries', 'Cart_Balancer__Assessment__N_assessment', \n                'Cauldron_Filler__Assessment__N_correct_tries', 'Cauldron_Filler__Assessment__N_incorrect_tries', \n                'Cauldron_Filler__Assessment__N_assessment', 'Chest_Sorter__Assessment__N_correct_tries', \n                'Chest_Sorter__Assessment__N_incorrect_tries', 'Chest_Sorter__Assessment__N_assessment', \n                'Mushroom_Sorter__Assessment__N_correct_tries', 'Mushroom_Sorter__Assessment__N_incorrect_tries',\n                'Mushroom_Sorter__Assessment__N_assessment', \n                'TotalHeightOfSandcastles',\n                'SandcastlesBuilt', 'SmallSandcastlesBuilt', 'MediumSandcastlesBuilt', 'BigSandcastlesBuilt',\n                'CloudsRained', 'SmallCloudSizes', 'MediumCloudSizes', 'BigCloudSizes', \n                'FireworksLaunched', 'FlowersCollected', 'BottlesFilled', \n                'EggsDropped', 'DinoScaleChanges', 'TotalBuglengthMeasured',\n                'TotalBugMeasurements', 'MaximumBuglengthMeasured', 'PointsPerCorrectContainers', 'MaximumCorrectNumberOfContainers',\n                'NCorrectlyEstimatedContainers', 'NIncorrectlyEstimatedContainers', 'TotalBathEstimates', \n                'RatioOfCorrectBathEstimates', 'ChickensOnSeesaw', 'PigsOnSeesaw', 'CorrectAirShows', 'BadAirShowBetweenCliff',\n                'BadAirShowBetweenTree', 'BadAirShowOver', 'BadAirShowUnder', 'IncorrectAirShows', 'MaxCrystalFinishedRound',\n                'CrystalsMeasuredCorrectly', 'CrystalsMeasuredIncorrectly', 'TotalCrystalMeasurements', \n                'RatioOfCorrectCrystalMeasurements', 'MaxScrubADubRound', 'MaxScrubADubLevel', 'CorrectScrubADubChoices', \n                'IncorrectScrubADubChoices', 'CumulativeAbsoluteScrubADubError', 'CumulativeSquaredScrubADubError',\n                'TotalScrubADubChoices', 'ScrubADubMAE', 'ScrubADubMSE', 'RatioOfCorrectScrubADubChoices', 'MaxDinoDrinkRound',\n                'CorrectDinoDrinks', 'IncorrectDinoDrinks', 'CumulativeAbsoluteDinoDrinkError', 'CumulativeSquaredDinoDrinkError',\n                'TooBigDinoDrinkErrors', 'TooLittleDinoDrinkErrors', 'TotalDinoDrinks', 'DinoDrinksMAE', 'DinoDrinksMSE',\n                'RatioOfCorrectDinoDrinks', 'MaxPanBalanceRound', 'CorrectPanBalances', 'IncorrectPanBalances', \n                'CumulativeAbsolutePanBalanceError', 'CumulativeSquaredPanBalanceError', 'TotalPanBalances', 'PanBalanceMAE', \n                'PanBalanceMSE', 'RatioOfCorrectPanBalances', 'MaxCamelRound', 'TotalCorrectCamelGuesses', 'TotalIncorrectCamelGuesses',\n                'TotalCamelBowlMoves', 'TotalCamelBowlsMovedToScale', 'TotalCamelNoInfoMoves', 'BadCamelMeasurements',\n                'GoodCamelMeasurements', 'PerfectCamelMeasurements', 'RatioOfBadCamelMeasurements', \n                'RatioOfGoodCamelMeasurements', 'RatioOfPerfectCamelMeasurements', 'MaxAllStarsRound', 'CorrectDinoHouses', \n                'IncorrectDinoHouses', 'CumulativeAbsoluteAllStarsError', 'CumulativeSquaredAllStarsError', 'TooBigAllStarsErrors',\n                'TooLittleAllStarsErrors', 'TotalDinoHouses', 'AllStarsMAE', 'AllStarsMSE', 'RatioOfCorrectDinoHouses', \n                'MaxDinoDiveRound', 'CorrectDinoDives', 'CorrectCumulativeDinoDiveSizes', 'IncorrectDinoDives',\n                'IncorrectCumulativeDinoDiveSizes', 'TotalDinoDives', 'RatioOfCorrectDinoDivesAgainstAll', \n                'RatioOfIncorrectDinoDivesAgainstCorrect', 'TotalLeafLeaderRounds', 'MaxLeafLeaderRound',\n                'LeafLeaderAdditionsPerRound', 'LeafLeaderRemovalsPerRound', 'LeafLeaderModesPerRound',\n                'LeafLeaderAvgDinoCount', 'MaxRoundNumber', 'N_rounds', 'TooLowErrors', 'TooHighErrors',\n                'TooMuchChewingRatio', 'TooFewChewingRatio', \n                'TimeSpentOnGames', 'TimeSpentOnActivities', 'TimeSpentInCrystalCaves', \n                'TimeSpentOnAssessments', 'TimeSpentOnClips', 'TimeSpentInTreetopCity', 'TimeSpentInMagmaPeak', \n                'Mushroom_Sorter__Assessment__PreviousAccuracyGroup', \n                'Bird_Measurer__Assessment__PreviousAccuracyGroup', 'Cauldron_Filler__Assessment__PreviousAccuracyGroup',\n                'Cart_Balancer__Assessment__PreviousAccuracyGroup', 'Chest_Sorter__Assessment__PreviousAccuracyGroup',\n                'Mushroom_Sorter__Assessment__BestAccuracyGroup', 'Bird_Measurer__Assessment__BestAccuracyGroup', \n                'Cauldron_Filler__Assessment__BestAccuracyGroup', 'Cart_Balancer__Assessment__BestAccuracyGroup', \n                'Chest_Sorter__Assessment__BestAccuracyGroup', 'Mushroom_Sorter__Assessment__AvgAccuracyGroup', \n                'Mushroom_Sorter__Assessment__StdAccuracyGroup', 'Bird_Measurer__Assessment__AvgAccuracyGroup', \n                'Bird_Measurer__Assessment__StdAccuracyGroup', 'Cauldron_Filler__Assessment__AvgAccuracyGroup', \n                'Cauldron_Filler__Assessment__StdAccuracyGroup', 'Cart_Balancer__Assessment__AvgAccuracyGroup',\n                'Cart_Balancer__Assessment__StdAccuracyGroup', 'Chest_Sorter__Assessment__AvgAccuracyGroup', \n                'Chest_Sorter__Assessment__StdAccuracyGroup', 'TotalAirShows', 'RatioOfCorrectAirShows']","7af2670a":"def reduce_classification_predictions(arr):\n    arr = np.round(arr)\n    new_arr = copy.deepcopy(arr[:, 0])\n    mask = (arr[:, 0] > 0.5) & (arr[:, 1] > 0.5)\n    new_arr += mask\n    mask = (arr[:, 0] > 0.5) & (arr[:, 1] > 0.5) & (arr[:, 2] > 0.5)\n    new_arr += mask\n    return new_arr","b43634a8":"lgbm_params_1 = {'num_leaves': 1000,\n          \"max_depth\": 250,\n          'learning_rate': 0.0025,\n          'objective': 'regression',\n          'metric': 'rmse',\n          'subsample': 1.0,\n          #'subsample_freq': 2,\n          'feature_fraction': 0.3,\n          \"reg_alpha\": 0.3,\n          \"reg_lambda\": 0.8\n          }\n\nlgbm_params_2 = {'num_leaves': 100,\n          \"max_depth\": 50,\n          'learning_rate': 0.023,\n          'objective': 'regression',\n          \"boosting\": \"gbdt\",\n          \"max_bin\": 50,\n          'metric': 'rmse',\n          'subsample': 0.05,\n          'subsample_freq': 5,\n          'feature_fraction': 0.025,\n          \"reg_alpha\": 12.0,\n          \"reg_lambda\": 4.0,\n          \"min_data_in_leaf\":10\n          }","693c7248":"n_repetitions = 2\nn_train_splits = 5\nn_test_splits = 6\ntest_single_select = 1\ntrain_single_select = 1\n\nverbose = 500\n\n# feature_importance = pd.DataFrame()\nthresholds_1 = list()\nthresholds_2 = list()\nthresholds_3 = list()\nthresholds_4 = list()\n\nsubmissions = copy.deepcopy(all_test_ids).drop_duplicates()\n\ntest_ShuffledGroupKFold = ShuffledGroupKFold(groupby=\"installation_id\", n_splits=n_test_splits, single_select=test_single_select)\ntest_ShuffledGroupKFold.generate_splits(test_ids.index.tolist())\ntrain_ShuffledGroupKFold = ShuffledGroupKFold(groupby=\"installation_id\", n_splits=n_train_splits, single_select=train_single_select)\ntrain_ShuffledGroupKFold.generate_splits(train_ids.index.tolist())\nfor i_repetition in range(n_repetitions):\n    print(f\"Repetition number {i_repetition}\")\n    print(\"=\"*20)\n    X_repetition_round_labels_1 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n    X_repetition_round_labels_1[f\"{i_repetition}th-Repetition-Predictions\"] = None\n    X_repetition_round_labels_2 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n    X_repetition_round_labels_2[f\"{i_repetition}th-Repetition-Predictions\"] = None\n    X_repetition_round_labels_3= copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n    X_repetition_round_labels_3[f\"{i_repetition}th-Repetition-Predictions\"] = None\n    X_repetition_round_labels_4 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n    X_repetition_round_labels_4[f\"{i_repetition}th-Repetition-Predictions\"] = None\n    X_repetition_round_labels_5 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n    X_repetition_round_labels_5[f\"{i_repetition}th-Repetition-Predictions\"] = None\n    for i_train, (train_train_ids, train_validation_ids) in enumerate(train_ShuffledGroupKFold.get_splits()):\n        print(f\"Training fold number {i_train}\")\n        print(\"-\"*20)\n        X_train_round_labels_1 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n        X_train_round_labels_1[f\"{i_train}th-Train-Predictions\"] = None\n        X_train_round_labels_2 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n        X_train_round_labels_2[f\"{i_train}th-Train-Predictions\"] = None\n        X_train_round_labels_3= copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n        X_train_round_labels_3[f\"{i_train}th-Train-Predictions\"] = None\n        X_train_round_labels_4 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n        X_train_round_labels_4[f\"{i_train}th-Train-Predictions\"] = None\n        X_train_round_labels_5 = copy.deepcopy(all_test_ids).set_index(\"installation_id\")\n        X_train_round_labels_5[f\"{i_train}th-Train-Predictions\"] = None\n        for i_test, (test_train_ids, test_test_ids) in enumerate(test_ShuffledGroupKFold.get_splits()):\n            print(f\"Test fold number {i_test}\")\n            print(\".\"*20)\n            train_train_samples = train_train[train_train[\"installation_id\"].isin(train_train_ids)]\n            test_train_samples = test_train[test_train[\"installation_id\"].isin(test_train_ids)]\n            train_samples = pd.concat((train_train_samples, test_train_samples), ignore_index=True)\n            valid_samples = train_train[train_train[\"installation_id\"].isin(train_validation_ids)]\n            test_samples = test_test[test_test[\"installation_id\"].isin(test_test_ids)]\n            X_train_orig = train_samples[data_columns].replace([np.inf, -np.inf],0)\n            y_train_true_1 = train_samples[\"accuracy_group\"]\n            y_train_true_2 = train_samples[[\"AG01\", \"AG12\", \"AG23\"]].values\n            X_valid_orig = valid_samples[data_columns].replace([np.inf, -np.inf],0)\n            y_valid_true_1 = valid_samples[\"accuracy_group\"]\n            y_valid_true_2 = valid_samples[[\"AG01\", \"AG12\", \"AG23\"]].values\n            X_test_orig = test_samples[data_columns].replace([np.inf, -np.inf],0)\n            X_test_round_labels_1 = copy.deepcopy(test_samples[[\"installation_id\"]]).set_index(\"installation_id\")\n            X_test_round_labels_2 = copy.deepcopy(test_samples[[\"installation_id\"]]).set_index(\"installation_id\")\n            X_test_round_labels_3 = copy.deepcopy(test_samples[[\"installation_id\"]]).set_index(\"installation_id\")\n            X_test_round_labels_4 = copy.deepcopy(test_samples[[\"installation_id\"]]).set_index(\"installation_id\")\n            X_test_round_labels_5 = copy.deepcopy(test_samples[[\"installation_id\"]]).set_index(\"installation_id\")\n            # Model 1 #\n            X_train, input_lengths, scaler, feature_lengths = prepare_for_predictions(X_train_orig)\n#             print(input_lengths)\n            X_valid, _, _, _ = prepare_for_predictions(X_valid_orig, scaler=scaler)\n            X_test, _, _, _ = prepare_for_predictions(X_test_orig, scaler=scaler)\n            # Define model\n            checkpoint = ModelCheckpoint('ModelCheckpoints1.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n            early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n            callbacks_list = [checkpoint, early_stopping]  \n            model_1 = define_advanced_model()   \n            # Learn\n            model_1.fit(split_to_inputs(X_train, input_lengths), [y_train_true_1, y_train_true_2], \n                      epochs=30, verbose=0, callbacks=callbacks_list,\n                      validation_data=(split_to_inputs(X_valid, input_lengths), [y_valid_true_1, y_valid_true_2]))\n            loss = np.min(model_1.history.history[\"val_loss\"])    \n            # Predict\n            y_valid_pred_1, y_valid_pred_2 = model_1.predict(split_to_inputs(X_valid, input_lengths))\n            y_valid_pred_1 = y_valid_pred_1.flatten()\n            y_valid_pred_2 = y_valid_pred_2\n            threshold_optimizer_1 = OptimizedRounder()\n            threshold_optimizer_1.fit(y_valid_pred_1, y_valid_true_1.values)\n            thresholds_1.append(threshold_optimizer_1.coefficients())\n            y_valid_pred_1 = np.round(np.clip(y_valid_pred_1, 0.0, 3.0))\n            y_valid_pred_2 = reduce_classification_predictions(y_valid_pred_2)\n            y_valid_true_2 = reduce_classification_predictions(y_valid_true_2)\n            y_test_pred_1, y_test_pred_2 = model_1.predict(split_to_inputs(X_test, input_lengths))\n            y_test_pred_1 = y_test_pred_1.flatten()\n            y_test_pred_2 = y_test_pred_2\n            y_test_pred_1 = np.clip(y_test_pred_1, -1.0, 4.0)\n            y_test_pred_2 = reduce_classification_predictions(y_test_pred_2)\n            # Save test predictions\n            X_test_round_labels_1[f\"{i_train}th-Train-Predictions\"] = y_test_pred_1\n            X_train_round_labels_1[f\"{i_train}th-Train-Predictions\"] = X_train_round_labels_1[f\"{i_train}th-Train-Predictions\"].combine_first(X_test_round_labels_1[f\"{i_train}th-Train-Predictions\"])\n            X_test_round_labels_2[f\"{i_train}th-Train-Predictions\"] = y_test_pred_2\n            X_train_round_labels_2[f\"{i_train}th-Train-Predictions\"] = X_train_round_labels_2[f\"{i_train}th-Train-Predictions\"].combine_first(X_test_round_labels_2[f\"{i_train}th-Train-Predictions\"])\n            del model_1\n            gc.collect()\n            # Model 2 #\n            scaler = StandardScaler()\n            scaler.fit(X_train_orig)\n            X_train = scaler.transform(X_train_orig)\n            X_valid = scaler.transform(X_valid_orig)\n            X_test = scaler.transform(X_test_orig)\n            # Define model\n            checkpoint = ModelCheckpoint('ModelCheckpoints2.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n            early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n            callbacks_list = [checkpoint, early_stopping]\n            model_2 = define_model()     \n            model_2.fit(X_train, y_train_true_1, \n                      epochs=30, verbose=0, callbacks=callbacks_list,\n                      validation_data=(X_valid, y_valid_true_1))\n            loss = np.min(model_2.history.history[\"val_loss\"])     \n            # Predict\n            y_valid_pred = model_2.predict(X_valid).flatten() \n            threshold_optimizer_2 = OptimizedRounder()\n            threshold_optimizer_2.fit(y_valid_pred, y_valid_true_1.values)\n            thresholds_2.append(threshold_optimizer_2.coefficients())\n            y_valid_pred = np.round(np.clip(y_valid_pred, 0.0, 3.0))\n            y_test_pred_3 = model_2.predict(X_test)\n            y_test_pred_3 = np.clip(y_test_pred_3, -1.0, 4.0)\n#             y_test_pred = np.random.randint(0, 3, X_test.shape[0])\n            # Save test predictions\n            X_test_round_labels_3[f\"{i_train}th-Train-Predictions\"] = y_test_pred_3\n            X_train_round_labels_3[f\"{i_train}th-Train-Predictions\"] = X_train_round_labels_3[f\"{i_train}th-Train-Predictions\"].combine_first(X_test_round_labels_3[f\"{i_train}th-Train-Predictions\"])         \n            del model_2\n            gc.collect()\n            # Model 3 #\n# Define model and learn\n            categorical_features = [\"AssessmentLabel\", \"WorldLabel\"]\n            dtrain = lgb.Dataset(X_train_orig, label=y_train_true_1, categorical_feature=categorical_features)\n            dvalid = lgb.Dataset(X_valid_orig, label=y_valid_true_1, categorical_feature=categorical_features)\n            model_3 = lgb.train(lgbm_params_1, dtrain, 5000, valid_sets = [dtrain, dvalid], \n                              verbose_eval=verbose, early_stopping_rounds=250)\n            # Predict\n            y_valid_pred = model_3.predict(X_valid_orig, num_iteration=model_3.best_iteration)   \n            threshold_optimizer_3 = OptimizedRounder()\n            threshold_optimizer_3.fit(y_valid_pred, y_valid_true_1.values)\n            thresholds_3.append(threshold_optimizer_3.coefficients())\n            y_valid_pred = np.round(np.clip(y_valid_pred, 0.0, 3.0))\n            y_test_pred_4 = model_3.predict(X_test_orig, num_iteration=model_3.best_iteration)   \n            y_test_pred_4 = np.clip(y_test_pred_4, -1.0, 4.0)\n            X_test_round_labels_4[f\"{i_train}th-Train-Predictions\"] = y_test_pred_4\n            X_train_round_labels_4[f\"{i_train}th-Train-Predictions\"] = X_train_round_labels_4[f\"{i_train}th-Train-Predictions\"].combine_first(X_test_round_labels_4[f\"{i_train}th-Train-Predictions\"])            \n            del model_3\n            gc.collect()\n            # Model 4 #\n            categorical_features = [\"AssessmentLabel\", \"WorldLabel\"]\n            dtrain = lgb.Dataset(X_train_orig, label=y_train_true_1, categorical_feature=categorical_features)\n            dvalid = lgb.Dataset(X_valid_orig, label=y_valid_true_1, categorical_feature=categorical_features)\n            model_4 = lgb.train(lgbm_params_2, dtrain, 10000, valid_sets = [dtrain, dvalid], \n                              verbose_eval=verbose, early_stopping_rounds=3000)\n            # Predict\n            y_valid_pred = model_4.predict(X_valid_orig, num_iteration=model_4.best_iteration)   \n            threshold_optimizer_4 = OptimizedRounder()\n            threshold_optimizer_4.fit(y_valid_pred, y_valid_true_1.values)\n            thresholds_4.append(threshold_optimizer_4.coefficients())\n            y_valid_pred = np.round(np.clip(y_valid_pred, 0.0, 3.0))\n            y_test_pred_5 = model_4.predict(X_test_orig, num_iteration=model_4.best_iteration)   \n            y_test_pred_5 = np.clip(y_test_pred_5, -1.0, 4.0)\n            # Save test predictions\n            X_test_round_labels_5[f\"{i_train}th-Train-Predictions\"] = y_test_pred_5\n            X_train_round_labels_5[f\"{i_train}th-Train-Predictions\"] = X_train_round_labels_5[f\"{i_train}th-Train-Predictions\"].combine_first(X_test_round_labels_5[f\"{i_train}th-Train-Predictions\"])            \n            del model_4\n            gc.collect()\n            print()\n        print()\n        print()\n    X_repetition_round_labels_1[f\"{i_repetition}th-Repetition-Predictions\"] = np.mean(X_train_round_labels_1, axis=1)\n    X_repetition_round_labels_2[f\"{i_repetition}th-Repetition-Predictions\"] = np.median(X_train_round_labels_2, axis=1)\n    X_repetition_round_labels_3[f\"{i_repetition}th-Repetition-Predictions\"] = np.mean(X_train_round_labels_3, axis=1)\n    X_repetition_round_labels_4[f\"{i_repetition}th-Repetition-Predictions\"] = np.mean(X_train_round_labels_4, axis=1)\n    X_repetition_round_labels_5[f\"{i_repetition}th-Repetition-Predictions\"] = np.mean(X_train_round_labels_5, axis=1)\n    print()\n    print()\n    print()\nthresholds_1 = np.array(thresholds_1).mean(axis=0)\nthresholds_2 = np.array(thresholds_2).mean(axis=0)\nthresholds_3 = np.array(thresholds_3).mean(axis=0)\nthresholds_4= np.array(thresholds_4).mean(axis=0)\nsubmissions[\"accuracy_group_1\"] = np.array(pd.cut(np.mean(X_repetition_round_labels_1, axis=1).values, [-np.inf] + list(np.sort(thresholds_1)) + [np.inf], labels = [0, 1, 2, 3]))\nsubmissions[\"accuracy_group_2\"] = np.median(X_repetition_round_labels_2.values, axis=1).astype(int)\nsubmissions[\"accuracy_group_3\"] = np.array(pd.cut(np.mean(X_repetition_round_labels_3, axis=1).values, [-np.inf] + list(np.sort(thresholds_2)) + [np.inf], labels = [0, 1, 2, 3]))\nsubmissions[\"accuracy_group_4\"] = np.array(pd.cut(np.mean(X_repetition_round_labels_4, axis=1).values, [-np.inf] + list(np.sort(thresholds_3)) + [np.inf], labels = [0, 1, 2, 3]))\nsubmissions[\"accuracy_group_5\"] = np.array(pd.cut(np.mean(X_repetition_round_labels_5, axis=1).values, [-np.inf] + list(np.sort(thresholds_4)) + [np.inf], labels = [0, 1, 2, 3]))\nsubmissions[\"accuracy_group\"] = np.median(submissions[[\"accuracy_group_1\", \"accuracy_group_2\", \"accuracy_group_3\", \"accuracy_group_4\", \"accuracy_group_5\"]].values, axis=1).astype(int)","43d6fd18":"final_submission = pd.merge(sample_submission[[\"installation_id\"]], submissions, how=\"left\", on=\"installation_id\", \n                            suffixes=('', '_2')).drop_duplicates(subset=(\"installation_id\"))\nfinal_submission[\"accuracy_group\"] = final_submission[\"accuracy_group\"].fillna(0).astype(int)\nfinal_submission = final_submission[[\"installation_id\", \"accuracy_group\"]].sort_values(by=\"installation_id\")\nfinal_submission.to_csv(\"submission.csv\", index=False) ","dfb8a2c0":"## Read data","c73e74d2":"## Learning and predicting","71aa01aa":"# Competition","6913d03e":"# Strategy\n\nIn this competition I wanted to try to use only experience to predict the score of children. Experience in the sense of computer games like e.g. in The Bindings of Isaac, where for the first time you play, you loose almost immediately and it takes a lot of iterations to advance to the next level and finally win the game. In the same sense, children here spend time watching videos, playing sandbox games or minigames and earn experience about height, length, volume and weight. ","b7b2dc67":"## Parameters","c7151097":"# Imports and settings","23f7ee83":"### Models\n\nAt first I started with a neural network and used all data without event data (because I didn't have them yet). It did not work well, so I switched to LightGBM which worked better. After I engineered the event data, I tried again neural network and it worked even better than LightGBM. \n\nIn the end I ended using four models, one with two outputs, so in total with five outputs. During CV, all four models were trained on same samples and did validation on same samples as well, so out of every repetition I got not one, but five outputs and I used median to reduce them to a single prediction.\n\nModels were:\n- LightGBM with highest public LB (0.525), not shallow but not deep\n- LightGBM regularized as much as it made sense (public LB 0.505)\n- Basic feedforward NN with all features standardly scaled as input (highest public LB 0.535, but that was just luck, average is much lower)\n- Complex NN with data divided by World and type of data (time, event, cumulative assessment data) with basic feedforward NN as head and two outputs - regression and ordinal classification (I think best LB was 0.496)","3dfa231f":"### Ordinal learning\n\nIn this competition people started to tackle this problem as classification. However, then it was shown that regression works better, so I guess most of the people switched to regression (including me). I also tried to find whether this problem has a name and it is apparently called ordinal learning or ordinal regression, i.e. predicted number are integer and they are ordered. I found out that one of basic tricks how to solve it is not to predict a category of a sample, but if a sample is at least in a category. So to belong to the category 2 (in this competition categories are 0, 1, 2 and 3) it has to predict 110, other examples in the figure below.","da845598":"# Model","f48b845a":"## Predictions","26651af3":"### Cross validation\n\nThis brings us to the cross validation. If I wanted to use even test data I had to ensure that an `installation_id` from the test set is not used later for prediction of the same `installation_id`. So I used 2D-Shuffled-Grouped-K-Fold validation, grouped by `installation_id`. How does it work is shown on the following image:\n\nFrom the train set, one fold is used for validation, all other folds are used for the training. From the test set I pick one fold and select its to-be-predicted samples - this fold is being predicted. From all other test folds I select training samples and they join training samples from the train set. On the figure there is 4x4 (4 train folds, 4 test folds) CV, so all models need to be trained on 16 different groups. In the end I used 5x6 so that the validation fold is not small and the model does not overfit due to low number of validation samples. I average solutions (using median for classification models and mean for regression models). This cross 4x4 validation is then repeated 3 times (bottom left corner) and I again average solutions (in the end everytime with median).","2a9d9f88":"## Data processing\n\nThe data we are given are one row per each event. There are multiple events in a single game session. Every child (`installation_id`) plays multiple games (have multiple different `game_session` IDs). As we are supposed only for assessments, game sessions needs to be filtered in the end, so that we have a single row for a single assessment.\n\nSo I have functions which process the data grouped into three groups:\n1. Funtions modifying full DataFrame\n2. Functions aggregating full DataFrame and returning data in IID-GID DataFrame (IID = `installation_id`, GID = `game_session`)\n3. Functions modifying aggregated DataFrame\n\nIn the first group there are functions like computing the actual time between events etc. The first function in the second group is `get_aggregated_base`, where I just get a skeleton of DataFrame with all IIDs and GIDs. I run functions from the second group and merge them with the base. In the third group I e.g. select samples based on whether they are for training or for testing.\n\nData processing in this competition needed to be done in chunks. It is not possible to load full data and work with them. Even if you somehow managed to, you wouldn't have enough memory for a model. But with functions divided in three groups like this, it was no problem. I just have to create a custom loading, so that installation IDs are not divided between chunks.\n\nPeople soon realized here, that they do not have to use only the last assessment for training, but they can use all assessments for training. It is called *Truncated CV* here. I figured out that we can also use all assessments even from the training set (except the last ones we are supposed to predict of course), so I used them. There is not many additional assessments in the public test set, but there is certainly more of them in the private test set. It is blind shot, but as there are only 21K samples to train on, I thought that any additional sample is worth using.","0dd2550b":"### Video checkpoints\n\nWhen I watched the videos, I realized that useful infomation are said only sometimes in the video, e.g. there may be a song, then someone says something informative, then the song continues. I watched videos again and marked times where an information was said. These became checkpoints for a video and I counted the number of times a children crossed a checkpoint.","be2bbe35":"### Events\n\nWith events I went through all of them and tried to use really only the ones that show how a player is experienced. There are events like \"click on the game in the main menu\", \"click somewhere where is no thing to click on\" etc. I think there may be some value by computing how many times a kid misses everything, but it seemed like low-importance feaure. So instead, I computed maximum round, how many rounds were played, how many levels, I computed MAE and MSE for things like putting a dinosaur to a house (sizes of dinosaurs and houses 1, 2, 3) so that I would know how big are mistakes kids usually make, I computed ratios of correct vs. incorrect etc.","ec86def2":"![OrdinalLearning%20%281%29.png](attachment:OrdinalLearning%20%281%29.png)","03e8df6e":"![Repeated2DGroupedKfoldCrossValidation.png](attachment:Repeated2DGroupedKfoldCrossValidation.png)","fbc5e72c":"# Functions","29c27b00":"# Results\n\nIt turns out that this notebook did not work very well (1277.  place) . Maybe I made an error somewhere, so if you find out, let me know, please. People from top used very similar features and similar models, so I don't really know why it did not work for me. ","9600fcfc":"## Features\n\n### Activity time\n\nAt first, I noticed that `game_time` is not really an useful feature. It is the timedelta from the last event or the time since the start of a game session. However, there is no ending event for the game session, so if you do not compute it yourself, you will not know how much time a kid actually spend doing the activity. \n\nSo I took videos first. I watched every video in the game and noted down how long it is. Later, when I already had these information, hosts provided length of each video, but nevermind... Videos are the most affected by `game_time` as they have only starting event with `game_time = 0`. The only problem here is that if you add up whole video time everytime you could just count event, but I coped with this with video checkpoints mentioned later and also if I would only count video events, I wouldn't know the total time spend watching videos.\n\nFor other activities I did it differently, because there are a lot of events and time between them is short. However, it may happen that a kid does not turn of the game and the time keeps ticking. I have seen this in many kernels, that they just summed up the time and then they count with this time as with time spend in a game. The easiest thing to do is to set up a threshold, i.e. maximum time between events is 10 minutes. When I did this I still found these times too high, so I made a statistics for all events for every activity (except videos) and looked at the distribution of times between events. It was different for every activity, but based on these histograms I clipped maximum times somewhere between 7 and 16 seconds."}}