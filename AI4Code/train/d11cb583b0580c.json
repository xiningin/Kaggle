{"cell_type":{"6667af9f":"code","9a4a68ea":"code","6538d7bb":"code","1b2830c2":"code","4361dbd8":"code","f6cea6b1":"code","60c5ba84":"code","ad49af9e":"code","b311e1e1":"code","0a8efab2":"code","95ac4729":"code","4b8e7b1b":"code","0fa60125":"code","fc83b9b0":"code","fef1d004":"code","d4a24590":"code","8c9ad877":"code","35b24e29":"code","bd362c3a":"code","893a0dca":"code","6088ab45":"code","f5629156":"code","84b18a35":"code","37134e6d":"code","cf704f87":"code","972cd33c":"code","226e2c9e":"code","696dea42":"code","74466fc1":"code","1e091ea1":"code","d4b47c42":"code","b090f649":"code","caeae224":"code","fedc0eb0":"code","bc134a50":"code","e54ddb22":"code","fc4534b7":"code","f3c530c1":"code","a4240197":"code","abec4920":"code","6ee2f30c":"code","5488fbfa":"code","50278a59":"code","de5c0ab8":"code","b933156f":"code","9ec998a7":"code","d83f0497":"code","80805d5b":"code","c58dba29":"code","f5bb9438":"code","d68e109a":"code","16845baf":"code","a8b19cc0":"code","8d8fd89c":"code","82916284":"code","679b4de3":"code","5ab727a3":"code","eaeea6fc":"code","b44778b2":"code","8c636f82":"code","aec49391":"code","f0d29f7d":"code","8100f81f":"code","96b1ebcc":"code","691b5fa9":"markdown","6a51a252":"markdown","d9059fbc":"markdown","661e3f80":"markdown","fd1c4f8c":"markdown","da4c02df":"markdown","2e44d671":"markdown","c4ac6739":"markdown","8edbc246":"markdown","072bdc6e":"markdown","6197fbd3":"markdown","84b928bb":"markdown","fac03a36":"markdown","6a449346":"markdown","d7d74b3f":"markdown","8d5602ee":"markdown","9387b9df":"markdown","e48aba83":"markdown","c87c1b9d":"markdown","fb9d4b25":"markdown","0c7cd06c":"markdown","946d5299":"markdown","07dd05e3":"markdown","24da58c3":"markdown","6eccb434":"markdown","75e3e68a":"markdown","242b0843":"markdown","9b30cde2":"markdown","fd305b8b":"markdown","033438b6":"markdown","0a26ac25":"markdown","f411c26b":"markdown","8308aa75":"markdown","68bb9b9c":"markdown","342b0102":"markdown","bb601036":"markdown","cc8099de":"markdown","d2ebbcdb":"markdown","7aabf6cf":"markdown","c670a8ac":"markdown","728afa93":"markdown","b90c7452":"markdown","77673c86":"markdown","fdd16e53":"markdown","25287a29":"markdown","755f382c":"markdown","fd6d52a0":"markdown","effe4c25":"markdown","790621aa":"markdown","c49e5f49":"markdown","d80f4978":"markdown","fa8b9a95":"markdown","3bbe1e54":"markdown","9318e986":"markdown","56867116":"markdown","4b118c60":"markdown","d613380c":"markdown","55039af3":"markdown","a0b6c0ad":"markdown","acd1921f":"markdown","3a07d58f":"markdown","f188b1e1":"markdown","7cfd82cc":"markdown","b2477f70":"markdown","cc955ecd":"markdown","f38a33da":"markdown","8c3b4076":"markdown","ca14b0ee":"markdown","230725be":"markdown","0b092c7d":"markdown","8aa4a071":"markdown","9740e14c":"markdown","6a9f8cf8":"markdown"},"source":{"6667af9f":"!pip install torchsummary\n!pip install gdown","9a4a68ea":"import os\nimport shutil\nimport json\nimport matplotlib.pyplot as plt\nimport random\nimport PIL.Image as Image\nimport numpy as np\nimport pandas as pd\nimport time\nimport shutil\nimport math\nimport pathlib\nimport sys\n\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\nfrom skimage.restoration import denoise_tv_chambolle\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torchsummary as summary\n\nimport tensorflow as tf\nimport keras\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Dropout, Flatten, Input\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.models import Model\nfrom keras.preprocessing.image import Iterator\nfrom keras.utils.np_utils import to_categorical\nimport keras.backend as K\n\nplt.ion()\nfrom IPython.display import clear_output","6538d7bb":"# For not Kaggle notebooks\n\n# # dataset can be found in https:\/\/www.kaggle.com\/sthabile\/noisy-and-rotated-scanned-documents\n# !gdown --id 1JitIKYYGZA69IYOuAf2E_GGH6oj5wNm8\n\n# try:\n#     os.mkdir('dataset')\n# except:\n#     pass\n# !unzip -qq \/content\/archive.zip -d dataset","1b2830c2":"try:\n    os.mkdir('dataset')\nexcept:\n    pass\n!cp ..\/input\/noisy-and-rotated-scanned-documents\/scan_doc_rotation -r .\/dataset\n\nroot = '.\/dataset\/scan_doc_rotation'         \nimage_paths = os.path.join(root, 'images')\nlabel_paths = os.path.join(root, 'labels')","4361dbd8":"test_list_path = os.path.join(root, 'test_list.json')   \n\nwith open(test_list_path) as f:\n    test_list = json.load(f)     # names of test images (test images dont have labels)","f6cea6b1":"train_list_path = os.path.join(root, 'train_list.json')\n\nwith open(train_list_path) as f:\n    train_list = json.load(f)   # names of train images \n\nlabel_list = [x.split('.')[0] + '.txt' for x in train_list]   # names of label text files.  ","60c5ba84":"def get_random_img(r=[0, 499]):\n    id = random.randint(r[0], r[1])\n    sample_image_path = os.path.join(image_paths, train_list[id])\n    sample_label_path = os.path.join(label_paths, label_list[id])\n    return id, sample_image_path, sample_label_path","ad49af9e":"def display_image(path=None, img=None):\n    if path is not None:\n        img = Image.open(path)\n\n    plt.figure(figsize=(8, 8))\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img, cmap='gray');","b311e1e1":"# generates random id and get it's image path and label path\nid, sample_image_path, sample_label_path = get_random_img()\n\n# displays image\ndisplay_image(path=sample_image_path)\n\n# writes id and angle of sample in output\nwith open(sample_label_path) as f:\n    angle = f.read()\nprint('ID of document is :', id)\nprint('Rotation angle is :', angle)","0a8efab2":"def display_image_grid(images, n=10, angles=None):\n    fig = plt.figure(figsize=(20, 20))\n    grid = ImageGrid(fig, 111,  \n                     nrows_ncols=(n, n),  \n                     axes_pad=0.25, \n                     )\n\n    i = 0\n    for ax, im in zip(grid, images):\n        ax.imshow(im, cmap='gray');\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if angles is not None:\n            angle = angles[i] - 5\n            ax.set_title(label=str(angle))\n        i += 1\n\n    plt.show()","95ac4729":"sampled_images = []\nsampled_ids = []\n\nfor i in range(16):\n    id, sample_image_path, sample_label_path = get_random_img()\n    if id not in sampled_ids:\n        img = Image.open(sample_image_path)\n        sampled_ids.append(id)\n        sampled_images.append(img)\n\ndisplay_image_grid(sampled_images, 4)","4b8e7b1b":"def denoise(img, to_PIL=True):\n    img = denoise_tv_chambolle(img, weight=0.5, multichannel=0)\n    if to_PIL:\n        Image.fromarray(img)\n    return img","0fa60125":"id, sample_image_path, sample_label_path = get_random_img()\nimg = Image.open(sample_image_path)\nimg = np.array(img)\n\nimg = denoise(img)\ndisplay_image(img=img)","fc83b9b0":"def fft(img, to_PIL=True):\n    f = cv2.dft(np.float32(img))    # Discrete Fourier transform\n    fshift = np.fft.fftshift(f)     # Shift the zero-frequency component to the center of the spectrum\n    f_abs = np.abs(fshift) + 1.0    # shift to ensure no zeroes are present in image array\n    f_img = 20 * np.log(f_abs)      # final result\n    if to_PIL:\n        return Image.fromarray(f_img)\n    return f_img","fef1d004":"id, sample_image_path, sample_label_path = get_random_img()\nimg = Image.open(sample_image_path)\nimg = np.array(img)\n# img = cv2.imread(sample_image_path)\n# img = img[:,:,0]\n\nf_img = fft(img)\ndisplay_image(img=f_img)","d4a24590":"def preprocess(img):\n    return fft(denoise(img))","8c9ad877":"id, sample_image_path, sample_label_path = get_random_img()\nimg = cv2.imread(sample_image_path)\nimg = img[:,:,0]\n\nf_img = preprocess(img)\ndisplay_image(img=img)\ndisplay_image(img=f_img)","35b24e29":"# ~5 minutes\nprep_image_paths = os.path.join(root, 'preprocessed images') \nos.mkdir(prep_image_paths)\n\nfor img_name in os.listdir(image_paths):\n    img_src_path = os.path.join(image_paths, img_name)\n    img = Image.open(img_src_path)\n    img = np.array(img)\n    img = preprocess(img)\n    \n    img = np.array(img)\n    img = Image.fromarray((img).astype(np.uint8))\n    img_dst_path = os.path.join(prep_image_paths, img_name)\n    img.save(img_dst_path)","bd362c3a":"labels = {}\nfor textfile in label_list:\n    path = os.path.join(label_paths, textfile)\n    with open(path) as f:\n        label = float(f.read())\n    file_name = textfile.split('.')[0]\n    labels[file_name] = round(label)","893a0dca":"labels['scan_000'], labels['scan_010'], labels['scan_499']","6088ab45":"df = pd.DataFrame(data={'name':labels.keys(), 'label':labels.values()})\ndf['categorical label'] = df['label'] + 5\ndf['categorical label'] = df['categorical label'].astype('int').astype('category')","f5629156":"df.head()","84b18a35":"df_train, df_val = train_test_split(df, test_size=0.2)\ndf_train.shape, df_val.shape","37134e6d":"class CustomDataset(Dataset):\n    def __init__(self, df, root_dir, transform=None, num_class=21):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): DataFrame of image names and labels. \n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = df\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_class = num_class\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_path = os.path.join(self.root_dir,\n                                self.df.iloc[idx, 0] + '.png')\n        image = Image.open(img_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        label = df.iloc[idx, 2]\n\n        return image, int(label)","cf704f87":"transform = transforms.Compose([transforms.Resize((128, 128)),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=0, std=1)\n])\n\ntrain_dataset = CustomDataset(df_train, prep_image_paths, transform)\nval_dataset = CustomDataset(df_val, prep_image_paths, transform)\n\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=64) \nval_loader = DataLoader(val_dataset, shuffle=True, batch_size=64)","972cd33c":"# based on RotNet Implementaion: https:\/\/github.com\/d4nst\/RotNet\n\nclass RotNet(nn.Module):\n    def __init__(self, num_classes=2, num_channels=1, in_channels=1, has_dropout=True, dropout_rate=0.25):\n        super(RotNet, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        \n        # Residual Blocks\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3))\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3))\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n        self.max_pool = nn.MaxPool2d((2, 2))\n        \n        # MLP\n        self.fc_1 = nn.Linear(12544, 128)\n        self.fc_2 = nn.Linear(128, self.num_classes)\n\n        self.ReLU = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.softmax = torch.nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.ReLU(x)\n        x = self.max_pool(x)\n        x = self.conv2(x)\n        x = self.ReLU(x)\n        x = self.max_pool(x)\n        x = self.conv3(x)\n        x = self.ReLU(x)\n        x = self.max_pool(x)\n        x = self.dropout(x)\n\n        x = x.view(x.shape[0], -1)\n        x = self.fc_1(x)\n        x = self.ReLU(x)\n        x = self.dropout(x)\n        \n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","226e2c9e":"model = RotNet(num_classes=11, has_dropout=True)\nmodel.to('cuda')\nsummary.summary(model, (1, 128, 128))","696dea42":"model = RotNet(num_classes=11).to('cuda')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)","74466fc1":"# Test input\nrandom_input = torch.rand((1, 1, 128, 128)).to('cuda')\noutput = model(random_input)\noutput, torch.sum(output)","1e091ea1":"trian_loss = {}\nval_loss = {}\n\nstart_time = time.time()\nfor epoch in range(50):\n    train_losses = []\n    val_losses = []\n    running_loss = 0\n    count = 0\n    for i, inp in enumerate(train_loader):\n        inputs = inp[0]\n        inputs = inputs.to('cuda')\n        labels = inp[1]\n        labels = labels.to('cuda')\n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        train_losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        count += 1    \n\n    if epoch%5 == 0:\n        for i, inp in enumerate(val_loader):\n            inputs = inp[0]\n            inputs = inputs.to('cuda')\n            labels = inp[1]\n            labels = labels.to('cuda')\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n\n        trian_loss[epoch] = np.mean(train_losses)\n        val_loss[epoch] = np.mean(val_losses)\n        print('Epcoh', epoch, ': train loss =', np.mean(train_losses), ', val loss =', np.mean(val_losses))\n\nend_time = time.time()\nprint('Training Done in', end_time - start_time, 'seconds')","d4b47c42":"plt.plot(list(trian_loss.keys()), list(trian_loss.values()))\nplt.plot(list(val_loss.keys()), list(val_loss.values()));","b090f649":"correct_train = 0\ntotal_train = 0\n\ncorrect_val = 0\ntotal_val = 0\n\nval_output = []\n\nwith torch.no_grad():\n    for data in train_loader:\n        tensor = data[0]\n        tensor = tensor.to('cuda')\n        label = data[1]\n        label = label.to('cuda')\n        outputs = model(tensor)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total_train += tensor.size(0)\n        correct_train += (predicted == label).sum().item()\n\n    for data in val_loader:\n        tensor = data[0]\n        tensor = tensor.to('cuda')\n        label = data[1]\n        label = label.to('cuda')\n        outputs = model(tensor)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        val_output.append(predicted)\n        total_val += tensor.size(0)\n        correct_val += (predicted == label).sum().item()\n\nprint('Accuracy on Train Data :', 100*(correct_train\/total_train), '%')\nprint('Accuracy on Validation Data :', 100*(correct_val\/total_val), '%')","caeae224":"test_df = pd.DataFrame(data={'image':list(map(lambda x:x.split('.')[0], test_list)), 'label':np.zeros(len(test_list), int)})\ndataset_test = CustomDataset(test_df, prep_image_paths, transform)\ntest_loader = DataLoader(dataset_test, batch_size=100)","fedc0eb0":"for i, inp in enumerate(test_loader):\n    inputs = inp[0]\n    inputs = inputs.to('cuda')\n    labels = inp[1]\n    labels = labels.to('cuda').float()\n    optimizer.zero_grad()\n    \n    outputs = model(inputs)\noutputs = outputs.cpu().detach().numpy()","bc134a50":"predicted_labels = np.argmax(outputs, axis=1)\n\ntest_images = []\nfor image_name in test_df['image'].values:\n    image_path = os.path.join(image_paths, image_name+'.png')\n    img = Image.open(image_path)\n    test_images.append(img)\n\ndisplay_image_grid(test_images, 10, predicted_labels)","e54ddb22":"# From RotNet Repo - https:\/\/github.com\/d4nst\/RotNet\ndef rotate(image, angle):\n    \"\"\"\n    Rotates an OpenCV 2 \/ NumPy image about it's centre by the given angle\n    (in degrees). The returned image will be large enough to hold the entire\n    new image, with a black background\n    Source: http:\/\/stackoverflow.com\/questions\/16702966\/rotate-image-and-crop-out-black-borders\n    \"\"\"\n    # Get the image size\n    # No that's not an error - NumPy stores image matricies backwards\n    image_size = (image.shape[1], image.shape[0])\n    image_center = tuple(np.array(image_size) \/ 2)\n\n    # Convert the OpenCV 3x2 rotation matrix to 3x3\n    rot_mat = np.vstack(\n        [cv2.getRotationMatrix2D(image_center, angle, 1.0), [0, 0, 1]]\n    )\n\n    rot_mat_notranslate = np.matrix(rot_mat[0:2, 0:2])\n\n    # Shorthand for below calcs\n    image_w2 = image_size[0] * 0.5\n    image_h2 = image_size[1] * 0.5\n\n    # Obtain the rotated coordinates of the image corners\n    rotated_coords = [\n        (np.array([-image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n        (np.array([ image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n        (np.array([-image_w2, -image_h2]) * rot_mat_notranslate).A[0],\n        (np.array([ image_w2, -image_h2]) * rot_mat_notranslate).A[0]\n    ]\n\n    # Find the size of the new image\n    x_coords = [pt[0] for pt in rotated_coords]\n    x_pos = [x for x in x_coords if x > 0]\n    x_neg = [x for x in x_coords if x < 0]\n\n    y_coords = [pt[1] for pt in rotated_coords]\n    y_pos = [y for y in y_coords if y > 0]\n    y_neg = [y for y in y_coords if y < 0]\n\n    right_bound = max(x_pos)\n    left_bound = min(x_neg)\n    top_bound = max(y_pos)\n    bot_bound = min(y_neg)\n\n    new_w = int(abs(right_bound - left_bound))\n    new_h = int(abs(top_bound - bot_bound))\n\n    # We require a translation matrix to keep the image centred\n    trans_mat = np.matrix([\n        [1, 0, int(new_w * 0.5 - image_w2)],\n        [0, 1, int(new_h * 0.5 - image_h2)],\n        [0, 0, 1]\n    ])\n\n    # Compute the tranform for the combined rotation and translation\n    affine_mat = (np.matrix(trans_mat) * np.matrix(rot_mat))[0:2, :]\n\n    # Apply the transform\n    result = cv2.warpAffine(\n        image,\n        affine_mat,\n        (new_w, new_h),\n        flags=cv2.INTER_LINEAR\n    )\n\n    return result","fc4534b7":"predicted_labels = np.argmax(outputs, axis=1)\n\ntest_images = []\nfor i, image_name in enumerate(test_df['image'].values):\n    image_path = os.path.join(image_paths, image_name+'.png')\n    img = Image.open(image_path)\n    img = np.array(img)\n    angle = predicted_labels[i]\n    img = rotate(img, - angle)\n    test_images.append(img)\n\ndisplay_image_grid(test_images, 10, predicted_labels)","f3c530c1":"sampled_images = []\nsampled_labels = []\nsampled_ids = []\n\nsampled_ids = random.sample(list(range(400)), 100)\nsampled_images = [train_dataset[i][0].squeeze().cpu().detach().numpy() for i in sampled_ids]\nsampled_labels = [train_dataset[i][1] for i in sampled_ids]\n\ndisplay_image_grid(sampled_images, 10, sampled_labels)","a4240197":"!git clone https:\/\/github.com\/d4nst\/RotNet.git","abec4920":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \".\/RotNet\/utils.py\", dst = \"..\/working\/my_functions.py\")\n\n# import all our functions\nfrom my_functions import *","6ee2f30c":"# Getting image names and labels and saving them in \"labels\" dictionary \nlabels = {}\nfor textfile in label_list:\n    path = os.path.join(label_paths, textfile)\n    with open(path) as f:\n        label = float(f.read())\n    file_name = textfile.split('.')[0]\n    labels[file_name] = label","5488fbfa":"# Creating new images by rotating them \nnew_images = []\nnew_labels = []\nfor image_id, angle in labels.items():\n    image_path = os.path.join(image_paths, image_id+'.png')\n    img = Image.open(image_path)                                # open image as PIL Image\n    img = np.array(img)                                         # convert PIL Image to np array\n    corrected_img = rotate(img, -angle)                         # rotate image to get corrected image\n    \n    sample_angles = random.sample(range(-45, 45), 50)           # sample 50 numbers between -45 to 45\n    for sample_angle in sample_angles:\n        new_img = generate_rotated_image(corrected_img,         # generate a valid rotated image based on sample_angle and\n                                         sample_angle,          #     it resize image to 64*64\n                                         size=(128, 128),\n                                         crop_center=True,\n                                         crop_largest_rect=True)\n        new_images.append(new_img)\n        new_labels.append(sample_angle)","50278a59":"# N is number of all images in dataset\nN = len(new_images)\n\n# Sampling 100 images to display\nsampled_image_indecies = random.sample(range(N), 100)\nsampled_images = [Image.fromarray(new_images[i]) for i in sampled_image_indecies]\nsampled_labels = [new_labels[i] for i in sampled_image_indecies]\n\n# Displays 10*10 grid of rotated images from new dataset. The number in above of each image, is rotated angle\ndisplay_image_grid(sampled_images, 10, sampled_labels)","de5c0ab8":"nb_classes = np.unique(new_labels).__len__()\ndata = np.stack(new_images, axis=0)                            # converting all images to a np array\n\nX = data.copy()\nY = to_categorical(np.array(new_labels) + 45, nb_classes)       # creating categorical labels\n\nX = X\/255                                                      # move image values to [0, 1] period \nX = np.where(X > 0.5, 1, 0)                                    # binarize images with threshold=0.5","b933156f":"# Sample\nsampled_image_indecies = random.sample(range(N), 100)\nsampled_images = [Image.fromarray((X[i]*255).astype(np.uint8)) for i in sampled_image_indecies]\nsampled_labels = [new_labels[i] for i in sampled_image_indecies]\n\ndisplay_image_grid(sampled_images, 10, sampled_labels)","9ec998a7":"# number of convolutional filters to use\nnb_filters = 64\n# size of pooling area for max pooling\npool_size = (2, 2)\n# convolution kernel size\nkernel_size = (3, 3)","d83f0497":"nb_train_samples, img_height, img_width = X.shape\nimg_channels = 1\ninput_shape = (img_height, img_width, img_channels)\n\nprint('Input shape:', input_shape)\nprint(nb_train_samples, 'train samples')","80805d5b":"# model definition\ninput = Input(shape=(img_height, img_width, img_channels))\nx = Conv2D(nb_filters, kernel_size, activation='relu')(input)\nx = Conv2D(nb_filters, kernel_size, activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(nb_filters, kernel_size, activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(nb_filters, kernel_size, activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(0.25)(x)\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.25)(x)\nx = Dense(nb_classes, activation='softmax')(x)\n\nmodel = Model(inputs=input, outputs=x)","c58dba29":"model.summary()","f5bb9438":"model.compile(loss='categorical_crossentropy',\n              optimizer= tf.keras.optimizers.Adam(lr=5e-4),\n              metrics=[angle_error, 'acc'])","d68e109a":"batch_size = 128\nnb_epoch = 30","16845baf":"class PlotLearning(keras.callbacks.Callback):\n    \"\"\"\n    Callback to plot the learning curves of the model during training.\n    \"\"\"\n    def on_train_begin(self, logs={}):\n        self.metrics = {}\n        for metric in logs:\n            self.metrics[metric] = []\n            \n\n    def on_epoch_end(self, epoch, logs={}):\n        # Storing metrics\n        for metric in logs:\n            if metric in self.metrics:\n                self.metrics[metric].append(logs.get(metric))\n            else:\n                self.metrics[metric] = [logs.get(metric)]\n        \n        # Plotting\n        metrics = [x for x in logs if 'val' not in x]\n        \n        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n        clear_output(wait=True)\n\n        for i, metric in enumerate(metrics):\n            axs[i].plot(range(1, epoch + 2), \n                        self.metrics[metric], \n                        label=metric)\n            if logs['val_' + metric]:\n                axs[i].plot(range(1, epoch + 2), \n                            self.metrics['val_' + metric], \n                            label='val_' + metric)\n                \n            axs[i].legend()\n            axs[i].grid()\n\n        plt.tight_layout()\n        plt.show()","a8b19cc0":"tensorboard = TensorBoard()\nplotlearning = PlotLearning()\nearly_stopping = EarlyStopping(patience=5)","8d8fd89c":"model.fit(\n    x = X,\n    y = Y,\n    batch_size=batch_size,\n    steps_per_epoch=nb_train_samples \/ batch_size,\n    epochs=nb_epoch,\n    validation_split=0.2,\n    verbose=1,\n    callbacks=[tensorboard, plotlearning, early_stopping]\n)","82916284":"# save image names in a dataframe\ntest_df = pd.DataFrame(data={'image':list(map(lambda x:x.split('.')[0], test_list)), 'label':np.zeros(len(test_list), int)})","679b4de3":"# get images and convert them to array and add to test_images list\ntest_images = []\nfor image_name in test_df['image'].values:\n    image_path = os.path.join(image_paths, image_name+'.png')\n    img = Image.open(image_path)\n    img = np.array(img)\n    new_img = generate_rotated_image(img,\n                                     0,\n                                     size=(128, 128),\n                                     crop_center=False,\n                                     crop_largest_rect=False)\n    test_images.append(new_img)","5ab727a3":"# preparing test data for model\nX_test = np.stack(test_images)\nX_test = X_test \/ 255\nX_test = np.where(X_test > 0.5, 1, 0)","eaeea6fc":"# display test images\ndisplay_image_grid(X_test, 10)","b44778b2":"# prediction with model\npredicted_angles = np.argmax(model(X_test), axis=1) - 45","8c636f82":"# correct image rotations\ncorrected_test_images = []\n\nfor i, image_name in enumerate(test_df['image'].values):\n    image_path = os.path.join(image_paths, image_name+'.png')\n    img = Image.open(image_path)\n    img = np.array(img)\n    angle = predicted_angles[i]\n    corrected_img = rotate(img, -angle)\n    corrected_test_images.append(corrected_img)","aec49391":"# display corrected test images\ndisplay_image_grid(corrected_test_images, 10)","f0d29f7d":"!gdown --id 1kCdMCugJA6V6w0CBbSL4N45noDUnlv_U","8100f81f":"def correct_image(model, img, threshold=0.6):\n    img_arr = np.array(img)\n    img_arr = generate_rotated_image(img_arr, 0, size=(128,128))\n    img_arr = img_arr\/255\n    img_arr = np.where(img_arr > threshold, 1, 0)\n    \n    predicted_angle = np.argmax(model(img_arr[np.newaxis, ...])) - 45\n    print(predicted_angle)\n    new_image = rotate(np.array(img), -predicted_angle)\n    return Image.fromarray(new_image)","96b1ebcc":"img = Image.open('.\/example.jpg').convert('L')\ncorrect_image(model, img)","691b5fa9":"We use a convolutional neural network named RotNet. It's designed to predict rotation angle (classification) in images.  ","6a51a252":"## Download Dataset","d9059fbc":"To investigate the model's performance, we use test documents (100 images), feed them to the model, and then find the rotated angles.","661e3f80":"### Single Image","fd1c4f8c":"### Lables","da4c02df":"We train this model with CrossEntropyLoss and Adam Optimizer with lr=1e-3 for 100 epochs.","2e44d671":"## Noisy and Rotated Scanned Documents Dataset","c4ac6739":"### train list","8edbc246":"## Model","072bdc6e":"It's necessary to convert the image to Tensor for the PyTorch model. So we use transforms.ToTensor() and transforms.Resize() for this purpose.","6197fbd3":"## Sample","84b928bb":"## Import Functions","fac03a36":"As you can see, almost half of the test images are deskewed (corrected). But in general, these images are challenging for the model to predict. Because the degree of rotation is [-5, 5]; But the model is trained on images rotated with a degree in [-45, 45].","6a449346":"## PyTorch Dataset & DataLoader","d7d74b3f":"## Model","8d5602ee":"## Preparing Paths of Dataset","9387b9df":"#### Corrected Test Imgaes","e48aba83":"### test list","c87c1b9d":"# Part 2","fb9d4b25":"## Building Better Dataset ","0c7cd06c":"For Training PyTorch models, we should create PyTorch datasets and DataLoader. ","946d5299":"# Part 1","07dd05e3":"# Other Example","24da58c3":"get_random_img() returns the id, the path of the image, and the path of the label of a random training image.","6eccb434":"**The results are terrible!** The model doesn't generalize well. There might be several reasons: <br>\n- The pre-processing (FFT) is not suitable for this kind of neural network. \n- The model is not suitable for this task.\n- We don't have enough data.\n- Angles are small, so detection of them is hard for the network.\n- Learning rate and other hyperparameters are not suitable for this model and data.\n- The model or data has problems (like dataset is imbalance).\n- Exploding \/ Vanishing gradients\n\nAlso, there are a lot of reasons for the NN model not to work well. You can check this [link](https:\/\/blog.slavv.com\/37-reasons-why-your-neural-network-is-not-working-4020854bd607). <br> <br>\n\nAll of the above reasons might be our problem. But, we think two factors are responsible for poor performance. The first one is size of the training dataset. As you notice, our training data has only 400 images, but our model has almost 800k parameters. So it was obvious that our model didn't work well. Solutions for this problem are: <br>\n- Gathering more data: it is hard. <br>\n- Data Augmentation: good idea! <br>\n- Using a smaller network: we tried this solution, but the model didn't work well. <br>\n\nThe second problem is data itself! Please look at the below section. In the below section, we display 100 training images.","75e3e68a":"For this part of the task, we use a TensorFlow model (very similar to RotNet) to learn rotation angle. ","242b0843":"In this part, we want to train a neural network to predict the angle of the rotated image. ","9b30cde2":"### TensorFlow Model","fd305b8b":"### CallBacks","033438b6":"In general, we can say the skew angle of documents (images) is slight. As mentioned in the dataset description, the angle is between -5 to 5 degrees. ","0a26ac25":"### Images","f411c26b":"In this part, we use \n<a href='https:\/\/github.com\/d4nst\/RotNet'>RotNet<\/a> repo and its functions to build new dataset and create a better model for rotation prediction.","8308aa75":"In the above, you can see one of the training data with id and rotation angle.","68bb9b9c":"### Training Parameters","342b0102":"In the below part, you can see test images with predicted angles as the image title (angles are above each image).","bb601036":"### Training","cc8099de":"**Not Bad!** But as you see, the model is a little bit overfitted! But in general, this model is better than the previous one.<br>\nAs you can see, the model generally learns how to predict rotation in images. We can do a lot of works to make the model better. For example, hyper-parameter tuning is a good option to improve the performance of the model. Also, we should add some regularization (adding another dropout layer to the network or applying other data augmentation methods) to the model. <br>\nAlso, we can change the network structure; For example, we can add more conv blocks, or we can change filter size, number of channels, ..., or changing the size of input images.","d2ebbcdb":"As you can notice, after denoising and applying FFT (fast Fourier transform) on the image, we get a better representation of the angle of the skewed text. So, we apply these two stages to all of the documents.  ","7aabf6cf":"#### FFT Example","c670a8ac":"**Obviously the model doesn't work well!**","728afa93":"As you can notice, in spite of diverse labels, **all of the images are almost the same**. So model can't learn enough to generalize. **Also, it seems that our preprocessing doesn't work well**. So in the next part, we use images without denoising and FFT.","b90c7452":"### Conclusion","77673c86":"#### Denoising Example","fdd16e53":"Note that the label of each image (only training images) is in a separate text file, and we need to read and process them. ","25287a29":"For denoising, we use \n<a href='https:\/\/scikit-image.org\/docs\/stable\/api\/skimage.restoration.html#skimage.restoration.denoise_tv_chambolle'>denoise_tv_chambolle()<\/a>\nfunction from skimage library.","755f382c":"**This notebook will investigate deskewing (rotating text images to correct skewness in text) methods using Neural Networks (especially Convolutional Neural Networks).**","fd6d52a0":"## Pre-Processing","effe4c25":"## Splitting Data ","790621aa":"## Evaluation","c49e5f49":"As you can see, the new labels are numbers between -5 to 5.","d80f4978":"Before splitting, we need to convert angles to categorical data. For convenience, the angles will map to 0, 1, 2, ..., 10. Also, we create a data frame to save image names and labels.","fa8b9a95":"### ImageGrid","3bbe1e54":"In this part, data is divided into train and validation sets. Note that we have a separate test set(without label).","9318e986":"### RotNet","56867116":"To create a better model, we want to create a new dataset. The new dataset is based on the previous one (Noisy and Rotated Scanned Documents). We deskew(correct rotation) images of the dataset and rotate them between (-45, 45) degrees. In other words, We are doing data augmentation by rotating images.","4b118c60":"First, we need to install and import the required libraries.","d613380c":"# Prelims","55039af3":"We perform the \n<a href='https:\/\/en.wikipedia.org\/wiki\/Fast_Fourier_transform'>fast fourier transform<\/a>\nto get the axis of text in the image. For more information, read these two: <br>\n<a href='https:\/\/indico.cern.ch\/event\/652131\/contributions\/2654301\/attachments\/1489307\/2314223\/Nicoloff_-_Skew_Detection_with_Fast_Fourier_Transform.pdf'>Skew Detection Using Fast Fourier Transform<\/a> <br>\n<a href='https:\/\/www.l3harrisgeospatial.com\/docs\/backgroundfastfouriertransform.html'>Fast Fourier Transform (FFT) Background<\/a>","a0b6c0ad":"### Kaggle","acd1921f":"### Model Fitting","3a07d58f":"There are several text image datasets. But most of them are not proper for this task (as in the task description mentioned, \"the lines are nearly aligned with each other\"). The \n<a href='https:\/\/www.kaggle.com\/sthabile\/noisy-and-rotated-scanned-documents'>Noisy and Rotated Scanned Documents Dataset<\/a> was proper for this project. So we chose this dataset for this project. The labels of this dataset are the rotation angle of skewed images. So we don't need to rotate images for generating labels.","f188b1e1":"### Evaluation on Test images","7cfd82cc":"display_image() function gets an image or path of the image and then displays it.","b2477f70":"## Preprocessing","cc955ecd":"Now our data is ready to be fed to the model. ","f38a33da":"In this part, we want to investigate images of the dataset and be ready.","8c3b4076":"Like RotNet, we binarize datset's images. The threshold is 0.5. ","ca14b0ee":"The range of angles in this dataset is [-5, 5], and the angle value is a float number. There are two approaches to predict the rotation angle of skewed image:\n- classification\n- regression <br>\n\n\"In my experiments, the network trained as a classifier worked much better than the regression one ...\" [[link]](https:\/\/d4nst.github.io\/2017\/01\/12\/image-orientation\/) <br>\nBased on the above article(about RotNet) and also for convenience, we consider our problem as an 11-class classification and divide this period into 11 classes (-5, -4, ..., 4, 5).","230725be":"To use the dataset, we need to download it. The main dataset is in Kaggle, and you can download it directly. For convenience, I uploaded the dataset to my Google Drive and then used the id of the file to download it. <br>\nNote that somewhere in the code, I commented some lines because of the difference between the file system of Colab notebook and Kaggle notebooks. So if you run this notebook on one of them, please consider it.","0b092c7d":"There is a method to detect skewness in the text images based on the Fast Fourier transform. In this method, the fast Fourier transform of each skewed image is given to the neural network as input. But for getting better results in fast Fourier transform, we need to denoise the image. So the overall procedure is: <br>\n1 - Denoising <br>\n2 - Fast Fourier Transform <br>\n3 - Neural Network <br>\nIn the following sections, we will provide denoising and FFT functions with examples.","8aa4a071":"#### Rotated Test Images","9740e14c":"#### Combine","6a9f8cf8":"### Model Compilation"}}