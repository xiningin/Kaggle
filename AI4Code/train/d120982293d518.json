{"cell_type":{"1c4898fa":"code","e9953dd9":"code","4711b0c2":"code","2b5080c7":"code","1646aa9b":"code","9233c567":"code","af4f716c":"code","fd275840":"code","29384935":"code","622c080c":"code","afc4c435":"code","6f343feb":"code","c2716837":"code","598f2394":"code","195cf648":"code","d21de9a3":"code","0c0b74cf":"code","f6adab63":"code","33d2b1cd":"code","592cf2f3":"code","e0ff865d":"code","c4a57fdc":"code","6c857945":"code","63148350":"code","8dc55254":"code","dfe27d29":"code","de2f79c5":"code","d89913a9":"code","b1b7cf94":"code","19deba5e":"code","e78d6de1":"code","46e45948":"code","2eb8ffa6":"code","65b0183c":"code","ac5d9eeb":"code","4666055b":"code","7e42e3df":"code","104011b3":"markdown","eb46e40d":"markdown","3d9db6ae":"markdown","64ea0247":"markdown","770c5ce2":"markdown","ef975ea3":"markdown","87381324":"markdown","b49e2caa":"markdown"},"source":{"1c4898fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e9953dd9":"!pip install pyspark","4711b0c2":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *","2b5080c7":"spark = SparkSession.Builder().getOrCreate()","1646aa9b":"train = spark.read.csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv',header = True,inferSchema=True)","9233c567":"train.limit(5).toPandas()","af4f716c":"train = train.na.drop(how='any')\ntrain.limit(5).toPandas()","fd275840":"train = train.withColumn(\"label\", train.PURCHASES_FREQUENCY>=0.5)\ntrain = train.withColumn(\"label\", train[\"label\"].cast(\"string\"))\n\nfrom pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol=\"label\", outputCol=\"target\")\ntraining = indexer.fit(train).transform(train)\n\ntraining.limit(5).toPandas()","29384935":"columns = [col for col in training.columns if col not in ['target','CUST_ID','label','PURCHASES_FREQUENCY']]","622c080c":"from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler()\\\n.setInputCols(columns)\\\n.setOutputCol(\"features\")\ntrain_calss = assembler.transform(training)","afc4c435":"train_calss.select(\"features\",\"target\").show(5)","6f343feb":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features',labelCol = \"target\")","c2716837":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator","598f2394":"paramGrid = ParamGridBuilder()\\\n   .addGrid(rf.numTrees, [100, 200, 300])\\\n   .addGrid(rf.maxDepth, [1, 2, 3, 4, 5, 6, 7, 8])\\\n   .addGrid(rf.maxBins, [25, 28, 31])\\\n   .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n   .build()","195cf648":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\") \n\ncrossval = CrossValidator(estimator = rf,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = evaluator,\n                          numFolds = 5)","d21de9a3":"train_rf, test_rf = train_calss.randomSplit([0.8, 0.2])","0c0b74cf":"cvModel = crossval.fit(train_rf)","f6adab63":"predictions = cvModel.transform(test_rf)","33d2b1cd":"predictions.select(\"features\",\"prediction\",\"target\").limit(5).toPandas()","592cf2f3":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\") \nevaluator.evaluate(predictions)","e0ff865d":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol=\"features\",labelCol=\"target\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n","c4a57fdc":"train_lr=train_calss.select(\"features\",\"target\")","6c857945":"training, testing = train_lr.randomSplit([0.8, 0.2])","63148350":"model = lr.fit(training)","8dc55254":"predictions = model.transform(testing)\npredictions.select(\"prediction\", \"target\", \"features\").show(5)","dfe27d29":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\")\nevaluator.evaluate(predictions)","de2f79c5":"from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\ncolumns = [col for col in training.columns if col not in ['target','CUST_ID','label']]\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler()\\\n.setInputCols(columns)\\\n.setOutputCol(\"features_clustering\")\n\ntrain_clustering = assembler.transform(training)","d89913a9":"import numpy as np\ncost = np.zeros(20)\nfor k in range(2,20):\n    kmeans = KMeans()\\\n            .setK(k)\\\n            .setSeed(1) \\\n            .setFeaturesCol(\"features_clustering\")\\\n            .setPredictionCol(\"cluster\")\n\n    model_k = kmeans.fit(train_clustering)\n    cost[k] = model_k.computeCost(train_clustering)","b1b7cf94":"import numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport seaborn as sbs\nfrom matplotlib.ticker import MaxNLocator\n\nfig, ax = plt.subplots(1,1, figsize =(8,6))\nax.plot(range(2,20),cost[2:20])\nax.set_xlabel('k')\nax.set_ylabel('cost')\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.show()","19deba5e":"kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol(\"features_clustering\")\nmodel = kmeans.fit(train_clustering)","e78d6de1":"# Make predictions\npredictions = model.transform(train_clustering)","46e45948":"# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))","2eb8ffa6":"# Show up the centers.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)","65b0183c":"predictions.select(\"features\",\"prediction\").limit(5).toPandas()","ac5d9eeb":"from pyspark.ml.clustering import BisectingKMeans\nbkm = BisectingKMeans().setK(2).setSeed(1)\nmodel2= bkm.fit(train_clustering)","4666055b":"predictions2 = model2.transform(train_clustering)","7e42e3df":"evaluator2= ClusteringEvaluator()\nsilhouette2 = evaluator.evaluate(predictions2)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette2))","104011b3":"# K-Means","eb46e40d":"# how many K do I need","3d9db6ae":"# Logistic regression","64ea0247":"# Random forest classifier","770c5ce2":"# BisectingKMeans","ef975ea3":"# Pre-processing","87381324":"# Lest's create our classification model","b49e2caa":"# Clustering"}}