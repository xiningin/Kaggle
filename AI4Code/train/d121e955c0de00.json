{"cell_type":{"4b2de967":"code","648e094b":"code","0995972d":"code","d51276e2":"code","a59989d1":"code","44e5aa42":"code","94753df8":"code","4b2e3c1d":"code","dae9e6de":"code","bed8a666":"code","8a7d018c":"code","0e60620c":"code","c1901277":"code","e1f36d4c":"code","86ab9728":"code","8b5022ce":"code","3d10b594":"code","e88bed8f":"code","7d3ad6a5":"code","ed080326":"code","e8787d81":"code","8839d69c":"code","39afc98a":"code","2c9d47db":"markdown","180e9837":"markdown","e1242666":"markdown","9629758a":"markdown","cf835a2d":"markdown","b2d0bb66":"markdown","05414913":"markdown","b82f0414":"markdown","af871c74":"markdown","28de4a9c":"markdown","b63aca66":"markdown","d82b8b6c":"markdown","be27c7a0":"markdown","c22dab7c":"markdown","dcb09bd6":"markdown","0ab5fffc":"markdown","fe4456da":"markdown","05fb4d7a":"markdown","26020f76":"markdown","a4930b95":"markdown","a9fbea6a":"markdown","9766853b":"markdown","b47ac2bf":"markdown","7d3632c7":"markdown","b44ffb8a":"markdown","cfcf3064":"markdown","8394e3ac":"markdown","55d90e7f":"markdown","bc7ca5c9":"markdown","63447d7f":"markdown","918ff9ae":"markdown","7155e2dd":"markdown","6ddfa574":"markdown"},"source":{"4b2de967":"from IPython.display import HTML\n%matplotlib inline","648e094b":"!apt-get -y install ffmpeg","0995972d":"from scipy.stats import beta\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# helper function for plotting\ndef plot_beta(a,b,ax, print_interval=True):\n    ax.set_xlabel(\"p\")\n    ax.set_ylabel(\"probability density\")\n    x = np.linspace(0.00,1, 100)\n    label = \"$\\\\alpha= \" + str(a) + \", \\\\beta=\" + str(b) + \"$\"\n    dist = beta(a,b)\n    # plot density\n    ax.plot(x, dist.pdf(x),\n            lw=2, alpha=0.6, label=label)\n    # determine the 95% HDI\n    if print_interval:\n        print(\"Interval containing 95% of the distribution: \", dist.interval(0.95))","d51276e2":"fig, ax = plt.subplots(1,1)\nplot_beta(10,10,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","a59989d1":"fig, ax = plt.subplots(1,1)\nplot_beta(100,100,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","44e5aa42":"fig, ax = plt.subplots(1,1)\nplot_beta(1000,1000,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","94753df8":"fig, ax = plt.subplots(1,1)\nplot_beta(10,1,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","4b2e3c1d":"fig, ax = plt.subplots(1,1)\nplot_beta(1,1,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","dae9e6de":"# the animations work for me on Ubuntu\nfrom scipy.stats import beta, binom\nfrom matplotlib import animation, rc\nplt.rcParams['animation.ffmpeg_path'] ='\/usr\/bin\/ffmpeg'\n\n# initializing plot\ndef init_plot():\n    fig = plt.figure()\n    ax = plt.axes(xlim=(0, 1), ylim=(0, 13))\n    line, = ax.plot([], [], lw=2)\n    ttl = ax.text(0.6,0.8,'', transform = ax.transAxes, va='center', size=20)\n    ax.set_xlabel(\"p\")\n    ax.set_ylabel(\"probability density\")\n    return fig, ax, ttl, line\n\n# random variates\nsamples = binom.rvs(1,0.4, size=200)\n\n# starting parameters and x values\na = 1\nb = 1\nx = np.linspace(0.00,1, 100)\n\n# init function\ndef init():\n    ttl.set_text(\"$\\\\alpha= \" + str(a) + \", \\\\beta=\" + str(b) + \"$\")\n    y =  beta.pdf(x,a,b)\n    line.set_data(x,y)\n    return line,\n\n# animating the stuff\ndef animate(i):\n    global a,b\n    # somehow the init frame is not drawn, so a small hack here\n    if i != 0:\n        a += samples[i-1]\n        b += 1 - samples[i-1]\n    ttl.set_text(\"$\\\\alpha= \" + str(a) + \", \\\\beta=\" + str(b) + \"$\")\n    y =  beta.pdf(x,a,b)\n    line.set_ydata(y)\n    return line,\n\n# let's animate\nfig, ax, ttl, line = init_plot()\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=200, interval=100, blit=True)\nplt.close()\nHTML(anim.to_html5_video())","bed8a666":"fig, ax, ttl, line = init_plot()\na = 50\nb = 10\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=200, interval=100, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n","8a7d018c":"fig, ax, ttl, line = init_plot()\na = 4\nb = 6\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=200, interval=100, blit=True)\nplt.close()\nHTML(anim.to_html5_video())","0e60620c":"dist = beta(1+80,1+120)\nprint(\"95% Credible interval:\", dist.interval(0.95))","c1901277":"binom.cdf(80,200,0.5)","e1f36d4c":"fig, ax = plt.subplots(1,1)\nplot_beta(81,121,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","86ab9728":"# for tractability work with the logarithm\nfrom scipy.special import betaln\n\np_alt = betaln(80+1,120+1) - betaln(1,1)\np_null = betaln(80+100,120+100) - betaln(100,100)\n\nbf = p_alt - p_null\nprint(\"Log Bayes factor: \", bf)","8b5022ce":"fig, ax = plt.subplots(1,1)\nplot_beta(100.,100.,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","3d10b594":"fig, ax = plt.subplots(1,1)\nplot_beta(1.,1.,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","e88bed8f":"fig, ax = plt.subplots(1,1)\nplot_beta(1000.,1000.,ax)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels);","7d3ad6a5":"# for tractability work with the logarithm\nfrom scipy.special import betaln\n\np_alt = betaln(80+1,120+1) - betaln(1,1)\np_null = betaln(80+1000,120+1000) - betaln(1000,1000)\n\nbf = p_alt - p_null\nprint(\"Log Bayes factor: \", bf)","ed080326":"print((80\/200.)**1)","e8787d81":"print((80\/200.)**10)","8839d69c":"pp = betaln(80+1+1,200-80+1) - betaln(81,121)\nprint(np.exp(pp))","39afc98a":"pp = betaln(80+10+1,200-80+1) - betaln(81,121)\nprint(np.exp(pp))","2c9d47db":"To summarize, with Bayesian inference and model fitting, we \"learn\" the posterior making probabilistic statements about the parameter(s) at interest based on prior intuitions and observed data. In contrast, frequentist statistics, would make points estimates, e.g., by maximum likelihood estimation (MLE).","180e9837":"Okay, interestingly the Bayes factor tells us to support the null hypothesis instead of the alternative hypothesis here. This is not surprising if we plot the null hypothesis and alternative.","e1242666":"We can see, that with our 200 samples, our highest posterior belief is around $p=0.5$. This indicates the influence of the prior. Actually, the posterior can be seen as a convex combination of prior belief (pseudo counts) and the data (\"heads\" and \"tails\" counts). If our prior belief is strong, we need more data to \"overrule\" the prior. \n\nHowever, if we \"steer\" our prior in the right direction, we can increase our credibility around the \"true\" parameter, much faster.","9629758a":"Pretty similar results to the frequentist ones above, but this time we evaluate the new data on the whole posterior.\n\nIn case of predicting new data points, one could just use those with highest posterior predictive probability.","cf835a2d":"# Introduction to Bayesian Inference: A Coin Flipping Example \n\nThis tutorial provides a short introduction to Bayesian inference.","b2d0bb66":"In Bayesian statistics, there are some ways of doing hypotheses testing. In this notebook, I want to elaborate the most trivial, but also clearest method first. This method is concerned with simply evaluating the posterior. After all, the posterior provides probabilistic statements about the parameter. So again, setting $\\alpha=1$ and $\\beta=1$ and observing $80$ heads and $120$ tails, we fit the model and end up with the following Beta posterior: ","05414913":"Now, we can directly examine the posterior and check the posterior probability for the null hypothesis parameter $p=0.5$. Just by looking at the graphical output, we can see that $p=0.5$ does not receive much posterior probability. So we could probably reject the hypothesis.\n\nHowever, in practise only saying that a coin is fair if it is exactly $p=0.5$ is a bit illusional. Thus, John Kruschke [1], suggests to use so-called regions of practival equivalence (ROPE) for the null hypothesis. This simply means that you give the null hypothesis some kind of tolerance.\n\nFor example, for a fair coin hypothesis, we could set the ROPE to [0.45, 0.55]. Then, we check whether the ROPE interval falls into the 95% credible interval [0.33, 0.47]. It indeed does partly, so we probably would not fully reject the null hypothesis, but we can be pretty sure, that the coin is not fair. Note that with more samples (data), the credible interval would then probably be more dense, which would lead to a possible clear rejection.\n\n[1] Kruschke, John. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014","b82f0414":"Here, the p-value would be below a significance level of $0.01$ clearly rejecting the null hypothesis that the coin is fair.","af871c74":"For demonstrating how the posterior now changes with an increasing number of observations, let us assume that we know the parameter of flipping \"heads\" in advance---i.e., we manually assume $p$ of the binomial distribution to be $0.4$. For simplicity, let us assume that our inital Beta prior has the hyperparameters $\\alpha=\\beta=1$ meaning that we have no real a-priori tendency and work with a flat prior---this can be seen as an \"anything is possible prior\". Next, we incrementally sample single random variates from the binomial distribution with $p=0.4$ (Bernoulli trials) and update our posterior with corresponding samples.","28de4a9c":"If we increase our symmetric shape parameters to a value of $100$, we can see a much higher density for parameters around the fair probability of $0.5$. The 95% highest density interval now lies between $0.43$ and $0.71$.","b63aca66":"Setting $\\alpha=10$ and $\\beta=1$ increases the belief in an unfair coin biased towards flipping \"heads\". Finally, let us shortly consider what happens when we set $\\alpha=\\beta=1$.","d82b8b6c":"Or for flipping $10$ times Heads out of $10$ new flips:","be27c7a0":"We can see that we now have a flat prior, meaning that all parameter configurations are equally likely. I suggest to play around with the shape parameters to see what happens with other configurations; also try to set the parameters lower than one.","c22dab7c":"You can play around with the parameters here. I also suggest to increase the prior pseudo counts to large numbers which results in the necessity to sample many variates to see differences regarding your prior knowledge (this may take a while though).","dcb09bd6":"In Bayesian inference, instead of using point estimates, we use our probabilistic statements about the parameter from the posterior for making prediction. This is usually refered to as <a href=\"https:\/\/en.wikipedia.org\/wiki\/Posterior_predictive_distribution\">posterior predictive distribution<\/a>. This is the distribution of unobserved data, conditioned on observed data and prior. It follows the exact same idea as the marginal likelihood discussed above (which can be seen as a prior predictive distribution), namely that we want to evaluate new data on our fitted model. We can write the posterior predictive distribution as,\n\n\n$$\nP(x |D, I) = \\int P(x|\\theta)P(\\theta |D, I)d\\theta\n$$\n\nwhere $D$ refers to our training data, and $x$ to some new unseen data.\n\nFor our beta-binomial model, we can write,\n\n$$\nP(j | m,k,n,\\alpha,\\beta) = \\int \nP(j|m,\\theta)P(\\theta |k,n,\\alpha,\\beta)d\\theta= \\binom{m}{j} \\frac{B(k+j+\\alpha,n-k+m-j+\\beta)} {B(\\alpha+k,\\beta+n-k)}\n$$\n\nwhere $j$ refers to the new number of Heads, $m$ to the new number of total flips, $k$ to the previous number of Heads, $n$ to the previous number of total flips, and $\\alpha$ and $\\beta$ to our prior hyperparameters.\n\nSo, for our previous examples, this results in, e.g.,:\n\n$$\nP(1 | 1,80,200,1,1) = \\binom{1}{1} \\frac{B(80+1+1,200-80+1-1+1)} {B(1+80,1+200-80)}\n$$","0ab5fffc":"### Prior\n\nWe know how we can calculate the likelihood based on the PMF of the binomial distribution. But, how can we now use the prior? As mentioned, the prior is our belief in the parameter(s) before observing the data. So basically, we can express our assumption about the parameter $p$ in our model in the form of another probability distribution. In case of the binomial distribution as likelihood, we can use the Beta distribution as a  <a href=\"https:\/\/en.wikipedia.org\/wiki\/Conjugate_prior\">conjugate prior<\/a>. This means that also the posterior distribution will be of the same family (i.e., Beta distribution) as the prior distribution.\n\nThe beta distribution is a continuous probability distribution over the interval $[0,1]$. The PDF of the Beta distribution is defined as:\n\n$$\nf(x|\\alpha,\\beta) =  \\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} =  \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, x^{\\alpha-1}(1-x)^{\\beta-1}\n$$\n\n$B$ is the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Beta_function\">beta function<\/a> and is a normalization constant to ensure that the probability integrates to 1. $\\Gamma$ is the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Gamma_function\">gamma function<\/a>.\n\n$\\alpha$ and $\\beta$ are positive shape parameters controlling the shape of the distribution. If they are $>=1$, we can think of them as pseudo counts; i.e., counts before observing the data. So for our example, this would mean that $\\alpha$ would specify the pseudo counts of observing a \"heads\" flip, while $\\beta$ would refer to the counts of observing a \"tails\" flip. The Wikipedia page of the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Beta_distribution#Shapes\">Beta distribution<\/a> goes into detail about the behavior of the shape parameters.\n\nFor a better understanding, let us visualize some examples next for the case of $\\alpha >= 1$ and $\\beta >= 1$. ","fe4456da":"In above animation, we can see that with an increasing amount of random samples from the binomial distribution, starting with an initial uniform (flat) belief (prior), we increasingly believe in the true parameter (i.e., $p=0.4$) from the underlying distribution that we manually specified beforehand. Bayesian inference not only allows us to make statements about the most probable values for $p$, but also about the credibility of these statements (highest density regions).\n\nWhat happens though, if we have stronger beliefs a-priori? Let us specify a Beta prior with $\\alpha=50$ and $\\beta=10$. Hence, we have decently high belief in a biased coin towards \"heads\", while our binomial distribution is slightly biased towards \"tails\" with $p=0.4$.","05fb4d7a":"Similar to frequentist statistics, Bayesian inference also allows to work with points estimates directly derived from the posterior; for example, by taking the posterior mean, median or mode (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Maximum_a_posteriori_estimation\">maximum a posteriori estimate MAP<\/a>). ","26020f76":"Another way of doing hypothesis tests, is to use the marginal likelihood which we have not talked about much yet.\n\nLet us re-iterate Bayesian inference:\n\n$$\n \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n$$\n\n$\\theta$ corresponds to the parameters of the binomial model, $D$ are the coin flips at interest and $I$ corresponds to additional information (e.g., a model or a hypothesis). \n\nThe <a href=\"https:\/\/en.wikipedia.org\/wiki\/Marginal_likelihood\">marginal likelihood<\/a> corresponds to the probability of the data where the parameters have been marginalized out:\n\n$$\nP(D | I) = \\int P(D | \\theta, I)P(\\theta | I)d\\theta\n$$\n\nThe evidence is the weighted average over all possible values of the parameters $\\theta$ where the weights come from the prior. So basically, the marginal likelihood is an average of the likelihood weighted by the prior.\n\nGenerally, we can say that if the prior is well aligned with the data,\nthen the evidence is rising with the strength of the prior. The evidence\nis the largest if the prior and the likelihood are concentrated over\nthe same parameter regions and it is the lowest if they concentrate\non different regions.\n\nFor comparing the plausibility of two models, we can resort to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Bayes_factor\">Bayes factors<\/a>. \nBayes factors are representing a Bayesian method for model comparison that include a natural Occam's razor guarding against overfitting.  We can define the Bayes factor---note that we apply unbiased comparison assuming that all models are equally likely a priori---as follows:\n\n$$\nB_{1,2} = \\frac{P(D | M_1)}{P(D|M_2)}\n$$\n\nThe strength of a Bayes factor can be determined by consulting the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Bayes_factor#Interpretation\">interpretation tables<\/a> of Jeffrey or Kass and Raftery.\n\nInstead of comparing models, we can now also compare hypotheses encoded in the form of priors. For example, for the null hypothesis of a fair coin, we could use a Beta prior with $\\alpha=100$ and $\\beta=100$. The stronger be believe in this hypothesis, the higher we would set the overall concentration of the Beta prior, e.g., $\\alpha=1000$ and $\\beta=1000$. This is a similar idea to the ROPE discussed above, with rising symmetric concentration, we would at one point only believe in $p=0.5$ without any tolerance. As an alternative hypothesis, we could use an \"anything is possible\" prior with $\\alpha=1$ and $\\beta=1$.\n\nFor the binomial model with a conjugate Beta prior, the marginal likelihood is defined as follows,\n\n$$\nP(D) = {{n}\\choose{k}}\\frac{B(k+\\alpha,n-k+\\beta)}{B(\\alpha,\\beta)}\n$$\n\nwhere $B()$ is the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Beta_function\">Beta function<\/a>.\n\nThen, for alternative hypotheses $H_{null}$ and $H_{alt}$, the Bayes factor is defined as:\n\n$$\n\\frac{P(n,k|H_{alt})}{P(n,k|H_{null})} = \\frac{{{n}\\choose{k}}\\frac{B(k+\\alpha_{alt},n-k+\\beta_{alt})}{B(\\alpha_{alt},\\beta_{alt})}}{{{n}\\choose{k}}\\frac{B(k+\\alpha_{null},n-k+\\beta_{null})}{B(\\alpha_{null},\\beta_{null})}}\n$$\n\nThe binomial coefficient cancelc out, and we can calculate the Bayes factor:","a4930b95":"Now, we have a preference (even though only a small one) for the alternative hypothesis which would state that the coin is not fair. \n\nThese results demonstrate, that the prior can encode hypothesis and we can then utilize the marginal likelihood and Bayes factors for comparing hypothesis. However, this is very much influenced by the choice of prior. But, similar to ROPEs, we can give hypothesis tolerance levels, allowing us to not make strict statements about hypothesis. Usually, I would advise to play around with the overall concentration\/tolerance and observe the behavior of the Bayes factor for making judgements about hypotheses.\n\nIn general, I think Bayesian statistics allows for more intuitive hypotheses tests, also allowing us to make probabilistic statements about the hypotheses themselves in form of tolerance levels.\n\nA more detailed discussion about this, is also provided in John Kruschkes book [1].","a9fbea6a":"Author: Philipp Singer\n\nWebsite: <a href=\"http:\/\/www.philippsinger.com\">www.philippsinger.com<\/a>\n\nTwitter: <a href=\"https:\/\/twitter.com\/ph_singer\">@ph_singer<\/a>\n\nVersion: 0.2","9766853b":"Finally, having our model, we also want to do prediction. For that, we refer to our fitted model. In frequentist statistics, we would simply take our MLE point estimate for that task. So, again for our example with $80$ Heads and $120$ Tails, if we would like to know the probability of observing unseen data, e.g., $1$ Heads, we would calculate:","b47ac2bf":"By further increasing the pseudo counts, we can further increase our \"fair\" coin belief.\n\nNext, let us also shortly consider non-symmetric shape parameters (but let us still follow the pseudo count interpretation and thus, only use parameters larger than one).","7d3632c7":"What we can see is that we allow our null hypothesis quite some tolerance here around $p=0.5$ and we know from our previous studies that fitting the data results in a posterior centered around $p=0.4$. Contrary, the flat prior gives equal probability to all possible parameter values. Thus, the marginal likelihood is higher for the alternative hypothesis, as it better captures the data. \n\nHowever, if we narrow our tolerance for the fair coin down, e.g., by setting $\\alpha=1000$ and $\\beta=1000$, we end up with different results.","b44ffb8a":"### Prediction","cfcf3064":"In general, statistical inference is the process of determining properties of a model\/distribution given some data. Bayesian inference can be seen as the Bayesian counterpart to frequentist inference. In frequentist inference,\nthere is usually the notion of some true, unknown, parameter which is a constant and point estimates are inferred from data.\nContrary, Bayesian inference treats the model parameters as random variables and usually wants to deduce probabilistic statements about the distribution of parameters. For a more thorough discussion about the differences between frequentist and Bayesian statistics, please refer to the following [blog post](http:\/\/jakevdp.github.io\/blog\/2014\/03\/11\/frequentism-and-bayesianism-a-practical-intro\/) by Jake Vanderplas.\n \n\n\nBayesian inference utilizes the famous Bayes rule:\n\n$$\n P(A|B) = \\frac{P(B | A) P(A)}{P(B)}\n$$\n\nFor model based inference, we can replace $A$ with the parameters $\\theta$ and $B$ with the data $D$ at interest. Furthermore, we can introduce $I$ which can be used to introduce an additional assumption (knowledge) to the inference such as which model to use.\n\n$$\n \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n$$\n\nThe prior distribution $P(\\theta|I)$ specifies our assumption about the parameters $\\theta$  before taking the data into account. The likelihood $P(D | \\theta, I)$ represents the probability of the data if the parameters $\\theta$ are specified. The marginal likelihood (or evidence) $P(D|I)$ is the distribution of the data $D$ given our additional assumption $I$. It is the normalization of the Bayes rule and plays an important rule for model comparison. Finally, the posterior $P(\\theta| D, I)$ is the distribution of the parameters after taking the observed data $D$ and our additional (prior) assumption $I$ into account. We can also say that the posterior is proportional to the likelihood and the prior.\n\n$$\nposterior \\propto likelihood \\times prior\n$$","8394e3ac":"### Credible intervals\n\nHaving fitted the model, we regularly want to get an interval for the true population mean. In frequentist statistics, these intervals are called \"confidence intervals\". Remember that in frequentist statistics the idea is that there is a true, fied unknown population parameter and the data is random. Then, the idea is to derive the confidence interval from a sample so that we can state that an interval constructed this way will contain the true parameter e.g., 95% of times. Hence, with confidence intervals, we can only make probabilistic statements about the interval, not the parameter. We cannot say that the interval contains the true parameter with 95% probability.\n\nIn contrast, Bayesian credible intervals allow for these kind of probabilistic statements about the parameter. They can be constructed very intuitively directly from the posterior by deriving e.g., the 95% highest density interval. Then, we can say that constructed interval contains the parameter with 95% probability. \n\nAgain, I want to mention the excellent blog post series by Jake Vanderplas discussing these differences in [greater detail](http:\/\/jakevdp.github.io\/blog\/2014\/06\/12\/frequentism-and-bayesianism-3-confidence-credibility\/).\n\nSo, as an example, say we use a flat Beta prior with $\\alpha=1$ and $\\beta=1$ and observe $80$ heads and $120$ tails. Then, we can simply derive the 95% credible interval from the posterior:","55d90e7f":"For $10$ Heads out of 10 flips this would mean:","bc7ca5c9":"## Example: coin flip\n\n### Introduction\n\nFor better understanding of Bayesian inference let us consider the classic <a href=\"http:\/\/www.behind-the-enemy-lines.com\/2008\/01\/are-you-bayesian-or-frequentist-or.html\">coin flip example<\/a>.\n\nWe are observing whether a coin flip results in \"heads\" or \"tails\". We are not sure whether the coin at interest is fair or whether it might be biased due to some asperity or similar things. Thus, we want to conduct statistical inference of the parameter $p$, which should describe the probability of flipping \"heads\", by utilizing the Bayesian framework. The probability of flipping \"tails\" is simply $1-p$. Further, we consider a set of observations $D$ by flipping the coin several times. Thus, by applying Bayes inference, we want to determine:\n\n$$\nP(p|D) \\propto P(D|p) \\times P(p)\n$$\n\n### Model\n\nAs underlying model, we can use the binomial distribution which is a discrete probability distribution for the number of successes in a sequence of n independent binary experiments (e.g., coin flips resulting in \"heads\" or \"tails\"). The binomial distribution is conditioned on the parameters $n$ (number of trials) and $p$ the probability of flipping \"heads\". The probability mass function of the binomial distribution---which determines the probability of observing $k$ \"heads\" with parameters $p$ and $n$---is defined as:\n\n$$\nf(k|p,n) = \\binom nk p^k (1-p)^{n-k}\n$$\nWe can rewrite above Bayes rules as:\n\n$$\nP(p|k,n) \\propto P(k|p,n) \\times P(p)\n$$","63447d7f":"Let us consider a symmetric Beta distribution. This means that we want to express our prior belief that the coin is fair. We can express this prior with the shape parameters $\\alpha$ and $\\beta$ which we can interpret as pseudo counts. As we have a fair belief, we set both shape parameters to the same value. Let us start with $\\alpha=\\beta=10$.","918ff9ae":"What we can see is the PDF of the Beta distribution with our given shape parameters. We can see that the parameter $p=0.5$ receives the highest density. However, as we keep our pseudo counts relatively low, we also allow other parameter configurations to receive a certain density. If we determine the interval containing 95% of the distribution, we receive $0.29$ as the lower and $0.71$ as the upper boundary.","7155e2dd":"### Hypothesis testing\n\nAnother very common task in statistical inference, is the task of doing hypotheses testing. In our example, a common hypotheses to test would be \"The coin is fair!\".\n\nIn frequentist, statistics, this is usually tackled by null hypotheses significance test, where we try to reject a null hypothesis. With this approach, we can only reject or not reject the null hypothesis though, but never accept it. A simple way of doing that, would be to do a binomial test, where we set our null hypothesis as the coin being fair by setting $p=0.5$.","6ddfa574":"### Posterior \/ Model fitting\n\nNow that we have specified the likelihood-distribution as well as the prior-distribution we can rewrite the posterior-distribution as:\n\n$$\nP(p|k,n,\\alpha,\\beta) = \\text{Beta}(p|\\alpha+k, \\beta+(n-k))\n$$\n\nAs mentioned earlier, the posterior is from the same distribution family (Beta) as the prior (Beta), if the prior distribution is the conjugate prior to the likelihood distribution (binomial). \n\nThus, based on our Bayesian inference, the posterior distribution of the parameter $p$ indicating the probability of flipping \"heads\", is expressed as a Beta distribution. The parameters of this Beta distribution are constituted by both our a-priori assumptions---i.e., the hyperparameters $\\alpha$ and $\\beta$---as well as observed data---i.e., the number of times we flipped \"heads\" $k$ and \"tails\" $n-k$. The resulting posterior can then be used as a new prior as we gather more data."}}