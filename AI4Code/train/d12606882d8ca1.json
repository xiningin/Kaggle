{"cell_type":{"a6d2a393":"code","8f2f7b28":"code","c2da8ebc":"code","2e124252":"code","14f204da":"code","77e89415":"code","a48a553c":"code","cc52bc80":"code","6e6c879b":"code","ca2b004c":"code","b32670d6":"code","d285f064":"code","df3f44be":"code","1b292edd":"code","ba9b7382":"code","c03c18c2":"code","74a4d585":"code","7c9e2b9e":"code","d34d95fd":"code","e1ddc950":"code","efb80d70":"code","98a9995a":"code","78dfbb21":"code","14ccf724":"code","d5b2042f":"code","f732032a":"code","61ca8072":"code","1e4503bc":"code","4eeec761":"code","01d2faa1":"code","9b1be4d8":"code","158a0c30":"code","14e6c9da":"code","ba59c173":"code","0cf49ef8":"code","57ac560d":"code","10c440c7":"code","b5726590":"code","f3991e6d":"code","aac606e4":"code","ffa2fa36":"markdown","24fd97ab":"markdown","05f5ec2e":"markdown","d2c0d524":"markdown","067ff3fb":"markdown","01470f06":"markdown","6a1cef6c":"markdown","01f3bca1":"markdown","21dfb67f":"markdown","83d38add":"markdown","3a595ad3":"markdown","ee5f91f0":"markdown","aa973f63":"markdown","1ac434b0":"markdown","1dde3103":"markdown","aa652a27":"markdown","91bffcd7":"markdown","cd1b6995":"markdown","37225744":"markdown","1489fad8":"markdown","2b265994":"markdown","b3d57b8a":"markdown","a2cff43e":"markdown","bdc60c64":"markdown","bb7bfdc5":"markdown","b59edd90":"markdown","fad592bb":"markdown","e29306ff":"markdown","4ab84ed4":"markdown","3d626b8e":"markdown","8badf867":"markdown","5e0b4c79":"markdown","ce1e3137":"markdown","5b4fab0b":"markdown"},"source":{"a6d2a393":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 140)\n\nimport os\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\n\n# import janestreet\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8f2f7b28":"# reading the paths of all the files present in the dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c2da8ebc":"# setting the paths to variables to access when required\nTRAINING_PATH = \"\/kaggle\/input\/jane-street-market-prediction\/train.csv\"\nFEATURES_PATH = \"\/kaggle\/input\/jane-street-market-prediction\/features.csv\"\nTEST_PATH = \"\/kaggle\/input\/jane-street-market-prediction\/example_test.csv\"\nSAMPLE_SUB_PATH = \"\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv\"","2e124252":"%%time\n\ns = !wc -l {TRAINING_PATH}\nn_train_rows = int(s[0].split(' ')[0])+1\n\ns = !wc -l {FEATURES_PATH}\nn_features_rows = int(s[0].split(' ')[0])+1\n\ns = !wc -l {TEST_PATH}\nn_test_rows = int(s[0].split(' ')[0])+1\n\nprint('-'*30)\nprint (f'Total Training Rows: {n_train_rows}')\nprint (f'Total Features Rows: {n_features_rows}')\nprint (f'Total Testing Rows: {n_test_rows}')\nprint('-'*30)","14f204da":"%%time\n\ndf_train_list = []\ndf_features_list = []\ndf_test_list = []\n\nchunksize = 1000\nn_rows = 10000\n\n\nprint(\"Reading Train Data!\")\nfor df_chunk in tqdm(pd.read_csv(TRAINING_PATH, chunksize=chunksize, nrows=n_rows)):\n    \n    # todo: I will implement data cleaning and feature engineering here soon.\n    \n    # Now, append the chunk to train list\n    df_train_list.append(df_chunk) \n    \n    \nprint('-'*30)\nprint(\"Reading Features Data!\")\nfor df_chunk in tqdm(pd.read_csv(FEATURES_PATH, chunksize=chunksize, nrows=n_rows)):\n    df_features_list.append(df_chunk) \n    \n    \nprint('-'*30)\nprint(\"Reading Test Data!\")\nfor df_chunk in tqdm(pd.read_csv(TEST_PATH, chunksize=chunksize, nrows=n_rows)):\n    df_test_list.append(df_chunk) ","77e89415":"# Merge all dataframes into one dataframe\ntrain_df = pd.concat(df_train_list)\nfeatures_df = pd.concat(df_features_list)\ntest_df = pd.concat(df_test_list)\n\n\n# Delete the dataframe list to release memory\ndel df_train_list\ndel df_features_list\ndel df_test_list","a48a553c":"test_df.head()","cc52bc80":"# reading data using pandas.\n\n# n_rows = 300000\n# train_df = pd.read_csv(TRAINING_PATH, nrows=n_rows)\n# features_df = pd.read_csv(FEATURES_PATH, nrows=n_rows)\n# test_df = pd.read_csv(TEST_PATH, nrows=n_rows)\n\n# train_df.head()","6e6c879b":"train_df.shape","ca2b004c":"print(\"Train data set dtypes: \\n\")\nprint(f\"Total Cols: {len(train_df.columns)}\")\nprint(f\"{train_df.dtypes.value_counts()}\")\nprint('-'*30)\n\nprint(\"Features data set dtypes: \\n\")\nprint(f\"Total Cols: {len(features_df.columns)}\")\nprint(f\"{features_df.dtypes.value_counts()}\")\nprint('-'*30)\n\nprint(\"Test data set dtypes: \\n\")\nprint(f\"Total Cols: {len(test_df.columns)}\")\nprint(f\"{test_df.dtypes.value_counts()}\")","b32670d6":"train_df.describe()","d285f064":"correlations = train_df.corr(method='pearson')","df3f44be":"fig, axs = plt.subplots(figsize=(16, 16))\nsns.heatmap(correlations)\nfig.savefig('correlation_map.png')","1b292edd":"#  Missing Values\nprint('Train Nan Valued colas: %d' %train_df.isna().any().sum())\nprint('Features Nan Valued cols: %d' %features_df.isna().any().sum())\nprint('Test Nan Valued cols: %d' %test_df.isna().any().sum())","ba9b7382":"n_features = 40\nnan_val = train_df.isna().sum()[train_df.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)\n\n\nfig, axs = plt.subplots(figsize=(10, 10))\n\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\n\nplt.title(f'NaN values of train dataset (Top {n_features})')\nplt.xlabel('NaN values')\nfig.savefig(f'nan_values_top_{n_features}_features.png')\nplt.show()","c03c18c2":"fig, axs = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train_df['resp'], ax=axs[0])\nsns.distplot(train_df['weight'], ax=axs[1])\nfig.savefig('resp_weight_distplot.png')","74a4d585":"fig, axs = plt.subplots(1, 4, figsize=(16, 6))\nsns.distplot(train_df['resp_1'], ax=axs[0])\nsns.distplot(train_df['resp_2'], ax=axs[1])\nsns.distplot(train_df['resp_3'], ax=axs[2])\nsns.distplot(train_df['resp_4'], ax=axs[3])\nfig.savefig('resp_distplot.png')","7c9e2b9e":"fig, ax = plt.subplots(figsize=(16, 8))\n\nresp = train_df['resp'].cumsum()\nresp_1 = train_df['resp_1'].cumsum()\nresp_2 = train_df['resp_2'].cumsum()\nresp_3 = train_df['resp_3'].cumsum()\nresp_4 = train_df['resp_4'].cumsum()\n\nresp.plot(linewidth=2)\nresp_1.plot(linewidth=2)\nresp_2.plot(linewidth=2)\nresp_3.plot(linewidth=2)\nresp_4.plot(linewidth=2)\n\nax.set_xlabel (\"Trade\", fontsize=12)\nax.set_title (\"Cumulative Trade returns\", fontsize=18)\n\nplt.legend(loc=\"upper left\");\nplt.savefig('cummulative_trade_growth.png')","d34d95fd":"fig, ax = plt.subplots(figsize=(16, 12))\nsns.violinplot(data=train_df[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]], \n               inner=\"points\", \n               linewidth=1, \n               palette=\"Set3\", \n               ax=ax)    \nfig.savefig('resp_violinplot.png')","e1ddc950":"fig = px.density_contour(train_df, x=\"date\", y=\"ts_id\")\nfig.update_traces(contours_coloring=\"fill\", contours_showlabels = True)\nfig.show()","efb80d70":"## TODO: uncomment the below if you want to visualize the distribution plots of resp features.\n\n\n# hist_data = [train_df[\"resp\"], train_df[\"resp_1\"], train_df[\"resp_2\"], train_df[\"resp_3\"], train_df[\"resp_4\"]]\n# group_labels = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n\n# # Create distplot with curve_type set to 'normal'\n# fig = ff.create_distplot(hist_data, group_labels, curve_type='normal', show_hist=False)\n# fig.show()","98a9995a":"date = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train_df[train_df[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 600], \n    range_x=[-7, 7]\n)\n\nhist.show()","78dfbb21":"## TODO: uncomment the below if you want to visualize the scatter plot between weights and resp features.\n\n# date = 0\n# n_features = 5\n# cols = [f'resp_{i}' for i in range(1, n_features)]\n# cols.insert(0, \"resp\")\n\n# sctr = px.scatter(train_df[train_df[\"date\"] == date], \n#                      x=cols, \n#                      y=\"weight\", \n#                      size=\"weight\", \n#                      color=\"date\",\n#                      hover_name=\"weight\", \n#                      animation_frame='variable',\n#                      log_x=True, \n#                      size_max=40)\n\n# sctr.show()","14ccf724":"feature_cols = [c for c in train_df.columns if 'feature' in c]\n\nfor f in feature_cols:\n    fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n    \n    # plot 1\n    sns.distplot(train_df[f], ax=axs[0])\n    \n    # plot 2\n    sns.distplot(train_df.query('weight > 0')[f], ax=axs[1])\n    \n    # plot 3\n    try:\n        sns.distplot(train_df.query('weight > 0 and resp > 0')[f].dropna().apply(np.log1p), ax=axs[2])\n        sns.distplot(train_df.query('weight > 0 and resp < 0')[f].dropna().apply(np.log1p), ax=axs[2])\n    except:\n        pass\n    \n    # plot 4\n    train_df.plot(kind='scatter', x=f, y='resp', ax=axs[3])\n    \n    fig.suptitle(f, fontsize=15, y=1.1)\n    axs[0].set_title('Feature Distribution (all weights)')\n    axs[1].set_title('Feature Distribution (weights > 0)')\n    axs[2].set_title('Log Transform')\n    axs[3].set_title('Feature vs. Response')\n    \n    plt.tight_layout()\n    plt.show()","d5b2042f":"def plot_outlier_graph_set(df):    \n    feature_cols = [c for c in df.columns if 'feature' in c]\n\n    for f in feature_cols:\n        fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n\n        # plot 1\n        sns.boxplot(y=f, data=df, ax=axs[0])\n\n        # plot 2\n        sns.boxenplot(y=f, data=df, ax=axs[1])\n\n        # plot 3\n        sns.violinplot(y=f, data=df, ax=axs[2]) \n\n        # plot 4\n        sns.stripplot(y=f, data=df, size=4, color=\".3\", linewidth=0, ax=axs[3])\n\n\n        fig.suptitle(f, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Boxen Plot')\n        axs[2].set_title('Violin Plot')\n        axs[3].set_title('Strip Plot')\n\n        plt.tight_layout()\n        plt.show()","f732032a":"plot_outlier_graph_set(train_df)","61ca8072":"imputer = SimpleImputer(strategy='mean')\nimputed_train_df = pd.DataFrame(imputer.fit_transform(train_df))\n\nimputed_train_df.columns=train_df.columns\nimputed_train_df.index=train_df.index\n\nprint(f\"Is there any missing values left? {imputed_train_df.isna().sum().any()}\")\nimputed_train_df.head()","1e4503bc":"# Calculating the Z score to calculate outliers\n\nthreshold = 4\n\nz = np.abs(stats.zscore(imputed_train_df, nan_policy='omit'))\nprint(f\"Z score array:\\n{z}\")\nprint('-'*30)\n\nrow_index = np.where(z > threshold)\nprint(f\"Outlier Data Rows: {len(set(row_index[0]))}\")\nprint('-'*30)\n\nprint(f\"Outlier Rows Index:\\n{row_index}\")\nprint('-'*30)\n\nprint(f\"Sample outlier row data:\\n{z[row_index[0][0]]}\")","4eeec761":"clean_train_df = imputed_train_df[(z < threshold).all(axis=1)].reset_index(drop=True)\nclean_train_df","01d2faa1":"# dropping duplicate values \nclean_train_df.drop_duplicates(keep=False,inplace=True)\nclean_train_df","9b1be4d8":"plot_outlier_graph_set(clean_train_df)","158a0c30":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_rescaled = scaler.fit_transform(clean_train_df)","14e6c9da":"def plot_PCA_threshold(threshold, data_rescaled):\n#     pca = PCA(n_components=threshold)\n#     pca.fit(data_rescaled)\n#     reduced = pca.transform(data_rescaled)\n\n    pca = PCA().fit(data_rescaled)\n\n    fig, ax = plt.subplots(figsize=(16, 6))\n    y = np.cumsum(pca.explained_variance_ratio_)\n    xi = np.arange(1, len(y)+1, step=1)\n\n    # print(f\"X vals:\\n{xi}\")\n    # print('-'*30)\n    # print(f\"Y vals:\\n{y}\")\n    # print('-'*30)\n\n    plt.ylim(0.0,1.1)\n    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n\n    plt.xlabel('Number of Components')\n    plt.xticks(np.arange(0, len(y), step=5)) #change from 0-based array index to 1-based human-readable label\n    plt.ylabel('Cumulative variance (%)')\n    plt.title('The number of components needed to explain variance')\n\n    plt.axhline(y=threshold, color='r', linestyle='-')\n    plt.text(0.5, 0.85, f'{threshold*100}% cut-off threshold', color = 'red', fontsize=16)\n\n    ax.grid(axis='x')\n    fig.savefig(f\"PCA_threshold_{threshold*100}p.png\")\n    plt.show()","ba59c173":"thresholds = [0.90, 0.95, 0.97, 0.99]\n\nfor th in thresholds:\n    plot_PCA_threshold(th, data_rescaled)","0cf49ef8":"chosen_threshold = 0.95\nn_components=35\n\nfeature_cols = [c for c in train_df.columns if 'feature' in c]\nnew_cols=[]\nfor i in range(n_components):\n    new_cols.append(f\"new_feature_{i}\")\n    \nprint(\"Principal Component Analysis (PCA)\")\nprint('-'*30)\nprint(f\"New Columns:\\n{new_cols}\")\nprint('-'*30)\n\n\npca = PCA(n_components=n_components)\nprincipalComponents = pca.fit_transform(clean_train_df[feature_cols])\nprincipal_df = pd.DataFrame(data=principalComponents, columns=new_cols )\n\nprint(f\"PCA Explained Variance Ratio:\\n{pca.explained_variance_ratio_}\")\nprint('-'*30)\nprint(f\"PCA Singular Values:\\n{pca.singular_values_}\")\nprint('-'*30)\nprincipal_df.head()","57ac560d":"squzzed_train_df = pd.concat([principal_df, clean_train_df[['date', 'weight', 'resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'ts_id']]], axis = 1)\nsquzzed_train_df.head()","10c440c7":"squzzed_train_df.to_csv(f\"PCA_reduced_{chosen_threshold*100}p_{n_rows}x{n_components}.csv\", index=False)","b5726590":"# fig, axs = plt.subplots(figsize=(6, 6))\n# sns.scatterplot(data=squzzed_train_df, \n#                 x=new_cols[0], \n#                 y=new_cols[1], \n#                 hue=\"weight\", \n#                 size=\"weight\")\n\n# fig.savefig(\"PCA_f1_f2_scatterplot.png\")","f3991e6d":"feature_cols = [c for c in squzzed_train_df.columns if 'feature' in c]\n\nfor f in feature_cols:\n    fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n    \n    # plot 1\n    sns.distplot(squzzed_train_df[f], ax=axs[0])\n    \n   \n    # plot 2\n    try:\n        sns.distplot(squzzed_train_df.query('weight > 0 and resp > 0')[f].dropna().apply(np.log1p), ax=axs[1])\n        sns.distplot(squzzed_train_df.query('weight > 0 and resp < 0')[f].dropna().apply(np.log1p), ax=axs[1])\n    except:\n        pass\n    \n     # plot 3\n    sns.violinplot(y=f, data=squzzed_train_df, palette=\"Set3\", linewidth=0.5, inner=\"points\", ax=axs[2]) \n\n    # plot 4\n    squzzed_train_df.plot(kind='scatter', x=f, y='resp', ax=axs[3])\n    \n    fig.suptitle(f, fontsize=15, y=1.1)\n    axs[0].set_title('Feature Distribution (all weights)')\n    axs[1].set_title('Feature Violin Plot')\n    axs[2].set_title('Log Transform')\n    axs[3].set_title('Feature vs. Response')\n    \n    plt.tight_layout()\n    plt.show()","aac606e4":"sns.pairplot(squzzed_train_df, \n             vars=new_cols)\n\nfig.savefig('PCA_new_features_pairplot.png')","ffa2fa36":"<a id=\"5a\"><\/a>\n## <center style=\"background-color:yellow; width:300px;\">a) Imputing Missing Data<\/center>\n### <center style=\"background-color:yellow; width:250px;\">Simple Imputer<\/center>","24fd97ab":"## <center style=\"background-color:#6abada; color:white;\">Contents in the Notebook \ud83d\udc49<\/center>\n\n[   About the competition](#0)\n1. [Import Libraries \ud83d\udcda](#1)\n2. [Import dataset \u270d](#2)\n3. [Understanding Data Features \ud83d\udcc8](#3)\n4. [Exploratory Data Analysis (EDA) \ud83d\udcca](#4)\n5. [Data Cleaning \ud83e\uddf9](#5)\n    * [Imputing Missing Data](#5a)\n    * [Remove Outliers](#5b)\n    * [Dropping Duplicates](#5c)\n6. [Feature Engineering \ud83d\udc77](#6)    \n    * [Dimensionality Reduction - PCA](#6a)\n7. [Work in Progress \ud83d\udea7](#1000)\n","05f5ec2e":"#### <center style=\"background-color:purple; color:white; width:200px;\">Missing Valued Rows<\/center>","d2c0d524":"<a id=\"5c\"><\/a>\n## <center style=\"background-color:yellow; width:350px;\">c) Dropping Duplicates<\/center>","067ff3fb":"### <center style=\"background-color:yellow; width:250px;\">Understanding Outliers <\/center>","01470f06":"### [Back to the Top \ud83d\udc46](#0)","6a1cef6c":"<div><b>Note: The data does not have any duplicates as the size of the dataframe does not change.<\/b><\/div>","01f3bca1":"#### <center style=\"background-color:purple; color:white; width:150px;\">Clean Plots<\/center>","21dfb67f":"<a id=\"5b\"><\/a>\n## <center style=\"background-color:yellow; width:350px;\">b) Remove Outliers<\/center>\n### <center style=\"background-color:yellow; width:250px;\">Z-score calculations<\/center>","83d38add":"<a id=\"3\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Understanding Data Features \ud83d\udcc8<\/center>","3a595ad3":"<h4 style=\"color: red;\">The notebook could be a bit slow to load at times as it uses plotly to load images which sometimes slows down page rendering due to intense graphs.<\/h4>","ee5f91f0":"### <center style=\"background-color:yellow; width:150px;\">Introduction<\/center>\nIn this competition, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to \u201cfair\u201d values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.\nThis is a Code Competition and you need to submit notebooks for evaluation.\n\n### <center style=\"background-color:yellow; width:150px;\">Deadlines<\/center>\n* February 15, 2021 - Entry deadline. You must accept the competition rules before this date in order to compete.\n* February 15, 2021 - Team Merger deadline. This is the last day participants may join or merge teams.\n* February 22, 2021 - Final submission deadline.\n\nStarting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks and the competition ends on August 23, 2021 finally.\n\n\n### <center style=\"background-color:yellow; width:150px;\">Evaluation<\/center>\nThis competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.\n\nFor each date i, we define:\np_i = \\sum_j(weight_{ij} * resp_{ij} * action_{ij})\n\nt = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}}\n\nwhere |i| is the number of unique dates in the test set. The utility is then defined as:\nu = min(max(t,0), 6)  \\sum p_i.\n\n\n\n\n### <center style=\"background-color:yellow; width:150px;\">Submission<\/center>\nYou must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the following template in Kaggle Notebooks:\n\n```python\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = 0 #make your 0\/1 prediction here\n    env.predict(sample_prediction_df)\n```\n\n### <center style=\"background-color:yellow; width:150px;\">Note<\/center>\n* Your notebook must use the time-series module to make predictions.\n* The max limit to use GPU and CPU are 4 hours each for training.\n* Forecasting phase will have an additional 10% extra time allowance.\n* The maximum team size is 5.\n* You may submit a maximum of 5 entries per day.\n* You may select up to 2 final Submissions for judging.\n* COMPETITION WEBSITE: [https:\/\/www.kaggle.com\/c\/jane-street-market-prediction](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction)\n\n","aa973f63":"### MY COMPETITION GEAR:","1ac434b0":"<a id=\"0\"><\/a>\n# <center style=\"background-color:#63809e; color:white;\">Jane Street Market Prediction<\/center>\n\n<center><img src=\"https:\/\/www.janestreet.com\/assets\/logo_horizontal.png\" width=70%><\/center>","1dde3103":"<a id=\"1\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Import Libraries \ud83d\udcda<\/center>","aa652a27":"<a id=\"6a\"><\/a>\n### <center style=\"background-color:yellow; width:350px;\">Principal Component Analysis<\/center>","91bffcd7":"<center style=\"color:black; background-color:yellow;\"><h4>Based on above plots we can choos how many number of components we need to use as final reduced featues after PCA, here we choose 35 components based on the 95% threshold.<\/h4><\/center>","cd1b6995":"### <center style=\"background-color:yellow; width:300px;\">Data Types<\/center>","37225744":"#### <center style=\"background-color:purple; color:white; width:250px;\">Feature_i Histogram Plots<\/center>","1489fad8":"#### <center style=\"background-color:purple; color:white; width:200px;\">Resp_i[](http:\/\/) Distribution Plots<\/center>","2b265994":"### Thanks :)","b3d57b8a":"<a id=\"2\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Importing Data \u270d<\/center>\n\n<center><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/c7\/Jane_Street_Sign.jpg\/1200px-Jane_Street_Sign.jpg\" width=50%><\/center>","a2cff43e":"#### <center style=\"background-color:purple; color:white; width:150px;\">Resp Violin Plots<\/center>","bdc60c64":"### <center style=\"background-color:yellow; width:250px;\">Features Correlation<\/center>\n","bb7bfdc5":"<a id=\"6\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Feature Engineering \ud83d\udc77<\/center>","b59edd90":"### <center style=\"background-color:yellow; width:150px;\">Note<\/center>\n1. The size of dataframe in this competition is huge, so using dask to read dataset could be a good option. Just for analysis we are using pandas to read a small set of the data to understand it well.<br><br>\n2. To read more about dask and how you can use it, read this article <a href=\"https:\/\/towardsdatascience.com\/dask-a-guide-to-process-large-datasets-using-parallelization-c5554889abdb\">here.<\/a>\n","fad592bb":"<a id=\"4\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Exploratory Data Analysis \ud83d\udcca<\/center>","e29306ff":"### <center style=\"background-color:yellow; width:500px;\">Understanding distributions based using Plots<\/center>\n#### <center style=\"background-color:purple; color:white; width:280px;\">Weight and Resp Distribution Plots<\/center>","4ab84ed4":"<a id=\"5\"><\/a>\n## <center style=\"background-color:#6abada; color:white;\">Data Cleaning \ud83e\uddf9<\/center>","3d626b8e":"### <center style=\"background-color:yellow; width:300px;\">Missing Values<\/center>\n#### <center style=\"background-color:purple; color:white; width:200px;\">Missing Valued Cols<\/center>","8badf867":"#### <center style=\"background-color:purple; color:white; width:150px;\">Unclean Plots<\/center>","5e0b4c79":"### <center style=\"background-color:yellow; width:250px;\">Understanding Data Spread<\/center>","ce1e3137":"<table style=\"width:100%\">\n  <tr>\n    <th>HP Z8 G4 Tower - 1125W PSU<\/th>\n    <th>HP ZBook Studio - G7 Mobile Workstation:<\/th>\n  <\/tr>\n  <tr>\n    <td>6234 3.3 GHz (8 Core each) i9 Processors x 2<\/td>\n    <td>6234 3.3 GHz (8 Core) i9 Processor x 1<\/td>\n  <\/tr>\n  <tr>\n    <td>NVIDIA Quadro RTX 8000 x 1<\/td>\n    <td>NVIDIA Quadro RTX 5000 x 1<\/td>\n  <\/tr>\n  <tr>\n    <td>96GB DDR4 RAM 2933<\/td>\n    <td>32GB DDR4 RAM 2933<\/td>\n  <\/tr>\n    \n  <tr>\n    <td>2 TB NVMe M.2 SSD<\/td>\n    <td>2 TB NVMe M.2 SSD<\/td>\n  <\/tr>\n  <tr>\n    <td><img src= \"https:\/\/ssl-product-images.www8-hp.com\/digmedialib\/prodimg\/lowres\/c05724976.png?imdensity=1&imwidth=1024\" width=200px><\/td>\n    <td><img src=\"https:\/\/www8.hp.com\/content\/dam\/sites\/worldwide\/personal-computers\/commercial\/workstations\/zbook-studio\/images\/color-accuracy-image-desktop.png\" width=200px><\/td>\n  <\/tr>\n<\/table>\n","5b4fab0b":"#### <center style=\"background-color:purple; color:white; width:250px;\">Date vs ts_id Contours Plot<\/center>"}}