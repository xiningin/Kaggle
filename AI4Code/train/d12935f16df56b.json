{"cell_type":{"9970475e":"code","96e6dde6":"code","2db6811b":"code","8f3e3e0e":"code","bca17cba":"code","6ab6c3e0":"code","78697257":"code","798b7f8f":"code","eccd573b":"code","a322f89c":"code","61081514":"code","6dc780be":"code","c5d84ba5":"code","c5d4cac4":"code","a39e1990":"code","cdf12c23":"code","16d4f8ba":"code","ef97f5c8":"code","fd954365":"code","fe6fa18c":"code","b0b281d9":"code","a59fceb3":"code","cad66e4b":"code","055444f2":"code","bdaa68d6":"code","48d3050d":"code","ae8e196f":"code","d5dfbe1e":"code","2a83806c":"code","d6fd4204":"code","3fedc4d9":"code","4a95d0e5":"code","f2279834":"code","116e195c":"code","e189e68d":"code","88696f1c":"code","43925ee0":"code","6b4dc8c0":"code","e0cfcdf1":"code","5fe0aca6":"code","ea332276":"markdown","5750014d":"markdown","952c10b6":"markdown","8c513019":"markdown","1606e2bf":"markdown","a4471c90":"markdown","24375b06":"markdown","08f694f6":"markdown","be9e7ed0":"markdown","1ceb48e9":"markdown","28d58085":"markdown","d0c2d9ed":"markdown","42ce1faa":"markdown","cfd266dc":"markdown","da140263":"markdown","095b0059":"markdown","b524cf6a":"markdown","67f98691":"markdown","07f54565":"markdown","d28041cf":"markdown","4b2e81e9":"markdown","7f9378ae":"markdown"},"source":{"9970475e":"import os\nimport pandas as pd\nimport numpy as np\nimport PIL.Image\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline ","96e6dde6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2db6811b":"DATA_FOLDER = '\/kaggle\/input\/bengaliai-cv19\/'\ntrain_df = pd.read_csv(os.path.join(DATA_FOLDER, 'train.csv'))\ntrain_df.head()","8f3e3e0e":"train_df.shape","bca17cba":"test_df = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\ntest_df.head()","6ab6c3e0":"test_df.shape","78697257":"class_map_df = pd.read_csv(os.path.join(DATA_FOLDER, 'class_map.csv'))\nclass_map_df.head()","798b7f8f":"class_map_df.shape","eccd573b":"sample_submission_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))\nsample_submission_df.head()","a322f89c":"sample_submission_df.shape","61081514":"start_time = time.time()\ntrain_0_df = pd.read_parquet(os.path.join(DATA_FOLDER,'train_image_data_0.parquet'))\nprint(f\"`train_image_data_0` read in {round(time.time()-start_time,2)} sec.\")                               ","6dc780be":"train_0_df.shape","c5d84ba5":"train_0_df.head()","c5d4cac4":"start_time = time.time()\ntrain_1_df = pd.read_parquet(os.path.join(DATA_FOLDER,'train_image_data_1.parquet'))\nprint(f\"`train_image_data_1` read in {round(time.time()-start_time,2)} sec.\")  ","a39e1990":"train_1_df.shape","cdf12c23":"train_1_df.head()","16d4f8ba":"start_time = time.time()\ntest_0_df = pd.read_parquet(os.path.join(DATA_FOLDER,'test_image_data_0.parquet'))\nprint(f\"`test_image_data_0` read in {round(time.time()-start_time,2)} sec.\")  ","ef97f5c8":"test_0_df.shape","fd954365":"test_0_df.head()","fe6fa18c":"print(f\"Train: unique grapheme roots: {train_df.grapheme_root.nunique()}\")\nprint(f\"Train: unique vowel diacritics: {train_df.vowel_diacritic.nunique()}\")\nprint(f\"Train: unique consonant diacritics: {train_df.consonant_diacritic.nunique()}\")\nprint(f\"Train: total unique elements: {train_df.grapheme_root.nunique() + train_df.vowel_diacritic.nunique() + train_df.consonant_diacritic.nunique()}\")\nprint(f\"Class map: unique elements: \\n{class_map_df.component_type.value_counts()}\")\nprint(f\"Total combinations: {pd.DataFrame(train_df.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'])).shape[0]}\")","b0b281d9":"cm_gr = class_map_df.loc[(class_map_df.component_type=='grapheme_root'), 'component'].values\ncm_vd = class_map_df.loc[(class_map_df.component_type=='vowel_diacritic'), 'component'].values  \ncm_cd = class_map_df.loc[(class_map_df.component_type=='consonant_diacritic'), 'component'].values   \n\nprint(f\"grapheme root:\\n{15*'-'}\\n{cm_gr}\\n\\n vowel discritic:\\n{18*'-'}\\n{cm_vd}\\n\\n consonant diacritic:\\n{20*'-'}\\n {cm_cd}\")","a59fceb3":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","cad66e4b":"most_frequent_values(train_df)","055444f2":"most_frequent_values(test_df)","bdaa68d6":"def plot_count(feature, title, df, size=1):\n    '''\n    Plot count of classes of selected feature; feature is a categorical value\n    param: feature - the feature for which we present the distribution of classes\n    param: title - title to show in the plot\n    param: df - dataframe \n    param: size - size (from 1 to n), multiplied with 4 - size of plot\n    '''\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show() ","48d3050d":"plot_count('grapheme_root', 'grapheme_root (first most frequent 20 values - train)', train_df, size=4)","ae8e196f":"plot_count('vowel_diacritic', 'vowel_diacritic (train)', train_df, size=3)","d5dfbe1e":"plot_count('consonant_diacritic', 'consonant_diacritic (train)', train_df, size=3)","2a83806c":"def plot_count_heatmap(feature1, feature2, df, size=1):  \n    '''\n    Heatmap showing the distribution of couple of features\n    param: feature1 - ex: vowel_diacritic\n    param: feature2 - ex: consonant_diacritic\n    '''\n    tmp = train_df.groupby([feature1, feature2])['grapheme'].count()\n    df = tmp.reset_index()\n    df\n    df_m = df.pivot(feature1, feature2, \"grapheme\")\n    f, ax = plt.subplots(figsize=(9, size * 4))\n    sns.heatmap(df_m, annot=True, fmt='3.0f', linewidths=.5, ax=ax)","d6fd4204":"plot_count_heatmap('vowel_diacritic','consonant_diacritic', train_df)","3fedc4d9":"plot_count_heatmap('grapheme_root','consonant_diacritic', train_df, size=8)","4a95d0e5":"plot_count_heatmap('grapheme_root','vowel_diacritic', train_df, size=8)","f2279834":"def display_image_from_data(data_df, size=5):\n    '''\n    Display grapheme images from sample data\n    param: data_df - sample of data\n    param: size - sqrt(sample size of data)\n    '''\n    plt.figure()\n    fig, ax = plt.subplots(size,size,figsize=(12,12))\n    # we show grapheme images for a selection of size x size samples\n    for i, index in enumerate(data_df.index):\n        image_id = data_df.iloc[i]['image_id']\n        flattened_image = data_df.iloc[i].drop('image_id').values.astype(np.uint8)\n        unpacked_image = PIL.Image.fromarray(flattened_image.reshape(137, 236))\n\n        ax[i\/\/size, i%size].imshow(unpacked_image)\n        ax[i\/\/size, i%size].set_title(image_id)\n        ax[i\/\/size, i%size].axis('on')","116e195c":"display_image_from_data(train_0_df.sample(25))","e189e68d":"display_image_from_data(train_1_df.sample(16), size = 4)","88696f1c":"def display_writting_variety(data_df=train_0_df, grapheme_root=72, vowel_diacritic=0,\\\n                             consonant_diacritic=0, size=5):\n    '''\n    This function get a set of grapheme root, vowel diacritic and consonant diacritic\n    and display a sample of 25 images for this grapheme\n    param: data_df - the dataset used as source of data\n    param: grapheme_root - the grapheme root label\n    param: vowel_diacritic - the vowel diacritic label\n    param: consonant_diacritic - the consonant diacritic label \n    param: size - sqrt(number of images to show)\n    '''\n    sample_train_df = train_df.loc[(train_df.grapheme_root == grapheme_root) & \\\n                                  (train_df.vowel_diacritic == vowel_diacritic) & \\\n                                  (train_df.consonant_diacritic == consonant_diacritic)]\n    print(f\"total: {sample_train_df.shape}\")\n    sample_df = data_df.merge(sample_train_df.image_id, how='inner')\n    print(f\"total: {sample_df.shape}\")\n    gr = sample_train_df.iloc[0]['grapheme']\n    cm_gr = class_map_df.loc[(class_map_df.component_type=='grapheme_root')& \\\n                             (class_map_df.label==grapheme_root), 'component'].values[0]\n    cm_vd = class_map_df.loc[(class_map_df.component_type=='vowel_diacritic')& \\\n                             (class_map_df.label==vowel_diacritic), 'component'].values[0]    \n    cm_cd = class_map_df.loc[(class_map_df.component_type=='consonant_diacritic')& \\\n                             (class_map_df.label==consonant_diacritic), 'component'].values[0]    \n    \n    print(f\"grapheme: {gr}, grapheme root: {cm_gr}, vowel discritic: {cm_vd}, consonant diacritic: {cm_cd}\")\n    sample_df = sample_df.sample(size * size)\n    display_image_from_data(sample_df, size=size)","43925ee0":"display_writting_variety(train_0_df,72,1,1,4)","6b4dc8c0":"display_writting_variety(train_0_df,64,1,2,4)","e0cfcdf1":"display_writting_variety(train_1_df,13,0,0,4)","5fe0aca6":"display_writting_variety(train_1_df,23,3,2,4)","ea332276":"Most frequent test values.","5750014d":"We look now to the combinations of consonant diacritic and grapheme roots.","952c10b6":"We can observe there is a large variety of writting for the selected graphemes.","8c513019":"We have both csv files and parquet files.  \nWe will start by exploring csv files and will follow with parquet files.","1606e2bf":"Let's follow by investigating the most frequent values.","a4471c90":"## Unique values\n\nWe look here to the distribution of grapheme roots, vowel diacritics and consonant diacritics.","24375b06":"Let's apply this function, this time to show not random graphemes, but the same grapheme, with different writing.   \n\nFor this we create a second function, to perform the sampling (based on variation of grapheme root, vowel diacritic and consonant diacritic, as parameters to the function).","08f694f6":"Here is the combinations of vowel diacritic and grapheme roots.","be9e7ed0":"Let's see first what consonant diacritics and vowel diacritics appears together.","1ceb48e9":"We show also a sample from the second set of images and with fewer samples size (16).","28d58085":"## Data distribution\n\nLet's start by viewing each grapheme.\n\nLet's show the grapheme roots first.","d0c2d9ed":"Most frequent train values.","42ce1faa":"We follow how with the parquet files. We will read only two of the parquet files for now, first train file.","cfd266dc":"# Prepare for data analysis\n\n## Load packages","da140263":"## Check the data\n\nWe verify what data is available.","095b0059":"Each `train_image_data_x` (x = 0...3) contains **50210** rows and **32333** columns - size of each image being: **(137, 236)**. Totally there are **50210** x **4** = **200840** rows in the training set.  \n\nWe also read one of the test files.\n","b524cf6a":"Let's show now distribution of combinations of features. We create a function to show a heatmap.","67f98691":"## Inspect grapheme images\n\n\nWe define a function to show a sample of size * size (ex: 5 x 5 = 25) handwritten graphemes.","07f54565":"<h1>Bengali.AI Handwritten Grapheme - Getting Started<\/h1>\n\n# Introduction\n\nBengali is the 5th most spoken language in the world with hundreds of million of speakers. Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English\u2019s 250 graphemic units).\n\nBangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education.\n\nFor this competition, we are given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics.","d28041cf":"Let's look now to the distribution of class values.","4b2e81e9":"# Data exploration\n\nWe start with the few csv files.","7f9378ae":"We apply the function for few combinations of grapheme root, vowel diacritic and consonant diacritic."}}