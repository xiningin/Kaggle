{"cell_type":{"6b6a0942":"code","e8b2c47c":"code","2fc90a8d":"code","58972b30":"code","7f2c0bca":"code","26d2ef7d":"code","2b2d7a2a":"code","9a6b1a5f":"code","6ce49c0a":"code","2834614f":"code","488749f7":"code","c3aa5daa":"code","633e679c":"code","08bdf186":"code","e47eedd6":"code","f5c04a85":"code","2154ef26":"code","752b1384":"code","74590aac":"code","b5287419":"code","8ce5ee70":"code","c8a94603":"code","ad45272e":"code","ec7167fd":"code","56f4eeaa":"code","8d9dfc6a":"code","2ea25f74":"markdown","0074ff07":"markdown","0649f35b":"markdown"},"source":{"6b6a0942":"# Import the required packages\nimport os\nimport numpy as np \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pathlib\nimport librosa.display\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split","e8b2c47c":"# a package to compute speech features, implemented using tf.keras\ntry:\n    from spela.spectrogram import Spectrogram \n    from spela.melspectrogram import Melspectrogram\nexcept:\n    !pip install spela\n    from spela.spectrogram import Spectrogram \n    from spela.melspectrogram import Melspectrogram","2fc90a8d":"# disable eager execution, my model couldn't train well while on eager mode\ntf.compat.v1.disable_eager_execution()","58972b30":"# Get the data directories\ndata_dir = \"..\/input\/speaker-recognition-dataset\/16000_pcm_speeches\/\"","7f2c0bca":"# inspect the folders inside the dataset\nos.listdir(data_dir)","26d2ef7d":"# for now we are concerned with the four speakers\n# lets get as a sample data from one of the speakers\nnelson_madela = [item for item in os.listdir(data_dir + \"Nelson_Mandela\")]\nnelson_madela[:10]","2b2d7a2a":"# lets create a function that takes in a raw wavfile and computes a spectrogram then plots it\ndef compute_spectrogram_melspectrogram_and_plot(wav_dir, compute_type):\n    with tf.compat.v1.Session(graph=tf.compat.v1.Graph()) as sess:\n        wav_filename_placeholder = tf.compat.v1.placeholder(tf.compat.v1.string, [])\n        wav_loader = tf.io.read_file(wav_filename_placeholder)\n        wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n        wav_data = sess.run(\n        wav_decoder, feed_dict={\n            wav_filename_placeholder: wav_dir\n        }).audio.flatten()\n        sess.close()\n    # audio has a sample rate of 16000 and the produced wav has a shape of (16000, 1)\n    # reshape to (1, 1600)\n    wav = wav_data.reshape(1, 16000)\n    wav_new = wav[np.newaxis, :] # introduce a new axis to have a shape of (1, 1, 16000)\n    height = wav_new.shape[1]\n    width = wav_new.shape[2]\n    # create a model to compute spectrogram\n    model = tf.keras.Sequential() \n    if compute_type == \"spectrogram\":\n        model.add(Spectrogram(n_dft=512, n_hop=256, input_shape=(height, width),\n                            return_decibel_spectrogram=True, power_spectrogram=2.0,\n                            trainable_kernel=False, name='static_stft'))\n    elif compute_type == \"melspectrogram\":\n        model.add(Melspectrogram(sr=16000, n_mels=128,n_dft=512, n_hop=256,\n                            input_shape=(height, width), return_decibel_melgram=True,\n                            trainable_kernel=False, name='melgram'))\n   \n    # producing a spectrogram\/melspectrogram from the model\n    pred = model.predict(x=wav_new)\n\n    if tf.keras.backend.image_data_format() == \"channel_first\":\n        result = pred[0, 0]\n    else:\n        result = pred[0, :, :, 0]\n\n    # show the spectrogram\/melspectrogram\n    librosa.display.specshow(result, y_axis='linear', sr=16000)","9a6b1a5f":"compute_spectrogram_melspectrogram_and_plot(data_dir + \"Nelson_Mandela\/\" + nelson_madela[0], \"spectrogram\")","6ce49c0a":"compute_spectrogram_melspectrogram_and_plot(data_dir + \"Nelson_Mandela\/\" + nelson_madela[0], \"melspectrogram\")","2834614f":"# get wav paths\ndef get_wav_paths(speaker):\n    speaker_path = data_dir + speaker\n    all_paths = [item for item in os.listdir(speaker_path)]\n    return all_paths","488749f7":"nelson_mandela_paths = get_wav_paths(\"Nelson_Mandela\")\nmargaret_thatcher_paths = get_wav_paths(\"Magaret_Tarcher\")\nbenjamin_netanyau_paths = get_wav_paths(\"Benjamin_Netanyau\")\njens_stoltenberg_paths = get_wav_paths( 'Jens_Stoltenberg')\njulia_gillard_paths = get_wav_paths(\"Julia_Gillard\")","c3aa5daa":"# load the data\ndef load_wav(wav_path, speaker):\n    with tf.compat.v1.Session(graph=tf.compat.v1.Graph()) as sess:\n        wav_path = data_dir +speaker + \"\/\"+ wav_path\n        wav_filename_placeholder = tf.compat.v1.placeholder(tf.compat.v1.string, [])\n        wav_loader = tf.io.read_file(wav_filename_placeholder)\n        wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n        wav_data = sess.run(\n            wav_decoder, feed_dict={\n                wav_filename_placeholder: wav_path\n            }).audio.flatten().reshape((1, 16000))\n        sess.close()\n    return wav_data\n    ","633e679c":"# create training data\ndef generate_training_data(speaker_paths, speaker, label):\n    wavs, labels = [], []\n    for i in tqdm(speaker_paths):\n        wav = load_wav(i, speaker)\n        wavs.append(wav)\n        labels.append(label)\n    return wavs, labels\n","08bdf186":"nelson_mandela_wavs, nelson_mandela_labels = generate_training_data(nelson_mandela_paths, \"Nelson_Mandela\", 0) \nmargaret_thatcher_wavs, margaret_thatcher_labels = generate_training_data(margaret_thatcher_paths, \"Magaret_Tarcher\", 1) \nbenjamin_netanyau_wavs, benjamin_netanyau_labels = generate_training_data(benjamin_netanyau_paths, \"Benjamin_Netanyau\", 2) \njens_stoltenberg_wavs, jens_stoltenberg_labels = generate_training_data(jens_stoltenberg_paths, \"Jens_Stoltenberg\", 3) \njulia_gillard_wavs, julia_gillard_labels = generate_training_data(julia_gillard_paths, \"Julia_Gillard\", 4) ","e47eedd6":"# remove the extra wav for Julia Gillard\njulia_gillard_labels = julia_gillard_labels[1:]\njulia_gillard_wavs = julia_gillard_wavs[1:]","f5c04a85":"all_wavs = nelson_mandela_wavs + margaret_thatcher_wavs + benjamin_netanyau_wavs + jens_stoltenberg_wavs + julia_gillard_wavs\nall_labels = nelson_mandela_labels + margaret_thatcher_labels + benjamin_netanyau_labels + jens_stoltenberg_labels + julia_gillard_labels","2154ef26":"# split the dataset into trainin and testing set\\\ntrain_wavs, test_wavs, train_labels, test_labels = train_test_split(all_wavs, all_labels, test_size=0.2)","752b1384":"train_x, train_y = np.array(train_wavs), np.array(train_labels)\ntest_x, test_y = np.array(test_wavs), np.array(test_labels)","74590aac":"train_y = tf.keras.utils.to_categorical(train_y)\ntest_y = tf.keras.utils.to_categorical(test_y)","b5287419":"# create a model\ndef create_model(speech_feature):\n    model = tf.keras.Sequential()\n    if speech_feature == \"spectrogram\":\n        model.add(Spectrogram(n_dft=512, n_hop=256, input_shape=(1, 16000),\n                            return_decibel_spectrogram=True, power_spectrogram=2.0,\n                            trainable_kernel=False, name='static_stft'))\n    elif speech_feature == \"melspectrogram\":\n        model.add(Melspectrogram(sr=16000, n_mels=128,n_dft=512, n_hop=256,\n                            input_shape=(1 , 16000),return_decibel_melgram=True,\n                            trainable_kernel=False, name='melgram'))\n   \n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4)\n            , loss = \"categorical_crossentropy\"\n            , metrics = [\"accuracy\"])\n    return model","8ce5ee70":"# spectrogam model\nmodel = create_model(\"spectrogram\")","c8a94603":"model.summary()","ad45272e":"model.fit(x=train_x, y=train_y, epochs=10, validation_data=(test_x, test_y))","ec7167fd":"# melspectrogram model\nmodel = create_model(\"melspectrogram\")","56f4eeaa":"model.summary()","8d9dfc6a":"model.fit(x=train_x, y=train_y, epochs=10, validation_data=(test_x, test_y))","2ea25f74":"# Create a simple model","0074ff07":"# Process training dataset","0649f35b":"we will be using [spela](https:\/\/github.com\/kongkip\/spela) a package used to compute speech features \ne.g spectrograms and melspectrograms implemented using tf.keras to take advantage of GPU during computations"}}