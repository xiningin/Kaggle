{"cell_type":{"7b9a8ba0":"code","c4106871":"code","9bae7ac7":"code","08cf4bb7":"code","be832ce8":"code","aab62a8d":"code","a97afddf":"code","80bc49d2":"code","2a67cd26":"code","4b58844c":"code","2231f06e":"code","3527c0bc":"code","d71e0d94":"code","efc58207":"code","5bfcd139":"code","e607bffd":"code","5b8ff017":"code","38f85fe6":"code","d60f22f8":"code","7658b07f":"code","5f31174b":"code","220a75ea":"code","ad331543":"code","4da1786f":"code","dd8f538e":"code","dc90fd6a":"code","e8c978aa":"code","abc71090":"code","57ba9400":"code","5eb894c3":"code","ab643269":"code","9c58cbff":"code","8e758b7e":"code","7e09a27a":"code","a3713c65":"code","039291ed":"code","d3d66afe":"code","f271f50e":"code","83f27f57":"code","441edf6d":"code","bb896182":"code","a6f382dc":"code","08717a5e":"code","331936d4":"code","f3b92824":"code","6a4dc3c1":"code","098f1fdf":"code","2e398b7c":"code","fb4528a5":"code","301dad00":"code","16241f55":"code","d27bdb45":"code","e9246498":"code","c1329878":"code","38c6e9a6":"code","ef126afa":"code","ddf1ae06":"code","66e3b3c4":"code","02dbcb3d":"code","b107dbaa":"code","bec7547d":"code","6ce995c3":"code","a9b3565f":"code","8aa0ff19":"code","351a9867":"code","66739e9d":"code","e30d8ad4":"code","0cb9d098":"code","2829829b":"code","09d371bc":"code","20f8bd7d":"code","79d95d5d":"code","4dd59fba":"markdown","b9ee6712":"markdown","1d0a56f6":"markdown","b0c2923c":"markdown","babf19b9":"markdown","319837a7":"markdown","9de7320e":"markdown","0f3dfd64":"markdown","6355ab6f":"markdown","db7b40fe":"markdown","b380e9cd":"markdown","c6719520":"markdown","0982a6ab":"markdown","3fec389c":"markdown","ddf04266":"markdown","96382cbb":"markdown","c0226599":"markdown","fb04d971":"markdown"},"source":{"7b9a8ba0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4106871":"ratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')","9bae7ac7":"rows=ratings.userId.unique()\ncolumns=ratings.movieId.unique()","08cf4bb7":"myData = np.array([0.0 for i in range(671*9066)])\nmydf = pd.DataFrame(myData.reshape(671, -1))","be832ce8":"mydf.columns=columns\nmydf.index=rows\nmydf.head()","aab62a8d":"ratings.head()","a97afddf":"for i in range(100004):\n    mydf.loc[ratings.loc[i,'userId'],ratings.loc[i,'movieId']] = ratings.loc[i,'rating']","80bc49d2":"user_list=list(mydf.index)\nmovie_list=list(mydf.columns)","2a67cd26":"from scipy.sparse import coo_matrix\n\nR = coo_matrix(mydf.values)\n\nprint (\"R Shape::\", R.shape)\nprint (\"R Columns::\", R.col)\nprint (\"R Rows::\",R.row)","4b58844c":"M,N=R.shape\n# No of Factors - 3\nK=3\n# using random values of P and Q \nP=np.random.rand(M,K)\nQ=np.random.rand(K,N)","2231f06e":"import random\ntrain_index=random.sample([i for i in range(100004)],80000)","3527c0bc":"test_index = set([i for i in range(100004)])-set(train_index)","d71e0d94":"test_index=list(test_index)","efc58207":"R.data[test_index]","5bfcd139":"from numpy.linalg import norm\n\ndef error(R,P,Q,index,lamda=0.02):\n    ratings = R.data[index]\n    rows = R.row[index]\n    cols = R.col[index]\n    e = 0 \n    for ui in range(len(ratings)):\n        rui=ratings[ui]\n        u = rows[ui]\n        i = cols[ui]\n        if rui>0:\n            e= e + pow(rui-np.dot(P[u,:],Q[:,i]),2)+\\\n                lamda*(pow(norm(P[u,:]),2)+pow(norm(Q[:,i]),2))\n    return e","e607bffd":"error(R,P,Q,train_index)","5b8ff017":"error(R,P,Q,test_index)","38f85fe6":"def SGD(R,K,train_index,test_index,lamda=0.02,steps=10,gamma=0.001):\n    \n    M,N = R.shape\n    P = np.random.rand(M,K)\n    Q = np.random.rand(K,N)\n    \n    rmse = np.sqrt(error(R,P,Q,test_index,lamda)\/len(R.data[test_index]))\n    print(\"Initial RMSE: \"+str(rmse))\n    \n    for step in range(steps):\n        for ui in range(len(R.data[train_index])):\n            rui=R.data[ui]\n            u = R.row[ui]\n            i = R.col[ui]\n            if rui>0:\n                eui=rui-np.dot(P[u,:],Q[:,i])\n                P[u,:]=P[u,:]+gamma*2*(eui*Q[:,i]-lamda*P[u,:])\n                Q[:,i]=Q[:,i]+gamma*2*(eui*P[u,:]-lamda*Q[:,i])\n        rmse = np.sqrt(error(R,P,Q,test_index,lamda)\/len(R.data[test_index]))\n        if rmse<0.5:\n            break\n    print(\"Final RMSE: \"+str(rmse))\n    return P,Q","d60f22f8":"for k in range(10,20):\n    print('k=',k)\n    P,Q=SGD(R,K=k,train_index=train_index,test_index=test_index,gamma=0.0007,lamda=0.01, steps=100)\n    print('------------------------')","7658b07f":"value=[0.9796,0.9171,0.9081,0.8918,0.8866,0.8903,0.9021,0.9488,0.9768,0.9924]\nplt.plot(range(10,20),value)\nplt.xlabel(\"Number of latent factors\")\nplt.ylabel(\"RMSE on test set\")\nplt.show()","5f31174b":"P,Q=SGD(R,K=14,train_index=train_index,test_index=test_index,gamma=0.0007,lamda=0.01, steps=100)","220a75ea":"np.sqrt(error(R,P,Q,test_index,0.01)\/len(R.data[test_index]))","ad331543":"est_df = pd.DataFrame(np.round(np.matmul(P,Q),4),columns=movie_list, index=user_list)","4da1786f":"metadata=pd.read_csv(\"..\/input\/the-movies-dataset\/movies_metadata.csv\")\nmetadata.shape","dd8f538e":"metadata['genres'].value_counts()","dc90fd6a":"links=pd.read_csv(\"..\/input\/the-movies-dataset\/links.csv\")\nlinks","e8c978aa":"metadata['imdb_id']=metadata['imdb_id'].str.lstrip(\"tt\")\nmetadata['imdb_id']=metadata['imdb_id'].str.lstrip(\"0\")\nmetadata.head()","abc71090":"metadata.drop(metadata[metadata['imdb_id'].isna()].index,inplace=True)","57ba9400":"metadata.shape","5eb894c3":"metadata=metadata[['imdb_id','genres']]","ab643269":"metadata","9c58cbff":"metadata.drop(metadata.loc[metadata['imdb_id']=='',:].index,inplace=True)","8e758b7e":"metadata","7e09a27a":"metadata['imdb_id']=metadata['imdb_id'].astype(int)","a3713c65":"metadata.drop_duplicates('imdb_id',keep='first',inplace=True)","039291ed":"metadata","d3d66afe":"links['imdb_id']=links['imdbId']","f271f50e":"data=pd.merge(metadata,links,how='left',on='imdb_id')","83f27f57":"data","441edf6d":"data.loc[data['movieId'].isna(),:]","bb896182":"data.drop(data.loc[data['movieId'].isna(),:].index,inplace=True)","a6f382dc":"data","08717a5e":"data['genres'][0]","331936d4":"list(data.loc[data['genres'].str.len()==264,:]['genres'])","f3b92824":"from ast import literal_eval\ndata['genres'] = data['genres'].apply(literal_eval)","6a4dc3c1":"set={''}\nfor i in data.index:\n    for j in data['genres'][i]:\n        set.add(j['name'])\n","098f1fdf":"set.remove('')","2e398b7c":"set","fb4528a5":"for column in set:\n    data[column]=0\ndata","301dad00":"for i in data.index:\n    for j in data['genres'][i]:\n        data.loc[i,j['name']] = 1","16241f55":"data","d27bdb45":"for genre in set:\n    mydf[genre+'_count']=0\n    mydf[genre+'_score']=0","e9246498":"mydf","c1329878":"data.set_index('movieId',inplace=True)","38c6e9a6":"data.index=data.index.astype(int)","ef126afa":"data","ddf1ae06":"for i in train_index:  # Traversing the training set\n    for genre in set:  # Traversing the genres\n        if mydf.columns[R.col[i]] in data.index:  # Whether the table 'data' contains such movieid\n            if data.loc[mydf.columns[R.col[i]],genre]==1:  # Whether the movie belongs to such genre\n                mydf.loc[mydf.index[R.row[i]],genre+'_count'] += 1  # Update the user's preference\n                mydf.loc[mydf.index[R.row[i]],genre+'_score'] += R.data[i]","66e3b3c4":"mydf","02dbcb3d":"for genre in set:\n    mydf[genre]=0\nmydf","b107dbaa":"for i in mydf.index:\n    for genre in set:\n        mydf.loc[i,genre]=mydf.loc[i,genre+'_score']\/mydf.loc[i,genre+'_count']","bec7547d":"classified_df=mydf.loc[:,mydf.columns[-20:]]","6ce995c3":"classified_df=classified_df.fillna(0)","a9b3565f":"classified_df","8aa0ff19":"R1 = coo_matrix(classified_df.values)","351a9867":"P1,Q1=SGD(R1,K=14,train_index=[i for i in range(len(R1.data))],test_index=[i for i in range(len(R1.data))],gamma=0.0007,lamda=0.01, steps=100)","66739e9d":"est_df1 = pd.DataFrame(np.round(np.matmul(P1,Q1),4),columns=list(set), index=user_list)","e30d8ad4":"est_df_95=est_df.copy()\nest_df_90=est_df.copy()\nest_df_85=est_df.copy()\nest_df_80=est_df.copy()\nest_df_75=est_df.copy()\nest_df_70=est_df.copy()\nest_df_65=est_df.copy()\nest_df_60=est_df.copy()","0cb9d098":"for i in est_df.index:\n    for movieid in est_df.columns:\n        count=0\n        sum=0\n        if movieid in data.index:\n            for genre in set:\n                if data.loc[movieid,genre]==1:\n                    count += 1\n                    sum += est_df1.loc[i,genre]\n        if count!=0:\n            est_df_95.loc[i,movieid]=0.05*sum\/count+0.95*est_df.loc[i,movieid]\n            est_df_90.loc[i,movieid]=0.1*sum\/count+0.9*est_df.loc[i,movieid]\n            est_df_85.loc[i,movieid]=0.15*sum\/count+0.85*est_df.loc[i,movieid]\n            est_df_80.loc[i,movieid]=0.2*sum\/count+0.8*est_df.loc[i,movieid]\n            est_df_75.loc[i,movieid]=0.25*sum\/count+0.75*est_df.loc[i,movieid]\n            est_df_70.loc[i,movieid]=0.3*sum\/count+0.7*est_df.loc[i,movieid]\n            est_df_65.loc[i,movieid]=0.35*sum\/count+0.65*est_df.loc[i,movieid]\n            est_df_60.loc[i,movieid]=0.4*sum\/count+0.6*est_df.loc[i,movieid]","2829829b":"from math import sqrt\nratings = R.data[test_index]\nrows = R.row[test_index]\ncols = R.col[test_index]\ne_95 = 0\ne_90 = 0\ne_85 = 0\ne_80 = 0\ne_75 = 0\ne_70 = 0\ne_65 = 0\ne_60 = 0\nfor ui in range(len(ratings)):\n    rui=ratings[ui]\n    u = rows[ui]\n    i = cols[ui]\n    if rui>0:\n        e_95= e_95 + pow(rui-est_df_95.iloc[u,i],2)\n        e_90= e_90 + pow(rui-est_df_90.iloc[u,i],2)\n        e_85= e_85 + pow(rui-est_df_85.iloc[u,i],2)\n        e_80= e_80 + pow(rui-est_df_80.iloc[u,i],2)\n        e_75= e_75 + pow(rui-est_df_75.iloc[u,i],2)\n        e_70= e_70 + pow(rui-est_df_70.iloc[u,i],2)\n        e_65= e_65 + pow(rui-est_df_65.iloc[u,i],2)\n        e_60= e_60 + pow(rui-est_df_60.iloc[u,i],2)\n#RMSE_improved=sqrt(e\/len(test_index))\n#RMSE_improved","09d371bc":"RMSE_improved_95=sqrt(e_95\/len(test_index))\nprint(RMSE_improved_95)\nRMSE_improved_90=sqrt(e_90\/len(test_index))\nprint(RMSE_improved_90)\nRMSE_improved_85=sqrt(e_85\/len(test_index))\nprint(RMSE_improved_85)\nRMSE_improved_80=sqrt(e_80\/len(test_index))\nprint(RMSE_improved_80)\nRMSE_improved_75=sqrt(e_75\/len(test_index))\nprint(RMSE_improved_75)\nRMSE_improved_70=sqrt(e_70\/len(test_index))\nprint(RMSE_improved_70)\nRMSE_improved_65=sqrt(e_65\/len(test_index))\nprint(RMSE_improved_65)\nRMSE_improved_60=sqrt(e_60\/len(test_index))\nprint(RMSE_improved_60)","20f8bd7d":"from math import sqrt\nratings = R.data[test_index]\nrows = R.row[test_index]\ncols = R.col[test_index]\ne = 0 \nfor ui in range(len(ratings)):\n    rui=ratings[ui]\n    u = rows[ui]\n    i = cols[ui]\n    if rui>0:\n        e= e + pow(rui-est_df.iloc[u,i],2)\nRMSE_original=sqrt(e\/len(test_index))\nRMSE_original","79d95d5d":"import matplotlib.pyplot as plt\nx=[60,65,70,75,80,85,90,95]\nvalue=[RMSE_improved_60,RMSE_improved_65,RMSE_improved_70,RMSE_improved_75,RMSE_improved_80,RMSE_improved_85,RMSE_improved_90,RMSE_improved_95]\nplt.plot(x,value)\nplt.xlabel(\"Weight of basic Fatent Factor Model (%)\")\nplt.ylabel(\"RMSE on test set\")","4dd59fba":"### Dropping Reduplicative Rows","b9ee6712":"## Using Gradient Descending Method to Approximate P, Q","1d0a56f6":"### Dropping rows with ''","b0c2923c":"# Genre-based Model","babf19b9":"We generate the rating table called 'mydf' whose rows refer to the users and the columns represent the movies. Obviously, the matrix is a sparse one, which makes us to use the Latent Factor Model rather than the item-based collaborative filtering.","319837a7":"# Combining Two Models","9de7320e":"### Dropping Rows with NaN movieId","0f3dfd64":"## Preprocessing","6355ab6f":"### Join Two Tables","db7b40fe":"# Basic Latent Factor Model","b380e9cd":"Here we define the function 'error' to calculate the square root of the average of squared errors (RMSE) of the original data and the predicted data. We also define function \u2018SGD\u2019, which helps calculate the best P and Q. Hyperparameter K is selected based on RMSE on the validation set.","c6719520":"## Preprocessing","0982a6ab":"## Calculating Users' Preference for Genres","3fec389c":"Here we combine the two models with different ratio and compare the RMSE on the validation set to determine the best ratio.","ddf04266":"### Striping \"tt0\" in metadata","96382cbb":"## Splitting the Dataset into Training Set and Validation Set","c0226599":"A basic and important idea is to split the full data into training set and validation set. The training set is used to train the matrix P and Q, and the validation set is used to determine the hyperparameter K. Due to the large amount of data, we simply split the full data into training set and validation set and this process can be further improved using cross validation.","fb04d971":"### Dropping rows with NaN"}}