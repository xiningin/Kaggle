{"cell_type":{"bc9a48ac":"code","107c3902":"code","c1620065":"code","cae459c1":"code","415dcadd":"code","9b0c17c5":"code","8e83ceb7":"code","d23d36dd":"code","8d523cdb":"code","e3004006":"code","87295fc6":"code","6f25c40e":"code","6f82c072":"code","1250c9f8":"code","87a35983":"code","a4e68768":"code","860eb7cc":"code","6cfb32c6":"code","c5ff4e4e":"code","c8fe94ae":"code","0d48550a":"code","b1d24db0":"code","90322874":"code","86dcdc31":"code","c9a157d0":"code","d358c612":"code","a47bbff8":"code","0fabd471":"markdown","18039c18":"markdown","7f0062b6":"markdown","5cd4a32d":"markdown","5c05aec8":"markdown","7d4def28":"markdown","99df90de":"markdown","dbec3286":"markdown","b418b53e":"markdown","35c7d899":"markdown","c6538111":"markdown","ddfa92a7":"markdown","efaeb5a1":"markdown","8c7d865e":"markdown","7442ef4a":"markdown","7f95a0b3":"markdown","35d5327f":"markdown","9f463742":"markdown","92885167":"markdown","9fb5a6c2":"markdown","a38f660d":"markdown"},"source":{"bc9a48ac":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport pandas_profiling\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","107c3902":"LOCAL = '..\/input\/'\n\ntrain_features = pd.read_csv(LOCAL +'train_features.csv')\ntrain_labels = pd.read_csv(LOCAL + 'train_labels.csv')\ntest_features = pd.read_csv(LOCAL + 'test_features.csv')\nsample_submission = pd.read_csv(LOCAL + 'sample_submission.csv')\n\nassert train_features.shape == (59400, 40)\nassert train_labels.shape == (59400, 2)\nassert test_features.shape == (14358, 40)\nassert sample_submission.shape == (14358, 2)","c1620065":"train_features.profile_report()","cae459c1":"print(\"Initial search space of our training data is {}\".format(train_features.shape[0] * train_features.shape[1]))","415dcadd":"train_labels['status_group'].value_counts()","9b0c17c5":"# extracting numerical column names and categorical column names from the feature columns\nnum_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train_features.select_dtypes(exclude=[np.number]).columns.tolist()\n\n# Sanity check to make sure the number of numerical features and categorical features\n# add up to the total number of features\nassert (len(num_cols) + len(cat_cols)) == train_features.shape[1]\n\n# from pandas profiling we know permit and public_meeting are boolean features.  And we make a list of rejected to reject.\nbool_cols = ['permit', 'public_meeting']\nrejected = ['quantity_group', 'recorded_by']\n\n# print out numerical features and categorical features\nprint(num_cols)\nprint(cat_cols)","8e83ceb7":"print(\"{} numerical features and {} categorical features have missing values, of which {} are boolean features\".format(\n    train_features[num_cols].isnull().any().sum(), \n    train_features[cat_cols].isnull().any().sum(),\n    train_features[bool_cols].isnull().any().sum()))","d23d36dd":"def preproc(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Naive preprocessing the input data by fixing missing values and dropping rejected features.\n    Boolean features and categorical features are processed differently.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n    \n    Returns\n    ----------\n    df : pandas.DataFrame\n    \"\"\"\n    \n    # drop the rejected columns first.\n    df = df.drop(columns=rejected)\n    \n    # since boolean features are included in categorical features, lets fillna of those first.\n    # we will prudently assume nan\/missing permits or public_meetings as False. \n    df[bool_cols] = df[bool_cols].fillna(False)\n    \n    # with boolean part of the categorical features fixed, we can proceed to fill missing values.\n    # only categorial features has missing values, so we can just fillna directly on the whole df\n    df = df.fillna('unknown')\n    \n    # date_recorded is in datetime format. We need to change that into numbers for the algorithm\n    # to work. Let's change the definition of date_recorded to number of days since its recorded\n    # between then and today. \n    today = dt.datetime.today()   \n    df.date_recorded = pd.to_datetime(df.date_recorded,format = '%Y-%m-%d')\n    df.date_recorded = (df.date_recorded - today).dt.days.abs()\n        \n    return df","8d523cdb":"X = preproc(train_features)\ny = train_labels.status_group\nprint(\"search space is now {}\".format(X.shape[0] * X.shape[1]))","e3004006":"logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=500)\nencoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\nscaler = StandardScaler(with_mean=False)\n\npipe = Pipeline(steps=[('encoder', encoder),\n                       ('scaler', scaler),\n                       ('logreg', logreg)\n                       ])","87295fc6":"%time pipe.fit(X, y);\npipe.score(X, y)","6f25c40e":"pipe.score(X, y)","6f82c072":"X_FI = X[num_cols+['date_recorded']]\nX_FI.columns\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\n%time forest.fit(X_FI, y)\nimportances = forest.feature_importances_\n\n# standard deviation of importance of each features\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\n\n# sort the importances by rank\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_FI.shape[1]):\n    print(\"%d. %s, feature #%d (%f)\" % (f + 1, X_FI.columns[indices[f]], indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_FI.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_FI.shape[1]), indices)\nplt.xlim([-1, X_FI.shape[1]])\nplt.show()","1250c9f8":"sns.relplot(x='longitude', y='latitude', hue='gps_height',data=train_features, alpha=0.1);","87a35983":"def clean_numeric(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Detailed cleaning of numeric dataframes. This function first change the 0s to np.nan and fill\n    iteratively fillna with the means of all categorical filters down to only using the means\n    of the least granular categorical feature, 'basin'. This function than also fix construction \n    year by changing the 0s to base year 1960.\n    \n    We elect to keep the original id, region_code, district_code, and num_private.  Also, this \n    function does not changing anything to amount_tsh since its not clear what that represents.  \n    \n    amount_tsh : Total static head (amount water available to waterpoint)            \n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n    \n    Returns\n    ----------\n    df : pandas.DataFrame\n    \"\"\"\n    \n    # construct overall location keys. The list is ordered from lowest cardinality to the highest so\n    # we can iterate fillna from the most granular means to lowest cardinality region codes.    \n    loc_keys = ['basin', 'region_code', 'district_code', 'lga', 'ward']\n    \n    # amount_tsh : Total static head (amount water available to waterpoint)        \n    # Not changing anything to amount_tsh since its not clear what that represents.  Also not changing\n    # district_code, region_code, id, and num_private.\n    \n    df.longitude = df.longitude.replace(0, np.nan)\n    df.latitude = df.latitude.replace(-2.000000e-08, np.nan)\n    df.gps_height = df.gps_height.replace(0, np.nan)\n    df.population = df.population.replace(0, np.nan)\n    \n    # Iteratively fillna with the means from the most granular area split to the most overall area split.\n    # https:\/\/stackoverflow.com\/questions\/19966018\/pandas-filling-missing-values-by-mean-in-each-group\n    for i in range(len(loc_keys), 0, -1):\n        df['longitude'] = df['longitude'].fillna(df.groupby(loc_keys[:i])['longitude'].transform('mean'))\n        df['latitude'] = df['latitude'].fillna(df.groupby(loc_keys[:i])['latitude'].transform('mean'))\n        df['gps_height'] = df['gps_height'].fillna(df.groupby(loc_keys[:i])['gps_height'].transform('mean'))\n        df['population'] = df['population'].fillna(df.groupby(loc_keys[:i])['population'].transform('mean'))\n    \n    # construction years has a lot of 0s. Outside of 0s, it starts at 1960. We will make the\n    # assumption that construction year for all the 0s\/unkowns as the base year 1960.\n    df['construction_year'] = df['construction_year'].apply(lambda x: 1960 if x == 0 else x)\n    \n    return df","a4e68768":"X = preproc(train_features)\nX = clean_numeric(X)\ny = train_labels.status_group\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=0, stratify=y)","860eb7cc":"sns.relplot(x='longitude', y='latitude', hue='gps_height',data=X, alpha=0.1);","6cfb32c6":"categorical_counts = X_train.select_dtypes(exclude='number').nunique().sort_values()\ncategorical_counts.plot.bar();\nhigh_cardi_cols = categorical_counts[categorical_counts > 150].index.tolist()\nlow_cardi_cols = categorical_counts[categorical_counts <= 150].index.tolist()","c5ff4e4e":"def clean_categorical(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Detailed cleaning of numeric dataframes. We change everything categorical labels to lower case\n    features. We also decide to collapse all unknown with other.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n    \n    Returns\n    ----------\n    df : pandas.DataFrame\n    \"\"\"\n    \n    # lower case everything\n    cat_cols = df.select_dtypes(exclude=[np.number, 'bool']).columns.tolist()\n    df[cat_cols] = df[cat_cols].applymap(lambda x: x.lower())\n    \n    # since for some categories there are both unknown and other categories, we feel we can collapse\n    # unknown with other since other can incoroporate unknown.\n    df = df.replace({'unknown':'other'})\n    \n    return df","c8fe94ae":"X = preproc(train_features)\nX = clean_numeric(X)\nX = clean_categorical(X)\ny = train_labels.status_group\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=45, stratify=y)","0d48550a":"logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=2000, C=100000.0)\nencoder = ce.OrdinalEncoder()#(use_cat_names=True)\nscaler = StandardScaler(with_mean=False)\n\nlr_pipe = Pipeline(steps=[('encoder', encoder),\n                          ('scaler', scaler),\n                          ('logreg', logreg)\n                         ])","b1d24db0":"%time lr_pipe.fit(X_train, y_train)\nlr_pipe.score(X_train, y_train)","90322874":"lr_pipe.score(X_val, y_val)","86dcdc31":"X = preproc(train_features)\nX = clean_numeric(X)\nX = clean_categorical(X)\ny = train_labels.status_group\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=45, stratify=y)","c9a157d0":"xgbc = XGBClassifier(random_state=45, objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx', \n                      num_class = 3, maximize = False, eval_metric = 'merror', eta = .1, verbosity=1,\n                      max_depth = 20, colsample_bytree = .4)\nencoder = ce.OrdinalEncoder()\n\nxgbc_pipe = Pipeline(steps=[('encoder', encoder),\n                            ('xgbc', xgbc)])","d358c612":"%time xgbc_pipe.fit(X_train, y_train)\nxgbc_pipe.score(X_train, y_train)","a47bbff8":"xgbc_pipe.score(X_val, y_val)","0fabd471":"and then we double check our score on the training data. The default scoring function of logistic regression in sklearn is accuracy.\n","18039c18":"## 2.1 Numeric","7f0062b6":"Lets make a preprocessing function. This function will also help us preprocess validation\/testing data when needed.","5cd4a32d":"# 1. Preprocessing","5c05aec8":"## 2.2 Categorical","7d4def28":"## 3.2 XGBoost","99df90de":"Logistic regression, with a pipeline to do ordinal encoding on categorical dataand standard scaling on numeric data.","dbec3286":"From pandas profile reports, we have noticed many categorical columns have very high cardinality. Those, if one-hot encoded, will become sparse data and significantly increase our search space without providing much values. We will encode them with OrdinalEncoder. For low cordinality values, we will OneHotEncode them. We will apply this encoding scheme using Pipeline.","b418b53e":"# 2. Detailing and Feature Engineering","35c7d899":"Using XGBoost Classifier. Since the target have three categories, functional, non functional, and functional need repair, we used multi:softmax with num_class=3. The eval_metric is set to merror for multi class classification error rate. And we like it deep.","c6538111":"We will use sklearn's pipeline library to hook the training process together. Since there are categorical features, we will first encode the data to turn all those categorical features into numbers.  Then, we will scale the numbers using standard scaler. This should help speedup the training process and quality. After encoding and scaling, we will shove the training data into logistic regressor.","ddfa92a7":"Before we start chopping, let's take at look at the baseline search space and the target we are shooting. Baseline search space for machine learning algorithm on tabular data is the size of the train_features.  \n\nAlso, we are going to split the features into numerical and categorical under the assumption that the two will be treated differently. \n\nFrom pandas_profiling, `quantity_group` is a recording of `quantity` and `recorded_by` is a constant value. `id` is 100% unique and likely to mark the id of each well. Would guess it contains no information. We will remove all three of these features.","efaeb5a1":"## 3.1 Logistic Regression","8c7d865e":"## 1.1 Naive Preprocessing Function","7442ef4a":"From our feature importance testing, we can see that `longitude`, `latitude`, `id`, `date_recorded`, `gps_height`, and `construction_year` are the top 6 relevant features. We will start with those. We note that while numerical categories do not contain any missing values, some of them have a lot of 0s or in the case of `latitude`, -2.000000e-08.  Also, `num_private` seem to have very low importance; looking at the pandas_profiling results, it only has 65 distinct values and over 99.9% are 0. We should drop `num_private`.","7f95a0b3":"So, with this naive preprocessing format, we are able to overfit the training data to a 1.0 accuracy in around 2 minute training time. This is much better than the previous buggy preprocessing, which only had 0.77 accuracy and 5 minute training time.\n\nBy overfitting to the training data, we have hopefully eliminated all potential errors compare to the last attempt. And we got the benefit of significant training time reduction. The testing accuracy came out to be 0.75 after submission.  \n\nFrom this point on, we do not have to worry about preprocessing. Instead, we can now focus on feature engineering and reducing overfitting with cross validation.","35d5327f":"Now, fit the training data. We should time this as well to see how slow it runs.\n","9f463742":"# 3. Modeling","92885167":"With more preprocessing and cleaning, we are going to first going to fit a simple logistic regression model. Then we will use a boosted tree model.","9fb5a6c2":"For sanity's sake, we will start the preprocessing data from scratch to make sure everything's clean.","a38f660d":"We might be able to infer longitude, latitude, GPS_height from basin, region, region_code, district_code, and ward. Lets plot a scatter with longitude, latitude and encode gps_height with different colors."}}