{"cell_type":{"b5507421":"code","43ba4bae":"code","2ef606d5":"code","324058ae":"code","3a55a1d2":"code","b12cfa1c":"code","8e392734":"code","a7a13b1e":"code","a48a9248":"code","aa1d8e1b":"code","32f8fe42":"code","4614cf1b":"code","23230136":"code","02abae5b":"markdown","9e3d5dd6":"markdown","fe264c48":"markdown","16d48259":"markdown","5ad6ac45":"markdown","d1ed81c6":"markdown","96d5bdb4":"markdown","7140f2db":"markdown","411432e7":"markdown","06f7e270":"markdown","11c0f694":"markdown","a37b6ef5":"markdown"},"source":{"b5507421":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","43ba4bae":"df=pd.read_csv(\"\/kaggle\/input\/deepnlp\/Sheet_1.csv\")\ndf.head()","2ef606d5":"df_data=df.drop([\"Unnamed: 3\",\"Unnamed: 4\",\"Unnamed: 5\",\"Unnamed: 6\",\"Unnamed: 7\"],axis=\"columns\")\ndf_data.count()","324058ae":"from wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\ntext_df = \" \".join(review for review in df_data.response_text)\nwordcloud = WordCloud(background_color=\"red\").generate(text_df)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","3a55a1d2":"from collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\nvectorizer = CountVectorizer(stop_words='english')\ndocs       = vectorizer.fit_transform(text for text in df_data['response_text'])\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(\n    features=features, size=(1080, 720)\n)\nvisualizer.fit(docs)\nvisualizer.show()","b12cfa1c":"import re\nfrom nltk.corpus import stopwords\nstops = stopwords.words('english')","8e392734":"from nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 80):\n    response_text = re.sub('[^a-zA-Z]', ' ', df_data['response_text'][i])\n    response_text = response_text.lower()\n    response_text = response_text.split()\n    ps = PorterStemmer()\n    response_text = [ps.stem(word) for word in response_text if not word in set(stops)]\n    response_text = ' '.join(response_text)\n    corpus.append(response_text)","a7a13b1e":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1000)\nX = cv.fit_transform(corpus).toarray()","a48a9248":"y_response = df_data.iloc[:, 1]","aa1d8e1b":"y=np.where(y_response==\"flagged\",0,1)","32f8fe42":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.4, \n                                                    random_state=1,\n                                                    stratify=y)","4614cf1b":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(random_state=1211,\n                                  n_estimators=500,oob_score=True)","23230136":"model_rf.fit( X_train , y_train )\ny_pred_probarf = model_rf.predict_proba(X_test)[:,1]\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_probarf)","02abae5b":"I am using here Stemmer to strim word.Also we can use Tokenizer or Lemmatizer.Some time stemmer strim word but word like \"GO\",\"Went\",\"Going\" are back to their root meaning.In such case we use Lemmatizer.\nTokenizer split sentence into seperate word.Stemmer remove \"ed\",\"tion\",\"Ing\" etc in short it try to word should be in it's root meaning by removing this word.","9e3d5dd6":"In each and every observation stopword are present and that are not importanat of our point of view.\" from nltk.corpus import stopwords \" Library help us to remove stopwords from our data.","fe264c48":"Some variable having NaN values and not important in further analysis.Removing such variable from data using simple drop command.","16d48259":"But this reponse variable in categorical form i.e.flagged and not_flagged.While doing operation we have need to convert it into binary form.Using Numpy we can convert it into binary.","5ad6ac45":"# **Word frequency Counter plot**","d1ed81c6":"# Reading data set","96d5bdb4":"Obervation\n    Word like \n1.     friend\n2.     help\n3.     talk\n4.     going\n are repeated more then other words.\n    ","7140f2db":"Train test split.Response variable is binary that why I use stratify=y.","411432e7":"# Word Cloud","06f7e270":"Model build using random forest algorithm.Because it gives me best accuracy than other algorithm.","11c0f694":"Creating data frame of important word from every observation.If some word are reated in any obervaton this feature data frame count it's frequency.","a37b6ef5":"Importing response variable from original data frame."}}