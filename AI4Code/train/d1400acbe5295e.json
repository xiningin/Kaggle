{"cell_type":{"bf0e3e25":"code","83beb657":"code","3dacb3a6":"code","005aa86c":"code","ef45f42e":"code","8e2a9395":"code","3e28ec32":"code","aa065347":"code","38ba363c":"code","d1c3fca1":"code","f5a826e9":"code","c1624539":"code","c3427020":"code","57a9f4bd":"code","9d1ab21d":"code","788b2839":"code","8f3a458b":"code","e0f4161e":"code","da08df68":"code","a9d2224e":"code","20a543da":"code","fb4d6edc":"code","0e427c2d":"code","e47a9708":"code","c3faff81":"code","b2c830db":"markdown","1f0b4a01":"markdown","ba741fef":"markdown","dd26bc12":"markdown","0586cf6f":"markdown","d83b7a32":"markdown","5aca9913":"markdown","2e463fe7":"markdown","6e6b09fc":"markdown","a0894ce4":"markdown","b868df2d":"markdown","bbd124d6":"markdown","abea9524":"markdown","2a5928d6":"markdown","542618fc":"markdown"},"source":{"bf0e3e25":"import os\nimport email\nimport random\nimport email.policy\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport sklearn","83beb657":"base_directory = \"\/kaggle\/input\/ham-and-spam-dataset\/hamnspam\/\"\nspam_email_names = os.listdir(base_directory + \"spam\")\nnormal_email_names = os.listdir(base_directory + \"ham\")","3dacb3a6":"def load_email(is_spam, filename):\n    directory = base_directory + (\"spam\" if is_spam else \"ham\")\n    with open(os.path.join(directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)","005aa86c":"spam_emails = [load_email(True, filename) for filename in spam_email_names]\nnormal_emails = [load_email(False, filename) for filename in normal_email_names]\nrandom.shuffle(spam_emails)\nrandom.shuffle(normal_emails)","ef45f42e":"print(\"Number of Spam emails: %d, Number of Normal Emails: %d\"%(len(spam_emails), len(normal_emails)))","8e2a9395":"def process_email(emails, label, data_dictionary, default_topic=None):\n    for mail in emails:\n        payload = mail.get_payload()\n        if isinstance(payload, list):\n            process_email(payload, label, data_dictionary, default_topic=mail[\"Subject\"])\n        else:\n            if \"Content-Type\" in mail.keys():\n                if \"html\" in mail[\"Content-Type\"].lower():\n                    try: \n                        soup = BeautifulSoup(mail.get_content())\n                        topic = mail[\"Subject\"]\n                        if topic == None:\n                            topic = default_topic\n                        content = soup.body.text\n                        data_dictionary[\"topic\"].append(topic)\n                        data_dictionary[\"content\"].append(content)\n                        data_dictionary[\"label\"].append(label)\n                    except:\n                        pass\n                elif \"plain\" in mail[\"Content-Type\"].lower():\n                    try: \n                        topic = mail[\"Subject\"]\n                        if topic == None:\n                            topic = default_topic\n                        content = mail.get_content()\n                        data_dictionary[\"topic\"].append(topic)\n                        data_dictionary[\"content\"].append(content)\n                        data_dictionary[\"label\"].append(label)\n                    except:\n                        pass\n                else:\n                    pass","3e28ec32":"data_dictionary = {\"topic\": [], \"content\": [], \"label\": []}\nprocess_email(spam_emails, 1, data_dictionary)\nprocess_email(normal_emails, 0, data_dictionary)\ndf = pd.DataFrame(data_dictionary)\ndf.dropna(inplace=True)\ndf = df.sample(frac=1)","aa065347":"df.head(10)","38ba363c":"df[\"label\"].value_counts().plot(kind=\"bar\")","d1c3fca1":"class_weight = 1 \/ df[\"label\"].value_counts()\nclass_weight = dict(class_weight \/ class_weight.sum())\nclass_weight","f5a826e9":"stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\ndef preprocess_text(content):\n    # Change text to lowercased\n    content = content.lower()\n    # Remove stop words\n    for stopword in stopwords:\n        content = content.replace(stopword + \" \", \"\")\n        content = content.replace(\" \" + stopword, \"\")\n    return content","c1624539":"num_words = 8196\nmax_content_length = 512\ntopic_and_contents = []\nfor (topic, content) in zip(df[\"topic\"], df[\"content\"]):\n    topic_and_contents.append(preprocess_text(topic + \" \" + content))\ndf[\"topic_content\"] = topic_and_contents\ntokenizer = Tokenizer(num_words=num_words - 1, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(topic_and_contents)","c3427020":"df.head()","57a9f4bd":"tokenized_topic_and_contents = tokenizer.texts_to_sequences(topic_and_contents)","9d1ab21d":"tokenized_topic_and_content_lengths = pd.DataFrame([len(item) for item in tokenized_topic_and_contents])","788b2839":"tokenized_topic_and_content_lengths.describe()","8f3a458b":"tokenized_topic_and_content_lengths[tokenized_topic_and_content_lengths < 1024].hist()","e0f4161e":"padding_tokenized_topic_and_contents = pad_sequences(\n    tokenized_topic_and_contents, \n    padding='post', \n    truncating='post', \n    maxlen=max_content_length\n)","da08df68":"X = np.array(padding_tokenized_topic_and_contents)\ny = np.array(df[\"label\"])\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.15)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a9d2224e":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words, 256, input_length=max_content_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(2, activation='softmax')\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","20a543da":"checkpoint_path = \"model.h5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), class_weight=class_weight, epochs=10, callbacks=[checkpoint])\nmodel.load_weights(checkpoint_path)","fb4d6edc":"import matplotlib.pyplot as plt\npd.DataFrame(history.history).plot(xlabel=\"Epoch\")\nplt.title(\"Loss & Accuracy over time\")","0e427c2d":"y_pred = np.argmax(model.predict(X_test), axis=-1)","e47a9708":"import seaborn as sns\ncm = sklearn.metrics.confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\")","c3faff81":"cls_report = sklearn.metrics.classification_report(y_test, y_pred)\nprint(cls_report)","b2c830db":"<a id=\"3.\"><\/a>\n## 3. Data Requirements\n\nIn order to solve this problem, we need to collect a list of Emails labeled as Spam or Normal.","1f0b4a01":"<a id=\"7.3\"><\/a>\n<h3> 7.3 Classification Report <\/h3>","ba741fef":"<a id=\"5.3\"><\/a>\n### 5.3 Preprocesing Emails\nLet's start preprocessing texts in following procedures:\n1. Change text to lowercased.\n2. Remove Stop words","dd26bc12":"<a id=\"5.4\"><\/a>\n### 5.4 Statistic info of total length of tokenized topic and content\nWe will use Word Embedding techniques, input length is very important, that's why we need to learn about statistic info of total length of tokenized topic and content. As we can see, max length of the Emails is 6758 and average length of Emails is about 160, most of the Email's length is less than 500. ","0586cf6f":"<a id=\"5.2\"><\/a>\n### 5.2 Calculate Class Weight for Training\nThere are 18% of Label are Spam and 82% of Label Noral, which is a Class Imblance problem. We can calcuate class_weight for training to mitigate this problem.","d83b7a32":"Train the Model for 10 epochs.","5aca9913":"# Spam Filter using Word Embedding & LSTM\n## Table of Contents\n* [1. Import packages](#1.)\n* [2. Analytic Approach](#2.)\n* [3. Data Requirements](#3.)\n* [4. Data Collection](#4.)\n* [5. Data Understanding and Preparation](#5.)\n    - [5.1 Parse Emails](#5.1)\n    - [5.2 Calculate Class Weight for Training](#5.2)\n    - [5.3 Preprocesing Emails](#5.3)\n    - [5.4 Statistic info of total length of tokenized topic and content](#5.4)\n* [6. Modeling](#6.)\n* [7. Evaluation](#7.)\n    - [7.1 Loss & Accuracy](#7.1)\n    - [7.2 Confusion Matrix](#7.2)\n    - [7.3 Classification Report](#7.3)\n* [8. Conclusions](#8.)","2e463fe7":"<a id=\"7.\"><\/a>\n<a id=\"7.1\"><\/a>\n<h2> 7. Evaluation <\/h2>\n<h3> 7.1 Loss & Accuracy <\/h3>\nPlot Loss & Accuracy over time. As we can see, the Model achieve a good result of 98% within 10 epochs.","6e6b09fc":"<a id=\"5.\"><\/a>\n<a id=\"5.1\"><\/a>\n\n### 5. Data Understanding and Preparation\n\n### 5.1 Parse Emails\n\nWe will use Python Email Library to Parse these emails. We can also see number of Normal Emails is 5 times of Spam Emails.","a0894ce4":"<a id=\"1.\"><\/a>\n## 1. Import packages","b868df2d":"<a id=\"4.\"><\/a>\n## 4. Data Collection\nIn this project I will use dataset from https:\/\/www.kaggle.com\/veleon\/ham-and-spam-dataset.\nNow data is already downloaded in Kaggle Platfrom. ","bbd124d6":"<a id=\"2.\"><\/a>\n## 2. Analytic Approach\nAn Email comprises of the following elements:\n- Sender Address\n- Receiver Address\n- Topic\n- Content\n- Attachments (optional)\n\nIn daily life, when we read Emails, we can quickly decide that an Email is a Spam if we find following things:\n- Topic or Content contains sex, violence, scam or other toxic content. Sometimes we can look at topic or first few words of content to judge that it's a Spam. \n- Attachments contains virus, toxic images, something like that.\n- The Sender is in blacklist.\n\nThe question is deciding whether an Email is a Spam or not and Email consists of a sequence of texts. This is a Text Classification Problem. We can build a Neural Network with Word Embedding, Bidirectional LSTM layers to solve NLP classification problem. For simplicity we only use topic and content to decide whether or not an Email is a Spam.","abea9524":"<a id=\"8.\"><\/a>\n<h2> 8. Conclusions <\/h2>\n- This Model can get a very good result. In all Emails that are predicted as Spam, there can hardly be any Normal Emails. Almost all Normal Emails are predicted correctly.\n- Data Preparation \/ Understanding \/ Preparation comprises of most of the work.\n- Modeling and Evaluation comprises a small amount of work while choosing the right Model and appropriate metrics to evaluate the Model is still very important.\n- Fine Turning can improve Model's performance.\n- Solving data imbalance, such as label imbalance can help the Model achieve a better results.\n- With appropriate data preprocessing, the Model can achieve a better performance.\n- When we evaluate the Model, we should not only watch out for total accuaracy, but also accuracy accross different labels.","2a5928d6":"<a id=\"6.\"><\/a>\n### 6. Modeling","542618fc":"<a id=\"7.2\"><\/a>\n<h3> 7.2 Confusion Matrix <\/h3>"}}