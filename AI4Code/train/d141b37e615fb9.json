{"cell_type":{"80faf714":"code","517e8932":"code","ae5655b3":"code","d2c64592":"code","dc564395":"code","b5e87fa8":"code","2431d898":"code","60353519":"code","5144ae02":"code","14f479ff":"code","2c9287ff":"code","e43ded13":"code","81c36f50":"code","51507cce":"code","a0fd16fe":"code","6dec2335":"code","d5cb5e4c":"code","1bb8f7e6":"code","4b227362":"code","1521bab4":"code","e70b9f31":"code","f2ce7c00":"code","e45123a3":"code","da9a3fdf":"code","810267fc":"code","113f8f1e":"markdown","f55a0bfd":"markdown","5811fb8e":"markdown","1e1e48a8":"markdown","2b55fd4f":"markdown","8bb466a2":"markdown","a3e0ce3b":"markdown"},"source":{"80faf714":"import pandas as pd\nimport spacy\nimport networkx as nx\nfrom itertools import combinations\nfrom collections import defaultdict\nimport operator\nimport matplotlib.pyplot as plt\nimport numpy as np\n# from math import log","517e8932":"def coocurrence(*inputs):\n    com = defaultdict(int)\n    \n    for named_entities in inputs:\n        # Build co-occurrence matrix\n        for w1, w2 in combinations(sorted(named_entities), 2):\n            com[w1, w2] += 1\n            com[w2, w1] += 1  #Including both directions\n\n    result = defaultdict(dict)\n    for (w1, w2), count in com.items():\n        if w1 != w2:\n            result[w1][w2] = {'weight': count}\n    return result","ae5655b3":"# Example coocurrence.\n# Originally d is not a key here (since included in previous coocurrences)\n# Altered to include ALL now, seems to make difference for my methodology below\n\ncoocurrence('abcddc', 'bddad', 'cdda')","d2c64592":"# check out the data\ndata = pd.read_csv('..\/input\/snopes.csv')\ndata.head(2)","dc564395":"# Could keep in dataframe format? \n# Can make use of other fields for analysis\/graph, since not a huge dataset","b5e87fa8":"# drop duplicate claims (and unneccesary columns?)\ndata.drop_duplicates(subset='claim', inplace=True)\n\n# remove 'examples' (Some odd artifacts that messed with analysis)\ndata = data.replace({'Example\\(s\\)': ''}, regex=True)\ndata = data.replace({'\\s+': ' '}, regex=True)","2431d898":"# remove duplicate claims (Not really needed since dropped already)\nclaims = data.claim.unique()\n\n# make sure it's all strings \n# added lower and whitespace strip just in case\n# claims = [str(claim).lower().strip() for claim in claims]\n# Turns out this ruins it... and reduced most docs to few claims for some reason\n\n# NER list we'll use - Perhaps could be expanded?\nnlp = spacy.load('en_core_web_sm')\n\n# intialize claim counter & lists for our entities\ncoocur_edges = {}\n\nprint('Number of claims: ', len(claims))","60353519":"# Lets look at the first few claims, along with the ents identified\n\nfor doc in nlp.pipe(claims[:5]):\n    print(doc)\n    print(list(doc.ents))\n    print('\\n')","5144ae02":"# Separating this lengthy step, and saving result as a list rather than generator\n# (Size isnt too big, and saves a lot of time when reused later)\n\n# Spacy seems to have error at 3k doc mark? \n# Related to this maybe? https:\/\/github.com\/explosion\/spaCy\/issues\/1927\n# Continuing on with the first 3000 of 3122 for now\n\ncorpus = list(nlp.pipe(claims[:3000]))","14f479ff":"# Looking at number of times each ent appears in the total corpus\n# nb. ents all appear as Spacy tokens, hence needing to cast as str\n\nall_ents = defaultdict(int)\n\nfor i, doc in enumerate(corpus):\n    #print(i,doc)\n    for ent in doc.ents:\n        all_ents[str(ent)] += 1\n        \nprint('Number of distinct entities: ', len(all_ents))","2c9287ff":"# Most popular ents\n\nsorted_ents = sorted(all_ents.items(), key=operator.itemgetter(1), reverse=True)\nsorted_ents[:20]","e43ded13":"# Number of ents that appear at least twice\n\nmulti_ents = [x for x in sorted_ents if x[1] > 1]\n\nprint('Number of ents that appear at least twice: ', len(multi_ents))","81c36f50":"# How many ents appear per claim?\n\nents_in_claim = [len(doc.ents) for doc in corpus]\n\nplt.hist(ents_in_claim, \n         rwidth=0.9, \n         bins=np.arange(max(ents_in_claim)+2)-0.5)  \n        # Futzing with bins just to fix column alignment - not really necessary\nplt.title('Entities per claim')\nplt.show()","51507cce":"# Listing claims as a list of their entities\n\nclaim_ents = []\nfor i, doc in enumerate(nlp.pipe(claims[:5])):\n    string_ents = list(map(str, doc.ents))\n    claim_ents.append(string_ents)\n    # Doubling some up to fake\/force coocurrence\n    if i%2==0:\n        claim_ents.append(string_ents)  \nclaim_ents\n\n# Could do as a one line list comprehension, though maybe not as readable:\n# claim_ents = [list(map(str, doc.ents)) for doc in nlp.pipe(claims[:10]*2)]","a0fd16fe":"# Can filter out claims with only 1 ent (nothing to coocur with)\n\nmulti_ent_claims = [c for c in claim_ents if len(c)>1]\n# single_ent_claims = [c for c in claim_ents if len(c)==1]\n# no_ent_claims = [c for c in claim_ents if len(c)==0]\n\nmulti_ent_claims","6dec2335":"# Generating coocurrence dict of dicts\n\ncoocur_edges = coocurrence(*multi_ent_claims)\ncoocur_edges","d5cb5e4c":"# Filter out ents with <2 weight - refactored into a function later.\n# (Could also use: del coocur_edges[k1][k2] rather than make new dict)\n\ncoocur_edges_filtered = defaultdict()\n\nfor k1, e in coocur_edges.items():\n    ents_over_2_weight = {k2: v for k2, v in e.items() if v['weight'] >= 1}\n    if ents_over_2_weight:  # ie. Not empty\n        coocur_edges_filtered[k1] = ents_over_2_weight\n\ncoocur_edges_filtered","1bb8f7e6":"# Summing all coocurrences in order to see most coocurring edges\n\ncoocur_sum = defaultdict(int)\nfor k1, e in coocur_edges_filtered.items():\n    for k2, v in e.items():\n        coocur_sum[k1] += v['weight']\n\nsorted_coocur = sorted(coocur_sum.items(), key=operator.itemgetter(1), reverse=True)\nsorted_coocur","4b227362":"# Making the list of claims\nclaim_ents = []\nfor doc in corpus:\n    string_ents = list(map(str, doc.ents))\n    claim_ents.append(string_ents)\n    \n    \n# Keeping only claims with multiple entities\nmulti_ent_claims = [c for c in claim_ents if len(c)>1]\n# single_ent_claims = [c for c in claim_ents if len(c)==1]\n# no_ent_claims = [c for c in claim_ents if len(c)==0]\n\n\n# Creating the coocurrance dict\ncoocur_edges = coocurrence(*multi_ent_claims)","1521bab4":"# Filter out ents with < min_weight - useful for graph clarity?\n\ndef filter_ents_by_min_weight(edges, min_weight):\n    coocur_edges_filtered = defaultdict()\n    for k1, e in edges.items():\n        ents_over_x_weight = {k2: v for k2, v in e.items() if v['weight'] > min_weight}\n        if ents_over_x_weight:  # ie. Not empty\n            coocur_edges_filtered[k1] = ents_over_x_weight\n    return coocur_edges_filtered","e70b9f31":"# Looking at the most coocurring edges\n\nfiltered_edges = filter_ents_by_min_weight(coocur_edges, 2)\n\ncoocur_sum = defaultdict(int)\nfor k1, e in filtered_edges.items():\n    for k2, v in e.items():\n        coocur_sum[k1] += v['weight']\n\nsorted_coocur = sorted(coocur_sum.items(), key=operator.itemgetter(1), reverse=True)\nprint('Most frequent CO-ocurring entity:')\nsorted_coocur[:20]","f2ce7c00":"# Getting the data - eg top 30, including only ents with min weight 2\ntop_n = 30\nmin_weight = 2\nfigsize = (20, 15)\nscale_nodes = lambda x: (x * 30) + 1\nscale_edges = lambda x: 15 * x\n\nfiltered_edges = filter_ents_by_min_weight(coocur_edges, min_weight)\n\ntop_cooccur = [x[0] for x in sorted_coocur[:top_n]]  \ngraph_edges = {k:filtered_edges[k] for k in top_cooccur}\n\n# Attempting to graph these top coocurrances\nG = nx.from_dict_of_dicts(graph_edges)\npos = nx.kamada_kawai_layout(G)\n# pos = nx.circular_layout(G)\n# pos = nx.spring_layout(G)\n# pos = nx.fruchterman_reingold_layout(G)\n# pos = nx.spectral_layout(G)\n# pos = nx.shell_layout(G)\n\n# Normalise, then scale the line weights\nweights = [G[u][v]['weight'] for u, v in G.edges() if u != v]\nweights = list(map(lambda x: (x - min(weights)) \/ (max(weights) - min(weights)), weights))\nweights = list(map(scale_edges, weights))\n\n# Scale node weights \nsum_weights = [coocur_sum[n] if coocur_sum[n]>0 else 1 for n in G.nodes]\nsum_weights = list(map(scale_nodes, sum_weights))\n# sum_weights = list(map(lambda x: 100*log(x), sum_weights))\n\n\nplt.figure(figsize=figsize)\n\n# nx.draw(G, pos)\nnx.draw_networkx_edges(G, pos, alpha=0.2, width=weights)\nnx.draw_networkx_nodes(G, pos, alpha=0.2, node_size=sum_weights)\nnx.draw_networkx_labels(G, pos)\n\nplt.xticks([])\nplt.yticks([])\n\nplt.title('Top coocurrances of named entities in Snopes claims')\nplt.show()","e45123a3":"# Colour based on average truthiness of claims including that entity? Some sort of clustering?\n# Might make sense to keep in dataframe format to make use of other info columns","da9a3fdf":"# No doubt I'm way off track, but hopefully something in here can help :)\n# Perhaps some sort of Sparse Matrix (rather than dict of dict) could have been another choice for representing this data?\n# Looking forward to next stream!","810267fc":"# A fun little wordcloud picture for the project\n# Just using all the named entities without considering coocurrence\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom PIL import Image\n\n# Join all the tweet entities into one list\nword_list = [j for i in claim_ents for j in i]\n\n# Count occurences of each entity \nword_count_dict=Counter(word_list)\n\n# Make the wordcloud - can use a black\/white image for shape mask\n#mask = np.array(Image.open(\"twitter_logo_bw.png\"))\nwordcloud = WordCloud(background_color=\"white\", max_words=100, \n                      width = 600, height = 300,\n                      #mask=mask, contour_width=5, contour_color=\"skyblue\"\n                     )\n                      \nwordcloud.generate_from_frequencies(word_count_dict)\n\n# Show the wordcloud\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()\n\n# Or save the file\n# plt.savefig('yourfile.png', bbox_inches='tight')\n# plt.close()","113f8f1e":"## Applying method to the whole dataset","f55a0bfd":"## Loading \/ organising the tweet data:","5811fb8e":"<img  align=\"right\" src=\"https:\/\/i.imgur.com\/qG5Qihv.png\" alt=\"Twitter entity wordcloud\" width=\"300\"\/>\n\n# Named Entity Recognition with Snopes data\n\nHere we take a look at named entities within snopes tweet data, and how they occur together  \n\nMain idea and original code thanks to Rachael Tatman: https:\/\/www.youtube.com\/watch?v=4SoLoo0fdH0  \n","1e1e48a8":"## Graphing the coocurrence relationships","2b55fd4f":"## Extra:","8bb466a2":"## Exploring\/Demonstrating with small subset first","a3e0ce3b":"## What we mean by 'cooccurence'\n\nFor each entity pair that occurs together in the same tweet, record the number of times they occur together in the whole corpus \n\nFunction below returns a count of co-occurances as a dictionary of dictionaries,\nbased on method by StackOverflow user [Patrick Maupin](https:\/\/stackoverflow.com\/questions\/32534655\/python-creating-undirected-weighted-graph-from-a-co-occurrence-matrix)."}}