{"cell_type":{"db690101":"code","1c371ac1":"code","0f524974":"code","6569cba4":"code","98ebb8dc":"code","7c4e1587":"code","72620f05":"code","75c13240":"code","76d5e535":"code","b133b74e":"code","b96dba23":"code","56d86740":"code","44939758":"code","daf735df":"code","d4ad0031":"code","789c5866":"code","1b6d30b7":"code","b20371b3":"code","660d5757":"code","f34fce6e":"code","c9c40e6a":"code","e6f8fdaa":"code","d9865f29":"code","a2063c72":"code","7c63ca1b":"code","540a4fbc":"code","379846be":"code","defdbfd2":"code","ddf5b656":"code","5afb11d2":"code","db3bb931":"code","b72e46e1":"code","d3a9d28f":"code","ee61822e":"code","f191060a":"code","91e3eb08":"code","a8fc079a":"code","de6ed1ac":"code","e61d8580":"code","7f6b42b3":"code","46d754cc":"code","1243ac0f":"code","078f7d33":"code","974b418a":"code","cff4325b":"code","6df4f7dd":"code","99d4cbbe":"code","f85daa9c":"code","53b4281b":"code","0b4579ed":"code","aca71202":"code","9a278f7e":"code","35151388":"code","95275b5c":"code","b46190ef":"code","018ce231":"code","3b767063":"code","c57f2fd2":"code","4f3c803c":"code","b0acff5f":"code","1acce5a9":"code","4672face":"code","53eb5193":"code","b9e5ae36":"code","33683fd8":"code","1b0fb2a2":"code","02391f79":"code","0f614848":"code","bfb3c0ef":"code","cee244f2":"code","0faae526":"code","9cfd1f0e":"code","b8f92119":"code","7fa89af5":"code","a40a6687":"code","195ad32b":"code","516246a0":"code","3b702101":"code","e3158cc8":"code","6b4bdfc5":"code","be65e1f4":"code","4da19580":"code","02959226":"code","c5023adc":"code","361271ac":"code","115966d0":"code","a7e1d291":"code","9a6df5d2":"code","efd2806a":"code","270f53ab":"code","e5b07362":"code","6c0d7c17":"code","0de61409":"code","a01425b8":"code","02fbd7ab":"code","7d98db27":"code","e76ea015":"code","1a3137d6":"code","04da23e8":"code","9155e568":"code","e73ff929":"code","a939f975":"code","a6270b0b":"code","03e59497":"code","9a579b44":"code","d8f0775b":"code","9eb6d6da":"code","f03b1f4d":"code","e55fa898":"code","84ac2969":"code","d99b0655":"code","dad4b990":"code","7578d895":"code","8448996f":"code","256e97f8":"code","0795391d":"code","423ac73d":"code","e4a11ad1":"code","7bbcf811":"code","92273f03":"code","1b0af16b":"code","8cb021fe":"code","554fbde2":"code","c96e0627":"code","91961e23":"code","c01f214c":"code","6b30236e":"code","ebcd9da2":"code","e05aef37":"code","322ea892":"code","be74bb1c":"code","434f5f47":"code","dbf35245":"code","5a41e156":"code","9505dc0c":"code","128dabac":"code","d533dbcf":"code","1fd7a5fe":"code","598cee7c":"code","bdbe8035":"code","42d1bc15":"code","b2081293":"code","57ff4496":"code","6e65cc7b":"code","7e33323e":"markdown","7799140a":"markdown","42d3c545":"markdown","79421c9b":"markdown","da2ddf3e":"markdown","c8e92751":"markdown","b8c21d01":"markdown","5d3047e4":"markdown","7db56c7d":"markdown","d9348f90":"markdown","c22c027c":"markdown","afd5d55e":"markdown","b88b2cb6":"markdown","a7e8cda8":"markdown","fde024bc":"markdown","eb3c5683":"markdown","cd5cea44":"markdown","73bb4ccd":"markdown","88705ff9":"markdown","5450fab0":"markdown","c0a55c75":"markdown","15cb29b2":"markdown","75592b4d":"markdown","901f780c":"markdown","3b910dec":"markdown","81f101fa":"markdown","bd834573":"markdown","4e5ac061":"markdown","ce4cabe2":"markdown","e7d77fc7":"markdown","cb3f2fc7":"markdown","92fc9eda":"markdown","a6799bf2":"markdown","86e94fff":"markdown","38fee177":"markdown","5f16f509":"markdown","b44004e5":"markdown","48084a4a":"markdown","b495c783":"markdown","77492f2b":"markdown","630a64fe":"markdown"},"source":{"db690101":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict,Counter\nfrom functools import partial\nfrom scipy.stats import spearmanr,pearsonr\nfrom sklearn.preprocessing import StandardScaler,label_binarize\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegressionCV,LogisticRegression\nfrom sklearn.metrics import auc,make_scorer,roc_auc_score,f1_score,roc_curve\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\nfrom hyperopt import Trials\nfrom hyperopt import tpe\nfrom hyperopt import fmin\nimport lightgbm as gbm \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1c371ac1":"import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\npd.options.display.max_columns = 150","0f524974":"plt.style.use('fivethirtyeight')","6569cba4":"unc_data = pd.read_csv('..\/input\/train.csv',index_col='Id')\nunc_data_test = pd.read_csv('..\/input\/test.csv',index_col='Id')\nunc_data.head()","98ebb8dc":"#shape of data\nprint('training_data:',unc_data.shape)\nprint('test_data:',unc_data_test.shape)","7c4e1587":"#for checking null values\ndef check_null(data):\n    for col in data.columns:\n        na_values = data[col].isnull().sum()\n        rows = data.shape[0]\n        print(col,na_values,(na_values\/rows)*100)\n#for recalculating the dependency column\ndef calc_dependency(data):\n    #dropping the old dependency column\n    data = data.drop('dependency',axis=1)\n    #recalculating the dependency\n    data['dependency'] = (data.hogar_nin + data.hogar_mayor) \/ (data.hogar_adul - data.hogar_mayor)\n    #new dependeny column calculated as per the formula with consistent entries\n    data['dependency'] = data['dependency'].replace([np.inf,-np.inf],np.nan).fillna(0).round(2)\n    return data\n#for performing sanity checks on the categorical columns to check if they have consistent values\ndef sanity_check(data,full_col_list):\n    for col_name,col_list in full_col_list.items():\n        if data[col_list].apply(lambda x:x.sum(),axis=1).sum() == data.shape[0]:\n            print('the {} columns look fine,no cleaning necessary'.format(col_name))\n        else:\n            print('there is a discrepancy in {} columns,requires investigation'.format(col_name))","72620f05":"#checking for null values--training data\nprint('training_data')\ncheck_null(unc_data)","75c13240":"#checking for null values--test data\nprint('test_data')\ncheck_null(unc_data_test)","76d5e535":"#rez_esc column has 9139 rows where the value is either null or zero and escolari is non zero\nprint(unc_data[['escolari','rez_esc']][(unc_data['rez_esc'].isna()) | (unc_data['rez_esc'] == 0)])","b133b74e":"#rez_esc column has 22916 rows where the value is either null or zero and escolari is non zero\nprint('test_data',unc_data_test[['escolari','rez_esc']][(unc_data_test['rez_esc'].isna()) | (unc_data_test['rez_esc'] == 0)])","b96dba23":"#dropping the rez_esc column in training and test set\nunc_data = unc_data.drop('rez_esc',axis=1)\nunc_data_test = unc_data_test.drop('rez_esc',axis=1)","56d86740":"#dependency column needs to be cleaned as it has inconsistent entries as per the formula\nunc_data['dependency'].unique()","44939758":"unc_data_test['dependency'].unique()","daf735df":"#dependency column needs to be recalculated as per the given formula in the description.\n#have checked the required columns consistency by adding up hogar_nin and hogar_adul and comparing with hogar_total\nprint((unc_data.hogar_nin + unc_data.hogar_adul == unc_data.hogar_total).sum())\n#the three columns are consistent in test set as well,hence recalculating the dependency column\nprint((unc_data_test.hogar_nin + unc_data_test.hogar_adul == unc_data_test.hogar_total).sum())","d4ad0031":"#recalculating the dependency column in training set\nunc_data = calc_dependency(unc_data)\n#recalculating the dependency column in test set\nunc_data_test = calc_dependency(unc_data_test)","789c5866":"#v18q1-no. of tablets owned has nan values for rows where the house does not own a tablet\n#replacing the nan values with 0\nunc_data['v18q1'] = unc_data['v18q1'].fillna(0)\nunc_data_test['v18q1'] = unc_data_test['v18q1'].fillna(0)","1b6d30b7":"#investigating the edjefe and edjefa columns which have numerical and categorical entries.\n#need to reconcile these columns into one entry type,either numerical or continuous\nunc_data[['edjefe','edjefa','parentesco1','male','female','escolari']]","b20371b3":"unc_data['edjefe'].unique()","660d5757":"#As per the observation of these entries,the calculation of edjefe and edjefa are inconsistent\n#they need to be recalculated correctly or dropped altogether to avoid inducing false\/incorrect information in the dataset\nunc_data[['edjefe','edjefa','parentesco1','male','female','escolari']][unc_data['edjefa']=='yes']","f34fce6e":"#dropping edjefe and edjefa columns\nunc_data = unc_data.drop(['edjefe','edjefa'],axis=1)\nunc_data_test = unc_data_test.drop(['edjefe','edjefa'],axis=1)","c9c40e6a":"#tamhog and hhsize have the same values accross the dataset,hence can drop one of the two\nprint((unc_data['tamhog'] == unc_data['hhsize']).sum())\nprint((unc_data_test['tamhog'] == unc_data_test['hhsize']).sum())","e6f8fdaa":"#dropping tamhog column\nunc_data = unc_data.drop('tamhog',axis=1)\nunc_data_test = unc_data_test.drop('tamhog',axis=1)","d9865f29":"#investigating the mobilephone columns to check if they are consistent with each other\nprint('training_data',unc_data[['mobilephone','qmobilephone']].query('mobilephone == 0 & qmobilephone != 0').sum())\nprint('test_data',unc_data_test[['mobilephone','qmobilephone']].query('mobilephone == 0 & qmobilephone != 0').sum())","a2063c72":"#grouping all the columns according to the attributes they represent about the household\/individual\ncategorical_columns = {'wall_attrib' :['paredblolad','paredzocalo','paredpreb','pareddes','paredmad','paredzinc','paredfibras','paredother'],\n                       'floor_attrib':['pisomoscer','pisocemento','pisoother','pisonatur','pisonotiene','pisomadera'],\n                       'roof_attrib' :['techozinc','techoentrepiso','techocane','techootro'],\n                       'water_attrib':['abastaguadentro','abastaguafuera','abastaguano'],\n                       'electric_attrib' : ['public','planpri','noelec','coopele'],\n                       'sanitary_attrib' : ['sanitario1','sanitario2','sanitario3','sanitario5','sanitario6'],\n                       'energy_attrib' : ['energcocinar1','energcocinar2','energcocinar3','energcocinar4'],\n                       'disposal_attrib' : ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6'],\n                       'wall_qual' : ['epared1','epared2','epared3'],\n                       'roof_qual' : ['etecho1','etecho2','etecho3'],\n                       'floor_qual' : ['eviv1','eviv2','eviv3'],\n                       'status' : ['estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6','estadocivil7'],\n                       'edu_level' : ['instlevel1','instlevel2','instlevel3','instlevel4','instlevel5','instlevel6','instlevel7','instlevel8','instlevel9'],\n                       'ownership_status' : ['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5'],\n                       'region' : ['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']\n                      }","7c63ca1b":"#performing sanity checks on the columns to check if they have consistent entries\nsanity_check(unc_data,categorical_columns)\n#we need to investigate the roof,electric and education level columns","540a4fbc":"#the same columns need to be investigated in test set as well\nsanity_check(unc_data_test,categorical_columns)","379846be":"#there are certain rows where all the roof columns have 0\n#as per the competition host it is a small glitch which implies the roof is made out of waste material\n#we need to introduce a new column which captures this information\nroof_attrib = categorical_columns['roof_attrib']\nunc_data[roof_attrib][unc_data.techozinc+unc_data.techoentrepiso+unc_data.techocane+unc_data.techootro != 1]","defdbfd2":"unc_data_test[roof_attrib][unc_data_test.techozinc+unc_data_test.techoentrepiso+unc_data_test.techocane+unc_data_test.techootro != 1]","ddf5b656":"#techowaste would be the new column that would be 1 for roof made out of waste material\nunc_data['techowaste'] = 0\nunc_data['techowaste'][unc_data.techozinc+unc_data.techoentrepiso+unc_data.techocane+unc_data.techootro == 0] = 1","5afb11d2":"unc_data_test['techowaste'] = 0\nunc_data_test['techowaste'][unc_data_test.techozinc+unc_data_test.techoentrepiso+unc_data_test.techocane+unc_data_test.techootro == 0] = 1","db3bb931":"#as per the competition host,when all the columns have zeroes,then the value should be other\n#need to create a new column that would capture this information\nelectric_attrib = categorical_columns['electric_attrib']\nunc_data[electric_attrib][unc_data.public+unc_data.planpri+unc_data.noelec+unc_data.coopele != 1]","b72e46e1":"#creating a new column elecother\nunc_data['elecother'] = 0\nunc_data['elecother'][unc_data.public+unc_data.planpri+unc_data.noelec+unc_data.coopele == 0] = 1","d3a9d28f":"unc_data_test['elecother'] = 0\nunc_data_test['elecother'][unc_data_test.public+unc_data_test.planpri+unc_data_test.noelec+unc_data_test.coopele == 0] = 1","ee61822e":"#these are the three ids where the education level is unknown\n#we do not know if they never had an education,so we cannot substitute 0\n#we can introduce another level to capture this information or assume they did not have an education\nedu_level = categorical_columns['edu_level']\nunc_data[edu_level][unc_data[edu_level].apply(lambda x:x.sum() != 1,axis=1)]","f191060a":"#creating a new column instlevel0\nunc_data['instlevel0'] = 0\nunc_data['instlevel0'][unc_data[edu_level].apply(lambda x:x.sum() != 1,axis=1)] = 1","91e3eb08":"#these are the ids where the education level is unknownin the test set\nunc_data_test['escolari'][unc_data_test[edu_level].apply(lambda x:x.sum() != 1,axis=1)]","a8fc079a":"unc_data_test['instlevel0'] = 0\nunc_data_test['instlevel0'][unc_data_test[edu_level].apply(lambda x:x.sum() != 1,axis=1)] = 1","de6ed1ac":"#dropping the squared_attributes.would be constructing features later during feature engineering\nsquared_attrib = ['SQBescolari','SQBage','SQBhogar_total','SQBedjefe','SQBhogar_nin','SQBovercrowding','SQBdependency','SQBmeaned','agesq']\nunc_data = unc_data.drop(squared_attrib,axis=1)\nunc_data_test = unc_data_test.drop(squared_attrib,axis=1)","e61d8580":"#cleaning the v2a1 column which contains null values\n#monthly rent payment would only be applicable to households which pay rent\n#we need to check the ownership status and decide what values to substitute in this column\nownership_status = categorical_columns['ownership_status']\nunc_data[ownership_status][(unc_data.v2a1.isnull()) & (unc_data.tipovivi3 != 1)].sum().plot(kind='bar')\nplt.show()","7f6b42b3":"unc_data_test[ownership_status][(unc_data_test.v2a1.isnull()) & (unc_data_test.tipovivi3 != 1)].sum().plot(kind='bar')\nplt.show()","46d754cc":"#substituting 0 for rows where the ownership status is 1,i.e the house is owned\nunc_data.loc[(unc_data.tipovivi1 == 1),'v2a1'] = 0\nunc_data_test.loc[(unc_data_test.tipovivi1 == 1),'v2a1'] = 0","1243ac0f":"#for the rest of the rows,where the rent is null,we would be imputing the values \n#and also adding a column indicating the same.\nmean_rent_train = unc_data['v2a1'].mean()\n\nunc_data['rent-missing'] = 0\n\nunc_data['rent-missing'][unc_data.v2a1.isnull()] = 1\n\nunc_data['v2a1']=unc_data['v2a1'].fillna(mean_rent_train)\n","078f7d33":"mean_rent_test = unc_data_test['v2a1'].mean()\nunc_data_test['rent-missing'] = 0\nunc_data_test['rent-missing'][unc_data_test.v2a1.isnull()] = 1\nunc_data_test['v2a1']=unc_data_test['v2a1'].fillna(mean_rent_test)","974b418a":"#the next step is to correct the labels where the members of the household,\n#have a different poverty label to the head of the household\nsame_labels = unc_data.groupby('idhogar')['Target'].apply(lambda x:x.nunique()==1)\ndiff_labels = same_labels[same_labels != True]","cff4325b":"#correcting the target labels\nfor idx in diff_labels.index:\n    correct_label = unc_data['Target'][(unc_data.idhogar == idx) & (unc_data.parentesco1 == 1)].values[0]\n    unc_data['Target'][(unc_data.idhogar == idx) & (unc_data.parentesco1 != 1)] = correct_label","6df4f7dd":"#imputing the mean_educ column\nedu_median = unc_data['meaneduc'].median()\nunc_data['meaneduc'] = unc_data['meaneduc'].fillna(edu_median)","99d4cbbe":"edu_median = unc_data_test['meaneduc'].median()\nunc_data_test['meaneduc'] = unc_data_test['meaneduc'].fillna(edu_median)","f85daa9c":"#lets see the number of samples present in the dataset for each poverty level\ntrain = unc_data\ntrain.Target.value_counts().plot.bar()","53b4281b":"#for plotting distributions with respect to each poverty level\ndef plot_distribution_cat(data,column_list):\n    plt.figure(figsize = (20, 16))\n    colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\n    poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n    for i,col in enumerate(column_list):\n        ax = plt.subplot(4,2,i+1)\n        for level,color in colors.items():\n            sns.kdeplot(train.loc[train['Target'] == level, col],ax=ax,color=color,label=poverty_mapping[level])\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n    plt.subplots_adjust(top = 2)\n\ndef plot_categorical(column,desc,label,kind):\n    grouped = train.groupby(['Target'])[column].value_counts(normalize=True)\n    grouped = grouped.rename('count')\n    grouped = grouped.reset_index(['Target',column])\n    if kind == 'bar':\n        sns.barplot(x=column, y=\"count\", hue=\"Target\", data=grouped,palette=OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}))\n        if isinstance(label[0],str):\n            rotation=60\n        else:\n            rotation=0\n    elif kind == 'point':\n        sns.pointplot(x=column, y=\"count\", hue=\"Target\", data=grouped,palette=OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}),dodge=0.18)\n        rotation=0\n    plt.xlabel(desc)\n    plt.ylabel('normalized_count')\n    locs,labels = plt.xticks()\n    plt.xticks(locs,label,rotation=rotation)\n    plt.legend(loc='best')\n    plt.show()\n    \ndef create_ordinal_columns(df,feat_dict):\n    for col_name,col_list in feat_dict.items():\n        df[col_name] = df[col_list].apply(lambda x:np.argmax(x),axis=1,raw=True)\n        print('created ordinal column {}'.format(col_name))\n\ndef corr_heatmap(column_list,method='pearson'):\n    plt.figure(figsize = (8, 8))\n    corr = train[column_list].corr(method)\n    sns.heatmap(corr,annot=True,cmap='binary_r')\n    plt.show()\n#calculate spearman and pearson correlation of columns with the Target column    \ndef calc_sp_pr(columns):\n    scorr = []\n    S_p_value = []\n    pcorr = []\n    P_p_value = []\n    for col in columns:\n        scorr.append(spearmanr(train[col],train['Target']).correlation)\n        pcorr.append(pearsonr(train[col],train['Target'])[0])\n        S_p_value.append(spearmanr(train[col],train['Target']).pvalue)\n        P_p_value.append(pearsonr(train[col],train['Target'])[1])\n    return pd.DataFrame({'spearman_r':scorr,'S_p_value':S_p_value,'pearson_r':pcorr,'P_p_value':P_p_value},index=columns).sort_values('spearman_r',ascending=False)\n\ndef boxplot_distribution(column_list):\n    plt.figure(figsize = (20, 16))\n    colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\n    poverty_mapping = ['extreme','moderate','vulnerable','non-vulnerable']\n    for i,col in enumerate(column_list):\n        ax = plt.subplot(4,2,i+1)\n        sns.boxplot(x='Target',y=col,data=train,ax=ax,palette=colors)\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n        locs,labels = plt.xticks()\n        plt.xticks(locs,poverty_mapping)\n    plt.subplots_adjust(top = 2)\n    \n","0b4579ed":"#let us check the distribution of our continuous value columns\nfloat_columns = train.select_dtypes('float')\nplot_distribution_cat(train,float_columns)","aca71202":"#let us check if there is a difference in the number of rooms in the households\nplot_categorical('rooms','No.of rooms',kind='bar',label=[i for i in range(0,13)])","9a278f7e":"plot_categorical('hogar_total','Total persons in the household',label=[i for i in range(0,20)],kind='bar')","35151388":"#let us check the no. of children,adults and elderly people for each poverty level.\nplot_categorical('hogar_nin','No. of children',kind='bar',label=[i for i in range(0,15)])\nplot_categorical('hogar_adul','No. of adults',kind='bar',label=[i for i in range(0,15)])\nplot_categorical('hogar_mayor','No. of 65+ individuals',kind='bar',label=[i for i in range(0,15)])","95275b5c":"regions = [reg for reg in train.columns if reg.startswith('lugar')]\ntrain['region'] = train[regions].apply(lambda x:np.argmax(x),axis=1,raw=True)\nregion_names=['Central','Chorotega','Pacafico central','Brunca','Huetar Atlantica','Huetar Norte']","b46190ef":"plot_categorical('region','region',region_names,kind='bar')","018ce231":"train.drop('region',axis=1,inplace=True)","3b767063":"plot_categorical('escolari','Years of Education',label=[i for i in range(0,22)],kind='bar')","c57f2fd2":"plot_categorical('v18q','Tablet',['no','yes'],kind='point')\nplot_categorical('refrig','Refrigerator',['no','yes'],kind='point')\nplot_categorical('mobilephone','Mobile Phone',['no','yes'],kind='point')\nplot_categorical('television','TV',['no','yes'],kind='point')\nplot_categorical('computer','Computer',['no','yes'],kind='point')\nplot_categorical('v14a','Bathroom',['no','yes'],kind='point')","4f3c803c":"\nordinal_attributes = {'wall_attrib' :['paredother','pareddes','paredfibras','paredmad','paredzinc','paredzocalo','paredblolad','paredpreb'],\n                       'floor_attrib':['pisonotiene','pisoother','pisonatur','pisomadera','pisocemento','pisomoscer'],\n                       'roof_attrib' :['techootro','techowaste','techocane','techozinc','techoentrepiso'],\n                       'water_attrib':['abastaguano','abastaguafuera','abastaguadentro'],\n                       'electric_attrib' : ['noelec','elecother','coopele','public','planpri'],\n                       'sanitary_attrib' : ['sanitario1','sanitario6','sanitario5','sanitario2','sanitario3'],\n                       'energy_attrib' : ['energcocinar1','energcocinar4','energcocinar3','energcocinar2'],\n                       'wall_qual' : ['epared1','epared2','epared3'],\n                       'roof_qual' : ['etecho1','etecho2','etecho3'],\n                       'floor_qual' : ['eviv1','eviv2','eviv3'],\n                       'edu_level' : ['instlevel1','instlevel2','instlevel3','instlevel4','instlevel5','instlevel6','instlevel7','instlevel8','instlevel9'],\n                       'ownership_status' : ['tipovivi5','tipovivi4','tipovivi3','tipovivi2','tipovivi1'],\n                       'area':['area2','area1']\n                  }","b0acff5f":"create_ordinal_columns(train,ordinal_attributes)","1acce5a9":"# we need to drop the redundant attributes from the dataset\nfor col_list in ordinal_attributes.values():\n    train.drop(col_list,axis=1,inplace=True)","4672face":"#we need to drop one of the columns from [male,female] and [area1,area2] and rename area1 to  area\n#male column dropped\n#area2 dropped,area1 renamed to area indicating 1-->area1,0-->area2\ntrain.drop('male',axis=1,inplace=True)\ntrain = train.rename({'area1':'area'},axis='columns')","53eb5193":"#### checking the correlation between household attributes\n# household_level attributes\nhousehold_attributes = ['hacdor','rooms','hacapo','r4t3','tamviv','hhsize','hogar_total','bedrooms','overcrowding']\ncorr_heatmap(household_attributes)","b9e5ae36":"#we can drop hhsize and r4t3 and keep hogar_total in our dataset\ntrain.drop(['hhsize','r4t3'],axis=1,inplace=True)","33683fd8":"house_attributes = ['wall_attrib','floor_attrib','roof_attrib','water_attrib','electric_attrib','sanitary_attrib','energy_attrib','wall_qual','roof_qual','floor_qual',\\\n'edu_level','ownership_status']\nresults = calc_sp_pr(house_attributes)","1b0fb2a2":"results[['spearman_r','pearson_r']].plot.barh()","02391f79":"# let us check the household attributes as well.\nhousehold_attributes = ['hacdor','rooms','hacapo','tamviv','hogar_total','bedrooms','overcrowding']\nresults = calc_sp_pr(household_attributes)","0f614848":"results[['spearman_r','pearson_r']].plot.barh()","bfb3c0ef":"amenities = ['v18q','refrig','mobilephone','television','computer','v14a']\nresults = calc_sp_pr(amenities)","cee244f2":"results[['spearman_r','pearson_r']].plot.barh()","0faae526":"#diff between persons living and size of the household\n#a positive score for having amenities like refrigerator,TV etc\n#a pos score for living in an urban area\n#score for good house condition\n#households with female heads,more number of young children and elderly,living in rural areas are indicators of poverty\n#school age children not attending school\n# standard of living feature to assess if households have amenities like refrigerator,TV and other assets etc\n#mean age of head of household\n#mean edu of household head\n#mean of no. of children age 12 and under,individuals age 65+ and over,mean of children under 18","9cfd1f0e":"#persons per room\ntrain['person\/room'] = train['rooms'] \/ train['hogar_total']\n#proportion of males\ntrain['prop_male'] = train['r4h3'] \/ train['hogar_total']\n#proportion of females\ntrain['prop_female'] = train['r4m3'] \/ train['hogar_total']\n#rent per person\ntrain['rent\/person'] = train['v2a1'] \/ train['hogar_total']\n#rent per room\ntrain['rent\/room'] = train['v2a1'] \/ train['rooms']\n#single,divorced,widowed,separated\ntrain['without_spouse'] = 0\ntrain['without_spouse'][(train.estadocivil4 == 1) | (train.estadocivil5 == 1) | (train.estadocivil6 == 1) | (train.estadocivil7 == 1)] = 1","b8f92119":"## if household head is female as a feature\ntrain['female_head'] = 0\ntrain['female_head'][(train.parentesco1 == 1) & (train.female == 1)] = 1","7fa89af5":"## proportion of children under 12\ntrain['prop_under_12'] = train['r4t1']\/train['hogar_total']","a40a6687":"#proportion of adults\ntrain['prop_adults'] = train['hogar_adul']\/train['hogar_total']","195ad32b":"#proportion of elderly\ntrain['prop_elderly'] = train['hogar_mayor']\/train['hogar_total']","516246a0":"#education of children 0-12\nidx_edu_chld = train[train.age <= 12].groupby('idhogar')['escolari'].mean()\nidx_edu_chld = idx_edu_chld.reset_index()","3b702101":"idx_edu_chld.rename(columns={\"escolari\": \"mean_edu_child\"},inplace=True)","e3158cc8":"train = train.merge(idx_edu_chld,on='idhogar',how='left')","6b4bdfc5":"#there would be some rows where mean_edu_child would be Nan as there would be some households that do not have children aged 0-12\n#because we are doing a left join on merge,some rows would be Nan as the idhogar values on our training set would not be in idx_edu_chld \n#as the households' do not have children aged 0-12\ntrain['mean_edu_child'].fillna(0,inplace=True)","be65e1f4":"#education of children 12-18\nidx_edu_teen = train[(train.age >= 12) & ( train.age < 19)].groupby('idhogar')['escolari'].mean()\nidx_edu_teen = idx_edu_teen.reset_index()","4da19580":"idx_edu_teen.rename(columns={\"escolari\": \"mean_edu_teen\"},inplace=True)","02959226":"train = train.merge(idx_edu_teen,on='idhogar',how='left')","c5023adc":"#there would be some rows where mean_edu_child would be Nan as there would be some households that do not have children aged 0-12\n#because we are doing a left join on merge,some rows would be Nan as the idhogar values on our training set would not be in idx_edu_chld \n#as the households' do not have children aged 0-12\ntrain['mean_edu_teen'].fillna(0,inplace=True)","361271ac":"#standard of living score\n#it would consider electricity,water,energy used for cooking,sanitation,amenities like refrigerator,TV,mobile phone etc.\n# it would have a low score 0 and a max score of 1\n#since we have ordered these features,it would be easy for us to normalize and add them for a final score.","115966d0":"train['standard_of_living'] = 1\/5 * (1\/4 * train['electric_attrib'] + 1\/2 * train['water_attrib'] + 1\/3 * train['energy_attrib'] + 1\/4 * train['sanitary_attrib']\\\n+ 1\/5 * (train['refrig'] + train['television'] + train['v18q'] + train['mobilephone'] + train['computer']))","a7e1d291":"train['house_quality'] = 1\/4 * ((1\/2 * train['wall_qual']) * (1\/6 * train['wall_attrib']) + (1\/2 * train['floor_qual']) * (1\/6 * train['floor_attrib']) + (1\/2 * train['roof_qual']) * (1\/6 * train['roof_attrib']) + train['cielorazo'])","9a6df5d2":"## variety of gadgets\ntrain['gadgets'] = 1\/5 * (train['refrig'] + train['television'] + train['v18q'] + train['mobilephone'] + train['computer'])\n## number of gadgets\ntrain['n_gadgets'] = train['qmobilephone'] + train['v18q1']\n##gadgets per person\ntrain['gadget\/person'] = train['n_gadgets'] \/ train['hogar_total']","efd2806a":"train['safe_waste_disposal'] = 0\ntrain['safe_waste_disposal'][(train.elimbasu1 == 1) | (train.elimbasu2 == 1)] = 1","270f53ab":"#aggregating age,escolari features\ndef agg_features(df,col_list):\n    # Define custom function\n    range_ = lambda x: x.max() - x.min()\n    range_.__name__ = 'range_'\n    \n    col_list.append('idhogar')\n    \n    ind = df[col_list]\n    \n    # Group and aggregate\n    ind_agg = ind.groupby('idhogar').agg(['min', 'max', 'sum', 'mean', 'std', range_])\n    ind_agg.head()\n    \n    new_col = []\n    for c in ind_agg.columns.levels[0]:\n        for stat in ind_agg.columns.levels[1]:\n            new_col.append(f'{c}-{stat}')\n\n    ind_agg.columns = new_col\n    \n    # Create correlation matrix\n    corr_matrix = ind_agg.corr()\n\n# Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n    \n    print('no. of features to drop having correlation > 0.95:',len(to_drop))\n    \n    if len(to_drop) > 0:\n        ind_agg = ind_agg.drop(columns = to_drop)\n        \n    print('dataset features shape before aggregation: ', df.shape)\n\n    ind_agg = ind_agg.fillna(0)\n    # Merge on the household id\n    final = df.merge(ind_agg, on = 'idhogar', how = 'left')\n\n    print('dataset features shape after aggregation: ', final.shape)\n    \n    return final\n    \n    \n    ","e5b07362":"agg_col_list = ['age','escolari','v2a1','house_quality','standard_of_living','dependency','gadgets','gadget\/person','n_gadgets']\ntrain = agg_features(train,agg_col_list)","6c0d7c17":"#we would be dropping the waste disposal columns as we have already created a new column indicating the same.\ncolumns = ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6']\ntrain.drop(columns,axis=1,inplace=True)\n","0de61409":"#dropping columns other than parentesco1\ncolumns = ['parentesco2','parentesco3','parentesco4','parentesco5','parentesco6','parentesco7','parentesco8','parentesco9','parentesco10','parentesco11','parentesco12']\ntrain.drop(columns,axis=1,inplace=True)","a01425b8":"columns = ['estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6','estadocivil7']\ntrain.drop(columns,axis=1,inplace=True)\n","02fbd7ab":"#No. of columns after feature engineering\nprint(train.shape)","7d98db27":"train.index = unc_data.index","e76ea015":"cont_variables = ['prop_adults','prop_under_12','prop_elderly','mean_edu_child','mean_edu_teen','standard_of_living','house_quality']","1a3137d6":"plot_distribution_cat(train,cont_variables)","04da23e8":"plot_categorical('safe_waste_disposal','waste_disposal',['unsafe','safe'],kind='point')\nplot_categorical('female_head','female_head',['no','yes'],kind='point')","9155e568":"new_features = ['female_head','prop_under_12','prop_adults','prop_elderly','mean_edu_child','mean_edu_teen','standard_of_living','house_quality','safe_waste_disposal']\nresults = calc_sp_pr(new_features)","e73ff929":"results[['spearman_r','pearson_r']].plot.barh()","a939f975":"col_list = ['prop_under_12','prop_adults','mean_edu_child','mean_edu_teen','standard_of_living','house_quality']\nboxplot_distribution(col_list)","a6270b0b":"test = unc_data_test.copy()\ntest.head()","03e59497":"test.drop(['hhsize','r4t3'],axis=1,inplace=True)","9a579b44":"create_ordinal_columns(test,ordinal_attributes)","d8f0775b":"for col_list in ordinal_attributes.values():\n    test.drop(col_list,axis=1,inplace=True)","9eb6d6da":"test.drop('male',axis=1,inplace=True)\ntest = test.rename({'area1':'area'},axis='columns')","f03b1f4d":"#persons per room\ntest['person\/room'] = test['rooms'] \/ test['hogar_total']\n#proportion of males\ntest['prop_male'] = test['r4h3'] \/ test['hogar_total']\n#proportion of females\ntest['prop_female'] = test['r4m3'] \/ test['hogar_total']\n#rent per person\ntest['rent\/person'] = test['v2a1'] \/ test['hogar_total']\n#rent per room\ntest['rent\/room'] = test['v2a1'] \/ test['rooms']\n#single,divorced,widowed,separated\ntest['without_spouse'] = 0\ntest['without_spouse'][(test.estadocivil4 == 1) | (test.estadocivil5 == 1) | (test.estadocivil6 == 1) | (test.estadocivil7 == 1)] = 1","e55fa898":"#if household head is female\ntest['female_head'] = 0\ntest['female_head'][(test.parentesco1 == 1) & (test.female == 1)] = 1","84ac2969":"## proportion of children under 12\ntest['prop_under_12'] = test['r4t1']\/test['hogar_total']","d99b0655":"#proportion of adults\ntest['prop_adults'] = test['hogar_adul']\/test['hogar_total']","dad4b990":"#proportion of elderly\ntest['prop_elderly'] = test['hogar_mayor']\/test['hogar_total']","7578d895":"#education of children 0-12\nidx_edu_chld = test[test.age <= 12].groupby('idhogar')['escolari'].mean()\nidx_edu_chld = idx_edu_chld.reset_index()\nidx_edu_chld.rename(columns={\"escolari\": \"mean_edu_child\"},inplace=True)\ntest = test.merge(idx_edu_chld,on='idhogar',how='left')\ntest['mean_edu_child'].fillna(0,inplace=True)","8448996f":"#education of children 12-18\nidx_edu_teen = test[(test.age >= 12) & (test.age <= 18)].groupby('idhogar')['escolari'].mean()\nidx_edu_teen = idx_edu_teen.reset_index()\nidx_edu_teen.rename(columns={\"escolari\": \"mean_edu_teen\"},inplace=True)\ntest = test.merge(idx_edu_teen,on='idhogar',how='left')\ntest['mean_edu_teen'].fillna(0,inplace=True)","256e97f8":"#standard of living score\ntest['standard_of_living'] = 1\/5 * (1\/4 * test['electric_attrib'] + 1\/2 * test['water_attrib'] + 1\/3 * test['energy_attrib'] + 1\/4 * test['sanitary_attrib']\\\n+ 1\/5 * (test['refrig'] + test['television'] + test['v18q'] + test['mobilephone'] + test['computer']))","0795391d":"#house quality score\ntest['house_quality'] = 1\/4 * ((1\/2 * test['wall_qual']) * (1\/6 * test['wall_attrib']) + (1\/2 * test['floor_qual']) * (1\/6 * test['floor_attrib']) + (1\/2 * test['roof_qual']) * (1\/6 * test['roof_attrib']) + test['cielorazo'])","423ac73d":"## variety of gadgets\ntest['gadgets'] = 1\/5 * (test['refrig'] + test['television'] + test['v18q'] + test['mobilephone'] + test['computer'])\n## number of gadgets\ntest['n_gadgets'] = test['qmobilephone'] + test['v18q1']\n##gadgets per person\ntest['gadget\/person'] = test['n_gadgets'] \/ test['hogar_total']","e4a11ad1":"#safe waste disposal\ntest['safe_waste_disposal'] = 0\ntest['safe_waste_disposal'][(test.elimbasu1 == 1) | (test.elimbasu2 == 1)] = 1","7bbcf811":"agg_col_list = ['age','escolari','v2a1','house_quality','standard_of_living','dependency','gadgets','gadget\/person','n_gadgets']\ntest = agg_features(test,agg_col_list)","92273f03":"#we would be dropping the waste disposal columns as we have already created a new column indicating the same.\ncolumns = ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6']\ntest.drop(columns,axis=1,inplace=True)\n#dropping columns other than parentesco1\ncolumns = ['parentesco2','parentesco3','parentesco4','parentesco5','parentesco6','parentesco7','parentesco8','parentesco9','parentesco10','parentesco11','parentesco12']\ntest.drop(columns,axis=1,inplace=True)\n#dropping relationship columns\ncolumns = ['estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6','estadocivil7']\ntest.drop(columns,axis=1,inplace=True)","1b0af16b":"test.index = unc_data_test.index","8cb021fe":"#We would only be using samples or rows of data where parentesco1 is 1\n#i.e we would only be using data pertaining to head of households\ntrain_head = train[train.parentesco1 == 1].reset_index()","554fbde2":"test = test.reset_index()","c96e0627":"print(train_head.shape)\nprint(test.shape)","91961e23":"#preparing the submission dataframe\nsubmission_base = test[['Id', 'idhogar']].copy()\ntest_ids = test['idhogar']\n","c01f214c":"train_head.drop(['Id','idhogar'],axis=1,inplace=True)\ntest.drop(['Id','idhogar'],axis=1,inplace=True)","6b30236e":"feat_cols = train_head.columns.difference(['Target'])\ntrain_feat = train_head[feat_cols].values\n#scaling the features to have 0 mean and unit variance\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train_feat)\ntest_scaled = scaler.fit_transform(test)","ebcd9da2":"features = train_scaled\nlabels = train_head['Target']","e05aef37":"#As we have seen,we have class imbalance in our dataset,which needs to be taken care of,\n#hence,we would be oversampling our minority classes using SMOTE algorithm\n#This algorithm creates synthetic samples from our dataset and upsamples the minority classes.","322ea892":"#creating the dataset to be used in LightGBM\nfeature_names = list(train_head.columns.difference(['Target']))\ntrain_set = gbm.Dataset(features, label = labels,feature_name=feature_names)","be74bb1c":"def avg_roc(multi_class_scores):\n    result = list()\n    for i in range(1,5):\n        roc_class = np.ravel(multi_class_scores[i])\n        result.append(np.mean(np.ravel(roc_class)))\n    avg_roc = np.mean(result)\n    return avg_roc\n    #print('average roc-auc value for each class')\n    #print('-' * 50)\n    #poverty_level = {1:'extreme',2:'moderate',3:'vulnerable',4:'non-vulnerable'}\n    #for i in range(0,4):\n        #print(poverty_level[i+1],' class--',result[i])\n    #print('-' * 50)\n    #print('overall average roc-auc value-{}'.format(np.round(avg_roc,3)))\n    \ndef plot_ROC(fpr,tpr,n_classes):\n    \n    colors = ['red', 'yellow', 'blue','green']\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=1.0,\n                 label='ROC of class {0} (area = {1:0.2f})'\n                 ''.format(i+1, roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--', lw=0.25)\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"best\")\n    plt.show()\n\ndef compare_distributions(param):\n    uniform_C = list()\n    for i in range(0,200):\n        x = sample(space)\n        uniform_C.append(x[param])\n\n    sns.kdeplot(uniform_C, label = 'uniform-dist')\n    sns.kdeplot(results[param], label = 'Bayes Optimization')\n    plt.legend(loc = 'best')\n    plt.title('{} Distribution'.format(param))\n    plt.xlabel('{}'.format(param)); plt.ylabel('Density');\n    plt.show();\n\ndef roc_auc_score_proba(y_true, proba):\n    return roc_auc_score(y_true, proba[:, 1])\n\n# define your scorer\nroc_auc_weighted = make_scorer(roc_auc_score, average='weighted')\n\ndef macro_f1_score(labels,predictions):\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True\n\ndef plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df\n","434f5f47":"#domain space\n'''space = {\n    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n}'''","dbf35245":"'''N_FOLDS=5\ndef gbm_objective(params, n_folds = N_FOLDS):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n    #y_bin = label_binarize(Y_resampled, classes=[1,2,3,4])\n    #n_classes = y_bin.shape[1]\n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # Retrieve the subsample if present otherwise set to 1.0\n    subsample = params['boosting_type'].get('subsample', 1.0)\n    \n    # Extract the boosting type\n    params['boosting_type'] = params['boosting_type']['boosting_type']\n    params['subsample'] = subsample\n    \n    # Make sure parameters that need to be integers are integers\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        params[parameter_name] = int(params[parameter_name])\n        \n    strkfold = StratifiedKFold(n_splits = n_folds, shuffle = True)\n    \n    clf = gbm.LGBMClassifier(**params,n_estimators=1000,objective='multiclass',importance_type='gain',n_jobs=-1,metric=None)\n    \n    valid_scores = []\n    best_estimators = []\n    run_times = []\n    \n    # Perform n_folds cross validation\n    for train_indices, valid_indices in strkfold.split(features, labels):\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        class_dist = Counter(y_train)\n        ratio_ = {4:class_dist[4],3:class_dist[3]*2,2:class_dist[2]*2,1:class_dist[1]*2}\n        \n        sm = SMOTE(ratio=ratio_,random_state=50)\n        X_train_res,Y_train_res = sm.fit_sample(X_train,y_train)\n    \n        start = timer()\n        \n        clf.fit(X_train_res, Y_train_res, early_stopping_rounds = 400, \n                  eval_metric = macro_f1_score, \n                  eval_set = [(X_train_res, Y_train_res), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 400)\n        \n        end = timer()\n        # Record the validation fold score\n        valid_scores.append(clf.best_score_['valid']['macro_f1'])\n        best_estimators.append(clf.best_iteration_)\n        \n        run_times.append(end - start)\n    \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1 - score\n    \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n    \n    estimators = int(np.mean(best_estimators))\n    params['n_estimators'] = estimators\n    \n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}'''\n","5a41e156":"#tpe_algorithm = tpe.suggest","9505dc0c":"#gbm_bayes_trials = Trials()","128dabac":"'''%%capture\n\n# Global variable\nglobal  ITERATION\n\nITERATION = 0\nMAX_EVALS = 100\n\n# Run optimization\nbest = fmin(fn = gbm_objective, space = space, algo = tpe.suggest, \n            max_evals = MAX_EVALS, trials = gbm_bayes_trials, rstate = np.random.RandomState(50))'''","d533dbcf":"'''gbm_results = gbm_bayes_trials.results\n#converting the results to dataframe\ndt = sorted([{'loss':trial['loss'],'boosting_type':trial['params']['boosting_type'],'colsample':trial['params']['colsample_bytree'],\\\n             'min_child_samples':trial['params']['min_child_samples'],'num_leaves':trial['params']['num_leaves'],'reg_alpha':trial['params']['reg_alpha'],'reg_lambda':trial['params']['reg_lambda'],\\\n             'subsample_for_bin':trial['params']['subsample_for_bin'],'subsample':trial['params']['subsample'],'train_time':trial['train_time']} for trial in gbm_results],key=lambda x:(x['loss'],x['train_time']))\nresults = pd.DataFrame(dt)\nresults.head()'''","1fd7a5fe":"hyper =  {'bagging_fraction': 0.9410068419634143,\n 'boosting_type': 'dart',\n 'colsample_bytree': 0.7568976937851579,\n 'min_child_samples': 35,\n 'min_child_weight': 26.709811008563438,\n 'min_split_gain': 0.05365377666160257,\n 'num_leaves': 24,\n 'reg_alpha': 1.0462496845733886,\n 'reg_lambda': 0.6874474257041001,\n 'subsample': 0.6621345483522493,\n 'subsample_for_bin': 240000}","598cee7c":"feature_names = list(train_head.columns.difference(['Target']))","bdbe8035":"from IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feat_names = feature_names\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = hyper\n    \n    # Build the model\n    model = gbm.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feat_names))\n    \n    # Convert to arrays for indexing\n    #features = np.array(features)\n    #test_features = np.array(test_features)\n    #labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        class_dist = Counter(y_train)\n        ratio_ = {4:class_dist[4],3:class_dist[3]*2,2:class_dist[2]*2,1:class_dist[1]*2}\n        \n        sm = SMOTETomek(random_state=50)\n        X_train_res,Y_train_res = sm.fit_sample(X_train,y_train)\n        \n        # Train with early stopping\n        model.fit(X_train_res, Y_train_res, early_stopping_rounds = 400, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train_res, Y_train_res), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ \/ nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feat_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","42d1bc15":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(features, labels, \n                                             test_scaled, test_ids, return_preds=False)","b2081293":"np.mean(valid_scores)","57ff4496":"submission.index = submission.Id\nsubmission.drop('Id',axis=1,inplace=True)\nsubmission.to_csv('gbm_baseline_13.csv')","6e65cc7b":"_ = plot_feature_importances(gbm_fi, threshold=0.95)","7e33323e":"## standard of living score\n1\/4(1\/4(electricity) + 1\/2(water) + 1\/3(energy) +  1\/5(sanitation) + 1\/6(refrig + TV + mobile + tablet + computer + bathroom))","7799140a":"#### we have an imbalanced class problem where we have a high number of non-vulnerable households.\n#### we can apply suitable methods such as oversampling to help our model with predicting these levels correctly.\n#### we should be taking this into consideration in the modelling phase.","42d3c545":"## GBM with bayesian optimization","79421c9b":"#### The code for bayesian optimization takes more than an hour to run and give results,\n#### I have run the respective code in my personal computer and would be using those values here as hyperparameter values for lgbm","da2ddf3e":"### Feature Scaling","c8e92751":"#### Rooms","b8c21d01":"### Data Cleaning process completed.\n### we would now be proceeding to the eda and feature engineering stages","5d3047e4":"## we need to prepare our testing set as well having the same features and ordinal columns as our training set","7db56c7d":"#### hhsize,r4t3 and hogar_total are prefectly correlated","d9348f90":"### r4t3, Total persons in the household, size of the household","c22c027c":"### we require 4 components to implement bayesian optimization\n#### 1.The objective function\n#### 2.the domain space\n#### 3.Hyperparameter optimization algorithm\n#### 4.History of results","afd5d55e":"#### some ideas on engineering new features based on research","b88b2cb6":"### Amenities(Refrigerator,TV,bathroom,tablet,mobile phone,computer)","a7e8cda8":"#### the poverty levels are more or less evenly distributed within these regions.\n#### There are a lot more samples from region 0 though.","fde024bc":"### safe waste disposal\n#### if disposal by tanker truck or burying ","eb3c5683":"#### floor,education level,wall,roof and floor quality have a slightly positive correlation with the poverty level\n#### pearson and spearman correlation are in agreement for these attributes\n#### these correlations have significant p-values as well.!!","cd5cea44":"### we'll first start by initializing the domain space,which are the values for different parameters to be used in logistic regression like penalty,solver and regularization\n#### It should be initialized as a dictionary with parameter names as keys and their respective search space as values","73bb4ccd":"## house_quality\n#### outside_wall + roof + floor + cieling\n1\/4(1\/2(wall qual) x 1\/6(wall_attrib) + 1\/2(floor qual) x 1\/5(floor_attrib) + 1\/2(roof qual) x 1\/4(roof_attrib) + ceiling)","88705ff9":"## helper functions","5450fab0":"## let us examine the distribution and relationship of new features with the target variable","c0a55c75":"# Data Exploration","15cb29b2":"#### we observe three prominent spikes in the figure at year 0,6 and 11 for all the poverty levels.\n####  we can also observe the fact that extreme poverty households have the highest count at year 0 followed by other poverty level households in descending order of the level.\n#### only non vulnerable households appear to pursue higher education from year 12 onwards","75592b4d":"#### As per the feature importance chart given by GBM,the engineered features have higher scores such as house quality and standard of living scores,even the ordinal columns which we created have fared better and have proven helpful in predicting our classes.\n#### The final step would be to predict our test labels and submit them to help us assess the scores.","901f780c":"### we would now be examining different columns and assess the differences with respect to poverty levels","3b910dec":"### let us also check the correlation of all the variables with the target","81f101fa":"### Education","bd834573":"# Data Cleaning","4e5ac061":"### oversampling with SMOTE","ce4cabe2":"#### helper functions to calculate average roc ,making roc_auc scorer function to pass it to our estimator and compare distributions after optimization","e7d77fc7":"## Helper Functions","cb3f2fc7":"### No. of children,adults and elderly people","92fc9eda":"#### overcrowding,hacdor,tamviv,hogar_total,hacapo have a negative correlation with poverty level.\n#### This is understandable as these columns represent overcrowding,household size,persons living in each household etc.\n#### We can see that as the poverty levels begin to decrease from non-vulnerable to extreme,these features begin to increase.\n#### eg.extreme poverty levels tend to have more overcrowding whereas non-vulnerable households tend to have less overcrowding","a6799bf2":"# a quick look at the figures allows to see if these columns have different distributions depending on the poverty level.\n\n#### 1.we can see a clear difference in the v18q1 column,where vulnerable households usually own a single tablet,whereas non-vulnerable households have a range of values.We can also see a small spike for extreme households at 4,which is quite unexpected for the number of tablets owned.\n#### 2.we can also see a difference in the overcrowding column where non-vulnerable households have spikes between 1 and 3 before its               distribution tapers off,whereas we see that much of the area of the distribution covers values from 1 to 4 before tapering off.\n#### 3.we see that the non-vulnerable distribution in the meaneduc column is slightly shifted to the right than the other distributions,indicating a higher mean education for these households.","86e94fff":"#### amenities like refrigerator,television,mobile phone etc have a positive correlation with the poverty level.\n#### as the poverty level increases from extreme to non-vulnerable,we tend to see an increase in the households\/individuals holding these amenities\n#### the correlations are somewhat weak ranging from 0.06-0.23","38fee177":"## Feature Engineering","5f16f509":"# Feature Construction\n#### some columns have an inherent ordering between them,and therefore it would be useful if we incorporate this ordering or structure in our data\n#### for e.g columns indicating wall quality have a natural ordering of bad < regular < good","b44004e5":"### Region","48084a4a":"#### non-vulnerable households have a much larger range of rooms ranging from 0 to 11\n#### the other households have 4-5 rooms and 6-7 in some cases","b495c783":"# Modelling","77492f2b":"#### lets use these hyper-parameter values and train our lgbm model and assess the results","630a64fe":"#### There are more number of children in extreme and moderate poverty households compared to the other poverty levels,sometimes 8 or 9 in some cases.Vulnerable and non-vulnerable poverty level households have upto 3 children after which the counts taper off sharply from 4."}}