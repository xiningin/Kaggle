{"cell_type":{"92760b6e":"code","af7a55bb":"code","67f5cc56":"code","b231383f":"code","f5fffb9a":"code","a861d66f":"code","f596fbf9":"code","560ed123":"code","8dbec7b4":"code","cd46197c":"code","fb9d2e7a":"code","9629358b":"code","10bc23e9":"code","aa41493d":"code","d18c3c93":"code","66c2a63b":"code","e94929cf":"code","a096432a":"code","45ed60df":"code","e37b5ad3":"code","f9f2543d":"code","b93bc667":"code","4e962614":"code","f14b6233":"code","73d2eafc":"code","eedc89fb":"code","047cb835":"code","57baa24a":"code","75d0c374":"code","434e658d":"code","34dfd4a0":"code","a7c8a4ca":"code","5ec53dda":"code","15f4b7d0":"code","7ad0a89d":"code","8ecd779b":"code","e43068c9":"code","3aa9e0ec":"code","058c03eb":"markdown","08796fd7":"markdown","512570ba":"markdown","d40d8d8f":"markdown","0fba9187":"markdown","12d2691b":"markdown","1e777610":"markdown","30781005":"markdown","f7db7e09":"markdown","6e8fb9d1":"markdown","f08cd77e":"markdown","1c36c3c6":"markdown","5dc3523d":"markdown","994c8462":"markdown","0bc72557":"markdown","ba7fd728":"markdown","7bcc514f":"markdown","9ca34b00":"markdown","9796cb0a":"markdown"},"source":{"92760b6e":"# import all libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport re\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import scale\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","af7a55bb":"# import Housing.csv\nhousing = pd.read_csv('..\/input\/cross-val\/Housing.csv')\nhousing.head()","67f5cc56":"# number of observations \nlen(housing.index)","b231383f":"# filter only area and price\ndf = housing.loc[:, ['area', 'price']]\ndf.head()","f5fffb9a":"# recaling the variables (both)\ndf_columns = df.columns\nscaler = MinMaxScaler()\ndf = scaler.fit_transform(df)\n\n# rename columns (since now its an np array)\ndf = pd.DataFrame(df)\ndf.columns = df_columns\n\ndf.head()","a861d66f":"# visualise area-price relationship\nsns.regplot(x=\"area\", y=\"price\", data=df, fit_reg=False)","f596fbf9":"# split into train and test\ndf_train, df_test = train_test_split(df, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 10)\nprint(len(df_train))\nprint(len(df_test))","560ed123":"# split into X and y for both train and test sets\n# reshaping is required since sklearn requires the data to be in shape\n# (n, 1), not as a series of shape (n, )\nX_train = df_train['area']\nX_train = X_train.values.reshape(-1, 1)\ny_train = df_train['price']\n\nX_test = df_test['area']\nX_test = X_test.values.reshape(-1, 1)\ny_test = df_test['price']","8dbec7b4":"len(X_train)","cd46197c":"# fit multiple polynomial features\ndegrees = [1, 2, 3, 6, 10, 20]\n\n# initialise y_train_pred and y_test_pred matrices to store the train and test predictions\n# each row is a data point, each column a prediction using a polynomial of some degree\ny_train_pred = np.zeros((len(X_train), len(degrees)))\ny_test_pred = np.zeros((len(X_test), len(degrees)))\n\nfor i, degree in enumerate(degrees):\n    \n    # make pipeline: create features, then feed them to linear_reg model\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    \n    # predict on test and train data\n    # store the predictions of each degree in the corresponding column\n    y_train_pred[:, i] = model.predict(X_train)\n    y_test_pred[:, i] = model.predict(X_test)\n    ","fb9d2e7a":"# visualise train and test predictions\n# note that the y axis is on a log scale\n\nplt.figure(figsize=(16, 8))\n\n# train data\nplt.subplot(121)\nplt.scatter(X_train, y_train)\nplt.yscale('log')\nplt.title(\"Train data\")\nfor i, degree in enumerate(degrees):    \n    plt.scatter(X_train, y_train_pred[:, i], s=15, label=str(degree))\n    plt.legend(loc='upper left')\n    \n# test data\nplt.subplot(122)\nplt.scatter(X_test, y_test)\nplt.yscale('log')\nplt.title(\"Test data\")\nfor i, degree in enumerate(degrees):    \n    plt.scatter(X_test, y_test_pred[:, i], label=str(degree))\n    plt.legend(loc='upper left')","9629358b":"# compare r2 for train and test sets (for all polynomial fits)\nprint(\"R-squared values: \\n\")\n\nfor i, degree in enumerate(degrees):\n    train_r2 = round(sklearn.metrics.r2_score(y_train, y_train_pred[:, i]), 2)\n    test_r2 = round(sklearn.metrics.r2_score(y_test, y_test_pred[:, i]), 2)\n    print(\"Polynomial degree {0}: train score={1}, test score={2}\".format(degree, \n                                                                         train_r2, \n                                                                         test_r2))","10bc23e9":"# data preparation\n\n# list of all the \"yes-no\" binary categorical variables\n# we'll map yes to 1 and no to 0\nbinary_vars_list =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n\n# defining the map function\ndef binary_map(x):\n    return x.map({'yes': 1, \"no\": 0})\n\n# applying the function to the housing variables list\nhousing[binary_vars_list] = housing[binary_vars_list].apply(binary_map)\nhousing.head()","aa41493d":"# 'dummy' variables\n# get dummy variables for 'furnishingstatus' \n# also, drop the first column of the resulting df (since n-1 dummy vars suffice)\nstatus = pd.get_dummies(housing['furnishingstatus'], drop_first = True)\nstatus.head()","d18c3c93":"# concat the dummy variable df with the main df\nhousing = pd.concat([housing, status], axis = 1)\nhousing.head()","66c2a63b":"# 'furnishingstatus' since we alreday have the dummy vars\nhousing.drop(['furnishingstatus'], axis = 1, inplace = True)\nhousing.head()","e94929cf":"# train-test 70-30 split\ndf_train, df_test = train_test_split(housing, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 100)\n\n# rescale the features\nscaler = MinMaxScaler()\n\n# apply scaler() to all the numeric columns \nnumeric_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\ndf_train[numeric_vars] = scaler.fit_transform(df_train[numeric_vars])\ndf_train.head()","a096432a":"# apply rescaling to the test set also\ndf_test[numeric_vars] = scaler.fit_transform(df_test[numeric_vars])\ndf_test.head()","45ed60df":"# divide into X_train, y_train, X_test, y_test\ny_train = df_train.pop('price')\nX_train = df_train\n\ny_test = df_test.pop('price')\nX_test = df_test","e37b5ad3":"# num of max features\nlen(X_train.columns)","f9f2543d":"# first model with an arbitrary choice of n_features\n# running RFE with number of features=10\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=10)             \nrfe = rfe.fit(X_train, y_train)","b93bc667":"# tuples of (feature name, whether selected, ranking)\n# note that the 'rank' is > 1 for non-selected features\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","4e962614":"# predict prices of X_test\ny_pred = rfe.predict(X_test)\n\n# evaluate the model on test set\nr2 = sklearn.metrics.r2_score(y_test, y_pred)\nprint(r2)","f14b6233":"# try with another value of RFE\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=6)             \nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = rfe.predict(X_test)\nr2 = sklearn.metrics.r2_score(y_test, y_pred)\nprint(r2)","73d2eafc":"# k-fold CV (using all the 13 variables)\nlm = LinearRegression()\nscores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=5)\nscores      ","eedc89fb":"# the other way of doing the same thing (more explicit)\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\nscores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=folds)\nscores   ","047cb835":"# can tune other metrics, such as MSE\nscores = cross_val_score(lm, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\nscores","57baa24a":"# number of features in X_train\nlen(X_train.columns)","75d0c374":"# step-1: create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n# step-2: specify range of hyperparameters to tune\nhyper_params = [{'n_features_to_select': list(range(1, 14))}]\n\n\n# step-3: perform grid search\n# 3.1 specify model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nrfe = RFE(lm)             \n\n# 3.2 call GridSearchCV()\nmodel_cv = GridSearchCV(estimator = rfe, \n                        param_grid = hyper_params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  \n","434e658d":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","34dfd4a0":"# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')","a7c8a4ca":"# final model\nn_features_optimal = 10\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = lm.predict(X_test)\nr2 = sklearn.metrics.r2_score(y_test, y_pred)\nprint(r2)","5ec53dda":"# reading the dataset\ncars = pd.read_csv(\"..\/input\/cross-val\/CarPrice_Assignment.csv\")","15f4b7d0":"# All data preparation steps in this cell\n\n# converting symboling to categorical\ncars['symboling'] = cars['symboling'].astype('object')\n\n\n# create new column: car_company\np = re.compile(r'\\w+-?\\w+')\ncars['car_company'] = cars['CarName'].apply(lambda x: re.findall(p, x)[0])\n\n\n# replacing misspelled car_company names\n# volkswagen\ncars.loc[(cars['car_company'] == \"vw\") | \n         (cars['car_company'] == \"vokswagen\")\n         , 'car_company'] = 'volkswagen'\n# porsche\ncars.loc[cars['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n# toyota\ncars.loc[cars['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n# nissan\ncars.loc[cars['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n# mazda\ncars.loc[cars['car_company'] == \"maxda\", 'car_company'] = 'mazda'\n\n\n# drop carname variable\ncars = cars.drop('CarName', axis=1)\n\n\n# split into X and y\nX = cars.loc[:, ['symboling', 'fueltype', 'aspiration', 'doornumber',\n       'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength',\n       'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber',\n       'enginesize', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio',\n       'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       'car_company']]\ny = cars['price']\n\n\n# creating dummy variables for categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\ncars_categorical.head()\n\n\n# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\ncars_dummies.head()\n\n\n# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)\n\n\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)\n\n\n# rescale the features\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\n\n\n# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=40)","7ad0a89d":"# number of features\nlen(X_train.columns)","8ecd779b":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n# specify range of hyperparameters\nhyper_params = [{'n_features_to_select': list(range(2, 40))}]\n\n# specify model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nrfe = RFE(lm)             \n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = rfe, \n                        param_grid = hyper_params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  ","e43068c9":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","3aa9e0ec":"# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')","058c03eb":"### 4.3 Types of Cross-Validation Schemes\n\n\n1. **K-Fold** cross-validation: Most common\n2. **Leave One Out (LOO)**: Takes each data point as the 'test sample' once, and trains the model on the rest n-1 data points. Thus, it trains n total models.\n    - Advantage: Utilises the data well since each model is trained on n-1 samples\n    - Disadvantage: Computationally expensive\n3. **Leave P-Out (LPO)**: Creat all possible splits after leaving p samples out. For n data points, there are (nCp) possibile train-test splits.\n4. (**For classification problems**) **Stratified K-Fold**: Ensures that the relative class proportion is approximately preserved in each train and validation fold. Important when ther eis huge class imbalance (e.g. 98% good customers, 2% bad).\n\n#### Additional Reading ####\nThe sklearn documentation enlists all CV schemes <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\">here.<\/a>\n","08796fd7":"### 4.2 Hyperparameter Tuning Using Grid Search Cross-Validation\n\nA common use of cross-validation is for tuning hyperparameters of a model. The most common technique is what is called **grid search** cross-validation.\n![image.png](attachment:image.png)","512570ba":"For the first experiment, we'll do regression with only one feature. Let's filter the data so it only contains `area` and `price`.","d40d8d8f":"Note that we haven't rescaled the test set yet, which we'll need to do later while making predictions.","0fba9187":"#### Using RFE \n\nNow, we have 13 predictor features. To build the model using RFE, we need to tell RFE how many features we want in the final model. It then runs a feature elimination algorithm. \n\nNote that the number of features to be used in the model is a **hyperparameter**.","12d2691b":"## 3. Cross-Validation: A Quick Recap\n\nThe following figure illustrates k-fold cross-validation with k=4. There are some other schemes to divide the training set, we'll look at them briefly later.\n![image.png](attachment:image.png)","1e777610":"### Another Example: Car Price Prediction","30781005":"## 0. Experiments to Understand Overfitting\n\nIn this section, let's quickly go through some experiments to understand what overfitting looks like. We'll run some experiments using polynomial regression.","f7db7e09":"Notice that the test score is very close to the 'mean test score' on the k-folds (about 60%). In general, the mean score estimated by CV will usually be a good estimate of the test score. ","6e8fb9d1":"Now we can choose the optimal value of number of features and build a final model.","f08cd77e":"### 4.1 K-Fold CV","1c36c3c6":"## 4. Cross-Validation in sklearn\n\nLet's now experiment with k-fold CV.","5dc3523d":"<table style=\"width:100%\">\n  <tr>\n    <th>   <\/th>\n    <th>degree-1<\/th>\n    <th>degree-2<\/th> \n    <th>degree-3<\/th>\n    <th>...<\/th>\n    <th>degree-n<\/th>\n  <\/tr>\n  <tr>\n    <th>x1<\/th>\n  <\/tr>\n  <tr>\n    <th>x2<\/th>\n  <\/tr>\n   <tr>\n    <th>x3<\/th>\n    <\/tr>\n    <tr>\n    <th>...<\/th>\n    <\/tr>\n    <tr>\n    <th>xn<\/th>\n    <\/tr>\n<\/table>","994c8462":"Let's now predict the y labels (for both train and test sets) and store the predictions in a table. Each row of the table is one data point, each column is a value of $n$ (degree).","0bc72557":"#### Splitting Into Train and Test","ba7fd728":"## 2. Problems in the Current Approach\n\nIn train-test split, we have three options:\n1. **Simply split into train and test**: But that way tuning a hyperparameter makes the model 'see' the test data (i.e. knowledge of test data leaks into the model)\n2. **Split into train, validation, test sets**: Then the validation data would eat into the training set\n3. **Cross-validation**: Split into train and test, and train multiple models by sampling the train set. Finally, just test once on the test set.\n","7bcc514f":"## Cross-Validation with Linear Regression\n\nThis notebook demonstrates how to do cross-validation (CV) with linear regression as an example (it is heavily used in almost all modelling techniques such as decision trees, SVM etc.). We will mainly use `sklearn` to do cross-validation.\n\nThis notebook is divided into the following parts:\n0. Experiments to understand overfitting\n1. Building a linear regression model without cross-validation\n2. Problems in the current approach\n3. Cross-validation: A quick recap\n4. Cross-validation in `sklearn`:\n    - 4.1 K-fold CV \n    - 4.2 Hyperparameter tuning using CV\n    - 4.3 Other CV schemes\n\n","9ca34b00":"## 1. Building a Model Without Cross-Validation\n\nLet's now build a multiple regression model. First, let's build a vanilla MLR model without any cross-validation etc. ","9796cb0a":"### Polynomial Regression\n\nYou already know simple linear regression:\n\n$y = \\beta_0 + \\beta_1 x_1$\n\nIn polynomial regression of degree $n$, we fit a curve of the form:\n\n$y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_1^2 + \\beta_3x_1^3 ... + \\beta_nx_1^n$\n\nIn the experiment below, we have fitted polynomials of various degrees on the housing data and compared their performance on train and test sets.\n\nIn sklearn, polynomial features can be generated using the `PolynomialFeatures` class. Also, to perform `LinearRegression` and `PolynomialFeatures` in tandem, we will use the module `sklearn_pipeline` - it basically creates the features and feeds the output to the model (in that sequence)."}}