{"cell_type":{"b4a6f876":"code","18690d26":"code","0f362bc8":"code","a3cfffed":"code","6e52f350":"code","65be9140":"code","93f36bfa":"code","d2038eef":"code","569b783f":"code","4f3fd58d":"code","259b4b2d":"code","0bb0fa8b":"code","fb0154a9":"code","b939748b":"code","0bfa5ede":"code","de7e4374":"code","15b1aaf0":"code","139cad75":"code","fd36e448":"code","7e902150":"code","fb296004":"code","3ae95777":"code","3731fe25":"code","a8e73c81":"code","94313ad3":"code","efd8b680":"code","13d6edf4":"code","a7daea07":"code","c8ebc9c2":"code","866601a7":"code","63a9701b":"code","78816cf7":"code","726c5a42":"markdown","be0c0875":"markdown","1ed416ef":"markdown","6c755f79":"markdown","3b8f4fb8":"markdown","f5c2fc39":"markdown","6e512de7":"markdown","f59a104c":"markdown","c5a8e063":"markdown","13acad77":"markdown","a3bddb90":"markdown","fcdd4e8f":"markdown","73774428":"markdown","64fca9ed":"markdown","d03b94a2":"markdown","e9c3aae2":"markdown","5ab3fd92":"markdown","c04f5b4f":"markdown","5ae606bc":"markdown","ac62f570":"markdown"},"source":{"b4a6f876":"#Library importing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom scipy import stats\nimport itertools\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.metrics import mean_squared_error\n\nimport datetime\nimport os\nimport math\nimport gc","18690d26":"def data_extraction(path):\n    \n    dataframe = pd.DataFrame()\n    \n    for i in os.listdir(path):\n        \n            df_temp = pd.read_csv(str(path) + \"\/\" + str(i))\n            df_temp = df_temp[[\"LCLid\",\"day\",\"energy_sum\"]]\n            df_temp.reset_index()\n            dataframe = dataframe.append(df_temp)\n    \n    return dataframe","0f362bc8":"path = \"..\/input\/smart-meters-in-london\/daily_dataset\/daily_dataset\"\ndf = data_extraction(path)\n\ndel path","a3cfffed":"### Energy per Household###\n\nenergy = df.groupby(\"day\")[[\"energy_sum\"]].sum()\ncount_of_house = df.groupby(\"day\")[[\"LCLid\"]].nunique()\n\ndf_energy = energy.merge(count_of_house, on=\"day\").reset_index()\n\ndf_energy[\"energy_per_household\"] = df_energy[\"energy_sum\"] \/ df_energy[\"LCLid\"]\ndf_energy[\"day\"] = pd.to_datetime(df_energy[\"day\"])\n\ndel energy, count_of_house\n\ngc.collect()","6e52f350":"#Weather and holiday data\nweather_df = pd.read_csv(\"..\/input\/smart-meters-in-london\/weather_daily_darksky.csv\")\nholiday_df = pd.read_csv(\"..\/input\/smart-meters-in-london\/uk_bank_holidays.csv\")","65be9140":"weather_df = weather_df[[\"temperatureMax\",\n                         \"windBearing\",\n                         \"dewPoint\",\n                         \"cloudCover\",\n                         \"windSpeed\",\n                         \"pressure\",\n                         \"time\",\n                         \"humidity\"]]\n\nweather_df[\"time\"] = pd.to_datetime(weather_df[\"time\"])\n\nweather_df.dropna(inplace=True)","93f36bfa":"weather_df.corr()","d2038eef":"scaler = MinMaxScaler()\nweather_scaled = scaler.fit_transform(weather_df[[\"cloudCover\",\"humidity\",\"windSpeed\"]]).astype(\"float64\")","569b783f":"kmeans_kwargs = {\"init\": \"k-means++\",\n                 \"n_init\": 10,\n                 \"max_iter\": 450,\n                 \"random_state\": 42}\n\ndef clustering (df):\n    \"\"\"\n    Tests posible k_mean cluster instances and scores them based on the silhouette score\n    \"\"\"\n    sc = []\n    \n    for k in range(2,15):\n        \n        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n        kmeans.fit_transform(df)\n        score = silhouette_score(df,kmeans.labels_)\n        sc.append(score)\n        \n    return sc","4f3fd58d":"sc = clustering(weather_scaled)\nplt.plot(range(2, 15), sc)","259b4b2d":"#Creating the KMean that will be used and droping the unused weather data\nkmeans = KMeans(n_clusters=5, **kmeans_kwargs)\nweather_df[\"Clusters\"]= kmeans.fit(weather_scaled).labels_\n\nto_drop = [\"windBearing\",\"dewPoint\",\"cloudCover\",\"windSpeed\",\"pressure\",\"humidity\"]\n\nweather_df.drop(to_drop,axis=1,inplace=True)","0bb0fa8b":"holiday_df[\"Bank holidays\"] = pd.to_datetime(holiday_df[\"Bank holidays\"])","fb0154a9":"df_energy = df_energy.merge(weather_df, left_on=\"day\",right_on=\"time\")\nfinal_df = df_energy.merge(holiday_df, left_on = \"day\",right_on = \"Bank holidays\",how = 'left')\nfinal_df[\"holiday_id\"] = np.where(final_df['Bank holidays'].isna(),0,1)","b939748b":"final_df.head()","0bfa5ede":"to_drop = [\"energy_sum\",\"LCLid\",\"time\",\"Bank holidays\",\"Type\"]\n\nfinal_df.drop(to_drop, axis=1, inplace=True)","de7e4374":"final_df.head()","15b1aaf0":"final_df.index = pd.DatetimeIndex(final_df[\"day\"]).to_period(\"D\")\n\nmodel_data = final_df[[\"energy_per_household\",\"temperatureMax\",\"Clusters\",\"holiday_id\"]]\n\n\ntrain = model_data.iloc[0:len(model_data)-30] \ntest = model_data.iloc[len(train):len(model_data)]\n\ndel model_data","139cad75":"train.head()","fd36e448":"###SARIMAX###\n\n#Constructs all possible parameter combinations.\np = d = q = range(0,2)\npdq = list(itertools.product(p,d,q))\n\nseasonal_pdq = [(x[0],x[1],x[2],12) for x in list(itertools.product(p,d,q))]","7e902150":"def sarimax_function(endog,exog,pdq,s_pdq):\n\n    \"\"\"\n    The function uses a brute force approach to apply all possible pdq combinations and evaluate the model\n    \"\"\"\n\n    result_list = []\n    for param in pdq:\n        for s_param in s_pdq:\n\n            model = sm.tsa.statespace.SARIMAX(endog=endog,exog=exog, order=param, seasonal_order=s_param,\n            enforce_invertibility=False,enforce_stationarity=True)\n\n            results = model.fit()\n            result_list.append([param,s_param,results.aic])\n            #print(\"ARIMA Parameters: {} x: {}. AIC: {}\".format(param,s_param,results.aic))\n\n    return result_list,results","fb296004":"endog = train[\"energy_per_household\"]\nexog = train[[\"Clusters\",\"holiday_id\",\"temperatureMax\"]]","3ae95777":"result_list,results = sarimax_function(endog,exog,pdq,seasonal_pdq)","3731fe25":"results_dataframe = pd.DataFrame(result_list, columns=[\"dpq\",\"s_dpq\",\"aic\"]).sort_values(by=\"aic\")\nresults_dataframe.head()","a8e73c81":"model = sm.tsa.statespace.SARIMAX(endog=endog,exog=exog, order=(1, 1, 1), seasonal_order=(1, 0, 1, 12),\n            enforce_invertibility=False,enforce_stationarity=True).fit()\n\nprint(model.summary().tables[1])","94313ad3":"exog = test[[\"Clusters\",\"holiday_id\",\"temperatureMax\"]]","efd8b680":"predict = model.predict(start = len(train),end = len(train)+len(test)-1,\n                            exog = test[[\"Clusters\",\"holiday_id\",\"temperatureMax\"]])\n\ntest[\"prediction\"] = predict.values","13d6edf4":"test[\"diff\"] = test[\"energy_per_household\"] - test[\"prediction\"]\nresults = mean_squared_error(test[\"energy_per_household\"],test[\"prediction\"])\nprint(results)","a7daea07":"MAE = test['diff'].sum()\/len(test)\nprint(MAE)","c8ebc9c2":"copy_test = test.copy()","866601a7":"copy_test.sort_values(by=[\"diff\"])","63a9701b":"### Results without the outlier ###\n\nresults = mean_squared_error(copy_test.iloc[:-1,:][\"energy_per_household\"],copy_test.iloc[:-1,:][\"prediction\"])\nprint(results)","78816cf7":"MAE =copy_test.iloc[:-1,:][\"diff\"].sum()\/len(test)\nprint(MAE)","726c5a42":"The optimal number of clusters can be determined by graphing the silhouette score. **Silhouette Coefficiency** measures how similar an object is to its own cluster. It's a great tool to determine the optimal amount of clusters. \n\nThe best practice is to take the \"L\" part of the curvature as the optimal number of clusters.","be0c0875":"# **SARIMAX & Feature clustering**\n\nThe following notebook will explore\/experiment with K-mean feature clustering on weather data. This was a small test project of mine that I wanted to upload.\n\nOutside conditions are one of the leading variables responsible for energy consumption. However, they are often large in number and increase dimensions for the predictive ML models. To solve this issue with a minimal loss of information, I have decided to experiment a bit with K-mean clustering.\n","1ed416ef":"Finalizing the data to be used and splitting it into train\/test portions.","6c755f79":"An important step at this point is to scale the data. This is to ensure that all the columns are valued equally in the K-mean clustering step.","3b8f4fb8":"The same metrics without the outlier look a lot better!","f5c2fc39":"Defining the test exog variables","6e512de7":"# *Clustering* #\n\nThe first step is to prepare the weather dataset for clustering. We will do this by filtering out some features and creating a new data frame. It is also important to convert the datatype of the \"time\" column into datetime.","f59a104c":"\nThe results are generally pretty ok. However, I noticed that there is an outlier in one day so we will also take a look at it.","c5a8e063":"We will test the prediction using MAE (Mean absolute error) and Mean squared error to get a general idea of how good the model is.","13acad77":"# **Prediction**\n\nWe first need to generate a model based on the information we gathered in this notebook and \"train\" it on the training portion of the data.","a3bddb90":"The results of the test indicate the optimal pdq combination based on AIC. AIC (Akaike Information Criterion -> AIC=ln (sm2) + 2m\/T). As a model selection tool, AIC has some limitations as it only provides a relative evaluation of the model. However, it is an excellent metric for checking the general quality of a model.","fcdd4e8f":"# **SARIMAX** #\n\nWe will use SARIMAX to predict the mean consumption of the dataset. However, before doing that we need to test out the optimal pqd combination of the model. I will use a very brute force method for his as the dataset isn't that large.","73774428":"Looking at the correlations we get a general idea of what weather data can be clustered. This was a bit of an experimental step and this is the best version I got. I also decided not to include temperature into the clustering portion as the column was too important and I would rather not lose information on it.","64fca9ed":"Predicting and storing the data in a data frame for comparison.","d03b94a2":"# **Data extraction**\n\nThe data used for this notebook is separated into different files\/blocks and needs to be assembled into a data frame first. ","e9c3aae2":"Importing the weather and holiday datasets. The holidays might be an interesting metric to look into further. They might have a different impact depending if the meter is placed on a household or business building. The data we are using originates from households so we might see an increase depending on the holiday.","5ab3fd92":"The core metric we will try to predict is the mean energy per unique household.  We need to calculate it as it isn't given initially in the data. To this end, we will first take the daily sum of energy consumption and divide it by the number of unique households on the same day. Its important to take unique households as the number varies per day.","c04f5b4f":"# **Additional data preparation**\n\nThis portion contains some additional data preparation before we can go on to SARIMAX predictions.\n\nThe holiday column will be coded based on a binary system, with the 1 representing that the date has a holiday and that the date is without a holiday.","5ae606bc":"When using a SARIMAX predictore we need to define the endog and exog variables to successfully run the model. To explain the two in simple terms:\n\n* The endog variable is the target variable or the response variable or the model.\n* The exog variable is the independent variable designed to explain the endog variable.","ac62f570":"# **Conclusion**\n\nThis was a small project on clustering and I see myself using this in specific situations. The speed we gain when during this might not outweigh the small decrease in accuracy when predicting consumption, but its an interesting alternative for budgeting larger scale portofolios."}}