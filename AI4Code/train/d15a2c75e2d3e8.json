{"cell_type":{"505aff8a":"code","f68a7fc8":"code","fd62ff2f":"code","b9e4f44a":"code","2e6e8007":"code","3a7d7c81":"code","f69dae28":"code","f6bf142e":"code","b086b16a":"code","244b83ae":"code","36bdff47":"code","ea9972cd":"code","936aabcf":"code","b3e3b53b":"code","df2aac96":"markdown","d64b3081":"markdown"},"source":{"505aff8a":"!pip install -qq git+https:\/\/www.github.com\/ildoonet\/tf-pose-estimation","f68a7fc8":"!pip install -qq pycocotools","fd62ff2f":"%load_ext autoreload\n%autoreload 2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})","b9e4f44a":"%matplotlib inline\nimport tf_pose\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm_notebook\nfrom PIL import Image\nimport numpy as np\nimport os\ndef video_gen(in_path):\n    c_cap = cv2.VideoCapture(in_path)\n    while c_cap.isOpened():\n        ret, frame = c_cap.read()\n        if not ret:\n            break\n        yield c_cap.get(cv2.CAP_PROP_POS_MSEC), frame[:, :, ::-1]\n    c_cap.release()","2e6e8007":"video_paths = glob('..\/input\/*.mp4')\nc_video = video_gen(video_paths[0])\nfor _ in range(300):\n    c_ts, c_frame = next(c_video)\nplt.imshow(c_frame)","3a7d7c81":"from tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\ntfpe = tf_pose.get_estimator()","f69dae28":"humans = tfpe.inference(npimg=c_frame, upsample_size=4.0)\nprint(humans)","f6bf142e":"new_image = TfPoseEstimator.draw_humans(c_frame[:, :, ::-1], humans, imgcopy=False)\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nax1.imshow(new_image[:, :, ::-1])","b086b16a":"body_to_dict = lambda c_fig: {'bp_{}_{}'.format(k, vec_name): vec_val \n                              for k, part_vec in c_fig.body_parts.items() \n                              for vec_name, vec_val in zip(['x', 'y', 'score'],\n                                                           (part_vec.x, 1-part_vec.y, part_vec.score))}\nc_fig = humans[0]\nbody_to_dict(c_fig)","244b83ae":"MAX_FRAMES = 200\nbody_pose_list = []\nfor vid_path in tqdm_notebook(video_paths, desc='Files'):\n    c_video = video_gen(vid_path)\n    c_ts, c_frame = next(c_video)\n    out_path = '{}_out.avi'.format(os.path.split(vid_path)[1])\n    out = cv2.VideoWriter(out_path,\n                          cv2.VideoWriter_fourcc('M','J','P','G'),\n                          10, \n                          (c_frame.shape[1], c_frame.shape[0]))\n    for (c_ts, c_frame), _ in zip(c_video, \n                                  tqdm_notebook(range(MAX_FRAMES), desc='Frames')):\n        bgr_frame = c_frame[:,:,::-1]\n        humans = tfpe.inference(npimg=bgr_frame, upsample_size=4.0)\n        for c_body in humans:\n            body_pose_list += [dict(video=out_path, time=c_ts, **body_to_dict(c_body))]\n        new_image = TfPoseEstimator.draw_humans(bgr_frame, humans, imgcopy=False)\n        out.write(new_image)\n    out.release()\n","36bdff47":"import pandas as pd\nbody_pose_df = pd.DataFrame(body_pose_list)\nbody_pose_df.describe()","ea9972cd":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, c_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_ax.plot(c_rows['time'], c_rows['bp_{}_y'.format(i)], label='x {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","936aabcf":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, n_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_rows = n_rows.query('bp_{}_score>0.6'.format(i)) # only keep confident results\n        c_ax.plot(c_rows['bp_{}_x'.format(i)], c_rows['bp_{}_y'.format(i)], label='BP {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","b3e3b53b":"body_pose_df.to_csv('body_pose.csv', index=False)","df2aac96":"# Overview\nThe kernel shows how to use the [tf_pose_estimation](https:\/\/github.com\/ildoonet\/tf-pose-estimation) package in Python on a series of running videos.","d64b3081":"## Libraries we need\nInstall tf_pose and pycocotools"}}