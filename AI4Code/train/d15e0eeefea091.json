{"cell_type":{"5c6ba9fc":"code","e9fc3efe":"code","19c9c9f1":"code","3bbdd0da":"code","30d9002d":"code","85c54f6e":"code","c18d897a":"code","b5e1d9ca":"code","44015480":"code","97e84bff":"code","276f8c99":"code","8d44b963":"code","834f1f59":"code","ba9542c1":"code","25f7ef16":"code","511c3f3a":"code","26bd4650":"code","27597dcf":"code","0ff7ba4e":"code","0ca6bda4":"code","10ab485e":"code","fd0d38af":"markdown","2426582a":"markdown","d7b2a522":"markdown","2122877d":"markdown","8a052451":"markdown","1be93a52":"markdown","b498043e":"markdown","db72a95a":"markdown","5d9dd804":"markdown","40f40481":"markdown","9253a1f6":"markdown","276ded36":"markdown","ec71680d":"markdown"},"source":{"5c6ba9fc":"#Colab\u4e2d\u6587\u5b57\u4f53\u89e3\u51b3\u529e\u6cd5\uff0chttps:\/\/blog.csdn.net\/xieyan0811\/article\/details\/80371201\n!wget -O \/usr\/share\/fonts\/truetype\/liberation\/simhei.ttf https:\/\/github.com\/StellarCN\/scp_zh\/raw\/master\/fonts\/SimHei.ttf","e9fc3efe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","19c9c9f1":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom string import punctuation","3bbdd0da":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nmatplotlib.rcParams['font.family']='simhei'#\u4fee\u6539\u4e86\u5168\u5c40\u53d8\u91cf\nmatplotlib.rcParams['font.size']=20\n\nzhfont = matplotlib.font_manager.FontProperties(fname='\/usr\/share\/fonts\/truetype\/liberation\/simhei.ttf')\nplt.rcParams['axes.unicode_minus'] = False # \u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7","30d9002d":"path = '\/kaggle\/input\/quora-question-pairs\/'\ncsv_train = path+\"train.csv\"\ncsv_test = path+\"test.csv\"\ntrain_orig = pd.read_csv(csv_train)\ntest_orig = pd.read_csv(csv_test)","85c54f6e":"display(train_orig.shape)\ndisplay(train_orig.head())\ndisplay(test_orig.shape)\ndisplay(test_orig.head())\n\nprint('\u6f5c\u5728\u76f8\u4f3c\u95ee\u9898\u5bf9\u6570\u91cf: {}'.format(train_orig.shape[0]))\nprint('\\'is_duplicate\\' \u6b63\u4f8b\u6bd4\u4f8b: {}%'.format(round(train_orig['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(train_orig['qid1'].tolist() + train_orig['qid2'].tolist())\nprint('\u603b\u95ee\u9898\u6570\u91cf: {}'.format(len(np.unique(qids))))\nprint('\"\u91cd\u590d\u95ee\u9898\" \u51fa\u73b0\u6b21\u6570: {}'.format(np.sum(qids.value_counts() > 1)))","c18d897a":"plt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('\u95ee\u9898\u51fa\u73b0\u6b21\u6570\u7684\u5bf9\u6570\u76f4\u65b9\u56fe', fontproperties=zhfont)\nplt.xlabel('\u95ee\u9898\u51fa\u73b0\u7684\u9891\u6570', fontproperties=zhfont)\nplt.ylabel('\u9891\u6570\u8ba1\u6570', fontproperties=zhfont)\n","b5e1d9ca":"print(train_orig.isnull().sum())\nprint(test_orig.isnull().sum())","44015480":"display(train_orig[train_orig.isnull().values==True])\ndisplay(test_orig[test_orig.isnull().values==True])","97e84bff":"train_orig = train_orig.fillna(\" \")\ntest_orig = test_orig.fillna(\" \")","276f8c99":"#apply\u663e\u793a\u8fdb\u5ea6\u6761\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"my bar!\")","8d44b963":"def common_words_transformation_remove_punctuation(text):\n    #\u8f6c\u6362\u4e3a\u5c0f\u5199\n    text = text.lower()\n\n    #\u6e05\u7406\u5b57\u7b26\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"when's\", \"when is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \", text)  # \u9664\u4e86\u4e0a\u9762\u7684\u7279\u6b8a\u60c5\u51b5\u5916\uff0c\u201c\\'s\u201d\u53ea\u80fd\u8868\u793a\u6240\u6709\u683c\uff0c\u5e94\u66ff\u6362\u6210\u201c \u201d\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" america \", text)\n    text = re.sub(r\" u s \", \" america \", text)\n    text = re.sub(r\" uk \", \" england \", text)\n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text)\n    text = re.sub(r\" ds \", \" data science \", text)\n    text = re.sub(r\" ee \", \" electronic engineering \", text)\n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iphone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the us\", \"america\", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" \", text)\n    text = re.sub(r\"\\+\", \" \", text)\n    text = re.sub(r\"\\-\", \" \", text)\n    text = re.sub(r\"\\=\", \" \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    \n    text = \"\".join([c for c in text if c not in punctuation])\n        \n    return text\n\n\n#train_orig[\"question1\"] = train_orig[\"question1\"].apply(common_words_transformation_remove_punctuation)\n#train_orig[\"question2\"] = train_orig[\"question2\"].apply(common_words_transformation_remove_punctuation)\n#test_orig[\"question1\"] = test_orig[\"question1\"].apply(common_words_transformation_remove_punctuation)\n#test_orig[\"question2\"] = test_orig[\"question2\"].apply(common_words_transformation_remove_punctuation)\n\ntrain_orig[\"question1\"] = train_orig[\"question1\"].progress_apply(common_words_transformation_remove_punctuation)\ntrain_orig[\"question2\"] = train_orig[\"question2\"].progress_apply(common_words_transformation_remove_punctuation)\ntest_orig[\"question1\"] = test_orig[\"question1\"].progress_apply(common_words_transformation_remove_punctuation)\ntest_orig[\"question2\"] = test_orig[\"question2\"].progress_apply(common_words_transformation_remove_punctuation)\n\n#\u4fdd\u5b58\u4e3a\u6587\u4ef6\ntrain_orig.to_csv(\"train_orig_trans.csv\", index = False)\ntest_orig.to_csv(\"test_orig_trans.csv\", index = False)\n\ntrain_orig.head()","834f1f59":"nltk.download('stopwords')\nstopwords.words(\"english\")\n\nnltk.download('punkt')","ba9542c1":"def remove_stopwords(text):\n    stops = set(stopwords.words(\"english\"))\n    text = word_tokenize(text)\n    text = [w for w in text if not w in stops]\n    text = \" \".join(text)\n    return text\n\ntrain_stop, test_stop = train_orig.copy(deep = True), test_orig.copy(deep = True)\n\n#train_stop[\"question1\"] = train_stop[\"question1\"].apply(remove_stopwords)\n#train_stop[\"question2\"] = train_stop[\"question2\"].apply(remove_stopwords)\n#test_stop[\"question1\"] = test_stop[\"question1\"].apply(remove_stopwords)\n#test_stop[\"question2\"] = test_stop[\"question2\"].apply(remove_stopwords)\n\ntrain_stop[\"question1\"] = train_stop[\"question1\"].progress_apply(remove_stopwords)\ntrain_stop[\"question2\"] = train_stop[\"question2\"].progress_apply(remove_stopwords)\ntest_stop[\"question1\"] = test_stop[\"question1\"].progress_apply(remove_stopwords)\ntest_stop[\"question2\"] = test_stop[\"question2\"].progress_apply(remove_stopwords)\n\n#\u4fdd\u5b58\u4e3a\u6587\u4ef6\ntrain_stop.to_csv(\"train_stop.csv\", index = False)\ntest_stop.to_csv(\"test_stop.csv\", index = False)\n\ntrain_stop.head()","25f7ef16":"def stem_words(text):\n    text = word_tokenize(text)\n    stemmer = SnowballStemmer(\"english\")\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text\n\ntrain_stem, test_stem = train_stop.copy(deep = True), test_stop.copy(deep = True)\n\n#train_stem[\"question1\"] = train_stem[\"question1\"].progress_apply(stem_words)\n#train_stem[\"question2\"] = train_stem[\"question2\"].progress_apply(stem_words)\n#test_stem[\"question1\"] = test_stem[\"question1\"].progress_apply(stem_words)\n#test_stem[\"question2\"] = test_stem[\"question2\"].progress_apply(stem_words)\n\ntrain_stem[\"question1\"] = train_stem[\"question1\"].progress_apply(stem_words)\ntrain_stem[\"question2\"] = train_stem[\"question2\"].progress_apply(stem_words)\ntest_stem[\"question1\"] = test_stem[\"question1\"].progress_apply(stem_words)\ntest_stem[\"question2\"] = test_stem[\"question2\"].progress_apply(stem_words)\n\n#\u4fdd\u5b58\u4e3a\u6587\u4ef6\ntrain_stem.to_csv(\"train_stem.csv\", index = False)\ntest_stem.to_csv(\"test_stem.csv\", index = False)\n\ntrain_stem.head()","511c3f3a":"nltk.download('wordnet')","26bd4650":"def lemmatize_words(text):\n    text = word_tokenize(text)\n    wordnet_lemmatizer = WordNetLemmatizer()\n    lammatized_words = [wordnet_lemmatizer.lemmatize(word) for word in text]\n    text = \" \".join(lammatized_words)\n    return text\n\ntrain_lem, test_lem = train_stop.copy(deep = True), test_stop.copy(deep = True)\n\n#train_lem[\"question1\"] = train_lem[\"question1\"].apply(lemmatize_words)\n#train_lem[\"question2\"] = train_lem[\"question2\"].apply(lemmatize_words)\n#test_lem[\"question1\"] = test_lem[\"question1\"].apply(lemmatize_words)\n#test_lem[\"question2\"] = test_lem[\"question2\"].apply(lemmatize_words)\n\ntrain_lem[\"question1\"] = train_lem[\"question1\"].progress_apply(lemmatize_words)\ntrain_lem[\"question2\"] = train_lem[\"question2\"].progress_apply(lemmatize_words)\ntest_lem[\"question1\"] = test_lem[\"question1\"].progress_apply(lemmatize_words)\ntest_lem[\"question2\"] = test_lem[\"question2\"].progress_apply(lemmatize_words)\n\ntrain_lem.to_csv(\"train_lem.csv\", index = False)\ntest_lem.to_csv(\"test_lem.csv\", index = False)\n\ntrain_lem.head()","27597dcf":"display(train_orig.head(3)) #\u7b26\u53f7\u8f6c\u4e49\u548c\u6e05\u9664\ndisplay(train_stop.head(3)) #\u53bb\u9664\u505c\u7528\u8bcd\ndisplay(train_stem.head(3)) #\u63d0\u53d6\u5355\u8bcd\u8bcd\u6839\ndisplay(train_lem.head(3)) #\u8fd8\u539f\u5355\u8bcd\u7684\u539f\u578b","0ff7ba4e":"#train_orig = pd.read_csv('train_orig_trans.csv')\n\ntrain_qs = pd.Series(train_orig['question1'].tolist() + train_orig['question2'].tolist()).astype(str)\n\nfrom wordcloud import WordCloud\ncloud_train = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud_train)\nplt.axis('off')\nplt.show()","0ca6bda4":"#test_orig = pd.read_csv('test_orig_trans.csv')\n\ntest_qs = pd.Series(test_orig['question1'].tolist() + test_orig['question2'].tolist()).astype(str)\n\nfrom wordcloud import WordCloud\ncloud_test = WordCloud(width=1440, height=1080).generate(\" \".join(test_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud_test)\nplt.axis('off')\nplt.show()","10ab485e":"!ls -lh ","fd0d38af":"## \u5904\u7406\u7f3a\u5931\uff0c\u8f6c\u6362\u540c\u4e49\u8bcd\uff0c\u8f6c\u4e49\u6807\u70b9\u7b26\u53f7","2426582a":"Public\u548cPrivate\u699c\u4e0a\u7684\u5f97\u5206\u4e5f\u53ef\u4ee5\u5728[Quora Question Pairs: XGBoost](https:\/\/www.kaggle.com\/benjaminkz\/quora-question-pairs-xgboost)\u4e0a\u67e5\u770b\u3002","d7b2a522":"\u7528NLTK\u4e2d\u7684stopwords\u6e05\u7406\u505c\u7528\u8bcd\n","2122877d":"\u4f7f\u7528NLTK\u4e2d\u7684WordNetLemmatizer\u8fd8\u539f\u8bcd\u7684\u539f\u578b","8a052451":"### \u53bb\u9664\u505c\u7528\u8bcd","1be93a52":"# 1 \u6570\u636e\u6e05\u6d17\n\n\u672c\u6b21\u4efb\u52a1\u7684\u6570\u636e\u6e05\u6d17\u5de5\u4f5c\u8f83\u4e3a\u7e41\u91cd\uff0c\u4e3a\u4e86\u7167\u987e\u5230\u540e\u7eed\u7279\u5f81\u63d0\u53d6\u7684\u4e0d\u540c\u9700\u8981\uff0c\u672c\u6587\u751f\u6210\u4e86\u4ee5\u4e0b\u591a\u4e2a\u7248\u672c\u7684\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a\n\n<table>\n<tr><td>train_orig.csv\u3001test_orig.csv<\/td><td>\u5728\u539f\u59cb\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\uff0c\u5904\u7406\u7f3a\u5931\uff0c\u8f6c\u6362\u540c\u4e49\u8bcd\uff0c\u8f6c\u4e49\u6807\u70b9\u7b26\u53f7<\/td><\/tr>\n<tr><td>train_stop.csv\u3001test_stop.csv<\/td><td>\u5728train_orig\/test_orig\u7684\u57fa\u7840\u4e0a\uff0c\u53bb\u9664\u505c\u7528\u8bcd<\/td><\/tr>\n<tr><td>train_stem.csv\u3001test_stem.csv<\/td><td>\u5728train_stop\/test_stop\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u53d6\u5355\u8bcd\u8bcd\u6839<\/td><\/tr>\n<tr><td>train_lem.csv\u3001test_lem.csv<\/td><td>\u5728train_stop\/test_stop\u7684\u57fa\u7840\u4e0a\uff0c\u8fd8\u539f\u5355\u8bcd\u7684\u539f\u578b<\/td><\/tr>\n<\/table>","b498043e":"**\u8bcd\u4e91**","db72a95a":"### \u8fd8\u539f\u5355\u8bcd\u7684\u539f\u578b","5d9dd804":"\u4f7f\u7528NLTK\u5e93\u4e2d\u7684SnowballStemmer\u63d0\u53d6\u8bcd\u6839","40f40481":"\u672c\u7bc7\u501f\u9274\u4e86[Zhu Kai](https:\/\/www.kaggle.com\/benjaminkz) \u7684 [Quora Question Pairs: XGBoost](https:\/\/www.kaggle.com\/benjaminkz\/quora-question-pairs-xgboost)\n\n\u53ef\u4ee5\u4f5c\u4e3aXGBoost\u7684\u5165\u95e8\u6848\u4f8b\uff0c\u5176\u4e2d\u7684\u6587\u672c\u7279\u5f81\u5206\u6790\u65b9\u6cd5\u4e5f\u6bd4\u8f83\u7ecf\u5178\u6613\u64cd\u4f5c\uff01\n\n---","9253a1f6":"### \u63d0\u53d6\u5355\u8bcd\u8bcd\u6839","276ded36":"\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u8bb8\u591a\u540c\u4e49\u8868\u8ff0\uff0c\u4ee5\u53ca\u6807\u70b9\u7b26\u53f7\u548c\u6742\u4e71\u7684\u7279\u6b8a\u7b26\u53f7\u3002\u672c\u6587\u9996\u5148\u5c06\u6240\u6709\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\uff0c\u5176\u6b21\u628a\u5e38\u89c1\u7f29\u5199\u66ff\u6362\u6210\u6ca1\u6709\u7f29\u5199\u7684\u5f62\u5f0f\uff0c\u518d\u6b21\u53bb\u9664\u6807\u70b9\u7b26\u53f7\u548c\u7279\u6b8a\u7b26\u53f7\uff0c\u6700\u540e\u662f\u4ece\u5176\u4ed6kernel\u4e0a\u6536\u96c6\u5230\u7684\u5355\u8bcd\u66ff\u6362\u3002\u5b9a\u4e49common_words_transformation_remove_punctuation\u51fd\u6570\u6765\u5b8c\u6210\u4ee5\u4e0a\u8fc7\u7a0b\uff0c\u5f97\u5230\u7b2c\u4e00\u5c42\u6837\u672ctrain_orig\/test_orig\u3002","ec71680d":"### \u5904\u7406\u7a7a\u503c\n\n\u586b\u8865\u7a7a\u503c\uff0c\u8fd9\u91cc\u4e0d\u80fd\u4e22\u5f03\u7a7a\u503c"}}