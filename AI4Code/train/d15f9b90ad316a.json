{"cell_type":{"89bee0e9":"code","86ee7bf3":"code","289dcb28":"code","aae3dfac":"code","79154f6d":"code","770dc544":"code","6caa3e06":"code","58caa67f":"code","0fb8f519":"code","9ebb2b49":"code","3c8815ed":"code","9bba6d9f":"code","b69e201d":"code","e27cc622":"code","20d740e8":"code","9e1be121":"code","ac3d5203":"code","6151fcc0":"code","79eb4e14":"code","c37a9bb9":"code","cd7c6dc2":"code","7f918293":"markdown","8d0558b1":"markdown","980816cf":"markdown","8c7bb3cb":"markdown","23e67423":"markdown","197791ff":"markdown","2618e7f4":"markdown","feda0eed":"markdown","a3f4e9fd":"markdown","4608e731":"markdown","57ded5d3":"markdown","fa2e17b1":"markdown","600e5b89":"markdown","83775210":"markdown","e6e64523":"markdown","8955ba4a":"markdown","9fc69402":"markdown","defb3c61":"markdown","4d99507e":"markdown","0e049c34":"markdown","8c232a56":"markdown","cd1959fc":"markdown","a09765a7":"markdown"},"source":{"89bee0e9":"%%capture\n!pip install coiled dask==2021.04.1 distributed==2021.04.1 mlforecast[distributed]","86ee7bf3":"%%capture\n%%bash\ngit clone --recursive https:\/\/github.com\/microsoft\/LightGBM.git \/kaggle\/tmp\/LightGBM\ncd \/kaggle\/tmp\/LightGBM\/python-package\npython setup.py install","289dcb28":"from functools import partial\nfrom pathlib import Path\n\nimport coiled\nimport dask.dataframe as dd\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom dask.distributed import Client\nfrom mlforecast.core import TimeSeries\nfrom mlforecast.distributed.forecast import DistributedForecast\nfrom mlforecast.distributed.models.lgb import LGBMForecast\nfrom window_ops.rolling import rolling_mean\n\nfrom kaggle_secrets import UserSecretsClient","aae3dfac":"assert lgb.__version__ > '3.2.1'","79154f6d":"%%time\nuser_secrets = UserSecretsClient()\nCOILED_TOKEN = user_secrets.get_secret('COILED_TOKEN')  # fill this in Add-ons -> Secrets\n\ncloud = coiled.Cloud(\n    user='jose-moralez',  # your coiled user here\n    token=COILED_TOKEN,\n)\ncluster = coiled.Cluster(\n    name='m5-mlforecast',\n    software='jose-moralez\/mlforecast',\n    n_workers=4,\n    worker_cpu=4,\n    worker_memory='8 GiB',\n    scheduler_cpu=1,\n    scheduler_memory='8 GiB',\n    cloud=cloud,\n    backend_options=dict(region='us-east-2'),\n    shutdown_on_close=True,\n)","770dc544":"client = Client(cluster)\nclient.wait_for_workers(4)\nclient","6caa3e06":"input_path = Path('..\/input\/m5-preprocess\/processed\/')\n\ndata = pd.read_parquet(input_path\/'sales.parquet')\ndata","58caa67f":"data = data.rename(columns={'id': 'unique_id', 'date': 'ds'})\ndata = data.set_index('unique_id')\ndata","0fb8f519":"remote_data = dd.from_pandas(data, npartitions=4).persist()\nremote_data","9ebb2b49":"prices = pd.read_parquet(input_path\/'prices.parquet')\nprices","3c8815ed":"cal = pd.read_parquet(input_path\/'calendar.parquet')\ncal = cal.rename(columns={'date': 'ds'})\ncal.head()","9bba6d9f":"lgb_params = {\n    'objective': 'poisson',\n    'metric': 'rmse',\n    'force_row_wise': True,\n    'learning_rate': 0.075,\n    'bagging_freq': 1,\n    'bagging_fraction': 0.75,\n    'lambda_l2': 0.1,\n    'n_estimators': 1200,\n    'num_leaves': 128,\n    'min_data_in_leaf': 100,\n}\n\nmodel = LGBMForecast(**lgb_params)\nmodel","b69e201d":"ts = TimeSeries(\n    freq='D',\n    lags=[7, 28],\n    lag_transforms = {\n        7:  [(rolling_mean, 7), (rolling_mean, 28)],\n        28: [(rolling_mean, 7), (rolling_mean, 28)],\n    },\n    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week'],\n)\nts","e27cc622":"fcst = DistributedForecast(model, ts)","20d740e8":"%%time\nfcst.fit(\n    remote_data,\n    dropna=True,    \n    keep_last_n=28+27,\n    static_features=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']        \n)","9e1be121":"def my_predict_fn(model, new_x, features_order, cal, prices, alpha):\n    new_x = new_x.reset_index()  # for sorting later\n    new_x = new_x.merge(cal)\n    new_x = new_x.merge(prices)\n    new_x = new_x.sort_values('unique_id')\n    new_x = new_x[features_order]\n    predictions = model.predict(new_x)\n    return alpha * predictions","ac3d5203":"cal_future = client.scatter(cal, broadcast=True)\nprices_future = client.scatter(prices, broadcast=True)","6151fcc0":"%%time\nalphas = [1.028, 1.023, 1.018]\npreds = None\nfor alpha in alphas:\n    alpha_preds = fcst.predict(\n        horizon=28,\n        predict_fn=my_predict_fn,\n        cal=cal_future,\n        prices=prices_future,\n        alpha=alpha\n    ).compute()\n    alpha_preds = alpha_preds.set_index('ds', append=True)\n    if preds is None:\n        preds = 1 \/ 3 * alpha_preds\n    else:\n        preds += 1 \/ 3 * alpha_preds\npreds","79eb4e14":"cluster.close()\nclient.close()","c37a9bb9":"wide = preds.reset_index().pivot_table(index='unique_id', columns='ds')\nwide.columns = [f'F{i+1}' for i in range(28)]\nwide.columns.name = None\nwide.index.name = 'id'\nwide","cd7c6dc2":"sample_sub = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sample_submission.csv', index_col='id')\nsample_sub.update(wide)\nnp.testing.assert_allclose(sample_sub.sum().sum(), preds['y_pred'].sum())\nsample_sub.to_csv('submission.csv')","7f918293":"### Model","8d0558b1":"# M5 using mlforecast\n\n[mlforecast](https:\/\/nixtla.github.io\/mlforecast\/) is a framework to perform time series forecasting using machine learning models. It abstracts away most of the details and tries to mimic the scikit-learn API.\n\nThis notebook is inspired by https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50.","980816cf":"## Training","8c7bb3cb":"# Build lightgbm from source\nThis is needed because there was a [bug](https:\/\/github.com\/microsoft\/LightGBM\/issues\/4026) in distributed training.","23e67423":"### Define forecaster\nOnce we have our model and time series, we instantiate a `Forecast` object with them.","197791ff":"At the time of making this notebook, LightGBM doesn't support evaluation sets in distributed training yet (follow [this PR](https:\/\/github.com\/microsoft\/LightGBM\/pull\/4101) if you're interested), so we'll just call `Forecast.fit` on our data which will perform the preprocessing and training on all available data.\n\n`Forecast.fit` takes the following additional arguments:\n\n* **dropna**: whether or not to drop rows with null values after building all the features. Using lags and transformations on the lags generates many rows with `np.nan`s, this is a flag to indicate whether we want to drop them when we're done.\n* **keep_last_n**: keep only last `n` samples from each time serie after computing the features. The updates are performed by applying the transformations on the series again and taking only the last value. This can save memory if you have very long series and your transformations only use a small window, like in this case where we have series with thousands of data points and our transformations require only 28 (lag) + 27 (window) samples.\n* **static_features**: define which features are static. By default all extra columns (other than **ds** and **y**) are considered static and are replicated when building the features for the next timestep, setting this overrides that and repeats only the ones defined here.","2618e7f4":"# Environment setup","feda0eed":"Metadata for predictions","a3f4e9fd":"## Forecast setup","4608e731":"Send this data to the cluster. Note that this isn't the best way to do it, a better way would be to save this to a remote storage like S3 and read it from there, however since this notebook reads the data from its inputs then this is probably the easiest way.\n\nWe set the number of partitions equal to the number of workers and the partitions will be made along the series ids, this ensures that every partition holds different series and that there aren't any series that are present in more than one partition.","57ded5d3":"By default the predictions are computed repeating the static features and updating the transformations and the date features. If you want to do something different you can define your own predict function as explained [here](https:\/\/nixtla.github.io\/mlforecast\/forecast.html#Custom-predictions).","fa2e17b1":"# Libraries","600e5b89":"# Cluster setup","83775210":"# Install distributed dependencies","e6e64523":"## Submission","8955ba4a":"# Data loading","9fc69402":"There are two inputs needed: a regressor that follows the scikit-learn API and a time series object which defines the features to be computed.","defb3c61":"## Shutdown cluster","4d99507e":"## Predictions","0e049c34":"Calling `Forecast.predict(horizon)` computes the predictions for the next `horizon` steps. We can also provide a custom `predict_fn` like we do in this case, using `my_predict_fn` defined above. This step uses multithreading if `num_threads` was set to a value greater than 1 or was left empty and you have more than 1 cpu (here we have 4).","8c232a56":"We'll send calendar and prices to each worker so the prediction function takes the local dataframes instead of serializing them in the function.","cd1959fc":"### TimeSeries\nThis is where we define the features. A brief description of each argument:\n\n* **freq**: frequency of our time series. This is a pandas abbreviation and is used to get the next dates when computing the predictions.\n* **lags**: lags that we want to use as features.\n* **lag_transforms**: dictionary where the keys are the lags that we want to use and the values are a list of transformations to apply to them. The transformations are defined as `numba` jitted functions. If the function takes more arguments than the input array, these are passed as a tuple `(func, arg1, arg2, ...)`.\n* **date_features**: date attributes to use for training. These are computed from the `ds` column and are updated in each timestep.\\n* **num_threads**: number of threads to use in preprocessing and updates, defaults to all cpus. Since the transformations are `numba` jitted functions, we can use multithreading to compute our features.","a09765a7":"mlforecast requires a dataframe with an index named **unique_id** which identifies each time serie, a column **ds** containing the datestamps and a column **y** with the series values."}}