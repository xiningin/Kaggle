{"cell_type":{"5166ccec":"code","4adf2bfa":"code","2b3c74fb":"code","62d8a909":"code","a05f6be4":"code","4424cb03":"code","22fa267d":"code","a9704f37":"code","620a9941":"code","f8b51a77":"code","fa23abde":"code","c4ca4d72":"code","1f8f4a80":"code","e4fd820d":"code","1a0f5109":"code","219c07bd":"code","84e7085c":"code","9bf60cff":"code","48718849":"code","121d1f0d":"code","30958045":"code","0616cfdb":"code","8d565130":"code","9eb43a7c":"code","06be3fee":"code","c0feb3ab":"code","3007b2ed":"code","7c310fed":"code","69f078d9":"code","7447ab4f":"code","4810644a":"code","a173b4a8":"code","d9c6ee72":"code","1044186e":"code","7a12ebd0":"code","67d3f7c0":"code","b67e2090":"code","42b5acda":"code","6eb1b550":"code","5ba31dd3":"code","5ad271c7":"code","dbd02cd3":"code","d58e751c":"code","1f98872f":"code","1284fc64":"code","1705d001":"code","ebf9413d":"code","871190fa":"code","149c02c8":"code","6128fab1":"code","45bf9df0":"code","0eb52c8b":"code","e37946b5":"code","5fbf13e0":"markdown","653218ad":"markdown","7b6bcca5":"markdown","b4707ff9":"markdown","3f0b9471":"markdown","a535e492":"markdown","1de12220":"markdown","2a63e362":"markdown","5aa5d91d":"markdown","ff0d4b99":"markdown","a33f04bb":"markdown","f0c0b519":"markdown","30bef2d1":"markdown"},"source":{"5166ccec":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4adf2bfa":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2b3c74fb":"train = pd.read_csv(\"\/kaggle\/input\/task-03\/train_full.csv\", sep = \",\")\ntrain.drop(\"Game\", axis = 1, inplace = True)","62d8a909":"test = pd.read_csv(\"\/kaggle\/input\/task-03\/test_without_label.csv\")\ngame = test.Game\ntest.drop(\"Game\", axis = 1, inplace = True)","a05f6be4":"train.columns = [x.strip() for x in train.columns] #Retira espa\u00e7o que tinha no final do nome das colunas\ntest.columns = [x.strip() for x in test.columns] #Retira espa\u00e7o que tinha no final do nome das colunas","4424cb03":"def getData(df):\n    \"\"\"\n    Pega dia da semana e mes que ocorreu o jogo\n    \"\"\"\n    \n    df[\"DiaSemana\"] = df[\"Data\"].apply(lambda x : x.split(\" \")[0].strip(\",\"))\n    df[\"Mes\"] = df[\"Data\"].apply(lambda x : x.split(\" \")[1])\n    \n    months = {\"January\":1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7,\n             \"August\":8, \"September\":9, \"October\":10, \"November\":11, \"December\":12}\n    \n    df[\"Mes\"] = df[\"Mes\"].replace(months)\n    df[\"Mes\"] = df[\"Mes\"].replace({8:4, 9:5, 10:6}) # Playoffs de 2020 atrasaram...\n    \n    \n    columns_to_drop = [\"Data\"]\n    df.drop(columns_to_drop, axis = 1, inplace = True)\n    \n    return df","22fa267d":"def arrumaTime(df):\n    \"\"\"\n    Pega nome antigo e muda pros atuais\n    \"\"\"\n    \n    dic = {\"New Jersey Nets\": \"Brooklyn Nets\",\n           \"New Orleans Hornets\": \"New Orleans Pelicans\",\n           \"Charlotte Bobcats\": \"Charlotte Hornets\"}\n    \n    df[\"H_Team\"] = df[\"H_Team\"].replace(dic)\n    df[\"A_Team\"] = df[\"A_Team\"].replace(dic)\n    \n    return df","a9704f37":"def getName(x, divisions):\n    \n    for k, v in divisions.items():\n        if x in divisions[k]:\n            return k\n\ndef getDivisao(df):\n    \"\"\" \n    Pega a divis\u00e3o q o time joga.\n    \"\"\"\n    \n    #East\n    atlantic = [\"Toronto Raptors\", \"Boston Celtics\", \"New York Knicks\",\"Brooklyn Nets\", \"Philadelphia 76ers\"]\n    central = [\"Cleveland Cavaliers\", \"Indiana Pacers\", \"Detroit Pistons\", \"Chicago Bulls\", \"Milwaukee Bucks\"]\n    southeast = [\"Miami Heat\", \"Atlanta Hawks\", \"Charlotte Hornets\", \"Washington Wizards\", \"Orlando Magic\"]\n    \n    #West\n    northwest = [\"Oklahoma City Thunder\", \"Portland Trail Blazers\", \"Utah Jazz\", \"Denver Nuggets\", \"Minnesota Timberwolves\"]\n    pacific = [\"Golden State Warriors\", \"Los Angeles Clippers\", \"Sacramento Kings\", \"Phoenix Suns\", \"Los Angeles Lakers\"]\n    southwest = [\"San Antonio Spurs\", \"Dallas Mavericks\", \"Memphis Grizzlies\", \"Houston Rockets\", \"New Orleans Pelicans\"]\n    \n    divisions = {\"Atlantic\":atlantic, \"Central\":central, \"Southeast\":southeast,\n                \"Northwest\":northwest, \"Pacific\":pacific, \"Southwest\":southwest}\n    \n    #df[\"H_Conference\"] = df[\"H_Team\"].apply(lambda x : \"West\" if x in west else \"East\")\n    #df[\"A_Conference\"] = df[\"A_Team\"].apply(lambda x : \"West\" if x in west else \"East\")\n    \n    df[\"H_Division\"] = df[\"H_Team\"].apply(getName, args = (divisions, ))\n    df[\"A_Division\"] = df[\"A_Team\"].apply(getName, args = (divisions, ))\n    \n    return df","620a9941":"def colocaDados(linha, base, nome_base):\n    \n    home_team = linha[\"H_Team\"]\n    away_team = linha[\"A_Team\"]\n    ano = linha[\"Year\"]\n    \n    base = base.set_index(\"Time\")\n    \n    linha[\"H_\"+nome_base] = base.loc[home_team, str(ano)]\n    linha[\"A_\"+nome_base] = base.loc[away_team, str(ano)]\n    \n    return linha","f8b51a77":"def getMoreData(df):\n    \"\"\"\n    Pega a dist\u00e2ncia que os times viajaram durante a temporada regular.\n    Pega o rank do time na temporada regular. (Pode pegar atrav\u00e9s da W\/L%).\n    Pega os gastos do time com sal\u00e1rio dos jogadores.\n    Pega a nota do time no draft.\n    \"\"\"\n    \n    distance = pd.read_csv(\"\/kaggle\/input\/dadosnba\/distance.csv\", sep = \";\")\n    rank = pd.read_csv(\"\/kaggle\/input\/dadosnba\/rank.csv\", sep = \";\")\n    salario = pd.read_csv(\"\/kaggle\/input\/dadosnba\/salario.csv\", sep = \";\")\n    draft = pd.read_csv(\"\/kaggle\/input\/dadosnba\/draft.csv\", sep = \";\") #A - 10; B - 8; C - 6; D - 4; E - 2; F - 0; I (N\u00e3o draftou) - 5.\n                                                      #Por exemplo, o Zion (Jogou em Duke em 2018\/2019)\n                                                            #escolhido no Draft de 2019 pelo NOP foi considerado nota A.\n    #Pega s\u00f3 o nome do time, n\u00e3o a cidade\n    df[\"H_Team\"] = df[\"H_Team\"].apply(lambda x: x.split()[-1])\n    df[\"A_Team\"] = df[\"A_Team\"].apply(lambda x: x.split()[-1])\n    \n    df = df.apply(colocaDados, axis = 1, args = (distance,\"DistTravelled\",))\n    df = df.apply(colocaDados, axis = 1, args = (rank,\"Rank\",))\n    df = df.apply(colocaDados, axis = 1, args = (salario,\"Salary\",))\n    df = df.apply(colocaDados, axis = 1, args = (draft,\"Draft\",))\n    \n    return df","fa23abde":"def triploDuplo(df):\n    \"\"\"\n    Calcula o n\u00famero m\u00e9dio de triplo-duplo e duplo-duplo que cada time (Home, OppHome, Away, OppAway) fez durante a temporada.\n    Depois, ve qual a diferen\u00e7a m\u00e9dia de TD e DD que os times tem (Home com Away, OppHome com OppAway).\n    \"\"\"\n    \n    \n    df_aux = pd.DataFrame(columns = list(df.columns.values)+[\"TDDif\", \"DDDif\", \"TDOppDif\", \"DDOppDif\"])\n    \n    for idx, linha in df.iterrows():\n        \n        #Home Team\n        H_n10 = linha[[\"H_AvgPointsPerGame\", \"H_TRB\", \"H_AST\", \"H_BLK\", \"H_TOV\"]].divide(10)\n        \n        H_TD = 0 #qnt m\u00e9dia de triplo duplos feitos pelo Home Team por jogo.\n        H_DD = 0 #qnt m\u00e9dia de duplo duplo feitos pelo Home Team por jogo.        \n      \n        aux = (H_n10 > 1).sum()\n        \n        \n        while aux >= 3:\n            H_TD += 1\n            H_n10 = H_n10 - 1\n            aux = (H_n10 > 1).sum()\n            \n        while aux >= 2:\n            H_DD += 1\n            H_n10 = H_n10 - 1\n            aux = (H_n10 > 1).sum()\n            \n            \n            \n            \n        #Opponent Home Team\n        H_On10 = linha[[\"H_AvgPointsPerGameOpp\", \"H_OTRB\", \"H_OAST\", \"H_OBLK\", \"H_OTOV\"]].divide(10)\n        \n        H_OTD = 0 #qnt m\u00e9dia de triplo duplos feitos pelo Home Team por jogo.\n        H_ODD = 0 #qnt m\u00e9dia de duplo duplo feitos pelo Home Team por jogo.        \n      \n        aux = (H_On10 > 1).sum()\n        \n        while aux >= 3:\n            H_OTD += 1\n            H_On10 = H_On10 - 1\n            aux = (H_On10 > 1).sum()\n            \n        while aux >= 2:\n            H_ODD += 1\n            H_On10 = H_On10 - 1\n            aux = (H_On10 > 1).sum()\n            \n            \n            \n            \n        \n        #Away Team\n        A_n10 = linha[[\"A_AvgPointsPerGame\", \"A_TRB\", \"A_AST\", \"A_BLK\", \"A_TOV\"]].divide(10)\n        \n        A_TD = 0 #qnt m\u00e9dia de triplo duplos feitos pelo Home Team por jogo.\n        A_DD = 0 #qnt m\u00e9dia de duplo duplo feitos pelo Home Team por jogo.        \n      \n        aux = (A_n10 > 1).sum()\n        \n        \n        \n        while aux >= 3:\n            A_TD += 1\n            A_n10 = A_n10 - 1\n            aux = (A_n10 > 1).sum()\n            \n        while aux >= 2:\n            A_DD += 1\n            A_n10 = A_n10 - 1\n            aux = (A_n10 > 1).sum()\n            \n        #Opponent Away Team\n        A_On10 = linha[[\"A_AvgPointsPerGameOpp\", \"A_OTRB\", \"A_OAST\", \"A_OBLK\", \"A_OTOV\"]].divide(10)\n        \n        A_OTD = 0 #qnt m\u00e9dia de triplo duplos feitos pelo Home Team por jogo.\n        A_ODD = 0 #qnt m\u00e9dia de duplo duplo feitos pelo Home Team por jogo.        \n      \n        aux = (A_On10 > 1).sum()\n        \n        while aux >= 3:\n            A_OTD += 1\n            A_On10 = A_On10 - 1\n            aux = (A_On10 > 1).sum()\n            \n        while aux >= 2:\n            A_ODD += 1\n            A_On10 = A_On10 - 1\n            aux = (A_On10 > 1).sum()\n    \n        \n        # Coloca isso no Data Frame\n        linha[\"TDDif\"] = pd.to_numeric(H_TD - A_TD)\n        linha[\"DDDif\"] = pd.to_numeric(H_DD - A_DD)\n        \n        linha[\"TDOppDif\"] = pd.to_numeric(H_OTD - A_OTD)\n        linha[\"DDOppDif\"] = pd.to_numeric(H_ODD - A_ODD)   \n        \n        df_aux.loc[idx] = linha\n    \n    \n    return df_aux","c4ca4d72":"def dificuldadeVitoria(df):\n    \"\"\"\n    Calcula a dificuldade que os times (Home e Away) tiveram pra vencer seus rivais.\n    Faz isso atrav\u00e9s de uma m\u00e9dia harm\u00f4nica ponderada da margem de vit\u00f3ria e dificuldade do calend\u00e1rio\n    \n    OBS: Ponderada pois uma vit\u00f3ria pequena em um time forte vale mais que uma vit\u00f3ria grande em um time fraco.\n    \"\"\"\n    \n    #Coloca os dois valores entre 0 e 1\n    df[\"H_MOV\"] = (df[\"H_MOV\"] - df[\"H_MOV\"].min()) \/ (df[\"H_MOV\"].max() - df[\"H_MOV\"].min())\n    df[\"A_MOV\"] = (df[\"A_MOV\"] - df[\"A_MOV\"].min()) \/ (df[\"A_MOV\"].max() - df[\"A_MOV\"].min())\n    \n    df[\"H_SOS\"] = (df[\"H_SOS\"] - df[\"H_SOS\"].min()) \/ (df[\"H_SOS\"].max() - df[\"H_SOS\"].min())\n    df[\"A_SOS\"] = (df[\"A_SOS\"] - df[\"A_SOS\"].min()) \/ (df[\"A_SOS\"].max() - df[\"A_SOS\"].min())\n    \n    #Qualidade da vit\u00f3ria (QOV)\n    # Vencer com uma margem grande de times fortes \u00e9 melhor do que vencer por muito de times fracos...\n    # Vou usar m\u00e9dia harm\u00f4nica (ela mitiga o impacto de grandes valores)\n    # Usarei a ponderada, pois acho mais forte um time que ganha de time grande por pouco\n    #                                         do q time que ganha de muito de time fraco\n    df[\"H_QOV\"] = 3 \/ (2\/df[\"H_SOS\"]  +  1\/df[\"H_MOV\"])\n    df[\"A_QOV\"] = 3 \/ (2\/df[\"A_SOS\"]  +  1\/df[\"A_MOV\"])\n    \n    columns_to_drop = [\"H_MOV\", \"A_MOV\", \"H_SOS\", \"A_SOS\"]\n    df.drop(columns_to_drop, axis = 1, inplace = True)\n    \n    return df","1f8f4a80":"def comparaRivais(df):\n    \"\"\"\n    Faz diversas compara\u00e7\u00f5es entre os times (Home e Away) tanto no aspecto ofensivo, quanto no aspecto defensivo.\n    \"\"\"\n    \n    #Rating\n    df[\"SRSDif\"] = (31 - df[\"H_SRS\"]) - (31 - df[\"A_SRS\"])\n    \n    df[\"OrtgDif\"] = (31 - df[\"H_Ortg\"]) - (31 - df[\"A_Ortg\"])\n    df[\"DrtgDif\"] = (31 - df[\"H_Drtg\"]) - (31 - df[\"A_Drtg\"])\n    \n    #Ritmo de Jogo\n    df[\"PaceDif\"] = df[\"H_Pace\"] - df[\"A_Pace\"]\n    \n    #Saldo de vit\u00f3ria\/derrotas\n    df[\"SaldoDif\"] = (df[\"H_Wins\"] - df[\"H_Loss\"]) - (df[\"A_Wins\"] - df[\"A_Loss\"])\n    \n    #Quantidade de jogos\n    df[\"GamesDif\"] = (df[\"H_Games\"] - df[\"A_Games\"])\n    \n    #Quantidade m\u00e9dia de pontos\n    df[\"AvgPointsMadeDif\"] = df[\"H_AvgPointsPerGame\"] - df[\"A_AvgPointsPerGame\"]\n    df[\"AvgPointsOppMadeDif\"] = df[\"H_AvgPointsPerGameOpp\"] - df[\"A_AvgPointsPerGameOpp\"]\n    \n    \n    #Precis\u00e3o do arremesso\n    df[\"FG%Dif\"] = df[\"H_FG%\"] - df[\"A_FG%\"]\n    df[\"FG%OppDif\"] = df[\"H_OFG%\"] - df[\"A_OFG%\"]\n    \n    df[\"FT%Dif\"] = df[\"H_FT%\"] - df[\"A_FT%\"]\n    df[\"FT%OppDif\"] = df[\"H_OFT%\"] - df[\"A_OFT%\"]\n    \n    df[\"3P%Dif\"] = df[\"H_3P%\"] - df[\"A_3P%\"]\n    df[\"3P%OppDif\"] = df[\"H_O3P%\"] - df[\"A_O3P%\"]\n    \n    df[\"2P%Dif\"] = df[\"H_2P%\"] - df[\"A_2P%\"]\n    df[\"2P%OppDif\"] = df[\"H_O2P%\"] - df[\"A_O2P%\"]\n    \n    \n    \n    columns_to_drop = [\"H_SRS\", \"A_SRS\", \"H_Ortg\", \"A_Ortg\", \"H_Drtg\", \"A_Drtg\", \"H_Pace\", \"A_Pace\",\n                       \"H_Wins\", \"H_Loss\", \"A_Wins\", \"A_Loss\", \"H_Games\", \"A_Games\",\n                       \"H_TotalPoints\", \"A_TotalPoints\", \"H_PointsOpp\", \"A_PointsOpp\",\n                     \"H_AvgPointsPerGame\", \"A_AvgPointsPerGame\", \"H_AvgPointsPerGameOpp\", \"A_AvgPointsPerGameOpp\",\n                      \"H_FG%\",\"A_FG%\",\"H_OFG%\",\"A_OFG%\", \"H_FT%\", \"A_FT%\", \"H_OFT%\", \"A_OFT%\",\n                      \"H_3P%\", \"A_3P%\", \"H_O3P%\", \"A_O3P%\", \"H_2P%\", \"A_2P%\", \"H_O2P%\", \"A_O2P%\"]\n    df.drop(columns_to_drop, axis = 1, inplace = True)\n    \n    return df","e4fd820d":"def getRank(df):\n\n    df[\"H_Rank\"] = df[\"H_Team\"].apply(lambda x: dic.get(x))\n    df[\"A_Rank\"] = df[\"A_Team\"].apply(lambda x: dic.get(x))\n    \n    return df","1a0f5109":"def converteTipo(df):\n    \n    colunas = [\"TDDif\", \"DDDif\", \"TDOppDif\", \"DDOppDif\", \"SaldoDif\",\n              \"GamesDif\", \"H_PW\", \"H_PL\", \"A_PW\", \"A_PL\",\n               \"Year\", \"H_DistTravelled\", \"A_DistTravelled\", \"A_Rank\", \"H_Rank\",\n              \"H_Salary\", \"A_Salary\", \"H_Draft\", \"A_Draft\"]\n    \n    for col in colunas:\n        df[col] = pd.to_numeric(df[col])\n        \n    return df","219c07bd":"def getAno(train, test):\n    \n    qnt_anos_train = [89, 79, 86, 85, 82, 81, 84, 85, 89, 81, 86, 79]\n    qnt_anos_test = [82, 83]\n    \n    train_ano = np.repeat(list(range(2006, 2018)), qnt_anos_train)\n    test_ano = np.repeat([2018,2019], qnt_anos_test)\n    \n    train[\"Year\"] = train_ano\n    test[\"Year\"] = test_ano\n\n    return train, test","84e7085c":"train, test = getAno(train, test)\n\ntrain = getData(train)\ntrain = arrumaTime(train)\ntrain = getDivisao(train)\ntrain = getMoreData(train)\ntrain = triploDuplo(train)\ntrain = dificuldadeVitoria(train)\ntrain = comparaRivais(train)\ntrain = converteTipo(train)\n\ntest = getData(test)\ntest = arrumaTime(test)\ntest = getDivisao(test)\ntest = getMoreData(test)\ntest = triploDuplo(test)\ntest = dificuldadeVitoria(test)\ntest = comparaRivais(test)\ntest = converteTipo(test)","9bf60cff":"target = train['WinOrLose'].replace('W', 1).replace('L', 0).astype(int)","48718849":"fig, ax = plt.subplots(ncols=2, figsize=(14,6))\n\nsns.countplot(x=\"H_Team\", data=train, color=\"red\", order=train[\"H_Team\"].unique(), ax=ax[0])\nhteam_win = train.loc[target == 1, \"H_Team\"]\nsns.countplot(x=hteam_win, data=train, color=\"green\", order=train[\"H_Team\"].unique(), ax=ax[0])\n\nsns.countplot(x=\"A_Team\", data=train, color=\"red\", order=train[\"H_Team\"].unique(), ax=ax[1])\nateam_win = train.loc[target == 0, 'A_Team']\nsns.countplot(x=ateam_win, data=train, color=\"green\", order=train[\"H_Team\"].unique(), ax=ax[1])\n\nax[0].set_title(\"Distribui\u00e7\u00e3o de Times da Casa\")\nax[1].set_title(\"Distribui\u00e7\u00e3o de Times Visitantes\")\nplt.setp(ax[0].xaxis.get_majorticklabels(), rotation=90)\nplt.setp(ax[1].xaxis.get_majorticklabels(), rotation=90)\nplt.show()","121d1f0d":"import datetime","30958045":"data = pd.read_csv(\"\/kaggle\/input\/task-03\/train_full.csv\", sep = \",\")['Data ']\ndata = data.replace('June', 'Jun', regex=True).replace('April', 'Apr', regex=True).replace('June', 'Jun', regex=True)\ndata = np.array(list(data.str.split(', ')))[:,1]\ndata = pd.to_datetime(data, format='%b %d').strftime('%d\/%m')","0616cfdb":"fig = plt.figure(figsize=(8,5))\nsns.countplot(x=\"DiaSemana\", data=train, color=\"red\", order=['Sun','Mon','Tue','Wed','Thu','Fri','Sat'])\nhteam_win = train.loc[target == 1, \"DiaSemana\"]\nsns.countplot(x=hteam_win, data=train, color=\"green\", order=['Sun','Mon','Tue','Wed','Thu','Fri','Sat'])\n\nplt.title(\"Dias da semana e Taxas de Vit\u00f3rias pela Casa\")\n\nplt.show()","8d565130":"teams = train['H_Team'].unique()\n\nfig, ax = plt.subplots(ncols = 2, nrows=15, figsize=(20,30))\n\ni = 0\nfor j in range(len(teams)):\n    if(j%2 == 0 and j > 0):\n        i += 1\n    \n    posh = train['H_Team'] == teams[j]\n    posa = train['A_Team'] == teams[j]\n    xh = data[posh]\n    yh = target.loc[posh]\n    \n    xa = data[posa]\n    ya = 1-target.loc[posa]\n    \n    ax[i,j%2].plot(xh, yh, 'o')\n    ax[i,j%2].plot(xa, ya, 'o')\n    plt.setp(ax[i,j%2].xaxis.get_majorticklabels(), rotation=90)\n    ax[i,j%2].xaxis.set_tick_params(labelsize=10)\n    ax[i,j%2].set_ylim(-0.2, 1.2)\n    ax[i,j%2].set_title(teams[j])\n  \nfig.delaxes(ax[14,-1])\nplt.subplots_adjust(hspace=2)","9eb43a7c":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,vmin = -1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","06be3fee":"from sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\nfor col in test.columns.values:\n    if train.loc[:,col].dtype == \"object\":\n        lbl.fit(train.loc[:,col].astype(str))\n        train.loc[:,col] = lbl.transform(train.loc[:,col].astype(str))\n        \n        if col == \"WinOrLose\":\n            pass\n        else:\n            test.loc[:,col] = lbl.transform(test.loc[:,col].astype(str))","c0feb3ab":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score","3007b2ed":"df = train.copy()\n\ncolumns = df.columns.values\ntarget = \"WinOrLose\"\ny_columns = [target]\nx_columns = [x for x in columns if x != target]\n\nX = df[x_columns]\ny = df[y_columns]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX=scaler.fit_transform(X)\n#test = scaler.transform(test)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 30)\nX = pca.fit_transform(X)\n#test = pca.transform(test)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 2)","7c310fed":"%%time\n\nparameters = {'max_depth':np.arange(2, 10, 1), \"n_estimators\":[5], \"learning_rate\":[0.01]}\n\nxgb_model = GridSearchCV(XGBClassifier(), parameters,\n                    cv = 2, scoring = \"balanced_accuracy\", n_jobs = -1, verbose = 3,\n                    refit = True)\n\n#xgb_model.fit(X, y.to_numpy().ravel())\nxgb_model.fit(X_train, y_train.to_numpy().ravel())\n\nprint(\"Melhor modelo: {}\".format(xgb_model.best_estimator_))\nprint(\"Melhor score: {}\".format(xgb_model.best_score_))","69f078d9":"%%time\n\nparameters = {'early_stopping_rounds':[10], \"learning_rate\":[0.01]}\n\ncat_model = GridSearchCV(CatBoostClassifier(silent=True), parameters,\n                    cv = 2, scoring = \"balanced_accuracy\", n_jobs = -1, verbose=3,\n                    refit = True)\n\ncat_model.fit(X_train, y_train.to_numpy().ravel())\n\nprint(\"Melhor modelo: {}\".format(cat_model.best_estimator_))\nprint(\"Melhor score: {}\".format(cat_model.best_score_))","7447ab4f":"from sklearn.model_selection import KFold\n\nfrom scipy.stats import pearsonr\nfrom scipy.stats import spearmanr","4810644a":"target = train['WinOrLose'].replace('W',1).replace('L',0).astype(int)","a173b4a8":"x_columns = df.columns[(df.columns != 'DiaSemana') & (df.columns != 'H_Team') & (df.columns != 'A_Team') & (df.columns != 'H_Division') & (df.columns != 'A_Division') & (df.columns != 'WinOrLose')]\n\ndf = train.loc[:,x_columns].copy()\ny = target","d9c6ee72":"names = df.columns\npearson = []\nspearman = []\n\nfor col in names:\n    X = df.loc[:, col].to_numpy()\n    pearson.append(abs(pearsonr(X,y)[0]))\n    spearman.append(abs(spearmanr(X,y)[0]))\n    \npearson = np.array(pearson)\nspearman = np.array(spearman)\n\nfeature_scores = pd.DataFrame({\"Column\":names, \"Pearson\":pearson, \"Spearman\":spearman})\n\nfeature_scores.sort_values('Spearman', ascending=False, inplace=True)\n\nbest_features = list(feature_scores['Column'][:50])\nfeature_scores = feature_scores.iloc[:50].reset_index(drop=True).copy()\n\nprint(\"15 Features com o melhor score total\")\nfeature_scores.head(15)","1044186e":"names = list(feature_scores['Column'])\n\nvotos = np.zeros(len(names))\ntau = 0.85\n\nfor i in range(0, len(names)-1):\n    for j in range(i+1, len(names)):\n        p = pearsonr(train.loc[:,names[i]], train.loc[:,names[j]])[0] \n        if(abs(p) > tau):\n            if(feature_scores.loc[i,'Spearman'] >= feature_scores.loc[j,'Spearman']):\n                votos[j] += 1\n            else:\n                votos[i] += 1","7a12ebd0":"votos","67d3f7c0":"# Retiramos as colunas votadas ao menos uma vez...\nbest_features = list(feature_scores[votos == 0]['Column'])\nprint(\"Total de {} features\".format(len(best_features)))\nprint(best_features)","b67e2090":"df = train.copy()\n\nx_columns = ['DiaSemana','H_Team','A_Team','H_Division','A_Division'] + best_features\n\nX = df[x_columns]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 2)\nX_train = X_train.copy()\nX_test = X_test.copy()","42b5acda":"print(X_train.shape)\nprint(X_test.shape)","6eb1b550":"params = {'max_depth':np.arange(5, 30, 1),\n          'criterion': ['gini', 'entropy']}\n\ndt_model = GridSearchCV(DecisionTreeClassifier(), params,\n                        scoring = \"accuracy\", n_jobs = -1, cv=10, verbose=4, refit=True)\ndt_model.fit(X_train, y_train)\n\ny_pred_train = dt_model.predict(X_train)\ny_pred_test = dt_model.predict(X_test)","5ba31dd3":"print(\"Melhor modelo: {}\".format(dt_model.best_estimator_.get_params()))\nprint(\"Melhor score: {}\".format(dt_model.best_score_))","5ad271c7":"print(\"Acur\u00e1cia Treino:\", accuracy_score(y_train, y_pred_train))\nprint(\"Acur\u00e1cia Teste:\", accuracy_score(y_test, y_pred_test))","dbd02cd3":"# params = {'n_estimators':[200, 500, 700, 800],\n#           'max_depth':np.arange(5, 10, 1)}\n\n# rf_model = GridSearchCV(RandomForestClassifier(), params,\n#                         scoring = \"accuracy\", n_jobs = -1, cv=10, verbose=4, refit=True)\n\nrf_model = RandomForestClassifier(n_estimators=800, max_depth=5) # Melhor modelo encontrado em uma itera\u00e7\u00e3o de GridSeachCV com params\nrf_model.fit(X_train, y_train)\n\ny_pred_train = rf_model.predict(X_train)\ny_pred_test = rf_model.predict(X_test)","d58e751c":"print(\"Acur\u00e1cia Treino:\", accuracy_score(y_train, y_pred_train))\nprint(\"Acur\u00e1cia Teste:\", accuracy_score(y_test, y_pred_test))","1f98872f":"# params = {\"learning_rate\":[0.01, 0.05, 0.1, 0.03],\n#           \"n_estimators\":[500, 700, 800]}\n\n# cat_model = GridSearchCV(CatBoostClassifier(verbose=False), params,\n#                     cv = 10, scoring = \"accuracy\", n_jobs=-1, verbose = 2,\n#                     refit = True)\n\ncat_model = CatBoostClassifier(learning_rate=0.01, n_estimators = 500, silent=True) # Melhor modelo encontrado em uma itera\u00e7\u00e3o de GridSeachCV com params\ncat_model.fit(X_train, y_train)\n\ny_pred_train = cat_model.predict(X_train)\ny_pred_test = cat_model.predict(X_test)","1284fc64":"print(\"Acur\u00e1cia Treino:\", accuracy_score(y_train, y_pred_train))\nprint(\"Acur\u00e1cia Teste:\", accuracy_score(y_test, y_pred_test))","1705d001":"# params = {'n_estimators': [200, 500], 'max_depth': [2, 3], 'learning_rate': [0.01]}\n\n# xgb_model = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), params,\n#                     cv = 10, scoring = \"accuracy\", n_jobs = -1, verbose = 2,\n#                     refit = True)\n\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, max_depth=2, learning_rate=0.01) # Melhor modelo encontrado em uma itera\u00e7\u00e3o de GridSeachCV com params\nxgb_model.fit(X_train, y_train)\n\n# Devido a vers\u00e3o do XGBoost no kaggle ele fica enchendo o saco com coisa boba\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\ny_pred_train = xgb_model.predict(X_train)\ny_pred_test = xgb_model.predict(X_test)","ebf9413d":"print(\"Acur\u00e1cia Treino:\", accuracy_score(y_train, y_pred_train))\nprint(\"Acur\u00e1cia Teste:\", accuracy_score(y_test, y_pred_test))","871190fa":"import time\n\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import KFold","149c02c8":"target = train['WinOrLose'].replace('W', 1).replace('L', 0).astype(int)\n\ndf = pd.concat([X, target], axis=1).copy()\ndf.head(5)","6128fab1":"k = 10 # Na pr\u00e1tica optamos por valores entre 750 e 1250...\n\nmodels = []\nweights = []\n\ntotal_start_time = time.time()\nfor i in range(k):\n    print(\"Sample \",i+1,\"... \", sep=\"\", end=\"\")\n    \n    sample = resample(df, replace=True, n_samples=df.shape[0], stratify=df['WinOrLose'])\n    y = sample['WinOrLose'].to_numpy()\n    X = sample.drop(columns = 'WinOrLose')\n    \n    kf = KFold(n_splits = 10, shuffle = True)\n    kf.get_n_splits(X)\n    \n    model_dt = DecisionTreeClassifier(criterion='gini', max_depth=19)\n    model_rf = RandomForestClassifier(n_estimators=800, max_depth=5)\n    model_cat = CatBoostClassifier(learning_rate=0.01, n_estimators = 500, silent=True)\n    model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, max_depth=2, learning_rate=0.01)\n    \n    acc_kfold_dt = []\n    acc_kfold_rf = []\n    acc_kfold_cat = []\n    acc_kfold_xgb = []\n    \n    start_time = time.time()\n    for train_index, test_index in kf.split(X_train):\n        model_dt.fit(X.iloc[train_index], y[train_index])\n        model_rf.fit(X.iloc[train_index], y[train_index])\n        model_cat.fit(X.iloc[train_index], y[train_index])\n        model_xgb.fit(X.iloc[train_index], y[train_index])\n        acc_kfold_dt.append( accuracy_score(y[test_index], model_dt.predict(X.iloc[test_index])) )\n        acc_kfold_rf.append( accuracy_score(y[test_index], model_rf.predict(X.iloc[test_index])) )\n        acc_kfold_cat.append( accuracy_score(y[test_index], model_cat.predict(X.iloc[test_index])) )\n        acc_kfold_xgb.append( accuracy_score(y[test_index], model_xgb.predict(X.iloc[test_index])) )\n    \n    candidate_models = [model_dt, model_rf, model_cat, model_xgb]\n    candidate_weights = [np.mean(acc_kfold_dt), np.mean(acc_kfold_rf), np.mean(acc_kfold_cat), np.mean(acc_kfold_xgb)]\n\n    # candidate_models = [model_dt, model_cat]\n    # candidate_weights = [np.mean(acc_kfold_dt), np.mean(acc_kfold_cat)]\n\n    i_max = np.argmax(candidate_weights)\n    \n    weights.append(candidate_weights[i_max])\n    \n    candidate_models[i_max].fit(X, y)\n    \n    elapsed_time = time.time() - start_time\n    \n    print(candidate_models[i_max], end=\" \")\n    print(elapsed_time, 's')\n    \n    models.append(candidate_models[i_max])\n    \ntotal_elapsed_time = time.time() - total_start_time\nprint(\"Total time:\", total_elapsed_time)\nprint(\"Average time per model:\", total_elapsed_time\/k)","45bf9df0":"test = test.loc[:, df.columns[:-1]].copy()\n\nprob_final = np.zeros(test.shape[0])\n\nfor i in range(len(models)):\n    prob_final += models[i].predict_proba(test)[:,1] * weights[i] # Probabilidade de 'W'\nprob_final \/= np.sum(weights)\nprob_final","0eb52c8b":"y_pred_final = np.zeros(len(prob_final)).astype(str)\ny_pred_final[np.where(prob_final >= 0.5)] = 'W'\ny_pred_final[np.where(prob_final < 0.5)] = 'L'\ny_pred_final","e37946b5":"print(\"Propor\u00e7\u00e3o de vit\u00f3rias preditas:\", (y_pred_final == 'W').sum() \/ len(y_pred_final)) # Pr\u00f3ximo de 35%... Parece bom","5fbf13e0":"## CatBoost","653218ad":"## Decision Tree","7b6bcca5":"# Modelagem (com feature selection)","b4707ff9":"## Random Forest","3f0b9471":"## CatBoost","a535e492":"# An\u00e1lise Explorat\u00f3ria","1de12220":"## XGBoost","2a63e362":"## XGBoost","5aa5d91d":"Neste caso, o modelo que apresentou melhor score em todas as 10 itera\u00e7\u00f5es foi o modelo DecisionTree... Os mais escolhidos na pr\u00e1tica foram DecisionTree e CatBoost...","ff0d4b99":"# Feature Selection","a33f04bb":"# Bootstrap","f0c0b519":"# Modelagem (SEM FEATURE SELECTION)","30bef2d1":"# Pr\u00e9 processamento"}}