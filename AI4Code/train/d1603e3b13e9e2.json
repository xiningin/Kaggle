{"cell_type":{"436c5664":"code","be166907":"code","3b5cda31":"code","e5364957":"code","3bd0d4c9":"code","4d6016e7":"code","4477688d":"code","59523f2b":"code","48401a6e":"code","2391922d":"code","e70b2ba0":"code","73c3d74b":"code","aeb0e7e1":"code","b14ea4d5":"code","7b2b756e":"code","371f3333":"code","d57b6de4":"code","8381568e":"code","2c1872b7":"code","acc30448":"code","bb0fcfa8":"code","e61b3d74":"code","85980b1d":"code","87d32bf3":"code","9a11df1a":"code","08533787":"code","be1c1e06":"code","a2b4530f":"code","fb05f3e7":"markdown","fe7f94b3":"markdown","69576d55":"markdown","20703d5a":"markdown","d88d235f":"markdown","d8c88b7b":"markdown","dffd63ee":"markdown","ecfc1649":"markdown","abc1b8a0":"markdown","4134a8d2":"markdown","b393fbbf":"markdown","b09b684e":"markdown","1766961f":"markdown","6da9ec09":"markdown","8fb5a35b":"markdown","d084dde5":"markdown","c2e56021":"markdown","202a4c46":"markdown","ccf08ab8":"markdown","33380f87":"markdown","5bb08ac7":"markdown","04fc67fe":"markdown","48a975c7":"markdown","9d71bbfa":"markdown","870b4abf":"markdown","97336446":"markdown","1794820b":"markdown","f6552c31":"markdown","23f335c7":"markdown","a6087232":"markdown","443a63bb":"markdown","847186bc":"markdown","0d72b157":"markdown","83fe98cb":"markdown","25ebd301":"markdown","f311f866":"markdown","12e116b4":"markdown","c8cc1dab":"markdown","bc565fef":"markdown","3c68979a":"markdown","f9d755a8":"markdown","7ca664c1":"markdown","8ed82212":"markdown","8497026e":"markdown","c5a7eedb":"markdown","36554c0f":"markdown","0052528d":"markdown","8b6514b7":"markdown","5dd135d7":"markdown","861729a5":"markdown","765cf69c":"markdown","1ca2f88b":"markdown"},"source":{"436c5664":"import os\nimport pandas as pd\n\n\nFOLDER_PATH = os.getcwd()\nFILE_NAME = r\"\/\/Traffic_Volume_Counts__2014-2019_.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\n\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/Traffic_Volume_Counts__2014-2019_.csv\" # Only for Kaggle\n\ntraffic_orig = pd.read_csv(FILE_PATH)\n\ntraffic = traffic_orig.copy()\n\ntraffic[\"Segment ID\"] = traffic[\"Segment ID\"].astype(int)\ntraffic[\"Is_Weekend\"] =  pd.to_datetime(traffic[\"Date\"]).dt.day_name().isin(['Saturday', 'Sunday'])\ntraffic[\"Season\"] =  pd.to_datetime(traffic[\"Date\"]).dt.month % 12 \/\/ 3 + 1\n\ntraffic_hour_column_names = ['12:00-1:00 AM', '1:00-2:00AM', '2:00-3:00AM', '3:00-4:00AM',\n       '4:00-5:00AM', '5:00-6:00AM', '6:00-7:00AM', '7:00-8:00AM',\n       '8:00-9:00AM', '9:00-10:00AM', '10:00-11:00AM', '11:00-12:00PM',\n       '12:00-1:00PM', '1:00-2:00PM', '2:00-3:00PM', '3:00-4:00PM',\n       '4:00-5:00PM', '5:00-6:00PM', '6:00-7:00PM', '7:00-8:00PM',\n       '8:00-9:00PM', '9:00-10:00PM', '10:00-11:00PM', '11:00-12:00AM']\n\n# renaming traffic hours\ntraffic = traffic.rename(columns=dict(zip(traffic_hour_column_names, list(range(24)))))\n\ncols = ['ID', 'Segment ID', 'Roadway Name', 'Date', 'Is_Weekend', 'Season']\n\ntraffic_v1 = traffic.melt(\n    id_vars = cols,\n    value_vars = list(range(24)),\n    var_name = 'Hour',\n    value_name = 'Traffic_Volume'\n)\n\n# the main purpose of this is because it seems like the data-table has split the road across 2 rows in terms of direction. \n# But we don't really care if the traffic in 1 side is different from the traffic on the other side heading the opposite \n# direction. To us, it's the same road, so we combine the traffic on either sides together.\ntraffic_v2 = traffic_v1.groupby(cols + [\"Hour\"], as_index=False)[\"Traffic_Volume\"].sum()\n\n# Finally, the hours were binned together into groups of 3; therefore, a single 24-hour day is now just 8 chunks.\n# these 3-hour subgroups were aggregated by mean traffic volume, rounded to 2 decimal places\ncols = [\"Segment ID\", \"Is_Weekend\", \"Season\"]\n\ntraffic_v2[\"3_Hr_Intvl\"] = ((traffic_v2[\"Hour\"] \/\/ 3) + 1)*3\ntraffic_v3 = traffic_v2.groupby(cols + [\"3_Hr_Intvl\"], as_index=False)[\"Traffic_Volume\"].mean().round(2)\n\nFILE_NAME = \"\\\\nyc_traffic_3hrInterval.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\nFILE_PATH = FILE_NAME\n\ntraffic_v3.to_csv(FILE_PATH)","be166907":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\n\ndef make_OHE(orig_df,df_column_name):\n    ohe_df = pd.get_dummies(orig_df[df_column_name].astype(str))\n    return ohe_df\n\ndef standardize_OHE(orig_df, df_column_names):\n    row_wise_sum = (orig_df.loc[:,df_column_names]).sum(axis=1)\n    orig_df.loc[:,df_column_names] = (orig_df.loc[:,df_column_names]).div(row_wise_sum, axis=0)\n    return orig_df.loc[:,df_column_names]","3b5cda31":"FOLDER_PATH = os.getcwd()\nFILE_NAME = \"\\\\gdb_join.txt\"\nFILE_PATH = FOLDER_PATH + \"\\\\\" + FILE_NAME\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/gdb_join.txt\" # Only for Kaggle\n\ngdb_join_orig = pd.read_csv(FILE_PATH)\ngdb_join = gdb_join_orig.copy()\n\n# FOR EDA\ngdb_join_EDA = gdb_join.copy()\n\ngdb_join_EDA[\"Is_Weekend\"] = gdb_join[\"Is_Weekend\"]\ngdb_join_EDA[\"Season\"] = gdb_join[\"Season\"]\ngdb_join_EDA[\"Segment_ID\"] = gdb_join[\"Segment_ID\"]\ngdb_join_EDA[\"3_Hr_Intvl\"] = gdb_join[\"F3_Hr_Intvl\"]\ngdb_join_EDA[\"Traffic_Volume\"] = gdb_join[\"Traffic_Volume\"] \n\nSEASON_NAMES = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n\ngdb_join_EDA[\"Season\"] = gdb_join_EDA[\"Season\"].apply(\n    lambda x: SEASON_NAMES[x-1])\n\nConcat_inputs = [\n    gdb_join_EDA[[\"Segment_ID\"]],\n    gdb_join_EDA[[\"Is_Weekend\"]],\n    gdb_join_EDA[[\"3_Hr_Intvl\"]],\n    gdb_join_EDA[[\"Season\"]],\n    gdb_join_EDA[[\"Traffic_Volume\"]],\n]\ngdb_join_EDA = pd.concat(Concat_inputs, axis=1)\n\nFILE_NAME = \"\\\\gdb_join_EDA.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\ngdb_join_EDA.to_csv(FILE_PATH, index=False)\n\n# FOR ML\n\ngdb_join_ml = gdb_join.copy()\n\ndef day_night_transform(x):\n    # flattens the range 0 to 24 to be between 0 and 1 as a bellcurve\n    return 2 * (1 + np.exp(((x - 12)\/4)**2))**(-1)\n\ngdb_join_ml[\"Is_Weekend\"] = gdb_join[\"Is_Weekend\"].astype(int)\n\ngdb_join_ml[\"Season\"] = \"Season\"+gdb_join[\"Season\"].astype(int).astype(str)\ngj_season_OHE = make_OHE(gdb_join_ml, \"Season\")\n\ngdb_join_ml[\"Daylight\"] = gdb_join[\"F3_Hr_Intvl\"].apply(day_night_transform)\n\ngdb_join_ml[\"F3_Hr_Intvl\"] = \"Hr\"+gdb_join[\"F3_Hr_Intvl\"].astype(int).astype(str)\ngj_hour_OHE = make_OHE(gdb_join_ml, \"F3_Hr_Intvl\")\ngj_hour_OHE = gj_hour_OHE[[\"Hr\"+str(i) for i in range(3,27,3)]]\n\n\n\nConcat_inputs = [\n    gdb_join_ml[[\"Segment_ID\"]],\n    gdb_join_ml[[\"Is_Weekend\"]],\n    gdb_join_ml[[\"Daylight\"]],\n    gj_season_OHE,\n    gj_hour_OHE,\n    gdb_join_ml[[\"Traffic_Volume\"]],\n]\n\ngdb_join_ml = pd.concat(Concat_inputs, axis=1)\n\n\nFILE_NAME = \"\\\\gdb_join_ml.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\ngdb_join_ml.to_csv(FILE_PATH, index=False)","e5364957":"FILE_NAME = \"\\\\StreetSegment_LandUse_Subsets.txt\"\nFILE_PATH = FOLDER_PATH + \"\\\\\" + FILE_NAME\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/StreetSegment_LandUse_Subsets.txt\" # Only for Kaggle\n\nStreetSegment_LandUse_Subsets_orig = pd.read_csv(FILE_PATH)\n\nFILE_NAME = \"\\\\Subway_Distances.txt\"\nFILE_PATH = FOLDER_PATH + \"\\\\\" + FILE_NAME\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/Subway_Distances.txt\" # Only for Kaggle\n\nSubway_Distances_orig = pd.read_csv(FILE_PATH)\n\nStreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets_orig.copy()\nSubway_Distances = Subway_Distances_orig.copy()\n\nSubway_Distances[\"SubwayProximity\"] = 1\/Subway_Distances[\"NEAR_DIST\"]\n\nStreetSegment_Subway_Distances = Subway_Distances.groupby([\"IN_FID\"], as_index=False)[\"SubwayProximity\"].sum()\n\nStreetSegment_Subway_Distances = StreetSegment_Subway_Distances.rename(columns={\"IN_FID\": \"ORIG_FID\"})\n\nStreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets.merge(StreetSegment_Subway_Distances, on=\"ORIG_FID\")","3bd0d4c9":"## ok\n\nSELECTED_SL_COLUMNS_NUMERICAL = []\nSELECTED_SL_COLUMNS_CATEGORICAL = []\nSELECTED_SL_COLUMNS_OTHERS = []\n\nSELECTED_SL_COLUMNS_NUMERICAL += ['lion_StreetWidt']\nSELECTED_SL_COLUMNS_NUMERICAL += ['NumFloors']\nSELECTED_SL_COLUMNS_NUMERICAL += ['UnitsRes', 'UnitsTotal']\nSELECTED_SL_COLUMNS_NUMERICAL += ['SubwayProximity']\n\n\nSELECTED_SL_COLUMNS_CATEGORICAL += ['BoroCode']\nSELECTED_SL_COLUMNS_CATEGORICAL += ['LandUse']\n\n\nSELECTED_SL_COLUMNS_OTHERS += ['ORIG_FID']\nSELECTED_SL_COLUMNS_OTHERS += ['lion_Segment_ID']\n\nSELECTED_SL_COLUMNS = (SELECTED_SL_COLUMNS_NUMERICAL + \n                       SELECTED_SL_COLUMNS_CATEGORICAL +\n                       SELECTED_SL_COLUMNS_OTHERS)\n\nStreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets[SELECTED_SL_COLUMNS]\n\nStreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets.replace(\" \", np.nan)\nStreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets.replace(\"\", np.nan)\nStreetSegment_LandUse_Subsets=StreetSegment_LandUse_Subsets.dropna()\n\nStreetSegment_LandUse_Subsets[\"LandUse\"].isnull().any()\n\nOVERLAP_THRESHOLD = 0.5\n\nLANDUSE_COLUMN_VALUES = []\n\nLANDUSE_COLUMN_VALUES += [1*(\n    StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"01\", \"02\", \"03\"])\n + OVERLAP_THRESHOLD*(StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"04\"])))]\nLANDUSE_COLUMN_VALUES += [1*(\n    StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"05\"])\n + OVERLAP_THRESHOLD*(StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"04\"])))]\nLANDUSE_COLUMN_VALUES += [1*(StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"06\", \"07\", \"10\"]))]\nLANDUSE_COLUMN_VALUES += [1*(StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"08\"]))]\nLANDUSE_COLUMN_VALUES += [1*(StreetSegment_LandUse_Subsets[\"LandUse\"].isin([\"09\", \"11\"]))]\nLANDUSE_COLUMNS = [\"LandUse_t{0}\".format(i+1) for i in range(len(LANDUSE_COLUMN_VALUES))]\n\nfor i in range(len(LANDUSE_COLUMNS)):\n    column_name = LANDUSE_COLUMNS[i]\n    column = LANDUSE_COLUMN_VALUES[i]\n    StreetSegment_LandUse_Subsets[column_name] = column \n\nSSLU_LandUse = StreetSegment_LandUse_Subsets.loc[:,LANDUSE_COLUMNS]\n\nStreetSegment_LandUse_Subsets[\"BoroCode\"] = \"BoroCode\" + StreetSegment_LandUse_Subsets[\"BoroCode\"].astype(str)\nSSLU_Borough_OHE = make_OHE(StreetSegment_LandUse_Subsets, \"BoroCode\")\n\nStreetSegment_LandUse_Subsets[\"StreetWidth\"] = StreetSegment_LandUse_Subsets[\"lion_StreetWidt\"]\n\nStreetSegment_LandUse_Subsets[\"StreetWidth_z\"] = (\n    (StreetSegment_LandUse_Subsets[\"StreetWidth\"] - StreetSegment_LandUse_Subsets[\"StreetWidth\"].mean())\/\n    StreetSegment_LandUse_Subsets[\"StreetWidth\"].std()\n)\n\nStreetSegment_LandUse_Subsets[\"Segment_ID\"] = StreetSegment_LandUse_Subsets[\"lion_Segment_ID\"]\n\nStreetSegment_LandUse_Subsets[\"NumFloors\"] = StreetSegment_LandUse_Subsets[\"NumFloors\"]\n\nStreetSegment_LandUse_Subsets[\"UnitsRes_Prop\"] = (\n    StreetSegment_LandUse_Subsets[\"UnitsRes\"]\/StreetSegment_LandUse_Subsets[\"UnitsTotal\"]\n)\n\nConcat_inputs = [\n    StreetSegment_LandUse_Subsets[\"StreetWidth\"],\n    StreetSegment_LandUse_Subsets[\"StreetWidth_z\"],\n    StreetSegment_LandUse_Subsets[\"NumFloors\"],\n    StreetSegment_LandUse_Subsets[\"UnitsRes_Prop\"],\n    StreetSegment_LandUse_Subsets[\"SubwayProximity\"],\n    SSLU_LandUse,\n    SSLU_Borough_OHE,\n    StreetSegment_LandUse_Subsets[\"ORIG_FID\"],\n    StreetSegment_LandUse_Subsets[\"Segment_ID\"]\n]\n\nStreetSegment_LandUse_Subsets_Grouping = pd.concat(Concat_inputs, axis=1)","4d6016e7":"StreetSegment_LandUse_Subsets_Aggregation = dict()\n\nStreetSegment_LandUse_Subsets_Aggregation.update(\n    {\n    \"NumFloors\": np.mean,\n    \"StreetWidth\": np.mean,\n    \"StreetWidth_z\": np.mean,\n    \"UnitsRes_Prop\": np.mean,\n    \"SubwayProximity\": np.mean,\n    }\n)\n\nStreetSegment_LandUse_Subsets_Aggregation.update(\n    dict([(col, np.sum) for col in SSLU_Borough_OHE.columns.values]))\n\nStreetSegment_LandUse_Subsets_Aggregation.update(\n    dict([(col, np.sum) for col in SSLU_LandUse.columns.values]))\n\nUnique_StreetSegment_LandUse_Subsets = StreetSegment_LandUse_Subsets_Grouping.groupby([\"Segment_ID\"], as_index=False).agg(\n    StreetSegment_LandUse_Subsets_Aggregation)\n\nOHE_columns = []\nOHE_columns += list(SSLU_Borough_OHE.columns.values)\nOHE_columns += LANDUSE_COLUMNS\nnon_OHE_columns = (\n    set(Unique_StreetSegment_LandUse_Subsets.columns.values) - set(OHE_columns))\n\nConcat_inputs = [\n    Unique_StreetSegment_LandUse_Subsets[list(non_OHE_columns)],\n    standardize_OHE(Unique_StreetSegment_LandUse_Subsets, list(SSLU_Borough_OHE.columns.values)),\n    standardize_OHE(Unique_StreetSegment_LandUse_Subsets, LANDUSE_COLUMNS) # added 2021\/12\/10\n]\n\nUnique_StreetSegment_LandUse_Subsets = pd.concat(Concat_inputs, axis=1)\n\nUnique_StreetSegment_LandUse_Subsets.isin([np.inf, -np.inf]).any().any()\nUnique_StreetSegment_LandUse_Subsets = Unique_StreetSegment_LandUse_Subsets.fillna(0)","4477688d":"# FOR EDA\n\nUnique_StreetSegment_LandUse_Subsets_EDA = Unique_StreetSegment_LandUse_Subsets.copy()\n\nBOROCODE_NAMES = [\"Manhattan\", \"Bronx\", \"Brooklyn\", \"Queens\", \"Staten Island\"]\n\nUnique_StreetSegment_LandUse_Subsets_EDA[\"BoroCode\"] = np.argmax(Unique_StreetSegment_LandUse_Subsets_EDA[[\"BoroCode1\", \"BoroCode2\", \"BoroCode3\", \"BoroCode4\", \"BoroCode5\"]].to_numpy(), axis=1)+1\nUnique_StreetSegment_LandUse_Subsets_EDA = Unique_StreetSegment_LandUse_Subsets_EDA.drop(\n    columns = [\"BoroCode{0}\".format(i+1) for i in range(len(pd.unique(Unique_StreetSegment_LandUse_Subsets_EDA[\"BoroCode\"])))]\n                                                                                        )\n\nUnique_StreetSegment_LandUse_Subsets_EDA[\"BoroCode\"] = Unique_StreetSegment_LandUse_Subsets_EDA[\"BoroCode\"].apply(\n    lambda x: BOROCODE_NAMES[x - 1])\n\nFILE_NAME = \"\\\\Unique_StreetSegment_LandUse_Subsets_EDA.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\nUnique_StreetSegment_LandUse_Subsets_EDA.to_csv(FILE_PATH, index=False)\n\n# FOR ML\n\nUnique_StreetSegment_LandUse_Subsets_ml = Unique_StreetSegment_LandUse_Subsets.copy()\n\nUnique_StreetSegment_LandUse_Subsets_ml = Unique_StreetSegment_LandUse_Subsets_ml.drop(columns=[\"StreetWidth\"])\nUnique_StreetSegment_LandUse_Subsets_ml = Unique_StreetSegment_LandUse_Subsets_ml.rename(\n    columns={\"StreetWidth_z\": \"StreetWidth\"})\n\nConcat_inputs = [\n    Unique_StreetSegment_LandUse_Subsets_ml,\n]\nUnique_StreetSegment_LandUse_Subsets_ml = pd.concat(Concat_inputs, axis=1)\n\nFILE_NAME = \"\\\\Unique_StreetSegment_LandUse_Subsets_ml.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\nUnique_StreetSegment_LandUse_Subsets_ml.to_csv(FILE_PATH, index=False)","59523f2b":"FOLDER_PATH = os.getcwd()\nFILE_NAME = \"\\\\nyc_traffic_EDA_orig.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/nyc_traffic_EDA_orig.csv\" # Only for Kaggle\n\nnyc_traffic_EDA_orig = pd.read_csv(FILE_PATH)\nnyc_traffic_EDA = nyc_traffic_EDA_orig.copy()","48401a6e":"LANDUSE_COLUMNS = [col for col in nyc_traffic_EDA.columns if col[:9] == \"LandUse_t\"]\n\nLANDUSE_TYPES = []\nLANDUSE_TYPES += [\"Residential\"]\nLANDUSE_TYPES += [\"Business\"]\nLANDUSE_TYPES += [\"Infrastructure\"]\nLANDUSE_TYPES += [\"Public Institutions\"]\nLANDUSE_TYPES += [\"Outside\"]\n\nLANDUSE_LOOKUP = dict(zip(LANDUSE_COLUMNS, LANDUSE_TYPES))","2391922d":"fig, axs = plt.subplots(len(LANDUSE_COLUMNS), sharex=False,figsize=(8,16))\nfig.suptitle('Borough Distribution of LandUses, as seen in Dataset')\nfor i in range(len(LANDUSE_COLUMNS)):\n    LANDUSE_COLUMN = LANDUSE_COLUMNS[i]\n    LANDUSE_TYPE = LANDUSE_TYPES[i]\n    mean_landuse_prop_by_borough = nyc_traffic_EDA.groupby([\"BoroCode\"])[LANDUSE_COLUMN].mean()\n    axs[i].bar(mean_landuse_prop_by_borough.index,mean_landuse_prop_by_borough.values)\n    axs[i].set_ylabel(LANDUSE_TYPE, rotation=0, labelpad=50)","e70b2ba0":"colors = [\"cyan\", \"purple\", \"blue\", \"magenta\", \"black\"]\n\nnyc_traffic_EDA_weekdays = nyc_traffic_EDA\nfig, ax = plt.subplots()\nax.set_xticks(range(3,27,3))\ndf_grouped = nyc_traffic_EDA_weekdays.groupby([\"BoroCode\"])\nfor i in range(len(df_grouped)):\n    key, group = list(df_grouped)[i]\n    group.groupby('3_Hr_Intvl')['Traffic_Volume'].mean().plot(ax=ax,label=str(key),c=colors[i])\nax.legend(bbox_to_anchor=(1.1, 1.05))\nax.set_title(\"Boroughs and Traffic Volume during Time of Day\")","73c3d74b":"colors = [\"orange\", \"purple\", \"cyan\", \"red\", \"green\"]\n\nnyc_traffic_EDA_weekdays = nyc_traffic_EDA#[nyc_traffic_EDA[\"Is_Weekend\"]==False]\nfig, ax = plt.subplots()\nax.set_xticks(range(0,27,3))\ndf_grouped = nyc_traffic_EDA_weekdays.groupby([\"Is_Weekend\"])\nfor i in range(len(df_grouped)):\n    key, group = list(df_grouped)[i]\n    group.groupby('3_Hr_Intvl')['Traffic_Volume'].mean().plot(ax=ax,label=str(key),c=colors[i])\nax.legend(bbox_to_anchor=(1.1, 1.05))\nax.set_title(\"Is_Weekend and Traffic Volume during Time of Day\")","aeb0e7e1":"hour_weekend_traffic = traffic_v2[traffic_v2[\"Is_Weekend\"]==True].groupby([\"Hour\"])[\"Traffic_Volume\"].mean()\nhour_weekend_traffic.plot(\n    kind='line', \n    title='Average Traffic Volume by Hour', \n    ylabel='Average Traffic Volume',\n    xlabel='Hour', figsize=(6, 5))","b14ea4d5":"nyc_traffic_EDA[\"Predominant_LandUse\"] = nyc_traffic_EDA.loc[:,LANDUSE_COLUMNS].idxmax(axis=1)\n\ncolors = [\"orange\", \"purple\", \"cyan\", \"red\", \"green\"]\n\nnyc_traffic_EDA_weekdays = nyc_traffic_EDA#[nyc_traffic_EDA[\"Is_Weekend\"]==False]\nfig, ax = plt.subplots()\nax.set_xticks(range(0,27,3))\ndf_grouped = nyc_traffic_EDA_weekdays.groupby([\"Predominant_LandUse\"])\nfor i in range(len(df_grouped)):\n    key, group = list(df_grouped)[i]\n    group.groupby('3_Hr_Intvl')['Traffic_Volume'].mean().plot(ax=ax,label=str(key)+\" - \"+LANDUSE_LOOKUP[key],c=colors[i])\nax.legend(bbox_to_anchor=(1.1, 1.05))\nax.set_title(\"LandUse and Traffic Volume during Time of Day\")","7b2b756e":"NUMERICAL_COLUMN_1_NAME = \"SubwayProximity\"\nNUMERICAL_COLUMN_2_NAME = \"Traffic_Volume\"\nNUMERICAL_COLUMN_3_NAME = \"NumFloors\"\nTABLE = nyc_traffic_EDA.copy()\nTITLE = \"{0} by {1}, \\n 3rd Variable [Color]: {2}\".format(NUMERICAL_COLUMN_1_NAME, NUMERICAL_COLUMN_2_NAME, NUMERICAL_COLUMN_3_NAME)\nX_LABEL = NUMERICAL_COLUMN_1_NAME\nY_LABEL = NUMERICAL_COLUMN_2_NAME\nNUMERICAL_COLUMN_1 = TABLE[NUMERICAL_COLUMN_1_NAME]\nNUMERICAL_COLUMN_2 = TABLE[NUMERICAL_COLUMN_2_NAME]\n# color column\nNUMERICAL_COLUMN_3 = TABLE[NUMERICAL_COLUMN_3_NAME]\n\n\nLEVEL = 2\nNUMERICAL_COLUMN_3 = NUMERICAL_COLUMN_3 #exagerate\nNUMERICAL_COLUMN_3 = (NUMERICAL_COLUMN_3).round(LEVEL)\nSCALING = 10**LEVEL\ncolor_ranger = np.arange(NUMERICAL_COLUMN_3.min()*SCALING, NUMERICAL_COLUMN_3.max()*SCALING)\ncolors = plt.cm.cool(np.linspace(0, 1, len(color_ranger)))\n#colors = plt.cm.cool(color_ranger)\n\nCOLUMN_3_COLORS = NUMERICAL_COLUMN_3.apply(lambda x: colors[int(x*100)-1])\n\nfig, ax = plt.subplots()\nax.scatter(NUMERICAL_COLUMN_1, NUMERICAL_COLUMN_2,s=2, alpha=0.1, c=COLUMN_3_COLORS)\nax.set_title(TITLE)\nax.set_xlabel(X_LABEL)\nax.set_ylabel(Y_LABEL)","371f3333":"NUMERICAL_COLUMN_1_NAME = \"SubwayProximity\"\nNUMERICAL_COLUMN_2_NAME = \"NumFloors\"\nNUMERICAL_COLUMN_3_NAME = \"Traffic_Volume\"\nTABLE = nyc_traffic_EDA.copy()\nTITLE = \"{0} by {1}, \\n 3rd Variable [Color]: {2}\".format(NUMERICAL_COLUMN_1_NAME, NUMERICAL_COLUMN_2_NAME, NUMERICAL_COLUMN_3_NAME)\nX_LABEL = NUMERICAL_COLUMN_1_NAME\nY_LABEL = NUMERICAL_COLUMN_2_NAME\n\n#TABLE = TABLE[(TABLE[\"BoroCode\"] != \"Manhattan\")]\n#TABLE = TABLE[(TABLE[\"SubwayProximity\"] <= 0.20)]\n\n\nNUMERICAL_COLUMN_1 = TABLE[NUMERICAL_COLUMN_1_NAME]\nNUMERICAL_COLUMN_2 = TABLE[NUMERICAL_COLUMN_2_NAME]\n# color column\nNUMERICAL_COLUMN_3 = TABLE[NUMERICAL_COLUMN_3_NAME]\n\n\nLEVEL = 2\nNUMERICAL_COLUMN_3 = NUMERICAL_COLUMN_3 #\/\/ 50 #exagerate\nNUMERICAL_COLUMN_3 = (NUMERICAL_COLUMN_3).round(LEVEL)\n\nSCALING = 10**LEVEL\ncolor_ranger = np.arange(NUMERICAL_COLUMN_3.min()*SCALING, NUMERICAL_COLUMN_3.max()*SCALING)\ncolors = plt.cm.cool(np.linspace(0, 1, len(color_ranger)))\n#colors = plt.cm.cool(color_ranger)\n\nCOLUMN_3_COLORS = NUMERICAL_COLUMN_3.apply(lambda x: colors[int(x*100)-1])\n\nfig, ax = plt.subplots()\nax.scatter(NUMERICAL_COLUMN_1, NUMERICAL_COLUMN_2,s=2, alpha=0.5, c=COLUMN_3_COLORS)\nax.set_title(TITLE)\nax.set_xlabel(X_LABEL)\nax.set_ylabel(Y_LABEL)","d57b6de4":"fig, axs = plt.subplots(1, 2)\nnyc_traffic_EDA.groupby([\"BoroCode\"])[\"SubwayProximity\"].mean().plot(kind=\"bar\",ax=axs[0])\nnyc_traffic_EDA.groupby([\"BoroCode\"])[\"NumFloors\"].mean().plot(kind=\"bar\", ax=axs[1])","8381568e":"def get_tvd(dist1, dist2):\n    tvd = (\n        (dist1-dist2).abs()\n    ).sum() * 0.5\n    return tvd\n\nnyc_traffic_EDA_temp = nyc_traffic_EDA.copy()\n#nyc_traffic_EDA_temp = nyc_traffic_EDA[nyc_traffic_EDA[\"BoroCode\"] != \"Manhattan\"]\n\nTABLE = nyc_traffic_EDA_temp\nGROUPBY_COLUMN_NAME = \"BoroCode\"\nAGGREG_COLUMN_NAME = \"Traffic_Volume\"\nSHUFFLED_COLUMN_NAME = GROUPBY_COLUMN_NAME\nSHUFFLED_COLUMN = TABLE[SHUFFLED_COLUMN_NAME].values\n\nOBSERVED_DIST = TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean()\n\nAGGREG_GENERAL_MEAN = TABLE[AGGREG_COLUMN_NAME].mean()\nGROUP_LABELS = pd.unique(TABLE[GROUPBY_COLUMN_NAME])\nEXPECTED_DIST = pd.Series(index = GROUP_LABELS, data=AGGREG_GENERAL_MEAN)\n\nOBSERVED_TVD = get_tvd(OBSERVED_DIST, EXPECTED_DIST)\n\nT = 1000\n\nTEST_STATS = []\n\nfor i in range(T):\n    SHUFFLED_COLUMN = np.random.permutation(SHUFFLED_COLUMN)\n    # shuffle\n    TABLE[SHUFFLED_COLUMN_NAME] = SHUFFLED_COLUMN\n    # re-assign\n    SAMPLE_DIST = TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean()\n    SAMPLE_TVD = get_tvd(SAMPLE_DIST, EXPECTED_DIST)\n    TEST_STATS.append(SAMPLE_TVD)\n\nplt.hist(TEST_STATS)\nplt.scatter(OBSERVED_TVD, 0, c=\"red\")","2c1872b7":"def ABTesting_Borough_Traffic(BORO):\n    nyc_traffic_EDA_temp = nyc_traffic_EDA.copy()\n    BORO_CHOICE = BORO\n    TABLE = nyc_traffic_EDA_temp\n    GROUPBY_COLUMN_NAME = \"BoroCode\"\n    AGGREG_COLUMN_NAME = \"Traffic_Volume\"\n    SHUFFLED_COLUMN_NAME = GROUPBY_COLUMN_NAME\n    #SHUFFLED_COLUMN_NAME = AGGREG_COLUMN_NAME\n    SHUFFLED_COLUMN = TABLE[SHUFFLED_COLUMN_NAME].values\n    AB_TESTING_PLOT_TITLE = \"A\/B Testing for \\n {0} vs. Non-{0} {1}\".format(BORO_CHOICE, AGGREG_COLUMN_NAME)\n    AB_TESTING_PLOT_X_LABEL = \"Difference Between Group Means\"\n    AB_TESTING_PLOT_Y_LABEL = \"Frequency\"\n\n    test_stats = []\n    T = 1000\n\n    nyc_traffic_EDA_temp[GROUPBY_COLUMN_NAME] = (nyc_traffic_EDA_temp[GROUPBY_COLUMN_NAME] == BORO_CHOICE)\n    SHUFFLED_COLUMN = TABLE[SHUFFLED_COLUMN_NAME].values\n\n    observed_test_stat = nyc_traffic_EDA_temp.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean().diff().loc[1]\n\n    for i in range(T):\n        SHUFFLED_COLUMN = np.random.permutation(SHUFFLED_COLUMN)\n        # shuffle\n        TABLE[SHUFFLED_COLUMN_NAME] = SHUFFLED_COLUMN\n        # re-assign\n        #print(len(TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean()))\n        test_stat = TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean().diff().loc[1]\n        test_stats.append(test_stat)\n    \n    fig, ax = plt.subplots()\n    ax.hist(test_stats)\n    ax.set_title(AB_TESTING_PLOT_TITLE)\n    ax.set_xlabel(AB_TESTING_PLOT_X_LABEL)\n    ax.set_ylabel(AB_TESTING_PLOT_Y_LABEL)\n    \n    ax.scatter(observed_test_stat,0,color=\"red\")\n    return (ax, test_stats, observed_test_stat)\n\nax, test_stats, observed_test_stat = ABTesting_Borough_Traffic(\"Manhattan\")\nprint(\"P-value is \", (np.array(test_stats) > (observed_test_stat)).mean())\nax","acc30448":"COLLAPSED = False\n\nTABLE = nyc_traffic_EDA.copy()\n\n# Categorizing and retrieving \"Heavy Traffic\" rows of the table\nTABLE = TABLE[[\"Segment_ID\", \"Traffic_Volume\"]+LANDUSE_COLUMNS]\n\nif COLLAPSED == True: # if this runs, then the table will be flattened so that all street segments only occur once.\n    agg_dict = dict([(col, np.mean) for col in [\"Traffic_Volume\"]+LANDUSE_COLUMNS])\n    TABLE = TABLE.groupby([\"Segment_ID\"], as_index=False).agg(agg_dict)\n\nSTDEV_THRESHOLD = 1\ndef categorize_traffic_volume(x):\n    if x < -STDEV_THRESHOLD:\n        category = 1\n    elif x > STDEV_THRESHOLD:\n        category = 3\n    else:\n        category = 2\n    return category\ntraffic_volume = np.log(TABLE[\"Traffic_Volume\"] + 1)\ntraffic_volume_z = (traffic_volume - traffic_volume.mean())\/(traffic_volume.std())\nTABLE[\"Traffic_Volume_Level\"] = (traffic_volume_z).apply(categorize_traffic_volume)\n\n# Observed Distribution will be landuse composition regularly\n# Expected Distribution will be landuse composition in heavy traffic volume level\n\nHEAVY_TRAFFIC_TABLE = TABLE[TABLE[\"Traffic_Volume_Level\"] == 3]\nHEAVY_TRAFFIC_LANDUSE = HEAVY_TRAFFIC_TABLE[LANDUSE_COLUMNS]\nC=1 # C is some constant\nHEAVY_TRAFFIC_LANDUSE_CDIST = (HEAVY_TRAFFIC_LANDUSE*C).sum(axis=0)\nHEAVY_TRAFFIC_LANDUSE_PDIST = HEAVY_TRAFFIC_LANDUSE_CDIST\/HEAVY_TRAFFIC_LANDUSE_CDIST.sum()\n\nMAIN_TRAFFIC_LANDUSE =  TABLE[LANDUSE_COLUMNS]\nC=1 # C is some constant\nMAIN_TRAFFIC_LANDUSE_CDIST = (MAIN_TRAFFIC_LANDUSE*C).sum(axis=0)\nMAIN_TRAFFIC_LANDUSE_PDIST = MAIN_TRAFFIC_LANDUSE_CDIST\/MAIN_TRAFFIC_LANDUSE_CDIST.sum()","bb0fcfa8":"from scipy.stats import chisquare\n\nPC = 100 # from proportions to constant. Both dists need same total.\nMAIN_TRAFFIC_LANDUSE_PCDIST = MAIN_TRAFFIC_LANDUSE_PDIST*PC\nHEAVY_TRAFFIC_LANDUSE_PCDIST = HEAVY_TRAFFIC_LANDUSE_PDIST*PC\n\nEXPECTED_DIST, OBSERVED_DIST = HEAVY_TRAFFIC_LANDUSE_PCDIST, MAIN_TRAFFIC_LANDUSE_PCDIST\n#OBSERVED_DIST, EXPECTED_DIST = HEAVY_TRAFFIC_LANDUSE_PDIST, MAIN_TRAFFIC_LANDUSE_PDIST\n\nresults = chisquare(list(OBSERVED_DIST), list(EXPECTED_DIST))\nresults_str = \"Test-Stat: {0} \\n P-value: {1}\".format(*results)\nprint(results_str)","e61b3d74":"nyc_traffic_EDA_temp = nyc_traffic_EDA.copy()\nLANDUSE_COLUMN = LANDUSE_COLUMNS[0]\nTABLE = nyc_traffic_EDA_temp\nGROUPBY_COLUMN_NAME = LANDUSE_COLUMN\nAGGREG_COLUMN_NAME = \"Traffic_Volume\"\nSHUFFLED_COLUMN_NAME = GROUPBY_COLUMN_NAME\n#SHUFFLED_COLUMN_NAME = AGGREG_COLUMN_NAME\nSHUFFLED_COLUMN = TABLE[SHUFFLED_COLUMN_NAME].values\n\nAB_TESTING_PLOT_TITLE = \"A\/B Testing for {0} \\n When {1} >50% vs. <50%\".format(AGGREG_COLUMN_NAME, GROUPBY_COLUMN_NAME)\nAB_TESTING_PLOT_X_LABEL = \"Difference Between Group Means\"\nAB_TESTING_PLOT_Y_LABEL = \"Frequency\"\n\ntest_stats = []\nT = 1000\n\nTABLE[GROUPBY_COLUMN_NAME] = (TABLE[GROUPBY_COLUMN_NAME] >= 0.5)\nSHUFFLED_COLUMN = TABLE[SHUFFLED_COLUMN_NAME].values\n\nobserved_test_stat = TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean().diff().loc[1]\n\nfor i in range(T):\n    SHUFFLED_COLUMN = np.random.permutation(SHUFFLED_COLUMN)\n    # shuffle\n    TABLE[SHUFFLED_COLUMN_NAME] = SHUFFLED_COLUMN\n    # re-assign\n    test_stat = TABLE.groupby([GROUPBY_COLUMN_NAME])[AGGREG_COLUMN_NAME].mean().diff().loc[1]\n    test_stats.append(test_stat)\n\nplt.hist(test_stats)\nplt.title(AB_TESTING_PLOT_TITLE)\nplt.xlabel(AB_TESTING_PLOT_X_LABEL)\nplt.ylabel(AB_TESTING_PLOT_Y_LABEL)\n\n\nplt.scatter(observed_test_stat,0,color=\"red\")\nprint(observed_test_stat)","85980b1d":"FOLDER_PATH = os.getcwd()\nFILE_NAME = \"\\\\nyc_traffic_ml_orig.csv\"\nFILE_PATH = FOLDER_PATH + FILE_NAME\n\nFILE_PATH = \"..\/input\/nyc-traffic-data\/nyc_traffic_ml_orig.csv\" # Only for Kaggle\n\nnyc_traffic_ml_orig = pd.read_csv(FILE_PATH)\nnyc_traffic_ml = nyc_traffic_ml_orig.copy()\n\n# You can drop any features you don't like by not adding them to to the SELECTED_COLUMNS list\n# but no matter what, Traffic_Volume must always be in it.\nMAIN_OUTPUT_COLUMN  = \"Traffic_Volume\"\n\nSELECTED_COLUMNS = [MAIN_OUTPUT_COLUMN]\nSELECTED_COLUMNS += [\"Segment_ID\"]\nSELECTED_COLUMNS += [\"Is_Weekend\"]\n\nSELECTED_COLUMNS += [\"Daylight\"]\nSELECTED_COLUMNS += [\"Season{0}\".format(i) for i in [1, 2, 4]]\nSELECTED_COLUMNS += [\"Hr{0}\".format(i) for i in range(3, 27, 3)]\nSELECTED_COLUMNS += [\"NumFloors\"]\nSELECTED_COLUMNS += [\"UnitsRes_Prop\"]\nSELECTED_COLUMNS += [\"StreetWidth\"]\nSELECTED_COLUMNS += [\"SubwayProximity\"]\n\nSELECTED_COLUMNS += [\"BoroCode{0}\".format(i) for i in range(1, 6,1)]\nSELECTED_COLUMNS += [\"LandUse_t{0}\".format(i) for i in range(1, 6,1)]\nnyc_traffic_ml = nyc_traffic_ml_orig.copy()[SELECTED_COLUMNS]\n\nnyc_traffic_ml = nyc_traffic_ml.sample(frac=1)","87d32bf3":"Corr_df = nyc_traffic_ml.drop(columns=[\"Traffic_Volume\"])\nCorr_df[\"Traffic_Volume\"] = nyc_traffic_ml[\"Traffic_Volume\"] # only to put traffic volume at end, + before log categorization\n\nCORR_METHOD = [\"pearson\",\"spearman\",\"kendall\"][1]\ncorrMatrix = Corr_df.corr(method = CORR_METHOD)\n\nquality = corrMatrix[corrMatrix.columns[-1]].abs().sum()\nprint(quality)\nfig, ax = plt.subplots(figsize=(20,20)) \n\nax = sns.heatmap(corrMatrix, annot=True) #notation: \"annot\" not \"annote\"\ny_lims = ax.get_ylim()\n\nax.set_ylim(sum(y_lims), 0)","9a11df1a":"from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","08533787":"#but we might only need the hour columns and the landuse columns\nMAIN_OUTPUT_COLUMN  = \"Traffic_Volume\"\n\nSELECTED_COLUMNS = [MAIN_OUTPUT_COLUMN]\n#SELECTED_COLUMNS += [\"Segment_ID\"]\n#SELECTED_COLUMNS += [\"Is_Weekend\"]\n\n#SELECTED_COLUMNS += [\"Daylight\"]\n#SELECTED_COLUMNS += [\"Season{0}\".format(i) for i in [1, 2, 4]]\nSELECTED_COLUMNS += [\"Hr{0}\".format(i) for i in range(3, 27, 3)]\n#SELECTED_COLUMNS += [\"NumFloors\"]\n#SELECTED_COLUMNS += [\"UnitsRes_Prop\"]\n#SELECTED_COLUMNS += [\"StreetWidth\"]\n#SELECTED_COLUMNS += [\"SubwayProximity\"]\n\n#SELECTED_COLUMNS += [\"BoroCode{0}\".format(i) for i in range(1, 6,1)]\nSELECTED_COLUMNS += [\"LandUse_t{0}\".format(i) for i in range(1, 6,1)]\nnyc_traffic_ml = nyc_traffic_ml_orig.copy()[SELECTED_COLUMNS]\n\n# re transforming the traffic volume column.\nSTDEV_THRESHOLD = 1\ndef categorize_traffic_volume(x):\n    if x < -STDEV_THRESHOLD:\n        category = 1\n    elif x > STDEV_THRESHOLD:\n        category = 3\n    else:\n        category = 2\n    return category\n\nMODEL_INPUT_DF_OUTPUT_COLUMN = MAIN_OUTPUT_COLUMN\n\nnyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN] = (nyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN]+1).apply(np.log)\n\nnyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN] = (\n    nyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN] - \n    nyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN].mean())\/(nyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN].std())\n\nnyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN] = nyc_traffic_ml[MODEL_INPUT_DF_OUTPUT_COLUMN].apply(categorize_traffic_volume)\n\nMODEL_INPUT_DF_OUTPUT_COLUMNS = []\nMODEL_INPUT_DF_OUTPUT_COLUMNS += [MODEL_INPUT_DF_OUTPUT_COLUMN]\nMODEL_INPUT_DF_INPUT_COLUMNS = []\nMODEL_INPUT_DF_INPUT_COLUMNS += list(nyc_traffic_ml.drop(columns=MODEL_INPUT_DF_OUTPUT_COLUMNS).columns.values)\nmodel_input_df = nyc_traffic_ml.copy()\n\ncateg_columns = []\nnumer_columns = MODEL_INPUT_DF_INPUT_COLUMNS\n\n## creating the baseline model pipeline\n\ncateg_columns_pipeline = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"NULL\")),\n    (\"one-hot\", OneHotEncoder(sparse=True,handle_unknown=\"ignore\")),\n])\n\nnumer_columns_pipeline = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))\n])\n\n\nct0 = ColumnTransformer([\n    (\"categ\",categ_columns_pipeline,categ_columns),\n    (\"numer\",numer_columns_pipeline,numer_columns)\n])\n\n## Implementing Random Forest Classification\nMODEL = RandomForestClassifier(class_weight=\"balanced\")\npl_cl_model = Pipeline([\n        (\"features\", ct0),\n        (\"model\", MODEL)\n])\n\ny = model_input_df[MODEL_INPUT_DF_OUTPUT_COLUMN]\nX = model_input_df[MODEL_INPUT_DF_INPUT_COLUMNS]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y)\npl_cl_model.fit(X_train, y_train)\nprint(\"Cross val score:\", cross_val_score(pl_cl_model, X_train, y_train, cv=4))\nprint(\"Score:\", pl_cl_model.score(X_test, y_test))","be1c1e06":"# USER INPUT GOES HERE\n# choose which hour of the day you're intersted in.\nHOUR = 2\n# input goes here\n\n# Decide the LandUse composition surrounding your street segment of interest\nLANDUSE_COMPOSITION = [0.9, 0, 0, 0.1, 0]","a2b4530f":"MODEL_INPUT_DF_INPUT_COLUMNS = []\nMODEL_INPUT_DF_INPUT_COLUMNS += [\"Hr{0}\".format(i) for i in range(3, 27, 3)]\nMODEL_INPUT_DF_INPUT_COLUMNS += [\"LandUse_t{0}\".format(i) for i in range(1, 6, 1)]\n\nHOUR_COLUMNS_VALUES = [0 for _ in range(8)]\nHOUR_COLUMNS_VALUES[(HOUR \/\/ 3)] = 1\n\nMODEL_INPUT_DF_INPUT_COLUMNS_VALUES = []\nMODEL_INPUT_DF_INPUT_COLUMNS_VALUES += HOUR_COLUMNS_VALUES\nMODEL_INPUT_DF_INPUT_COLUMNS_VALUES += LANDUSE_COMPOSITION\n\nsample_input_df = pd.Series(index=MODEL_INPUT_DF_INPUT_COLUMNS,\n                            data=MODEL_INPUT_DF_INPUT_COLUMNS_VALUES)\n\ntraffic_volume_level = pl_cl_model.predict(sample_input_df.to_frame().T)[0]\nprint(\"traffic volume level is\", traffic_volume_level)","fb05f3e7":"Deeper Analysis into Subway Proximity\n* The reason data was obtained for Subway Proximity (Part 1C) is because New York City's subway system is one of the most heavily used urban public transportation systems in the world, much like the London Tube. \n* The result is a hypothesis question about whether or not the availability of nearby subway entrances affects a given street segment's traffic. \n* It would also be nice to know how the feature NumFloors interacts.\n* These 2 scatterplots are 3D graphs, with the 3rd variable being color **(blue=low, purple=high)**.","fe7f94b3":"This geospatial data science project is an exploration of traffic volume statistics in New York City, and includes ArcGIS data preparation, main EDA graphs, and output plots from using Random Forest Classification as the predictive machine learning model. There are 4 parts to this, but parts 1 and 2 are only important for users with custom ArcGIS data, and can be skipped. Users can try out the classification prediction model for their own. The dataset is already provided under nyc_traffic_data for each part.\n\nhttps:\/\/github.com\/benduong2001\/ArcGIS_Project_nyc_traffic\n\n1. [Data Preparation in ArcGIS](#part1)\n    - 1A. [Getting the Street Segment Traffic Data into a GIS-friendly Format](#part1a)\n    - 1B. [Retrieval of Geospatial Data Surrounding the Street Segments](#part1b)\n    - 1C. [Optional - Getting Subway Proximity](#part1c)\n2. [Data Cleaning](#part2)\n    - 2A. [Refining Data for Time](#part2a)\n    - 2B. [Refining Data for Place](#part2b)\n3. [EDA and Hypothesis Testing](#part3)\n4. [Predictive Modelling with Basic Machine Learning](#part4)","69576d55":"**\"Time\"** will be defined particularly in terms of the time of day of the traffic. This refers to the 1st **base dataset** from the beginning, Traffic, which helped to form the new table \"gdb_join\". However, we technically already cleaned the dataset during the very beginning of part 1 before the ArcGIS section, so most of this section is just going into further detail about what happened in it.\n\nSeveral adjustments were done to the original traffic CSV table, mostly adding more tiers of the data granularity of time, such as time of year, time of week, and time of day.\n* A categorical \"Season\" column was added (In this dataset, Summer seemed to be absent).\n    * Season is a reasonable variable to monitor since different seasons have different weathers that might affect traffic volume (for example, traffic maybe reduced during winter due to snowy roads)\n* A boolean \"Is Weekend\" column is added\n    * Weekend is a reasonable variable to monitor since traffic may differ from the business week traffic volume.\n* In the original traffic csv file, the hours of the day seem to be their own seperate columns; they were melted so that each street segment now corresponds to 24 rows.\n* The hours of the day are grouped into sizes of 3 (e.g., 12am to 3 am), etc, with the traffic volume being the group's aggregated average. Now, each day has 8 subdivisions rather than 24.\n    * This is entirely optional, and is mainly to speed up calculations later on.\n* The table seems to have a seperate row for the 2 opposing lanes of the road; the traffic volumes for each pair were added together.","20703d5a":"Remember that \"failed\" OneToOneJoin shapefile layer created back then? This is when it gets used again.\nUse the Geoprocessing > Buffer Tool with the following parameters:\n* Input Features: the \"failed\" one-to-one join layer from earlier.\n* Distance [value or field]: set to 500, with units as Feet (This radius is arbitrary)\n* Dissolve: None\n\nRun the buffer tool, and rename the resulting layer as \"StreetSegmentBlobs\". You should have a result like this: These \"blobs\" are the area 500ft around the the street segment. The reason that the \"Failed join\" layer (\"OneToOneJoin\" layer) was used is to avoid having redundant \"blobs\" stacked onto each other; only 1 is necessary for each street segment.\n\n![cropped58](https:\/\/raw.githubusercontent.com\/benduong2001\/ArcGIS_Project_nyc_traffic\/main\/ArcGIS_Project_nyc_traffic_pics\/cropped58.png?token=APAB24X5UQUKBTBLLRKZTPTBXPD7W)","d88d235f":"# Part 4: *Predictive Modelling with Basic Machine Learning* <a id=\"part4\"><\/a>","d8c88b7b":"# Conclusion\n* Custom traffic predictions for New York City can now be made using the model created.\n* Its accuracy is up to the judgement of NYC Kaggle users; I don't live there.\n* The baseline model requires the following inputs from the user\n    - 1 int \n        * domain is [0, 24]\n        * Represents the hour\n    - 5 floats in a list\n        * Each float must be in the domain [0,1]\n        * **All 5 numbers must add up to closely 1.0**\n        * Represents the LandUse composition of the 500-ft-radius surrounding the user's street of interest \n        * the 5 numbers are LandUse_t1, LandUse_t2, LandUse_t3, LandUse_t4, LandUse_t5, in that order\n* The output is 1, 2, or 3:\n    1. Low Traffic\n    2. Medium Traffic\n    3. High Traffic\n\n\n\n* Note:\n* Recap of LandUse types:\n    - LandUse_t1 = **Residential**\n    - LandUse_t2 = **Commercial**: Office, Retail\n    - LandUse_t3 = **Infrastructure**: Industrial, Transportation, Utility, Parking Structures\n    - LandUse_t4 = **Public Institution**: Schools, libraries, etc.\n    - LandUse_t5 = **Open Space**: parks, vacant land, outdoor\/outside space, public land","dffd63ee":"**\"Place\"** will be defined particularly in terms of the nature of the ***surroundings***, i.e. the ***environmental context***. This refers to the 2nd and 3rd **base dataset** from the beginning, Lion and LandUse, which together helped to form the new table \"StreetSegment_LandUse_Subsets\"\n\nBefore doing anything else, we'll get the Subway_Distances table from the optional Part1C out of the way first (if you did it).\n* The Subway_Distances table shows the distances for each NYC subway entrances from the column \"IN_FID\". It's not certain what exactly \"IN_FID\" is, but we can safely assume it can represent StreetSegment, because after creating a temporary table, it seems to have a functional dependency on Street Segment. \n* A new column \"SubwayProximity\" is made which is 1\/NEAR_DIST.\n    * This ensures that very far away subway entrances (high NEAR_DIST values) are extremely small, almost 0. \n* \"SubwayProximity\" is aggregated for \"IN_FID\" by summing.","ecfc1649":"* During weekdays, the traffic volume of NYC has 2 \"peaks\" during the morning and late evening, which might be the 2 rush hours\/peak traffic commutes of the day.\n* During weekends, the traffic volume has an extremely sinusoidal shape.","abc1b8a0":"By now, there should be 2 new tables created after the ArcGIS part:\n* **gdb_join** is a table for certain street segments in New York City, with each street segment having information on the traffic volume during different parts of the day.\n* **StreetSegment_LandUse_Subsets** is a table of the surrounding geospatial information (municipal land-use) of each street segments. Specifically, it shows within the 500-foot radius of the street segment\n\n We will decide to collapse StreetSegment_LandUse_Subsets so that each street segment has 1 row, essentially aggregating all the info about the street segment's nearby surroundings into 1 compact row.\n\nBoth tables have more than 100 columns, so there's a lot of information we don't really need. Part 2 is about the selection of which columns will seem useful, transformations of them into a useful form, imputation, and the explained thought process throughout.\n\nThis project is after all, about traffic in street segments in NYC. Reasonably, the most important features to determine traffic would be **\"Time\"** and **\"Place\"**.","4134a8d2":"We could move onto data-cleaning and wrap up ArcGIS, but we could also go a bit further. So far, we have data on **time** (hourly traffic data) and **place** (street segment), which are very useful for analyzing traffic. But could **place** be improved? Specifically, could more be done to learn about a street segment's **surroundings** and **local environment**?\n\nEnvironmental context can imply a lot about the traffic volume; for example, if the street segment's **local surrounding area** is mostly residential, like say, Queens or Staten Island, you likely won't see noisy urban traffic.\n\n* But the issue is definining the **local surrounding area** that a street segment falls within. NYC has many subdivision types as options like census tract, zip code, community districts (CT), neighborhood tabulation areas (NTA), even congressional districts and school districts.\n* But, ... the issue is that these options will all lead to the **Modifiable Areal Unit Problem**; they are are human-created, and thus unequally shaped and sized, and overall arbitrary.\n* A better, un-biased solution is to define the \"surrounding area\" as a region drawn around the street segment at a certain radius. For example, 500 feet.","b393fbbf":"# Part 1: Data Preparation in ArcGIS <a id=\"part1\"><\/a>","b09b684e":"## Hypothesis Testing for Traffic Volume","1766961f":"### Hypothesis Testing For Borough and Traffic Volume\n* Permutation Testing with Total Variation Distance\n* Permutation Testing for Individual Boroughs\n\nAs seen below, Manhattan stands out from other boroughs in terms of the 2 previously mentioned attributes SubwayProximity and NumFloors. This prompts hypothesis testing about whether boroughs can have any statistical link to traffic volume (or are otherwise independent), specifically Manhattan.","6da9ec09":"First, we'll need 3 **base datasets** taken from online.\n* A dataset tracking hourly traffic volume in New York City streets over the time of day\n\nhttps:\/\/data.cityofnewyork.us\/Transportation\/Traffic-Volume-Counts-2014-2019-\/ertz-hr4r\n* A dataset that will help us map the New York City street segments to their exact location\n\nhttps:\/\/data.cityofnewyork.us\/City-Government\/LION\/2v4z-66xt\n* A dataset that shows the Landuse and Census parcels of New York City\n\nhttps:\/\/www1.nyc.gov\/site\/planning\/data-maps\/open-data\/dwn-pluto-mappluto.page","8fb5a35b":"## Part 1A: *Getting the Street Segment Traffic Data into a GIS-friendly Format* <a id=\"part1a\"><\/a>","d084dde5":"The ArcGIS portion is complete: each street segment now has its little carved-out, 500ft-radius subset of New York City! But remember, this was done with the OneToOneJoin. So later on, we have to rejoin this layer with the gdb_join dataframe from the end of Part1A. But that can easily be done on pandas, outside of ArcGIS.","c2e56021":"#### Permutation Testing: Individual LandUse and Traffic Volume\n* Similar to the earlier hypothesis test for Manhattan, but this time with LandUse_t1 (Residential)\n* LandUse_t1 is specifically chosen because it seems to have the strongest correlation with Traffic Volume \n* Since LandUse_t1 is continuous, it gets split up as <50% vs >50%","202a4c46":"A (spearman) correlation heatmap to see all of the current features' relation to the response variable \"Traffic Volume\".","ccf08ab8":"## Part 1B: *Retrieval of Geospatial Data Surrounding the Street Segments* <a id=\"part1b\"><\/a>","33380f87":"This is the goal of Part 1B:\n* For each of the street segments in the gdb_join dataframe,\n* Draw  a region that's X distance radius around the street segment (which I chose X as 500 ft, but its up to personal choice)\n* That way, later on in the project, we can gather up all data on the buildings\/land-parcels within that region to form statistics for that street segment.","5bb08ac7":"SubwayProximity and NumFloors are almost linearly correlated. Traffic, on the other hand, is tougher to interpret without re-ordering the variables as seen in the next scatterplot.","04fc67fe":"* First, load the 3rd dataset, which has the name mappluto. \n* Rename this shapefile layer as \"LandUse\". \n\nZooming in, the LandUse shapefile layer (pink) represents the parcels and lots on each city-block. The lion shapefile layer (dark red), and the new gdb_join shapefile layer (bright green) are also shown.\n\n![cropped55](https:\/\/raw.githubusercontent.com\/benduong2001\/ArcGIS_Project_nyc_traffic\/main\/ArcGIS_Project_nyc_traffic_pics\/cropped55.png?token=APAB24SVXSZ4GVY6DIFMY73BXPDZQ)","48a975c7":"* Manhattan seems to be the most traffic-heavy borough throughout the day.","9d71bbfa":"* This means for a chosen street segment in New York City, such that the local surrounding area is 90% residential, and 10% public institution, then at 2 am, the traffic volume would be tier 1 = low.","870b4abf":"Next, use the Geoprocessing > Clip tool with the following parameters:\n* Input Features: the new LandUse dataframe layer\n* Clip Features: the StreetSegmentBlobs Layer from the last step\n\nRun the Clip tool, which might take a while. Name the new clipped layer as StreetSegment_LandUse_Clipped.\n\n![cropped79](https:\/\/raw.githubusercontent.com\/benduong2001\/ArcGIS_Project_nyc_traffic\/main\/ArcGIS_Project_nyc_traffic_pics\/cropped79.png?token=APAB24STJ5F2ZRUOZSPGOX3BXPEF2)","97336446":"## Part 2B:  *Refining Data for 'Place'* <a id=\"part2b\"><\/a>","1794820b":"The resulting table is then duplicated under 2 slightly differing versions, one for Part 3 (EDA), and the other for Part 4 (ML).\n\n**For EDA**:\n* The Boroughs are \"de-one-hot-encoded\"\n\n**For ML**:\n* The standardized StreetWidth (StreetWidth_z) replaces the regular streetwidth","f6552c31":"Opening the traffic CSV file (made from the code before) on ArcMap, you'll come across the first problem. **It's an ordinary CSV file, with no usable geospatial data formatting**. The closest column we have are the street names, which we don't really want to resort to, since string data like this can be wildly inconsistent from one data source to another (East 73rd Street in one data source might be 73 St. E in another).\n\nBut there's no reason to panic. Luckily, the metadata for that traffic CSV file shows that the column \"Segment ID\" is an identifier for each street segment. After some internet sleuthing, you will eventually find an online data source with a name like \"nyc_lion\" (the second base dataset), which has extensive data on NYC street segments, including a column with the exact same identifier. Most importantly, it includes shapefiles that will allow us to use geospatial data.","23f335c7":"Now, we must do a join between the shapefile dataframe and the csv dataframe on the foreign key Segment ID. But there's a side-issue that must be resolved first. In the traffic csv layer, the Segment ID's type is a long integer. Meanwhile, in the lion shapefile layer, the Segment ID is a 7-character string that's zero-padded in the front. This formatting difference will ruin the join.\n\nThis can be resolved by making a new column with the correctly formatted segment ID. \n* Add a new 7-digit, long-integer field to the dataframe of the lion shapefile layer with the name \"Segment_ID\"\n* Use the field calculator to convert that string column to a long integer. \n* I used the python calculator, and used the code int(!SegmentID!).\n\nNow we can go ahead with the join. But after running, you'll notice a problem: ArcGIS's inbuilt join tool will only treat it as one-to-one, keeping only the first matching row it finds for each street segment, and ignoring the duplicate rows after. This means that each street segment row will have just the first hour of the day, rather than all of them. This is bad, because it needs to be a one-to-many join, because each street segment has many rows describing its traffic volume at different times of the day. **However, we won't delete this failed join layer**. It will be useful later, so rename it as something like \"OneToOneJoin\" and tuck it away for now.","a6087232":"Finally, use the Customize > ArcToolbox > Analysis Tools > Overlay > Spatial Join tool with the following parameters:\n* Target Features: the LandUse_StreetSegmentBlobs_Clipped layer\n* Join Features: the StreetSegmentBlobs layer\n* Join Operation: One to Many\n* Match Option: Intersect\n\nName the resulting layer as StreetSegment_LandUse_Subsets, and export the dataframe of it to a txt file for later use as a CSV.","443a63bb":"That final, resulting join layer (\"gdb_join\") should appear like this (It does not need to be green). All the other layers were hidden and a BaseMap was added underneath to show the purpose of this join.\n\n![cropped40](https:\/\/raw.githubusercontent.com\/benduong2001\/ArcGIS_Project_nyc_traffic\/main\/ArcGIS_Project_nyc_traffic_pics\/cropped40.png?token=APAB24U725EUI6MAISWAKNLBXPC4O)\n\nIn summary, for the 1st base dataset listed at the beginning, we had to join it with the 2nd base dataset, so that its street segment traffic data could be geospatially usable and visually seen when loaded up onto ArcMap.","847186bc":"## Higher Dimensional Data Analysis of Traffic Volume and Time","0d72b157":"But having all of these rows for each land parcels is not neceessary. The info for all of the buildings in a single street segment's surrounding area can be compactly summarized into 1 row through aggregation: \n\n* StreetWidth (mean)\n* A duplicate of StreetWidth but with it standardized by z-scoring (mean)\n* SubwayProximity (mean)\n* UnitsRes_Prop (mean)\n* NumFloors (mean)\n* Borough (One Hot Encoded, then summed and then proportionalized i.e. divided by the row-wise sum)\n* LandUse (One Hot Encoded, then summed and then proportionalized i.e. divided by the row-wise sum)\n\nFor Borough and LandUse, restandardize **after** the aggregation.","83fe98cb":"But all of these features aren't really needed, since we're planning to predict traffic volume discretely. Classification problems tends to be less demanding, and as a result, we might only need to use the hour columns and the landuse columns to predict the traffic volume, each representing time and place. Had this been a not-so-discrete regression problem, then it would likely be necessary to keep way more features.\n* Random Forest Classification will be used as the machine learning model.","25ebd301":"## Part 1C: *Getting Subway Proximity* <a id=\"part1c\"><\/a>","f311f866":"Now is the eventual modelling of NYC traffic with the current data so far.\n* The traffic volume prediction will be treated as a classification problem (which, after all, is pretty similar to most traffic forecasting\/reporting apps in real life, which documents traffic ordinally). \n* The Traffic Volume column will be transformed to have levels, 1: mild traffic, 2: medium traffic, 3: heavy traffic\n    * But before that, it will be logged to make its currently skewed distribution to be more normal\n    * And then, the normal distribution will be standardized, since the standard deviations are being used as the categorical separators.","12e116b4":"Get the first dataset, and run it through the following code. It mostly undergoes data-cleaning, melting, and creation of columns for different tiers of data granularity in terms of time, for time of year, time of week, and time of day. The resulting csv \"nyc_traffic_3hrInterval\", will be referred to as the \"traffic CSV file\" for the rest of this project.","c8cc1dab":"#### Chi-Square Testing: LandUse and Traffic Volume \n\n* For this test, the Traffic Volume column underwent transformation and ended up being 3 categorical tiers of traffic congestion\n    - The specific procedure for it will also be done in Part 4 \n* The LandUse composition when Traffic Volume is tier-3 (high) will be the Expected\n* The LandUse composition for all traffic volume tiers (in other words, just the dataset's general landuse composition) will be the Observed.\n","bc565fef":"These tests show that boroughs might have some statistical influence on traffic volume. The next section goes into whether similar conclusions can be said for locational information other than borough, specifically LandUse.","3c68979a":"#### Permutation Testing: Individual Borough and Traffic Volume\n* Hypothesis Testing, but this time specifically focusing on our most conspicuous borough - Manhattan.\n* Since this is just 2 categories, Difference of Means was the test statistic.\n* This section is made to test on other boroughs too, depending on the input for ABTesting_Borough_Traffic\n* Null Hypothesis:  Manhattan and Not-Manhattan traffic volume means should be the same, and any differences are just due to chance\n* Alternative Hypothesis: Manhattan and Not-Manhattan traffic volume means are different\n* **Result**:  A very low p-value (less than 0.05) shows the Null Hypothesis is rejected.","f9d755a8":"The conclusion that could be reached using both plots is:\n* When Subway Proximity is high (meaning there are many available subway entrances around you) and NumFloors is high, traffic volume tends to be low.\n    - A likely explanation could be that commuters would rather take the subway than add onto the road's congestion.\n    - But many other explanations are possible\n* But as the first scatterplot shows, the opposite end isn't necessarily true: when Subway Proximity and NumFloors is low, Traffic volume isn't strictly high, and can still vary as low too.","7ca664c1":"## Part 2A:  *Refining Data for 'Time'* <a id=\"part2a\"><\/a>","8ed82212":"# Part 3: *Exploratory Data Analysis and Hypothesis Testing* <a id=\"part3\"><\/a>","8497026e":"### Hypothesis Testing For LandUse and Traffic Volume\n* Chi-Square Testing\n* Permutation Testing for Individual LandUse\n\n","c5a7eedb":"* This part will mostly focus on Traffic Volume, and its interactions with temporal features and spatial features and various combinations. Hypothesis Testing focused on Traffic volume will also be done.","36554c0f":"Now is mainly the data-cleaning for StreetSegment_Landuse_Subsets\n* Lion Metadata: https:\/\/archive.nyu.edu\/bitstream\/2451\/34565\/3\/lion_metadata.pdf\n* LandUse Metadata: https:\/\/www1.nyc.gov\/assets\/planning\/download\/pdf\/data-maps\/open-data\/pluto_datadictionary.pdf?v=21v3\n\nReading through both the Lion and LandUse dataset's original metadata pdf files, many features aren't quite relevant to traffic, but the ones that *do* show promise, are:\n* StreetWidth, \n    * For which we'll make a secondary standardized version (StreetWidth_z)\n* NumFloors\n* UnitsRes and UnitsTotal (can be divided to show percentage of residential units under the new column UnitsRes_Prop)\n* Borough \n    * (for higher-level granularity with geographic data)\n* (and most importantly) *LandUse*. \n    * LandUse is a categorical column that designates the use of a building, for example if it's Commercial\/office, Industrial, etc. Some of the LandUse category are redundant, and were grouped together. For example, 01, 02, 03, and 04 can be grouped as \"residential\". This is useful because for traffic, the \"environmental context\" can be helpful. For example, here may be less traffic in heavily residential \"surroundings\".","0052528d":"#### Permutation Testing with Total Variation Distance: Borough and Traffic Volume\n* Hypothesis Testing to test any relations between Borough and Traffic Volume will use permutation.\n* This should be slightly reminiscent to ANOVA.\n* Since this deals with more than 2 categories,  Total Variation Distance had to be used.\n* Null Hypothesis: The group-wise traffic volume means per borough are the same, and any differences are just due to chance.\n* Alternative Hypothesis: The distribution of borough-wise traffic volume means are unequal\n* **Result**:  A very low p-value (less than 0.05) shows the Null Hypothesis is rejected.","8b6514b7":"Back to part 2, apart from some column renamings, the resulting table is then duplicated under 2 slightly differing versions, one for Part 3 (EDA), and the other for Part 4 (ML).\n\n**For EDA**:\n* The Season column is changed from numbers to nominal labels. 1-> Winter, 2->Spring, 4->Autumn.\n\n**For ML**:\n* The Season column is converted to be One-Hot-Encoded (set of boolean columns)\n* The hour column (or the 3 Hour Interval column) is converted to be One-Hot-Encoded (set of boolean columns)\n    * While the 3 Hour Interval column is numerical, I'm treating it as categorical because traffic volume is fully un-related with the linearly increasing nature of the hour label. \n* [Optional] Adding a new numerical variable \"Daylight\" will help solve the above issue, by putting the hour column through a bell-curving function f(x) = (2)\/(1 + np.exp(((x - 12)\/4)^2)), transforming it to look like traffic's hump-like timeline. Because like traffic, daylight is low at the start and end of the day, but high in the middle.","5dd135d7":"For each street segment, we'd like to calculate it's distance to all NYC subway entrances:\n* Subway entrance dataset: https:\/\/data.cityofnewyork.us\/Transportation\/Subway-Entrances\/drex-xx56\n\nLoad the shapefile onto the same ArcMap file.\nUse the Customize > ArcToolbox > Analysis Tools > Proximity > Generate Near Table tool with the following parameters:\n* Input Features: the OneToOneJoin shapefile layer\n* Near Features: the new Subway Entrances shapefile layer\n* Turn off the box that says to only include the nearest subway station\n\nRun it. It will take awhile since it's very computationally expensive, but once it's done, export the new table as a txt file under the name \"Subway_Entrances\".","861729a5":"# Part 2: *Data Cleaning* <a id=\"part2\"><\/a>","765cf69c":"* In fact, this unusually perfect sinusoidal shape for weekend traffic is even more noticeable if you revert back to 1-hour intervals rather than 3-hour intervals (using the data from the codeblock at the very start of Part1).","1ca2f88b":"* Download the nyclion dataset, and open up it on ArcMaps. \n* Drag the \"lion\" layer into ArcMaps from the catalog. This will show a comprehensive polyline street map of New York City. This is the only layer we will actually need from nyclion; the rest of the data folder wastes up a lot of storage, so export the lion layer as a lone shape file and delete the other stuff. Now, we just have the lion shapefile, and the traffic csv.\n\n![cropped12](https:\/\/raw.githubusercontent.com\/benduong2001\/ArcGIS_Project_nyc_traffic\/main\/ArcGIS_Project_nyc_traffic_pics\/cropped12.png?token=APAB24WZ3HZXRC7J2FHDWBDBXPO4E)"}}