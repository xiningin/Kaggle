{"cell_type":{"7e52fd05":"code","a5738d02":"code","8a628350":"code","b40396ce":"code","7cab0408":"code","bb7ef485":"code","26daac70":"code","05ffb9fe":"code","e5fc2ea1":"code","14eb3530":"code","f6b3eed8":"code","a0591bca":"code","5516ad44":"code","26436c66":"code","b6586dc7":"code","37d97c42":"code","bb40d40d":"code","5afb9871":"code","59607352":"code","d4b39bbf":"code","82a42934":"code","b90f4665":"code","6ba7a97b":"code","8ab86b85":"code","666a9572":"code","dc03ebca":"code","96254838":"code","41ef0425":"code","93443d50":"code","c7cc15e9":"code","484ed08e":"code","241905f6":"code","d8b96448":"code","e40b9bac":"code","b7305a2f":"code","4efc030c":"code","58b72d58":"code","52cab521":"code","fb0d7d21":"code","f26d635e":"code","02dfd784":"code","8971f915":"code","f6ca02a6":"code","183fccb1":"code","c3b49cc6":"code","efa494bc":"code","857fc920":"code","8f7c4ff0":"code","6c3d0795":"code","0b89836e":"code","f8fa8c9e":"code","f62cc0d4":"code","6d53885f":"code","5766a53b":"code","3b3b53a3":"code","e866c69a":"code","92c5f4c0":"markdown","9c84d8ae":"markdown","819cdf95":"markdown","ab6f77e7":"markdown","4ee16fb4":"markdown","257d39ed":"markdown","19d83fa9":"markdown","c3cde07a":"markdown"},"source":{"7e52fd05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n#models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5738d02":"train_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","8a628350":"train_df.describe()","b40396ce":"print(train_df.shape)\nprint(test_df.shape)","7cab0408":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train_df.columns:\n    if train_df[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_df[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train_df)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","bb7ef485":"corr = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9,cmap='Blues', square=True)","26daac70":"sns.scatterplot(x=train_df['1stFlrSF'],y=train_df['SalePrice'])","05ffb9fe":"sns.scatterplot(x=train_df['LotFrontage'],y=train_df['SalePrice'])","e5fc2ea1":"sns.scatterplot(x=train_df['OverallQual'],y=train_df['SalePrice'])","14eb3530":"sns.scatterplot(x=train_df['YearBuilt'],y=train_df['SalePrice'])","f6b3eed8":"sns.scatterplot(x=train_df['TotalBsmtSF'],y=train_df['SalePrice'])","a0591bca":"sns.scatterplot(x=train_df['GrLivArea'],y=train_df['SalePrice'])","5516ad44":"train_df.drop(['Id'], axis=1, inplace=True)\ntest_df.drop(['Id'], axis=1, inplace=True)","26436c66":"sns.distplot(train_df['SalePrice'])","b6586dc7":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","37d97c42":"cat_cols = [x for x in train_df.columns if train_df[x].dtype == 'object']\nnum_cols = [x for x in train_df.columns if train_df[x].dtype != 'object']","bb40d40d":"train_df.drop(train_df[(train_df['OverallQual']<5)&train_df['SalePrice']>200000].index,inplace=True)\ntrain_df.drop(train_df[(train_df['GrLivArea']<4500)&train_df['SalePrice']>300000].index,inplace=True)\ntrain_df.drop(train_df[(train_df['OpenPorchSF']<500)&train_df['SalePrice']>200000].index,inplace=True)\ntrain_df.reset_index(drop=True,inplace=True)","5afb9871":"y_train=train_df['SalePrice'].reset_index(drop=True)\nX_train=train_df.drop(['SalePrice'],axis=1)\nX_test=test_df","59607352":"print('Unique values are:', train_df.MiscFeature.unique())","d4b39bbf":"#analysing the categorical data\ncategoricals = train_df.select_dtypes(exclude= [np.number])\ncategoricals.describe()","82a42934":"#One-hot encoding to convert the categorical data into integer data\ntrain_df['enc_street'] = pd.get_dummies(train_df.Street, drop_first= True)\ntest_df['enc_street'] = pd.get_dummies(test_df.Street, drop_first= True)","b90f4665":"print('Encoded: \\n')\nprint(train_df.enc_street.value_counts())","6ba7a97b":"def encode(x): \n    if x == 'Partial':\n        return 1\n    else:\n        return 0\n#Treating partial as one class and other all sale condition as other\ntrain_df['enc_condition'] = train_df.SaleCondition.apply(encode)\ntest_df['enc_condition'] = test_df.SaleCondition.apply(encode)","8ab86b85":"df=pd.concat([X_train, X_test]).reset_index(drop=True)\ndf.shape","666a9572":"def missing_val(dat):\n    dict_x={}\n    data = pd.DataFrame(dat)\n    df_cols = list(pd.DataFrame(data))\n    for i in range(0,len(df_cols)):\n        dict_x.update({df_cols[i]:round(data[df_cols[i]].isnull().mean()*100,2)})\n    return dict_x\nmissing=missing_val(df)\ndf_missing=sorted(missing.items(),key=lambda x:x[1],reverse=True)\nprint('Percent of missing data')\ndf_missing[0:10]","dc03ebca":"missing = train_df.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","96254838":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\nhandle_missing(df)","41ef0425":"print('Number of Numerical Columns:',len(num_cols))\nprint('Number of Categorical Columns:',len(cat_cols))","93443d50":"num_cols.remove('SalePrice')","c7cc15e9":"num_cat = ['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']\nfor i in num_cat:\n    df[i] = df[i].apply(str)\nnum_only = list(set(num_cols)-set(num_cat))","484ed08e":"#Lets normalize numeric coloumns with Normalizer\nfrom sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler(feature_range = (0,10))\ndf[num_only] = mm_scaler.fit_transform(df[num_only])","241905f6":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in df.columns:\n    if df[i].dtype in numeric_dtypes:\n        numeric.append(i)","d8b96448":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","e40b9bac":"# Find skewed numerical features\nskew_features = df[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","b7305a2f":"for i in skew_index:\n    df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))\n# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","4efc030c":"from sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\ndf[num_cat] = oe.fit_transform(df[num_cat])","58b72d58":"df[num_cols].tail()","52cab521":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features = 3, input_type = 'string')\ndf_cat_cols = pd.DataFrame()","fb0d7d21":"for i in cat_cols:\n    df_cat_cols = pd.concat([pd.DataFrame(fh.fit_transform(df[i]).toarray()).add_prefix(i+'_'),df_cat_cols], axis = 1)\n\ndf_cat_cols","f26d635e":"df = pd.concat([df[num_cols].reset_index(drop = True), df_cat_cols], axis = 1)","02dfd784":"train = df[:y_train.shape[0]]\ntest = df[y_train.shape[0]:]","8971f915":"train.shape,test.shape","f6ca02a6":"for i in train.columns:\n    if train[i].dtype == 'object':\n        print(i)","183fccb1":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train, y_train, test_size = 0.2)","c3b49cc6":"X_train.shape,y_train.shape,X_val.shape,y_val.shape","efa494bc":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\n#function for error calculation\ndef rmsle_func(actual, pred):\n    return np.sqrt(((np.log(pred + 1) - np.log(actual + 1))**2).mean())\n\nrmsle = make_scorer(rmsle_func, greater_is_better = False)","857fc920":"#function to get best parameters for our model\ndef generate_clf(clf, params, x, y):\n    rs = RandomizedSearchCV(clf, params)\n    rs_obj = rs.fit(x, y)\n    best_rs = rs_obj.best_estimator_\n    print('Best parameters:',rs_obj.best_params_)\n    pred = rs.predict(X_valid)\n    print('Training score:',rs.score(x, y))\n    print('Validation score:',rs.score(X_valid, y_valid))\n    print('Validation RMSLE error:',rmsle_func(pred, y_valid))\n    kf = KFold(n_splits = 5, shuffle = True)\n    return np.sqrt(-1 * cross_val_score(best_rs, x, y, cv = kf, scoring = rmsle)) , rs_obj.best_params_","8f7c4ff0":"from lightgbm import LGBMRegressor\n\n# lgbm_clf = LGBMRegressor()\n\n# lgbm_params = {'application':['regression'],\n#               'num_iterations':[500, 700, 1000, 1500],\n#               'max_depth':[3, 5, 7, 9],\n#               'min_data_in_leaf':[4, 5, 6, 7, 8],\n#               'feature_fraction':[0.2, 0.3, 0.4],\n#               'bagging_fraction':[0.6, 0.7, 0.8, 0.9],\n#                'num_leaves':[5, 7, 10],\n#               'learning_rate':[0.01, 0.1, 0.02, 0.05, 0.03]}\n\n# lgbm_score ,lgbm_best_params = generate_clf(lgbm_clf, lgbm_params, X_train, y_train)\n# print('LGBM RMSLE on Complete Train set:',lgbm_score.mean())\n\nlgbm_clf_final = LGBMRegressor(application = 'regression', num_iterations = 1000, max_depth = 7, num_leaves = 70)\n# lgbm_clf_final = LGBMRegressor(**lgbm_best_params)\n\n","6c3d0795":"from xgboost import XGBRegressor\n\n# xgb_clf = XGBRegressor()\n\n# xgb_params = {'n_etimators':[2000, 2500, 3000, 3500, 4000],\n#              'gamma':[0.02, 0.04, 0.05, 1],\n#              'max_depth':[3, 5, 7, 9],\n#              'alpha':[0.02, 0.04, 0.05, 1],\n#              'eta':[0.02, 0.04, 0.05, 0.1]}\n\n# xgb_score , xgb_best_params = generate_clf(xgb_clf, xgb_params, train, target)\n# print('XGB RMSLE on Complete Train set:',xgb_score.mean())\nxgb_clf_final = XGBRegressor(n_estimators = 3000)\n# xgb_clf_final = XGBRegressor(**xgb_best_params)","0b89836e":"from sklearn.linear_model import Lasso\n\n# las_clf = Pipeline([('scaler', RobustScaler()),\n#                     ('clf', Lasso())])\n# las_clf = Lasso()\n\n# las_params = {'alpha':[1e-4, 1e-3, 1e-2, 0.1, 0.05]}\n# las_score , las_best_params = generate_clf(las_clf, las_params, train, target)\n# print('Lasso RMSLE on Training set:',las_score.mean())\n\nlas_clf_final = Lasso(alpha = 0.01)\n# las_clf_final = Lasso(**las_best_params)","f8fa8c9e":"target=train_df['SalePrice']","f62cc0d4":"from sklearn.ensemble import StackingRegressor\n\nmy_estimators = [#('lgbm', lgbm_clf_final),\n                ('xgb', xgb_clf_final)]\n\nstreg_clf = StackingRegressor(estimators = my_estimators, final_estimator = las_clf_final)\nstreg_clf.fit(train, target)\nstreg_clf_pred = np.exp(streg_clf.predict(test))","6d53885f":"print(streg_clf.score(X_train, y_train))\nprint(streg_clf.score(X_val, y_val))\nprint(streg_clf.score(train, target))","5766a53b":"final_results = streg_clf_pred\nfinal_results","3b3b53a3":"submission_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission_data['SalePrice'] = final_results\nsubmission_data.to_csv('output.csv', index = False)\n\nsubmission_data.info()","e866c69a":"submission_data.head()","92c5f4c0":"# **Train Models**","9c84d8ae":"# #Lets drop the unwanted features","819cdf95":"# **Lets see how some of the features are related to SalePrice**","ab6f77e7":"**Defining functions for error calculations**","4ee16fb4":"# **Filling Missing Values**","257d39ed":"# **Feature Transformation**","19d83fa9":"# **Removing Outliers**","c3cde07a":"no output means we are ready to train models"}}