{"cell_type":{"414ab073":"code","25bf1c01":"code","c285df8b":"code","996173f2":"code","4e2c71f9":"code","8423dbe9":"code","f8e01fd6":"code","c87ee476":"code","c74e2698":"code","19ec3e6e":"code","65d5c491":"code","76d2d6bd":"code","106b3e59":"code","77f9d945":"code","a0d28201":"code","255f319a":"code","1ca6c4b3":"code","8c1fead7":"code","15324b72":"code","154e02d3":"code","5ca6de3f":"code","f82808b0":"code","2a1197aa":"code","2744ebee":"code","2d7ef346":"code","306fffbc":"code","dfbf3314":"code","9061cd88":"code","875f1756":"code","9838adc9":"code","da9722f3":"code","61914c8b":"code","23457414":"code","a3c10afa":"code","585ff10c":"code","0f4a54a3":"code","b5dea79c":"code","098df2af":"code","118b26ee":"code","6b967119":"code","91af8493":"code","08516e45":"code","fde085da":"code","52d1f0e8":"code","ce74d327":"code","3a932b37":"code","1db568f7":"code","8d8d3184":"code","bed3d565":"code","eeb0b2df":"code","49792c63":"code","3b499a78":"code","7cb1382a":"code","fa68b143":"code","91e124cc":"code","7818ba06":"code","da04378d":"code","96ba6cfb":"code","b9d32200":"code","cd8b99d3":"code","d9cd0f88":"code","f7da5442":"code","01291d4f":"code","0318d979":"code","d405d315":"markdown","5086aebe":"markdown","2c47b7ba":"markdown","62e2e244":"markdown","be19f084":"markdown","cf8b551e":"markdown","9f5c48d4":"markdown","ba6b57e1":"markdown","f4125e4a":"markdown","a480177b":"markdown","18b78932":"markdown","d49c8afa":"markdown","b3013eb6":"markdown","ef5f4eb0":"markdown","2089c1fe":"markdown","c4f207ae":"markdown","3c2d5647":"markdown","78028dfa":"markdown","87de6052":"markdown","6187c5da":"markdown","e6bc5ba3":"markdown","ba7b2129":"markdown","9af3ef7b":"markdown","22ef16a0":"markdown","5ff74fc9":"markdown","27b61a1c":"markdown"},"source":{"414ab073":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","25bf1c01":"#Importing the datasets\nemployee_df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/employee_survey_data.csv')\nmanager_df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/manager_survey_data.csv')\ngeneral_df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/general_data.csv')\nIn_df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/in_time.csv')\nOut_df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/out_time.csv')","c285df8b":"general_df.info()","996173f2":"general_df.describe()","4e2c71f9":"general_df.isnull().sum()","8423dbe9":"sns.pairplot(general_df, hue='Attrition',  diag_kind='hist')\n","f8e01fd6":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.heatmap(general_df.corr(),cmap =\"YlGnBu\", linewidths = 0.1, ax = ax)","c87ee476":"feature_cat = ['BusinessTravel', 'Department', 'Education', 'EducationField', 'Gender', 'JobLevel',  'MaritalStatus', 'NumCompaniesWorked', 'StockOptionLevel'\n           ,  'TrainingTimesLastYear', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ni = 0\nj = 0\nfig, ax = plt.subplots(4, 3, figsize=(24, 20))\nfor feature in feature_cat:\n    sns.countplot(x=feature, hue='Attrition', data=general_df, ax = ax[i,j])\n    j = j + 1\n    if j > 2:\n        j = 0\n        i = i + 1 ","c74e2698":"def pie(category):\n    label = []\n    label_percent = []\n    for cat in general_df[category].unique():\n        label.append(cat)\n        t1 = general_df[(general_df[category] == cat) & (general_df['Attrition'] == 'Yes')].shape[0]\n        t2 = general_df[general_df[category] == cat].shape[0]\n        label_percent.append(t1\/t2 * 100)\n    fig1, ax1 = plt.subplots()\n    ax1.pie(label_percent, labels=label, autopct='%1.1f%%', shadow=True, startangle=180)\n    centre_circle = plt.Circle((0,0),0.75,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    ax1.axis('equal')\n    plt.title(category)\n    plt.show()\n        \npie('EducationField')","19ec3e6e":"pie('Department')","65d5c491":"pie('BusinessTravel')","76d2d6bd":"pie('MaritalStatus')","106b3e59":"def violin_plot(main_cat):\n    fig, ax = plt.subplots(figsize=(24, 0.1))\n    plt.title(main_cat)\n    plt.axis('off')\n    plt.show\n    sns.catplot(x=main_cat, y='Age', data=general_df, hue='Attrition', kind='violin', bw=0.2, col='Gender', inner='quartile', height=6, aspect=1.5)\n    sns.catplot(x=main_cat, y='YearsAtCompany', data=general_df, hue='Attrition',kind='violin', bw=0.2, col='Gender', inner='quartile', height=6, aspect=1.5)\n    sns.catplot(x=main_cat, y='MonthlyIncome', data=general_df, hue='Attrition',kind='violin', bw=0.2, col='Gender', inner='quartile', height=6, aspect=1.5)\n    sns.catplot(x=main_cat, y='NumCompaniesWorked', data=general_df, hue='Attrition',kind='violin', bw=0.2, col='Gender', inner='quartile', height=6, aspect=1.5)\n    ","77f9d945":"violin_plot('BusinessTravel')","a0d28201":"violin_plot('Department')","255f319a":"violin_plot('EducationField')","1ca6c4b3":"employee_df.info()","8c1fead7":"employee_df.describe()","15324b72":"employee_df.isnull().sum()","154e02d3":"manager_df.info()","5ca6de3f":"manager_df.describe()","f82808b0":"manager_df.isnull().sum()","2a1197aa":"print(employee_df['EnvironmentSatisfaction'].median())\nprint(employee_df['JobSatisfaction'].median())\nprint(employee_df['WorkLifeBalance'].median())","2744ebee":"#since all the 3 columns haas 3 as median value so we fill null value with 3 rating\nemployee_df.fillna(3.0, inplace=True)\nemployee_df.isnull().sum()","2d7ef346":"general_df[general_df['NumCompaniesWorked'].isnull()]","306fffbc":"general_df[general_df['TotalWorkingYears'].isnull()]","dfbf3314":"#Correcting the column name\nIn_df.rename(columns={'Unnamed: 0' : 'EmployeeID'}, inplace=True)\nOut_df.rename(columns={'Unnamed: 0' : 'EmployeeID'}, inplace=True)","9061cd88":"#Code for generating average in out time diff\ntime_diff = []\nfor i in range(4410):\n    time_diff.append([])\n    for j in range(262):\n        time_diff[i].append(j)\ntime_diff_sec = []\nfor i in range(4410):\n    time_diff_sec.append([])\n    for j in range(262):\n        time_diff_sec[i].append(j)\n\nfrom datetime import datetime\nIn_df.fillna(In_df.iloc[2,2], inplace=True)\nOut_df.fillna(In_df.iloc[2,2], inplace=True)\nfor i in range(0,4410):\n    for j in range(1, 262):\n        In_Time = In_df.iloc[i,j]\n        out_Time = Out_df.iloc[i,j]\n        d1 = datetime.strptime(In_Time, \"%Y-%m-%d %H:%M:%S\")\n        d2 = datetime.strptime(out_Time, \"%Y-%m-%d %H:%M:%S\")\n        time_diff[i][j] = d2 - d1\n        a = d2.hour*3600 + d2.minute*60 + d2.second\n        b = d1.hour*3600 + d1.minute*60 + d1.second\n        time_diff_sec[i][j] = a - b\n        \nIn_Out_Diff = pd.DataFrame(time_diff, columns=In_df.columns)\nIn_Out_Diff_sec = pd.DataFrame(time_diff_sec, columns=In_df.columns)\nIn_Out_Diff['EmployeeID'] = In_df['EmployeeID']\nIn_Out_Diff_sec['EmployeeID'] = In_df['EmployeeID']        ","875f1756":"#Time format in seconds\nIn_Out_Diff_sec.head(5)","9838adc9":"#Time format in HH:MM:SS\nIn_Out_Diff.head(5)","da9722f3":"#Code for generating mean time for each employee\nimport datetime\nmean_time = []\nmean_sec = []\nfor i in range(4410):\n    mean = In_Out_Diff_sec.iloc[i : i+1, 1: ].values.mean()\n    mean_format = str(datetime.timedelta(seconds = mean))\n    mean_sec.append(mean)\n    mean_time.append(mean_format)\n\nIn_Out_Diff['Mean_Sec'] = mean_sec\nIn_Out_Diff['Mean'] = mean_time\nIn_Out_Diff.head()","61914c8b":"Emp_in_out = In_Out_Diff[['EmployeeID', 'Mean', 'Mean_Sec']]\n#Code for giving rating basis on meant time\n#25200 - less than this - 1\n#between two - 2\n#30600 - moe than - 3\nrating = []\nfor i in range(Emp_in_out.shape[0]):\n    mean_time = Emp_in_out['Mean_Sec'].iloc[i]\n    if mean_time < 25200:\n        rating.append(1) \n    elif (mean_time > 25200) & (mean_time < 30600):\n        rating.append(2) \n    else:\n        rating.append(3)\nrating\nEmp_in_out['Work_Time_Rating'] = rating\n\nEmp_in_out.head()","23457414":"Employee = general_df.merge(Emp_in_out[['EmployeeID','Work_Time_Rating']], how='outer', on='EmployeeID')\nEmployee = Employee.merge(employee_df, how='outer', on='EmployeeID')\nEmployee = Employee.merge(manager_df, how='outer', on='EmployeeID')\ndf = Employee[['EmployeeID']]\nEmployee = Employee.drop('EmployeeID', axis=1)\nEmployee = pd.concat([df, Employee], axis=1)\ndf = Employee[['Attrition']]\nEmployee = Employee.drop('Attrition', axis=1)\nEmployee = pd.concat([Employee, df], axis=1)","a3c10afa":"Employee.isnull().sum()","585ff10c":"#Droping Null values\nEmployee.dropna(inplace=True)\nEmployee.isnull().sum()","0f4a54a3":"a = ['Attrition', 'BusinessTravel', 'Department', 'Education', 'EducationField', 'EmployeeCount', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'StandardHours', 'StockOptionLevel', 'TrainingTimesLastYear', 'Work_Time_Rating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement', 'PerformanceRating']\nfor col in a:\n    print( '_______________________________________  ' + col + '  _______________________________________' )\n    print(str(Employee[col].unique()))\n    print( '_________________________________________________________________________________' )","b5dea79c":"Employee.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1, inplace=True)","098df2af":"Employee","118b26ee":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","6b967119":"def encoder(X_i):\n    label_encoder = LabelEncoder()\n    X_i =  label_encoder.fit_transform(X_i)\n    return X_i","91af8493":"temp = Employee.drop('Attrition', axis=1)","08516e45":"#Transforming BussineesTravel\ntemp['BusinessTravel'] = encoder(temp['BusinessTravel'])\n\n#Transforming Department\ntemp['Department'] = encoder(temp['Department'])\n\n#Transforming EducationField\ntemp['EducationField'] = encoder(temp['EducationField'])\n\n#Transforming Gender\ntemp['Gender'] = encoder(temp['Gender'])\n\n#Transforming JobRole\ntemp['JobRole'] = encoder(temp['JobRole'])\n\n#Transforming MaritialStatus\ntemp['MaritalStatus'] = encoder(temp['MaritalStatus'])\n\n#Transforming StockOptionLevel\ntemp['StockOptionLevel'] = encoder(temp['StockOptionLevel'])","fde085da":"temp","52d1f0e8":"i = temp[['BusinessTravel', 'Department', 'EducationField', 'Gender' , 'JobRole', 'MaritalStatus']]\nj = temp.drop(['BusinessTravel', 'Department', 'EducationField', 'Gender' , 'JobRole', 'MaritalStatus'], axis=1)","ce74d327":"onehotencoder = OneHotEncoder(categorical_features=None, categories ='auto' , drop = 'first'  )\ni = onehotencoder.fit_transform(i).toarray()","3a932b37":"X = np.concatenate((j, i),axis=1)\npd.DataFrame(X)","1db568f7":"X = X[:, 1:]\npd.DataFrame(X)","8d8d3184":"Y = Employee.iloc[:, 26].values\nlabel_encoder1 = LabelEncoder()\nY =  label_encoder1.fit_transform(Y)\nY","bed3d565":"#Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX  = sc_x.fit_transform(X)\npd.DataFrame(X)","eeb0b2df":"#Train Test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state =0 )","49792c63":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","3b499a78":"#SVC\nfrom sklearn.svm import SVC\nclassifier_svc = SVC(kernel='rbf', random_state = 0)\nclassifier_svc.fit(X_train, y_train)\ny_pred = classifier_svc.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm","7cb1382a":"#Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier()\nclassifier_dt.fit(X_train, y_train)\ny_pred = classifier_dt.predict(X_test)\ncm_dt = confusion_matrix(y_test, y_pred)\ncm_dt","fa68b143":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators=50, criterion='gini')\nclassifier_rf.fit(X_train, y_train)\ny_pred = classifier_rf.predict(X_test)\ncm_rf = confusion_matrix(y_test, y_pred)\ncm_rf","91e124cc":"#K Fold Cross Validation for logistic Regression\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier, X=X_train, y = y_train, cv = 10)\naccuracies.mean()","7818ba06":"#K Fold Cross Validation for svc\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier_svc, X=X_train, y = y_train, cv = 10)\naccuracies.mean()","da04378d":"#K Fold Cross Validation for descision tree\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier_dt, X=X_train, y = y_train, cv = 10)\naccuracies.mean()","96ba6cfb":"#K Fold Cross Validation for random forest\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier_rf, X=X_train, y = y_train, cv = 10)\naccuracies.mean()","b9d32200":"#Model tuning for Logistic Regression\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{ 'C' : [ 0.5, 1, 5 , 10], 'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag' ],'penalty' : ['l2']},\n               {'C' : [ 0.5, 1, 5 , 10], 'solver' : [ 'saga'],'penalty' : ['elasticnet'], 'l1_ratio' : [0.1, 0.2, 0.3]}\n              ]\ngrid_search = GridSearchCV(estimator=classifier, param_grid = parameters, scoring = 'accuracy', cv=10)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameter = grid_search.best_params_\nprint(best_accuracy)\nprint(best_parameter)","cd8b99d3":"#Model Tuning for SVC\nfrom sklearn.model_selection import GridSearchCV\nparameters = [\n               {'C' : [ 0.5, 1, 5 ], 'gamma' : [0.1, 0.2],'kernel' : ['poly',  'sigmoid'], 'coef0' : [0.05, 0.1]},\n                 {'C' : [ 0.5, 1, 5 ], 'gamma' : [0.1, 0.2],'kernel' : [ 'rbf']}\n              ]\ngrid_search = GridSearchCV(estimator=classifier_svc, param_grid = parameters, scoring = 'accuracy', cv=10)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameter = grid_search.best_params_\nprint(best_accuracy)\nprint(best_parameter)","d9cd0f88":"#Model Tuning for Decision Tree\nfrom sklearn.model_selection import GridSearchCV\nparameters = [\n               { 'criterion' : ['gini', 'entropy'], }\n              ]\ngrid_search = GridSearchCV(estimator=classifier_dt, param_grid = parameters, scoring = 'accuracy', cv=10)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameter = grid_search.best_params_\nprint(best_accuracy)\nprint(best_parameter)","f7da5442":"#Model Tuning for Random Forest\nfrom sklearn.model_selection import GridSearchCV\nparameters = [\n               {'n_estimators' : [ 10, 100, 200 ], 'criterion' : ['gini', 'entropy']}\n              ]\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid = parameters, scoring = 'accuracy', cv=10)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameter = grid_search.best_params_\nprint(best_accuracy)\nprint(best_parameter)","01291d4f":"classifier_rf = RandomForestClassifier(n_estimators=200, criterion='gini')\nclassifier_rf.fit(X_train, y_train)\ny_pred = classifier_rf.predict(X_test)\ncm_rf = confusion_matrix(y_test, y_pred)\ncm_rf","0318d979":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))","d405d315":"We are having null values in employee_df and general_df, lets see how to handel them.","5086aebe":"**Checking employee_df**","2c47b7ba":"**Checking general_df**","62e2e244":"**Running the model with best parameters**","be19f084":"From the above tho plots **pairplot and heatmap**, it can be inferred that there are some variables which are co related to each other, however most of the variables are independent and not corellated.\n\nNow further we will be analyzing the categorical variable through count plot, For this we have created a list **feature_cat** having the required categorical variable, Lets see how the plot comes out and what can be inferred from this.","cf8b551e":"* **To be dropped** - EmployeeCount , Over18 , StandardHours\n* **To be Encoded **- Attrition , BusinessTravel , Department , EducationField , Gender , JobRole , MaritalStatus, StockOptionLevel","9f5c48d4":"From the above graph it seems like company need to check its policy on travelling as almost 83% people who travels has a tendency to leave\nAmong them frequent travellers has a higher possibility of leaving the company.","ba6b57e1":"Married people havbe 50% chances of leaving the company.","f4125e4a":"# Data Cleaning","a480177b":"**Checking manager_df**","18b78932":"As obeserved earlier people from HR background has high tendency to leave.","d49c8afa":"**Final Analysis**\n* Company should check its policy wrt HR Department as attrition rate is almost 50% there.\n* Frequent travelers are most likely to leave hence travelling compensation should be revised to stop the attrition.\n* While recruting people it should be kept in mind that the people for whom it will be the second company are most likely to leave.\n* Policy regarding promotion should be reviewed and promotion should not be kept pending for 4+ years\n","b3013eb6":"Now based on the data provided by the company lets develope a Machine Learning model to predict the attrition.","ef5f4eb0":"* Since both the parameter are very important in respect of attrition and we have no way to guess these values so its better to drop the data corresponding to null values as there are a toatl of 27 null values in 4410 employee.\n* We will drop thes values after merging all the dataframe\n\n* Since we have to predict the probability of an employee leaving or staying thus we need to consolidate all data in a single dataframe\n\n* Lets start with in_time and out_time\n\n* We will find the diff of time between in and out time of the day and then find the average over time and rate the employee on basis of the average time","2089c1fe":"**Evaluating the Model Performance**","c4f207ae":"Now to futher we will pe plotting pie chart to deeply analyse above points.","3c2d5647":"Since Only two colums NumCompaniesWorked and TotalWorkingYears have null values, lets analyse these one at a time","78028dfa":"**We have 5 CSV's given, Their details are as below:**\n* **employee_survey_data.csv** - having detail of survey taken by employess\n* **manager_survey_data.csv** - having details of ratings given by manager to employess\n* **general_data.csv** - having general details regarding employess\n* **in_time.csv** - having details of in time of employess over a year\n* **out_time.csv** - having details of out time of employess over a year","87de6052":"**cleaning general_df**","6187c5da":"**parameter tuning using Grid Search**","e6bc5ba3":"# Machine Learning Model","ba7b2129":"# Exploratory Data Analysis","9af3ef7b":"From this plot we can see that for different categories what are the number of people who stayed and who left.\nObservetions are as follows:\n* Employess those travel very frequently are most likely to leave.\n* A lot of people from R&D department has left there can be some issue in this department, which need to be analysed.\n* People from HR department also leaves the company more frequently.\n* Single Employess has a high tendency to leave.\n* Employess for whom this is second company are most likely to leave.\n* If an employee promotion is pending from almost 6-7 years, he is likely to leave.","22ef16a0":"From the above violin plots it can be seen the frquency distribution for attrition among male and female for various parameters.","5ff74fc9":"Now we will be analysing 4 parameters **(Age, YearsAtCompany, MonthlyIncome, NumCompaniesWorked)**\nwrt **BusinessTravel, EducationField and Department**","27b61a1c":"**cleaning employee_df**"}}