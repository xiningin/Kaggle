{"cell_type":{"c2c98069":"code","708ba41c":"code","f7369e12":"code","efaff2d2":"code","27a56541":"code","501c28d4":"code","b5a7f63b":"code","fc71cdf5":"code","1b5d141a":"code","b86dbd14":"code","bc39a21d":"code","5ee03865":"code","f521325e":"code","7e6ee9b7":"code","49d887bc":"code","f809096e":"code","b6cb3615":"code","77d5004e":"code","dcc0321f":"code","d692be44":"code","851e6c83":"code","b0286520":"code","4d0b997e":"code","1084c8ab":"code","cb804f64":"code","8b92d8d4":"code","914b5976":"code","25a9f6ca":"code","4ea8fa4b":"code","65c46911":"code","1eb5ef93":"code","62bce245":"code","316799e3":"code","667a2a8c":"code","9803148e":"code","3da849fb":"code","c1311432":"code","67f52aa4":"code","a3c74855":"code","9383b5ca":"code","beac4565":"code","9d370c41":"code","8fcc69a2":"code","7ce05d39":"code","d1dda79b":"code","f9fd901c":"code","5888a1de":"code","58435888":"code","44f09f6f":"code","9d9e9c1b":"code","817344ad":"code","8fa634c3":"code","d63ff556":"code","4596a17a":"code","4ae78af5":"code","552cc014":"code","85772de2":"code","12921e01":"code","0087da2f":"code","4f290638":"code","65ced7f1":"code","636471ab":"code","89272d67":"code","f728d60f":"code","937d5f26":"code","27de4c6c":"code","80561c9b":"code","fd335a91":"code","4b554200":"code","f5c35c08":"code","2f335fae":"code","e5d9cf7d":"code","bf835bc3":"code","7f7fd323":"code","b41ef46e":"code","fe1087d0":"code","7bc9e532":"code","1c23d136":"code","e051068b":"code","86d0ce40":"code","553b8ac6":"markdown","a31fa216":"markdown","8357f01a":"markdown","be701e8c":"markdown","98cd52ef":"markdown","5eebfb45":"markdown","b43f58a7":"markdown","0f40800f":"markdown","d757e10d":"markdown","799b5f73":"markdown","24bb38b5":"markdown","2cc20c14":"markdown","f5c9005f":"markdown","bc6b718a":"markdown","f61b46d5":"markdown","457785ff":"markdown","8d5ed4ac":"markdown","e8499d12":"markdown","283a8c25":"markdown","7bcb53e9":"markdown","7668aecb":"markdown","e2c7f0f7":"markdown","c85d72e9":"markdown","7cc6982b":"markdown","071a8f42":"markdown","477069ae":"markdown","c0d279f0":"markdown","fa35c800":"markdown","9fd3c0ee":"markdown","31462f4e":"markdown","8eea37dd":"markdown","77caf2c9":"markdown","97c76c7a":"markdown","bf387c48":"markdown","bb9294c1":"markdown","fbe9ce6a":"markdown"},"source":{"c2c98069":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","708ba41c":"import pandas as pd","f7369e12":"#Let's see the training set\ntrain = pd.read_csv('..\/input\/c\/titanic\/train.csv')","efaff2d2":"train","27a56541":"X_train = train.drop(['Survived'], axis = 1)","501c28d4":"y_train = train['Survived']","b5a7f63b":"X_test = pd.read_csv('..\/input\/c\/titanic\/test.csv')","fc71cdf5":"train.info()","1b5d141a":"X_train['Sex'].value_counts()","b86dbd14":"X_train['Ticket'].value_counts()","bc39a21d":"X_train['Cabin'].value_counts()","5ee03865":"X_train['Embarked'].value_counts()","f521325e":"X_train['Pclass'].value_counts()","7e6ee9b7":"X_train['SibSp'].value_counts()","49d887bc":"X_train['Parch'].value_counts()","f809096e":"train.describe()","b6cb3615":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain.hist(bins = 50, figsize= (20,15))\nplt.show()","77d5004e":"X_train = X_train.drop(['PassengerId','Cabin','Name','Ticket'], axis = 1)","dcc0321f":"X_train","d692be44":"X_train['Family'] = X_train['SibSp']+X_train['Parch']+1\nX_test['Family'] = X_test['SibSp']+X_test['Parch']+1","851e6c83":"from sklearn.base import BaseEstimator, TransformerMixin","b0286520":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","4d0b997e":"categorical_columns = ['Pclass', 'Embarked']\nbincat_columns = ['SibSp', 'Sex', 'Parch']\nnumerical_columns = ['Age', 'Fare']","1084c8ab":"class binary_cat(BaseEstimator, TransformerMixin):\n    \"\"\"\n    It converts the SibSp, Parch and Sex attributes into binary categories\n    \"\"\"\n    def fit(self, X,y=None):\n        return self\n    def transform(self, X):\n        X.loc[X.SibSp > 0, 'SibSp'] = 1\n        X.loc[X.Parch > 0, 'Parch'] = 1\n        X['Sex'].replace(['male','female'], [1,0],inplace=True)\n        return X[bincat_columns]","cb804f64":"bin_cat = binary_cat()","8b92d8d4":"X_train.dtypes","914b5976":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"Age\", \"Fare\",\"Family\"])),\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler())\n    ])","25a9f6ca":"num_pipeline.fit_transform(X_train)","4ea8fa4b":"from sklearn.preprocessing import OneHotEncoder\n\ncategorical_encoder = OneHotEncoder(handle_unknown='ignore')","65c46911":"class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","1eb5ef93":"cat_pipeline = Pipeline([\n        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Embarked\"])),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])","62bce245":"cat_pipeline.fit_transform(X_train)","316799e3":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n        (\"bincat\", bin_cat)\n    ])","667a2a8c":"X_train = preprocess_pipeline.fit_transform(X_train)\nX_train","9803148e":"X_train[0]","3da849fb":"y_train","c1311432":"from sklearn.model_selection import train_test_split","67f52aa4":"xtrain, X_val, ytrain, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify = y_train)","a3c74855":"from sklearn.linear_model import LogisticRegression","9383b5ca":"log_clf = LogisticRegression(max_iter = 300)\n\nlog_clf.fit(xtrain, ytrain)","beac4565":"y_pred = log_clf.predict(X_val)","9d370c41":"from sklearn.metrics import accuracy_score","8fcc69a2":"accuracy_score(y_val, y_pred)","7ce05d39":"log_clf = LogisticRegression(max_iter = 300)\n\nlog_clf.fit(X_train,y_train)","d1dda79b":"from sklearn.model_selection import cross_val_score\n\nsvm_scores = cross_val_score(log_clf, X_train, y_train, cv=10)\nsvm_scores.mean()","f9fd901c":"from sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(log_clf, X_train, y_train, cv=3)","5888a1de":"from sklearn.metrics import confusion_matrix\n\nconf_mt = confusion_matrix(y_train, y_train_pred)\nconf_mt","58435888":"plt.matshow(conf_mt, cmap=plt.cm.gray)\nplt.show()","44f09f6f":"from sklearn.metrics import precision_score, recall_score\n\nprecision_score(y_train, y_train_pred)","9d9e9c1b":"recall_score(y_train, y_train_pred)","817344ad":"y_scores = cross_val_predict(log_clf, X_train, y_train, cv=3,\n                             method=\"decision_function\")","8fa634c3":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)","d63ff556":"import numpy as np","4596a17a":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n    plt.xlabel(\"Threshold\", fontsize=16)        \n    plt.grid(True)                          \n\nrecall_90_precision = recalls[np.argmax(precisions >= 0.90)]\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n\nplt.figure(figsize=(8, 4))                                                                  \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.axis([-4, 4, 0, 1])\nplt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")             \nplt.plot([-4, threshold_90_precision], [0.9, 0.9], \"r:\")                                \nplt.plot([-4, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\nplt.plot([threshold_90_precision], [0.9], \"ro\")                                         \nplt.plot([threshold_90_precision], [recall_90_precision], \"ro\")         \nplt.show()","4ae78af5":"from sklearn.metrics import f1_score","552cc014":"def prediction_by_threshold(X,k):\n    df = log_clf.decision_function(X)\n    \n    # Set the value of decision threshold.\n    decision_threshold = k\n\n    # Desired prediction to increase precision value.\n    desired_predict =[]\n\n    # Iterate through each value of decision function output\n    # and if  decision score is > than Decision threshold then,\n    # append (1) to the empty list ( desired_prediction) else\n    # append (0).\n    for i in df:\n        if i<decision_threshold:\n            desired_predict.append(0)\n        else:\n            desired_predict.append(1)\n            \n    return desired_predict","85772de2":"f1_score1 = 0\nfor i in np.linspace(-4,4,800):\n    y_pred = prediction_by_threshold(X_val, i)\n    l = f1_score(y_val, y_pred)\n    if l > f1_score1:\n        f1_score1 = l\n        m = i\nprint(f1_score1, m)","12921e01":"y_pred = prediction_by_threshold(X_val, m)\n\nprint('Accuracy Score :',accuracy_score(y_val, y_pred),\n      '\\nPrecision Score :', precision_score(y_val, y_pred),\n      '\\nRecall Score :', recall_score(y_val, y_pred),\n      '\\nF1 Score:', f1_score(y_val, y_pred))","0087da2f":"X_test = pd.read_csv(\"..\/input\/c\/titanic\/test.csv\")\nX_test['Family'] = X_test['SibSp']+X_test['Parch']+1\nX_test1 = preprocess_pipeline.fit_transform(X_test)","4f290638":"log_clf_y_pred = prediction_by_threshold(X_test1, m)","65ced7f1":"my_submission = pd.DataFrame({'PassengerId': X_test.PassengerId, 'Survived': log_clf_y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('log_clf_submission.csv', index=False)","636471ab":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train, y_scores)","89272d67":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n    plt.axis([0, 1, 0, 1])                                    \n    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n    plt.ylabel('True Positive Rate (Recall)', fontsize=16)  \n    plt.grid(True)                                          \n\nplt.figure(figsize=(8, 6))                                    \nplot_roc_curve(fpr, tpr)                                              \nplt.show()","f728d60f":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_train, y_scores)","937d5f26":"from sklearn.ensemble import RandomForestClassifier","27de4c6c":"forest_clf = RandomForestClassifier()\n\nforest_clf.fit(xtrain, ytrain)","80561c9b":"forest_y_pred = forest_clf.predict(X_val)\n\naccuracy_score(y_val, forest_y_pred)","fd335a91":"y_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3)\n\nconf_mt = confusion_matrix(y_train, y_train_pred)\nconf_mt","4b554200":"plt.matshow(conf_mt, cmap=plt.cm.gray)\nplt.show()","f5c35c08":"y_probas_forest = cross_val_predict(forest_clf, X_train, y_train, cv=3,\n                                    method=\"predict_proba\")","2f335fae":"y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train,y_scores_forest)","e5d9cf7d":"plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"Logistic Regression\")\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\nplt.grid(True)\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","bf835bc3":"roc_auc_score(y_train, y_scores_forest)","7f7fd323":"precision_score(y_train, y_train_pred)","b41ef46e":"recall_score(y_train, y_train_pred)","fe1087d0":"f1_score(y_train, y_train_pred)","7bc9e532":"X_test = pd.read_csv('..\/input\/c\/titanic\/test.csv')\nX_test['Family'] = X_test['SibSp'] + X_test['Parch'] + 1\nX_test1 = preprocess_pipeline.fit_transform(X_test)","1c23d136":"X_test1","e051068b":"forest_y_test_pred = forest_clf.predict(X_test1)","86d0ce40":"my_submission = pd.DataFrame({'PassengerId': X_test.PassengerId, 'Survived': forest_y_test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","553b8ac6":"By observing the plots, I have made few decisions:\n\n- We can drop __PassengerId__ from our model as survival does not seem to depend on it by it's consistent patterns which is probably because the bins contain odd numbers which is why either the value is 17.5 or it is 16.5.\n\n- __Parch__ categorical attribute will get converted into either have parents or children (1) or not (0).\n\n- __SibSp__ categorical attribute will get converted into either have siblings\/spouse (1) or not (0).\n\n- __Pclass__ we may think of converting this into either lower class or not. But right now, haven't decided.","a31fa216":"Again, so many categories!!","8357f01a":"Let's create a dataframe selector which we can use while apply some specific preprocessing steps on only the selected data attributes.","be701e8c":"But one disadvantage is that if we run the transformation over categorical_columns, then all the columns will get that processing, which in some cases we would not want to do, but still they are useful, so we'll keep them.","98cd52ef":"### Processing Categorical Attributes","5eebfb45":"At -0.60 threshold, we are getting precision = recall\n\nLet's try our luck on test set of the competition.","b43f58a7":"81.56% It's better than our previous model. But would we get this on our test test, that's the question.","0f40800f":"### Exploring Data through visualizations","d757e10d":"__Port of Embarkation__: C = Cherbourg, Q = Queenstown, S = Southampton","799b5f73":"### Error Analysis for Logistic Regression Model","24bb38b5":"# Titanic Survival Prediction","2cc20c14":"Using a validation set for evaluation gets us 80.4% accuracy.\n\nNow, using sklearn k-fold cross validation method for evaluation.","f5c9005f":"### Error Analysis for Random Forest Classifier","bc6b718a":"## Loading the Data","f61b46d5":"Area under the curve of Random Forest Classifier is better than Logistic Regression but just a little bit.","457785ff":"- __PassengerId__ does tell anything interesting.\n- __Survived__'s mean is 0.38 (approx.) meaning that only 38% survived!\n- __Pclass__'s mean is around 2.3 meaning majority of the people were middle of lower class. But by looking at the distribution of Pclass we found that 55% of the people belong to lower class while upper and middle class are close in numbers.\n- __Age__'s mean is around 29 with std deviation around 14.5, min is 0.4 and max is 80. median is 28 and 25% of the people are less than 20 years old, and 75% of the people are less than 38 years old. (We should probably plot a histogram also).\n- __SibSp__ 52% have siblings\/ spouse. max is 8. Could be converted into feature telling if passenger have sibling\/spouse or not because 68% of the people don't have them in the ship.\n- __Parch__ 38% have parents\/children. max is 6. Could be converted into feature telling if passenger have sibling\/spouse or not because 76% of the people don't have them in the ship.\n- __Fare__ mean is 32 (approx.). 75% of the people pay 31 or less while max is 512! (A Histogram would be great representation).","8d5ed4ac":"### RandomForestClassifier","e8499d12":"## Data Preprocessing\n\nOkay, so we have work to do!\n\n1. Drop __PassengerId__ and __Cabin__ from the dataset.\n2. Use One-hot Encoding on __Pclass__.\n3. Convert __Sex__ into categorical attribute.\n4. Impute __Age__ with median values.\n5. Feature scale the __SibSp__ and __Parch__.\n6. Add a column named __Family__.\n7. Impute __Embarked__ with most frequent class.\n8. Drop __Ticket__ and __Name__ for now as they are little more complicated.\n\nLet's create our data preprocessing Pipelines!","283a8c25":"Don't worry about different libraries, best way to understand their usage is to go through their documentation, so I recommend you to keep the Scikit-Learn documentation handy.","7bcb53e9":"__Age, Cabin and Embarked__ have missing values, we need to take care of them.\n\nWe can use median strategy imputer on age and for embarked the most frequent one. But Cabin has so much missing data over 77%, so it's better to drop it.\n\nLet's understand the categorical values in the data.","7668aecb":"We are getting 80.13% accuracy, Can we do better?\n\nYes, we can try out different values of parameters in our model to see which one gives best result on our data.","e2c7f0f7":"## Applying a Machine Learning Model on our processed data\n\nLet's try a simple Logistic Regression model, which I think is great binary classifier using sigmoid function to classify. Simple and effective!","c85d72e9":"So many categories!!","7cc6982b":"So, we are getting around 75.1% on the Kaggle test set, though we are getting 81% on our validation set. We may have to get more training data to perform better on test set which is not possible.\n\nEven setting a classification threshold we are doing not so great on the test set.\n\nLet's plot ROC Curve and explore some more on our error analysis.","071a8f42":"## Understanding the Data","477069ae":"So, the precision is little bumpier than recall curve, as we go up the threhold, the precision gets bumpier while the recall is consistent with it's going down trend. At the threshold where precision and recall are equal, the F1 score will be highest. \n\nTo get an understanding of Classification threshold, do check the link: https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/thresholding","c0d279f0":"Though we are getting over 80% accuracy score but we aren't doing good on precision and recall metrics. And for classification, these metrics provide more picture than only accuracy score.\n\nWe are also getting 70 false positives and 103 false negatives on our logistic regression model.\n\nWe need to know why.","fa35c800":"### Combining all the processed columns","9fd3c0ee":"__Ticket Class__: 1 = 1st, 2 = 2nd, 3 = 3rd","31462f4e":"Another way to do this same thing will be to specify initially which are the numerical and categorical columns.","8eea37dd":"So, the precision and recall are pretty close, and still we are doing just a little better than Logistic Regression.","77caf2c9":"A good classifier stays as far away from that line as possible (toward the top-left corner), which is obviously our model is not achieving.","97c76c7a":"Let's split this data set into train and test first as in previous scenario, I was getting 84% cross_val_score here but only 77% on test set after Kaggle submission. So, I will try to tune the each model's parameter using Grid Search now and see if I can do any better.\n\nIf you have any ideas to better my model, do let me know in the comments and please upvote my work, thanks!","bf387c48":"At -0.6057, we are getting the maximum f1 score possible now.","bb9294c1":"### Processing Numerical Attributes","fbe9ce6a":"The attributes have the following meaning:\n\n- **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.\n- **Pclass**: passenger class.\n- __Name, Sex, Age__: self-explanatory\n- __SibSp__: how many siblings & spouses of the passenger aboard the Titanic.\n- __Parch__: how many children & parents of the passenger aboard the Titanic.\n- __Ticket__: ticket id\n- __Fare__: price paid (in pounds)\n- __Cabin__: passenger's cabin number\n- __Embarked__: where the passenger embarked the Titanic"}}