{"cell_type":{"7322f4fa":"code","a22cd2bb":"code","dadb7c9d":"code","380a816a":"code","6f4091dd":"code","d62a3b2a":"code","a1227a35":"code","a71c1b9b":"code","64ff2bca":"code","d74230a2":"code","81b3dd96":"code","27b5b0bc":"code","c272d5b8":"code","bfd5b628":"code","4898601b":"code","d364fa1d":"code","010fa06d":"code","87549c05":"code","8d0403a6":"code","9a0e44f9":"code","eb37d18b":"code","bb00d35b":"code","3bf44a0c":"code","8c07f421":"code","5f003cb3":"code","d03c04b8":"code","a8d1b066":"code","28966adc":"code","c0981e17":"code","36d65495":"code","c6b2fc4a":"code","15901a41":"code","4d5cf887":"code","d89b71a4":"code","2e223712":"code","b6d45a2a":"markdown","b14dc5a9":"markdown","a6aa9ff4":"markdown","7cd0f445":"markdown","fad9997c":"markdown","ec99db47":"markdown","cd5bb27c":"markdown","12cb11b2":"markdown","12aaef9f":"markdown","3dac7540":"markdown","75e2b56d":"markdown","78fe7b4b":"markdown","0c891811":"markdown","48746edc":"markdown","5cd34e50":"markdown","9ff24191":"markdown","b987c999":"markdown","a379c04b":"markdown","cc3bd9cb":"markdown","c3346479":"markdown","86f4cacb":"markdown","fda04aa8":"markdown","fb9407c7":"markdown"},"source":{"7322f4fa":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\npd.options.display.max_colwidth=300","a22cd2bb":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(10))","dadb7c9d":"\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']\/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","380a816a":"df['y'].value_counts()","6f4091dd":"n_folds = 2\n\nfrac_1 = 0.7\nfrac_1_factor = 1.5\n\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'\/kaggle\/working\/df_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","d62a3b2a":"def clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    \n    return data","a1227a35":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how\/are\/you ???\",\n                               \"hey?????\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","a71c1b9b":"df = clean(df,'text')","64ff2bca":"n_folds = 2\n\nfrac_1 = 0.7\nfrac_1_factor = 1.5\n\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'\/kaggle\/working\/df_clean_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","d74230a2":"del df,tmp_df\ngc.collect()","81b3dd96":"df_ = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nprint(df_.shape)\n\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})\n\ndf_['y'] = (df_['y'] - df_.y.min()) \/ (df_.y.max() - df_.y.min()) \ndf_.y.hist()","27b5b0bc":"n_folds = 2\n\nfrac_1 = 0.7\n\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))\n    tmp_df.to_csv(f'\/kaggle\/working\/df2_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","c272d5b8":"del tmp_df, df_; \ngc.collect()","bfd5b628":"# Validation data \n\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")","4898601b":"# Test data\n\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n","d364fa1d":"# NOT USED \nclass LengthTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[(len(x)-360)\/550] for x in X])\n    def get_feature_names(self):\n        return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[sum([1 for y in x if y.isupper()])\/len(x)] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","010fa06d":"\ndf_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1,1)\ndf_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1,1)\n\nprint(df_val['upper_1'].mean(), df_val['upper_1'].std())\nprint(df_val['upper_2'].mean(), df_val['upper_2'].std())\n\ndf_val['upper_1'].hist(bins=100)\ndf_val['upper_2'].hist(bins=100)","87549c05":"val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])","8d0403a6":"val_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arrc = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df_clean_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1c[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2c[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arrc[:,fld] = pipeline.predict(df_sub['text'])","9a0e44f9":"val_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df2_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1_[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2_[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr_[:,fld] = pipeline.predict(df_sub['text'])","eb37d18b":"del df, pipeline, feature_wts\ngc.collect()","bb00d35b":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n","3bf44a0c":"print(\"Find right weight\")\n\nwts_acc = []\nfor i in range(30,70,1):\n    for j in range(0,20,1):\n        w1 = i\/100\n        w2 = (100 - i - j)\/100\n        w3 = (1 - w1 - w2 )\n        p1_wt = w1*p1 + w2*p3 + w3*p5\n        p2_wt = w1*p2 + w2*p4 + w3*p6\n        wts_acc.append( (w1,w2,w3, \n                         np.round((p1_wt < p2_wt).mean() * 100,2))\n                      )\nsorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]","8c07f421":"w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n#print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5\np2_wt = w1*p2 + w2*p4 + w3*p6\n","5f003cb3":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n","d03c04b8":"\n### Incorrect predictions with similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)","a8d1b066":"### Incorrect predictions with dis-similar scores\n\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","28966adc":"# Predict using pipeline\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + w2*test_preds_arr_.mean(axis=1) + w3*test_preds_arrc.mean(axis=1)","c0981e17":"#test_preds_arr","36d65495":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","c6b2fc4a":"same_score = df_sub['score'].value_counts().reset_index()[:10]\nsame_score","15901a41":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","4d5cf887":"# Same comments have same score - which is ok ","d89b71a4":"# # Rank the predictions \n\n# df_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\n# print(df_sub['score'].rank().nunique())","2e223712":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","b6d45a2a":"The best version of this notebook is version 3 with 0.843 score.","b14dc5a9":"## Built on top of the amazing notebook here : \nhttps:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n","a6aa9ff4":"### Toxic data","7cd0f445":"## Do upvote if you copy","fad9997c":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","ec99db47":"#### Some of these just look incorrectly tagged \n","cd5bb27c":"### Does % of uppercase characters have effect on toxicity\n","12cb11b2":"# Create 3 versions of data","12aaef9f":"## Load Validation and Test data  \n","3dac7540":"# Create 3 versions of __clean__ data","75e2b56d":"## Ruddit data","78fe7b4b":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data","0c891811":"## Create 3 versions of the data","48746edc":"# Validate the pipeline ","5cd34e50":"## Ruddit data pipeline","9ff24191":"# Predict on test data ","b987c999":"# Imports","a379c04b":"## 0.82+ score by ensemble of simple TF-Idf and Ridge regression\n\n### Ensemble of TfIdf - Ridge models using data from \n- Toxic competition\n- Ruddit toxic data\n\n### Analysis of bad predictions\n","cc3bd9cb":"# Toxic __clean__ data","c3346479":"## Correct the rank ordering","86f4cacb":"# Create Sklearn Pipeline with \n## TFIDF - Take 'char_wb' as analyzer to capture subwords well\n## Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","fda04aa8":"**This notebook is a fork of https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-842?scriptVersionId=80165344**","fb9407c7":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)"}}