{"cell_type":{"e619db93":"code","e4b9b7f4":"code","018d347f":"code","a12fe753":"code","7ee525c3":"code","5d91e75f":"code","89ce7412":"code","280ec795":"code","33894b30":"code","83072010":"code","3a3ae8bb":"code","9b1df119":"code","963106ec":"code","e63db4b9":"code","e4090dd8":"code","8d893aee":"code","4fff433b":"code","317b0e87":"code","6e614b46":"code","368f6cd9":"code","cc12e17f":"code","55fb825d":"code","37e5c8cd":"code","0c144552":"code","a7947574":"code","b930ca0b":"code","635279c2":"code","c29fe579":"code","5d30c0c7":"code","852008e0":"code","28f4c7b0":"code","83b33eed":"code","7e1aa403":"code","efd0e3b7":"code","c027678a":"code","7853a1c3":"code","b8e600eb":"code","b72c8be5":"code","1ae58cfa":"code","522695ee":"code","2e84ec36":"markdown","456ac7ed":"markdown","a763de6c":"markdown","15a149b6":"markdown","753b490c":"markdown","10e2e7a3":"markdown","f3edabbf":"markdown","c85d0e9c":"markdown","b6e386cb":"markdown","bdf06dd4":"markdown","52700b59":"markdown","6f4b707f":"markdown","6ab98ee7":"markdown","fbc0df49":"markdown","6e336d6e":"markdown","4b200631":"markdown","b3682dd3":"markdown","73d1be3b":"markdown","d3cc46cb":"markdown","82f86e1a":"markdown","33b3c8fa":"markdown","381e63f7":"markdown","7cd7acec":"markdown","cce0402d":"markdown","3b08053a":"markdown","2d031d77":"markdown","8c8dca10":"markdown","e1696da4":"markdown","a6c08c04":"markdown","55de50d6":"markdown","7f78f21b":"markdown","d4e8c4a9":"markdown","0cb67430":"markdown","8b3e30ac":"markdown","a7bfa900":"markdown","3b3de35a":"markdown","ad435df6":"markdown","6606857d":"markdown","587f8204":"markdown","c0cd9d8a":"markdown","a31974a2":"markdown"},"source":{"e619db93":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, fbeta_score, roc_auc_score, make_scorer\n\nfrom xgboost import XGBClassifier, plot_importance\n\n# Pretty display for notebooks\n%matplotlib inline\n\nsns.set_palette(\"Set2\")","e4b9b7f4":"# Load the Census dataset\ndf_train = pd.read_csv(\"..\/input\/udacity-mlcharity-competition\/census.csv\")\n\n# Total number of records\nn_records = df_train.shape[0]\n\n# Print the results\nprint(\"Total number of records in training set: {}\".format(n_records))","018d347f":"# Load the Census dataset\ndf_test = pd.read_csv(\"..\/input\/udacity-mlcharity-competition\/test_census.csv\")\n\n# Total number of records\nn_records = df_test.shape[0]\n\n# Print the results\nprint(\"Total number of records in testing set: {}\".format(n_records))","a12fe753":"data = [df_train, df_test]","7ee525c3":"df_train.head()","5d91e75f":"df_train.describe()","89ce7412":"plt.figure(figsize=[10, 5])\n\nfig = sns.countplot(data=df_train, x=\"income\")\n\nplt.title(\"Distribution of Income Class\")\nplt.xlabel(\"Income Class\")\nplt.ylabel(\"Number of Records\")\nplt.ylim(0, 40000)\n\nincome_means = df_train[df_train.income == '<=50K'].shape[0] \/ df_train.shape[0], df_train[df_train.income == '>50K'].shape[0] \/ df_train.shape[0]\ni = 0\n\nfor bar in fig.patches:\n    fig.annotate(\"{} ({:.2f}%)\".format(bar.get_height(), 100 * income_means[i]), \n                 (bar.get_x() + bar.get_width() \/ 2., bar.get_height()), \n                 ha='center', \n                 va='center',\n                 xytext=(0, 10),\n                 textcoords = 'offset points',\n                 fontsize=12,\n                 fontweight='bold')\n    i += 1","280ec795":"# Number of records where individual's income is more than $50,000\nn_greater_50k = df_train[df_train['income'] == '>50K'].shape[0]\n\n# Number of records where individual's income is at most $50,000\nn_at_most_50k = df_train[df_train['income'] == '<=50K'].shape[0]\n\n# Percentage of individuals whose income is more than $50,000\ngreater_percent = (n_greater_50k \/ n_records) * 100\n\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent))","33894b30":"# Encode the 'income' data to numerical values\ndf_train.income = df_train.income.replace(to_replace=['<=50K', '>50K'], value=[0, 1])","83072010":"# get the education_level indexes by income average\neduc_levels = df_train.groupby('education_level')['education-num'].mean().sort_values(ascending=False).index.tolist()\n\n# grouping by education-num and calculating the income average\ndf_temp = df_train.groupby('education-num').income.mean().sort_index(ascending=False)\n\n# get education-num indexes\neduc_nums = df_temp.index\n\n# get income average\nincome_means = df_temp.values\n\n# creating a dataframe\ndf_temp = pd.DataFrame({'education_level': educ_levels, 'education-num': educ_nums, 'income average': income_means})\n\n# sorting by education-num\ndf_temp = df_temp.sort_values('education-num', ascending=False)\ndf_temp","3a3ae8bb":"plt.figure(figsize=[10, 7])\n\n# formating labels\nlabels = [\"{} ({})\".format(level, int(num)) for level, num in zip(df_temp['education_level'].values, df_temp['education-num'].values)]\n\nsns.barplot(data=df_temp, x='income average', y='education_level', color=sns.color_palette()[0])\n\nplt.ylabel('Education level and completed years of study')\nplt.xlabel('% of people making more than 50k annually')\nplt.title('Average of people that makes more than 50k annually in each education level')\n\nplt.xlim(0, 1)\nplt.yticks(np.arange(0, 16, 1), labels)\nplt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], [\"0%\", \"20%\", \"40%\", \"60%\", \"80%\", \"100%\"]);","9b1df119":"plt.figure(figsize=[10, 5])\n\nfig = sns.barplot(data=df_train.groupby('income')['hours-per-week'].mean().reset_index(), x=\"income\", y=\"hours-per-week\")\n\nplt.title(\"Mean hours worked per week of each income class\")\nplt.xlabel(\"Income Class\")\nplt.ylabel(\"Hours worked per week\")\nplt.ylim(0, df_train['hours-per-week'].max())\n\n\nplt.xticks([0, 1], [\"<=50k\", \">50k\"])\n\nfor bar in fig.patches:\n    fig.annotate(\"{:.2f} hours per week\".format(bar.get_height()), \n                 (bar.get_x() + bar.get_width() \/ 2., bar.get_height()), \n                 ha='center', \n                 va='center',\n                 xytext=(0, 10),\n                 textcoords = 'offset points',\n                 fontsize=12,\n                 fontweight='bold')","963106ec":"plt.figure(figsize=[12, 6])\n\nincome_by_hours = df_train.groupby('hours-per-week').income.mean().reset_index()\nincome_by_hours['hours-per-week'] = income_by_hours['hours-per-week'].astype(int)\n\nax = sns.barplot(data=income_by_hours, x='hours-per-week', y=\"income\", palette=[sns.color_palette()[0] if x != 39 else 'red' for x in np.arange(0, 99, 1)],)\nplt.errorbar(y=df_train.income.mean(), x=np.arange(-1, 97, 1), linestyle=\"--\", color=sns.color_palette()[1])\n\nax.margins(0)\n\nplt.tight_layout()\nplt.xticks([0, 9, 19, 29, 39, 49, 59, 69, 79, 89], [1, 10, 20, 30, 40, 50, 60, 70, 80, 90])\nplt.xlabel('Hours worked per week')\nplt.ylabel('% of people making more than 50k annually')\nplt.title('Average of people that makes more than 50k annually by hours worked per week');","e63db4b9":"plt.figure(figsize=[10, 5])\n\nfig = sns.barplot(data=df_train.groupby('income')['age'].mean().reset_index(), x=\"income\", y=\"age\")\n\nplt.title(\"Mean age of each income class\")\nplt.xlabel(\"Income Class\")\nplt.ylabel(\"Age\")\nplt.ylim(0, df_train['age'].max())\n\n\nplt.xticks([0, 1], [\"<=50k\", \">50k\"])\n\nfor bar in fig.patches:\n    fig.annotate(\"{:.2f}\".format(bar.get_height()), \n                 (bar.get_x() + bar.get_width() \/ 2., bar.get_height()), \n                 ha='center', \n                 va='center',\n                 xytext=(0, 10),\n                 textcoords = 'offset points',\n                 fontsize=12,\n                 fontweight='bold')","e4090dd8":"fig = sns.FacetGrid(df_train, hue='income', height=4, aspect=3)\nfig.map(sns.kdeplot, 'age', shade=True)\n\nfig.set(xlim=(0, df_train['age'].max()))\n\nplt.title('Age density curve of each income class')\nplt.xlabel('Age')\nplt.ylabel('Probability density')\nplt.legend(title='Income Class', labels=['<=50K', '>50K'])","8d893aee":"print('Training set')\ndf_train.isna().sum()","4fff433b":"print('Testing set')\ndf_test.isna().sum()","317b0e87":"df_test.drop('Unnamed: 0', axis=1, inplace=True)","6e614b46":"plt.figure(figsize = [13, 6])\nbase_color = sns.color_palette()[0]\n\nlabels = [0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]\nlabels_names = ['0', '250', '500', '750', '1000', '1250', '1500', '1750', '>=2000']\n\nplt.subplot(1, 2, 1)\nplt.hist(data=df_train, x='capital-gain', bins=25)\nplt.ylim(0, 2000)\nplt.ylabel('Number of Records')\nplt.xlabel('Values')\nplt.title('Capital Gain Distribution')\nplt.yticks(labels, labels_names)\n\nplt.subplot(1, 2, 2)\nplt.hist(data=df_train, x='capital-loss', bins=25)\nplt.ylim(0, 2000)\nplt.xlabel('Values')\nplt.title('Capital Loss Distribution')\nplt.yticks(labels, labels_names)\n\nplt.suptitle('Skewed Distributions of Continuous Census Data Features');","368f6cd9":"# Log-transform the skewed features\nskewed = ['capital-gain', 'capital-loss']\n\nfor df in data:\n    df[skewed] = df[skewed].apply(lambda x: np.log(x + 1))","cc12e17f":"plt.figure(figsize = [13, 6])\nbase_color = sns.color_palette()[0]\n\nlabels = [0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]\nlabels_names = ['0', '250', '500', '750', '1000', '1250', '1500', '1750', '>=2000']\n\nplt.subplot(1, 2, 1)\nplt.hist(data=df_train, x='capital-gain', bins=25)\nplt.ylim(0, 2000)\nplt.ylabel('Number of Records')\nplt.xlabel('Values')\nplt.title('Capital Gain Distribution')\nplt.yticks(labels, labels_names)\n\nplt.subplot(1, 2, 2)\nplt.hist(data=df_train, x='capital-loss', bins=25)\nplt.ylim(0, 2000)\nplt.xlabel('Values')\nplt.title('Capital Loss Distribution')\nplt.yticks(labels, labels_names)\n\nplt.suptitle('Log-Transformed Distributions of Continuous Census Data Features');","55fb825d":"for df in data:\n    df.drop('education_level', axis=1, inplace=True)","37e5c8cd":"numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\ndf_test[numerical] = df_test[numerical].transform(lambda x: x.fillna(x.mean()))\n\ncategorical=['workclass', 'marital-status', 'occupation', 'relationship', 'sex', 'race', 'native-country']\nfor cat in categorical:\n    df_test[cat].fillna(df_test[cat].mode()[0], inplace=True)\n\nprint('Testing set')\ndf_test.isna().sum()","0c144552":"for df in data:\n    df.sex = df.sex.replace(to_replace=['Male', 'Female'], value=[1, 0])","a7947574":"# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\ndf_train = pd.get_dummies(data=df_train)\ndf_test = pd.get_dummies(data=df_test)\n\n# Print the number of features after one-hot encoding\nencoded = list(df_train.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))","b930ca0b":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df_train.drop('income', axis=1), \n                                                    df_train.income, \n                                                    test_size = 0.3, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","635279c2":"# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler()\n\nX_train[numerical] = scaler.fit_transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])\ndf_test[numerical] = scaler.transform(df_test[numerical])","c29fe579":"X_train[numerical].describe()","5d30c0c7":"MODELS = [\n    #Ensemble Methods\n    GradientBoostingClassifier(random_state=42),\n    RandomForestClassifier(random_state=42),\n    \n    #GLM\n    RidgeClassifierCV(),\n    \n    #Navies Bayes\n    GaussianNB(),\n    \n    #Nearest Neighbor\n    KNeighborsClassifier(),\n    \n    #Discriminant Analysis\n    LinearDiscriminantAnalysis(),\n\n    #xgboost\n    XGBClassifier(random_state=42)    \n]\n\ncolumns = ['Model Name', 'Train AUC Mean', 'Test AUC Mean', 'Test AUC STD * 3', 'Time']\nmodels = pd.DataFrame(columns=columns)\n\nrow_index = 0\nfor ml in MODELS:\n    model_name = ml.__class__.__name__\n    models.loc[row_index, 'Model Name'] = model_name\n    \n    cv_results = cross_validate(ml, df_train.drop('income', axis=1), df_train.income, scoring=make_scorer(roc_auc_score), cv=5, return_train_score=True, return_estimator=True)\n    \n    models.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n    models.loc[row_index, 'Train AUC Mean'] = cv_results['train_score'].mean()\n    models.loc[row_index, 'Test AUC Mean'] = cv_results['test_score'].mean()\n    models.loc[row_index, 'Test AUC STD * 3'] = cv_results['test_score'].std() * 3\n    \n    row_index+=1\n\nmodels.sort_values(by=['Test AUC Mean'], ascending=False, inplace=True)\nmodels.reset_index(drop=True, inplace=True)\nmodels","852008e0":"plt.figure(figsize=[10, 7])\n\nsns.barplot(x='Test AUC Mean', y='Model Name', data=models, color=sns.color_palette()[0])\n\nplt.title('Machine Learning AUC Scores')\nplt.xlabel('ROC AUC')\nplt.ylabel('Machine Learning Model')\n\nplt.xlim(0, 1);","28f4c7b0":"importances = []\nfor i in range(10):\n    rf = XGBClassifier()\n    rf.fit(df_train.drop('income', axis=1), df_train.income)\n    if len(importances) > 0:\n        importances = [x + y for x, y in zip(importances, rf.feature_importances_)]\n    else:\n        importances = rf.feature_importances_\n\nimportances = [x \/ 10 for x in importances]\nimportances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\nimportances.sort_values('importance', ascending=False, inplace=True)\n\nacc = []\nfor i in importances.importance.values:\n    acc.append(i + acc[-1] if len(acc) > 0 else i)\nimportances['acc'] = acc","83b33eed":"plt.figure(figsize=[15, 6])\n\nsns.barplot(data=importances.loc[:10, :], x='feature', y='importance', color=sns.color_palette()[0])\nplt.plot(importances.loc[:10, 'feature'], importances.loc[:10, 'acc'], '--', color=sns.color_palette()[1])\nplt.ylabel('Importance')\nplt.xlabel('Features')\nplt.title('Accumulative Feature Importances')\nplt.xticks(rotation=90)\nplt.ylim(0, 1)\n\nplt.show()","7e1aa403":"parameters = {\n    'min_child_weight': [1, 5, 10],\n    'gamma': [0.5, 1, 1.5, 2, 5],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'max_depth': [3, 4, 5]\n}\n\nscorer = make_scorer(roc_auc_score)\ngrid_obj = GridSearchCV(estimator=XGBClassifier(random_state=42), param_grid=parameters, scoring=scorer, cv=5)\ngrid_fit = grid_obj.fit(X_train, y_train)\nbest_model = grid_fit.best_estimator_","efd0e3b7":"best_predictions = best_model.predict(X_test)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\")\n# print(\"ROC\/AUC on the testing data: {:.4f}\".format(models.loc[0]['Test AUC Mean']))\n\nprint(\"\\nOptimized Model\")\nprint(\"Final ROC\/AUC on the testing data: {:.4f}\".format(roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])))","c027678a":"best_model","7853a1c3":"df_submission = pd.read_csv('..\/input\/udacity-mlcharity-competition\/test_census.csv')\ndf_submission.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)","b8e600eb":"final_pred = best_model.predict_proba(df_test)[:, 1]","b72c8be5":"submission_data = pd.DataFrame(df_submission[\"id\"])\nsubmission_data[\"income\"] = final_pred","1ae58cfa":"submission_data.head()","522695ee":"submission_data.to_csv(\".\/submission.csv\", index=False)","2e84ec36":"# Data Science for Social Good in Practice: Finding Donors for a Charity Company\n\nThis notebook is also available on [medium](https:\/\/medium.com\/@brendalf\/data-science-for-social-good-in-practice-93e1783d1f30), without python code.","456ac7ed":"Looking at the mean age of each income class, it appears that people in >50K group are in average older than the other group. Let's look at the age distribution of each income class.","a763de6c":"* **Random Forest** has the best score in the training set, but it\u2019s clearly over fitting.\n* **XGBoost** has the best score in the validation set and the lowest standard deviation across folds.\n* **Logistic Regression**, **Ridge** and **Linear Discriminant Analysis** are the fastest models to train and have good overall performances.\n* **GaussianNB** has the lowest performance, but it\u2019s always important to consider a naive prediction as benchmark for whether a model is performing well.","15a149b6":"For highly-skewed feature distributions like these, it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully.","753b490c":"## Deploying\n\nOne approach to deploy this model could be making it available through an API Endpoint. Now, we just have to send the information of a new person and the model will predict to each income class it belongs. The CharityML can use this information to send a letter or not.\n\nAs I said before, the Udacity created a ongoing Kaggle private competition. Students can send their models and see how well it performs against the models from others students worldwide.","10e2e7a3":"### Treating missing values","f3edabbf":"After running the Grid Search, here are the results for the unoptimized and the optimized model:","c85d0e9c":"## Final Words\n\nI hope that you have found this a good example on how to apply data science for social good. Feel free to comment below any questions or feedback. You can also find me on [Medium](https:\/\/medium.com\/@brendalf\/), [GitHub](https:\/\/github.com\/brendalf) or [Linkedin](https:\/\/linkedin.com\/brendalf).\n\nLast but not least, I like to say #stayhome and if you could, take a little time to sponsor any of these Brazilian charity organizations:\n\n![ChildFund Brasil](https:\/\/miro.medium.com\/max\/591\/0*CiUraqx6rGX1t3-s.jpg)\n\n**ChildFundBrazil**<br>\n\u201cSince 1966 in the country, ChildFund Brazil, the International Agency for Child Development, has benefited thousands of people, including children, adolescents, young people and their families.\u201d\n\nDonate [here](https:\/\/www.childfundbrasil.org.br\/doe-agora\/).\n\n![Todos Pela Sa\u00fade](https:\/\/miro.medium.com\/max\/238\/0*PHNu7CwJo3yQkujq.png)\n\n**Todos Pela Sa\u00fade**<br>\n\u201cWe are an initiative to collaborate in the fight against the corona virus and the aim is to contribute to fighting the pandemic in different social classes and to support public health initiatives.\u201d\n\nDonate [here](https:\/\/www.todospelasaude.org\/doacoes\/).\n\n![SOS Mata Atl\u00e2ntica](https:\/\/miro.medium.com\/max\/700\/0*-godh5ykqfp2ct-B.jpg)\n\n**SOS Mata Atl\u00e2ntica**<br>\n\u201cIt acts in the promotion of public policies for the conservation of the Atlantic Forest through the monitoring of the biome, the production of studies, demonstration projects, dialogue with public and private sectors, the improvement of environmental legislation, communication and the engagement of society.\u201d\n\nDonate [here](https:\/\/www.sosma.org.br\/doacao\/).","b6e386cb":"Here we are using a KDE plot instead of histograms because it provides a smooth estimate of the overall distribution of data. The total area of each curve is 1 and the probability of an outcome falling between two values if found by computing the area under the curve that fall between those values.\n\n* The peaks are different and the group >50K is in average older than the other group.\n* The >50K group distribution is like a normal distribution and most of the data is between 37 and 50 years.\n* The distribution of the group <=50K is right skewed.\n\nWith this we can conclude that age and experience seems to have a correlation with the income class. Younger people are usually starting a career and therefore tends to earn less than $50,000 annually. As the person gets older and more experienced, the salary starts to increase.\n\nIt is also important to notice that after 60 years of age the distributions meets again.\n\n## Preparing the Data\n\nBefore data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured \u2014 this is typically known as preprocessing.\n\nLet\u2019s see if our data has any missing values:","bdf06dd4":"Our hypothesis is almost totally true, it has just a few wrong ranked `education-level`, perhaps because of some data noise. Notice that the elementary school years rank order are a little messy and `Prof-school` has a higher average of `income` class `>50K` but is a lower level than `Doctorate`.\n\n#### People that works more hours per week are more likely to have a bigger annually income?\n\nIn our data set we have the hours-per-week feature that represents how many hours each Census participant works per week and we can use this variable to answer our question.","52700b59":"Looking at the distribution of classes (those who make at most `$50,000`, and those who make more), it's clear most individuals do not make more than `$50,000`. This can greatly affect accuracy, since we could simply say \"this person does not make more than `$50,000`\" and generally be right, without ever looking at the data! Making such a statement would be called naive, since we have not considered any information to substantiate the claim.","6f4b707f":"At first sight, looking at the mean of each `income` class, it appears that people in \">50k\" group in fact works more hours per week than the other group. Diving a little deep, the image below shows the average of people that makes more than 50k annually distributed by hours worked per week.\n","6ab98ee7":"## Evaluating the Results\n\nCharityML is interested in predicting who makes more than `$50,000` accurately. It would seem that using accuracy as a metric for evaluating a particular model's performance would be appropriate.\n\nAdditionally, identifying someone that does not make more than `$50,000` as someone who does would be detrimental to CharityML, since they are looking to find individuals willing to donate.\n\nTherefore, a model's ability to precisely predict those that make more than `$50,000` is more important than the model's ability to recall those individuals. There are several metrics that could be used here, but we'll use ROC AUC because it's the metric chosen as evaluation method by Udacity in the Kaggle competition.\n\nIf you want to learn more about classification metrics, take a look at this post: [The ultimate guide to binary classification metrics](https:\/\/towardsdatascience.com\/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a).","fbc0df49":"As a developer, I believe that the best way to learn a new programming language, framework or technology is by getting my hands dirty. I was always watching some videos, reading the documentations and making projects to increase my [GitHub](https:\/\/github.com\/obrendalf) portfolio.\n\nWhen I started my path on data science in early 2018, I didn\u2019t know how to apply what I was learning. Unlike before, I couldn\u2019t make interesting projects because I needed data and real world data is hard to find.\n\nOne day I found out about Udacity and decided to start the Data Scientist Nanodegree Program. It was love at first sight. The proposal of the nanodegree is to be a complete course, with classes, exercises, online instructor, real world projects evaluated by professionals and career coach.\n\n>Udacity has nanodegrees and free courses on several different topics. Check out the catalog [here](https:\/\/www.udacity.com\/courses\/all).\n\n## Business Understanding\n\n**CharityML** is a fictitious charity organization and like most of these organizations, survive from donations. They send letters to US residents asking for donations and after sending more than 30.000 letters, they determined that every donation received came from someone that was making more than $50.000 annually. So our goal is to build a machine learning model to best identify potential donors, expanding their base while reducing costs by sending letters only to the ones who would most likely donate.","6e336d6e":"**Training Data**","4b200631":"The orange line shows the mean worked hours in the entire dataset. The average increases after forty hours worked per week (red bar). We have a few hours without bar because we don't have reports with to those. \n\n#### How experience is related in earning more than $50,000 annually?\n\nWe don't know how many years each person has worked, but the older a person is, the more years of work and life experience they usually have. Let's see if age tell us something.","b3682dd3":"### Transforming Skewed Continuous Features\n\nA data set may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census data set two features fit this description: capital-gain and capital-loss.","73d1be3b":"Maybe you might think the improvement is minimal, but 1% in every metric without spending a lot of time in the Data Preparation step, it's a great result.","d3cc46cb":"### Optimizing the Model\n\nNow that we know that the competition will evaluate our model based on ROC AUC metric, we can use this as a guide to optimize the model.\n\nEach machine learning model has a set of parameters that we can set to change the way that the models works. For example, in the XGBoost algorithm we can set the depth of each decision tree, the regularization factor in the loss function, the learning rate and several others. You can look at the full list [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html).\n\nWith all these parameters to use, how can we know the best values to every one? We can use an approach called Grid Search. Inputting a metric and a dictionary of parameters with an possible values, the Grid Search will train a model for each combination of values and return the best model based on the metric.","82f86e1a":"The `education-num` works the same way as a numeric ranked transformation applied to the values of education_level. Putting in a bar chart we obtain the following:","33b3c8fa":"### One Hot Encoding\n\nTypically, learning algorithms expect input to be numeric, which requires that non-numeric features (categorical variables) be converted. One popular way to convert these variables is by using the one-hot encoding scheme. One-hot encoding creates a \"dummy\" variable for each possible category variable. For example, assume `someFeature` has three possible entries: A, B, or C. We then encode this feature into `someFeature_A`, `someFeature_B` and `someFeature_C`.","381e63f7":"With that in mind, we'll choose the **XGBoost** as best model for this problem and perform a few further investigations.\n\n> If our dataset was bigger and time to train was important, we could use **Logistic Regression**, **Rigde** or **Linear Discriminant Analysis**.","7cd7acec":"**Categorical Variables**\n* **workclass**: `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, `Never-worked`. \n* **education_level**: `Bachelors`, `Some-college`, `11th, HS-grad`, `Prof-school`, `Assoc-acdm`, `Assoc-voc`, `9th`, `7th-8th`, `12th`, `Masters`, `1st-4th`, `10th`, `Doctorate`, `5th-6th`, `Preschool`.\n* **marital-status**: `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, `Married-AF-spouse`. \n* **occupation**: `Tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial`, `Prof-specialty`, `Handlers-cleaners`, `Machine-op-inspct`, `Adm-clerical`, `Farming-fishing`, `Transport-moving`, `Priv-house-serv`, `Protective-serv`, `Armed-Forces`. \n* **relationship**: `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, `Unmarried`. \n* **race**: `Black`, `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Other`. \n* **sex**: `Female`, `Male`. \n* **native-country**: `United-States`, `Cambodia`, `England`, `Puerto-Rico`, `Canada`, `Germany`, `Outlying-US(Guam-USVI-etc)`, `India`, `Japan`, `Greece`, `South`, `China`, `Cuba`, `Iran`, `Honduras`, `Philippines`, `Italy`, `Poland`, `Jamaica`, `Vietnam`, `Mexico`, `Portugal`, `Ireland`, `France`, `Dominican-Republic`, `Laos`, `Ecuador`, `Taiwan`, `Haiti`, `Columbia`, `Hungary`, `Guatemala`, `Nicaragua`, `Scotland`, `Thailand`, `Yugoslavia`, `El-Salvador`, `Trinadad&Tobago`, `Peru`, `Hong`, `Holand-Netherlands`.\n\n**Continuous Variables**\n* **age**: Years of age.\n* **education-num**: Number of educational years completed.\n* **capital-gain**: Monetary Capital Gains.\n* **capital-loss**: Monetary Capital Losses.\n* **hours-per-week**: Average Hours Per Week Worked.","cce0402d":"We can see that `capital-gain` and `capital-loss` is highly-skewed. We're gonna fix these features later.\n\n**Target**\n\n* **income**: \u226450K, >50K.","3b08053a":"### Feature Importances\n\nLike the weights in a regression, machine learning models based on decision trees has a nice way to see the how important each feature is in predicting the results.","2d031d77":"### Sex","8c8dca10":"**Testing Data**","e1696da4":"|   | someFeature |                        | someFeature_A | someFeature_B | someFeature_C |\n|---|-------------|------------------------|---------------|---------------|---------------|\n| 0 |      B      |                        |       0       |       1       |       0       |\n| 1 |      C      | --> one-hot encode --> |       0       |       0       |       1       |\n| 2 |      A      |                        |       1       |       0       |       0       |","a6c08c04":"### XGBoost\n\nThe XGBoost (eXtreme Gradient Boosting) basically generates a number of decision trees in sequence where the objective of the next tree is to reduce the error from the previous one.\nWe\u2019ll not explain how the model exactly works but we have great articles here on Medium for that. For instance, if you want to learn more about the XGBoost, check out this article [here](https:\/\/towardsdatascience.com\/xgboost-mathematics-explained-58262530904a).","55de50d6":"Machine learning expects input to be numeric and I\u2019ll talk about this later, but for now, in order to simplify our analysis, we need to convert the target income to numerical values. Since there are only two possible categories for the target (<=50K and >50K), we can simply encode these two categories as 0 and 1, respectively.","7f78f21b":"### Early Data Analysis\n\nNow it's time to take a closer look in the distributions and correlations with the target for each feature. This is a good place to make questions\/hypotheses about the problem and use visualizations to answer it.\n\n#### Question 1: People with a higher education are more likely to have a bigger annually income?\n\nThe `education_level` is categorical and has a notion of ranked associated. For example, we can think that bachelors, masters and doctors are more likely to have a bigger annually income than people that didn't make through collage. Is this hypotheses true?\n\nAnother variable that we can look for answers is the `education-num` that represents the total education years completed by someone. If we group by `education_level` or `education-num` and calculate the average of each income class we will see that for every value in education_level we have a equivalent value in `education-num`.","d4e8c4a9":"### Dropping Unused Feature","0cb67430":"## Data Understanding","8b3e30ac":"Fortunately, for this data set, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. The preprocessing step can help tremendously with the outcome and predictive power of nearly all learning algorithms.\n\n> The test set has some missing values and a unnamed column. We'll fix these missing values as we go to each feature. For now, let's drop this unnamed column.","a7bfa900":"### Splitting Data in Training and Validation\n\nNow that all categorical variables have been converted into numerical features, we're going to split our `data` into two sets: training and validation. We can't use the same data to train and validate our model because it will learn noise from the data and loose it's ability to generalize well. That's why this step is so important.  \nYou can learn more about bias and variance [here](https:\/\/medium.com\/@mp32445\/understanding-bias-variance-tradeoff-ca59a22e2a83).","3b3de35a":"![Photo by Larm Rmah on Unsplash](https:\/\/miro.medium.com\/max\/700\/0*J6bRe_BFd-y14c9O)","ad435df6":"## Data Modeling\n\nIn this section, we will investigate a couple different algorithms and determine which is best at modeling the data. For now, we'll use accuracy and cross validation with `k_fold = 5`.\n\nYou can learn more about cross validation [here](https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85).\n\n### Models Exploration\n\nApplying eight different machine learning models to the data we have the following results:","6606857d":"### Checking for missing values","587f8204":"### Normalizing Numerical Features\n\nIn addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features.\n\nApplying a scaling to the data does not change the shape of each feature\u2019s distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners and changes the raw data meaning, as showed below.\n\n> This step comes after splitting our data because we want to avoid [data leakage](https:\/\/towardsdatascience.com\/data-leakage-in-machine-learning-10bdd3eec742).\n\nWe will use [`sklearn.preprocessing.MinMaxScaler`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) for this.","c0cd9d8a":"The training census dataset consists of approximately 45,222 number of records, with each record having 14 columns. Let's take a look at these columns.\n\n> This dataset is a modified version of the dataset published in the paper [\"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\"](https:\/\/www.aaai.org\/Papers\/KDD\/1996\/KDD96-033.pdf) by Ron Kohavi.","a31974a2":"Here we are plotting the first features with their importance. The orange line show the accumulative importance across features. With that we can see that approximately 10 features are responsible for more than 90% of the entire model importance.\n\nReading the XGBoost documentation, there are three main different types of how the importance can be calculated and here we used the weight type, that is the number of times a feature appears in a tree. You can read about the other ones [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html)."}}