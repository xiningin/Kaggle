{"cell_type":{"322ee312":"code","14bc5668":"code","fa035b61":"code","6c3c8499":"code","6e77a5b5":"code","0289645b":"code","89ae20a6":"code","1a444610":"code","f436cf44":"code","f8dd95f2":"code","3f525f9d":"code","f089a98e":"code","8a63e94f":"code","ee310e94":"code","2d505aa6":"code","256e0fa2":"code","c11c2c69":"code","0575bbee":"code","05b0c0b5":"code","b30231d8":"code","755682c6":"code","ed0fe13d":"code","27c9aeeb":"code","6d05b888":"code","9ef5aaf6":"markdown","b42058a6":"markdown","5248ca7c":"markdown","643c67cc":"markdown","90ac81b6":"markdown","524be291":"markdown","1864a1ab":"markdown","4c95d62b":"markdown","3df5a566":"markdown","6667fb1b":"markdown","e384d4b7":"markdown","c57e4a5b":"markdown","06a4d092":"markdown","379a1d46":"markdown","b0fe7ce2":"markdown","235b3d46":"markdown","f27d4df1":"markdown","b2385382":"markdown","1cf0558e":"markdown","f462c4d2":"markdown","fc2b7ebc":"markdown","508c37ce":"markdown","24035904":"markdown","9ccd6372":"markdown","0d1d2ca4":"markdown"},"source":{"322ee312":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom pprint import pprint\n","14bc5668":"data_full = pd.read_csv(\"..\/input\/wine-quality\/winequalityN.csv\")","fa035b61":"#Check of null values in the whole dataset\ntotal_entries = (data_full.shape[0] * data_full.shape[1])\nmissing_entries_max = data_full.isnull().sum().sum()\nmissing_entries_max_percentage = (missing_entries_max \/ total_entries) * 100\n\nprint(f\"Total entries in the dataset: %i\" % total_entries)\nprint(f\"Maximum missing values in the dataset: {missing_entries_max}\")\nprint(f\"Percentage of maximum missing values in the dataset: {missing_entries_max_percentage:.2f}%\")","6c3c8499":"#Deletion of the null values in the dataset\ndata_full = data_full.dropna(axis=0)\ndata_full.shape","6e77a5b5":"#Convert 'char' value to 'integer' value\ndata_full = data_full.replace('red', 0)\ndata_full = data_full.replace('white', 1)\n\ndata_full.describe()","0289645b":"#Plot of histograms for each parameter\nplt.tight_layout()\ndata_full.hist(bins = 100, figsize = (24, 16))\nplt.show()","89ae20a6":"#Correlation matrix using heatmap plot\nplt.figure(figsize = (24, 16))\nsns.heatmap(data_full.corr(), annot = True)\nplt.show()","1a444610":"#Deletion of the unnecessary parameters we've found \ndata_full = data_full.drop('total sulfur dioxide',axis=1)\ndata_full = pd.get_dummies(data_full, drop_first=True)\ndata_full.head()","f436cf44":"#Define the normalize method\nscaler = MinMaxScaler()\n\n#Compute the minimum and maximum to use \n#it for the normalization, and then\n#use the transform for providing\n#the scaled results\ndata_fit = scaler.fit(data_full)\ndata_fit = data_fit.transform(data_full)","f8dd95f2":"#Rebuild of the dataset\ndata_full = pd.DataFrame(data_fit, columns=['type', 'fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','density','pH','sulphates','alcohol','quality'])\ndata_full.head()","3f525f9d":"#defining the parameters of the data, \n#train parameters and target parameter \nX = data_full[['type', 'fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','density','pH','sulphates','alcohol']]\ny = data_full.quality\n#splitting the dataset, 80% train set, 20% test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)","f089a98e":"def get_rmse(y_test, predictions):\n    return mean_squared_error(y_test, predictions) ** 0.5","8a63e94f":"#Using the RandomForest algorithm\nn_estimators = [50 + i*50 for i in range(10)]\nmodels_n_estimators = [RandomForestRegressor(n_estimators = n_estimators[i], random_state = 42) for i in range(10)]\n\nn_estimators_rmses = []\n                       \nfor model in models_n_estimators:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    n_estimators_rmses += [rmse]\n    print(f\"RMSE of model with n_estimators={model.n_estimators}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable n_estimators\nplt.plot(n_estimators, n_estimators_rmses)\nplt.title(\"RMSE vs n_estimators\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"rmse\")\nplt.show()\n    \n    ","ee310e94":"max_depths = [10 + i*10 for i in range(10)]\nmodels_max_depths = [RandomForestRegressor(max_depth = max_depths[i], random_state = 42) for i in range(10)]\n\nmax_depths_rmses = []\n                       \nfor model in models_max_depths:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_depths_rmses += [rmse]\n    print(f\"RMSE of model with max_depth={model.max_depth}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable max_depth\nplt.plot(max_depths, max_depths_rmses)\nplt.title(\"RMSE vs max_depth\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"rmse\")\nplt.show()","2d505aa6":"min_samples_splits = [50 + i*50 for i in range(10)] + [500 + i*500 for i in range(10)]\nmodels_min_samples_splits = [RandomForestRegressor(min_samples_split = min_samples_splits[i], random_state = 42) for i in range(20)]\n\nmin_samples_splits_rmses = []\n                       \nfor model in models_min_samples_splits:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    min_samples_splits_rmses += [rmse]\n    print(f\"RMSE of model with min_samples_split={model.min_samples_split}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable min_samples_splits\nplt.plot(min_samples_splits, min_samples_splits_rmses)\nplt.title(\"RMSE vs min_samples_split\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"rmse\")\nplt.show()","256e0fa2":"max_leaf_nodes = [200 + i*200 for i in range(10)]\nmodels_max_leaf_nodes = [RandomForestRegressor(max_leaf_nodes = max_leaf_nodes[i], random_state = 42) for i in range(10)]\n\nmax_leaf_nodes_rmses = []\n                       \nfor model in models_max_leaf_nodes:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_leaf_nodes_rmses += [rmse]\n    print(f\"RMSE of model with max_leaf_nodes={model.max_leaf_nodes}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable max_leaf_nodes\nplt.plot(max_leaf_nodes, max_leaf_nodes_rmses)\nplt.title(\"RMSE vs max_leaf_nodes\")\nplt.xlabel(\"max_leaf_nodes\")\nplt.ylabel(\"rmse\")\nplt.show()","c11c2c69":"min_samples_leaves =  [25 + i*25 for i in range (10)] + [250 + i*250 for i in range(10)]\nmodels_min_samples_leaves = [RandomForestRegressor(min_samples_leaf = min_samples_leaves[i], random_state = 42) for i in range(20)]\n\nmin_samples_leaves_rmses = []\n                       \nfor model in models_min_samples_leaves:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    min_samples_leaves_rmses += [rmse]\n    print(f\"RMSE of model with min_samples_leaf={model.min_samples_leaf}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable min_samples_leaves\nplt.plot(min_samples_leaves, min_samples_leaves_rmses)\nplt.title(\"RMSE vs min_samples_leaf\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"rmse\")\nplt.show()","0575bbee":"max_samples =  [0.05 + i*0.049 for i in range(20)]\nmodels_max_samples = [RandomForestRegressor(max_samples = max_samples[i], random_state = 42) for i in range(20)]\n\nmax_samples_rmses = []\n                       \nfor model in models_max_samples:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_samples_rmses += [rmse]\n    print(f\"RMSE of model with max_samples={model.max_samples}: {rmse:.5f}\")\n\n#plot of the RMSE result vs variable min_samples_leaves\nplt.plot(max_samples, max_samples_rmses)\nplt.title(\"RMSE vs max_samples\")\nplt.xlabel(\"max_samples\")\nplt.ylabel(\"rmse\")\nplt.show()","05b0c0b5":"\"\"\"\ndef get_best_parameter(params, rmses):\n    return params[rmses.index(min(rmses))]\n\"\"\"","b30231d8":"\"\"\"\nbest_n_estimators = get_best_parameter(n_estimators, n_estimators_rmses)\nbest_max_depth = get_best_parameter(max_depths, max_depths_rmses)\nbest_min_samples_split = get_best_parameter(min_samples_splits, min_samples_splits_rmses)\nbest_max_leaf_nodes = get_best_parameter(max_leaf_nodes, max_leaf_nodes_rmses)\nbest_min_samples_leaf = get_best_parameter(min_samples_leaves, min_samples_leaves_rmses)\nbest_max_samples = get_best_parameter(max_samples, max_samples_rmses)\n\nprint(f\"Best n_estimators: {best_n_estimators}\\nBest max_depth: {best_max_depth}\\nBest min_samples_split: {best_min_samples_split}\\nBest max_leaf_nodes: {best_max_leaf_nodes}\\nBest min_samples_leaf: {best_min_samples_leaf}\\nBest max_samples: {best_max_samples}\")\n\"\"\"","755682c6":"best_n_estimators = 450\nbest_max_depth = 20\nbest_min_samples_split = 2\nbest_max_leaf_nodes = 1000\nbest_min_samples_leaf = 2\nbest_max_samples = 1\n\nprint(f\"Best n_estimators: {best_n_estimators}\\nBest max_depth: {best_max_depth}\\nBest min_samples_split: {best_min_samples_split}\\nBest max_leaf_nodes: {best_max_leaf_nodes}\\nBest min_samples_leaf: {best_min_samples_leaf}\\nBest max_samples: {best_max_samples}\")\n","ed0fe13d":"#Defining the optional values for each parameter\nparam_grid = {'n_estimators': [100, 300, 500],\n              'max_depth': [20, 30, 40],\n              'min_samples_split': [2, 500, 1000],\n              'max_leaf_nodes': [333, 666, 1000],\n              'min_samples_leaf': [1, 50, 100],\n              'max_samples': [0.33,0.66,0.9810000000000001]}\n\n#Using the GridSearchCV algorithm\nbest_model = model_selection.GridSearchCV(estimator = RandomForestRegressor(),\n                                          param_grid = param_grid,\n                                          scoring = 'neg_root_mean_squared_error',\n                                          verbose=10,\n                                          n_jobs=-1,\n                                          cv=5)\n\nbest_model.fit(X_train, y_train)\n","27c9aeeb":"#Extract the best parameters the algorithm has find\nparams = best_model.get_params()\npprint(params)\n","6d05b888":"#Running Random Forest Algorithm with the best \n#parameters, according to the RMSE's graphs we've\n#created only\nbest_model = RandomForestRegressor(max_depth= 40,\n max_leaf_nodes= 1000,\n max_samples= 0.9810000000000001,\n min_samples_leaf= 1,\n min_samples_split= 2,\n n_estimators= 500,\n random_state= 42)\n\nbest_model.fit(X_train, y_train)\nbest_predictions = best_model.predict(X_test)\nbest_rmse = get_rmse(y_test, best_predictions)\nprint(f\"RMSE of the optimized model according to our manual check: {best_rmse:.5f}\")\n\n\n\"\"\"\n#edited GridSearch based model, \n#according to the RMSE's graphs we've created\n##JUST FOR US##\n\nbest_model = RandomForestRegressor(bootstrap= True,\n ccp_alpha= 0.0,\n criterion='mse',\n max_depth=20,\n max_features='auto',\n max_leaf_nodes= 1000,\n max_samples= 0.9810000000000001,\n min_impurity_decrease= 0.0,\n min_impurity_split= None,\n min_samples_leaf= 1,\n min_samples_split= 2,\n min_weight_fraction_leaf= 0.0,\n n_estimators= 500,\n n_jobs= None,\n oob_score= False,\n random_state= 42,\n verbose =0,\n warm_start =False)\n\nbest_model.fit(X_train, y_train)\nbest_predictions = best_model.predict(X_test)\nbest_rmse = get_rmse(y_test, best_predictions)\nprint(f\"RMSE of the optimized model: {best_rmse:.5f}\")\n\"\"\"\n\"\"\"\n#edited GridSearch based model, \n#according to the RMSE's graphs we've created, \n#but only with the 6 parameters we've worked with.\n##JUST FOR US##\nbest_model = RandomForestRegressor(max_depth= 20,\n max_leaf_nodes= 1000,\n max_samples= 0.9810000000000001,\n min_samples_leaf= 1,\n min_samples_split= 2,\n n_estimators= 500,\n random_state= 42)\n\nbest_model.fit(X_train, y_train)\nbest_predictions = best_model.predict(X_test)\nbest_rmse = get_rmse(y_test, best_predictions)\nprint(f\"RMSE of the optimized model: {best_rmse:.5f}\")\n\"\"\"\n\n#Running Random Forest Algorithm with the best \n#parameters,recommended from GridSearch algorithm\nbest_model = RandomForestRegressor(bootstrap= True,\n ccp_alpha= 0.0,\n criterion='mse',\n max_depth=None,\n max_features='auto',\n max_leaf_nodes= None,\n max_samples= None,\n min_impurity_decrease= 0.0,\n min_impurity_split= None,\n min_samples_leaf= 1,\n min_samples_split= 2,\n min_weight_fraction_leaf= 0.0,\n n_estimators= 100,\n n_jobs= None,\n oob_score= False,\n random_state= 42,\n verbose =0,\n warm_start =False)\n\nbest_model.fit(X_train, y_train)\nbest_predictions = best_model.predict(X_test)\nbest_rmse = get_rmse(y_test, best_predictions)\nprint(f\"RMSE of the optimized model according to GridSearch algorithm: {best_rmse:.5f}\")\n","9ef5aaf6":"3rd Model - the variable is 'min_samples_splits'.\nWe will run the algorithm on the model with 20 different values(for optimization of the accuracy),\nand then check the RMSE result.","b42058a6":"According to the above results, we will build now a final model with the parameters that gave us the best results.\nFrom the above graph's results, the best parameters are:","5248ca7c":"# 3. Preprocessing","643c67cc":"5th Model - the variable is 'min_samples_leaves'.\nWe will run the algorithm on the model with 20 different values(for optimization of the accuracy),\nand then check the RMSE result.","90ac81b6":"Since the percentage of missing entries is negligable, we can drop each entry that contains a null without affecting the accuracy of the prediction.","524be291":"# 1. Imports","1864a1ab":"1st Model - the variable is 'n_estimators'.\nWe will run the algorithm on the model with 10 different values,\nand then check the RMSE result.","4c95d62b":"For the comparison, we will run the RandomForest agorithm for the model we've choose to create, and for the model  which we've created according to the GridSearch algorithm, and see the differences, if exist.\n*Our check will be according to the Optimal Criterion - RMSE","3df5a566":"First, let us see how many missing entries are in the dataset:","6667fb1b":"We want to verify our discovery of the optimal parameters, and we've choose to check it with GridSearch algorithm.\nAccording to the total range of each parameter we've checked, we've choose 3 different parameters accordingly.","e384d4b7":"Now, we will take care about the non-numeric parameter, i.e. applying ordinal encoding to the wine types:","c57e4a5b":"Now, we will show the results of the algorithm GridSearchCV, and see which parameter it choosen for the model.","06a4d092":"# 5. Developing the models","379a1d46":"Now, notice that because our dataset in unbalance(i.e. there are parameters which have high ranges in respect of others), we will normalize our values to be from 0 to 1.","b0fe7ce2":"As we can see, we got now 6463 observations, and 13 parameters each.","235b3d46":"6th Model - the variable is 'max_samples'.\nWe will run the algorithm on the model with 20 different values(for optimization of the accuracy),\nand then check the RMSE result.","f27d4df1":"# 2. Loading the Data","b2385382":"We will chose to set that if there are corrlation above 0.7 between parameters (which are not the same, i.e. not  a parameter with itself), we say that the correlation is high enough, so we can remove one of the correlated parameters. Hence, we decided to remove 'total sulfur dioxide'.","1cf0558e":"Let us develope a function for the calculation of the RMSE quality criterion.","f462c4d2":"4th Model - the variable is 'max_leaf_nodes'.\nWe will run the algorithm on the model with 10 different values,\nand then check the RMSE result.","fc2b7ebc":"# 6. Training the models","508c37ce":"2nd Model - the variable is 'max_depth'.\nWe will run the algorithm on the model with 10 different values,\nand then check the RMSE result.","24035904":"As we can see, most of the parameters (except the \"type\" parameter, which is binary parameter) are normally distributed.\n\nNow, we will show the correlation between each parameter with another, using heatmap(it describes a correlation matrix).","9ccd6372":"Now, we will plot a histogram for each of the parameters, and see the distribution.","0d1d2ca4":"# 4. Splitting the data"}}