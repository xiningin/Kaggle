{"cell_type":{"3595abcf":"code","106c07bd":"code","f2f4d480":"code","d37e9e33":"code","4bff07fa":"code","8995420f":"code","3c829055":"code","367265fa":"code","4ff1c871":"code","6a937e86":"code","3d08216d":"code","ca64b010":"code","dce73910":"code","6b8c2b08":"code","977c6450":"code","34e42eeb":"code","9cc4211a":"code","be1b9494":"code","285df2bf":"code","ee1586f5":"code","dc56539b":"markdown","8ee29823":"markdown","29a3f747":"markdown","ad5a48c0":"markdown","9d7b54f1":"markdown","27f98f04":"markdown","0090c349":"markdown","40118b90":"markdown","01a54cbf":"markdown","545f09b7":"markdown"},"source":{"3595abcf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","106c07bd":"image_train = pd.read_csv('..\/input\/fashion-mnist_train.csv')\nimage_test = pd.read_csv('..\/input\/fashion-mnist_test.csv')","f2f4d480":"print('Shape Train:', image_train.shape)\nprint('Shape Test:', image_test.shape)","d37e9e33":"image_train.head(5)","4bff07fa":"image_test.head(5)","8995420f":"#create model\nmodel = Sequential()\n\n#let's add some layers baby!\nmodel.add((Conv2D(64, kernel_size = 3, activation = 'relu', input_shape = (28,28,1))))\nmodel.add((Conv2D(32, kernel_size = 3, activation = 'relu')))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax'))\nmodel.summary()","3c829055":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","367265fa":"history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 5)\n\nprint(history.history.keys())\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","4ff1c871":"y_pred = model.predict(x_test)","6a937e86":"from sklearn import metrics\nimport seaborn as sns\n\ncm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\ncm = cm.astype('float64') \/ cm.sum(axis=1)[:, np.newaxis]\ndf_cm = pd.DataFrame(cm)\nfig_corr, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt='g', cmap='Blues', ax=ax)\nplt.show()","3d08216d":"#create model\nmodel = Sequential()\n\n#let's add some layers baby!\nmodel.add((Conv2D(64, kernel_size = 3, activation = 'relu', input_shape = (28,28,1))))\nmodel.add(Dropout(0.25))\nmodel.add((Conv2D(32, kernel_size = 3, activation = 'relu')))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax'))\nmodel.summary()","ca64b010":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","dce73910":"history1 = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 5)\n\nprint(history1.history.keys())\n\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6b8c2b08":"y_pred = model.predict(x_test)","977c6450":"from sklearn import metrics\nimport seaborn as sns\n\ncm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\ncm = cm.astype('float64') \/ cm.sum(axis=1)[:, np.newaxis]\ndf_cm = pd.DataFrame(cm)\nfig_corr, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt='g', cmap='Blues', ax=ax)\nplt.show()","34e42eeb":"#accuracy\nplt.figure(figsize=(10,10))\n\nplt.plot(history1.history['acc'], color = 'darkorange')\nplt.plot(history1.history['val_acc'], color = 'orange' )\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n\nplt.plot(history.history['acc'], color = 'darkred' )\nplt.plot(history.history['val_acc'], color = 'brown' )\nplt.legend(['Train D = 0.25','Test D = 0.25', 'Train D = 0', 'Test D = 0'], loc='upper left')\n\nplt.show()\n\n#loss\nplt.figure(figsize=(10,10))\nplt.plot(history1.history['loss'], color = 'darkorange')\nplt.plot(history1.history['val_loss'], color = 'orange')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.plot(history.history['loss'], color = 'darkred')\nplt.plot(history.history['val_loss'], color = 'brown')\n\nplt.legend(['Train D = 0.25','Test D = 0.25', 'Train D = 0', 'Test D = 0'], loc='upper left')\n\nplt.show()\n\n","9cc4211a":"#create model\nmodel = Sequential()\n\n#let's add some layers baby!\nmodel.add((Conv2D(64, kernel_size = 3, activation = 'relu', input_shape = (28,28,1))))\nmodel.add(Dropout(0.4))\nmodel.add((Conv2D(32, kernel_size = 3, activation = 'relu')))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax'))\nmodel.summary()","be1b9494":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","285df2bf":"history2 = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 5)\n","ee1586f5":"#accuracy\nplt.figure(figsize=(10,10))\nplt.plot(history2.history['acc'], color = 'darkslategrey')\nplt.plot(history2.history['val_acc'], color = 'teal' )\n\nplt.plot(history1.history['acc'], color = 'darkorange')\nplt.plot(history1.history['val_acc'], color = 'orange' )\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n\nplt.plot(history.history['acc'], color = 'darkred' )\nplt.plot(history.history['val_acc'], color = 'brown' )\nplt.legend(['Train D = 0.4','Test D = 0.4','Train D = 0.25','Test D = 0.25', 'Train D = 0', 'Test D = 0'], loc='upper left')\n\nplt.show()\n\n#loss\nplt.figure(figsize=(10,10))\nplt.plot(history2.history['acc'], color = 'darkslategrey')\nplt.plot(history2.history['val_acc'], color = 'teal' )\n\nplt.plot(history1.history['loss'], color = 'darkorange')\nplt.plot(history1.history['val_loss'], color = 'orange')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.plot(history.history['loss'], color = 'darkred')\nplt.plot(history.history['val_loss'], color = 'brown')\n\nplt.legend(['Train D = 0.4','Test D = 0.4','Train D = 0.25','Test D = 0.25', 'Train D = 0', 'Test D = 0'], loc='upper left')\n\nplt.show()\n\n","dc56539b":"## Datasets","8ee29823":"## Plotting with and without Dropout","29a3f747":"# Model with Dropout","ad5a48c0":"## Lets train this modeeeel","9d7b54f1":"### Shapes","27f98f04":"### Compiling the model baby!","0090c349":"## Lets train this modeeeel","40118b90":"## Importing the datasets","01a54cbf":"### Compiling the model baby!","545f09b7":"## Lets train this modeeeel"}}