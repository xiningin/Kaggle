{"cell_type":{"b7db05f2":"code","e75ea9a8":"code","e6bee99a":"code","c6a8a9e1":"code","5bee48e4":"code","708623ca":"code","8253f2a7":"code","4a00b205":"code","449efe29":"code","828db54f":"code","93bc012f":"code","c97dfd28":"code","358dd880":"code","2d80b9c9":"code","cd8a810b":"code","2252d6d7":"code","28a636f3":"code","a99229b1":"markdown","2c278ff7":"markdown","1958699a":"markdown","9fc5abb0":"markdown","ee1508a7":"markdown","0ce478aa":"markdown","de1b96c9":"markdown","d0fb2773":"markdown","275ee822":"markdown","881188ab":"markdown","107f1358":"markdown","4c6c1969":"markdown","46f607d3":"markdown","528fbf07":"markdown","bbf9a7bd":"markdown","0d033ef1":"markdown","fff08ab8":"markdown","dfce4dd5":"markdown","16c1c4aa":"markdown","cc668ae3":"markdown","e4b67aaf":"markdown","75ac035d":"markdown"},"source":{"b7db05f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom scipy.stats import norm\nimport scipy.stats as st\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e75ea9a8":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\ndisplay(train_df.head())\ntrain_df.describe()","e6bee99a":"FEATURES = ['cont%d' % (i) for i in range(1, 15)]","c6a8a9e1":"train_df.info()","5bee48e4":"def plot_outliers(df, feature, threshold=3):\n    mean, std = np.mean(df), np.std(df)\n    z_score = np.abs((df-mean) \/ std)\n    good = z_score < threshold\n\n    print(f\"Rejection {(~good).sum()} points\")\n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good\n    \ndef plot_lof_outliers(df, feature):\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.001, p=1)\n    good = lof.fit_predict(df) > 0.5 # change this value to set the threshold for outliers\n    print(f\"Rejection {(~good).sum()} points\")\n    \n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good","708623ca":"good = plot_outliers(train_df['target'], 'target', threshold=4)","8253f2a7":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","4a00b205":"good = plot_lof_outliers(train_df['target'].values.reshape(train_df['target'].shape[0], -1), 'target')","449efe29":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","828db54f":"for feature in FEATURES:\n    plot_outliers(train_df[feature], feature)","93bc012f":"for feature in FEATURES:\n    # There some reshaping done here for syntax sake\n    plot_lof_outliers(train_df[feature].values.reshape(train_df[feature].shape[0], -1), feature)","c97dfd28":"for feature in FEATURES:\n    sns.violinplot(x=train_df[feature], inner='quartile', bw=0.1)\n    plt.title(feature)\n    plt.show();","358dd880":"def plot_cdf(df, feature):\n    ps = 100 * st.norm.cdf(np.linspace(-4, 4, 10)) # The last number in this tuple is the number of percentiles\n    x_p = np.percentile(df, ps)\n\n    xs = np.sort(df)\n    ys = np.linspace(0, 1, len(df))\n\n    plt.plot(xs, ys * 100, label=\"ECDF\")\n    plt.plot(x_p, ps, label=\"Percentiles\", marker=\".\", ms=10)\n    plt.legend()\n    plt.ylabel(\"Percentile\")\n    plt.title(feature)\n    plt.show();\n\nfor feature in FEATURES:\n    plot_cdf(train_df[feature], feature)","2d80b9c9":"# This plots a 16x16 matrix of correlations between all the features and the target\n# Note: I sometimes comment this out because it takes a few minutes to run and doesn't show any useful information.\n\npd.plotting.scatter_matrix(train_df, figsize=(10, 10));","cd8a810b":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(train_df.drop(columns=['id']).corr(), annot=True, cmap='viridis', fmt='0.2f', ax=ax)","2252d6d7":"sns.violinplot(x=train_df['target'], inner='quartile', bw=0.1)\nplt.title('target')\nplt.show();","28a636f3":"for feature in FEATURES:\n    #sns.kdeplot(x=train_df['target'], y=train_df[feature], bins=20, cmap='magma', shade=True) \n    plt.hist2d(x=train_df['target'], y=train_df[feature], bins=20)\n    plt.xlabel(feature)\n    plt.ylabel('target')\n    plt.title(feature)\n    plt.show()","a99229b1":"We can see that the above graph is far too busy to show us any useful information. However, at least we know that there isn't any clear correlations between a particular variable and the target.\n\nThe one interesting thing from this plot is that the target values are almost exclusively in the upper half of the range.","2c278ff7":"The above is harder to read as it has picked some points inside grouping. However, since there are only 300 points and I trust the LOF measurement, I am going to remove these points from dataset as well.","1958699a":"Next we will look at the LOF outliers.","9fc5abb0":"We will now look at the **LOF (Local Outlier Factor)** outliers.","ee1508a7":"# Analysing Distributions\n\nHere we will look at correlations between the features, distributions of the features.","0ce478aa":"### Invalid Values","de1b96c9":"# Load the Data\n\nHere we will load the data into a pandas dataframe.","d0fb2773":"Here we can see that there are no *non-null* values so there is nothing to remove here.\n\n### Outliers\n\n#### **Removing outliers is less of a science and more of an art form. So I will leave the choice up to you, but show you how to visualise these points.**\n\nFirst we will look at outliers for the *target*.\n\nWe will add noise to the one dimensional features in order to \"explode\" the points out, helping us see the distributions and potential outliers.\n\nWe will use two methods for finding outliers:\n* The first will consider a point to be an outlier if it is N standard deviations from the mean. N is defined as the threshold.\n* A more complex form of outlier detection is LOF (Local Outlier Factor) which uses a points 20 nearest neighbours to determine if it is in a low density region (and therefore potentially and outlier).","275ee822":"We can see from the above that there are a small number of reasonable outliers selected here. I am therefore not going to remove any of these points as outliers.","881188ab":"Finally we will look at the 2D histogram plots for each features vs. the target, this can be a clue of unusual correlations between the target and features. \n\n**Note:** There is also code for a KDE plot but these take a long time to run.","107f1358":"This is perhaps the most revealing visualisations. It shows us that our features (especially '*cont2*' and '*cont5*') have unusual distributions. '*cont2*' appears to turn into an categorical variable when greater than 0.4 and '*cont5*' is a linear distribution once above 0.3. \n\nThis could suggest that these variables need to split into additional features or have functions applied to their values to create a bigger distinction between very similar values.","4c6c1969":"Above we can see a cluster of features (cont1, cont6-cont13) that appear to be quite highly correlated together. This suggests that dimensionality reduction techniques could be used to reduce these features to a smaller set.","46f607d3":"### Target Outliers","528fbf07":"Above we can see that these points are very reasonable outliers. There is a clear grouping for the target values however these points marked in red fall outside this grouping. I will therefore remove these 17 rejected points.","bbf9a7bd":"### Feature Outliers\n\nFirst we will look at the threshold outliers.","0d033ef1":"# Correlation\n\nHere we can look at the correlation between the features and each other (and the target)","fff08ab8":"# Empirical CDFs\n\nThe below graphs show us where the 10th\/20th\/....\/90th percentiles lie for each of the features.","dfce4dd5":"This doesn't show us much that is interesting other than the target is grouped around it's mean of 8, with some long tails out to either side.","16c1c4aa":"# Cleaning the Dataset\n\nFollowing the steps of the Machine Learning Checklist we will start by cleaning out invalid values and outliers from the dataset.","cc668ae3":"The above shows us that each feature has a unique distribution which could likely be used to help our models make predictions.","e4b67aaf":"# Analyse the Target","75ac035d":"So above we can see that the majority of the features do not contain outliers, however features *cont7*, *cont9*, *cont10* and *cont13* do contain some points that are could be considered as outliers."}}