{"cell_type":{"e030228d":"code","dcb975ae":"code","fe16db81":"code","debffa81":"code","3b9f525a":"code","552e361e":"code","f4c900d7":"code","7fcc2c8b":"code","b7547848":"code","4d5ec072":"code","0d48de29":"code","88fc58de":"code","95a23b10":"code","ade3fa55":"code","02107453":"code","232f0847":"markdown"},"source":{"e030228d":"!mkdir \/kaggle\/working\/generative-dog-images\n!unzip \/kaggle\/input\/generative-dog-images\/all-dogs.zip -d \/kaggle\/working\/generative-dog-images > \/dev\/null 2>&1\n!unzip \/kaggle\/input\/generative-dog-images\/Annotation.zip -d \/kaggle\/working\/generative-dog-images > \/dev\/null 2>&1","dcb975ae":"import os\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport xml.etree.ElementTree as ET ","fe16db81":"all_dogs_dir = '\/kaggle\/working\/generative-dog-images\/all-dogs'\nannotation_dir = '\/kaggle\/working\/generative-dog-images\/Annotation'","debffa81":"def get_bndbox(filename, square=False):\n    tree = ET.parse(filename)\n    root = tree.getroot()\n    box = root.find('object').find('bndbox')\n    xmin = int(box.find('xmin').text)\n    ymin = int(box.find('ymin').text)\n    xmax = int(box.find('xmax').text)\n    ymax = int(box.find('ymax').text)\n    \n    if square:\n        center_x, center_y = (xmin + xmax)\/\/2, (ymin+ymax)\/\/2\n        max_w = max(xmax-xmin, ymax-ymin)\n        xmin = center_x - max_w\/\/2\n        xmax = xmin + max_w\n        ymin = center_y - max_w\/\/2\n        ymax = ymin + max_w\n        \n    return xmin, ymin, xmax, ymax  ","3b9f525a":"dogs_dir = '\/kaggle\/working\/dogs'\nif not os.path.exists(dogs_dir):\n    os.makedirs(dogs_dir)\n\nfor breed in os.listdir(annotation_dir):\n    for dog in os.listdir(os.path.join(annotation_dir, breed)):\n        bndbox = get_bndbox(os.path.join(annotation_dir, breed, dog), square=True)\n        jpg_name = os.path.join(all_dogs_dir, dog+'.jpg')\n        if os.path.exists(jpg_name):\n            img = Image.open(jpg_name).crop(bndbox)\n            img.save(os.path.join(dogs_dir, dog+'.jpg'))\nprint('number of dogs:', len(os.listdir(dogs_dir)))","552e361e":"from torch.utils.data import Dataset, DataLoader\n\nclass DogDataset(Dataset):\n    def __init__(self, data_dir, transforms=None):\n        self.files = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, index):\n        img = Image.open(self.files[index])\n        if self.transforms is not None:\n            img = self.transforms(img)\n        return img","f4c900d7":"img_size = (64, 64)\nimage_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(img_size),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nbatch_size = 128\ntrainloader = DataLoader(\n    DogDataset(data_dir=dogs_dir, transforms=image_transforms),\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = 3,\n)","7fcc2c8b":"class Generator(nn.Module):\n    def __init__(self, z_channels, out_channels=3):\n        super(Generator, self).__init__()\n        \n        convs = []\n        channels = [z_channels, 1024, 512, 256, 128, 64]\n        for i in range(1, len(channels)):\n            convs.append(nn.ConvTranspose2d(channels[i-1], channels[i], 2, stride=2, bias=False))\n            convs.append(nn.BatchNorm2d(channels[i]))\n            convs.append(nn.ReLU(inplace=True))\n        convs.append(nn.ConvTranspose2d(channels[-1], out_channels, 2, stride=2, bias=False))\n        convs.append(nn.Tanh())\n        \n        self.convs = nn.Sequential(*convs)\n        \n    def forward(self, x):\n        return self.convs(x)","b7547848":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        channels = [3, 64, 128, 256, 512] # size: 64->32->16->8->4\n        convs = []\n        for i in range(1, len(channels)):\n            convs.append(nn.Conv2d(channels[i-1], channels[i], 3, padding=1, stride=2, bias=False))\n            if i != 1:\n                convs.append(nn.BatchNorm2d(channels[i]))\n            convs.append(nn.LeakyReLU(0.2, inplace=True))\n        \n        convs.append(nn.Conv2d(channels[-1], 1, 4, bias=False))\n        convs.append(nn.Sigmoid())\n        \n        self.convs = nn.Sequential(*convs)\n    \n    def forward(self, x):\n        x = self.convs(x)\n        return x.view(-1)","4d5ec072":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","0d48de29":"z_channels = 100\nG = Generator(z_channels, 3)\nG.apply(weights_init)\nD = Discriminator()\nD.apply(weights_init)\ncriterion = nn.BCELoss()\n\ncuda = torch.cuda.is_available()\nif cuda:\n    print('Use GPU')\n    G = G.cuda()\n    D = D.cuda()\n    criterion = criterion.cuda()\nelse:\n    print('No GPU')","88fc58de":"lr = 0.0002\nb = 0.5\noptimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b, 0.999))\noptimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b, 0.999))","95a23b10":"def range_rand(low, high, shape):\n    a = torch.rand(shape)\n    a = ((a - torch.min(a)) \/ (torch.max(a) - torch.min(a)) * (high-low)) + low\n    return a","ade3fa55":"fixed_noise = torch.normal(0, 0.1, size=(64, z_channels, 1, 1))\nif cuda:\n    fixed_noise = fixed_noise.cuda()\n\nepoches = 100\ngenerate_imgs = []    \nfor epoch in range(epoches):\n    for i, img in enumerate(trainloader):\n        z = torch.normal(0, 0.1, size=(img.size(0), z_channels, 1, 1))\n        # real = range_rand(0.9, 1, (img.size(0), 1))\n        # fake = range_rand(0, 0.1, (img.size(0), 1))\n        real = torch.ones(img.size(0))\n        fake = torch.zeros(img.size(0))\n        if cuda:\n            img, z = img.cuda(), z.cuda()\n            real, fake = real.cuda(), fake.cuda()\n\n        # train D\n        D.zero_grad()\n        loss_real = criterion(D(img), real)\n        loss_real.backward()\n\n        fake_img = G(z)\n        loss_fake = criterion(D(fake_img.detach()), fake)\n        loss_fake.backward()\n\n        loss_D = (loss_real + loss_fake) \/ 2\n\n        optimizer_D.step()\n\n        # train G\n        G.zero_grad()\n        loss_G = criterion(D(fake_img), real)\n        loss_G.backward()\n        optimizer_G.step()\n\n    with torch.no_grad():\n        noise_img = G(fixed_noise)\n        generate_imgs.append(noise_img)\n        print(f'[Epoch {epoch+1}\/{epoches}] [G loss: {loss_G.item()}] [D loss: {loss_D.item()} | loss_real: {loss_real.item()} loss_fake: {loss_fake.item()}]')","02107453":"import matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\n\nimgs = []\nfor batch_images in generate_imgs:\n    imgs.append([plt.imshow(make_grid(batch_images[:64], padding=2, normalize=True).cpu().permute(1,2,0))])\n\nani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\nHTML(ani.to_jshtml())","232f0847":"# DCGAN"}}