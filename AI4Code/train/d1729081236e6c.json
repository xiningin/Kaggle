{"cell_type":{"524853dd":"code","b04f90e1":"code","75f9e956":"code","5c48279e":"code","a1d8bfe8":"code","e86c786d":"code","bb6b40ba":"code","04a39871":"code","22ef89a7":"code","5fc5a32a":"code","e28527bc":"code","df4c5140":"code","cbd8930b":"code","32318269":"code","0b5e1781":"code","953e5afb":"code","35d1b06c":"code","8ad4ae88":"code","1adfd4a0":"code","f9b2115d":"code","bf172b86":"code","c3e11eda":"code","30930aa5":"code","1e764dc7":"code","9f201390":"code","17eec06e":"code","98083ad2":"markdown","dbee0d65":"markdown","d2a8f5ab":"markdown","d1f6f9e3":"markdown","569af93e":"markdown","4f297210":"markdown","197fd26a":"markdown","22322bac":"markdown","feee9279":"markdown","213dfde9":"markdown","26ad555b":"markdown","7443a55e":"markdown","15cd59d3":"markdown","fab474d9":"markdown","05e5756d":"markdown","882d6e19":"markdown","c5058952":"markdown","18e6c915":"markdown","d0e70a31":"markdown","1aa48797":"markdown"},"source":{"524853dd":"#Importing required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b04f90e1":"train_data = pd.read_csv(\"..\/input\/sign-dataset\/sign_mnist_train.csv\")\ntest_data = pd.read_csv(\"..\/input\/sign-dataset\/sign_mnist_test.csv\")","75f9e956":"train_data.head()","5c48279e":"test_data.head()","a1d8bfe8":"train_y = np.array(train_data)[:,0]\ntrain_x = np.array(train_data,dtype = \"float64\")[:,1:]\ntest_y = np.array(test_data)[:,0]\ntest_x = np.array(test_data,dtype = \"float64\")[:,1:]\nprint(\"The shape of train_x is \",train_x.shape)\nprint(\"The shape of test_x is \",test_x.shape)\nprint(\"The shape of train_y is \",train_y.shape)\nprint(\"The shape of test_y is \",test_y.shape)","e86c786d":"plt.imshow(train_x[0].reshape([28,28]))\nplt.title(\"Labelled \"+str(train_y[0]))\nplt.show()\n\nplt.imshow(train_x[1].reshape([28,28]))\nplt.title(\"Labelled \"+str(train_y[1]))\nplt.show()\n\nplt.imshow(train_x[2].reshape([28,28]))\nplt.title(\"Labelled \"+str(train_y[2]))\nplt.show()","bb6b40ba":"def sigma(X,W):\n    \"\"\"\n        This function  is the function sigma, which taker W and X as arguments and returns the sigma value.\n    \"\"\"\n    s = np.exp(np.matmul(X,W))\n    total = np.sum(s, axis=1).reshape(-1,1)\n    return s\/total\n\n\ndef loss(W,X,y,K):\n    \"\"\"\n        This function  is to get the value of loss, here K is the number of classes and others are as usual.\n    \"\"\"\n    f = sigma(X,W)\n    loss_vector = np.zeros(X.shape[0])\n    #iterating through all values of K to get the loss\n    for i in range(K):\n        loss_vector += np.log(f+1e-10)[:,i] * (y == i) # avoid nan issues\n    \n    return float(-np.mean(loss_vector))","04a39871":"def loss_gradient(W,X,y,K):\n    \"\"\"\n        this function below uses the formula above to get the gradient of the loss function given the arguments.\n        It takes weights, data and number of classes as arguments and returns the gradient. \n    \"\"\"\n    f = sigma(X,W)\n    #initializing with zeros, then in the for loop below its is calculated\n    dLdW = np.zeros((X.shape[1],K))\n    for k in range(K):\n        dLdW[:,k] = np.mean((f[:,k] - (y==k)).reshape(-1,1)*X , axis=0) \n    return dLdW","22ef89a7":"def logistic_fit(X,y,test_x,test_y,n_iterations = 1000, l_r = 0.1):\n    \"\"\"\n        This function fits the logistic classification model using the functions defined above. \n        it can be used for multiclass classificationa also. This takes data, number of iterations and learning rate as\n        arguments and returns the final weight matrix, which can be used for prediction. \n    \"\"\"\n    #number of classification is taken as pne more than max value of y as it includes 0 \n    K = max(y)+1 \n    ones = np.ones((X.shape[0],1))\n    #ones are concatenated to include the constant factor \n    saved_X = X\n    X = np.concatenate((ones,X),axis = 1) \n    #initializing the weight matrix\n    W = np.zeros((np.shape(X)[1],max(y)+1))  \n    for k in range(n_iterations):\n        dW =loss_gradient(W,X,y,K) \n        #below is the formula to update the weights\n        W = W - l_r * dW \n        #this prints the loss value every 100 iterations\n        if k % 50 == 0 or k%n_iterations==0:\n            test_acc = score(test_x,test_y,W)\n            train_acc = score(saved_X,y,W)\n            print(\"Iteration number : \", k+1, \"|| Loss : \", \"{:.3f}\".format(loss(W,X,y,K)), \" || Train accuracy : \",\"{:.2f}\".format(100*train_acc),\"% || Test accuracy : \",\"{:.2f}\".format(100*test_acc),\"%\")\n            if(k%4000 == 0):\n                l_r = l_r\/2\n    return W\n\ndef predict(X,W):\n    \"\"\" \n        This function takes the X data and the weights which are obtained by using the logistic fit function above.\n        It returns the predicted values for the data which is given as arguments. \n    \"\"\"\n    #need to concatenate a one dimensional array to X to multiply it with weights to get the final probabilities.\n    ones = np.ones((X.shape[0],1)) \n    X = np.concatenate((ones, X), axis = 1) \n    return np.argmax(sigma(X,W), axis =1)  #returns the position at which probability is maximum\n\ndef score(X, y_true,W):\n    \"\"\"This function returns the accuracy score for the given X and true values y.\n        data and weights are given as argument and it returns the accuracy score.\n    \"\"\"\n    y_pred = predict(X,W)\n    return float(np.mean(y_pred == y_true))\n","5fc5a32a":"#normalizing the train_x data before training\ntrain_X = train_x\/255\ntest_X = test_x\/255\nW = logistic_fit(train_X,train_y,test_X,test_y,4000,0.3)","e28527bc":"print(\"The accuracy of this model on test data is : \",100*score(test_X,test_y,W),\"5\")","df4c5140":"class PCA():\n    \"\"\"\n        This is the class which implements PCA from covariance matrix\n    \"\"\"\n    def __init__(self,n_components = 1):\n        \"\"\" \n            Constructor for PCA class which takes number of components to which required array\n            is to be reduced as argument, default value for number of components is 1.\n        \"\"\"\n        self.n_components = n_components\n    \n    def fit(self,X):\n        \"\"\"\n            This method fits the data to the model to reduce the components to n_components. it saves them in principal \n            components and also calculates variance ratio and eigen values.\n        \"\"\"\n        covariance_matrix = np.cov(X.T)\n        #np.linalg.eigh() returns eigen values and orthogonal eigen-vectors in ascending order\n        eigen_val, eigen_vec = np.linalg.eigh(covariance_matrix) \n        #these are reversed to get in descending order\n        eigen_val, eigen_vec = np.flip(eigen_val), np.flip(eigen_vec,axis=1) \n        #eigen_vec = np.flip(eigen_vec,axis=1) # reverse the order\n        #top n_components values are selected \n        self.eigen_values = eigen_val[:self.n_components] \n        self.principle_components = eigen_vec[:,:self.n_components] \n        self.variance_ratio = self.eigen_values\/eigen_val.sum() \n        \n    def transform(self,X):\n        \"\"\"\n            This method reduces X from original dimesions to n_components dimensions. \n        \"\"\"\n        transformed = np.matmul(X-X.mean(axis = 0),self.principle_components)\n        return transformed\n    \n    def fit_transform(self,X):\n        \"\"\"\n            This method fits the data using fit method above and then transforms it into required number\n            of components and returns it.\n        \"\"\"\n        self.fit(X)\n        return self.transform(X) ","cbd8930b":"mypca = PCA(25)\n#fit and transfor train_x taking n_components as 25\ntest_x_pca = mypca.fit_transform(test_x)","32318269":"#visualization\nsns.set()\nfigure = plt.figure(dpi = 120)\nplt.scatter(test_x_pca[:, 0], test_x_pca[:, 1],c=test_y, s=25, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('tab20', 25))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar()","0b5e1781":"print(mypca.variance_ratio)","953e5afb":"fig, axs = plt.subplots(3,3, figsize=(18,18))\nfig.subplots_adjust(hspace = .5, wspace=.1)\naxs = axs.ravel()\nfor i in range(9):\n    axs[i].imshow(mypca.principle_components[:,i].reshape(28,28),cmap=plt.cm.gray)\n    axs[i].set_title('Principle Component '+str(i+1))","35d1b06c":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(train_x,train_y)\nprint(\"The accuracy that we get by using DecisionTreeClassifier is :\",\"{:.2f}\".format(100*dtc.score(test_x,test_y)),\"%\")","8ad4ae88":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(train_x,train_y)\nprint(\"The accuracy that we get by using SVM is :\",\"{:.2f}\".format(100*clf.score(test_x,test_y)),\"%\")","1adfd4a0":"from sklearn.cluster import KMeans\nkmeans = KMeans().fit(train_x)\nprint(\"The number of clusters is \",kmeans.get_params()[\"n_clusters\"])","f9b2115d":"#importing required functions\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout,BatchNormalization, Conv2D,MaxPool2D,Flatten\nfrom keras.models import Sequential","bf172b86":"def one_hot_encoder(data):\n    \"\"\"\n        This function is used for one hot encoding the labels.\n    \"\"\"\n    ans = np.zeros([data.shape[0],np.max(data)+1])\n    for i in range(data.shape[0]):\n        ans[i][data[i]] = 1\n    return ans","c3e11eda":"# Data preprocessing to use it in our model.\ny_train_encoded = one_hot_encoder(train_y)\nx_train = np.zeros([train_x.shape[0],32,32,3])\nx_test = np.zeros([test_x.shape[0],32,32,3])\ny_train = train_y\ny_test = test_y\nfor i in range(train_x.shape[0]):\n    temp =  np.stack((train_x[i].reshape([28,28]),)*3,axis = -1)\n    x_train[i,:,:,0] = np.pad(temp[:,:,0],(2),\"constant\")\n    x_train[i,:,:,1] = np.pad(temp[:,:,1],(2),\"constant\")\n    x_train[i,:,:,2] = np.pad(temp[:,:,2],(2),\"constant\")\nfor i in range(test_x.shape[0]):\n    temp = np.stack((test_x[i].reshape([28,28]),)*3,axis = -1)\n    x_test[i,:,:,0] = np.pad(temp[:,:,0],(2),\"constant\")\n    x_test[i,:,:,1] = np.pad(temp[:,:,1],(2),\"constant\")\n    x_test[i,:,:,2] = np.pad(temp[:,:,2],(2),\"constant\")","30930aa5":"#Preparing the model that is used\nmodel = Sequential()\nmodel.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\nmodel.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\n# output layer\nmodel.add(Dense(25, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\nmodel.summary()","1e764dc7":"#preparing training and validation data\nX_val = x_train[-5000:]\nY_val = one_hot_encoder(y_train[-5000:])\nX_train = x_train[:-5000]\nY_train = one_hot_encoder(y_train[:-5000])","9f201390":"#Fitting the model \nmodel.fit(X_train, Y_train, batch_size=128, epochs=20, validation_data=(X_val, Y_val))","17eec06e":"print(\"The accuracy of this model on test data is \",\"{:.2f}\".format(100*model.evaluate(x_test,one_hot_encoder(test_y))[1]),\"%\")","98083ad2":"### 3. Modelling\nHere in this model, the predicted probability for K classes is given by, here x denotes the augmented row vector.\n\n![image.png](attachment:42f0a0c3-85bf-4a9e-a6c7-16d2bc8cb64a.png)\n\nhere in the above formula all w are the model parameters which makes it to get the probability result for each class. W is a (p+1) X k matrix,which contains parameters for all K predictions. Now, since we have our loss function ready and the gradient descent alss. so now we can use our algorithm to fit the data","dbee0d65":"#### Using head funtion on test data.","d2a8f5ab":"# Task 5\n#### Using tensorflow to make neural network model for the classification. Convolution and maxpool layers are also used for classificationt to get good accuracy. ","d1f6f9e3":"#### From all above models we can observe that using deep learning we can achieve high accuracy. This deep neural network model performs best of all the models we have used so far.","569af93e":"# Task 2\n## Logistic regression multi class classification\n### 1. Loss Function\nUsing the crossentropy loss function\n\n![image.png](attachment:c3ace087-12fe-42a8-a2b7-d5ce6df5cc3a.png)\n\nwhere the indicator function is\n\n![image.png](attachment:f0b274f3-1dee-454a-8350-8d068a7c54a9.png)\n","4f297210":"##### we can see above that SVM performs much better than decision tree classifier. Accuracy by SVM is 84% and that by decision tree classifier is 44%.","197fd26a":"#### Using the PCA implemented above on train_x","22322bac":"#### All our functions are ready, so now the model can be used on the data we have to make predictions.","feee9279":"# Task 3\n## Principal Component Analysis","213dfde9":"### 2. Gradient Descent\nThe grandient of the loss function with respect to the kth set of weights is given by\n\n![image.png](attachment:0f116f45-326c-464f-b002-72fca6e6a2c2.png)","26ad555b":"#### 2. Support Vector Machine - SVM\n    In this algorithm, each data item is plotted in an n-dimensional space (n is number of features), where the value of each feature is its value at that particular coordinate. Then classification is performed by using the hyperplance that differentiates this features well.\n    The classifier minimizes the hinge loss function in cases where the data is not linearly separable. Hinge loss function has formula :\n   ![image.png](attachment:bdd54494-58eb-46bf-be39-64db19090442.png)\n   \n   The optimizer in this case minimizes the following loss function\n   \n   ![image.png](attachment:1f21392b-6943-47f1-ac9a-48f32e01b6e9.png)\n\n    \n   \n    ","7443a55e":"#### Let us now visualize some of the signs","15cd59d3":"#### It is observed that the prepared model overfits the training data. We will see better models to fit well in the models below.","fab474d9":"#### 3. K-means clustering\n    K-means clustering algorithm groups the dataset into k different clusters. Expectation maximization approach is used to solve the problem. E-step assigns the data points to the closest cluster, then M-step computes centroid of each cluster. The objective funtion is :\n  ![image.png](attachment:fc535a54-bbf2-4bce-a6ad-fc995ad7b212.png)\n  \n  where wik=1,  if it belongs to cluster k else wik=0. \n  \n  \u03bck is the centroid of xi\u2019s cluster.","05e5756d":"#### Visualizing some principal components","882d6e19":"#### We now need to convert these dataframes into numpy array, first column is label and hence this needs to be as train_y test_y respectively and all other columns are included in train_x and test_x.","c5058952":"#### Using head funtion on train data.","18e6c915":"# Task 4\n### Applying other algorithms from sklearn.\n#### 1. DecisionTreeClassifier\n    This tree classification model designs several questions to reach the target variable, it travels down the tree asking several questions and at the leaf we reach the result. It learns a sequence of if then questions along with a split point at each edge to travel down to the leaf node. ","d0e70a31":"# Task 1\n#### Train and test data files are read using pandas module, it reads them as dataframe. Using .head() finction of dataframe we can see the top 5 rows of the data to get some insights into it. ","1aa48797":"Decision tree classifier does not perform good, the accuracy is quite bad."}}