{"cell_type":{"29c8e985":"code","10bcaa99":"code","9451578a":"code","6fd2ece2":"code","37f4c6bf":"code","621ba794":"code","2ce9edb2":"code","500b9219":"code","ddb274d8":"code","5e35ef0f":"code","de8b13ea":"code","1437e2c2":"code","e1f36d63":"code","11de6c31":"code","77c1fec7":"code","cb69e10c":"code","df75bd04":"markdown","f983f7c7":"markdown"},"source":{"29c8e985":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport re\nimport unidecode\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom scipy.stats import rankdata\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\ntrain_set = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ntrain_set.head(3)","10bcaa99":"# Function for cleaning comments\ndef clean_data(data):\n    final = []\n    for sent in data:\n        sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n        soup = BeautifulSoup(sent, \"html.parser\")\n        sent = soup.get_text(separator=\" \")\n        remove_https = re.sub(r'http\\S+', '', sent)\n        sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n        sent = unidecode.unidecode(sent)\n        sent = sent.lower()\n        sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n        sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n        stoplist = stopwords.words(\"english\")\n        sent = [word for word in word_tokenize(sent) if word not in stoplist]\n        sent = \" \".join(sent)\n        final.append(sent)\n    \n    return final","9451578a":"train_set['toxicity'] = train_set.drop(['id', 'comment_text'], axis=1).sum(axis=1)\ntrain_set.toxicity.value_counts()","6fd2ece2":"# Use only 'n' non-toxic comments\nn = 15000\n\nnontoxic_sample = train_set[train_set.toxicity==0].sample(n, random_state = seed)\ntrain = pd.concat([train_set[train_set.toxicity!=0], nontoxic_sample]).sort_index()\ntrain = train[['comment_text', 'toxicity']]\ntrain.toxicity.value_counts()","37f4c6bf":"train['comment_text'] = clean_data(train.comment_text)\ntest['text'] = clean_data(test.text)\n\nmax_sequence_len = 250\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.comment_text)\ntotal_words = len(tokenizer.word_index)+1\n\nX = tokenizer.texts_to_sequences(train.comment_text)\nX = pad_sequences(X, maxlen = max_sequence_len, padding='pre')\n\nx_test = tokenizer.texts_to_sequences(test.text)\nx_test = pad_sequences(x_test, maxlen = max_sequence_len)\n\ny = train.toxicity.astype(np.int8)#.clip(0,4)","621ba794":"non_toxic_tokenizer = Tokenizer()\nnon_toxic_tokenizer.fit_on_texts(train[train.toxicity==0].comment_text)\nnon_toxic_count = non_toxic_tokenizer.word_counts\nnon_toxic_count = sorted(dict(non_toxic_count).items(), key=lambda tup: tup[1], reverse=True)\n\ntoxic_tokenizer = Tokenizer()\ntoxic_tokenizer.fit_on_texts(train[train.toxicity>0].comment_text)\ntoxic_count = toxic_tokenizer.word_counts\ntoxic_count = sorted(dict(toxic_count).items(), key=lambda tup: tup[1], reverse=True)[:200]\n\nall_words = pd.DataFrame(toxic_count).merge(pd.DataFrame(non_toxic_count), how='left', on=0)\nall_words = all_words.dropna().reset_index()\n\nall_words['ratio'] = np.log(all_words['1_x'] \/ all_words['1_y'])","2ce9edb2":"import plotly.express as px\n\nfig = px.scatter(all_words.iloc[:100], x=\"1_y\", y=\"1_x\", text=0, log_x=True, log_y=True, color=\"ratio\", color_continuous_scale='Portland',\n                labels={\n                     \"1_x\": \"Number of word's appearance in toxic comments\",\n                     \"1_y\": \"Number of word's appearance in non-toxic comments\"})\n\nfig.update_traces(textposition='top center')\n\nfig.update_layout(\n    height=800,\n    title_text='100 most frequent words in toxic comments',\ncoloraxis_showscale=False)\n\nfig.show()","500b9219":"embedding_dim = 256  # Embedding size for each token.\nnum_heads = 4  # Number of attention heads\nff_dim = 384 #  Hidden layer size in feedforward network.\nbatch_size = 128  # Batch size.\nclasses = len(y.unique())\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embedding_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n    \nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embedding_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n","ddb274d8":"inputs = layers.Input(shape=(max_sequence_len,))\nembedding_layer = TokenAndPositionEmbedding( max_sequence_len, total_words, embedding_dim)\nx = embedding_layer(inputs)\nx = TransformerBlock(embedding_dim, num_heads, ff_dim)(x)\nx = TransformerBlock(embedding_dim, num_heads, ff_dim)(x)\nx = layers.Flatten()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(classes, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate = 7e-5), metrics=['accuracy'])","5e35ef0f":"model.summary()","de8b13ea":"tf.keras.utils.plot_model(model, show_shapes=True)","1437e2c2":"tf.random.set_seed(seed)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=seed)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience = 7,restore_best_weights = True)\nmodel.fit(X_train, y_train, validation_data = (X_val, y_val),\n    epochs = 40, \n    batch_size = batch_size, \n    shuffle = True,\n    callbacks = [early_stopping]\n)","e1f36d63":"### validate\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n\ndf_val['less_toxic'] = clean_data(df_val['less_toxic'])\ndf_val['more_toxic'] = clean_data(df_val['more_toxic'])\n\nX_less_toxic = tokenizer.texts_to_sequences(df_val['less_toxic'])\nX_more_toxic = tokenizer.texts_to_sequences(df_val['more_toxic'])\n\nX_less_toxic = pad_sequences(X_less_toxic, maxlen = max_sequence_len)\nX_more_toxic = pad_sequences(X_more_toxic, maxlen = max_sequence_len)\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n\np1 = (np.linspace(0,classes-1,classes) * p1).sum(axis=1)\np2 = (np.linspace(0,classes-1,classes) * p2).sum(axis=1)\n\n# Validation Accuracy\n(p1< p2).mean()","11de6c31":"x_test = tokenizer.texts_to_sequences(test.text)\nx_test = pad_sequences(x_test, maxlen = max_sequence_len)","77c1fec7":"preds = model.predict(x_test)\npreds = (np.linspace(0,classes-1,classes) * preds).sum(axis=1)","cb69e10c":"# Making submission file\n\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = rankdata(preds)\nfinal.to_csv(\"submission.csv\", index=False)","df75bd04":"## Multi-label classification with simple keras transformer\nTraining set is based on the data from previous competitions that are available in this dataset: https:\/\/www.kaggle.com\/julian3833\/jigsaw-toxic-comment-classification-challenge\n<br>The target is a sum of initial toxic labels from the dataset and ranges from 0 to 6. Final score calculated as dot product of labels and their probabilities for each comment.\n<br>The model obtained from the 'Keras Code examples' section.\n\nCredits:\n* https:\/\/www.kaggle.com\/steubk\/jrsotc-ridgeregression \n* https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n* https:\/\/www.kaggle.com\/devkhant24\/jigsaw-comment-toxicity-gru\/\n* https:\/\/keras.io\/examples\/nlp\/text_classification_with_transformer\/","f983f7c7":"#### The most of the comments in loaded dataset are non-toxic. Only 'n' of them are used for training."}}