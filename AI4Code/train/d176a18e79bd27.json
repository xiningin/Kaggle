{"cell_type":{"f5ec41dc":"code","7f21fd6f":"code","573912f0":"code","5a5f3aa3":"code","0dd1fefa":"code","b7357450":"code","acc1ad65":"code","bf3c9706":"code","e3be7c88":"code","49ad654a":"code","162184c1":"code","d489fb58":"code","d90d68cc":"markdown","920a04bf":"markdown","476be6a4":"markdown","ca91892b":"markdown","b0d47fd4":"markdown","0c55c695":"markdown"},"source":{"f5ec41dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7f21fd6f":"data= pd.read_csv(\"..\/input\/heart.csv\")\ndata.head()","573912f0":"data.columns","5a5f3aa3":"data.info()","0dd1fefa":"assert data.target.notnull().all()\n#returns nothing it means we don't have any nan values.","b7357450":"X = data.iloc[:,1:14]  #independent columns (except age)\ny = data.iloc[:,0]    #target column i.e age ","acc1ad65":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class ","bf3c9706":"#feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title(\"top 10 most important features in data\")\nplt.show()","e3be7c88":"x_data= data.drop([\"cp\"],axis=1)\nX = x_data.iloc[:,0:13]  #independent columns (except cp)\ny = data.iloc[:,2]    #target column i.e cp ","49ad654a":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class ","162184c1":"#feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title(\"top 10 most important features in data\")\nplt.show()","d489fb58":"X = x_data.iloc[:,0:13]  #independent columns\ny = data.iloc[:,2]    #target column i.e cp\n#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d90d68cc":"### Another method:","920a04bf":"Also we can use different methods for feature selection. I used \"Feature Importance\" and \"Correlation\" methods. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable, as you can see!","476be6a4":"So I can say; chol, thalach, oldpeak, trestbps and ca (maybe cp) are the most important features for this problem.\n> Let's think our problem is guess the risk of heart diseases according to cp values. So our target is \"cp\"","ca91892b":"**> Let's think our problem is guess the risk of heart diseases according to ages. So our target is \"age\".**","b0d47fd4":"**If I made a mistake in any part, please correct me! Thanks \ud83c\udf89**","0c55c695":"> Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of our model. \nAccording to my experience (not so many \ud83d\udc40), Feature selection and Data cleaning should be the first and most important step of your model designing.\nSo I want to share what I learned. \ud83c\udf88"}}