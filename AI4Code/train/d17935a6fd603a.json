{"cell_type":{"00c24f4f":"code","a5f5f329":"code","84d957c9":"code","edeb98e0":"code","57454086":"code","2e78101c":"code","08a328ba":"code","5acff9a0":"code","179c07da":"code","beef749f":"code","89935160":"code","0756eb5c":"code","642c8b04":"code","51f14971":"code","c983e816":"code","7f12f3d3":"code","95844c05":"code","74f658f7":"code","813c1601":"markdown","23ee8caa":"markdown","f05c8846":"markdown","39e692f9":"markdown","dbd68dae":"markdown","b3caec39":"markdown","6fb10287":"markdown","ce3a60a8":"markdown","c1a0de65":"markdown","c178d7b0":"markdown"},"source":{"00c24f4f":"import os\nimport gc\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing GPU warnings\n\nimport numpy as np\nimport pandas as pd\nimport feather\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-darkgrid')   #for plotting learning curves\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='medium',\n       titleweight='bold', titlesize=12, titlepad=10)\n\nSEED = 2311","a5f5f329":"data_dir = '..\/input\/tabular-playground-series-nov-2021\/'","84d957c9":"%%time\ntrain = pd.read_csv(data_dir + 'train.csv', index_col='id')\ntest = pd.read_csv(data_dir + 'test.csv', index_col='id')","edeb98e0":"train.head()","57454086":"train.info()","2e78101c":"print(f'Missing values - Train data: {train.isna().sum().sum()}, Test data: {test.isna().sum().sum()}')","08a328ba":"target = train.pop('target')\nsubmission_index = test.index","5acff9a0":"target.value_counts()","179c07da":"features = list(train.columns)\nfeatures[:3], features[-3:]","beef749f":"numerical_transformer = make_pipeline(\n    StandardScaler(), #Standardization\n    MinMaxScaler(),    #Normalization\n)\n\npreprocessor = make_column_transformer(\n    (numerical_transformer, features), #since all features are numerical\/continous\n)\n\ntrain = preprocessor.fit_transform(train)\ntest = preprocessor.transform(test)\n\ngc.collect()","89935160":"xtrain, xval, ytrain, yval = train_test_split(train, target, \n                                              stratify=target, \n                                              test_size=0.3, \n                                              random_state=SEED)","0756eb5c":"del train, target\n\ngc.collect()","642c8b04":"input_shape = [xtrain.shape[1]]\nPATIENCE = 10\nMIN_DELTA = 0.0005\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(units=64, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(units=1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['AUC'])\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=PATIENCE,\n    min_delta=MIN_DELTA,\n    restore_best_weights=True,\n)","51f14971":"model.summary()","c983e816":"%%time\n\nBATCH_SIZE = 128\nEPOCHS = 100\n\nhistory = model.fit(\n    xtrain, ytrain,\n    validation_data=(xval, yval),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[\n        early_stopping\n    ]\n)","7f12f3d3":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Loss\")\nhistory_df.loc[:, ['auc', 'val_auc']].plot(title=\"AUC\")","95844c05":"predictions = model.predict(test).ravel()","74f658f7":"output = pd.DataFrame({'id': submission_index, 'target': predictions})\noutput.to_csv('submission.csv', index=False)\n\n!head submission.csv","813c1601":"<a id=\"training\"><\/a>\n# Model Training  \n\n[Back to top](#top)","23ee8caa":"<a id=\"imports\"><\/a>\n# Imports","f05c8846":"Balanced target variable. No need for resampling.","39e692f9":"[Back to top](#top)","dbd68dae":"**Time to submit!**","b3caec39":"No missing data. No need for imputation.","6fb10287":"<a id=\"top\"><\/a>\nTable of contents:  \n1. [Imports](#imports)\n2. [Data Exploration](#eda)\n3. [Data Preparation](#dataprep)\n4. [Network architecture](#arch)\n5. [Model Training](#training)","ce3a60a8":"<a id=\"eda\"><\/a>\n# Data exploration  \n\n[Back to top](#top)","c1a0de65":"<a id=\"dataprep\"><\/a>\n# Data Preparation  \nAll features are continous variables. We will apply standardization followed by normalization.  \n\n[Back to top](#top)","c178d7b0":"<a id=\"arch\"><\/a>\n# Building the network architecture  \n\n[Back to top](#top)"}}