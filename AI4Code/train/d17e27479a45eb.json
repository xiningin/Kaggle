{"cell_type":{"16f40e2f":"code","a8db6c0a":"code","668f9225":"code","ac01b8bc":"code","f2cba09d":"code","c2926dbc":"code","bf95981d":"code","0a05b21c":"code","f5cf16d2":"code","2dbfba2c":"code","f1e5c3c2":"code","bb7d8974":"code","f4bdd7bf":"code","16303465":"code","5c8f362e":"code","2e376739":"code","6603b0cd":"code","21fece43":"code","45ce9476":"code","a22c9016":"code","1915c2c7":"code","4d5b6206":"code","f9d98633":"code","c5c29c70":"code","024722b9":"code","719bbd7c":"code","483b8aa2":"code","afbb67e8":"code","f47234bd":"code","ae43596b":"code","4804fda6":"code","7e0c95e7":"code","2f8418bd":"code","323eb245":"code","2d27d0f3":"code","7c752c55":"code","5a49e562":"code","ee033804":"code","97bc0636":"markdown","bd346983":"markdown","91eea32c":"markdown","fdf17b8d":"markdown","94fa4d6c":"markdown","f798f503":"markdown","ff2bb0df":"markdown","0dc3e85c":"markdown","c5b33e8d":"markdown","7d4ef925":"markdown","c9d3e575":"markdown","7923cde6":"markdown","7870184b":"markdown","98a7db61":"markdown","bddfd013":"markdown","877dfaf5":"markdown","64a40664":"markdown","128273d9":"markdown","006c5dae":"markdown","69244560":"markdown","4f7ad01a":"markdown","304292ff":"markdown","0a0fd7ee":"markdown","18cba2c9":"markdown","bf00f172":"markdown","ffe8affe":"markdown","db53941c":"markdown","4771ea56":"markdown","97244185":"markdown","88a5cd99":"markdown","665088bf":"markdown","3df66e26":"markdown","6cd1d0bf":"markdown","2f58c0a4":"markdown","efc8d410":"markdown","8758f049":"markdown"},"source":{"16f40e2f":"# Python libraries\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, roc_auc_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nimport xgboost as xgb\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nwarnings.filterwarnings('ignore')","a8db6c0a":"data = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","668f9225":"null_feat = pd.DataFrame(len(data['Attrition']) - data.isnull().sum(), columns = ['Count'])\n\ntrace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'lightgrey',\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  \"Missing Values\")\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","ac01b8bc":"# Reassign target\ndata.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)\n# Drop useless feat\ndata = data.drop(columns=['StandardHours', \n                          'EmployeeCount', \n                          'Over18',\n                        ])","f2cba09d":"# head\ndata.head()","c2926dbc":"# describe\ndata.describe()","bf95981d":"attrition = data[(data['Attrition'] != 0)]\nno_attrition = data[(data['Attrition'] == 0)]\n\n#------------COUNT-----------------------\ntrace = go.Bar(x = (len(attrition), len(no_attrition)), y = ['Yes_attrition', 'No_attrition'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=['gold', 'lightskyblue'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of attrition variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)\n\n#------------PERCENTAGE-------------------\ntrace = go.Pie(labels = ['No_attrition', 'Yes_attrition'], values = data['Attrition'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of attrition variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","0a05b21c":"def plot_distribution(var_select, bin_size) : \n# Calculate the correlation coefficient between the new variable and the target\n    corr = data['Attrition'].corr(data[var_select])\n    corr = np.round(corr,3)\n    tmp1 = attrition[var_select]\n    tmp2 = no_attrition[var_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Yes_attrition', 'No_attrition']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, curve_type='kde', bin_size = bin_size)\n    \n    fig['layout'].update(title = var_select+' '+'(corr target ='+ str(corr)+')')\n\n    py.iplot(fig, filename = 'Density plot')","f5cf16d2":"def barplot(var_select, x_no_numeric) :\n    tmp1 = data[(data['Attrition'] != 0)]\n    tmp2 = data[(data['Attrition'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Attrition']), )\n    tmp3['Attr%'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n    if x_no_numeric == True  : \n        tmp3 = tmp3.sort_values(1, ascending = False)\n\n    color=['lightskyblue','gold' ]\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        name='Yes_Attrition',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    \n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        name='No_Attrition', opacity = 0.8, marker=dict(\n        color='lightskyblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['Attr%'],\n        yaxis = 'y2',\n        name='% Attrition', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select),\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% Attrition'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","2dbfba2c":"plot_distribution('Age', False)\nbarplot('Age', False)\nplot_distribution('DailyRate', 100)\nplot_distribution('DistanceFromHome', False)\nbarplot('DistanceFromHome', False)\nplot_distribution('HourlyRate', False)\nplot_distribution('MonthlyIncome', 100)\nplot_distribution('MonthlyRate', 100)\nplot_distribution('NumCompaniesWorked', False)\nbarplot('NumCompaniesWorked',False)\nplot_distribution('PercentSalaryHike', False)\nbarplot('PercentSalaryHike', False) \nplot_distribution('TotalWorkingYears', False)\nbarplot('TotalWorkingYears', False)\nplot_distribution('TrainingTimesLastYear', False)\nbarplot('TrainingTimesLastYear',False)\nplot_distribution('YearsAtCompany', False)\nbarplot('YearsAtCompany', False)\nplot_distribution('YearsInCurrentRole', False)\nbarplot('YearsInCurrentRole', False)\nplot_distribution('YearsSinceLastPromotion', False)\nbarplot('YearsSinceLastPromotion', False)\nplot_distribution('YearsWithCurrManager', False)\nbarplot('YearsWithCurrManager', False)","f1e5c3c2":"def plot_pie(var_select) :\n    \n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue', 'lightgrey', 'orange', 'white', 'lightpink']\n    trace1 = go.Pie(values  = attrition[var_select].value_counts().values.tolist(),\n                    labels  = attrition[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"attrition employes\",\n                    marker  = dict(colors = colors, line = dict(width = 1.5)))\n    trace2 = go.Pie(values  = no_attrition[var_select].value_counts().values.tolist(),\n                    labels  = no_attrition[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(colors = colors, line = dict(width = 1.5)),\n                    domain  = dict(x = [.52,1]),\n                    name    = \"Non attrition employes\" )\n\n    layout = go.Layout(dict(title = var_select + \" distribution in employes attrition \",\n                            annotations = [dict(text = \"Yes_attrition\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .22, y = -0.1),\n                                            dict(text = \"No_attrition\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = -.1)]))\n                                          \n\n    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n    py.iplot(fig)","bb7d8974":"plot_pie(\"Gender\")\nbarplot('Gender',True)\nplot_pie('OverTime')\nbarplot('OverTime',True)\nplot_pie('BusinessTravel')\nbarplot('BusinessTravel',True)\nplot_pie('JobRole')\nbarplot('JobRole',True)\nplot_pie('Department') \nbarplot('Department',True)\nplot_pie('MaritalStatus') \nbarplot('MaritalStatus',True)\nplot_pie('EducationField') \nbarplot('EducationField',True)\nplot_pie('Education') \nbarplot('Education',False)\nplot_pie('EnvironmentSatisfaction')\nbarplot('EnvironmentSatisfaction',False)\nplot_pie('JobInvolvement')\nbarplot('JobInvolvement', False)\nplot_pie('JobLevel')\nbarplot('JobLevel',False)\nplot_pie('JobSatisfaction')\nbarplot('JobSatisfaction',False)\nplot_pie('PerformanceRating')\nbarplot('PerformanceRating',False)\nplot_pie('RelationshipSatisfaction')\nbarplot('RelationshipSatisfaction', False)\nplot_pie('StockOptionLevel')\nbarplot('StockOptionLevel', False)\nplot_pie('WorkLifeBalance')\nbarplot('WorkLifeBalance', False)","f4bdd7bf":"def SalesDpt(data) :\n    if data['Department'] == 'Sales':\n        return 1\n    else:\n        return 0\ndata['SalesDpt'] = data.apply(lambda data:SalesDpt(data) ,axis = 1)\n\ndef JobInvCut(data) :\n    if data['JobInvolvement'] < 2.5 :\n        return 1\n    else:\n        return 0\ndata['JobInvCut'] = data.apply(lambda data:JobInvCut(data) ,axis = 1)\n\ndef MiddleTraining(data) :\n    if data['TrainingTimesLastYear'] >= 3 and data['TrainingTimesLastYear'] <= 6:\n        return 1\n    else:\n        return 0\ndata['MiddleTraining'] = data.apply(lambda data:MiddleTraining(data) ,axis = 1)\n\ndef MoovingPeople(data) :\n    if data['NumCompaniesWorked'] > 4:\n        return 1\n    else:\n        return 0\ndata['MoovingPeople'] = data.apply(lambda data:MoovingPeople(data), axis = 1)\n\ndata['TotalSatisfaction_mean'] = (data['RelationshipSatisfaction']  + data['EnvironmentSatisfaction'] + data['JobSatisfaction'] + data['JobInvolvement'] + data['WorkLifeBalance'])\/5\n\ndef NotSatif(data) : \n    if  data['TotalSatisfaction_mean'] < 2.35 :\n        return 1\n    else : \n        return 0\ndata['NotSatif'] = data.apply(lambda data:NotSatif(data) ,axis = 1)\n\ndef LongDisWL1(data) : \n    if  data['DistanceFromHome'] > 11 and data['WorkLifeBalance'] == 1 :\n        return 1\n    else : \n        return 0\ndata['LongDisWL1'] = data.apply(lambda data:LongDisWL1(data) ,axis = 1)\n\ndef LongDis(data) : \n    if  data['DistanceFromHome'] > 11:\n        return 1\n    else : \n        return 0\ndata['LongDis'] = data.apply(lambda data:LongDis(data) ,axis = 1)\n\ndef LongDisJobS1(data) : \n    if  data['DistanceFromHome'] > 11 and data['JobSatisfaction'] == 1 :\n        return 1\n    else : \n        return 0\ndata['LongDisJobS1'] = data.apply(lambda data:LongDisJobS1(data) ,axis = 1)\n\ndef LongDisJL1(data) : \n    if  data['DistanceFromHome'] > 11 and data['JobLevel'] == 1 :\n        return 1\n    else : \n        return 0\ndata['LongDisJL1'] = data.apply(lambda data:LongDisJL1(data) ,axis = 1)\n\ndef ShortDisNotSingle(data) : \n    if  data['MaritalStatus'] != 'Single' and data['DistanceFromHome'] < 5:\n        return 1\n    else : \n        return 0\ndata['ShortDisNotSingle'] = data.apply(lambda data:ShortDisNotSingle(data) ,axis = 1)\n\ndef LongDisSingle(data) : \n    if  data['MaritalStatus'] == 'Single' and data['DistanceFromHome'] > 11:\n        return 1\n    else : \n        return 0\ndata['LongDisSingle'] = data.apply(lambda data:LongDisSingle(data) ,axis = 1)\n\ndef Engaged(data) : \n    if data['Age'] > 35 and data['MaritalStatus'] != 'Single':\n        return 1\n    else : \n        return 0\ndata['Engaged'] = data.apply(lambda data:Engaged(data) ,axis = 1)\n\ndef YoungAndBadPaid(data) : \n    if data['Age'] < 35 and data['Age'] > 23 and (data['MonthlyIncome'] < 3500):\n        return 1\n    else : \n        return 0\ndata['YoungAndBadPaid'] = data.apply(lambda data:YoungAndBadPaid(data) ,axis = 1)\n\ndef YoungNeverEngaged(data) : \n    if data['Age'] < 24 and data['MaritalStatus'] == 'Single' :\n        return 1\n    else : \n        return 0\ndata['YoungNeverEngaged'] = data.apply(lambda data:YoungNeverEngaged(data) ,axis = 1)\n\ndata['Time_in_each_comp'] = (data['Age'] - 20) \/ ((data)['NumCompaniesWorked'] + 1)\ndata['RelSatisf_mean'] = (data['RelationshipSatisfaction']  + data['EnvironmentSatisfaction']) \/ 2\ndata['JobSatisf_mean'] = (data['JobSatisfaction'] + data['JobInvolvement']) \/ 2\ndata['Income_Distance'] = data['MonthlyIncome'] \/ data['DistanceFromHome']\ndata['Hrate_Mrate'] = data['HourlyRate'] \/ data['MonthlyRate']\ndata['Stability'] = data['YearsInCurrentRole'] \/ data['YearsAtCompany']\ndata['Stability'].fillna((data['Stability'].mean()), inplace=True)\ndata['Income_YearsComp'] = data['MonthlyIncome'] \/ data['YearsAtCompany']\ndata['Income_YearsComp'] = data['Income_YearsComp'].replace(np.Inf, 0)\ndata['Fidelity'] = (data['NumCompaniesWorked']) \/ data['TotalWorkingYears']\ndata['Fidelity'] = data['Fidelity'].replace(np.Inf, 0)","16303465":"barplot('Engaged', False)\nbarplot('YoungAndBadPaid', False)\nbarplot('YoungNeverEngaged', False)\nbarplot('LongDisSingle', False)\nbarplot('LongDisJL1', False)\nbarplot('ShortDisNotSingle', False)","5c8f362e":"data = data.drop(columns=[\n                        'Age',\n                        'MonthlyIncome',\n                        'YearsAtCompany',\n                        'DistanceFromHome',\n                        'PerformanceRating',\n                        'NumCompaniesWorked'\n                     ])\n\nprint (\"\\nMissing values :  \", data.isnull().sum().values.sum())","2e376739":"#customer id col\nId_col     = ['EmployeeNumber']\n#Target columns\ntarget_col = [\"Attrition\"]\n#categorical columns\ncat_cols   = data.nunique()[data.nunique() < 10].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")\ndata = data.drop(['EmployeeNumber'],axis = 1)","6603b0cd":"#correlation\ncorrelation = data.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   colorscale='Viridis',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        #height  = 1400,\n                        #width   = 1600,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","21fece43":"# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = data.corr().abs()\ncorr_matrix.head()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove :' % (len(to_drop)))\n\ndata = data.drop(columns = to_drop)\n\nto_drop","45ce9476":"def model_performance_plot(model) : \n    #conf matrix\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n\n    #show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                   y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto',\n                   orientation = 'h', opacity = 0.8,marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n    \n    #plot roc curve\n    model_roc_auc = round(roc_auc_score(y_test, y_score) , 3)\n    fpr, tpr, t = roc_curve(y_test, y_score)\n    trace3 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \",\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    # Precision-recall curve\n    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, print_grid=False, \n                        subplot_titles=('Confusion Matrix',\n                                        'Metrics',\n                                        'ROC curve'+\" \"+ '('+ str(model_roc_auc)+')',\n                                        'Precision - Recall curve'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance<\/b><br>'+str(model),\n                        autosize = False, height = 900,width = 830,\n                        plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                        paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                        margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1])))\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05])\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05])\n    fig.layout.titlefont.size = 14\n    \n    py.iplot(fig)","a22c9016":"def features_imp(model, cf) : \n\n    coefficients  = pd.DataFrame(model.feature_importances_)\n    column_data     = pd.DataFrame(list(data))\n    coef_sumry    = (pd.merge(coefficients,column_data,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    coef_sumry = coef_sumry[coef_sumry[\"coefficients\"] !=0]\n    trace = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Viridis\",\n                                  line = dict(width = .6,color = \"black\")))\n    layout = dict(title =  'Feature Importances xgb_cfl')\n                    \n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","1915c2c7":"#cumulative gain curve\ndef cum_gains_curve(model):\n    pos = pd.get_dummies(y_test).as_matrix()\n    pos = pos[:,1] \n    npos = np.sum(pos)\n    index = np.argsort(y_score) \n    index = index[::-1] \n    sort_pos = pos[index]\n    #cumulative sum\n    cpos = np.cumsum(sort_pos) \n    #recall\n    recall = cpos\/npos \n    #size obs test\n    n = y_test.shape[0] \n    size = np.arange(start=1,stop=369,step=1) \n    #proportion\n    size = size \/ n \n    #plots\n    model = 'xgb_cfl'\n    trace1 = go.Scatter(x = size,y = recall,\n                        name = \"Lift curve\",\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace2 = go.Scatter(x = size,y = size,\n                        name = \"Baseline\",\n                        showlegend=False,\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n\n    layout = dict(title = 'Cumulative gains curve'+' '+str(model),\n                  yaxis = dict(title = 'Percentage positive targeted',zeroline = False),\n                  xaxis = dict(title = 'Percentage contacted', zeroline = False)\n                 )\n\n    fig  = go.Figure(data = [trace1,trace2], layout = layout)\n    py.iplot(fig)","4d5b6206":"# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","f9d98633":"# Def X and Y\ny = np.array(data.Attrition.tolist())\ndata = data.drop('Attrition', 1)\nX = np.array(data.as_matrix())","c5c29c70":"data","024722b9":"\n# iterating the columns \nfor col in data.columns: \n    print(col) ","719bbd7c":"X[0]","483b8aa2":"# Train_test split\nrandom_state = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = random_state)","afbb67e8":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n        \nxgb_cfl = xgb.XGBClassifier(n_jobs = -1)\n\n\n# A parameter grid for XGBoost\nparams = {\n        'n_estimators' : [100, 200, 500, 750],\n        'learning_rate' : [0.01, 0.02, 0.05, 0.1, 0.25],\n        'min_child_weight': [1, 5, 7, 10],\n        'gamma': [0.1, 0.5, 1, 1.5, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 10, 12]\n        }\n\nfolds = 5\nparam_comb = 800\n\nrandom_search = RandomizedSearchCV(xgb_cfl, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=5, verbose=3, random_state=42)\n\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\n#----------------------------# random_search.fit(X, y)\ntimer(start_time) # timing ends here for \"start_time\" variable","f47234bd":"#print('\\n All results:')\n#print(random_search.cv_results_)\n#print('\\n Best estimator:')\n#print(random_search.best_estimator_)\n#print('\\n Best accuracy for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n#print(random_search.best_score_ )\n#print('\\n Best hyperparameters:')\n#print(random_search.best_params_)\n#results = pd.DataFrame(random_search.cv_results_)\n#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","ae43596b":"X_train[0]","4804fda6":"X_train","7e0c95e7":"y_train","2f8418bd":"# xgb \nxgb_clf = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                           colsample_bytree=0.8, gamma=1.5, learning_rate=0.05,\n                           max_delta_step=0, max_depth=3, min_child_weight=7, missing=None,\n                           n_estimators=200, n_jobs=-1, nthread=None,\n                           objective='binary:logistic', random_state=0, reg_alpha=0,\n                           reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n                           subsample=0.6)\n\nxgb_clf.fit(X_train, y_train)\ny_pred = xgb_clf.predict(X_test)\ny_score = xgb_clf.predict_proba(X_test)[:,1]\n\nmodel_performance_plot('xgb_clf')","323eb245":"features_imp(xgb_clf, 'features')","2d27d0f3":"#feature importance plot TOP 40\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef plot_feature_importance(model):\n    tmp = pd.DataFrame({'Feature': list(data), 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False).head(30)\n    plt.figure(figsize = (10,12))\n    plt.title('Top 30 - Features importance - XGBoost',fontsize=14)\n    s = sns.barplot(y='Feature',x='Feature importance',data=tmp, orient='h')\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()","7c752c55":"plot_feature_importance(xgb_clf)","5a49e562":"cum_gains_curve(xgb_clf)","ee033804":"# Cross val score\ncross_val_metrics(xgb_clf)","97bc0636":"## <a id='1.3'>1.3. Missing values<\/a>","bd346983":"## <a id='2.3'>2.3. Features distribution and barplot (hue = Attrition)<\/a> ","91eea32c":"## <a id='7.3'>7.3. XGBoost - Cumulative gain curve<\/a> ","fdf17b8d":"# <a id='6'>6. XGBoost - RandomizedSearchCV to optimize hyperparameters<\/a>","94fa4d6c":"## <a id='3.3'>3.3. Features encoding and scaling<\/a> ","f798f503":"## <a id='2.2'>2.2. Target distribution (number and %)<\/a> ","ff2bb0df":"----------\n**IBM Attrition Analysis and Prediction**\n=====================================\n\n***XGB : CV - Accuracy (5 folds) =  .891***\n\n***Vincent Lugat***\n\n***Copied and modified by Anantha***\n\n*October 2019*\n\n----------","0dc3e85c":"## <a id='4.2'>4.2. Define feature importance plot<\/a> ","c5b33e8d":"- <a href='#1'>1. Load libraries and read the data<\/a>  \n    - <a href='#1.1'>1.1. Load libraries<\/a> \n    - <a href='#1.2'>1.2. Read the data<\/a> \n    - <a href='#1.3'>1.3. Missing values<\/a> \n    - <a href='#1.4'>1.4. Reassign target and drop useless features<\/a> \n- <a href='#2'>2. Exploratory Data Analysis (EDA)<\/a> \n    - <a href='#2.1'>2.1. Head and describe<\/a> \n    - <a href='#2.2'>2.2. Target distribution (number and %)<\/a> \n    - <a href='#2.3'>2.3. Features distribution and barplot (hue = Attrition)<\/a> \n    - <a href='#2.4'>2.4. Pie plot and barplot<\/a> \n- <a href='#3'>3. Feature engineering and selection<\/a>\n    - <a href='#3.1'>3.1. New features (24) <\/a> \n    - <a href='#3.2'>3.2. Drop some features<\/a> \n    - <a href='#3.3'>3.3. Features encoding and scaling<\/a>\n    - <a href='#3.4'>3.4. Correlation Matrix<\/a>\n    - <a href='#3.5'>3.5. Remove collinear features<\/a>\n- <a href='#4'>4. Define functions<\/a>\n    - <a href='#4.1'>4.1. Define model performance plot <\/a> \n    - <a href='#4.2'>4.2. Define feature importance plot <\/a> \n    - <a href='#4.3'>4.3. Define cumulative gains curve<\/a>\n    - <a href='#4.4'>4.4. Define cross validation metrics<\/a>\n- <a href='#5'>5. Prepare dataset<\/a>\n    - <a href='#5.1'>5.1. Define (X,  y)<\/a> \n    - <a href='#5.2'>5.2. Train test split<\/a> \n- <a href='#6'>6. XGBoost - RandomizedSearchCV to optimize hyperparameters (800 comb)<\/a> \n\n- <a href='#7'>7. XGBoost - With best hyperparameters = 89.11<\/a>\n    - <a href='#7.1'>7.1. XGBoost - Modeling and performance plot<\/a> \n    - <a href='#7.2'>7.2. XGBoost - Feature importance <\/a> \n    - <a href='#7.3'>7.3. XGBoost - Cumulative gains curve<\/a> \n    - <a href='#7.4'>7.4. XGBoost - Cross validation (5 folds)<\/a> ","7d4ef925":"## <a id='3.5'>3.5. Remove collinear features<\/a>","c9d3e575":"## <a id='2.1'>2.1. Head and describe<\/a> ","7923cde6":"## <a id='4.3'>4.3. Define cumulative gains curve<\/a> ","7870184b":"## <a id='4.1'>4.1. Define model performance plot<\/a> ","98a7db61":"## <a id='3.2'>3.2. Drop some features<\/a> ","bddfd013":"\n\n**RESULT : **\n\nBest estimator:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.8, gamma=1.5, learning_rate=0.05,\n       max_delta_step=0, max_depth=3, min_child_weight=7, missing=None,\n       n_estimators=200, n_jobs=-1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=0.6)\n\n Best accuracy for 5-fold search with 800 parameter combinations:\n0.891156462585034\n\n Best hyperparameters:\n{'subsample': 0.6, 'n_estimators': 200, 'min_child_weight': 7, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 1.5, 'colsample_bytree': 0.8}","877dfaf5":"# <a id='2'>2. Exploratory Data Analysis (EDA)<\/a>","64a40664":"# <a id='3'>3. Feature engineering and selection<\/a>","128273d9":"# <a id='4'>4. Define functions<\/a>","006c5dae":"## <a id='5.2'>5.2. Train test split<\/a> ","69244560":"## <a id='7.2'>7.2. XGBoost - Feature importance <\/a>","4f7ad01a":"![](http:\/\/image.noelshack.com\/fichiers\/2018\/41\/1\/1539014632-1-bye.png)\n\nsource : http:\/\/thecontextofthings.com\/2017\/01\/06\/employee-attrition\/","304292ff":"## <a id='2.4'>2.4. Pie plot and barplot<\/a> ","0a0fd7ee":"## <a id='1.4'>1.4. Reassign target and drop useless features<\/a>","18cba2c9":"## <a id='1.2'>1.2. Read the data<\/a>","bf00f172":"## <a id='3.4'>3.4. Correlation Matrix<\/a> ","ffe8affe":"## <a id='7.4'>7.4. XGBoost - Cross validation (5 folds)<\/a> ","db53941c":"# <a id='7'>7. XGBoost - Modeling with best hyperparameters = 89.11<\/a>","4771ea56":"**Thank you all ! Merci \u00e0 tous ! :)**","97244185":"# <a id='5'>5. Prepare dataset<\/a>","88a5cd99":"## <a id='5.1'>5.1. Define (X, y)<\/a> ","665088bf":"## <a id='7.1'>7.1. XGBoost - Modeling and performance plot<\/a> ","3df66e26":"# <a id='1'>1. Load libraries and read the data<\/a> ","6cd1d0bf":"Remove \"#----------------------------#\" to lunch random_search","2f58c0a4":"## <a id='1.1'>1.1. Load libraries<\/a> ","efc8d410":"## <a id='4.4'>4.4. Define cross validation metrics<\/a> ","8758f049":"## <a id='3.1'>3.1. New features : 24<\/a> "}}