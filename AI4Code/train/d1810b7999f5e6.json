{"cell_type":{"b71fafe3":"code","8d8eeb8a":"code","325432d0":"code","74f1c337":"code","b832f708":"code","ceca37c0":"code","26ef3b8b":"code","504a7887":"code","0c6b68a2":"code","5de8d063":"code","15d9af2e":"code","ae774f9e":"code","8b88fca2":"code","df21adab":"code","eb5ab674":"code","db7b2fc2":"code","d104ada6":"code","306a3de3":"code","a8e2e892":"code","9b638ae3":"code","93beba5f":"code","49898663":"code","c4707a92":"code","b7eecf0b":"code","180864f8":"code","549c46f9":"code","cef163ac":"code","0add9d66":"code","31eb54e0":"code","199aeac8":"code","73f08ab9":"code","9242d411":"code","863b8b6f":"code","e24fbd34":"code","b5216b5d":"markdown","025c39e7":"markdown","51422c98":"markdown","53ead25d":"markdown","c3dcd295":"markdown","fb958237":"markdown","23203218":"markdown","d52d23bf":"markdown","906d75f8":"markdown","e92edfd3":"markdown","90cb038e":"markdown","d319cb94":"markdown"},"source":{"b71fafe3":"!pip install --upgrade scipy==1.3.3","8d8eeb8a":"### Importing Reqired Libraries\nimport os\nimport cv2\nimport time\nimport pickle\nimport string\nimport operator\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom torchtext.data.metrics import bleu_score\nfrom torchtext.data.utils import get_tokenizer\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom queue import PriorityQueue\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n","325432d0":"### Use GPU if available for all computations\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","74f1c337":"### Helper Function - for resigtering forward hooks, retrives activation of specified layer\ndef get_activation(name, _dict):\n    \"\"\"\n    Input:\n            - name: name of the layer whose activations you want to retrive\n    Output:\n            - hook: returns a forward hook to the specified layer\n    \"\"\"\n    \n    def hook(model, input, output):\n        _dict[name] = output.detach()\n    \n    return hook\n","b832f708":"### Helper Function - for applying transforms to image\ndef get_transforms():\n    \n    # Values for each channel (RGB) for normalizing\n    normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n    \n    # Convert image to tensor and normalize\n    return transforms.Compose([transforms.ToTensor(), normalize])\n","ceca37c0":"### A Function that extracts features from images\ndef extract_features(path, model, transforms):\n    \"\"\"\n    Input:\n            - path: path to directory where all the images are stored\n            - model: pre-trained CNN model, that will be used as feature extractor\n    Output:\n            - dict: {image_name: feature_vector}\n    \"\"\"\n    \n    # Register a forward hook to the layer that outputs a feature vector - we are using Inception V3, and feature vector is returned by Mixed_7c layer\n    activation = {}\n    model.Mixed_7c.register_forward_hook(get_activation(name = \"Mixed_7c\", _dict = activation))\n    \n    # Pooling Function\n    pool = nn.AdaptiveAvgPool2d((1, 1))\n    \n    # Dict to store features\n    image_features = {}\n    \n    # Iterate through all images, extract features, store it in dict\n    for file in tqdm(os.listdir(path)):\n        \n        img_name = file.split('.')[0]\n        img_path = path + '\/' + file\n        \n        # Load and re-size the image\n        img = Image.open(img_path)\n        img = img.resize((299, 299))\n        \n        # Pre-process the image, apply transforms\n        img_tensor = transforms(img)\n        img_tensor = torch.unsqueeze(img_tensor, dim = 0)\n        \n        # Forward prop image through model to extract features\n        img_tensor = img_tensor.to(device)\n        output = model(img_tensor)\n        features = activation[\"Mixed_7c\"]\n        \n        # Get 2048 dim feature vector by applying pooling\n        features = pool(features)\n        features = torch.squeeze(features).cpu().numpy()\n        \n        # Add image name and feature vector to dict\n        image_features[img_name] = features\n    \n    return image_features\n    ","26ef3b8b":"### Creating an instance of pre-trained InceptionV3 - feature extractor\ninception_v3 = models.inception_v3(pretrained = True)\n\n### Set model to evaluation mode and load to device\ninception_v3.eval()\ninception_v3.to(device)\n","504a7887":"### Specify path to directory where all the images are stored\nimages_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\"\n\n### Extract image features by calling 'extract_features' function\nimage_features = extract_features(path = images_path, model = inception_v3, transforms = get_transforms())\n","0c6b68a2":"### Dump extracted features into a pickle file for future use\npickle.dump(image_features, open('image_features.pkl', 'wb'))\n","5de8d063":"### Helper function - for loading captions from txt file\ndef load_captions(path):\n    \"\"\"\n    Input:\n            - path: path to .txt file in which captions are stored\n    Output:\n            - list: a list of all captions\n    \"\"\"\n    \n    # A list & dict to store all captions for image\n    captions_list = []\n    image_captions = {}\n    \n    # Open the file containing captions\n    with open(path, \"r\") as file:\n        \n        #Iterate through each line, append each caption to the list\n        for line in file.readlines():\n            \n            # Extract caption by splitting the line into words\n            words = line.strip(\"\\n\").split()\n            \n            # Join the words to form a caption, append to the list\n            caption = ' '.join(words[1:])\n            captions_list.append(caption)\n    \n    return captions_list\n","15d9af2e":"### Helper Function - removing punctuation, tokenizing caption\ndef preprocess_caption(caption):\n    \"\"\"\n    Input:\n            - caption: raw caption to pre-process\n    Output:\n            - list: list of tokens; tokenized caption\n    \"\"\"\n    \n    # Tokenizer - it will automatically convert words to lower case and tokenize them\n    tokenizer = get_tokenizer(\"basic_english\")\n    \n    # Removing punctuations from caption\n    caption = \"\".join([char for char in caption if char not in string.punctuation])\n        \n    # Tokenizing caption\n    tokenized_caption = tokenizer(caption)\n    \n    return tokenized_caption","ae774f9e":"### Helper Function - to build a vocabulary, we will apply all basic pre-processing here such as lower case, removing punctuation, tokenization, etc.\ndef build_vocabulary(captions_list, min_freq = 5):\n    \"\"\"\n    Input:\n            - list: a list containing all captions\n            - min_freq: minimum count of a word to be part of the vocabulary\n    Output:\n            - dict: word mapping \/ vocabulary\n    \"\"\"\n    \n    # Frequency counter\n    word_freq = Counter()\n    \n    # Pre-process the caption and update the frequency counter\n    for caption in captions_list:\n        \n        # Preprocessing caption\n        tokenized_caption = preprocess_caption(caption)\n        \n        # Update freq count\n        word_freq.update(tokenized_caption)\n    \n    # Create word mapping \/ vocabulary\n    words = [w for w in word_freq.keys() if word_freq[w] >= min_freq]\n    word_map = {k: v + 1 for v, k in enumerate(words)}\n    word_map['<unk>'] = len(word_map) + 1\n    word_map['<start>'] = len(word_map) + 1\n    word_map['<end>'] = len(word_map) + 1\n    word_map['<pad>'] = 0\n        \n    return word_map\n    ","8b88fca2":"### Load Captions from given .txt file\ntokens_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr8k.token.txt\"\ncaptions_list = load_captions(path = tokens_path)\n\n### Build Vocabulary from captions\nword_map = build_vocabulary(captions_list = captions_list, min_freq = 5)\n","df21adab":"### Dump vocabulary into a pickle file for future use\npickle.dump(word_map, open('word_map_min5.pkl', 'wb'))\n","eb5ab674":"### Helper Function - to create seperate file containing img - caption for given img list\ndef split_data(all_captions_path, img_list_path, name):\n    \"\"\"\n    Input:\n            - all_captions_path: path to .txt file containing all img - captions\n            - img_list_path: path to .txt file containing images list\n            - name: name to store the new .txt file containing image and caption Ex; train \/ dev \/ test\n    \"\"\"\n    \n    # Temp List to store all img names\n    img_names = []\n    \n    # Extract Images Names from given file\n    with open(img_list_path, \"r\") as file:\n        \n        # Iterate through each line, append each image to the list\n        for line in file.readlines():\n            \n            # Get image name by reading lines, append it to the list\n            img_name = line.strip(\"\\n\").split(\".\")[0]\n            img_names.append(img_name)\n            \n    # Temp List to store lines for new file: img_name caption\n    lines = []\n    \n    # Extract captions for images in temp img list\n    with open(all_captions_path, \"r\") as file:\n    \n        # Iterate through each line and compare image name\n        for line in file.readlines():\n            \n            # Get image name \n            words = line.strip(\"\\n\").split()\n            img_name = words[0].split(\".\")[0]\n            \n            if img_name in img_names:\n                new_line = img_name + \" \" + \" \".join(words[1:])\n                lines.append(new_line)\n    \n    # Create new .txt file with given name\n    with open(name + \".txt\", \"w\") as file:\n        \n        # Write all lines to new txt file\n        lines = map(lambda x:x + '\\n', lines)\n        file.writelines(lines)\n    ","db7b2fc2":"### Splitting the data as per the given files\nall_captions_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr8k.token.txt\"\ntrain_list_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr_8k.trainImages.txt\"\ndev_list_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr_8k.devImages.txt\"\ntest_list_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_text\/Flickr_8k.testImages.txt\"\n\n# Creating train.txt\nsplit_data(all_captions_path, train_list_path, \"train\")\n\n# Creating dev.txt\nsplit_data(all_captions_path, dev_list_path, \"dev\")\n\n# Creating test.txt\nsplit_data(all_captions_path, test_list_path, \"test\")\n","d104ada6":"### Helper Function - to convert raw captions to vectors; preprocess, apply word mapping, pad with zeros\ndef caption_to_vectors(caption, word_map, max_length):\n    \"\"\"\n        Input:\n                - caption: caption to process\n                - word_map: vocabulary \/ dict containing mapping of all words to no\n                - max_length: maximum length of sentence \/ caption\n        Output:\n                - _input: input vector\n                - _output: output vector\n    \"\"\"\n    \n    # Pre-process caption\n    caption = preprocess_caption(caption)\n    \n    # Append start token\n    caption.insert(0, \"<start>\")\n    \n    # Create input and output words\n    input_words = caption\n    output_words = caption[1:]\n    output_words.append(\"<end>\")\n    \n    # Apply word mapping\n    input_map = [word_map[k] if k in word_map.keys() else word_map[\"<unk>\"] for k in input_words ]\n    output_map = [word_map[k] if k in word_map.keys() else word_map[\"<unk>\"] for k in output_words ]\n    \n    # Vectors of size max_len - used for 0 padding\n    input_vector = np.zeros(max_length, dtype = np.int_)\n    output_vector = np.zeros(max_length, dtype = np.int_)\n    \n    for i in range(len(input_map)):\n        input_vector[i] = input_map[i]\n        output_vector[i] = output_map[i]\n    \n    \n    return input_vector, output_vector\n    ","306a3de3":"### Overriding Dataset Class \nclass NICDataset(Dataset):\n    \n    def __init__(self, data_path, image_features_path, word_map_path, max_length = 40):\n        \"\"\"\n            Input:\n                    - data_path: path to .txt file which contains img name caption\n                    - image_features_path: path to the pkl file in which image_features dict is stored\n                    - word_map_path: path to pkl file in which word mapping \/ vocabulary is stored\n                    - max_length: maximum length of sentence\n        \"\"\"\n        \n        super().__init__()\n        self.data_list = []\n        self.image_features = pickle.load(open(image_features_path, \"rb\"))\n        self.word_map = pickle.load(open(word_map_path, \"rb\"))\n        self.max_length = max_length\n        \n        # Initialize data_list by reading .txt file from data path\n        with open(data_path, \"r\") as file:\n            \n            #Iterate through each line to extract image names\n            for line in file.readlines():\n\n                # Append line to data list\n                words = line.strip(\"\\n\")\n                self.data_list.append(words)\n    \n    def __getitem__(self, index):\n        \n        # Retriving Line (img name caption) from data_list\n        line = self.data_list[index]\n        \n        # Extracting img name & caption from line\n        img_name = line.split()[0]\n        caption = \" \".join(line.split()[1:])\n        \n        # Creating Input and Target Variables\n        \"\"\"\n            - input1: image feature vector of size 2048\n            - input2: text; pre-processed caption corresponding to that image\n            - target: output for each time stamp i.e. input shifted by left\n        \"\"\"\n        \n        input1 = self.image_features[img_name]\n        input2, target = caption_to_vectors(caption, self.word_map, self.max_length)\n        \n        return input1, input2, target\n    \n    def __len__(self):\n        return len(self.data_list)\n        ","a8e2e892":"### Creating dataset objects\ntrain_dataset = NICDataset(\n        data_path = \"train.txt\", \n        image_features_path = \"..\/input\/flicker8k-image-captioning\/image_features.pkl\",\n        word_map_path = \"word_map_min5.pkl\",\n        max_length = 40\n)\n\ndev_dataset = NICDataset(\n        data_path = \"dev.txt\", \n        image_features_path = \"..\/input\/flicker8k-image-captioning\/image_features.pkl\",\n        word_map_path = \"word_map_min5.pkl\",\n        max_length = 40\n)\n\ntest_dataset = NICDataset(\n        data_path = \"test.txt\", \n        image_features_path = \"..\/input\/flicker8k-image-captioning\/image_features.pkl\",\n        word_map_path = \"word_map_min5.pkl\",\n        max_length = 40\n)\n","9b638ae3":"### Creating Data loaders for model\ntrain_data_loader = DataLoader(\n        dataset = train_dataset,\n        batch_size = 64,\n        shuffle = True,\n        num_workers = 4\n)\n\ndev_data_loader = DataLoader(\n        dataset = dev_dataset,\n        batch_size = 64,\n        shuffle = True,\n        num_workers = 4\n)\n\ntest_data_loader = DataLoader(\n        dataset = test_dataset,\n        batch_size = 64,\n        shuffle = True,\n        num_workers = 4\n)\n","93beba5f":"### Class NICModel\nclass NICModel(nn.Module):\n    \n    def __init__(self, vocab_size, emb_dim, hidden_units):\n        \"\"\"\n            Input:\n                    - vocab_size: size of the vocabulary \/ word map, will be used for creating dense layers\n                    - emb_dim: word embedding dimensions\n                    - hidden_units: no. of hidden units in LSTM cell\n        \"\"\"\n        \n        # Initializing Model\n        super(NICModel, self).__init__()\n        self.vocab_size = vocab_size\n        self.emb_dim = emb_dim\n        self.hidden_units = hidden_units\n        \n        \"\"\"\n            dense1: it will be used to convert image features of size 2048 into emb_dim size\n        \"\"\"\n        # Initializing Layers\n        self.dropout = nn.Dropout(p = 0.5)\n        self.softmax = nn.Softmax(dim = -1)\n        self.batchnorm = nn.BatchNorm1d(num_features = self.emb_dim)\n        self.embedding = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.emb_dim)\n        self.dense1 = nn.Linear(in_features = 2048, out_features = self.emb_dim, bias = False)\n        self.dense2 = nn.Linear(in_features = self.hidden_units, out_features = self.vocab_size)\n        self.lstm = nn.LSTM(input_size = self.emb_dim, hidden_size = self.hidden_units, batch_first = True)\n    \n    def forward(self, inputs, mode = \"train\"):\n        \"\"\"\n            Input:\n                    - list: [input1, input2] where input1 is 2048 dim img feature vector & input2 is input sentence\n            Output:\n                    - tensor: shape [batch_size * max_len, vocab_size]\n        \"\"\"\n        \n        # Extract inputs\n        input1 = inputs[0]\n        input2 = inputs[1]\n        \n        # Image Embedding\n        x_img = self.dropout(input1)\n        x_img = self.dense1(x_img)\n        x_img = self.batchnorm(x_img)\n        x_img = x_img.unsqueeze(dim = 1)\n        \n        # Text Embedding\n        x_text = self.embedding(input2)\n        x_text = self.dropout(x_text)\n        \n        # Encode Image using lstm\n        _, h = self.lstm(x_img)\n        \n        \n        \"\"\"\n            With Dropout - Forward Text through LSTM\n            Logic: \n                    For each sentence, create a mask Ex: if there are 64 sentences in a batch, 64 different masks will be created\n                    Apply same mask on each time step (word) of the sentence\n        \n        if mode == \"train\":\n            \n            # Create masks for each sentence in the batch\n            mask = torch.ones((x_text.shape[0],x_text.shape[2]), device = device).fill_(0.5).bernoulli()\n            mask = mask.unsqueeze(dim = 1)\n\n            # Apply mask to x_text\n            x_text = x_text * mask\n        \"\"\"\n        \n        # Initialize decoder with img encoding (h) and forward text\n        A, _ = self.lstm(x_text, h)\n        \n        # Apply Dense to get op of shape vocab_size, reshaping from [batch_size, max_len, vocab_size] to [batch_size * max_len, vocab_size] \n        outputs = self.dense2(A)\n        outputs = outputs.view(-1, self.vocab_size)\n        \n        \"\"\"\n        We don't need to apply softmax here, CrossEntropyLoss in PyTorch will do that internally.\n        \"\"\"\n    \n        return outputs\n        ","49898663":"### Creating instance of NICModel\nnic = NICModel(vocab_size = len(word_map), emb_dim = 512, hidden_units = 512)\n#nic.load_state_dict(torch.load(\"..\/input\/flicker8k-image-captioning\/nic_weights\/nic_500_epochs_0.01_lr_no_l2.pth\", map_location = device))\n","c4707a92":"### Loading model to available device GPU \/ CPU\nnic.to(device)\n\n### Defining Model Parameters\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(nic.parameters(), lr = learning_rate)\nloss_criterion = nn.CrossEntropyLoss()\nepochs = 1\n\n### Logs\ntrain_loss = []\ndev_loss = []\n","b7eecf0b":"### Training Code\nfor epoch in range(epochs):\n    \n    nic.train()\n    \n    # Running Variables\n    start_time = time.time()\n    train_run_loss = []\n    dev_run_loss = []\n    index = 0\n    \n    # Iterate through training batches\n    for input1, input2, target in train_data_loader:\n        \n        # Prepare input & target variables, load to device\n        _input = [input1.to(device), input2.to(device)]\n        target = target.view(-1)\n        target = target.to(device)\n        \n        # Reset Gradients\n        optimizer.zero_grad()\n        \n        # Get Predictions - Outputs\n        output = nic(_input, mode = \"train\")\n        \n        # Calculate loss, gradients and step backward\n        loss = loss_criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        # Append Loss and print iteration details\n        train_run_loss.append(loss.item())\n        \"\"\"\n        if (index+1) % 25 == 0:\n            print(\"[Train {}] Iteration {} | Running Loss: {}\".format(epoch+1, index+1, round(np.mean(train_run_loss), 4)))\n        index += 1\n        \"\"\"\n        \n    # Final Epoch Loss\n    avg_loss = np.mean(train_run_loss)\n    train_loss.append(avg_loss)\n    end_time = time.time()\n    print(\"[Train] Epoch {} | Average Loss: {} | Time: {}\".format(epoch+1, round(avg_loss, 4), round(end_time-start_time, 2)))\n    \n    # Perform validation after every 10 epochs\n    if (epoch+1) % 10 == 0:\n        \n        nic.eval()\n        start_time = time.time()\n        index = 0\n        \n        # Iterate through dev batches\n        for input1, input2, target in dev_data_loader:\n            \n            with torch.no_grad():\n                \n                # Prepare input & target variables, load to device\n                _input = [input1.to(device), input2.to(device)]\n                target = target.view(-1)\n                target = target.to(device)\n                \n                # Get Predictions - Outputs\n                output = nic(_input, mode = \"valid\")\n\n                # Calculate loss, gradients and step backward\n                loss = loss_criterion(output, target)\n                \n                # Append Loss and print iteration details\n                dev_run_loss.append(loss.item())\n                \"\"\"\n                if (index+1) % 25 == 0:\n                    print(\"[Valid {}] Iteration {} | Running Loss: {}\".format(epoch+1, index+1, round(np.mean(dev_run_loss), 4)))\n                index += 1\n                \"\"\"\n        \n        # Final Epoch Loss\n        avg_loss = np.mean(dev_run_loss)\n        dev_loss.append(avg_loss)\n        end_time = time.time()\n        print(\"\\n[Valid] Epoch {} | Average Loss: {} | Time: {}\\n\".format(epoch+1, round(avg_loss, 4), round(end_time-start_time, 2)))\n        \n    # Save model after every 100 epochs\n    if (epoch+1) % 100 == 0:\n        torch.save(nic.state_dict(), \"nic_{}_epochs.pth\".format(epoch+1))\n            \n","180864f8":"### Encoder Model - for encoding image features\nclass NIC_Encoder(NICModel):\n    \n    # Here, we need to override only the forward method, to encode the image.\n    \n    def __init__(self, vocab_size, emb_dim, hidden_units):\n        \"\"\"\n            Inputs:\n                    same as NICModel\n        \"\"\"\n        super(NIC_Encoder, self).__init__(vocab_size, emb_dim, hidden_units)\n    \n    # Overriding the forward method\n    def forward(self, x_img):\n        \"\"\"\n            Input:\n                    - x_img: [batch_size, 2048] shape img feature vector\n            Output:\n                    - A, h: outputs of lstm\n        \"\"\"\n        \n        # Image Embedding\n        x_img = self.dense1(x_img)\n        x_img = self.batchnorm(x_img)\n        x_img = x_img.unsqueeze(dim = 1)\n        \n        # Encode Image\n        A, h = self.lstm(x_img)\n        \n        return A, h\n        ","549c46f9":"### Decoder Model - for decoding text\nclass NIC_Decoder(NICModel):\n    \n    def __init__(self, vocab_size, emb_dim, hidden_units):\n        \"\"\"\n            Inputs:\n                    same as NICModel\n        \"\"\"\n        super(NIC_Decoder, self).__init__(vocab_size, emb_dim, hidden_units)\n    \n    def forward(self, x_text, h):\n        \"\"\"\n            Input:\n                    - x_text: [batch_size, 1] shape input word\n                    -h: previous hidden states of lstm (img enc in case of 1st word)\n            Output:\n                    - A, h: outputs of lstm\n        \"\"\"\n\n        # Text Embedding\n        x_text = self.embedding(x_text)\n        \n        # Get predictions\n        A, h = self.lstm(x_text, h)\n        A = self.dense2(A)\n        A = self.softmax(A)\n        \n        return A, h\n    ","cef163ac":"### Greedy Inference - max prob word from each time step\ndef inference_greedy(cnn_model, nic_encoder, nic_decoder, img_path, id_to_word, transforms, max_length = 40):\n    \"\"\"\n        Inputs:\n                cnn_model: CNN Model from which image features are to be extracted, here inception V3\n                nic_encoder: Encoder model to retrive img encodings\n                nic_decoder: Decoder model for text\n                img_path: path where the image is stored ex: bla\/bla\/bla\/img.jpg\n                id_to_word: reverse mapping of word vocabulary \/ word map\n                transforms: transforms to apply on img\n                max_length: maximum length allowrd for caption\n        \n        Outputs:\n                caption: predicted caption for given image\n                \n        Warning:\n                I've used batch size of 1 only, so call inference_greedy function for each image\n    \"\"\"\n    \n    # Register a forward hook to the layer that outputs a feature vector\n    # We are using Inception V3, and feature vector is returned by Mixed_7c layer\n    activation = {}\n    cnn_model.Mixed_7c.register_forward_hook(get_activation(name = \"Mixed_7c\", _dict = activation))\n    \n    # Pooling Function\n    pool = nn.AdaptiveAvgPool2d((1, 1))\n    \n    # Extract Image Features\n\n    img = Image.open(img_path)\n    img = img.resize((299, 299))\n        \n    # Pre-process the image, apply transforms\n    img_tensor = transforms(img)\n    img_tensor = torch.unsqueeze(img_tensor, dim = 0)\n        \n    # Forward prop image through model to extract features\n    img_tensor = img_tensor.to(device)\n    output = cnn_model(img_tensor)\n    features = activation[\"Mixed_7c\"]\n        \n    # Get 2048 dim feature vector by applying pooling\n    features = pool(features)\n    features = features.view(1, 2048)\n    \n    # Encode image\n    _, h = nic_encoder(features)\n    \n    # Prepare 1st input word\n    start = 2993 # index of '<start>' in vocabulary\n    x_text = torch.tensor(start, device = device).view(1,1)\n    \n    # Array to store output\n    output = torch.zeros((1, max_length), device = device)\n    \n    # Generate caption from nic_decoder\n    for i in range(max_length):\n        \n        # Get predictions\n        A, h = nic_decoder(x_text, h)\n        A = torch.argmax(A, dim = 2)\n        \n        # break the loop if <end> is generated\n        if id_to_word[A.item()] == \"<end>\":\n            break\n        \n        # Append to outputs array\n        output[:, i] = A.item()\n        \n        # Update x_text\n        x_text = A\n    \n    # Process the output sentence\n    output = [id_to_word[key] for key in output.cpu().numpy()[0] if id_to_word[key] != \"<pad>\"]\n    output = \" \".join(output)\n        \n    \n    return output\n","0add9d66":"### Node - to store intermediate informations\nclass BeamSearchNode(object):\n    \n    def __init__(self, hidden_state, previous_node, word_id, log_prob, length):\n        \"\"\"\n            Inputs:\n                    - hidden_state: returned from lstm for previous input word, used to init lstm for predicting next word\n                    - previous_node: used for backtracking\n                    - word_id: index of the word in vocabulary\n                    - log_prob: log of P(y_hat^<t> | y_hat^<0>...y_hat^<t-1>)\n                    - length: length of beam so far, if it exceeds max_length, we need to stop\n        \"\"\"\n        self.hidden_state = hidden_state\n        self.previous_node = previous_node\n        self.word_id = word_id\n        self.log_prob = log_prob\n        self.length = length\n    \n    def eval(self, alpha = 1.0):\n        \"\"\"\n            Used to calculate normalized score (length independent)\n        \"\"\"\n        reward = 0\n        return self.log_prob \/ float(self.length - 1 + 1e-6) + alpha * reward\n    \n    \n    ","31eb54e0":"### Function to execute beam search\ndef beam_search(cnn_model, nic_encoder, nic_decoder, img_path, id_to_word, transforms, max_length = 40, beam_width = 3, no_of_op = 1):\n    \"\"\"\n        Inputs:\n                cnn_model: CNN Model from which image features are to be extracted, here inception V3\n                nic_encoder: Encoder model to retrive img encodings\n                nic_decoder: Decoder model for text\n                img_path: path where the image is stored ex: bla\/bla\/bla\/img.jpg\n                id_to_word: reverse mapping of word vocabulary \/ word map\n                transforms: transforms to apply on img\n                max_length: maximum length allowrd for caption\n                beam_width: top k to select\n                no_of_op: number of output sentences \/ captions you want per image\n        \n        Outputs:\n                caption: predicted caption for given image\n                \n        Warning:\n                I've used batch size of 1 only, so call inference_greedy function for each image\n    \"\"\"\n    \n    # Register a forward hook to the layer that outputs a feature vector\n    # We are using Inception V3, and feature vector is returned by Mixed_7c layer\n    activation = {}\n    cnn_model.Mixed_7c.register_forward_hook(get_activation(name = \"Mixed_7c\", _dict = activation))\n    \n    # Pooling Function\n    pool = nn.AdaptiveAvgPool2d((1, 1))\n    \n    # Extract Image Features\n\n    img = Image.open(img_path)\n    img = img.resize((299, 299))\n        \n    # Pre-process the image, apply transforms\n    img_tensor = transforms(img)\n    img_tensor = torch.unsqueeze(img_tensor, dim = 0)\n        \n    # Forward prop image through model to extract features\n    img_tensor = img_tensor.to(device)\n    output = cnn_model(img_tensor)\n    features = activation[\"Mixed_7c\"]\n        \n    # Get 2048 dim feature vector by applying pooling\n    features = pool(features)\n    features = features.view(1, 2048)\n    \n    # Encode image\n    _, h = nic_encoder(features)\n    \n    # Prepare 1st input word\n    start = 2993 # index of '<start>' in vocabulary\n    x_text = torch.tensor(start, device = device).view(1,1)\n    \n    # Priority Queue - to store nodes, a list to store end nodes\n    nodes = PriorityQueue()\n    endnodes = []\n    \n    # Starting Node for beam search\n    node = BeamSearchNode(hidden_state = h, previous_node = None, word_id = x_text, log_prob = 0, length = 1)\n    nodes.put((-node.eval(), node))\n    queue_size = 1\n    \n    # Start Beam Search\n    while True:\n        \n        # If queue size exceeds, then stop\n        #if queue_size > 2000: break\n        \n        # Fetch node with best score from queue\n        score, node = nodes.get()\n        x_text = node.word_id\n        h = node.hidden_state\n        \n        # Check for <end>\n        if id_to_word[x_text.item()] == \"<end>\" and node.previous_node != None:\n            \n            # Append this endnode to the list\n            endnodes.append((score, node))\n            \n            # Check for max no of outputs\n            if len(endnodes) >= no_of_op: \n                break\n            else:\n                continue\n        \n        # Decode for one step\n        A, h = nic_decoder(x_text, h)\n        \n        # Get top K elements\n        A_topK, indices_topK = torch.topk(A, beam_width)\n        \n        # Create Nodes for these topK elements, and push it on the queue\n        for topK in range(beam_width):\n            \n            # Get word id and log_prob for node\n            word_id = indices_topK[0][0][topK].view(1, 1)\n            prob = A_topK[0][0][topK].item()\n            log_prob = np.log(prob)\n            \n            # Create node and add to queue\n            new_node = BeamSearchNode(hidden_state = h, previous_node = node, word_id = word_id, \n                                      log_prob = node.log_prob + log_prob, length = node.length + 1)\n            nodes.put((-new_node.eval(), new_node))\n        \n        queue_size += beam_width - 1\n    \n    # If it does not generate <end> in any step, then endnodes will be empty\n    if len(endnodes) == 0:\n        endnodes = [nodes.get() for _ in range(no_of_op)]\n    \n    # Process nodes, assemble caption\n    top_n_captions = []\n    for score, node in sorted(endnodes, key = operator.itemgetter(0)):\n        \n        # Append word from node to sentence\n        caption = []\n        caption.append(node.word_id.item())\n        \n        # Iterate till <start> (here we are backtracking, so caption is reverse)\n        while node.previous_node != None:\n            node = node.previous_node\n            caption.append(node.word_id.item())\n        \n        # Reverse the caption\n        caption = caption[::-1]\n        \n        # Convert word id to word, remove start & end tokens\n        caption = [id_to_word[key] for key in caption if id_to_word[key] != \"<start>\" if id_to_word[key] != \"<end>\"]\n        caption = \" \".join(caption)\n        \n        top_n_captions.append(caption)\n    \n    return top_n_captions\n    ","199aeac8":"\"\"\"\n    No need to load inception v3 again if loaded from previous cells.\n\"\"\"\n\n### Creating an instance of pre-trained InceptionV3 - feature extractor, nic_encoder and nic_decoder models\ninception_v3 = models.inception_v3(pretrained = True)\n\nnic_encoder = NIC_Encoder(vocab_size = len(word_map), emb_dim = 512, hidden_units = 512)\nnic_encoder.load_state_dict(torch.load(\"..\/input\/flicker8k-image-captioning\/nic_weights\/nic_500_epochs_0.05_lr_no_l2.pth\", \n                                                map_location = device))\n\nnic_decoder = NIC_Decoder(vocab_size = len(word_map), emb_dim = 512, hidden_units = 512)\nnic_decoder.load_state_dict(torch.load(\"..\/input\/flicker8k-image-captioning\/nic_weights\/nic_500_epochs_0.05_lr_no_l2.pth\", \n                                                map_location = device))\n\n### Set model to evaluation mode and load to device\ninception_v3.eval()\ninception_v3.to(device)\n\nnic_encoder.eval()\nnic_encoder.to(device)\n\nnic_decoder.eval()\nnic_decoder.to(device)\n\n### word_to_index: reverse mapping of vocab\nid_to_word = {v:k for k,v in word_map.items()}\n","73f08ab9":"### Sample Inference - Greedy\nimg_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\/3385593926_d3e9c21170.jpg\"\nimg = Image.open(img_path)\ncaption = inference_greedy(cnn_model = inception_v3, nic_encoder = nic_encoder, nic_decoder = nic_decoder,\n                           img_path = img_path, id_to_word = id_to_word, transforms = get_transforms(), max_length = 40)\n\nplt.imshow(img)\n\nprint(caption)\n","9242d411":"### Sample Inference - Greedy\nimg_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\/3048597471_5697538daf.jpg\"\nimg = Image.open(img_path)\ncaptions = beam_search(cnn_model = inception_v3, nic_encoder = nic_encoder, nic_decoder = nic_decoder,img_path = img_path, \n                           id_to_word = id_to_word, transforms = get_transforms(), max_length = 40, beam_width = 10, no_of_op = 3)\n\nplt.imshow(img)\n\nfor caption in captions:\n    print(caption)","863b8b6f":"### Function to evaluate performance by Blue Score\ndef evaluate_bleu(test_path, imgs_path, models, id_to_word, transforms, max_length, beam_width):\n    \"\"\"\n        Inputs:\n                test_path: path to test.txt file containing img ids and gt captions\n                imgs_path: path to dir where all images are stored\n                models: (cnn, encoder, decoder) tuple containing all 3 req models\n                id_to_word: reverse mapping of word vocabulary \/ word map\n                transforms: transforms to apply on img\n                max_length: maximum length allowrd for caption\n                beam_width: top k to select\n        Outputs:\n                bleu score\n    \"\"\"\n    \n    #A dict containing img ids and list of gt captions : {'img_id' : []}\n    img_caps = {}\n    with open(test_path, \"r\") as file:\n        \n        #Iterate through each line to extract image names\n        for line in file.readlines():\n\n            #Get img id\n            words = line.strip(\"\\n\")\n            img_id = words.split()[0]\n            caption = words.split()[1:]\n            caption = preprocess_caption(\" \".join(caption))\n            \n            #Append to dict\n            if img_id in img_caps.keys():\n                img_caps[img_id].append(caption)\n            else:\n                img_caps[img_id] = [caption]\n    \n    #Get Models\n    cnn_model = models[0]\n    nic_encoder = models[1]\n    nic_decoder = models [2]\n    \n    #Lists to store bleu_score of each image\n    b1_l = []; b2_l = []; b3_l = []; b4_l = [];\n    \n    #Find Bleu Score for each image\n    for img, captions in tqdm(img_caps.items()):\n        \n        #For PriorityQueue Node error\n        try:\n        \n            #Image path\n            img_path = imgs_path + img + \".jpg\"\n\n            #Beam search to get top-1 caption\n            caption = beam_search(cnn_model = inception_v3, nic_encoder = nic_encoder, nic_decoder = nic_decoder,img_path = img_path, \n                               id_to_word = id_to_word, transforms = transforms, max_length = max_length, beam_width = 10, no_of_op = 1)\n\n            #Generate candidates and references for bleu_score function\n            caption = preprocess_caption(caption)\n            candidate = [caption]\n            references = [captions]\n\n            #Calculate Bleu Score\n            b1 = bleu_score(candidate, references, weights=[1.0, 0, 0, 0])\n            b2 = bleu_score(candidate, references, weights=[0.5, 0.5, 0, 0])\n            b3 = bleu_score(candidate, references, weights=[0.33, 0.33, 0.33, 0])\n            b4 = bleu_score(candidate, references, weights=[0.25, 0.25, 0.25, 0.25])\n\n            #Append to lists\n            b1_l.append(b1); b2_l.append(b2); b3_l.append(b3); b4_l.append(b4)\n        \n        except:\n            continue\n        \n    #Calculate Avg\n    b1_avg = np.mean(b1_l); b2_avg = np.mean(b2_l); b3_avg = np.mean(b3_l); b4_avg = np.mean(b4_l)\n    \n    return b1_avg, b2_avg, b3_avg, b4_avg\n","e24fbd34":"### Set paths\ntest_path = \"test.txt\"\nimgs_path = \"..\/input\/flicker8k-image-captioning\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"\n\n### Evaluate\nb1, b2, b3, b4 = evaluate_bleu(test_path = test_path, imgs_path = imgs_path, models = (inception_v3, nic_encoder, nic_decoder),\n             id_to_word = id_to_word, transforms = get_transforms(), max_length = 40, beam_width = 20)\n\nprint(\"B1 Score: \",b1)\nprint(\"B2 Score: \",b2)\nprint(\"B3 Score: \",b3)\nprint(\"B4 Score: \",b4)\n","b5216b5d":"# Creating NIC Model","025c39e7":"# Preparing Dataset and Data Loaders for Model","51422c98":"# Let's train it","53ead25d":"# Implementation of [Show and Tell: A Neural Image Caption Generator](https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf) in PyTorch \ud83d\udd26","c3dcd295":"# Greedy Inference","fb958237":"# Bleu Score","23203218":"# Encoder - Decoder for inference","d52d23bf":"# Inference using Beam Search","906d75f8":"# Creating train \/ val \/ test files to directly use for data loaders","e92edfd3":"Here, I am using pre-trained Inception V3 as a feature extractor CNN, features stored in dictonary are dumped using pickle for future use.","90cb038e":"# Pre-processing Captions, building Vocabulary (word map)","d319cb94":"# Extracting Image features from pre-trained CNN "}}