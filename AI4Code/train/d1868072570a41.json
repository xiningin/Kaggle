{"cell_type":{"fc981582":"code","7c0b7ee7":"code","f5dd6a62":"code","29686b02":"code","b2582941":"code","7da08c98":"code","fd3f163c":"code","70fa102c":"code","a070b944":"code","41ac5743":"code","0cbbfa10":"code","e3d3977e":"code","81afc9aa":"code","65721459":"code","0a888887":"code","5df8d6a9":"code","dd94ef17":"code","5d34aecc":"code","4e87ffdb":"code","2cf71cac":"code","a0c5a16c":"code","8aee2439":"code","2e0b7fb5":"code","36be1349":"code","d8ea010c":"code","eb609747":"code","37ea8141":"code","f9e6e4bb":"code","1ae0e36a":"code","2dd31bbf":"code","e71ff073":"code","796a299b":"code","614756af":"code","9725963d":"code","2a12ffd6":"code","8d01f51d":"code","449c5a2c":"code","8f04786a":"code","950e9633":"code","c0828085":"code","0f64be1d":"code","7f63b348":"code","617b43bc":"code","e2f0449e":"code","090a4da9":"code","20a69ce6":"code","60c24598":"code","ad37bd61":"code","9f334f57":"code","edc82983":"code","ac5a4b21":"code","bcd5f92c":"code","28f8f8e1":"code","ff571f81":"code","104fbb06":"code","0bc16c93":"code","c57ec73e":"code","e60c05eb":"code","05e72c74":"code","4ef64401":"code","45264424":"code","6fb0bf20":"code","a38fa46b":"code","95b8bae1":"code","e0e93bf1":"code","2cd05927":"code","436662fb":"code","ffd2da51":"code","e66d7a86":"code","663d2118":"code","432e407f":"code","c2a286d5":"code","37ae67b8":"code","185567aa":"code","8a16cd1d":"code","f693649c":"code","f8b62447":"code","a35ee33c":"code","fc4edfa3":"code","5bcbf2f8":"code","4bb95a50":"code","f4d85e1e":"markdown","24ddfeb9":"markdown","283ddc25":"markdown","1f47652f":"markdown","74794844":"markdown","365192b0":"markdown","edb9979a":"markdown","ad7d98be":"markdown","adb7b274":"markdown","0629fa67":"markdown","475aa39b":"markdown","c31f21ae":"markdown","58bdca81":"markdown","b953b246":"markdown","77999958":"markdown","99492d5d":"markdown","1c102e7b":"markdown","36969a85":"markdown","a5fb2762":"markdown","1eef5c9f":"markdown","b59ba7d9":"markdown","90b7e600":"markdown","66abcf7b":"markdown","fd228f2a":"markdown","28f500c1":"markdown","a3084a82":"markdown","5e45cde7":"markdown","b6560019":"markdown","336382ab":"markdown","efd2727f":"markdown","78ae670e":"markdown","8d994a39":"markdown","ab5f5c32":"markdown","b39a1c68":"markdown","8013a896":"markdown","edf096a7":"markdown","b34a06c9":"markdown","79039568":"markdown","e8c6f503":"markdown"},"source":{"fc981582":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","7c0b7ee7":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f5dd6a62":"train = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\nvalidation = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","29686b02":"print(train.shape)\nprint(validation.shape)\nprint(test.shape)","b2582941":"display(train.head())\ndisplay(test.head())","7da08c98":"train = train.loc[:15000,:]\ntrain.shape","fd3f163c":"train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","70fa102c":"def roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","a070b944":"X_train, X_test, y_train, y_test = train_test_split(train.comment_text.values, train.toxic.values, \n                                                  stratify=train.toxic.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","41ac5743":"max_features = 5000\nmaxlen = 1400","0cbbfa10":"## Check lenght of text in the data\ntrain['comment_text'].apply(lambda x:len(str(x).split())).max()","e3d3977e":"# using keras tokenizer here\ntoken = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n\ntoken.fit_on_texts(list(X_train) + list(X_test))\nX_train_seq = token.texts_to_sequences(X_train)\nX_test_seq = token.texts_to_sequences(X_test)\n\n#zero pad the sequences\nX_train_pad = sequence.pad_sequences(X_train_seq, maxlen=maxlen)\nX_test_pad = sequence.pad_sequences(X_test_seq, maxlen=maxlen)\n\nword_index = token.word_index","81afc9aa":"X_train[0]","65721459":"X_train_pad[0]","0a888887":"len(token.word_index)##49768","5df8d6a9":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","dd94ef17":"#unzip the file, we get multiple embedding files. We can use either one of them\n!unzip glove.6B.zip","5d34aecc":"from gensim.scripts.glove2word2vec import glove2word2vec","4e87ffdb":"from gensim.scripts.glove2word2vec import glove2word2vec\n\n#Glove file - we are using model with 50 embedding size\nglove_input_file = 'glove.6B.50d.txt'\n\n#Name for word2vec file\nword2vec_output_file = 'glove.6B.50d.txt.word2vec'\n\n#Convert Glove embeddings to Word2Vec embeddings\nglove2word2vec(glove_input_file, word2vec_output_file)","2cf71cac":"### We will extract word embedding for which we are interested in; the pre trained has 400k words each with 50 embedding vector size.\nfrom gensim.models import Word2Vec, KeyedVectors\n\n# Load pretrained Glove model (in word2vec form)\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\n#Embedding length based on selected model - we are using 50d here.\nembedding_vector_length = 50","a0c5a16c":"#Initialize embedding matrix\nembedding_matrix = np.zeros((max_features + 1, embedding_vector_length))\nprint(embedding_matrix.shape)","8aee2439":"for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n    if i > (max_features+1):\n        break\n    try:\n        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix[i] = embedding_vector\n    except:\n        pass","2e0b7fb5":"embedding_matrix.shape","36be1349":"#Initialize model\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","d8ea010c":"# A simpleRNN without any pretrained embeddings and one dense layer\nmodel = Sequential()\nmodel.add(tf.keras.layers.Embedding(max_features + 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=maxlen) #Number of words in each review\n         )\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","eb609747":"history=model.fit(X_train_pad,y_train,\n          epochs=10,\n          batch_size=32,          \n          validation_data=(X_test_pad, y_test))","37ea8141":"test_pred = model.predict(X_test_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(test_pred,y_test)))","f9e6e4bb":"test_pred.shape","1ae0e36a":"test_pred=test_pred.reshape((test_pred.shape[0],))","2dd31bbf":"test_pred=np.where(test_pred>0.5,1,0)","e71ff073":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,test_pred))\nprint(classification_report(y_test,test_pred))","796a299b":"scores_model = []\nscores_model.append({'Model': 'SimpleRNN','AUC_Score': round(roc_auc(model.predict(X_test_pad),y_test),2)})","614756af":"scores_model","9725963d":"import fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')  # English","2a12ffd6":"!ls -l","8d01f51d":"ft = fasttext.load_model('cc.en.300.bin')","449c5a2c":"### reduct the vector dimension to 50\nfasttext.util.reduce_model(ft, 50)","8f04786a":"ft.get_dimension()","950e9633":"#Initialize embedding matrix\nembedding_matrix_fasttext = np.zeros((max_features + 1, embedding_vector_length))\nprint(embedding_matrix_fasttext.shape)","c0828085":"for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n    if i > (max_features+1):\n        break\n    try:\n        embedding_vector = ft[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix_fasttext[i] = embedding_vector\n    except:\n        pass","0f64be1d":"embedding_matrix_fasttext.shape","7f63b348":"len(word_index)","617b43bc":"# A simple LSTM with glove embeddings and one dense layer\nmodel = Sequential()\nmodel.add(Embedding(max_features+1,\n                    embedding_vector_length, ### 50 here\n                    weights=[embedding_matrix_fasttext],\n                    input_length=maxlen, ### 1400 here\n                    trainable=False))\n\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()","e2f0449e":"history=model.fit(X_train_pad,y_train,\n          epochs=10,\n          batch_size=32,          \n          validation_data=(X_test_pad, y_test))","090a4da9":"history.history.keys()","20a69ce6":"test_pred = model.predict(X_test_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(test_pred,y_test)))","60c24598":"test_pred.shape","ad37bd61":"test_pred=test_pred.reshape((test_pred.shape[0],))","9f334f57":"test_pred=np.where(test_pred>0.5,1,0)","edc82983":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,test_pred))\nprint(classification_report(y_test,test_pred))","ac5a4b21":"scores_model.append({'Model': 'LSTM_with_fasttext','AUC_Score': round(roc_auc(model.predict(X_test_pad),y_test),2)})","bcd5f92c":"scores_model","28f8f8e1":"# GRU with fasttext embeddings\nmodel = Sequential()\nmodel.add(Embedding(max_features+1,\n                    embedding_vector_length, ### 50 here\n                    weights=[embedding_matrix_fasttext],\n                    input_length=maxlen, ### 1400 here\n                    trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n\nmodel.summary()","ff571f81":"history=model.fit(X_train_pad,y_train,\n          epochs=10,\n          batch_size=32,          \n          validation_data=(X_test_pad, y_test))","104fbb06":"test_pred = model.predict(X_test_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(test_pred,y_test)))","0bc16c93":"test_pred=test_pred.reshape((test_pred.shape[0],))","c57ec73e":"test_pred=np.where(test_pred>0.5,1,0)","e60c05eb":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,test_pred))\nprint(classification_report(y_test,test_pred))","05e72c74":"scores_model.append({'Model': 'GRU_with_fasttext','AUC_Score': round(roc_auc(model.predict(X_test_pad),y_test),2)})","4ef64401":"scores_model","45264424":"# A simple bidirectional LSTM with glove embeddings and one dense layer\nmodel = Sequential()\nmodel.add(Embedding(max_features+1,\n                    embedding_vector_length, ### 50 here\n                    weights=[embedding_matrix],\n                    input_length=maxlen, ### 1400 here\n                    trainable=False))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nmodel.summary()","6fb0bf20":"### we will just run for 1 epoch as it is for understanding purpose\nhistory=model.fit(X_train_pad,y_train,\n          epochs=1,\n          batch_size=32,          \n          validation_data=(X_test_pad, y_test))","a38fa46b":"test_pred = model.predict(X_test_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(test_pred,y_test)))","95b8bae1":"#comments and labels\ncomments = train.comment_text.values\nlabels = train.toxic.values","e0e93bf1":"### tokenize data using bert tokenizer\nfrom transformers import *\n#Get BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntokenized_texts = [tokenizer.tokenize(com) for com in comments]\n\n#We will use only first 200 tokens to do classification (this value can be changed)\nmax_length = 200\ntokenized_texts = [sent[:max_length] for sent in tokenized_texts]","2cd05927":"for i in range(len(tokenized_texts)):\n    sent = tokenized_texts[i]\n    sent = ['[CLS]'] + sent + ['[SEP]']\n    tokenized_texts[i] = sent\n    \n#Convert tokens into IDs\ninput_ids = [tokenizer.convert_tokens_to_ids(com) for com in tokenized_texts]","436662fb":"#Pad our tokens which might be less than max_length size\ninput_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_length+2, truncating='post', padding='post')","ffd2da51":"from sklearn.model_selection import train_test_split\ntrainX, testX, trainY, testY = train_test_split(input_ids, labels, test_size=0.2, random_state=123)","e66d7a86":"# Create attention masks for training\ntrain_attn_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in trainX:\n  seq_mask = [float(i>0) for i in seq]\n  train_attn_masks.append(seq_mask)","663d2118":"# Create attention masks for Test\ntest_attn_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in testX:\n  seq_mask = [float(i>0) for i in seq]\n  test_attn_masks.append(seq_mask)","432e407f":"#Load Pre-trained Bert Model with a Binary Classification layer at the top.\n#Huggingface library provides TFBertForSequenceClassification for the same\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')","c2a286d5":"# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])","37ae67b8":"model.summary()","185567aa":"train_x_data = {'input_ids': np.array(trainX), 'attention_mask': np.array(train_attn_masks)}\ntest_x_data = {'input_ids': np.array(testX), 'attention_mask': np.array(test_attn_masks)}","8a16cd1d":"model.fit(train_x_data, trainY, validation_data=(test_x_data, testY), batch_size=16, epochs=2)","f693649c":"test_pred = model.predict(test_x_data)[0]\n### this object return scored before softmax","f8b62447":"test_pred\n### scores before softmax","a35ee33c":"tf_prediction = tf.nn.softmax(test_pred, axis=1).numpy()\n## this will return probability score","fc4edfa3":"tf_prediction=np.argmax(tf_prediction,axis=1)\n## this will return predicted class labels","5bcbf2f8":"np.unique(tf_prediction)","4bb95a50":"print(\"Auc: %.2f%%\" % (roc_auc(tf_prediction,testY)))","f4d85e1e":"GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.\nTo solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.","24ddfeb9":"### Get word vectors for the words in the train data","283ddc25":"### Fit the model","1f47652f":"![Screenshot 2021-05-16 at 4.14.16 PM.png](attachment:6a0fa8c2-ccad-4b76-95ce-55a66a0d8655.png)","74794844":"## <center>Check some records<\/center>","365192b0":"The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.\n\nA limitation of the architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences.\n\n#### 1. Problem With Long Sequences\nThe encoder-decoder recurrent neural network is an architecture where one set of LSTMs learn to encode input sequences into a fixed-length internal representation, and second set of LSTMs read the internal representation and decode it into an output sequence.\n\nThis architecture has shown state-of-the-art results on difficult sequence prediction problems like text translation and quickly became the dominant approach.\n\nThe encoder-decoder architecture still achieves excellent results on a wide range of problems. Nevertheless, it suffers from the constraint that all input sequences are forced to be encoded to a fixed length internal vector.\n\nThis is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems.\n\n#### 2. Attention within Sequences\nAttention is the idea of freeing the encoder-decoder architecture from the fixed-length internal representation.\n\nThis is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence.\n\nPut another way, each item in the output sequence is conditional on selective items in the input sequence.\nThis increases the computational burden of the model, but results in a more targeted and better-performing model.\n\nIn addition, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.\n\n#### 3. Self Attention\n\nWe are not only talking about architectures bearing the name \u201cBERT\u2019, but more correctly Transformer-based architectures. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew the use of recurrence in neural network and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs.\n\nA self-attention module takes in n inputs, and returns n outputs. What happens in this module? In layman\u2019s terms, the self-attention mechanism allows the inputs to interact with each other (\u201cself\u201d) and find out who they should pay more attention to (\u201cattention\u201d). The outputs are aggregates of these interactions and attention scores.\nIllustration:\n\n\n","edb9979a":"<html>\n    <p style='background:teal; color:coral; font-size:40px; padding:10px;text-align:center'><b>Import libraries<\/b><\/p>\n<\/html>","ad7d98be":"### Model evaluation metric : AUC","adb7b274":"### More about GRU :\nhttps:\/\/towardsdatascience.com\/understanding-gru-networks-2ef37df6c9be","0629fa67":"### Prepare the data ","475aa39b":"### Embedding with Fasttext","c31f21ae":"<html>\n    <p style='background:teal; color:coral; font-size:40px; padding:10px;text-align:center'><b>Configuring TPU's<\/b><\/p>\n<\/html>","58bdca81":"<html>\n    <h1 style='background:gray;  color:pink; font-size:40px; padding:15px; border:5px solid red;'><center><b>4. Bi-Directional LSTM<\/b><\/center><\/h1>\n<\/html>","b953b246":"<html>\n    <h1 style='background:gray;  color:pink; font-size:40px; padding:15px; border:5px solid red;'><center><b>1. RNN - Recurrent Neural Network<\/b><\/center><\/h1>\n<\/html>","77999958":"<html>\n    <p style='background:teal; color:coral; font-size:40px; padding:10px;text-align:center'><b>Take away Notes<\/b><\/p>\n<\/html>\n1. This notebook is completely for the understanding purpost and hence the outputs are not shown completely for every model architechture.\n2. The number of epochs are also kept low for couple of architechtures as it is a time taking training tasks.\n3. For Bi directional LSTM and BERT models i have used GPU for training.\n4. Loading embedding from fastText is time consuming process.","99492d5d":"<html>\n    <h1 style='background:gray;  color:pink; font-size:40px; padding:15px; border:5px solid red;'><center><b>2. LSTM(Long Short Term Memory)- Improvement over simpleRNN<\/b><\/center><\/h1>\n<\/html>","1c102e7b":"### Drop the columns not required","36969a85":"### Split the data into Train and Test ","a5fb2762":"![Screenshot 2021-05-16 at 12.19.57 PM.png](attachment:8cae4629-b4b2-44d2-abea-9337e95bb863.png)","1eef5c9f":"<html>\n    <p style='background:teal; color:coral; font-size:40px; padding:10px;text-align:center'><b>Read the dataset<\/b><\/p>\n<\/html>","b59ba7d9":"Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\nThe idea of Bidirectional Recurrent Neural Networks (RNNs) is straightforward.\n\nIt involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.\n\nThis approach has been used to great effect with Long Short-Term Memory (LSTM) Recurrent Neural Networks.\n\nThe use of providing the sequence bi-directionally was initially justified in the domain of speech recognition because there is evidence that the context of the whole utterance is used to interpret what is being said rather than a linear interpretation.\n\nThe use of bidirectional LSTMs may not make sense for all sequence prediction problems, but can offer some benefit in terms of better results to those domains where it is appropriate.\n\n","90b7e600":"### Build a simple RNN model","66abcf7b":"**Advantages of Recurrent Neural Network**\n1. RNN can model sequence of data so that each sample can be assumed to be dependent on previous ones\n2. Recurrent neural network are even used with convolutional layers to extend the effective pixel neighbourhood.\n**Disadvantages of Recurrent Neural Network**\n1. Gradient vanishing and exploding problems.\n2. Training an RNN is a very difficult task.\n3. It cannot process very long sequences if using tanh or relu as an activation function.","fd228f2a":"Recurrent Neural Network is a generalization of feedforward neural network that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. After producing the output, it is copied and sent back into the recurrent network. For making a decision, it considers the current input and the output that it has learned from the previous input.\nUnlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other. Below is the simple RNN cell.","28f500c1":"Create Attention masks : Attention masks are useful to ignore padding tokens. Mask value will be set to 0 for padding tokens and 1 for actual tokens. We will create mask both for training and test data","a3084a82":"Before every model development i shall be covering the basic concept and architechture behind the mechanism","5e45cde7":"<html>\n    <h1 style='background:gray;  color:pink; font-size:40px; padding:15px; border:5px solid red;'><center><b>5. Attention Mechanism<\/b><\/center><\/h1>\n<\/html>","b6560019":"### Fit the model","336382ab":"### Since there are around 223k records in train set, we shall use a portion of it to train","efd2727f":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">About the Problem \ud83d\udcdd <\/p>\nThe aprroach here that this notebook is going to solve is using multiple models based on NLP. Some of them are listed below which this notebook will cover.\n1. Using RNN's\n2. Using word embeddings like Glove\n3. LSTM's\n4. GRU\n5. Seq2seq mechanism\n6. Transformers\n7. Attention mechanism","78ae670e":"![Screenshot 2021-05-16 at 1.30.11 PM.png](attachment:49e290b2-deeb-4bca-add6-bb9834fcf444.png)","8d994a39":"FastText is a vector representation technique developed by facebook AI research. As its name suggests its fast and efficient method to perform same task and because of the nature of its training method, it ends up learning morphological details as well.\nFastText is unique because it can derive word vectors for unknown words or out of vocabulary words \u2014 this is because by taking morphological characteristics of words into account, it can create the word vector for an unknown word. Since morphology refers to the structure or syntax of the words, FastText tends to perform better for such task, word2vec perform better for semantic task.","ab5f5c32":"<html>\n    <p style='background:teal; color:coral; font-size:40px; padding:10px;text-align:center'><b>Evaluation metric<\/b><\/p>\n<\/html>","b39a1c68":"### Initiate the RNN architecture","8013a896":"### Fit the model","edf096a7":"Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:\n\n* Input gate \u2014 discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.\n* Forget gate \u2014 discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct\u22121.\n* Output gate \u2014 the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.","b34a06c9":"### Embedding with Glove ","79039568":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\nTrue Positive Rate\nFalse Positive Rate\nTrue Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:\n\n![Screenshot 2021-05-16 at 8.02.24 PM.png](attachment:b9363bf1-ed9a-4273-bfc7-11c11e5301cf.png)\n\nFalse Positive Rate (FPR) is defined as follows:\n\n![Screenshot 2021-05-16 at 8.02.29 PM.png](attachment:83871910-461c-4863-aeee-fd79bad67bbb.png)\n\nAUC: Area Under the ROC Curve\nAUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n\n![Screenshot 2021-05-16 at 8.03.55 PM.png](attachment:0ff1fc05-e966-4aa5-957f-55cc3d5eeec7.png)\n\nAUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n\nAUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n\n## AUC\n\nAUC is desirable for the following two reasons:\n\nAUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\nAUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\nHowever, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n\nScale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won\u2019t tell us about that.\n\nClassification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.\n","e8c6f503":"<html>\n    <h1 style='background:gray;  color:pink; font-size:40px; padding:15px; border:5px solid red;'><center><b>3. GRU - Gated Recurrent Unit<\/b><\/center><\/h1>\n<\/html>"}}