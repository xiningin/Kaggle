{"cell_type":{"9a413b28":"code","c8f7b721":"code","1c991d7b":"code","714b6e45":"code","5a3de39b":"code","ea839cfc":"code","30119d9d":"code","1fab36a2":"code","ff616332":"code","e3747a99":"code","203d2d30":"code","e7eb769a":"code","7e08254f":"code","6a7d37da":"code","fcdcedaf":"code","d0228f70":"code","5ac86c15":"code","d17ee204":"code","0fc5cdfb":"code","f02eb5f8":"code","fa45bd13":"code","ef10abbf":"code","7c7da3c8":"code","f52cd7d2":"code","3a0600a1":"code","5d7ba95a":"code","b3c65d72":"markdown","367f7d16":"markdown","d21f3a9e":"markdown","99b44063":"markdown","bb9d75e6":"markdown","e6a771a2":"markdown","cf0db151":"markdown","9294e740":"markdown"},"source":{"9a413b28":"import numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\n\n# these imports are used to convert the tree to PNG\nfrom cairosvg import svg2png\nfrom PIL import Image\nfrom io import BytesIO\n\nimport gc\nimport os\nimport sys","c8f7b721":"VERSION = 'V1E'\nNUM_BOOST_ROUND = 2500\nVERBOSE_EVAL = 10\nMETRICS = ['auc']\nN_ROWS = 99271300\n\ndef get_index_np():\n    return np.arange(N_ROWS) # speeds up dataframe creation","1c991d7b":"# features are saved as compressed numpy arrays, much more efficient than a pandas DataFrame!\nFEATURES = np.load(f'\/kaggle\/input\/riiid-answer-correctness-prediction-features\/train_features_{VERSION}.npz', allow_pickle=True)","714b6e45":"given_features = [\n    'prior_question_elapsed_time',\n]\n\ndeduced_features = [\n    # user features\n    'mean_user_accuracy',\n    'answered_correctly_user',\n    'answered_user',\n    # content features\n    'mean_content_accuracy',\n    # part features\n    'part',\n    # other features\n    'hmean_user_content_accuracy',\n    'attempt',\n]\n\nfeatures = given_features + deduced_features\n\ntarget = 'answered_correctly'\n\n# add categorical features indices\ncategorical_feature = ['part', 'tags', 'tags_label', 'prior_question_had_explanation']\ncategorical_feature_idxs = []\nfor v in categorical_feature:\n    try:\n        categorical_feature_idxs.append(features.index(v))\n    except:\n        pass","5a3de39b":"def get_train_val_idxs(TRAIN_SIZE, VAL_SIZE):\n    train_idxs = []\n    val_idxs = []\n    NEW_USER_FRAC = 1\/4 # fraction of new users in \n    np.random.seed(42)\n    \n    # create df with user_ids and indices\n    df = pd.DataFrame(index=get_index_np())\n    for col in ['user_id']:\n        df[col] = FEATURES[col]\n\n    df['index'] = df.index.values.astype(np.uint32)\n    user_id_index = df.groupby('user_id')['index'].apply(np.array)\n    \n    # iterate over users in random order\n    for indices in user_id_index.sample(user_id_index.size, random_state=42):\n        if len(train_idxs) > TRAIN_SIZE:\n            break\n\n        # fill validation data\n        if len(val_idxs) < VAL_SIZE:\n            # add new user\n            if np.random.rand() < NEW_USER_FRAC:\n                val_idxs += list(indices)\n            # randomly split user between train and val otherwise\n            else:\n                offset = np.random.randint(0, indices.size)\n                train_idxs += list(indices[:offset])\n                val_idxs += list(indices[offset:])\n        else:\n            train_idxs += list(indices)\n        \n    return train_idxs, val_idxs\n\ntrain_idxs, val_idxs = get_train_val_idxs(int(50e6), 2.5e6)\nprint(f'len train_idxs: {len(train_idxs)}, len validation_idxs: {len(val_idxs)}')","ea839cfc":"def make_x_y(FEATURES, train_idxs, val_idxs):\n    # create numpy arrays\n    X_train = np.ndarray(shape=(len(train_idxs), len(features)), dtype=np.float32)\n    X_val = np.ndarray(shape=(len(val_idxs), len(features)), dtype=np.float32)\n    \n    # now fill them up\n    for idx, feature in enumerate(tqdm(features)):\n        X_train[:,idx] = FEATURES[feature][train_idxs].astype(np.float32)\n        X_val[:,idx] = FEATURES[feature][val_idxs].astype(np.float32)\n    \n    # add the target\n    y_train = FEATURES[target][train_idxs].astype(np.int8)\n    y_val = FEATURES[target][val_idxs].astype(np.int8)\n                         \n    return X_train, y_train, X_val, y_val\n    \nX_train, y_train, X_val, y_val = make_x_y(FEATURES, train_idxs, val_idxs)","30119d9d":"print(f'X_train.shape: {X_train.shape}\\t y_train.shape: {y_train.shape}')\nprint(f'X_val.shape: {X_val.shape}\\t y_val.shape: {y_val.shape}')","1fab36a2":"# show train features\npd.DataFrame(X_train[:10], columns=features)","ff616332":"y_train[:10]","e3747a99":"# make train and validation dataset\ntrain_data = lgb.Dataset(\n    data = X_train,\n    label = y_train,\n    categorical_feature = None,\n)\n\nval_data = lgb.Dataset(\n    data = X_val,\n    label = y_val,\n    categorical_feature = None,\n)","203d2d30":"del X_train, y_train, X_val, y_val\ngc.collect()","e7eb769a":"# NEW from: \nlgbm_params = {\n    'objective': 'binary',\n    'metric': METRICS,\n}","7e08254f":"%%time\ndef train():\n    evals_result = {}\n    model = lgb.train(\n        params = lgbm_params,\n        train_set = train_data,\n        valid_sets = [val_data],\n        num_boost_round = NUM_BOOST_ROUND,\n        verbose_eval = VERBOSE_EVAL,\n        evals_result = evals_result,\n        early_stopping_rounds = 10,\n        categorical_feature = categorical_feature_idxs,\n        feature_name = features,\n    )\n\n    # save model\n    model.save_model(f'model_{VERSION}_{NUM_BOOST_ROUND}.lgb')\n    \n    return model, evals_result\n    \nmodel, evals_result = train()","6a7d37da":"# plots the training history\ndef plot_history(evals_result):\n    for metric in METRICS:\n        plt.figure(figsize=(20,8))\n        \n        for key in evals_result.keys():\n            history_len = len(evals_result.get(key)[metric])\n            history = evals_result.get(key)[metric]\n            x_axis = np.arange(1, history_len + 1)\n            plt.plot(x_axis, history, label=key)\n        \n        x_ticks = list(filter(lambda e: (e % (history_len \/\/ 100 * 10) == 0) or e == 1, x_axis))\n        plt.xticks(x_ticks, fontsize=12)\n        plt.yticks(fontsize=12)\n\n        plt.title(f'{metric.upper()} History of training', fontsize=18);\n        plt.xlabel('EPOCH', fontsize=16)\n        plt.ylabel(metric.upper(), fontsize=16)\n        \n        if metric in ['auc']:\n            plt.legend(loc='upper left', fontsize=14)\n        else:\n            plt.legend(loc='upper right', fontsize=14)\n        plt.grid()\n        plt.show()\n\nplot_history(evals_result)","fcdcedaf":"# plot the feature importance in terms of gain and split\ndef show_feature_importances(model, importance_type, max_num_features=10**10):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = features\n    feature_importances['value'] = pd.DataFrame(model.feature_importance(importance_type))\n    feature_importances = feature_importances.sort_values(by='value', ascending=False) # sort feature importance\n    feature_importances.to_csv(f'feature_importances_{importance_type}.csv') # write feature importance to csv\n    feature_importances = feature_importances[:max_num_features] # only show max_num_features\n    \n    plt.figure(figsize=(20, 8))\n    plt.xlim([0, feature_importances.value.max()*1.1])\n    plt.title(f'Feature {importance_type}', fontsize=18);\n    sns.barplot(data=feature_importances, x='value', y='feature', palette='rocket');\n    for idx, v in enumerate(feature_importances.value):\n        plt.text(v, idx, \"  {:.2e}\".format(v))\n\nshow_feature_importances(model, 'gain')\nshow_feature_importances(model, 'split')","d0228f70":"# show tree and save as png\ndef save_tree_diagraph(model):\n    tree_digraph = lgb.create_tree_digraph(model, show_info=['split_gain', 'internal_count'])\n\n    tree_png = svg2png(tree_digraph._repr_svg_(), output_width=3840)\n    tree_png = Image.open(BytesIO(tree_png))\n\n    tree_png.save('create_tree_digraph.png')\n\n    display(tree_png)\n    \nsave_tree_diagraph(model)","5ac86c15":"# remove train and validation data to free memory before prediction phase\ndel train_data\ngc.collect()","d17ee204":"# dataframe with question features used for merging with test_df\ndef get_features_questions_df():\n    # create DataFrame of features\n    features_df = pd.DataFrame(index=get_index_np())\n    for col in tqdm(['content_id', 'part', 'tags', 'tags_label', 'mean_content_accuracy']):\n        features_df[col] = FEATURES[col]\n\n    # content features\n    features_questions_df = features_df.groupby('content_id')[[\n        # merge keys\n        'content_id',\n        'part',\n        'tags',\n        'tags_label',\n        # content\n        'mean_content_accuracy',\n    ]].first().reset_index(drop=True).sort_values('content_id')\n    \n    return features_questions_df\n    \nfeatures_questions_df = get_features_questions_df()\nprint(f'features_questions_df, rows: {features_questions_df.shape[0]}')\ndisplay(features_questions_df.head())","0fc5cdfb":"#user\u3054\u3068\u306e\u5e73\u5747\u6b63\u7b54\u6570\u3084\u6b63\u7b54\u6570\u7b49\u306e\u60c5\u5831\u304c\u5165\u3063\u305f\u9023\u60f3\u914d\u5217state\u3092\u4f5c\u6210\u3059\u308b\u95a2\u6570\uff08\u8981\u306fuser_table\uff09\ndef get_state():\n    # create DataFrame of features\n    #\u7a7a\u306eDF\u4f5c\u6210\n    features_df = pd.DataFrame(index=get_index_np())\n    \n    #\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u8ffd\u52a0\n    for col in tqdm(['user_id', 'content_id', 'answered_correctly']):\n        features_df[col] = FEATURES[col]\n        \n        \n    # compute user features over all train data\n    #\u3000user\u306e\u7279\u5fb4\u91cf\uff08\u5e73\u5747\u6b63\u7b54\u7387\u3001\u6b63\u7b54\u6570\u3001\u56de\u7b54\u6570\uff09\u3092\u305d\u308c\u305e\u308c\u5909\u6570\u306b\u683c\u7d0d\n    mean_user_accuracy = features_df.groupby('user_id')['answered_correctly'].mean().values\n    answered_correctly_user = features_df.groupby('user_id')['answered_correctly'].sum().values\n    answered_user = features_df.groupby('user_id')['answered_correctly'].count().values  \n    \n    \n    # fill dictionary with default values\n    #\u7a7a\u306estate\u9023\u60f3\u914d\u5217\u4f5c\u6210\n    state = dict()\n    #user_id\u3054\u3068\u306b\u3055\u3089\u306b\u5165\u308c\u5b50\u306e\u9023\u60f3\u914d\u5217\u4f5c\u6210\uff08JSON\u30c1\u30c3\u30af\uff09\n    for user_id in features_df['user_id'].unique():\n        state[user_id] = {}\n    #\u30e6\u30cb\u30fc\u30afuser\u6570\n    total = len(state.keys())\n        \n    # add user content attempts\n    \n    #user_id\u3054\u3068\u306e\u56de\u7b54\u554f\u984c\u30ea\u30b9\u30c8\n    user_content = features_df.groupby('user_id')['content_id'].apply(np.array).apply(np.sort).apply(np.unique)\n    \n    #user\u3054\u3068\u3001\u554f\u984c\u3054\u3068\u306e\u56de\u7b54\u6570\n    user_attempts = features_df.groupby(['user_id', 'content_id'])['content_id'].count().astype(np.uint8).groupby('user_id').apply(np.array).values\n    user_attempts -= 1\n    \n    #state\u306euser_id\u306e\u4e2d\u306bcontent_id\u304c\u30ad\u30fc\u3067\u56de\u7b54\u6570\u304c\u30d0\u30ea\u30e5\u30fc\u306e\u8f9e\u66f8\u3092\u633f\u5165\n    for user_id, content, attempt in tqdm(zip(state.keys(), user_content, user_attempts),total=total):\n        state[user_id]['user_content_attempts'] = dict(zip(content, attempt))\n    #\u4f7f\u3044\u7d42\u308f\u3063\u305fuser_content, user_attempts\u3092\u6d88\u53bb    \n    del user_content, user_attempts\n    gc.collect()\n    \n    #state\u306euser_id\u306e\u4e2d\u306b\u5e73\u5747\u6b63\u7b54\u7387\u3001\u6b63\u7b54\u6570\u3001\u56de\u7b54\u6570\u3092\u633f\u5165\n    for idx, user_id in enumerate(state.keys()):\n        state[user_id]['mean_user_accuracy'] = mean_user_accuracy[idx]\n        state[user_id]['answered_correctly_user'] = answered_correctly_user[idx]\n        state[user_id]['answered_user'] = answered_user[idx]\n    \n    return state\n\nstate = get_state()\nprint('Example of the state for user 2746, attempt counting starts at 0 as the pandas cumcount function is used to create the attempt feature')\ndisplay(state[2746])","f02eb5f8":"#state\u30c7\u30fc\u30bf\u30681\u30eb\u30fc\u30d7\u5206\u306e\u30e6\u30fc\u30b6\u30fc\u30ea\u30b9\u30c8\u3092\u6e21\u3059\u3068\u3001\u305d\u306e\u30ea\u30b9\u30c8\u9806\u306b\u8a72\u5f53\u30e6\u30fc\u30b6\u30fc\u306e\u305d\u306e\u554f\u984c\u306e\u56de\u7b54\u6570\u3001\u5e73\u5747\u6b63\u7b54\u7387\u3001\u6b63\u7b54\u6570\u3001\u7dcf\u56de\u7b54\u6570\u3092\u30ea\u30b9\u30c8\u3067\u8fd4\u3057\u3066\u304f\u308c\u308b\u95a2\u6570\ndef get_user_data(state, test_df):\n    # updated data\n    attempt, mean_user_accuracy, answered_correctly_user, answered_user = [], [], [], []\n    \n    #\n    for idx, (user_id, content_id) in test_df[['user_id', 'content_id']].iterrows():\n        # check if user exists\n        #state\u306euser_id\u30ea\u30b9\u30c8\u306b\u3044\u308b\u5834\u5408\uff08\u4ee5\u524d\u306e\u5229\u7528\u5c65\u6b74\u304c\u3042\u308buser\u306e\u5834\u5408\uff09\n        if user_id in state:\n            # check if user already answered the question, if so update it to a maximum of 4\n            #\u56de\u7b54\u6570\u3092\u53cd\u6620\uff08\u4e0a\u9650\u30924\u3067\u30af\u30ea\u30c3\u30d7\uff09\n            if content_id in state[user_id]['user_content_attempts']:\n                state[user_id]['user_content_attempts'][content_id] = min(4, state[user_id]['user_content_attempts'][content_id] + 1)\n            # if user did not answered the question already, set the number of attempts to 0\n            #\u56de\u7b54\u3057\u3066\u306a\u3051\u308c\u30700\n            else:\n                state[user_id]['user_content_attempts'][content_id] = 0\n        \n        # else create user with default values\n        #\u65b0\u898f\u30e6\u30fc\u30b6\u30fc\u306e\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u901a\u308a\u306bstate\u306euser_id\u306e\u4e2d\u306b\u30c7\u30fc\u30bf\u633f\u5165\n        else:\n            dict_keys = ['mean_user_accuracy', 'answered_correctly_user', 'answered_user', 'user_content_attempts']\n            dict_default_vals = [0.680, 0, 0, dict(zip([content_id],[0]))]\n            state[user_id] = dict(zip(dict_keys, dict_default_vals))\n            \n        # add user data to lists\n        1\u30eb\u30fc\u30d7\u3067\u6e21\u3055\u308c\u305f\u30e6\u30fc\u30b6\u30fc\u7fa4\u306e\u305d\u306e\u554f\u984c\u306e\u56de\u7b54\u6570\u3001\u5e73\u5747\u6b63\u7b54\u7387\u3001\u6b63\u7b54\u6570\u3001\u7dcf\u56de\u7b54\u6570\u3092\u30ea\u30b9\u30c8\u5316\n        attempt.append(state[user_id]['user_content_attempts'][content_id])\n        mean_user_accuracy.append(state[user_id]['mean_user_accuracy'])\n        answered_correctly_user.append(state[user_id]['answered_correctly_user'])\n        answered_user.append(state[user_id]['answered_user'])\n    #\u30ea\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u8fd4\u5374\n    return attempt, mean_user_accuracy, answered_correctly_user, answered_user","fa45bd13":"# updates the user data\n# state\u306euser_id\u306e\u6b63\u7b54\u6570\u3001\u56de\u7b54\u6570\u3001\u5e73\u5747\u6b63\u7b54\u7387\u3092\u66f4\u65b0\uff08features_questions_df\u304c\u5f15\u6570\u306b\u3042\u308b\u306e\u306f\u8b0e\u3000\u8a66\u884c\u932f\u8aa4\u306e\u7d50\u679c\uff1f\uff09\ndef update_user_data(state, features_questions_df, prev_test_df):\n    for user_id, content_id, answered_correctly in prev_test_df[['user_id', 'content_id', 'answered_correctly']].values:\n        # update user features\n        state[user_id]['answered_correctly_user'] += answered_correctly\n        state[user_id]['answered_user'] += 1\n        state[user_id]['mean_user_accuracy'] = state[user_id]['answered_correctly_user'] \/ state[user_id]['answered_user']","ef10abbf":"import riiideducation\n\nenv = riiideducation.make_env()\niter_test = env.iter_test()","7c7da3c8":"prev_test_df = None\nmean_attempt_acc_factor = FEATURES['mean_attempt_acc_factor']\n\nfor idx, (test_df, _) in tqdm(enumerate(iter_test)):\n    # from 2nd iteration, update user data\n    if prev_test_df is not None:\n        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        update_user_data(state, c, prev_test_df.loc[prev_test_df['content_type_id'] == 0])\n        if idx is 1:\n            display(test_df)\n            display(prev_test_df)\n    \n    # get user data from state and update attempt\n    attempt, mean_user_accuracy, answered_correctly_user, answered_user = get_user_data(state, test_df)\n\n    # set updated user data\n    test_df['attempt'] = attempt\n    test_df['mean_user_accuracy'] = mean_user_accuracy\n    test_df['answered_correctly_user'] = answered_correctly_user\n    test_df['answered_user'] = answered_user\n\n    # merge with all features\n    test_df = features_questions_df.merge(test_df, how='right', on='content_id')\n\n    # fill prior question had explenation\n    test_df['prior_question_elapsed_time'].fillna(23916, inplace=True)\n\n    # add harmonic mean\n    test_df['hmean_user_content_accuracy'] = 2 * (\n        (test_df['mean_user_accuracy'] * test_df['mean_content_accuracy']) \/\n        (test_df['mean_user_accuracy'] + test_df['mean_content_accuracy'])\n    )\n\n    test_df['answered_correctly'] = model.predict(test_df[features])\n\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n\n    # set previour test_df\n    prev_test_df = test_df.copy()","f52cd7d2":"submission = pd.read_csv('.\/submission.csv')","3a0600a1":"submission.info()","5d7ba95a":"submission.head()","b3c65d72":"# Make actual prediction","367f7d16":"# Make train and validation datasets","d21f3a9e":"# Prediction Preparation","99b44063":"This next function is the most important part of this notebook, it creates a state to keep track of attempt, mean user accuracy and the total amount of (correct) questions answered. This takes ~2 minutes to generate.\n\nThe state is implemented using dictionaries, as this is more efficient than using a DataFrame.\n\nAn example of the state for a user is given below.","bb9d75e6":"During prediction the previous test_df is saved and updated using the prior_group_answers_correct field of the next test_df.\n\nThis data is than used to update the user features in the state.","e6a771a2":"# Training","cf0db151":"# Training History","9294e740":"# Welcome!\n\nThis is the first notebook I ever published, any tips are welcome.\n\nThe trick that boosted my score significantly was to keep track of the attempt of every user for every question, using this discussion as inspiration.\n\nA state was used during the prediction pahse where also the mean user accuracy and answered (correctly) feature were updated.\n\nI will not disclose my features dataset as this notebook is purely meant for inspiration and not for copying and rerunning.\n\nAll steps will be provided of a short explanation, but if you have any questions feel free to ask them in the comments!"}}