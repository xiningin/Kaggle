{"cell_type":{"7450ee74":"code","299e56c3":"code","7cdb4f85":"code","34ff2524":"code","747edd3a":"code","259392bc":"code","aee669db":"code","9c0a4cd1":"code","fd00f1da":"code","bd36fb8f":"code","f153c76a":"markdown","40c442a0":"markdown","35181ed4":"markdown","ca84b7d8":"markdown","209e8033":"markdown","294828aa":"markdown"},"source":{"7450ee74":"%matplotlib notebook\nimport numpy as np\nimport random\nimport time","299e56c3":"#MAKE MATRIX OF THE ANSWERS- USES ONE-HOT ENCODING,\n#e.g. for number answer space 0-2: 0 =[1,0,0], 1 = [0,1,0], 2 = [0,0,1]\nanswers_matrix = np.identity(10)","7cdb4f85":"def sigmoid_activation(z):\n    return 1\/(1+np.exp(-z))\n    \ndef diff_sigmoid_activation(z):\n    \n    s = sigmoid_activation(z)\n    return s*(1-s)\n\ndef softmax_activation(z_array):\n\n    softmax_sum = np.sum(np.exp(z_array))\n    return np.exp(z_array)\/np.sum(np.exp(z_array))\n\ndef least_squares(answer,forecast):\n    \n    differences = answer-forecast\n    cost = np.sum(np.square(differences))\n    return cost","34ff2524":"class Layer: # Hidden Layer class: Base class for other Layer objects\n    \n    def __init__(self,layer_index, number_of_nodes,\n                 number_of_nodes_after, activation_function = np.vectorize(sigmoid_activation)):\n        print(\"Layer created...\")\n        self.index = layer_index #number the layers\n        self.activation_function = activation_function # allows for easy variation of activation func for layers\n        \n        self.zs     = np.random.rand(number_of_nodes).transpose()# column vector  \n        self.values = np.random.rand(number_of_nodes).transpose() # column vector \n        self.biases = np.random.rand(number_of_nodes).transpose() # column vector \n        self.deltas = np.random.rand(number_of_nodes).transpose() # column vector \n        self.weights_matrix = (np.random.rand(number_of_nodes,number_of_nodes_after)-0.5)","747edd3a":"class InputLayer(Layer):\n    \n    def __init__(self,number_of_inputs,number_of_nodes_after):\n        print(\"Input layer created...\")\n        Layer.__init__(self,0,number_of_inputs,number_of_nodes_after)","259392bc":"class OutputLayer(Layer):\n    def __init__(self,index,number_of_outputs):\n        print(\"output layer created...\")\n        Layer.__init__(self,index,number_of_outputs,0, softmax_activation)\n        \n    def get_forecast(self): # return prediction of NN.\n        return self.values      \n    \n    def softmax_sum(self):\n        return np.sum(np.exp(self.zs))","aee669db":"class NeuralNet:\n    \n    def __init__(self,layer_sizes):\n        \n        # Variables for analysis of NN performance \n        self.learning_speed = 0.05\n        self.number_correct = 0\n        self.predictions = np.zeros((1800,10,10))\n        self.accuracies = np.empty((1,0))\n        \n        # NN architecture variables \n        self.architecture = layer_sizes # array of layer sizes\n        self.number_of_layers = len(layer_sizes) # number of layers\n        self.network_layers = [] # array of layers\n        \n        for i in range(0,self.number_of_layers):\n            if i==0:\n                self.network_layers.append(InputLayer(layer_sizes[0],layer_sizes[1])) # add input layer\n            elif i==len(layer_sizes)-1:\n                self.network_layers.append(OutputLayer(i,layer_sizes[-1])) # add output layer\n            else:\n                self.network_layers.append(Layer(i,layer_sizes[i],layer_sizes[i+1])) # add hidden layers \n        \n        \n        np.asarray(self.network_layers)     \n        self.values_array = [] \n        self.z_array = [] \n        self.biases_array = []\n        self.weights_matrices = [] \n        self.deltas_array = []\n\n        print(\"Neural Network build complete.\")\n    \n    def describe(self): # Display NN architecture info\n        \n        print(\"\\nN.N. Architecture: \")\n        for x in self.architecture:\n            print(x)\n    \n    def feed_forward(self): # feed forward through the network, updating the Z's and values of Nodes.\n    \n        for layer in self.network_layers:\n            if layer.index == 0: #leave inputs alone\n                continue\n            \n            previous_layer = self.network_layers[layer.index-1]\n            layer.zs = np.matmul(previous_layer.weights_matrix.transpose(),np.array(previous_layer.values).astype(float)) + layer.biases                         \n            layer.values = layer.activation_function(layer.zs)\n   \n    def backpropagate(self,answer): # Calculate all the deltas needed for the gradient descent\n        \n        #Output layer delta calculations for back prop to start\n        output_layer = self.network_layers[-1]\n        output_layer.deltas = (output_layer.values-(output_layer.values\/output_layer.softmax_sum()))*(output_layer.values-answer)\n        \n    \n        #Deltas for all other layers\n        for layer in self.network_layers[-2::-1]: #backwards from layer before output layer\n            layer_infront = self.network_layers[layer.index+1]\n            for index in range(len(layer.deltas)):\n                layer.deltas[index] = diff_sigmoid_activation(layer.zs[index])*np.matmul(layer.weights_matrix,layer_infront.deltas)[index]\n    \n    \n    \n    def update(self): # Update all the weights and biases for the NN, uses stochastic gradient descent\n\n        for layer in self.network_layers[:-2]:\n            layer_infront = self.network_layers[layer.index+1]\n            layer.weights_matrix -= self.learning_speed*np.outer(layer.values,layer_infront.deltas)\n            layer.biases -= self.learning_speed*layer.deltas\n\n            \n    def calc_cost_func(self,answer,i,cost_function = least_squares):\n        \n        answer_forecast = np.asarray(self.network_layers[-1].get_forecast())\n        forecast = answer_forecast.argmax() \n        correct_answer = answer.argmax()\n        sample_number = int(np.floor(i\/100))\n        self.predictions[sample_number,correct_answer,forecast]+=1\n        \n        # This section will output performance data on the NN during training.\n        \n        if(forecast == correct_answer):\n            self.number_correct +=1\n        if (i%10 ==0):\n            print(i,\": Prediction: \", forecast, \"Actual: \", correct_answer) # show the NN's prediction versus actual\n        if(i%1000 ==0):\n            accuracy = self.number_correct\/10\n            self.accuracies = np.append(self.accuracies,accuracy)\n            print(\"\\n\\n\\n###########   \",accuracy,\"% accuracy (over last 1000 results)   ###########\\n\\n\\n\")\n            self.number_correct = 0 #reset after every 1000 entries\n            \n        return cost_function(answer,answer_forecast)\n\n\n        ","9c0a4cd1":"NN = NeuralNet([784,128,64,10]) # build NN with architecture 784,128,64,10\n\nepoch = 0\nwhile epoch <1: # only 1 epoch\n    NN.learning_speed \/= (10**epoch) # attempt to surpass 95% accuracy by implementing learning speed scheme \n    training_lines = open('..\/input\/mnist-dataset-60-10-split-processed\/MNIST_TRAINING_DATA_60K.txt',\"r\") \n    testing_lines = open('..\/input\/mnist-dataset-60-10-split-processed\/MNIST_TEST_DATA_10K.txt',\"r\")\n    i=0\n    sum = 0\n    initial_avg = 0\n    old_avg = 0\n    start_time = time.time()\n    for line in training_lines:\n        i+=1\n        NN.network_layers[0].values = np.asarray(line.split()[1:]).astype(float)\/255 -0.5 # normalise inputs\n        NN.feed_forward() # pass data through NN\n        cost = NN.calc_cost_func(answers_matrix[int(line.split()[0])],i) # calc cost for data point\n        NN.backpropagate(answers_matrix[int(line.split()[0])]) # backprop with the correct label for the data point\n        NN.update() # update using deltas array and learning speed\n\n        sum +=cost\n\n        if i%10==0:\n            print(i,\" Training data points complete... Cost = \",cost)\n        if i%1000 ==0:\n            if i == 1000:\n                initial_avg = sum\/1000\n            print(i, \" training data points complete, average cost change: \", sum\/1000-old_avg)\n            print(\"Total change in cost average: \", sum\/1000-initial_avg)\n            print(\"Time taken = \", (time.time()-start_time))\n            old_avg = sum\/1000\n            sum=0\n\n    print(\" ----------------- epoch \",epoch,\" complete -------------------\")\n    epoch+=1\n\n","fd00f1da":"import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.set_title(\"Neural Net Accuracy During Training (60K training data points)\")\nax.set_ylim(0,100)\nax.set_xlim(0,60)\nax.set_ylabel(\"Accuracy (%)\")\nax.set_xlabel(\"Data Point (1000's)\")\nax.grid(True)\nax.plot(np.insert(NN.accuracies,0,0), color = 'r', marker = '+') # accuracies must start at zero","bd36fb8f":"#TEST\ni=0\nfor line in testing_lines:\n    i+=1\n    NN.network_layers[0].values = np.asarray(line.split()[1:]).astype(float)\/255 -0.5 # normalise inputs\n    NN.feed_forward() # pass data through NN\n    cost = NN.calc_cost_func(answers_matrix[int(line.split()[0])],i) # calc cost for data point\n    sum +=cost\n\n    if i%10==0:\n        print(i,\" Testing data points complete... Cost = \",cost)\n\n","f153c76a":"# FUNCTIONS","40c442a0":"# Neural Network Number Reading\n\nTake in data from MNIST dataset and reads numbers 0-9. \n\nDate: 05\/08\/2021\n\nVersion: 3.0 \n\nAuthor: Elliot Hicks\n\nEmail: elliot.hicks.student@outlook.com","35181ed4":"# Loading MNIST data\nRead data from files, the data was moved using the MNIST READER file in this dir\nFile names:\n- MNIST_TRAINING_DATA_60K.txt\n- MNIST_TEST_DATA_10K.txt\n\n-FMT example\n- 7 (LABEL) 0 0 0 12 0 0 257 0 0...","ca84b7d8":"# NETWORK CLASS","209e8033":"## Initialisation\n","294828aa":"# LAYER CLASSES"}}