{"cell_type":{"ddbd9561":"code","11ad5852":"code","383aad2e":"code","9c49c123":"code","1ca48a10":"code","842dbbbb":"code","ba841658":"code","7cb51ca0":"code","ec8fed34":"code","d46d92ee":"code","8da6b5e4":"code","55bba1c1":"code","85930147":"code","75bd01bb":"code","08f69e6a":"code","4d076395":"code","2025577b":"code","cad4c22b":"code","db79a43b":"code","b104e6fa":"code","b4ef2210":"code","5d0a12f4":"code","668da24c":"code","1cd8e133":"code","15b1b921":"code","6bf408da":"code","a6545994":"code","48286eba":"code","e8c5dd82":"code","2bc39ff6":"code","46932581":"code","db1ea068":"code","f5b456f6":"code","c7e32384":"code","6db70713":"code","ad72ca0a":"code","38361950":"code","ffd12b54":"code","7f1a8c1b":"code","fa4dfab0":"code","19ff1a76":"markdown","f22a8361":"markdown","28275a7c":"markdown","1766edb1":"markdown","08208c5d":"markdown","9e029e80":"markdown","e067a19c":"markdown","c38a5627":"markdown","dc4cf6e5":"markdown","26a52e92":"markdown","3f32ff51":"markdown","bc74f61d":"markdown","657cd7f4":"markdown","ec191241":"markdown","68c4f48f":"markdown"},"source":{"ddbd9561":"# Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Importing libraries for data analysis and cleaning\nimport numpy as np\nimport pandas as pd\n\n#importing visualisation libraries for data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\n\n#load datasets\ndf = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf = df.rename(columns = {'Chance of Admit ' : 'Chance of Admit'})\ndf.head()","11ad5852":"df.info()","383aad2e":"#Dropping the serial column (Irrelavant for predictions)\ndf = df.drop('Serial No.',axis=1)","9c49c123":"df.describe()","1ca48a10":"#Checking for null values\ndf.isnull().sum()","842dbbbb":"plt.figure(figsize=(12,5))\nsns.distplot(df['Chance of Admit'],bins=30)\nplt.show()\n\nplt.figure(figsize=(12,5))\nsns.distplot(df['CGPA'],bins=30)\nplt.show()\n\nplt.figure(figsize=(12,5))\nsns.distplot(df['GRE Score'],bins=30)\nplt.show()\n\nplt.figure(figsize=(12,5))\nsns.distplot(df['TOEFL Score'],bins=30)\nplt.show()","ba841658":"px.pie(df,'Research',title='Research Experience Distribution')","7cb51ca0":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(),cmap='coolwarm',vmax=1,vmin=-1,annot=True);\n# fix for mpl bug that cuts off top\/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show() ","ec8fed34":"#Linear fit for CGPA and Chance of Admission\nsns.lmplot(data=df,y='Chance of Admit',x='CGPA');","d46d92ee":"#Bar chart showing highest correlation with chance of admit\ndf.corr()['Chance of Admit'].drop('Chance of Admit').sort_values(ascending=False).plot(kind='bar',figsize=(10,5));","8da6b5e4":"#University ranking\npx.pie(df,names='University Rating')","55bba1c1":"df.corr()['University Rating'].drop(['University Rating','Chance of Admit']).sort_values(ascending=False)","85930147":"plt.figure(figsize=(8,4))\nplt.title('PER STATEMENT OF PURPOSE')\nsns.barplot(data=df,x='University Rating',y='SOP')\nplt.show()\n\nplt.figure(figsize=(8,4))\nplt.title('PER CGPA')\nsns.barplot(data=df,x='University Rating',y='CGPA')\nplt.show()\n\nplt.figure(figsize=(8,4))\nplt.title('PER TOEFL Score')\nsns.barplot(data=df,x='University Rating',y='TOEFL Score')\nplt.show()\n","75bd01bb":"plt.figure(figsize=(10,5))\nsns.barplot(data=df,x='University Rating',y='Research');","08f69e6a":"df.head()","4d076395":"#Defining the variables X and y Where; \n\n#X are the features for training \nX = df.drop('Chance of Admit',axis=1)\n\n#y is the target(Chance of admittance) to be predicted\ny = df['Chance of Admit']","2025577b":"#Splitting the train data into train and test purposes.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","cad4c22b":"#Scaling features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","db79a43b":"#mean average of admitance\ndf['Chance of Admit'].mean()","b104e6fa":"from sklearn.model_selection import cross_val_score,GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost","b4ef2210":"#Validation function\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(estimator=model, X = X_train,y= y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)","5d0a12f4":"#Linear regression deafult params\nrmse_cv(LinearRegression()).mean()","668da24c":"#Random forest regressor default params\nrmse_cv(RandomForestRegressor()).mean()","1cd8e133":"#XGBOOST regrssor Default parameters\nrmse_cv(xgboost.XGBRegressor()).mean()","15b1b921":"LR = LinearRegression()\nLR.fit(X_train,y_train)\ny_pred = LR.predict(X_test)","6bf408da":"from sklearn.metrics import mean_squared_error,explained_variance_score\nrmse_lr = np.sqrt(mean_squared_error(y_pred, y_test))\nprint('RMSE:',np.sqrt(mean_squared_error(y_pred, y_test)))\nprint('R2:',explained_variance_score(y_pred,y_test))","a6545994":"coeff = pd.DataFrame(LR.coef_,X.columns)\ncoeff.columns = ['Coefficient']\ncoeff","48286eba":"sns.distplot((y_test-y_pred),bins=30);","e8c5dd82":"# Our predictions\nplt.scatter(y_test,y_pred)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r');","2bc39ff6":"param_grid = {\n    'bootstrap': [True,False],\n    'max_depth': range(20,200,20),\n    'max_features': ['auto','sqrt'],\n    'min_samples_leaf': range(1,10,2),\n    'min_samples_split': range(1,10,2),\n    'n_estimators': range(100,1000,100),\n}\n\n# Create a based model\nrf = RandomForestRegressor()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","46932581":"#grid_search.fit(X_train,y_train)\n#grid_search.best_estimator","db1ea068":"rr = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=60, max_features='sqrt', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=9, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=0, warm_start=False)","f5b456f6":"rr.fit(X_train,y_train)","c7e32384":"pred_rr = rr.predict(X_test)\nrmse_rr = np.sqrt(mean_squared_error(pred_rr, y_test))\nprint('RMSE:',np.sqrt(mean_squared_error(pred_rr, y_test)))\nprint('R2:',explained_variance_score(pred_rr,y_test))","6db70713":"# Our predictions\nplt.scatter(y_test,pred_rr)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r');","ad72ca0a":"param_tuning = {\n        'learning_rate': [0.01, 0.1,0.03],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 4,5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.6, 0.7],\n        'n_estimators' : [100, 200, 500]\n}\n\nxgb_model = xgboost.XGBRegressor()\n\nrsearch = RandomizedSearchCV(estimator=xgb_model,\n                             param_distributions=param_tuning,\n                            n_jobs=-1,cv=5,verbose=1)\n #scoring = 'neg_mean_squared_error',  #MSE\n\nrsearch.fit(X_train,y_train)","38361950":"rsearch.best_params_","ffd12b54":"xgr_pred = rsearch.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(xgr_pred, y_test))\nprint('RMSE:',np.sqrt(mean_squared_error(xgr_pred, y_test)))\nprint('R2:',explained_variance_score(xgr_pred,y_test))","7f1a8c1b":"plt.bar(['Linear Reg', 'Random Forest','XGB Reg'], [rmse_lr, rmse_rr,rmse_xgb])\nplt.ylabel('RMSE')\nplt.title('Compare Models')\nplt.show()","fa4dfab0":"print('RMSE:',np.sqrt(mean_squared_error(pred_rr, y_test)))","19ff1a76":"**Correlations**\n\nWe can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n\nThe correlation coefficient gives us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n1. .00 -.19 \u201cvery weak\u201d\n2. .20 -.39 \u201cweak\u201d\n3. .40 -.59 \u201cmoderate\u201d\n4. .60 -.79 \u201cstrong\u201d\n5. .80 -1.0 \u201cvery strong\u201d","f22a8361":"Unlike universities ranked 2nd or 3rd, univeristies under the 4th and 5th positions demand research experience as well","28275a7c":"### Feauture scaling\n\nThis is done to normalise or standardise the features of a dataset. Since each column has data in different magnitudes or units of measurement, if the data is not scaled, the machine learning algorithm would consider some features as higher than the others. \n\nExample: GRE scores and TOEFL scores are measured or scored diffrently. If these are not scaled, it could output wrong predictions.","1766edb1":"Linear regression","08208c5d":"## Models for predictions (Regression)\n\n1. Linear regression\n\n2. Random Forest regression\n\n4. XGBOOST regression","9e029e80":"## Train\/Test Split\n\nThe train dataset would be split into two. One for training and the other for testing the model to see how well it will perform against the true lables. This enables the data scientist to see the actual performance of the model on data it hasnt seen before(test data). When satisfied with the performance, the model can then be deployed for use on the actual test dataset provided for predictions.\n\nIn this scenario, the train data would be split into a 80% (for training) and the 20% for testing to measure performance.","e067a19c":"XGB regressor","c38a5627":"It is evident that the higher the university rating, the higher the score or demand. Universities ranked 5 demand a higher CGPA, SOP,as well as TOEL scores as expected.","dc4cf6e5":"# Graduate Admissions\n\n## Context\n\nThis dataset is created for prediction of Graduate Admissions from an Indian perspective.\n\n### Content\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs. \nThe parameters included are :\n\nGRE Scores ( out of 340 )\n\nTOEFL Scores ( out of 120 )\n\nUniversity Rating ( out of 5 )\n\nStatement of Purpose and Letter of Recommendation Strength ( out of 5 )\n\nUndergraduate GPA ( out of 10 )\n\nResearch Experience ( either 0 or 1 )\n\nChance of Admit ( ranging from 0 to 1 )\n\n**Acknowledgements**\n\nThis dataset is inspired by the UCLA Graduate Dataset. The test scores and GPA are in the older format.\nThe dataset is owned by Mohan S Acharya.\n\n**Inspiration**\n\nThis dataset was built with the purpose of helping students in shortlisting universities with their profiles. The predicted output gives them a fair idea about their chances for a particular university.\n\n**Citation**\n\nPlease cite the following if you are interested in using the dataset :\nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019","26a52e92":"From the correlation plot, its evident that all features have a postive correlation coefficent of 0.55 and above in the determination of an admission into the university for a masters program. CGPA being the highest at 0.88 which is evident.","3f32ff51":"### Compare Models\n\nAveargely, all models seem to attain the same level of performance. Through the bar chart below, random forest appears to have to least rmse of 0.0621. \n\nThis model would be adopted for predictions.","bc74f61d":"Model already excuted. Creating model based on its best estimators from grid search.","657cd7f4":"Distribution of scores seems to be fairly normally distibuted. The same goes for the chances of admission. Data isnt skewed","ec191241":"Random Forest","68c4f48f":"Most students end up in Universities ranked as 2nd or 3rd.\n\nLets investigate what the determinants are for being admitted into a higher ranking university."}}