{"cell_type":{"03b32df7":"code","ddf28db3":"code","28d2170d":"code","b9077632":"code","2c1aca32":"code","587efc2a":"code","40607c9e":"code","93556183":"code","a3723cd5":"code","47ce23ed":"code","6841a5c6":"code","52476169":"code","2621710c":"code","04d285ed":"code","aef42598":"code","8597cecf":"code","8b78c091":"code","641c85e0":"code","fed373d7":"code","1b17c5e7":"code","e3ab0f6a":"code","ea2e512c":"code","08eb47ac":"code","6ec61100":"code","2a570af9":"code","b8882f6e":"code","abeeda4d":"code","d4a8a57c":"code","258be454":"code","6e9d37f3":"code","9059deee":"code","4e42d242":"code","7fe1cc24":"code","7eb03420":"code","a77031c8":"code","96cd0f4d":"code","08dd4758":"code","f8963764":"code","6f94f3c0":"code","69c22090":"code","3641cc45":"code","3fdab790":"code","4ac154cd":"code","d37de2d9":"code","90458cb6":"code","eb2d1f1a":"code","0982d141":"code","27ee8542":"code","1091eeaf":"code","1a7acbb1":"code","c20bde08":"code","ea6cdd46":"code","8ba367b8":"code","6be5dfd5":"code","42d540ac":"code","4326a343":"code","1c3fe97a":"code","b2920499":"code","55db0a5c":"code","972df55a":"code","49324b32":"code","bc5b69f2":"code","d77cbe7e":"code","04f8d02d":"code","70bf8682":"code","ce54bbf2":"code","fe383a93":"code","02793995":"code","085ab6eb":"code","2937c986":"code","79c91891":"code","e141036d":"code","3ca5e4f0":"code","a87d3fa5":"markdown","ca330d66":"markdown","43553652":"markdown","c7e25b6a":"markdown","6e0004c5":"markdown","a96229b0":"markdown","0850455a":"markdown","8b9fd822":"markdown","8093380f":"markdown","2e6e749d":"markdown","35c0cd0e":"markdown","8a4655c8":"markdown","101a33e0":"markdown","883b7b9c":"markdown","3f5f9930":"markdown","701b7980":"markdown","9c4fa222":"markdown","c7953e2b":"markdown","98ac30b4":"markdown","fadbc492":"markdown","9be1d3e4":"markdown","11a8d2f1":"markdown","0c7f16dd":"markdown","914c65f6":"markdown"},"source":{"03b32df7":"# Importing libarieries ","ddf28db3":"! pip install sklearn","28d2170d":"! pip install ipywidgets","b9077632":"!pip install sweetviz","2c1aca32":"! pip install imblearn  ","587efc2a":"# Lets import libaries ","40607c9e":"import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport os \n\n# setting size of figures\nplt.rcParams['figure.figsize']=(16,5)\n# setting style of plot\nplt.style.use('fivethirtyeight')\n\n# For interactivity\nimport ipywidgets as widgets \nfrom ipywidgets import interact\nfrom ipywidgets import interact_manual\n\n# for EDA(explorary Data Analysis)\nimport sweetviz\n\n# for machine learning\nimport sklearn \nimport imblearn ","93556183":"# used for selecting style of plot \n# fivethirtyeight is best \nplt.style.available","a3723cd5":"# Reading DataSets\nprint(os.listdir(\"..\/input\/\"))","47ce23ed":"# Show first 5 columns\ntrain = pd.read_csv(\"\/kaggle\/input\/hranalysis\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hranalysis\/test.csv\")","6841a5c6":"# show last 5 columns\ntrain.head()","52476169":"# Checking the datatype \ntrain.info()","2621710c":"train.describe()","04d285ed":"train['is_promoted'].value_counts().plot(kind = 'bar')","aef42598":"# Let's check the target class Balance\nplt.rcParams['figure.figsize']=(15,5)\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nsns.countplot(train['is_promoted'],)\n\n# Labelling \nplt.xlabel('Promoted or Not?', fontsize= 10)\n\n# for piechart\nplt.subplot(1,2,2)\ntrain['is_promoted'].value_counts().plot(kind = 'pie',explode = [0,0.1], autopct= '%.2f%%',\n                                         startangle = 90,\n                                         labels= [ 'Non Promoted Employees',\n                                                  'Promoted Employees'],shadow = True,pctdistance = 0.5)\n\nplt.axis('off')\n\nplt.suptitle('Target Class Balance', fontsize = 15)\nplt.show()","8597cecf":"# Let make a report using sweetviz for complete EDA\nmy_report = sweetviz.compare([train, 'Train'],[test,'Test'], 'is_promoted')\nmy_report.show_html('Report.html')","8b78c091":"# let's check  descritptive statistics for numerical columns\ntrain.describe()","641c85e0":"# let's check descriptive statistics for categorical columns\ntrain.describe(include='object')","fed373d7":"# let's make interactive function to check the statistics of these numerical columns at a time \n\n@interact\ndef check(column=list(train.select_dtypes('number').columns[1:])):\n    print('Maximum Value :', train[column].max())\n    print('Minimum Value:',train[column].min())\n    print('Mean:{0:2f}'.format(train[column].mean()))\n    print('Median:',train[column].median())    \n    print('Standard Deviation:{0:2f}'.format(train[column].std()))   ","1b17c5e7":"# missing vlues in training datasets \ntrain.isnull().sum()","e3ab0f6a":"# missing values in training data set\n\n# lets calculate the total missing values in the dataset\ntrain_total = train.isnull().sum()\n\n# lets calculate the percentage of missing values in the dataset\ntrain_percent = ((train.isnull().sum()\/train.shape[0])*100).round(2)\n\n# lets calculate the percentage of missing values in the dataset\ntest_percent = ((test.isnull().sum()\/test.shape[0])*100).round(2)\n\n# lets calculate the total missing values in the dataset\ntest_total = test.isnull().sum()\n\n# Let's check the percentage of missing vlues in training datasets \ntrain.percent=((train.isnull().sum()\/train.shape[0])*100).round(2)\n\n# lets make a dataset consisting of total no. of missing values and percentage of missing values in the dataset\ntrain_missing_data = pd.concat([train_total, train_percent, test_total, test_percent],\n                                axis=1, \n                                keys=['Train_Total', 'Train_Percent %','Test_Total', 'Test_Percent %'],\n                                sort = True)\n\n# lets check the head\ntrain_missing_data.style.bar(color = ['gold'])","ea2e512c":"# lets impute the missing values in the Training Data\ntrain['education']= train['education'].fillna(train[ 'education'].mode()[0])\ntrain['previous_year_rating'] = train['previous_year_rating'].fillna(train['previous_year_rating'].mode()[0])\n\n# lets check whether the Null values are still present or not?\nprint(\"Number of Missing Values Left in the Training Data :\", train.isnull().sum())","08eb47ac":"# lets check the columns where we can have outliers \ntrain.select_dtypes('number').head()","6ec61100":"train['awards_won?'].value_counts()","2a570af9":"# lets check the boxplots for the columns where we suspect for outliers\nplt.rcParams['figure.figsize']= (15,5)\nplt.style.use('fivethirtyeight')\n\n\n# Box plot for average training score\nplt.subplot(1,2,1)\nsns.boxplot(train['avg_training_score'],color = 'red')\nplt.xlabel('Average Training Score',fontsize = 12)\nplt.ylabel('Range',fontsize = 12)\n\n# Box plot for length of  service \nplt.subplot(1,2,2)\nsns.boxplot(train['length_of_service'],color = 'red')\nplt.xlabel('Length of Service',fontsize = 12)\nplt.ylabel('Range',fontsize=12)\n\nplt.suptitle('Box Plot', fontsize=12)\nplt.show()","b8882f6e":"# let's plot pie charts for the columns where we have few categories\nplt.rcParams['figure.figsize']=(12,5)\nplt.style.use('fivethirtyeight')\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,1)\nlabels = ['0','1']\nsizes = train['KPIs_met >80%'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0, 0]\n\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('KPIs Met > 80%', fontsize = 15)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,2)\nlabels = ['1','2','3','4', '5']\nsizes = train['previous_year_rating'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0,1,5))\nexplode = [0,0,0,0,0.1]\n\nplt.pie(sizes,labels = labels , colors = colors , explode = explode , shadow = True , startangle = 90)\nplt.title('Previous year Rating ', fontsize = 20)\n\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,3)\nlabels = ['0', '1']\nsizes = train['awards_won?'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0,1,5))\nexplode = [0,0.1]\n\nplt.pie(sizes, labels = labels , colors = colors , explode = explode , shadow = True , startangle = 90)\nplt.title('Awards Won',fontsize = 20)\n\nplt.legend()\nplt.show()","abeeda4d":"# legend() Demonstration \n# A legend is an area describing the elements of the graph. In the matplotlib library, there\u2019s a function called legend() which is used to Place a legend on the axes.\nx = [1,2,4,5,9]\ny = [1,4,5,5,6]\nplt.plot(x,y)\nplt.legend(['single element']) \nplt.show()","d4a8a57c":"# lets check the distribution of trainings undertaken by employees\nplt.rcParams['figure.figsize']=(15,5)\nsns.countplot(train['no_of_trainings'], palette = 'viridis')\nplt.xlabel( ' ', fontsize = 14)\nplt.title('Distribution of trainings taken by Employees')\nplt.show()","258be454":"# let's Check the Age of Employees\nplt.rcParams['figure.figsize']= (8,4)\nplt.hist(train['age'],color = 'black')\nplt.title('Distribution of Age among the Employees',fontsize = 15)\nplt.xlabel('Age of the Employees')\nplt.grid()\nplt.show()","6e9d37f3":"# lets check different Departments\nplt.rcParams['figure.figsize']= (12,6)\nsns.countplot(y = train['department'], palette = 'cividis', orient = 'v')\nplt.xlabel('')\nplt.ylabel('Department Name')\nplt.title('Distribution of Employees in Different Departments', fontsize = 15)\nplt.grid()\nplt.show()","9059deee":"# Let's check the distribution of different regions \nplt.rcParams['figure.figsize'] = (12,15)\nsns.countplot(y= train['region'], palette = 'inferno',orient= 'v')\nplt.xlabel('')\nplt.ylabel('Region')\nplt.xticks(rotation = 90)\nplt.title('Different regions', fontsize = 15)\nplt.grid()\nplt.show()","4e42d242":"# let's plot the pie charts of the columns where we have few categories \nplt.rcParams['figure.figsize'] = (15,5)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,1)\nlabels = train['education'].value_counts().index\nsizes = train[ 'education'].value_counts()\ncolors = plt.cm.copper(np.linspace(0,1,5))\nexplode = [0,0,0.1]\n\nplt.pie(sizes , labels = labels , colors = colors , explode = explode , shadow = True , startangle = 90)\nplt.title('Education', fontsize = 15)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,2)\nlabels = train['gender'].value_counts().index\nsizes = train['gender'].value_counts()\ncolors = plt.cm.copper(np.linspace(0,1,5))\nexplode = [0,0]\n\nplt.pie(sizes , labels = labels , colors = colors , explode = explode , shadow = True , startangle = 90)\nplt.title('Gender', fontsize =  15)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1,3,3)\nlabels = train['recruitment_channel'].value_counts().index\nsizes = train['recruitment_channel'].value_counts()\ncolors = plt.cm.copper(np.linspace(0,1,5))\nexplode = [0,0,0.1]\n\nplt.pie(sizes,labels = labels , colors = colors , explode = explode , shadow = True , startangle = 90)\nplt.title('Recruitment Channel')\n#Explode \n# The explode argument in pyplot pie decides which part should explode (separate and move a distance from the center). This argument accepts a tuple of numeric values, and each non-zero value represents the distance of that slice from the center of a pie chart. In this Python pie chart example, we exploded medium priority from the center.","7fe1cc24":"# interactive function for plotting univariate charts for categorical data\n\nplt.rcParams['figure.figsize'] = (15, 4)\n@interact_manual\ndef check(column = list(train.select_dtypes('object').columns),\n          palette = ['cividis','copper','spring','Reds','Blues']):\n    sns.countplot(train[column], palette = palette)\n   \n    plt.show()","7eb03420":"# Compare gender gap in promotion \n\nplt.rcParams['figure.figsize']= (15,5)\nx = pd.crosstab(train['gender'], train['is_promoted'])\ncolors = plt.cm.Wistia(np.linspace(0,1,5))\nx.div(x.sum(1).astype(float),axis=0).plot(kind = 'bar' , stacked = False, color = colors)\nplt.title('Effect of Gender Promotion', fontsize = 15)\nplt.xlabel('')\nplt.show()","a77031c8":"# Compare different departments and Promotion\nplt.rcParams['figure.figsize'] = (15,5)\nx = pd.crosstab(train['department'],train['is_promoted'])\ncolors = plt.cm.copper(np.linspace(0,1,3))\nx.div(x.sum(1).astype(float),axis=0).plot(kind = 'area', stacked = False , color = colors)\nplt.title('Effect of Department on Promotion', fontsize = 15)\nplt.xticks(rotation = 20)\nplt.xlabel('')\nplt.show()","96cd0f4d":"# Checking effect of number of trainings for promotion\nplt.rcParams['figure.figsize'] = (15,3)\nsns.barplot(train['no_of_trainings'], train['is_promoted'])\nplt.title('Effect of Trainings' ,fontsize = 15)\nplt.xlabel('No of Trainings', fontsize = 10)\nplt.ylabel('Employee promoted or not',fontsize = 10)\nplt.show()","08dd4758":"# Effect of Age on Promotion\nplt.rcParams['figure.figsize'] = (15,4)\nsns.boxenplot(train['is_promoted'], train['age'], palette = 'PuRd')\nplt.title('Effect of Age on Promotion' , fontsize = 15)\nplt.xlabel('Employee get promoted or not ?', fontsize = 10)\nplt.xlabel('Age of Employee', fontsize = 10)\nplt.show()","f8963764":"# Department vs Average training score\nplt.rcParams['figure.figsize'] = (17,6)\nsns.boxplot(train['department'], train['avg_training_score'], palette = 'autumn')\nplt.title('Average Training score from each department', fontsize = 15)\nplt.xlabel('Department', fontsize = 10)\nplt.ylabel('Avg training score', fontsize = 10)\nplt.show()","6f94f3c0":"# Let's Make interactive function for Bivariate Analysis\nplt.rcParams['figure.figsize'] = (15,5)\n@interact_manual\ndef  bivariate_plot(column1 = list(train.select_dtypes('object').columns),\n                    column2 = list(train.select_dtypes('number').columns[1:])):\n    sns.boxplot(train[column1],train[column2])","69c22090":"# lets make an Interactive Function for Bivariate Analysis\nplt.rcParams['figure.figsize'] = (15,5)\n@interact_manual\ndef bivariate_plot(column1 = list(train.select_dtypes('object').columns),\n                  column2 = list(train.select_dtypes('number').columns[1:])):\n    sns.boxenplot(train[column1],train[column2])                  ","3641cc45":"# lets check the Heat Map for the Data with respect to correlation.\nplt.rcParams['figure.figsize'] = (15,5)\nsns.heatmap(train.corr(),annot = True ,linewidth = 0.5,cmap = 'Wistia')\nplt.title('Correaltion Heat Map',fontsize = 15)\nplt.show()","3fdab790":"# Check the relation of recruitment Channel, length of service and Promotions when they won awards ?\nplt.rcParams['figure.figsize'] =(16,7)\nsns.boxplot(train['recruitment_channel'],\n           train['length_of_service'],\n            hue = train['is_promoted'],\n            palette = 'cividis')\nplt.title('Recruitment Channel vs Length of Service')\nplt.ylabel('Recruitment Channel')\nplt.xlabel('Length of Service')\nplt.show()","4ac154cd":"train","d37de2d9":"# lets check the relation of Departments and Promotions when they won awards ?\nplt.rcParams['figure.figsize'] = (16,7)\nsns.barplot(train['department'],train[ 'length_of_service'],hue = train[ 'awards_won?'],palette = 'autumn')\nplt.title('Chance of Promotion in each department', fontsize = 15)\nplt.ylabel('Length of service',fontsize=10)\nplt.xlabel('Departments',fontsize = 10)","90458cb6":"# lets create some extra features from existing features to improve our Model\n# creating a metric of sum\ntrain['sum_metric'] = train['awards_won?']+ train['KPIs_met >80%'] + train['previous_year_rating']\ntest['sum_metric'] = test['awards_won?']+test['KPIs_met >80%'] + test['previous_year_rating']\n\n# Creating total score column\ntrain['total_score'] = train['avg_training_score'] * train['no_of_trainings']\ntest['total_score'] = test['avg_training_score'] * test['no_of_trainings']","eb2d1f1a":"# lets remove some of the columns which are not very useful for predicting the promotion.\n\ntrain = train.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\ntest = test.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\n\n# lets check the columns in train and test data set after feature engineering\ntrain.columns","0982d141":"# let's check the realtionship between KPIs and promotion \nx = pd.crosstab(train['KPIs_met >80%'],train['is_promoted'])\nx.style.background_gradient(cmap = 'bone')","27ee8542":"#lets check the relation between the Awards and Promotion\nx = pd.crosstab(train['awards_won?'],train['is_promoted'])\nx.style.background_gradient(cmap='bone')","1091eeaf":"# lets check the no. eployees who won awards from each Department\ntrain[['department', 'awards_won?']].groupby(['department']).agg('sum').sort_values(by = 'awards_won?',\n                                                            ascending = False).style.background_gradient('magma')","1a7acbb1":"# lets group the employees based on their Education\n@interact\ndef group(column = list(train.select_dtypes('object').columns)):\n    return train[[column,'is_promoted']].groupby([column]).agg(['count',\n                                                              'sum','mean','max']).style.background_gradient(cmap='viridis')","c20bde08":"# lets use the interactive function to make it more reusable\n@interact\ndef group_operations(column = list(train.select_dtypes('object').columns),\n                     column2 = list(train.select_dtypes('number').columns)[1:]):\n    return train[[column,column2]].groupby([column]).agg('count').style.background_gradient(cmap = 'Wistia')","ea6cdd46":"# lets get the names of all the employees who have taken trainings more than 7 Times\n\n@interact\ndef check(column = 'no_of_trainings', x = 5):\n    y = train[train['no_of_trainings'] > x]\n    return y['is_promoted'].value_counts()","8ba367b8":"# lets also check the value counts of the number of trainings employee took.\ntrain['no_of_trainings'].value_counts()","6be5dfd5":"# lets cap the values of number of trainings after 5, as the chances of promotion is negligible after 5th training \n\ntrain['no_of_trainings'] = train['no_of_trainings'].replace((6, 7, 8, 9, 10),(5, 5, 5, 5, 5))\n\n# lets check the values of no. of trainings after capping the values\ntrain['no_of_trainings'].value_counts()","42d540ac":"'''\nlets check the no. of employee who did not get an award, did not acheive 80+ KPI, previous_year_rating as 1\nand avg_training score is less than 40\nbut, still got promotion.\n''' \n\ntrain[(train['KPIs_met >80%'] == 0) & (train['previous_year_rating'] == 1.0) & \n      (train['awards_won?'] == 0) & (train['avg_training_score'] < 60) & (train['is_promoted'] == 1)]","4326a343":"# lets remove the above two columns as they have a huge negative effect on our training data\n\n# lets check shape of the train data before deleting two rows\nprint(\"Before Deleting the above two rows :\", train.shape)\n\ntrain = train.drop(train[(train['KPIs_met >80%'] == 0) & (train['previous_year_rating'] == 1.0) & \n      (train['awards_won?'] == 0) & (train['avg_training_score'] < 60) & (train['is_promoted'] == 1)].index)\n\n# lets check the shape of the train data after deleting the two rows\nprint(\"After Deletion of the above two rows :\", train.shape)","1c3fe97a":"# lets check how many of the employees have greater than 30 years of service and still do not get promotion\n\n@interact\ndef check_promotion(x = 20):\n    x = train[(train['length_of_service'] > x)]\n    return x['is_promoted'].value_counts()","b2920499":"# We gonna check first what are the categorical column is present in data\ntrain.select_dtypes('object').head()","55db0a5c":"# Counts of Education \ntrain['education'].value_counts()","972df55a":"# lets start encoding these categorical columns to convert them into numerical columns\n\n# lets encode the education in their degree of importance \ntrain['education'] = train['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\ntest['education'] = test['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\n\n# lets use Label Encoding for Gender and Department to convert them into Numerical\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain['department'] = le.fit_transform(train['department'])\ntest['department'] = le.fit_transform(test['department'])\ntrain['gender'] = le.fit_transform(train['gender'])\ntest['gender'] = le.fit_transform(test['gender'])\n\n# lets check whether we still have any categorical columns left after encoding\nprint(train.select_dtypes('object').columns)\nprint(test.select_dtypes('object').columns)","49324b32":"# Lets check whether we still have any categorical columns left after encoding\nprint(train.select_dtypes('object').columns)\nprint(test.select_dtypes('object').columns)\ntrain.head(3)","bc5b69f2":"# Let's split the target data  from train data\n\ny = train['is_promoted']  # target variable \nx = train.drop(['is_promoted'],axis = 1)\nx_test = test\n\n# let's print the shapes of newly formed datasets \nprint('Shape of x :', x.shape)\nprint('Shape of y :', y.shape)\nprint('Shape of x test:', x_test.shape)","d77cbe7e":"# It is very important to resample the data, as the target class is highly imbalanced.\n# Here We are going to use Over Sampling Technique to resample the data.\n# lets import the SMOTE algorithm to do the same.\n\nfrom imblearn.over_sampling import SMOTE\n\nx_resample , y_resample = SMOTE().fit_sample(x , y.values.ravel())\n\nprint(x_resample.shape)\nprint(y_resample.shape)","04f8d02d":"# Let's check the value counts of target variable 4\nprint('Before Sampling :')\nprint(y.value_counts())\nprint('After sampling :')\ny_resample = pd.DataFrame(y_resample)\nprint(y_resample[0].value_counts())","70bf8682":"# lets create a validation set from the training data so that we can check whether the model that we have created is good enough\n# lets import the train_test_split library from sklearn to do that\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_valid , y_train , y_valid = train_test_split(x_resample , y_resample, test_size = 0.2,random_state = 0)\n\n# Let's print shape\nprint('Shape of x train :',x_train.shape)\nprint('Shape of y train :',y_train.shape)\nprint('Shape of x valid tain:', x_valid.shape)\nprint('Shape of y valid train:',y_valid.shape)\nprint('Shape of x Test :',x_test.shape)","ce54bbf2":"# Feature Scaling is important for scaling the datasets - we will use standardization method (commnly used)\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\nx_test = sc.transform(x_test)","fe383a93":"train.head()","02793995":"# Lets use Decision Trees to classify the data\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Testing Accuracy :\", model.score(x_valid, y_valid))\n\ncm = confusion_matrix(y_valid, y_pred)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'Wistia', fmt = '.8g')\nplt.show()","085ab6eb":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nmodel = DecisionTreeClassifier() \nrfecv = RFECV(estimator = model, step = 1, cv = 5, scoring = 'accuracy')\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nx_train = pd.DataFrame(x_train)\nprint('Best features :', x_train.columns[rfecv.support_])","2937c986":"# lets take a look at the Classification Report\ncr = classification_report(y_valid, y_pred)\nprint(cr)","79c91891":"train.describe()","e141036d":"# lets perform some Real time predictions on top of the Model that we just created using Decision Tree Classifier\n\n# lets check the parameters we have in our Model\n'''\ndepartment -> The values are from 0 to 8, (Department does not matter a lot for promotion)\neducation -> The values are from 0 to 3 where Masters-> 3, Btech -> 2, and secondary ed -> 1\ngender -> the values are 0 for female, and 1 for male\nno_of_trainings -> the values are from 0 to 5\nage -> the values are from 20 to 60\npreviou_year_rating -> The values are from 1 to 5\nlength_of service -> The values are from 1 to 37\nKPIs_met >80% -> 0 for Not Met and 1 for Met\nawards_won> -> 0-no, and 1-yes\navg_training_score -> ranges from 40 to 99\nsum_metric -> ranges from 1 to 7\ntotal_score -> 40 to 710\n'''","3ca5e4f0":"prediction = rfecv.predict(np.array([[2, #department code\n                                      3, #masters degree\n                                      1, #male\n                                      1, #1 training\n                                      30, #30 years old\n                                      5, #previous year rating\n                                      10, #length of service\n                                      1, #KPIs met >80%\n                                      1, #awards won\n                                      95, #avg training score\n                                      7, #sum of metric \n                                      700 #total score\n                                     ]]))\n\nprint(\"Whether the Employee should get a Promotion : 1-> Promotion, and 0-> No Promotion :\", prediction)","a87d3fa5":"# Dealing with Cateogrical model\n\n\nDealing with categorical columns how?\n###### Converting categorical column (objects) to numerical columns interms of  1 0r 0 becuase machine learning model only works with numerical columns.\n## Various ways to encode cateogrical columns into Numerical Columns:\nI am gonna use label encoder to Department and Gender columns","ca330d66":"# PROJECT\n\n\n\n\n\n\n##                 Predicting wheather employee of an organization should get Promotion or Not ?\n\n\n\n\n\n\n\n\n\nYour client is very large MNC Company they have 9 broad verticals organization. One of the problem your client is facing is around identifying the right people for promotion (only for managers position and below) and prepare them in time.\n\n\nCurrently process , they are following\n\n\n1) They first identify set of employees based on recommendation\/past peformance \n\n2) Selected employees go through the training and evaluation program for each vertical. These program are based on he required skill of each vertical.\n\n3) At the end of program based on various factors sucha as training peoformance , and employee gets an promotion.","43553652":"# Decisions Tree\n\nA decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.","c7e25b6a":"# Univariate Analysis\n\nUnivariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.\n\nThis is an Essential step, to understand the variables present in the dataset one by one.\n\nFirst, we will check the Univariate Analysis for Numerical Columns to check for Outliers by using Box plots.\n\nThen, we will use Distribution plots to check the distribution of the Numerical Columns in the Dataset.\n\nAfter that we will check the Univariate Analysis for Categorical Columns using Pie charts, and Count plots.\n\nWe Use Pie charts, when we have very few categories in the categorical column, and we use count plots we have more categorises in the dataset.","6e0004c5":"#### Treatment of missing values is very imp step in ML Model Creation\n\n### Types of Missing values\n1) Missing values Random\n2)Missing values are not random\n3) Missing values at completely random\n\n### What we can do to impute or treate missing values to make good ML Model?\n\nUse Business logic to impute the missing values\n\nuse Statistical methods - Mean , Median and Mode\n\nuse ML techniques to impute the misssing values.\n\ncan delete missing values when the missing values is very high.\n\n### When to  use Mean , Median and Mode ?\n\nUse Mean - when we don't have outliers in the datasets for Numerical Variables.\n\nUse Median - when we  have outliers in the datasets for Numerical Variables.\n\nUse Mode - when we have cateorgical  variables","a96229b0":"From above graphical representation it shows there is alomost similar effect on promotion.Wecan sayt hat all departments have similar effect on promotion.It doesn't contribute a lot in making a Machine Learning Model.(Not able to prdeict wheather Employee should get promotion or not) ","0850455a":"# Outliers Detection using Box Plots\n\nThe presence of outliers in a classification or regression dataset can result in a poor fit and lower predictive modeling performance. Instead, automatic outlier detection methods can be used in the modeling pipeline and compared, just like other data preparation transforms that may be applied to the dataset","8b9fd822":"# Splitting Data\n\n###### This is one of the most Important step to perform Machine Learning Prediction on a Dataset, you have to separate the target and independent Columns.","8093380f":"The abov Countplot, where are checking the distribution of trainings undertaken by the Employee, It is clearly visible that 80 % of the employees have taken the training only once, and there are negligible no. of employees, who took trainings more than thrice.","2e6e749d":"# Descrtiptive Statistics","35c0cd0e":"It is clear hat from above graph who takes more  than 5 trainings will not get promoted.","8a4655c8":"We, can see that there are some pie charts, we have for representing KPIs, Previous year Ratings, and Awards Won?\n\nAlso, The one Big Pattern is that only some of the employees could reach above 80% of KPIs set.\nMost of the Employees have a very low rating for the previous year, and\nvery few employees, probably 2% of them could get awards for their work, which is normal.","101a33e0":"We can see from the above table, that Only two columns have missing values in Train and Test Dataset both. Also, the Percentage of Missing values is around 4 and 7% in education, and previous_year_rating respectively. So, do not have delete any missing values, we can simply impute the values using Mean, Median, and Mode Values. \n\nLets check the Data Types of these Columns, so that we can impute the missing values in these columns.","883b7b9c":"Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.\n\n#### Types of Bivariate Analysis\n\n##### 1)Categorical vs Categorical\n###### 2)Categorical vs Numerical\n###### 3)Numerical vs Numerical","3f5f9930":"# Libary Usages\n\n\n### Matplotlib vs seaborn\nSeborn provides advance staistical operations and graphs \n\n### sweetviz libary \nIt is used for EDA purpose.\n\n### Sklearn \nThis is used for Machine Learning Projects\n\n### ipywidgets \nThis libary \nwidgets used for making a program interactive\n\n\n### imblearn\n used for imblanced datasets","701b7980":"###  Treatment of Missing values ","9c4fa222":"From above graph we can say that length of service and age are highly coorelated.\nAnd also previous year rating and KPI are correlated to each othe","c7953e2b":"# Resampling \n\nResampling is the method that consists of drawing repeated samples from the original data samples. The method of Resampling is a nonparametric method of statistical inference.\n\n#### In this Problem we have noticed that the target column is highly imbalanced, we need to balance the data by using some Statistical Methods.\n\n### There are many Statistical Methods we can use for Resampling the Data such as:\nOver Samping\n\nCluster based Sampling\n\nUnder Sampling","98ac30b4":"### Grouping & Filtering Operations\n###### The grouping operations are the fundamental components of the entity clustering technique. They define what collections of entities and relationships comprise higher-level objects, the entity clusters.","fadbc492":"# FEATURE ENGINEERING\n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.\n\n#### There are mutliple ways of performing feature engineering.\n\nSo many people in the Industry consider it the most important step to improve the Model Performance.\n\nWe should always understand the columns well to make some new features using the old existing features.\n\n## Let's discuss the ways how we can perform feature engineering\n\nWe can perform Feature Engineering by removing Unnecassary Columns\n\nExtracting Features from the Date and Time Features.\n\nExtracting Features from the Categorcial Features.\n\nBinnning the Numerical and Categorical Features.\n\nAggregating Multiple Features together by using simple Arithmetic operations.","9be1d3e4":"# Multivariate Analysis\n\n### Multivariate analysis is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time.\n\n####  First, we will use the Correlation Heatmap to check the correlation between the Numerical Columns\n#### Check the ppscore or the Predictive Score to check the correlation between all the columns present in the data.\n#### Use Bubble Charts, split Violin plots, Hue with Bivariate Plots.","11a8d2f1":"# Feature Scaling\n\nFeature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing \n\n![image.png](attachttp:\/\/localhost:8888\/files\/Workshop%20Datasets\/image.pnghment:image.png)","0c7f16dd":"## Bivariate Anlaysis","914c65f6":"Descriptive Statistics is important step to understand the  Data and take out insights.\n\nfor Categorical columns we check for for stats ,count , frequency and unique elements."}}