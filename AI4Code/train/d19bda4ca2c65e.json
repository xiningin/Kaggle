{"cell_type":{"2edd7e5d":"code","fe625afd":"code","50049536":"code","20fab83e":"code","bf09fc28":"code","4f951de3":"code","9724d2a8":"code","2aba3091":"code","1682b7ba":"code","7a1968ae":"code","41339f7a":"code","953b033e":"code","8f0661b0":"code","aa2ef1fb":"code","fda6674c":"code","9dca9950":"code","8678ef8b":"code","1ef658b0":"code","c63f3c80":"code","988c6dee":"code","48313509":"code","a1f6131e":"code","471232ae":"code","2d80e754":"code","f24be383":"code","00a99c4e":"code","29f29d46":"code","96dbe802":"code","a25eba51":"code","916c50b6":"code","1d933d7a":"code","c0cc4ab2":"code","b70427fa":"code","0015c4f4":"code","f0ee200a":"code","63ddd7e0":"code","9a9a181f":"code","60540742":"code","ef41155a":"code","944a8493":"code","7cc7f391":"code","797106e9":"code","6c3b44a0":"code","b0ef5773":"code","c5d9f658":"code","bc2925e9":"markdown","91ba989e":"markdown","affaf5fc":"markdown","c2062fb1":"markdown","e2ddb088":"markdown","03b841c1":"markdown","b596976c":"markdown","27424a16":"markdown","4f24cc82":"markdown","4219b124":"markdown","a608e596":"markdown","6f66a586":"markdown","cdb80501":"markdown","e9a7a335":"markdown","bad3ec2f":"markdown","feaf25ec":"markdown","cdb62130":"markdown","78f1c1ea":"markdown","9b94dad0":"markdown","a0ab1479":"markdown","d8d99b36":"markdown"},"source":{"2edd7e5d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport joblib","fe625afd":"# display all the columns in the dataset\npd.pandas.set_option('display.max_columns', None)","50049536":"# Load the Data\ntrain= pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsample = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\n","20fab83e":"print(\"train:\",train.shape,\",\",\"calendar:\",calendar.shape,\",\",\"sell_prices:\",sell_prices.shape)","bf09fc28":"train.info()","4f951de3":"calendar.info()","9724d2a8":"train.head()","2aba3091":"calendar.head()","1682b7ba":"sell_prices.head()","7a1968ae":"train.isnull().sum().sort_values(ascending = False)","41339f7a":"calendar.isnull().sum().sort_values(ascending = False)","953b033e":"for i in range(1942,1970):\n    col = \"d_\"+ str(i)\n    train[col] = 0","8f0661b0":"train.head()","aa2ef1fb":"#Downcasting\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  \n\ntrain = downcast(train)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","fda6674c":"train_ad = np.round(train.memory_usage().sum()\/(1024*1024),1)\ncalendar_ad = np.round(calendar.memory_usage().sum()\/(1024*1024),1)\nsell_prices_ad = np.round(sell_prices.memory_usage().sum()\/(1024*1024),1)","9dca9950":"#melt data\nmelt_df = pd.melt(train, \n                  id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                  var_name = 'd', \n                  value_name = \"sales\").dropna()","8678ef8b":"melt_df.head()\n","1ef658b0":"single= pd.merge(melt_df,calendar, on='d', how = \"left\")\nsingle.head()","c63f3c80":"del calendar,melt_df\ngc.collect()","988c6dee":"single = pd.merge(single, sell_prices, on=['store_id','item_id','wm_yr_wk'], how='left') \nsingle.head(20)","48313509":"del sell_prices\ngc.collect()","a1f6131e":"print(single.shape)\nsingle2= single[single['cat_id']== 'HOBBIES']\nprint(single2.info())\nprint(single2.shape)","471232ae":"#Sales by Store\nstates = pd.DataFrame(single2.state_id.unique(),columns=['state_id'])\nbar_data = single2.groupby(['state_id', 'store_id'])['sales'].sum().dropna()\nbar_data\nplt.ticklabel_format(style = 'plain')\nplt.ylabel(\"State\/Store\")\nplt.xlabel(\"Number of sales\")\nplt.title(\"Sales by Store\")\n\nbar_data.plot(kind='barh',figsize=(18, 10))\n\n\n","2d80e754":"#Revenue by State\nsingle2['revenue'] = single2['sales']*single2['sell_price'].astype(np.float32)\nbar_data2 = single2.groupby(['state_id'])['revenue'].sum().dropna()\nbar_data2\nplt.ticklabel_format(style = 'plain')\nplt.ylabel(\"State\")\nplt.xlabel(\"Revenue\")\nplt.title(\"Revenue by State\")\nbar_data2.plot(kind='barh',figsize=(10, 7))","f24be383":"#Revenue by Store\nbar_data3=single2.groupby(['store_id'])['revenue'].sum().dropna()\nplt.ticklabel_format(style = 'plain')\nplt.ylabel(\"Store\")\nplt.xlabel(\"Revenue\")\nplt.title(\"Revenue by Store\")\nbar_data3.plot(kind='barh',figsize=(10,8))","00a99c4e":"group_price_store = single2.groupby(['state_id','store_id','item_id'],as_index=False)['sell_price'].mean().dropna()\nfig = px.violin(group_price_store, x='store_id', color='state_id', y='sell_price',box=True, hover_name='item_id')\nfig.update_xaxes(title_text='Store')\nfig.update_yaxes(title_text='Selling Price($)')\nfig.update_layout(template='seaborn',title='Distribution of Item prices across Stores',legend_title_text='State')\nfig.show()                                                                                        ","29f29d46":"data = single2.groupby(['store_id', pd.Grouper(key='date', freq='M')])['sales'].sum()\ndata = data[data>0]\nfig = px.line(\n    data_frame=data.reset_index(),\n    x = 'date',\n    y = 'sales',\n    color = 'store_id'\n    \n)\nfig.update_xaxes(nticks=7)\nfig.update_layout(\n    title = dict(text = 'units sold by month-year')\n)\nfig.show()","96dbe802":"#Average Weekly sales by day\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nsingle2[\"weekday\"] = pd.Categorical(single2.weekday, categories=days, ordered=True)\ndata = single2.loc[single2.sales>0].groupby(['weekday','store_id',])['sales'].mean().reset_index()\nfig = px.line(              \n    data_frame=data,\n    x = 'weekday',\n    y = 'sales',\n    color = 'store_id'\n    \n)\nfig.update_layout(\n    title = dict(text='Average units sold by store')\n)\n\nfig.show()","a25eba51":"data2 = single2.sort_values('revenue',ascending=False).groupby(['year']).nth(1)\ndata2\ndata3 = data2.reset_index()\n\nfig = px.bar(\n    data_frame=data3,\n    x = 'year',\n    y = 'revenue',\n    color = 'store_id'\n    \n)\nfig.update_layout(\n    title = dict(text='Store with highest Revenues by year')\n)\n\nfig.show()\n","916c50b6":"data3 = single2.sort_values('sales',ascending=False).groupby(['year']).nth(1)\ndata2\ndata4 = data3.reset_index()\n\nfig = px.bar(\n    data_frame=data4,\n    x = 'year',\n    y = 'sales',\n    color = 'store_id'\n    \n)\nfig.update_layout(\n    title = dict(text='Store with most sales by year')\n)\n\nfig.show()","1d933d7a":"# convert numeric variables into categorical variables\nconv_id = dict(zip(single2.id.cat.codes, single2.id))\nconv_item_id = dict(zip(single2.item_id.cat.codes, single2.item_id))\nconv_dept_id = dict(zip(single2.dept_id.cat.codes, single2.dept_id))\nconv_cat_id = dict(zip(single2.cat_id.cat.codes, single2.cat_id))\nconv_store_id = dict(zip(single2.store_id.cat.codes, single2.store_id))\n","c0cc4ab2":"single2.d = single2['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = single2.dtypes.index.tolist()\ntypes = single2.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        single2[cols[i]] = single2[cols[i]].cat.codes","b70427fa":"single2.head()","0015c4f4":"single2.drop('date',1,inplace = True)","f0ee200a":"#Introduce lags\nlags = [1,2,3,6,12,24,36]\nfor lag in lags:\n    single2['sold_lag_'+str(lag)] = single2.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sales'].shift(lag).astype(np.float16)","63ddd7e0":"single2['rolling_sold_mean'] = single2.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)","9a9a181f":"single2['daily_avg_sold'] = single2.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sales'].transform('mean').astype(np.float16)\nsingle2['avg_sold'] = single2.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform('mean').astype(np.float16)\nsingle2['selling_trend'] = (single2['daily_avg_sold'] - single2['avg_sold']).astype(np.float16)\nsingle2.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","60540742":"single2.info()","ef41155a":"valid = single2[(single2['d']>=1914) & (single2['d']<1942)][['id','d','sales']]\ntest = single2[single2['d']>=1942][['id','d','sales']]\neval_preds = test['sales']\nvalid_preds = valid['sales']","944a8493":"single2 = single2[single2['d']>=36]","7cc7f391":"single2.to_pickle('data.pkl')\ndel single2\ngc.collect();","797106e9":"data = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sales']]\ntest = data[data['d']>=1942][['id','d','sales']]\neval_preds = test['sales']\nvalid_preds = valid['sales']","6c3b44a0":"#Get the store ids\nstores = train.store_id.cat.codes.unique().tolist()\nfor store in stores:\n    df = data[data['store_id']==store]\n    \n    #Split the data\n    X_train, y_train = df[df['d']<1914].drop('sales',axis=1), df[df['d']<1914]['sales']\n    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sales',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sales']\n    X_test = df[df['d']>=1942].drop('sales',axis=1)\n    \n    #Train and validate\n    model = LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        max_depth=8,\n        num_leaves=50,\n        min_child_weight=300\n    )\n    print('*****Prediction for Store: {}*****'.format(conv_store_id[store]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n             eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n    valid_preds[X_valid.index] = model.predict(X_valid)\n    eval_preds[X_test.index] = model.predict(X_test)\n    filename = 'model'+str(conv_store_id[store])+'.pkl'\n    # save model\n    joblib.dump(model, filename)\n    \n    del model, X_train, y_train, X_valid, y_valid\n    gc.collect()","b0ef5773":"#Feature importance\nfeature_importance_df = pd.DataFrame()\nfeatures = [f for f in data.columns if f != 'sales']\nfor filename in os.listdir('\/kaggle\/working\/'):\n    if 'model' in filename:\n        # load model\n        model = joblib.load(filename)\n        store_importance_df = pd.DataFrame()\n        store_importance_df[\"feature\"] = features\n        store_importance_df[\"importance\"] = model.feature_importances_\n        store_importance_df[\"store\"] = filename[5:9]\n        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n    \ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:20].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (averaged over store predictions)')\n    plt.tight_layout()\n    \ndisplay_importances(feature_importance_df)","c5d9f658":"valid['sales'] = valid_preds\nvalidation = valid[['id','d','sales']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sales').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(conv_id).str.replace('evaluation','validation')\n\n  \n  #Get the evaluation results\ntest['sales'] = eval_preds\nevaluation = test[['id','d','sales']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sales').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(conv_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.to_csv('submission1.csv',index=False)","bc2925e9":"# Inspect the data","91ba989e":"# Memory usage control","affaf5fc":"<img src=\"https:\/\/33.media.tumblr.com\/f6d253eff4ed0701713e4f4df06bead4\/tumblr_nhpzm2MsRf1qzeylbo1_r1_500.gif\">","c2062fb1":"# Get the Data","e2ddb088":"# Make submission\n","03b841c1":"# Create Train,Validity and Test Dataframes","b596976c":"# Introduction","27424a16":"# Importing important libraries","4f24cc82":"**Prices for Hobbies 1 items by store of each state**","4219b124":"# Rolling Window Statistics","a608e596":"**Let's take a look in sales by month-year**","6f66a586":"![image.png](attachment:image.png)","cdb80501":"# Introduce Lags","e9a7a335":"Welcome to our first try","bad3ec2f":"We start by converting the train dataframe from wide to long format.","feaf25ec":"# Model Building - LGBM - Feature Engineering","cdb62130":"# EDA","78f1c1ea":"#  **Check for Null**","9b94dad0":"# Melt data to create single dataset","a0ab1479":"# Add Zero sales for dates d_1942 to d_1969","d8d99b36":"# Trends"}}