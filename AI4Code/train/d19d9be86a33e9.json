{"cell_type":{"7ac24504":"code","048760ad":"code","e58926bf":"code","842db8fd":"code","11879382":"code","3c66954f":"markdown","b44ae4e9":"markdown","e9eea9d2":"markdown","669b6ef0":"markdown","f308beea":"markdown"},"source":{"7ac24504":"!pip install torchtext","048760ad":"class DataLoader(object):\n    def __init__(self,\n                 max_vocab=9999,\n                 min_freq=1,\n                 init_token='<bos>',\n                 eos_token='<eos>'):\n        super().__init__()\n\n        self.max_vocab = max_vocab\n        self.min_freq = min_freq\n        self.label = data.Field(sequential=False,\n                                unk_token=None)\n        self.text = data.Field(init_token=init_token,\n                               eos_token=eos_token,\n                               batch_first=True,\n                               )\n\n    def get_loaders(self, config):\n        train, valid = data.TabularDataset(\n            path=config.file_path,\n            format='tsv',\n            fields=[\n                ('label', self.label),\n                ('text', self.text),\n            ],\n        ).split(split_ratio=config.train_ratio)\n\n        self.train_loader, self.valid_loader = data.BucketIterator.splits(\n            (train, valid),\n            batch_size=config.batch_size,\n            device= 'cuda:{}'.format(config.gpu_id) if config.gpu_id >= 0 else 'cpu',\n            shuffle=True,\n            sort_key = lambda x: len(x.text),\n            sort_within_batch = True\n        )\n\n        self.label.build_vocab(train)\n        self.text.build_vocab(train, max_size=self.max_vocab, min_freq=self.min_freq)\n\n        return self.train_loader, self.valid_loader","e58926bf":"class RNNclassifier(nn.Module):\n    def __init__(self, input_size, emb_dim, hidden_size, n_layers, n_classes, dropout):\n        self.input_size = input_size\n        self.emb_dim = emb_dim\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.n_classes = n_classes\n\n        super().__init__()\n\n        # x = (bs, length)\n        self.emb = nn.Embedding(input_size, emb_dim)\n        # x = (bs, length, dim)\n        self.rnn = nn.LSTM(\n            input_size=emb_dim,\n            hidden_size=hidden_size,\n            dropout=dropout,\n            num_layers=n_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        # x = (bs, length, hidden * 2)\n        self.generator = nn.Linear(in_features=hidden_size * 2,\n                                   out_features=n_classes)\n        self.activation = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        # x = (bs, length)\n        x1 = self.emb(x)\n        # x = (bs, length, dim)\n        x2, _ = self.rnn(x1)\n        # x = (bs, length, hidden * 2)\n        y = self.activation(self.generator(x2[:, -1]))\n\n        return y\n\nclass CNNclassifier(nn.Module):\n    def __init__(self,\n                 input_size,\n                 emb_dim,\n                 window_sizes,\n                 n_filters,\n                 use_padding,\n                 use_batchnorm,\n                 dropout,\n                 n_classes):\n        # emb\n        self.input_size = input_size\n        self.emb_dim = emb_dim\n        # cnn module\n        self.window_sizes = window_sizes\n        self.n_filters = n_filters\n        self.use_padding = use_padding\n        self.use_batchnorm = use_batchnorm\n        self.dropout = dropout\n        # generator\n        self.n_classes = n_classes\n        super().__init__()\n\n        # x = (bs, length)\n        self.emb = nn.Embedding(input_size, emb_dim)\n        # x = (bs, length, emb)\n        self.cnnmodule = nn.ModuleList()\n\n        for window_size in window_sizes:\n            self.cnnmodule.append(nn.Sequential(\n                    nn.Conv2d(in_channels=1,\n                              out_channels=n_filters,\n                              kernel_size=(window_size, emb_dim)),\n                              # padding=(1, 1) if use_padding else 0),\n                    nn.ReLU(),\n                    nn.BatchNorm2d(n_filters) if use_batchnorm else nn.Dropout(dropout),\n                )\n            )\n        # x = (bs, n_filters, length -  window_size + 1, 1)\n        self.generator = nn.Linear(in_features=n_filters * len(window_sizes),\n                                   out_features=n_classes)\n        self.activation = nn.LogSoftmax(dim=-1)\n\n\n    def forward(self, x):\n        # x = (bs, length)\n        x = self.emb(x)\n        # x = (bs, length, emb)\n        min_length = max(self.window_sizes)\n        if min_length > x.size(1):\n            pad = x.new(x.size(0), min_length - x.size(1), self.emb_dim).zero_()\n            x = torch.cat([x, pad], dim=1)\n\n        x = x.unsqueeze(dim=1)\n        # x = (bs, 1, length, emb)\n\n        cnn_outs = []\n        for block in self.cnnmodule:\n            cnn_out = block(x)\n            # x = (bs, n_filters, length - window_size + 1, 1)\n            cnn_out = nn.functional.max_pool1d(input=cnn_out.squeeze(dim=-1),\n                                               kernel_size=cnn_out.size(-2)).squeeze(dim=-1)\n            # x = (bs, n_filters)\n            cnn_outs += [cnn_out]\n\n        x = torch.cat(cnn_outs, dim=-1)\n        # x = (bs, sum(n_filters))\n        x = self.generator(x)\n        y = self.activation(x)\n\n        return y","842db8fd":"SILNET = 0\nBATCHWISE = 1\nEPOCHWISE = 2\n\n\nclass KerasLikeEngine(Engine) :\n    def __init__(self, func, model, optimizer, loss, config) :\n        self.model = model\n        self.optimizer = optimizer\n        self.loss = loss\n        self.config = config\n\n        super().__init__(func)\n\n        self.best_loss = np.inf\n        self.best_model = None\n\n        self.device = next(model.parameters()).device\n\n    @staticmethod\n    def train(engine, minibatch) :\n        engine.model.train()\n\n        x, y = minibatch.text, minibatch.label\n        x, y = x.to(engine.device), y.to(engine.device)\n\n        y_hat = engine.model(x)\n        loss_i = engine.loss(y_hat.squeeze(), y)\n\n        ### ---- train only---- ###\n        engine.optimizer.zero_grad()\n        loss_i.backward()\n        engine.optimizer.step()\n        ### ---- train only---- ###\n\n        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor) :\n            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() \/ float(y.size(0))\n        else :\n            accuracy = 0\n\n        return {\n            'loss' : float(loss_i),\n            'accuracy' : float(accuracy),\n        }\n\n    @staticmethod\n    def valid(engine, minibatch) :\n        engine.model.eval()\n\n        with torch.no_grad() :\n            x, y = minibatch.text, minibatch.label\n            x, y = x.to(engine.device), y.to(engine.device)\n\n            y_hat = engine.model(x)\n            loss_i = engine.loss(y_hat.squeeze(), y)\n\n            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor) :\n                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() \/ float(y.size(0))\n            else :\n                accuracy = 0\n\n            return {\n                'loss' : float(loss_i),\n                'accuracy' : float(accuracy),\n            }\n\n    @staticmethod\n    def attach(train_engine, val_engine, verbose=EPOCHWISE) :\n        def attach_running_average(engine, metric_name) :\n            RunningAverage(output_transform=lambda x : x[metric_name]).attach(\n                engine,\n                metric_name,\n            )\n\n        training_metric_names = ['loss', 'accuracy']\n\n        for metric_name in training_metric_names :\n            attach_running_average(train_engine, metric_name)\n\n        if verbose == BATCHWISE :\n            pbar = ProgressBar(bar_format=None, ncols=120)\n            pbar.attach(train_engine, ['loss', 'accuracy'])\n\n        if verbose == EPOCHWISE :\n            pbar = ProgressBar(bar_format=None, ncols=120)\n            pbar.attach(train_engine, ['loss', 'accuracy'])\n\n            @train_engine.on(Events.EPOCH_COMPLETED)\n            def print_logs(engine) :\n                print('Epoch {} Train - Accuracy: {:.4f} Loss: {:.4f}'.format(\n                    engine.state.epoch,\n                    engine.state.metrics['accuracy'],\n                    engine.state.metrics['loss'],\n                ))\n\n        validation_metric_names = ['loss', 'accuracy']\n\n        for metric_name in validation_metric_names :\n            attach_running_average(val_engine, metric_name)\n\n        if verbose == BATCHWISE :\n            pbar = ProgressBar(bar_format=None, ncols=120)\n            pbar.attach(val_engine, ['loss', 'accuracy'])\n\n        if verbose == EPOCHWISE :\n            pbar = ProgressBar(bar_format=None, ncols=120)\n            pbar.attach(val_engine, ['loss', 'accuracy'])\n\n            # @val_engine.on(Events.EPOCH_COMPLETED)\n            # def print_logs(engine) :\n            #     print('        Valid - Accuracy: {:.4f} Loss: {:.4f} Lowest_loss={:.4f}'.format(\n            #         engine.state.metrics['accuracy'],\n            #         engine.state.metrics['loss'],\n            #         engine.best_loss,\n            #     ))\n\n    @staticmethod\n    def check_best(engine) :\n        loss = float(engine.state.metrics['loss'])\n        if loss <= engine.best_loss :\n            engine.best_loss = loss\n            engine.best_model = deepcopy(engine.model.state_dict())\n\n    @staticmethod\n    def save_model(engine, config, **kwargs) :\n        torch.save({\n            'model' : engine.best_model,\n            'config' : config,\n            **kwargs\n        }, config.model_fn)\n\n    @staticmethod\n    def print_logs(engine) :\n        print('        Valid - Accuracy: {:.4f} Loss: {:.4f} Lowest_loss={:.4f}'.format(\n            engine.state.metrics['accuracy'],\n            engine.state.metrics['loss'],\n            engine.best_loss,\n        ))\n\n\nclass Trainer :\n    def __init__(self, config) :\n        self.config = config\n\n    def train(self, model, optimizer, loss, train_loader, val_loader) :\n        # Make Engine object\n        train_engine = KerasLikeEngine(KerasLikeEngine.train, model, optimizer, loss, self.config)\n        val_engine = KerasLikeEngine(KerasLikeEngine.valid, model, optimizer, loss, self.config)\n\n        # Attach Metrics\n        KerasLikeEngine.attach(train_engine, val_engine, verbose=self.config.verbose)\n\n        # Event Handling\n        def run_validation(engine, validation_engine, valid_loader) :\n            validation_engine.run(valid_loader, max_epochs=1)\n\n        train_engine.add_event_handler(Events.EPOCH_COMPLETED,\n                                       run_validation, val_engine, val_loader)\n        val_engine.add_event_handler(Events.EPOCH_COMPLETED,\n                                     KerasLikeEngine.check_best)\n        val_engine.add_event_handler(Events.EPOCH_COMPLETED,\n                                     KerasLikeEngine.save_model, train_engine, self.config)\n        val_engine.add_event_handler(Events.EPOCH_COMPLETED,\n                                     KerasLikeEngine.print_logs, val_engine)\n\n\n        # running\n        train_engine.run(train_loader, max_epochs=self.config.n_epochs)\n\n        model.load_state_dict(val_engine.best_model)\n\n        return model","11879382":"import argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom model import RNNclassifier, CNNclassifier\nfrom trainer import Trainer\nfrom data_loader import DataLoader\n\n\ndef define_argparse():\n\n    p.add_argument('--model_fn', required=True)\n    p.add_argument('--model_name', type=str, required=True)\n    p.add_argument('--file_path', type=str, required=True)\n\n    p.add_argument('--gpu_id', type=int, default= 0 if torch.cuda.is_available() else -1)\n    p.add_argument('--batch_size', type=int, default=64)\n    p.add_argument('--n_epochs', type=int, default=5)\n    p.add_argument('--train_ratio', type=float, default=.8)\n    p.add_argument('--verbose', type=int, default=2)\n\n    p.add_argument('--emb_dim', type=int, default=32)\n    p.add_argument('--dropout', type=float, default=.2)\n    # rnn\n    p.add_argument('--hidden_size', type=int, default=32)\n    p.add_argument('--n_layers', type=int, default=3)\n    # cnn\n    p.add_argument('--window_sizes', type=list, default=[2, 3, 4])\n    p.add_argument('--n_filters', type=int, default=32)\n    p.add_argument('--use_padding', type=bool, default=True)\n    p.add_argument('--use_batchnorm', type=bool, default=True)\n\n    config = p.parse_args()\n\n    return config\n\n\ndef main(config) :\n    dataloader = DataLoader()\n    train_loader, valid_loader = dataloader.get_loaders(config)\n\n    print('Train size : {} Valid size : {}'.format(\n        len(train_loader.dataset),\n        len(valid_loader.dataset),\n    ))\n\n    input_size = len(dataloader.text.vocab)\n    n_classes = len(dataloader.label.vocab)\n\n    if config.model_name == 'rnn':\n        model = RNNclassifier(input_size, config.emb_dim, config.hidden_size, config.n_layers, n_classes, config.dropout)\n    elif config.model_name == 'cnn':\n        model = CNNclassifier(input_size, config.emb_dim, config.window_sizes, config.n_filters, config.use_padding, config.use_batchnorm,\n                              config.dropout, n_classes)\n    loss = nn.NLLLoss() # expect 1d tensor\n    optimizer = optim.Adam(model.parameters())\n\n    trainer = Trainer(config)\n    trainer.train(model, optimizer, loss, train_loader, valid_loader)\n\nif __name__ == '__main__':\n    config = define_argparse()\n    main(config)","3c66954f":"## 2. Load Dataset","b44ae4e9":"## 3. Modeling","e9eea9d2":"## 1. Requirements","669b6ef0":"## 5. Run on CLI","f308beea":"## 4. Train loop"}}