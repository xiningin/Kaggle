{"cell_type":{"673e1b83":"code","2c839714":"code","b729f628":"code","5bfedf6e":"code","54072543":"code","b8e65293":"code","01aac026":"code","c2f59bea":"code","de167f09":"code","5e6390a5":"code","6a38526c":"code","a6ef00da":"code","de733f65":"code","0d071d4a":"code","b6b7332a":"code","1f459755":"code","54011703":"code","f8a09525":"code","cd491513":"code","25cc0643":"code","caca4538":"code","3f83eed2":"code","bcc3549c":"code","ee8a665e":"code","7ea259e6":"code","5da839d7":"code","3248645a":"code","53ba7a9f":"code","118218bc":"markdown","ca48b396":"markdown","18afbf31":"markdown","a5dc6063":"markdown","85c05248":"markdown","08291608":"markdown","aa0fc4e7":"markdown","ffd8c0bd":"markdown","f2f3eb31":"markdown","42161a98":"markdown","16d7a73e":"markdown","954000f8":"markdown","aaaadc7a":"markdown","fb927252":"markdown","3c69757d":"markdown","48cf2a23":"markdown","18ae197c":"markdown","3df65ebc":"markdown","4aee6595":"markdown","3cd7cf6f":"markdown","423bc751":"markdown","c1c333ce":"markdown"},"source":{"673e1b83":"# Import modules\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt","2c839714":"nRowsRead = 10\n\ndf = pd.read_csv(\"..\/input\/final_data.csv\", delimiter=',', nrows = nRowsRead)\ndf = df.sample(frac=1)\ndf.head(3)","b729f628":"df.columns","5bfedf6e":"# Loading the data\ndf = pd.read_csv(\"..\/input\/final_data.csv\", delimiter=',', encoding = \"ISO-8859-1\", nrows = None)\n# Shuffling the data\ndf = df.sample(frac=1)\n# Selecting the interesting columns\ndf = df[[\"reviews.doRecommend\", \"reviews.text\", \"reviews.title\"]]","54072543":"df.head()","b8e65293":"df.info()","01aac026":"df = df.dropna()\ndf.info()","c2f59bea":"df[\"reviews.doRecommend\"].value_counts()","de167f09":"df[\"reviews.doRecommend\"].astype(float).hist(figsize=(8,5))","5e6390a5":"df.rename(columns={'reviews.title':'title','reviews.doRecommend':'doRecommend'}, inplace=True)","6a38526c":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\nfor train_index, test_index in split.split(df, df[\"doRecommend\"]):\n    train_set = df.iloc[train_index]\n    test_set = df.iloc[test_index]","a6ef00da":"print(train_set[\"doRecommend\"].value_counts()\/len(train_set))\nprint(test_set[\"doRecommend\"].value_counts()\/len(test_set))","de733f65":"train_set = train_set.drop(columns = [\"reviews.text\"])\ntest_set = test_set.drop(columns = [\"reviews.text\"])\n\ntrain_set[\"title\"] = train_set[\"title\"].astype(str)\ntest_set[\"title\"] = test_set[\"title\"].astype(str)\n\ntrain_set[\"doRecommend\"] = train_set[\"doRecommend\"].astype(float)\ntest_set[\"doRecommend\"] = test_set[\"doRecommend\"].astype(float)","0d071d4a":"train_set.head()","b6b7332a":"maxlen = 10 # Maximal length of sequence considered\nnum_words = 600 # Number of words in your vocabulary\nfilters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789' # Some chars you want to remove in order to have a clean text\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=num_words, filters=filters) # Word tokenizer\ntokenizer.fit_on_texts(train_set['title'].tolist()) # Fit on training samples\n\n\ntrain_titles_tok = tokenizer.texts_to_sequences(train_set['title'].tolist())\ntest_titles_tok = tokenizer.texts_to_sequences(test_set['title'].tolist())\n\n\nfrom keras.preprocessing.sequence import pad_sequences\ntrain_titles_pad = pad_sequences(train_titles_tok, maxlen=maxlen)\ntest_titles_pad = pad_sequences(test_titles_tok, maxlen=maxlen)","1f459755":"print(train_set.iloc[0][\"title\"],train_titles_pad[0])\nprint(train_set.iloc[1][\"title\"],train_titles_pad[1])\nprint(train_set.iloc[2][\"title\"],train_titles_pad[2])\nprint(train_set.iloc[3][\"title\"],train_titles_pad[3])","54011703":"from sklearn.preprocessing import LabelBinarizer\nlabel_enc = LabelBinarizer().fit(list(set(train_set['doRecommend'].values))) \ntrain_labels = label_enc.transform(train_set['doRecommend'].values)\ntest_labels = label_enc.transform(test_set['doRecommend'].values)","f8a09525":"from keras.optimizers import Adam, RMSprop, SGD, Adagrad\nfrom keras.callbacks import EarlyStopping\n\nclass LearningModel():\n    def __init__(self, dim=20):\n        self.dim= dim # Dimension of word embeddings\n        self.n_label = 2 # Number of labels\n        self.optimizer = Adam(lr=0.01) # Optimizer method for stochastic gradient descent\n        self.epochs = 20\n        self.batch_size = 128\n        self.callbacks = EarlyStopping(monitor='val_loss', patience=2)\n        self.model = None # Keras model, it will be instantiated later\n        \n    def compile(self):\n        print(self.model.summary())\n        self.model.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n        \n    def train(self, mode=''):\n        self.compile()\n        if mode == 'EarlyStopping':\n            self.model.fit(train_titles_pad, train_labels,\n                           epochs=self.epochs, \n                           batch_size=self.batch_size, \n                           validation_data=(test_titles_pad, test_labels), \n                           callbacks=[self.callbacks], verbose=2)\n        else: \n            for _ in range(self.epochs):\n                self.model.fit(train_titles_pad, train_labels,\n                               epochs=1, \n                               batch_size=self.batch_size, \n                               validation_split=0.1, verbose=2)\n                self.test()\n    \n    def test(self):\n        print('Evaluation : ', self.model.evaluate(test_titles_pad, test_labels, batch_size=2048))","cd491513":"from keras.models import Sequential\nfrom keras.layers import Embedding, GlobalAveragePooling1D, Dense, BatchNormalization\n\nlm = LearningModel(dim=20)\n\nlm.model = Sequential()\n\nlm.model.add(Embedding(num_words, lm.dim, input_length=maxlen))\nlm.model.add(GlobalAveragePooling1D())\nlm.model.add(Dense(1, activation='sigmoid'))\n\nlm.train()","25cc0643":"print(train_set[\"doRecommend\"].value_counts()\/len(train_set))\nprint(test_set[\"doRecommend\"].value_counts()\/len(test_set))","caca4538":"df = df.reset_index()\ndf.head()","3f83eed2":"df1 = df[(df[\"doRecommend\"] == False) | (df[\"index\"]<3000)]\ndf1.info()","bcc3549c":"print(df1[\"doRecommend\"].value_counts()\/len(df1))\ndf1[\"doRecommend\"].astype(float).hist(figsize=(8,5))","ee8a665e":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\nfor train_index, test_index in split.split(df1, df1[\"doRecommend\"]):\n    train_set1 = df1.iloc[train_index]\n    test_set1 = df1.iloc[test_index]","7ea259e6":"train_set1[\"title\"] = train_set1[\"title\"].astype(str)\ntest_set1[\"title\"] = test_set1[\"title\"].astype(str)\n\ntrain_set1[\"doRecommend\"] = train_set1[\"doRecommend\"].astype(float)\ntest_set1[\"doRecommend\"] = test_set1[\"doRecommend\"].astype(float)","5da839d7":"tokenizer = Tokenizer(num_words=num_words, filters=filters) # Word tokenizer\ntokenizer.fit_on_texts(train_set1['title'].tolist()) # Fit on training samples\n\n\ntrain_titles_tok = tokenizer.texts_to_sequences(train_set1['title'].tolist())\ntest_titles_tok = tokenizer.texts_to_sequences(test_set1['title'].tolist())\n\n\nfrom keras.preprocessing.sequence import pad_sequences\ntrain_titles_pad = pad_sequences(train_titles_tok, maxlen=maxlen)\ntest_titles_pad = pad_sequences(test_titles_tok, maxlen=maxlen)","3248645a":"label_enc = LabelBinarizer().fit(list(set(train_set1['doRecommend'].values))) \ntrain_labels = label_enc.transform(train_set1['doRecommend'].values)\ntest_labels = label_enc.transform(test_set1['doRecommend'].values)","53ba7a9f":"lm = LearningModel(dim=20)\n\nlm.model = Sequential()\n\nlm.model.add(Embedding(num_words, lm.dim, input_length=maxlen))\nlm.model.add(GlobalAveragePooling1D())\nlm.model.add(Dense(1, activation='sigmoid'))\n\nlm.train()","118218bc":"### Distribution of the values","ca48b396":"This dataset contains many useless columns for our objective, we will remove them.","18afbf31":"### Loading the dataset","a5dc6063":"## Looking at a sample of the data","85c05248":"### Removing missing values","08291608":"## Creating a model","aa0fc4e7":"## Training the model","ffd8c0bd":"Many rows do not have a value for the recommendation, we will drop them as they are useless for the training.\n\nWe can also drop the reviews without titles, it is a very small portion of the dataset.","f2f3eb31":"We can see the correlation between the text and the associated arrays : every word corresponds to an integer, we only consider the ten first words of the title.","42161a98":"Our dataframe only contains useful information, we can now separate the data into a training and a testing dataset.\n\nHowever, we need to make sure that the training set is representative of the complete dataset.","16d7a73e":"### Processing the text","954000f8":"The proportions are respected.\n\nLet's only consider the titles for now.","aaaadc7a":"This model will take into input a vector indicating the words present in the title and create an embedding vector out of it. This vector will help determine if the review is positive or negative.","fb927252":"### Creating training and testing sets","3c69757d":"From now on, the dataset contains 64% of \"True\", a model with a better precision on the validation set actually learns from the training set.","48cf2a23":"Very few reviews do not recommend the product, we must be careful not to create a sample bias.\n\nWe will use stratified sampling to ensure a representative proportion of \"False\" in the training dataset.","18ae197c":"We will analyse the text in the reviews to assess if the user recommends the tablet. The main issue of this dataset is the proportion of positive reviews, it creates an imbalance leading to a biased model.","3df65ebc":"## Preprocessing the full dataset","4aee6595":"# Do the user recommend the product ?","3cd7cf6f":"**Our final accuracy is around 91%, this value is not as good as the previous one but is more representative of the real capabilities of the model.**\n\nWe could improve the accuracy by looking at \"n-grams\" of words, for example by considering each word with the following one.\n\nIt would allow the network to detect the difference between \"very good\" and \"not good\"","423bc751":"## Let's try the same method with a balanced dataset","c1c333ce":"**We achieve an accuracy of 96%, which seems good but given the unbalance in the data, a classifier giving always the output 'True' would have an accuracy close to that.**"}}