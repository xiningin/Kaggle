{"cell_type":{"ab56b1a9":"code","466e07cc":"code","2170f533":"code","9eb804fd":"code","9ff95525":"code","0705fe58":"code","fbbe3ba4":"code","07fcd375":"code","0bdb8f8c":"code","388e92e2":"code","944d4aae":"code","5d12f745":"code","b03a4e8c":"code","e0dd5b4d":"code","e380573b":"code","617ffbd2":"code","4ba1c4d2":"code","f5be4df2":"code","adc4e0a0":"markdown","e1033adf":"markdown","3a42fa25":"markdown","300a259b":"markdown","17adf27c":"markdown","882f17fa":"markdown","011a79f3":"markdown","8a281a4e":"markdown","9267033c":"markdown","ad75b830":"markdown","286bc670":"markdown","69bbcde6":"markdown","edd2d06b":"markdown","a923c21a":"markdown","a4a25be6":"markdown","608917cf":"markdown","388df5d8":"markdown","75662f64":"markdown","a3830009":"markdown","0b0be29f":"markdown"},"source":{"ab56b1a9":"import numpy as np \nimport pandas as pd\nfrom scipy import stats\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.rc('font', family='serif')\n\n# Preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer, StandardScaler\n\n# Modelling\nfrom shutil import rmtree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom tempfile import mkdtemp\nfrom xgboost import XGBClassifier","466e07cc":"# Load the data\ndata = pd.read_csv(\n    '\/kaggle\/input\/credit-card-customers\/BankChurners.csv',\n    index_col='CLIENTNUM',\n    na_values=['Unknown']  # interpret \"Unknown\" as missing \n)\ndata.head()","2170f533":"# Drop the last 2 columns as advised in the dataset's description\n# See https:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers\ndata = data.iloc[:, :-2]\ndata.info()","9eb804fd":"# Check for missing values\nmissing = data.isna().sum()\npd.DataFrame({\n    'No. of missing values': missing,\n    '% missing': missing.apply(lambda x: f'{x\/len(data):.2%}')\n}).style.bar()","9ff95525":"numeric_cols = data.select_dtypes(include=\"number\")\nnumeric_cols.describe()","0705fe58":"# Plot histograms of the numeric columns\nhistograms = numeric_cols.hist(figsize=(14, 12))","fbbe3ba4":"total_rev_bal = data['Total_Revolving_Bal'].value_counts()\nprint(\n    f'A very large number of customers ({total_rev_bal[0]:,}) have '\n    '\"Total_Revolving_Bal\" == 0.',\n    total_rev_bal.nlargest(5),  # top 5 frequencies\n    '\\nFrequencies of the first 5 values confirm that the peak is specifically at 0:',\n    total_rev_bal.sort_index().head(),\n    sep=\"\\n\"\n)","07fcd375":"_ = numeric_cols.plot(kind=\"box\", subplots=True, layout=(5, 3), figsize=(12, 20))","0bdb8f8c":"# Plot probability plots (qq-plots)\nfig, axes = plt.subplots(nrows=5, ncols=3, figsize=(12, 18))\n\nfig.tight_layout(h_pad=5)  # Add padding to sub-plots\n\nfor col, ax in zip(numeric_cols.columns, axes.flatten()):\n    stats.probplot(numeric_cols[col], dist='norm', plot=ax)\n    ax.set_title(col)\n\nfig.delaxes(axes[-1, -1])  # Remove 15th (last) axes which has no content","388e92e2":"categorical_cols = data.select_dtypes(include='O')\ncategorical_cols.describe()","944d4aae":"# Plot countplots of categorical columns\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 14))\n\nfor col, ax in zip(categorical_cols.drop('Attrition_Flag', 1), axes.flatten()):\n    sns.countplot(data=categorical_cols, x=col, hue='Attrition_Flag', ax=ax)\n    ax.tick_params(axis='x', rotation=45)\n    ax.set_title(col, size=14)\n    \nfig.delaxes(axes[-1, -1])  # Drop 6th (last) axes as it has no content\nplt.tight_layout()","5d12f745":"_ = sns.countplot(x=categorical_cols['Attrition_Flag'])","b03a4e8c":"# Select the features and target\nX = data.drop('Attrition_Flag', axis=1)\ny = data['Attrition_Flag']\n\n# Prepare a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n\n# Preprocessing pipeline for numeric cols\nnumeric_pipe = Pipeline([\n    ('normalize', QuantileTransformer(output_distribution='normal')),\n    ('rescale', StandardScaler())\n])\n\n# Preprocessing pipeline for categorical cols\ncategorical_pipe = Pipeline([\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('encode', OneHotEncoder())\n])\n\n# Combined preprocessing pipeline\nextract_features = ColumnTransformer([\n    ('numeric', numeric_pipe, numeric_cols.columns),\n    ('categorical', categorical_pipe, categorical_cols.columns.drop('Attrition_Flag'))\n])\n\n# Apply random oversampling to training data to counteract target class imbalance\nrandom_oversampler = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = random_oversampler.fit_resample(X_train, y_train)\n\n\ndef fit_and_evaluate(classifier, params=None):\n    \"\"\"Fit a classification model and print metrics.\n    \n    Parameters\n    ----------\n    classifier: sklearn predictor instance\n        The model to fit.\n    params: dict\n        A dictionary of hyper-parameter values to pass to GridSearchCV \n        for model tuning.\n    \n    Returns\n    -------\n    sklearn predictor\n        The model with hyper-parameters yielding the highest cross-validated score.\n    \"\"\"\n    cache_dir = mkdtemp()\n    pipe = Pipeline(\n        [('features', extract_features), ('classifier', classifier)],\n        memory=cache_dir\n    )\n    \n    params = params or {} # use empty dict if params not provided\n    \n    model = GridSearchCV(estimator=pipe, param_grid=params, scoring='roc_auc',\n                         n_jobs=2, cv=4, verbose=1)\n    model.fit(X_resampled, y_resampled)\n\n    rmtree(cache_dir)\n    print(\n        f'\\nCross Validation AUC: {model.best_score_:.4f}',\n        '\\nClassification Report (on test data):\\n' + '-'*58,\n        classification_report(model.predict(X_test), y_test),\n        sep='\\n'\n    )\n    \n    return model.best_estimator_","e0dd5b4d":"clf = RandomForestClassifier(random_state=7)\nparams = {'classifier__max_depth': range(3, 8),\n          'classifier__n_estimators': [100, 200],\n          'classifier__class_weight': [\"balanced\", \"balanced_subsample\"]}\n\nrf_model = fit_and_evaluate(clf, params)","e380573b":"clf = XGBClassifier(learning_rate=0.02, random_state=0, subsample=0.8)\nparams = {'classifier__max_depth': range(3, 7),\n          'classifier__reg_lambda': np.logspace(0, 3, 4),\n          'classifier__n_estimators': [100, 250]}\n\ngb_model = fit_and_evaluate(clf, params)","617ffbd2":"clf = SVC(class_weight='balanced', random_state=0)\nparams = {'classifier__C': np.logspace(0, 4, 10)}\n\nsvc_model = fit_and_evaluate(clf, params)","4ba1c4d2":"clf = LogisticRegression(random_state=2, class_weight='balanced')\nparams = {'classifier__C': np.logspace(0, 4, 10)}\n\nlr_model = fit_and_evaluate(clf, params)","f5be4df2":"sample = X.sample(25, random_state=5)\n\nresults = pd.DataFrame({\n    'Random Forest Classifier': rf_model.predict(sample),\n    'Gradient Boosting Classifier': gb_model.predict(sample),\n    'Support Vector Classifier': svc_model.predict(sample),\n    'Logistic Regression': lr_model.predict(sample)\n})\n\n\ndef color_code(cell):\n    \"\"\"Set a DataFrame cell's background color according to its value.\"\"\"\n    if cell == 'Existing Customer':\n        color = 'aqua'\n    elif cell == 'Attrited Customer':\n        color = 'orangered'\n    return f'background-color: {color}'\n\nresults.style.applymap(color_code)","adc4e0a0":"## 3.3 Support Vector Classifier","e1033adf":"### 2.3.1 Count-plots","3a42fa25":"### 2.2.2 Box-plots","300a259b":"# 4. Conclusion\n\nAmong the models tested above, the `XGBClassifier` seems the most promising.\n\nLet's visualise sample predictions to check if the predictions from the various models are consistent:","17adf27c":"> There exist strategies for handling such situations, some of which can be implemented using the [imbalanced-learn][1] package.\n\n> In this case, we'll use [Randomized over-sampling][2].\n\n[1]: https:\/\/imbalanced-learn.org\/stable\/user_guide.html\n[2]: https:\/\/imbalanced-learn.org\/stable\/over_sampling.html#naive-random-over-sampling","882f17fa":"# 3. Modelling & Prediction\n\nLet's now attempt to fit several classification models to predict whether a customer will leave.\n\nThe target variable - `Attrition_Flag` - is heavily imbalanced, with one class having significantly higher occurences than the rest.","011a79f3":"- `Customer_Age`, `Dependent_count`, `Months_on_book` and `Total_Trans_Ct` are somewhat normally distributed, which is good.","8a281a4e":"## 2.2 Numeric Features\n\nThere are 14 numeric features.","9267033c":"### 2.2.3 Normal Probability Plots","ad75b830":">`Credit_Limit`, `Avg_Open_To_Buy`, `Total_Amt_Chng_Q4_Q1`, `Total_Trans_Amt`, `Total_Ct_Chng_Q4_Q1` and `Avg_Utilization_Ratio` are skewed to the right (positively skewed).\n\n>`Total_Revolving_Bal` has a curious peak close to the origin, which is investigated below:","286bc670":"## 3.4 Logistic Regression","69bbcde6":"## 3.1 Random Forest Classifier","edd2d06b":"## 2.1 Missing Values\n\n`Education_Level`, `Marital_Status` and `Income_Category` have missing values. ","a923c21a":"> `Credit_Limit`, `Avg_Open_To_Buy`, `Total_Amt_Chng_Q4_Q1`, `Total_Trans_Amt` and `Total_Ct_Chng_Q4_Q1` have a very large number of outliers.\n>\n> The `QuantileTransformer` can be used to normalize them.","a4a25be6":"# 1. Introduction\n\n## 1.1 Premise\n\nA manager at a bank is concerned that more and more customers are leaving the bank's credit card services. \n\nThe bank would really appreciate it if someone could help it predict who is going to churn, so that it can proactively approach such customers to offer better services, and turn them back.\n\n## 1.2 Plan\n\n- Perform **exploratory data analysis** to learn the *properties\/characteristics* of the features present.\n- Fit several **classification models** to predict whether a customer will churn or not.\n- Apply *hyper-parameter optimization* techniques.\n- **Evaluate performance** of fitted models.","608917cf":"### 2.2.1 Histograms","388df5d8":"### Strategy for Handling Missing Values\n\nA common and straight-forward way of dealing with missing values is to *drop affected rows or columns*. The advantage of this is that you'll be left with genuine, unaltered data. The disadvantage is that you lose some data; which is especially undesirable if the dataset is small, or large proportions of its values are missing.\n\nAnother common tactic is *imputation*, which involves determining values to fill in the blanks. The advantage here is that no data is thrown out. But then, depending on the method used, the imputed values might be misleading.\n\nRemoving rows with missing values would in this case leave only 7,081 rows for modelling. That is rather small, so we'll use imputation to get as much of the data as possible. This will be implemented as a component in the model fitting pipeline.","75662f64":"## 2.3 Categorical Features\n\nThere are 5 categorical features. The target variable `Attrition_Flag` is also categorical.","a3830009":"# 2. Exploratory Data Analysis\n\nThe dataset consists of 10,127 rows and 20 columns.","0b0be29f":"## 3.2 Gradient Boosting Classifier"}}