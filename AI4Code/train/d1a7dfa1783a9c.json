{"cell_type":{"ec871bb9":"code","b545171e":"code","f3cf1a5b":"code","37dce2c4":"code","04b488cc":"code","5cbd174b":"code","149dbb8b":"code","b5f4dfce":"code","df12b23f":"code","d54c4e1b":"code","c917c1a7":"code","96d94c16":"code","9223d91e":"code","472d08a1":"code","6c2fc579":"code","1c665208":"code","d13c0ed9":"code","b0666e1d":"code","a1081467":"code","a22033a3":"code","f62b2e57":"code","54832d5f":"code","b822ac52":"code","1f71b6f4":"code","545760e0":"code","b0e83275":"markdown","33c8072e":"markdown","c0e5b1ad":"markdown","196506ea":"markdown","1b271f97":"markdown","ac914378":"markdown","7533b7ec":"markdown","0c9878f0":"markdown","0b10a87c":"markdown","08bec410":"markdown","f053eea8":"markdown","004d582d":"markdown"},"source":{"ec871bb9":"import pandas as pd\nimport numpy as np","b545171e":"import matplotlib.pyplot as plt\nimport seaborn as sns","f3cf1a5b":"import plotly.graph_objects as go\nimport plotly.express as ex","37dce2c4":"sns.set_theme(\"paper\")\nsns.set_style(\"white\")","04b488cc":"pd.set_option( \"display.max_rows\",120)\npd.set_option( \"display.max_columns\", 120)","5cbd174b":"train_file = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntrain_master_df = pd.read_csv( train_file ,sep =\",\")\ntrain_master_df.head()","149dbb8b":"print ( \" Data frame shape = {}\".format(train_master_df.shape ))","b5f4dfce":"train_master_df.describe().T","df12b23f":"fig = plt.figure( figsize =(8,8), dpi = 90 )\nsns.countplot( train_master_df[\"claim\"])\nplt.show()","d54c4e1b":"train_master_df.isnull().sum(axis = 0)","c917c1a7":"print( \"Max percentage of missing values in dataframe ={}\".format( train_master_df.isnull().sum().max()\/ train_master_df.shape[0]))","96d94c16":"print( \"by droping all missing values remaining % data ={} %\".format(100* train_master_df.dropna(axis =0 ).shape[0]\/train_master_df.shape[0] ) )","9223d91e":"for each_column in train_master_df.columns:\n    if each_column not in [\"claim\",\"id\"] : train_master_df[each_column].fillna( train_master_df[each_column].median(), inplace = True )\ntrain_master_df.isnull().sum()","472d08a1":"'''\n# hacountplottal 118 columns, For ploting we can have 10 column and 12 rows\nfig = plt.figure( figsize = ( 10, 10 ), dpi = 90  )\ncounter =1\nfor each_column in train_master_df.columns:\n    if each_column not in [\"claim\", \"id\", \"f11\"]:\n        ax = plt.subplot( 12,10, counter )\n        sns.scatterplot( x = train_master_df[\"f11\"], y = train_master_df[each_column], ax = ax ,hue = train_master_df[\"claim\"] )\n        counter +=1\nplt.tight_layout()\nplt.show()\n'''","6c2fc579":"feature_columns = train_master_df.columns.to_list()\nfeature_columns.remove(\"id\")\nfeature_columns.remove(\"claim\")","1c665208":"corr_matrix = train_master_df[feature_columns].corr()\ncorr_matrix.round( 2)","d13c0ed9":"fig = plt.figure( figsize = (25,25), dpi = 90 )\nsns.heatmap (corr_matrix.round(2),annot = True,annot_kws={\"size\":  5 } )\nplt.show()","b0666e1d":"if False :\n\n    fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\n    counter =1\n    for each_column in train_master_df.columns:\n        if each_column not in [\"claim\", \"id\"]:\n            plt.subplot( 24,5, counter )\n            sns.histplot( x= train_master_df[each_column],hue = train_master_df[\"claim\"] )\n            counter +=1\n    plt.tight_layout()\n    plt.show()","a1081467":"feature_columns = train_master_df.columns.to_list()\nfeature_columns.remove(\"id\")\nfeature_columns.remove(\"claim\")","a22033a3":"def df_plot( df ):\n    fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\n    counter =1\n    for each_column in train_master_df.columns:\n        if each_column not in [\"claim\", \"id\"]:\n            plt.subplot( 24,5, counter )\n            sns.histplot( x= df[each_column],hue = df[\"claim\"] )\n            counter +=1\n    plt.tight_layout()\n    plt.show()","f62b2e57":"from sklearn.neighbors import LocalOutlierFactor","54832d5f":"outlier_detector = LocalOutlierFactor( n_neighbors = 1, algorithm = \"auto\",metric =\"l1\" ,n_jobs = -1)\ndata = outlier_detector.fit_predict( train_master_df[ feature_columns[0:-1] ] )","b822ac52":"len( data[data ==-1 ])","1f71b6f4":"train_master_df_2 = train_master_df[data ==1 ]\ntrain_master_df_2.shape","545760e0":"fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\ncounter =1\nfor each_column in train_master_df_2.columns:\n    if each_column not in [\"claim\", \"id\"]:\n        plt.subplot( 24,5, counter )\n        sns.boxplot( y= train_master_df_2[each_column],x = train_master_df_2[\"claim\"] )\n        counter +=1\n\nplt.tight_layout()\nplt.show()\n#sns.boxplot( y = train_master_df_2[feature_columns[0]], x = train_master_df_2[\"claim\"])\n#sns.swarmplot( y = train_master_df_2[feature_columns[0]], x = train_master_df_2[\"claim\"], color = \"black\")","b0e83275":"* ** 37.52% data after removing all missing values, is not a good one we should do simthing better**\n* ** Let us do missing values imputation, I would select adding median value of respective column to mising places **\n* ** With this we can retain 100% of data **\n* ** There may be a question why median, why not mean or min or max, median will take middle value of distribution, were as mean will have high influence by outliers **\n","33c8072e":"# Visualize claim distribution with respect to other features\n# Using pair plot will create 117*117 plots which not suitable for visualization, might very hard to view as well\n# Instead of that we can use one feature and see how it is trending with respect to other. toget feel of data distribution of claim and non claim","c0e5b1ad":"## Above mising percentage might be differenet, taken only max number\n## Below Code to check missing values, we can make decission based on final percentage of data ","196506ea":"* We have 117 features, that is plenty. We can reduce features wich are highly correlating and thos features which doesn't show different grouping\n* To remove features we can look at Correlation matrix or use RFE ( Recursive Feature Elimination ) to remove features. This we can set as baseline\n* Other way is to do PCA, by doing that we can remove all correlating features & dimensanality reduction as well.","1b271f97":"# Check number of claim and non claim count, to see data is imbalanced or not ","ac914378":"# From plot we can clearly see, have good number of outliers\n# We should remove those outliersor clamp those values with certain threshold\n# First methos we do is removing outliers, and see how well the model behaves then followed by we can use clamping method","7533b7ec":"# Good news is, we don't have multi colinearity problem. we can use all features for model \n# Next step is to check is the data points trend, i.e min to max varaition, this give clues for us type of data scalar to use ","0c9878f0":"# With this, now we have 0 missing values, we can use 10% of data","0b10a87c":"# Check data  each column value distribution ","08bec410":"# Check for any missing values in dataframe ","f053eea8":"# Working is still ongoing","004d582d":"* Have 118 features, and none of feaure are constant,\n* Values are linearly increasing"}}