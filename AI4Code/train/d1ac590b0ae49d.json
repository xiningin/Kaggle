{"cell_type":{"9039652a":"code","038a98ca":"code","7ff179b3":"code","87b64559":"code","2458063e":"code","a1d349fa":"code","a4d252ce":"code","87a90549":"code","313b742d":"code","26813836":"code","8574a2ee":"code","01974163":"code","e9cb3cf8":"code","ec251ea2":"code","fb0b1980":"code","d23078d0":"code","62ad9b69":"code","7d0b3cfe":"code","4849a25a":"code","ac275ae1":"code","ba4d79aa":"code","3323c6f4":"code","31718101":"code","f5b354a7":"code","809d205d":"code","b3bdcbfa":"code","138a7d09":"code","0bd6c74e":"code","b7803f03":"code","6e27e3a1":"code","99ae1cb6":"code","4d420c37":"code","85b2a5e1":"code","45d22780":"code","fa8504e9":"code","733b9a5b":"code","96fff761":"code","2408deec":"markdown","525a4aa2":"markdown","64522cda":"markdown","7b22ba8e":"markdown","93c511c5":"markdown","1823c769":"markdown","0d7358c1":"markdown","a5388e4d":"markdown","c9ef63bf":"markdown","bcae8b2b":"markdown","d5749593":"markdown","68623514":"markdown","e2e124ec":"markdown","3f394c18":"markdown","e2823c7d":"markdown","dba091b6":"markdown","4d9b1fb6":"markdown","d006e803":"markdown","196e74cf":"markdown","f80d0df5":"markdown","b79ecd7b":"markdown","af5a1750":"markdown","5a4c90b5":"markdown","7f508779":"markdown","e4cf4c26":"markdown","7ad1ec45":"markdown","e130ef3e":"markdown","11f7bbe9":"markdown","b877141f":"markdown","39b6836b":"markdown","e868ea2d":"markdown","46d5ff36":"markdown","ed17011c":"markdown","1e6b4ead":"markdown","374a4639":"markdown","988b0676":"markdown","f24ea1bd":"markdown","24d32f2e":"markdown","6364ea7a":"markdown","6217c42c":"markdown","af8d829b":"markdown","4ff29ba1":"markdown","68c54d62":"markdown","2fcb7c08":"markdown","ae5ee91e":"markdown","f0659a2e":"markdown","fa198e09":"markdown","effe8189":"markdown","b9e44aad":"markdown","ed9cb22c":"markdown","dc326e7b":"markdown","f1c045ab":"markdown","02508bf1":"markdown","6da7350a":"markdown"},"source":{"9039652a":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\ncolor = sns.color_palette()\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\n%matplotlib inline\nfrom sklearn.svm import SVR, LinearSVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nimport scipy.stats as stats\nfrom sklearn.externals import joblib\n\n","038a98ca":"# Load the Drive helper and mount\n#from google.colab import drive\n# This will prompt for authorization.\n#drive.mount('\/content\/drive')","7ff179b3":"#loading data from google drive\n#train_df = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Personal Case STudy\/train.csv')\n#print(\"Train shape : \", train_df.shape)\n#test_df = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Personal Case STudy\/test.csv')\n#print(\"Train shape : \", test_df.shape)\n","87b64559":"#loading data from HDD\ntrain_df = pd.read_csv('..\/input\/train.csv')\nprint(\"Train shape : \", train_df.shape)\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(\"Train shape : \", test_df.shape)\n\n","2458063e":"train_df.info()","a1d349fa":"train_df.head()","a4d252ce":"def check_missing_values(df):\n    \n    if df.isnull().any().any():\n        print(\"There are missing values in the data\")  \n    else: \n        print(\"There are no missing values in the data\")\n","87a90549":"#calling functions to check missing values on training and test datasets\ncheck_missing_values(train_df)\ncheck_missing_values(test_df)","313b742d":"#we are checking 'y' column\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()\n\n\"\"\"here we have observed 1 outlier at apporx 260\"\"\"","26813836":"# we again check by visualising in BoxPlot\n\nplt.figure(figsize=(15,5))\nsns.boxplot(train_df.loc[:,'y'])\nplt.show()","8574a2ee":"# we need to remove that outlier \n# i have used zscore method and set threshold 10 acc to our data\n# https:\/\/www.geeksforgeeks.org\/scipy-stats-zscore-function-python\/\n\ntrain_df['x'] = np.abs(stats.zscore(train_df.loc[:,'y']))\n\noutlier_ids = train_df[train_df['x']>10].ID\n\ntrain_df_final = train_df[~train_df['ID'].isin(list(outlier_ids))]","01974163":"#now plotting again without outlier\n#we are checking 'y' column\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df_final.shape[0]), np.sort(train_df_final.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","e9cb3cf8":"# we again check by visualising in BoxPlot\n\nplt.figure(figsize=(15,5))\nsns.boxplot(train_df_final.loc[:,'y'])\nplt.show()","ec251ea2":"ulimit = 180# we have taken 180 data points\ntrain_df_final['y'].ix[train_df_final['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))#plot size\nsns.distplot(train_df_final.y.values, bins=50, kde=True)\nplt.xlabel('y value', fontsize=12)\nplt.show()","fb0b1980":"#removing that x helper row for outlier from main row\n\ntrain_df_final = train_df_final.drop([\"x\"], axis=1)","d23078d0":"dtype_df = train_df_final.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","62ad9b69":"#here we can see their types\ndtype_df.ix[:15,:]\n","7d0b3cfe":"#https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-mercedes\nvar_name = ['X0','X1','X2','X3','X4','X5','X6','X8']\nfor val in var_name:\n    col_order = np.sort(train_df_final[val].unique()).tolist()\n    plt.figure(figsize=(12,6))\n    sns.stripplot(x=val, y='y', data=train_df_final, order=col_order)\n    plt.xlabel(val, fontsize=12)\n    plt.ylabel('y', fontsize=12)\n    plt.title(\"Distribution of y variable with \"+val, fontsize=15)\n    plt.show()\n","4849a25a":"train_df = pd.read_csv('..\/input\/train.csv')\nprint(\"Train shape : \", train_df.shape)\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(\"Train shape : \", test_df.shape)\n\ny_train = train_df['y'].values\nid_test = test_df['ID'].values\n\nusable_columns = list(set(train_df.columns) - set(['ID', 'y']))#taking only important coloumns\nprint(len(usable_columns))\n\nx_train_final = train_df[usable_columns]\nx_test_final = test_df[usable_columns]","ac275ae1":"# Converting training dataset object categorical values to numerical categorical types\n#taken help from link: https:\/\/www.kaggle.com\/anokas\/mercedes-eda-xgboost-starter-0-55\n\nfor column in usable_columns:\n    cardinality = len(np.unique(x_train_final[column]))\n    \n    if cardinality == 1:\n        x_train_final.drop(column, axis=1) # Column with only one value is useless so we drop it.\n        x_test_final.drop(column, axis=1)\n        \n    if cardinality > 2: # Column is categorical.\n        mapper = lambda x: sum([ord(digit) for digit in x])\n        x_train_final[column] = x_train_final[column].apply(mapper)\n        x_test_final[column] = x_test_final[column].apply(mapper)","ba4d79aa":"# spiltting it into 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(x_train_final, y_train, test_size=0.3, random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3323c6f4":"#taken help from kaggle discussion and kernels for xgboost\n#setting up xtrain and xtrain\n\ny_mean = y_train.mean()\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_cvalid  = xgb.DMatrix(X_test, label=y_test)\nd_test = xgb.DMatrix(x_test_final)\n","31718101":"# evaluation r2_score metric\ndef r2_score_metric(y_pred, y):\n    y_true = y.get_label()\n    return 'r2', r2_score(y_true, y_pred)\n  ","f5b354a7":"%%time\n#xgb parameters\n#just cross validation our model \n\nparams = {\n    \n   'n_trees': 500, \n    'eta': 0.005,\n    'max_depth': 4,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\n\nnum_boost_round=2000\n\n#Cross Validation of XGBoost \ncv_result = xgb.cv(params, \n                   d_train, \n                   num_boost_round, \n                   nfold = 3,\n                   early_stopping_rounds=50,\n                   feval=r2_score_metric,#here we have used our metric method\n                   verbose_eval=100, \n                   show_stdv=False\n                  )\n","809d205d":"#Training the model\n#taken help from link: https:\/\/www.kaggle.com\/anokas\/mercedes-eda-xgboost-starter-0-55\n\n#model = joblib.load('model_xgb.pkl')#from load\n\nwatchlist = [(d_train, 'train'), (d_cvalid, 'valid')]\n\nmodel = xgb.train(params, d_train, num_boost_round, watchlist, early_stopping_rounds=50,\n                  feval=r2_score_metric, maximize=True, verbose_eval=10)\n\n#joblib.dump(model, 'model_xgb.pkl')#to load","b3bdcbfa":"# Predict on test\n\ny_pred = model.predict(d_test)\n","138a7d09":"# Predicting R2SCORE\n#from sklearn.metrics import r2_score\n\n#r2_score = r2_score(y_test, y_pred)#taking r2score on traing data\n\n#print('r2_score = ',r2_score)","0bd6c74e":"#exporting final results into csv file\n\ncsvfile = pd.DataFrame()\ncsvfile['ID'] = test_df['ID']\ncsvfile['y'] = y_pred\ncsvfile.to_csv('xgb.csv', index=False)","b7803f03":"#https:\/\/www.kaggle.com\/satadru5\/mercedes-benz-xgb-modeling-lb-score-0-54472 \nfig, ax = plt.subplots(1, 1, figsize=(8, 13))\nxgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)","6e27e3a1":"# PCA Implementation\npca = PCA(n_components=2)\npca_data = pca.fit_transform(X_train)","99ae1cb6":"cmap = sns.cubehelix_palette(as_cmap=True)\nf, ax = plt.subplots(figsize=(10,7))\npoints = ax.scatter(pca_data[:,0], pca_data[:,1], c=y_train, s=50, cmap=cmap)\nf.colorbar(points)\nplt.show()","4d420c37":"# TSNE Implementation\nmodel = TSNE(n_components=2,random_state=0,perplexity=30)\n\ntsne_data = model.fit_transform(X_train)","85b2a5e1":"\ncmap = sns.cubehelix_palette(as_cmap=True)\nf, ax = plt.subplots(figsize=(10,7))\npoints = ax.scatter(tsne_data[:,0], tsne_data[:,1], c=y_train, s=50, cmap=cmap)\nf.colorbar(points)\nplt.show()","45d22780":"#KNN implementation\n#biulding model\n\nknn = KNeighborsRegressor(n_neighbors=5)#k=5 gives best results\n\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nr2_score_knn = round(r2_score(y_test, y_pred),3)#taking r2score\naccuracy = round(knn.score(X_train, y_train) *100,2)#taking accuracy\n\nresults = {'r2_score':r2_score_knn, 'accuracy':accuracy}\nprint (results)\n","fa8504e9":"#SVR implementation\nfrom sklearn.metrics import r2_score\nclf = SVR()\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nr2_score = round(r2_score(y_test, y_pred),3)#taking r2score\naccuracy = round(clf.score(X_train, y_train) * 100, 2)\n\nresults = {'r2_score':r2_score, 'accuracy':accuracy}\nprint(results)","733b9a5b":"#RFR implementation\nfrom sklearn.metrics import r2_score\nclf = RandomForestRegressor(n_estimators = 60 ,max_depth=5,oob_score=True)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nr2_score = round(r2_score(y_test, y_pred),3)#taking r2score\naccuracy = round(clf.score(X_train, y_train) * 100, 2)\n\nresults = {'r2_score':r2_score, 'accuracy':accuracy}\nprint (results)","96fff761":"#Linear Regression implementation\nfrom sklearn.metrics import r2_score\nclf = LinearRegression()\n  \nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nr2_score = round(r2_score(y_test, y_pred),3)#taking r2score\naccuracy = round(clf.score(X_train, y_train) * 100, 2)\n\nresults = {'r2_score':r2_score, 'accuracy':accuracy}\nprint (results)","2408deec":"### 4.3 T-SNE (t-distributed Stochastic Neighbor Embedding)","525a4aa2":"#### Final Test result given by kaggle is: 0.54818","64522cda":"### 2.2 Mapping the real world problem to a Machine Learning Problem","7b22ba8e":"## <img src='https:\/\/www.mercedes-benz.com\/wp-content\/uploads\/sites\/3\/2018\/02\/mercedes-benz-passenger-cars-2018-a-class-w-177-amg-line-digital-white-pearl-2560x1440-2560x1440.jpg'>\n\n\n\n","93c511c5":"### 4.7 Linear Regression","1823c769":"### 3.3 Ploting","0d7358c1":"#### 3.3.3 Data type of all the variables present in the dataset.","a5388e4d":"Main Source: https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/overview\/description\n<br>\nData: https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/data\n<br>\nDiscussion: https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/discussion","c9ef63bf":"###  4.6 Random Forest Regressor","bcae8b2b":"**Target Variable:**\n<br>\n  <li>\"y\" is the variable we need to predict. So let us do some analysis on this variable first.\n<br>\n <li>  Varible y is of type float\n<br>\n <li>X0,X1,X2,X3,X4,X5,X6,X8 are of type object\n<br>\n <li>Rest of the columns are int type\n<br>\n <li>We will convert [X0,X1,X2,X3,X4,X5,X6,X8] to categorical types and plot to see the distribution of values.","d5749593":"#### 2.2.2 Performance Metric ","68623514":"#### Feature Importance","e2e124ec":"#### 2.2.1 Type of Machine Learning Problem","3f394c18":"### 1.4 Sources\/Useful Links","e2823c7d":"#### 2.1.1 Data Overview","dba091b6":"Observation: here we can see how PCA visualize all the data point far separated from each other, they are not forming tightly group.","4d9b1fb6":"### 4.4 K-Nearest Neighbors Regressor","d006e803":"#### 2.1.2 Example Data point","196e74cf":"Get the data from :  https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/data ","f80d0df5":"Observation: here we can see how TSNE visualize all the data point closly attached from each other, They are well grouped .","b79ecd7b":"X0 to X8 are the categorical columns","af5a1750":"\n\nThis dataset contains an anonymized set of variables, each representing a custom feature in a Mercedes car. For example, a variable could be 4WD, added air suspension, or a head-up display.\n\nThe ground truth is labeled \u2018y\u2019 and represents the time (in seconds) that the car took to pass testing for each variable.","5a4c90b5":"## 1.Business Problem","7f508779":"## 5. Conclution","e4cf4c26":"### 2.1 Data","7ad1ec45":"link: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.r2_score.html<br>\n##### r2_score:\n<br>\n<li>sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=\u2019uniform_average\u2019)<\/li>\n<li>R^2 (coefficient of determination) regression score function.<\/li>\n\n<li>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.","e130ef3e":"We need to Reduce the time that cars spend on the test bench.","11f7bbe9":"Observation: \n<li>we have observed that X0, X1, X2, X5, X6 and X8 have larger data point.\n  <li> X4 and X3 have lesser data point.\n   \n","b877141f":"### 3.2 Checking for missing values","39b6836b":"#### 3.4 Plotting these categorical Values","e868ea2d":"1 one of doing is this","46d5ff36":"<img src='https:\/\/imgur.com\/sy5SLYa.jpg'>","ed17011c":"## 3. Exploratory Data Analysis\n### 3.1  Importing important libraries","1e6b4ead":"### XGBoost","374a4639":"(CLOUMNS)ID\ty\tX0\tX1\tX2\tX3\tX4\tX5\tX6\tX8\tX10\tX11\tX12\tX13\tX14\tX15\tX16\tX17\tX18\tX19\tX20\tX21\tX22\tX23\tX24\tX26\tX27\tX28\tX29\tX30\tX31\tX32\tX33\tX34\tX35\tX36\tX37\tX38\tX39\tX40\tX41\tX42\tX43\tX44\tX45\tX46\tX47\tX48\tX49\tX50\tX51\tX52\tX53\tX54\tX55\tX56\tX57\tX58\tX59\tX60\tX61\tX62\tX63\tX64\tX65\tX66\tX67\tX68\tX69\tX70\tX71\tX73\tX74\tX75\tX76\tX77\tX78\tX79\tX80\tX81\tX82\tX83\tX84\tX85\tX86\tX87\tX88\tX89\tX90\tX91\tX92\tX93\tX94\tX95\tX96\tX97\tX98\tX99\tX100\tX101\tX102\tX103\tX104\tX105\tX106\tX107\tX108\tX109\tX110\tX111\tX112\tX113\tX114\tX115\tX116\tX117\tX118\tX119\tX120\tX122\tX123\tX124\tX125\tX126\tX127\tX128\tX129\tX130\tX131\tX132\tX133\tX134\tX135\tX136\tX137\tX138\tX139\tX140\tX141\tX142\tX143\tX144\tX145\tX146\tX147\tX148\tX150\tX151\tX152\tX153\tX154\tX155\tX156\tX157\tX158\tX159\tX160\tX161\tX162\tX163\tX164\tX165\tX166\tX167\tX168\tX169\tX170\tX171\tX172\tX173\tX174\tX175\tX176\tX177\tX178\tX179\tX180\tX181\tX182\tX183\tX184\tX185\tX186\tX187\tX189\tX190\tX191\tX192\tX194\tX195\tX196\tX197\tX198\tX199\tX200\tX201\tX202\tX203\tX204\tX205\tX206\tX207\tX208\tX209\tX210\tX211\tX212\tX213\tX214\tX215\tX216\tX217\tX218\tX219\tX220\tX221\tX222\tX223\tX224\tX225\tX226\tX227\tX228\tX229\tX230\tX231\tX232\tX233\tX234\tX235\tX236\tX237\tX238\tX239\tX240\tX241\tX242\tX243\tX244\tX245\tX246\tX247\tX248\tX249\tX250\tX251\tX252\tX253\tX254\tX255\tX256\tX257\tX258\tX259\tX260\tX261\tX262\tX263\tX264\tX265\tX266\tX267\tX268\tX269\tX270\tX271\tX272\tX273\tX274\tX275\tX276\tX277\tX278\tX279\tX280\tX281\tX282\tX283\tX284\tX285\tX286\tX287\tX288\tX289\tX290\tX291\tX292\tX293\tX294\tX295\tX296\tX297\tX298\tX299\tX300\tX301\tX302\tX304\tX305\tX306\tX307\tX308\tX309\tX310\tX311\tX312\tX313\tX314\tX315\tX316\tX317\tX318\tX319\tX320\tX321\tX322\tX323\tX324\tX325\tX326\tX327\tX328\tX329\tX330\tX331\tX332\tX333\tX334\tX335\tX336\tX337\tX338\tX339\tX340\tX341\tX342\tX343\tX344\tX345\tX346\tX347\tX348\tX349\tX350\tX351\tX352\tX353\tX354\tX355\tX356\tX357\tX358\tX359\tX360\tX361\tX362\tX363\tX364\tX365\tX366\tX367\tX368\tX369\tX370\tX371\tX372\tX373\tX374\tX375\tX376\tX377\tX378\tX379\tX380\tX382\tX383\tX384\tX385\n<br>\n<br>\n(1st ROW)0\t130.81\tk\tv\tat\ta\td\tu\tj\to\t0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\n","988b0676":"here we have observed 1 outlier ","f24ea1bd":"### 1.3 Objective\n1. Reduce the time that cars spend on the test bench.\n2. To speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards.","24d32f2e":"### 3.1 Reading data and basic stats","6364ea7a":"Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler\u2019s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler\u2019s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler\u2019s production lines.","6217c42c":"###### best cv_result is: train-r2:0.67011\ttest-r2:0.591521","af8d829b":"# Mercedes-Benz Greener Manufacturing\nThe Personal Case Study","4ff29ba1":"### 1.1 Description","68c54d62":"<table style=\"width:100%\">\n  <tr>\n    <th>S.no <\/th>\n    <th>Model Algo<\/th> \n    <th>R2 Score<\/th>\n     <th>Accuracy<\/th>\n  <\/tr>\n  <tr>\n    <td>1.<\/td>\n    <td>K-Nearest Neighbors Regressor<\/td> \n    <td> 0.267<\/td>\n    <td>56,69<\/td>\n  <\/tr>\n  <tr>\n    <td>2.<\/td>\n    <td>Support Vector Regressor<\/td> \n    <td>0.208<\/td>\n    <td> 32.5<\/td>\n  <\/tr>\n    <tr>\n    <td>3.<\/td>\n    <td>Random Forest Regressor<\/td> \n    <td> 0.476<\/td>\n    <td>  64.74<\/td>\n  <\/tr>\n    <tr>\n    <td>3.<\/td>\n    <td>Linear Regression<\/td> \n    <td>-4.882113861562633e+16<\/td>\n    <td> 63.06<\/td>\n  <\/tr>\n   <tr>\n    <td>4.<\/td>\n    <td>XGBoost<\/td> \n    <td>0.54818<\/td>\n    <td>  -  <\/td>\n  <\/tr>\n<\/table>\n\n#### Here we can see from conclution that\n##### 1. XGBoost perform best in this case by obtaining R2_SCORE =0.54818\n##### 2.Next Our RANDOM FOREST REGRESSOR also perform well with  nearly R2_SCORE=0.47\n##### 4. Linear Regression model not suit for this type of problem.","2fcb7c08":"### 4.1 Data preparation\n","ae5ee91e":"## 4. Machine Learning Models ","f0659a2e":"### 4.5 Support Vector Regressor","fa198e09":"### 4.2 PCA - Principal component analysis","effe8189":"<p> \n- Data will be in a file Train.csv and Test.csv <br>\n- Size of Train.csv - 3.07MB <br>\n- Size of Train.csv - 3.04 MB <br>\n- Number of rows and columns in Train.csv = 4210 x 378 <br>\n- Number of rows and columns in Test.csv = 4210 x 377 \n\n<\/p>","b9e44aad":"## 2. Machine Learning Problem","ed9cb22c":"###  2.3 Train and Test Construction\n<p>  <\/p>\n<p> We build train and test by randomly splitting in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with. <\/p>","dc326e7b":"#### 3.3.1 Ploting y values","f1c045ab":"Maximum of the columns are integers. \n<br>\n8 categorical columns. \n<br>\n1 float column (target variable) i.e. 'y'","02508bf1":"### 1.2 Data Description:","6da7350a":"#### 3.3.2 Plotting y distribution graph."}}