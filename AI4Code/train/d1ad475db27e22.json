{"cell_type":{"064d7305":"code","4bd24449":"code","0b17d5fc":"code","a8168f2c":"code","57b9179a":"code","3e7797ec":"code","689ac4aa":"code","0c5af514":"code","cf28605b":"code","7d7630d3":"code","c438252f":"code","8a40db68":"code","27c3bed0":"code","ac67bc4b":"code","55a6c55a":"code","70c3c370":"code","8487409e":"code","8296e340":"markdown","439f3b1e":"markdown","c7ab8c97":"markdown","96d79f09":"markdown","87f41886":"markdown","d7f602ee":"markdown","754e71a9":"markdown","16533011":"markdown","aa9261de":"markdown","182087fc":"markdown","30300287":"markdown","090494df":"markdown","c33c3760":"markdown"},"source":{"064d7305":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom statistics import mean\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nTRAINING = True\nRS = 69420\nDATA_PATH = \"..\/input\/tabular-playground-series-mar-2021\/train.csv\"","4bd24449":"train = pd.read_csv(DATA_PATH, index_col=0)\n\ncat_features = [c for c in train.columns if 'cat' in c]\nle = LabelEncoder()\nfor col in cat_features:\n    train[col] = le.fit_transform(train[col])\n\nX = train.iloc[:, :-1].values\ny = train.iloc[:, -1].values","0b17d5fc":"def create_autoencoder(input_dim, output_dim,noise=0.05):\n    i = Input(input_dim)\n    encoded = BatchNormalization()(i)\n    encoded = GaussianNoise(noise)(encoded)\n    encoded = Dense(64,activation='relu')(encoded)\n    decoded = Dropout(0.2)(encoded)\n    decoded = Dense(input_dim,name='decoded')(decoded)\n    x = Dense(32,activation='relu')(decoded)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = Model(inputs=i,outputs=encoded)\n    autoencoder = Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n    return autoencoder, encoder","a8168f2c":"# An area of further research is to tune the Dense Layers, Dropouts, Learning rate and Label Smoothing\n\ndef create_model(input_dim,output_dim,encoder):\n    inputs = Input(input_dim)\n    \n    x = encoder(inputs)\n    x = Concatenate()([x,inputs]) #use both raw and encoded features\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    for i in range(3):\n        x = Dense(64)(x)\n        x = BatchNormalization()(x)\n        x = Lambda(tf.keras.activations.swish)(x)\n        x = Dropout(0.3)(x)\n    \n    x = Dense(output_dim, activation='sigmoid')(x)\n    model = Model(inputs=inputs,outputs=x)\n    model.compile(optimizer=Adam(0.00001),\n                  loss=BinaryCrossentropy(label_smoothing=0),\n                  metrics=[tf.keras.metrics.AUC(name ='auc')])\n    return model","57b9179a":"autoencoder, encoder = create_autoencoder(X.shape[-1], 1, noise=0.1)","3e7797ec":"# Tune the number of Epochs\nif TRAINING:\n    autoencoder.fit(X,(X,y),\n                    epochs=5,\n                    batch_size=32, \n                    validation_split=0.1,\n                    callbacks=[EarlyStopping('val_loss',patience=5,restore_best_weights=True)])\n    encoder.save_weights('.\/encoder.hdf5')\nelse:\n    encoder.load_weights('encoder.hdf5')\nencoder.trainable = False","689ac4aa":"model_fn = create_model(X.shape[-1], 1, encoder)","0c5af514":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","cf28605b":"%%time\nhistory = model_fn.fit(X_train, y_train,\n                       epochs=1000,\n                       batch_size=32, \n                       validation_split=0.1,\n                       callbacks=[EarlyStopping('val_loss',patience=5,restore_best_weights=True)])","7d7630d3":"# from numba import cuda\n# cuda.select_device(0)\n# cuda.close()\n# import gc\n# del cuda\n# gc.collect()","c438252f":"from catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score","8a40db68":"# Here are some tuned parameters - feel free to run Optuna for Catboost aswell, I couldnt be bothered\n\nXGB_Params = {'n_estimators': 450,\n              'max_depth': 15,\n              'reg_lambda': 5,\n              'min_child_weight': 0,\n              'subsample': 0.8832278322447424,\n              'learning_rate': 0.014255981518563889,\n              'colsample_bytree': 0.28}\n\nLGBM_Params = {'lambda_l1': 0.048263765268859345,\n               'lambda_l2': 0.002059552723754179,\n               'num_leaves': 138,\n               'feature_fraction': 0.4090885438608842,\n               'bagging_fraction': 0.833157756558512,\n               'bagging_freq': 1,\n               'min_child_samples': 97}","27c3bed0":"# Tune the number of models in the ensemble with the range\n# GPU turned off when submitting due to kaggle submission taking the damn piss\n\nestimators = []\n\nfor i in range(3):\n    estimators.append((f\"model_lgbm{i}\",\n                       LGBMClassifier(**LGBM_Params,\n                                      random_seed=np.random.randint(0, 100000))))\n\n    estimators.append((f\"model_xgb{i}\",\n                       XGBClassifier(**XGB_Params,\n                                     objective='binary:logistic',\n                                     random_state=np.random.randint(0, 100000))))\n    \n    estimators.append((f\"model_cat{i}\",\n                       CatBoostClassifier(random_seed=np.random.randint(0, 100000),\n                                          verbose=False)))","ac67bc4b":"from sklearn.ensemble import VotingClassifier\n\nclf = VotingClassifier(estimators=estimators,\n                       verbose=1,\n                       voting='soft')","55a6c55a":"%%time\nclf.fit(X_train, y_train)","70c3c370":"# Predict\n# y_pred = clf.predict(X_test)\n# y_pred = y_pred.reshape(-1,1)\n# print(f\"Testing Precision (Pure Ensemble): {precision_score(y_test, y_pred, 'weighted')}\")\n\n# y_nnpred = model_fn.predict(X_test)\n# # By default the neural net outputs probabilites, thus this line converts those into binary with a threshold of 0.5, maybe expe3riment\n# y_nnpred[:] = y_nnpred[:]>0.5\n# print(f\"Testing Precision (Pure NN): {precision_score(y_test, y_nnpred, 'weighted')}\")\n\n# y_pred = np.average((y_nnpred, y_pred), axis=0)\n# # Compute Metrics\n# print(f\"Testing Precision (Ensemble): {precision_score(y_test, y_pred, 'weighted')}\")","8487409e":"test = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\", index_col=0)\ncat_cols = [c for c in train.columns if 'cat' in c]\n\nfor col in cat_cols:\n    test[col] = le.fit_transform(test[col])\n\nsubmission = pd.DataFrame(index=test.index)\n\nnn_pred = model_fn.predict(test.values)\nclf_pred = clf.predict_proba(test.values)[:, 1]\nclf_pred = clf_pred.reshape(-1,1)\n\nsubmission['target'] = np.average((nn_pred, clf_pred), axis=0)\n\nsubmission.to_csv(\"submission.csv\")","8296e340":"# Voting Classifier","439f3b1e":"**Create the MLP**","c7ab8c97":"# **Ensembles galore!**","96d79f09":"**Train the AutoEncoder**","87f41886":"**Create the AutoEncoder**","d7f602ee":"# Stage 1: AutoEncoder + MLP","754e71a9":"# Produce Submission","16533011":"# Stage 2: Boosted Ensemble","aa9261de":"**The idea of ensembling is very simple. Rather than trying to produce a single very strong learner, you should produce many weaker learners - all which are specialised in one aspect of the data and take their aggregate predictions.**\n\nThis is what led me to produce this notebook, first you can see below I have built a denoising autoencoder + MLP - this stage of the model aims to denoise the data (that likely crept in during the CTGAN process of generating the data.)\n\nIn the second stage I have 3 boosted treees models which are combined using a voting classifier - all of these models were tuned using Optuna and thus are highly specialised too.\n\nI do not have the energy to continue the research and so I have written this notebook with clean code to allow you all to take over and thus I would love to see where you all take this notebook!","182087fc":"**You might notice the NN is missing, this is because Sklearn seems to have a fit when you include it :( I manually take the mean later to ensemble the NN**","30300287":"**Merge the AutoEncoder to the MLP**","090494df":"**There is an issue with Tensorflow that after a model is finished training, the GPU memory is not released - the following code releases it manually!**\n\n**I suggest you leave it commented out when submitting final notebook - some wierd CUDA errors show up elsewise**","c33c3760":"# Import Libraries"}}