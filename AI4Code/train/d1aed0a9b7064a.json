{"cell_type":{"e5df930b":"code","ec300b3c":"code","2d7f445c":"code","33f1ef65":"code","2bc2306b":"code","dae25a0f":"code","28127577":"code","21bb644b":"code","66725d04":"code","e20f4ab8":"code","6cd814d4":"code","74db6053":"code","75e478fb":"code","c84cdff1":"code","266598f8":"code","8d99a4ea":"code","63a391fa":"code","c565ff23":"code","7a00eb80":"code","b26cdec7":"code","746e442d":"code","868270ee":"code","57be58c1":"code","99dd3ee5":"code","a7904aed":"code","dde69e96":"code","bc12aa6b":"code","1229524e":"code","4a238f24":"code","692c9e0a":"code","0ef5f23f":"code","9cb6a533":"code","af4f5eca":"code","e56f9c29":"code","30b31b27":"code","78fd7cf2":"code","ec8ec6b8":"code","542e96a2":"code","e113e470":"code","d1e387e6":"code","9f9d2462":"code","9aeb8e88":"code","fbf816c1":"code","02e7a6ce":"code","8f016822":"code","b445f89e":"code","23066da3":"code","54d013db":"code","3a80e04a":"code","51df8e4c":"code","f8470809":"code","19ce4d87":"code","35bdbf4f":"code","c665836e":"code","08162bf1":"code","2f7762de":"code","bda1e920":"code","9e301dc4":"code","abd7be50":"code","d03e6064":"code","448ba471":"code","fc95b984":"code","496451fb":"markdown","b4f2402e":"markdown","19904617":"markdown","c8d82a44":"markdown","3f998f49":"markdown","87b1f869":"markdown","b6fac75f":"markdown","71c56b7d":"markdown","25c80b2d":"markdown","784e10d0":"markdown","87370b75":"markdown","1b96a083":"markdown"},"source":{"e5df930b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport featuretools as ft\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport pickle\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ec300b3c":"new = pd.read_csv('..\/input\/new_merchant_transactions.csv',parse_dates =[\"purchase_date\"])\nhis = pd.read_csv('..\/input\/historical_transactions.csv',parse_dates =[\"purchase_date\"])\ntrain = pd.read_csv( '..\/input\/train.csv',parse_dates =[\"first_active_month\"])\ntest = pd.read_csv( '..\/input\/test.csv',parse_dates =[\"first_active_month\"])\nmerchants = pd.read_csv( '..\/input\/merchants.csv')","2d7f445c":"merchants = merchants.drop_duplicates(subset=\"merchant_id\",keep=\"first\")\nprint(train.shape[0]==train[\"card_id\"].nunique())\nprint(merchants.shape[0]==merchants[\"merchant_id\"].nunique())\n#print(merchants[merchants[\"merchant_id\"].duplicated()])","33f1ef65":"new_drop=new.drop(['purchase_date',\"merchant_category_id\",\n                                                   \"subsector_id\",\"city_id\",\n                                                   \"state_id\"], axis=1)\nhis_drop=his.drop(['purchase_date',\"merchant_category_id\",\n                                                   \"subsector_id\",\"city_id\",\n                                                   \"state_id\"], axis=1)","2bc2306b":"mer_drop = merchants.drop(['merchant_group_id',\"merchant_category_id\",\n                                                   \"subsector_id\",\"most_recent_sales_range\",\n                                                   \"most_recent_purchases_range\",\n                           'city_id','state_id'], axis=1)","dae25a0f":"train_his=train[[\"card_id\"]].merge(his_drop, how='left', on=\"card_id\")","28127577":"print(train_his.shape)\ntrain_his.head()","21bb644b":"#train_his_transactions=train_his_transactions.dropna()\ntrain_his_sub=train_his.loc[np.random.choice(train_his.index, 1000000, replace=False)]","66725d04":"print(train_his_sub.shape)\ntrain_his_sub.head()","e20f4ab8":"authorized_flag = pd.get_dummies(train_his_sub['authorized_flag'])\nauthorized_flag.columns = ['authorized_flag_N', 'authorized_flag_Y']\ntrain_his_sub=train_his_sub.drop(['authorized_flag'], axis=1)\ntrain_his_sub=pd.concat([train_his_sub, authorized_flag], axis=1)","6cd814d4":"train_his_sub.head()","74db6053":"category_1 = pd.get_dummies(train_his_sub['category_1'])\ncategory_1.head()\ncategory_1.columns = ['category_1_N', 'category_1_Y']\ntrain_his_sub=train_his_sub.drop(['category_1'], axis=1)\ntrain_his_sub=pd.concat([train_his_sub, category_1], axis=1)\ntrain_his_sub.head()","75e478fb":"category_2 = pd.get_dummies(train_his_sub['category_2'])\n#category_2.head()\ncategory_2.columns = ['category_2_1', 'category_2_2',\"category_2_3\",\"category_2_4\",\"category_2_5\"]\ntrain_his_sub=train_his_sub.drop(['category_2'], axis=1)\ntrain_his_sub=pd.concat([train_his_sub, category_2], axis=1)\ntrain_his_sub.head()","c84cdff1":"category_3 = pd.get_dummies(train_his_sub['category_3'])\n#category_3.head()\ncategory_3.columns = ['category_3_A', 'category_3_B',\"category_3_C\"]\ntrain_his_sub=train_his_sub.drop(['category_3'], axis=1)\ntrain_his_sub=pd.concat([train_his_sub, category_3], axis=1)\ntrain_his_sub.head()","266598f8":"train_his_sub.columns","8d99a4ea":"train_his_sub.columns.values[1:]=[\"his_trans_\" + str(col) for col in list(train_his_sub)[1:]]\n","63a391fa":"train_his_sub.columns.values[2] = \"merchant_id\"","c565ff23":"train_his_sub.columns.values","7a00eb80":"mer_drop.columns.values[1:]=[\"his_mer_\" + str(col) for col in mer_drop.columns.values[1:]] \nmer_drop.columns","b26cdec7":"train_his_sub_mer = train_his_sub.merge(mer_drop,how=\"left\",on='merchant_id')\ntrain_his_sub_mer.shape","746e442d":"train_his_sub_mer.head()","868270ee":"category_1 = pd.get_dummies(train_his_sub_mer['his_mer_category_1'])\n#category_1.head()\ncategory_1.columns = ['his_mer_category_1_N', 'his_mer_category_1_Y']\ntrain_his_sub_mer=train_his_sub_mer.drop(['his_mer_category_1'], axis=1)\ntrain_his_sub_mer=pd.concat([train_his_sub_mer, category_1], axis=1)\ntrain_his_sub_mer.head()","57be58c1":"category_2 = pd.get_dummies(train_his_sub_mer['his_mer_category_2'])\n#category_2.head()\ncategory_2.columns = ['his_mer_category_2_1', 'his_mer_category_2_2',\"his_mer_category_2_3\",\"his_mer_category_2_4\",\"his_mer_category_2_5\"]\ntrain_his_sub_mer=train_his_sub_mer.drop(['his_mer_category_2'], axis=1)\ntrain_his_sub_mer=pd.concat([train_his_sub_mer, category_2], axis=1)\ntrain_his_sub_mer.head()","99dd3ee5":"category_4 = pd.get_dummies(train_his_sub_mer['his_mer_category_4'])\n#category_1.head()\ncategory_4.columns = ['his_mer_category_4_N', 'his_mer_category_4_Y']\ntrain_his_sub_mer=train_his_sub_mer.drop(['his_mer_category_4'], axis=1)\ntrain_his_sub_mer=pd.concat([train_his_sub_mer, category_4], axis=1)\ntrain_his_sub_mer.head()","a7904aed":"with open('train_his_sub_mer.pickle', 'wb') as f:\n    pickle.dump(train_his_sub_mer, f)","dde69e96":"with open('train_his_sub_mer.pickle', 'rb') as f:\n    train_his_sub_mer = pickle.load(f)","bc12aa6b":"## extract history transactions record for training data\n\ntrain_new=train[[\"card_id\"]].merge(new_drop, how='left', on=\"card_id\")\n\nprint(train_new.shape)\ntrain_new.head()\n\n## **one hot encode** for train_his_sub : authorized_flag; category_1; category_2; category_3\ntrain_new['authorized_flag']=train_new['authorized_flag'].fillna(\"N\")\nauthorized_flag = pd.get_dummies(train_new['authorized_flag'])\n#print(train_new['authorized_flag'].value_counts())\n#print(train_new['authorized_flag'].isna().sum())\nauthorized_flag.columns = ['authorized_flag_N', 'authorized_flag_Y']\ntrain_new=train_new.drop(['authorized_flag'], axis=1)\ntrain_new=pd.concat([train_new, authorized_flag], axis=1)\n\ntrain_new.head()\n\ncategory_1 = pd.get_dummies(train_new['category_1'])\ncategory_1.head()\ncategory_1.columns = ['category_1_N', 'category_1_Y']\ntrain_new=train_new.drop(['category_1'], axis=1)\ntrain_new=pd.concat([train_new, category_1], axis=1)\ntrain_new.head()\n\ncategory_2 = pd.get_dummies(train_new['category_2'])\n#category_2.head()\ncategory_2.columns = ['category_2_1', 'category_2_2',\"category_2_3\",\"category_2_4\",\"category_2_5\"]\ntrain_new=train_new.drop(['category_2'], axis=1)\ntrain_new=pd.concat([train_new, category_2], axis=1)\ntrain_new.head()\n\ncategory_3 = pd.get_dummies(train_new['category_3'])\n#category_3.head()\ncategory_3.columns = ['category_3_A', 'category_3_B',\"category_3_C\"]\ntrain_new=train_new.drop(['category_3'], axis=1)\ntrain_new=pd.concat([train_new, category_3], axis=1)\ntrain_new.head()\n\n## rename column for merging simplicity\n\nprint(train_new.columns)\n\ntrain_new.columns.values[1:]=[\"new_trans_\" + str(col) for col in list(train_new)[1:]]\n\n\ntrain_new.columns.values[2] = \"merchant_id\"\n\nprint(train_new.columns.values)\n\n","1229524e":"mer_drop = merchants.drop(['merchant_group_id',\"merchant_category_id\",\n                                                   \"subsector_id\",\"most_recent_sales_range\",\n                                                   \"most_recent_purchases_range\",\n                           'city_id','state_id'], axis=1)\nmer_drop.columns.values[1:]=[\"new_mer_\" + str(col) for col in mer_drop.columns.values[1:]] \nprint(mer_drop.columns)\n\n## merge with merchants (X : his transaction, Y : merchant)\n\ntrain_new_mer = train_new.merge(mer_drop,how=\"left\",on='merchant_id')\ntrain_new_mer.shape\n\ntrain_new_mer.head()\n\n## **one hot encode** for train_his_sub_mer(merchants part) : category_1; category_2; category_3\n\ncategory_1 = pd.get_dummies(train_new_mer['new_mer_category_1'])\n#category_1.head()\ncategory_1.columns = ['new_mer_category_1_N', 'new_mer_category_1_Y']\ntrain_new_mer=train_new_mer.drop(['new_mer_category_1'], axis=1)\ntrain_new_mer=pd.concat([train_new_mer, category_1], axis=1)\ntrain_new_mer.head()\n\ncategory_2 = pd.get_dummies(train_new_mer['new_mer_category_2'])\n#category_2.head()\ncategory_2.columns = ['new_mer_category_2_1', 'new_mer_category_2_2',\"new_mer_category_2_3\",\"new_mer_category_2_4\",\"new_mer_category_2_5\"]\ntrain_new_mer=train_new_mer.drop(['new_mer_category_2'], axis=1)\ntrain_new_mer=pd.concat([train_new_mer, category_2], axis=1)\ntrain_new_mer.head()\n\ncategory_4 = pd.get_dummies(train_new_mer['new_mer_category_4'])\n#category_1.head()\ncategory_4.columns = ['new_mer_category_4_N', 'new_mer_category_4_Y']\ntrain_new_mer=train_new_mer.drop(['new_mer_category_4'], axis=1)\ntrain_new_mer=pd.concat([train_new_mer, category_4], axis=1)\ntrain_new_mer.head()\n\nwith open('train_new_mer.pickle', 'wb') as f:\n    pickle.dump(train_new_mer, f)\n\nwith open('train_new_mer.pickle', 'rb') as f:\n    train_new_mer = pickle.load(f)\n\ntrain_new_mer.head()","4a238f24":"# #inner join (some training card id is not in new_transactions,delete these)\n# train_new_transactions=train[[\"card_id\"]].merge(new_transactions, how='inner', on=\"card_id\")","692c9e0a":"# print(train_new_transactions.shape)\n# train_new_transactions.head()","0ef5f23f":"# train_new_transactions.columns=[\"new_trans_\" + str(col) for col in train_new_transactions.columns]","9cb6a533":"# merchants.columns=[\"merchants_\" + str(col) for col in merchants.columns] ","af4f5eca":"#drop merchants with same merchant id,return the first one\n#merchants.drop_duplicates(subset=\"merchants_merchant_id\",keep=\"first\",inplace=True)","e56f9c29":"# merchants.head()","30b31b27":"# merge_new_trans_merchants=pd.merge(train_new_transactions,merchants,how=\"left\",left_on=\"new_trans_merchant_id\",right_on=\"merchants_merchant_id\")","78fd7cf2":"# merge_new_trans_merchants['new_trans_authorized_flag'] = merge_new_trans_merchants['new_trans_authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)\n# merge_new_trans_merchants['new_trans_category_1'] = merge_new_trans_merchants['new_trans_category_1'].apply(lambda x: 1 if x == 'Y' else 0)","ec8ec6b8":"# autorized_card_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_authorized_flag'].mean()","542e96a2":"# new_trans_cate_1_rate_Y = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_1'].mean()","e113e470":"# #create dummy variable for new_trans_category_2\n# merge_new_trans_merchants['new_trans_category_2_1'] = merge_new_trans_merchants['new_trans_category_2'].apply(lambda x: 1 if x == 1 else 0)\n# merge_new_trans_merchants['new_trans_category_2_2'] = merge_new_trans_merchants['new_trans_category_2'].apply(lambda x: 1 if x == 2 else 0)\n# merge_new_trans_merchants['new_trans_category_2_3'] = merge_new_trans_merchants['new_trans_category_2'].apply(lambda x: 1 if x == 3 else 0)\n# merge_new_trans_merchants['new_trans_category_2_4'] = merge_new_trans_merchants['new_trans_category_2'].apply(lambda x: 1 if x == 4 else 0)\n# merge_new_trans_merchants['new_trans_category_2_5'] = merge_new_trans_merchants['new_trans_category_2'].apply(lambda x: 1 if x == 5 else 0)","d1e387e6":"# #calculate mean of each category in cate_2 group by card_id\n# cate_2_1_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_2_1'].mean()\n# cate_2_2_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_2_2'].mean()\n# cate_2_3_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_2_3'].mean()\n# cate_2_4_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_2_4'].mean()\n# cate_2_5_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_2_5'].mean()","9f9d2462":"# #create dummy variable for new_trans_category_3\n# merge_new_trans_merchants['new_trans_category_3_A'] = merge_new_trans_merchants['new_trans_category_3'].apply(lambda x: 1 if x == \"A\" else 0)\n# merge_new_trans_merchants['new_trans_category_3_B'] = merge_new_trans_merchants['new_trans_category_3'].apply(lambda x: 1 if x == \"B\" else 0)\n# merge_new_trans_merchants['new_trans_category_3_C'] = merge_new_trans_merchants['new_trans_category_3'].apply(lambda x: 1 if x == \"C\" else 0)\n","9aeb8e88":"# #calculate mean of each category in cate_3 group by card_id\n# cate_3_A_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_3_A'].mean()\n# cate_3_B_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_3_B'].mean()\n# cate_3_C_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['new_trans_category_3_C'].mean()\n","fbf816c1":"# #create dummy variable for merchant_category_1\n# merge_new_trans_merchants['merchants_category_1'] = merge_new_trans_merchants['merchants_category_1'].apply(lambda x: 1 if x == \"Y\" else 0)\n","02e7a6ce":"# #calculate mean of each category in merchant_cate_1 group by card_id\n# merchant_cate_1_Y_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_1'].mean()\n","8f016822":"# #create dummy variable for merchants_category_2\n# merge_new_trans_merchants['merchants_category_2_1'] = merge_new_trans_merchants['merchants_category_2'].apply(lambda x: 1 if x == 1 else 0)\n# merge_new_trans_merchants['merchants_category_2_2'] = merge_new_trans_merchants['merchants_category_2'].apply(lambda x: 1 if x == 2 else 0)\n# merge_new_trans_merchants['merchants_category_2_3'] = merge_new_trans_merchants['merchants_category_2'].apply(lambda x: 1 if x == 3 else 0)\n# merge_new_trans_merchants['merchants_category_2_4'] = merge_new_trans_merchants['merchants_category_2'].apply(lambda x: 1 if x == 4 else 0)\n# merge_new_trans_merchants['merchants_category_2_5'] = merge_new_trans_merchants['merchants_category_2'].apply(lambda x: 1 if x == 5 else 0)","b445f89e":"# #calculate mean of each category in merchant cate_2 group by card_id\n# merchants_cate_2_1_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_2_1'].mean()\n# merchants_cate_2_2_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_2_2'].mean()\n# merchants_cate_2_3_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_2_3'].mean()\n# merchants_cate_2_4_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_2_4'].mean()\n# merchants_cate_2_5_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_2_5'].mean()","23066da3":"# merge_new_trans_merchants['merchants_category_4'] = merge_new_trans_merchants['merchants_category_4'].apply(lambda x: 1 if x == \"Y\" else 0)\n# merchant_cate_4_Y_rate = merge_new_trans_merchants.groupby(['new_trans_card_id'])['merchants_category_4'].mean()\n","54d013db":"# new_trans_merchants_by_card_id={\n#     \"authorized_card_rate\":autorized_card_rate,\n#     \"new_trans_category_1_rate_Y\":new_trans_cate_1_rate_Y,\n#     \"new_trans_category_2_1_rate\":cate_2_1_rate,\n#     \"new_trans_category_2_2_rate\":cate_2_2_rate,\n#     \"new_trans_category_2_3_rate\":cate_2_3_rate,\n#     \"new_trans_category_2_4_rate\":cate_2_4_rate,\n#     \"new_trans_category_2_5_rate\":cate_2_5_rate,\n#     \"new_trans_category_3_A_rate\":cate_3_A_rate,\n#     \"new_trans_category_3_B_rate\":cate_3_B_rate,\n#     \"new_trans_category_3_C_rate\":cate_3_C_rate,\n#     \"merchants_category_1_Y_rate\":merchant_cate_1_Y_rate,\n#     \"merchants_category_2_1_rate\":merchants_cate_2_1_rate,\n#     \"merchants_category_2_2_rate\":merchants_cate_2_2_rate,\n#     \"merchants_category_2_3_rate\":merchants_cate_2_3_rate,\n#     \"merchants_category_2_4_rate\":merchants_cate_2_4_rate,\n#     \"merchants_category_2_5_rate\":merchants_cate_2_5_rate,\n#     \"merchants_category_4_Y_rate\":merchant_cate_4_Y_rate\n# }","3a80e04a":"# new_trans_merchants_by_card_id_df=pd.DataFrame(new_trans_merchants_by_card_id)","51df8e4c":"# new_trans_merchants_by_card_id_df=new_trans_merchants_by_card_id_df.reset_index()","f8470809":"# new_trans_merchants_by_card_id_df.head()","19ce4d87":"# with open('new_trans_merchants_by_card_id_df.pickle', 'wb') as f:\n#     pickle.dump(new_trans_merchants_by_card_id_df, f)","35bdbf4f":"# with open('new_trans_merchants_by_card_id_df.pickle', 'rb') as f:\n#     new_trans_merchants_by_card_id_df = pickle.load(f)","c665836e":"train_his_sub_mer = reduce_mem_usage(train_his_sub_mer)\ntrain_new_mer = reduce_mem_usage(train_new_mer)","08162bf1":"es = ft.EntitySet(id = 'card')\nes = es.entity_from_dataframe(entity_id = 'train', dataframe = train,index = 'card_id')\n","2f7762de":"es = es.entity_from_dataframe(entity_id = 'train_his_sub_mer', \n                              dataframe = train_his_sub_mer,\n                              make_index = True,\n                              index = \"train_his_sub_mer_id\")","bda1e920":"es = es.entity_from_dataframe(entity_id = 'train_new_mer', \n                              dataframe = train_new_mer,\n                              make_index = True,\n                              index = \"train_new_mer_id\")","9e301dc4":"r_train_his = ft.Relationship(es['train']['card_id'],\n                                   es['train_his_sub_mer']['card_id'])\nr_train_new = ft.Relationship(es['train']['card_id'],\n                                    es['train_new_mer']['card_id'])\n\n# Add the relationship to the entity set\nes = es.add_relationship(r_train_his)\nes = es.add_relationship(r_train_new)\nes","abd7be50":"features, feature_names = ft.dfs(entityset=es, target_entity='train', \n                                 max_depth = 2,\n                                 agg_primitives = ['mean', 'max', 'percent_true', 'last'],\n                                 trans_primitives = ['years', 'month', 'subtract', 'divide'])\n\nwith open('features.pickle', 'wb') as f:\n    pickle.dump([features, feature_names], f)\n\nfeatures.head()","d03e6064":"dic = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='train')\ndic","448ba471":"# with open('features.pickle', 'wb') as f:\n#     pickle.dump([features, feature_names], f)","fc95b984":"e = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='history')\ne","496451fb":"## check duplicates in merchants and train datasets, remove any duplicates","b4f2402e":"## import dataset","19904617":"## **one hot encode** for train_his_sub : authorized_flag; category_1; category_2; category_3","c8d82a44":"## drop non-important column for his, new and merchant","3f998f49":"## Automated Feature Engineering","87b1f869":"## rename column for merging simplicity","b6fac75f":"# done","71c56b7d":"## **subsample his transactions**","25c80b2d":"## same for new transaction","784e10d0":"## merge with merchants (X : his transaction, Y : merchant)","87370b75":"## extract history transactions record for training data","1b96a083":"## **one hot encode** for train_his_sub_mer(merchants part) : category_1; category_2; category_3"}}