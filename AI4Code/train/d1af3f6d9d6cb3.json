{"cell_type":{"402e61e0":"code","1511503c":"code","708af6bd":"code","693e3b85":"code","2d107a23":"code","cc9f46fd":"code","0b48eb61":"code","2133ef7b":"code","7712362e":"code","55d5f069":"code","32863b8b":"code","c7a08704":"code","01da0cab":"code","e6787a79":"code","4cde320f":"code","19f7791f":"code","57affe10":"code","c11f747c":"markdown","cfe6c1eb":"markdown","025ebbeb":"markdown","d6ab1449":"markdown","1e8eda8f":"markdown","f26019cd":"markdown","e2229c48":"markdown","ce18c187":"markdown","dccd21a1":"markdown","70756a73":"markdown","e87e4dd9":"markdown","97d765e7":"markdown","9c504e05":"markdown","e3073e6d":"markdown"},"source":{"402e61e0":"from keras.models import Input, Model\nfrom keras.layers import Dense\nfrom keras.datasets import fashion_mnist\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport json\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1511503c":"import os\nprint(os.listdir(\"..\/input\"))","708af6bd":"x_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")","693e3b85":"x_test = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")","2d107a23":"x_train = np.array(x_train)\nx_test = np.array(x_test)","cc9f46fd":"x_train = x_train.astype(\"float32\")\/255.0\nx_test = x_test.astype(\"float32\")\/255.0\nx_train = x_train[:,:-1]\nx_test = x_test[:,:-1]\nprint(x_train.shape)\nprint(x_test.shape)","0b48eb61":"print(x_train.shape,x_test.shape)","2133ef7b":"plt.imshow(x_train[4000].reshape(28,28))\nplt.show()\nplt.imshow(x_train[1500].reshape(28,28))\nplt.show()\nplt.imshow(x_train[150].reshape(28,28))\nplt.show()","7712362e":"input_img = Input(shape=(784,))\nencoded = Dense(32, activation=\"relu\")(input_img)\nencoded = Dense(16, activation=\"relu\")(encoded)","55d5f069":"decoded = Dense(32, activation=\"relu\")(encoded)\noutput_img = Dense(784, activation=\"sigmoid\")(decoded)","32863b8b":"autoencoder = Model(input_img, output_img)","c7a08704":"autoencoder.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")","01da0cab":"history = autoencoder.fit(x_train, x_train, \n                          epochs=200, \n                          batch_size=256, \n                          shuffle=True,\n                          validation_data=(x_train, x_train))","e6787a79":"plt.plot(history.history[\"loss\"], label=\"Train Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\nplt.show()","4cde320f":"x_test_pred = autoencoder.predict(x_test)","19f7791f":"plt.imshow(x_test[100].reshape(28,28))\nplt.title(\"Ger\u00e7ek Resim\")\nplt.show()\nplt.imshow(x_test_pred[100].reshape(28,28))\nplt.title(\"Auto Encoder \\nSonucu \u00c7\u0131kan Resim\")\nplt.show()","57affe10":"n = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(x_test_pred[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","c11f747c":"**AutoEncoder Modeli OLu\u015fturma A\u015famas\u0131**","cfe6c1eb":"**Verisetinin Y\u00fcklenmesi**","025ebbeb":"**Daha fazla resim ile g\u00f6rselle\u015ftirelim**\n\nKaynak: https:\/\/blog.keras.io\/building-autoencoders-in-keras.html","d6ab1449":"**K\u00fct\u00fcphanelerin Y\u00fcklenmesi**","1e8eda8f":"Sonu\u00e7 olarak modelimizin hi\u00e7 g\u00f6rmedi\u011fi resimler \u00fczerinde de kabul edilebilir sonu\u00e7lar verdi\u011fini resimleri g\u00f6rselle\u015ftirerek anlam\u0131\u015f olduk. Tabi burada yaz\u0131lanlar e\u011fitim ama\u00e7l\u0131 ve optimize edilmemi\u015f bir modelden ortaya \u00e7\u0131kt\u0131. Daha iyi sonu\u00e7lar elde edilmesi m\u00fcmk\u00fcn.","f26019cd":"**Sonu\u00e7ta ortaya \u00e7\u0131kan resimlere test setinden tahmin yap\u0131p bakal\u0131m**","e2229c48":"**Verinin Yeniden Boyutland\u0131r\u0131lmas\u0131**","ce18c187":"**Model Compile ve Train A\u015famalar\u0131**","dccd21a1":"**Verisetini \u00d6rnek Olarak G\u00f6rselle\u015ftirme**","70756a73":"**Sonu\u00e7lar\u0131n G\u00f6rselle\u015ftirilmesi**","e87e4dd9":"**Verinin Tipini D\u00f6n\u00fc\u015ft\u00fcme ve Scale \u0130\u015flemi**","97d765e7":"# Auto Encoders\n\nAuto Encoder modeli al\u0131nan bir girdiyi gizli katmanlarda daha az n\u00f6ron kullanarak s\u0131k\u0131\u015ft\u0131rarak veriyi en iyi temsil eden \u00f6zelliklerin \u00e7\u0131kar\u0131lmas\u0131na yard\u0131mc\u0131 olur. Daha sonra bu \u00f6zelliklerden ba\u015ftaki \u00e7\u0131ky\u0131t\u0131 en iyi verecek output layer a\u011f\u0131rl\u0131klar\u0131n\u0131 olu\u015fturmaya \u00e7al\u0131\u015f\u0131r. Yani girdi olarak bir resim dosyas\u0131 verdi\u011fimizde, bu resmi en iyi temsil eden \u00f6zellikleri \u00e7\u0131kar\u0131r ve daha sonra bu \u00f6zelliklerden ayn\u0131 resmi olu\u015fturmaya \u00e7al\u0131\u015f\u0131r. Sadece resim de\u011fil, sinyal i\u015flemede g\u00fcr\u00fclt\u00fc(noise) kald\u0131rma i\u015flemlerinde de kullan\u0131labilir. Veya bir zaman serisinde verinin genel e\u011filimlerini bulur ve verideki g\u00fcr\u00fclt\u00fcy\u00fc ortadan kald\u0131rabilir. Bunun gibi bir\u00e7ok kullan\u0131m alan\u0131 olsa da daha iyi anlamak ad\u0131na bu yap\u0131n\u0131n g\u00f6rsel haline bakmak daha iyi anlamam\u0131za yard\u0131mc\u0131 olabilir.\n![alt text](https:\/\/static.packt-cdn.com\/products\/9781787121089\/graphics\/B12043_04_06.png)\n\nResmin al\u0131nd\u0131\u011f\u0131 kaynak: https:\/\/subscription.packtpub.com\/book\/big_data_and_business_intelligence\/9781787121089\/4\/ch04lvl1sec51\/setting-up-stacked-autoencoders","9c504e05":"Encoder ve Decoder layerlar\u0131n\u0131n olu\u015fturulmas\u0131","e3073e6d":"**AutoEncoder Modeli Olu\u015fturma**\n\n\u0130nput k\u0131sm\u0131ndan output k\u0131sm\u0131na kadar olu\u015fturdu\u011fumuz layerlar\u0131 bir model i\u00e7ine depolay\u0131p AutoEncoder kurmu\u015f olaca\u011f\u0131z."}}