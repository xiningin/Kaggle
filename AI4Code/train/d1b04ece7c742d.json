{"cell_type":{"3be845ec":"code","0a9384b4":"code","eaae1923":"code","0c8af0e1":"code","942245d6":"code","4a311b83":"code","facfdc21":"code","79f66e43":"code","493c86ba":"markdown","0db10a74":"markdown","880f578f":"markdown","3fc28fbd":"markdown","012b93ff":"markdown","4aeac094":"markdown"},"source":{"3be845ec":"!pip install kaggle-environments --upgrade","0a9384b4":"%%writefile submission.py\n\nimport numpy as np\nfrom scipy import stats\nimport random\n\nclass Bandit:\n  def __init__(self, k=10, eps=0.2, lr=0.1, ucb=False, soft_max=False, c=2):\n    \"\"\"\n    k: the number of bandits\n    eps: e-greedy parameter\n    lr: step size in the incremental formula\n    ucb: upper confident bound\n    c: a parameter of ucb\n    \"\"\"\n    self.k = k\n    self.eps = eps\n    self.lr = lr\n    self.initial_values = [] #optimistic initial value of each arm\n    for i in range(self.k):\n      self.initial_values.append(np.random.randn() + 1) #normal distribution\n    #for ucb\n    self.ucb = ucb\n    self.times = 0\n    self.c = c\n    #for softmax action selection\n    self.soft_max = soft_max\n\n    #columns: Observation and avg reward\n    self.record = np.zeros((self.k, 2))\n    \n    #total reward\n    self.total_reward = 0\n    \n  def get_reward(self, observation):\n    no_reward_step = 0.3\n    last_reward = observation[\"reward\"] - self.total_reward\n    self.total_reward = observation[\"reward\"]\n    \n    if last_reward > 0:\n        return last_reward\n    return no_reward_step\n  \n  def update_record(self, action, r):\n    #update avg reward using incremental formula\n#     self.record[action, 1] += self.lr*(r-self.record[action, 1])\n    #update avg reward using original fomular\n    new_avg_reward = (self.record[action, 0] * self.record[action, 1] + r) \/ (self.record[action, 0]+1)\n    self.record[action, 1] = new_avg_reward\n    #update observations\n    self.record[action, 0] += 1\n  \n  def softmax(self, av, tau=1.12):\n    softm = np.exp(av\/tau)\/np.sum(np.exp(av\/tau))\n    return softm\n\n  def choose_action(self):\n    if self.soft_max:\n      p = self.softmax(self.record[:, 1], tau=0.7)\n      action = np.random.choice(np.arange(self.k), p=p)  \n      return action\n\n    #explore\n    if random.random() > self.eps:\n      action=np.random.randint(self.k)\n    #exploit\n    else:\n      if self.ucb:\n        if self.times == 0:\n          action = np.random.randint(self.k)\n        else:\n          confidence_bound = self.record[:, 1] + self.c*np.sqrt(np.log(self.times)\/(self.times+0.1))\n          action = np.argmax(confidence_bound)\n      else:\n        action=np.argmax(self.record[:, 1], axis=0)\n\n    return action\n\n  def one_play(self, observation):\n    action = self.choose_action()\n\n    self.times = observation.step #update for ucb\n    r = self.get_reward(observation)\n#     r += self.initial_values[action] #optimistic initial value\n    self.update_record(action, r)\n    \n    return int(action)\n\nbandit = None\ndef multi_armed_bandit_agent(observation, configuration):\n    global bandit\n    if observation.step == 0:        \n        bandit = Bandit(k=configuration['banditCount'], ucb=True, soft_max=False)\n        action = bandit.one_play(observation)\n    else:\n        action = bandit.one_play(observation)\n    \n    return action\n\n\n","eaae1923":"# # %%writefile submission_contextualbandit.py\n\n# import torch as th\n# import numpy as np\n# from torch.autograd import Variable\n# from matplotlib import pyplot as plt\n# import random\n\n# class ContextBandit:\n#     def __init__(self, arms=10):\n#         self.arms = arms\n#         self.init_distribution(arms)\n#         self.update_state()\n        \n#     def init_distribution(self, arms):\n#         #num states equals num arms to keep things simple\n#         self.bandit_matrix = np.random.rand(arms, arms)\n        \n#     def reward(self, prob, n=10):\n#         reward = 0\n#         for i in range(n):\n#             if random.random() < prob:\n#                 reward += 1\n#         return reward\n#     def get_state(self):\n#         return self.state\n    \n#     def update_state(self):\n#         self.state = np.random.randint(0, self.arms)\n        \n#     def get_reward(self, arm):\n#         return self.reward(self.bandit_matrix[self.get_state()][arm])\n    \n#     def choose_arm(self, arm):\n#         reward = self.get_reward(arm)\n#         self.update_state()\n#         return reward\n\n\n# def softmax(av, tau=0.7):\n#     n = len(av)\n#     probs = np.zeros(n)\n#     for i in range(n):\n#         softm = (np.exp(av[i]\/tau)\/np.sum(np.exp(av[:]\/tau)))\n#         probs[i] = softm\n#     return probs\n\n# def one_hot(N, pos, val=1):\n#     one_hot_vec = np.zeros(N)\n#     one_hot_vec[pos] = val\n#     return one_hot_vec\n\n# arms = 10\n# # N is the batch size, D_in is input dimension\n# # H is hidden dimension D_out is output dimension\n# N, D_in, H, D_out = 1, arms, 100, arms\n\n# model = th.nn.Sequential(\n#     th.nn.Linear(D_in, H),\n#     th.nn.ReLU(),\n#     th.nn.Linear(H, D_out),\n#     th.nn.ReLU(),\n# )\n# loss_fn = th.nn.MSELoss(size_average=False)\n\n# env = ContextBandit(arms)\n\n# def train(env):\n#     epochs = 50000\n#     # one-hot encode current state\n#     cur_state = Variable(th.Tensor(one_hot(arms, env.get_state())))\n#     reward_hist = np.zeros(50)\n#     reward_hist[:] = 5\n#     runningMean = np.average(reward_hist)\n#     lr = 1e-2\n#     optimizer = th.optim.Adam(model.parameters(), lr=lr)\n#     plt.xlabel(\"Plays\")\n#     plt.ylabel(\"Mean Reward\")\n#     for i in range(epochs):\n#         y_pred = model(cur_state) # produce reward prediction\n#         av_softmax = softmax(y_pred.data.numpy(), tau=2.0) #turn reward distribution into probability distribution\n#         av_softmax \/= av_softmax.sum() #make sure total prob adds to 1\n#         action = np.random.choice(arms, p=av_softmax)\n#         cur_reward = env.choose_arm(action)\n#         one_hot_reward = y_pred.data.numpy().copy()\n#         one_hot_reward[action] = cur_reward\n#         reward = Variable(th.Tensor(one_hot_reward))\n#         loss=loss_fn(y_pred, reward)\n\n#         if i%50 == 0:\n#             runningMean = np.average(reward_hist)\n#             reward_hist[:] = 0\n#             plt.scatter(i, runningMean)\n#         reward_hist[i%50]=cur_reward\n#         optimizer.zero_grad()\n#         loss.backward()\n\n#         optimizer.step()\n#         cur_state = Variable(th.Tensor(one_hot(arms, env.get_state())))\n        \n#         th.save(model.state_dict(),'contextual_weights.pt')\n\n# train(env)","0c8af0e1":"%%writefile submission2new.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n\n    no_reward_step = 0.3\n    decay_rate = 0.97 # how much do we decay the win count after each call\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward\n        else:\n            bandit_state[observation.lastActions[player]][1] += no_reward_step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    last_step = best_agent\n    return best_agent","942245d6":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","4a311b83":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nfor i in range(5):\n    env.run([\"submission.py\", \"..\/input\/santa-2020\/submission.py\"])\n    p1_score = env.steps[-1][0]['reward']\n    p2_score = env.steps[-1][1]['reward']\n    env.reset()\n    print(f\"Round {i+1}: {p1_score} - {p2_score}\")","facfdc21":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nfor i in range(5):\n    env.run([\"submission.py\", \"submission2new.py\"])\n    p1_score = env.steps[-1][0]['reward']\n    p2_score = env.steps[-1][1]['reward']\n    env.reset()\n    print(f\"Round {i+1}: {p1_score} - {p2_score}\")","79f66e43":"env.reset()\nenv.run([\"submission.py\", \"submission.py\"])\n# env.render(mode=\"ipython\", width=800, height=700)\n\np1_score = env.steps[-1][0]['reward']\np2_score = env.steps[-1][1]['reward']\nprint(f\"Round: {p1_score} - {p2_score}\")","493c86ba":"This notebook shows how to use multi-armed bandit. In this notebook, I will present softmax strategy and upper confidence bound strategy for selecting an action.\nIf you find it helpful, give it a vote :3","0db10a74":"# Test with default Agent","880f578f":"# Random Agent","3fc28fbd":"# Self Test","012b93ff":"# Contextual Bandit","4aeac094":"# Test with other Agent"}}