{"cell_type":{"0e8f87e3":"code","0e1e7b42":"code","1285b5d5":"code","a477764b":"code","95938f6d":"code","3e775450":"code","2837d08c":"code","3c3503af":"code","96406489":"code","b318ec05":"code","0cd68488":"code","5984e706":"code","a8d0ef1c":"code","3067a4e9":"code","6daad71b":"code","0c4eaeaf":"code","0c0a8121":"code","c9dd0568":"code","8d864e75":"code","e4c6a11a":"code","60729a1a":"code","6f18ec6c":"code","34771407":"code","694c3cb3":"code","48e17f7e":"code","d0f673b3":"code","45f29346":"markdown","fae8e161":"markdown","bd519ec6":"markdown","2c9a0f32":"markdown","73931f3d":"markdown","51966eca":"markdown","a8cd10e1":"markdown","aa11828d":"markdown","b5fbaff5":"markdown","f24f76f0":"markdown","2495af24":"markdown","40506fcd":"markdown","c48bb3b9":"markdown","f61d972e":"markdown","363a39c8":"markdown","c6b2969f":"markdown","16ce777e":"markdown","9610bda6":"markdown","24d4a8d2":"markdown","4d4bf60c":"markdown"},"source":{"0e8f87e3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten","0e1e7b42":"train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","1285b5d5":"train.shape, test.shape","a477764b":"X=train.drop(['label'],axis=1)\ny=train['label']\n\n\n# check the shape\nX.shape, y.shape","95938f6d":"X_test = test.drop(['label'],axis=1)\ny_test = test['label']\n\n# check the shape \nX_test.shape,y_test.shape","3e775450":"y.value_counts()","2837d08c":"y_test.value_counts()","3c3503af":"plt.subplots(figsize = (10,8))\nplt.title('Counts in numbers to their labels ')\nsns.countplot(x=y, data=train)\nplt.show()","96406489":"plt.subplots(figsize = (10,8))\nplt.title('Counts in numbers to their labels ')\nsns.countplot(x=y_test, data=train)\nplt.show()","b318ec05":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=99)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","0cd68488":"X_train = X_train.values.astype('float32')\nX_val = X_val.values.astype('float32')\nX_test = X_test.values.astype('float32')","5984e706":"X_train\nX_val\nX_test","a8d0ef1c":"X_train = X_train.reshape(X_train.shape[0],28,28)\nX_val = X_val.reshape(X_val.shape[0],28,28)\nX_test = X_test.reshape(X_test.shape[0],28,28)","3067a4e9":"X_train.shape, X_val.shape, X_test.shape, ","6daad71b":"X_train.max(),X_train.min()","0c4eaeaf":"X_train = X_train\/255\nX_val = X_val\/255\nX_test = X_test\/255\n\n# cheking the value range \nX_test.max(), X_test.min()","0c0a8121":"# import the model\nmodel=Sequential()\n# import the layers\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))","c9dd0568":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","8d864e75":"# checking the summary\nmodel.summary()","e4c6a11a":"%%time\nhistory=model.fit(X_train,y_train, batch_size=64, epochs=10, validation_data=(X_val,y_val))","60729a1a":"accuracy,loss=model.evaluate(X_test,y_test)\naccuracy,loss","6f18ec6c":"# plot the figure now\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.show()","34771407":"# plot confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","694c3cb3":"y_pred = model.predict_classes(X_test)\ny_pred","48e17f7e":"class_names=[ 'T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nclass_names","d0f673b3":"mat=confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(conf_mat=mat, class_names= class_names,show_normed=True, figsize=(7,7))","45f29346":"Rechecking the shape of the data","fae8e161":"Now we will check the value range of the data","bd519ec6":"Now let  us make the prediction using confusion matrix","2c9a0f32":"checking the same by plotting the data","73931f3d":"Here we can see that our accuracy comes to 91% but our validation accuracy comes to 88% , so here we can say that our model is overfitting\n","51966eca":"Plotting the same","a8cd10e1":"Splitting the data into independent and dependent variables","aa11828d":"Now let us check whether our target variable is imbalanced or not in training data","b5fbaff5":"checking the shape of the data","f24f76f0":"check the shape of the train and test data","2495af24":"This is an example of using simple ANN on MNIST Dataset in the simplest way possible by me.\n\nHope everyone watching this likes it.\n\nImport the important libraries","40506fcd":"Now let us check whether our target variable is imbalanced or not in testing data","c48bb3b9":"now let us divide the same for the testing data","f61d972e":"We can easily conclude from the above that the data is not unbalanced\n\nNow we will split the data into training and testing","363a39c8":"Now our data is scaled, now we will build the model","c6b2969f":"Our data is in range of 0 to 255, we will need to convert the data in the range of 0 to 1, so that our data\ncan be processed according to the ANN architecture.\n\nWe will scale the data by dividing the data by 255","16ce777e":"Here the shape of our data is not according to the ANN architecture, so we will reshape the data into CNN architecture that is (images,rows,cols,channels) Here the images will be the no of the images used , rows and columns will be the pixels of the images mentioned in the dataset descriptions which are 28 * 28 . First we will change the data into an array format.\n\nwe will change the data of training, validation and testing data","9610bda6":"Import the data from train and test csv file","24d4a8d2":"Compiling the model","4d4bf60c":"fitting the model"}}