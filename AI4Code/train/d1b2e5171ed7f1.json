{"cell_type":{"17a09bd5":"code","90e6b67e":"code","a06856d6":"code","8267aa24":"code","b90395e7":"code","73a075e9":"code","be261b86":"code","31b6616a":"code","63f7c922":"code","c86ee578":"code","2aceb0a7":"code","642b73f8":"code","e0c1875f":"code","adbdf456":"code","873e75e7":"code","dc7c779c":"code","c9249c18":"code","ec592042":"code","d567051c":"code","9da7c8c0":"code","b6bfb85f":"code","d71339ee":"code","6b10b442":"code","61145d6f":"code","49fb974b":"code","68f3e74c":"code","0cb87adf":"code","f199f6c1":"code","38a4fdb8":"code","b71db6b3":"code","8752799d":"code","62689936":"code","1b617f93":"code","ac4b9825":"code","e0b0abf5":"code","f1f758e1":"code","52cac275":"code","ac85fb05":"code","a8df0190":"code","a22cd5c7":"code","edcada43":"code","b52f1204":"code","88550830":"code","3ce643aa":"code","1f4672c4":"code","9f53e72e":"code","5a0d7929":"code","93ae9b97":"code","2234deb7":"code","370bf6d4":"code","b6ac4e19":"code","41bb02e2":"code","e0e895d9":"code","f324481c":"code","0ac39588":"code","5c9897a6":"code","b30eb7b9":"code","1d564c84":"code","faffcbf1":"code","644dba2d":"code","34ac1c42":"code","83784321":"code","512a7ff9":"code","bd81931c":"code","ac71b1d6":"code","97bc6dcb":"code","10166924":"code","4cc03695":"code","8cd08070":"code","26286097":"code","486599aa":"code","5fd76e05":"code","bd8996c6":"code","ed3222a7":"code","81bf50e1":"code","ca6eae23":"code","ef1b26ce":"code","fa71ec17":"code","28722f9e":"code","c2208831":"code","90c9fabb":"code","f2e7f348":"code","e008337b":"code","52c7cc48":"code","4a7d17ae":"code","212198bf":"code","f07b29dd":"code","c1a568bf":"code","e8e5dc22":"code","013d1e45":"code","87bcd423":"code","dbf689b1":"code","8e04993d":"code","34ee8786":"code","18bf8674":"code","c1f32233":"code","208e0852":"code","066da98d":"code","dd2c53f4":"code","53022574":"code","7eedf7a5":"code","74ac5bf0":"code","d0b72965":"code","88f0af32":"code","69cbd213":"code","53715713":"code","e94996f7":"code","fd52ce11":"code","2f41ed16":"code","7727f481":"code","7c990b24":"code","a0bc343b":"code","5fd8d164":"code","7c848a8d":"code","708d6c27":"code","7cba3fe4":"code","39c0bdbb":"code","25d564d9":"code","1ed62e6d":"code","43602359":"code","76e5a0d3":"code","3c5ab0b6":"code","ab738d70":"code","c6dca9bb":"code","c32f529f":"code","6ae91abf":"code","99af1ec8":"code","8aae3300":"code","114bcc48":"code","73dc7f21":"code","000dba5b":"code","fa93db92":"code","dc996aab":"code","95a9e281":"code","1e4abf9e":"code","68bbacd1":"code","8bbfdc1e":"code","1d8d579a":"code","4bcb484e":"code","b1b32aff":"code","2e469ff7":"code","adac34b7":"code","291d282c":"code","b641391e":"code","13cef281":"code","1d5a7881":"code","902f0cde":"code","783086cb":"code","7b39e519":"code","3bd0f649":"code","fe23759d":"code","9a70f220":"code","063651b7":"code","bc8dee81":"code","9ba3b5bc":"code","8bf02257":"code","b4c7c81b":"code","0c885664":"code","32c66a5a":"code","60a44d5c":"code","46876ecf":"code","41e92d5b":"code","075ca246":"code","4a7f8077":"code","25f99ebe":"code","2ec36315":"code","64972308":"code","6f86b48d":"code","e89a1f34":"code","576b1f22":"code","c67b5ec9":"code","64f38619":"code","8b9114dc":"code","5c288a12":"code","141798b3":"code","bfb84c90":"code","6b73dfad":"code","41cc6c1e":"code","7836b2d2":"code","0a88ef3b":"code","3d38ebfb":"code","6dfe4f9a":"code","d682897a":"code","c75c292c":"code","b1bb4440":"code","4087bede":"code","50948cae":"code","53b0c775":"code","ad99a817":"code","9876f6d0":"code","0c607369":"code","b09697fd":"code","b255d151":"code","a28d5d58":"code","a467731a":"code","84f68de9":"code","0b79c132":"code","0ab3d601":"code","3c15f553":"code","f0456b5a":"code","0eed3aaa":"code","2bdb2317":"code","a1909187":"code","3b43724d":"code","5a512251":"code","0378a473":"code","dc7e4b55":"code","3f644a9f":"code","d1058e70":"code","06825cda":"code","41a17b9f":"code","8acbaa2a":"code","479d4b72":"code","122ce10c":"code","2b538442":"code","ac7dbb34":"code","dcc0b797":"code","d5a1632c":"code","7a4753e7":"code","fc9c3c4d":"code","ce400a41":"code","6231d782":"code","e2ae2e3d":"code","54493d13":"code","cfe8ca42":"code","34713cce":"code","7b9bad0b":"code","1a4fe675":"code","48c98b8a":"code","cc24c198":"code","8763ac43":"code","a3397dfc":"code","537f0fc4":"code","640028a3":"code","ae379b0f":"code","e2bccfbb":"code","39bf5e9a":"code","d61916b7":"code","b5e9b32f":"code","da4eb8fa":"code","03b5d3e5":"code","28df8451":"code","a2f191d7":"code","bdd1726e":"code","a34c58cc":"code","76d27d78":"code","e2f5ebda":"code","7ba5a994":"code","0ab2f21c":"code","e5d0500e":"code","b1271e8f":"code","4c1f726e":"code","3e3e1501":"code","76ef174b":"code","876b305b":"code","6a8f2aa1":"code","1d1f479d":"code","54970e87":"code","039dae5f":"code","8e4352af":"code","e1a26aac":"code","c505ecaa":"code","39b05a39":"code","2cdbb972":"code","49b5bfa9":"code","4ed3bb9a":"code","7fbe0037":"code","38e2353c":"code","0650d914":"code","11aaf648":"code","34d47b22":"code","b61e9a68":"code","98051bc5":"code","301d2f6a":"code","316b16af":"code","e34275be":"code","6618a877":"code","8c03de1e":"code","957ecee5":"code","2c00c635":"code","33416643":"code","b30d9a9a":"code","be9c09e5":"code","7db9d8b8":"code","06bc996e":"code","442ab264":"code","4ef671bb":"code","de63af83":"code","68d78886":"code","be1ee9fc":"code","ee91fc9c":"code","4742f336":"code","6e1e862e":"code","286ce2f0":"code","4bd68ea7":"code","bf4ce079":"code","39c258e5":"code","b3bccc4e":"code","e562ceb2":"code","ba9ec5d9":"code","1bb98496":"code","f7c79d2f":"code","f3f9945f":"code","1d0a12bd":"code","09514b9b":"code","a076941b":"code","5f42b4d9":"code","0ec85be5":"code","c2e51645":"code","02e51fb1":"code","4b7390c5":"code","992be1e4":"code","d513e2bd":"code","4d055b44":"code","7c20fbf9":"code","b411ebfd":"code","7bbf800d":"code","c877c99e":"code","1ed912b9":"code","faa83691":"code","83c388d3":"code","49c45620":"code","1e4ba84d":"code","fd9a8f2c":"code","c25693de":"code","6af81bd8":"markdown","ca7821f1":"markdown","19b1fe50":"markdown","c3926c2c":"markdown","5dac3796":"markdown","7fb8baea":"markdown","0b653ac4":"markdown","b1619add":"markdown","1050116d":"markdown","53df6d77":"markdown","715374d1":"markdown","cee08e97":"markdown","ed002891":"markdown","f9a70323":"markdown","883d1a87":"markdown","e4877d9d":"markdown","32987288":"markdown","76ed3643":"markdown","1fd9223c":"markdown","4d23a356":"markdown","1458cc98":"markdown","595bb792":"markdown","b44976f4":"markdown","f3d2ef3c":"markdown","be5600da":"markdown","f58add5e":"markdown","7065b726":"markdown","ee6632d8":"markdown","11fcae2e":"markdown","8333617e":"markdown","95fa79cf":"markdown","ebec4b9f":"markdown","eca4228b":"markdown","f737adc6":"markdown","f20be8f4":"markdown","b7cf64c8":"markdown","45c50ec0":"markdown","cadfb0cf":"markdown","dac6e3cb":"markdown","63cb9efd":"markdown","894e5e05":"markdown","21cd157e":"markdown","ad9fe342":"markdown","e4f14cd1":"markdown","f967736e":"markdown","6395d19d":"markdown","2b8b6f9e":"markdown","e2aee4df":"markdown","556ffe2d":"markdown","162d8bcc":"markdown","866c2734":"markdown","6df9f984":"markdown","24fcfe77":"markdown","efbd3f74":"markdown","bcde1fda":"markdown","e4a267da":"markdown","ad38669c":"markdown","182cb051":"markdown","a2ebb42f":"markdown","4c179a09":"markdown","a7d587c5":"markdown","ddf5d672":"markdown","72595216":"markdown","6287df51":"markdown","74008241":"markdown","c295341e":"markdown","8014e32c":"markdown","1af27ce1":"markdown","17657b4a":"markdown","70db702a":"markdown","18a3f86a":"markdown","7b769b6d":"markdown","6535858e":"markdown","12047f30":"markdown","eb883622":"markdown","43300b41":"markdown","693c83f0":"markdown","d07aef5c":"markdown","3c824202":"markdown","0cb8c958":"markdown","b85ad3a2":"markdown","cb8512e1":"markdown","d5dca452":"markdown","8a8503b1":"markdown","402cc0aa":"markdown","78cf5998":"markdown","10e90f20":"markdown","31368e19":"markdown","361eb3e4":"markdown","43647ef1":"markdown","ee4548b1":"markdown","44b010e5":"markdown","9fd6852a":"markdown","3975745e":"markdown","59dde1f3":"markdown","fe1a15ac":"markdown","ac848bf3":"markdown","a0b05d53":"markdown","8338a23a":"markdown","45a4f168":"markdown","c0f3cd80":"markdown","229bda97":"markdown","9c24051b":"markdown","cd644ab0":"markdown","de1f1e43":"markdown","ad339396":"markdown","2565817c":"markdown","9516c4d5":"markdown","96b7a502":"markdown","cb665835":"markdown","12723e64":"markdown","6228dd42":"markdown","4bb7e020":"markdown","d9097ce1":"markdown","e8709064":"markdown","40002d52":"markdown","34b94ed8":"markdown","7754e95d":"markdown","1e94e4c3":"markdown","cba937a0":"markdown","ebc116d0":"markdown","689fd5c9":"markdown","39d6b19c":"markdown","6a8515c6":"markdown","68062d65":"markdown","2b90e4ab":"markdown","c5eeedab":"markdown","fd6c67eb":"markdown","78e57e6c":"markdown","3186290d":"markdown","44f1d7b2":"markdown","b448e98b":"markdown","7aa40b04":"markdown","3606280d":"markdown","e665bed9":"markdown","20d42277":"markdown","31b2926f":"markdown","43bf739d":"markdown","c55072fa":"markdown","916abb85":"markdown","36560405":"markdown","32dea53b":"markdown","057f099c":"markdown","214e06b1":"markdown","163c62e0":"markdown","86d4e1f7":"markdown","ff458ef3":"markdown","9a08bfd9":"markdown","222e24fe":"markdown","d597ba52":"markdown","1420be52":"markdown","4a86ef27":"markdown","3dac90ec":"markdown","eeb591cc":"markdown","96103fc8":"markdown","2e900cfe":"markdown","a097d603":"markdown","116c68cf":"markdown","e8abd709":"markdown","6de39bae":"markdown","9e75328a":"markdown","21325ace":"markdown","3149e913":"markdown","6d634fd1":"markdown","a18025fa":"markdown","d1c6ffeb":"markdown","99407d54":"markdown","7ed7c783":"markdown","23a37c46":"markdown","4e9ddfe0":"markdown","6340f6a9":"markdown","16e802ef":"markdown","d0e17456":"markdown","a734cba3":"markdown","838c9ea8":"markdown","48d2716c":"markdown","602a0b2f":"markdown","ba55193d":"markdown","950eb5d7":"markdown","c0cac2d7":"markdown","8fd9136d":"markdown","a6d3c1b5":"markdown","56fde0c1":"markdown","22f17a21":"markdown","d795c743":"markdown","c7c53cae":"markdown","a773f94c":"markdown","594a5ee8":"markdown","d38d0b72":"markdown","bc9aa624":"markdown","6683a316":"markdown","70b4c24c":"markdown","dde5717f":"markdown","b5d3081c":"markdown","565f9b1b":"markdown","0ad98bb5":"markdown","c01bec53":"markdown","a85c01d1":"markdown","1c6f0ca2":"markdown","8fe24494":"markdown","e5c71979":"markdown","a5dc2e3f":"markdown","08fbd78a":"markdown","89c1d82a":"markdown","7880b0bc":"markdown","7045ae6f":"markdown","f062e50e":"markdown","29dd0220":"markdown","3b4d49d6":"markdown","eb117295":"markdown","d91aa570":"markdown","991a4535":"markdown","efa3a109":"markdown","474d9ee1":"markdown","70e84b98":"markdown","efd6c5f4":"markdown","af3d15ed":"markdown","56fe760d":"markdown","2f0dd90d":"markdown","ca074d68":"markdown","184ebc67":"markdown","21181d16":"markdown","16eb898d":"markdown"},"source":{"17a09bd5":"# Importing liberaries for data preprocessing, visualization, modeling and scoring.\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport sklearn.ensemble as ens\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgb\nimport sklearn.feature_selection\nimport sklearn.metrics\nfrom sklearn.preprocessing import MaxAbsScaler, StandardScaler, Normalizer, LabelEncoder, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n%matplotlib inline\nsns.set_style('white')","90e6b67e":"labelled = pd.read_csv('..\/input\/train.csv') # Labelled Data for training, validation, and model assessment. ","a06856d6":"unlabelled = pd.read_csv('..\/input\/test.csv') # Unlabelled Data for final submission.","8267aa24":"# Keep PassengerId for final submission in seperate variable.\npassengerID = unlabelled[['PassengerId']]","b90395e7":"data = pd.concat([labelled, unlabelled], axis= 0, sort= False)","73a075e9":"data.head()","be261b86":"sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap= 'viridis')","31b6616a":"data.info()","63f7c922":"sns.countplot(data = data, x= 'Survived')","c86ee578":"sns.countplot(data = data, x= 'Survived', hue= 'Sex')\nplt.legend(loc =(1.1,0.9)),","2aceb0a7":"sns.countplot(data = data, x='Survived', hue='Pclass')","642b73f8":"sns.distplot(data['Age'].dropna(), kde = False, bins = 35)","e0c1875f":"sns.countplot(x = 'SibSp', data = data)","adbdf456":"sns.countplot(data= data.dropna(), x='Pclass')","873e75e7":"sns.countplot(data= data, x='Pclass', hue= 'Sex')","dc7c779c":"sns.boxplot(data= data.dropna(), x='Pclass', y= 'Fare')","c9249c18":"data.describe()","ec592042":"class_mean_age = data.pivot_table(values='Age', index='Pclass', aggfunc='median')","d567051c":"null_age = data['Age'].isnull()","9da7c8c0":"data.loc[null_age,'Age'] = data.loc[null_age,'Pclass'].apply(lambda x: class_mean_age.loc[x] )","b6bfb85f":"data.Age.isnull().sum()","d71339ee":"class_mean_fare = data.pivot_table(values= 'Fare', index= 'Pclass', aggfunc='median')","6b10b442":"null_fare = data['Fare'].isnull()","61145d6f":"data.loc[null_fare, 'Fare'] = data.loc[null_fare, 'Pclass'].apply(lambda x: class_mean_fare.loc[x] )","49fb974b":"data.Fare.isnull().sum()","68f3e74c":"data.Embarked.value_counts()","0cb87adf":"data['Embarked'] = data.Embarked.fillna('S')","f199f6c1":"data.Embarked.isnull().sum()","38a4fdb8":"data['Title'] = data.Name.apply(lambda x : x[x.find(',')+2:x.find('.')])","b71db6b3":"data.Title.value_counts()","8752799d":"rare_titles = (data['Title'].value_counts() < 10)","62689936":"data['Title'] = data['Title'].apply(lambda x : 'Other' if rare_titles.loc[x] == True else x)","1b617f93":"data['FamilySize'] = data['SibSp'] + data['Parch'] + 1","ac4b9825":"data['IsAlone'] = 0","e0b0abf5":"data['IsAlone'].loc[ data['FamilySize'] == 1] = 1","f1f758e1":"data['AgeBins'] = 0","52cac275":"data['AgeBins'].loc[(data['Age'] >= 11) & (data['Age'] < 20)] = 1\ndata['AgeBins'].loc[(data['Age'] >= 20) & (data['Age'] < 60)] = 2\ndata['AgeBins'].loc[data['Age'] >= 60] = 3","ac85fb05":"data['FareBins'] = pd.qcut(data['Fare'], 4)","a8df0190":"data.columns","a22cd5c7":"data.drop(columns=['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare', 'SibSp', 'Parch'], inplace= True)","edcada43":"data = pd.get_dummies(\n    data, columns=['Embarked', 'Sex', 'Title'], drop_first=True)","b52f1204":"label = LabelEncoder()\ndata['FareBins'] = label.fit_transform(data['FareBins'])","88550830":"data.head(7)","3ce643aa":"labelled = data[data.Survived.isnull() == False].reset_index(drop=True)\nunlabelled = data[data.Survived.isnull()].drop(columns = ['Survived']).reset_index(drop=True)","1f4672c4":"labelled['Survived'] = labelled.Survived.astype('int64')","9f53e72e":"scalers = [MinMaxScaler(), MaxAbsScaler(), StandardScaler(), RobustScaler(),\n            Normalizer(), QuantileTransformer(), PowerTransformer()]","5a0d7929":"scaler_score = {}\nlabelled_copy = labelled.copy(deep= True) # Creat a copy of the original Labelled DF.\nfor scaler in scalers:\n    scaler.fit(labelled_copy[['FamilySize']])\n    labelled_copy['FamilySize'] = scaler.transform(labelled_copy[['FamilySize']])\n    lr = LogisticRegressionCV(cv = 10, scoring= 'accuracy')\n    lr.fit(labelled_copy.drop(columns=['Survived']), labelled_copy.Survived)\n    score = lr.score(labelled_copy.drop(columns=['Survived']), labelled_copy.Survived)\n    scaler_score.update({scaler:score})","93ae9b97":"scaler_score","2234deb7":"scaler = StandardScaler()\nscaler.fit(labelled[['FamilySize']])\nlabelled['FamilySize'] = scaler.transform(labelled[['FamilySize']])\nunlabelled['FamilySize'] = scaler.transform(unlabelled[['FamilySize']])","370bf6d4":"x_train, x_other, y_train, y_other = train_test_split(\n                labelled.drop(columns=['Survived']), labelled.Survived, train_size=0.7)","b6ac4e19":"x_valid, x_test, y_valid, y_test = train_test_split(\n                                    x_other, y_other, train_size=0.5)","41bb02e2":"features = labelled.drop(columns=['Survived'])\ntarget = labelled.Survived","e0e895d9":"logistic_reg = LogisticRegressionCV(cv= 7)","f324481c":"threshold = np.arange(1, 10, 0.5) *1e-1","0ac39588":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    logistic_reg.fit(selected_features, target)\n    y_pred = logistic_reg.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(np.arange(1, 10, 0.5) *1e-1, np.array(scores))","5c9897a6":"print('The highest accuracy score is:', np.max(np.array(scores)))","b30eb7b9":"number_of_features = list(range(1,13))","1d564c84":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    logistic_reg.fit(selected_features, target)\n    y_pred = logistic_reg.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(list(range(1,13)), scores_k)","faffcbf1":"print(\"Maximum accuracy score is :\", max(scores_k))","644dba2d":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","34ac1c42":"selector = sklearn.feature_selection.RFECV(logistic_reg, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","83784321":"print(\"Optimal number of features : %d\" % selector.n_features_)","512a7ff9":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","bd81931c":"threshold = np.arange(1, 5, 0.1) *1e-1","ac71b1d6":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(logistic_reg, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    logistic_reg.fit(selected_features, target)\n    y_pred = logistic_reg.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(np.arange(1, 5, 0.1) *1e-1, scores_sfm)","97bc6dcb":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","10166924":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","4cc03695":"# Fit the model with features selected by SelectFromModel method and the training set\nselector = sklearn.feature_selection.SelectFromModel(logistic_reg, threshold= 0.25)\nselector.fit(features, target)\nlr_selected_features = selector.get_support()","8cd08070":"logistic_reg = LogisticRegressionCV(\n    Cs=1, cv= 7, scoring='accuracy', max_iter=1000, refit=True)","26286097":"lr_parameters_1 = {'solver': ['liblinear', 'saga'], 'penalty': ['l1']}\nlr_parameters_2 = {'solver': ['newton-cg', 'lbfgs', 'sag'], 'penalty': ['l2']}","486599aa":"rs_lr = RandomizedSearchCV(logistic_reg, param_distributions= lr_parameters_2, n_iter= 100)","5fd76e05":"rs_lr.fit(x_train.loc[:, lr_selected_features], y_train)","bd8996c6":"print('Best Parameters are:\\n', rs_lr.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_lr.best_score_)","ed3222a7":"print('Validation accuracy score is:\\n', rs_lr.score(\n    x_valid.loc[:, lr_selected_features], y_valid))","81bf50e1":"param_name = 'Cs'\nparam_range = [1, 10, 100, 1000]\ntrain_score, valid_score = [], []\nfor cs in param_range:\n    lr = LogisticRegressionCV(Cs=cs, cv=7, scoring='accuracy', solver= 'newton-cg',\n                              penalty= 'l2', refit=True, max_iter=1000)\n    lr.fit(x_train.loc[:, lr_selected_features], y_train)\n    train_score.append(\n        lr.score(x_train.loc[:, lr_selected_features], y_train))\n    valid_score.append(\n        lr.score(x_valid.loc[:, lr_selected_features], y_valid))","ca6eae23":"# Plot Regularization factor VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Regularization factor\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot([1, 10, 100, 1000], train_score, color = 'blue')\nplt.plot([1, 10, 100, 1000], valid_score, color = 'red')","ef1b26ce":"train_test_diff = np.array(train_score) - np.array(valid_score)\n\n# Plot number of folds VS. difference of cross-validated scores between train and Dev sets.\nplt.figure()\nplt.xlabel(\"Regularization Factor\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot([1, 10, 100, 1000], train_test_diff)","fa71ec17":"lr = LogisticRegressionCV(Cs= 10, cv= 7, solver= 'newton-cg', penalty= 'l2')\nlr.fit(x_train.loc[:, lr_selected_features], y_train)","28722f9e":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_lr = lr.predict_proba(x_test.loc[:, lr_selected_features])[:, 1]\nlr_fpr, lr_tpr, lr_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_lr)","c2208831":"# Finding the AUC for the logistic classification model.\nlr_auc = sklearn.metrics.auc(x=lr_fpr, y=lr_tpr)","90c9fabb":"# Model accuracy score on test data.\nlr_acc = lr.score(x_test.loc[:, lr_selected_features], y_test)","f2e7f348":"print('For logistic Regression: \\n Area Under Curve: {}, \\n Test Accuracy score: {}'.format(\n    lr_auc, lr_acc))","e008337b":"nb = GaussianNB()","52c7cc48":"threshold = np.arange(1, 10, 0.5) *1e-1","4a7d17ae":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    nb.fit(selected_features, target)\n    y_pred = nb.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(np.arange(1, 10, 0.5) *1e-1, np.array(scores))","212198bf":"print('The highest accuracy score is:', np.max(np.array(scores)))","f07b29dd":"number_of_features = list(range(1,13))","c1a568bf":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    nb.fit(selected_features, target)\n    y_pred = nb.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(list(range(1,13)), scores_k)","e8e5dc22":"print(\"Maximum accuracy score is :\", max(scores_k))","013d1e45":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","87bcd423":"# Fit the model with features selected by Variance threshold method and the training set\nselector = sklearn.feature_selection.VarianceThreshold(threshold= 0.1)\nselector.fit(features, target)\nnb_selected_features = selector.get_support()","dbf689b1":"nb_params = {'priors': [[0.7, 0.3], [0.6, 0.4],\n                        [0.5, 0.5], [0.4, 0.6], [0.3, 0.7]]}","8e04993d":"rs_nb = RandomizedSearchCV(nb, param_distributions= nb_params,cv= 7 ,n_iter= 200)","34ee8786":"rs_nb.fit(x_train.loc[:, nb_selected_features], y_train)","18bf8674":"print('Best Parameters are:\\n', rs_nb.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_nb.best_score_)","c1f32233":"print('Validation accuracy score is:\\n', rs_nb.score(\n    x_valid.loc[:, nb_selected_features], y_valid))","208e0852":"nb = GaussianNB(priors= [0.4, 0.6])\nnb.fit(x_train.loc[:, nb_selected_features], y_train)","066da98d":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_nb = nb.predict_proba(x_test.loc[:, nb_selected_features])[:, 1]\nnb_fpr, nb_tpr, nb_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_nb)","dd2c53f4":"# Finding the AUC for the naive bayes classification model.\nnb_auc = sklearn.metrics.auc(x=nb_fpr, y=nb_tpr)","53022574":"# Model Accuracy score on test data\nnb_acc = nb.score(x_test.loc[:, nb_selected_features], y_test)","7eedf7a5":"print('For Gaussian Naive Bayes: \\n Area Under Curve: {}, \\n Test Accuracy score: {}'.format(\n    nb_auc, nb_acc))","74ac5bf0":"knn = KNeighborsClassifier(n_neighbors= 5)","d0b72965":"threshold = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2]","88f0af32":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    knn.fit(selected_features, target)\n    y_pred = knn.predict(selected_features)\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot([0.001, 0.005, 0.01, 0.05, 0.1, 0.2], np.array(scores))","69cbd213":"np.max(np.array(scores))","53715713":"number_of_features = list(range(1,13))","e94996f7":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    knn.fit(selected_features, target)\n    y_pred = knn.predict(selected_features)\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(list(range(1,13)), scores_k)","fd52ce11":"print(\"Maximum accuracy score is :\", max(scores_k))","2f41ed16":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","7727f481":"selector = sklearn.feature_selection.VarianceThreshold(threshold= 0.1)\nselector.fit(features, target)\nknn_selected_features = selector.get_support()","7c990b24":"knn_params = {'n_neighbors': [5, 7, 9] , 'weights': [\n    'uniform', 'distance'], 'leaf_size': [5, 10, 20], 'p': [1, 2, 3]}","a0bc343b":"rs_knn = RandomizedSearchCV(knn, param_distributions= knn_params,\n                      scoring='accuracy', cv= 7, n_iter= 200, refit=True)","5fd8d164":"rs_knn.fit(x_train.loc[:, knn_selected_features], y_train)","7c848a8d":"print('Best Parameters are:\\n', rs_knn.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_knn.best_score_)","708d6c27":"print('Validation accuracy score is:\\n', rs_knn.score(\n    x_valid.loc[:, knn_selected_features], y_valid))","7cba3fe4":"param_name = 'n_neighbors'\nparam_range = np.arange(3,21)\ntrain_score, valid_score = [], []\nfor k in param_range:\n    knn = KNeighborsClassifier(n_neighbors= k, weights= 'uniform', p= 2,leaf_size= 5)\n    knn.fit(x_train.loc[:, knn_selected_features], y_train)\n    train_score.append(\n        knn.score(x_train.loc[:, knn_selected_features], y_train))\n    valid_score.append(\n        knn.score(x_valid.loc[:, knn_selected_features], y_valid))","39c0bdbb":"# Plot number of neighbours VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Number of Neighbours\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(np.arange(3,21), train_score, color = 'blue')\nplt.plot(np.arange(3,21), valid_score, color = 'red')","25d564d9":"train_test_diff = np.array(train_score) - np.array(valid_score)\n\n# Plot Number of Neighbours VS. difference of cross-validated scores between train and validation sets.\nplt.figure()\nplt.xlabel(\"Number of Neighbours\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot(np.arange(3,21), train_test_diff)","1ed62e6d":"knn = KNeighborsClassifier(n_neighbors= 4, weights= 'uniform', p= 2, leaf_size= 5)","43602359":"knn.fit(x_train.loc[:, knn_selected_features], y_train)","76e5a0d3":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_knn = knn.predict_proba(x_test.loc[:, knn_selected_features])[:, 1]\nknn_fpr, knn_tpr, knn_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_knn)","3c5ab0b6":"# Finding the AUC for the naive bayes classification model.\nknn_auc = sklearn.metrics.auc(x=knn_fpr, y=knn_tpr)","ab738d70":"# Model Accuracy score on test data\nknn_acc = knn.score(x_test.loc[:, knn_selected_features], y_test)","c6dca9bb":"print('Area Under Curve: {}, Accuracy: {}'.format(knn_auc, knn_acc))","c32f529f":"svm = SVC(probability=True)","6ae91abf":"threshold = np.arange(1, 10, 0.5) *1e-1","99af1ec8":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    svm.fit(selected_features, target)\n    y_pred = svm.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(np.arange(1, 10, 0.5) *1e-1, np.array(scores))","8aae3300":"print('The highest accuracy score is:', np.max(np.array(scores)))","114bcc48":"number_of_features = list(range(1,13))","73dc7f21":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    svm.fit(selected_features, target)\n    y_pred = svm.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(list(range(1,13)), scores_k)","000dba5b":"print(\"Maximum accuracy score is :\", max(scores_k))","fa93db92":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","dc996aab":"# Fit the model with features selected by SelectFromModel method and the training set\nselector = sklearn.feature_selection.VarianceThreshold(threshold= 0.1)\nselector.fit(features, target)\nsvm_selected_features = selector.get_support()","95a9e281":"svm = SVC(probability=True)","1e4abf9e":"svm_parameters = {'kernel': ['linear', 'rbf', 'sigmoid'], 'gamma': [\n    'auto', 'scale'], 'shrinking': [True, False]}","68bbacd1":"rs_svm = RandomizedSearchCV(svm, cv= 7, param_distributions= svm_parameters, n_iter= 200)","8bbfdc1e":"rs_svm.fit(x_train.loc[:, svm_selected_features], y_train)","1d8d579a":"print('Best Parameters are:\\n', rs_svm.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_svm.best_score_)","4bcb484e":"print('Validation accuracy score is:\\n', rs_svm.score(\n    x_valid.loc[:, svm_selected_features], y_valid))","b1b32aff":"param_name = 'C'\nparam_range = np.arange(1,31)\ntrain_score, valid_score = [], []\nfor c in param_range:\n    svm = SVC(C= c,probability= True)\n    svm.fit(x_train.loc[:, svm_selected_features], y_train)\n    train_score.append(\n        svm.score(x_train.loc[:, svm_selected_features], y_train))\n    valid_score.append(\n        svm.score(x_valid.loc[:, svm_selected_features], y_valid))","2e469ff7":"# Plot Regularization factor VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Regularization factor\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(np.arange(1,31), train_score, color = 'blue')\nplt.plot(np.arange(1,31), valid_score, color = 'red')","adac34b7":"train_test_diff = np.array(train_score) - np.array(valid_score)\n\n# Plot number of folds VS. difference of cross-validated scores between train and Dev sets.\nplt.figure()\nplt.xlabel(\"Regularization Factor\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot(np.arange(1,31), train_test_diff)","291d282c":"svm = SVC(C=3, probability= True)\nsvm.fit(features.loc[:, svm_selected_features], target)","b641391e":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_svm = svm.predict_proba(x_test.loc[:, svm_selected_features])[:, 1]\nsvm_fpr, svm_tpr, svm_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_svm)","13cef281":"# Finding the AUC for the logistic classification model.\nsvm_auc = sklearn.metrics.auc(x=svm_fpr, y=svm_tpr)","1d5a7881":"# Model accuracy score on test data.\nsvm_acc = svm.score(x_test.loc[:, svm_selected_features], y_test)","902f0cde":"print('For logistic Regression: \\n Area Under Curve: {}, \\n Test Accuracy score: {}'.format(\n    svm_auc, svm_acc))","783086cb":"dt = DecisionTreeClassifier()","7b39e519":"threshold = np.arange(1, 10, 0.5) *1e-1","3bd0f649":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    dt.fit(selected_features, target)\n    y_pred = dt.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","fe23759d":"print('The highest accuracy score is:', np.max(np.array(scores)))","9a70f220":"number_of_features = list(range(1,13))","063651b7":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    dt.fit(selected_features, target)\n    y_pred = dt.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","bc8dee81":"print(\"Maximum accuracy score is :\", max(scores_k))","9ba3b5bc":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","8bf02257":"selector = sklearn.feature_selection.RFECV(dt, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","b4c7c81b":"print(\"Optimal number of features : %d\" % selector.n_features_)","0c885664":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","32c66a5a":"threshold = [0.001, 0.0025, 0.005, 0.01, 0.025 ,0.05, 0.1]","60a44d5c":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(dt, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    dt.fit(selected_features, target)\n    y_pred = dt.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","46876ecf":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","41e92d5b":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","075ca246":"dt_params = {'criterion': ['gini'], 'min_samples_split': [\n     21, 22, 23], 'max_features': ['auto', 'log2', None]}","4a7f8077":"rs_dt = RandomizedSearchCV(dt, param_distributions= dt_params,\n                     scoring='accuracy', cv= StratifiedKFold(7), refit=True, n_iter= 500)","25f99ebe":"rs_dt.fit(x_train, y_train)","2ec36315":"print('Best Parameters are:\\n', rs_dt.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_dt.best_score_)","64972308":"print('Validation accuracy score is:\\n', rs_dt.score(x_valid, y_valid))","6f86b48d":"param_name = 'max_depth'\nparam_range = np.arange(1, 21)\ntrain_score, valid_score = [], []\nfor depth in param_range:\n    dt = DecisionTreeClassifier(\n        criterion='gini', max_features=None, min_samples_split=22, max_depth= depth)\n    dt.fit(x_train, y_train)\n    train_score.append(dt.score(x_train, y_train))\n    valid_score.append(dt.score(x_valid, y_valid))","e89a1f34":"# Plot Regularization factor VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Regularization factor\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(param_range, train_score, color = 'blue')\nplt.plot(param_range, valid_score, color = 'red')","576b1f22":"train_test_diff = np.array(train_score) - np.array(valid_score)\n\n# Plot number of folds VS. difference of cross-validated scores between train and Dev sets.\nplt.figure()\nplt.xlabel(\"Regularization Factor\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot(param_range, train_test_diff)","c67b5ec9":"dt = DecisionTreeClassifier(criterion='gini', max_features=None, min_samples_split=22, max_depth= 3)\ndt.fit(features,target)","64f38619":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_dt = dt.predict_proba(x_test)[:, 1]\ndt_fpr, dt_tpr, dt_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_dt)\n# Finding the AUC for the Decision Tree classification model.\ndt_auc = sklearn.metrics.auc(x=dt_fpr, y=dt_tpr)","8b9114dc":"dt_acc = dt.score(x_test, y_test)","5c288a12":"print('Area Under Curve: {}, Accuracy: {}'.format(dt_auc, dt_acc))","141798b3":"rf = ens.RandomForestClassifier()","bfb84c90":"threshold = np.arange(1, 10, 0.5) *1e-1","6b73dfad":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    rf.fit(selected_features, target)\n    y_pred = rf.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","41cc6c1e":"print('The highest accuracy score is obtained after execluding features whose variance is less than: ', \n              np.round(threshold[np.argmax(np.array(scores))],3))","7836b2d2":"print('The highest accuracy score is:', np.max(np.array(scores)))","0a88ef3b":"number_of_features = list(range(1,13))","3d38ebfb":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    rf.fit(selected_features, target)\n    y_pred = rf.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","6dfe4f9a":"print(\"Maximum accuracy score is :\", max(scores_k))","d682897a":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","c75c292c":"selector = sklearn.feature_selection.RFECV(rf, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","b1bb4440":"print(\"Optimal number of features : %d\" % selector.n_features_)","4087bede":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","50948cae":"threshold = [0.001, 0.0025, 0.005, 0.01, 0.025 ,0.05, 0.1, 0.15]","53b0c775":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(rf, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    rf.fit(selected_features, target)\n    y_pred = rf.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","ad99a817":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","9876f6d0":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","0c607369":"rf_params = {'n_estimators': [200, 300, 400], 'criterion': ['gini'], 'min_samples_split': [\n    22, 20, 25], 'max_features': ['auto', 'log2', None], 'class_weight': [{0: 0.6, 1: 0.4}, {0: 0.6, 1: 0.4}, {0: 0.5, 1: 0.5}]}","b09697fd":"rs_rf = RandomizedSearchCV(rf, param_distributions= rf_params,\n                     scoring='accuracy', cv= StratifiedKFold(7), refit=True, n_iter= 200)","b255d151":"rs_rf.fit(x_train, y_train)","a28d5d58":"print('Best Parameters are:\\n', rs_rf.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_rf.best_score_)","a467731a":"print('Validation accuracy score is:\\n', rs_rf.score(x_valid, y_valid))","84f68de9":"param_name = 'max_depth'\nparam_range = np.arange(1, 31)\ntrain_score, valid_score = [], []\nfor depth in param_range:\n    rf = ens.RandomForestClassifier(n_estimators= 300,\n        criterion='gini', max_features= 'auto', min_samples_split=22, \n                                    class_weight= {0: 0.5, 1: 0.5},max_depth= depth)\n    rf.fit(x_train, y_train)\n    train_score.append(rf.score(x_train, y_train))\n    valid_score.append(rf.score(x_valid, y_valid))","0b79c132":"# Plot Regularization factor VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Regularization factor\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(param_range, train_score, color = 'blue')\nplt.plot(param_range, valid_score, color = 'red')","0ab3d601":"train_test_diff = np.array(train_score) - np.array(valid_score)\n\n# Plot number of folds VS. difference of cross-validated scores between train and Dev sets.\nplt.figure()\nplt.xlabel(\"Regularization Factor\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot(param_range, train_test_diff)","3c15f553":"rf = ens.RandomForestClassifier(n_estimators= 300,\n        criterion='gini', max_features= 'auto', min_samples_split=22, \n                                class_weight= {0: 0.5, 1: 0.5}, max_depth= 6)\nrf.fit(features,target)","f0456b5a":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_rf = rf.predict_proba(x_test)[:, 1]\nrf_fpr, rf_tpr, rf_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_rf)\n# Finding the AUC for the Decision Tree classification model.\nrf_auc = sklearn.metrics.auc(x=rf_fpr, y=rf_tpr)","0eed3aaa":"rf_acc = rf.score(x_test, y_test)","2bdb2317":"print('Area Under Curve: {}, Accuracy: {}'.format(rf_auc, rf_acc))","a1909187":"bg = ens.BaggingClassifier()","3b43724d":"threshold = np.arange(1, 10, 0.5) *1e-1","5a512251":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    bg.fit(selected_features, target)\n    y_pred = bg.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","0378a473":"print('The highest accuracy score is obtained after execluding features whose variance is less than: ', \n              np.round(threshold[np.argmax(np.array(scores))],3))","dc7e4b55":"print('The highest accuracy score is:', np.max(np.array(scores)))","3f644a9f":"number_of_features = list(range(1,13))","d1058e70":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    bg.fit(selected_features, target)\n    y_pred = bg.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","06825cda":"print(\"Maximum accuracy score is :\", max(scores_k))","41a17b9f":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","8acbaa2a":"bg_params = {'n_estimators': [20, 25, 100], 'base_estimator': [\n    None, svm], 'max_features': [0.6, 0.7, 0.8], 'oob_score' : [True, False], \n            'max_samples': [0.6,0.7,0.8]}","479d4b72":"rs_bg = RandomizedSearchCV(bg, param_distributions= bg_params,\n                     scoring='accuracy', cv=StratifiedKFold(7), n_iter= 2000,refit=True)","122ce10c":"rs_bg.fit(x_train, y_train)","2b538442":"print('Best Parameters are:\\n', rs_bg.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_bg.best_score_)","ac7dbb34":"print('Validation accuracy score is:\\n', rs_bg.score(x_valid, y_valid))","dcc0b797":"bg = ens.BaggingClassifier(n_estimators= 25,\n        max_features= 0.8, base_estimator= svm, oob_score= True, max_samples= 0.8)\nbg.fit(features,target)","d5a1632c":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_bg = bg.predict_proba(x_test)[:, 1]\nbg_fpr, bg_tpr, bg_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_bg)\n# Finding the AUC for the Decision Tree classification model.\nbg_auc = sklearn.metrics.auc(x=bg_fpr, y=bg_tpr)","7a4753e7":"bg_acc = bg.score(x_test, y_test)","fc9c3c4d":"print('Area Under Curve: {}, Accuracy: {}'.format(bg_auc, bg_acc))","ce400a41":"ada = ens.AdaBoostClassifier()","6231d782":"threshold = np.arange(1, 10, 0.5) *1e-1","e2ae2e3d":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    ada.fit(selected_features, target)\n    y_pred = ada.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","54493d13":"print('The highest accuracy score is obtained after execluding features whose variance is less than: ', \n              np.round(threshold[np.argmax(np.array(scores))],3))","cfe8ca42":"print('The highest accuracy score is:', np.max(np.array(scores)))","34713cce":"number_of_features = list(range(1,13))","7b9bad0b":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    ada.fit(selected_features, target)\n    y_pred = ada.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","1a4fe675":"print(\"Maximum accuracy score is :\", max(scores_k))","48c98b8a":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","cc24c198":"selector = sklearn.feature_selection.RFECV(ada, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","8763ac43":"print(\"Optimal number of features : %d\" % selector.n_features_)","a3397dfc":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","537f0fc4":"threshold = [0.001, 0.0025, 0.005, 0.01, 0.025 ,0.05, 0.1, 0.15]","640028a3":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(ada, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    ada.fit(selected_features, target)\n    y_pred = ada.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","ae379b0f":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","e2bccfbb":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","39bf5e9a":"ada_params = {'n_estimators': [90, 100, 110], 'base_estimator': [None, svm],\n             'learning_rate': [0.09 ,0.1, 0.11]}","d61916b7":"rs_ada = RandomizedSearchCV(ada, param_distributions= ada_params,\n                     scoring='accuracy', cv= StratifiedKFold(7), refit=True, n_iter= 500)","b5e9b32f":"rs_ada.fit(x_train, y_train)","da4eb8fa":"print('Best Parameters are:\\n', rs_ada.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_ada.best_score_)","03b5d3e5":"print('Validation accuracy score is:\\n', rs_ada.score(x_valid, y_valid))","28df8451":"ada = ens.AdaBoostClassifier(n_estimators= 110, learning_rate= 0.09)\nada.fit(features, target)","a2f191d7":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_ada = ada.predict_proba(x_test)[:, 1]\nada_fpr, ada_tpr, ada_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_ada)\n# Finding the AUC for the Decision Tree classification model.\nada_auc = sklearn.metrics.auc(x=ada_fpr, y=ada_tpr)","bdd1726e":"ada_acc = ada.score(x_test, y_test)","a34c58cc":"print('Area Under Curve: {}, Accuracy: {}'.format(ada_auc, ada_acc))","76d27d78":"gb = ens.GradientBoostingClassifier()","e2f5ebda":"threshold = np.arange(1, 10, 0.5) *1e-1","7ba5a994":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    gb.fit(selected_features, target)\n    y_pred = gb.predict(features.loc[:, selector.get_support()])\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","0ab2f21c":"print('The highest accuracy score is obtained after execluding features whose variance is less than: ', \n              np.round(threshold[np.argmax(np.array(scores))],3))","e5d0500e":"print('The highest accuracy score is:', np.max(np.array(scores)))","b1271e8f":"number_of_features = list(range(1,13))","4c1f726e":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    gb.fit(selected_features, target)\n    y_pred = gb.predict(features.loc[:, selector.get_support()])\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","3e3e1501":"print(\"Maximum accuracy score is :\", max(scores_k))","76ef174b":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","876b305b":"selector = sklearn.feature_selection.RFECV(gb, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","6a8f2aa1":"print(\"Optimal number of features : %d\" % selector.n_features_)","1d1f479d":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","54970e87":"threshold = [0.001, 0.0025, 0.005, 0.01, 0.025 ,0.05, 0.1, 0.15]","039dae5f":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(gb, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    gb.fit(selected_features, target)\n    y_pred = gb.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","8e4352af":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","e1a26aac":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","c505ecaa":"# Fit the model with features selected by SelectFromModel method and the training set\nselector = sklearn.feature_selection.SelectKBest(k= 11)\nselector.fit(features, target)\ngb_selected_features = selector.get_support()","39b05a39":"gb_params = {'n_estimators': [150, 160, 170], 'loss': ['deviance', 'exponential'],\n             'subsample': [0.7, 0.8, 0.9], 'max_features': ['auto', 'log2', None]}","2cdbb972":"rs_gb = RandomizedSearchCV(gb, param_distributions= gb_params,\n                     scoring='accuracy', cv= StratifiedKFold(7), refit=True, n_iter= 2000)","49b5bfa9":"rs_gb.fit(x_train.loc[:,gb_selected_features], y_train)","4ed3bb9a":"print('Best Parameters are:\\n', rs_gb.best_params_,\n      '\\nTraining accuracy score is:\\n', rs_gb.best_score_)","7fbe0037":"print('Validation accuracy score is:\\n',\n      rs_gb.score(x_valid.loc[:,gb_selected_features], y_valid))","38e2353c":"param_name = 'max_depth'\nparam_range = np.arange(1, 31)\ntrain_score, valid_score = [], []\nfor depth in param_range:\n    gb = ens.GradientBoostingClassifier(n_estimators= 170,\n        subsample= 0.9, max_features= 'auto', loss= 'exponential',max_depth= depth)\n    gb.fit(x_train.loc[:,gb_selected_features], y_train)\n    train_score.append(gb.score(x_train.loc[:,gb_selected_features], y_train))\n    valid_score.append(gb.score(x_valid.loc[:,gb_selected_features], y_valid))","0650d914":"# Plot Regularization factor VS. cross-validated scores for training and Validation sets.\nplt.figure()\nplt.xlabel(\"Maximum Depth\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(param_range, train_score, color = 'blue')\nplt.plot(param_range, valid_score, color = 'red')","11aaf648":"train_test_diff = np.abs(np.array(train_score) - np.array(valid_score))\n\n# Plot number of folds VS. difference of cross-validated scores between train and Dev sets.\nplt.figure()\nplt.xlabel(\"Maximum depth\")\nplt.ylabel(\"Diff. Cross validated accuracy score\")\nplt.plot(param_range, train_test_diff)","34d47b22":"gb = ens.GradientBoostingClassifier(n_estimators= 170, subsample= 0.9, max_features= 'auto',\n                                    loss= 'exponential',max_depth= 4)\ngb.fit(features.loc[:, gb_selected_features],target)","b61e9a68":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_gb = gb.predict_proba(x_test.loc[:, gb_selected_features])[:, 1]\ngb_fpr, gb_tpr, gb_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_gb)\n# Finding the AUC for the Decision Tree classification model.\ngb_auc = sklearn.metrics.auc(x=gb_fpr, y=gb_tpr)","98051bc5":"gb_acc = gb.score(x_test.loc[:, gb_selected_features], y_test)","301d2f6a":"print('Area Under Curve: {}, Accuracy: {}'.format(gb_auc, gb_acc))","316b16af":"xgboost = xgb.XGBClassifier()","e34275be":"threshold = np.arange(1, 10, 0.5) *1e-2","6618a877":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    xgboost.fit(selected_features, target)\n    y_pred = xgboost.predict(selected_features)\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot variance threshold VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"variance threshold\")\nplt.ylabel(\"Cross validated accuracy score\")\nplt.plot(threshold, np.array(scores))","8c03de1e":"print('The highest accuracy score is obtained after execluding features whose variance is less than: ', \n              np.round(threshold[np.argmax(np.array(scores))],3))","957ecee5":"print('The highest accuracy score is:', np.max(np.array(scores)))","2c00c635":"number_of_features = list(range(1,13))","33416643":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    xgboost.fit(selected_features, target)\n    y_pred = xgboost.predict(selected_features)\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of selected features VS. cross-validated scores for training sets.\nplt.figure()\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"Cross validated accuracy score\")    \nplt.plot(number_of_features, scores_k)","b30d9a9a":"print(\"Maximum accuracy score is :\", max(scores_k))","be9c09e5":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","7db9d8b8":"selector = sklearn.feature_selection.RFECV(xgboost, step= 1, cv= 5)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","06bc996e":"print(\"Optimal number of features : %d\" % selector.n_features_)","442ab264":"print(\"Maximum accuracy score is :\", np.max(selector.grid_scores_))","4ef671bb":"threshold = [0.001, 0.0025, 0.005, 0.01, 0.025 ,0.05, 0.1, 0.15]","de63af83":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(xgboost, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    xgboost.fit(selected_features, target)\n    y_pred = xgboost.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot(threshold, scores_sfm)","68d78886":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","be1ee9fc":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","ee91fc9c":"xgboost = xgb.XGBClassifier()\nxgboost.fit(features, target)","4742f336":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_xgb = xgboost.predict_proba(x_test)[:, 1]\nxgb_fpr, xgb_tpr, xgb_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_xgb)\n# Finding the AUC for the Decision Tree classification model.\nxgb_auc = sklearn.metrics.auc(x=xgb_fpr, y=xgb_tpr)","6e1e862e":"xgb_acc = xgboost.score(x_test, y_test)","286ce2f0":"print('Area Under Curve: {}, Accuracy: {}'.format(xgb_auc, xgb_acc))","4bd68ea7":"lgboost = lgb.LGBMClassifier()","bf4ce079":"threshold = [0.001, 0.01,0.1,0.5]","39c258e5":"scores = []\nfor i in threshold:\n    selector = sklearn.feature_selection.VarianceThreshold(threshold= i)\n    selected_features = selector.fit_transform(features)\n    lgboost.fit(selected_features, target)\n    y_pred = lgboost.predict(selected_features)\n    scores.append(sklearn.metrics.accuracy_score(target, y_pred))\nplt.plot([0.001, 0.01,0.1,0.5], np.array(scores))","b3bccc4e":"np.max(np.array(scores))","e562ceb2":"number_of_features = list(range(1,13))","ba9ec5d9":"scores_k = []\nfor i in number_of_features:\n    selector = sklearn.feature_selection.SelectKBest(k=i)\n    selected_features = selector.fit_transform(features, target)\n    lgboost.fit(selected_features, target)\n    y_pred = lgboost.predict(selected_features)\n    scores_k.append(sklearn.metrics.accuracy_score(target, y_pred))\nplt.plot(list(range(1,13)), scores_k)","1bb98496":"max(scores_k)","f7c79d2f":"print(\"Optimal number of features :\", np.argmax(np.array(scores_k)) + 1)","f3f9945f":"selector = sklearn.feature_selection.RFECV(lgboost, step= 1, cv= 7)\nselector.fit(features, target)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)","1d0a12bd":"print(\"Optimal number of features : %d\" % selector.n_features_)","09514b9b":"np.max(selector.grid_scores_)","a076941b":"threshold = [0.001, 0.01, 0.05, 0.1 , 0.5]","5f42b4d9":"scores_sfm = []\nfor i in threshold:\n    selector = sklearn.feature_selection.SelectFromModel(lgboost, threshold= i)\n    selector.fit(features, target)\n    selected_features = features.loc[:, selector.get_support()]\n    lgboost.fit(selected_features, target)\n    y_pred = lgboost.predict(selected_features)\n    scores_sfm.append(sklearn.metrics.accuracy_score(target, y_pred))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Threshold Value\")\nplt.ylabel(\"Cross validation score\")    \nplt.plot([0.001, 0.01, 0.05, 0.1 , 0.5], scores_sfm)","0ec85be5":"print(\"Maximum accuracy score is :\", np.max(np.array(scores_sfm)))","c2e51645":"print(\"Optimal threshold :\", threshold[np.argmax(np.array(scores_sfm))])","02e51fb1":"# Fit the model with the best 11 features selected.\nselector = sklearn.feature_selection.SelectKBest(k= 11)\nselector.fit(features, target)\nlgb_selected_features = selector.get_support()","4b7390c5":"lgboost = lgb.LGBMClassifier()\nlgboost.fit(features.loc[:,lgb_selected_features], target)","992be1e4":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_lgb = lgboost.predict_proba(x_test.loc[:,lgb_selected_features])[:, 1]\nlgb_fpr, lgb_tpr, lgb_thresholds = sklearn.metrics.roc_curve(y_test, y_scores_lgb)\n# Finding the AUC for the Decision Tree classification model.\nlgb_auc = sklearn.metrics.auc(x=lgb_fpr, y=lgb_tpr)","d513e2bd":"lgb_acc = lgboost.score(x_test.loc[:,lgb_selected_features], y_test)","4d055b44":"print('Area Under Curve: {}, Accuracy: {}'.format(lgb_auc, lgb_acc))","7c20fbf9":"v = ens.VotingClassifier(estimators=[\n    ('lr', lr),('NB', nb),('KNN', knn),('SVM', svm),('DT', dt),\n    ('RF', rf), ('BG', bg),('AdaBoost', ada),('GBM', gb),\n    ('XGBM', xgboost),('LightGBM', lgboost)], \n                         voting='soft', \n                         weights= [1,1,1, 1.25, 1.25, 1.25, 1.25, 1.25, 1.75, 1.5, 1.5])","b411ebfd":"# Fit the model with the best 11 features selected.\nselector = sklearn.feature_selection.SelectKBest(k= 11)\nselector.fit(features, target)\nvoting_selected_features = selector.get_support()","7bbf800d":"v.fit(features.loc[:, voting_selected_features], target)","c877c99e":"# Finding the ROC curve for different threshold values.\n# probability estimates of the positive class.\ny_scores_v = v.predict_proba(features.loc[:, voting_selected_features])[:, 1]\nv_fpr, v_tpr, v_thresholds = sklearn.metrics.roc_curve(target, y_scores_v)\n# Finding the AUC for the Voting classification model.\nv_auc = sklearn.metrics.auc(x=v_fpr, y=v_tpr)","1ed912b9":"v_acc = v.score(x_test.loc[:,voting_selected_features], y_test)","faa83691":"print('Area Under Curve: {}, Accuracy: {}'.format(v_auc, v_acc))","83c388d3":"pd.DataFrame([(lr_auc, lr_acc), (nb_auc, nb_acc), (knn_auc, knn_acc), (dt_auc, dt_acc),\n              (rf_auc, rf_acc), (svm_auc, svm_acc), (bg_auc, bg_acc), (ada_auc, ada_acc),\n              (v_auc, v_acc), (gb_auc, gb_acc), (xgb_auc, xgb_acc), (lgb_auc, lgb_acc)],\n             columns=['AUC', 'Accuracy'],\n             index=['Logistic Regression', 'Naive Bayes', 'KNN', 'Decision Tree',\n                    'Random Forest', 'SVM', 'Bagging', 'AdaBoost', 'Voting',\n                   'Gradient Boost', 'XGBoost', 'Light Boost'])","49c45620":"plt.figure(figsize=(8, 5))\nplt.title('Receiver Operating Characteristic Curve')\nplt.plot(lr_fpr, lr_tpr, label='LR_AUC = %0.2f' % lr_auc)\nplt.plot(nb_fpr, nb_tpr, label='NB_AUC = %0.2f' % nb_auc)\nplt.plot(knn_fpr, knn_tpr, label='KNN_AUC = %0.2f' % knn_auc)\nplt.plot(svm_fpr, svm_tpr, label='SVM_AUC = %0.2f' % svm_auc)\nplt.plot(dt_fpr, dt_tpr, label='DT_AUC = %0.2f' % dt_auc)\nplt.plot(rf_fpr, rf_tpr, label='RF_AUC = %0.2f' % rf_auc)\nplt.plot(bg_fpr, bg_tpr, label='BG_AUC = %0.2f' % bg_auc)\nplt.plot(ada_fpr, ada_tpr, label='Ada_AUC = %0.2f' % ada_auc)\nplt.plot(v_fpr, v_tpr, label='Voting_AUC = %0.2f' % v_auc)\nplt.plot(lgb_fpr, lgb_tpr, label='LBoost_AUC = %0.2f' % lgb_auc)\nplt.legend(loc='lower right')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve')\nplt.show()","1e4ba84d":"y_pred_v = pd.DataFrame(v.predict(unlabelled.loc[:, voting_selected_features]), columns=[\n                        'Survived'], dtype='int64')","fd9a8f2c":"v_model = pd.concat([passengerID, y_pred_v], axis=1)","c25693de":"v_model.to_csv('voting.csv', index= False)","6af81bd8":"### 6.2.2 Gaussian NB Hyper-parameters tunning","ca7821f1":"This is an important step before scaling features. Since the scaler should be fit on the training set only and then applied to both training and test sets.","19b1fe50":"### 6.1.1 Feature selection","c3926c2c":"### 6.2.3 Model Assessment","5dac3796":"### 6.4.4 Model Assessment","7fb8baea":"This method can't be used with KNN algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","0b653ac4":"### 6.7.1 Feature selection","b1619add":"## 6.2 Gaussian Naive Bayes Model","1050116d":"In this notebook, we tackle the problem of Titanic Survival Prediction.\n\nThis will include:\n    1. Data exploration: \n        - Check whether the data is balanced.\n        - Check features with missing values.\n        - Try to find a pattern in data.\n        - What features are most correlated with the target?\n        - What are the continuous and categorical features?\n        - Check the distribution of continuous features.\n        - Check the frequency of category occurrence for categorical features.\n    2. Data Cleaning.\n        - Impute missing values.\n    3. Feature Engineering.\n        - Create hand crafted features from existing features.\n        - Convert categorical features into numeric form.\n        - Rescale features.\n    4. Data splitting.\n    5. Seperating features from target.\n    6. Classification models.\n        - Features selection.\n        - Model hyperparameters tuning.\n        - Variance check to avoid overfitting.\n        - Model Assessment.\n    8. Models comparison.","53df6d77":"### 6.9.3 Variance check","715374d1":"The highest accuracy score is obtained after selecting the whole 12 features.","cee08e97":"#### 6.4.1.2 SelectKbest method","ed002891":"# 5. Features\/Target","f9a70323":"Make Prediction for test data","883d1a87":"## 2.2 Imputing the missing value in Fare with the median fare for the corresponding class.","e4877d9d":"### 6.5.4 Model Assessment","32987288":"By experimentation, we found that the best hyperparameters for the XGBoost classifiers are the default.","76ed3643":"## 2.1 Imputing missing values in Age with the median age for the corresponding class","1fd9223c":"We conclude the best feature selection method is Variance threshold method with threshold = 0.1.","4d23a356":"We conclude that SelectKBest method results in the highest accuracy score when K = 11 features.","1458cc98":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","595bb792":"#### 6.11.1.3 RFECV method","b44976f4":"### 3.1.6 Drop unused columns from data.","f3d2ef3c":"Fit the model with best parameters that minimize both bias and variance.","be5600da":"- Fare column has only one null value.<br\/>\n- Age column has many null values.<br\/>\n- Cabin column has a majority of null values.<br\/>\n- Survived column has null values for the test data.","f58add5e":"We will use the max_depth of tree to minimize the variance. We'll use the validation curve to select the best value of max_depth.","7065b726":"After trying different paramters, we found that the best paramters are the default.","ee6632d8":"#### 6.1.1.4 SelectFromModel method","11fcae2e":"### 6.3.3 Variance check","8333617e":"From the above graphs, we find that the maximum validation accuracy score is at max_depth of 1. but, we will choose max_depth of 4 because the validation accuracy score is good and the variance is relatively small.","95fa79cf":"We will use cross-validation with number of folds = 7 because the size of training data is divisible by 7.","ebec4b9f":"We conclude the highest accuracy score can be also obtained while using the whole 12 features for prediction. So we will use all features to the model.","eca4228b":"#### 6.11.1.4 SelectFromModel method","f737adc6":"We conclude the highest accuracy score can be also obtained while using the whole 12 features for prediction. So we will use all features to the model.","f20be8f4":"### 3.2.2 Convert qualitative ordinal features (FareBins) into numeric form.","b7cf64c8":"#### 6.6.1.3 RFECV method","45c50ec0":"We conclude the highest accuracy score can be obtained while using the whole 12 features for prediction. So we will use all features to the model.","cadfb0cf":"### 6.7.3 Model Assessment","dac6e3cb":"### 6.2.1 Feature selection","63cb9efd":"Fit our model and the use the cross-validation value of 7 since the training set size if divisible by 7.","894e5e05":"### 3.1.5 Create new feature by discretizing Fare into 4 buckets\/bins based on quantiles.","21cd157e":"## 3.2 Convert qualitative features into numeric form.","ad9fe342":"We found that, using only train set for training the KNN model will result in higher accuracy score when submiting predictions compared to using the whole labelled data for training the model.","e4f14cd1":"### 3.1.1 Create a new feature with the title of each passenger.","f967736e":"## 1.9 Ticket fare for each class.","6395d19d":"We've noticed that, the SelectKBest method with K = 11 works very well with most of classifiers. So, we will use this method with voting classifier.","2b8b6f9e":"# Loading Data","e2aee4df":"### 6.6.1 Feature selection","556ffe2d":"### 6.10.2 XGBoost hyperparameter optimization","162d8bcc":"### 3.1.3 Create a new feature to indicate whether the passenger was alone.","866c2734":"### 6.3.4 Model Assessment","6df9f984":"#### 6.9.1.1 VarianceThreshold method","24fcfe77":"### 6.12.2 Model Assessment","efbd3f74":"#### 6.6.1.1 VarianceThreshold method","bcde1fda":"Fit the model with best parameters that minimize both bias and variance.","e4a267da":"This method can't be used with KNN algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","ad38669c":"### 6.4.2 Logistic Regression Hyper-parameters tunning","182cb051":"#### 6.6.1.2 SelectKbest method","a2ebb42f":"Fit the model with best parameters that minimize both bias and variance.","4c179a09":"Fit the model with best parameters that minimize both bias and variance.","a7d587c5":"### 6.4.1 Feature selection","ddf5d672":"We'll use randomized search to tune the hyperparamters of KNN. And we'll follow a coarse-to-fine strategy.","72595216":"### 6.4.3 Variance check","6287df51":"We will use Randomized search to find the best priors.","74008241":"#### 6.8.1.3 RFECV method","c295341e":"We can notice that only 4 titles have significant frequency and the others are repeated only 8 time or less.<br\/> So, we will combine all titles with small frequency under one title (say, Other).","8014e32c":"#### 6.11.1.2 SelectKbest method","1af27ce1":"### 6.8.2 Adaboost hyperparamters tunning","17657b4a":"### 6.5.1 Feature selection","70db702a":"### 6.12.1 Feature selection for Voting Classifier","18a3f86a":"Fit the model with best parameters that minimize both bias and variance.","7b769b6d":"#### 6.3.1.1 VarianceThreshold method","6535858e":"#### 6.10.1.1 VarianceThreshold method","12047f30":"### 6.9.1 Feature selection","eb883622":"We conclude that SelectKBest method results in the highest accuracy score with K = 11.","43300b41":"# 2. Data cleaning","693c83f0":"The highest accuracy is obtained after execluding features whose variance is less than 0.1","d07aef5c":"#### 6.2.1.3 RFECV method","3c824202":"Concatenate both labelled and unlabelled data so that all data cleaning and feature engineering will applied to both of them.","0cb8c958":"The highest accuracy score is obtained after selecting the best 11 features.","b85ad3a2":"#### 6.4.1.4 SelectFromModel method","cb8512e1":"### 6.9.4 Model Assessment","d5dca452":"#### 6.11.1.1 VarianceThreshold method","8a8503b1":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","402cc0aa":"### 6.6.2 Random Forest hyperparamters tunning","78cf5998":"The highest accuracy score is obtained after selecting the best 11 features.","10e90f20":"#### 6.10.1.2 SelectKbest method","31368e19":"### 6.1.4 Model Assessment","361eb3e4":"Fit the model with best parameters that minimize both bias and variance.","43647ef1":"At regularization factor C = 3, the accuracy score of validation set is maximum and the difference between the train and validation sets is minimum.","ee4548b1":"## 6.10 XGBoost Classifier","44b010e5":"### 6.7.2 Bagging hyperparamters tunning","9fd6852a":"## 6.11 LightGBM Classifier","3975745e":"The highest accuracy is obtained after execluding features whose variance is less than 0.1","59dde1f3":"We conclude the highest accuracy score can be obtained while using the whole 12 features for prediction. So we will use all features to the model.","fe1a15ac":"### 3.1.2 Create a new feature for the family size","ac848bf3":"#### 6.10.1.3 RFECV method","a0b05d53":"#### 6.4.1.1 VarianceThreshold method","8338a23a":"## 1.3 Which is the most survived gender?","45a4f168":"## 7.1 Models score","c0f3cd80":"## 6.3 KNN Classification Model","229bda97":"# 6. Classification Models","9c24051b":"#### 6.10.1.4 SelectFromModel method","cd644ab0":"# 3. Feature Engineering","de1f1e43":"We will use randomized search method and we will follow coarse-to-fine strategy.","ad339396":"Fit the model with best parameters that minimize bias and variance.","2565817c":"From the above graphs, we find that the maximum validation accuracy score is at max_depth of 3. So, we will choose max_depth of 6 because the validation accuracy score is maximum and the variance is relatively small.","9516c4d5":"## 3.4 Rescaling features using different scalers","96b7a502":"## 1.4 Does first class have more survival rate?","cb665835":"This method can't be used with SVC algorithm because of a bug in the code of RFECV function, it shows an error message claiming that SVC classifier does not expose 'coef_' or 'feature_importances_' attributes. But it does expose 'coef_' attribute.","12723e64":"### 6.6.3 Variance check","6228dd42":"#### 6.4.1.3 RFECV method","4bb7e020":"### 6.5.2 Decision tree hyperparamters tunning","d9097ce1":"### 6.9.2 Gradient Boost hyperparamters tunning","e8709064":"We will use the max_depth of tree to minimize the variance. We'll use the validation curve to select the best value of max_depth.","40002d52":"#### 6.6.1.4 SelectFromModel method","34b94ed8":"We will try the following scalers on a copy of the original data frame and we'll select the best one:\n1. MinMaxScaler\n2. MaxAbsScaler\n3. StandardScaler\n4. RobustScaler\n5. Normalizer\n6. QuantileTransformer\n7. PowerTransformer","7754e95d":"## 7.2 Plotting the ROC curve","1e94e4c3":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","cba937a0":"#### 6.1.1.3 RFECV method","ebc116d0":"#### 6.2.1.2 SelectKbest method","689fd5c9":"##  6.1 Logistic Regression Model - Baseline model","39d6b19c":"### 6.10.1 Feature selection for XGBoost","6a8515c6":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","68062d65":"First, we want to tune our model such that we minimize the variance, which is sensitivity of the prediction score to the change in training set. We will use the validation curve to help us choose the best number of neighbours (K).","2b90e4ab":"#### 6.9.1.2 SelectKbest method","c5eeedab":"## 1.2 Is data balanced?","fd6c67eb":"#### 6.8.1.1 VarianceThreshold method","78e57e6c":"We will split the labelled data into 3 sets:\n1. Training set: used for model training. (Size = %70)\n2. Validation set: used for hyperparameter tunning. (Size = %15)\n3. Test set: used for model assessment and comparison of different models. (Size = %15)","3186290d":"### 6.3.1 Feature selection","44f1d7b2":"First, we want to tune our model such that we minimize the variance, which is sensitivity of the prediction score to the change in training set. We will use the validation curve to help us choose the best value for regularization factor.","b448e98b":"#### 6.5.1.3 RFECV method","7aa40b04":"## 6.12 Voting Classifier","3606280d":"## 2.3 Imputing the missing values in Embarked with the most common port for corresponding class.","e665bed9":"We will use cross-validation with number of folds = 7 because the size of training data is divisible by 7.","20d42277":"## 1.5 The distribution of passengers' age.","31b2926f":"## 1.1 Visualizing null values.","43bf739d":"We will use randomized search method and we will follow coarse-to-fine strategy.","c55072fa":"### 6.3.2 KNN hyperparamters tunning","916abb85":"Fit the model with best parameters that minimize both bias and variance.","36560405":"Fit the model with the selected features.","32dea53b":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","057f099c":"We will use Randomized search to find the best solver and penalty. we decided to make this process on two steps because of the limitations of some solvers that work with only one type of penalty.","214e06b1":"We conclude the best feature selection method is SelectFromModel with threshold = 0.26.","163c62e0":"# 7. Models Comaprison","86d4e1f7":"We will seperate the features and target columns from the label data so that it can be used in the feature selection step.","ff458ef3":"Fit the model with best parameters that minimize both bias and variance.","9a08bfd9":"This method can't be used with Bagging Classifier algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","222e24fe":"## 6.4 Support Vector Machine Classification model","d597ba52":"This method can't be used with Gaussian Naive Bayes algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","1420be52":"We will use the max_depth of tree to minimize the variance. We'll use the validation curve to select the best value of max_depth.","4a86ef27":"We conclude that, the highest accuracy is obtained using VarianceThreshold method after execluding features whose variance is less than 0.1","3dac90ec":"### 6.11.2 Model Assessment","eeb591cc":"### 6.10.3 Model Assessment","96103fc8":"1. Some features are expected to not have effect of the classification such as PassengerId, Name and Ticket. <br\/> \n2. Also some futures have too much missing values such as the Cabin which render it useless.\n3. We'll also drop the original features we used to create the new features because there will be high correlation between these features which may confuse the model about feature importance.","2e900cfe":"We conclude the best feature selection method is Variance threshold with threshold = 0.1.","a097d603":"The highest accuracy is obtained after execluding features whose variance is less than 0.001","116c68cf":"We will use randomized search method and we will follow coarse-to-fine strategy.","e8abd709":"#### 6.2.1.1 VarianceThreshold method","6de39bae":"### 3.1.4 Create a new feature by discretizing Age into buckets\/bins","9e75328a":"### 6.1.2 Logistic Regression Hyper-parameters tunning","21325ace":"We will use Randomized search to find the best solver and penalty. we decided to make this process on two steps because of the limitations of some solvers that work with only one type of penalty.","3149e913":"#### 6.5.1.2 SelectKbest method","6d634fd1":"Fit the model with best parameters that minimize both bias and variance.","a18025fa":"## 1.7 Number of passenger's in each class.","d1c6ffeb":"#### 6.7.1.3 RFECV method","99407d54":"At regularization factor Cs = 10, the accuracy score is high and the difference between the train and validation sets is minimum.","7ed7c783":"#### 6.9.1.3 RFECV method","23a37c46":"#### 6.3.1.2 SelectKbest method","4e9ddfe0":"It seems that the minimum variance is obtained at number of neighbours K = 4.","6340f6a9":"From the above graphs, we find that the maximum validation accuracy score is at max_depth of 3. So, we will choose max_depth of 3 because the validation accuracy score is maximum and the variance is relatively small.","16e802ef":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","d0e17456":"We will use randomized search method and we will follow coarse-to-fine strategy.","a734cba3":"We will use randomized search method and we will follow coarse-to-fine strategy.","838c9ea8":"Fit the model with best parameters that minimize both bias and variance.","48d2716c":"### 6.5.3 Variance check","602a0b2f":"Age is discretized into 4 bins coresponding to 4 stages of human life:<br\/>\n1. Childhood.\n2. Adolescence.\n3. Adulthood.\n4. Old Age. <br\/>\nCheck this link for more details: https:\/\/bit.ly\/2LkPFPf","ba55193d":"### 6.1.3 Variance check","950eb5d7":"#### 6.3.1.3 RFECV method","c0cac2d7":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","8fd9136d":"#### 6.7.1.1 VarianceThreshold method","a6d3c1b5":"Fit the model with best parameters that minimize both bias and variance.","56fde0c1":"### 6.8.3 Model Assessment","22f17a21":"## 1.6 The distribution of number of siblings.","d795c743":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","c7c53cae":"This method can't be used with Bagging Classifier algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","a773f94c":"## 6.5 Decision Tree Classification Model","594a5ee8":"The highest accuracy is obtained after execluding features whose variance is less than 0.1","d38d0b72":"#### 6.3.1.4 SelectFromModel method","bc9aa624":"### 6.11.1 Feature selection for LightGBM","6683a316":"#### 6.1.1.1 VarianceThreshold method","70b4c24c":"We will perform data split on two steps using train_test_split function:\n   1. we split data into training set and other set.\n   2. we split the other set into validation set and test set.","dde5717f":"The highest accuracy is obtained after execluding features whose variance is less than 0.1","b5d3081c":"#### 6.8.1.2 SelectKbest method","565f9b1b":"We can notice that the top scalers: MinMaxScaler, MaxAbsScaler, StandardScaler, and RobustScaler results in the same accuracy score. So, we will use the StandardScaler.","0ad98bb5":"## 1.8 Proportion of each gender in different classes.","c01bec53":"## 6.6 Random Forest Classification Model","a85c01d1":"# 1. Data Exploring","1c6f0ca2":"#### 6.2.1.4 SelectFromModel method","8fe24494":"### 6.6.4 Model Assessment","e5c71979":"## 3.3 Splitting Data back to labelled\/unlabelled sets.","a5dc2e3f":"#### 6.1.1.2 SelectKbest method","08fbd78a":"#### 6.5.1.1 VarianceThreshold method","89c1d82a":"This method can't be used with Gaussian Naive Bayes algorithm because this classifier does not expose 'coef_' or 'feature_importances_' attributes.","7880b0bc":"## 6.9 Gradient Boost Classifier","7045ae6f":"#### 6.5.1.4 SelectFromModel method","f062e50e":"We conclude the highest accuracy score can be also obtained while using the whole 12 features for prediction. So we will use all features to the model.","29dd0220":"## 6.8 Adaboost Classifier","3b4d49d6":"This feature combines the number of siblings and parents\/children (SibSp and Parch) +1 (The passenger himself).","eb117295":"#### 6.8.1.4 SelectFromModel method","d91aa570":"The highest accuracy score is obtained when using all features for model fitting.","991a4535":"## 3.1 Create New features","efa3a109":"First, we want to tune our model such that we minimize the variance, which is sensitivity of the prediction score to the change in training set. We will use the validation curve to help us choose the best value for regularization factor (C).","474d9ee1":"#### 6.7.1.2 SelectKbest method","70e84b98":"Here, we will try different method to select the features with the highest explainatory power. We will try the following methods, then we select the best method:\n1. VarianceThreshold\n2. SelectKBest\n3. RFECV\n4. SelectFromModel","efd6c5f4":"#### 6.9.1.4 SelectFromModel method","af3d15ed":"# 4. Train\/Validation\/Test.","56fe760d":"## 6.7 Bagging Classification Model","2f0dd90d":"### 6.8.1 Feature selection","ca074d68":"The highest accuracy score is obtained after selecting the best 11 features.","184ebc67":"### 3.2.1 Convert categorical features (Embarked, Sex, Title) to numerical features and drop one dummy variable for each.","21181d16":"#### 6.7.1.4 SelectFromModel method","16eb898d":"This method can't be used with SVC algorithm because of a bug in the code of SelectFromModel function, it shows an error message claiming that SVC classifier does not expose 'coef_' or 'feature_importances_' attributes. But it does expose 'coef_' attribute."}}