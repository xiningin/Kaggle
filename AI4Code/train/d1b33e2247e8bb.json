{"cell_type":{"c4d5c12f":"code","36c156c2":"code","95224e8d":"code","1bdcb237":"code","5ec59770":"code","888914ea":"code","2b7d7ef2":"code","ed72334e":"code","5222425a":"code","235dd971":"code","d34e955e":"code","04a6bbbf":"code","effdbac9":"code","ce63bd95":"code","ce6fb4d2":"code","99898d39":"code","e0b8bfc9":"code","e5a04aa7":"code","170df34b":"code","cee098b3":"code","9a90cb34":"code","59140009":"code","e6e30576":"code","958861a0":"code","0150634e":"code","eab33120":"code","697c1390":"code","537ce18a":"code","679658c3":"code","6b759680":"code","dc264b02":"code","12309054":"code","fda0f70e":"markdown","21434ead":"markdown","d4fb4c68":"markdown","f034253d":"markdown"},"source":{"c4d5c12f":"import os\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as mpimg\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\n# Map 1 library\nimport plotly.express as px\n\n# Map 2 libraries\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\n# Librosa Libraries\nimport librosa\nimport glob\nimport librosa.display\nimport IPython.display as ipd\n\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')","36c156c2":"%ls \/kaggle\/input\/","95224e8d":"#!pip install librosa","1bdcb237":"import librosa\naudio_data = '\/kaggle\/input\/clips-audio\/clips\/5a63686de58fecd09a634ea412556f913f787063041c5564f68592ae0ac7bc78b5fa5c13d82183eae4f7d6ce3468c3aada46d85e59868c45eefc3bca2ffb64a7.mp3'\nx , sr = librosa.load(audio_data)\nprint(type(x), type(sr))\nprint(x.shape, sr)","5ec59770":"librosa.load(audio_data, sr=22050)","888914ea":"import IPython.display as ipd\nipd.Audio(audio_data)","2b7d7ef2":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)","ed72334e":"#audio_list = []\n#for filename in glob.glob('\/kaggle\/input\/clips-audio\/clips\/*.mp3'): #assuming mp3\n #   x , sr = librosa.load(filename)\n #   aduio=ipd.Audio(filename)\n #   audio_list.append(aduio)","5222425a":"def load_audio_file(file_path):\n    input_length = 16000\n    data = librosa.core.load(file_path)[0] #, sr=16000\n    if len(data)>input_length:\n        data = data[:input_length]\n    else:\n        data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\")\n    return data\n\n\n\ndef plot_time_series(data):\n    fig = plt.figure(figsize=(14, 8))\n    plt.title('Raw wave ')\n    plt.ylabel('Amplitude')\n    plt.plot(np.linspace(0, 1, len(data)), data)\n    plt.show()","235dd971":"data = load_audio_file(\"\/kaggle\/input\/clips-audio\/clips\/5a63686de58fecd09a634ea412556f913f787063041c5564f68592ae0ac7bc78b5fa5c13d82183eae4f7d6ce3468c3aada46d85e59868c45eefc3bca2ffb64a7.mp3\")","d34e955e":"plot_time_series(data)","04a6bbbf":"#Hear it ! \nipd.Audio(data, rate=16000)","effdbac9":"# Import data\ntrain_csv = pd.read_csv(\"..\/input\/baamtu\/Train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/baamtu\/Test.csv\")\n","ce63bd95":"print(\"There are {:,} unique  age in the dataset.\".format(len(train_csv['age'].unique())))","ce6fb4d2":"# Inspect text_csv before checking train data\ntest_csv","99898d39":"len(train_csv['ID'].value_counts())","e0b8bfc9":"train_csv.head()","e5a04aa7":"x = train_csv['transcription'].value_counts().index.to_list()\ne_code_path = 'https:\/\/www.weegolines.com\/'\ntranscription = [e_code_path+p for p in x]","170df34b":"transcription[0]","cee098b3":"from IPython.display import IFrame\nIFrame(transcription[0], width=800, height=450)","9a90cb34":"IFrame(transcription[100], width=800, height=450)","59140009":"IFrame(transcription[200], width=800, height=450)","e6e30576":"train_csv['transcription'].nunique()","958861a0":"!pip install chart_studio","0150634e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\nimport librosa.display\n%matplotlib inline\n\n# Preprocessing\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport datetime as dt\nfrom datetime import datetime   \n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n# Settings for pretty nice plots\nplt.style.use('fivethirtyeight')\nplt.show()\n","eab33120":"train_csv['transcription'].value_counts()[:10].sort_values().iplot(kind='barh',color='#3780BF')","697c1390":"train_csv['age'].fillna('Not Defined',inplace=True);\ntrain_csv['age'].value_counts()","537ce18a":"train_csv['age'].value_counts()\n\nlabels = train_csv['age'].value_counts().index\nvalues = train_csv['age'].value_counts().values\ncolors=['#3795bf','#bfbfbf']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n                             insidetextorientation='radial',marker=dict(colors=colors))])\nfig.show()","679658c3":"train_csv.head()","6b759680":"train_csv['up_votes'].value_counts().iplot(kind='bar',color='#3780BF')","dc264b02":"\ntrain_csv['gender'].value_counts().plot(figsize=(12,8))","12309054":"train_csv['transcription'].value_counts().iplot()","fda0f70e":"<img src=\"https:\/\/miro.medium.com\/max\/540\/0*slh208x5lzN0NloD.png\">\n\n\n<h1><center>\ud83e\udd89Analyse de donn\u00e9es des audios\ud83e\udd89<\/center><\/h1>\n\n\n\n","21434ead":"## Loading an audio file:","d4fb4c68":"\nIn a country such as Senegal, where about 50% of the population is illiterate, technologies and applications that are designed to be used by people who can read are not as effective as they could be. In this competition, your aim is to use Automatic Speech Recognition (ASR) techniques in the Wolof language to help illiterate people to interact with apps with just their voice, in a language they can already speak.\n\nThe challenge will focus on a public transport use case for two reasons. First, many users of public transport can\u2019t read or speak French, so they can\u2019t interact with existing apps that help passengers to find a bus for a given destination. And second, there is already an existing app in Senegal, [WeeGo](https:\/\/www.weegolines.com), which helps passengers to get transport information.\n\nThe goal of this competition is to build an ASR model that will help illiterate people use existing apps to find which bus they can take to reach their destination, without having to know how to read or write.\n\nAbout Baamtu Datamation ([Baamtu](https:\/\/baamtu.com))","f034253d":"## Audio content analysis  \n\n- Signal Processing \n\n- Machine learning \n\n\n\n## Feature extraction process\n   - Preprocessing\n   - Feature Temporal computation\n   - Temporal integration\n   - Normalize and visualize audio \n   \n## Signal Framing  \n     - Static temporal segmentation\n     - Dynamic temporal segmentation\n     \n     \n Temporal features extracted  directly from the waveform samples\n \n Spectral features extracted from a frequential  representation of the signal\n \n Perceptual features extracted using a perceptual representation based on psychoacoustic considerations\n \n \n Compute short-term spectra of an audio signal using FTT\n\uf0a7Compute and display spectrogram\n\nscipy.fftpack and librosa\n\nsource: excitation \uf0e0 fine spectral structure\nfilter: resonator \uf0e0 coarse structure\n\nUse librosa to extract MFCCs from an audio file and visualise them"}}