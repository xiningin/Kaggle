{"cell_type":{"33b24fe9":"code","3e21a89a":"code","2a2cd7e8":"code","6ec8945c":"code","efedb2bc":"code","46fe1a37":"code","994e571f":"code","6cf6edd4":"code","fac3a651":"code","9eb1ee35":"code","a12e0a65":"code","89e89e96":"code","3873e2cd":"code","37be7f84":"code","3a1332ae":"code","081cd435":"code","fd169e89":"code","5005d664":"code","dc71e77c":"code","54dc013b":"code","61c544e8":"code","67eea5a7":"code","bda177af":"code","4ea6edb3":"code","df49021c":"code","e0e34d88":"code","7bf8ba26":"code","ba470e0e":"code","9a534ee8":"markdown","9f12018a":"markdown","ee02eb48":"markdown","9d18cf89":"markdown","c0e413fd":"markdown","d6fd12c9":"markdown","d580e68e":"markdown","70c06ef0":"markdown","dd1734da":"markdown","4c995cf5":"markdown","50db5d38":"markdown","64153658":"markdown","5feb228c":"markdown","83381526":"markdown","c78c2ccb":"markdown","6953a279":"markdown","ea87a84f":"markdown","b6c07b69":"markdown","fb9c5bc3":"markdown","afccfc34":"markdown","bd590308":"markdown","c5a0998c":"markdown","3aa1b35a":"markdown","97566ed8":"markdown","ffc45d30":"markdown","56f6a067":"markdown","ec31ca20":"markdown","dfe49bae":"markdown","595ade89":"markdown","3b39c9cc":"markdown","0a187e53":"markdown","a3f00682":"markdown","b543c4a8":"markdown","10f89718":"markdown","d666a697":"markdown","df555b85":"markdown","c1572d9e":"markdown","87b22576":"markdown","5d07f46b":"markdown","3012dfef":"markdown","1874c559":"markdown","5a2177f5":"markdown"},"source":{"33b24fe9":"import random\nimport pandas as pd\nimport numpy as np\n\n# Load training and testing data\ndf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',index_col=0)\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',index_col=0)\n# Extract 'text' and 'target' information from dataframe and shuffle the data\ntemp = [(x,y) for x,y in zip(list(df['text']),list(df['target']))]\nrandom.shuffle(temp)\ntweets = [t[0] for t in temp]\ny = [t[1] for t in temp]\n# Cast the target labels as a float in preparation for passing it to Tensorflow\ny = np.array(y).astype('float32')","3e21a89a":"print('Observations in training set')\nprint(df['target'].count())\nprint()\nprint('Label proportion in training set')\nprint(df['target'].value_counts()\/(sum(df['target'].value_counts())))\nprint()\nprint('Observations in test set')\nprint(df_test['text'].count())","2a2cd7e8":"import tensorflow as tf\nfrom transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification","6ec8945c":"model_name = 'roberta-large'\nroberta_tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\nroberta_seq = TFRobertaForSequenceClassification.from_pretrained(model_name)","efedb2bc":"roberta_seq.summary()","46fe1a37":"# Contains unicode codes (e.g. emojis)?\nfor t in tweets:\n    if 'U+' in t:\n        print(t)","994e571f":"import re\n\n# Contains HTML character entities?\nfor t in tweets:\n    if '&' in re.sub(r'(&amp|&gt|&lt)','',t):\n        print(t)","6cf6edd4":"for t in tweets:\n    if any([x in t for x in [' btw ',' omg ',' lol ',' thx ']]):\n        print(t)","fac3a651":"import re\n\ndef process_tweets(tweets):\n    r = tweets\n    r = [re.sub(r'https?:\/\/t.co\/\\w+','',t) for t in r]\n    r = [re.sub('&amp;','&',t) for t in r]\n    r = [re.sub('&gt;','gt',t) for t in r]\n    r = [re.sub('&lt;','lt',t) for t in r]\n    return r\n\ntweets = process_tweets(tweets)","9eb1ee35":"temp = roberta_tokenizer(tweets[:5],padding='max_length',max_length=50)\ntemp.keys()","a12e0a65":"print('Original tweet:')\nprint(tweets[0])\nprint('Encoded tweet:')\nprint(temp['input_ids'][0])\nprint('Decoded tweet:')\nprint(roberta_tokenizer.decode(temp['input_ids'][0]))","89e89e96":"print(temp['attention_mask'][0])","3873e2cd":"# Combine all data in a separate list used only for determining the length of sequences\nall_tweets = list(pd.concat([df,df_test],axis=0)['text'])\n# Comment out this line if you don't apply any pre-processing\nall_tweets = process_tweets(all_tweets)\nmax_len = max([len(t) for t in roberta_tokenizer(all_tweets)['input_ids']])\nprint(max_len)","37be7f84":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(tweets,y,test_size=0.30)\nX_train = roberta_tokenizer(X_train,padding='max_length',max_length=max_len,return_tensors='tf')\nX_test = roberta_tokenizer(X_test,padding='max_length',max_length=max_len,return_tensors='tf')","3a1332ae":"print(y.sum()\/len(y))\nprint(y_train.sum()\/len(y_train))\nprint(y_test.sum()\/len(y_test))","081cd435":"batch_size = 8\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train),y_train))\ntrain_dataset = train_dataset.batch(batch_size)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test),y_test))\ntest_dataset = test_dataset.batch(batch_size)","fd169e89":"print(train_dataset)\nprint(test_dataset)","5005d664":"temp_x, temp_y = next(iter(test_dataset))\ntemp = roberta_seq(temp_x,temp_y)\ntemp","dc71e77c":"optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nroberta_seq.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","54dc013b":"chkpt = '.\/checkpoint'\ncallback_chkpt = tf.keras.callbacks.ModelCheckpoint(chkpt,\n                                              monitor='val_accuracy',\n                                              save_weights_only=True,\n                                              save_best_only=True,\n                                              mode='max')","61c544e8":"history = roberta_seq.fit(train_dataset,epochs=3,\n                          validation_data=test_dataset,\n                          callbacks=[callback_chkpt])","67eea5a7":"roberta_seq.load_weights(chkpt)","bda177af":"outputs = roberta_seq.predict(test_dataset)\ny_pred = outputs[0].argmax(axis=1)","4ea6edb3":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred,labels=[0,1]))\nprint()\nprint('Classification report:')\nprint(classification_report(y_test,y_pred,labels=[0,1],target_names=['not a disaster','disaster']))","df49021c":"tweets_test = list(df_test['text'])\n# Comment out this line if you are not doing pre-processing\ntweets_test = process_tweets(tweets_test)\nX_real_test = roberta_tokenizer(tweets_test,padding='max_length',max_length=max_len,return_tensors='tf')\nreal_test_dataset = tf.data.Dataset.from_tensor_slices(dict(X_real_test))\nreal_test_dataset = real_test_dataset.batch(batch_size)\nreal_test_dataset","e0e34d88":"outputs_test = roberta_seq.predict(real_test_dataset)\ny_pred_test = outputs_test[0].argmax(axis=1)","7bf8ba26":"y_pred_test.sum()\/len(y_pred_test)","ba470e0e":"results = pd.Series(y_pred_test,index=df_test.index,name='target')\nresults.to_csv('.\/submission.csv')","9a534ee8":"As a test to make sure we have the data in the right format, we can evaluate the model on the first batch of the training set and see what comes out. In the following cell, \"iter\" is used to cast the Dataset object as an iterator and \"next\" to take the first element, i.e. the first batch of 8 examples, which is spearated into inputs (temp_x) and labels (temp_y). We can verify that the model returns a pair of logits per example, as mentioned earlier.","9f12018a":"This Kaggle kernel includes the training and testing data in the folder '..\/input\/nlp-getting-started\/'. The files are given in CSV format so they can easily be loaded using pandas. The files contain several fields but we will only be interested in 'text' and 'target', containing the tweets and annotated classification, respectively. We place the labels in a numpy array and cast them as a float as this is the data type we will later use when we place this labels into a tensor. ","ee02eb48":"Before we tokenise the text, we should try to understand what the tokenizer does. As an experiment, we can call the tokenizer on the first 5 tweets in the data set. For illustrative purposes, we arbitrarily add a padding to obtain sequences of 50 tokens. The tokenizer returns an object which contains a dictionary with two elements: 'input_ids' and 'attention_mask'","9d18cf89":"BERT is available in two versions, BERT-base and BERT-large. The former counts with ~110M parameters and is a reduced version of the latter, which has ~340M parameters. Similarly, RoBERTa is also available in both versions. For the purpose of this competition, we use the large version. However, the difference in performance is relatively small, and the base version can still yield very good results. Moreover, one could also consider DistilBERT, [a compact version of BERT](https:\/\/arxiv.org\/abs\/1910.01108) with 40% less parameters than BERT-base. The difference in score in this competition when using DistilBERT-base compared to BERT-large was of about 0.02 points, however training and inference were much more faster. In real-life scenarios, it is important to consider the trade-off between performance and speed, and choose the appropriate model according to the requirements of the task.\n\nIn the following cell, we instantiate the model and use 'from_pretrained' to specify that we want to load weights from an existing checkpoint.","c0e413fd":"## 2. Loading the model and tokenizer","d6fd12c9":"The 'attention_mask' indicates whether a token in the encoded sequence corresponds to the \"< pad >\" token or not. It's function is to let the model know that these padding tokens are effectively blank spaces, so there is no need to pay attention to them.","d580e68e":"## 3. Processing the data","70c06ef0":"## 5. Inference","dd1734da":"Once training finishes, we restore the checkpoint with the best accuracy on the validation set","4c995cf5":"## Contents\n\n1. Loading the data\n2. Loading the model and tokenizer\n3. Processing the data\n4. Fine-tuning RoBERTa\n5. Inference\n6. Summary","50db5d38":"This instance contains the RoBERTa-large model with a classifier on top.","64153658":"Finally, we add a callback to save the checkpoints of the model and keep the one with the best performance only, measured by accuracy on the validation set.","5feb228c":"The available data is split into a training and a testing (or validation) set. Note that we ask the tokenizer to return Tensorflow tensors as that's the library we will be using here, however, one could also use PyTorch.","83381526":"'input_ids' are the indices assigned to each token. Decoding the 'input_ids' recovers the original tweet plus some special tokens that the tokenizer has introduced for the model. These special tokens include a start of sequence token \"< s >\" at the start of the document, an end of sequence token \"< \\s >\" at the end of a sequence and a padding token \"< pad >\" to fill a sequence to the maximum specified length.","c78c2ccb":"However, the tweets contain some HTML character entities, these are text representations of special characters for HTML. For example: \"&gt\" is to be interpreted as \">\" (greater than). We can easily verify that this only occurs for 3 types of characters, \"&amp\", \"&gt\" and \"&lt\", corresponding to \"&\", \">\" and \"<\", respectively.","6953a279":"This notebook has illustrated how to use the \ud83e\udd17 Huggingface's transformers library to solve a classification task applied to tweets. The RoBERTa-large model with a classifier layer on top is easy to use and can be fine-tuned in just a few epochs. Even with very limited pre-processing of the text, the model achieves a good F1-score showing that it can classify tweets correctly most of the time. Smaller models are also available which sacrifice only a little performance for a great boost in training and inference speed.","ea87a84f":"We can now train the model on our tweets dataset. The dataset objects are already batched, so there is no need to specify the batch size in here. It does not take too long to obtain a good validation score, so 2-3 epochs of training should be enough.","b6c07b69":"# Disaster tweets with RoBERTa","fb9c5bc3":"The confusion matrix and classification report are printed using sklearn's functions","afccfc34":"When dealing with text data, some pre-processing may be necessary as data may not be of the same type that the model was trained on. Here we briefly consider only a few pre-processing steps.\n\nFirst, we know that tweets may contain tags (#tag) and mentions (@name). We will shortly see that the tokenizer can separate the special characters and read the tags and names as a word. It's not clear whether tags and names consisting of multiple words and special characters without space would repesent additional difficulties for the model. An option could be to remove these altogether. However, in this exercise, we keep tags and mentions as they appear.\n\nNext, note that the datasets do not contain unicode codes, which could stand for emojis, for example. This means this data already underwent some type of pre-processing before being published, as tweets often feature emojis in some form. In any case, we wont't have to worry about them in this example.","bd590308":"Although we could replace these expressions for their word equivalents (e.g. \"tbh\": \"to be honest\"), we can immediately see that there are only a handful of examples that contain these irregularities. Given that our training and testing files consist of thousands of examples, replacing these expressions will not have a large impact. In fact, these may be consider as adding some noise, which may help to prevent overfitting.\n\nNote that it is not immediately clear that we would benefit from more intrusive transformations, such as removing punctuation, numbers, undoing contractions or adding special tokens, because RoBERTa has been trained on text that contains all of these elements. Thus, the model should already be able to capture these basic elements of language as it has already seen them before. Misspelings are different, of course, however, we make no effort to fix them in this approach. You can consider running the tweets through a spelling checker and compare the results.\n\nIn summary, we only perform two transformations to the data set, removing URLs and converting HTML character entities to their intended representation. If you prefer not to apply these transformations, simply comment out the following cell.","c5a0998c":"Next, the data is loaded into Tensorflow Datasets. These objects have built-in methods for shuffling and batching the data, and are more efficient for training and inference when dealing with large volumes of data. As RoBERTa-large is a rather heavy model, we have to choose a small batch size, otherwise the examples won't fit in memory.","3aa1b35a":"In this notebook we tackle the [disaster tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started) Kaggle competition using \ud83e\udd17 Huggingface's transformers. Given a tweet, the task is to predict whether it is about a disaster or not. We will leverage an implementation of RoBERTa to solve this task, a language model based on the transformer architecture.\n\nThe challenge is to predict if a tweet refers to an ocurring disaster or if it is about something else. Language is full of figuritave expressions, so it is not straightforward to come up with a rule to classify text as belonging to one or other category that works every time. Even for a human, a message can be difficult to interpret without the appropriate context, which can lead to, sometimes funny, misunderstandings.\n\nAs this is a binary classification task, in principle, any type of classifier can be used, such as logistic regression, SVM, random forest and feed-forward neural networks. These methods make use of the bag of words (BOW) approach to create numerical features, where the order of the words in the text and ther relations are ignored. However, language is a sequential phenomenon and words in a sentence have complex relations between them. More sophisticated language models must be used to capture these relations and extract meaningful information from textual data. The present analysis makes use of RoBERTa, a type of transformer language model put forward by [Facebook AI](https:\/\/ai.facebook.com\/blog\/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems\/). RoBERTa is a version of BERT which has been trained on a larger corpus for a longer time to achieve better performance in NLU (natural language understanding) tasks. BERT, in turn, is a transformer model originally proposed by [Google Research](https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html). A short technical introduction is in order now before we start the analysis.\n\nA [transformer](https:\/\/arxiv.org\/abs\/1706.03762) is a type of artificial neural network that consists of an encoder and a decoder, these are processing blocks that are composed of 'attention' layers. In this context, 'attention' can be thought of as a mechanism to relate inputs and outputs through time. The encoder constructs a high-dimensional numerical representation of textual data. In this form, documents are converted into numeric tensors. The decoder produces an output which depends on both the information from the encoder and on all the previous outputs of the decoder. An example of this type of architecture is [BERT](https:\/\/arxiv.org\/abs\/1810.04805v2) (Bidirectional Encoder Representations from Transformers), a highly-complex model composed of stacks of bi-directional transformers and trained on the BooksCorpus (800M words) and English Wikipedia (2,500M words). BERT can be used for many NLU (natural laguage understanding) tasks, including document classification. RoBERTa is a version of BERT that has been trained on a slightly modified task and with a larger corpus, including news articles, outperforming BERT on all GLUE tasks.\n\nA publicly available implementation of this model (and many more) is offered by a popular python library called [transformers](https:\/\/huggingface.co\/transformers\/) created by \ud83e\udd17 [Huggingface](https:\/\/huggingface.co\/). The models are available both as PyTorch and Tensorflow models, and checkpoints are available which allow easy access to trained models ready for use. This notebook will show step by step how to use this library to solve a classification task with relevant explanations in place.","97566ed8":"There is an element of randomness arising from how the data is split intro training and evaluation sets, and in the training process. This can lead to small variations in the performance of the model. Over several iterations of loading and fine-tuning the model, I have obtained an F1-score of 0.83-0.85 on the evaluation set, and 0.8274-0.8424 on the leaderboard of the competition (the real test set). Thus, the impact of these random factors on the score is rather small.","ffc45d30":"Next, we can produce a classification report to compare the different metrics of the model. To do this, we compute and save the predictions on the labeled testing set. Calling 'predict' on the data outputs a tuple with a single element, an array of two logits. We asign a label based on the index of the largest value, which can be found using 'argmax' (we use 'axis=1' because the model returns a pair of logits for each example). Note that for inference, it is not necessary to apply an activation function to the logits, as  this function does not change the result of taking 'argmax'. You can verify and convince yourself this is true.","56f6a067":"We can verify that both sets contain a similar proportion of positive labels as the original set to make sure that the random splitting has not unintendedly introduced a class imbalance","ec31ca20":"We are now ready to make predictions on the real test set. We extract the tweets from the test file, pass them through the tokeniser and place them into a batched dataset.","dfe49bae":"Finally, we can check the proportion of the predicted positive cases in the test set. Assuming that the observations in the test and training sets come from the same distribution and are randomly sampled, the proportion of positive cases should be similar to what we saw before, ~43%. This does not say anything about how good the model is, but if there is a large difference, it can indicate that something is going wrong, either with the model, or with the way the data is distributed.","595ade89":"Since the encoded tweets will be placed in a tensor for training and inference, they should all be of the same length, so we have to find the length of the longest encoded sequence and pad all tweets to that value. Note that it could be possible that the testing set contains a longer sequence than the training set. Thus, to make sure that we pad to the longest sequence we will find in this exercise, we combine training and testing sets, tokenise them together, and extract the maximum sequence length. In real-life applications where we don't know beforehand what is longest sequence we will find, we can add some arbitrary extra padding just to be safe.\n\n**Important note**: this is the only time we make use of this combined set, as training must be carried out only over the training set to prevent data leakage.","3b39c9cc":"Let's verify the structure of data sets. Each one contains a tuple, where the first element is a dictionary of the encoded tweets and the second is an array of labels","0a187e53":"Assign the id to each prediction and export the data as a csv file ready for submission to the Kaggle competiton.","a3f00682":"We can investigate the distribution of labels. There are slightly fewer positive than negative examples, however this does not represent a significant imbalance, so no further action is required.","b543c4a8":"## 1. Loading the data","10f89718":"The labels are assigned the same way as before, taking the 'argmax' from the outputs of the model for each observation.","d666a697":"You can experiment by calling the tokenizer on more tweets to see how it treats numbers, tags (#), mentions (@), links and other elements present in the tweets.","df555b85":"The 'transformers' library contains many architectures useful for NLU. What makes this library particularly useful is that model checkpoints are available for a wide variety of models. This means that we don't have to train a new model from scratch but can instead load a pre-trained model and fine-tune it for whatever task we want. This is known as transfer learning and allows users to reuse previous knowledge, which is a more efficient way of advancing research. Within RoBERTa, different implementations are available, including a model returning the last hidden states of RoBERTa as-is, and one with a classification head stacked on top which is useful for sentiment analysis and document classification. This notebook will make use of the version that is already prepared for classification as a Tensorflow model. For an analyses that make use of a transformer model as-is and stacks a customer-made classification head on top, see [this approach (BERT)](https:\/\/www.kaggle.com\/dhruv1234\/huggingface-tfbertmodel).\n\nWe also need to load the tokenizer that was used to originally train the model. The tokenizer includes the rules employed to tokenise text, the vocabulary and the dictionary mapping tokens to numerical indices. It is important we use exactly this tokenizer, as the model contains token representations that are identified by the token indices given by this tokenizer.","c1572d9e":"I hope this short tutorial has been useful and please feel free to share your comments, questions and any feedback. Thanks!","87b22576":"Addionally, tweets may contain links, which are rendered in a standard format as \"http(s):\/\/t.co\/xxx\" where the (s) is optional and \"xxx\" stands for an alphanumeric string. Since we know that RoBERTa was trained on books, wikipedia articles and news articles, which do not feature links of this form, we could opt for removing these URLs.\n\nOther processing measures can also be considered. For instance, people often use slang, abbreviations and alternative spellings in their tweets, which are unlikely in the data set that RoBERTa was trained on. For instance, consider the common abbreviations in the following cell","5d07f46b":"Before we can fine-tune the model, we must add an optimiser and a loss function for training. We choose the 'adam' optimiser and set a small learning rate, as we are only doing a fine-tuning of the weights.\n\nAs for the loss function, we choose cross entropy as this is a classification task. Since the last layer of this model contains 2 units, while our training targets (y) are given as a single value per example (i.e. indices, [0] or [1]), the function we must call from Tensorflow is SparseCategoricalCrossentropy. If our targets were given as two values per example (i.e. one-hot encoded, [0,1] or [1,0]), we would use CategoricalCrossentropy; and if the model had only one output, same as our targets, then we would use BinaryCrossentropy. Note that in this example (binary classification) these three functions are all equivalent, which one we choose depends only on the format of the data.\n\nIn summary:\n* Model output: n elements. Target: 1 element.  Use: SparseCategoricalCrossentropy\n* Model output: n elements. Target: n elements. Use: CategoricalCrossentropy\n* Model output: 1 element.  Target: 1 element.  Use: BinaryCrossentropy\n\nFurthermore, recall that the output of the last layer has no activation function, so the model is returning logits. Therefore, when we call the loss function from Tensorflow, we must pass the argument 'from_logits=True' to indicate that the outputs of the model should be passed through an activation function first when computing the loss. According to Tensorflow's [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/SparseCategoricalCrossentropy), this is more numerically stable than adding an activation function explicitly to the last layer of the model.","3012dfef":"Once we have prepared the data sets in the next section, we will verify that the classifier consists of two units with no activation function, giving as output their bare activation values, also known as logits.","1874c559":"## 6. Summary","5a2177f5":"## 4. Fine-tuning RoBERTa"}}