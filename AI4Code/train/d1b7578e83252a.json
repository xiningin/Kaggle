{"cell_type":{"523cec0b":"code","8737c3de":"code","71ca8864":"code","e18bf637":"code","f37bb9b7":"code","64d862fc":"code","f57d258b":"code","6ac801e1":"code","0a4e058c":"code","a801c562":"code","fdb7d655":"code","4e0d1cd8":"code","a9e944c6":"code","49b2e58b":"code","b5aa5b32":"code","5a5db167":"code","2dacb761":"code","4e3eced7":"code","bf5d1ee5":"code","7b32ccd0":"markdown"},"source":{"523cec0b":"import pandas as pd\nimport numpy as np","8737c3de":"data = pd.read_csv('..\/input\/pump-sensor-data\/sensor.csv')\ndata.head()","71ca8864":"data.drop(['sensor_15','Unnamed: 0','sensor_01','sensor_03','sensor_14','sensor_16','sensor_17','sensor_18','sensor_19','sensor_20','sensor_21',\n           'sensor_22','sensor_23','sensor_24','sensor_25','sensor_26','sensor_27','sensor_28','sensor_29','sensor_30',\n           'sensor_31','sensor_33','sensor_34','sensor_37','sensor_36','sensor_48'],\n          inplace=True,axis=1) #droping unwanted feature","e18bf637":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nfrom sklearn.metrics import roc_auc_score","f37bb9b7":"mean=data.iloc[0:int((data.shape[0]*50)\/100)][data.columns[1:-1]].mean().values\nfor i in tqdm(range(1,len(data.columns)-1)):\n    col=data.columns[i]\n    data[col]=data[col].fillna(mean[i-1])\nnp.save(open('NA_replace','wb'),mean)","64d862fc":"def map_fun(x):\n    if x=='NORMAL':\n        return 0\n    else:\n        return 1\n\ndata['machine_status']=data.machine_status.map(map_fun)","f57d258b":"columns=[]\nfor col in data.columns[1:-1]:\n    columns.append('s{0}_median'.format(col[7:])) #to select sensor number\n    columns.append('s{0}_mean'.format(col[7:]))\n    columns.append('s{0}_std'.format(col[7:]))\n    columns.append('s{0}_min'.format(col[7:]))\n    columns.append('s{0}_max'.format(col[7:]))\ncolumns.append('machine_status')","6ac801e1":"columns","0a4e058c":"w=10\n\nX = []\nfor i in data.columns[1:]:\n    X1,X2,X3,X4,X5,X6=[],[],[],[],[],[]\n    if not i =='machine_status':\n        X1.append(data[i].rolling(w).median()) #creating mean min etc for each sensor window\n        X2.append(data[i].rolling(w).mean())\n        X3.append(data[i].rolling(w).std())\n        X4.append(data[i].rolling(w).min())\n        X5.append(data[i].rolling(w).max())\n        fea_data = np.hstack([np.array(X1).reshape(-1,1),np.array(X2).reshape(-1,1),\\\n                       np.array(X3).reshape(-1,1),np.array(X4).reshape(-1,1),\\\n                       np.array(X5).reshape(-1,1)])\n    else:    \n        X6.append(data[i].rolling(w).max()) # taking class label, if there is even singal failure we consider whole window as failure window.\n        fea_data=np.array(X6).reshape(-1,1)\n    X.append(fea_data)","a801c562":"temp_data = X[0]\nfor i in range(1,len(X)):\n    temp_data = np.hstack([temp_data, X[i]])\n\ndata_df = pd.DataFrame(temp_data, columns=columns)\ndata_df","fdb7d655":"data_df=data_df.loc[w-1:]\ntemp1=data_df['machine_status'].iloc[w+w:].values\ntemp2=data['timestamp'].iloc[w:-(w+w-1)].values\ndata_df=data_df.iloc[:-(w+w)].copy()\ndata_df['machine_status']=temp1\ndata_df['timestamp']=temp2\n","4e0d1cd8":"data_df","a9e944c6":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import  recall_score, confusion_matrix","49b2e58b":"X = data_df\nX_train=X.iloc[0:int((X.shape[0]*50)\/100)]\nX_cv=X.iloc[int((X.shape[0]*50)\/100):int((X.shape[0]*75)\/100)]\nX_test=X.iloc[int((X.shape[0]*75)\/100):]\nprint('train shape:',X_train.shape)\nprint('cv shape:',X_cv.shape)\nprint('test shape:',X_test.shape)","b5aa5b32":"X_train_fail=X_train[X_train['machine_status']==1].copy()\nX_train_fail['timestamp']=pd.to_datetime(X_train_fail['timestamp'])\ng1=X_train_fail.groupby(by=X_train_fail['timestamp'].dt.date)\nprint('train failure dates:\\n',g1.groups.keys())\n\nX_cv_fail=X_cv[X_cv['machine_status']==1].copy()\nX_cv_fail['timestamp']=pd.to_datetime(X_cv_fail['timestamp'])\ng2=X_cv_fail.groupby(by=X_cv_fail['timestamp'].dt.date)\nprint('cv failure dates:\\n',g2.groups.keys())","5a5db167":"y_train = X_train['machine_status']\nX_train = X_train.drop(['machine_status'], axis=1)\ny_cv = X_cv['machine_status']\nX_cv = X_cv.drop(['machine_status'], axis=1)\ny_test = X_test['machine_status']\nX_test = X_test.drop(['machine_status'], axis=1)","2dacb761":"for col in X_train.columns[0:-1]:\n    scaler = MinMaxScaler()\n    X_train[col]=scaler.fit_transform(X_train[col].values.reshape(-1, 1))\n    X_cv[col]=scaler.transform(X_cv[col].values.reshape(-1, 1))\n    X_test[col]=scaler.transform(X_test[col].values.reshape(-1, 1))","4e3eced7":"def plot_confusion_matrix(test_y, predict_y):\n    \"\"\" this function print cunfusion matrix and recall matrix\n    \"\"\"\n    labels = [0,1]\n    C = confusion_matrix(test_y, predict_y,labels=labels)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(5,4))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(5,4))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","bf5d1ee5":"clf = RandomForestClassifier(max_features='sqrt',n_jobs=-1,random_state=1)\nclf.fit(X_train[X_train.columns[0:-1]], y_train)\nprint('train confusion and recall matrix:\\n')\nplot_confusion_matrix(y_train, clf.predict(X_train[X_train.columns[0:-1]]))\nprint('cv confusion and recall matrix:\\n')\nplot_confusion_matrix(y_cv, clf.predict(X_cv[X_train.columns[0:-1]]))\nprint('test confusion and recall matrix:\\n')\nplot_confusion_matrix(y_test, clf.predict(X_test[X_train.columns[0:-1]]))\nprint('train AUC score:',roc_auc_score(y_train, clf.predict_proba(X_train[X_train.columns[0:-1]])[:,1]))\nprint('cv AUC score:',roc_auc_score(y_cv, clf.predict_proba(X_cv[X_train.columns[0:-1]])[:,1]))\nprint('test AUC score:',roc_auc_score(y_test, clf.predict_proba(X_test[X_train.columns[0:-1]])[:,1]))","7b32ccd0":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}