{"cell_type":{"56b1dd9f":"code","4066e378":"code","fd69c622":"code","580a9578":"code","7bf34d24":"code","220860cd":"code","6d55e1cf":"code","d568761c":"code","22f660e0":"code","fd812dd2":"code","1f1717c3":"code","ee7b044f":"code","acbf9f0e":"code","a6b5d50b":"code","2b532b56":"code","1ab72491":"code","580b7601":"code","22070dfc":"code","3437c21d":"code","ec021734":"code","6d40dc92":"code","3dd6095f":"code","965159fb":"code","da599826":"code","8bb4a918":"code","a7775d5f":"code","cc9a0378":"code","24195221":"code","612fafe6":"code","bd828812":"code","41f4982f":"code","e8025c23":"code","72e31f42":"code","68798a3e":"code","d6d6c0c4":"code","9843cbd7":"code","7b2965d2":"code","c4acca06":"code","c6039fb4":"code","b5fc2340":"code","fb938b3d":"markdown","905b1c00":"markdown","60b81a8c":"markdown","0e646cb1":"markdown","06153568":"markdown","9b5e4991":"markdown","b0d9afda":"markdown","629a925e":"markdown","39a7cbbb":"markdown","8062ae7c":"markdown","aae5d8a7":"markdown","656642ed":"markdown","f23ec5ea":"markdown","4ed6f569":"markdown","7dab978e":"markdown","381ed2ab":"markdown","d757ee3d":"markdown","e8db4dad":"markdown","9ef3b276":"markdown","408029ba":"markdown","5f9824b7":"markdown","5b191509":"markdown","15ba1918":"markdown","18d44431":"markdown","a0f03c60":"markdown","ee921729":"markdown","48606af0":"markdown","68cbddda":"markdown","c2329197":"markdown"},"source":{"56b1dd9f":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\nfrom sklearn.datasets import make_blobs\nfrom sklearn import preprocessing\nfrom sklearn.cluster import *\nfrom sklearn.mixture import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","4066e378":"from sklearn.datasets import make_blobs\n\nX, y_true = make_blobs(n_samples=300, centers=4,  n_features = 2, cluster_std=0.60)\n\nplt.scatter(X[:, 0], X[:, 1], s=50);","fd69c622":"plt.figure(figsize=(7, 7))\n\nn_samples = 1000\nrandom_state = 17\n\n\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# Incorrect number of clusters\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(\"Incorrect Number of Blobs\")\n\n\n\n\n# Anisotropicly distributed data\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c = y_pred)\nplt.title(\"Anisotropicly Distributed Blobs\")\n\n\n\n\n# Different variance\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(\"Unequal Variance\")\n\n\n\n# Unevenly sized blobs\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\ny_pred = KMeans(n_clusters=3,\n                random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(\"Unevenly Sized Blobs\")\n\nplt.show()\n\n#SOURCE : SKLEARN DOC","580a9578":"ms = MeanShift()\nms.fit(X)\ncluster_centers = ms.cluster_centers_\nplt.scatter(X[:,0], X[:,1],cmap='rainbow', alpha=0.7, edgecolors='r')","7bf34d24":"spectral_model_rbf = SpectralClustering(affinity ='rbf')\n  \n# Training the model and Storing the predicted cluster labels\nlabels_rbf = spectral_model_rbf.fit_predict(X)\nplt.scatter(X[:,0], X[:,1])","220860cd":"brc = Birch(branching_factor=50, n_clusters=None, threshold=1.5)\nbrc.fit(X)\nplt.scatter(X[:,0], X[:,1])","6d55e1cf":"df = pd.read_csv(\"..\/input\/wine-pca\/Wine.csv\")","d568761c":"df.head()","22f660e0":"df.describe()","fd812dd2":"df.columns","1f1717c3":"sns.regplot(x=\"Flavanoids\",y=\"Total_Phenols\",data=df)\nplt.figure(figsize=(16,6))","ee7b044f":"X = df.iloc[:,1:].values\nError =[]\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i).fit(X)\n    kmeans.fit(X)\n    Error.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), Error)\nplt.title('Elbow method')\nplt.xlabel('No of clusters')\nplt.ylabel('Error')\nplt.show()","acbf9f0e":"K = KMeans(n_clusters = 3).fit(X)\ny = K.fit_predict(X)","a6b5d50b":"K.cluster_centers_","2b532b56":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","1ab72491":"# This is a function for removing the outliers\n\ndef outlier_removal(data:pd.DataFrame,std_multi:int=3, inplace:bool=False):\n    df_copy = data if inplace else data.copy()\n    data_shape_before = data.shape\n    print('Before outlier removal   : ', data_shape_before)\n    for name in df_copy.select_dtypes(exclude=\"object\").columns:\n        upper_limit = df_copy[name].mean() + std_multi*df_copy[name].std()\n        lower_limit = df_copy[name].mean() - std_multi*df_copy[name].std()\n        df_copy.drop(df_copy[(lower_limit>df_copy[name]) | (df_copy[name]>upper_limit)].index, inplace = True)\n    print('After removal of outliers: ' ,df_copy.shape)","580b7601":"outlier_removal(data= df,std_multi=2.5, inplace=True)","22070dfc":"new_df = preprocessing.StandardScaler().fit_transform(df)\nnew_df = pd.DataFrame(new_df, columns=['Alcohol', 'Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium','Total_Phenols', 'Flavanoids', \n                                       'Nonflavanoid_Phenols','Proanthocyanins', 'Color_Intensity', 'Hue', 'OD280', 'Proline','Customer_Segment'])","3437c21d":"new_df.head()","ec021734":"X = new_df.iloc[:,1:].values\n\nError =[]\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i).fit(X)\n    kmeans.fit(X)\n    Error.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), Error)\nplt.title('Elbow method')\nplt.xlabel('No of clusters')\nplt.ylabel('Error')\nplt.show()","6d40dc92":"K = KMeans(n_clusters = 3).fit(X)\ny = K.fit_predict(X)","3dd6095f":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","965159fb":"K = AffinityPropagation(damping=0.9, preference=-50).fit(X)\ny = K.fit_predict(X)","da599826":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","8bb4a918":"estimate_bandwidth(new_df, quantile=0.057, n_samples=None, random_state=0, n_jobs=None)","a7775d5f":"K = MeanShift(bandwidth=2.7).fit(X)\ny = K.fit_predict(X)","cc9a0378":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","24195221":"K = SpectralClustering(n_clusters=3).fit(X)\ny = K.fit_predict(X)","612fafe6":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","bd828812":"K = DBSCAN(eps=1,min_samples=4).fit(X)\ny = K.fit_predict(X)","41f4982f":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","e8025c23":"K = Birch(n_clusters= 3,threshold=1.5).fit(X)\ny = K.fit_predict(X)","72e31f42":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","68798a3e":"K = OPTICS(min_samples=3).fit(X)\ny = K.fit_predict(X)","d6d6c0c4":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","9843cbd7":"import scipy.cluster.hierarchy as sch\ndendrogrm = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.show()","7b2965d2":"K = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny = K.fit_predict(X)","c4acca06":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","c6039fb4":"K = GaussianMixture( n_components=3, covariance_type='full').fit(X)\ny = K.fit_predict(X)","b5fc2340":"plt.scatter(X[:,5],X[:,4],c=y,cmap=\"rainbow\")","fb938b3d":"**Lets start with training from a real dataset...**\nHere I have taken a Wine quality dataset.","905b1c00":"Balanced Iterative Reducing and Clustering using Hierarchies(BIRCH)\nThe Birch builds a tree called the Clustering Feature Tree (CFT) . The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\n\nThe BIRCH algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.","60b81a8c":"**OPTICS**","0e646cb1":"Let's start with a artificial samples","06153568":"K-means is often called as Lloyd\u2019s algorithm. It consist of few steps:\nChooses the initial centroids\nAnd then...\n- The first assigns each sample to its nearest centroid.\n- The second step creates new centroids by taking the mean value of the existing centroids.\n- And the rest is looping between the above steps","9b5e4991":"**BIRCH**","b0d9afda":"Th this there is no need to specify the number of clusters. Its more suitable where we don't know the optimal number of clusters. The interesting thing about this machine learning techniques is that you don\u2019t have to configure the number of clusters in advance. \nThe algorithm has a high time complexity, which is a demerit of using it.","629a925e":"Spectral Clustering...","39a7cbbb":"1. Agglomerative Clustering\nScale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.\n\n2. Divisive Clustering\nHere the the whole is considered as one cluster, and there is division on that until right centroids and clusters are found.","8062ae7c":"**GUASSIAN MIXTURE**","aae5d8a7":"**HEIRARCHICAL CLUSTERING**","656642ed":"## Type of Algorithms\n1. K -mean\n2. Affinity propagation\n3. Mean Shift\n4. Spectral Clustering\n5. Hierarchical clustering\n6. DBSCAN\n7. OPTICS\n8. Guassian Matrix\n9. BIRCH","f23ec5ea":"There is low-dimension embedding of the affinity matrix between samples, followed by clustering\nThe present version requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters.\nSpectral clustering is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them.","4ed6f569":"Gaussian Mixture Models assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster.","7dab978e":"The same for BIRCH...","381ed2ab":"**DBSCAN**","d757ee3d":"The example below provides insights on how the data distribution of resulting clusters may be present after the fitting and how can it affect the results...","e8db4dad":"**MEAN SHIFT**","9ef3b276":"**K - MEAN**","408029ba":"The clustering aims on finding blobs in a smooth density of samples present. \n- Centroids with mean of the given region\n- They are revised to remove the duplicates, thus increasing the efficiency of cluster formation.\n\nTher is auto setting of the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through.\nThe algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm.\nAgain the algorithm stops iteration if the change is no more significant.","5f9824b7":"Ordering points to identify the clustering structure(OPTICS).The most common density based approach, DBSCAN, requires only two parameters.\nThe OPTICS algorithm shares many similarities with the DBSCAN algorithm. But its different in some parameters.","5b191509":"Here we can see there is a lot of outlier so will remove the outlier for better visibility of the clusters","15ba1918":"**AFFINITY PROPAGATION**","18d44431":"**AUTHOR** : Prasansha Satpathy<br>\n\n\n**CLUSTERING ALGORITHMS**","a0f03c60":"Using sample data for Mean Shift","ee921729":"Density-based spatial clustering of applications with noise (DBSCAN).\nClusters as areas of high density separated by areas of low density.\nClusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped.\n- DBSCAN groups together points that are close to each other based on a distance measurement (usually Euclidean distance)","48606af0":"**SPECTRAL CLUSTERING**","68cbddda":"This is to provide overview of the clustering algorithms present. Please upvote if it was helpful and would love to know your feedbacks!!","c2329197":"This is more clear than the previous but the shape of the dataset obviously decreased."}}