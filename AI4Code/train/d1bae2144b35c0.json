{"cell_type":{"91a64bcc":"code","0aafe491":"code","d72d0c33":"code","24335611":"code","a69d2532":"code","d8c4fc93":"code","67a1d295":"code","1d35ad57":"code","0be82791":"code","3d3c76c2":"code","9df4e917":"code","11e7d9cc":"code","db163af0":"code","6a2aeaf3":"markdown","36163bf6":"markdown","ea1027aa":"markdown","d375084e":"markdown","f149b1f0":"markdown","003c5b99":"markdown","dcc61037":"markdown","6365905f":"markdown"},"source":{"91a64bcc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0aafe491":"import time \nimport matplotlib.pyplot as plt\nimport seaborn as sns","d72d0c33":"df = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv',index_col = 0)  \ndf","24335611":"y = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv',index_col = 0 )\ny","a69d2532":"mode_which_part_to_process = 'full'\nif mode_which_part_to_process == 'full':\n    # consider only gene expression part \n    X = df[[c for c in df.columns if ('c-' in c) or ('g-' in c)]].values\nif mode_which_part_to_process == 'genes':\n    # consider only gene expression part \n    X = df[[c for c in df.columns if 'g-' in c]].values\nif mode_which_part_to_process == 'c':\n    # consider only gene expression part \n    X = df[[c for c in df.columns if 'c-' in c]].values\n\nX_original_save = X.copy()\nprint(X.shape)\n\ny_sum = y.sum(axis = 1)\ndf['y_sum']=y_sum\nprint(y_sum.shape)\ny_sum.value_counts()\n\n","d8c4fc93":"from sklearn.decomposition import PCA\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npca = PCA()\nt0 = time.time()\nr = pca.fit_transform(X.copy())\nprint(time.time()-t0, 'secs passed for PCA')\n\n\nfig = plt.figure(figsize = (15,7) )\nc = 0\nfor f in ['cp_dose', 'cp_type','cp_time', 'y_sum']:\n    c+=1; fig.add_subplot(1, 4 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1] , hue = df[f]  )\n    plt.title('Colored by '+f)\nplt.show()\n\nfig = plt.figure(figsize = (15,7) )\nfig.add_subplot(1, 2, 1) \nplt.plot(pca.singular_values_,'o-')\nplt.title('Singular values')\nfig.add_subplot(1, 2, 2) \nplt.plot(pca.explained_variance_ratio_,'o-')\nplt.title('explained variance')","67a1d295":"import umap\n\nt0 = time.time()\nr = umap.UMAP().fit_transform(X.copy())\nprint(time.time()-t0, 'secs passed for umap')\n\n\nfig = plt.figure(figsize = (15,7) )\nc = 0\nfor f in ['cp_dose', 'cp_type','cp_time','y_sum']:\n    c+=1; fig.add_subplot(1, 4 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1] , hue = df[f]  )\nplt.show()","1d35ad57":"# Based on: \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\n# See also:\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_lle_digits.html#\n\n\n\n# To speed-up reduce dimensions by PCA first\nX_save = X.copy( )\n#r = pca.fit_transform(X)\n#X = r[:1000,:20]\n\n\n\nimport umap \nfrom sklearn import manifold\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.decomposition import NMF\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.ensemble import RandomTreesEmbedding\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import TruncatedSVD\n\n\nfrom collections import OrderedDict\nfrom functools import partial\nfrom matplotlib.ticker import NullFormatter\n\n\nn_neighbors = 10\nn_components = 2\n# Set-up manifold methods\nLLE = partial(manifold.LocallyLinearEmbedding,\n              n_neighbors, n_components, eigen_solver='auto')\n\nmethods = OrderedDict()\nmethods['PCA'] = PCA()\nmethods['umap'] = umap.UMAP(n_components = n_components)\nmethods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nmethods['ICA'] = FastICA(n_components=n_components,         random_state=0)\nmethods['FA'] = FactorAnalysis(n_components=n_components, random_state=0)\nmethods['LLE'] = LLE(method='standard')\nmethods['Modified LLE'] = LLE(method='modified')\nmethods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\nmethods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\nmethods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n                                           n_neighbors=n_neighbors)\nmethods['NMF'] = NMF(n_components=n_components,  init='random', random_state=0) \nmethods['RandProj'] = SparseRandomProjection(n_components=n_components, random_state=42)\n\nrand_trees_embed = make_pipeline(RandomTreesEmbedding(n_estimators=200, random_state=0, max_depth=5), TruncatedSVD(n_components=n_components) )\nmethods['RandTrees'] = rand_trees_embed\nmethods['LatDirAll'] = LatentDirichletAllocation(n_components=n_components,  random_state=0)\nmethods['LTSA'] = LLE(method='ltsa') \nmethods['Hessian LLE'] = LLE(method='hessian') \n\nlist_fast_methods = ['PCA','umap','FA', 'ICA','NMF','RandProj','RandTrees']\nlist_slow_methods = ['t-SNE','LLE','Modified LLE','Isomap','MDS','SE','LatDirAll','LTSA','Hessian LLE']\n\n# transformer = NeighborhoodComponentsAnalysis(init='random',  n_components=2, random_state=0) # Cannot be applied since supervised - requires y \n# methods['LinDisA'] = LinearDiscriminantAnalysis(n_components=n_components)# Cannot be applied since supervised - requires y \n\n\n# Create figure\nfig = plt.figure(figsize=(25, 16))\n\n# Plot results\nc = 0\nfor i, (label, method) in enumerate(methods.items()):\n    if label not in  list_fast_methods :\n        continue\n        \n    t0 = time.time()\n    try:\n        r = method.fit_transform(X.copy())\n    except:\n        print('Got Exception', label )\n        continue \n    t1 = time.time()\n    print(\"%s: %.2g sec\" % (label, t1 - t0))\n    c+=1\n    fig.add_subplot(2, 3 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1],hue =  df['cp_time'])\n    plt.title(label )\n    plt.legend('')\n\nplt.show()\nX = X_save.copy()\n","0be82791":"# To speed-up reduce dimensions by PCA first and cut the size\nX_save = X.copy( )\nr = pca.fit_transform(X)\ni_cut = 5000\nX = r[:i_cut,:50]\n\n\nlist_slow_methods = ['t-SNE','LLE','Modified LLE','Isomap','MDS','SE','LatDirAll','LTSA','Hessian LLE']\n\n# transformer = NeighborhoodComponentsAnalysis(init='random',  n_components=2, random_state=0) # Cannot be applied since supervised - requires y \n# methods['LinDisA'] = LinearDiscriminantAnalysis(n_components=n_components)# Cannot be applied since supervised - requires y \n\n\n# Create figure\nfig = plt.figure(figsize=(25, 4))\n\n# Plot results\nc = 0\nfor i, (label, method) in enumerate(methods.items()):\n    #if label not in list_slow_methods: # list_fast_methods :\n    #    continue\n        \n    t0 = time.time()\n    try:\n        r = method.fit_transform(X.copy())\n    except:\n        print('Got Exception', label )\n        continue \n    t1 = time.time()\n    print(\"%s: %.2g sec\" % (label, t1 - t0))\n    c+=1\n    fig.add_subplot(1, 4 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1],hue =  df['cp_time'][:i_cut])\n    plt.title(label )\n    plt.legend('')\n    if c%4 == 0:\n        c = 0\n        plt.show()\n        fig = plt.figure(figsize=(25, 4))\n        \n    \n\nplt.show()\nX = X_save.copy()\n","3d3c76c2":"X_save.shape","9df4e917":"from sklearn.decomposition import PCA\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preliminary reduce X to speed up:\nr = pca.fit_transform(X.copy())\ni_cut = 5000\nr = r[:i_cut,:50]\n\n\nn_neighbors = 10\nn_components = 2\nmethod = manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', method='standard')\nt0 = time.time()\nr = method.fit_transform(r)\nt1 = time.time()\nprint(\"%s: %.2g sec\" % ('LLE', t1 - t0))\n\nfig = plt.figure(figsize = (15,7) )\nc = 0\nfor f in ['cp_dose', 'cp_type','cp_time', 'y_sum']:\n    c+=1; fig.add_subplot(1, 4 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1] , hue = df[f][:i_cut]  )\n    plt.title('Colored by '+f)\nplt.show()\n","11e7d9cc":"# Apply transform to full dataset \n\nr = pca.transform(X.copy() )\nr = r[:,:50]\n\nt0 = time.time()\nr = method.transform(r)\nprint(time.time()-t0,'seconds passed')\n\nfig = plt.figure(figsize = (15,7) )\nc = 0\nfor f in ['cp_dose', 'cp_type','cp_time', 'y_sum']:\n    c+=1; fig.add_subplot(1, 4 , c) \n    sns.scatterplot(x=r[:,0], y=r[:,1] , hue = df[f]  )\n    plt.title('Colored by '+f)\nplt.show()\n","db163af0":"df[f].value_counts() # strange that seaborn plots only 0,2,5,7 values","6a2aeaf3":"# UMAP plots","36163bf6":"# PCA plots","ea1027aa":"## Fast methods","d375084e":"# Visualize data by various dimensional reduction methods","f149b1f0":"# LLE ","003c5b99":"# Slow methods \n\nto speed up we cut X to smaller size ","dcc61037":"# What is about ?\n\n**Briefly:** visualization via  dimensional-reductions (pca, umap, tsne,...) are presented for datasets of MoA kaggle competition. The idea was to look for some clusters which often show up \nin genes expressions dataset (clusters corresponds for example to different cell-types).\n\nUnfortunately the result seems to be NEGATIVE - there seems to be no clear  cluster structure show ups at glance in that dataset.\n\n\nIt might not be a surpise since rows of the data are drugs not cells\/tissues. \n\nThe only thing we observe some clustering seeing by LLE method. Clusters more or less   corresponds to  cp_time feature.\nWhich is natural, since drug use duration should affect strongly the viability and genes expressions. \nOne may try to use LLE as additional feature. \n\n\n**Biological context:**\n772 out of 876 features at MoA competition are said to be \"gene expressions\". \nWhich biologically means how \"hard\" particular gene is working in the particular cell (or group of cells).\nNote that all cells in the ogranism have the same genes, and one cell differs from another only by these gene expressions.\nCurrently much is known  what genes are expressed in partcilar cell types. \nFor a random example - \"KIT\"-gene is highly expressed in must\/stem cells https:\/\/en.wikipedia.org\/wiki\/KIT_(gene) \n\nTo summarize: \"gene expressions\" give a kind of a \"portrait\" of a cell and bioinformaticians\nare working how to extract as much information as possible from these \"portraits\" and further use it for disease and drug studies. \n\n\nStarting from around 2014  next-generation sequencing (NGS) technologies \nhttps:\/\/en.wikipedia.org\/wiki\/Single_cell_sequencing\nprovided huge amounts of these  \"portraits\" (datasets) for various cells from various organisms. \n\nSome examples of such datasets and information is available at kaggle also:\nhttps:\/\/www.kaggle.com\/chrispr\/single-cell-rna-seq-from-stoeckius-et-al-2017\nSome general tutorial: \nhttps:\/\/www.kaggle.com\/usharengaraju\/single-cell-rna-sequencing\n\nWe follow:\nhttps:\/\/www.kaggle.com\/alexandervc\/dim-reduction-and-plots-scrnaseq-nestorova16\n\n\nPS\n\nIt might be unfortune that genes names are anonimized in the MoA competition.\n","6365905f":"# Load data"}}