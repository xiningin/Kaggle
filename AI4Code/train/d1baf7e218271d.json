{"cell_type":{"094d1d5f":"code","e8659c39":"code","96855c48":"code","d1182c9b":"code","6f9d42a8":"code","54a6de71":"code","65d804fc":"code","e0fbecd8":"code","7d70f739":"code","8eac8a57":"code","7ebcc747":"code","d7315f87":"code","e14157ce":"code","719064e9":"code","ad2a883f":"code","431a99c8":"code","d9fce61c":"markdown","5c121e08":"markdown","8e39f939":"markdown","8a56daf7":"markdown","0e6863c6":"markdown","c7d6619d":"markdown","1d52fde7":"markdown","595f368e":"markdown","7ae0f003":"markdown","85681a00":"markdown","8bdfde8e":"markdown","7edbd217":"markdown","5d839936":"markdown","95285ef2":"markdown","0c856eec":"markdown","ffad17c6":"markdown","e55a6ad1":"markdown"},"source":{"094d1d5f":"# Essential\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy import special \nfrom scipy.stats import boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os","e8659c39":"# Read in the dataset as a dataframe\nprint('Loading data...')\ntrain = pd.read_csv('..\/input\/news-popularity-ml-thon-2021\/news_train.csv')\ntest = pd.read_csv('..\/input\/news-popularity-ml-thon-2021\/news_test.csv')\nprint('Data loaded')","96855c48":"#train = train.head(10000)\n\n# Remove the Ids and covariant features from train and test\ntrain_ID = train['row_id']\ntest_ID = test['row_id']\ntrain.drop(['row_id', 'n_non_stop_unique_tokens', 'avg_positive_polarity', 'weekday'], axis=1, inplace=True)\ntest.drop(['row_id', 'n_non_stop_unique_tokens', 'avg_positive_polarity', 'weekday'], axis=1, inplace=True)\nprint(train.shape, test.shape)\n\n# remove labels from dataset make separate\ntrain_labels = train['shares'].reset_index(drop=True)\ntrain_features = train.drop(['shares'], axis=1)","d1182c9b":"# split into train and cross val\nX,cross_features,Y,cross_labels = train_test_split(train_features, train_labels, test_size=0.2, shuffle=False)","6f9d42a8":"# combining train features and labels in one dataset\ntrain = X.copy()\ntrain['shares'] = Y\n\ntest_features = test\n\n#Check the share distribution\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(train['shares'], color=\"b\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Shares\")\nax.set(title=\"Shares distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\ntrain.boxplot(column='shares')\nplt.show()","54a6de71":"# Transforming train target variables (log transformation)\ntrain[\"shares\"] = np.log1p(train[\"shares\"])\n(mu, sigma) = norm.fit(train['shares'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nsns.distplot(train['shares'], color=\"b\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Shares\")\nax.set(title=\"Shares distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\n# separate train labels from features\ntrain_labels = train['shares'].reset_index(drop=True)\ntrain_features = train.drop(['shares'], axis=1)\n\n# Fill in missing data\ndef handle_missing(features):\n    return features.fillna(-(features.mean()+10*features.std()))\n\ntrain_features = handle_missing(train_features)\ncross_features = handle_missing(cross_features)\ntest_features = handle_missing(test_features)\n\n#Remove outliers\nfor i in train.columns:\n  train = train[(np.abs((train[i]-train[i].mean())\/train[i].std()) < 2.5)]\n\ntrain.boxplot(column='shares')\nplt.show()","65d804fc":"# normalize features (the test and cross_val datasets are normalized based on the train dataset to reduce data leakage)\ndef normalizeTrain(df):\n    result = df.copy()\n    minmax = []\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        minmax.append([max_value, min_value])\n        result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result, minmax\n\ndef normalizeTest(df, minmax):\n    result = df.copy()\n    i = 0\n    for feature_name in df.columns:\n        max_value = minmax[i][0]\n        min_value = minmax[i][1]\n        i += 1\n        result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result\n\n(train_features, Xmaxmin) = normalizeTrain(train_features)\ncross_features = normalizeTest(cross_features, Xmaxmin)\ntest_features = normalizeTest(test_features, Xmaxmin)","e0fbecd8":"ax = sns.boxplot(data=train_features, orient=\"h\", palette=\"Set1\")\nplt.show()","7d70f739":"# define metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_log_error(y, y_pred))\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","8eac8a57":"# define model(s)\ngbr = GradientBoostingRegressor(n_estimators=350, \n                                learning_rate=0.05,\n                                max_depth=6,\n                                subsample=1,\n                                max_features='sqrt',\n                                min_samples_leaf=25,\n                                min_samples_split=5,\n                                loss='huber',\n                                random_state=42)\n\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=800,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.8,\n                       subsample=0.6,\n                       colsample_bytree=0.6,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=26,\n                       reg_alpha=0.00005,\n                       random_state=42)\n\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=10,\n                       learning_rate=0.01, \n                       n_estimators=3000,\n                       max_bin=20, \n                       bagging_fraction=0.9,\n                       bagging_freq=5, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\nsvr = make_pipeline(RobustScaler(), SVR(C=1, epsilon=0.006, gamma=0.005))\n\nlinear = LinearRegression()\n\nstack_gen = StackingCVRegressor(regressors=(xgboost, gbr, lightgbm),\n                            meta_regressor=lightgbm,\n                            use_features_in_secondary=True)","7ebcc747":"# fit model(s)\nprint('StackGen')\nstack_gen_model = stack_gen.fit(np.array(train_features), np.array(train_labels))\nprint('stack_gen done!')\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(train_features, train_labels)\nprint('gbr done!')\nprint('XGBoost')\nxgb_model_full_data = xgboost.fit(train_features, train_labels)\nprint('xgboost done!')\nprint('LightGBM')\nlgbm_model_full_data = lightgbm.fit(train_features, train_labels)\nprint('Lgbm done!')\nprint('Ridge')\nridge_model_full_data = ridge.fit(train_features, train_labels)\nprint('ridge done!')\nprint('Svr')\nsvr_model_full_data = svr.fit(train_features, train_labels)\nprint('svr done!')\nprint('Linear')\nlinear_model_full_data = linear.fit(train_features, train_labels)\nprint('linear done!')","d7315f87":"# get predictions\npredictions_stack_gen = stack_gen_model.predict(np.array(cross_features))\npredictions_gbr = gbr_model_full_data.predict(cross_features)\npredictions_xgb = xgb_model_full_data.predict(cross_features)\npredictions_lgbm = lgbm_model_full_data.predict(cross_features)\npredictions_ridge = ridge_model_full_data.predict(cross_features)\npredictions_svr = svr_model_full_data.predict(cross_features)\npredictions_linear = linear_model_full_data.predict(cross_features)","e14157ce":"#Minimization function for weights in the blended model\nx0 = np.array([0.16,0.16,0.16,0.16,0.16,0.2])\ndef cost(arr):\n    return (rmsle(cross_labels,abs(np.expm1(\n            (arr[0] * predictions_ridge) +\n            (arr[1] * predictions_svr) +\n            (arr[2] * predictions_gbr) +\n            (arr[3] * predictions_xgb) +\n            (arr[4] * predictions_lgbm) +\n            (arr[5] * predictions_stack_gen)))))\n\n\nres = minimize(cost, x0, method='SLSQP', constraints=({'type': 'eq', 'fun': lambda x:  (x[0]+x[1]+x[2]+x[3]+x[4]+x[5])-1}),\n            bounds=((0,1),(0,1),(0,1),(0,1),(0,1),(0,1)), tol=1e-6)\n\npredictions_blended = (\n            (res.x[0] * predictions_ridge) +\n            (res.x[1] * predictions_svr) +\n            (res.x[2] * predictions_gbr) +\n            (res.x[3] * predictions_xgb) +\n            (res.x[4] * predictions_lgbm) +\n            (res.x[5] * predictions_stack_gen))","719064e9":"# scale results\ndef scaler(predictions, cross):\n    print('finding optimum scaling factor...')\n    s = 0.855\n    upsc = 999\n    fins = 1\n    while (s<1.25):\n        sc = rmsle(cross, (np.expm1(predictions)*s))\n        if sc<upsc:\n            upsc=sc\n            fins = s\n        s+=0.005\n    return fins\n\nscale_stackgen = scaler(predictions_stack_gen, cross_labels)\nscale_blended = scaler(predictions_blended, cross_labels)","ad2a883f":"# getting scores\nprint(\"----------RMSLE SCORE------------\")\nprint(\"STACKGEN\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_stack_gen))*scale_stackgen))\nprint(\"GBR\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_gbr))))\nprint(\"XGB\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_xgb))))\nprint(\"LGBM\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_lgbm))))\nprint(\"RIDGE\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_ridge))))\nprint(\"SVR\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_svr))))\nprint(\"Linear Regression\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_linear))))\nprint(\"BLENDED\")\nprint(rmsle(cross_labels, abs(np.expm1(predictions_blended))*scale_blended))","431a99c8":"# submissions\nsubmission = pd.read_csv(\"..\/input\/news-popularity-ml-thon-2021\/sampleSubmission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(stack_gen_model.predict(np.array(test_features)))*scale_stackgen)\n# Fix outlier predictions\nq1 = submission['shares'].quantile(0.0045)\nq2 = submission['shares'].quantile(0.99)\nsubmission['shares'] = submission['shares'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['shares'] = submission['shares'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submissionRegressionStackGen.csv\", index=False)\n\nsubmission = pd.read_csv(\"..\/input\/news-popularity-ml-thon-2021\/sampleSubmission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(\n            (res.x[0] * ridge_model_full_data.predict(test_features)) +\n            (res.x[1] * svr_model_full_data.predict(test_features)) +\n            (res.x[2] * gbr_model_full_data.predict(test_features)) +\n            (res.x[3] * xgb_model_full_data.predict(test_features)) +\n            (res.x[4] * lgbm_model_full_data.predict(test_features)) +\n            (res.x[5] * stack_gen_model.predict(np.array(test_features))))*scale_blended)\n# Fix outlier predictions\nq1 = submission['shares'].quantile(0.0045)\nq2 = submission['shares'].quantile(0.99)\nsubmission['shares'] = submission['shares'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['shares'] = submission['shares'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submissionRegressionBlended.csv\", index=False)\n\nprint('submission complete!')","d9fce61c":"We temproarily recombined the train features and labels in order to help with plotting some graphs to get an idea of the distribution of the target variable and features.","5c121e08":"We then read in the required files: the train and test datasets","8e39f939":"When we were testing our models, we found it easier to see if they worked by restricting the amount of data fed to train them. The scores from the restricted data was not used to evaluate the models but rather to see if they ran properly in a short period of time, which allowed for faster development. We commented this line out in the final version to use all the training data. After this, we dropped the \"useless\" columns, such as train_id and test_id, and the features which had high covariance with each other. We used backwards stepwise regression (not shown here) to reduce collinearity amongst features and reduce overfitting. This was also to remove redundant features and make it easier and faster for the models to train.","8a56daf7":"We decided to exploit the nature of RMSLE, which tends to punish under-predictions over over-predictions. For example, predicting a value of 1100 when the true value is 1000 will be less penalized than when predicting 900. A scaling function here boosts the value of the results a bit in order to improve the RMSLE score this way. ","0e6863c6":"Then we defined all our models. Our plan was to train multiple models, and then use stacking and blending. Despite having defined many models, we only ended up using the few most effective ones after testing. We used GridSearchCV to find the optimum hyperparameters for some of these models, and intuitively experimented for the rest ;)","c7d6619d":"For the blended model, we used the minimize function from scipy to find the optimum weights to multiply into each of the models' predictions.","1d52fde7":"After separating the labels from features, we split them into train and cross_validation sets","595f368e":"**Hi all! In this notebook we're going to outline the steps we took to place 1st in the all India 'News Popularity - ML-thon 2021'.**","7ae0f003":"We then obtained the predictions for the cross_val dataset:","85681a00":"We begin by importing our packages. We used seaborn for plotting, and models from sklearn. We used scipy for stats. ","8bdfde8e":"And then we wrote the submission output file (one for stackgen and one for blended because at times stackgen outperformed blended):","7edbd217":"We'll now normalize the train features so that all variables lie between 0 and 1. Notice how we used our own algorithms for flexibility.","5d839936":"We then trained our models:","95285ef2":"So now we know the 'shares' distribution is heavily skewed to the right and there are quite a few outliers. We then perform some data processing to solve these issues: a log transformation on the target variables and removing outliers based on the target variable. We also filled in missing data and removed outliers from features.","0c856eec":"We then set up metrics and cv folds (only for training Ridge)","ffad17c6":"We plotted some boxplots of the features to see if there was any severe skew or other issues to fix. We were good to go though!","e55a6ad1":"We finally display the scores:"}}