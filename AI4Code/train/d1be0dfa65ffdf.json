{"cell_type":{"eff8a675":"code","2c4b3034":"code","86f16b19":"code","6219be2e":"code","a099da67":"code","c86f61bd":"code","855f8916":"code","c6042a0f":"code","6dc83cc9":"code","46d1cedc":"code","a7fd3aa9":"code","cf48f3b6":"code","58b033ef":"code","6bdbd714":"code","03fa3489":"markdown","758dbd48":"markdown","88ba8ca5":"markdown","38141c3b":"markdown","77e9a8f8":"markdown","1d8e91f9":"markdown","e90970b3":"markdown","b935edad":"markdown","3f76ee47":"markdown","4a255353":"markdown","6bd51af0":"markdown","94a1a2b1":"markdown","e1ef013e":"markdown","bfb76783":"markdown","a000bd5b":"markdown","9eec82b5":"markdown"},"source":{"eff8a675":"import time\nimport numpy as np\nimport pandas as pd\nimport gresearch_crypto\nfrom numpy import dtype\n\nasset_details = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\nid_2_weight = dict(zip(asset_details.Asset_ID, asset_details.Weight))\n\ndtypes = {'timestamp': np.int64, 'Asset_ID': np.int8,\n          'Count': np.int32,     'Open': np.float64,\n          'High': np.float64,    'Low': np.float64,\n          'Close': np.float64,   'Volume': np.float64,\n          'VWAP': np.float64,    'Target': np.float64}\n\ndef datestring_to_timestamp(ts):\n    return int(pd.Timestamp(ts).timestamp())\n\ndef read_csv_slice(file_path='..\/input\/g-research-crypto-forecasting\/train.csv', dtypes=dtypes, use_window=None):\n    df = pd.read_csv(file_path, dtype=dtypes)\n    if use_window is not None: \n        df = df[(df.timestamp >= use_window[0]) & (df.timestamp < use_window[1])]\n    return df\n\ndef weighted_correlation(a, b, weights):\n    w = np.ravel(weights)\n    a = np.ravel(a)\n    b = np.ravel(b)\n    sum_w = np.sum(w)\n    mean_a = np.sum(a * w) \/ sum_w\n    mean_b = np.sum(b * w) \/ sum_w\n    var_a = np.sum(w * np.square(a - mean_a)) \/ sum_w\n    var_b = np.sum(w * np.square(b - mean_b)) \/ sum_w\n    cov = np.sum((a * b * w)) \/ np.sum(w) - mean_a * mean_b\n    corr = cov \/ np.sqrt(var_a * var_b)\n    return corr","2c4b3034":"start = datestring_to_timestamp('2021-06-13T00:00')\nend = datestring_to_timestamp('2021-09-22T01:00')\ntrain_df = read_csv_slice(use_window=[start, end])","86f16b19":"class API:\n    def __init__(self, df):\n        df = df.astype(dtypes)\n        df['row_id'] = df.index\n        dfg = df.groupby('timestamp')\n        \n        self.data_iter = dfg.__iter__()\n        self.init_num_times = len(dfg)\n        self.next_calls = 0\n        self.pred_calls = 0\n        self.predictions = []\n        self.targets = []\n        \n        print(\"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set. ;)\")\n\n    def __iter__(self):\n        return self\n    \n    def __len__(self):\n        return self.init_num_times - self.next_calls\n        \n    def __next__(self):\n        assert self.pred_calls == self.next_calls, \"You must call `predict()` successfully before you can get the next batch of data.\"\n        timestamp, df = next(self.data_iter)\n        self.next_calls += 1\n        data_df = df.drop(columns=['Target'])\n        true_df = df.drop(columns=['timestamp','Count','Open','High','Low','Close','Volume','VWAP'])\n        true_df = true_df[['row_id', 'Target', 'Asset_ID']]\n        self.targets.append(true_df)\n        pred_df = true_df.drop(columns=['Asset_ID'])\n        pred_df['Target'] = 0.\n        return data_df, pred_df\n    \n    def predict(self, pred_df):\n        assert self.pred_calls == self.next_calls - 1, \"You must get the next batch of data from the API before making a new prediction.\"\n        assert pred_df.columns.to_list() == ['row_id', 'Target'], \"Prediction dataframe should have columns `row_id` and `Target`.\"\n        pred_df = pred_df.astype({'row_id': dtype('int64'), 'Target': dtype('float64')})\n        self.predictions.append(pred_df)\n        self.pred_calls += 1\n        \n    def score(self, id_2_weight=id_2_weight):\n        pred_df = pd.concat(self.predictions).rename(columns={'Target':'Prediction'})\n        true_df = pd.concat(self.targets)\n        scoring_df = pd.merge(true_df, pred_df, on='row_id', how='left')\n        scoring_df['Weight'] = scoring_df.Asset_ID.map(id_2_weight)\n        scoring_df = scoring_df[scoring_df.Target.isna()==False]\n        if scoring_df.Prediction.var(ddof=0) < 1e-10:\n            score = -1\n        else:\n            score = weighted_correlation(scoring_df.Prediction, scoring_df.Target, scoring_df.Weight)\n        return scoring_df, score","6219be2e":"api = API(train_df)","a099da67":"(data_df, pred_df) = next(api)\ndata_df.head(3)","c86f61bd":"# next(api)","855f8916":"api.predict(pred_df)","c6042a0f":"(data_df, pred_df) = next(api)\napi.predict(pred_df)","6dc83cc9":"api.predictions","46d1cedc":"len(api)","a7fd3aa9":"api2 = API(train_df)","cf48f3b6":"start_time = time.time()\n\nfor (data_df, pred_df) in api:\n    pred_df['Target'] = 0.\n    api.predict(pred_df)\n    \nfinish_time = time.time()\n\ntotal_time = finish_time - start_time\niter_speed = api.init_num_times\/total_time\n\nprint(f\"Iterations\/s = {round(iter_speed, 2)}.\")\ntest_iters = 60 * 24 * 100\nprint(f\"Expected number of iterations in test set is approx. {test_iters}\",\n      f\"which will take {round(test_iters \/ iter_speed, 2)}s\",\n      \"using this API emulator while making dummy predictions.\")","58b033ef":"df, score = api.score()\nprint(f\"Your LB score is {round(score, 4)}\")","6bdbd714":"api = API(train_df)\n\nfor (data_df, pred_df) in api:\n    pred_df['Target'] = np.random.randn(len(pred_df), 1)\n    api.predict(pred_df)\n    \ndf, score = api.score()\n\nprint(f\"Your LB score is {round(score, 4)}\")","03fa3489":"Select and load the subset that you're interested in filling the API with. *The current setting is for the slice used by the public LB.*","758dbd48":"#### Calculate your LB score!\n\nThe API now has a `score` method. This returns:\n+ a dataframe containing your predictions, the targets, and weights,\n+ the LB score: weighted correlation between predictions and targets.","88ba8ca5":"We'll get an error if we try to continue on to the next batch without making our predictions for the current batch. *Commented out so that the notebook doesn't Fail.*","38141c3b":"# Local API Emulator\n\n**NEW**: the updated version of Local API Emulator is neater and packaged as a module with a demo available here:\nhttps:\/\/www.kaggle.com\/jagofc\/local-api-emulator-is-now-a-module\n\n**OLD**: Wouldn't it be great if we could test the API locally on our own slices of data? Well, now you can - here's my crack at a local API emulator with LB scoring. \n\n#### It has the properties you've come to expect and love from the real API:\n\n+ Delivers data with the same formatting as that from the real API, grouped by `timestamp`.\n+ Has a `predict()` method.\n+ Berates you if you try to:\n    + get data for time `t_2` before calling `predict()` for time `t_1`.\n    + predict for time `t_1` before getting the data for time `t_1`.\n\n#### It also has the following functionality:\n\n+ Collects your predictions in a list of dataframe slices which is accessible as an attribute `predictions`.\n+ Has a length method which counts the number of unique timestamps remaining to be served.\n+ Gives familiar error messages if you don't follow protocol.\n+ Has a `score` method which:\n    + computes the weighted correlation between your predictions and the true targets.\n    + correctly handles constant predictions (giving a score of -1).\n\n#### This is not the *real* API.\n\nThis is just an emulator. It is easy to cheat it in ways that I hope won't be possible for real API.\\\ne.g. I think it's highly unlikely that - when running code for the private LB - the hosts store a list containing all of your predictions that can be modified post-hoc by participants... (However if you've played around with API in your interactive notebook you'll see that **is** possible to modify the predictions list in the local `gresearch_crypto` env :O)\n\n#### You might find this code useful for:\n\n+ realistically testing your models on a different time period to that used for the public LB,\n+ avoiding the opaque submission procedure,\n+ racking up more submissions attempts than the ordained 5 per day.","77e9a8f8":"Let's make a dummy prediction using `pred_df`.","1d8e91f9":"#### Main loop\n\nAn example loop (making dummy predictions of Target=0) with a timing estimate for 100 days worth of data.","e90970b3":"Now you can continue to iterate. Lets get another slice of data and make another dummy prediction:","b935edad":"Preliminaries.","3f76ee47":"Get the first batch of data.","4a255353":"Your predictions are stored by the API. Let's just look at the first two prediction batches we made:","6bd51af0":"The API also has a length method, which tracks the number of timestamps still to be served:","94a1a2b1":"Note that you don't need to to restart the notebook kernel in order to make a new emulator (or to refresh the current one), in contrast with the `gresearch_crypto` env.","e1ef013e":"Create an API instance.","bfb76783":"Here's code for the local API emulator.","a000bd5b":"## Code & step-by-step Demo","9eec82b5":"### A TL;DR example with random predictions"}}