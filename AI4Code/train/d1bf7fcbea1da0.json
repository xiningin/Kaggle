{"cell_type":{"2c557352":"code","b8028ebc":"code","bf30f693":"code","3130b89e":"code","86d2165f":"code","7d5843d1":"code","7e363689":"code","650eea15":"code","05b6d2bd":"code","188976c1":"code","2cd4d71c":"code","af0e7236":"code","1c072763":"code","fa8403d7":"code","42bcdac7":"code","3d3afbc2":"code","f2dfc4f9":"code","f43ad636":"code","cda88be9":"code","ef7be373":"code","0b8d53e0":"code","958a2350":"code","f61feaf8":"code","356e9431":"code","efe40665":"code","84c27c06":"code","cf93ba93":"code","5bc9dafd":"code","f4d09a44":"markdown","e1e65f86":"markdown","7c5d6f86":"markdown","9b8ac98f":"markdown","9c0b3375":"markdown","bd5e63ec":"markdown","586031be":"markdown","f6c446aa":"markdown","5c87b76e":"markdown","6209b76c":"markdown","7be66b75":"markdown","fadde2a6":"markdown","f3e3c6c9":"markdown","98e26407":"markdown","489046c3":"markdown","6fac90e3":"markdown","f0341f23":"markdown","40e1ba12":"markdown","516238ec":"markdown"},"source":{"2c557352":"!pip install -U lightautoml","b8028ebc":"# Standard python libraries\nimport os\nimport time\nfrom tqdm import tqdm\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score,roc_auc_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.report.report_deco import ReportDeco\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDecoNLP","bf30f693":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 22\nTIMEOUT =  2*3600\nTARGET_NAME = 'TARGET'\n\ntarget = 'TARGET'","3130b89e":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)   ","86d2165f":"import sys\n\ndef optimize_dataframe(df):\n    \"\"\"Optimize pandas dataframe size:\n    - downcast numeric (int and float) types columns.\n    - convert to Categorical type categorical columns with 2x or more \"values\/unique\" values rate.\n    :param df:\n    :return:\n    \"\"\"\n\n    #return df  # TODO: remove - check for failure!!!\n\n    int_cols = []\n    float_cols = []\n    category_cols = []\n    other_cols = []\n\n    old_size = sys.getsizeof(df)\n\n    for col_name in df.columns:\n        col_type = df.dtypes[col_name]\n\n        if col_type in ['int', 'int16', 'int32', 'int64']:\n            int_cols.append(col_name)\n        elif col_type in ['float', 'float16', 'float32', 'float64']: # float 16\n            float_cols.append(col_name)\n        elif col_type == 'object':\n            total = len(df[col_name])\n            n_uniq = df[col_name].nunique()\n            if n_uniq \/ total < 0.5:\n                category_cols.append(col_name)\n            else:\n                other_cols.append(col_name)\n        else:\n            other_cols.append(col_name)\n\n    df_opt = pd.DataFrame()\n\n    if len(int_cols) > 0:\n        df_opt[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n\n    if len(float_cols) > 0:\n        df_opt[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n\n    if len(category_cols) > 0:\n        df_opt[category_cols] = df[category_cols].astype('category')\n\n    if len(other_cols) > 0:\n        df_opt[other_cols] = df[other_cols]\n\n    new_size = sys.getsizeof(df_opt)\n    print('optimize dataframe ({} to {}, ratio: {})'.format(old_size, new_size, round(old_size\/new_size, 2)))\n\n    return df_opt","7d5843d1":"dtypes = {\n    'OKFS_GROUP' : str, 'OKOPF_GROUP' : str, 'OKOGU_GROUP' : str, 'OKATO_REGIONCODE' : str,\n    'OKATO_FED' : str, 'OKTMO_CODE' : str,  'OKTMO_FED' : str, 'OKVED_CODE' : str }\n\n\ntrain = pd.read_csv('..\/input\/data-science-battle-mkb\/train_dataset_hackathon_mkb.csv', sep=';', encoding ='cp1251', dtype=dtypes)\ntest = pd.read_csv('..\/input\/data-science-battle-mkb\/test_dataset_hackathon_mkb.csv', sep=';', encoding ='cp1251', dtype=dtypes)\n\nprint('train', train.shape)\nprint('test', test.shape)\n\n# train = optimize_dataframe(train) # \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0437\u0430\u043f\u0438\u0441\u044c \u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0447\u0438\u0441\u0435\u043b \u0432 pandas dataframe\n# test = optimize_dataframe(test) ","7e363689":"print('train', train.shape)\n# train = train[~train.drop(['id_contract'], axis=1).duplicated(keep='first')]\ntrain.drop(['id_contract'], axis=1).drop_duplicates(inplace=True)\nprint('train', train.shape)","650eea15":"def make_features(data):\n    print('\u041d\u0430\u0447\u0430\u043b \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438')\n    data = data.copy()\n\n    \n    str2num = { '5000 \u0438 \u0431\u043e\u043b\u0435\u0435': 12, '1001 .. 5000': 11, '501 .. 1000': 10, '251 .. 500': 9, '201 .. 250': 8, '151 .. 200': 7,\n             '101 .. 150': 6, '51 .. 100': 5, '16 .. 50': 4, '11 .. 15': 3, '6 .. 10': 2, '0 .. 5': 1}\n    data['WORKERSRANGE'] = data['WORKERSRANGE'].map(str2num).astype(float)\n    \n    data['debt'] = (data['F1410']+data['F1510'])\n    data['debt_on_profit'] = (data['F1410']+data['F1510'])\/data['F2110']\n    data['debt_on_profit_LAG1'] = (data['F1410_LAG1']+data['F1510_LAG1'])\/data['F2110_LAG1']\n    data['debt_on_profit_diff']  = data['debt_on_profit'] - data['debt_on_profit_LAG1']\n    data['debt_on_profit_diff_ratio']  = (data['debt_on_profit'] - data['debt_on_profit_LAG1'])\/data['debt_on_profit']\n    \n    #  \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043b\u0430\u0433\u043e\u0432 \u043f\u0440\u043e\u0448\u043b\u044b\u0445 \u043b\u0435\u0442\n    lags = [ 'F1150_LAG1', 'F1230_LAG1', 'F1410_LAG1', 'F1510_LAG1', 'F1520_LAG1', 'F2110_LAG1', 'F2120_LAG1', \n         'F2200_LAG1', 'F2210_LAG1', 'F2220_LAG1', 'F2300_LAG1', 'F2320_LAG1', 'F2330_LAG1', 'F2400_LAG1']\n\n    for lag in lags:\n        col = lag.split('_')[0]\n        data[col + '_delta_ratio'] = (data[col] - data[lag])\/data[col]\n\n    \n    data['fit_on_all']=data['F2200']\/data['F2110']\n    data['in'] = data['F1300'] + data['F2120'] + data['F2110'] + data['F1210'] + data['F2300']\n    data['out'] = data['F1250'] + data['F1230'] + data['F1520'] + data['F1510'] + data['F2410']\n    data['waste'] = data['F2350'] + data['F2210']\n    data['in_on_out'] = data['in']\/data['out']\n    data['in_on_waste'] = data['in']\/data['waste']\n    data['in_on_waste_out'] = data['in']\/(data['waste'] + data['out'])\n    \n    #### \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f Year\/EVER \n     \n    data.rename(columns={'NOTADMITTEDNUMBER_EVER': 'NOTADMITTEDNUMBER_95_EVER','THIRDOROTHERPERSON_YEAR':'THIRDOROTHERPERSON_CASESNUMBER_YEAR',\n                         'THIRDOROTHERPERSON_EVER':'THIRDOROTHERPERSON_CASESNUMBER_EVER'}, inplace=True)    \n\n    ever_year_info = ['PLAINTIFF_SUM','PLAINTIFF_CASESNUMBER','DEFENDANT_CASESNUMBER','DEFENDANT_SUM', 'THIRDOROTHERPERSON_CASESNUMBER',\n                      'ADMITTEDNUMBER_233','NOTADMITTEDNUMBER_233','WINNERNUMBER_233', 'SIGNEDNUMBER_233','SUM_233','ADMITTEDNUMBER_95',\n                      'NOTADMITTEDNUMBER_95','SIGNEDNUMBER_95', 'WINNERNUMBER_95','SUM_95','COUNT_CHANGE']\n    \n    for col in ever_year_info:\n        data['{}_YEAR'.format(col)] = data['{}_YEAR'.format(col)].fillna(0)\n        data['{}_EVER'.format(col)] = data['{}_EVER'.format(col)].fillna(0)\n        \n        func_ration = lambda x: -1 if x['{}_EVER'.format(col)] == 0 else x['{}_YEAR'.format(col)]\/x['{}_EVER'.format(col)]\n        data['RATIO_YEAR_EVER_{}'.format(col)] = data[['{}_YEAR'.format(col), '{}_EVER'.format(col)]].apply(func_ration, axis=1)\n    \n   #  Sum\/Number Features\n    arb_info = ['PLAINTIFF', 'DEFENDANT']\n    num2sum = {'CASESNUMBER_YEAR' : 'SUM_YEAR', 'CASESNUMBER_EVER' : 'SUM_EVER'}\n    for number, summa in num2sum.items():\n        for col in arb_info:\n            data['{}_{}'.format(col, number)] = data['{}_{}'.format(col, number)].fillna(0)\n            data['{}_{}'.format(col, summa)] = data['{}_{}'.format(col, summa)].fillna(0)\n            func_ration = lambda x: -1 if x['{}_{}'.format(col, number)] == 0 else x['{}_{}'.format(col, summa)]\/x['{}_{}'.format(col, number)]\n            data['RATIO_SUM_NUMBER_{}_{}'.format(col, number.split('_')[-1])] = data[['{}_{}'.format(col, summa), '{}_{}'.format(col, number)]].apply(func_ration, axis=1)\n\n    # \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0431 \u0443\u0447\u0430\u0441\u0442\u0438\u0438 \u0432 \u0442\u0435\u043d\u0434\u0435\u0440\u0430\u0445\n    \n    tender_info = ['NUMBER_233_YEAR','NUMBER_233_EVER', 'NUMBER_95_YEAR','NUMBER_95_EVER']\n\n    for col in tender_info:\n        data['NOTADMITTED{}'.format(col)] = data['NOTADMITTED{}'.format(col)].fillna(0)\n        data['ADMITTED{}'.format(col)] = data['ADMITTED{}'.format(col)].fillna(0)\n        data['WINNER{}'.format(col)] = data['WINNER{}'.format(col)].fillna(0)\n        data['SIGNED{}'.format(col)] = data['SIGNED{}'.format(col)].fillna(0)\n        data['SUM_ADM_NOTADM_{}'.format(col)] = (data['ADMITTED{}'.format(col)] + data['NOTADMITTED{}'.format(col)] )\n        \n        func_ratio = lambda x: -1 if x['NOTADMITTED{}'.format(col)] + x['ADMITTED{}'.format(col)] == 0 else x['ADMITTED{}'.format(col)] \/ (x['NOTADMITTED{}'.format(col)]+x['ADMITTED{}'.format(col)])\n        data['RATIO_ADMIT_{}'.format(col)] = data[['ADMITTED{}'.format(col), 'NOTADMITTED{}'.format(col)]].apply(func_ratio, axis=1)\n        func_ratio = lambda x: -1 if x['ADMITTED{}'.format(col)] == 0 else x['WINNER{}'.format(col)] \/ x['ADMITTED{}'.format(col)]\n        data['RATIO_WIN_{}'.format(col)] = data[['WINNER{}'.format(col), 'ADMITTED{}'.format(col)]].apply(func_ratio, axis=1)\n        func_ratio = lambda x: -1 if x['WINNER{}'.format(col)] == 0 else x['SIGNED{}'.format(col)] \/ x['WINNER{}'.format(col)]\n        data['RATIO_WIN_SIG_{}'.format(col)] = data[['WINNER{}'.format(col), 'SIGNED{}'.format(col)]].apply(func_ratio, axis=1)\n        \n    #### Aleron's \u0444\u0438\u0447\u0438 \n\n\n    judge_info = ['CASESNUMBER_YEAR', 'CASESNUMBER_EVER']\n    judge_info_2 = ['SUM_EVER', 'SUM_YEAR']\n\n    for col in judge_info:\n        data['PLAINTIFF_{}'.format(col)] = data['PLAINTIFF_{}'.format(col)].fillna(0)\n        data['DEFENDANT_{}'.format(col)] = data['DEFENDANT_{}'.format(col)].fillna(0)\n        data['THIRDOROTHERPERSON_{}'.format(col)] = data['THIRDOROTHERPERSON_{}'.format(col)].fillna(0)\n        data['SUM_PLA_DEF_THI_{}'.format(col)] = data['PLAINTIFF_{}'.format(col)] + data['DEFENDANT_{}'.format(col)] + data['THIRDOROTHERPERSON_{}'.format(col)]\n        \n        func_ration = lambda x: -1 if x['THIRDOROTHERPERSON_{}'.format(col)] == 0 else x['DEFENDANT_{}'.format(col)] \/x['THIRDOROTHERPERSON_{}'.format(col)]\n        data['RATIO_DEF_THI_{}'.format(col)] = data[['DEFENDANT_{}'.format(col), 'THIRDOROTHERPERSON_{}'.format(col)]].apply(func_ration, axis=1)\n        func_ration = lambda x: -1 if x['PLAINTIFF_{}'.format(col)] + x['DEFENDANT_{}'.format(col)] == 0 else x['DEFENDANT_{}'.format(col)] \/(x['PLAINTIFF_{}'.format(col)] + x['DEFENDANT_{}'.format(col)])\n        data['RATIO_DEF_THI_DEF_{}'.format(col)] = data[['DEFENDANT_{}'.format(col), 'PLAINTIFF_{}'.format(col)]].apply(func_ration, axis=1)\n\n    for col in judge_info_2:\n        data['PLAINTIFF_{}'.format(col)] = data['PLAINTIFF_{}'.format(col)].fillna(0)\n        data['DEFENDANT_{}'.format(col)] = data['DEFENDANT_{}'.format(col)].fillna(0)\n        data['SUM_PLA_DEF_{}'.format(col)] = data['PLAINTIFF_{}'.format(col)] + data['DEFENDANT_{}'.format(col)]\n        func_ration = lambda x: -1 if x['PLAINTIFF_{}'.format(col)] == 0 else x['DEFENDANT_{}'.format(col)]\/x['PLAINTIFF_{}'.format(col)]\n        data['RATIO_DEF_THI_{}'.format(col)] = data[['DEFENDANT_{}'.format(col), 'PLAINTIFF_{}'.format(col)]].apply(func_ration, axis=1)\n        func_ratio = lambda x: -1 if x['PLAINTIFF_{}'.format(col)] + x['DEFENDANT_{}'.format(col)] == 0 else x['DEFENDANT_{}'.format(col)]\/(x['PLAINTIFF_{}'.format(col)] + x['DEFENDANT_{}'.format(col)])\n        data['RATIO_DEF_THI_DEF_{}'.format(col)] = data[['DEFENDANT_{}'.format(col),'PLAINTIFF_{}'.format(col)]].apply(func_ratio, axis=1)\n    \n    \n    #####  \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0430\u0445\n    \n    dates = [\"TAXREG_REGDATE\", \"TAXREGPAY_REGDATE\", \"SIGN_DATE\", \"DATEFIRSTREG\"]\n    \n    for date in dates:\n        data[date] = pd.to_datetime(data[date], format='%d%b%Y:%H:%M:%S')\n\n    for i in dates:\n        for j in dates:\n            if i != j:\n                data[\"{}_{}_year_delta\".format(i, j)] = data[i].dt.year - data[j].dt.year    \n                data[\"{}_{}_month_delta\".format(i, j)] = ((data[i] - data[j]).astype('timedelta64[D]')\/ 28).round()\n                data[\"{}_{}_day_delta\".format(i, j)] = (data[i] - data[j]).astype('timedelta64[D]')\n    \n    data[\"F_sing_date_year\"] = data['SIGN_DATE'].dt.year\n    \n    # \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437 \u043a\u043e\u0434\u043e\u0432 (\u043a\u043b\u0430\u0441\u0441, \u043f\u043e\u0434\u043a\u043b\u0430\u0441\u0441, \u0433\u0440\u0443\u043f\u043f\u0430, \u043f\u043e\u0434\u0433\u0440\u0443\u043f\u043f\u0430)\n    for step in [2, 4, 5, 7, 8]:\n        data['OKVED_CODE_{}'.format(step)] = data['OKVED_CODE'].str[:step]\n    for step in [2, 5, 8]:\n        data['OKTMO_CODE_{}'.format(step)] = data['OKTMO_CODE'].str[:step]\n    \n    ##############################\n    #### \u0424\u0438\u043d\u0441\u043e\u0432\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438  ####\n    ##############################\n    \n    # \u041f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0440\u0435\u043d\u0442\u0430\u0431\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n    data['F_fin_2200_2'] = data['F2200']\/(data['F2120'] + data['F2210'] + data['F2220'])\n    data['F_fin_2400_2110'] = data['F2400']\/data['F2110'] # top    \n    data['F_fin_2300_1300'] = data['F2300']\/(data['F1300'] + data['F1530'] + data['F1400'])\n    data['F_fin_2300_2110'] = data['F2300']\/data['F2110'] # top\n    data['F_fin_2400_1300_1500'] = data['F2400']\/(data['F1300'] + data['F1530'])\n    data['F_fin_2400_1600'] = data['F2400']\/data['F1600']\n    data['F_fin_2400_2110_LAG1'] = data['F2400_LAG1']\/data['F2110_LAG1']\n    data['F_fin_2400_2110_diff_ratio'] = (data['F_fin_2400_2110'] - data['F_fin_2400_2110_LAG1'])\/data['F_fin_2400_2110']\n    data['F_fin_2300_2110_LAG1'] = data['F2300_LAG1']\/data['F2110_LAG1']\n    data['F_fin_2300_2110_LAG1_diff_ratio'] = (data['F_fin_2300_2110'] - data['F_fin_2300_2110_LAG1'])\/data['F_fin_2300_2110']\n    data = data.copy()\n    # \u041f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u043b\u0438\u043a\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u043e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438\n    data['F_fin_1200_2'] = data['F1200'] - data['F1500'] - data['F1530'] \n    data['F_fin_1240_1'] = (data['F1240'] + data['F1250'])\/( data['F1500'] + data['F1530'])\n    data['F_fin_1240_2'] = (data['F1240'] + data['F1250'] + data['F1230'])\/( data['F1500'] + data['F1530'])\n    data['F_fin_1200_1'] = data['F1200']\/( data['F1500'] + data['F1530'] )\n    data['F_fin_2110_1230'] = data['F2110']\/data['F1230']  # top\n    data['F_fin_2120_1210'] = data['F2120']\/data['F1210']  # top\n    data['F_fin_2110_1230_LAG1'] = data['F2110_LAG1']\/data['F1230_LAG1']\n    data['F_fin_2110_1230_diff_ratio'] = (data['F_fin_2110_1230'] - data['F_fin_2110_1230_LAG1'])\/data['F_fin_2110_1230']\n     # \u041f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 2\n    data['F_21'] = data['F2110'] \/ ((data['F1230'] + data['F1230_LAG1'])\/2) # \u043e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u0434\u0435\u0431\u0438\u0442\u043e\u0440\u0441\u043a\u043e\u0439 \u0437\u0430\u0434\u043e\u043b\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438\n    data['F_22'] = data['F2120'] \/ ((data['F1520'] + data['F1520_LAG1'])\/2) # \u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0440\u0441\u043a\u043e\u0439 \u0437\u0430\u0434\u043e\u043b\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438\n    # \u0424\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u0430\u044f \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u0441\u0442\u044c\n    data['F_fin_1_1700'] = (data['F1300'] + data['F1530'] )\/data['F1700']\n    data['F_fin_2_1700'] = (data['F1400'] + data['F1500']  - data['F1530'] )\/data['F1700']\n    data['F_fin_1400'] = (data['F1400'] + data['F1500']  - data['F1530'] )\/(data['F1300'] + data['F1530'])\n    data['F_fin_1300'] = (data['F1300'] + data['F1530']  - data['F1100'])\/(data['F1300'] + data['F1530'])\n    data['F_fin_1300_1'] = (data['F1300'] + data['F1530'] - data['F1100'])\/data['F1200']\n    # \u0415\u0449\u0435 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438\n    data['FE1700'] = data['F2110'] + data['F2120'] + data['F2100'] + data['F2210']+ data['F2220'] + data['F2200'] + data['F2350'] + data['F2300'] + data['F2410'] + data['F2400']\n    data['F_sum_active_pass'] = data['F1700'] + data['F1600']\n    data['F_active_on_pass'] = data['F1600']\/(data['F1600']+data['F1700'])\n    data['F_active_on_pass_ration'] = data['F1600']\/(data['F1700'])\n    func_ratio = lambda x: -1 if x['F1410'] + x['F1510'] == 0 else (x['F1600'] + x['F1700'])\/(x['F1410'] + x['F1510'])\n    data['debt_on_sum_act_pas'] = data[['F1700','F1600','F1410','F1510']].apply(func_ratio, axis=1)\n    \n    print('\u0417\u0430\u043a\u043e\u043d\u0447\u0438\u043b \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438!')\n    return data\n\ntrain_data = make_features(train)\ntest_data = make_features(test)\n","05b6d2bd":"def make_grouped_features(tr_data, te_data=None, test_flag=False):\n    print('\u041d\u0430\u0447\u0430\u043b \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438')\n    if test_flag: # \u0415\u0441\u043b\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430, \u0442\u043e \u043f\u0440\u0438\u0448\u0438\u0432\u0430\u0435\u043c \u0435\u0449\u0435 \u0438\u0441\u0442\u043e\u0440\u0438\u044e \u0438\u0437 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n        data = pd.concat([tr_data, te_data], ignore_index = True)\n    else:\n        data = tr_data.copy() # \u0415\u0441\u043b\u0438 \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0447\u043d\u0430\u044f, \u0442\u043e \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0434\u0435\u043b\u0430\u0435\u043c, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0437\u0430\u0433\u043b\u044f\u043d\u0443\u0442\u044c \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u0435\n         \n    # \u0412\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    data['year_from_DATEFIRSTREG'] = ((data['SIGN_DATE'] - data['DATEFIRSTREG']).astype('timedelta64[D]') \/ 365).round()\n    data['year_from_TAXREG_REGDATE'] = ((data['SIGN_DATE'] - data['TAXREG_REGDATE']).astype('timedelta64[D]') \/ 365).round()\n    data['year_from_TAXREGPAY_REGDATE'] = ((data['SIGN_DATE'] - data['TAXREGPAY_REGDATE']).astype('timedelta64[D]') \/ 365).round()\n    data['day_from_first_SIGN_DATE'] = (data.groupby('id_client')['SIGN_DATE'].transform('min') - data['SIGN_DATE']).astype('timedelta64[D]')    \n    data['F_IP_flag_sum'] = data[['id_client','IP_flag']].groupby('id_client', as_index=False).transform('sum')\n    data['F_contract_count'] = data[['id_client','id_contract']].groupby('id_client', as_index=False).transform('count')\n    data['F_contract_count_in_day'] = data.groupby(['id_client','SIGN_DATE'])['SIGN_DATE'].transform('count')    \n    trend = lambda x : x.iloc[-1] - x.iloc[0]\n    data['F_contract_count_in_day_trend'] = data[['id_client','F_contract_count_in_day']].groupby('id_client', as_index=False)['F_contract_count_in_day'].transform(trend)\n    # \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 (std  (\u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f)- \u0442\u0443\u0442 \u0431\u0443\u0434\u0435\u0442 \u0441\u043b\u0443\u0436\u0438\u0442\u044c \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438)\n    data['F_contract_count_in_day_std'] = data[['id_client','F_contract_count_in_day']].groupby('id_client', as_index=False).transform('std')\n    data['F_contract_count_mean'] = data[['id_client','F_contract_count']].groupby('id_client', as_index=False).transform('mean')\n    data['F_WORKERSRANGE_change_mean'] = data[['id_client','WORKERSRANGE']].groupby('id_client', as_index=False).transform('mean')\n    data['F_WORKERSRANGE_change_std'] = data[['id_client','WORKERSRANGE']].groupby('id_client', as_index=False).transform('std')\n    data['TELEPHONECOUNT_std'] = data[['id_client','TELEPHONECOUNT']].groupby('id_client', as_index=False).transform('std')    \n    data['RATIO_WIN_NUMBER_95_YEAR_std'] = data[['id_client','RATIO_WIN_NUMBER_95_YEAR']].groupby('id_client', as_index=False).transform('std')\n    data['RATIO_WIN_NUMBER_95_EVER_std'] = data[['id_client','RATIO_WIN_NUMBER_95_EVER']].groupby('id_client', as_index=False).transform('std')\n    data['RATIO_ADMIT_NUMBER_95_EVER_std'] = data[['id_client','RATIO_ADMIT_NUMBER_95_EVER']].groupby('id_client', as_index=False).transform('std')\n    data['MANAGERINNCOUNT_std'] = data[['id_client','MANAGERINNCOUNT']].groupby('id_client', as_index=False).transform('std')\n        \n    if test_flag: # \u0415\u0441\u0442\u044c \u044d\u0442\u043e \u0442\u0435\u0441\u0442, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\n        data = data.iloc[len(tr_data):]\n        assert len(data)==len(te_data)\n        return data\n    \n    print('\u041d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0433\u043e\u0442\u043e\u0432\u044b!')\n    return data\n\ntrain_data = make_grouped_features(train_data, None, test_flag=False)\ntest_data = make_grouped_features(train_data, test_data, test_flag = True)\n    ","188976c1":"cat_features = ['OKTMO_CODE', 'OKVED_CODE', 'CITIZENSHIP_NAME'] \nnum_features = [] \n\ncol2ignor =  ['SUM_ADM_NOTADM_NUMBER_233_EVER', 'TAXREG_REGDATE_DATEFIRSTREG_month_delta',\n              'ADMITTEDNUMBER_233_EVER', 'DATEFIRSTREG_TAXREG_REGDATE_day_diff', # check drop, here was comma\n              'DATEFIRSTREG_TAXREG_REGDATE_day_delta', 'TAXREG_REGDATE_DATEFIRSTREG_day_delta',\n              'DATEFIRSTREG_TAXREG_REGDATE_day_delta', 'TAXREGPAY_REGDATE_SIGN_DATE_day_delta']\n\nfeatures_to_ignore = col2ignor + ['TARGET','id_contract', 'id_client',\n                                  'TAXREG_REGDATE', 'TAXREGPAY_REGDATE','SIGN_DATE', 'BIRTHDATE','DATEFIRSTREG']\n\n\nfor col in tqdm(train_data.columns):\n    \n    if col not in features_to_ignore: # col in feature2use: \n        \n        if 'CODE' in col:\n            if col not in cat_features:\n                cat_features.append(col)   \n                \n        elif train_data[col].nunique() <= 85 and 'DATE' not in col and 'NUMBER' not in col and 'COUNT' not in col and 'F' != col[0] and 'SUM' not in col and 'WORKERSRANGE' not in col and 'AGE' not in col:\n            if col not in cat_features:\n                cat_features.append(col) \n                \n        else:\n            if col not in cat_features:\n                num_features.append(col)\n        \n\nprint('\u0412\u0441\u0435\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432', len(train_data.columns))  \nprint('\u0427\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432', len(num_features))\nprint('\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432', len(cat_features))\n\nfiltered_features = cat_features  + num_features\nprint('filtered_features', len(filtered_features))","2cd4d71c":"train_data = train_data[filtered_features + ['TARGET']]\ntest_data = test_data[filtered_features]\n    \nfor col_name in cat_features: \n    train_data[col_name] = train_data[col_name].astype(str)\n    test_data[col_name] = test_data[col_name].astype(str)\n","af0e7236":"# pseudo = False\n# if pseudo:\n#     pseudolabels = pd.read_csv('..\/input\/data-science-battle-mkb\/lightautoml.csv', sep=';')\n# #pseudolabels.head()","1c072763":"# if pseudo:\n#     test_data[TARGET_NAME] = pseudolabels[TARGET_NAME].values\n#     del pseudolabels\n#     train_data = optimize_dataframe(train_data)\n#     test_data = optimize_dataframe(test_data)","fa8403d7":"# if pseudo:\n#     print(train_data.shape, test_data.shape)\n#     ALL_DF = pd.concat([train_data, test_data], axis=0).reset_index(drop = True)\n#     print(ALL_DF.shape)\n\n#     ALL_DF['weight'] = [1.001] * len(train_data) + [0.999] * len(test_data)  #### Fixed!!!!!!!!!!!\n#     ALL_DF = optimize_dataframe(ALL_DF)\n#     # del train_data, test_data\n# else:\n#     ALL_DF = train_data","42bcdac7":"task = Task('binary', metric ='auc')# metric = metric, greater_is_better = True)","3d3afbc2":"roles = {'target': 'TARGET',\n        # 'weights': 'weight',  # for pseudolabelling\n         'drop' : ['id_client','id_contract'] # + while_to_drop # TODO \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0444\u0438\u0447\u0435\u0439 \u0438\u0437 APPLICATION_MONTH  \n         }","f2dfc4f9":"# tr_data, te_data = train_test_split(ALL_DF, \n#                                     test_size=0.2, \n#                                     stratify=ALL_DF[TARGET_NAME], \n#                                     random_state=42)\n# print('Data splitted. Parts sizes: tr_data = {}, te_data = {}'.format(tr_data.shape, te_data.shape))","f43ad636":"# TabularUtilizedAutoML\n\nautoml = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,                        \n                           general_params = {'nested_cv': False, 'use_algos': [[\"cb\"]]}, # 'lgb', 'cb_tuned', 'cb', 'lgb_tuned',\n                           selection_params = {'mode': 0},\n                           reader_params = {'n_jobs': N_THREADS, 'random_state': 22,'cv':  3} \n                          )\n\n\nRD = ReportDeco(output_path = 'tabularAutoML_model_report')\nautoml_rd = RD(automl)\n\n\noof_pred = automl.fit_predict(train_data, roles = roles, verbose=2)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","cda88be9":"#new_test_data = ALL_DF#.iloc[len(train_data):] \ntest_pred = automl.predict(test_data) \n#print('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))","ef7be373":"test[target] = test_pred.data[:, 0]\ntest[['id_contract', 'TARGET']].to_csv('..\/working\/lama_best_.csv', sep=';', index=False) # -> 0.9152\ntest[['id_contract', 'TARGET']].head(8)","0b8d53e0":"from sklearn.model_selection import KFold # k-\u0444\u043e\u043b\u0434\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\nfrom catboost import CatBoostClassifier\n\nn_splits = 3\n\n\nclfs = []\nX = train_data[filtered_features] \ny = train_data['TARGET'].values\n\n# \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438, \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u0434\u0442\u0438 \u043d\u0430 6 X n_splits \u0444\u043e\u043b\u0434\u0430\u0445\n\nkFold_random_state = [42]# , 666, 228, 777, 2021, 75]\nN = len(kFold_random_state)*n_splits\n\nfor ind_k, random_state in enumerate(kFold_random_state):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        clf = CatBoostClassifier(iterations = 10000,\n                              loss_function = 'Logloss', eval_metric = 'F1', #'AUC:hints=skip_train~false',\n                              cat_features = cat_features, random_seed=random_state, \n                              task_type='CPU', auto_class_weights = 'SqrtBalanced',\n                              early_stopping_rounds=250 + ind_k*10 )\n        clfs.append(clf)\n\n        clf.fit(X_train, y_train, eval_set=(X_test, y_test),\n                verbose = 500, use_best_model = True, plot = False)\n        \nassert len(clfs) == N\n\n# \u043c\u0430\u0441\u0441\u0438\u0432 \u0434\u043b\u044f \u0437\u0430\u043f\u0438\u0441\u0438 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\ny_pred = np.zeros((len(test_data),))\nscores = []\nfor clf in clfs:\n    y_pred += clf.predict_proba(test_data)[:,1]\n    scores.append(clf.best_score_['validation']['F1'])\ny_pred \/= N\nprint('mean F1', np.mean(scores))\n\ntest[target] = y_pred\ntest[['id_contract', 'TARGET']].to_csv('..\/working\/catboost_test.csv', sep=';', index=False) # -> 0.9311\ntest[['id_contract', 'TARGET']].head(6)\n","958a2350":"# Number of features used by LightAutoML model\nprint(len(automl_rd.model.collect_used_feats()))\n\nautoml_rd.model.collect_used_feats()\n# 86 \u0444\u0438\u0447\u0435\u0439","f61feaf8":"# RD = ReportDeco(output_path = '..\/working\/tabularAutoML_model_report\/')\n# automl_rd = RD(automl)","356e9431":"# automl_rd.model.get_feature_scores('fast')","efe40665":"# # Fast feature importances calculation\n# fast_fi = automl_rd.model.get_feature_scores('fast')\n# fast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (40, 8), grid = True)","84c27c06":"# fast_fi[fast_fi.Importance < 88]['Feature'].to_list()","cf93ba93":"# # accurate_fi = automl_rd.model.get_feature_scores('accurate')\n# # accurate_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)\n\n# accurate_fi = automl_rd.model.get_feature_scores('accurate', te_data.sample(2100), silent = False)\n# accurate_fi.set_index('Feature')['Importance'].plot.bar(figsize = (40, 8), grid = True)","5bc9dafd":"# accurate_fi[accurate_fi.Importance < 0]['Feature'].to_list()\n\n","f4d09a44":"# Additional materials","e1e65f86":"## Official LightAutoML github repository is [here](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n\n## Upvote is the best motivator \ud83d\udc4d\n\n# Step 0.0. LightAutoML installation","7c5d6f86":"# Feature Engineering","9b8ac98f":"In next the cell we are going to create LightAutoML model with `TabularAutoML` class - preset with default model structure like in the image below:\n\n<img src=\"https:\/\/github.com\/sberbank-ai-lab\/lightautoml-datafest-workshop\/raw\/master\/imgs\/tutorial_blackbox_pipeline.png\" alt=\"TabularAutoML preset pipeline\" style=\"width:70%;\"\/>\n\nin just several lines. Let's discuss the params we can setup:\n- `task` - the type of the ML task (the only **must have** parameter)\n- `timeout` - time limit in seconds for model to train\n- `cpu_limit` - vCPU count for model to use\n- `reader_params` - parameter change for Reader object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc. For example, we setup `n_jobs` threads for typization algo, `cv` folds and `random_state` as inside CV seed.\n- `general_params` - we use `use_algos` key to setup the model structure to work with (Linear and LGBM model on the first level and their weighted composition creation on the second). This setup is only to speedup the kernel, you can remove this `general_params` setup if you want the whole LightAutoML model to run.\n\n**Important note**: `reader_params` key is one of the YAML config keys, which is used inside `TabularAutoML` preset. [More details](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/blob\/master\/lightautoml\/automl\/presets\/tabular_config.yml) on its structure with explanation comments can be found on the link attached. Each key from this config can be modified with user settings during preset object initialization. To get more info about different parameters setting (for example, ML algos which can be used in `general_params->use_algos`) please take a look at our [article on TowardsDataScience](https:\/\/towardsdatascience.com\/lightautoml-preset-usage-tutorial-2cce7da6f936).\n\nMoreover, to receive the automatic report for our model we will use `ReportDeco` decorator and work with the decorated version in the same way as we do with usual one. ","9c0b3375":"# Step 0.1. Import libraries\n\nHere we will import the libraries we use in this kernel:\n- Standard python libraries for timing, working with OS etc.\n- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n- LightAutoML modules: presets for AutoML, task and report generation module","bd5e63ec":"## Do not forget to upvote if you like the kernel \ud83d\udc4d","586031be":"This step can be used if you are working inside Google Colab\/Kaggle kernels or want to install LightAutoML on your machine:","f6c446aa":"# Step 3. LightAutoML model creation - TabularAutoML preset","5c87b76e":"# Step 0.5. Pseudolabelling\nLet's import the predictions from the NLP benchmark and use them in pseudolabelling technique:","6209b76c":"# Step 0.3. Imported models setup\n\nFor better reproducibility we fix numpy random seed with max number of threads for Torch (which usually try to use all the threads on server):","7be66b75":"# Step 0.2. Constants\n\nHere we setup the constants to use in the kernel:\n- `N_THREADS` - number of vCPUs for LightAutoML model creation\n- `N_FOLDS` - number of folds in LightAutoML inner CV\n- `RANDOM_STATE` - random seed for better reproducibility\n- `TEST_SIZE` - houldout data part size \n- `TIMEOUT` - limit in seconds for model to train\n- `TARGET_NAME` - target column name in dataset","fadde2a6":"# Grouped Features Engineering","f3e3c6c9":"# Step 0.4. Data loading\nLet's check the data we have:","98e26407":"# Step 2. Feature roles setup","489046c3":"# =============== LightAutoML model building ===============\n\n\n# Step 1. Task setup\n\nOn the cell below we create Task object - the class to setup what task LightAutoML model should solve with specific loss and metric if necessary (more info can be found [here](https:\/\/lightautoml.readthedocs.io\/en\/latest\/generated\/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task) in our documentation):","6fac90e3":"# Step 7. Create submission file","f0341f23":"# Step 6. Prediction on test dataset\n\nIn the cell below we need to use `automl_rd.model` as we don't have target variable inside test dataset and there is no point to include it into the report:","40e1ba12":"- [Official LightAutoML github repo](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n- [LightAutoML documentation](https:\/\/lightautoml.readthedocs.io\/en\/latest)\n- [Pseudolabelling in the nutshell](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231738#1268903)","516238ec":"To solve the task, we need to setup columns roles. The **only role you must setup is target role**, everything else (drop, numeric, categorical, group, weights etc.) is up to user - LightAutoML models have automatic columns typization inside:"}}