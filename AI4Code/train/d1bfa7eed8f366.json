{"cell_type":{"a69ad0da":"code","bfecf67c":"code","bcf3ac81":"code","5e36421b":"code","c530990d":"code","8ca819be":"code","3bb63fdf":"code","1a66830b":"code","eebe01db":"code","ab0a1803":"code","4f5355e4":"code","8aeca4e9":"code","1852d07a":"code","5a9ca76c":"code","90dc3f8b":"code","38577cb3":"code","d11a4b44":"code","c7aae5db":"code","15c08b43":"code","5ec716e7":"code","dc921590":"code","0d7308ff":"code","c095111b":"code","c45656cc":"code","e902dc28":"code","9b1023b2":"code","f3c2b177":"code","a233c699":"code","3d031075":"code","4fc7e300":"code","5335f6e1":"code","1faa0620":"code","0eb583d3":"code","476f90b0":"code","0ec2d2a9":"code","e8bd4e78":"code","34f31a89":"code","781db336":"code","1df8fb90":"code","3cd58ce9":"code","48a830fb":"code","001172ad":"code","44d4f1e4":"code","8e336d7f":"code","dd9e1e6a":"code","d4b4cbf0":"code","53714e14":"code","e9928a87":"code","d1d21636":"code","4d130a06":"code","b1cd2eea":"code","10956450":"code","f0c1a80b":"code","6cebe143":"code","1b0fb360":"code","b1a3bc80":"code","51f8b34c":"code","a66eb7e2":"markdown","46385bf3":"markdown","c052e74b":"markdown","e8594445":"markdown","46249ed9":"markdown","1c1a065f":"markdown","2997cfbf":"markdown","a87b0477":"markdown","6839aab4":"markdown","e9582bb1":"markdown","5215cc65":"markdown"},"source":{"a69ad0da":"import pandas as pd\nimport numpy as np\ntweets = pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")\ntweets.head(0)","bfecf67c":"tweets.head(3)","bcf3ac81":"del tweets[\"tweet_id\"]\ndel tweets[\"airline_sentiment_confidence\"]\ndel tweets[\"negativereason\"]\ndel tweets[\"negativereason_confidence\"]\ndel tweets[\"airline_sentiment_gold\"]\ndel tweets[\"negativereason_gold\"]\ndel tweets[\"tweet_coord\"]\ndel tweets[\"user_timezone\"]\ndel tweets[\"tweet_location\"]\ntweets.head(3)","5e36421b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ntweets['n_words'] = [len(t.split()) for t in tweets.text]\n\nfig = plt.figure(figsize = (15, 6))\nsns.distplot(tweets['n_words'][tweets['airline_sentiment']=='positive'], color='g', label = 'positive')\nsns.distplot(tweets['n_words'][tweets['airline_sentiment']=='negative'], color='r', label = 'negative')\nsns.distplot(tweets['n_words'][tweets['airline_sentiment']=='neutral'], color='b', label = 'neutral')\nplt.legend(loc='best')\nplt.xlabel('# of Words', size = 14)\nplt.ylabel('Count', size = 14)\nplt.title('The Distribution of Number of Words for each Class', fontsize = 14)\nplt.show()","c530990d":"import matplotlib.pyplot as plt\nsentiment_counts = tweets.airline_sentiment.value_counts()\nnames = ['negative', 'neutral', 'positive']\nvalues = sentiment_counts.values\nplt.figure(figsize=(30, 3))\nplt.subplot(131)\nplt.bar(names, values)\nplt.show()","8ca819be":"#check each airline's numbers in each sentiment \ndef plot_sub_sentiment(Airline):\n    df=tweets[tweets['airline']==Airline]\n    count=df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count)\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Moods of '+Airline)\nplt.figure(1,figsize=(12, 12))\nplt.subplot(231)\nplot_sub_sentiment('US Airways')\nplt.subplot(232)\nplot_sub_sentiment('United')\nplt.subplot(233)\nplot_sub_sentiment('American')\nplt.subplot(234)\nplot_sub_sentiment('Southwest')\nplt.subplot(235)\nplot_sub_sentiment('Delta')\nplt.subplot(236)\nplot_sub_sentiment('Virgin America')","3bb63fdf":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')","1a66830b":"import re, nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef normalizer(tweet):\n    text = re.sub(r\"http:(\\\/\\\/t\\.co\\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet)\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",text) \n   #tokens = nltk.word_tokenize(only_letters)[2:] #delete airline name\n    tokens = nltk.word_tokenize(only_letters)[:] #include airline name\n    lower_case = [l.lower() for l in tokens]\n    #filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in lower_case]\n    \n    return ' '.join(lemmas)\n   #return lemmas\ndef normalizer2(tweet):\n    text = re.sub(r\"http:(\\\/\\\/t\\.co\\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet)\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",text) \n    tokens = nltk.word_tokenize(only_letters)[:] #include airline name\n    lower_case = [l.lower() for l in tokens]\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n    return lemmas\n    \ndef column(matrix, i):\n    return [row[i] for row in matrix]","eebe01db":"#pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\ntweets['normalized_tweet'] = tweets.text.apply(normalizer)\ntweets[['text','normalized_tweet']].head()\ntweets['normalized_tweet_tokens'] = tweets.text.apply(normalizer2)\n","ab0a1803":"from nltk import ngrams\ndef ngrams(input_list):\n    onegrams = input_list\n    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n    return bigrams+trigrams\ntweets['grams'] = tweets.normalized_tweet_tokens.apply(ngrams)\ntweets[['grams']].head()","4f5355e4":"import collections\ndef count_words(input):\n    cnt = collections.Counter()\n    for row in input:\n        for word in row:\n            cnt[word] += 1\n    return cnt","8aeca4e9":"positivewords =tweets[(tweets.airline_sentiment == 'positive')][['grams']].apply(count_words)['grams'].most_common(50)\nnegativewords =tweets[(tweets.airline_sentiment == 'negative')][['grams']].apply(count_words)['grams'].most_common(50)\nneutralwords  =tweets[(tweets.airline_sentiment == 'neutral')][['grams']].apply(count_words)['grams'].most_common(50)","1852d07a":"from wordcloud import WordCloud,STOPWORDS\ndef column(matrix, i):\n    return [row[i] for row in matrix]\npositiveword=' '.join(column(positivewords, 0))\nnegativeword=' '.join(column(negativewords, 0))\nneutralword=' '.join(column(neutralwords, 0))","5a9ca76c":"wordcloud  = WordCloud(background_color='black').generate(positiveword)\nwordcloud2 = WordCloud(background_color='black').generate(negativeword)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.figure(2,figsize=(12, 12))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.show()","90dc3f8b":"def sentiment2target(sentiment):\n    return {\n        'negative': 0,\n        'neutral': 1,\n        'positive' : 2\n    }[sentiment]\ntargets = tweets.airline_sentiment.apply(sentiment2target)","38577cb3":"from sklearn.model_selection import train_test_split\nimport numpy as np\n#data_train, data_test, y_train, y_test = train_test_split(tweets.normalized_tweet, targets, test_size=0.2, random_state=1)\ndata_train, data_test, y_train, y_test = train_test_split(tweets.normalized_tweet, tweets.airline_sentiment, test_size=0.2, random_state=1)","d11a4b44":"traindf = pd.DataFrame(np.array(data_train), columns=[\"tweet\"])\ntraindf[\"label\"] = np.array(y_train)\ndevdf = pd.DataFrame(np.array(data_test), columns=[\"tweet\"])\ndevdf[\"label\"] = np.array(y_test)","c7aae5db":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression","15c08b43":"union = FeatureUnion([ (\"w_v2\", TfidfVectorizer(analyzer = 'char', ngram_range=(1,5)  )),\n                      (\"w_v3\", TfidfVectorizer(analyzer = 'char_wb', ngram_range=(1,5)  )),\n                      (\"w_v\", CountVectorizer( ngram_range=(1,3),stop_words=None )),],\ntransformer_weights={\n            'w_v': 1, \n        'w_v2': 1,   \n           'w_v3': 1, \n           },)\nX_train = union.fit_transform(data_train)\nX_test = union.transform(data_test)\nX_train.shape","5ec716e7":"Accuracy=[]\nF1=[]","dc921590":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score\n\nmodel = make_pipeline( (MultinomialNB(alpha=0.03))).fit(X_train, y_train)\npredicted = model.predict(X_test) \nscore = metrics.accuracy_score(y_test, predicted)*100\nAccuracy.append(score)\nprint(\"MultinomialNB accuracy:   %0.3f\" % score) \n\nf1=f1_score(y_test, predicted, average='macro')*100\nF1.append(f1)\n\nprint(\"MultinomialNB F1-score:   %0.3f\" % f1) \nprint()\nprint()\nprint(classification_report(y_test, predicted))","0d7308ff":"import seaborn as sn\nconfusion_matrix = pd.crosstab(y_test, predicted, rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True, fmt='.0f')\nplt.show()","c095111b":"#@title Make a predication:\nsent = 'i did not like my last flight' #@param {type:\"string\"}\nsent = union.transform([sent])\nprint(model.predict(sent)[0])","c45656cc":"y_test = np.array(y_test)\npredicted = np.array(predicted)\ndata_test = np.array(data_test)\nprint()","e902dc28":"#what did the model misclassifiy\npredictedprob = model.predict_proba(X_test) \nfor i in range(len(predictedprob)):\n    for j in range(len(predictedprob[i])):\n        predictedprob[i][j] = '{0:.2f}'.format(predictedprob[i][j])\nmissclassified=[]\ntrue=[]\nprd=[]\nprdprob=[]\nother_prdprob=[]\nfor i in range(len(y_test)):\n    if y_test[i] != predicted[i]:\n        missclassified.append(data_test[i])\n        true.append(str(y_test[i]))\n        prd.append(str(predicted[i]))\n        indx=np.argmax(predictedprob[i])\n        prdprob.append(str(predictedprob[i][indx]))\n        other_prdprob.append(str(predictedprob[i]))\nmiss = pd.DataFrame(missclassified,columns=[\"sentence\"])\nmiss[\"True\"] = true\nmiss[\"Predicated\"] = prd\nmiss[\"Confidance\"] = prdprob\nmiss[\"All_prob\"] = other_prdprob","9b1023b2":"pd.set_option('display.max_colwidth', -1)\nmiss.head(100)","f3c2b177":"data_train, data_test, y_train, y_test = train_test_split(tweets.normalized_tweet, targets, test_size=0.2, random_state=1)","a233c699":"#convert the labels to one hot encoder \na = y_train\nb = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size),a] = 1\nb","3d031075":"#one-vs-all explicitly\nmodel11 = make_pipeline( (LinearRegression())).fit(X_train, column(b, 0))\npredicted11 = model11.predict(X_test) \n\nmodel22 = make_pipeline( (LinearRegression())).fit(X_train, column(b, 1))\npredicted22 = model22.predict(X_test) \n\nmodel33 = make_pipeline( (LinearRegression())).fit(X_train, column(b, 2))\npredicted33 = model33.predict(X_test) \n\nLinearRegression_preds = np.stack((predicted11, predicted22,predicted33), axis=-1)\n","4fc7e300":"preds_LinearRegression =[]\nfor i in LinearRegression_preds:\n    preds_LinearRegression.append(np.argmax(i))\n\nscore = metrics.accuracy_score(y_test, preds_LinearRegression)*100\nAccuracy.append(score)\nprint(\"LinearRegression accuracy:   %0.3f\" % score) \nf1=f1_score(y_test, preds_LinearRegression, average='macro')*100\nF1.append(f1)\n\nprint(\"LinearRegression F1-score:   %0.3f\" % f1) \nprint()\nprint()\nprint(classification_report(y_test, preds_LinearRegression))","5335f6e1":"import seaborn as sn\nconfusion_matrix = pd.crosstab(np.array(y_test), np.array(preds_LinearRegression), rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True, fmt='.0f')\nplt.show()","1faa0620":"Index = [1,2]\nsns.set()\nplt.subplots(figsize=(20,4),tight_layout=True)\nplt.subplot(1,2,1)\nplt.bar(Index,Accuracy)\n\nplt.xticks(Index, [\"MultinomialNB\",\"LinearRegression\"],rotation=0)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')\n\nfor i in range(len(Accuracy)):\n    plt.annotate(int(Accuracy[i]), xy=(Index[i],Accuracy[i]))\nplt.show()","0eb583d3":"%%capture cup\n!pip install simpletransformers==0.40.2","476f90b0":"%%capture cup\n!git clone https:\/\/github.com\/NVIDIA\/apex\n%cd apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/","0ec2d2a9":"data_train, data_test, y_train, y_test = train_test_split(tweets.normalized_tweet, targets, test_size=0.2, random_state=1)\ntraindf = pd.DataFrame(np.array(data_train), columns=[\"tweet\"])\ntraindf[\"label\"] = np.array(y_train)\ndevdf = pd.DataFrame(np.array(data_test), columns=[\"tweet\"])\ndevdf[\"label\"] = np.array(y_test)\n","e8bd4e78":"from simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport sklearn\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Create a ClassificationModel\nbertmodel = ClassificationModel('bert', 'bert-base-cased', num_labels=3, use_cuda=True, cuda_device=0, \n                            args={\n    'reprocess_input_data': True,\n    \"learning_rate\": 4e-5,\n    'overwrite_output_dir': True,\n    'num_train_epochs': 3,    \"save_eval_checkpoints\": False,\n    \"save_steps\": -1,}\n    )\nprint(traindf.head())\n\n# Train the bertmodel\nbertmodel.train_model(traindf, eval_df=devdf)\n","34f31a89":"predictions, raw_outputs = bertmodel.predict(np.array(data_test))","781db336":"from sklearn.metrics import f1_score,classification_report\nf1_score(y_true=np.array(y_test), y_pred=predictions, average='micro')\nF1.append(f1)\n","1df8fb90":"score = metrics.accuracy_score(np.array(y_test), predictions)*100\nAccuracy.append(score)","3cd58ce9":"print(classification_report(np.array(y_test),predictions, labels=None))","48a830fb":"import seaborn as sn\nconfusion_matrix = pd.crosstab(np.array(y_test),predictions, rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True, fmt='.0f')\nplt.show()","001172ad":"b=[]\nfor i in y_test:\n    if i==2:\n        b.append(\"positive\")\n    elif i == 0:\n        b.append(\"negative\")\n    else:\n        b.append(\"neutral\")","44d4f1e4":"#what did the model misclassifiy\ndata_test = np.array(data_test)\npredictedprob = raw_outputs\nfor i in range(len(predictedprob)):\n    for j in range(len(predictedprob[i])):\n        predictedprob[i][j] = '{0:.2f}'.format(predictedprob[i][j])\nmissclassified=[]\ntrue=[]\nprd=[]\nprdprob=[]\nother_prdprob=[]\nfor i in range(len(b)):\n    if b[i] != predicted[i]:\n        missclassified.append(data_test[i])\n        true.append(str(b[i]))\n        prd.append(str(predicted[i]))\n        indx=np.argmax(predictedprob[i])\n        prdprob.append(str(predictedprob[i][indx]))\n        other_prdprob.append(str(predictedprob[i]))\nmiss = pd.DataFrame(missclassified,columns=[\"sentence\"])\nmiss[\"True\"] = true\nmiss[\"Predicated\"] = prd\nmiss[\"Confidance\"] = prdprob\nmiss[\"All_prob\"] = other_prdprob","8e336d7f":"miss.head(100)","dd9e1e6a":"predictions_train, raw_outputs_train = bertmodel.predict(np.array(data_train))","d4b4cbf0":"model1 = make_pipeline( (LinearRegression())).fit(X_train, column(raw_outputs_train, 0))\npredicted1 = model1.predict(X_test) \n\nmodel2 = make_pipeline( (LinearRegression())).fit(X_train, column(raw_outputs_train, 1))\npredicted2 = model2.predict(X_test) \n\nmodel3 = make_pipeline( (LinearRegression())).fit(X_train, column(raw_outputs_train, 2))\npredicted3 = model3.predict(X_test) ","53714e14":"knowledge_distil_preds = np.stack((predicted1, predicted2,predicted3), axis=-1)\npreds_knowlsge_ditil =[]\nfor i in knowledge_distil_preds:\n    preds_knowlsge_ditil.append(np.argmax(i))","e9928a87":"score = metrics.accuracy_score(y_test, preds_knowlsge_ditil)*100\nAccuracy.append(score)\nprint(\"Knowledge distilation accuracy:   %0.3f\" % score) \nf1=f1_score(y_test, preds_knowlsge_ditil, average='macro')*100\nprint(\"Knowledge distilation F1-score:   %0.3f\" % f1) \nF1.append(f1)\n\n","d1d21636":"import seaborn as sn\nconfusion_matrix = pd.crosstab(np.array(y_test),np.array(preds_knowlsge_ditil), rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True, fmt='.0f')\nplt.show()","4d130a06":"Index = [1,2,3,4]\nsns.set()\nplt.subplots(figsize=(20,4),tight_layout=True)\nplt.subplot(1,2,1)\nplt.bar(Index,Accuracy)\nplt.xticks(Index, [\"MultinomialNB\",\"LinearRegression\",\"BERT\",\"DistilBERT\"],rotation=0)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","b1cd2eea":"time_taken=[]","10956450":"#count time to predict 1000 sentences:\nimport time\nstart_time = time.time()\n\nsent = union.transform(data_test[:1000])\nmodel.predict(sent)\n\nprint(\"MultinomialNB --- %s seconds ---\" % (time.time() - start_time))\ntime_taken.append((time.time() - start_time))","f0c1a80b":"#count time to predict 1000 sentences:\nimport time\nstart_time = time.time()\n\nsent = union.transform(data_test[:1000])\npredicted11 = model11.predict(sent) \npredicted22 = model22.predict(sent) \npredicted33 = model33.predict(sent) \nLinearRegression_preds = np.stack((predicted11, predicted22,predicted33), axis=-1)\npreds_LinearRegression =[]\nfor i in LinearRegression_preds:\n    preds_LinearRegression.append(np.argmax(i))\n\nprint(\"LinearRegression --- %s seconds ---\" % (time.time() - start_time))\ntime_taken.append((time.time() - start_time))","6cebe143":"#count time to predict 1000 sentences:\nimport time\nstart_time = time.time()\n\nbertmodel.predict(np.array(data_test[:1000]))\n\nprint(\"BERT --- %s seconds ---\" % (time.time() - start_time))\ntime_taken.append((time.time() - start_time))","1b0fb360":"\n#count time to predict 1000 sentences:\nimport time\nstart_time = time.time()\nsent = union.transform(data_test[:1000])\n\npredicted1 = model1.predict(sent) \npredicted2 = model2.predict(sent) \npredicted3 = model3.predict(sent) \nLinearRegression_preds = np.stack((predicted1, predicted2,predicted3), axis=-1)\npreds_LinearRegression =[]\nfor i in LinearRegression_preds:\n    preds_LinearRegression.append(np.argmax(i))\n\nprint(\"BERT_distil --- %s seconds ---\" % (time.time() - start_time))\ntime_taken.append((time.time() - start_time))","b1a3bc80":"Index = [1,2,3,4]\nsns.set()\nplt.subplots(figsize=(20,4),tight_layout=True)\nplt.subplot(1,2,1)\nplt.bar(Index,time_taken)\nplt.xticks(Index, [\"MultinomialNB\",\"LinearRegression\",\"BERT\",\"DistilBERT\"],rotation=0)\nplt.ylabel('Time')\nplt.xlabel('Model')\nplt.title('Time taken to Predict 1000 sentence')","51f8b34c":"Index = [1,2,3,4]\nsns.set()\nplt.subplots(figsize=(20,4),tight_layout=True)\nplt.subplot(1,2,1)\nplt.bar(Index,Accuracy)\nplt.xticks(Index, [\"MultinomialNB\",\"LinearRegression\",\"BERT\",\"DistilBERT\"],rotation=0)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')\n\nIndex = [1,2,3,4]\nsns.set()\nplt.subplots(figsize=(20,4),tight_layout=True)\nplt.subplot(1,2,1)\nplt.bar(Index,F1)\nplt.xticks(Index, [\"MultinomialNB\",\"LinearRegression\",\"BERT\",\"DistilBERT\"],rotation=0)\nplt.ylabel('F1-Score')\nplt.xlabel('Model')\nplt.title('F1-Score of Models')","a66eb7e2":"# Import data + first insight + preprocessing\n","46385bf3":"# **This dataframe shows missedclassified sentences that was made by the MNB classifier, however, when we manually check what sentences were missclassified and what was our models confidence on its prediction, we can tell that the given labels are wrong in some of the sentences. On the other hand, our model can not detect when a text is being sarcastic, some tweets may be negative but in the same time contain some words used in positive tweets, such tweets will be missedclassified by our model. This due to the fact that the extracted features (N-grams) are not good in understanding the language. **","c052e74b":"# Display most freq. words with each sentiment","e8594445":"# **Conclusion**\nUsing the tradiational machine learning models achieved a decent result in this task, 79.269% accuracy and 72.558% F1-score using MNB classifier with n-grams features. \nRecently, Google and others introduced pre-trained models which can achieve higher results than machine learning models, but these models are heavy to train and may take long time to make a predciton. In order to take advantage of both the high accuracy from BERT and the speed from machine learning models, we trained a LR model on BERT's predictions. This model achieived 80.567% accuracy and 74.665% F1-Score and maintained the same speed as machine learning models to make a new prediciton.","46249ed9":"# Comapre all models\n","1c1a065f":"# Bert","2997cfbf":"# Machine Learning classifiers (Baseline)","a87b0477":"# Split the data","6839aab4":"#  DistilBert LinearRegression trained on BERT's probs \n","e9582bb1":" **LinearRegression**","5215cc65":"Most of the time, BERT outperformes machine learning models, but it is conisdered as a heavy model. So we will try to get a high result using BERT and use its prediction to train a machine learning model. \n\n"}}