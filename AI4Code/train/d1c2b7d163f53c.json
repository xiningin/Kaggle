{"cell_type":{"d61a8b66":"code","91016b81":"code","95326d1c":"code","2744b4cd":"code","1581ef0a":"code","3c5774ac":"code","587d2910":"code","bc40749d":"code","442fddad":"code","5e9ca70a":"code","18dee5fb":"code","378e7edd":"code","148df2a6":"code","b1eab389":"code","041c8f67":"code","2acb7fba":"code","c616615d":"code","aae34463":"code","b89101a6":"code","27994fe4":"code","ad609151":"code","ae2451c5":"code","6ab489a3":"code","73f32913":"code","810c0d4c":"code","a29046dc":"code","2099c0b7":"code","6587ebbc":"code","9a5920a6":"code","fe8d07c9":"code","23890b25":"code","cfe41e94":"markdown","48b23192":"markdown","65b5289f":"markdown","9df009c1":"markdown","b7f2a5f6":"markdown","baf072e7":"markdown","bf13e67f":"markdown","dc9d58e9":"markdown","ee61859c":"markdown","c2731cd5":"markdown","ae13b36e":"markdown","eb441627":"markdown","0a06b31d":"markdown","21cc11f2":"markdown","4de50422":"markdown","72c0958a":"markdown"},"source":{"d61a8b66":"!pip install textstat\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nimport plotly.express as px\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom transformers import TFRobertaForSequenceClassification, RobertaTokenizerFast\nnltk.download('stopwords')","91016b81":"train_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/train.csv'\ntest_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/test.csv'\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\nprint(f'Train samples: {len(train)}')\ndisplay(train.head())\n\nprint(f'Test samples: {len(test)}')\ndisplay(test.head())\n","95326d1c":"train.head()","2744b4cd":"train.sort_values(by=['target']).head(7)","1581ef0a":"train.sort_values(by=['target'], ascending=False).head(7)","3c5774ac":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#003f5c\",\"#374c80\",\"#bc5090\",\"#ff764a\",\"#ffa600\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\n\n","587d2910":"train.nunique()","bc40749d":"msno.bar(train,color=palette[4], sort=\"ascending\", figsize=(10,5), fontsize=15)\nplt.show()","442fddad":"fig, ax = plt.subplots(1, 1, figsize=(15, 6))\nsns.distplot(train['target'], ax=ax,color=palette[0])\nplt.title(\"Target Distribution\",font=\"Serif\",size=\"18\")\nplt.show()","5e9ca70a":"fig, ax = plt.subplots(1, 1, figsize=(15, 6))\nsns.distplot(train['standard_error'], ax=ax,color=palette[0])\nplt.title(\"Standard_error Distribution\",font=\"Serif\",size=\"18\")\nplt.show()","18dee5fb":"sns.jointplot(x=train['target'], y=train['standard_error'], kind='hex',height=8,color=palette[1])\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\",size=\"18\")\nplt.subplots_adjust(top=0.94)\nplt.show()","378e7edd":"plt.figure(figsize=(10, 6))\nsns.countplot(y=\"license\",data=train,linewidth=3,color=palette[4])\nplt.title(\"License Distribution\",font=\"Serif\",size=\"18\")\nplt.show()","148df2a6":"BASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/'\ntokenizer = RobertaTokenizerFast.from_pretrained(BASE_MODEL)\ntrain['excerpt_len'] = train['excerpt'].apply(lambda x : len(x))\ntrain['excerpt_wordCnt'] = train['excerpt'].apply(lambda x : len(x.split(' ')))\ntrain['excerpt_tokenCnt'] = train['excerpt'].apply(lambda x : len(tokenizer.encode(x, add_special_tokens=False)))\n\nfig, ax = plt.subplots(1, 1, figsize=(18, 7))\nsns.distplot(train['excerpt_len'], ax=ax,color=palette[0]).set_title('Excerpts length',font=\"Serif\",size=\"18\")\nplt.show()","b1eab389":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_wordCnt'], ax=ax,color=palette[0]).set_title('Excerpts word count',font=\"Serif\",size=\"18\")\nplt.show()","041c8f67":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_tokenCnt'], ax=ax,color=palette[0]).set_title('Excerpt token count',font=\"Serif\",size=\"18\")\nplt.show()","2acb7fba":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\n\nall_stopwords = stopwords.words('english')\n#########################################\nsw_list = ['know','\\'re','n\\'t','going','-','_','\u2014','\u2019','\u2018','us','say','said']\nall_stopwords.extend(sw_list)\n#########################################\n\ndef remove_stopwords(text):\n    text_tokens = word_tokenize(text)\n    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n    return \" \".join(tokens_without_sw)\n\ndef remove_punctuation(text):\n    return ''.join(c for c in text if c not in punctuation)\n\nsentences=train['excerpt'].tolist()\nsentences=\"\".join(sentences)\n\nsentences=remove_stopwords(sentences)\nsentences=remove_punctuation(sentences)\n\nplt.rcParams['figure.figsize'] = (20,7)\nwordcloud = WordCloud().generate(sentences)\n\nwordcloud = WordCloud(width=1000,\n                     height=600,\n                      stopwords=STOPWORDS,\n                      random_state=42,\n                       background_color='white',\n                       collocations=False).generate(sentences)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\n","c616615d":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef plot_bt(x,w,p):\n    common_words = x(train['excerpt_preprocessed'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='freq', y='word', data=common_words_df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(\"Top 20 \"+ w,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","aae34463":"def preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed\n\ntrain[\"excerpt_preprocessed\"] = preprocess(train)\ntest[\"excerpt_preprocessed\"] = preprocess(test)","b89101a6":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n","27994fe4":"def bar_plot(df,x,x_title,y,title,colors=None,text=None):\n    fig = px.bar(x=x,\n                 y=y,\n                 text=text,\n                 labels={x: x_title.title()},   \n                 data_frame=df,\n                 color=colors,\n                 barmode='group',\n                 template=\"simple_white\")\n    \n    texts = [df[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'\n        \n    fig['layout'].title=title\n\n    fig.update_layout(title_font_size=19)\n    fig.update_layout(title_font_family='Droid Serif')\n        \n  \n\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').title()\n\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n\n    fig.show()","ad609151":"common_words = get_top_n_words(train['excerpt_preprocessed'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nbar_plot(common_words_df1.iloc[:20],\n         'word',\n         'Word',\n         ['freq'],\n         title='Top 20 unigrams')","ae2451c5":"common_words = get_top_n_bigram(train['excerpt_preprocessed'], 20)\ncommon_words_df2 = DataFrame(common_words,columns=['word','freq'])\nbar_plot(common_words_df2.iloc[:20],\n         'word',\n         'Word',\n         ['freq'],\n         title='Top 20 bigrams')","6ab489a3":"common_words = get_top_n_trigram(train['excerpt_preprocessed'], 20)\ncommon_words_df3 = DataFrame(common_words,columns=['word','freq'])\nbar_plot(common_words_df3.iloc[:20],\n         'word',\n         'Word',\n         ['freq'],\n         title='Top 20 trigrams')","73f32913":"fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\ntext_len = train[train['target'] <= 0]['excerpt'].str.split().map(lambda x: len(x))\nsns.distplot(text_len, ax=ax[0], color=palette[0])\nax[0].set_title('High Complexity')\n\ntext_len = train[train['target'] > 0]['excerpt'].str.split().map(lambda x: len(x))\nsns.distplot(text_len, ax=ax[1],color=palette[4])\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Number of Words in text')\nplt.show()","810c0d4c":"def avg_word_len(text):\n    avg_len = text.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return avg_len\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\navg_len = avg_word_len(train[train['target'] <= 0]['excerpt'])\nsns.distplot(avg_len, ax=ax[0], color=palette[0])\nax[0].set_title('High Complexity')\n\navg_len = avg_word_len(train[train['target'] > 0]['excerpt'])\nsns.distplot(avg_len, ax=ax[1], color=palette[4])\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Average word length in a text')\nplt.show()","a29046dc":"fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nno_sents = train[train['target'] <= 0]['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.distplot(no_sents, ax=ax[0], color=palette[0])\nax[0].set_title('High Complexity')\n\nno_sents = train[train['target'] > 0]['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.distplot(no_sents, ax=ax[1], color=palette[4])\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Number of Sentences in text')\nplt.show()","2099c0b7":"def clean_text(text):\n    text = re.sub(r'\\n', ' ', text)\n    text = re.sub(r'[\\.\\,]', '', text)\n    text = text.lower()\n    return text\n\ntrain['excerpt_clean'] = train['excerpt'].apply(clean_text)\neasier_words = ' '.join(train[train['target'] > 0]['excerpt_clean'].values)\nharder_words = ' '.join(train[train['target'] < 0]['excerpt_clean'].values)\n\neasier_words_wordcloud = WordCloud(width=900,\n                                   height=600,\n                                   stopwords=STOPWORDS,\n                                   random_state=42,\n                                   background_color='white',\n                                   collocations=False).generate(easier_words)\nharder_words_wordcloud = WordCloud(width=900,\n                                   height=600,\n                                   stopwords=STOPWORDS,\n                                   random_state=42,\n                                   background_color='white',\n                                   collocations=False).generate(harder_words)\n\nfig, axes = plt.subplots(ncols=2, figsize=(32, 10))\naxes[0].imshow(easier_words_wordcloud)\naxes[1].imshow(harder_words_wordcloud)\nfor i in range(2):\n    axes[i].tick_params(axis='x', labelsize=15, pad=10)\n    axes[i].tick_params(axis='y', labelsize=15, pad=10)    \naxes[0].set_title('Low Complexity Excerpts', pad=15,font=\"Serif\",size=\"20\")\naxes[0].axis(\"off\")\naxes[1].set_title('High Complexity Excerpts',  pad=15,font=\"Serif\",size=\"20\")\naxes[1].axis(\"off\")\n\nplt.show()","6587ebbc":"text_props = train.copy()\n\ndef avg_word_len(df):\n    df = df.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return df\n\ntext_len = train['excerpt'].str.len()\ntext_len_pre = train['excerpt_preprocessed'].str.len()\navg_text = avg_word_len(train['excerpt'])\navg_text_pre = avg_word_len(train['excerpt_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(train)):\n    lc = textstat.lexicon_count(train['excerpt'][i])\n    lcp = textstat.lexicon_count(train['excerpt_preprocessed'][i])\n    sc = textstat.sentence_count(train['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\n\ndef plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(15,6))\n    sns.kdeplot(data=text_props, x=col1,color=palette[0],label=\"Excerpt\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,color=palette[4],label=\"Excerpt preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n\n    sns.scatterplot(data=text_props,x=col1,y='target',color= palette[0],ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='target',color= palette[4],ax=ax[1],markers='.')\n    ax[1].set_title(title2,font=\"Serif\")\n\n    plt.show()\n\ncustom_lines = [Line2D([0], [0], color=palette[0], lw=4),\n                Line2D([0], [0], color=palette[4], lw=4)]\n\nplt.figure(figsize=(20, 1))\nlegend = plt.legend(custom_lines, ['Excerpt', 'Excerpt preprocessed'],loc=\"center\")\nplt.setp(legend.texts, family='Serif')\nplt.axis('off')\nplt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Target\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Target\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Target\")\n\nfig, ax = plt.subplots(1,2,figsize=(15,6))\nsns.kdeplot(data=text_props, x=sentence_count,color=palette[0],label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='target',color= palette[0],ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Target\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','lexicon_count','lexicon_count_pre','avg_text','avg_text_pre','sentence_count','target']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='Oranges', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","9a5920a6":"text_props['pos_tags'] = train['excerpt_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)","fe8d07c9":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))","23890b25":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(20,8))\nax = sns.barplot(x=pos.index, y=pos.values,color=palette[1])\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.title('POS Tags Frequency',fontsize=15,font=\"Serif\")\nplt.show()","cfe41e94":"# Let's take around Dataset","48b23192":"## Distribution","65b5289f":"# References\n* https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline\n* https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\n* https:\/\/www.kaggle.com\/alaasedeeq\/commonlit-readability-eda\n* https:\/\/www.kaggle.com\/utcarshagrawal\/commonlit-eda-model-ml-dl\n* https:\/\/www.kaggle.com\/gunesevitan\/commonlit-readability-prize-eda","9df009c1":"# Load data\n","b7f2a5f6":"# Data\n\n## Files\n* **train.csv** - the training set\n* **test.csv** - the test set\n* **sample_submission.csv** - a sample submission file in the correct format\n\n## Columns\n* **id** - unique ID for excerpt\n* **url_legal** - URL of source - this is blank in the test set.\n* **license** - license of source material - this is blank in the test set.\n* **excerpt** - text to predict reading ease of\n* **target** - reading ease\n* **standard_error** - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","baf072e7":"## The 7 highest  target values","bf13e67f":"![header (1).jpg](attachment:23ee4f2d-6afb-45ce-a211-4b6024f17c2f.jpg)","dc9d58e9":"## Unigrams & Bigrams & Trigrams","ee61859c":"* target <= 0 --> High Complexity\n* target > 0 --> Low Complexity\n","c2731cd5":"![download (1).jfif](attachment:bd475181-af0e-4a1d-b1dc-e145bd3d696b.jfif)","ae13b36e":"# Introduction \n* In this competition, we\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.\n* This will aid administrators, teachers, and students. And literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms","eb441627":"# Import libraries","0a06b31d":"## The first 7 rows","21cc11f2":"# Part Of Speech (POS) Tagging\n* The process of assigning a part-of-speech to each word in a sentence \n\n![20190613_part-of-speech-620x243.png](attachment:b80d028f-5955-4000-990d-0ac8d361a272.png)\n\n![POS-Tags.png](attachment:e72f4f41-739e-4439-b9a4-935e4c49d51d.png)\n","4de50422":"## The 7 lowest target values","72c0958a":"# **CommonLit Readability - EDA**"}}