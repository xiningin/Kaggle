{"cell_type":{"7f923d92":"code","5cefb45b":"code","f3f37452":"code","d66f2495":"code","0356c9ac":"code","81b9ba7d":"code","d01ab6e2":"code","5b77ffed":"code","2bb93f94":"code","27d7b4a5":"code","c7cad64e":"code","e6227d7f":"code","95be6f09":"code","48bd09ee":"code","2215a267":"code","8fbd1dbc":"code","b149fcdd":"code","8fde47dc":"code","87b9f06f":"markdown","3269967d":"markdown","75fc48d5":"markdown","7f8afef1":"markdown","8d396cec":"markdown","65703660":"markdown","21ebe422":"markdown"},"source":{"7f923d92":"import pandas as pd\nimport numpy as np\nfrom sklearn import metrics, preprocessing, linear_model\n\n# deixa fixo o fator de aleatoriedade\nnp.random.seed(0)","5cefb45b":"# carrega os dados\ndata = pd.read_csv('..\/input\/training_data.csv', header=0)\n\ndata = data.drop([\n    'id', 'era', 'data_type', 'target_charles', 'target_elizabeth',\n    'target_jordan', 'target_ken', 'target_frank', 'target_hillary'],axis=1)","f3f37452":"# transforma o CSV em numpy\nfeatures = [f for f in list(data) if \"feature\" in f]\nX = data[features]\ny = data['target_bernie']","d66f2495":"# exibe quantidade de amostras e atributos\nprint(X.shape)\nprint(y.shape)","0356c9ac":"# a partir daqui \u00e9 com voc\u00ea...\n# 1) separe 30% dos dados para teste e utilize os outros 70% como achar melhor\n# 2) lembre-se de aplicar os conceitos vistos em aula","81b9ba7d":"data.info()","d01ab6e2":"data.describe()","5b77ffed":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\nX_train_raw, X_test_raw, y_train_raw, y_test_raw = X_train, X_test, y_train, y_test","2bb93f94":"print('Using the entire dataset for baseline definition')\nprint('Using no Feature Extraction and simple Logistic Regression. Mostly a baseline suggested by Numer.ai\\' guide')\nprint('-------')\n\nfrom sklearn import metrics, preprocessing, linear_model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nmodel = linear_model.LogisticRegression(n_jobs=-1, solver='lbfgs')\nmodel.fit(X_train, y_train)\ny_pred_class = model.predict(X_test)\ny_pred_proba = model.predict_proba(X_test)\n\nprint(f'With Logistic Regression - Accuracy: {accuracy_score(y_test, y_pred_class) * 100}% | Log Loss: {log_loss(y_test, y_pred_proba, eps=1e-15)}')","27d7b4a5":"# Define a quantidade de features que queremos extrair do processo de sele\u00e7\u00e3o\nrelevant_columns_amount = 4\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nprint('Since we\\'ll be using PCA for extraction (and it\\'s quite efficient at it),we\\'ll use the entire datasets from this round of modelling')\nprint(f'Using PCA for Feature Extraction, with the {relevant_columns_amount} most relevant features as output')\nprint('-------')\n\n# Extra\u00e7\u00e3o de Features com PCA\npca = PCA(n_components=relevant_columns_amount)\nfit = pca.fit(X_train)\n\n# Efetuamos o fit() com dados de treino e os tranform() diretamente nas duas massas de dados que geramos durante a separa\u00e7\u00e3o (X_train e X_test)\nX_train = pd.DataFrame(pca.transform(X_train))\nX_test = pd.DataFrame(pca.transform(X_test))\n\nfor i in np.arange(11,21,2):\n    # treinando o modelo \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n\n    # testando o modelo contra os 30% de testes que foram removidos anteriormente\n    y_pred_class = knn.predict(X_test)\n    y_pred_proba = knn.predict_proba(X_test)\n    print(f'KNN with k={i} - Accuracy: {accuracy_score(y_test, y_pred_class) * 100}% | Log Loss: {log_loss(y_test, y_pred_proba, eps=1e-15)}')\n    print('-------')","c7cad64e":"# Thanks to our previous usage of PCA on the train\/validation\/test data, we'll need to regenerate the datasets, based on the same random_state for ensured replication of the experiment\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n\n# How many features we want our RFE to output for further use in the algorithms\nrelevant_columns_amount = 4\n\nprint(f'From now on, we\\'ll use a reduced version of the datasets, with only {relevant_columns_amount} features instead of the whole 50. Mostly for optimization reasons, since time is still a constraint.')\nprint(f'Using PCA for discarting half of the features and following up with RFE with Logistic Regression for Feature Extraction, with the {relevant_columns_amount} most relevant features as output')\nprint('-------')\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=25)\nfit = pca.fit(X_train)\n\nX_train = pd.DataFrame(pca.transform(X_train))\nX_test = pd.DataFrame(pca.transform(X_test))\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, relevant_columns_amount)\nfit = rfe.fit(X_train, y_train)\n\nX_train = pd.DataFrame(rfe.transform(X_train))\nX_test = pd.DataFrame(rfe.transform(X_test))\n\ny_train = np.ravel(pd.DataFrame(y_train).reset_index(drop=True))\ny_test = np.ravel(pd.DataFrame(y_test).reset_index(drop=True))\n\nprint(f'Will use KNN with k ranging from 9 to 21, both for validation and testing datasets')\nprint('-------')\nfor i in np.arange(9,23,2):\n    # treinando o modelo \n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n\n    # testando o modelo contra os 30% de testes que foram removidos anteriormente\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import log_loss\n    y_pred_class = knn.predict(X_test)\n    y_pred_proba = knn.predict_proba(X_test)\n    print(f'KNN with k={i} - Accuracy: {accuracy_score(y_test, y_pred_class) * 100}% | Log Loss: {log_loss(y_test, y_pred_proba, eps=1e-15)}')\n    print('-------')","e6227d7f":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, ShuffleSplit\n\nsvm = LinearSVC()\nclf = CalibratedClassifierCV(svm, cv=ShuffleSplit())\nclf.fit(X_train, y_train)\n\ny_pred_class = clf.predict(X_test)\ny_pred_proba = clf.predict_proba(X_test)\n\nprint(f'Linear Support Vector Classifier - Accuracy: {accuracy_score(y_test, y_pred_class) * 100}% | Log Loss: {log_loss(y_test, y_pred_proba, eps=1e-15)}')","95be6f09":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nfor i in np.arange(1,5,1):\n    # treinando o modelo \n    clf = DecisionTreeClassifier(max_depth=i)\n    clf.fit(X_train_raw,y_train_raw)\n\n    # testando o modelo contra os 30% de testes que foram removidos anteriormente\n    from sklearn.metrics import accuracy_score\n    y_pred_class = clf.predict(X_test_raw)\n    y_pred_proba = clf.predict_proba(X_test_raw)\n    print(f'Decision Tree with max_depth={i} - Accuracy: {round(accuracy_score(y_test_raw, y_pred_class) * 100,6)}% | Log Loss: {round(log_loss(y_test_raw, y_pred_proba, eps=1e-15),6)}')","48bd09ee":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nfor i in np.arange(20,31,1):\n    # treinando o modelo \n    clf = RandomForestClassifier(n_estimators=i, max_depth=6, random_state=0)\n    clf.fit(X_train_raw,y_train_raw)\n\n    # testando o modelo contra os 30% de testes que foram removidos anteriormente\n    y_pred_class = clf.predict(X_test_raw)\n    y_pred_proba = clf.predict_proba(X_test_raw)\n    print(f'Random Forest with n_estimators={i} and max_depth=6 - Accuracy: {round(accuracy_score(y_test_raw, y_pred_class) * 100,6)}% | Log Loss: {round(log_loss(y_test_raw, y_pred_proba, eps=1e-15),6)}')","2215a267":"## With Logistic Regression - Accuracy: 51.7650179% | Log Loss: 0.692209\n\n# 0.691975 0.692793","8fbd1dbc":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\n# treinando o modelo \nclf = SGDClassifier(loss='modified_huber', shuffle=True, random_state=0, max_iter=1000, tol=1e-3)\nclf.fit(X_train_raw,y_train_raw)\n\n# testando o modelo contra os 30% de testes que foram removidos anteriormente\ny_pred_class = clf.predict(X_test_raw)\ny_pred_proba = clf.predict_proba(X_test_raw)\nprint(f'Stochastic Gradient Descent with loss=modified_huber - Accuracy: {accuracy_score(y_test, y_pred_class) * 100}%  | Log Loss: {log_loss(y_test, y_pred_proba, eps=1e-15)}')","b149fcdd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nprint('Nosso Random Forest ter\u00e1 29 estimadores e utiliza 6 n\u00edveis de profundidade, servindo como base para nosso estudo final.')\nprint('Alias, nossa base \u00e9 de: Accuracy: 51.765018% | Log Loss: 0.692209')\n\nclf = RandomForestClassifier(n_estimators=29, max_depth=6, random_state=0)\nclf.fit(X_train_raw,y_train_raw)\n\n# testando o modelo contra os 30% de testes que foram removidos anteriormente\ny_pred_class = clf.predict(X_test_raw)\ny_pred_proba = clf.predict_proba(X_test_raw)\nprint(f'Random Forest with n_estimators=29 and max_depth=6 - Accuracy: {round(accuracy_score(y_test_raw, y_pred_class) * 100,6)}% | Log Loss: {round(log_loss(y_test_raw, y_pred_proba, eps=1e-15),6)}')","8fde47dc":"predictionResult = pd.DataFrame(y_pred_class)\npredictedProbability = pd.DataFrame(y_pred_proba)\npredictionResult.rename(index=str, columns={0: \"predicted_class\"}, inplace=True)\npredictionResult.index = X_test_raw.index\npredictedProbability.index = X_test_raw.index\npredictionResult['actual_class'] = pd.DataFrame(y_test_raw)\npredictionResult['probability_of_target'] = predictedProbability[1]\npredictionResult = predictionResult[['actual_class', 'predicted_class','probability_of_target']]\nmatches = len(predictionResult[(predictionResult.actual_class == predictionResult.predicted_class)])\n\nprint(f'Dos {len(predictionResult)} valores previstos, {matches} est\u00e3o corretos, totalizando {round((matches \/ len(predictionResult)) * 100, 6)}% de acur\u00e1cia')\ndisplay(predictionResult)","87b9f06f":"# P\u00f3s Ci\u00eancia dos Dados - Aplica\u00e7\u00f5es de Aprendizado de M\u00e1quina\n\n## Atividade 1\n\nUtilize as c\u00e9lulas a seguir como base para carregar as informa\u00e7\u00f5es, e a partir da \u00faltima c\u00e9lula desse modelo, desenvolva seu racioc\u00ednio e como seria a abordagem do seu grupo para esse problema. *N\u00e3o altere as c\u00e9lulas apresentadas aqui para n\u00e3o danificar a leitura dos dados.*\n\nOs dados est\u00e3o mascarados, mas basicamente, a vari\u00e1vel *y* aponta para quando uma determinada a\u00e7\u00e3o vai subir ou vai descer na bolsa.\n\n---\n\n**Nome dos integrantes**\n\nAluno 1: Roberto Leonardo Rodrigues - RA: 060235\n\nAluno 2: Renan Renger Ribeiro - RA: 183148\n\nAluno 3: Jos\u00e9 Diniz\n\nAluno 4: Ciro Mora\n\n---\n\nLembrem-se, o grupo deve ter no m\u00e1ximo 4 integrantes por grupo, e dever\u00e3o ser entregues at\u00e9 **08\/02 \u00e0s 23h59** esse notebook preenchido com os experimentos realizados e as conclus\u00f5es observadas, al\u00e9m de um PDF com 5 slides para o grupo apresentar em 3 minutos para o restante da sala.","3269967d":"# Resultado dos algoritmos\nUm adendo antes de seguirmos: as duas grandezas observadas e ponderadas foram a acur\u00e1cia do modelo e o Logloss do mesmo.\n\nComo dito l\u00e1 em cima, a ideia era ter um algoritmo que performasse melhor do que a Regress\u00e3o Logistica dada como exemplo pelo Numerai.\n\n## Ap\u00f3s todos os testes (e uma boa dose de discuss\u00e3o entre a equipe), chegamos a conclus\u00e3o que utilizar **Random Forest** seria a melhor abordagem, visto que o resultado \u00e9 o mais pr\u00f3ximo da regress\u00e3o logistica feita.","75fc48d5":"# Tempo - Um grande problema\nAinda durante discuss\u00e3o com a equipe, percebemos que operar com as 50 features do dataset original seria muito mais custoso do que estamos dispostos a lidar, ao menos at\u00e9 definirmos um algoritmo que fosse claramente mais eficiente na tarefa do que os outros.\n\nPartindo disso, todos nossos testes utilizam Extra\u00e7\u00e3o de Features para selecionar as features mais significativas do dataset, tanto via PCA quanto via RFE.\nVale mencionar que utilizamos teste Qui-Quadrado + RFE em um momento passado, obtendo resultados inferiores a combina\u00e7\u00e3o escolhida.","7f8afef1":"# Well, now that's sad :(\nWhile our inicial objective was to use KNN and generate some sort of useful result, the algorithm outputted some pretty weak models, either with or without the extraction.\n\nFrom here on, we'll try other approaches in what will most likely be a silly attempt to generate better models, using different algorithms and","8d396cec":"# Descoberta da fonte dos dados - Numer.ai\nDurante discuss\u00e3o entre os membros da equipe, n\u00f3s descobrimos que os dados apresentados vinham de um projeto chamado \"Numerai\" ou \"Numer.ai\",  focado em prever o mercado de a\u00e7\u00f5es via algoritmos de aprendizado de maquina.\n\nPartindo do exemplo b\u00e1sico deles, uma \"simples\" Regress\u00e3o Logistica envolvendo todas as features dispon\u00edveis no dataset, definimos qual seria nosso padr\u00e3o a \"bater\": conseguir achar uma forma de classificar os dados com maior acur\u00e1cia e menor erro do que o exemplo do Numer.ai","65703660":"# Analise preliminar do Dataset\nMesmo antes de saber a origem do dataset, pudermos ver que os dados haviam sido normalizados previamente, o que auxilia drasticamente o processamento dos dados pela nossa parte.\n\nAl\u00e9m disso, n\u00e3o temos nenhum valor NaN ou nulo, o que nos livra de fazer algum processamento pr\u00e9vio antes de inserir os dados no algoritmo.","21ebe422":"# Resultado final da analise - \"Qu\u00e3o errado estamos?\"\n\n## Nosso alvo (o \"target\" de \"probability_of_target\") \u00e9 de 1 (que, para efeitos de simplica\u00e7\u00e3o, chamaremos de \"Fazer Algo\"). \n\nNos passos acima, buscamos saber quando devemos \"Fazer Algo\" e quando n\u00e3o devemos, baseado nas 50 features que nos foram dadas originalmente. Para tanto, treinamos nosso algoritmo de **Random Forest** com 70% dos dados do Dataset que foi passado, retornando tanto as classes previstas quanto a probabilidade (ou confian\u00e7a, por assim dizer) que o modelo tinha daquela resposta.\n\nNo Dataframe abaixo, pegamos cada previs\u00e3o e as comparamos com o valor real, extra\u00eddo do nosso dataset de testes, juntamente com a confian\u00e7a\/probabilidade que o modelo tem de que aquela previs\u00e3o bate com nosso alvo (lembrando que nosso alvo \u00e9 o **\"Fazer A\u00e7\u00e3o\"** ou a classe bin\u00e1ria **1**)."}}