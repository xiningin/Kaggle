{"cell_type":{"ebddec3c":"code","cf86d862":"code","7b7c7220":"code","08af5eae":"code","66dc553b":"code","f175656b":"code","c5ee88af":"code","0b649a7b":"code","c1d001c4":"code","dcc5e2ef":"code","2551b46d":"code","5ea6b285":"code","5ef5576c":"code","df19e79f":"code","a95ca87f":"code","8dfc5060":"code","f19069ab":"code","c19f4cb4":"code","d2ab27c1":"code","c3a7dd56":"code","7eb968ca":"code","abfc8f83":"code","a764e4b6":"code","67d9a710":"code","758bfc60":"code","c0157497":"code","06314cbb":"code","99aa101f":"code","011ebaee":"code","031ec324":"markdown","33a9a0a1":"markdown","f4d8bf9c":"markdown","d324f39c":"markdown","7b0cec81":"markdown","df9578d6":"markdown"},"source":{"ebddec3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nfrom PIL import Image\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","cf86d862":"run_model1 = False\nrun_model2 = True","7b7c7220":"def load_cifar10_data(filename):\n    with open('..\/input\/cifar10\/'+ filename, 'rb') as file:\n        batch = pickle.load(file, encoding='latin1')\n\n    features = batch['data']\n    labels = batch['labels']\n    return features, labels","08af5eae":"# Load files\nbatch_1, labels_1 = load_cifar10_data('data_batch_1')\nbatch_2, labels_2 = load_cifar10_data('data_batch_2')\nbatch_3, labels_3 = load_cifar10_data('data_batch_3')\nbatch_4, labels_4 = load_cifar10_data('data_batch_4')\nbatch_5, labels_5 = load_cifar10_data('data_batch_5')\n\ntest, label_test = load_cifar10_data('test_batch')","66dc553b":"# Merge files\nX_train = np.concatenate([batch_1,batch_2,batch_3,batch_4,batch_5], 0)\nY_train = np.concatenate([labels_1,labels_2,labels_3,labels_4,labels_5], 0)\n","f175656b":"classes = ('airplane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndef return_photo(batch_file):\n    assert batch_file.shape[1] == 3072\n    dim = np.sqrt(1024).astype(int)\n    r = batch_file[:, 0:1024].reshape(batch_file.shape[0], dim, dim, 1)\n    g = batch_file[:, 1024:2048].reshape(batch_file.shape[0], dim, dim, 1)\n    b = batch_file[:, 2048:3072].reshape(batch_file.shape[0], dim, dim, 1)\n    photo = np.concatenate([r,g,b], -1)\n    return photo\n\n\n","c5ee88af":"X_train = return_photo(X_train)\nX_test = return_photo(test)\nY_test = np.array(label_test)","0b649a7b":"def plot_image(number, file, label, pred=None):\n    fig = plt.figure(figsize = (3,2))\n    #img = return_photo(batch_file)\n    plt.imshow(file[number])\n    if pred is None:\n        plt.title(classes[label[number]])\n    else:\n        plt.title('Label_true: ' + classes[label[number]] + '\\nLabel_pred: ' + classes[pred[number]])\n    \nplot_image(12345, X_train, Y_train)","c1d001c4":"# The cifar-10 is designed to balance distribution that the counts for each classification are 5000\nimport seaborn as sns\nsns.countplot(Y_train)\nhist_Y_train = pd.Series(Y_train).groupby(Y_train).count()\nprint(hist_Y_train)","dcc5e2ef":"# Final check for dimensions before pre-pocessing\nprint('X_train shape:', X_train.shape)\nprint('Y_train shape:', Y_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('Y_test shape:', Y_test.shape)","2551b46d":"# split the validation set out\nfrom sklearn.model_selection import train_test_split\nX_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(\n    X_train, Y_train, test_size=0.2, random_state=42)","5ea6b285":" \n### Prepare for training & testing dataset. Define dataset class.\nimport torch\nimport torchvision.transforms as transforms\nimport random\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\n# define the random seed for reproducible result\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n\n\nclass CIFAR10_from_array(Dataset): \n    def __init__(self, data, label, transform=None):\n        ##############################################\n        ### Initialize paths, transforms, and so on\n        ##############################################\n        #self.data = torch.from_numpy(data).float()\n        #self.label = torch.from_numpy(label).long()\n        self.data = data\n        self.label = label\n        self.transform = transform\n        self.img_shape = data.shape\n        \n    def __getitem__(self, index):\n        ##############################################\n        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n        # 2. Preprocess the data (torchvision.Transform).\n        # 3. Return the data (e.g. image and label)\n        ##############################################\n        \n        img = Image.fromarray(self.data[index])\n        label = self.label[index]\n        if self.transform is not None:\n            img = self.transform(img)\n        else:\n            img_to_tensor = transforms.ToTensor()\n            img = img_to_tensor(img)\n            #label = torch.from_numpy(label).long()\n        return img, label\n        \n    def __len__(self):\n        ##############################################\n        ### Indicate the total size of the dataset\n        ##############################################\n        return len(self.data)\n    \n    def plot_image(self, number):\n        file = self.data\n        label = self.label\n        fig = plt.figure(figsize = (3,2))\n        #img = return_photo(batch_file)\n        plt.imshow(file[number])\n        plt.title(classes[label[number]])\n        \n        \n","5ef5576c":"class CIFAR10_from_url(Dataset): \n    pass","df19e79f":"# Normalize for R, G, B with img = img - mean \/ std\ndef normalize_dataset(data):\n    mean = data.mean(axis=(0,1,2)) \/ 255.0\n    std = data.std(axis=(0,1,2)) \/ 255.0\n    normalize = transforms.Normalize(mean=mean, std=std)\n    return normalize\n\n\n# \ntrain_transform_aug = transforms.Compose([\n###\n#insert your code here  \n#implement some data augmentation methods here\n#for example, transforms.Resize((40, 40)),\n    \n###\n    \n    transforms.ToTensor(),\n    normalize_dataset(X_train)\n])\n\n# Also use X_train in normalize since train\/val sets should have same distribution\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    normalize_dataset(X_train)\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    normalize_dataset(X_test)\n])\n\ntrainset = CIFAR10_from_array(data=X_train_split, label=Y_train_split, transform=train_transform_aug)\nvalset = CIFAR10_from_array(data=X_val_split, label=Y_val_split, transform=val_transform)\ntestset = CIFAR10_from_array(data=X_test, label=Y_test, transform=test_transform)","a95ca87f":"print('data shape check')\nprint('training set:'.ljust(20) + '{}'.format(trainset.img_shape))\nprint('validation set:'.ljust(20) + '{}'.format(valset.img_shape))\nprint('testing set:'.ljust(20) + '{}'.format(testset.img_shape))\nprint('label numbers:'.ljust(20) + '{}'.format(len(set(trainset.label))))","8dfc5060":"# put into the data loader\nfrom torch.utils.data import DataLoader\n\nbatch_size = 64\nnum_workers = 1\n\ntrain_loader = DataLoader(dataset=trainset,\n                          batch_size=batch_size, \n                          shuffle=True,\n                          num_workers=num_workers)\n\n\nval_loader = DataLoader(dataset=valset,\n                          batch_size=batch_size, \n                          shuffle=False,\n                          num_workers=num_workers)\n\ntest_loader = DataLoader(dataset=testset,\n                          batch_size=batch_size, \n                          shuffle=False,\n                          num_workers=num_workers)","f19069ab":"imgs, lbls = iter(train_loader).next()\nprint ('Size of image:', imgs.size())  # batch_size*3*224*224\nprint ('Type of image:', imgs.dtype)   # float32\nprint ('Size of label:', lbls.size())  # batch_size\nprint ('Type of label:', lbls.dtype)   # int64(long)\n","c19f4cb4":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torch.utils.data as Data\nimport torchvision.transforms as transforms","d2ab27c1":"# Build the model\nclass Net(nn.Module):\n        #implement your own network here\n        #you can use any simple CNN structure learned from class\n        #for example, LeNet\n        \n    \n    def __init__(self):\n        super(Net, self).__init__()\n        \n            \n\n    def forward(self, x):\n        \n        \n        return x\n\n\n\n","c3a7dd56":"#Model training\nimport torch.optim as optim\nimport time\n\ndef update_info(idx, length, epoch_loss, acc, mode):\n    \n    if length >= 250:\n        update_size = int(length\/250)\n    else:\n        update_size = 5\n    \n    if idx % update_size == 0 and idx != 0:\n        #print ('=', end=\"\")        \n        finish_rate = idx\/length * 100\n        print (\"\\r   {} progress: {:.2f}%  ......  loss: {:.4f} , acc: {:.4f}\".\n               format(mode, finish_rate, epoch_loss\/idx, acc), end=\"\", flush=True)\n        \n\ndef val_per_epoch(model, loss_fn, dataloader, verbose):\n    # In validation, we only compute loss value\n    model.eval()\n    epoch_loss = 0.0\n    acc = 0.0\n    val_size = 0\n    with torch.no_grad(): \n        for i, (feature, target) in enumerate(dataloader):\n            \n            #feature, target = feature.to(device), target.to(device)\n            if torch.cuda.is_available():\n                feature = feature.cuda()\n                target = target.cuda()\n            \n            output = model(feature) #outputs.data.shape= batches_num * num_class\n            \n            #compute acc\n            _, pred = torch.max(output.data, dim=1) \n            correct = (pred == target).sum().item() #convert to number\n            val_size += target.size(0)\n            acc += correct\n            \n            \n            loss = loss_fn(output, target)\n            epoch_loss += loss.item()\n            \n            \n            idx = i\n            length = len(dataloader)\n            \n            #display progress\n            if verbose:\n                update_info(idx, length, epoch_loss, acc\/val_size, 'validating')\n                \n        acc = acc\/val_size\n    print('')\n    return epoch_loss\/len(dataloader), acc\n\n\ndef train_per_epoch(model, loss_fn, dataloader, optimizer, verbose): \n    #train mode\n    model.train()\n    \n    #initialize loss\n    epoch_loss = 0.0\n    acc = 0.0\n    train_size = 0\n    \n    for i, (feature, target) in enumerate(dataloader):\n        #feature, target = feature.to(device), target.to(device)\n        \n        if torch.cuda.is_available():\n            feature = feature.cuda()\n            target = target.cuda()\n        \n        #set zero to the parameter gradients for initialization\n        optimizer.zero_grad()\n        output = model(feature)\n        loss = loss_fn(output, target)\n        \n        \n        #compute acc\n        _, pred = torch.max(output.data, dim=1) \n        correct = (pred == target).sum().item() #convert to number\n        train_size += target.size(0)\n        acc += correct\n        \n        #compute current loss. Loss is a 0-dim tensor, so use tensor.item() to get the scalar value\n        epoch_loss += loss.item()  \n        \n        #backward propagation\n        loss.backward()\n        \n        #this represents one update on the weight\/bias for a mini-batch(16 images in our case): \n        #weights[k] + alpha * d_weights[k]\n        optimizer.step()\n        \n        #show the update information\n        idx = i\n        length = len(dataloader)\n        \n        #display progress\n        if verbose:\n            update_info(idx, length, epoch_loss, acc\/train_size, '  training')\n            \n    acc = acc\/train_size\n    print('') \n    return epoch_loss\/len(dataloader), acc\n\n\n\n\n\ndef model_training(num_epochs, model, loss_fn, train_loader, optimizer, val_loader=None, verbose=True):\n    \n    train_batch_num = len(train_loader)\n    history = {}\n    history['train_loss'] = []\n    history['val_loss'] = []\n    history['train_acc'] = []\n    history['val_acc'] = []\n    \n    if val_loader is not None:\n        \n        val_batch_num = len(val_loader)\n        \n        print('Total Sample: Train on {} samples, validate on {} samples.'.\n             format(trainset.img_shape[0], valset.img_shape[0]))\n        \n        print(' Total Batch: Train on {} batches, validate on {} batches. {} samples\/minibatch \\n'.\n         format(train_batch_num, val_batch_num, batch_size))\n    \n    else:\n        print('Total Sample: Train on {} samples.'.\n             format(train_batch_num*batch_size))\n        \n        print(' Total Batch: Train on {} batches, {} samples\/minibatch \\n'.\n         format(train_batch_num, batch_size))\n    \n    \n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        train_loss, train_acc = train_per_epoch(model, loss_fn, train_loader, optimizer, verbose=verbose)\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        \n        \n        if val_loader is not None:\n            val_loss, val_acc = val_per_epoch(model, loss_fn, val_loader, verbose=verbose)\n            print('\\n        Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(train_loss,val_loss))\n            print('         Training acc: {:.4f},  Validation acc: {:.4f}\\n'.format(train_acc,val_acc))\n            history['val_loss'].append(val_loss)\n            history['val_acc'].append(val_acc)\n                        \n        else:\n            print('\\n        Training Loss: {:.4f}\\n'.format(train_loss))\n            print('\\n         Training acc: {:.4f}\\n'.format(train_acc))\n        \n    \n    return history","7eb968ca":"# Training\/Validating the model\nclasses = ('airplane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndef lr_decay(parm):\n    pass\n\nif __name__ == '__main__' and run_model1 == True:\n\n    num_epochs = 10\n    learning_rate = 0.001\n\n    net = Net()\n\n    if torch.cuda.is_available():\n        net = net.cuda()\n    print(net)\n    print('=================================================================')\n\n    #insert your code here\n    #implement criterion(also known as loss funtion) and optimizer here\n    #we suggest you to use CrossEntropyLoss and Adam\n    \n    \n\n    #training and validating\n    hist1 = model_training(num_epochs, net, criterion, train_loader, optimizer, val_loader, verbose=True)\n","abfc8f83":"def imshow(img):\n    img = img    # unnormalize\n    #print(img)\n    npimg = img.numpy()\n    print(np.transpose(npimg, (1, 2, 0)).shape)\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nif __name__ == '__main__' and run_model1 == True:\n    dataiter = iter(test_loader)\n    images, labels = dataiter.next()\n\n\n\n    for i in range(len(images)):\n        plot_image(i, images.permute(0, 2, 3, 1).numpy(), labels.numpy())\n\n\n    if torch.cuda.is_available():\n        images = images.cuda()\n    outputs = net(images)\n\n    _, predicted = torch.max(outputs, 1)\n\n    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                                  for j in range(5)))","a764e4b6":"def model_testing(model, loss_fn, dataloader, verbose=True):\n    Y_pred = []\n    correct = 0\n    total = 0\n    epoch_loss = 0.0\n    acc = 0.0\n    test_size = 0\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(dataloader):\n            if torch.cuda.is_available():\n                feature = feature.cuda()\n                target = target.cuda()\n\n            outputs = model(feature)  #outputs.data.shape= batches_num * num_class\n            \n            #compute acc\n            _, pred = torch.max(outputs.data, 1)\n            correct = (pred == target).sum().item() #convert to number\n            test_size += target.size(0)\n            #print(test_size)\n            acc += correct\n            \n            loss = loss_fn(outputs, target)\n            epoch_loss += loss.item()\n            \n            idx = i\n            length = len(dataloader)\n\n\n            #if torch.cuda.is_available():\n            #    pred = pred.cuda()\n            \n            #Pred labels \n            Y_pred += pred.cpu().numpy().tolist()\n            \n            if verbose:\n                update_info(idx, length, epoch_loss, acc\/test_size, 'testing')    \n            \n    acc = acc\/test_size\n    print('\\n\\n Accuracy of the network on the {} test images: {}%'.format(test_size, 100*acc))\n    \n    return Y_pred\n\n\n\n\nif __name__ == '__main__' and run_model1 == True:\n    Y_pred1 = model_testing(net, criterion, test_loader, True)\n    \nif __name__ == '__main__' and run_model2 == True:\n    Y_pred2 = model_testing(net, criterion, test_loader, True)","67d9a710":"# Plot the loss and accuracy curves for training and validation \ndef loss_acc_plt(history):\n    fig, ax = plt.subplots(2,1)\n    ax[0].plot(history['train_loss'], color='b', label=\"Training loss\")\n    ax[0].plot(history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n\n    ax[1].plot(history['train_acc'], color='b', label=\"Training accuracy\")\n    ax[1].plot(history['val_acc'], color='r',label=\"Validation accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)\n    ","758bfc60":"if __name__ == '__main__' and run_model1 == True:\n    loss_acc_plt(hist1)\n    \nif __name__ == '__main__' and run_model2 == True:\n    loss_acc_plt(hist2)\n","c0157497":"if __name__ == '__main__' and run_model1 == True:\n    for i in range(10):\n        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred1)\n    \nif __name__ == '__main__' and run_model2 == True:\n    for i in range(10):\n        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred2)\n\n","06314cbb":"from sklearn.metrics import confusion_matrix\n\n\nif __name__ == '__main__' and run_model1 == True:\n    cm = confusion_matrix(Y_test, Y_pred1)\n    \n    \nif __name__ == '__main__' and run_model2 == True:\n    cm = confusion_matrix(Y_test, Y_pred2)\n\ncm","99aa101f":"plt.figure(figsize = (10,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n\nplt.title('Confusion Matrix', fontsize=14)\nplt.colorbar()\nn_classes = cm.shape[0]\nrange_class = range(n_classes)\ntick_marks = np.arange(len(range_class))\nplt.xticks(tick_marks, range_class, rotation=-45, fontsize=14)\nplt.yticks(tick_marks, range_class, fontsize=14)\nplt.xlabel('Predicted label', fontsize=14)\nplt.ylabel('True label', fontsize=14)\n\nfor i in range_class:\n    for j in range_class:        \n        plt.text(j, i, cm[i,j], horizontalalignment=\"center\", fontsize=14, \n                color=\"white\" if i==j else \"black\")\nplt.plot","011ebaee":"for i in range(len(classes)):\n    correct = ((Y_test == i)*1) * ((np.array(Y_pred2) == Y_test)*1)\n    print('{}, {}: '.rjust(10).format(i, classes[i]) + '{}%'.\n          format(100*correct.sum()\/Y_test[Y_test == i].shape[0]))","031ec324":"### Model Testing","33a9a0a1":"### Confusion Matrix","f4d8bf9c":"![pytorch flow](https:\/\/cdn-images-1.medium.com\/max\/800\/1*uZrS4KjAuSJQIJPgOiaJUg.png)","d324f39c":"### Build Model\n\n### Model with out augmentation","7b0cec81":"### Preprocessing","df9578d6":"### Load Files"}}