{"cell_type":{"1a0d9083":"code","3dd930f4":"code","6581bc05":"code","cfe4df70":"code","6845e4a9":"code","e7eee65b":"code","e1c3517e":"code","2df495f3":"code","e4abef76":"code","b7216628":"code","2c6f72a8":"code","9c84440d":"code","672fba7d":"code","5a307a63":"code","714886b0":"code","f088582b":"code","80aaf50c":"code","0303064c":"code","b3f024d6":"code","bc825254":"code","30d58113":"code","5a2ecc52":"code","3c7e179e":"code","f1710f14":"code","a8bad60f":"code","0c4fd047":"code","70aa7188":"code","381f228a":"code","6c1e0a3e":"code","df6184c8":"code","22a00aa6":"code","eec8e20c":"markdown","50c0d487":"markdown","2736d3a2":"markdown","2daa0101":"markdown","6a135ead":"markdown","6f1df097":"markdown","5fd3637a":"markdown","5b27b5a8":"markdown","408eef9a":"markdown","9ad4550f":"markdown","bc2f5116":"markdown","759e8fb0":"markdown","7a2d6332":"markdown","7c3953ba":"markdown","90e08894":"markdown","fad1e9f8":"markdown","c421ddcc":"markdown"},"source":{"1a0d9083":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, Activation, Flatten, Dense, Dropout\nfrom keras.models import Model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3dd930f4":"table = pd.read_csv('\/kaggle\/input\/master_table.csv')\ntable","6581bc05":"table.isna().sum()","cfe4df70":"table['First'] = table['NAME'].apply(lambda x : x.split()[0])\ntable['Last'] = table['NAME'].apply(lambda x : x.split()[1])","6845e4a9":"table.nunique()","e7eee65b":"df = pd.read_csv('\/kaggle\/input\/training_data.csv')\ndf","e1c3517e":"df.isna().sum()","2df495f3":"df.nunique()","e4abef76":"df['MFirst'] = df['MTEXT'].apply(lambda x : x.split()[-2:-1][0] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else x.split()[-1])\ndf['MLast'] = df['MTEXT'].apply(lambda x : x.split()[-1] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else float('NaN'))\ndf","b7216628":"combined = pd.merge(df, table, how='left', left_on=['T1','T2'], right_on=['ID1','ID2'])\ncombined","2c6f72a8":"len(combined[combined['CODE'] == combined['C2']])","9c84440d":"combined['MFirst'] = combined['MTEXT'].apply(lambda x : x.split()[-2:-1][0] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else x.split()[-1])\ncombined['MLast'] = combined['MTEXT'].apply(lambda x : x.split()[-1] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else float('NaN'))\ncombined","672fba7d":"new = pd.merge(df,table,how='inner',left_on=['MFirst','C2'], right_on=['First','CODE'])\nnew","5a307a63":"new.groupby(['C1','C2']).nunique().sum()","714886b0":"new['text'] = new['MTEXT'].apply(lambda x : ' '.join([a for a in x.split() if len(re.findall(r'\\d+',a)) != 0]))\nnew['text'] = new['C1'] + ' '+ new['text']\nnew","f088582b":"bad = new[(new['ID1']==new['T1'])&(new['ID2']!=new['T2'])]\nbad['label'] = False\nbad = bad.groupby(['text']).first().reset_index()\n\nfiltered = new[(new['ID1']==new['T1'])&(new['ID2']==new['T2'])]\nfiltered['label'] = True\n\ndata = filtered[['text','ID2','label']].copy()\ndata = data.append(bad[['text','ID2','label']].copy())\ndata['text'] = data['text'] + ' , ' + data['ID2']\ndata","80aaf50c":"X, y = data['text'], data['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ntrain_texts = list(X_train.values)\ntest_texts = list(X_test.values)","0303064c":"tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=False)\ntk.fit_on_texts(train_texts)\n\nalphabet = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,\"\nchar_dict = {}\nfor i, char in enumerate(alphabet):\n    char_dict[char] = i + 1\n\n# Use char_dict to replace the tk.word_index\ntk.word_index = char_dict.copy()\n# Add 'UNK' to the vocabulary\ntk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n\ntrain_sequences = tk.texts_to_sequences(train_texts)\ntest_texts = tk.texts_to_sequences(test_texts)\n\n# Padding\ntrain_data = pad_sequences(train_sequences, maxlen=70, padding='post')\ntest_data = pad_sequences(test_texts, maxlen=70, padding='post')\n\n# Convert to numpy array\ntrain_data = np.array(train_data, dtype='float32')\ntest_data = np.array(test_data, dtype='float32')\n\n# =======================Get classes================\ntrain_classes = y_train.values\ntrain_class_list = [1 if x else 0 for x in train_classes]\n\ntest_classes = y_test.values\ntest_class_list = [1 if x else 0 for x in test_classes]\n\nfrom keras.utils import to_categorical\n\ntrain_classes = to_categorical(train_class_list)\ntest_classes = to_categorical(test_class_list)","b3f024d6":"np.random.seed(0)\ntf.random.set_seed(2)\n\n\n## =====================Char CNN=======================\n# parameter\ninput_size = 70\nvocab_size = len(tk.word_index)\nembedding_size = len(tk.word_index) # 64\nnum_of_classes = 2\ndropout_p = 0.3\noptimizer = 'adam'\nloss = 'binary_crossentropy'\n\nfor char, i in tk.word_index.items():\n    onehot = np.zeros(vocab_size)\n    onehot[i - 1] = 1\n\n# Embedding layer Initialization\nembedding_layer = Embedding(vocab_size + 1,\n                            embedding_size,\n                            input_length=input_size)\n# Model Construction\n# Input\ninputs = Input(shape=(input_size,), name='input', dtype='int64')\n\n# Embedding\nx = embedding_layer(inputs)\nx = Flatten()(x)\n\n# MLP\nx = Dense(512, activation='relu')(x)\nx = Dropout(dropout_p)(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(dropout_p)(x)\nx = Dense(2056, activation='relu')(x)\nx = Dropout(dropout_p)(x)\n\n# Output Layer\npredictions = Dense(num_of_classes, activation='sigmoid')(x)\n# Build model\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(5e-4), loss=loss, metrics=['accuracy'])\nmodel.summary()\n\n# Shuffle\nindices = np.arange(train_data.shape[0])\nnp.random.shuffle(indices)\n\nx_train = train_data[indices]\ny_train = train_classes[indices]\n\nx_test = test_data\ny_test = test_classes\n\n# Training\nmodel.fit(x_train, y_train,\n          validation_data=(x_test, y_test),\n          batch_size=128,\n          epochs=10,\n          verbose=2)","bc825254":"test_loss, test_acc = model.evaluate(x=x_test, y=y_test)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","30d58113":"predicted = model.predict(x_test)\npred = [np.argmax(row) for row in predicted]\nprob = [np.max(row) for row in predicted]\nprob","5a2ecc52":"test = pd.read_csv('\/kaggle\/input\/test_data.csv')\ntest","3c7e179e":"test['MFirst'] = test['MTEXT'].apply(lambda x : x.split()[-2:-1][0] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else x.split()[-1])\ntest['MLast'] = test['MTEXT'].apply(lambda x : x.split()[-1] if len(re.findall(r'\\d+',x.split()[-2:-1][0])) == 0 else float('NaN'))\ntest","f1710f14":"test_joined = pd.merge(test,table,how='inner',left_on=['MFirst','C2'], right_on=['First','CODE'])\ntest_joined['text'] = test_joined['MTEXT'].apply(lambda x : ' '.join([a for a in x.split() if len(re.findall(r'\\d+',a)) != 0]))\ntest_joined['text'] = test_joined['C1'] + ' '+ test_joined['text'] + ' , ' + test_joined['ID2']\ntest_joined","a8bad60f":"ans_texts = tk.texts_to_sequences(test_joined['text'])\nans_data = pad_sequences(ans_texts, maxlen=70, padding='post')\nans_data = np.array(ans_data, dtype='float32')","0c4fd047":"predicted = model.predict(ans_data)\ntest_joined['prediction'] = [np.argmax(row) for row in predicted]\ntest_joined['prob'] = [np.max(row) for row in predicted]\ntest_joined","70aa7188":"answer = test_joined[test_joined['prediction']==1]\nanswer","381f228a":"print('Remaining values after guessing ID1: {} out of 5000.'.format(sum(test['row_id'].isin(answer['row_id']))))","6c1e0a3e":"output = answer.groupby('row_id',as_index=False).agg({'prob':'max'})\noutput = pd.merge(answer,output,how='inner',on=['prob','row_id'])\noutput = output[['row_id','md_id','prob']]\noutput","df6184c8":"output['prob'].hist()\nprint('Predicted accuracy: {}%.'.format(len(output[output['prob']>.90])\/5000*100))","22a00aa6":"output.to_csv('predictions.csv', index=False)","eec8e20c":"Reading master_table file","50c0d487":"Get first name from MTEXT","2736d3a2":"Model is quite confident of its outputs","2daa0101":"Generating our training dataset, data","6a135ead":"Saving file to output drive","6f1df097":"Splitting NAME to obtain the first and last names","5fd3637a":"With C2 as CODE and First Name from MTEXT, we can get ID1 99% of the time. ID1 is the last name","5b27b5a8":"Insight: C2 in training data corresponds to CODE of desired row","408eef9a":"Splitting name in MTEXT to obtain the first and last names","9ad4550f":"Verifying that there are no missing values.","bc2f5116":"Manual mapping: Get ID1 from C2 + name in MTEXT","759e8fb0":"Visualizing probability output from validation set","7a2d6332":"Insight: MTEXT in training data is part of NAME in desired row","7c3953ba":"EDA on training_data","90e08894":"Running algorithm on test dataset","fad1e9f8":"Author: @billptw, https:\/\/billptw.github.io\/\n\n## Task\nPredict the corresponding person (identified by a ID1,ID2 pair) for a given row in test data.\n\n## Observations\nFirst, we visualize the dataset to understand it better. From this EDA, we note that 1) C2 from the train\/test data corresponds to CODE in master_table, and 2) MTEXT in train\/test contains the first name of our target person. By matching the CODE and First name from the test data to master_table, we generate a list of candidate persons that the test row could correspond to. This brings us to observation 3) ID1 and First name has an almost 1-1 correspondence (1% of the dataset is unmapped). \n\n## Solution\nUsing the 3 observations above, we can reduce the complexity of the problem from requiring a mapping of C1,C2,MTEXT -> ID1,ID2 to C1,MTEXT -> ID2. This is formulated as a binary text classification problem, where the input sentence is the concatenation of C1, MTEXT and the candidate ID2, with the output predicting if the ID2 is the correct output. The input text is converted into a sequence of character-level one-hot embeddings, before feeding into a deep neural network classifier.\n\n## Results\nUsing a multi-layer perceptron, we obtain a test accuracy of ~70% on a held-out validation set. The predicted test accuracy of our entire system (considering the ~1% loss of matching CODE and First name to ID1) is about 69%.","c421ddcc":"How do we convert text into numbers? Using Char level language modeling"}}