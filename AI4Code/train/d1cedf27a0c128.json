{"cell_type":{"cfbe8501":"code","d3c41d1f":"code","34c5ea98":"code","18bb38d3":"code","9891f64f":"code","7bb2e23c":"code","3af2612a":"code","8e1f2893":"code","1aac4e50":"code","1b773d4a":"code","4433f03a":"code","356d737a":"code","ddb15e2d":"code","5d76239d":"code","25d5b9f8":"code","32d5dc56":"code","e49765ec":"code","f7b5b109":"code","4cc06a94":"code","8b5458bf":"code","13c20fde":"code","249aa900":"code","0de79016":"code","4c087010":"code","472a949e":"code","0a46a814":"code","6b118e5e":"code","dcba0a99":"code","9cae4145":"code","ae2d4876":"markdown","42fcada9":"markdown","163a0d2a":"markdown","74a07924":"markdown","1c9c0504":"markdown","04a4a7c0":"markdown","1e1949d9":"markdown","e8539243":"markdown","9c95711d":"markdown","4cd87bfb":"markdown","514ba7ac":"markdown","428886bc":"markdown","bb53aeb9":"markdown","69f63b4c":"markdown","4500b8f6":"markdown","e5101703":"markdown","c865597b":"markdown","a1ea1fcc":"markdown","bd7c6252":"markdown","bde3c849":"markdown","7dd714e9":"markdown","eb8cd230":"markdown","4fa39f7c":"markdown","f7d7736c":"markdown","3db40800":"markdown","7e576204":"markdown","dfe239c4":"markdown","575fa6ab":"markdown"},"source":{"cfbe8501":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","d3c41d1f":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")","34c5ea98":"train.tail()","18bb38d3":"train.describe()","9891f64f":"sns.barplot(x = \"Sex\", y = \"Survived\", data = train,palette=\"Blues\")\nplt.title(\"survival based on gender\")\nplt.show()\n\n\ntotal_survived_female = train[train.Sex == 'female'][\"Survived\"].sum()\ntotal_survived_male = train[train.Sex == 'male'][\"Survived\"].sum()\nprint(f\"Total people survived is: {total_survived_female + total_survived_male}\")\nprint(f\"Proportion of Female who survived: {total_survived_female\/(total_survived_female + total_survived_male)}\")\nprint(f\"Proportion of male who survived: {total_survived_male\/(total_survived_female + total_survived_male)}\")","7bb2e23c":"sns.barplot(x = \"Pclass\",y = \"Survived\", data = train, palette=\"Blues\")\nplt.title(\"Survival based on class\")\nplt.show()\n\ntotal_survived_one = train[train.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = train[train.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = train[train.Pclass == 3][\"Survived\"].sum()\n\nprint(f\"total people survived: {total_survived_one + total_survived_two + total_survived_three}\")\nprint(f\"proportion of class 1 pessangers who survived: {total_survived_one\/(total_survived_one + total_survived_two + total_survived_three)}\")\nprint(f\"proportion of class 2 pessangers who survived: {total_survived_two\/(total_survived_one + total_survived_two + total_survived_three)}\")\nprint(f\"proportion of class 3 pessangers who survived: {total_survived_three\/(total_survived_one + total_survived_two + total_survived_three)}\")","3af2612a":"sns.barplot(x = \"Pclass\",y = \"Survived\", data = train, palette=\"Blues\", hue = \"Sex\")\nplt.title(\"Survived rate based on Gender and Class\")\nplt.show()","8e1f2893":"sns.barplot(x = \"Sex\", y = \"Survived\", data = train, hue=\"Pclass\", palette=\"Blues\")\nplt.title(\"Survived rate based on Gender and Class\")\nplt.show()","1aac4e50":"fig = plt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nsns.histplot(train[train.Survived == 1][\"Age\"], bins = 25)\nplt.subplot(1,2,2)\nplt.title(\"Didn't Survived\")\nplt.ylabel(\"Proportion\")\nsns.histplot(train[train.Survived == 0][\"Age\"], bins = 25)\n\nplt.show()","1b773d4a":"corr_metrics = train.corr()\ncorr_metrics['Survived'].sort_values(ascending=False)","4433f03a":"#correlation matrix\nfig = plt.figure(figsize=(12,12))\nsns.heatmap(corr_metrics,annot=True, square=True, cmap = \"Blues\");","356d737a":"def draw_missing_data_table(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum()\/data.isnull().count() * 100).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n\ndraw_missing_data_table(train).head(20)","ddb15e2d":"train = train.drop([\"Cabin\",\"PassengerId\",\"Ticket\"],axis = 1)","5d76239d":"train[\"Age\"] = train[\"Age\"].fillna(np.mean(train.Age)) \n\ntrain = train.drop(train.loc[train['Embarked'].isnull()].index)\ntrain.isnull().sum().max() #just checking that there's no missing data..","25d5b9f8":"train[\"FamSize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n","32d5dc56":"train[\"IsAlone\"] = train.FamSize.apply(lambda x: 1 if x == 1 else 0)","e49765ec":"train[\"Title\"] = train[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\", \"Rev\": \"Other\", \"Dr\": \"Other\"}\n\ntrain.replace({\"Title\": title_replacements}, inplace=True)\n\ntrain[\"Title\"].unique()","f7b5b109":"train.drop([\"Name\",\"SibSp\",\"Parch\"],axis = 1,inplace=True)","4cc06a94":"#One-Hot-Encoding\n\ntrain.loc[train[\"Sex\"] == \"male\", \"Sex\"] = 0\ntrain.loc[train[\"Sex\"] == \"female\", \"Sex\"] = 1","8b5458bf":"Feature_final = pd.get_dummies(train)\nFeature_final","13c20fde":"from sklearn.model_selection import train_test_split","249aa900":"y = Feature_final[\"Survived\"]\nX = Feature_final.drop(\"Survived\",axis = 1)","0de79016":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25, random_state = 2)","4c087010":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_roc_curve\n\n\nclf_A = SVC()\nclf_B = LinearSVC()\nclf_C = RandomForestClassifier()\nclf_D = LogisticRegression()\nclf_E = KNeighborsClassifier()\nclf_F = GaussianNB()\nclf_G = DecisionTreeClassifier()\n\nclfs = [clf_A,clf_B,clf_C,clf_D,clf_E,clf_F,clf_G]","472a949e":"from sklearn.metrics import roc_auc_score\n\nAUC_scores = pd.DataFrame(index=None,columns = [\"Model\",\"AUC_score\"])\nfor clf in clfs:\n    clf.fit(X_train,y_train)\n    pred = clf.predict(X_test)\n    score = roc_auc_score(y_test,pred)\n    AUC_scores = AUC_scores.append(pd.Series({\"Model\" : clf.__class__.__name__,\n                                              \"AUC_score\" : score}),ignore_index=True)","0a46a814":"AUC_scores","6b118e5e":"from sklearn.metrics import f1_score,accuracy_score\nModel_performance = pd.DataFrame(index = None,columns = [\"Model\",\"f1_score\",\"Accuracy_score\"])\nfor clf in clfs:\n    clf.fit(X_train,y_train)\n    pred = clf.predict(X_test)\n    score1 = f1_score(y_test,pred)\n    score2 = accuracy_score(y_test,pred)\n    Model_performance = Model_performance.append(pd.Series({\"Model\" : clf.__class__.__name__,\n                                              \"f1_score\" : score1,\n                                              \"Accuracy_score\" : score2}),ignore_index=True)","dcba0a99":"Model_performance","9cae4145":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparameters = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n              \"max_depth\": [2, 3, 5, 10], \n              \"min_samples_split\": [2, 3, 5, 10],\n              \"min_samples_leaf\": [1, 5, 8, 10]\n             }\n\ngrid_cv = GridSearchCV(clf_C, parameters, scoring = make_scorer(f1_score))\ngrid_cv = grid_cv.fit(X_train, y_train)\n\nprint(\"Our optimized Random Forest model is:\")\ngrid_cv.best_estimator_","ae2d4876":"\n* It appears that class also plays a role in survival.","42fcada9":"### 5. Feature Engineering","163a0d2a":"### Variable Notes\n\n**pclass:** A proxy for socio-economic status (SES)\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower\n\n**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp:** The dataset defines family relations in this way...\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch:** The dataset defines family relations in this way...\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","74a07924":"\n**Defining Features in Training\/Test Set**","1c9c0504":"# Machine Learning to Predict Titanic Survivors","04a4a7c0":"Now that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. We will try different types of models to see which one gives the best accuracy for its predictions.","1e1949d9":"**We can combine SibSp and Parch into one synthetic feature called family size, which indicates the total number of family members on board for each membe**","e8539243":"### 4. Handling Missing values","9c95711d":"Now, We are looking for correlation between all the attributes.","4cd87bfb":"**Age**","514ba7ac":"\nAlthough it may not seem like it, we can also extract some useful information from the name column. Not the actual names themselves, but the title of their names like Ms. or Mr. This may also provide a hint as to whether the passenger survived or not. Therefore we can extract this title and then encode it like we did for Sex and Embarked.","428886bc":"\n* Gender appears to be a very good feature to use to predict survival, as shown by the large difference in propotion survived. Let's take a look at how class plays a role in survival as well.","bb53aeb9":"This IsAlone feature also may work well with the data we're dealing with, telling us whether the passenger was along or not on the ship.","69f63b4c":"### 6. Model Fitting and Predicting","4500b8f6":"It appears that the Random Forest model works the best with our data so we will use it.","e5101703":"**We just analysed Three variables, but there are many other that we should analyse.**","c865597b":"\nWe can improve the accuracy of our model by turning the hyperparameters of our Random Forest model. We will run a GridSearchCV to find the best parameters for the model and use that model to train and test our data.","a1ea1fcc":"### 8. Tuning Parameters with GridSearchCV","bd7c6252":"<i>Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself.<\/i>","bde3c849":"**Class**","7dd714e9":"**RandomForestClassifier**","eb8cd230":"**Gender**","4fa39f7c":"### 7. Evaluating Model Performances","f7d7736c":"We have 3 columns with missing values.\n\nWe'll consider that when more than 25% of the data is missing, we should delete the corresponding variable and pretend it never existed.\n\nWe imputing mean value inplace of missing `Age`.","3db40800":"----","7e576204":"### 1. Importing Libraries and Packages","dfe239c4":"### 2. Loading and Viewing Data Set","575fa6ab":"### 3. Visulized and Plotting the data"}}