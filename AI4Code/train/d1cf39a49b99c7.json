{"cell_type":{"93d318fe":"code","5bb1d141":"code","a98dd48c":"code","a44a405d":"code","3b894863":"code","f3c7db54":"code","0fb006e5":"code","bdd900d5":"code","69ab061e":"code","b586a8ae":"code","fdced3b9":"code","c0628953":"code","aa5ff909":"code","c03542a1":"code","be88f9e4":"code","939bf452":"code","b23c5fea":"code","a5651976":"code","38bb92b5":"code","32294719":"code","47bf1a10":"code","1eae9fed":"code","9dd7101a":"markdown","1f98d21d":"markdown","f982278b":"markdown","3cbd6c67":"markdown","5917b542":"markdown"},"source":{"93d318fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5bb1d141":"# imports\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import signal\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix\nfrom keras.models import Model\nimport keras.layers as L\nfrom keras.utils import to_categorical, plot_model","a98dd48c":"# read data\ndata = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ndata.head()","a44a405d":"def calc_gradients(s, n_grads=4):\n    '''\n    Calculate gradients for a pandas series. Returns the same number of samples\n    '''\n    grads = pd.DataFrame()\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n        \n    return grads","3b894863":"def calc_low_pass(s, n_filts=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.3, n_filts)\n    \n    low_pass = pd.DataFrame()\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, s.values)\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, s.values)\n        \n    return low_pass","f3c7db54":"def calc_high_pass(s, n_filts=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.1, n_filts)\n    \n    high_pass = pd.DataFrame()\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        high_pass['hihgpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, s.values)\n        high_pass['hihgpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, s.values)\n        \n    return high_pass","0fb006e5":"def calc_roll_stats(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats = pd.DataFrame()\n    for window in windows:\n        roll_stats['roll_mean_' + str(window)] = s.rolling(window=window, min_periods=1).mean()\n        roll_stats['roll_std_' + str(window)] = s.rolling(window=window, min_periods=1).std()\n        roll_stats['roll_min_' + str(window)] = s.rolling(window=window, min_periods=1).min()\n        roll_stats['roll_max_' + str(window)] = s.rolling(window=window, min_periods=1).max()\n        roll_stats['roll_range_' + str(window)] = roll_stats['roll_max_' + str(window)] - roll_stats['roll_min_' + str(window)]\n        roll_stats['roll_q10_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.10)\n        roll_stats['roll_q25_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.25)\n        roll_stats['roll_q50_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.50)\n        roll_stats['roll_q75_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.75)\n        roll_stats['roll_q90_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.90)\n    \n    # add zeros when na values (std)\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats","bdd900d5":"def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates exponential weighted functions\n    '''\n    ewm = pd.DataFrame()\n    for window in windows:\n        ewm['ewm_mean_' + str(window)] = s.ewm(span=window, min_periods=1).mean()\n        ewm['ewm_std_' + str(window)] = s.ewm(span=window, min_periods=1).std()\n        \n    # add zeros when na values (std)\n    ewm = ewm.fillna(value=0)\n        \n    return ewm","69ab061e":"def add_features(s):\n    '''\n    All calculations together\n    '''\n    \n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n    ewm = calc_ewm(s)\n    \n    return pd.concat([s, gradients, low_pass, high_pass, roll_stats, ewm], axis=1)\n\n\ndef divide_and_add_features(s, signal_size=500000):\n    '''\n    Divide the signal in bags of \"signal_size\".\n    Normalize the data dividing it by 15.0\n    '''\n    # normalize\n    s = s\/15.0\n    \n    ls = []\n    for i in tqdm(range(int(s.shape[0]\/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    \n    return pd.concat(ls, axis=0)","b586a8ae":"# apply every feature to data\ndf = divide_and_add_features(data['signal'])\ndf.head()","fdced3b9":"print('df.shape=', df.shape)","c0628953":"# Get train and test data\nx_train, x_test, y_train, y_test = train_test_split(df.values, data['open_channels'].values, test_size=0.2)\n\nprint('x_train.shape=', x_train.shape)\nprint('x_test.shape=', x_test.shape)\nprint('y_train.shape=', y_train.shape)\nprint('y_test.shape=', y_test.shape)","aa5ff909":"def create_mpl(shape):\n    '''\n    Returns a keras model\n    '''\n    \n    X_input = L.Input(shape)\n    \n    X = L.Dense(150, activation='relu')(X_input)\n    X = L.Dense(125, activation='relu')(X)\n    X = L.Dense(75, activation='relu')(X)\n    X = L.Dense(50, activation='relu')(X)\n    X = L.Dense(25, activation='relu')(X)\n    X = L.Dense(11, activation='softmax')(X)\n    \n    model = Model(inputs=X_input, outputs=X)\n    \n    return model\n\n\nmlp = create_mpl(x_train[0].shape)\nmlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\nprint(mlp.summary())","c03542a1":"def get_class_weight(classes):\n    '''\n    Weight of the class is inversely proportional to the population of the class\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/hist\n    \n    return class_weight\n\nclass_weight = get_class_weight(y_train)","be88f9e4":"# fit the model\nmlp.fit(x=x_train, y=y_train, epochs=30, batch_size=1024, class_weight=class_weight)","939bf452":"# plot history\nplt.figure(1)\nplt.plot(mlp.history.history['loss'], 'b', label='loss')\nplt.xlabel('epochs')\nplt.legend()\nplt.figure(2)\nplt.plot(mlp.history.history['sparse_categorical_accuracy'], 'g', label='sparse_categorical_accuracy')\nplt.xlabel('epochs')\nplt.legend()","b23c5fea":"# predict on test\ny_pred = mlp.predict(x_test)\ny_pred = np.argmax(y_pred, axis=-1)","a5651976":"# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(16,16)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)\n\n# f1 score\nf1 = f1_score(y_test, y_pred, average='macro')\n\n# plot confusion matrix\nplot_cm(y_test, y_pred, 'MLP f1_score=' + str('%.4f' %f1))","38bb92b5":"# read test\ntest = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')\ntest.head()","32294719":"# create df_submit\ndf_submit = divide_and_add_features(test['signal'])\nprint('df_submit.shape=', df_submit.shape)\ndf_submit.head()","47bf1a10":"# predict open channels\ny_submit = mlp.predict(df_submit.values)\ny_submit = np.argmax(y_submit, axis=-1)","1eae9fed":"# create submission\nsubmission = pd.DataFrame()\nsubmission['time'] = test['time']\nsubmission['open_channels'] = y_submit\n\n# write file\nsubmission.to_csv('submission.csv', index=False, float_format='%.4f')","9dd7101a":"# Load data","1f98d21d":"# MLP model\nWe will build a simple multilayer perceptron.","f982278b":"We now have 105 columns: the original signal and other 104 features extracted from it.","3cbd6c67":"# Submit result\nNow we only have to submit the result","5917b542":"# Feature extraction\nLets add to each signal several other features like rolling stats, filters."}}