{"cell_type":{"5180796d":"code","7a688e08":"code","bb80265c":"code","12112545":"code","36ceb29d":"code","ec4208a0":"code","7439f429":"code","e7f83e4c":"code","81763de6":"code","4a8b5b2d":"code","75b384f9":"code","e74bfedd":"code","25e26bb5":"code","af84bb58":"code","e14fb769":"code","50585a98":"code","85568d93":"code","f98ab678":"code","be2dde1c":"code","c9500582":"code","579d4ac3":"code","9fd2b5aa":"code","b3b542f6":"code","9c03ffe4":"code","60966eb1":"code","fb22a82a":"code","a85ddd94":"code","d93b3c7d":"code","44433d2b":"code","79bba131":"code","1eb7a9c3":"code","62bc35c8":"code","417f0fb9":"code","977a106f":"code","bb74f703":"markdown","01e40972":"markdown","d46ad739":"markdown","4edd1da4":"markdown","42c08a78":"markdown","72a4da51":"markdown","d6a5fafd":"markdown","3127449e":"markdown","3af6e490":"markdown","7bd74c4b":"markdown","b4476cd5":"markdown","1b7594db":"markdown","c23c42f7":"markdown","382b50ef":"markdown","16302ef2":"markdown"},"source":{"5180796d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a688e08":"import sklearn\nimport matplotlib\nimport keras\nimport sys\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns","bb80265c":"# read the csv\ndf = pd.read_csv('\/kaggle\/input\/diabetes-pimaindiansdiabetesdatabase\/diabetes.csv')","12112545":"df.head()","36ceb29d":"df.info()","ec4208a0":"# print the shape of the DataFrame, so we can see how many examples we have\nprint( 'Shape of DataFrame: {}'.format(df.shape))\nprint (df.loc[1])","7439f429":"# print the last twenty or so data points\ndf.loc[748:]","e7f83e4c":"# remove missing data (indicated with a \"?\")\ndata = df[~df.isin(['?'])]\ndata.loc[1:]","81763de6":"# drop rows with NaN values from DataFrame\ndata = data.dropna(axis=0)\ndata.loc[1:]","4a8b5b2d":"# print the shape and data type of the dataframe\nprint(data.shape)\nprint(data.dtypes)","75b384f9":"# transform data to numeric to enable further analysis\ndata = data.apply(pd.to_numeric)\ndata.dtypes","e74bfedd":"# print data characteristics, usings pandas built-in describe() function\ndata.describe()","25e26bb5":"# plot histograms for each variable\ndata.hist(figsize = (12, 12))\nplt.show()","af84bb58":"pd.crosstab(data.Age,data.Outcome).plot(kind=\"bar\",figsize=(20,8))\nplt.title('Diabetes Frequency for Ages')\nplt.xlabel('Age',color='red',size=25)\nplt.ylabel('Frequency',color='red',size=25)\nplt.show()","e14fb769":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True,fmt='.1f')\nplt.show()","50585a98":"age_unique=sorted(data.Age.unique())\nage_DiabetesPedigreeFunction_values=data.groupby('Age')['DiabetesPedigreeFunction'].count().values\nmean_DiabetesPedigreeFunction=[]\nfor i,Age in enumerate(age_unique):\n    mean_DiabetesPedigreeFunction.append(sum(data[data['Age']==Age].DiabetesPedigreeFunction)\/age_DiabetesPedigreeFunction_values[i])\n    \nplt.figure(figsize=(25,10))\nsns.pointplot(x=age_unique,y=mean_DiabetesPedigreeFunction,color='red',alpha=0.9)\nplt.xlabel('Age',fontsize = 25,color='black')\n\nplt.ylabel('DiabetesPedigreeFunction',fontsize = 20,color='black')\nplt.title('Age vs DiabetesPedigreeFunction',fontsize = 35,color='black')\nplt.grid()\nplt.show()","85568d93":"X = np.array(data.drop(['Outcome'],1))\ny = np.array(data['Outcome'])","f98ab678":"X[0]","be2dde1c":"mean = X.mean(axis=0)\nX -= mean\nstd = X.std(axis=0)\nX \/= std","c9500582":"X[0]","579d4ac3":"# create X and Y datasets for training\nfrom sklearn import model_selection\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.75)","9fd2b5aa":"# convert the data to categorical labels\nfrom keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(y_train, num_classes=None)\nY_test = to_categorical(y_test, num_classes=None)\nprint (Y_train.shape)\nprint (Y_train[:768])","b3b542f6":"X_train[0]\n","9c03ffe4":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\n# define a function to build the keras model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(64, input_dim=8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(64, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation='softmax'))\n    \n    # compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nmodel = create_model()\n\nprint(model.summary())","60966eb1":"# fit the model to the training data\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs= 50,batch_size=10)","fb22a82a":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","a85ddd94":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","d93b3c7d":"# convert into binary classification problem - outcome\nY_train_binary = y_train.copy()\nY_test_binary = y_test.copy()\n\nY_train_binary[Y_train_binary > 0] = 1\nY_test_binary[Y_test_binary > 0] = 1\n\nprint(Y_train_binary[:768])","44433d2b":"# define a new keras model for binary classification\ndef create_binary_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(80, input_dim=8, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(80, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nbinary_model = create_binary_model()\n\nprint(binary_model.summary())","79bba131":"# fit the binary model on the training data\nhistory=binary_model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10)","1eb7a9c3":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","62bc35c8":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","417f0fb9":"# generate classification report using predictions for categorical model\nfrom sklearn.metrics import classification_report, accuracy_score\n\ncategorical_pred = np.argmax(model.predict(X_test), axis=1)\n\nprint('Results for Categorical Model')\nprint(accuracy_score(y_test, categorical_pred))\nprint(classification_report(y_test, categorical_pred))","977a106f":"# generate classification report using predictions for binary model\nfrom sklearn.metrics import classification_report, accuracy_score\n# generate classification report using predictions for binary model \nbinary_pred = np.round(binary_model.predict(X_test)).astype(int)\n\nprint('Results for Binary Model')\nprint(accuracy_score(Y_test_binary, binary_pred))\nprint(classification_report(Y_test_binary, binary_pred))","bb74f703":"**Model accuracy**","01e40972":"**Now that we have preprocessed the data appropriately, we can split it into training and testings datasets. We will use Sklearn's train_test_split() function to generate a training dataset (80 percent of the total data) and testing dataset (20 percent of the total data).**\n\n","d46ad739":"# 1.Importing the Dataset","4edd1da4":"# 3.Building and Training the Neural Network","42c08a78":"# 2.Create Training and Testing Datasets","72a4da51":"# 4.Improving Results - A Binary Classification Problem","d6a5fafd":"** Data characteristics, usings pandas built-in describe() function**","3127449e":"**Heatmap for each variable**","3af6e490":"**Model accuracy**","7bd74c4b":"**Age vs DiabetesPedigreeFunction**","b4476cd5":"**Transform data to numeric to enable further analysis**","1b7594db":"** Histograms for each variable**","c23c42f7":"# 5.Results and Metrics","382b50ef":"**Now that we have our data fully processed and split into training and testing datasets, we can begin building a neural network to solve this classification problem. Using keras, we will define a simple neural network with one hidden layer. Since this is a categorical classification problem, we will use a softmax activation function in the final layer of our network and a categorical_crossentropy loss during our training phase.**","16302ef2":"**Model Losss**"}}