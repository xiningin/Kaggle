{"cell_type":{"3e1d1c61":"code","19ea7c7f":"code","9adfbc40":"code","da50ab46":"code","322e1f48":"code","8b8411cc":"code","17a99444":"code","61828f24":"code","2a8fda80":"code","61069d0c":"code","71777758":"code","76652801":"code","4c72ffec":"code","2a0d0d2b":"code","afc8eb9e":"code","b3260e21":"code","0b9903f9":"code","55be9886":"code","bcc28186":"code","db671c08":"code","0325ed34":"code","7fe51ba1":"code","d6264533":"code","858d085d":"code","21f2e913":"code","1ed7d842":"code","8dc3095e":"code","a3bbe76c":"code","f2d57844":"code","ebaca7c7":"code","55921652":"code","39c8b2bc":"code","610eb8d2":"code","747de54f":"code","c6559e36":"code","6835563d":"code","fd16fb4d":"code","db16901d":"code","5f0f8f31":"code","30c77457":"code","e2f61f98":"code","43e8e70b":"code","fb2165c6":"code","a09f5e12":"code","241c7bb5":"code","3b1d0e8b":"code","6d5b9f86":"code","10c316aa":"code","6c6e91d5":"code","3ba8d78b":"code","985308c1":"code","5f9f668a":"code","74be1b65":"code","e2703be1":"code","e34e073f":"code","6657dbcf":"code","54859dd1":"code","99792037":"code","8133f62e":"code","a67ef4aa":"code","89a682d0":"code","3caa2127":"code","ee3a900e":"code","49b6ca18":"code","d5bd1be1":"code","7918f439":"code","bb9ec5fe":"code","f4b60f4f":"code","332b0236":"code","d4b7a831":"code","478fa0cb":"code","968c2592":"code","c572d04a":"code","7ec2e598":"code","08897f91":"code","3004bb9a":"code","15122960":"code","df3948b1":"code","ad2ced30":"code","b488be48":"code","5b9b516c":"code","3b01ae9d":"code","2436d530":"code","4ebffe4b":"code","a3a66759":"code","edfabecc":"code","196788f6":"code","c160ed93":"code","995b4705":"code","35638f92":"code","ca091d3a":"code","41bfc59c":"code","2be4ca91":"code","730bfbd8":"code","f7ed0223":"code","5735986a":"markdown","b3d6419d":"markdown","962c9b0c":"markdown","8f19c029":"markdown","1b76ac99":"markdown","65879adb":"markdown","c3deab46":"markdown","e3c146ee":"markdown","2b42ca6b":"markdown","4645b2ec":"markdown","947f26b7":"markdown","59c0b0ce":"markdown","ea0f020b":"markdown"},"source":{"3e1d1c61":"import pandas as pd","19ea7c7f":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","9adfbc40":"df.head()","da50ab46":"df.isnull().sum() \n#there are no missing values.","322e1f48":"df.dtypes\n#there is only numerical data, no categorical features.","8b8411cc":"import seaborn as sns","17a99444":"df.columns","61828f24":"df.shape","2a8fda80":"df.describe()","61069d0c":"# sns.pairplot(df,hue='Outcome')","71777758":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score,auc,confusion_matrix,classification_report","76652801":"X = df.drop('Outcome',axis=1)","4c72ffec":"y = df['Outcome']","2a0d0d2b":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10)","afc8eb9e":"lr = LogisticRegression(max_iter=1000).fit(X_train,y_train)","b3260e21":"y_pred = lr.predict(X_test)","0b9903f9":"lr.score(X_train,y_train)","55be9886":"accuracy_score(y_pred,y_test)","bcc28186":"print(classification_report(y_pred,y_test))","db671c08":"sns.heatmap(confusion_matrix(y_pred,y_test),annot=True)","0325ed34":"df['Outcome'].value_counts()","7fe51ba1":"#since there are uneven number of positives and negatives, we will need to do either undersampling or oversampling.\n#In this case there is a less number of records so we should do oversampling.","d6264533":"# Random oversampling just increases the size of the training data set through repetition of the original examples. \n# It does not cause any increase in the variety of training examples.\n\n# Oversampling using SMOTE not only increases the size of the training data set, it also increases the variety.","858d085d":"from imblearn.over_sampling import SMOTE","21f2e913":"X_resampled,y_resampled = SMOTE(k_neighbors=5).fit_resample(X,y)","1ed7d842":"y_resampled.value_counts()","8dc3095e":"X_train2,X_test2,y_train2,y_test2 = train_test_split(X_resampled,y_resampled,test_size=0.3,random_state=10)","a3bbe76c":"lr = LogisticRegression(max_iter=1000).fit(X_train2,y_train2)","f2d57844":"y_pred2 = lr.predict(X_test2)","ebaca7c7":"print(classification_report(y_pred2,y_test2))","55921652":"sns.heatmap(confusion_matrix(y_pred2,y_test2),annot=True)","39c8b2bc":"#here we can see that the recall values are better. \n#We need to focus on the TPR as the problem is related to diabetes and the False Negatives should be decreased.","610eb8d2":"df.head()","747de54f":"sns.countplot(x='Pregnancies',data=df,hue='Outcome')","c6559e36":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure","6835563d":"plt.hist(df['Glucose'])\nplt.xlabel('Glucose')\nplt.ylabel('Count')","fd16fb4d":"sns.heatmap(df.corr())","db16901d":"fig,axes = plt.subplots(9,1,figsize=(5,40))\ni=0\nfor x in df.columns:\n    sns.boxplot(x=x,data=df,ax=axes[i])\n    i+=1","5f0f8f31":"import numpy as np","30c77457":"df.shape","e2f61f98":"df2 = df[df['Pregnancies']<=12]","43e8e70b":"df2 = df2[df2['Glucose']>25]","fb2165c6":"df2 = df2[df2['BloodPressure']>=40]","a09f5e12":"df2 = df2[df2['BloodPressure']<=110]","241c7bb5":"df2 = df2[df2['SkinThickness']<80]","3b1d0e8b":"df2 = df2[df2['Insulin']<=420]","6d5b9f86":"df2 = df2[(df2['BMI']<=50) & (df2['BMI']>=10)]","10c316aa":"df2 = df2[df2['DiabetesPedigreeFunction']<=1.5]","6c6e91d5":"df2 = df2[df2['Age']<70]","3ba8d78b":"#By this, most of the outliers have been removed.","985308c1":"df2.shape","5f9f668a":"df2['Outcome'].value_counts()","74be1b65":"# Getting the logistic regression results again:","e2703be1":"X2 = df2.drop('Outcome',axis=1)","e34e073f":"y2 = df2['Outcome']","6657dbcf":"X_train3,X_test3,y_train3,y_test3 = train_test_split(X2,y2,test_size=0.3,random_state=10)","54859dd1":"lr2 = LogisticRegression(max_iter=1000).fit(X_train3,y_train3)","99792037":"y_pred3 = lr2.predict(X_test3)","8133f62e":"print(classification_report(y_pred3,y_test3))","a67ef4aa":"sns.heatmap(confusion_matrix(y_pred3,y_test3),annot=True)","89a682d0":"df2.corr()","3caa2127":"#There is strong positive correlation between Pregnancies and Age.\n#Logistic regression is Prone to Correlation between features.\n#So lets try different models.","ee3a900e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","49b6ca18":"model = RandomForestClassifier().fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))","d5bd1be1":"#a serious drawback of SVM is that it is sensitive to outliers. So we train it on df2 data\nmodel = SVC().fit(X_train3,y_train3)\ny_pred = model.predict(X_test3)\nprint(classification_report(y_pred,y_test3))","7918f439":"model = DecisionTreeClassifier(criterion='gini').fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))","bb9ec5fe":"model = KNeighborsClassifier().fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))\n#worked better on resampled data and with outliers.","f4b60f4f":"model = GaussianNB().fit(X_train3,y_train3)\ny_pred = model.predict(X_test3)\nprint(classification_report(y_pred,y_test3))\n#Sensitive to outliers, so training on df2","332b0236":"model = xgboost.XGBClassifier().fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))\n#worked best on oversampled data with outliers.","d4b7a831":"model = AdaBoostClassifier().fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))\n#Worked best on oversampled data with outliers.","478fa0cb":"model = GradientBoostingClassifier().fit(X_train2,y_train2)\ny_pred = model.predict(X_test2)\nprint(classification_report(y_pred,y_test2))\n#Not much effect of oversampling and removing outliers.","968c2592":"# So till now after oversampling and removing outliers:\n#(The accuracies keep varying a bit)    ","c572d04a":"print(\"{:>30}{:>20}\".format(\"Model\",\"Best Accuracy\"))\nprint(\"{:>30}{:>20}\".format(\"Logistic Regression\",\"79%\"))\nprint(\"{:>30}{:>20}\".format(\"Random Forest Classifier\",\"82%\"))\nprint(\"{:>30}{:>20}\".format(\"Support Vector Classifier\",\"77%\"))\nprint(\"{:>30}{:>20}\".format(\"Decision Tree Classifier\",\"78%\"))\nprint(\"{:>30}{:>20}\".format(\"K Neighbours Classifier\",\"73%\"))\nprint(\"{:>30}{:>20}\".format(\"GaussianNB\",\"77%\"))\nprint(\"{:>30}{:>20}\".format(\"XGBoost Classifier\",\"78%\"))\nprint(\"{:>30}{:>20}\".format(\"AdaBoost Classifier\",\"76%\"))\nprint(\"{:>30}{:>20}\".format(\"Gradient Boosting Classifier\",\"79%\"))\n#These values may change with every run.","7ec2e598":"#Now we shall do hyperparameter tuning for all models that performed well.","08897f91":"from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold","3004bb9a":"model = LogisticRegression(max_iter=1000)\nsolvers = ['newton-cg','lbfgs','liblinear']\npenalty = ['l2']\nc_values = [100,10,1,0.1,0.01]\n\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)\ngrid_search = GridSearchCV(estimator=model,param_grid=grid,n_jobs=-1,cv=cv,scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_resampled,y_resampled)","15122960":"grid_result.best_score_,grid_result.best_params_","df3948b1":"model = RandomForestClassifier()\nn_estimators = [100,300,500]\nmax_depth = [5,8,15,25]\nmin_samples_split = [2,5,10,15]\nmin_samples_leaf = [1,2,5,10]\n\ngrid = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngrid_search = GridSearchCV(model,grid,verbose=0,cv=2,n_jobs=-1)\ngrid_result = grid_search.fit(X_resampled,y_resampled)\ngrid_result.best_score_,grid_result.best_params_","ad2ced30":"#This one took more time.","b488be48":"model = SVC()\ngrid = {'C':[0.1,1,10,100,1000],\n       'gamma':[1,0.1,0.01,0.001,0.0001],\n       'kernel':['rbf']}\ngrid_search = GridSearchCV(model,grid,verbose=0)\ngrid_result = grid_search.fit(X2,y2)\n#used the data which does not contain outliers.\ngrid_result.best_score_,grid_result.best_params_","5b9b516c":"model = GaussianNB()\ngrid = {\n    'var_smoothing': np.logspace(0,-9, num=100)\n}\ngrid_search = GridSearchCV(model,grid,verbose=0,cv=10,n_jobs=-1)\ngrid_result = grid_search.fit(X_resampled,y_resampled)\ngrid_result.best_score_,grid_result.best_params_","3b01ae9d":"#This cell is taking too much time to run on kaggle, when run on local it had around 80% accuracy\n# model = xgboost.XGBClassifier()\n# grid = {\n#     'max_depth':[3,5,7,9],\n#     'min_child_weight':[1,2,4,6,8],\n#     'gamma':[0,0.1,0.2,0.3],\n#     'reg_alpha':[0,0.001,0.01],\n#     'learning_rate':[0.1,0.001],\n#     'eval_metric':['logloss']\n# }\n# grid_search = GridSearchCV(model,grid,verbose=0,cv=2,n_jobs=-1)\n# grid_result = grid_search.fit(X_resampled,y_resampled)\n# grid_result.best_score_,grid_result.best_params_\n","2436d530":"model = AdaBoostClassifier()\ngrid = {\n    'n_estimators':[10,50,100,500],\n    'learning_rate':[0.01,0.1]\n}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\ngrid_search = GridSearchCV(model,grid,verbose=0,cv=cv,n_jobs=-1)\ngrid_result = grid_search.fit(X_resampled,y_resampled)\ngrid_result.best_score_,grid_result.best_params_","4ebffe4b":"model = GradientBoostingClassifier()\ngrid = {\n    'n_estimators':[10,20,50,80,100],\n    'learning_rate':[0.01,0.1],\n    'max_depth':[3,5,7,10],\n    \n}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\ngrid_search = GridSearchCV(model,grid,verbose=0,cv=cv,n_jobs=-1)\ngrid_result = grid_search.fit(X_resampled,y_resampled)\ngrid_result.best_score_,grid_result.best_params_","a3a66759":"print(\"{:>30}{:>20}\".format(\"Model\",\"Best Accuracy\"))\nprint(\"{:>30}{:>20}\".format(\"Logistic Regression\",\"75%\"))\nprint(\"{:>30}{:>20}\".format(\"Random Forest Classifier\",\"81.7%\"))\nprint(\"{:>30}{:>20}\".format(\"Support Vector Classifier\",\"76.5%\"))\nprint(\"{:>30}{:>20}\".format(\"GaussianNB\",\"74%\"))\nprint(\"{:>30}{:>20}\".format(\"XGBoost Classifier\",\"80%\"))\nprint(\"{:>30}{:>20}\".format(\"AdaBoost Classifier\",\"79%\"))\nprint(\"{:>30}{:>20}\".format(\"Gradient Boosting Classifier\",\"80.9%\"))\n#These values may change with every run.","edfabecc":"import evalml\nevalml.problem_types.ProblemTypes.all_problem_types","196788f6":"from evalml.automl import AutoMLSearch\nautoml = AutoMLSearch(X_train=X_train3, y_train=y_train3, problem_type='binary')\nautoml.search()","c160ed93":"automl.rankings","995b4705":"automl.best_pipeline","35638f92":"best_pipeline=automl.best_pipeline","ca091d3a":"automl.describe_pipeline(automl.rankings.iloc[0][\"id\"])","41bfc59c":"best_pipeline.score(X_test, y_test, objectives=[\"auc\",\"f1\",\"Precision\",\"Recall\"])","2be4ca91":"#We have made better models than this using Hyperparameter tuning.","730bfbd8":"# So our best model is the Random Forest Classifier with an accuracy of around 81%,\n# with the following parameters:\n#   'max_depth': 15,\n#   'min_samples_leaf': 1,\n#   'min_samples_split': 2,\n#   'n_estimators': 100\n\n# We could use standard scaler to further transform the data,\n# But Tree-Based Models are not affected by it.","f7ed0223":"#Any feedback will be appreciated :)","5735986a":"### 7. Gradient Boosting Classifier","b3d6419d":"# Verifying with AutoML (Running on local)","962c9b0c":"#### Best Accuracies in Hyperparameter Tuning:","8f19c029":"### 2. Random Forest Classifier","1b76ac99":"### 5. XGBoost Classifier","65879adb":"# Hyperparameter Tuning","c3deab46":"### 6. AdaBoost Classifier","e3c146ee":"### So we have solved the problem of imbalanced classification.\n### Now we will see outlier treatment.","2b42ca6b":"## EDA + Feature extraction + Feature scaling:","4645b2ec":"### 1. Logistic Regression","947f26b7":"### 3. SVC","59c0b0ce":"## Making a simple base model first:","ea0f020b":"### 4. Gaussian NB"}}