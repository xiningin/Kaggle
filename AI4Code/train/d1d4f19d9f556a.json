{"cell_type":{"1040373a":"code","e4354e23":"code","3dd08a55":"code","04e91dc4":"code","569a16a5":"code","8c26bba0":"code","d7233faf":"code","5bd5665d":"code","4079af33":"code","f72f3cbc":"code","128cd19c":"code","65b54136":"code","b3446839":"code","08b0abdf":"code","b23d2dd2":"code","45d5afce":"code","48e207e1":"code","ddeab4e0":"code","981c2e82":"code","c383520e":"code","c3e58528":"code","90236016":"code","4667b72c":"code","80e3ae16":"code","af636e5b":"code","182b1f28":"code","6775b140":"code","5f357622":"code","1ef9af6d":"code","288467c7":"code","1cd1618f":"markdown"},"source":{"1040373a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nfrom nltk import word_tokenize\n\nfrom tqdm import tqdm","e4354e23":"data = pd.read_csv(\"\/kaggle\/input\/name-entity-recognition-ner-dataset\/NER dataset.csv\", encoding='latin1')\ndata = data.fillna(method=\"ffill\") # Deal with N\/A\"","3dd08a55":"data.head()","04e91dc4":"tags = list(set(data.POS.values)) #Read POS values\ntags #List of possible POS values","569a16a5":"#Creating the dictionary and displaying the 1st 20 words\nwords = list(set(data.Word.values))\nnp.array(words[:20]).flatten() ","8c26bba0":"#adding the word dummy to dictionary so that we can pad sentences.\nwords.append('dummy')","d7233faf":"#Creating a class to read senteces\n\nclass read_sentences():\n    \n    def __init__(self,data):\n        self.data = data\n        agg_func = lambda s: [(w,p,t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","5bd5665d":"sentences = read_sentences(data).sentences","4079af33":"# Convert words and tags into numbers\nword2id = {w: i for i, w in enumerate(words)}\ntag2id = {t: i for i, t in enumerate(tags)}","f72f3cbc":"word2id","128cd19c":"tag2id","65b54136":"# Prepare input and output data\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.preprocessing.sequence import pad_sequences\nmax_len = 50\nX = [[word2id[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=len(words)-1)\ny = [[tag2id[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2id[\".\"])","b3446839":"len(tags)","08b0abdf":"# Convert output to one-hot bit\n\nfrom keras.utils import to_categorical\ny = [to_categorical(i, num_classes=len(tags)) for i in y]","b23d2dd2":"# Training and test split by sentences\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","45d5afce":"from keras.models import Model, Input, Sequential\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional","48e207e1":"X.shape[1]","ddeab4e0":"model = Sequential([Embedding(input_dim=len(words), output_dim=50, input_length=X.shape[1]),\n                    Dropout(0.1),\n                    Bidirectional(LSTM(X.shape[1], return_sequences = True, recurrent_dropout=0.2)),\n                    TimeDistributed(Dense(len(tags), activation=\"softmax\"))])\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # Compile with an optimizer","981c2e82":"# Train\nhistory = model.fit(X_train, np.array(y_train), batch_size=32, epochs=3, validation_split=0.1, verbose=1) ","c383520e":"#Model Accuracy\naccuracy = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_accuracy = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]\n\nplt.plot(accuracy,'g',label='training accuracy')\nplt.plot(val_accuracy, 'r', label='validation accuracy')\nplt.legend()\nplt.show()\n\n\nplt.plot(loss,'g',label='training loss')\nplt.plot(val_loss, 'r', label='validation loss')\nplt.legend()\nplt.show()","c3e58528":"# Demo test on one sample. \n\ni = 1213 # Some test sentence sample\np = model.predict(np.array([X_test[i]])) # Predict on it\np = np.argmax(p, axis=-1) # Map softmax back to a POS index\nfor i, (w, pred) in enumerate(zip(X_test[i], p[0])): # for every word in the sentence\n    print(\"{:20} -- {}\".format(words[w], tags[pred])) # Print word and tag\n    if pred == 23 and p[0][i+1] == 23:\n        break","90236016":"sentence = 'That was a nice jump'\n\nsentence = word_tokenize(sentence)\nX_Samp = pad_sequences(maxlen=max_len, sequences=[[word2id[word] for word in sentence]], padding=\"post\", value=len(words)-1)\n\np = model.predict(np.array([X_Samp[0]])) # Predict on it\np = np.argmax(p, axis=-1) # Map softmax back to a POS index\nfor i, (w, pred) in enumerate(zip(X_Samp[0], p[0])): # for every word in the sentence\n    print(\"{:20} -- {}\".format(words[w], tags[pred])) # Print word and tag\n    if pred == 23 and p[0][i+1] == 23:\n        break","4667b72c":"sentence = 'Do you want to jump'\n\nsentence = word_tokenize(sentence)\nX_Samp = pad_sequences(maxlen=max_len, sequences=[[word2id[word] for word in sentence]], padding=\"post\", value=len(words)-1)\n\np = model.predict(np.array([X_Samp[0]])) # Predict on it\np = np.argmax(p, axis=-1) # Map softmax back to a POS index\nfor i, (w, pred) in enumerate(zip(X_Samp[0], p[0])): # for every word in the sentence\n    print(\"{:20} -- {}\".format(words[w], tags[pred])) # Print word and tag\n    if pred == 23 and p[0][i+1] == 23:\n        break","80e3ae16":"data.head()","af636e5b":"embeddings_index = {}\nglovefile = open('..\/input\/glove6b50dtxt\/glove.6B.50d.txt','r',encoding='utf-8')\nfor line in tqdm(glovefile):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n#     coefs.shape\n    embeddings_index[word] = coefs\nglovefile.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","182b1f28":"#creating embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word2id), 50))\nfor word, index in tqdm(word2id.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","6775b140":"model = Sequential([Embedding(input_dim=len(words), output_dim=50,weights = [embedding_matrix], input_length=X.shape[1]),\n                    Dropout(0.1),\n                    Bidirectional(LSTM(X.shape[1], return_sequences = True, recurrent_dropout=0.2)),\n                    TimeDistributed(Dense(len(tags), activation=\"softmax\"))])\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # Compile with an optimizer","5f357622":"model.summary()","1ef9af6d":"# Train\nhistory = model.fit(X_train, np.array(y_train), batch_size=32, epochs=3, validation_split=0.1, verbose=1) ","288467c7":"#Model Accuracy\naccuracy = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_accuracy = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]\n\nplt.plot(accuracy,'g',label='training accuracy')\nplt.plot(val_accuracy, 'r', label='validation accuracy')\nplt.legend()\nplt.show()\n\n\nplt.plot(loss,'g',label='training loss')\nplt.plot(val_loss, 'r', label='validation loss')\nplt.legend()\nplt.show()","1cd1618f":"### Using glove"}}