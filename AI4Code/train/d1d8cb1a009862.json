{"cell_type":{"871052bf":"code","9f2d7904":"code","fda807fc":"code","a05031e9":"code","eae77c69":"code","59ebc58f":"code","8a9de536":"code","b55d9e67":"code","1de18f56":"code","4cfb0dc1":"code","f1867435":"code","d6a72266":"code","933c463e":"code","2e9cd83a":"code","d0390c3c":"code","1f8c58aa":"code","3bb43cba":"code","0bb75fc0":"code","36e450ff":"code","2b8da861":"code","91cf83af":"code","27257bbc":"code","7d33ef1a":"code","c6a66fd8":"code","94ccb40b":"code","6b796914":"code","79c6b2fb":"code","b3c272cc":"code","4f48d039":"code","43e0ca60":"code","31bec738":"code","dbe40172":"code","d840964a":"code","b640846d":"code","01ac2366":"code","6812dd10":"code","987b2678":"code","d0b36ca3":"code","532337b7":"markdown","0a485161":"markdown","481be44c":"markdown","830f009c":"markdown","dec9ae76":"markdown","0fb49508":"markdown","3c22ab1c":"markdown","563f82c2":"markdown","f7a767d4":"markdown","7a9b0143":"markdown","05a095b3":"markdown","036cd528":"markdown","552f55c7":"markdown","76b65695":"markdown","c838dc5e":"markdown","36878e99":"markdown","dff9fd7f":"markdown","f9eb39c2":"markdown","9b62a58b":"markdown","0e393ce0":"markdown"},"source":{"871052bf":"#importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","9f2d7904":"cars = pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ncars.head()","fda807fc":"cars.shape","a05031e9":"cars.info()","eae77c69":"cars.describe()","59ebc58f":"cars.isnull().sum()","8a9de536":"#Splitting company name from CarName column\nCompanyName = cars['CarName'].apply(lambda x : x.split(' ')[0])\ncars.insert(3,\"CompanyName\",CompanyName)\ncars.drop(['CarName'],axis=1,inplace=True)\ncars.head()","b55d9e67":"cars.CompanyName.unique()","1de18f56":"cars.CompanyName = cars.CompanyName.str.lower()\n\ndef replace_name(a,b):\n    cars.CompanyName.replace(a,b,inplace=True)\n\nreplace_name('maxda','mazda')\nreplace_name('porcshce','porsche')\nreplace_name('toyouta','toyota')\nreplace_name('vokswagen','volkswagen')\nreplace_name('vw','volkswagen')\n\ncars.CompanyName.unique()","4cfb0dc1":"cars.columns","f1867435":"#Fuel economy\ncars['fueleconomy'] = (0.55 * cars['citympg']) + (0.45 * cars['highwaympg'])","d6a72266":"#Binning the Car Companies based on avg prices of each Company.\ncars['price'] = cars['price'].astype('int')\ntemp = cars.copy()\ntable = temp.groupby(['CompanyName'])['price'].mean()\ntemp = temp.merge(table.reset_index(), how='left',on='CompanyName')\nbins = [0,10000,20000,40000]\ncars_bin=['Budget','Medium','Highend']\ncars['carsrange'] = pd.cut(temp['price_y'],bins,right=False,labels=cars_bin)\ncars.head()","933c463e":"cars_lr = cars[['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase',\n                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower', \n                    'fueleconomy', 'carlength','carwidth', 'carsrange']]\ncars_lr.head()","2e9cd83a":"sns.pairplot(cars_lr)\nplt.show()","d0390c3c":"# Defining the map function\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], drop_first = True)\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n# Applying the function to the cars_lr\n\ncars_lr = dummies('fueltype',cars_lr)\ncars_lr = dummies('aspiration',cars_lr)\ncars_lr = dummies('carbody',cars_lr)\ncars_lr = dummies('drivewheel',cars_lr)\ncars_lr = dummies('enginetype',cars_lr)\ncars_lr = dummies('cylindernumber',cars_lr)\ncars_lr = dummies('carsrange',cars_lr)","1f8c58aa":"cars_lr.head()","3bb43cba":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(cars_lr, train_size = 0.7, test_size = 0.3, random_state = 100)","0bb75fc0":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnum_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","36e450ff":"df_train.head()","2b8da861":"#Correlation using heatmap\nplt.figure(figsize = (30, 25))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","91cf83af":"#Dividing data into X and y variables\ny_train = df_train.pop('price')\nX_train = df_train","27257bbc":"#RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7d33ef1a":"lm = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm, 10)\nrfe = rfe.fit(X_train, y_train)","c6a66fd8":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","94ccb40b":"X_train.columns[rfe.support_]","6b796914":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()","79c6b2fb":"def build_model(X,y):\n    X = sm.add_constant(X) #Adding the constant\n    lm = sm.OLS(y,X).fit() # fitting the model\n    print(lm.summary()) # model summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)","b3c272cc":"X_train_new = build_model(X_train_rfe,y_train)","4f48d039":"lm = sm.OLS(y_train,X_train_new).fit()\ny_train_price = lm.predict(X_train_new)","43e0ca60":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\n# plt.show()","31bec738":"#Scaling the test set\nnum_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']\ndf_test[num_vars] = scaler.fit_transform(df_test[num_vars])","dbe40172":"#Dividing into X and y\ny_test = df_test.pop('price')\nX_test = df_test","d840964a":"# Now let's use our model to make predictions.\nX_train_new = X_train_new.drop('const',axis=1)\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","b640846d":"# Making predictions\ny_pred = lm.predict(X_test_new)","01ac2366":"y_pred.head(10)","6812dd10":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","987b2678":"#EVALUATION OF THE MODEL\n# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16) ","d0b36ca3":"print(lm.summary())","532337b7":"## 8,9. Prediction and Evaluation","0a485161":"1.Load Data\n\n2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n\n3.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n\n4.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu\/tanh (check by experiment).\n\n5.Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)\n\n6.Train the Model with Epochs (100) and validate it\n\n7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .\n\n8.Evaluation Step\n\n9.Prediction","481be44c":"### Model 1","830f009c":">*Inference :*\n\n* R-sqaured and Adjusted R-squared (extent of fit) - 0.929 and 0.923 - 95% variance explained.\n* F-stats and Prob(F-stats) (overall model fit) - 172.0 and 1.29e-70(approx. 0.0) - Model fir is significant and explained 95% variance is just not by chance.\n* p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the predictors are statistically significant.","dec9ae76":">Highly correlated variables to price are - `curbweight`, `enginesize`, `horsepower`,`carwidth` and `highend`.","0fb49508":"## 1.Load Data","3c22ab1c":">**Dummy Variables**","563f82c2":">*Fixing invalid values*\n\nThere seems to be some spelling error in the CompanyName column.\n\n`maxda` = `mazda`\n\n`Nissan` = `nissan`\n\n`porsche` = `porcshce`\n\n`toyota` = `toyouta`\n\n`vokswagen` = `volkswagen` = `vw`","f7a767d4":"A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\n>**Which variables are significant in predicting the price of a car**\n\n>**How well those variables describe the price of a car**\n\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n\n# task::\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.","7a9b0143":"## 4. Model Building","05a095b3":"# WORKFLOW ::","036cd528":"### Evaluation of the model using Statistics","552f55c7":"# Problem Statement::","76b65695":"# Car Price Prediction::","c838dc5e":"## 2.Check Missing Values","36878e99":">Error terms seem to be approximately normally distributed, so the assumption on the linear modeling seems to be fulfilled.","dff9fd7f":">**Deriving some new features**","f9eb39c2":"Evaluation of test via comparison of y_pred and y_test","9b62a58b":"### Residual Analysis of Model","0e393ce0":"## 3.Train-Test Split and feature scaling"}}