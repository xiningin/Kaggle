{"cell_type":{"cf0f1f29":"code","e928dc25":"code","ae89503a":"code","99949171":"code","4a443fac":"code","f34820f2":"code","53a88812":"code","7d5f9475":"code","3ca4ec41":"code","a124d460":"code","52424723":"code","a07f74bc":"code","d969a8c2":"code","a851534e":"code","a4f382f6":"code","1d5d6f87":"code","08204055":"code","f36a69a4":"code","d56fefdd":"code","e8829c2c":"code","86403e03":"code","16888b69":"code","4eb05efb":"code","fd2f360e":"code","c152903a":"code","4c4864fd":"code","1be568e8":"code","e98f1ef0":"code","7e6a1807":"code","315f0155":"code","cdf60140":"code","c661db18":"code","c3e57c85":"code","07a2a22d":"code","01247eb0":"code","7a184573":"markdown","027236d4":"markdown","7b782d49":"markdown","eecec8c2":"markdown"},"source":{"cf0f1f29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e928dc25":"df = pd.read_csv('..\/input\/online-news-popularity-dataset\/OnlineNewsPopularityReduced.csv')\ndf1 = df.drop(['url'], axis=1)\ndf1.head()","ae89503a":"df1['shares'].hist(bins=15); #\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0430\u043b\u0435\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u043c \u043b\u043e\u0433\u0430\u0440\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439","99949171":"df1['shares'] = np.log(df1['shares'])#\u041b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u0443\u0435\u043c\n\ndf1['shares'].hist(bins=15);","4a443fac":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(df1)","f34820f2":"from sklearn.model_selection import train_test_split\nX = df1.drop('shares', axis=1)\ny = df1['shares']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=727)","53a88812":"from sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(max_depth=3, random_state=727)\ntree.fit(X_train, y_train)","7d5f9475":"from sklearn.tree import export_graphviz\n\nexport_graphviz(tree, out_file='tree.dot', feature_names=X.columns)\nprint(open('tree.dot').read()) ","3ca4ec41":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\ny_pred = tree.predict(X_valid)\nmean_squared_error(y_valid, y_pred)","a124d460":"r2_score(y_valid, y_pred)\n","52424723":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 5, shuffle = True, random_state = 30)","a07f74bc":"from sklearn.model_selection import GridSearchCV\ntree_params_depth = {'max_depth': np.arange(2, 25)}\n\ntree_grid_depth = GridSearchCV(tree, tree_params_depth, cv=5, scoring='r2')\ntree_grid_depth.fit(X_train, y_train)\nprint(tree_grid_depth.best_params_)\nprint(tree_grid_depth.best_score_)","d969a8c2":"tree = DecisionTreeRegressor(max_depth = 3)\ntree_params_split = {'min_samples_split': np.arange(2, 25)}\n\ntree_grid_split = GridSearchCV(tree, tree_params_split , cv=5, scoring='r2')\ntree_grid_split.fit(X_train, y_train)\nprint(tree_grid_split.best_params_)\nprint(tree_grid_split.best_score_)","a851534e":"tree = DecisionTreeRegressor(max_depth = 3, min_samples_split = 22)\ntree_params_leaf = {'min_samples_leaf': np.arange(2, 25)}\n\ntree_grid_leaf = GridSearchCV(tree, tree_params_leaf, cv=5, scoring='r2')\ntree_grid_leaf.fit(X_train, y_train)\nprint(tree_grid_leaf.best_params_)\nprint(tree_grid_leaf.best_score_)","a4f382f6":"tree = DecisionTreeRegressor(max_depth = 3, min_samples_split = 23, min_samples_leaf = 20)\ntree_params_features = {'max_features': np.arange(2, 25)}\n\ntree_grid_features = GridSearchCV(tree, tree_params_features, cv=5, scoring='r2')\ntree_grid_features.fit(X_train, y_train)\nprint(tree_grid_features.best_params_)\nprint(tree_grid_features.best_score_)","1d5d6f87":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(2, 2, figsize = (10,10))\n\nax[0,0].set_xlabel(\"Max depth\")\nax[0,0].set_ylabel(\"Score\")\nax[0,0].plot(tree_params_depth['max_depth'], tree_grid_depth.cv_results_[\"mean_test_score\"]);\n\nax[0,1].set_xlabel(\"Min samples split\")\nax[0,1].set_ylabel(\"Score\")\nax[0,1].plot(tree_params_split[\"min_samples_split\"], tree_grid_split.cv_results_[\"mean_test_score\"]);\n\nax[1,0].set_xlabel(\"Min samples leaf\")\nax[1,0].set_ylabel(\"Score\")\nax[1,0].plot(tree_params_leaf[\"min_samples_leaf\"],tree_grid_leaf.cv_results_[\"mean_test_score\"]);\n\nax[1,1].set_xlabel(\"Max features\")\nax[1,1].set_ylabel(\"Score\")\nax[1,1].plot(tree_params_features[\"max_features\"], tree_grid_features.cv_results_[\"mean_test_score\"]);","08204055":"# \u0412\u044b\u0431\u043e\u0440 \u0438 \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430\n\ntree = DecisionTreeRegressor(max_depth = 3, min_samples_split = 21, min_samples_leaf = 20, max_features =  24, random_state = 727)\ntree.fit(X_train, y_train)\nr2_score(y_valid, y_pred)\n","f36a69a4":"export_graphviz(tree, out_file = 'tree.dot', feature_names = X.columns)\nprint(open('tree.dot').read())","d56fefdd":"features = {'f' + str(i + 1):name for (i, name) in zip(range(len(df1.columns)), df1.columns)}\nimportances = tree.feature_importances_\n\nindices = np.argsort(importances)[:: -1]\nnum_to_plot = 10\nfeature_indices = [ind + 1 for ind in indices[:num_to_plot]]\n\nprint(\"Feature ranking:\")\nfor f in range(num_to_plot):\n    print(f + 1, features[\"f\" + str(feature_indices[f])], importances[indices[f]])","e8829c2c":"plt.figure(figsize=(15,5))\nplt.title(\"Feature importances\")\nbars = plt.bar(range(num_to_plot), \n               importances[indices[:num_to_plot]],\n               color=([str(i \/ float(num_to_plot +  1)) for i in range(num_to_plot)]),\n               align=\"center\")\nticks = plt.xticks(range(num_to_plot), \n                   feature_indices)\nplt.xlim([-1, num_to_plot])\nplt.legend(bars, [u''.join(features[\"f\" + str(i)]) for i in feature_indices]);","86403e03":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=727)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_valid)\nmean_squared_error(y_valid, y_pred)","16888b69":"r2_score(y_valid, y_pred)","4eb05efb":"rf_params_estimators = {'n_estimators': [150, 200, 250]}\nrf_grid_estimators = GridSearchCV(rf, rf_params_estimators, cv=kf, scoring='r2', n_jobs=-1)\nrf_grid_estimators.fit(X_train, y_train)","fd2f360e":"print(rf_grid_estimators.best_params_)\nprint(rf_grid_estimators.best_score_)","c152903a":"rf = RandomForestRegressor(random_state=727, n_estimators=250)\nrf_params_depth = {'max_depth': np.arange(2, 11)}\nrf_grid_depth = GridSearchCV(rf, rf_params_depth, cv=kf, scoring='r2', n_jobs=-1)\nrf_grid_depth.fit(X_train, y_train)","4c4864fd":"print(rf_grid_depth.best_params_)\nprint(rf_grid_depth.best_score_)","1be568e8":"rf = RandomForestRegressor(random_state=727, n_estimators=250, max_depth=8)\nrf_params_split = {'min_samples_split': np.arange(10, 21)}\nrf_grid_split = GridSearchCV(rf, rf_params_split, cv=kf, scoring='r2', n_jobs=-1)\nrf_grid_split.fit(X_train, y_train)","e98f1ef0":"print(rf_grid_split.best_params_)\nprint(rf_grid_split.best_score_)","7e6a1807":"rf = RandomForestRegressor(random_state=727, n_estimators=250, max_depth=8, min_samples_split=20)\nrf_params_leaf = {'min_samples_leaf': np.arange(18, 31)}\nrf_grid_leaf = GridSearchCV(rf, rf_params_leaf, cv=kf, scoring='r2', n_jobs=-1)\nrf_grid_leaf.fit(X_train, y_train)","315f0155":"print(rf_grid_leaf.best_params_)\nprint(rf_grid_leaf.best_score_)","cdf60140":"rf = RandomForestRegressor(random_state=727, n_estimators=250, max_depth=8, min_samples_split=20, min_samples_leaf=18)\nrf_params_features = {'max_features': np.arange(15, 25)}\nrf_grid_features = GridSearchCV(rf, rf_params_features, cv=kf, scoring='r2', n_jobs=-1)\nrf_grid_features.fit(X_train, y_train)","c661db18":"print(rf_grid_features.best_params_)\nprint(rf_grid_features.best_score_)","c3e57c85":"fig, ax = plt.subplots(2, 3, figsize=(20 , 20))\nax[0, 0].set_xlabel(\"Max depth\")\nax[0, 0].set_ylabel(\"Score\")\nax[0, 0].plot(rf_params_depth['max_depth'], rf_grid_depth.cv_results_[\"mean_test_score\"])\nax[0, 1].set_xlabel(\"Min samples split\")\nax[0, 1].set_ylabel(\"Score\")\nax[0, 1].plot(rf_params_split[\"min_samples_split\"], rf_grid_split.cv_results_[\"mean_test_score\"])\nax[0, 2].set_xlabel(\"Min samples leaf\")\nax[0, 2].set_ylabel(\"Score\")\nax[0, 2].plot(rf_params_leaf[\"min_samples_leaf\"],rf_grid_leaf.cv_results_[\"mean_test_score\"])\nax[1, 0].set_xlabel(\"Max features\")\nax[1, 0].set_ylabel(\"Score\")\nax[1, 0].plot(rf_params_features[\"max_features\"], rf_grid_features.cv_results_[\"mean_test_score\"])\nax[1, 1].set_xlabel(\"N Estimators\")\nax[1, 1].set_ylabel(\"Score\")\nax[1, 1].plot(rf_params_estimators[\"n_estimators\"], rf_grid_estimators.cv_results_[\"mean_test_score\"])","07a2a22d":"features = {'f' + str(i + 1): name for (i, name) in zip(range(len(df1.columns)), df1.columns)}\nimportances = tree.feature_importances_\nindices = np.argsort(importances)[:: -1]\nnum_to_plot = 10\nfeature_indices = [ind + 1 for ind in indices[:num_to_plot]]\nprint(\"Feature ranking:\")\nfor f in range(num_to_plot):\n    print(f + 1, features[\"f\" + str(feature_indices[f])], importances[indices[f]])","01247eb0":"plt.figure(figsize=(15 , 5))\nplt.title(\"Feature importances\")\nbars = plt.bar(range(num_to_plot),\n               importances[indices[:num_to_plot]],\n               color=([str(i \/ float(num_to_plot + 1)) for i in range(num_to_plot)]),\n               align=\"center\")\nticks = plt.xticks(range(num_to_plot), feature_indices)\nplt.xlim([-1, num_to_plot])\nplt.legend(bars, [u''.join(features[\"f\" + str(i)]) for i in feature_indices])","7a184573":"1.  \u0414\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0439: 0.17681345291716566\n2. \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441: 0.21209222028275726\n","027236d4":"![](https:\/\/i.imgur.com\/7azWp1z.png)","7b782d49":"**\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430**","eecec8c2":"![](https:\/\/i.imgur.com\/QKsGFGF.png)"}}