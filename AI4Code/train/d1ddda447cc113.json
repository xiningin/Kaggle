{"cell_type":{"15cc044e":"code","126c00de":"code","e01a6bdb":"code","d6e9f257":"code","1c9b2523":"code","8c708b51":"code","41243879":"code","f4268be2":"code","6c6a74a0":"code","dcbf2828":"code","e44a4f84":"code","cf98eaff":"code","38a72445":"code","8ccf79c9":"code","11dc585d":"code","b1315048":"code","eb3d9026":"code","a7753dda":"code","92941fdd":"code","963e6ede":"code","ad4fa8a4":"code","7595cc27":"code","eb2e45c2":"code","3d910b36":"code","22f62567":"code","32d6d7d8":"code","a791dcef":"code","62f6bc0f":"code","c098f5e1":"code","16ab3db9":"code","b264df18":"code","edf1ac4a":"code","16e835f9":"code","430bb5c7":"markdown","f18145f8":"markdown","6b1350dc":"markdown","e6fef066":"markdown","437f9818":"markdown","b66d0176":"markdown","250a0abe":"markdown"},"source":{"15cc044e":"from __future__ import print_function\n\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nfrom keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\nfrom keras.models import Model, load_model\n\nINPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","126c00de":"# Load the data\nlines = open('..\/input\/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\nconv_lines = open('..\/input\/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')","e01a6bdb":"# Create a dictionary to map each line's id with its text\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","d6e9f257":"# Create a list of all of the conversations' lines' ids.\nconvs = []\nfor line in conv_lines[:-1]:\n    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n    convs.append(_line.split(','))","1c9b2523":"#id and conversation sample\nfor k in convs[300]:\n    print (k, id2line[k])","8c708b51":"# Sort the sentences into questions (inputs) and answers (targets)\nquestions = []\nanswers = []\nfor conv in convs:\n    for i in range(len(conv)-1):\n        questions.append(id2line[conv[i]])\n        answers.append(id2line[conv[i+1]])\n        \n# Compare lengths of questions and answers\nprint(len(questions))\nprint(len(answers))","41243879":"def clean_text(text):\n    '''Clean text by removing unnecessary characters and altering the format of words.'''\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|]\", \"\", text)\n#     text = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = \" \".join(text.split())\n    return text","f4268be2":"# Clean the data\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\nclean_answers = []    \nfor answer in answers:\n    clean_answers.append(clean_text(answer))","6c6a74a0":"# Find the length of sentences (not using nltk due to processing speed)\nlengths = []\n# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\nfor question in clean_questions:\n    lengths.append(len(question.split()))\nfor answer in clean_answers:\n    lengths.append(len(answer.split()))\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])\nprint(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))","dcbf2828":"# Remove questions and answers that are shorter than 1 word and longer than 20 words.\nmin_line_length = 2\nmax_line_length = 20\n\n# Filter out the questions that are too short\/long\nshort_questions_temp = []\nshort_answers_temp = []\n\nfor i, question in enumerate(clean_questions):\n    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n        short_questions_temp.append(question)\n        short_answers_temp.append(clean_answers[i])\n\n# Filter out the answers that are too short\/long\nshort_questions = []\nshort_answers = []\n\nfor i, answer in enumerate(short_answers_temp):\n    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n        short_answers.append(answer)\n        short_questions.append(short_questions_temp[i])\n        \nprint(len(short_questions))\nprint(len(short_answers))","e44a4f84":"r = np.random.randint(1,len(short_questions))\n\nfor i in range(r, r+3):\n    print(short_questions[i])\n    print(short_answers[i])\n    print()","cf98eaff":"#choosing number of samples\nnum_samples = 30000  # Number of samples to train on.\nshort_questions = short_questions[:num_samples]\nshort_answers = short_answers[:num_samples]\n#tokenizing the qns and answers\nshort_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\nshort_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]","38a72445":"#train-validation split\ndata_size = len(short_questions_tok)\n\n# We will use the first 0-80th %-tile (80%) of data for the training\ntraining_input  = short_questions_tok[:round(data_size*(80\/100))]\ntraining_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\ntraining_output = short_answers_tok[:round(data_size*(80\/100))]\n\n# We will use the remaining for validation\nvalidation_input = short_questions_tok[round(data_size*(80\/100)):]\nvalidation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\nvalidation_output = short_answers_tok[round(data_size*(80\/100)):]\n\nprint('training size', len(training_input))\nprint('validation size', len(validation_input))","8ccf79c9":"# Create a dictionary for the frequency of the vocabulary\n# Create \nvocab = {}\nfor question in short_questions_tok:\n    for word in question:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n\nfor answer in short_answers_tok:\n    for word in answer:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1            ","11dc585d":"# Remove rare words from the vocabulary.\n# We will aim to replace fewer than 5% of words with <UNK>\n# You will see this ratio soon.\nthreshold = 15\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1","b1315048":"print(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","eb3d9026":"#we will create dictionaries to provide a unique integer for each word.\nWORD_CODE_START = 1\nWORD_CODE_PADDING = 0\n\n\nword_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\nencoding = {}\ndecoding = {1: 'START'}\nfor word, count in vocab.items():\n    if count >= threshold: #get vocabularies that appear above threshold count\n        encoding[word] = word_num \n        decoding[word_num ] = word\n        word_num += 1\n\nprint(\"No. of vocab used:\", word_num)","a7753dda":"#include unknown token for words not in dictionary\ndecoding[len(encoding)+2] = '<UNK>'\nencoding['<UNK>'] = len(encoding)+2","92941fdd":"dict_size = word_num+1\ndict_size","963e6ede":"def transform(encoding, data, vector_size=20):\n    \"\"\"\n    :param encoding: encoding dict built by build_word_encoding()\n    :param data: list of strings\n    :param vector_size: size of each encoded vector\n    \"\"\"\n    transformed_data = np.zeros(shape=(len(data), vector_size))\n    for i in range(len(data)):\n        for j in range(min(len(data[i]), vector_size)):\n            try:\n                transformed_data[i][j] = encoding[data[i][j]]\n            except:\n                transformed_data[i][j] = encoding['<UNK>']\n    return transformed_data","ad4fa8a4":"#encoding training set\nencoded_training_input = transform(\n    encoding, training_input, vector_size=INPUT_LENGTH)\nencoded_training_output = transform(\n    encoding, training_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_training_input', encoded_training_input.shape)\nprint('encoded_training_output', encoded_training_output.shape)","7595cc27":"#encoding validation set\nencoded_validation_input = transform(\n    encoding, validation_input, vector_size=INPUT_LENGTH)\nencoded_validation_output = transform(\n    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_validation_input', encoded_validation_input.shape)\nprint('encoded_validation_output', encoded_validation_output.shape)","eb2e45c2":"import tensorflow as tf\ntf.keras.backend.clear_session()","3d910b36":"INPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nencoder_input = Input(shape=(INPUT_LENGTH,))\ndecoder_input = Input(shape=(OUTPUT_LENGTH,))","22f62567":"from keras.layers import SimpleRNN\n\nencoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\nencoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\nencoder_last = encoder[:,-1,:]\n\nprint('encoder', encoder)\nprint('encoder_last', encoder_last)\n\ndecoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\ndecoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n\nprint('decoder', decoder)\n\n# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)","32d6d7d8":"from keras.layers import Activation, dot, concatenate\n\n# Equation (7) with 'dot' score from Section 3.1 in the paper.\n# Note that we reuse Softmax-activation layer instead of writing tensor calculation\nattention = dot([decoder, encoder], axes=[2, 2])\nattention = Activation('softmax', name='attention')(attention)\nprint('attention', attention)\n\ncontext = dot([attention, encoder], axes=[2,1])\nprint('context', context)\n\ndecoder_combined_context = concatenate([context, decoder])\nprint('decoder_combined_context', decoder_combined_context)\n\n# Has another weight + tanh layer as described in equation (5) of the paper\noutput = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\noutput = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\nprint('output', output)","a791dcef":"model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.summary()","62f6bc0f":"training_encoder_input = encoded_training_input\ntraining_decoder_input = np.zeros_like(encoded_training_output)\ntraining_decoder_input[:, 1:] = encoded_training_output[:,:-1]\ntraining_decoder_input[:, 0] = WORD_CODE_START\ntraining_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n\nvalidation_encoder_input = encoded_validation_input\nvalidation_decoder_input = np.zeros_like(encoded_validation_output)\nvalidation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\nvalidation_decoder_input[:, 0] = WORD_CODE_START\nvalidation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]","c098f5e1":"model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n          #validation_split=0.05,\n          batch_size=64, epochs=150)\n\nfrom keras.models import load_model\n\nmodel.save('model_attention.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model","16ab3db9":"from keras.models import load_model\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('model_attention.h5')","b264df18":"def prediction(raw_input):\n    clean_input = clean_text(raw_input)\n    input_tok = [nltk.word_tokenize(clean_input)]\n    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n    encoder_input = transform(encoding, input_tok, 20)\n    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n    decoder_input[:,0] = WORD_CODE_START\n    for i in range(1, OUTPUT_LENGTH):\n        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n        decoder_input[:,i] = output[:,i]\n    return output\n\ndef decode(decoding, vector):\n    \"\"\"\n    :param decoding: decoding dict built by word encoding\n    :param vector: an encoded vector\n    \"\"\"\n    text = ''\n    for i in vector:\n        if i == 0:\n            break\n        text += ' '\n        text += decoding[i]\n    return text","edf1ac4a":"for i in range(20):\n    seq_index = np.random.randint(1, len(short_questions))\n    output = prediction(short_questions[seq_index])\n    print ('Q:', short_questions[seq_index])\n    print ('A:', decode(decoding, output[0]))","16e835f9":"raw_input = input()\noutput = prediction(raw_input)\nprint (decode(decoding, output[0]))","430bb5c7":"### 2.2  Attention Mechanism\nReference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https:\/\/arxiv.org\/pdf\/1508.04025.pdf","f18145f8":"### 1.1  Preprocessing for word based model","6b1350dc":"## 2  Model Building\n### 2.1  Sequence-to-Sequence in Keras","e6fef066":"## 3. Model testing","437f9818":"### 1.3  Vectorizing dataset","b66d0176":"### Resources:\nhttps:\/\/wanasit.github.io\/attention-based-sequence-to-sequence-in-keras.html","250a0abe":"### 1.2  Word en\/decoding dictionaries"}}