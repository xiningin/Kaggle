{"cell_type":{"a56befbc":"code","d5aa3ce2":"code","4d52bc0f":"code","f1069987":"code","e56e234d":"code","68167da6":"code","d4ae9ff6":"code","def8d320":"code","f7ef5836":"code","9877f3e8":"code","a6be7aac":"code","891bfbdc":"code","dd54fbfc":"code","509e1e06":"code","c6837a66":"code","853815b9":"code","d13bba64":"code","3aa9a874":"code","a29054c7":"code","4874edf5":"code","87205630":"code","162378e1":"code","df0617f6":"markdown","55edb138":"markdown","46dc460e":"markdown","7cb53f04":"markdown","7a4c89e6":"markdown","c1c610e0":"markdown","544fc023":"markdown","dd60f52a":"markdown","1825ae6d":"markdown","7d1788fb":"markdown","33f3e7f1":"markdown","ccbc5367":"markdown","e3cd216b":"markdown","ae1f0857":"markdown","9776aa6e":"markdown","3fe9d12e":"markdown","bbd7e99a":"markdown","347bd6e5":"markdown","5a40fdda":"markdown","18d1c3fb":"markdown","63d9ad88":"markdown","3177041b":"markdown","99a9c3fe":"markdown","71982482":"markdown"},"source":{"a56befbc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","d5aa3ce2":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","4d52bc0f":"train_df.head(5)","f1069987":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]","e56e234d":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","68167da6":"len(train_df)","d4ae9ff6":"train_df[\"text\"].str.len().hist()","def8d320":"train_df[\"text\"].str.split(\".\").apply(lambda x : [len(s.split()) for s in x]).map(lambda x: np.mean(x)).hist()","f7ef5836":"train_df[\"text\"].str.split().\\\n    map(lambda x: len(x)).\\\n    hist()","9877f3e8":"train_df[\"text\"].str.split().\\\n   apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist()","a6be7aac":"import nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))","891bfbdc":"corpus = []\nnew = train_df[\"text\"].str.split()\nnew = new.values.tolist()\ncorpus = [word for i in new for word in i]\n\nfrom collections import defaultdict\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:12] \nx,y=zip(*top)\nplt.bar(x,y)","dd54fbfc":"import seaborn as sns\nfrom collections import  Counter\n\ncounter = Counter(corpus)\nmost=counter.most_common()\n\nx, y= [], []\nfor word,count in most[:60]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","509e1e06":"from nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]\n\ntop_n_bigrams=get_top_ngram(train_df[\"text\"], 2)[:10]\nx,y=map(list, zip(*top_n_bigrams))\nsns.barplot(x=y,y=x) ","c6837a66":"top_tri_grams=get_top_ngram(train_df[\"text\"],n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)","853815b9":"train_df[\"target\"].hist()","d13bba64":"import nltk\nimport gensim\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport pyLDAvis.gensim","3aa9a874":"additional_stop_words = {\"http\", \"https\", \"-\", \"amp\", \"&amp;\", \"??\", \"???\", \"????\", \"\\x89\u00db_\", \"\\x89\u00db\u00d3\",\"co\", \"|\", \"...\"}\n\nstop = stop.union(additional_stop_words)","a29054c7":"def preprocess(df):\n    corpus = []\n    stem = PorterStemmer()\n    lem = WordNetLemmatizer()\n    for text in df['text']:\n        words = [w for w in word_tokenize(text) if (w not in stop)]\n        \n        words=[lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus\n\ncorpus=preprocess(train_df)","4874edf5":"dic = gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]","87205630":"lda_model = gensim.models.LdaMulticore(bow_corpus, \n                                       num_topics = 5, \n                                       id2word = dic,                                    \n                                       passes = 10,\n                                       workers = 2)\nlda_model.show_topics()","162378e1":"from wordcloud import WordCloud\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stop,\n        max_words=128,\n        max_font_size=30,\n        scale=4,\n        random_state=1)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus)","df0617f6":"Here I will be adding a few additional words to the list of stop words in order to better clean up the data.","55edb138":"Here we we see something that looks much closer to a normal distribution, with most tweets containing between 10 and 22 words. Additionally there are few tweets with over 25 words.\n\nNext, let's check the avarage word length in each tweet:","46dc460e":"Again, more weird link stuff potentially related to sharing videos, however the non-garbage results actually look quite promissing. At least 3 of the trigrams on the top ten appear to be natural disaster related.\n\nAnother item to check before moving on are the targets. How many of our 7,613 tweets are actually about natural disasters?","7cb53f04":"Unfortunately, the model seems to have some trouble with topic analysis. I would guess this is due to the source of the datasets being tweets, which can discuss a wide range of topics.\n\nNext we will create a wordcloud which can help give us a general idea of our vocabulary:","7a4c89e6":"Here we can see that the dataset does indeed match what was described on competition page. One thing to note here is that the keyword and location columns do not always contain a value.\n\nNow let's take a look at a tweet that is not about a natural disaster:","c1c610e0":"We can see most sentences avarage around 5 words however there appears to be quite a few tweets with longer sentences or that avoided periods altogether.\n\nNow lets check the number of words in each tweet:","544fc023":"Here we can see a wide range of words, with none particulary sticking out. Overall, it seems like \"The\" is the most frequent word, with not to great a difference between word frequencey. Additionaly, you can see a string 'x89U_' which most likely represents an emoji, which may present a challenge to the model.\n\nFrom our analysis we have clearly defined this challenge as a classification problem, Natural Disaster tweet or not a Natural Disaster tweet","dd60f52a":"Here we can see the avarage word length of the tweets falls between 4 and 7 characters. However this could include a lot of stopwords or other short words that provide little to no information. So let us take a try and filter some of that out. Here we will use an [nltk](https:\/\/www.nltk.org\/) library of stopwords to acomplish the task.","1825ae6d":"### Text Statistics Visualizations and Analysis\n\nNow lets take a closer look at the overall dataset. First, Let's check the lengths of the tweets and see how many characters they contain. For the following histograms, the Y axis shows the number of tweets in each category, while the x-axis is the metric we are examing.","7d1788fb":"After creating a Bag of words model from out corpus we will use it to create n LDA in order to do some topic analysis.","33f3e7f1":"Overall the training dataset has 5 columns and 7613 unique entries corresponding to tweets.","ccbc5367":"And one that is:","e3cd216b":"### Data Pre-Processing\n\nAs our earlier exporation showed there is a significant ammount of extraneous data that we should try and remove before proceeding with tokenizing the words in the tweets so they can be processed by our model.","ae1f0857":"Build a new corpus without any stopwords:","9776aa6e":"As we can see from this image, there are a significant number of stopwords contained in the tweets, with the words \"the\", \"a\", and \"to\" being the most frequent.\n\nNow lets take a look at the most common words in our tweets that are not stop words.","3fe9d12e":"Here, most of the bigrams are quite short and provide very little information, or seem to be some part of a link that might have been partially removed.\n\nNow let's look at trigrams:","bbd7e99a":"First, let's check to see which stop words occur most frequently in our dataset. This will also help give us an idea of the amount data that needs to be trimmed.","347bd6e5":"In this histogram we can see there are around 4,400 tweets that are not talking about natural disasters (target = 0), and 3,200 that are related to natural disasters (target = 1).","5a40fdda":"### A quick look at the data\n\nGoing over the competition page's data section they describe the contents of the data as a csv with the following columns:\n\n    id - a unique identifier for each tweet\n    text - the text of the tweet\n    location - the location the tweet was sent from (may be blank)\n    keyword - a particular keyword from the tweet (may be blank)\n    target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\nIn order to confirm this we can take a quick glance at data we have imported.","18d1c3fb":"### Ngram exploration\n\nHere we will examine the most frequent n-grams in our dataset using another [nltk](https:\/\/www.nltk.org\/) library.","63d9ad88":"## NLP - Processing Disaster Tweets Data Analysis\nAuthor: Matthew Williams\n\nThis noteboook provides an analysis of the dataset provided by the Natural Language Processing with Disaster Tweets competion. This notebook started as a copy of a [Getting Started Tutorial](https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial) provided by [phil culliton](http:\/\/https:\/\/www.kaggle.com\/philculliton). In this analysis I will be following this [Guide to exploratory data analysis for NLP](https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools).","3177041b":"Here we can see that the majority of tweets contain at most 140 characters, with only a few having more characters. Additionally, a significant portion of the data resides in the range of 110-140 characters, about 3,400 tweets, which is close to half of the total number of tweets (7613).\n\nNext, we can check the avarage number of words per sentence in each tweet:","99a9c3fe":"Almost unsuprisingly, \"I\" is the most commonly found word in tweets outside of stopwords. Following \"I\", we have \"-\", \"The\", and \"like\" appearing the most fequent. There also appear to be some tokens that might be worth removign from the dataset, like '-', '??' and '&amp'.","71982482":"Finally, before we take a deeper look at the overall dataset, let us check the total number of datapoints the dataset contains:"}}