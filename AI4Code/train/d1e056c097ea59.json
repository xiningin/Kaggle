{"cell_type":{"e6e5a68c":"code","e982d5d0":"code","194f7315":"code","379f21a4":"code","662854fe":"code","f0f1e9d2":"code","82f460de":"code","fc161381":"code","208435ec":"code","643a2982":"code","0f7e920e":"code","69a6fc59":"code","e29e2172":"code","939c1a7d":"code","d9aba32b":"code","9b28f746":"code","280cafb3":"code","a431abea":"code","8d655201":"code","348d69e6":"code","270000d2":"code","6807aa97":"code","3b026832":"code","4b2bc25e":"code","742bcc1d":"code","cc29b6e3":"code","32955576":"code","15a3d930":"code","6a978bf8":"code","3ea0ca56":"code","e95929e4":"markdown","a187558b":"markdown","c248a8d2":"markdown","55e0f7e9":"markdown","02936c1d":"markdown","c40aba5d":"markdown","37aa0521":"markdown","201d01d6":"markdown","42d8d3b6":"markdown","6cbbd9bc":"markdown","bb337fde":"markdown","b9c99ee3":"markdown","b2d8ddee":"markdown","d5fdbab2":"markdown","21095e56":"markdown","4f96de99":"markdown","ec521065":"markdown","a69786ee":"markdown","d310d2cc":"markdown","5e071cac":"markdown","28f898f6":"markdown","e44dd3af":"markdown","460f208c":"markdown","64875b86":"markdown","85f2b902":"markdown"},"source":{"e6e5a68c":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","e982d5d0":"fish = pd.read_csv(\"..\/input\/fish-market\/Fish.csv\")","194f7315":"fish.head(5)","379f21a4":"fish.info()","662854fe":"fish.columns = ['Species', 'Weight', 'LengthV', 'LengthD', 'LengthC', 'Height','Width']","f0f1e9d2":"fish.describe().round(1)","82f460de":"fish.iloc[40,:]","fc161381":"fish = fish.loc[fish[\"Weight\"]>0,:]","208435ec":"sns.set()\nplt.figure(figsize=[12,6])\n\nsns.distplot(fish[\"Weight\"],kde_kws={\"bw\":40})\nplt.show()","643a2982":"sns.pairplot(fish,hue=\"Species\")\nplt.show()","0f7e920e":"sns.lmplot(x=\"Height\", y=\"Weight\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"Height\", y=\"Weight\", data=fish, scatter=False)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","69a6fc59":"sns.lmplot(x=\"Width\", y=\"Weight\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"Width\", y=\"Weight\", data=fish, scatter=False)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","e29e2172":"sns.lmplot(x=\"LengthV\", y=\"Weight\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"LengthV\", y=\"Weight\", data=fish, scatter=False)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","939c1a7d":"fish[\"Weightlog\"] = np.log(fish[\"Weight\"])\nfish[\"Widthlog\"] = np.log(fish[\"Width\"])\nfish[\"Heightlog\"] = np.log(fish[\"Height\"])\nfish[\"LengthVlog\"] = np.log(fish[\"LengthV\"])","d9aba32b":"sns.lmplot(x=\"Heightlog\", y=\"Weightlog\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"Heightlog\", y=\"Weightlog\", data=fish, scatter=False)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","9b28f746":"sns.lmplot(x=\"Widthlog\", y=\"Weightlog\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"Widthlog\", y=\"Weightlog\", data=fish, scatter=False)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","280cafb3":"sns.lmplot(x=\"LengthVlog\", y=\"Weightlog\", hue=\"Species\", data=fish, fit_reg=False)\nsns.regplot(x=\"LengthVlog\", y=\"Weightlog\", data=fish, scatter=False,ci=None)\nplt.gcf().set_size_inches(14, 8)\nplt.show()","a431abea":"fish_x = fish.loc[:,[\"LengthV\",\"LengthD\",\"LengthC\",\"Height\",\"Width\"]]\n\nfish_x.corr()","8d655201":"y = fish[\"Weight\"]\nx = fish.loc[:,[\"LengthV\",\"LengthD\",\"LengthC\",\"Height\",\"Width\"]]\n\nx = sm.add_constant(x)\nfish_reg = sm.OLS(y,x).fit()\nfish_reg.summary()  ","348d69e6":"y = fish[\"Weight\"]\nx = fish.loc[:,[\"LengthV\",\"Height\",\"Width\"]]\n\nx = sm.add_constant(x)\nfish_reg_red = sm.OLS(y,x).fit()\nfish_reg_red.summary()  ","270000d2":"print( \"P-Value of the F-Test:\\t\\t\\t\",round(fish_reg.compare_f_test(fish_reg_red)[1],4),\"\\n\")\n\n\nprint(\"AIC for the non-Restricted model:\\t\",round(fish_reg.aic,4))\nprint(\"AIC for the non-Restricted model:\\t\",round(fish_reg_red.aic,4),\"\\n\")\n\nprint(\"BIC for the non-Restricted model:\\t\",round(fish_reg.bic,4))\nprint(\"BIC for the non-Restricted model:\\t\",round(fish_reg_red.bic,4))","6807aa97":"plt.figure(figsize=(16,10))\nplt.hlines(0,xmin=0,xmax=1700,linestyle=\"dashed\",alpha=0.6)\nsns.scatterplot(y,fish_reg_red.resid,s=100)\n\nplt.show()","3b026832":"y_log = fish[\"Weightlog\"]\nx_log = fish.loc[:,[\"LengthVlog\",\"Heightlog\",\"Widthlog\"]]\n\nx_log = sm.add_constant(x_log)\nfish_reg_ll = sm.OLS(y_log,x_log).fit()\nfish_reg_ll.summary()  ","4b2bc25e":"plt.figure(figsize=(16,10))\nplt.hlines(0,xmin=1.5,xmax=8,linestyle=\"dashed\",alpha=0.6)\nsns.scatterplot(y_log,fish_reg_ll.resid,s=100)\n\nplt.show()","742bcc1d":"print(\"P-Value of Breusch & Pagan Test: \",round(het_breuschpagan(fish_reg_ll.resid,x_log)[1],4))","cc29b6e3":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=5)\n\nX_log_train, X_log_test, y_log_train, y_log_test = train_test_split(x_log, y_log, test_size=0.3,random_state=5)","32955576":"X_train = sm.add_constant(X_train)\nfish_reg_red = sm.OLS(y_train,X_train).fit()\n\nX_log_train = sm.add_constant(X_log_train)\nfish_reg_ll = sm.OLS(y_log_train,X_log_train).fit()","15a3d930":"pred_lin = fish_reg_red.predict(X_test)\nprint(\"R2 of the Non-Transformed Linear Model:\",round(r2_score(y_test,pred_lin),4))\n\npred_ll = np.exp(fish_reg_ll.predict(X_log_test))\nprint(\"R2 of the LogLog Linear Model:\",round(r2_score(y_test,pred_ll),4))","6a978bf8":"\nunit =np.r_[1:49]\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(unit,y_test,color=\"red\",s=250)\nsns.scatterplot(unit,pred_ll,color=\"blue\",s=150)\nsns.scatterplot(unit,pred_lin,color=\"green\",s=150)\nax.set_xticks(range(1,49))\nplt.gcf().set_size_inches(20, 10)\nplt.show()","3ea0ca56":"pd.DataFrame({\"Pred_Lin\":round(pred_lin[pred_lin<0],2),\"Pred_LogLog\":round(pred_ll[pred_lin<0],2),\"Real_Values\":y_test[pred_lin<0]})","e95929e4":"The dataset doesn't have any missing value, and there are no negative values. Our data is already fairly clean!\n\nThere's only one observation that we need to fix:\n","a187558b":"\nUsing only **LengthV**,**Height** and **Width**, all the p-values decreased, and the variance of the parameter associated with **LengthV** is now acceptable. \n\nObviusly the **R2** decreased, but that's not a problem. \n(*Don't use the R2 statistic to compare two model with different number of indipendent variable!*)\n\nTo understand which model performs better on our data, we use three useful tools:  **F-test**,**AIC** and **BIC**","c248a8d2":"**These relationships aren't linear**. So, fitting a straight line through the data doesn't give us the best results. A linear model doesn't capture the **non-linearity** of the relationships between our dependend variable and the other features that we are going to use in our regression models. \n\nWe have two ways to fix this issue:\n\n - Find a more complex model that can explain what a simple straight line cannot;\n - Transform our data to linearize the relationship between **Weight** and the other variables;\n \nThe second option is really simple and works well for our necessities.\nBy applying a logarithmic transformation to **Weight**,**LenghtV**,**Height** and **Width** the problem is solved!\n","55e0f7e9":"# 4. Analysis and Visulization\n\nUsing the *describe()* and *info()* functions we collected a lot of interesting informations. But that's not enough!.\nBefore diving in the modelling and prediction phase, we need to deeply understand our data, through some really simple visualization and data analysis techniques.\n\nLet's take a look at the distribution of our dependent variable, **Weight**, using an **distplot**","02936c1d":"This **multivariate plot** gives us two notable informations that we need to deepen before modelling:\n\n1. *The relationship between **Weight** and the other variables **isn't linear***\nThe following scatterplots analize more in-depth the distribution of **Weight** compared to **LengthV**, **Height** and **Width**.","c40aba5d":"The fish at row 40 weights **0 g**. So, to avoid errors when using the logarithmic transformation, it's better to drop this observation.","37aa0521":"The **p-value** of the variables **LengthV**,**LengthC** and **LengthD** is quite high, suggesting us that these variables are not significant. These p-values are caused by the high correlation between the three types of Length. We can solve **collinearity** in three ways:\n\n- Increasing the number of observation (**n**)\n- Introducing new variables\n- Removing the collinear variables\n\nThe third approch is the best in our case. We can't add new observations, and by removing redundant variables we don't lose too much informations. So, we can remove **LengthD** and **LengthC**, and just consider **LengthV**\n","201d01d6":"And fit our **two models** only using the observations that are in the **training set**.\n","42d8d3b6":"# 7. Conclusion\n\nIn this study we predicted the **Weight** of a fish using two multiple linear model. We can conlude that the **Log-Log linear model** is by far the best regression model to predict the **Weight**, but if you really want to keep it simple, using a **Non-transformed linear regression** is a good choice too.\n\nIf you have read this far, I sincerely want to thank you for your attention and patience. This is my first notebook here on Kaggle, so if you find any type of error, or you just want to some more info or even if you have a different type of solution that you want to discuss, I invite you to leave a comment or to send me an e-mail.\n\n### If this notebook helped you in any way or you liked it, please upvote and\/or leave a comment!","6cbbd9bc":"To rapidly visualize all of our continuous data, We can use the function *pairplot()*.","bb337fde":"\n# 1. Introduction\n\nIn this study we'll try to predict the **Weight** of a fish using the informations contained in the **Fish Market Dataset**, downloadable on [Kaggle](https:\/\/www.kaggle.com\/aungpyaeap\/fish-market).\nIn the first part, we're going to focus on understanding the data and all of his variables, using statistical analysis and visualization techniques. After that, we're going to use **Multiple Linear Regression** and a series of tests and plots to build different models, and decide which is the optimal choice to complete our task.\n\n## This is the \"Python Version\" of this Notebook, but if you're more familiar with R, you can check out the \"R Version\" [here](https:\/\/www.kaggle.com\/gaetanochiriaco\/weight-prediction-loglog-linear-regression) \n\n\n# 2. Libraries","b9c99ee3":"\nThese are the libraries we need:\n\n- **seaborn** and **matplotlib**: For various visualization functions. \n- **pandas**: For data cleaning and manipulation.\n- **numpy**: For some useful math functions\n- **statsmodel** and **sklearn**: For modelling,Tests,Predictions, and Train\/Test splitting. \n    We're going to use this libraries also for the function *het_breuschpagan()*, that allows us to perform a **Breusch & Pagan Test**. This test is used to check if the residuals of our linear       model are **Heteroskedastical** or **Homoskedastical**.\n    \n    If you don't know what i'm talking about, I raccomend reading more about heteroskedasticity and **B&P Test** at this [link](https:\/\/rstudio-pubs-static.s3.amazonaws.com\/187387_3ca34c107405427db0e0f01252b3fbdb.html).\n\n# 3. Importing Data and first Analysis\n\nLet's start by importing the dataset.","b2d8ddee":"The **R2** of the prediction of the **Log-Log multiple linear regression** is much higher then the one obtained with the **non-trasformed multiple regression model**. \n\nThe **Log-Log model** is better at predicting than the *fish_reg_red* model!\n\nLet's use a scatterplot to visualize the real values of **Weight**, and the predictions made by our two different model","d5fdbab2":"\nThe predictions made with the **Log-Log model** (blue dots) are clearly closer to the **real values** (red dots) then the predictions made with the **non-transformed linear model** (green dots).\n\nIt's also important to underline an useful propriety of the Log-Log model : **all the predictions made by this model are Positive**. \nWhy? Because we used the exponential function to bring the fitted values back to the same unit of measurement of the real values. In our case this constraint is a great advantage, because obviously the weight of a fish cannot be negative.\n\nOn the other side, a **non-transformed multiple linear model** can also assume negative fitted values, which is a big issue if we want only positive predictions.","21095e56":"Great! \nNow the residuals don't follow a scheme, and so we are sure that the **Log-Log** model captures the real relationship between **Weight** and **LengthV**,**Height** and **Width**.\n\nJust to be sure, we can use a **Breusch & Pagan Test** to check if the residuals are **heteroskedastical**\n","4f96de99":"\nA straight line now perfectly fits our data, and the differences between the various **Species** are less markable.\n\n\n2. The correlation between **LengthV**, **LengthD** and **LengthC** is almost equal to **1**\n\nWith the *cor()* function, we obtain a [variance-covariance matrix](https:\/\/datascienceplus.com\/understanding-the-covariance-matrix\/) for our **indipendent variables**.\n","ec521065":"\nThe **Null Hypothesis** is not rejected (it means that the residuals are **homoskedastical**), so we don't need to fix heteroskedasticity issues.\n\n\n# 6. Prediction\n\nLet's now use all the things that we have learned to predict the **Weight** of a fish. We're going to make predictions with two models, *fish_reg_red* and *fish_reg_ll*, and check which model does the better job.\n\nWith the following script, we split our data in **Train** set and **Test** set...","a69786ee":"Let's use the *describe()* method to see how our dataset is structured and check if there are any obvious **outliers** or **missing values**","d310d2cc":"\nUsing our two fitted multiple linear regression model, we can finally make predictions on the observations that are in the **test set**.\n\n","5e071cac":"This type of model is called **Log-Log linear regression** (learn more about it [here](https:\/\/www.dummies.com\/education\/economics\/econometrics\/econometrics-and-the-log-log-model\/))\n\nAll the variables are very statistically significant and the **R2** is really high!. But we can't use the **R2**, or tools like the **AIC** and the **BIC** to compare the **Log-Log model** with the *fish_red_reg* model because the dependent variable is not the same (**y** is not equal to **log(y)**)\n\nBut since our final objective is prediction, we can try to predict the Weight with both models and then check wich model does it better.\n\nLet's check if the logharitmic tranformation fixed our problems with the distribution of residuals:","28f898f6":"\nThat's not how residuals should look like! As you can see, the residuals follow a **visible pattern**. \nIf a model really explains the relationship between the dependent variable and the indipendent variables this should not happen!\n\nThe model *fish_reg_red* has an high **R2**,and all of the parameters are significant, but it certainly fails in capturing the real relationship between the **Y** and the **Xs**.\n\nLuckly, we already now that applying a loghritimic transformation to our data the relationship becomes linear.\n\nSo let's transform our data and fit a new model\n","e44dd3af":"As you can see, all the variables are well specified.\n**Species** is the only factor variable, while the other six are continuous.\nLater, **Weight** is going to be our dependent variable, and we are going to predict it's value using the other features as **indipendent variables**.\n\nFor a better understanding of the meaning of the variables, it's preferable to change their names.","460f208c":"The correlation values between **LengthV**, **LengthD** and **LengthC** are **0.9995**,**0.9920** and **0.9941**. \n\nHaving two or more variables which are almost a **linear combination** of each other can cause some serious issues to our regression model. This problem is called **Multicollinearity**(learn more about collinearity [here](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/)). The fact that these three variables are Multicollinear means that we have redundant informations.\n\nWe will discuss more deeply about this problem later.\n\nWe have plotted and analyzed our data enough. Now let's start modelling!\n\n\n# 5. Multiple Linear Regression\n\nThe first model that we're going to use is **Multiple Linear Regression**. Our dependent variable is **Weight**, and our indipendent variables are **LengthV**,**LengthD**,**LengthC**,**Height** and **Width**. I didn't use the **Species** coloumn to not over-complicate the model. **Species** is a factor variable with **7 different levels**. Using this variable means creating 7 dummy variables, and have 7 different straight lines,each one with a unique intecept. \nToo complicated! **KISS** . \n\nThe following script creates the multiple linear model that we need:","64875b86":"\nThe *compare_f_test()* method performs for us a F-test. The resultant **p-value** is higher than **0.10**, so we don't refuse the null Hypotesis. Also, the **AIC** and **BIC** have smaller values with the *fish_reg_red* model. In simple terms, using the two \"extra\" variables **LengthC** and **LengthD** didn't increase enough our knowledge about the dependent variable (**Weight**), and so it's better to use a simpler model (*fish_reg_red*). \n\nLet's take a look at the distribution of residuals:","85f2b902":"\nWe can take a first look at the our data using the *head()* and *info()* methods.\n"}}