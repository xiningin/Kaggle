{"cell_type":{"dfee0ed8":"code","c9d0f661":"code","5da0ea13":"code","51d4b437":"code","004fd9e9":"code","6d02ecb7":"code","f3fa26db":"code","350c8ae1":"code","4b04dda0":"code","292c54f5":"code","ed65fdbe":"code","ebe081e0":"code","e23e2964":"code","c80d29e5":"code","92fffb84":"code","9a770e37":"code","56babbaa":"code","07a3d414":"code","af55eb77":"code","11e68c2d":"code","246d9dc5":"code","c83a40cd":"code","402478f4":"code","da10ee9c":"code","d2fc1b8d":"code","17f6530b":"code","776beb2f":"code","0acc199f":"code","9c4a564e":"code","e80b7f1e":"code","bfa41c10":"code","a2f4a480":"code","bd55f2f5":"code","9677af6e":"code","129a9a1e":"code","1bfa2a63":"code","d89236c0":"code","d657fd35":"code","8a906b0f":"code","9288f8f9":"code","e415b789":"code","eddf96be":"code","1586650f":"code","65f39dd0":"code","6397cea9":"code","211f4d3c":"code","fe23d95f":"code","c0f64e67":"code","18c03b89":"code","779dfd5d":"code","9d9d8f54":"code","02b63503":"code","3c1617e8":"code","4cd5ddc6":"code","feffab2c":"code","3e818367":"code","b6f28d47":"code","b63e77f3":"code","0aa77748":"code","dd915ef2":"code","de67a0d0":"code","ad570a76":"code","2fbac6c9":"code","9fa12942":"code","15eeebee":"code","039bbdcb":"code","c7be3ce4":"markdown","338f0b11":"markdown","f978814c":"markdown","13a874ec":"markdown","732e1cfa":"markdown","9956b296":"markdown","95c43459":"markdown","0cffb381":"markdown","c2871c2c":"markdown","6197af43":"markdown","b8ea1b8f":"markdown","54511ece":"markdown","6dfe767e":"markdown","6e85b547":"markdown","cd91bad1":"markdown","4fe1c732":"markdown","1425db94":"markdown","10297fc2":"markdown","67a1f770":"markdown","1e222eab":"markdown","18293123":"markdown","8f083d23":"markdown","11343aac":"markdown","06d7c1ac":"markdown","a99d02df":"markdown","c244bb6e":"markdown","b34de447":"markdown","c8989833":"markdown","3be94c3f":"markdown","c13d6be8":"markdown","bfac86d9":"markdown","7da4280b":"markdown","54a65dda":"markdown","499f21f1":"markdown","655cf28c":"markdown","2ef3f4f2":"markdown","e3f498d6":"markdown","bfd4c3d3":"markdown","47617276":"markdown","2d786a90":"markdown","cea3789d":"markdown","8fd2a126":"markdown","e4400f94":"markdown","62c14050":"markdown","a95f7e5d":"markdown","db79520c":"markdown","7fdb5d1a":"markdown","f21cb974":"markdown","1cb5de3c":"markdown","d9365e91":"markdown","34cb5a13":"markdown","253c9ee2":"markdown","ddb874ca":"markdown","6fba6bea":"markdown","1d17beb4":"markdown","5c188edc":"markdown","1de4188c":"markdown","cf9de210":"markdown","2f877218":"markdown","f555261a":"markdown","c174b1ec":"markdown","c0f02f3a":"markdown","2dc7293a":"markdown","8151141e":"markdown","37b795fd":"markdown","e8213d00":"markdown","1dc272b2":"markdown","b93c1a43":"markdown","6b7b258b":"markdown","9b4904d9":"markdown","0b9ddb29":"markdown","775a6e46":"markdown","0d6b1b14":"markdown","026877dc":"markdown","1ef60c8b":"markdown","6f76029e":"markdown","63fc9e5b":"markdown","cf3938c5":"markdown","9fc3b87b":"markdown","4df0c2ec":"markdown","d567c88b":"markdown","380e28fa":"markdown","3591bcaf":"markdown","c1a2c951":"markdown","9cc95e25":"markdown","160b641b":"markdown","b49d1bcb":"markdown","7eeb7181":"markdown","d5e90be5":"markdown","521233b5":"markdown","c88f16d8":"markdown","c8c44f7e":"markdown","1519dc7d":"markdown","c3baf3f1":"markdown","e8b37f27":"markdown","766c659b":"markdown","79fc6660":"markdown","ccc81f0a":"markdown","0dfda8a0":"markdown","5c0615e9":"markdown","e095aac6":"markdown","8af4e313":"markdown","0ef6d3d8":"markdown","e648013e":"markdown","508357b4":"markdown","61cc30ea":"markdown","5cbf2206":"markdown","2512a34f":"markdown","f6a1a5bd":"markdown","e85c8a1e":"markdown","67a4bf29":"markdown"},"source":{"dfee0ed8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c9d0f661":"%matplotlib inline","5da0ea13":"from IPython.display import display","51d4b437":"import warnings\nwarnings.filterwarnings(\"ignore\")","004fd9e9":"from sklearn.datasets import load_iris\niris=load_iris()\n#printing feature names\nprint('features: %s'%iris['feature_names'])\n#printing species of iris\nprint('target categories: %s'%iris['target_names'])\n#iris data shape\nprint(\"Shape of data: {}\".format(iris['data'].shape))","6d02ecb7":"full_data=pd.DataFrame(iris['data'],columns=iris['feature_names'])\nfull_data['Classification']=iris['target']\nfull_data['Classification']=full_data['Classification'].apply(lambda x: iris['target_names'][x])","f3fa26db":"#myfile='MyFileName.csv'\n#full_data=pd.read_csv(myfile, thousands=',',delimiter=';',encoding='latin1',na_values=\"n\/a\")","350c8ae1":"display(full_data.head())","4b04dda0":"full_data.info()","292c54f5":"full_data.dtypes","ed65fdbe":"full_data.isnull().sum()","ebe081e0":"def NullValues(theData):\n    null_data = pd.DataFrame(\n                              {'columns': theData.columns, \n                               'Sum': theData.isnull().sum(), \n                               'Percentage': theData.isnull().sum() * 100 \/ len(theData), \n                               'zeros Percentage': theData.isin([0]).sum() * 100 \/ len(theData)\n                              }\n                            )\n    return null_data ","e23e2964":"NullValues(full_data)","c80d29e5":"full_data.describe()","92fffb84":"# Store the 'Survived' feature in a new variable and remove it from the dataset\n\nLabels = full_data['Classification']\nmydata = full_data.drop('Classification', axis = 1)\n\n# Show the new dataset with 'Survived' removed\ndisplay(mydata.head())","9a770e37":"display(Labels.head())","56babbaa":"def filter_data(data, conditionField,conditionOperation,conditionValue):\n    \"\"\"\n    Remove elements that do not match the condition provided.\n    Takes a data list as input and returns a filtered list.\n    Conditions passed as separte parameresfor the field,operation and value.\n    \n    \"\"\"\n   \n    #field, op, value = condition.split(\" \") # Example: [\"field == 'value'\", 'field < 18']\n    field, op, value = conditionField,conditionOperation,conditionValue\n    \n    # convert value into number or strip excess quotes if string\n    try:\n        value = float(value)\n    except:\n        value = value.strip(\"\\'\\\"\")\n    \n    # get booleans for filtering\n    if op == \">\":\n        matches = data[field] > value\n    elif op == \"<\":\n        matches = data[field] < value\n    elif op == \">=\":\n        matches = data[field] >= value\n    elif op == \"<=\":\n        matches = data[field] <= value\n    elif op == \"==\":\n        matches = data[field] == value\n    elif op == \"!=\":\n        matches = data[field] != value\n    else: # catch invalid operation codes\n        raise Exception(\"Invalid comparison operator. Only >, <, >=, <=, ==, != allowed.\")\n    \n    # filter data and outcomes\n    data = data[matches].reset_index(drop = True)\n    return data","07a3d414":"filtered_data=filter_data(mydata, 'sepal width (cm)','<','3')\ndisplay(filtered_data.head())","af55eb77":"sorted_data = mydata.sort_values(by=['sepal length (cm)'],ascending=False)\ndisplay(sorted_data.head())","11e68c2d":"def BarGraph(theData,target,attributeName):\n    xValues = np.unique(target)\n    yValues=[]\n    for label in xValues:\n        yValues.append(theData.loc[target==label, attributeName].idxmax())\n    plt.bar(xValues,yValues)\n    plt.xticks(xValues, target)\n    plt.title(attributeName) \n    plt.show()","246d9dc5":"#BarGraph(mydata,Labels,'petal length (cm)')","c83a40cd":"def BarGraphs(theData,target,attributeNamesList,graphsTitle='Attributes Classifications'):\n    xValues = np.unique(target)\n    fig, ax = plt.subplots(nrows=int(len(attributeNamesList)\/2), ncols=2,figsize=(16, 8))\n    k=0\n    for row in ax:\n        for col in row:\n            yValues=[]\n            for label in xValues:\n                yValues.append(theData.loc[target==label, attributeNamesList[k]].idxmax()) \n            col.set_title(attributeNamesList[k])\n            #col.set(xlabel=\" x Label\", ylabel=' y Label')\n            col.bar(xValues,yValues)\n            k=k+1\n    fig.suptitle(graphsTitle)    \n    plt.show()","402478f4":"BarGraphs(mydata,Labels,['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'])","da10ee9c":"def Distribution(theData,datacolumn,type='value'):\n    if type=='value':\n        print(\"Distribution for {} \".format(datacolumn))\n        theData[datacolumn].value_counts().plot(kind='bar')\n    elif type=='normal':\n        attr_mean=theData[datacolumn].mean() # Mean of the attribute values\n        attr_std_dev=full_data[datacolumn].std() # Standard Deviation of the attribute values\n        ndist=np.random.normal(attr_mean, attr_std_dev, 100)\n        norm_ax = sns.distplot(ndist, kde=False )\n        plt.show()\n        plt.close()\n    elif type=='uniform':\n        udist = np.random.uniform(-1,0,1000)\n        uniform_ax = sns.distplot(udist, kde=False )\n        plt.show()    \n    elif type=='hist':\n        theData[datacolumn].hist()","d2fc1b8d":"Distribution(full_data, 'Classification')","17f6530b":"Distribution(full_data, 'sepal length (cm)',type='normal')","776beb2f":"Distribution(full_data, 'sepal length (cm)',type='uniform')","0acc199f":"Distribution(full_data, 'sepal length (cm)',type='hist')","9c4a564e":"sns.distplot(full_data['sepal length (cm)'])","e80b7f1e":"plt.figure(figsize=(10,11))\nsns.heatmap(mydata.corr(),annot=True)\nplt.plot()","bfa41c10":"sns.FacetGrid(full_data,hue=\"Classification\").map(plt.scatter,\"sepal length (cm)\",\"sepal width (cm)\").add_legend()\nplt.show()","a2f4a480":"sns.FacetGrid(full_data,hue=\"Classification\").map(plt.scatter,\"petal length (cm)\",\"petal width (cm)\").add_legend()\nplt.show()","bd55f2f5":"sns.pairplot(mydata)","9677af6e":"from pandas.plotting import scatter_matrix\ncolors = list()\npalette = {0: \"red\", 1: \"green\", 2: \"blue\"}\nfor c in np.nditer(iris.target): colors.append(palette[int(c)])\ngrr = scatter_matrix(mydata, alpha=0.3,figsize=(10, 10), diagonal='hist', color=colors, marker='o', grid=True)\n","129a9a1e":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.violinplot(x=\"Classification\",y=\"sepal length (cm)\",data=full_data)\nplt.subplot(2,2,2)\nsns.violinplot(x=\"Classification\",y=\"sepal width (cm)\",data=full_data)\nplt.subplot(2,2,3)\nsns.violinplot(x=\"Classification\",y=\"petal length (cm)\",data=full_data)\nplt.subplot(2,2,4)\nsns.violinplot(x=\"Classification\",y=\"petal width (cm)\",data=full_data)","1bfa2a63":"sns.boxplot(y=full_data['sepal length (cm)'])","d89236c0":"sns.boxplot(x=full_data['sepal length (cm)'])","d657fd35":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"Classification\",y=\"sepal length (cm)\",data=full_data)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"Classification\",y=\"sepal width (cm)\",data=full_data)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"Classification\",y=\"petal length (cm)\",data=full_data)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"Classification\",y=\"petal width (cm)\",data=full_data)","8a906b0f":"def count_unique_values(theData, categorical_columns_list):\n    cats = theData[categorical_columns_list]\n    rValue = pd.DataFrame({'cardinality': cats.nunique()})\n    return rValue","9288f8f9":"categorical_features_list = [ 'Classification','sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nval_df = count_unique_values(full_data, categorical_features_list) \nval_df","e415b789":"def demographic(theData,demographic_features,bin_min,bin_max,bin_length):\n    demo = full_data[demographic_features]\n    bins = np.arange(bin_min, bin_max, bin_length)\n    a_bin = [str(x) for x in np.arange(bin_min, bin_max, bin_length) ]\n    a_labels = [\"\".join(x) for x in zip( [x + \" - \" for x in a_bin[:-1]], a_bin[1:])]\n    demo['bins'] = pd.cut(demo['sepal length (cm)'], bins=bins, labels=a_labels)\n    return demo ","eddf96be":"demo_features = [ 'Classification','sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nrValue=demographic(full_data,demo_features,0,10,2)\ndisplay(rValue) ","1586650f":"ax = sns.countplot(x=\"bins\", data=rValue)","65f39dd0":"ax = sns.countplot(x=\"bins\", hue=\"Classification\", data=rValue)","6397cea9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(mydata,Labels, random_state=0)","211f4d3c":"print(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))","fe23d95f":"print(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))","c0f64e67":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)","18c03b89":"knn.fit(X_train, y_train)","779dfd5d":"X_new = np.array([[5, 2.9, 1, 0.2]])\nprint(\"X_new.shape: {}\".format(X_new.shape))","9d9d8f54":"prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))","02b63503":"def GetAccuracy(datasetClasses, PredictedClasses):\n    \"\"\" Returns accuracy score for input Dataset Classes\/labels and Predicted Classes\/labels \"\"\"\n    # Ensure that the number of predictions matches number of classes\/labels\n    if len(datasetClasses) == len(PredictedClasses): \n        # Calculate and return the accuracy as a percent\n        return \"Predictions have an accuracy of {:.2f}%.\".format((datasetClasses == PredictedClasses).mean()*100)\n    else:\n        return \"Number of predictions does not match number of Labels\/Classes!\"","3c1617e8":"# Test the 'accuracy_score' function\npredictions = ['setosa','versicolor','versicolor','setosa','setosa']\nprint(GetAccuracy(Labels[:5], predictions))","4cd5ddc6":"def predictions_example(data):\n    predictions = []\n    for flower in data:\n        # Predict the survival of 'passenger'\n        predictions.append('setosa')\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions\npredictions = predictions_example(Labels)","feffab2c":"print(GetAccuracy(Labels, predictions))","3e818367":"y_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))","b6f28d47":"print(GetAccuracy(y_test, y_pred))","b63e77f3":"print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))","0aa77748":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \nimport itertools","dd915ef2":"CM = confusion_matrix(y_test, y_pred) ","de67a0d0":"print ('Confusion Matrix :')\nprint(CM) ","ad570a76":"print ('Accuracy Score :',accuracy_score(y_test, y_pred) )","2fbac6c9":"print ('Report : ')\nprint(classification_report(y_test, y_pred))","9fa12942":"def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","15eeebee":"# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(CM, classes=iris['target_names'], title='Confusion matrix, without normalization',normalize=True)\nplt.show()","039bbdcb":"# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(CM, classes=iris['target_names'], title='Confusion matrix, without normalization')\nplt.show()","c7be3ce4":"## Build the model","338f0b11":"Before building a machine learning model it is a good idea to look at our data for more understanding about the relationships between the various components constituting it. Inspecting our data is a good way to find abnormalities and peculiarities. Maybe some of your irises were measured using inches and not centimeters, for example. In the real world, inconsistencies in the data and unexpected measurements are very common. One of the best ways to inspect data is to do some visualization over our data. We could use python `matplotlib` library or `seaborn` library for plotting and visualizing the data using different types of plots like bar plot, box plot, scatter plot etc.","f978814c":"# Starter Framework for Machine Learning Projects ","13a874ec":"Before making the split, the train_test_split function shuffles the dataset using a pseudorandom number generator. If we just\ntook the last 25% of the data as a test set, all the data points would have the label 2, as the data points are sorted by the label (see the output for iris['target'] shown earlier). Using a test set containing only one of the three classes would not tell us much about how well our model generalizes, so we shuffle our data to make sure the test data contains data from all classes. To make sure that we will get the same output if we run the same function several times, we provide the pseudorandom number generator with a fixed seed using the random_state parameter. This will make the outcome deterministic, so this line will always have the same outcome. ","732e1cfa":"For uniform distribution visualization check the following:","9956b296":"`Precision` reflects the fraction of reported actual cases\/classes that are so (i.e the proportion of positive identifications was actually correct). It is calculated by dividing the total number of correctly classified positive examples by the total number of predicted positive examples(TP\/Predicted Yes) as follows : -<br>\n\n![precision.PNG](attachment:eb1049bb-9cff-4b0e-b21b-b9c716c6a4a1.PNG)\n\n<br>\nPrecision is used when it predicts Yes, and measures how often is it correct. High Precision indicates an example labelled as positive is indeed positive (a small number of FP).","95c43459":"To explore how these features are correlated to each other we could use `heatmap` in seaborn library. We can see that Sepal Length and Sepal Width features are slightly correlated with each other.","0cffb381":"We can also use the `score` method of the knn object, which will compute the test set accuracy for us:","c2871c2c":"Another way to do scatter plotting is to use `scatter matrix` existed in `plotting` module comes with `pandas` library. the following code creates a scatter matrix from the dataframe, and colors will be by Classes or labels:-","6197af43":"![photobyyosrynegm1.png](attachment:ba98e9f0-2975-4a75-af33-1545a7682fdc.png)","b8ea1b8f":"To show the Histograph diagram for a certain attribute we use the following:","54511ece":"In this article I'll demonstrate some sort of a framework for working on machine learning projects. As you may know, machine learning in general is about extracting knowledge from data therefore, most of machine learning projects will depend on a data collection - called <font color='green'>dataset<\/font> - from a specific domain on which,  we are investigating a certain problem to build a predictive model suitable for it. This model should follow certain set of steps to accomplish its purpose, In the following sections I will practically, introduce a simplified clarification about the main steps for performing statistical learning or building machine learning model. I Assumed that the explanation project is  implemented in Python programming language inside Jupiter Notebook (IPython) depending on using Numpy, Pandas, and Scikit-Learn packages.","6dfe767e":"Then we start loading our dataset into a pandas DataFrame. The data that will be used here is the Iris dataset, a classical dataset in machine learning and statistics. It is included in scikit-learn within datasets module.","6e85b547":"The knn object encapsulates the algorithm that will be used to build the model from the training data, as well the algorithm to make predictions on new data points. It will also hold the information that the algorithm has extracted from the training data. In the case of KNeighborsClassifier, it will just store the training set. To build the model on the training set, we call the `fit` method of the knn object, which takes as arguments the NumPy array `X_train` containing the training data and the NumPy array `y_train` of the corresponding training labels:","cd91bad1":"###  Fall-out (False Positive Rate) ","4fe1c732":"Note that :-\n- High recall, low precision: This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives.\n- Low recall, high precision: This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP)","1425db94":"`Accuracy` or the classification rate measures how often is the classifier correct and it is defined as the fraction of predictions our model got right. The accuracy score is computed from the following formula:<br>\n\n![acc1.png](attachment:2b262b15-0f36-41ff-bc4b-7eccb3c36536.png)","10297fc2":"The data of isis flowers contains the numeric measurements of sepal length, sepal width, petal length, and petal width and is stored as `NumPy array`, so to convert it to `pandas DataFrame` we will write the following code:","67a1f770":"Also as second example we could assume as a prediction that all flowers in our dataset are predicted as `setosa`. So, the code below will always predict  that all flowers in our dataset are `setosa`.","1e222eab":"![acc2.png](attachment:b45754b4-088d-4e25-81e6-45d3e5afb086.png)\n<br>\nThere are problems with accuracy measure that it assumes equal costs for both kinds of errors. A 99% accuracy can be excellent, good, mediocre, poor or terrible depending upon the problem. it could be a reasonable initial measure if the classes in our dataset are all of similar sizes. ","18293123":"It measures  how often is the classifier wrong and also known as misclassification Rate((FP+FN)\/total). It is computed as follows:-<br>\n![errrate.PNG](attachment:647b12b9-6d65-4a5f-ad0b-a96a4677b86f.PNG)","8f083d23":"And to get more details about these null values we use:","11343aac":" We can measure how well the model works by computing the accuracy, which is the fraction of flowers for which the right species was predicted:","06d7c1ac":"## Data Visualization","a99d02df":"To sort our data we could use:","c244bb6e":"Now, let's use `SciKit-Learn` to create our `Confusion Matrix`and calculate the performance evaluation metrics:","b34de447":"Before building a machine learning model, we must first analyze the dataset we are using for the problem, and Always we are expected to assess for common issues that may require preprocessing. So data exploration is a necessary action should be done before proceeding with our model implementation. This is done by showing a quick sample from the data, describing a The type of data, Knowing its shape or dimensions, and if needed, having basic statistics and information related to the loaded dataset. As well as exploring of input features and any abnormalities or interesting qualities about the data that may need to be addressed. Data exploration provides you with a deeper understanding of your datasets, including Dataset schemas, Value distributions, Missing values and Cardinality of categorical features.","c8989833":"Confusion Matrix used typically in supervised learning algorithms (in unsupervised learning it is usually called a matching matrix) and most performance measures are computed from the confusion matrix. Therefore, if we considered the `accuracy` score it would be calculated as ((TP+TN)\/total) using the confusion matrix as follows:","3be94c3f":"To start exploring our dataset represented by The iris object that is returned by load_iris() and stored in mydata pandas DataFrame, we could display the first few entries for examination using the\u00a0.head() function.","c13d6be8":"### Error Rate","bfac86d9":"Depending on the nature and characteristics of the problem under investigation we need to select algorithms and techniques suitable for solving it. Since we have measurements for which we know the correct species of iris, this is a <font color='green'>Supervised Learning<\/font> problem. In this problem, we want to predict one of several options (the species of iris). This is an example of a <font color='green'>classification<\/font> problem. The possible outputs (different species of irises) are called classes. Every iris in the data set belongs to one of three classes, so this problem is a three-class classification problem. The desired output for a single data point (an iris) is the species of this flower. For a particular data point, the species it belongs to is called its label or class.","7da4280b":"- Define the problem\n- Describe the solution & select an algorithm \n- Prepare the project environment \n- Explore & visualize the data\n- Prepare data and split it\n- Build and train an initial model\n- Evaluate & fine-tune \n- Build final model & evaluate it\n- Apply the final model to unseen data","54a65dda":"To show density of the length and width in the species we could use `violin plot` of all the input variables against output variable which is Species.","499f21f1":"Another visualization example is for normal value distribution as shown below:-","655cf28c":"Similarly scatter plot of data based on Petal Length and Width features","2ef3f4f2":"Now we can start building the actual machine learning model. There are many classification algorithms in scikit-learn that we could use. Here we will use a `k-nearest neighbors classifier`, which is easy to understand. Building this model only consists of\nstoring the training set. To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point. Then it assigns the label of this training point to the new data point.","e3f498d6":"And similarly we may use `boxplot` to see how the categorical feature `Classification` is distributed with all other input and also, to check for Outliers variables :-","bfd4c3d3":"To try calculating our previously mentioned evaluation metrics, this is where the test set that we created earlier comes in. This data was not used to build the model, but we do know what the correct species is for each iris in the test set. Therefore, we can make a prediction for each iris in the test data and  compare it against its label (the known species). use the following code:","47617276":"The `fit` method returns the knn object itself (and modifies it in place), so we get a string representation of our classifier. The representation shows us which parameters were used in creating the model. Nearly all of them are the default values, but you can also find n_neighbors=1, which is the parameter that we passed. Most models in scikit-learn have many parameters, but the majority of them are either speed optimizations or for very special use cases. You don\u2019t have to worry about the other\nparameters shown in this representation. Printing a scikit-learn model can yield very long strings, but don\u2019t be intimidated by these. So, we will not show the output of fit because it doesn\u2019t contain any new information.","2d786a90":"To filter the input data by Removing elements that do not match certain provided condition. The following function takes a data list as input and returns a filtered list as shown in the code below:-","cea3789d":"It is used when it's actually No, and measures how often does the classifier predicts yes. I is computed as (FP\/Actual No)\n<br>\n\n![fpr.PNG](attachment:55f6f5a4-56a0-4150-8f6a-e9bf93f3d540.PNG)","8fd2a126":"### Precision ","e4400f94":"As an example we could filter our data to be list of all flowers that has the attribute `sepal width (cm)` less than `3` as follows:-","62c14050":"We cannot use the same data we used to build the model to evaluate it. This is because our model can always simply remember the whole training set, and will therefore always predict the correct label for any point in the training set. This `remembering` does not indicate to us whether our model will generalize well i.e. whether it will also perform well on new data. So, Before using a machine learning model that can predict from unseen data, we should have some way  to  know whether it actually works or not. Hence, we need to split the labeled data into two parts. One part of the data is used to build our machine learning model, and is called the `training data` or `training set`. The rest of the data will be used to measure how well the model works; this is called the `test data`, or `test set`.","a95f7e5d":"And to allow the use of the function display() for pandas data frames we use the following code:","db79520c":"### Specificity","7fdb5d1a":"### Recall","f21cb974":"### Confusion Matrix","1cb5de3c":"From the above sample of our dataset, we can see that the dataset contains measurements for 150 different flowers. Each individual items are called examples, instances or samples in machine learning, and their properties (the 5 columns) are called features (4 features and one column is the `Target` or the class for each instance or example). The shape of the data array is the number of samples multiplied by the number of features. This is a convention in scikit-learn, and our data will always be assumed to be in this shape. Below, the detailed explaination of the features and the classes \ncontained in our dataset:-<br>  \n    \n- **sepal length**: Represents the length of the sepal of the specified iris flower in centimeters. \n- **sepal width**: Represents the width of the sepal of the specified iris flower in centimeters. \n- **petal length**: Represents the length of the petal of the specified iris flower in centimeters. \n- **petal width**:  Represents the width of the petal of the specified iris flower in centimeters. \n","d9365e91":"It appears that the attribute `sepal length (cm)` has a normal distribution.","34cb5a13":"As a final word, Note that The `fit`, `predict`, and `score` methods are the common interface to supervised models in scikit-learn, and with the concepts introduced in this article, you can apply these models to many machine learning tasks. ","253c9ee2":"### ROC","ddb874ca":"The code below tests the above function by displaying bar graph illustration for `sepal length (cm)` as an example.","6fba6bea":"## Data Exploration","1d17beb4":"`scikit-learn` contains a function that shuffles the dataset and splits it for you: the `train_test_split` function. This function extracts 75% of the rows in the data as the `training set`, together with the corresponding labels for this data. The remaining 25% of the data, together with the remaining labels, is declared as the `test set`.","5c188edc":"Since we're interested in the classification of iris flowers i.e we are observing only the classes or label of each flower based on the given measurements or features, we can remove the `Classification` feature from this dataset and store it as its own separate variable `Labels`. We will use these labels as our prediction targets.\nthe code below will remove `Classification` as a feature of the dataset and store it in `Labels`.","1de4188c":"To observe relationships between features in our dataset we could use a scatter plot. A scatter plot of the data puts one feature along the x-axis and another along the y-axis, and draws a dot for each data point. For scatter plotting how our data is distributed based on Sepal Length Width features we could use the code below :","cf9de210":"To begin working with our project data, we'll first import the functionalities we need in our implementation such as Python libraries, setting our environment to allow us accomplish our mission as well as loading our dataset successfully: ","2f877218":"For this model, the test set accuracy is about 0.97, which means we made the right prediction for 97% of the irises in the test set. Under some mathematical assumptions, this means that we can expect our model to be correct 97% of the time for new irises. For our application, this high level of accuracy means that our model may be trustworthy enough to use. In most of case the initial model we have built is fine-tuned to  improve its performance and we re-evaluate it again and again to get final accepted model for application.","f555261a":"The output of the train_test_split function is X_train, X_test, y_train, and y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset, and X_test contains the remaining 25%:","c174b1ec":"><font color='blue'>Note<\/font>: Here we are using built-in dataset that comes with `Sci-kit Learn` library but in practice, the data set often comes as csv files so we could use a code looks like the following commented one:","c0f02f3a":"To know how *accurate* our predictions are, we will calculate the proportion of cases where our prediction of their diagnosis is correct. The code below will create our `GetAccuracy` function that calculates the accuracy score.","2dc7293a":"In `scikit-learn`, data is usually denoted with a capital X, while labels are denoted by a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics, where x is the input to a function and y is the output. Following more conventions\nfrom mathematics, we use a capital X because the data is a two-dimensional array (a matrix) and a lowercase y because the  target is a one-dimensional array (a vector). Let\u2019s call train_test_split on our data and assign the outputs using this  following code:-","8151141e":"### Accuracy","37b795fd":"Our model predicts that this new iris belongs to the class or label or species `setosa`. But how do we know whether we can trust our model? We don\u2019t know the correct species of this sample, which is the whole point of building the model!","e8213d00":"The `k` in `k-nearest neighbors` signifies that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the training (for example, the closest three or five neighbors). Then, we can make a prediction using the majority class among these neighbors. For simplification, we\u2019ll use only a single neighbor.","1dc272b2":"### F1-Score ","b93c1a43":"To plot datasets with more than three features we could use a `pair plot`, which looks at all possible pairs of features. If you have a small number of features, such as the four we have here, this is quite reasonable. You should keep in mind, however, that a pair plot does not show the interaction of all of features at once, so some interesting aspects of the data may not be revealed when visualizing it this way. We could use the `pairplot` function in the seaborn library as follows:","6b7b258b":"## Project Preparation","9b4904d9":"- True Positives (`TP`) : Observations that are `positive`, and is predicted to be `positive`.\n- False Positives (`FP`) : Observations that are `negative`, but is predicted `positive` (Type 1 Error).\n- True Negatives (`TN`) : Observations that are `negative`, and is predicted to be `negative`.\n- False Negatives (`FN`) : Observations that are `positive`, but is predicted `negative` (Type 2 Error).","0b9ddb29":"In the case of statistical `classification`, we could use the so called `confusion matrix`, also known as an `error matrix`. A confusion matrix is considered as a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.","775a6e46":"## Algorithm Selection","0d6b1b14":"For Pretty display for plots in jupyter notebooks we may use:","026877dc":"To display more information about the structure of the data :","1ef60c8b":"Let\u2019s summarize our story here. We started with Problem statement and Algorithm selection by formulating of the task of predicting which species of iris a particular flower belongs to by using physical measurements of the flower and then we prepare our project and load the associated dataset. We used a dataset of measurements that was annotated by an expert with the correct species to build our model, making this a supervised learning task (three-class classification problem). After that we start explore and visualize the data to understand it, then split our data to training and test sets reaching to build our model and fitting it by training and then test. Simply we could summarize the basic steps in building machine learning application as follows:-","6f76029e":"Visualizing value distribution across our dataset, gives more understanding of attribute distributions inside the dataset to check the nature of that distribution if it is normal or uniform distribution and this could be done as follows:-","63fc9e5b":"![fw.jpg](attachment:f46217e1-19f8-416e-8cb2-82b767003c52.jpg)","cf3938c5":"To build better models, you should clearly define the problem that you are trying to solve, including the strategy you will use to achieve the desired solution. I've chosen a simple application of Iris Species Classification in which we will create a simple machine learning model that could be used in distinguishing the species of some iris flowers by identifying some measurements associated with each iris such as the petals' length and width as well as sepals' length and the width, all measured in centimetres. We will depend on a data set of previously identified measurements by experts, they have been classified the flowers into the species stosa, versicolor, or virginica. Our mission is to build a model that can learn from these measurement, So that we can predict the species for a new iris<br>","9fc3b87b":"## By Yosry Negm","4df0c2ec":"To Ignore warnings of loaded modules during runtime excution we could use the following code if needed:","d567c88b":"`True` here means cases that correctly classified either Positve or negative while `False` indicates cases that are incorrectly classified as positive or negative.","380e28fa":"## Problem Statement","3591bcaf":"The following function displays `Bar graph` for any single arbitrary attribute from our data:-","c1a2c951":"Displaying statistics about the new dataset we use the following code :","9cc95e25":"Grouping by bins :","160b641b":"We can now make predictions using this model on new data for which we might not know the correct labels. Imagine we found an iris in the wild with a sepal length of 5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by calculating the shape\u2014that is, the number of samples (1) multiplied by the number of features (4):","b49d1bcb":"This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (TPR) on y-axis against the False Positive Rate (FPR)  on x-axis as you vary the threshold for assigning observations to a given class and to study how the discrimination threshold of binary classifiers is varied and demonstrate the trade-off between false positives and false negatives.","7eeb7181":"You could compare min and max and see if scale is large and there's a need for Scaling of numerical features.","d5e90be5":"## Splitting dataset into training and testing data","521233b5":"To perform demographic analysis you may use some code like:","c88f16d8":"Specificity measures ALL the negative cases in the dataset, how many of them are successfully identified by the algorithm, i.e. the true negatives. In other words, it measures the proportion of accurately-identified negative cases. You can think of highly specific tests as being good for ruling in negative. If someone has a positive result on a highly specific test, it is extremely likely that they have the positive since a high specific algorithm has low false positive. It is know also as `True Negative Rate`, and is used when it's actually No, hence it measures how often does the classifer predicts  No.<br>\n\n![spec.PNG](attachment:532699bf-161d-4f05-b0fc-530568c31476.PNG)\n","c8c44f7e":"![cm.PNG](attachment:83ed2c6b-1002-4b82-bf9d-99f71ce7532f.PNG)","1519dc7d":"Since we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F-measure which uses `Harmonic Mean` in place of Arithmetic Mean as it punishes the extreme values more. The F-Measure will always be nearer to the smaller value of Precision or Recall. `F1-Score` is a measure of a test's accuracy and is expressed in terms of Precision and Recall (Harmonic Mean between precision and recall), it could be considered as a measure that punishes false negatives and false positives equally but weighted by their inverse fractional contribution to the full set to account for large class number hierarchies. It computed as follows: -<br>\n\n![f1.PNG](attachment:0e806d1b-7706-44e5-80ea-4b5332f12537.PNG)","c3baf3f1":"## Conclusion ","e8b37f27":"From the plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. This means that a machine learning model will likely be able to learn to separate them.","766c659b":"For any project, We need to clearly define the metrics or calculations we will use to measure performance of our model or the\nresults in our project i.e to measure the performance of our predictions, we need a metric to score our predictions against the true classifications of the given examples. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. In machine learning classification models, we usually use a variety of performance measure metrics. From the measures that are commonly applied to classification problems we could mention  `Accuracy (A)`, `Precision (P)`, `Recall (R)`, and `F1-Score`, ... etc.\n\n[Classification Performance metrics]\n\n[Classification Performance metrics]: https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/true-false-positive-negative","79fc6660":"To check if there are any null values present in the dataset. ","ccc81f0a":"All machine learning models in `scikit-learn` are implemented in their own classes, which are called Estimator classes. The `k-nearest neighbors classification algorithm` is implemented in the `KNeighborsClassifier class` in the `neighbors` module. Before we can use the model, we need to instantiate the class into an object. This is when we will set any parameters of the model. The most important parameter of KNeighbor sClassifier is the `number of neighbors`, which we will set to 1:","0dfda8a0":"**Example:** *Out of the first five flowers, if we predict that all of them are the list `predictions = ['setosa','versicolor','versicolor','setosa','setosa']`, So we would expect the accuracy of our predictions to be as follows:-","5c0615e9":"### <font color='grey'>Loading Data<\/font>","e095aac6":"Note that we made the measurements of this single flower into a row in a twodimensional NumPy array, as scikit-learn always expects two-dimensional arrays for the data. To make a prediction, we call the predict method of the knn object:","8af4e313":"To check Cardinality:","0ef6d3d8":"<br>\n<center>The above diagram summarizes a simplified framework for working with machine learning projects.<\/center>","e648013e":"We could define `Recall` as the fraction of true classes that are found by the classifier(TP\/Actual Yes), i.e. the proportion of actual positives was identified correctly. It is sometimes called `Sensitivity` and it is the ratio of the total number of correctly classified positive examples divide to the total number of positive examples so is calculated as follows: -<br>\n\n![recall.PNG](attachment:ea6812c3-3916-41e9-b3f6-ab12e8cf3e3d.PNG)\n\n<br>\nRecall is used when it's actually yes, and measures how often does the classifier predicts yes. It is known as `Sensitivity` or `True Positive Rate(TPR)`. High Recall indicates the class is correctly recognized (a small number of FN).\nSensitivity is a metric that tells us among ALL the positive cases in the dataset, how many of them are successfully identified by the algorithm, i.e. the true positive. In other words, it measures the proportion of accurately-identified positive cases.\nYou can think of highly sensitive tests as being good for ruling out negative. If someone has a negative result on a highly sensitive algorithm, it is extremely likely that they don\u2019t have the positive since a high sensitive algorithm has low false negative.","508357b4":"The `Classification` feature removed from the DataFrame. Note that data (the iris flowers data) and `Labels` (the labels or classifications of iris flowers) are now paired. That means for any iris flower `mydata.loc[i]`, they have the a classification or label  `Labels[i]`.","61cc30ea":"To test visualizing value ditribution for the `Classification` attribute we could write:-","5cbf2206":"The `thinner part denotes that there is less density` whereas the `fatter part conveys higher density`.","2512a34f":"## Model Evaluation ","f6a1a5bd":"* To Know how accurate would a prediction be that all of the flowers has specis of `setosa` ? The code below shows the accuracy of this prediction.","e85c8a1e":"A `confusion matrix` is constructed as a table that is often used to describe the performance of a `classification` model (or \u201cclassifier\u201d) on a set of test data for which the true\/real\/actual values are known.  They are defined in terms of `Positive classes` which are the observations that are `positive` (for example: Chest X-ray image indicates presence of Pneumonia) and the `Negative Classes` which are the observations that are `not positive` (for example: Chest X-ray image indicates absence of Pneumonia i.e Normal). To compute performance metrics using confusion matrix we should count the following four values:-","67a4bf29":"Also, we could display multiple bar graphs together at the same time using the following function:"}}