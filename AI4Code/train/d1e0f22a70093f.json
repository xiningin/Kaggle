{"cell_type":{"0686ec19":"code","05143c50":"code","a244bfd3":"code","2a11a7db":"code","7908371d":"code","09e9f544":"code","c2dceda1":"code","d066b492":"code","3819e05d":"code","eb239716":"code","d460d896":"code","0545c639":"code","1c6bd1c7":"code","2a93b127":"code","25960fbb":"code","c73ade3a":"code","c4d9ac88":"code","0dd06734":"code","a00ef661":"code","06b9944b":"code","379af096":"markdown","95bfa229":"markdown","9fccb4cb":"markdown","b92dac1c":"markdown","3dc5b376":"markdown","1d54fff3":"markdown","0b37673d":"markdown","f31ab197":"markdown","aa1fb441":"markdown","56425af1":"markdown","be2e968c":"markdown","c28030d8":"markdown","9100b831":"markdown"},"source":{"0686ec19":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","05143c50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a244bfd3":"MAX_LEN = 96\n\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","2a11a7db":"df_train=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\").fillna('')\ndf_test=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\").fillna('')","7908371d":"print(\"Training Data \\n %s\" %df_train.head())\nprint(\"Test Data \\n %s\" %df_test.head())","09e9f544":"print(\" Training Dataset Information \\n %s\" %df_train.info())","c2dceda1":"print(\" Testing Dataset Information \\n %s\" %df_test.info())","d066b492":"df_train.isnull().sum()","3819e05d":"df_test.isnull().sum()","eb239716":"import seaborn as sns\nimport matplotlib.pyplot as plt","d460d896":"sns.set(style=\"darkgrid\")\nsns.countplot(x=df_train[\"sentiment\"])\nplt.title(\"Training Set\")","0545c639":"sns.set(style=\"darkgrid\")\nsns.countplot(x=df_test[\"sentiment\"])\nplt.title(\"Test set\")","1c6bd1c7":"ct = df_train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(df_train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(df_train.loc[k,'text'].split())\n    text2 = \" \".join(df_train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[df_train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","2a93b127":"ct = df_test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(df_test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(df_test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[df_test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","25960fbb":"def R_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","c73ade3a":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","c4d9ac88":"train=df_train\ntest=df_test\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=3,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = R_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=5, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","0dd06734":"print('>>>> OVERALL 3Fold CV Jaccard =',np.mean(jac))","a00ef661":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","06b9944b":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","379af096":"# Model Max Length and Tokenizer Creation","95bfa229":"We will now convert the training data into arrays that roBERTa algorithm understands.","9fccb4cb":"# Visualizing Data","b92dac1c":"## Training Model","3dc5b376":"# Checking Files","1d54fff3":"### Loading Libraries","0b37673d":"# The Below Notebook uses RoBERTa: A Robustly Optimized BERT Pretraining Approach to predict the  tweet sentiments.\n","f31ab197":"## Tokenizing the Test Data in the Same way we tokenized the train data","aa1fb441":"Thanks To Chris For Providing the Data and pretrained RoBERTa model data.","56425af1":"**What is Roberta?**\n\nAs the BERT model previously was significantly undertrained so the team of Facebook AI  build an improved recipe for training BERT models, which we they called RoBERTa model, that can match or exceed the performance of all of the post-BERT methods.\n\nBy making some modifications to BERT model RoBERTa was created., \nthey include: \n\n1. Training the model longer, with bigger batches,over more data;\n2. Removing the next sentence prediction objective;\n3. Training on longer sequences; and \n4. Dynamically changing the masking pattern applied to the training data\n","be2e968c":"# Importing Train and Test Data","c28030d8":"# Building Roberta Model","9100b831":"** RoBERTa- Model Architecture**\n\nPlease watch following video for better clarity on the model.\n[Link](https:\/\/www.youtube.com\/watch?v=-MCYbmU9kfg)\n[RoBerta Paper](https:\/\/arxiv.org\/pdf\/1907.11692.pdf)\nSpecial Thanks to Chris Who provided Roberta Offline Data To make execution it possible.\n\n1. Setup : BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM. Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: <\/br>\n    * [CLS], x1, . . . , xN ,  \n    * [SEP], y1, . . . , yM, \n    * [EOS].M and N are constrained such that M + N < T, where T is a parameter that controls the maximum sequence length during training.\n"}}