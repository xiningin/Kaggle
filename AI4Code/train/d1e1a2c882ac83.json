{"cell_type":{"3a4fa64f":"code","ef80ac45":"code","095a1aad":"code","c6795dcb":"code","6a6bda38":"code","f151db9b":"code","73d423b8":"code","35e094d5":"code","138b7612":"code","4b1590c4":"code","d4f9c6db":"code","3946a544":"code","1fffe1d4":"code","3b7657ef":"code","88e04cc5":"code","de6fed81":"code","e89f3bdf":"code","f90e3f88":"code","958b667b":"code","c664d20d":"code","c4df8a21":"code","9b154c0a":"code","18d4f6dd":"code","f001d5cd":"code","c94011f6":"code","6d14c287":"code","d92055b8":"code","9667e033":"code","c8b383fc":"code","a85de540":"code","47e4fa22":"code","f6aba40e":"code","c9b9f26c":"code","278b5342":"code","71b0a703":"code","26ce957b":"code","875e9d83":"code","b56a21ae":"code","e3112b6e":"code","12f4ba1e":"code","57c6d07e":"code","926dc3f4":"code","6fda1f0b":"code","e99252a2":"code","44eb09a8":"code","e05b45e8":"code","a3c84588":"code","099d9083":"code","f301e59d":"code","1cfdd0bf":"code","72c7abf6":"code","499d20e0":"code","e1e54d8f":"code","19fa8852":"code","a838f88e":"code","b9194bd4":"code","13198ba8":"code","71e60278":"markdown","ce13c37c":"markdown","c55dae86":"markdown","51bca195":"markdown","d41e212f":"markdown","8c8909bc":"markdown","ca3082a6":"markdown","32921c69":"markdown","640a3b80":"markdown","84a865a4":"markdown","92f0bef2":"markdown","3e021463":"markdown","fa77e02a":"markdown","daf0ae92":"markdown","8eb64052":"markdown","7f1a6e2b":"markdown"},"source":{"3a4fa64f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef80ac45":"train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nsample_submission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","095a1aad":"#importing packages that we used for forecasting Future Sales\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\n!pip install wordcloud","c6795dcb":"#Train Data\ntrain.head()","6a6bda38":"#Test Data\ntest.head()","f151db9b":"#Item Categories Data\nitem_categories.head()","73d423b8":"#Item Data\nitems.head()","35e094d5":"#Shop Data\nshops.head()","138b7612":"#Sample Submission Data (i.e., we want ID and item_cnt_month in our output)\nsample_submission.head()","4b1590c4":"train.head(5).append(train.tail(5))","d4f9c6db":"#Checking the data types of the variables and here date is object type and we have to convert it into date type\ndict(train.dtypes)","3946a544":"#Here we are converting data type of date from object to date\ntrain['date'] = pd.to_datetime(train['date'],format='%d.%m.%Y')","1fffe1d4":"#Checking the info of Train data\ntrain.describe().T","3b7657ef":"test.shape","88e04cc5":"train.head()","de6fed81":"#Checking outliers \n\nfor c in ['item_cnt_day','item_price']:\n    plt.figure()\n    plt.title(c)\n    sns.boxplot(train[c])","e89f3bdf":"#Handling Outliers\n\ntrain = train[train['item_cnt_day']<=1000]\ntrain = train[train['item_price']<100000]","f90e3f88":"#there is this one column in the train data where item price is less than 0 which is not possible so I am replacing the value with median.\n\ntrain[train.item_price < 0]\n\nmedian = train[(train.date_block_num==4)&(train.shop_id==32)&(train.item_id==2973)&(train.item_cnt_day==1.0)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0,'item_price'] = median\n","958b667b":"#We can find that there are values in item_cnt_day (item sold per day) less than 0 so lets replace that values with 0\ntrain[train.item_cnt_day<0]\ntrain.loc[train.item_cnt_day<0,'item_cnt_day'] = 0","c664d20d":"train.isnull().sum()","c4df8a21":"#The objective is to predict the sales for the next month (no. of items sold next month i.e., Nov 2015)\n#And to predict the sales for nov 2015 month lets first compute the total sales per month.\n\nsales_ts = train.groupby(['date_block_num'])['item_cnt_day'].sum()\nsales_ts","9b154c0a":"plt.figure(figsize=(10,5))\nplt.plot(sales_ts)\nplt.xlabel(\"month\")\nplt.ylabel(\"Sale\")\nplt.show()","18d4f6dd":"fig, axes = plt.subplots(2,2)\nfig.set_figwidth(14)\nfig.set_figheight(8)\n\naxes[0][0].plot(sales_ts.index, sales_ts, label='original')\naxes[0][0].plot(sales_ts.index, sales_ts.rolling(window=4).mean(), label = '4-months rolling window')\naxes[0][0].set_xlabel(\"Months\")\naxes[0][0].set_ylabel(\"Sales\")\naxes[0][0].set_title(\"$-Months Moving average\")\naxes[0][0].legend(loc='best')\n\n\naxes[0][1].plot(sales_ts.index,sales_ts,label='original')\naxes[0][1].plot(sales_ts.index, sales_ts.rolling(window=8).mean(), label=\"8-months rolling window\")\naxes[0][1].set_xlabel(\"months\")\naxes[0][1].set_ylabel(\"Sales\")\naxes[0][1].set_title(\"8-Months moving average\")\naxes[0][1].legend(loc='best')\n\naxes[1][0].plot(sales_ts.index,sales_ts,label=\"original\")\naxes[1][0].plot(sales_ts.index,sales_ts.rolling(window=10).mean(), label='10-months rolling window')\naxes[1][0].set_xlabel(\"months\")\naxes[1][0].set_ylabel(\"sales\")\naxes[1][0].set_title(\"10-Months Moving Average\")\naxes[1][0].legend(loc='best')\n\naxes[1][1].plot(sales_ts.index, sales_ts, label='original')\naxes[1][1].plot(sales_ts.index, sales_ts.rolling(window=12).mean(), label=\"12-months rolling window\")\naxes[1][1].set_xlabel(\"months\")\naxes[1][1].set_ylabel(\"12-months moving average\")\naxes[1][1].legend(loc='best')","f001d5cd":"monthly_sales_data = pd.pivot_table(train,values=\"item_cnt_day\",index=['shop_id','item_id'],columns='date_block_num',\n                                   aggfunc='sum',fill_value=0)\nmonthly_sales_data.reset_index(inplace=True)\nmonthly_sales_data","c94011f6":"monthly_sales_data.plot()","6d14c287":"decomposition = sm.tsa.seasonal_decompose(sales_ts,period=12,model='multiplicative')\nfig = decomposition.plot()\nfig.set_figwidth(12)\nfig.set_figheight(8)\nfig.suptitle(\"Decomposition of multiplicative time series\")\nplt.show()","d92055b8":"decomp_output = pd.DataFrame(pd.concat([decomposition.observed,decomposition.trend,decomposition.seasonal,decomposition.resid],axis=1))\ndecomp_output\ndecomp_output['TSI'] = decomp_output.trend*decomp_output.seasonal*decomp_output.resid\ndecomp_output","9667e033":"!pip install fbprophet  #installing fbprophet using pip install","c8b383fc":"from fbprophet import Prophet","a85de540":"ts = sales_ts.copy()\n","47e4fa22":"ts.index = pd.date_range(start='2013-01-01',end='2015-10-01',freq = 'MS')\nts = ts.reset_index()\nts.head()\nts.columns=['ds','y']\nts","f6aba40e":"sales_model = Prophet(interval_width=0.95,seasonality_mode = 'multiplicative')\nsales_model.fit(ts)","c9b9f26c":"sales_forecast = sales_model.make_future_dataframe(periods=2,freq='MS')\nsales_forecast = sales_model.predict(sales_forecast)\nsales_forecast.tail()","278b5342":"plt.figure(figsize=(10,10))\nsales_model.plot(sales_forecast)\nplt.xlabel('months')\nplt.ylabel('sales')","71b0a703":"sales_model.plot_components(sales_forecast)","26ce957b":"x = items.item_category_id.nunique()\nprint(\"number of unique item category = \" +str(x))","875e9d83":"#Making a wordcloud for item category name.\nfrom wordcloud import STOPWORDS\nfrom wordcloud import WordCloud\n\nstopwords = set(STOPWORDS)\nplt.figure(figsize=(12,10))\n\nwordcloud = WordCloud(max_font_size=100,width=2000,height=1000).generate(str(items.item_name))\nplt.title(\"Wordcloud of item names\",fontsize=30)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","b56a21ae":"plt.figure(figsize=(20,5))\nsns.barplot('item_category_id','item_id',data=items)","e3112b6e":"x= shops.shop_id.nunique()\nprint(\"Number of unique shops = \"+str(x))","12f4ba1e":"#Wordcloud for shop names\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nplt.figure(figsize=(12,10))\nwordcloud = WordCloud().generate(str(shops.shop_name))\nplt.title(\"Wordcloud of Shop names\",fontsize=30)\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","57c6d07e":"item_categories.head(2)","926dc3f4":"x = item_categories.item_category_id.nunique()\nprint(\"unique item_categories ID = \"+str(x))","6fda1f0b":"#Wordcloud of item category names\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nplt.figure(figsize=(12,10))\nwordcloud = WordCloud().generate(str(item_categories.item_category_name))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Wordcloud of Item category Names\",fontsize=30)\nplt.axis(\"off\")\nplt.show()","e99252a2":"train['year'] = train.date.dt.strftime('%Y')\ntrain['month'] = train.date.dt.strftime('%m')\ntrain['day'] = train.date.dt.strftime('%d')","44eb09a8":"train.head()","e05b45e8":"#lets check which year, months and days are the busiest\nfor x in ['year','month','day']:\n    plt.figure(figsize=(10,5))\n    sns.countplot(x,data=train)\n    plt.show()","a3c84588":"train.tail()","099d9083":"#month wise data\nmonthly_data = train.groupby(['date_block_num','shop_id','item_id']).sum().reset_index()\nmonthly_data = monthly_data[['date_block_num','shop_id','item_id','item_cnt_day']]\nmonthly_data.head()","f301e59d":"#lets create Pivot table represent monthwise data\ndata = monthly_data.pivot_table(values='item_cnt_day',index=['shop_id','item_id'],columns='date_block_num',fill_value=0)\ndata.reset_index()","1cfdd0bf":"data = pd.merge(test, data, on=['shop_id','item_id'],how='left')\ndata.fillna(0,inplace=True)\ndata","72c7abf6":"data.drop(['shop_id','item_id','ID'],axis=1,inplace=True)\ndata","499d20e0":"# for train we will keep all the columns except last one\ntrain_x = data.values[:,:-1]\ntrain_x.shape\ntrain_y = data.values[:,-1:]\ntrain_y.shape","e1e54d8f":"# for test we will keep all the columns except the first one\ntest_x = data.values[:,1:]\ntest_x.shape","19fa8852":"from xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split","a838f88e":"\"\"\"\nparams = {\n    'max_depth': [6,8,10,12],\n    'n_estimators': [50,100,200,300],\n    'learning_rate': [0.01,0.1,0.02,0.2]\n}\nkfold = KFold(n_splits=5,shuffle=True,random_state=1)\ngscv = GridSearchCV(XGBRegressor(),param_grid=params, verbose=1, cv=kfold, n_jobs=-1)\ngscv.fit(train_x, train_y)\nprint(gscv.best_score_)\nprint(gscv.best_params_)\n\"\"\"","b9194bd4":"#Modelling\n#get the test set prediction and clip value to the specified range\nxgbr  = XGBRegressor(learning_rate=0.01,max_depth=8,n_estimators=100)\nxgbr.fit(train_x,train_y)\npred_y = xgbr.predict(test_x).clip(0.,20.)","13198ba8":"Submission = pd.DataFrame(pred_y,columns=['item_cnt_month'])\nSubmission.to_csv(\"submission.csv\",index_label='ID')","71e60278":"### Item Data","ce13c37c":"#### From graph it seems that the sales have been decreasing without fail. \n\nNovember seems as the most peak month when the sales are higher mean and variance are also much higher value in November than any other months of the year.","c55dae86":"### Trend","51bca195":"**Inputing Data Files**","d41e212f":"## Timeseries Forecasting Using FBProphet","8c8909bc":"### Item Category Data","ca3082a6":"### Applying Model to Predict Values","32921c69":"### Sales Train Data","640a3b80":"### Remainder","84a865a4":"#### As we see that 12-Months moving average produce a wrinkle free curve as desired","92f0bef2":"### Shops Data","3e021463":"#### Now we start with time series decomposition to understand the underlying patterns such as trend, seasonality, cyclicity and irregular reminder of sales.\nWe observe a decreasing Trend. We build our model based on overall the following function\n- yt=f(Trendt, Seasonalityt, Remaindert)\n\nDickey Fuller Test : this is one of the statistical test for checking the stationarity of the data. Here, Null hypothesis is TS is Stationary. The test result comprises of Test Statistics and some critical value for different confidence interval.If the value of test statistics is lower than the value of different confidence interval then ,we can reject Null Hypothesis and say that TS is Stationary.\n\nMoving Averages : is to remove zigzag motion from the time series to produce a steady trend through average adjacent value of a time period","fa77e02a":"### Train Data","daf0ae92":"#### Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.**","8eb64052":"### Seasonality","7f1a6e2b":"### Trend and Pattern"}}