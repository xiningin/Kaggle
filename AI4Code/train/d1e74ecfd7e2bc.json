{"cell_type":{"b3502a57":"code","66995632":"code","37c067bd":"code","1dfa6f1d":"code","5bd4e1ea":"code","bf72447f":"code","f4a327a3":"code","4d175cea":"code","ef4f2211":"code","51ffa1c1":"code","318789f1":"code","16871b97":"code","30e11141":"code","330c8d53":"code","74bff18a":"code","b2d22d43":"code","832208b9":"code","ff305130":"code","c2141688":"code","7c7c1a5f":"code","0f84f490":"code","87ac93ba":"code","b707031e":"code","95b772d6":"code","bd705897":"code","94257bd0":"code","95f9022e":"code","b4559f67":"code","7214f986":"code","d9c29f70":"code","c2cd2912":"code","7f65b39c":"code","f71db368":"code","a0e46261":"code","4ef3b933":"code","2bf61124":"code","a44beb65":"code","7be9ee4d":"code","0e6783df":"code","6603c384":"code","e25ae6a7":"code","307f7f38":"code","3e0cebfd":"code","d09f1f96":"code","8fdff2ab":"code","609a19fc":"code","7f648f02":"code","d5b3d733":"code","c49052f5":"code","d322c97c":"code","5f56aa87":"code","9cfd15cc":"code","ba03d6b3":"code","0f8db1ff":"code","9c45b7e2":"code","3cd4a7fc":"code","9e06efd6":"code","be02b8b6":"code","8a967135":"code","5dbbb5d1":"code","b6d767d4":"code","b0bb7b4d":"code","3e8bb024":"code","8abb0f9a":"markdown","b872d6ec":"markdown","821e4455":"markdown","f311a304":"markdown","68052e18":"markdown","89bcdc54":"markdown","d2dc13f1":"markdown","b7548948":"markdown","ccc3ca54":"markdown","37ebd7f1":"markdown","8127f86b":"markdown","25c899bb":"markdown","d0bc8ad1":"markdown","b9134ed9":"markdown","811be43d":"markdown","7a366a25":"markdown","7f88812b":"markdown","f44d2fbf":"markdown","910f502a":"markdown","5301b5db":"markdown","c28b8294":"markdown","0d41f0be":"markdown","e1bfbfd0":"markdown","b5513981":"markdown","fd9c9f7a":"markdown"},"source":{"b3502a57":"from sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.datasets import load_iris, load_digits\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nplt.style.use('seaborn')","66995632":"X, y = load_iris(return_X_y=True)","37c067bd":"kmeans = KMeans(n_clusters=3)","1dfa6f1d":"y_pred = kmeans.fit_predict(X)","5bd4e1ea":"np.unique(y_pred == y, return_counts=True)","bf72447f":"plt.scatter(X[:,2],X[:,3], c=y, cmap='jet')","f4a327a3":"plt.scatter(X[:,2],X[:,3], c=y_pred, cmap='jet') # As we can see couple of instances were misclassified.","4d175cea":"plt.scatter(X[:,0],X[:,1], c=y, cmap='jet') # Sepal length and width to labels","ef4f2211":"plt.scatter(X[:,0],X[:,1], c=y_pred, cmap='jet') # Sepal length and width to labels","51ffa1c1":"y_pred","318789f1":"kmeans.labels_","16871b97":"y_pred is kmeans.labels_","30e11141":"c = kmeans.cluster_centers_","330c8d53":"plt.scatter(X[:,2],X[:,3], c=y_pred, cmap='jet') # As we can see couple of instances were misclassified.\nplt.scatter(c[:,2],c[:,3], c='r', marker='o')","74bff18a":"plt.scatter(X[:,0],X[:,1], c=y_pred, cmap='jet') # As we can see couple of instances were misclassified.\nplt.scatter(c[:,0],c[:,1], c='r', marker='o')","b2d22d43":"kmeans.inertia_ ","832208b9":"kmeans.score(X)","ff305130":"minibatch_kmeans = MiniBatchKMeans(n_clusters=3)\nminibatch_kmeans.fit(X)","c2141688":"y_pred = minibatch_kmeans.predict(X)","7c7c1a5f":"np.unique(y_pred == y, return_counts=True)","0f84f490":"minibatch_kmeans.inertia_","87ac93ba":"inertia = []\nfor k in range(2,10):\n    minibatch_kmeans = MiniBatchKMeans(n_clusters=k)\n    minibatch_kmeans.fit(X)\n    inertia.append(minibatch_kmeans.inertia_)","b707031e":"plt.plot(range(2,10), inertia)","95b772d6":"sil_score = []\n\nfor k in range(2,10):\n    minibatch_kmeans = MiniBatchKMeans(n_clusters=k)\n    minibatch_kmeans.fit(X)\n    sil_score.append(silhouette_score(X,minibatch_kmeans.labels_))","bd705897":"plt.plot(range(2,10), sil_score)","94257bd0":"# Image was taken from https:\/\/unsplash.com\/photos\/0_YVMsXO4WU\nimage = imread('..\/input\/aparador-rio\/rodrigo-diogo-0_YVMsXO4WU-unsplash.jpg')","95f9022e":"image.shape","b4559f67":"X = image.reshape(-1, 3)\nkmeans = MiniBatchKMeans(n_clusters=2).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img = segmented_img.reshape(image.shape)","7214f986":"plt.figure(figsize=(25,25))\nplt.imshow(segmented_img)","d9c29f70":"X_digits, y_digits = load_digits(return_X_y=True)","c2cd2912":"X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)","7f65b39c":"X_train.dtype","f71db368":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train, y_train)","a0e46261":"log_reg.score(X_test, y_test)","4ef3b933":"pipeline = Pipeline([(\"kmeans\", KMeans(n_clusters=50)),(\"log_reg\", LogisticRegression(solver='liblinear'))])\npipeline.fit(X_train, y_train)","2bf61124":"pipeline.score(X_test, y_test)","a44beb65":"param_grid = dict(kmeans__n_clusters=range(2, 150))\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\ngrid_clf.fit(X_train, y_train)","7be9ee4d":"grid_clf.best_params_, grid_clf.score(X_test, y_test)","0e6783df":"n_labeled = 100\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])","6603c384":"log_reg.score(X_test, y_test)","e25ae6a7":"y_train[:1]","307f7f38":"k = 100\nkmeans = KMeans(n_clusters=k)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)","3e0cebfd":"X_representative_digits = X_train[representative_digit_idx]","d09f1f96":"X_digits_dist[1163,0]","8fdff2ab":"np.min(X_digits_dist[:,0]), representative_digit_idx[:1]","609a19fc":"# This chunk of code is copied from https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\n# PREVIEW IMAGES\nplt.figure(figsize=(15,4.5))\nfor i in range(100):  \n    plt.subplot(10, 10, i+1)\n    plt.imshow(X_representative_digits[i].reshape((8,8)),cmap=plt.cm.binary)\n    plt.axis('off')\n    #print('\\n\\n')\nplt.subplots_adjust(wspace=.1, hspace=0.1)\nplt.show()","7f648f02":"y_representative_digits = y_train[representative_digit_idx]","d5b3d733":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)","c49052f5":"y_train_propagated = np.empty(len(X_train), dtype=np.int32)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]","d322c97c":"y_train_propagated[:5], y_train[:5]","5f56aa87":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train, y_train_propagated)\nlog_reg.score(X_test, y_test)","9cfd15cc":"percentile_closest = 20\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]","ba03d6b3":"X_cluster_dist[:5]","0f8db1ff":"np.min(X_digits_dist[:5], axis=1)","9c45b7e2":"in_cluster = (kmeans.labels_ == 1)\nX_cluster_dist[in_cluster].shape, np.unique(in_cluster, return_counts=True)[1][1]","3cd4a7fc":"cluster_dist = X_cluster_dist[in_cluster]\ncluster_dist","9e06efd6":"np.percentile(cluster_dist, percentile_closest)","be02b8b6":"for i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist > cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1 # distinguish one we want to avoid by naming -1","8a967135":"partially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]","5dbbb5d1":"np.unique(partially_propagated, return_counts=True)","b6d767d4":"X_train_partially_propagated.shape[0]","b0bb7b4d":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)","3e8bb024":"np.mean(y_train_partially_propagated == y_train[partially_propagated])","8abb0f9a":"## Choose optimal k using Gridsearch","b872d6ec":"Seem to be doing better than kmeans.","821e4455":"So with 121 clusters, doing preprocessing was worth it :)","f311a304":"So we have improved from 89% to 93% accuracy. Quite good! You may try some more values of clusters :)","68052e18":"So 93% accuracy. Well, you may play with percentile and see what if :)","89bcdc54":"But perhaps we can go one step further: what if we propagated the labels to all the\nother instances in the same cluster? This is called label propagation:","d2dc13f1":"I do not understand why image is not displayed. If anyone has any clue, I will be very grateful to know. Thanks:)","b7548948":"We got a reasonable accuracy boost, but nothing absolutely astounding. The problem\nis that we propagated each representative instance\u2019s label to all the instances in the\nsame cluster, including the instances located close to the cluster boundaries, which\nare more likely to be mislabeled. Let\u2019s see what happens if we only propagate the\nlabels to the 20% of the instances that are closest to the centroids:","ccc3ca54":"This good performance is due to\nthe fact that the propagated labels are actually pretty good\u2014their accuracy is very\nclose to 99%, as the following code shows:","37ebd7f1":"Since image is so ugly, just to get more confidence on recognition we will see what representatives represents in next cell.","8127f86b":"So we have improved to 95%.","25c899bb":"So 3 is the elbow was until 3 slop was steep. So k = 3 could be a good choice, as we expected.","d0bc8ad1":"Just to see if representative are nearby or not.","b9134ed9":"Now, you see we have improved score to 97%.","811be43d":"So we can very clearly see one class is very well separable linearly. Other two classes is the real work to be done.","7a366a25":"# Using Clustering for Semi-Supervised Learning\nAnother use case for clustering is in semi-supervised learning, when we have plenty\nof unlabeled instances and very few labeled instances. Let\u2019s train a Logistic Regression\nmodel on a sample of 50 labeled instances from the digits dataset:","7f88812b":"Work in progress:)","f44d2fbf":"So only 16 misclassified objects.","910f502a":"# Choose k. \n# 1. Elbow method.","5301b5db":"# Using Clustering for Image Segmentation\nImage segmentation is the task of partitioning an image into multiple segments. In\nsemantic segmentation, all pixels that are part of the same object type get assigned to\nthe same segment. For example, in a self-driving car\u2019s vision system, all pixels that are\npart of a pedestrian\u2019s image might be assigned to the \u201cpedestrian\u201d segment (there\nwould be one segment containing all the pedestrians). In instance segmentation, all\npixels that are part of the same individual object are assigned to the same segment. In\nthis case there would be a different segment for each pedestrian. The state of the art","c28b8294":"# 2. Silhoutte Method.\nA more precise approach (but also more computationally expensive) is to use the\nsilhouette score, which is the mean silhouette coefficient over all the instances.","0d41f0be":"# Using Clustering for Preprocessing\nClustering can be an efficient approach to dimensionality reduction, in particular as a\npreprocessing step before a supervised learning algorithm. As an example of using\nclustering for dimensionality reduction, let\u2019s tackle the digits dataset, which is a sim\u2010\nple MNIST-like dataset containing 1,797 grayscale 8 \u00d7 8 images representing the dig\u2010\nits 0 to 9. First, load the dataset:","e1bfbfd0":"So we shall be propagating to 310 instances. ","b5513981":"Let us see 100 representatives of the data.","fd9c9f7a":"Now, preprocessing the digits we will try to improve the performance.\nWe will create a pipeline that will first cluster the training set\ninto 50 clusters and replace the images with their distances to these 50 clusters, then\napply a Logistic Regression model:"}}