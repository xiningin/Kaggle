{"cell_type":{"a7692459":"code","5ffb0212":"code","7d0aed7b":"code","ed846470":"code","90245381":"code","01fcb1d1":"code","37a6dde7":"code","90a54ba1":"code","fb7a9d23":"code","b3db26b4":"code","5afc0a38":"code","b8c66e1e":"code","3b9b18b4":"code","dad89e1d":"code","59e67a19":"code","df98ca82":"code","a612e7b8":"code","c54cf135":"code","d4ea9ca8":"code","1582a788":"code","b852c67b":"code","c366fbe8":"code","22db5989":"code","eb433ca5":"code","964e14b9":"code","151e9c00":"code","702ec953":"code","e2f0dddd":"code","841cbab4":"code","3581f956":"code","09bc33f5":"code","e746f96a":"code","d637d628":"code","eaabc341":"code","bd8f0d81":"code","5ae1f89b":"code","bc61c850":"code","a4fb5609":"code","e3d6281f":"code","53cf9342":"code","4beaa86c":"code","78139103":"code","0d373ff4":"code","af44ef20":"code","44f6a950":"code","57aa765f":"code","b2e3a276":"code","41b096dd":"code","897de75a":"code","b24ca7b2":"code","8edc7d27":"code","c6fb7cb4":"code","a11b31e0":"code","e76b2307":"code","f2591e64":"code","b38c77af":"markdown","9bde5d40":"markdown","4cb3f0f4":"markdown","61fff09b":"markdown","33be824d":"markdown","dfc18627":"markdown","97b1ca4c":"markdown","44d51a7d":"markdown","dec788b1":"markdown","b6f8721a":"markdown","1d23988c":"markdown","d2766cf8":"markdown","ff29afdb":"markdown","a058197f":"markdown","56a6720d":"markdown","d602a7a4":"markdown","40e337b7":"markdown","fed750e9":"markdown","0aa60efd":"markdown","a89a2ddd":"markdown","b66b0210":"markdown","e4095604":"markdown","f8243af2":"markdown","9c66e9aa":"markdown","8f4b596e":"markdown","c272bc8f":"markdown","9fbc434d":"markdown","c47b7f92":"markdown","a328023f":"markdown","c85ef362":"markdown","5b919bed":"markdown","56319072":"markdown","d3a1c943":"markdown","59532b2b":"markdown","cc173fcd":"markdown","8cd1d7fb":"markdown","76668bb0":"markdown","d966902d":"markdown"},"source":{"a7692459":"# Data Exploration\nimport pandas as pd \nimport numpy as np \nfrom numpy import mean\nfrom numpy import std\nnp.random.seed(10)\n\n# machine learning\nimport sklearn\nimport xgboost # may need to install\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron, SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\nfrom xgboost import XGBClassifier\n\n# Visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os \nprint(os.listdir())","5ffb0212":"training = pd.read_csv('..\/input\/titanic\/train.csv')\ntesting = pd.read_csv('..\/input\/titanic\/test.csv')\ncombined = [training, testing] # combined to simplify combined operations to both sets\n\ntraining.head()","7d0aed7b":"training.info()   # 3 Features with NaNs; 'Age', 'Cabin', 'Embarked' ","ed846470":"testing.info()    # 3 Features with NaNs; 'Age', 'Fare', 'Cabin'","90245381":"training.describe(include=\"all\")","01fcb1d1":"percent_missing = training.isnull().sum() * 100 \/ len(training)\nnumber_missing = training.isnull().sum()\nmissing_value_traindf = pd.DataFrame({'column_name': training.columns,\n                                 'number_missing' : number_missing,\n                                 'percent_missing (%)': percent_missing.round(1)})\nprint(missing_value_traindf)","37a6dde7":"percent_missingTest = testing.isnull().sum() * 100 \/ len(testing)\nnumber_missingTest = testing.isnull().sum()\nmissing_valueTest_testdf = pd.DataFrame({'column_name':testing.columns,\n                                    'number_missing':number_missingTest,\n                                    'percent_missing (%)': percent_missingTest.round(1)})\n\nprint(missing_valueTest_testdf)","90a54ba1":"fig, axs = plt.subplots(figsize=[16, 6], ncols=3)\nsns.scatterplot(data=training[\"Age\"], ax=axs[0])\nsns.boxplot(y=training[\"Age\"], ax=axs[1])\nsns.distplot(training[\"Age\"], ax=axs[2])","fb7a9d23":"fig, axs = plt.subplots(figsize=[16, 6], ncols=3)\nsns.scatterplot(data=testing[\"Age\"], ax=axs[0])\nsns.boxplot(y=testing[\"Age\"], ax=axs[1])\nsns.distplot(testing[\"Age\"], ax=axs[2])","b3db26b4":"training[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) \ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)","5afc0a38":"print(\"Before\", training.shape, testing.shape, combined[0].shape, combined[1].shape)\n\ntraining = training.drop(['Ticket', 'Cabin'], axis=1)\ntesting = testing.drop(['Ticket', 'Cabin'], axis=1)\ncombined = [training, testing]\n\nprint(\"After\", training.shape, testing.shape, combined[0].shape, combined[1].shape)","b8c66e1e":"combined = [training, testing]\n\ntitle_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in combined:\n    dataset['Embarked'] = dataset['Embarked'].map(title_mapping)\n    dataset['Embarked'] = dataset['Embarked'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].astype(int)","3b9b18b4":"for dataset in combined:\n    dataset['Sex'] = dataset['Sex'].map({'female':1, 'male':0}).astype(int)\n    \ntraining.head()","dad89e1d":"TrainEvalData = training\nTestEvalData = testing","59e67a19":"#Drop non-numeric fields for analysis \nX_train_A = TrainEvalData.drop([\"Survived\", \"Name\", \"PassengerId\"], axis=1)\nY_train_A = TrainEvalData[\"Survived\"]\nX_test_A = TestEvalData.drop([\"Name\", \"PassengerId\"], axis=1).copy()\n\nX_train_A.shape, Y_train_A.shape, X_test_A.shape","df98ca82":"X = X_train_A\ny = Y_train_A\n\n# define the model\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\nimportance = model.feature_importances_\n# summarize feature importance\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='bar') # 7 Variables at this point, descending\nplt.xticks(rotation=0, horizontalalignment=\"center\")\nplt.title(\"Feature Importance (Checkpoint 1)\", fontsize=20)","a612e7b8":"for dataset in combined:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \npd.crosstab(training['Title'], training['Sex'])","c54cf135":"for dataset in combined:\n    dataset['Title'] = dataset['Title'].replace([\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Jonkheer\"], \"Misc\")     \n    dataset['Title'] = dataset['Title'].replace([\"Mlle\", \"Ms\"], \"Miss\")\n    dataset['Title'] = dataset['Title'].replace(\"Mme\", \"Mrs\")\n    dataset['Title'] = dataset['Title'].replace([\"Major\", \"Rev\", \"Sir\"], \"Mr\")\n    # dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n                               \ntraining[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","d4ea9ca8":"plt.subplots(figsize=(8,4))\nsns.countplot(training['Title'], hue=training['Survived'])\nplt.title(\"Passenger Title vs Count\", fontsize=20)","1582a788":"title_ordinal = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Misc\": 5}\nfor dataset in combined:\n    dataset['Title'] = dataset['Title'].map(title_ordinal)\n    dataset['Title'] = dataset['Title'].fillna(0)  # Check One hot encoding\n    dataset['Title'] = dataset['Title'].astype(int)","b852c67b":"training = training.drop(['Name', 'PassengerId'], axis=1)\ntesting = testing.drop(['Name'], axis=1)\ncombined = [training, testing]\ntraining.shape, testing.shape","c366fbe8":"for dataset in combined:\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntraining[\"AgeBand\"] = pd.cut(training[\"Age\"], 5) # Split into 5 equal bands\ntraining[[\"AgeBand\", \"Survived\"]].groupby([\"AgeBand\"], as_index=False).mean().sort_values(by=\"AgeBand\", ascending=True)\n","22db5989":"# AgeBand Beginning Value above changes after swap to integer\n\nfor dataset in combined:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3","eb433ca5":"sns.countplot(training['SibSp'],hue=training['Survived'])","964e14b9":"grid = sns.FacetGrid(training, row='Pclass', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Age', 'Survived', 'Sex', palette='deep')\ngrid.add_legend(title='Male=0 \/ Female=1')","151e9c00":"training = training.drop(['AgeBand'], axis=1)\ncombined = [training, testing]\ntraining.head()","702ec953":"for dataset in combined:\n    dataset['FamilySize'] = dataset['Parch'] + dataset['SibSp'] + 1\n    \ntraining[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e2f0dddd":"for dataset in combined:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \ntraining[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index = False).mean()","841cbab4":"training = training.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntesting = testing.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombined = [training, testing]\n\ntraining.head()","3581f956":"training['FareBand'] = pd.qcut(training['Fare'], 4) # Qcut bins (\"FareBand\") approx equal populated\ntraining[['FareBand','Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","09bc33f5":"for dataset in combined:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntraining = training.drop(['FareBand'], axis=1)\ncombined = [training, testing]","e746f96a":"for dataset in combined:\n    dataset['Age*PClass'] = dataset.Age * dataset.Pclass\n    \ntraining.head()","d637d628":"X_train = training.drop(\"Survived\", axis=1)\nY_train = training[\"Survived\"]\n\nX = X_train\ny = Y_train\n\n# define the model\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\nimportance = model.feature_importances_\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfig, axs = plt.subplots(figsize=[12, 6], ncols=2)\nfig.suptitle('Feature Importance Comparison', fontsize=20)\nfeat_importances.nlargest(10).plot(kind='bar', rot=30, ax=axs[0], title='Feature Importance') # 8 Variables at this point, descending\n\n# Datasets From Prior Feature Importance Checkpoint\nXa = X_train_A\nya = Y_train_A\n\nmodela = RandomForestClassifier()\nmodela.fit(Xa, ya)\nimportance = modela.feature_importances_\nfeat_importances = pd.Series(modela.feature_importances_, index=Xa.columns)\nfeat_importances.nlargest(10).plot(kind='bar', ax=axs[1]) # 8 Variables at this point, descending\nplt.xticks(rotation=0, horizontalalignment=\"center\")\nplt.title(\"Feature Importance (Checkpoint 1)\", fontsize=12)","eaabc341":"testing.info()","bd8f0d81":"X_train = training.drop(\"Survived\", axis=1)\ny_train = training[\"Survived\"]\nX_test = testing.drop(\"PassengerId\", axis=1).copy()\n\nX_train.shape, Y_train.shape, X_test.shape","5ae1f89b":"X = X_train\ny = Y_train","bc61c850":"# This creates a valuation dataset from the training data, not using the testing dataset.\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)","a4fb5609":"cv = 10  # Define Cross-Validation Number\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_val)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log_val = round(logreg.score(X_val, y_val) * 100, 2)","e3d6281f":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\ndash = '-' * 54\nprint(dash)\nprint('LR Confusion Matrix')\nprint(' ')\nprint(confusion_matrix(y_val,Y_pred))\nprint(dash)\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"LR Accuracy score (training): {0:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"LR Accuracy score (validation): {0:.3f}\".format(logreg.score(X_val, y_val)))","53cf9342":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_val)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn_val = round(knn.score(X_val, y_val) * 100, 2)","4beaa86c":"print(dash)\nprint('kNN Confusion Matrix')\nprint(' ')\nprint(confusion_matrix(y_val,Y_pred))\nprint(dash)\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"kNN Accuracy score (training): {0:.3f}\".format(knn.score(X_train, y_train)))\nprint(\"kNN Accuracy score (validation): {0:.3f}\".format(knn.score(X_val, y_val)))","78139103":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_val)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest_val = round(random_forest.score(X_val, y_val) * 100, 2)","0d373ff4":"print(dash)\nprint('RF Confusion Matrix')\nprint(' ')\nprint(confusion_matrix(y_val,Y_pred))\nprint(dash)\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"RF Accuracy score (training): {0:.3f}\".format(random_forest.score(X_train, y_train)))\nprint(\"RF Accuracy score (validation): {0:.3f}\".format(random_forest.score(X_val, y_val)))","af44ef20":"# Gradient Boosting (GBM)\n\ngbm = GradientBoostingClassifier(n_estimators=20, random_state=0)\ngbm.fit(X_train, y_train)\nY_pred = gbm.predict(X_val)\nacc_gbm = round(gbm.score(X_train, y_train) * 100, 2)\nacc_gbm_val = round(gbm.score(X_val, y_val) * 100, 2)","44f6a950":"print(dash)\nprint('GBM Confusion Matrix')\nprint(' ')\nprint(confusion_matrix(y_val,Y_pred))\nprint(dash)\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"GBM Accuracy score (training): {0:.3f}\".format(gbm.score(X_train, y_train)))\nprint(\"GBM Accuracy score (validation): {0:.3f}\".format(gbm.score(X_val, y_val)))","57aa765f":"# XGBoosting\n\nXGB = XGBClassifier(n_estimators=5000, eta=0.018, subsample=0.5, colsample_bytree=0.5)\n# define the datasets to evaluate each iteration\nevalset = [(X_train, y_train), (X_val,y_val)]\nXGB.fit(X_train, y_train, eval_metric='logloss', eval_set=evalset, verbose=False)\nY_pred = XGB.predict(X_val)\n# retrieve performance metrics\nresults = XGB.evals_result()\nxgb_cv = XGBClassifier(n_estimators=100)\nscores = cross_val_score(XGB, X, y, cv=10, scoring = \"accuracy\")\nacc_xgb = round(XGB.score(X_train, y_train) * 100, 2)\nacc_xgb_val = round(XGB.score(X_val, y_val) * 100, 2)\n\nprint(dash)\nprint('XGB Confusion Matrix')\nprint(' ')\nprint(confusion_matrix(y_val,Y_pred))\nprint(dash)\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"CV Mean:\", scores.mean())\nprint(\"CV Std:\", scores.std())\nprint(\"XGB Accuracy score (training): \", acc_xgb)\nprint(\"XGB Accuracy score (validation): \", acc_xgb_val)","b2e3a276":"# plot logloss learning curve\nplt.plot(results['validation_0']['logloss'], label='train')\nplt.plot(results['validation_1']['logloss'], label='validation')\nplt.title('XGBoost Log Loss')\nplt.legend()\nplt.show()","41b096dd":"# AdaBoost\nadaboost = AdaBoostClassifier()\nadaboost.fit(X_train, y_train)\nY_pred_ada = adaboost.predict(X_test)\nacc_ada = round(adaboost.score(X_train, y_train)*100, 4)\nacc_ada_val = round(adaboost.score(X_val, y_val)*100, 4)","897de75a":"print(dash)\nprint('AdaBoost Confusion Matrix')\nprint(dash)\nprint(confusion_matrix(y_val,Y_pred))\nprint(classification_report(y_val,Y_pred))\nprint(dash)\nprint(\"AdaBoost Accuracy score (training): \", acc_ada)\nprint(\"AdaBoost Accuracy score (validation): \", acc_ada_val)","b24ca7b2":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'Random Forest', 'GBM', 'XGB', 'AdaB'],\n    'Training Score': [acc_log, acc_knn, acc_random_forest, acc_gbm, acc_xgb, acc_ada],\n    'Validation Score': [acc_log_val, acc_knn_val, acc_random_forest_val, acc_gbm_val, acc_xgb_val, acc_ada_val]})\n\nmodels.sort_values(by='Training Score', ascending=False)","8edc7d27":"def test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    n_jobs = -1)\n\n        result_table.loc[row_index, 'CV Mean Accuracy'] = cv_results['test_score'].mean()\n        result_table.loc[row_index, 'CV Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by=['CV Mean Accuracy'], ascending = False, inplace = True)\n\n    return result_table\n\nlr = LogisticRegression()\nknn = KNeighborsClassifier(n_neighbors = 3)\nrf = RandomForestClassifier(random_state = 1)\ngbm = GradientBoostingClassifier(n_estimators=20, random_state=0)\n# xgb = XGBClassifier(n_estimators=4200, eta=0.018, early_stopping_rounds=250, subsample=0.5, colsample_bytree=0.5)\nada = AdaBoostClassifier()\n\nestimators = [lr,\n              knn,\n              rf,\n              gbm,\n              # xgb,\n              ada,\n              ]\n\nlabels = ['Log Regression',\n          'kNN',\n          'Random Forest',\n          'Gradient Boosting', \n          # 'Extreme Gradient Boosting',\n          'AdaBoost',]\n\nresults = test_estimators(X, y, estimators, labels, cv = 10)\nresults.style.background_gradient(cmap = 'Blues')\n","c6fb7cb4":"# The logistic regression model\nlm = LogisticRegression()\nlm.fit(X_train, y_train)\ny_pred_lm = lm.predict_proba(X_val)[:, 1]\nfpr_lm, tpr_lm, _ = roc_curve(y_val, y_pred_lm)\n\n# The KNN model\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict_proba(X_val)[:, 1]\nfpr_knn, tpr_knn, _ = roc_curve(y_val, y_pred_knn)\n\n# The Random Forest model\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict_proba(X_val)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_val, y_pred_rf)\n\n# GradientBoostingClassifier() \ngbm = GradientBoostingClassifier(n_estimators=10)\nmodel = GradientBoostingClassifier(n_estimators=10) \ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\ngbm.fit(X_train, y_train)\ny_pred_gbm = gbm.predict_proba(X_val)[:, 1]\nfpr_gbm, tpr_gbm, _ = roc_curve(y_val, y_pred_gbm)\n\n# The XGB model\nxgb = XGBClassifier(n_estimators=5000, eta=0.02, subsample=0.5, colsample_bytree=0.5)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict_proba(X_val)[:, 1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_val, y_pred_xgb)\n\n# The AdaBoost Model\nada = AdaBoostClassifier()\nada.fit(X_train, y_train)\ny_pred_ada = ada.predict_proba(X_val)[:, 1]\nfpr_ada, tpr_ada, _ = roc_curve(y_val, y_pred_ada)","a11b31e0":"plt.subplots(figsize=(12,8))\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_lm, tpr_lm, label='LogReg')\nplt.plot(fpr_rf, tpr_rf, label='RandFor')\nplt.plot(fpr_knn, tpr_knn, label='kNN')\nplt.plot(fpr_ada, tpr_ada, label='AdaBoost')\nplt.plot(fpr_gbm, tpr_gbm, label='GBM')\nplt.plot(fpr_xgb, tpr_xgb, label='XGB')\n\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.title('ROC Curve', fontsize=20)\nplt.legend(loc='best')\nplt.show()","e76b2307":"# random_forest = RandomForestClassifier(n_estimators=100)\n# random_forest.fit(X_train, y_train)\n# Y_pred = random_forest.predict(X_test)\n\nXGB = XGBClassifier(n_estimators=4200, eta=0.018, subsample=0.5, colsample_bytree=0.5)\nXGB.fit(X, y, eval_metric='logloss', verbose=False)\nY_pred = XGB.predict(X_test)","f2591e64":"output = pd.DataFrame({'PassengerId': testing.PassengerId,\n                       'Survived': Y_pred})\noutput.to_csv('submission.csv', index=False)","b38c77af":"### Generate and Plot ROC Comparison Curves","9bde5d40":"We'll combine low frequency titles into the created miscellaneous category to simplify along with a few others recategroized.","4cb3f0f4":"Review of scatterplot, boxplot and distribution plot show the age feature is basically normal with a slight right skew for both the training and testing datasets. Seems reasonable to use the column median to replace NaNs.","61fff09b":"Prior Feature Importance (Checkpoint 1) was prior to creating the Title, Age*PClass, and IsAlone features and scaling other categorical to numeric. We went from 'Fare', 'Sex', 'Age' features intitally dominating with equal importance to the newly created 'Title' feature then 'Sex', and 'PClass' with decreasing importance. This shows the newly created fields add value and exisiting Fare & Age features less important post conversion.","33be824d":"The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 68% of the passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. Specifically, the women, children and upper-class passengers.","dfc18627":"# Train Model and Predict\u00b6\n\nNow we are ready to train models and predict survivability of the test set. There are many predictive modeling algorithms available. Our problem is a supervised binary classification problem. Supervised since we're using labeled data. We want to predict a binary output between Survived or Didn't-Survived using dataset features. Supervised Learning and Binary Classification narrow down model selection. Several are selected below. There are others available.\n\nThese include:  \nLogistic Regression  \nKNN or k-Nearest Neighbors    \nRandom Forest  \nGradient Boosting Machine (GBM)  \nExtreme Gradient Boosting (XGBoosting)\n\nWe'll review results.score(X_train, y_train) as the training accuracy, while   \naccuracy_score(y_val, results.predict(X_val)) is based on the valuation dataset created.","97b1ca4c":"### Let's create new Age*PClass Feature","44d51a7d":"### Clear the Random Forest and XGBoosting better at False Positive Rate Range of 0.15 to 0.25.\n\n### ROC Results\n\nA Receiver Operator Characteristic (ROC) curve is a graphical plot used to show the diagnostic ability of binary classifiers. It was first used in signal detection theory but is now used in many other areas such as medicine, radiology, natural hazards and machine learning. The ROC curve is the plot between the True Positive rate and the False positive rate (aka Sensitivity and the (1-Specificity)).\n\n To compare different classifiers, it can be useful to summarize the performance of each classifier into a single measure. One common approach is to calculate the area under the ROC curve (AUC). It is equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n\nRandom Forest had the highest accuracy score based on the training dataset. The ROC plot shows kNN lower and almost linear between False positive rate of [0.2 and 0.8]. Random Forest and XGB appear to have a better AUC overall. \n\nRandom Forest vs Gradient Boosting  \nLike random forests, gradient boosting is a set of decision trees. The two main differences are:  \nHow trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners.   \nCombining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way.\nIf you carefully tune parameters, gradient boosting can result in better performance than random forests. However, gradient boosting may not be a good choice if you have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than random forests.\n\nRandom forests and gradient boosting each excel in different areas. Random forests perform well for multi-class object detection and bioinformatics, which tends to have a lot of statistical noise. Gradient Boosting performs well when you have unbalanced data such as in real time risk assessment.","dec788b1":"### Create Datasets for Variable Importance and Feature Engineering Improvements\nWe create the Evaluation datasets to compare model accuracy scores at this point in process with the additional feature engineering below to quantify the benefit of additional feature engineering.","b6f8721a":"Review of the 2 datasets indicate \"training.age\", \"training.cabin\", \"training.embarked\", \"testing.age\", \"testing.fare\", and \"testing.cabin\" features contain NaNs. Tables show shape of datasets (891 x 12) and (418 x 11). In addition, table shows data types of each feature.","1d23988c":"## Addressing the Missing Data (NaNs)\nMost machine learning algorithms like numeric inputs, as well as, a value present for each row and column in a dataset. Missing values can cause problems for machine learning algorithms. It's common to identify missing values and replace with a numeric value. This is data imputation.\n\nA simple popular approach involves statistical methods to calculate a value for a feature column from those values present, then replace the missing values with the calculated statistic.\nCommon statistics include:\n    The column: mean, median, mode, or other constant value.\n\nRecall that the training cabin variable has 77% missing values and 78% missing in the testing dataset. We'll remove the cabin feature. The most frequent value (mode) for the 'Embarked' variable is \"S\" and this will be used to replace the 2 NaNs in the training dataset. The training dataset 'Ticket' feature has 681 unique values and will also be removed. There is one NA in the Fare variable of the testing dataset and the median will be used to replace that value.\n\n#### We'll review distribution of the Age feature to determine best statistic for filling missing data.","d2766cf8":"### Extreme Gradient Boosting (XGBoosting)\n\nBoosting algorithms build a weak model(i.e. predictions slightly >50%), making conclusions about the various feature importance and parameters, and then using those conclusions to build a new, stronger model and capitalize on the misclassification error of the previous model and try to reduce it. XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost and gradient boosting machines are both ensemble tree methods that apply the principle of boosting weak learners using the gradient descent architecture. However, XGBoost improves upon the base gradient boosting framework through systems optimization and algorithmic enhancements  \nsuch as:\n - hardware optimization & parallelized tree building\n - efficient handling of missing data and tree pruning using \u2018depth-first\u2019 approach\n - built-in cross-validation capability (at each iteration)  \n - regularization through both LASSO (L1) and Ridge (L2) for avoiding overfitting\n\nfor additional information about the XGB model and tuning visit https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\n\n\nLet's fit the XGBoost model, retrieve the calculated metrics, and plot learning curve.\nYou can experiment with Hyperparamaters (e.g. Learning Rate = eta value, estimators); change and see impact on curve!","ff29afdb":"## Create new features combining existing values\nLet's create new FamilySize feature by combining Parch and SibSP from our datasets and then drop Parch and SibSp. ","a058197f":"Let's compare someone alone to not alone\n","56a6720d":"### Potential follow up activity\nCompare and manipulate feature engineering datasets and compare results.  \nCheck alternatives for age NA imputing.  \nTry different bands for split intervals \"AgeBand\"  \nCompare One Hot Encoding to mapping changes for Categorical field change to numeric; Embarked, Sex  \nEvaluate additional scaling techniques.  \nHyperparameter Tuning for the RF & XGB models  \n\n\n### Submit Selected Model Results Below","d602a7a4":"### K-Nearest Neighbors (kNN)\nIn pattern recognition, the k-Nearest Neighbors algorithm is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference Wikipedia.","40e337b7":"Most males with \"Mr\" title did not survive, but most with Master did. Most female, Mrs and Miss, titles did survive. \n\n\n#### Convert the categorical 'Title' to ordinal for our analysis. ","fed750e9":"### Inspired By:  \nSabarish Sridhar - https:\/\/www.kaggle.com\/dudewhat\/titanic-with-random-forest-top-7  \nManav Sehgal - https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions  \nJason Brownlee - https:\/\/machinelearningmastery.com\/tune-xgboost-performance-with-learning-curves\/  \nSKLearn - https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_feature_transformation.html#sphx-glr-download-auto-examples-ensemble-plot-feature-transformation-py  \nTowardsDataScience - https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74  \nError - https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/11-important-model-evaluation-error-metrics\/?utm_source=blog&utm_medium=decision-tree-vs-random-forest-algorithm  \nROC - https:\/\/www.displayr.com\/what-is-a-roc-curve-how-to-interpret-it\/    ","0aa60efd":"#### Now let's drop 'Name' and 'PassengerID' features from training dataset and 'Name' from testing dataset.","a89a2ddd":"We see that the Fare, Sex, and Age features are relatively close in higher importance and the remaining features ranked less important. We'll compare this to after additional feature engineering is performed. \n\n### Continue feature engineering on training and testing datasets\nRecall the Name field had a title available for viewed records. We can extract to review potential of adding a Title feature.","b66b0210":"Confirmed high percentage of cabin variable missing for both datasets. Age is missing 20% for both datasets and others 0.2%.\n\n### Correlations of the training dataset. \n\nBivariate Analysis\nBivariate analysis tries to find the relationship between two variables. We look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. Different methods are used to tackle these combinations during analyses. The methods are:\n\n- Numerical & Numerical: Pearson's correlation, or Spearman correlation (doesn't require normal distribution). \n- Numerical & Categorical: Point biserial correlation (only if categorical variable is binary type), or ANOVA test. For this problem, you can use either biserial correlation or ANOVA. ANOVA comes in handy if categorical variable has more than two classes. \n- Categorical & Categorical: We can use Chi-square test for bivariate analysis between categorical variables.\n\nMultivariate Analysis\nAttempts to find the relationship among more than two variables. Number of predictor variable in bivariate analysis is one. On the contrary, number of predictor variables for multivariate analysis are more than one. We try to associate more than one predictor variable with the response variable.\n\nWe'll begin with visuals and different table views to identify relationships amongst the data.### A word on Correlations\n\n#### Bivariate Analysis \nBivariate analysis tries to find the relationship between two variables. We look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. Different methods are used to tackle these combinations during analyses. The methods are:  \n\nNumerical & Numerical: Pearson's correlation, or Spearman correlation (the later doesn't require normal distribution).\nNumerical & Categorical: Point biserial correlation (only if categorical variable is binary type), or ANOVA test. For this problem, you can use either biserial correlation or ANOVA. ANOVA comes in handy if categorical variable has more than two classes.\nCategorical & Categorical: We can use Chi-square test for bivariate analysis between categorical variables.\n\n#### Multivariate Analysis\nWe try to find the relationship among more than two variables. Number of predictor variable in bivariate analysis is one. On the contrary, number of predictor variables for multivariate analysis are more than one. More specifically, we try to associate more than one predictor variable with the response variable. \n\nThis analysis begins with visuals and views of grouping to identify relationships amongst the data. \n","e4095604":"### Initial Feature Importance (Random Forest) Plot - Checkpoint #1\n\nThe feature importance plot describes which features are relevant. It can help with better understanding of the problem and sometimes lead to model improvements. Random Forest is used to make decision on how to divide the data set into two separate sets with similar response. The features for internal nodes are selected, which for classification tasks can be gini impurity or infomation gain. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. ","f8243af2":"Women survival higher across all passenger classes and most ages. Men survival higher for First Class compared to Class 2 and Class 3. ","9c66e9aa":"Let's use IsAlone and drop Parch, SibSp, and FamilySize.","8f4b596e":"We see most without Sibling or Spouse (i.e. Alone) perish. ","c272bc8f":"Describe (include = \"all\") option provides Count, Unique, Top, Freq info for the 4 object categories in addition to the quartile statistics for the Int64 and float64 Dtype features. We see that 681 of the 891 tickets are unique for the training dataset. For the Cabin feature, 147 of the 204 cabin values are unique, but 77%-78% are missing. Likely we can drop these two features without significant impact on analysis. \n\n### Assess missing data.  ","9fbc434d":"### Logistic Regression\nLogistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference Wikipedia.\n\nNote the confidence score generated by the model based on our training dataset.","c47b7f92":"### We'll review distribution of the Age feature of Test Dataset to confirm consistent for filling missing data.","a328023f":"## Begin Journey ... Load Libraries, Modules, and Data","c85ef362":"### Create Feature Importance Plots for Comparison","5b919bed":"### Random Forest\nRandom Forests is one of the most popular and a personal favorite. Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference Wikipedia.","56319072":"Let's create FareBand similar to AgeBand methodology and categorize per band.","d3a1c943":"As previously mentioned, we will drop the Cabin and Ticket features. Let's confirm shape after Cabin and Ticket features removed.","59532b2b":"## Titanic EDA, Feature Engineering, and ML Model Evaluations","cc173fcd":"## Boosting Algorithms \nplay a crucial role in dealing with bias variance trade-off.  Unlike bagging algorithms, which only controls for high variance in a model, boosting controls both the aspects (bias & variance), and is considered to be more effective.  \nBoosting is a sequential technique which works on the principle of ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. \n\n\n### Gradient Boosting \nbuilds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. Gradient boosting can be challenging to configure as the algorithm as many key hyperparameters that influence the behavior of the model on training data and the hyperparameters interact with each other. Reasonable rresults found at the default settings, but plan to explore optimal settings. For additional information about GBM and tuning the model visit https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/","8cd1d7fb":"#### Create Age Bands","76668bb0":"### Convert categorical, strings, to numeric values\nThis assists many algorithms. Let's start with the 'Embarked' and 'Sex' features. Making 'Sex' numeric also introduces a strong correlation between the 'Survived' with 'Sex' features.","d966902d":"## Model Comparison"}}