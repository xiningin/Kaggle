{"cell_type":{"6d4c5a88":"code","8eb389d5":"code","0a998596":"code","fa75984a":"code","15c43216":"code","62302c71":"code","a42aa45b":"code","e6093bd6":"code","6463305b":"code","27cbc4b4":"code","3a5b756e":"code","3b4f9f3c":"code","27422f84":"code","343a7f33":"code","eaac3689":"code","0629c9bc":"code","89c7e4e8":"markdown","f9e396df":"markdown","94c8b5aa":"markdown"},"source":{"6d4c5a88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom collections import Counter\nfrom pandas_profiling import ProfileReport \nfrom wordcloud import  STOPWORDS\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport random\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8eb389d5":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nss = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","0a998596":"print(train.shape)\nprint(test.shape)","fa75984a":"report = ProfileReport(train)\nreport","15c43216":"train.dropna(inplace=True)\n","62302c71":"train.head()\n","a42aa45b":"train.sentiment.value_counts(normalize=True)*100","e6093bd6":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n","6463305b":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","27cbc4b4":"train.head()\n","3a5b756e":"train['text'].values","3b4f9f3c":"# integer encode the documents\nvocab_size = 10000\nencoded_docs = [one_hot(d, vocab_size) for d in train['selected_text'].values]\nprint(encoded_docs[0])","27422f84":"# pad documents to a max length of 4 words\nmax_length = 4+6\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\nprint(padded_docs[0])","343a7f33":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())","eaac3689":"print(model.predict(padded_docs))\n","0629c9bc":"print(model.predict(padded_docs)[0])\n","89c7e4e8":"most comments are neutral followed by positive and then negative..","f9e396df":"since we can see that words have now been converted to vectors , we can use this representation for prediction purposes . this will be the goal for next notebook ..","94c8b5aa":"acknowledgements :- https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model,\nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/"}}