{"cell_type":{"26c9b46b":"code","9ff6ae63":"code","a0473af2":"code","b7b8aeba":"code","467738d5":"code","7515b663":"code","6e251c7d":"code","9a24cd62":"code","952f7831":"code","45a6c6d6":"code","033b925f":"code","89fef68a":"code","a54c922e":"code","b75c4d0f":"code","2a83a080":"code","8fb2632c":"code","f291cbf6":"code","6a80164d":"code","8555f998":"code","1ec4b714":"code","8a700221":"code","13f8ac2b":"markdown","5ed9fb2f":"markdown","f60ac776":"markdown","f4e063c0":"markdown","8523309e":"markdown","49af7fc5":"markdown","87aa95ba":"markdown","a23d3fad":"markdown","9a8b6e27":"markdown","e6eb0834":"markdown","03c89b3f":"markdown"},"source":{"26c9b46b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","9ff6ae63":"concrete_df = pd.read_csv('..\/input\/concrete_data.csv')\nconcrete_df.head()","a0473af2":"X = concrete_df.drop('csMPa', axis=1)\nY = concrete_df['csMPa']","b7b8aeba":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","467738d5":"from sklearn.tree import DecisionTreeRegressor","7515b663":"# First weak learner \ntree_reg1 = DecisionTreeRegressor(max_depth=3)\ntree_reg1.fit(X_train, y_train)","6e251c7d":"# Residual Error from the first decision tree\ny2 = y_train - tree_reg1.predict(X_train)\ny2[:10]","9a24cd62":"# Second weak learner \ntree_reg2 = DecisionTreeRegressor(max_depth=4)\n\n'''\ny2: the residial error of the previous Decision Tree\n'''\ntree_reg2.fit(X_train, y2)","952f7831":"# Residual error of the second weak learner\ny3 = y2 - tree_reg2.predict(X_train)\ny3[:10]","45a6c6d6":"# Third weak learner\ntree_reg3 = DecisionTreeRegressor(max_depth=4)\ntree_reg3.fit(X_train, y3)","033b925f":"y4 = y3 - tree_reg3.predict(X_train)\ny4[:10]","89fef68a":"y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, \n                                               tree_reg2, \n                                               tree_reg3))","a54c922e":"from sklearn.metrics import r2_score","b75c4d0f":"r2_score(y_test, y_pred)","2a83a080":"#Additional learner\ntree_reg4 = DecisionTreeRegressor(max_depth=4)\ntree_reg4.fit(X_train, y4) ","8fb2632c":"y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, \n                                               tree_reg2, \n                                               tree_reg3,\n                                               tree_reg4))\n\nr2_score(y_test, y_pred)","f291cbf6":"from sklearn.ensemble import GradientBoostingRegressor","6a80164d":"gbr = GradientBoostingRegressor(max_depth=3, \n                                n_estimators=3,\n                                learning_rate=1.0)\n\ngbr.fit(X_train, y_train)","8555f998":"y_pred = gbr.predict(X_test)\nr2_score(y_test, y_pred)","1ec4b714":"gbr = GradientBoostingRegressor(max_depth=3, \n                                n_estimators=3,\n                                learning_rate=0.1)\n\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nr2_score(y_test, y_pred)","8a700221":"gbr = GradientBoostingRegressor(max_depth=3, \n                                n_estimators=40,\n                                learning_rate=0.1)\n\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nr2_score(y_test, y_pred)","13f8ac2b":"**Partial conclusion**:  \nOur model has a performance of 79%. This performance should improve if we add more learners in the sequence.  \nWe have so far 3 weak learns in our sequence. \nLet's add a new one in the sequence to see is the performance of the ensemble will increase or not.  ","5ed9fb2f":"**Note**: It is important to know that if you reduce the learning rate, you must increase the number of estimators (weak learners) in order to have a better prediction value.  \nLet's see the two cases.  \n\n* Case n\u00b01: reduce learning_rate","f60ac776":"We can see that the predictions get better as additional trees are added to the ensemble.","f4e063c0":"We see that the prediction performance has reduced.  \n\n* Case n\u00b02: reduce learning_rate, increase the number of weak learners","8523309e":"## II- Process using GradientBoostingRegressor\nInstead of manually calculating our weak learners, we are going to use the **GradientBoostingRegressor**, and that is what happens in the real world. ","49af7fc5":"Here, we can see the gap of improvement of the performance","87aa95ba":"# Understand Gradient Boosting using regression.  \nIn this notebook, you will understand how **Gradient Boosting** works. This is an ensemble technic that uses a sequence of weak learners, and each one of them in the sequence learns from the residual errors of the previous weak learner.  \nYour will first understand it by manual process, then use the GrandientBoostingRegressor.  ","a23d3fad":"The weak learners we will make up here are all Decision Tree Regressors. This is just for convenience. We could use other regression models such as Support Vector Regressors for example. ","9a8b6e27":"## I- Manually mimic Gradient Boosting  \nAs said in the introduction, the Gradient Boosting uses a sequence of multiple weak learners. In this case, we will use only three weak learners (Decision Trees). In the sequence, the next will learn from the mistakes (residual errors) of the previous one. ","e6eb0834":"Now that we have our three weak learners in the sequence, we can finally calculate the final score. ","03c89b3f":"Remember that **y4** is the residual error from the 3rd weak learner.  \nNow, let's get the final prediction. "}}