{"cell_type":{"34bdcfc0":"code","91ed7d99":"code","9ac9b3af":"code","4986b65f":"code","efa855fa":"code","320c8bd0":"code","09137084":"code","1314a5b4":"code","9c0ab3b5":"code","54bed19a":"code","404a562a":"markdown","79f68938":"markdown","80e647f6":"markdown","5f010cc3":"markdown","03515b5d":"markdown","fd830d07":"markdown","00568a7a":"markdown","7e3fc5c8":"markdown","176309dd":"markdown","5707fce9":"markdown","8160b088":"markdown","28ea171a":"markdown","9474279e":"markdown","4c78724b":"markdown","0ec95a88":"markdown","e27ac11d":"markdown"},"source":{"34bdcfc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91ed7d99":"! pip install git+https:\/\/github.com\/openai\/CLIP.git\n! pip install imutils","9ac9b3af":"import torch\nimport clip\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport glob, random\nimport imutils, os, time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport plotly.express as px\nfrom tqdm import tqdm","4986b65f":"train_path='..\/input\/intel-image-classification\/seg_train\/seg_train'\ntest_path='..\/input\/intel-image-classification\/seg_test\/seg_test'","efa855fa":"def plotImagesAndLabels(folder):\n    c=1\n    directory=os.listdir(folder)\n    plt.figure(figsize=(28,20))\n    for each in directory:\n        currentFolder=folder+ \"\/\" +each\n        for i, file in enumerate(os.listdir(currentFolder)[0:4]):\n            full_path=currentFolder+\"\/\"+file\n            plt.subplot(3, 8, c)\n            img = mpimg.imread(full_path)\n            plt.imshow(img)\n            plt.title(each)\n            c+=1\n    plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n    plt.show()","320c8bd0":"plotImagesAndLabels(test_path)","09137084":"def get_ramdom_path():\n        file_path = '..\/input\/intel-image-classification\/seg_test\/seg_test\/'\n\n        first_path = os.listdir(file_path)\n        class_path = random.choice(first_path)\n        rfile = os.path.join(file_path, class_path)\n        img_list = os.listdir(rfile + '\/')\n        sel_img = random.choice(img_list)\n        rfile= rfile+'\/'+ sel_img\n        return class_path, rfile","1314a5b4":"def clip_predict_label(rfile):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        model_use = \"ViT-B\/32\"\n        # model_use = \"RN50x16\" # ViT-B\/32, RN50x16\n        model, preprocess = clip.load(model_use, device=device)\n        image = preprocess(Image.open(rfile)).unsqueeze(0).to(device)\n\n        class_list = ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']\n\n        text = clip.tokenize(class_list).to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            text_features = model.encode_text(text)\n\n            logits_per_image, logits_per_text = model(image, text)\n            probs = np.array(logits_per_image.softmax(dim=-1).cpu().numpy())\n\n        label = np.argmax(probs[0])\n        #print(probs[0][label])\n\n        return class_list[label], rfile\n    \ndef clip_predict_discription(rfile):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        model_use = \"ViT-B\/32\"\n        # model_use = \"RN50x16\" # ViT-B\/32, RN50x16\n        model, preprocess = clip.load(model_use, device=device)\n        image = preprocess(Image.open(rfile)).unsqueeze(0).to(device)\n\n        class_list = ['a photo of a mountain', 'a photo of a street', 'a photo of a buildings', 'a photo of a sea', 'a photo of a forest', 'a photo of a glacier']\n\n        text = clip.tokenize(class_list).to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            text_features = model.encode_text(text)\n\n            logits_per_image, logits_per_text = model(image, text)\n            probs = np.array(logits_per_image.softmax(dim=-1).cpu().numpy())\n\n        label = np.argmax(probs[0])\n        #print(probs[0][label])\n\n        return class_list[label], rfile","9c0ab3b5":"if __name__ == \"__main__\":\n    labels = 0\n    discrip = 0\n\n    for i in tqdm(range(1000)):\n        gt, rfile = get_ramdom_path()\n        \n        predict_label, rfile_label = clip_predict_label(rfile)\n        predict_discrip, rfile_discrip = clip_predict_discription(rfile)\n        \n        if (i+1) % 100 == 0:\n            print('stage: ', i+1)\n            print(\"Ground Truth :\", gt)\n            print(\"Predict Class label : \", predict_label)\n            print(\"Predict Class discription : \", predict_discrip, \"\\n\")\n\n        if gt == predict_label:\n            labels+=1\n        if gt == predict_discrip[13:]:\n            discrip+=1","54bed19a":"print(\"Only Label Prediction Accurcy:\", labels\/1000)\nprint(\"Image Discription Prediction Accurcy:\", discrip\/1000)","404a562a":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nImport Libraries\n<\/h1>\n<\/div>\n<\/div>","79f68938":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nGetting Image from Dataset\n<\/h1>\n<\/div>\n<\/div>","80e647f6":"**\"clip_predict_label\" function will only contain labels.  \n\"clip_predict_discription\" function will have labels in form of \"a photo of a {object}\" which is a discription of an image.**","5f010cc3":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/111880\/269359\/a16c143f44e79d17f54d5e670f16e03b\/dataset-cover.jpg?t=2019-02-01-19-30-12\" alt=\"\">","03515b5d":"## If you like this notebook then please give an upvote \ud83d\udc4d","fd830d07":"<img src= \"https:\/\/syncedreview.com\/wp-content\/uploads\/2021\/05\/openai-cover.png\" alt =\"openai\" style='width: 500px;'>","00568a7a":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nTesting Accuracy \"Only Label\" VS \"With Image Discription\"\n<\/h1>\n<\/div>\n<\/div>","7e3fc5c8":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nInstall OpenAI's CLIP from Github\n<\/h1>\n<\/div>\n<\/div>","176309dd":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nIntel Image Classification : Comparison of Performance using OpenAI's CLIP\ud83d\udcce\n<\/h1>\n<\/div>\n<\/div>","5707fce9":"## Our Task\n**The zero-shot learning results for ImageNet were very high at 76.2%.**  \n**On top of that, a little trick could further enhance recognition performance.**  \n**The recognition rate was slightly higher when it was clearly stated that it wanted to recognize objects in the image, such as \"a photo of a {object}\", instead of using only class labels.**\n\n**In this work, we will test accuracy between using only label VS using description of an image such as \"a photo of a {object}\"**","8160b088":"**Looks like using image discription as a label really enhance recognition performance!!**  \n**\ud83d\udcccAccuracy 94.8% is super awesome\ud83d\udcaf**","28ea171a":"## About CLIP\n**CLIP is a multimodal neural network that trains on image and text pairs.**  \n**Without the labeling process for images, images and associated natural language text were extracted through web crawl to build and learn datasets for 400 million images and text pairs.**  \n**It consists of image transformer encoders and text transformer encoders and learns the connection relationship between images and text feature vectors that came through each encoder.**  \n**This means that images and text can be encoded into virtually the same space, effectively representing the simplicity between images and text.**  \n\n**Due to these characteristics, we can use CLIP as a zero-shot prediction model.**","9474279e":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nLoad Data\n<\/h1>\n<\/div>\n<\/div>","4c78724b":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nModel Inference using CLIP\n<\/h1>\n<\/div>\n<\/div>","0ec95a88":"<img src= \"https:\/\/miro.medium.com\/max\/3662\/1*tg7akErlMSyCLQxrMtQIYw.png\" alt =\"clip_Arch\" style='width: 1200px;'> ","e27ac11d":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nAbout OpenAI's CLIP\n<\/h1>\n<\/div>\n<\/div>"}}