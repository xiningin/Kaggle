{"cell_type":{"1ad3926d":"code","f67ea808":"code","8ee80f2b":"code","6f7d8fad":"code","528912c2":"code","3f5e3da1":"code","a9cfef70":"code","08bdee66":"code","394dd33a":"code","5ab3adc1":"code","9e2a867e":"code","5467ef69":"code","a7bb0bb9":"code","7d895ba3":"code","fc7e6511":"code","3019b7ca":"code","be5171f3":"code","39890275":"code","fa90d2c6":"code","2896a5ca":"code","646d7988":"code","77a5d94f":"code","b123697c":"code","57281069":"code","f58d4654":"code","4b40035d":"code","652703cd":"code","66632bb4":"code","2b2f5eef":"code","8f4ea88c":"code","89c312f1":"code","325416dc":"code","b80ff0ef":"code","d44ecb50":"code","ac1c3ae5":"code","3e81d4a4":"code","9c243332":"code","94564fc1":"code","1fe0bd15":"code","e3d954c7":"code","2f26e3b6":"code","bd03d07f":"code","cb73a097":"code","181eb5e9":"code","bb3b15b3":"code","21235656":"code","691ead08":"code","ec04422d":"code","1a672378":"code","d4ac6eb6":"code","6fe58305":"code","e47779bd":"code","211e9f78":"code","4c4bea44":"code","1a4b9dfa":"code","74736095":"code","5b500623":"code","54372a71":"code","02051984":"code","48119f94":"code","8ce3867a":"code","011264a3":"code","4be40b28":"code","280b4970":"code","bce0b33f":"code","41c34b11":"code","3c4bfe83":"code","8f036dcb":"code","3deae20b":"code","5ddfd6d0":"code","cca5cf70":"code","617149ff":"code","5eb92ebf":"markdown","5eb25fbd":"markdown","79a41c0e":"markdown","97a44afd":"markdown","5e0a0a8a":"markdown","5e7dbfb0":"markdown","f9c2bd2f":"markdown","ed80d249":"markdown","1d24b741":"markdown","0afa25a5":"markdown","93dd0569":"markdown","40a8a55f":"markdown","308faf26":"markdown","996576b2":"markdown","e6529ec2":"markdown"},"source":{"1ad3926d":"import re\n\n# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for visualization\nimport matplotlib.pyplot as plt\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# to build the models\nfrom sklearn.linear_model import LogisticRegression\n\n# to evaluate the models\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# to persist the model and the scaler\nimport joblib\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","f67ea808":"# load the data - it is available open source and online\n\ndata = pd.read_csv('..\/input\/phpMYEkMl.csv')\n\n# display data\ndata.head()","8ee80f2b":"#getting to know all the columns\ndata.columns","6f7d8fad":"#shape of the data\ndata.shape","528912c2":"#analysing cabin columns\ndata['cabin'].values[:5]","3f5e3da1":"type(data['cabin'].values[0])","a9cfef70":"# replace interrogation marks by NaN values\n\ndata = data.replace('?', np.nan)","08bdee66":"# retain only the first cabin if more than\n# 1 are available per passenger\n\ndef get_first_cabin(row):\n    try:\n        return row.split()[0]\n    except:\n        return np.nan\n    \ndata['cabin'] = data['cabin'].apply(get_first_cabin)","394dd33a":"# extracts the title (Mr, Ms, etc) from the name variable\n\ndef get_title(passenger):\n    line = passenger\n    if re.search('Mrs', line):\n        return 'Mrs'\n    elif re.search('Mr', line):\n        return 'Mr'\n    elif re.search('Miss', line):\n        return 'Miss'\n    elif re.search('Master', line):\n        return 'Master'\n    else:\n        return 'Other'\n    \ndata['title'] = data['name'].apply(get_title)","5ab3adc1":"# cast numerical variables as floats\n\ndata['fare'] = data['fare'].astype('float')\ndata['age'] = data['age'].astype('float')","9e2a867e":"# drop unnecessary variables\n\ndata.drop(labels=['name','ticket', 'boat', 'body','home.dest'], axis=1, inplace=True)\n\n# display data\ndata.head()","5467ef69":"# save the data set\n\ndata.to_csv('titanic.csv', index=False)","a7bb0bb9":"data = pd.read_csv(\"titanic.csv\")\ndata.head()","7d895ba3":"data.info()","fc7e6511":"vars_num =['survived','age','sibsp','parch','fare'] # fill your code here\nvars_cat =['pclass','sex','cabin','embarked','title'] # fill your code here","3019b7ca":"# for column in data.columns:\n#     if(data[column].dtype == object):\n#         vars_cat.append(column)\n#     else:\n#         vars_num.append(column)","be5171f3":"target = 'survived'","39890275":"data.columns","fa90d2c6":"print('Number of numerical variables: {}'.format(len(vars_num)))\nprint('Number of categorical variables: {}'.format(len(vars_cat)))","2896a5ca":"vars_num","646d7988":"vars_cat","77a5d94f":"# first in numerical variables\ndata_num = data[vars_num]\ndata_num.head()","b123697c":"len(data)","57281069":"data_num.isna().mean()","f58d4654":"# now in categorical variables\ndata_cat = data[vars_cat]\ndata_cat.head()","4b40035d":"data_cat.isna().mean()","652703cd":"def analyse_cat(df, var):\n    df = df.copy()\n    df[var].value_counts().plot.bar()\n    plt.title(var)\n    plt.xlabel('attributes')\n    plt.ylabel('No of passengers')\n    plt.show()","66632bb4":"for col in vars_cat:\n    print(25*'-'+col+'-'*25)\n    print(data[col].value_counts())\n    print(len(data[col].value_counts()))\n    analyse_cat(data, col)","2b2f5eef":"def analyse_continuous(df, var):\n    df = df.copy()\n    df[var].hist(bins=30)\n    plt.ylabel('Number of passengers')\n    plt.xlabel(var)\n    plt.title(var)\n    plt.show()","8f4ea88c":"for col in vars_num:\n    analyse_continuous(data, col)","89c312f1":"X_train, X_test, Y_train, Y_test = train_test_split(\n    data.drop('survived', axis=1),  # predictors\n    data['survived'],  # target\n    test_size=0.2,  # percentage of obs in test set\n    random_state=0)  # seed to ensure reproducibility\n\nX_train.shape, X_test.shape","325416dc":"X_train.head()","b80ff0ef":"Y_train.value_counts()","d44ecb50":"Y_train[:5]","ac1c3ae5":"X_train[vars_cat].head()","3e81d4a4":"vars_num =['age','sibsp','parch','fare']","9c243332":"X_train[vars_num].head()","94564fc1":"#for numerical values\nX_train[vars_num].isna().sum()","1fe0bd15":"for var in vars_num:\n    X_train[var].fillna(data[var].median(),inplace=True)\n    X_test[var].fillna(data[var].median(),inplace=True)","e3d954c7":"X_train[vars_num].isna().sum()","2f26e3b6":"#for categorical values\nX_train[vars_cat].isna().sum()","bd03d07f":"X_train[vars_cat] = X_train[vars_cat].fillna('missing')\nX_test[vars_cat] = X_test[vars_cat].fillna('missing')","cb73a097":"X_train[vars_cat].isna().sum()","181eb5e9":"X_train.isna().sum()","bb3b15b3":"X_test.isna().sum()","21235656":"X_train.cabin.value_counts()","691ead08":"X_train['cabin'] = X_train['cabin'].apply(lambda x: 'missing' if x == 'missing' else x[0])\nX_test['cabin'] = X_test['cabin'].apply(lambda x: 'missing' if x == 'missing' else x[0])","ec04422d":"X_train.cabin.value_counts()","1a672378":"X_test.cabin.value_counts()","d4ac6eb6":"vars_cat","6fe58305":"for col in vars_cat:\n    print(25*'-'+col+'-'*25)\n    print(X_train[col].value_counts())\n    print(25*'.')\n    print(X_test[col].value_counts())\n    print(len(X_train[col].value_counts()))","e47779bd":"temp = X_train.sex.value_counts() \nlist(temp[temp > 200].index)","211e9f78":"def find_frequent_labels(df, var, rare_perc):\n    \n    # function finds the labels that are shared by more than\n    # a certain % of the houses in the dataset\n\n    df = df.copy()\n    temp = df[var].value_counts() \/ len(df)\n    return list(temp[temp > rare_perc].index)\n\n\n\nfor var in vars_cat:\n    \n    # find the frequent categories\n    frequent_ls = find_frequent_labels(X_train, var, 0.05)\n    \n    # replace rare categories by the string \"Rare\"\n    X_train[var] = np.where(X_train[var].isin(frequent_ls), X_train[var], 'Rare')\n    \n    X_test[var] = np.where(X_test[var].isin(frequent_ls), X_test[var], 'Rare')","4c4bea44":"for col in vars_cat:\n    print(25*'-'+col+'-'*25)\n    print(X_train[col].value_counts())\n    print(25*'.')\n    print(X_test[col].value_counts())\n    print(len(X_train[col].value_counts()))","1a4b9dfa":"from sklearn.preprocessing import OneHotEncoder","74736095":"X_train = pd.get_dummies(X_train, columns=vars_cat, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=vars_cat, drop_first=True)","5b500623":"X_train.columns","54372a71":"X_test.columns","02051984":"X_train.head()","48119f94":"X_train.drop(columns=['embarked_Rare'],inplace=True)","8ce3867a":"len(X_train.columns)","011264a3":"len(X_test.columns)","4be40b28":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","280b4970":"X_train.head()","bce0b33f":"X_train = pd.DataFrame(scaler.fit_transform(X_train),columns = X_train.columns)\nX_train.head()","41c34b11":"X_test = pd.DataFrame(scaler.fit_transform(X_test),columns = X_test.columns)\nX_test.head()","3c4bfe83":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score","8f036dcb":"logreg = LogisticRegression(C=0.05,n_jobs=-1 ,random_state=0)\nlogreg.fit(X_train, Y_train)","3deae20b":"Y_pred = logreg.predict(X_test)","5ddfd6d0":"print(confusion_matrix(Y_test,Y_pred))","cca5cf70":"print(accuracy_score(Y_test,Y_pred))","617149ff":"print(roc_auc_score(Y_test,logreg.predict_proba(X_test)[:,1]))","5eb92ebf":"### Determine the distribution of numerical variables","5eb25fbd":"## Train the Logistic Regression model\n\n- Set the regularization parameter to 0.0005\n- Set the seed to 0","79a41c0e":"## Feature Engineering\n\n### Extract only the letter (and drop the number) from the variable Cabin","97a44afd":"## Predicting Survival on the Titanic\n\n### History\nPerhaps one of the most infamous shipwrecks in history, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 people on board. Interestingly, by analysing the probability of survival based on few attributes like gender, age, and social status, we can make very accurate predictions on which passengers would survive. Some groups of people were more likely to survive than others, such as women, children, and the upper-class. Therefore, we can learn about the society priorities and privileges at the time.\n\n","5e0a0a8a":"### Determine cardinality of categorical variables","5e7dbfb0":"## Data Exploration\n\n### Find numerical and categorical variables","f9c2bd2f":"## Make predictions and evaluate model performance\n\nDetermine:\n- roc-auc\n- accuracy\n\n**Important, remember that to determine the accuracy, you need the outcome 0, 1, referring to survived or not. But to determine the roc-auc you need the probability of survival.**","ed80d249":"### Remove rare labels in categorical variables\n\n- remove labels present in less than 5 % of the passengers","1d24b741":"### Fill in Missing data in numerical variables:\n\n- Add a binary missing indicator\n- Fill NA in original variable with the median\n### Replace Missing data in categorical variables with the string **Missing**","0afa25a5":"### Find missing values in variables","93dd0569":"## Prepare the data set","40a8a55f":"### Perform one hot encoding of categorical variables into k-1 binary variables\n\n- k-1, means that if the variable contains 9 different categories, we create 8 different binary variables\n- Remember to drop the original categorical variable (the one with the strings) after the encoding","308faf26":"That's it! Well done\n\n**Do Upvote if you really liked the kernel!!**","996576b2":"## Separate data into train and test\n\nUse the code below for reproducibility. Don't change it.","e6529ec2":"### Scale the variables\n\n- Use the standard scaler from Scikit-learn"}}