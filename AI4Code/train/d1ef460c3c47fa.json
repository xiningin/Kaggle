{"cell_type":{"7f4aff1a":"code","986131dc":"code","81432668":"code","ee0f03b8":"code","4daa991f":"code","0289dd24":"code","e3b14c73":"code","d366e4c3":"code","02782a94":"code","841b815e":"code","ff0c7d5e":"code","8427a7eb":"code","6af1fdcb":"code","96dff1a7":"code","632dd875":"code","53854ad1":"code","ab0378e4":"code","5b4a8303":"code","08a9f9dd":"code","2b67dc42":"code","0ace6f83":"code","75af0f92":"code","ba453e08":"code","6a471702":"code","5fa8b41e":"code","2a58b16f":"code","f7b2bc49":"code","6522f0c5":"code","f309dd80":"code","ff72a5f9":"code","6cf07925":"code","7b71e6c8":"code","0893930d":"markdown","630e1c06":"markdown","acb5037b":"markdown","99a7ec84":"markdown","0c2acb46":"markdown","bdc4bd0d":"markdown","34da0323":"markdown","28a73325":"markdown","61acfc09":"markdown","6085fe91":"markdown","f6c65db4":"markdown","0e75ab4e":"markdown","f941178b":"markdown","d42ef1a7":"markdown"},"source":{"7f4aff1a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","986131dc":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","81432668":"df.info()","ee0f03b8":"df.isna().sum()","4daa991f":"df.sex.value_counts()","0289dd24":"df.sex[df.target==1].value_counts()","e3b14c73":"df.sex[df.target==1].value_counts().plot(kind='bar',figsize=(10,6),color=['green','blue'])\nplt.title(\"Count of the number of males and females with heart disease\")\n","d366e4c3":"pd.crosstab(df.target,df.sex)","02782a94":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=[\"lightblue\",\"pink\"])\nplt.title(\"Frequency of Heart Disease vs Sex\")\nplt.xlabel(\"0= Heart Disease, 1= No disease\")\nplt.ylabel(\"Number of people with heart disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","841b815e":"df.corr()","ff0c7d5e":"cor_mat=df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")\n","8427a7eb":"from sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\nfeat=['age', \t'sex', \t'cp', 'trestbps', 'chol', \t'fbs', \t'restecg', \t'thalach' ,\t'exang', \t'oldpeak' ,\t'slope', \t'ca', 'thal']\ndf[feat] = scal.fit_transform(df[feat])\ndf.head()","6af1fdcb":"X=df.drop(\"target\",axis=1)\nY=df.target","96dff1a7":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0,test_size=0.2)","632dd875":"from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix,roc_auc_score\n\ndef evaluation(Y_test,Y_pred):\n  acc=accuracy_score(Y_test,Y_pred)\n  rcl=recall_score(Y_test,Y_pred)\n  f1=f1_score(Y_test,Y_pred)\n  auc_score=roc_auc_score(Y_test,Y_pred)\n  prec_score=precision_score(Y_test,Y_pred)\n \n\n  metric_dict={'accuracy': round(acc,3),\n               'recall': round(rcl,3),\n               'F1 score': round(f1,3),\n               'auc score': round(auc_score,3),\n               'precision': round(prec_score,3)\n               \n              }\n\n  return print(metric_dict)\n","53854ad1":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier()\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\n#print(Knn_score)\nevaluation(Y_test,Knn_Y_pred)","ab0378e4":"np.random.seed(42)\nfrom sklearn.linear_model import LogisticRegression\nLR_clf=LogisticRegression()\nLR_clf.fit(X_train,Y_train)\nLR_Y_pred=LR_clf.predict(X_test)\nLR_score=LR_clf.score(X_test,Y_test)\n#print(LR_score)\nevaluation(Y_test,LR_Y_pred)","5b4a8303":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nRF_clf=RandomForestClassifier(n_estimators=450)\nRF_clf.fit(X_train,Y_train)\nRF_score=RF_clf.score(X_test,Y_test)\nRF_Y_pred=RF_clf.predict(X_test)\n#print(RF_score)\nevaluation(Y_test,RF_Y_pred)","08a9f9dd":"np.random.seed(42)\nfrom sklearn.svm import SVC\nSVC_clf=SVC()\nSVC_clf.fit(X_train,Y_train)\nSVC_score=SVC_clf.score(X_test,Y_test)\nSVC_Y_pred=SVC_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,SVC_Y_pred)","2b67dc42":"from xgboost import XGBClassifier\nXGB_clf=XGBClassifier()\nXGB_clf.fit(X_train,Y_train)\nXGB_score=XGB_clf.score(X_test,Y_test)\nXGB_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,XGB_Y_pred)","0ace6f83":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine',\"XGBoost\"], 'Accuracy': [LR_score*100,\n                    RF_score*100,Knn_score*100,SVC_score*100,XGB_score*100]})\nmodel_comp","75af0f92":"neighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    print(f\"Accuracy with {i} no. of neighbors: {knn.fit(X_train, Y_train).score(X_test,Y_test)}%\")","ba453e08":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier(n_neighbors=7)\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\nevaluation(Y_test,Knn_Y_pred)","6a471702":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(42)\nfor i in range(1,40,1):\n  print(f\"With {i*10} estimators:\")\n  clf2=RandomForestClassifier(n_estimators=i*10,max_depth=i,random_state=i).fit(X_train,Y_train)\n  print(f\"Accuracy: {clf2.score(X_test,Y_test)*100:2f}%\")","5fa8b41e":"from sklearn.ensemble import RandomForestClassifier\nRF_clf2=RandomForestClassifier(n_estimators=30,max_depth=3,random_state=3)\nRF_clf2.fit(X_train,Y_train)\nRF2_acc_score=RF_clf2.score(X_test,Y_test)\nRF2_Y_pred=RF_clf2.predict(X_test)\nprint(RF2_acc_score)\nevaluation(Y_test,RF2_Y_pred)","2a58b16f":"xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\n\nxgb.fit(X_train,Y_train)\nxgb_score=XGB_clf.score(X_test,Y_test)\nxgb_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,xgb_Y_pred)","f7b2bc49":"\nfrom sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1,2, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \ngs_clf = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngs_clf.fit(X_train, Y_train)\n\nprint(gs_clf.best_params_)\n\nprint(f\"Accuracy score:{gs_clf.score(X_test,Y_test)}%\")\n\n","6522f0c5":"knn_grid={'n_neighbors': np.arange(1,20,1),\n          'leaf_size': np.arange(1,30,1)}\n\ngs_knn=GridSearchCV(KNeighborsClassifier(),param_grid=knn_grid,cv=5,verbose=True)\n\ngs_knn.fit(X_train, Y_train)\n\ngs_knn.best_params_\n\nprint(f\"Accuracy score:{gs_knn.score(X_test,Y_test)*100}%\")","f309dd80":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine','Extreme Gradient Boost'], 'Accuracy': [LR_score*100,\n                    RF2_acc_score*100,Knn_score*100,SVC_score*100, XGB_score*100]})\nmodel_comp","ff72a5f9":"print(\" Best evaluation parameters achieved with KNN:\") \nevaluation(Y_test,Knn_Y_pred)\n\n","6cf07925":"KNN_final_metrics={'Accuracy': Knn_clf.score(X_test,Y_test),\n                   'Precision': precision_score(Y_test,Knn_Y_pred),\n                   'Recall': recall_score(Y_test,Knn_Y_pred),\n                   'F1': f1_score(Y_test,Knn_Y_pred),\n                   'AUC': roc_auc_score(Y_test,Knn_Y_pred)}\n\nKNN_metrics=pd.DataFrame(KNN_final_metrics,index=[0])\n\nKNN_metrics.T.plot.bar(title='KNN metric evaluation',legend=False);","7b71e6c8":"from sklearn.metrics import confusion_matrix\n\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(Y_test,Knn_Y_pred),annot=True,cbar=True);","0893930d":"## **Create a function for evaluating metrics**","630e1c06":"# **Conclusion**\n\n\n1. Used MinMaxScaler to scale down the values instead of StandardScaler\n2.  Accuracy of 90.2% with KNeighbors Classifier\n\n\n\nI am still a beginner and have a lot to learn. \n\n\nI hope you found it resourceful!\n\n\n","acb5037b":"# **Hyper parameter tuning  SVC using GridSearchCV**","99a7ec84":"## **Splitting the data into train and test sets**","0c2acb46":"# **Looking at the evaluation metrics for our best model**","bdc4bd0d":"# **Tuning Random Forest**","34da0323":"# **Tuning XGBoost manually**","28a73325":"## **Tuning KNN**","61acfc09":"### **Number of males and females whose heart data is stored in the dataset**","6085fe91":"## **Building a Correlation Matrix**","f6c65db4":"# **Hyper parameter tuning KNN using GridSearchCV**","0e75ab4e":"## **Creating Features and Target variable**","f941178b":"## **Fitting and Comparing different Models**","d42ef1a7":" ### **Count of the number of males and females who have heart disease**"}}