{"cell_type":{"4cda3c8f":"code","51d8a79a":"code","e02480e8":"code","b3a1a72d":"code","ed54ef0a":"code","452de71a":"code","686f46fe":"code","22540e9a":"code","f1c773de":"code","641f9725":"code","324dfc39":"code","0f7fc866":"code","13ecd394":"code","421076c5":"code","e630ee84":"code","a5289e5d":"code","69f79dc9":"code","d7963bbd":"code","56fc6515":"code","ca21b189":"code","8df06a2c":"code","748cec1e":"code","140476a8":"code","7cf9e713":"code","da17b951":"code","f9e0c895":"code","e44f6457":"code","22333765":"code","224778ea":"code","9ada45b2":"code","4b84db56":"code","9d54019f":"code","67f58312":"code","6db0b455":"code","25c8c667":"code","0853b6df":"code","4c8a1fc6":"code","a14338cc":"code","102117e4":"code","7f019751":"markdown","9b8bef30":"markdown","7b2fddb2":"markdown","2d0cbb8f":"markdown","e79a9b20":"markdown","b042ca7e":"markdown","feab6437":"markdown","dec2ba0b":"markdown","85f1a503":"markdown","5d6ba35e":"markdown","84c6b8c2":"markdown","12ccc412":"markdown","cf750873":"markdown","58375ba3":"markdown","6f5c6a43":"markdown","57bf1c07":"markdown","4f176b41":"markdown","4f0bc6ec":"markdown","b6c5213e":"markdown","18213728":"markdown","d779ee25":"markdown","5e5be746":"markdown","54fbbe3b":"markdown","01b8ccee":"markdown","8a9868ce":"markdown","c2c86e50":"markdown","db3c7203":"markdown"},"source":{"4cda3c8f":"import covid19_tools as cv19\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport gensim\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.spatial.distance import cdist\nfrom IPython.core.display import display, HTML\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.options.mode.chained_assignment = None ","51d8a79a":"METADATA_FILE = '\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv'\n\n# load metadata\nmeta = cv19.load_metadata(METADATA_FILE)\n# add covid19 tags\nmeta, covid19_counts = cv19.add_tag_covid19(meta)","e02480e8":"# select only Covid-19 related articles\nmeta = meta[meta.tag_disease_covid19]","b3a1a72d":"SOC_ETHIC_TERMS = ['social',\n'social concern',\n'ethical principles',\n'ethical concern',\n'ethical framework',\n'restriction',\n'movement',\n'restriction of movement',\n'movement restriction',\n'tracking',\n'tracing',\n'freedom',\n'assembly',\n'gathering',\n'detention',\n'vaccination',\n'censorship',\n'internet',\n'access',\n'medical treatment',\n'disinformation',\n'misinformation',\n'fake news',\n'isolation',\n'individual rights',\n'human rights',\n'liberty',\n'self-determination',\n'ethical',\n'antiviral',\n'force',\n'force measures',\n'privacy',\n'surveillance',\n'digital rights',\n'democracy',\n'discrimination',\n'anti-asian',\n'unemployment',\n'politics',\n'compliance',\n'bed shortage',\n'ICU bed shortage',\n'hospitals overloaded',\n'public health']","ed54ef0a":"# add social and ethic tags\nmeta, soc_ethic_counts = cv19.count_and_tag(meta,SOC_ETHIC_TERMS,'soc_ethic')\n# include only social and ethic related articles\nmeta_rel = meta[meta.tag_soc_ethic]","452de71a":"meta_rel.info()","686f46fe":"meta_rel.head(5)","22540e9a":"print('Loading full text for articles')\nfull_text_repr = cv19.load_full_text(meta_rel,'..\/input\/CORD-19-research-challenge')","f1c773de":"full_text_repr[0]","641f9725":"def get_body_text(full_text_repr):\n    body_text = []\n    for article in full_text_repr:\n        text = [body_text['text'] for body_text in article['body_text']]\n        body_text.append(''.join(text))\n    return body_text\n\n# extract body text\nbody_text_repr = get_body_text(full_text_repr)\n\n# store body text if exists\nfull_text_ids = [article['paper_id'] for article in full_text_repr]\nmeta_rel['full_text'] = None\nmeta_rel['full_text'] = meta_rel['sha'].apply(lambda x: full_text_ids.index(x) if x in full_text_ids else -1)\nmeta_rel['full_text'] = meta_rel['full_text'].apply(lambda x: body_text_repr[x] if x != -1 else None)","324dfc39":"meta_rel.head(5)","0f7fc866":"print('Number of articles without title:')\nprint(meta_rel['title'].isnull().sum())\nprint('Number of articles without abstract:')\nprint(meta_rel['abstract'].isnull().sum())\nprint('Number of articles without full text:')\nprint(meta_rel['full_text'].isnull().sum())","13ecd394":"def read_corpus(df, column, tokens_only=False):\n    for i, line in enumerate(df[column]):\n        # get text tokens\n        tokens = gensim.utils.simple_preprocess(line)\n        if tokens_only:\n            yield tokens\n        else:\n            # for training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])","421076c5":"# we use as training data the full text of the articles\ntrain_corpus = meta_rel[meta_rel['full_text'].notnull()]\ntrain_corpus = list(read_corpus(train_corpus, 'full_text'))","e630ee84":"train_corpus[0]","a5289e5d":"# train the model\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=512, min_count=2, epochs=20, seed=42, workers=3)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)","69f79dc9":"# this functions calculates text embedding using our trained model\ndef get_doc_vector(doc):\n    tokens = gensim.utils.simple_preprocess(doc)\n    embedding = model.infer_vector(tokens)\n    return embedding","d7963bbd":"# get the full text embeddings, if full text is missing then set embedding to 512 dimensional vector of Nones\ndoc2vec_full_text_embedding = meta_rel.apply(lambda x: get_doc_vector(x['full_text']) if \n                                             pd.notnull(x['full_text']) else np.full(512, None), axis = 1)\n\nmeta_rel['doc2vec_full_text_embedding'] = doc2vec_full_text_embedding\ndel doc2vec_full_text_embedding","56fc6515":"# download the Universal Sentence Encoder module and uncompress it to the destination folder. \n!mkdir \/kaggle\/working\/universal_sentence_encoder\/\n!curl -L 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5?tf-hub-format=compressed' | tar -zxvC \/kaggle\/working\/universal_sentence_encoder\/","ca21b189":"# load the Universal Sentence Encoder module\nuse_embed = hub.load('\/kaggle\/working\/universal_sentence_encoder\/')\n\n# this function calculates text embedding using Universal Sentence Encoder\ndef get_use_embedding(text):\n    embedding = use_embed([text])[0]\n    return embedding.numpy()","8df06a2c":"# get the title embeddings, if title is missing then set embedding to 512 dimensional vector of Nones\nuse_title_embedding = meta_rel.apply(lambda x: get_use_embedding(x['title']) if pd.notnull(x['title']) \n                                else np.full(512, None), axis = 1)\n\nmeta_rel['use_title_embedding'] = use_title_embedding\ndel use_title_embedding","748cec1e":"# get the abstract embeddings, if abstract is missing then set embedding to 512 dimensional vector of Nones\nuse_abstract_embedding = meta_rel.apply(lambda x: get_use_embedding(x['abstract']) if pd.notnull(x['abstract']) \n                                else np.full(512, None), axis = 1)\nmeta_rel['use_abstract_embedding'] = use_abstract_embedding\ndel use_abstract_embedding","140476a8":"# compute cosine similarity metric between embeddings, note that if embedding is None similarity is nan\ndef cosine_similarity(embedding, embeddings):\n    emb_shape = embedding.shape[0]\n    embeddings = np.stack(embeddings.to_numpy(), axis = 0)\n    similarities = 1 - cdist(embedding.reshape(1,emb_shape), embeddings, 'cosine')\n    return similarities[0].astype('float32')","7cf9e713":"# compute the similarity metrics between each embedding and the rest\nuse_title_simil = meta_rel['use_title_embedding'].apply(lambda x: cosine_similarity(x , meta_rel['use_title_embedding']))\nmeta_rel['use_title_cosine_sim'] = use_title_simil\ndel use_title_simil\n\nuse_abs_simil = meta_rel['use_abstract_embedding'].apply(lambda x: cosine_similarity(x , meta_rel['use_abstract_embedding']))\nmeta_rel['use_abstract_cosine_sim'] = use_abs_simil\ndel use_abs_simil\n\ndoc2vec_full_text_simil = meta_rel['doc2vec_full_text_embedding'].apply(lambda x: cosine_similarity(x , meta_rel['doc2vec_full_text_embedding']))\nmeta_rel['doc2vec_full_text_cosine_sim'] = doc2vec_full_text_simil\ndel doc2vec_full_text_simil","da17b951":"meta_rel.head()","f9e0c895":"# save DataFrame with embeddings and similarities\n!mkdir \/kaggle\/working\/output_data\/\nmeta_rel.to_pickle('\/kaggle\/working\/output_data\/data.pkl')","e44f6457":"# compute similarity matrix between the embeddings, note that if embedding is None similarity is nan\ndef similarity_matrix(embeddings):\n    embeddings = np.stack(embeddings.to_numpy(), axis = 0)\n    similarities = 1 - cdist(embeddings, embeddings, 'cosine')\n    return similarities.astype('float32')","22333765":"# get similarity matrices for titles, abstract and full texts\nuse_title_sim_matrix = similarity_matrix(meta_rel['use_title_embedding'])\nuse_abstract_sim_matrix = similarity_matrix(meta_rel['use_abstract_embedding'])\ndoc2vec_full_text_sim_matrix = similarity_matrix(meta_rel['doc2vec_full_text_embedding'])","224778ea":"use_title_sim_matrix","9ada45b2":"# create dataframes of similarities\nindex_id = meta_rel['sha'].values\nuse_title_sim_matrix_df = pd.DataFrame(use_title_sim_matrix, index = index_id, columns = index_id)\nuse_abstract_sim_matrix_df = pd.DataFrame(use_abstract_sim_matrix, index = index_id, columns = index_id)\ndoc2vec_full_text_sim_matrix_df = pd.DataFrame(doc2vec_full_text_sim_matrix, index = index_id, columns = index_id)","4b84db56":"use_title_sim_matrix_df.head(5)","9d54019f":"# save similarity matrices as Numpy arrays and Pandas DataFrames\noutput_dir = '\/kaggle\/working\/output_data\/'\n\nnp.save(output_dir + 'title_sim.npy',use_title_sim_matrix)\nnp.save(output_dir + 'abstract_sim.npy',use_abstract_sim_matrix)\nnp.save(output_dir + 'full_text_sim.npy',doc2vec_full_text_sim_matrix)\n\nuse_title_sim_matrix_df.to_pickle(output_dir + 'title_sim_df.pkl')\nuse_abstract_sim_matrix_df.to_pickle(output_dir + 'abstract_sim_df.pkl')\ndoc2vec_full_text_sim_matrix_df.to_pickle(output_dir + 'full_text_sim_df.pkl')","67f58312":"task_1 = ('Efforts to articulate and translate existing ethical principles and standards to salient '\n            'issues in COVID-2019')\ntask_2 = ('Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise '\n            'and coordinate to minimize duplication of oversight')\ntask_3 = 'Efforts to support sustained education, access, and capacity building in the area of ethics '\ntask_4 = ('Efforts to establish a team at WHO that will be integrated within multidisciplinary research '\n            'and operational platforms and that will connect with existing and expanded global networks '\n            'of social sciences.')\ntask_5 = ('Efforts to develop qualitative assessment frameworks to systematically collect information '\n            'related to local barriers and enablers for the uptake and adherence to public health measures ' \n            'for prevention and control. This includes the rapid identification of the secondary impacts of '\n            'these measures. (e.g. use of surgical masks, modification of health seeking behaviors for '\n            'SRH, school closures)')\ntask_6 = ('Efforts to identify how the burden of responding to the outbreak and implementing public '\n            'health measures affects the physical and psychological health of those providing care for '\n            'Covid-19 patients and identify the immediate needs that must be addressed.')\ntask_7 = ('Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation '\n            'and rumor, particularly through social media.')","6db0b455":"tasks = [task_1, task_2, task_3, task_4, task_5, task_6, task_7]\n# compute USE embeddings for tasks\nuse_tasks_embeddings = [get_use_embedding(task) for task in tasks]","25c8c667":"def get_train_embeddings(df, column):\n    # remove rows in which embedding is vector of Nones (i.e. title or abstract is missing)\n    train_embeddings = df[column].apply(lambda x: None if x[0] is None else x).dropna()\n    index = train_embeddings.index\n    train_embeddings = train_embeddings.to_list()\n    # return list of not None embeddings and DataFrame without articles whose embedding is None\n    return train_embeddings, df.loc[index]\n\n# auxiliar function to print info about found neighbors\ndef print_neighbors_info(tasks, meta_rel_mod, neigh_dist, neigh_indices):\n    for i, task in enumerate(tasks):\n        print(\"-\"*80, f'\\n\\nTask = {task}\\n')\n        df =  meta_rel_mod.iloc[neigh_indices[i]]\n        abstracts = df['abstract']\n        titles = df['title']\n        dist = neigh_dist[i]\n        for neighbour in range(len(dist)):\n            print(f'Distance = {neigh_dist[i][neighbour]:.4f} \\n')\n            print(f'Title: {titles.iloc[neighbour]} \\n\\nAbstract: {abstracts.iloc[neighbour]}\\n\\n')","0853b6df":"train_data, meta_rel_mod = get_train_embeddings(meta_rel, 'use_title_embedding')\nnn_model = NearestNeighbors().fit(train_data)\nneigh_dist, neigh_indices = nn_model.kneighbors(use_tasks_embeddings, n_neighbors=3)","4c8a1fc6":"print_neighbors_info(tasks, meta_rel_mod, neigh_dist, neigh_indices)","a14338cc":"train_data, meta_rel_mod = get_train_embeddings(meta_rel, 'use_abstract_embedding')\n# options: 'use_title_embedding', 'use_abstract_embedding', 'doc2vec_full_text_embedding'\nnn_model = NearestNeighbors().fit(train_data)\nneigh_dist, neigh_indices = nn_model.kneighbors(use_tasks_embeddings, n_neighbors=3)","102117e4":"print_neighbors_info(tasks, meta_rel_mod, neigh_dist, neigh_indices)","7f019751":"# Get title and abstract embeddings with USE model <a id=\"5\" ><\/a>","9b8bef30":"We are going to train the model for 20 epochs, we set the embeddings dimension size to 512.","7b2fddb2":"We fit the NN model on the abstract embeddings and query for the 3 nearest neighbors for each task.","2d0cbb8f":"- [Setting up the notebook](#1)\n- [Load metadata and filter articles](#2)\n- [Load full text for articles](#3)\n- [Train Doc2Vec model and get the full text embeddings](#4)\n- [Get title and abstract embeddings with USE model](#5)\n- [Compute semantic similarities between texts](#6)\n- [Nearest Neighbors search of embeddings](#7)\n    - [Search across titles](#8)\n    - [Search across abstracts](#9)","e79a9b20":"We fit the NN model on the title embeddings and query for the 3 nearest neighbors for each task.","b042ca7e":"Now we can print nearest neighbors information, as title and abstract text, for each task.","feab6437":"By using the embeddings obtained in previous sections we can now compute the semantic similarity between texts using cosine distance between vectors. We are going to get the semantic similarity between titles, abstracts and full texts of articles.","dec2ba0b":"# Train Doc2Vec model and get the full text embeddings <a id=\"4\" ><\/a>","85f1a503":"# Compute semantic similarities between texts <a id=\"6\" ><\/a>","5d6ba35e":"# Setting up the notebook <a id=\"1\"><\/a>","84c6b8c2":"We are going to extract the body text of the articles and store it in a new column.","12ccc412":"In this section we are going to train a Doc2Vec model on the full text of the articles by following \n[this tutorial](https:\/\/radimrehurek.com\/gensim\/auto_examples\/tutorials\/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py).","cf750873":"# Load metadata and filter articles <a id=\"2\" ><\/a>","58375ba3":"## Search across titles <a id=\"8\" ><\/a>","6f5c6a43":"# Nearest Neighbors search of embeddings <a id=\"7\" ><\/a>","57bf1c07":"Now we can store the similarities as Pandas DataFrames or Numpy arrays.","4f176b41":"## Search across abstracts <a id=\"9\" ><\/a>","4f0bc6ec":"In this section we are going to use the Universal Sentence Encoder model to obtain the title and abstract embeddings.","b6c5213e":"We compute the title and abstract embeddings and store them in two new columns.","18213728":"Finally, we compute the body text embeddings and store them in a new column.","d779ee25":"# Load full text for articles <a id=\"3\" ><\/a>","5e5be746":"We have to define a couple of auxiliar functions:","54fbbe3b":"We can also obtain the similarities in matrix form, this can be mode suitable if we want to use the values later as  weights of the edges in a graph.","01b8ccee":"Now we can print nearest neighbors information, as title and abstract text, for each task.","8a9868ce":"# About this notebook\nThis notebook is focused on the task: \"What has been published about ethical and social science considerations?\", so I won't be using the whole CORD-19 dataset, but a smaller database of articles filtered by social and ethical terms.\n\nThe Universal Sentence Encoder is a pre-trained model based on Transformer architecture created by Google. It is trained on sentences and paragraphs and is capable of encoding input text into 512 dimensional vectors that can be used for tasks such as semantic similarity.\n\nUnfortunately, this model doesn't work well for long documents, such as the full text of the articles. Therefore, using a not Transformer based algorithm like Doc2Vec would probably be better for this kind of task.\n\nIn this notebook, I am going to use the Universal Sentence Encoder model to obtain a semantic based similarity metric between article titles and abstracts, I will use a Doc2Vec model to obtain a similarity metric between article full texts. The metric used is the cosine distance between vector embeddings. The results can be used later as weights of the edges in a multigraph.\n\nIn order to check whether the embeddings actually contain information about the semantic meaning of the texts, I have fitted a Nearest Neighbors model on the obtained title and abstract embeddings, then I have calculated the embeddings of the tasks texts and queried the NN model to obtain the k-NN for each task. Overall I am happy with the results obtained by the model, since it is returning ethic and social related articles, this probably means that the calculated embeddings make sense.  ","c2c86e50":"Finally, as said in the introduction, we can calculate the embeddings of the tasks and find their Nearest Neighbors in the space of titles and abstract embeddings, we expect them to belong to articles related to the corresponding task.","db3c7203":"Now we are going to filter the articles by social and ethics terms:"}}