{"cell_type":{"3803195d":"code","a8748ab2":"code","4521d855":"code","5852872c":"code","e4588a8f":"code","89731414":"code","dd97eeab":"code","2239990a":"code","193318f1":"code","868ad959":"code","1a8d487d":"code","53cbe682":"code","08babd63":"code","84b9dabf":"code","d8f2cee8":"code","ec0a7606":"code","57e43500":"code","8a595487":"code","856b3d1b":"code","1fed4a9b":"code","ced4330c":"code","f76c9440":"code","aad21321":"code","93c2740d":"code","b18d7d25":"code","80d84ca6":"code","5a7db3cd":"code","8f6bf1fe":"code","075c8798":"code","3e56db51":"markdown","1f478a00":"markdown","782e380d":"markdown","62dcdf23":"markdown","5dbe5055":"markdown","afcd10f9":"markdown","321a9d12":"markdown","fb9d0c62":"markdown","ec8cf30f":"markdown"},"source":{"3803195d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a8748ab2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport multiprocessing\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport emoji\nimport re\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup","4521d855":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\", sep=\",\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\", sep=\",\")","5852872c":"train_df.head()","e4588a8f":"test_df.head()","89731414":"print(f\"Length of training data: {len(train_df)}\")\nprint(f\"Length of testing data: {len(test_df)}\")","dd97eeab":"print(f\"Missing data in training:\\n{train_df.isnull().sum()}\")\nprint(\"-\" * 20)\nprint(f\"Missing data in testing:\\n{test_df.isnull().sum()}\")","2239990a":"def clean_tweet(txt):\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    txt = re.sub(r'\\n','',txt)\n    # to remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    txt = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+','',txt)\n    txt = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\",\"\",txt)\n    txt = re.sub(r\"<.*?>\",\"\",txt)\n    return str.lower(txt)","193318f1":"tqdm.pandas()\n\nprint(train_df.iloc[0, -2])\ntrain_df.text = train_df.text.progress_apply(clean_tweet)\nprint(train_df.iloc[0, -2])","868ad959":"print(test_df.iloc[0, -1])\ntest_df.text = test_df.text.progress_apply(clean_tweet)\nprint(test_df.iloc[0, -1])                            ","1a8d487d":"# model_name = \"sentence-transformers\/bert-base-nli-mean-tokens\"\nmodel_name = \"cardiffnlp\/twitter-roberta-base\"\n# model_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","53cbe682":"def encode_sentences(sentences):\n    encoded = tokenizer(sentences, padding=True, return_attention_mask=True, return_tensors='pt')\n    return encoded","08babd63":"def collate_batch(batch):\n    sentences, targets = list(zip(*batch))\n    encoded = encode_sentences(list(sentences))\n    targets = torch.tensor(targets)\n    return encoded, targets\n\nclass DisasterDataset(torch.utils.data.Dataset):    \n    def __init__(self, df):\n        self.df = df.text.to_list()\n        self.targets = df.target.to_list()\n     \n    def __getitem__(self, idx):\n        sentence = self.df[idx]\n        target = self.targets[idx]\n        return sentence, target\n     \n    def __len__(self):\n        return len(self.df)","84b9dabf":"dataset = DisasterDataset(train_df)","d8f2cee8":"# Display text and label.\nprint('\\nFirst iteration of data set: ', next(iter(dataset)), '\\n')\n# Print how many items are in the data set\nprint('Length of data set: ', len(dataset), '\\n')","ec0a7606":"class Model(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super(Model, self).__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        self.classify = nn.Sequential(\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, num_classes)\n        )\n        \n    def mean_pooling(self, outputs, attention_mask):\n        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n    def forward(self, inputs):\n        outputs = self.encoder(**inputs)\n        return self.classify(outputs[1])","57e43500":"model = Model(model_name, 2)\nmodel = model.to(device)","8a595487":"with torch.no_grad():\n    example_sentence = \"On the plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE\"\n    example_enc = encode_sentences([example_sentence]).to(device)\n    example_output = model(example_enc)\n    print(example_output)","856b3d1b":"sns.countplot(train_df.target, data=train_df)\nplt.show()","1fed4a9b":"def make_weights_for_balanced_classes(targets, n_classes):                        \n    count = [0] * n_classes                                                      \n    for t in targets:                                                         \n        count[t] += 1                                                     \n    weight_per_class = [0.] * n_classes                                      \n    N = float(sum(count))                                                   \n    for i in range(n_classes):                                                   \n        weight_per_class[i] = N \/ float(count[i])                                 \n    weight = [0] * len(targets)    \n    for idx, val in enumerate(targets):                                          \n        weight[idx] = weight_per_class[val]                                  \n    return weight ","ced4330c":"def prepare_dataloaders(train_df):\n    X_train, X_val = train_test_split(train_df, test_size=0.10, random_state=0, stratify=train_df.target)\n    \n#     weights = make_weights_for_balanced_classes(X_train.target, 2)\n#     sampler = torch.utils.data.sampler.WeightedRandomSampler(torch.DoubleTensor(weights), len(weights))\n    \n    train_dataset = DisasterDataset(X_train)\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, collate_fn=collate_batch, num_workers=(multiprocessing.cpu_count() - 1), pin_memory=True)\n\n    val_dataset = DisasterDataset(X_val)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, collate_fn=collate_batch, num_workers=(multiprocessing.cpu_count() - 1), pin_memory=True)\n\n    return train_dataloader, val_dataloader","f76c9440":"train_dataloader, val_dataloader = prepare_dataloaders(train_df)","aad21321":"EPOCHS = 5\nloss_fn = nn.CrossEntropyLoss().to(device)\noptimizer = optim.AdamW(model.parameters(), betas = (0.99, 0.98), lr=2e-5)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)","93c2740d":"def train(model, dataloader, loss_fn, optimizer, scheduler):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for sentences, targets in tqdm(dataloader):\n        input_ids = sentences[\"input_ids\"].to(device)\n        attention_mask = sentences[\"attention_mask\"].to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()        \n        outputs = model(dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ))\n        \n        train_loss = loss_fn(outputs, targets)\n        train_loss.backward()\n        losses.append(train_loss.item())\n        \n        _, preds = torch.max(outputs, dim=1)\n        correct_predictions += torch.sum(preds == targets)\n        \n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n    return correct_predictions.double() \/ len(dataloader.dataset), np.mean(losses)","b18d7d25":"def validate(model, dataloader, loss_fn):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for sentences, targets in tqdm(dataloader):\n            input_ids = sentences[\"input_ids\"].to(device)\n            attention_mask = sentences[\"attention_mask\"].to(device)\n            targets = targets.to(device)\n            \n            outputs = model(dict(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            ))\n            \n            val_loss = loss_fn(outputs, targets)\n            losses.append(val_loss.item())\n            \n            _, preds = torch.max(outputs, dim=1)\n            correct_predictions += torch.sum(preds == targets)\n            \n    return correct_predictions.double() \/ len(dataloader.dataset), np.mean(losses)","80d84ca6":"for epoch in range(EPOCHS):\n    print(f\"Epoch: {epoch + 1} \/ {EPOCHS}\")\n    \n    train_accuracy, train_loss = train(model, train_dataloader, loss_fn, optimizer, scheduler)\n    val_accuracy, val_loss = validate(model, val_dataloader, loss_fn)\n    \n    print(f\"Training Loss: {train_loss} | Training Accuracy: {train_accuracy}\")\n    print(f\"Validation Loss: {val_loss} | Validation Accuracy: {val_accuracy}\")    ","5a7db3cd":"model.eval()\nwith torch.no_grad():\n    test_encoded = encode_sentences(test_df.text.to_list())\n    input_ids = test_encoded[\"input_ids\"].to(device)\n    attention_mask = test_encoded[\"attention_mask\"].to(device)\n    predictions = model(dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ))","8f6bf1fe":"predictions = np.argmax(predictions.cpu(), axis=1)","075c8798":"submission = pd.concat([test_df.id, pd.Series(predictions)], axis=1)\nsubmission.rename(columns = {0:'target'}, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","3e56db51":"# Create Model","1f478a00":"# Create Dataset","782e380d":"Cleaning the data.\n\nWe will clean the tweets for each entry in text.","62dcdf23":"# Introduction\n\nThis notebook can use `sentence-transformers\/bert-base-nli-mean-tokens` or `cardiffnlp\/twitter-roberta-base` or `roberta-base`\n\n## References\n1. [roBERTA-base + PyTorch for Sent. Classification](https:\/\/www.kaggle.com\/bumjunkoo\/roberta-for-sentiment-classification)\n2. [NLP with Disaster Tweet](https:\/\/www.kaggle.com\/theblackmamba31\/nlp-with-disaster-tweet)\n3. [Balanced Sampling between classes with torchvision DataLoader](https:\/\/discuss.pytorch.org\/t\/balanced-sampling-between-classes-with-torchvision-dataloader\/2703\/3)","5dbe5055":"We will try to balance the dataset","afcd10f9":"# Data Loading\n\nWe will now load our data into different DataFrames.","321a9d12":"# Training","fb9d0c62":"# Imports","ec8cf30f":"# Submission"}}