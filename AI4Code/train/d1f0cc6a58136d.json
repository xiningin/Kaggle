{"cell_type":{"a01ca0da":"code","b4b925fa":"code","f54764ac":"code","331d9527":"code","dad5ff52":"code","9d10b50a":"code","da0bc2a2":"code","7ed042ca":"code","d95deb86":"code","8e0a1aa2":"code","ff998290":"code","3808eec7":"code","2f21bd84":"code","cffd12dc":"code","13c2ff35":"code","57a6aa34":"code","f25e2ea4":"code","774acfd4":"code","51ff3f5b":"code","ef502147":"code","09c4c2da":"code","50611703":"code","a6692542":"code","f2d2ad31":"code","58c763f5":"code","fd356714":"code","29d44154":"code","7cc02b72":"code","e412879b":"code","d5c61107":"code","aad082ce":"code","52c51573":"code","c68905d4":"code","7aa9a89a":"markdown","7b6ae3ac":"markdown","8c0725f6":"markdown","1e3dde7f":"markdown","ed7e42b8":"markdown","07675a13":"markdown","fdd33f6b":"markdown","3e350320":"markdown","72c3b140":"markdown","e109caf1":"markdown","8d0de070":"markdown","23217094":"markdown","d9e31f21":"markdown","e4297202":"markdown","df4289a6":"markdown","cc9a0349":"markdown","11d62ab1":"markdown","dc1d13b4":"markdown","685ed61e":"markdown","b55647d2":"markdown","9d084cce":"markdown","c2af8fe3":"markdown","5511d1ab":"markdown","333a9cfb":"markdown","4527b762":"markdown","67345761":"markdown","d38df4cc":"markdown","ba84a20b":"markdown","088cf9e8":"markdown","53e164b6":"markdown","8296eafa":"markdown"},"source":{"a01ca0da":"import numpy as np \nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn import preprocessing, decomposition\nfrom tsfresh.feature_extraction import feature_calculators as fc","b4b925fa":"import shap, lime\nfrom lime import lime_tabular","f54764ac":"train = pd.read_parquet('..\/input\/playgroundkfold\/train_kfold_play_nov_orig.parquet')\ntest = pd.read_parquet('..\/input\/playgroundkfold\/test_play_nov.parquet')","331d9527":"features = [feature for feature in train.columns if feature not in ('id','kfold', 'target')]","dad5ff52":"X_train = train.query('kfold != 0')\nX_valid = train.query('kfold == 0')\n\ny_train = X_train['target'].copy()\ny_valid = X_valid['target'].copy()\nX_train = X_train[features].values.copy()\nX_valid = X_valid[features].values.copy()","9d10b50a":"model1 = xgb.XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric = 'auc',\n                          tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\").fit(X_train, y_train)","da0bc2a2":"booster_xgb1 = model1.get_booster()\nshap_values_xgb1 = booster_xgb1.predict(xgb.DMatrix(X_train, y_train), pred_contribs=True)","7ed042ca":"shap_values_xgb1 = shap_values_xgb1[:, :-1]","d95deb86":"shap.summary_plot(shap_values_xgb1, X_train, feature_names=train[features].columns);","8e0a1aa2":"shaped_features = ['f34','f55','f8','f43','f91','f71','f80','f27','f50','f97',\n                   'f41','f66','f57','f25','f22','f96','f82','f81','f26','f40']","ff998290":"lime_explainer = lime_tabular.LimeTabularExplainer(training_data=X_train,\n                                                   feature_names=train[features].columns,\n                                                   mode='regression',\n                                                   verbose=False,\n                                                   random_state=42)","3808eec7":"test_sample = X_valid[5,:]","2f21bd84":"lime_exp = lime_explainer.explain_instance(data_row=test_sample,\n                                           num_features=20,\n                                           predict_fn=model1.predict_proba)","cffd12dc":"plot = lime_exp.as_pyplot_figure()\nplot.tight_layout()","13c2ff35":"lime_exp.show_in_notebook(show_table=True, show_all=False)","57a6aa34":"!pip install eli5 --upgrade --quiet","f25e2ea4":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model1,scoring = 'roc_auc', random_state=0).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = train[features].columns.tolist())","774acfd4":"scl = preprocessing.StandardScaler()\ntrain_scl = scl.fit_transform(train[shaped_features])\ntest_scl = scl.transform(test[shaped_features])","51ff3f5b":"# Create principal components\npca = decomposition.PCA()\ntrain_pca = pca.fit_transform(train_scl)\ntest_pca = pca.transform(test_scl)\n\n# Convert to dataframe\ncomponent_names_1 = [f\"PC{i+1}\" for i in range(train_pca.shape[1])]\ntrain_pca = pd.DataFrame(train_pca, columns=component_names_1)\n\ncomponent_names_2 = [f\"PC{i+1}\" for i in range(test_pca.shape[1])]\ntest_pca = pd.DataFrame(test_pca, columns=component_names_2)","ef502147":"scl = preprocessing.StandardScaler()\ntrain_scl = scl.fit_transform(train[features])\ntest_scl = scl.transform(test[features])","09c4c2da":"# Create svd components\nsvd = decomposition.TruncatedSVD()\ntrain_svd = svd.fit_transform(train_scl)\ntest_svd = svd.transform(test_scl)\n\n# Convert to dataframe\ncomponent_names_1 = [f\"SVD{i+1}\" for i in range(train_svd.shape[1])]\ntrain_svd = pd.DataFrame(train_svd, columns=component_names_1)\n\ncomponent_names_2 = [f\"SVD{i+1}\" for i in range(test_svd.shape[1])]\ntest_svd = pd.DataFrame(test_svd, columns=component_names_2)","50611703":"poly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(train[shaped_features])\ntest_poly = poly.fit_transform(test[shaped_features])\n\ntrain_poly_df = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ntest_poly_df = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n","a6692542":"for feature in shaped_features:\n    train[f'AbsEnergy_{feature}'] = fc.abs_energy(train[feature])\n    train[f'AbsSumChanges_{feature}'] = fc.absolute_sum_of_changes(train[feature])\n    train[f'MeanAbsChange_{feature}'] = fc.mean_abs_change(train[feature])\n    train[f'MeanChange_{feature}'] = fc.mean_change(train[feature])","f2d2ad31":"for col in features:\n    train[col+'_bin'] = pd.cut(train[col], bins=5, labels=False)","58c763f5":"train['mean'] = train[features].mean(axis=1)\ntrain['median'] = train[features].median(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['var'] = train[features].var(axis=1)\ntrain['kurt'] = train[features].kurtosis(axis=1)","fd356714":"train = pd.concat ([train,train_pca], axis=1)\ntest = pd.concat ([test,test_pca], axis=1)","29d44154":"train = pd.concat ([train,train_svd], axis=1)\ntest = pd.concat ([test,test_svd], axis=1)","7cc02b72":"train = pd.concat ([train,train_poly_df], axis=1)\ntest = pd.concat ([test,test_poly_df], axis=1)","e412879b":"X_train = train.query('kfold != 0')\nX_valid = train.query('kfold == 0')\n\nX_train = X_train[features].copy()\nX_valid = X_valid[features].copy()","d5c61107":"model2 = xgb.XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric = 'auc',\n                          tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\").fit(X_train, y_train)","aad082ce":"booster_xgb2 = model2.get_booster()\nshap_values_xgb2 = booster_xgb2.predict(xgb.DMatrix(X_train, y_train), pred_contribs=True)","52c51573":"shap_values_xgb2 = shap_values_xgb2[:, :-1]","c68905d4":"shap.summary_plot(shap_values_xgb2, X_train, feature_names=X_train.columns);","7aa9a89a":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:300%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nFeature selection & engineering\n<\/div>","7b6ae3ac":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nAnother way to see the result it's this one, I really like the format, and for that reason I show it to you.\n    <br><br>","8c0725f6":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis graph tells us which are the most important characteristics and their range of effects on the data set. The features are sorted by rank based on their impact on the target value. This technique helped me a lot in the previous competition, on the one hand in the training times of the models, and I also think that to avoid some overfitting by reducing the number of features and making a more generalizable model.\n    <br><br>","1e3dde7f":"<a id=\"subsection-three-three\"><\/a>\n<h2> \nPermutation feature importance\n<\/h2>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis is another technique wich consist in randomly shuffle a single column of the validation data, leaving the target and all other columns in place, and how this shuffle affect the accuracy of predictions in that now-shuffled data. There is a mini Kaggle course where you can learn more about it. I share the link for that down below.\nhttps:\/\/www.kaggle.com\/learn\/machine-learning-explainability    <br><br>","ed7e42b8":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis graph shows the top 20 most important features in the row 6 (index 5), remember that. The red values correspond to negatives correlations, and grees positives. You can check this for more features just only need to change the num_features param in the lime_expleainer.\n    <br><br>","07675a13":"<a id=\"section-four\"><\/a>\n# Feature engineering","fdd33f6b":"<a id=\"section-three\"><\/a>\n# Initial feature selection with Shap, Lime & Permutation (Baseline)","3e350320":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nWe can see how the permutation results are very similar to Shap, that I think it's because the process tho do the predictions of the features it's similar, but it's nice to se some confirmations\n    <br><br>","72c3b140":"<h3>\nStandardizing the features\n<\/h3>","e109caf1":"<a id=\"subsection-three-two\"><\/a>\n<h2> \nLime\n<\/h2>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nTake this approach as a grain of salt, because it's an \"unirow\" method, that means only can explain one row at the time, very different than Shap and Permutation. I just wanna show you because the result are quite nice, and also the fact it's a unirow approach maybe can helps to understood more the problem, by the oviuos reduction of the complexity. And you can use the results to do some experimentations and see what happens. I've think you can do this study randomizing the sample rows to check some variation on the output. Maybe I can do this in the future.\n    <br><br>","8d0de070":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n    We can se how the features we created are useful, because provide some information to predict the target. I will be adding more features as they occur to me. If you have any questions, suggestions, or if I make some mistake, please let me know. Good luck to all!\n    <br><br>","23217094":"<a id=\"subsection-four-one\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   <b> 1 - PCA decomposition:<\/b>\n    PCA or Principal Components Analysis gives us our ideal set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, and so on), uncorrelated, and low in number (we can throw away the lower ranked components as they contain little signal). In this case I just apply the PCA decomposition to the features selected by Shap, not all the set. To more info about the PCA, you can check the Kaggle course, https:\/\/www.kaggle.com\/ryanholbrook\/principal-component-analysis.\n    <br><br>","d9e31f21":"<a id=\"subsection-four-six\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n<b>6 - Stats features: <\/b> \n    <br><br>","e4297202":"<h3>\nStandardizing the features\n<\/h3>","df4289a6":"<a id=\"subsection-four-two\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   <b> 2 - SVD decomposition:<\/b>\n    Im going to use the SVD or Singular Value Decomposition for dimensionality reduction and also see if helps to denoise the data. In this case Im going to apply the SVD to the whole dataset. You can check the documentation here, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html, or explanations on TDS here, https:\/\/towardsdatascience.com\/search?q=svd.\n    <br><br>","cc9a0349":"<a id=\"subsection-four-four\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   <b>4 - Tsfresh Features:<\/b>\n     tsfresh offers hundreds of features and tens of variations of different features that you can use for time series based features. You can learn more about it here, https:\/\/tsfresh.readthedocs.io\/en\/latest\/text\/list_of_features.html.\n    <br><br>","11d62ab1":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   Splitting the data for the shap study and train a xgb model\n    <br><br>","dc1d13b4":"<a id=\"subsection-three-one\"><\/a>\n<h2> SHAP study\n    <\/h2> ","685ed61e":"<a id=\"section-six\"><\/a>\n# Final thoughts","b55647d2":"<a id=\"subsection-four-five\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n<b>5 - Binning: <\/b> \n    <br><br>","9d084cce":"<h3> \nSHAP Summary Plot\n<\/h3>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   With this plot we can visualize the overall impact of the features across multiple instances. For that reason the result of the shapley study it's much more reliable to establish what features are the most relevant in comparassion with for example the feature importance of a tree base model.    \n    <br><br>","c2af8fe3":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n  The first thing we do is concatenate the train and test datasets with their respective PCA and SVD datasets.\n    <br><br>","5511d1ab":"<a id=\"section-two\"><\/a>\n# Data loading","333a9cfb":"<a id=\"section-one\"><\/a>\n\n# Introduction\n\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   Throughout this notebook I will first make an initial features selection with Shap, Lime and permutation feature importance, to see if there is any difference regards to feature selection, and then try to make a feature engineering process and see what results sheds. The Shap  technique helped me in the previous competition. I hope you like it.\n    <br><br>","4527b762":"<a id=\"subsection-five-one\"><\/a>\n<h2>\nFinal feature selection\n<\/h2>","67345761":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nFor the final selection of features I'm going to use only Shap, because the Permutation approch it's quite resource demanding. \n<br><br>","d38df4cc":"<h3> \nLime results\n<\/h3>","ba84a20b":"# Table of Contents\n\n* [Introduction](#section-one)\n* [Data loading](#section-two)\n* [Initial feature selection(Baseline)](#section-three)\n    - [Shap study](#subsection-three-one)\n    - [Lime study](#subsection-three-two)\n    - [Permutation feature importance](#subsection-three-three)\n* [Feature engineering](#section-four)\n    - [PCA decomposition](#subsection-four-one)\n    - [SVD decomposition](#subsection-four-two)\n    - [Polynomial features](#subsection-four-three)\n    - [Tsfresh features](#subsection-four-four)\n    - [Binning](#subsection-four-five)\n    - [Stats features](#subsection-four-six)\n    - [Log transformation](#subsection-four-seven)\n* [Final feature selection](#section-five)\n    - [Final shap study](#subsection-five-one)\n* [Final thoughts](#section-six)","088cf9e8":"<h3>\nSHAP Summary Plot\n<\/h3>","53e164b6":"<a id=\"section-five\"><\/a>\n# Final feature selection","8296eafa":"<a id=\"subsection-four-three\"><\/a>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   <b> 3 - Polynomial features:<\/b>\n    This process generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. You can learn more about it here, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html.\n    <br><br>"}}