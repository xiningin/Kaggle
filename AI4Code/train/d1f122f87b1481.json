{"cell_type":{"011186c2":"code","f633ab9d":"code","0265b1e9":"code","f5ddcb92":"code","52139c06":"code","18dd7a06":"code","dc9e2aa6":"code","61ce8f94":"code","5f866791":"code","7c97cd3e":"code","5ea7f2a0":"code","66faa4f3":"code","65b6a59b":"code","788bc4e0":"code","76d1c4c3":"code","cada2fbb":"code","a950e9e5":"code","7c59fabe":"code","a7b8f048":"code","3b38b70c":"code","c056d6fc":"code","e2dd5905":"code","f774b000":"code","3a0f49f6":"code","fa78f406":"code","6cb151e1":"code","8dcde185":"code","0840a50c":"code","8c84f4da":"code","c5825ab5":"code","4bfb7498":"code","f596d6cb":"code","b1abe614":"code","34598032":"code","ae49fd65":"code","83093e9d":"code","69580632":"code","991e09ff":"code","7f9a0d8e":"code","ca748b1d":"code","d7fd3b99":"code","e58f620b":"code","ce9d6055":"code","fd590b03":"code","fe76bb1b":"code","ad944678":"code","a21e1f3e":"code","594b532f":"code","e5240e8d":"markdown","5410f1fc":"markdown","651ccea7":"markdown","d5b564c1":"markdown","c1e1c2b1":"markdown","6efd6ff1":"markdown","43f9015c":"markdown","bc662125":"markdown","cc6463c1":"markdown","67ade4d1":"markdown","15675b76":"markdown","3eaa8ca0":"markdown","141fdfc5":"markdown","885536ed":"markdown","120d60c3":"markdown","d757de2d":"markdown"},"source":{"011186c2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns',1000)\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nimport warnings\nwarnings.filterwarnings('ignore')","f633ab9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0265b1e9":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","f5ddcb92":"df.dtypes","52139c06":"df.isnull().sum()","18dd7a06":"df.drop(columns = ['Serial No.'], inplace = True)","dc9e2aa6":"for col in df.columns:\n    if col != 'Chance of Admit':\n        plt.figure(figsize = (10,5))\n        sns.boxplot(x = df[col])\n        plt.title(col)\n        plt.show()","61ce8f94":"sns.distplot(df['LOR '])\nplt.title('LOR')\nplt.show()","5f866791":"q25, q75 = df['LOR '].quantile(0.25), df['LOR '].quantile(0.75)\nIQR = q75-q25\ncutoff = IQR*1.5\nlower, upper = q25-cutoff, q75+cutoff\ndf = df[(df['LOR ']>lower) & (df['LOR ']<upper)]","7c97cd3e":"plt.figure(figsize=(10,5))\nsns.boxplot(df['LOR '])\nplt.title('LOR after removing outliers')\nplt.show()","5ea7f2a0":"plt.figure(figsize=(10,7))\nsns.heatmap(df.corr(), \n            annot = True, \n            fmt = '.3g',\n           vmax = 1, vmin = -1, center = 0,\n           cmap = 'coolwarm',\n           square = True)","66faa4f3":"rel = df.corr()\nrel.reset_index(inplace = True)\nrel.rename(columns={'index' : 'Factor'}, inplace = True)\nrel = rel[['Factor', 'Chance of Admit ']]\nrel.drop([7], axis = 0, inplace = True)\nrel = rel.sort_values(by='Chance of Admit ', ascending = False)\nrel","65b6a59b":"plt.figure(figsize = (7,5))\nsns.barplot(x='Chance of Admit ',\n                 y='Factor',\n                 data=rel)","788bc4e0":"plt.scatter(x = df['University Rating'],\n          y = df['Chance of Admit '])\nplt.title('Rating Vs Chance of Admission')","76d1c4c3":"df.head()","cada2fbb":"plt.scatter(x = df['GRE Score'],\n           y = df['TOEFL Score'])\nplt.title('GRE Vs TOEFL Score')","a950e9e5":"bins = [200, 300, 340]\nname = ['<300', '>=300']\ndf['GRERange'] = pd.cut(df['GRE Score'], bins, labels=name)\nplt.scatter(x = df['GRERange'],\n           y = df['Chance of Admit '])","7c59fabe":"S_300 = df[df['GRE Score'] >= 300]\nmean_300 = S_300['Chance of Admit '].mean()\nS_sub300 = df[df['GRE Score'] < 300]\nmean_sub300 = S_sub300['Chance of Admit '].mean()\nlikelihood_GREScore = mean_300\/mean_sub300\nlikelihood_GREScore","a7b8f048":"df[['Research', 'Chance of Admit ']].groupby('Research').mean()","3b38b70c":"pie = df[['Research', 'Chance of Admit ']].groupby('Research').count().reset_index().rename(columns = {'index' : 'Research', 'Chance of Admit ' : 'Count'})\npie","c056d6fc":"labels = ['Yes', 'No']\nplt.pie(pie['Count'], \n        labels=labels,\n        autopct='%.2f%%')\nplt.title('Candidates who have done research vs those who haven\\'t')\nplt.show()","e2dd5905":"df[['University Rating', 'Chance of Admit ']].groupby('University Rating').mean()","f774b000":"pie = df[['University Rating', 'Chance of Admit ']].groupby('University Rating').count().reset_index().rename(columns = {'index' : 'University Rating', 'Chance of Admit ' : 'Count'})\nplt.pie(pie['Count'], \n       labels = pie['University Rating'],\n       autopct = '%.2f%%')\nplt.title('No. of Universities of different rating')\nplt.show()","3a0f49f6":"df[['LOR ', 'Chance of Admit ']].groupby('LOR ').mean()","fa78f406":"pie = df[['LOR ', 'Chance of Admit ']].groupby('LOR ').count().reset_index().rename(columns = {'index' : 'LOR', 'Chance of Admit ' : 'Count'})\nplt.pie(pie['Count'],\n       labels=pie['LOR '],\n       autopct='%.2f%%')\nplt.title('%age of candidates getting various LOR scores')\nplt.show()","6cb151e1":"df[['SOP', 'Chance of Admit ']].groupby('SOP').mean()","8dcde185":"pie = df[['SOP', 'Chance of Admit ']].groupby('SOP').count().reset_index().rename(columns={'index' : 'SOP', 'Chance of Admit ' : 'Count'})\nplt.pie(pie['Count'],\n       labels=pie['SOP'],\n       autopct='%.2f%%')\nplt.title('%age of candidates getting various SOP scores')\nplt.show()","0840a50c":"bins = [0, 100, 120]\nnames = ['<100', '>=100']\ndf['TOEFLRange'] = pd.cut(df['TOEFL Score'], bins, labels = names)\ndf[['TOEFLRange', 'Chance of Admit ']].groupby('TOEFLRange').mean()","8c84f4da":"pie = df[['TOEFLRange', 'Chance of Admit ']].groupby('TOEFLRange').count().reset_index().rename(columns={'index' : 'TOEFLRange', 'Chance of Admit ' : 'Count'})\nplt.pie(pie['Count'],\n       labels=pie['TOEFLRange'],\n       autopct='%.2f%%')\nplt.title('>=300 GRE Score Vs <300 GRE Score')\nplt.show()","c5825ab5":"bins = [0, 300, 320]\nnames = ['<300', '>=300']\ndf['GRERange'] = pd.cut(df['GRE Score'], bins, labels = names)\ndf[['GRERange', 'Chance of Admit ']].groupby('GRERange').mean()","4bfb7498":"pie = df[['GRERange', 'Chance of Admit ']].groupby('GRERange').count().reset_index().rename(columns={'index' : 'GRERange', 'Chance of Admit ' : 'Count'})\nplt.pie(pie['Count'],\n       labels=pie['GRERange'],\n       autopct='%.2f%%')\nplt.title('>=300 GRE Score Vs <300 GRE Score')\nplt.show()","f596d6cb":"df.drop(columns = ['TOEFLRange', 'GRERange', 'Research'], inplace=True)","b1abe614":"X = df.drop(columns = ['Chance of Admit '])\ny = df['Chance of Admit ']","34598032":"for col in X.columns:\n    X[col] = (X[col]-X[col].min())\/(X[col].max()-X[col].min())","ae49fd65":"from sklearn.metrics import SCORERS\nSCORERS.keys()","83093e9d":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nlin = LinearRegression()\nr2score=cross_val_score(lin, X, y, cv=10, scoring='r2')\nmse = cross_val_score(lin, X, y, cv=10, scoring='neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nLinear_rmse = np.sqrt(np.abs(mse.mean()))\nLinear_r2score = r2score.mean()","69580632":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=80, max_depth=10, min_samples_split=20, min_samples_leaf=9)\nr2score = cross_val_score(rfr, X, y, cv=10, scoring = 'r2')\nmse = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nRfr_rmse = np.sqrt(np.abs(mse.mean()))\nRfr_r2score = r2score.mean()","991e09ff":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(learning_rate=0.03, \n                                n_estimators=80, \n                                min_samples_split=2, \n                                min_samples_leaf=9, \n                                max_depth=3)\nr2score = cross_val_score(gbr, X, y, cv=10, scoring = 'r2')\nscore = cross_val_score(gbr, X, y, cv=10, scoring='neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nGbr_rmse = np.sqrt(np.abs(mse.mean()))\nGbr_r2score = r2score.mean()","7f9a0d8e":"from sklearn.model_selection import GridSearchCV\nparams = {\n    'learning_rate' : [0.03],\n    'n_estimators' : [80],\n    'min_samples_split' : [2],\n    'min_samples_leaf' : [9],\n    'max_depth' : [3, 4, 5]\n}\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(estimator=model, param_grid = params)\ngrid.fit(X,y)\ngrid.best_estimator_","ca748b1d":"from sklearn.ensemble import AdaBoostRegressor\nreg = AdaBoostRegressor()\nr2score = cross_val_score(reg, X, y, cv=10, scoring = 'r2')\nscore = cross_val_score(reg, X, y, cv = 10, scoring = 'neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nAbr_rmse = np.sqrt(np.abs(mse.mean()))\nAbr_r2score = r2score.mean()","d7fd3b99":"from sklearn.linear_model import Ridge\nrid=Ridge(alpha=0.01)\nr2score = cross_val_score(rid, X, y, cv=10, scoring = 'r2')\nscore = cross_val_score(rid, X, y, cv = 10, scoring = 'neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nRidge_rmse = np.sqrt(np.abs(mse.mean()))\nRidge_r2score = r2score.mean()","e58f620b":"from sklearn.model_selection import GridSearchCV\nalphas = [0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20]\nreg=Ridge()\ngrid=GridSearchCV(estimator=reg,\n                 param_grid=dict(alpha=alphas))\ngrid.fit(X,y)\ngrid.best_estimator_","ce9d6055":"params = {'n_estimators' : [70, 80, 90],\n         'max_depth' : [5, 10, 15, 20],\n         'min_samples_split' : [10,12,14,16, 20, 25],\n         'min_samples_leaf' : [ 5, 7, 9, 11],\n         }\nmodel = RandomForestRegressor()\ngrid = GridSearchCV(estimator = model, param_grid = params)\ngrid.fit(X,y)\ngrid.best_estimator_","fd590b03":"from sklearn.svm import SVR\nsvr = SVR(kernel='linear', C=13, gamma=1e-09, epsilon=0.1)\nr2score = cross_val_score(svr, X, y, cv=10, scoring = 'r2')\nscore = cross_val_score(svr, X, y, cv = 10, scoring = 'neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nSVR_rmse = np.sqrt(np.abs(mse.mean()))\nSVR_r2score = r2score.mean()","fe76bb1b":"params = {\n    'kernel' : ['linear'],\n    'C' : [12, 13, 15, 17],\n    'gamma' : [1e-9, 1e-8, 1e-7, 1e-6],\n    'epsilon' : [0.1, 0.2, 0.3, 0.4]\n}\nmodel = SVR()\ngrid = GridSearchCV(estimator=model, param_grid=params)\ngrid.fit(X,y)\ngrid.best_estimator_","ad944678":"from mlxtend.regressor import StackingRegressor\nstreg = StackingRegressor(regressors=[lin, rfr, gbr, svr], \n                           meta_regressor=lin,\n                         use_features_in_secondary=True)\nr2score = cross_val_score(streg, X, y, cv=10, scoring = 'r2')\nscore = cross_val_score(streg, X, y, cv = 10, scoring = 'neg_mean_squared_error')\nprint(np.sqrt(np.abs(mse.mean())))\nprint(r2score.mean())\nStacking_rmse = np.sqrt(np.abs(mse.mean()))\nStacking_r2score = r2score.mean()","a21e1f3e":"result = pd.DataFrame({'Algorithm' : ['Linear Regression', 'Random Forest Regression', 'Gradient Boosting', 'AdaBoost Regression', 'Ridge Regression', 'SVR', 'Stacking Regression'], 'r2 Score' : [Linear_r2score, Rfr_r2score, Gbr_r2score, Abr_r2score, Ridge_r2score, SVR_r2score, Stacking_r2score]})\nresult = result.sort_values(by='r2 Score', ascending = False)\nresult","594b532f":"result = pd.DataFrame({'Algorithm' : ['Linear Regression', 'Random Forest Regression', 'Gradient Boosting', 'AdaBoost Regression', 'Ridge Regression', 'SVR', 'Stacking Regression'], 'RMSE' : [Linear_rmse, Rfr_rmse, Gbr_rmse, Abr_rmse, Ridge_rmse, SVR_rmse, Stacking_rmse]})\nresult = result.sort_values(by='RMSE', ascending = True)\nresult","e5240e8d":"The above values shows that CGPA, GRE and TOEFL boost the chance of admission the most ","5410f1fc":" From above plot, it can be seen that chance of admission are overall higher for admission in universities with higher rating. Many students have more than 90% chance of admission in university of ratiing 4 and 5 with almost all of them having more than 50% chance of admission.","651ccea7":"## Correlation of various factors on admission","d5b564c1":"## Normalization","c1e1c2b1":"Only 16.38% candidates got a score less than 300 in GRE","6efd6ff1":"Only 14.55% of candidates got a score < 100 in TOEFL","43f9015c":"There is a clear relationship between TOEFL Score and GRE Score. The fact that both TOEFL and GRE has reading and writing section might explain this to a certain extent","bc662125":"## Outliers","cc6463c1":"Again, as expected.","67ade4d1":"A candidate having TOEFL Score in three digit is 1.34 time more likely to get admitted ","15675b76":"From the above boxplot, it can be inferred that only LOR contains outliers. The outliers in the column can be removed by IQR range method","3eaa8ca0":"A candidate who has done research is 1.23 times more likely to get admitted compared to the one who hasn't","141fdfc5":"This was expected. Better the LOR, higher the chance of getting Admitted","885536ed":"Someone who has scored more than 300 in GRE is 1.41 times more likely to get admission compared to someone who has scored below 300","120d60c3":"To check for the presence of outliers in the dataset, we can make use of boxplot.","d757de2d":"Whoh!! A university having higher rating has a higher chance of admit"}}