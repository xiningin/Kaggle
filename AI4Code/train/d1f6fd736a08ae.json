{"cell_type":{"99fbd66d":"code","2ab5fb2d":"code","b8f7f7c6":"code","a9f3f563":"code","9de2140b":"code","d00694b0":"code","bfe2e890":"markdown"},"source":{"99fbd66d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ab5fb2d":"X_train = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/X_train.csv')\ny_train = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/y_train.csv')\nX_sub = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/X_test.csv')\ny_sub = pd.read_csv('\/kaggle\/input\/used-car-price-dataset-competition-format\/test_label\/y_test.csv')\ntrain_df = pd.merge(X_train, y_train, on = 'carID')\nsub_df = pd.merge(X_sub, y_sub, on = 'carID')\ntrain_df.drop('carID', axis = 1, inplace = True)\nsub_df.drop('carID', axis = 1, inplace = True)","b8f7f7c6":"#EDA\n#\ub370\uc774\ud130 \uae30\ubcf8\uc815\ubcf4\ntrain_df_eda = train_df.drop('price', axis = 1)\nprint('\ub370\uc774\ud130 \uae30\ubcf8\uc815\ubcf4')\nprint(train_df_eda.describe())\nprint('***********************************************\\n')\n#\uceec\ub7fc\uc815\ubcf4\nprint('\uceec\ub7fc\uc815\ubcf4')\nprint(train_df_eda.info())\nprint('***********************************************\\n')\n#\uceec\ub7fc\ub0b4\uc6a9\nprint('\uceec\ub7fc\ub0b4\uc6a9')\ncolumns = list(train_df_eda.columns)\nprint(columns)\nprint('***********************************************\\n')\n#Null data\nprint('\uacb0\uce21\uce58 \ud0d0\uc0c9')\nprint(train_df_eda.isnull().sum())\nprint('***********************************************\\n')\n#NUMERIC \/ CATEGORICAL \ub098\ub204\uae30\nprint('Columns \ub098\ub204\uae30')\nnum = [col for col in columns if train_df_eda[col].dtype != 'object']\ncat = [col for col in columns if train_df_eda[col].dtype == 'object']\nprint('Numeric Columns :', num, '\\nCategorical Columns :', cat)\nprint('***********************************************\\n')\n#Cat \ud0d0\uc0c9\n#-nunique()\nprint('Categorical \ud0d0\uc0c9 - nunique()')\nprint(train_df_eda[cat].nunique())\nprint('***********************************************\\n')\n#-unique()\nprint('Categorical \ud0d0\uc0c9 - unique()')\nfor col in cat:\n    print(col, ':' ,train_df_eda[col].unique())\nprint('***********************************************\\n')\n#Num \ud0d0\uc0c9\n#-Outlier\ndef outlier_detector(x):\n    Q1, Q3 = x.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    outlier_idx = x.loc[(x < Q1 - IQR * 1.5) | (x > Q3 - IQR * 1.5)].index\n    return outlier_idx\nprint('Numerical \ud0d0\uc0c9 - Outlier')\nfor col in num:\n    print(col, ':', len(outlier_detector(train_df_eda[col])))\nprint('***********************************************\\n')\n#-correlation (Pearson's)\nprint('Numerical \ud0d0\uc0c9 - correlation')\nprint(train_df_eda[num].corr())","a9f3f563":"#Preprocessing\n#year\ubcc0\uc218\uc0ad\uc81c?\ntrain_df_prep = train_df.drop('year', axis = 1)\nnum.remove('year')\n#Dummy\ntrain_df_prep = pd.get_dummies(train_df_prep, columns = cat)\nsub_df_prep = sub_df.drop('year', axis = 1)\nsub_df_prep = pd.get_dummies(sub_df_prep, columns = cat)\ntrain_df_prep, sub_df_prep = train_df_prep.align(sub_df_prep, join = 'outer', axis = 1, fill_value = 0)\nprint(train_df_prep.shape, sub_df_prep.shape)\n#Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_df_prep[num] = scaler.fit_transform(train_df_prep[num])\nsub_df_prep[num] = scaler.fit_transform(sub_df_prep[num])\n#data split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df_prep.drop('price', axis = 1), train_df_prep['price'], test_size = 0.33, random_state = 26)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","9de2140b":"#Training \/ Evaluation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n\n#RandomForest Regressor\nrf_rg = RandomForestRegressor(random_state = 26)\nrf_rg.fit(X_train, y_train)\nrf_preds = rf_rg.predict(X_test)\nrf_r2_score = r2_score(y_test, rf_preds)\nrf_mae = mean_absolute_error(y_test, rf_preds)\nrf_mse = mean_squared_error(y_test, rf_preds)\nrf_rmse = rf_mse ** 0.5\n\n#SVM Regressor\nsvm_rg = SVR()\nsvm_rg.fit(X_train, y_train)\nsvm_preds = svm_rg.predict(X_test)\nsvm_r2_score = r2_score(y_test, svm_preds)\nsvm_mae = mean_absolute_error(y_test, svm_preds)\nsvm_mse = mean_squared_error(y_test, svm_preds)\nsvm_rmse = svm_mse ** 0.5\n\n\n#xgb Regressor\nxgb_rg = XGBRegressor(random_state = 26)\nxgb_rg.fit(X_train, y_train)\nxgb_preds = xgb_rg.predict(X_test)\nxgb_r2_score = r2_score(y_test, xgb_preds)\nxgb_mae = mean_absolute_error(y_test, xgb_preds)\nxgb_mse = mean_squared_error(y_test, xgb_preds)\nxgb_rmse = xgb_mse ** 0.5\n\n\nprint('RandomFroest\\nR2 score : {:.4f}\\nMAE : {:.4f}\\nMSE : {:.4f}\\nRMSE : {:.4f}'.format(rf_r2_score, rf_mae, rf_mse, rf_rmse))\nprint('SVM\\nR2 score : {:.4f}\\nMAE : {:.4f}\\nMSE : {:.4f}\\nRMSE : {:.4f}'.format(svm_r2_score, svm_mae, svm_mse, svm_rmse))\nprint('XGB\\nR2 score : {:.4f}\\nMAE : {:.4f}\\nMSE : {:.4f}\\nRMSE : {:.4f}'.format(xgb_r2_score, xgb_mae, xgb_mse, xgb_rmse))","d00694b0":"#submittion\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nx = np.array(sub_df_prep.drop('price', axis = 1))\ny = np.array(sub_df_prep['price'])\ny_hat = xgb_rg.predict(x)\nr2_score = r2_score(y, y_hat)\nmae = mean_absolute_error(y, y_hat)\nmse = mean_squared_error(y, y_hat)\nrmse = xgb_mse ** 0.5\nprint('R2 score : {:.4f}\\nMAE : {:.4f}\\nMSE : {:.4f}\\nRMSE : {:.4f}'.format(r2_score, mae, mse, rmse))\ny_sub['price'] = y_hat\ny_sub.to_csv('submission.csv', index = False)","bfe2e890":"**Insight**\n1. \ubcc0\uc218\uc0c1\uad00\uad00\uacc4 \uace0\ub824\ud544\uc694 (year - mileage \/ tax - enginesize)\n2. outlier -> scaling"}}