{"cell_type":{"baf5a63b":"code","f716f086":"code","b06292db":"code","f133e22f":"code","2ac5cfb5":"code","b398e70c":"code","46756cf8":"code","9c69a046":"code","b9fb2b32":"code","e8be3367":"code","6575e169":"code","32f363e8":"code","92572dfa":"code","a1dfe7b3":"code","fe608d93":"code","7a9d3efb":"markdown","79c7502a":"markdown","da401601":"markdown","0b7133a9":"markdown","c4622986":"markdown","d3d9e985":"markdown","939c1024":"markdown","d2ca1c77":"markdown"},"source":{"baf5a63b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n","f716f086":"df=pd.read_csv('..\/input\/advertising-dataset\/advertising.csv')\ndf.head()\n","b06292db":"X=df.drop('Sales',axis=1)\ny=df['Sales']","f133e22f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)","2ac5cfb5":"corr = df.corr()\nfig, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")\nplt.xticks(range(len(corr.columns)), corr.columns);\nplt.yticks(range(len(corr.columns)), corr.columns)\nplt.show()","b398e70c":"mi = mutual_info_regression(X_train, y_train)\nmi = pd.Series(mi)\nmi.index = X_train.columns\nmi.sort_values(ascending=False, inplace = True)\nmi","46756cf8":"plt.title('Mutual information with respect to features')\nmi.plot.bar()\nplt.show()","9c69a046":"regressor_linear = LinearRegression()\nregressor_linear.fit(X_train, y_train)","b9fb2b32":"cv_linear = cross_val_score(estimator = regressor_linear, X = X_train, y = y_train, cv = 10)\n\ny_pred_linear_train = regressor_linear.predict(X_train)\nr2_score_linear_train = r2_score(y_train, y_pred_linear_train)\n\ny_pred_linear_test = regressor_linear.predict(X_test)\nr2_score_linear_test = r2_score(y_test, y_pred_linear_test)\n\nrmse_linear = (np.sqrt(mean_squared_error(y_test, y_pred_linear_test)))\nprint(\"CV: \", cv_linear.mean())\nprint('R2_score (train): ', r2_score_linear_train)\nprint('R2_score (test): ', r2_score_linear_test)\nprint(\"RMSE: \", rmse_linear)","e8be3367":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X_train)\npoly_reg.fit(X_poly, y_train)\nregressor_poly2 = LinearRegression()\nregressor_poly2.fit(X_poly, y_train)","6575e169":"cv_poly2 = cross_val_score(estimator = regressor_poly2, X = X_train, y = y_train, cv = 10)\n\ny_pred_poly2_train = regressor_poly2.predict(poly_reg.fit_transform(X_train))\nr2_score_poly2_train = r2_score(y_train, y_pred_poly2_train)\n\ny_pred_poly2_test = regressor_poly2.predict(poly_reg.fit_transform(X_test))\nr2_score_poly2_test = r2_score(y_test, y_pred_poly2_test)\n\nrmse_poly2 = (np.sqrt(mean_squared_error(y_test, y_pred_poly2_test)))\nprint('CV: ', cv_poly2.mean())\nprint('R2_score (train): ', r2_score_poly2_train)\nprint('R2_score (test): ', r2_score_poly2_test)\nprint(\"RMSE: \", rmse_poly2)","32f363e8":"models = [('Linear Regression', rmse_linear, r2_score_linear_train, r2_score_linear_test, cv_linear.mean()),\n          ('Polynomial Regression (2nd)', rmse_poly2, r2_score_poly2_train, r2_score_poly2_test, cv_poly2.mean()),\n         ]         ","92572dfa":"predict = pd.DataFrame(data = models, columns=['Model', 'RMSE', 'R2_Score(training)', 'R2_Score(test)','Cross-Validation'])\npredict","a1dfe7b3":"f, axes = plt.subplots(2,1, figsize=(8,8))\npredict.sort_values(by=['R2_Score(training)'], ascending=False, inplace=True)\nsns.barplot(x='R2_Score(training)', y='Model', data = predict, palette='Blues_d', ax = axes[0])\naxes[0].set_xlabel('R2 Score (Training)', size=10)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\npredict.sort_values(by=['R2_Score(test)'], ascending=False, inplace=True)\nsns.barplot(x='R2_Score(test)', y='Model', data = predict, palette='Reds_d', ax = axes[1])\naxes[1].set_xlabel('R2 Score (Test)', size=10)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\nplt.show()","fe608d93":"predict.sort_values(by=['RMSE'], ascending=False, inplace=True)\nf, axe = plt.subplots(1,1, figsize=(6,6))\nsns.barplot(x='Model', y='RMSE', data=predict, ax = axe)\naxe.set_xlabel('Model', size=16)\naxe.set_ylabel('RMSE', size=16)\n\nplt.show()","7a9d3efb":"# Compare two Regressions Results","79c7502a":"# Define Features and Lable","da401601":"Since R2 score of polynomial regresstion is a liitle bot higher than linear regresstion the plynomial model fits better on our data","0b7133a9":"The r2_score function computes the coefficient of determination, usually denoted as R\u00b2.\n\nIt represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\n\nAs such variance is dataset dependent, R\u00b2 may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R\u00b2 score of 0.0.\nNote that r2_score calculates unadjusted R\u00b2 without correcting for bias in sample variance of y.","c4622986":"# Linear Regresstion","d3d9e985":"The MI between two random variables is a non-negative value, which measures the dependency between the variables.\n\nIt is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","939c1024":"# Check Mutual Information","d2ca1c77":"# PolyNomialRegression"}}