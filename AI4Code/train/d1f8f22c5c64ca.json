{"cell_type":{"4aaa4f8c":"code","cf0b6cfd":"code","4f9a392c":"code","a48f4297":"code","70e5a4f1":"code","5eca9591":"code","6c4d1f01":"code","66d91ccc":"code","0ab0a5a2":"code","b7208fb9":"code","a3dcedbe":"code","94d31e76":"code","87bca0b4":"markdown","8e84b39f":"markdown","fe50c4db":"markdown","1509a88e":"markdown","b665dc81":"markdown","d98a42c5":"markdown","5389371a":"markdown","6b5edf22":"markdown","a2ef729e":"markdown","5d6675c8":"markdown","15988871":"markdown","d25fc709":"markdown"},"source":{"4aaa4f8c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cf0b6cfd":"SEED = 123","4f9a392c":"all_data = pd.read_csv('..\/input\/calcofi\/bottle.csv', usecols=[\"Salnty\", \"T_degC\"])\nprint(all_data.shape)\nall_data.head()","a48f4297":"lr_data = all_data[:500].dropna(axis=0, how='any') #dropping rows with null values\nlr_data = lr_data.sample(frac=1, random_state=SEED).reset_index(drop=True) #random shuffle of the data\n\n#60-20-20 split\nlr_train_data = lr_data[:int(len(lr_data)*0.6)]\nlr_valid_data = lr_data[int(len(lr_data)*0.6):int(len(lr_data)*0.8)]\nlr_test_data = lr_data[int(len(lr_data)*0.8):]\n\n#checking we have no null vals\nlr_train_data.isnull().sum()","70e5a4f1":"sns.pairplot(data=lr_train_data, height=5)","5eca9591":"#converting from pandas dataframe - to numpy array\nx = lr_data['Salnty'].values.reshape(-1, 1) #-1 = all\ny = lr_data['T_degC'].values.reshape(-1, 1)\naug_x = np.hstack([x, np.ones_like(x)]) # the column of ones is for?","6c4d1f01":"def ordinary_least_squares(x, y):\n    xTx = x.T.dot(x)\n    xTx_inv = np.linalg.inv(xTx)\n    w = xTx_inv.dot(x.T.dot(y))\n    return w","66d91ccc":"def mean_squared_error(x, y, w):\n    y_hat = x.dot(w)\n    loss = np.mean((y - y_hat) ** 2)\n    return loss","0ab0a5a2":"def polynomial(values, coeffs):\n    assert len(values.shape) == 2\n    # Coeffs are assumed to be in order 0, 1, ..., n-1\n    expanded = np.hstack([coeffs[i] * (values ** i) for i in range(0, len(coeffs))])\n    return np.sum(expanded, axis=-1)\n\ndef plot_polynomial(coeffs, x_range=[x.min(), x.max()], color='red', label='polynomial', alpha=1.0):\n    values = np.linspace(x_range[0], x_range[1], 1000).reshape([-1, 1])\n    poly = polynomial(values, coeffs)\n    plt.plot(values, poly, color=color, linewidth=2, label=label, alpha=alpha)","b7208fb9":"linear_coeff = ordinary_least_squares(aug_x, y)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(x, y, color='green')\nplot_polynomial([linear_coeff[1,0], linear_coeff[0,0]])","a3dcedbe":"def polynomial_features(x, order):\n    features = np.hstack([x**i for i in range(0, order+1)])\n    return features\n\ndef plot_regression(x, y, degree):\n    features = polynomial_features(x, degree)\n    w = ordinary_least_squares(features, y)\n    loss = mean_squared_error(features, y, w)\n    \n    plt.figure(figsize=(10, 5))\n    plt.scatter(x, y, color='green')\n    plot_polynomial(w)\n    plt.title(\"Polynomial degree: {}, loss: {}\".format(degree,loss))","94d31e76":"for i in range(1,4):\n    plot_regression(x, y, i)","87bca0b4":"### Data\n\nI decided to use the CALCOFI Dataset which consists of over 60 years of oceanographic data samples. This dataset is nice and clean and great for practicing implementing regression models -> https:\/\/www.kaggle.com\/mirichoi0218\/insurance\n\nI will be using only the water temperature and water salinity features in the dataset.","8e84b39f":"### MSE (Mean Squared Absolute Error)\n\nFor our loss function we will be using the MSE (Mean Squared Error). This is a measure that tells us how far\/close the points are to the regression line. In other words, we are taking the square of the residuals.\n\nWe could simply take the absolute value of the residuals (distance from regression line) but then getting a close point even closer, or a far point a little but closer has the same value. If we take the squared sum of the residuals then we will have a better generalization for the whole of our data.\n\n![Mean Square Error Formula](https:\/\/www.gstatic.com\/education\/formulas2\/397133473\/en\/mean_squared_error.svg)\n\nNote that a downside of MSE is that it is sensitive to outliers as they have a strong effect on the loss function and we can be mislead by this loss value.\n\n","fe50c4db":"In the following cell we can see that we are able to use the two functions above and ordinary least squares to create a linear regression model that fits to our dataset.","1509a88e":"## Linear Regression (Single Input\/Single Output)\n\nFor this example I am using a single input variable and a single output variable in this linear regression model. Note that this dataset is extremely large, and I am only sampling a small number of rows. If we sampled all the rows it is more difficult to visualize the model.\n\nNote: I dont know how representative of the real-world the dataset is, but it will be good enough for practice!","b665dc81":"### Visualizing the Data","d98a42c5":"### OLS (Ordinary Least Squares)\n\nOrdinary least squares is a method for getting the unknown parameters of a linear regression model. It chooses these parameters by minimizing the sum of the squares of the differences between the observed label and the predicted label.\n\nThe OLS formula is shown below and basically we are use this formula to find the Beta parameters which will minimize our loss.\n\n![Ordinary Least Squares Formula](https:\/\/www.gstatic.com\/education\/formulas2\/397133473\/en\/ordinary_least_squares.svg)","5389371a":"Seaborn Pairs Plot https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html\n- A pairs plot allows us to see both distribution of single variables and relationships between two variables.\n- This is a really helpful plot type that I should be using more.\n\nIn linear regression, the model is really sensitive to outliers so I may have to filter values that are in the extremes (depending on the sample).","6b5edf22":"## Notes\n\n**Regression Model Practice**\n\nThe following notebooks and links helped me understand linear regression and further develop my Machine Learning Skill-set.\n\n- [Sudhir Kumar's Linear Regression Tutorial](https:\/\/www.kaggle.com\/sudhirnl7\/linear-regression-tutorial)\n- [Fares Sayah's Linear Regression \ud83d\udcc8 House \ud83c\udfe1 price \ud83d\udcb5 prediction](https:\/\/www.kaggle.com\/faressayah\/linear-regression-house-price-prediction)\n- [Ordinary Least Squares Explained Visually](https:\/\/setosa.io\/ev\/ordinary-least-squares-regression\/)\n\nThis list is not exhaustive, and I used so many more resources than this to furhter understand OLS. If you have some links that you think I should add feel free to comment them below.","a2ef729e":"### Linear Regression that is non-linear?\n\nThis really confused me when I first heard it, but once it makes sense it is very simple.\n\nThe co-efficients that are the parameters in our model are linear in nature, and when multiply these by their respective degree's of x, we can map a non-linear model to these linear inputs.\n\n![Linear Coefficients Pic](https:\/\/s0.wp.com\/latex.php?latex=Y+%3D%5Cbeta+_%7B0%7D+%2B+%5Cbeta+_%7B1%7DX_%7B1%7D+%2B+%5Cbeta+_%7B2%7DX_%7B2%7D+%2B+%5Ccdots+%2B+%5Cbeta+_%7Bk%7DX_%7Bk%7D&bg=ffffff&fg=000&s=0&c=20201002&is-pending-load=1)\n\nFor more info on Linear Regression, check out this article here -> [Linear vs Non-Linear Regression](https:\/\/statisticsbyjim.com\/regression\/difference-between-linear-nonlinear-regression-models\/)","5d6675c8":"Although we may be fitting well to the training data, in practice we should be evaluating the model on a seperate dataset to get an unbiased estimate of model performance.\n\nThis can be done in two ways.\n- Using a Training, Validation and Test set.\n- Using Cross-Validation.\n\nI will skip this step in this example, but hope that this notebook helped someone else get a better intution behind Ordinary Least Squares!","15988871":"So, by increasing the degree of the polynomial above 1, we can fit to \"non-linear\" data.\n\nIn the following cell we are plotting degree 1-3 polynomials. We can see that as we increase the degree of the polynomial, we are able to better fit our model to the training data. On every degree increase the loss decreases!","d25fc709":"### Linear Regression Model\n\nTo start we are defining two functions here. They are both use to plot a line to the polynomial function that we calculated to best fit our data.\n\nSee more about the specifics of Numpy here -> [Numpy Docs](https:\/\/numpy.org\/doc\/1.21\/)"}}