{"cell_type":{"5d81e34a":"code","bde6559a":"code","40db3298":"code","b807726b":"code","153244cb":"code","24752b0e":"code","df77bd40":"code","110c4c9c":"code","af51c981":"code","8f46142b":"code","7d121600":"code","c92f40a0":"code","30b28bc4":"code","d5de7463":"code","251b116f":"code","3abf9852":"code","7fb6a1a4":"code","92cbf7ee":"code","187bbc77":"code","f5e18e01":"code","6111caff":"code","dcd015f3":"code","7acd85b6":"code","53e4475e":"code","5e6427fb":"code","fc235006":"code","24509717":"code","2b47b910":"code","6ab82b7b":"code","ee949183":"code","c5a4bbe4":"code","6dc53d54":"code","13367466":"code","c5790ce9":"code","d776cdd1":"code","9c3ea358":"code","d894565d":"code","7bae5cf4":"code","31c0c4c2":"code","edb92312":"code","fc199b86":"code","4ebc148a":"code","8ba9f28a":"code","3abeeba6":"code","e249ea23":"code","02ec951d":"code","721247d2":"markdown","595a299c":"markdown","2a68d0e2":"markdown","391a87a3":"markdown","10db2ae8":"markdown","7fd80a22":"markdown","79e1260f":"markdown","810ac12a":"markdown","dd7ef125":"markdown","e354503b":"markdown","5d25403b":"markdown","94d0c77a":"markdown","575a9efa":"markdown","a8481dec":"markdown","af3d9e4b":"markdown","5f85e90c":"markdown","cd3f1123":"markdown","5146757e":"markdown","830ccfad":"markdown","dcb39035":"markdown","a1704f31":"markdown","63b899ea":"markdown","e4783ab2":"markdown","b2979041":"markdown","1497cede":"markdown","83668998":"markdown","77b3d69d":"markdown"},"source":{"5d81e34a":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom matplotlib.gridspec import GridSpec\nfrom time import time\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, cross_validate\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport os\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(['ggplot', 'seaborn'])","bde6559a":"df = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","40db3298":"df.info()","b807726b":"df.describe().T","153244cb":"df.sample(10)","24752b0e":"data = df['Attrition'].value_counts()\n\n_ = sns.barplot(data.index, data.values, palette='muted')\n\ndf.loc[df['Attrition'] == 'Yes', 'Attrition'] = 1\ndf.loc[df['Attrition'] == 'No', 'Attrition'] = 0","df77bd40":"def PlotDists(feature, position):\n    '''\n    '''\n    g = sns.catplot(x=feature, y='Attrition',\n                    data=df, palette='muted', kind='bar', height=6, ax=position)\n\n    g.despine(left=True)\n\n    g = g.set_ylabels('Attrition probability')\n\n    # This is needed, see: https:\/\/stackoverflow.com\/questions\/33925494\/seaborn-produces-separate-figures-in-subplots\n    plt.close(g.fig)","110c4c9c":"to_plot = ['BusinessTravel', 'Department', 'EducationField', 'Gender',\n           'JobRole', 'MaritalStatus', 'OverTime']\n\nfig, ax = plt.subplots(4, 2, figsize=(20, 20), sharex=False, sharey=False)\n\n# Flatten out the axis object\nax = ax.ravel()\n\nfor i in range(7):\n\n    PlotDists(to_plot[i], ax[i])\n\nplt.tight_layout()\nplt.show()","af51c981":"# How old are the employeed by gender?\nprint('The average age for men is:',\n      df.loc[df['Gender'] == 'Male', 'Age'].mean())\n\nprint('The average age for women is:',\n      df.loc[df['Gender'] == 'Female', 'Age'].mean())\n\ng = sns.FacetGrid(df, col='Gender')\ng = g.map(sns.distplot, 'Age')","8f46142b":"# Let us explore the likelihood of quitting by gender.\ng = sns.barplot(x='Gender', y='Attrition', data=df)","7d121600":"# I want to explore the probability of leaving based on job satisfaction by gender.\n_ = sns.violinplot(x='Attrition', y='JobSatisfaction',\n                   data=df, hue='Gender', palette='muted')","c92f40a0":"# How about salary? Is there any disparity between sexes?\ng = sns.barplot(x='Gender', y='MonthlyIncome', data=df,\n                hue='Attrition', palette='muted')","30b28bc4":"_ = sns.barplot(x='JobSatisfaction', y='MonthlyIncome',\n                data=df, hue='Attrition', palette='muted')","d5de7463":"corr = df.corr()\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\n_ = sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f', linewidths=.5, ax=ax)","251b116f":"df.describe(include=['O'])","3abf9852":"to_drop = ['EmployeeCount', 'StandardHours', 'Over18', 'EmployeeNumber']\n\ndf.drop(columns=to_drop, inplace=True)","7fb6a1a4":"# Get numerical and categorical features\nnumerical = df.select_dtypes(exclude=['object'])\ncategorical = df.select_dtypes(['object'])","92cbf7ee":"# One hot encode the remaining variables.\ntravel_map = {'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}\n\ndf['BusinessTravel'] = df['BusinessTravel'].map(travel_map)\n\n# One hot encode categorical features\ndf = pd.get_dummies(df, drop_first=True)","187bbc77":"# Split the dataset\nX = df.drop('Attrition', axis=1)\ny = df['Attrition']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=.2, random_state=42, stratify=y)","f5e18e01":"# Oversampling the minority class\nX_train_up, y_train_up = resample(X_train[y_train == 1],\n                                  y_train[y_train == 1],\n                                  replace=True,\n                                  n_samples=X_train[y_train == 0].shape[0],\n                                  random_state=1)\n\nX_train_up = pd.concat([X_train[y_train == 0], X_train_up])\ny_train_up = pd.concat([y_train[y_train == 0], y_train_up])","6111caff":"# Downsample majority class\nX_train_dw, y_train_dw = resample(X_train[y_train == 0],\n                                  y_train[y_train == 0],\n                                  replace=True,\n                                  n_samples=X_train[y_train == 1].shape[0],\n                                  random_state=1)\n\nX_train_dw = pd.concat([X_train[y_train == 1], X_train_dw])\ny_train_dw = pd.concat([y_train[y_train == 1], y_train_dw])","dcd015f3":"# Check the shapes of the classes\nprint(\"Original shape:\", X_train.shape, y_train.shape)\nprint(\"Upsampled shape:\", X_train_up.shape, y_train_up.shape)\nprint(\"Downsampled shape:\", X_train_dw.shape, y_train_dw.shape)","7acd85b6":"# Check the principal components\n\npca = PCA(n_components=None, svd_solver=\"full\")\n\nscaler = StandardScaler()\nscaler = scaler.fit_transform(X_train)\n\npca.fit(scaler)\n\ncum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n\nplt.figure(figsize=(12, 6))\n\nn_features = len(cum_var_exp) + 1\n\nplt.bar(range(1, n_features), pca.explained_variance_ratio_, align=\"center\",\n        color='magenta', label=\"Individual explained variance\")\n\nplt.step(range(1, n_features), cum_var_exp, where=\"mid\",\n         label=\"Cumulative explained variance\", color='blue')\n\nplt.xticks(range(1, n_features))\nplt.legend(loc=\"best\")\n\nplt.xlabel(\"Principal component index\", {\"fontsize\": 14})\nplt.ylabel(\"Explained variance ratio\", {\"fontsize\": 14})\nplt.title(\"PCA on training data\", {\"fontsize\": 16})","53e4475e":"print('We need', np.where(cum_var_exp > 0.90)[\n      0][0], 'features to explain 90% of the variation of the data.')\nprint('We need', np.where(cum_var_exp > 0.95)[\n      0][0], 'features to explain 95% of the variation of the data.')\nprint('We need', np.where(cum_var_exp > 0.99)[\n      0][0], 'features to explain 99% of the variation of the data.')","5e6427fb":"# Utility function to report best scores\n\n\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","fc235006":"# Preparing the parameters grid\n# Number of trees in the random forest\nn_estimators = [int(n) for n in np.linspace(200, 1000, 100)]\n\n# Number of features to consider at each split\nmax_features = ['auto', 'sqrt', 'log2']\n\n# Maximum number of levels in tree\nmax_depth = [int(n) for n in np.linspace(10, 100, 10)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of leaf required at each node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting each samples for training each tree\nbootstrap = [True, False]\n\n# Construct the grid\nparam_grid = {\n    'randomforestclassifier__n_estimators': n_estimators,\n    'randomforestclassifier__max_features': max_features,\n    'randomforestclassifier__max_depth': max_depth,\n    'randomforestclassifier__min_samples_split': min_samples_split,\n    'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n    'randomforestclassifier__bootstrap': bootstrap\n}\n\ndatasets = {'imbalanced': (X_train, y_train),\n            'up_sampled': (X_train_up, y_train_up),\n            'dw_sampled': (X_train_dw, y_train_dw)}\n\nfor dataset in datasets:\n\n    pipeline = make_pipeline(StandardScaler(),\n                             RandomForestClassifier(n_estimators=200,\n                                                    class_weight='balanced',\n                                                    random_state=42))\n\n    n_iter_search = 20\n\n    gs_rf = RandomizedSearchCV(pipeline, param_distributions=param_grid,\n                               scoring='f1', cv=10, n_jobs=-1, refit=True, iid=False, n_iter=n_iter_search)\n\n    start = time()\n\n    gs_rf.fit(datasets[dataset][0], datasets[dataset][1])\n\n    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n          \" parameter settings.\" % ((time() - start), n_iter_search))\n\n    report(gs_rf.cv_results_)\n\n    print(\"The best hyperparameters for {} data:\".format(dataset))\n    for hyperparam in gs_rf.best_params_.keys():\n        print(hyperparam[hyperparam.find(\"__\") + 2:],\n              \": \", gs_rf.best_params_[hyperparam])\n\n    print(\n        \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_rf.best_score_) * 100))","24509717":"def plot_roc_and_conf_matrix(clf, X, y, figure_size=(15, 5)):\n    '''\n    Plot both confusion matrix and ROC curce on the same figure.\n    Parameters:\n    -----------\n    clf : sklearn.estimator\n        model to use for predicting class probabilities.\n    X : array_like\n        data to predict class probabilities.\n    y : array_like\n        true label vector.\n    figure_size : tuple (optional)\n        size of the figure.\n    Returns:\n    --------\n    plot : matplotlib.pyplot\n        plot confusion matrix and ROC curve.\n    '''\n    # Compute tpr, fpr, auc and confusion matrix\n    fpr, tpr, thresholds = roc_curve(y, clf.predict_proba(X)[:, 1])\n    auc = roc_auc_score(y, clf.predict_proba(X)[:, 1])\n    conf_mat_clf = confusion_matrix(y, clf.predict(X))\n\n    # Define figure size and figure ratios\n    plt.figure(figsize=figure_size)\n    gs = GridSpec(1, 2, width_ratios=(1, 2))\n\n    # Plot confusion matrix\n    ax0 = plt.subplot(gs[0])\n    ax0.matshow(conf_mat_clf, cmap='coolwarm', alpha=0.2)\n\n    for i in range(2):\n        for j in range(2):\n            ax0.text(x=j, y=i, s=conf_mat_clf[i, j], ha=\"center\", va=\"center\")\n\n    plt.title(\"Confusion matrix\", y=1.1, fontdict={\"fontsize\": 20})\n    plt.xlabel(\"Predicted\", fontdict={\"fontsize\": 14})\n    plt.ylabel(\"Actual\", fontdict={\"fontsize\": 14})\n\n    # Plot ROC curce\n    ax1 = plt.subplot(gs[1])\n    ax1.plot(fpr, tpr, label=\"auc = {:.3f}\".format(auc))\n    plt.title(\"ROC curve\", y=1, fontdict={\"fontsize\": 20})\n    ax1.plot([0, 1], [0, 1], \"r--\")\n    plt.xlabel(\"False positive rate\", fontdict={\"fontsize\": 16})\n    plt.ylabel(\"True positive rate\", fontdict={\"fontsize\": 16})\n    plt.legend(loc=\"lower right\", fontsize=\"medium\")","2b47b910":"# Rename variables\nX_train, y_train = X_train_up.copy(), y_train_up.copy()\n\n# Free some memory\ndel X_train_dw, y_train_dw, X_train_up, y_train_up\n\n# Refit the classifier with best parameters\npipeline = make_pipeline(StandardScaler(),\n                         RandomForestClassifier(n_estimators=765,\n                                                min_samples_split=2,\n                                                min_samples_leaf=1,\n                                                max_features='log2',\n                                                max_depth=30,\n                                                bootstrap=False,\n                                                class_weight='balanced',\n                                                n_jobs=-1,\n                                                random_state=42))\n\nrf = pipeline.fit(X_train, y_train)\n\nrf_pred = rf.predict(X_test)\n\nprint('F1 score for the RandomForrest is', f1_score(y_test, rf_pred))\n\n\n# Since the classes are balanced now, we can use the ROC curve to estimate the model skill. Otherwise we should have also plot a precision-recall curve.\nplot_roc_and_conf_matrix(pipeline, X_test, y_test)","6ab82b7b":"# Plot feature importance according to the RandomForrest calssifier\n# can also use: pipeline.named_steps['predictor'].feature_importances_\nimportant_features = pd.Series(\n    data=pipeline.steps[1][1].feature_importances_, index=X_train.columns)\nimportant_features.sort_values(ascending=False, inplace=True)\n\n_ = sns.barplot(x=important_features.values,\n                y=important_features.index, orient='h')","ee949183":"# Drop features that are highly correlated with each other but that are less important.\nto_drop = ['JobLevel', 'TotalWorkingYears',\n           'YearsAtCompany', 'PerformanceRating']\n\nX_train_red = X_train.drop(columns=to_drop).copy()\nX_test_red = X_test.drop(columns=to_drop).copy()\n\ny_train_red = y_train.drop(columns=to_drop).copy()\ny_test_red = y_test.drop(columns=to_drop).copy()","c5a4bbe4":"# Let us train again our classifier and see the performance with the reduced dataset.\npipeline = make_pipeline(StandardScaler(),\n                         RandomForestClassifier(n_estimators=765,\n                                                min_samples_split=2,\n                                                min_samples_leaf=1,\n                                                max_features='log2',\n                                                max_depth=30,\n                                                bootstrap=False,\n                                                class_weight='balanced',\n                                                n_jobs=-1,\n                                                random_state=42))\n\nrf = pipeline.fit(X_train_red, y_train_red)","6dc53d54":"rf_pred = rf.predict(X_test_red)\n\n\nscorer = make_scorer(f1_score)\n\nscore = cross_val_score(pipeline, X_test_red, y_test_red, scoring=scorer, cv=5)\n\nprint(score.mean())\n\nprint('F1 score for the RandomForrest is', f1_score(y_test_red, rf_pred))\n\nplot_roc_and_conf_matrix(pipeline, X_test_red, y_test_red)","13367466":"important_features = pd.Series(\n    data=pipeline.steps[1][1].feature_importances_, index=X_train_red.columns)\nimportant_features.sort_values(ascending=False, inplace=True)\n\n_ = sns.barplot(x=important_features.values,\n                y=important_features.index, orient='h')","c5790ce9":"# Let us try different algorithms\n\nclfs = []\n\nclfs.append(LogisticRegression(random_state=42))\nclfs.append(SVC(random_state=42))\nclfs.append(DecisionTreeClassifier(random_state=42))\nclfs.append(RandomForestClassifier(random_state=42))\nclfs.append(KNeighborsClassifier())\nclfs.append(LinearDiscriminantAnalysis())\nclfs.append(ExtraTreesClassifier(random_state=42))\n\ncv_results = []\n\nfor clf in clfs:\n\n    result = cross_val_score(clf, X_train, y_train,\n                             scoring='f1', cv=10, n_jobs=-1)\n\n    cv_results.append(result)\n","d776cdd1":"# Plot the CV results\ncv_means = []\ncv_stds = []\n\nfor result in cv_results:\n\n    cv_means.append(result.mean())\n    cv_stds.append(result.std())\n\nalgs = ['LogisticRegression', 'SVC', 'DecisionTree',\n        'RandomForrest', 'KNN', 'LinearDiscriminant', 'ExtraTrees']\n\ndf_results = pd.DataFrame(\n    {'cv_mean': cv_means, 'cv_std': cv_stds, 'algorithm': algs})\n\ng = sns.barplot('cv_mean', 'algorithm', data=df_results,\n                palette='muted', orient='h', xerr=cv_stds)\n\ng.set_xlabel('F1 score')\ng.set_title('CV Scores')","9c3ea358":"pip_svc = make_pipeline(StandardScaler(),\n                        SVC(class_weight='balanced',\n                            probability=True))\n\nsvc_params = {'svc__kernel': ['rbf', 'poly'],\n              'svc__gamma': [0.001, 0.01, 0.1, 1],\n              'svc__C': [1, 10, 50, 100, 200, 300, 1000]}\n\ngs_svc = RandomizedSearchCV(pip_svc, param_distributions=svc_params,\n                            scoring='f1', cv=10, n_jobs=-1, refit=True, iid=False, n_iter=n_iter_search)\n\ngs_svc.fit(X_train, y_train)\n\nprint(\"The best hyperparameters:\")\nprint(\"-\" * 25)\n\nfor hyperparam in gs_svc.best_params_.keys():\n\n    print(hyperparam[hyperparam.find(\"__\") + 2:],\n          \": \", gs_svc.best_params_[hyperparam])\n\n# Print CV\nprint('The best 10-folds CV f1-score is: {:.2f}%'.format(\n    np.mean(gs_svc.best_score_) * 100))","d894565d":"plot_roc_and_conf_matrix(gs_svc, X_test, y_test)","7bae5cf4":"dtc = DecisionTreeClassifier(class_weight='balanced')\n\npip_ada = make_pipeline(StandardScaler(),\n                        AdaBoostClassifier(dtc, random_state=42))\n\nada_params = {\"adaboostclassifier__base_estimator__criterion\": [\"gini\", \"entropy\"],\n              \"adaboostclassifier__base_estimator__splitter\":   [\"best\", \"random\"],\n              \"adaboostclassifier__algorithm\": [\"SAMME\", \"SAMME.R\"],\n              \"adaboostclassifier__n_estimators\": [1, 10, 50, 100],\n              \"adaboostclassifier__learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 1.5]\n              }\n\ngs_ada_dtc = RandomizedSearchCV(pip_ada, param_distributions=ada_params,\n                                cv=10, scoring='f1', refit=True, iid=False, n_jobs=-1, verbose=1, n_iter=n_iter_search)\n\ngs_ada_dtc.fit(X_train, y_train)\n\nprint(\"The best hyperparameters:\")\nprint(\"-\" * 25)\n\nfor hyperparam in gs_ada_dtc.best_params_.keys():\n\n    print(hyperparam[hyperparam.find(\"__\") + 2:],\n          \": \", gs_ada_dtc.best_params_[hyperparam])\n\n# Print CV\nprint('The 10-folds CV f1-score is: {:.2f}%'.format(\n    np.mean(gs_ada_dtc.best_score_) * 100))\n","31c0c4c2":"plot_roc_and_conf_matrix(gs_ada_dtc, X_test, y_test)","edb92312":"pip_rf = make_pipeline(StandardScaler(),\n                       RandomForestClassifier(class_weight='balanced',\n                                              random_state=42))\n\nn_estimators = [int(n) for n in np.linspace(200, 1000, 100)]\nmax_features = ['auto', 'sqrt', 'log2']\nmax_depth = [int(n) for n in np.linspace(10, 100, 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrf_params = {'randomforestclassifier__n_estimators': n_estimators,\n             'randomforestclassifier__max_features': max_features,\n             'randomforestclassifier__max_depth': max_depth,\n             'randomforestclassifier__min_samples_split': min_samples_split,\n             'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n             'randomforestclassifier__bootstrap': bootstrap\n             }\n\ngs_rf = RandomizedSearchCV(pip_rf,\n                           param_distributions=rf_params,\n                           scoring='f1',\n                           cv=10,\n                           n_jobs=-1,\n                           refit=True,\n                           iid=False,\n                           n_iter=n_iter_search)\n\ngs_rf.fit(X_train, y_train)\n\nprint(\"The best hyperparameters:\")\nprint(\"-\" * 25)\n\nfor hyperparam in gs_rf.best_params_.keys():\n\n    print(hyperparam[hyperparam.find(\"__\") + 2:],\n          \": \", gs_rf.best_params_[hyperparam])\n\n# Print CV\nprint('The 10-folds CV f1-score is: {:.2f}%'.format(\n    np.mean(gs_rf.best_score_) * 100))","fc199b86":"plot_roc_and_conf_matrix(gs_rf, X_test, y_test)","4ebc148a":"pip_et = make_pipeline(StandardScaler(),\n                       ExtraTreesClassifier(class_weight='balanced',\n                                            random_state=42))\n\net_params = {\"extratreesclassifier__max_depth\": [None],\n             \"extratreesclassifier__max_features\": [1, 3, 10],\n             \"extratreesclassifier__min_samples_split\": [2, 3, 10],\n             \"extratreesclassifier__min_samples_leaf\": [1, 3, 10],\n             \"extratreesclassifier__bootstrap\": [False],\n             \"extratreesclassifier__n_estimators\": [100, 300],\n             \"extratreesclassifier__criterion\": [\"gini\"]}\n\ngs_et = RandomizedSearchCV(pip_et,\n                           param_distributions=et_params,\n                           scoring='f1',\n                           cv=10,\n                           n_jobs=-1,\n                           refit=True,\n                           iid=False,\n                           n_iter=n_iter_search)\n\ngs_et.fit(X_train, y_train)\n\nprint(\"The best hyperparameters:\")\nprint(\"-\" * 25)\n\nfor hyperparam in gs_et.best_params_.keys():\n\n    print(hyperparam[hyperparam.find(\"__\") + 2:],\n          \": \", gs_et.best_params_[hyperparam])\n\n# Print CV\nprint('The 10-folds CV f1-score is: {:.2f}%'.format(\n    np.mean(gs_et.best_score_) * 100))","8ba9f28a":"plot_roc_and_conf_matrix(gs_et, X_test, y_test)","3abeeba6":"# important_features = pd.Series(\n#     data=pipeline.steps[1][1].feature_importances_, index=X_train_red.columns)\n# important_features.sort_values(ascending=False, inplace=True)\n\n# _ = sns.barplot(x=important_features.values,\n#                 y=important_features.index, orient='h')\n\nsvc_best = gs_svc.best_estimator_\ndtc_best = gs_ada_dtc.best_estimator_\nrf_best = gs_rf.best_estimator_\net_best = gs_et.best_estimator_\n\nclfs = [(dtc_best, 'BDT'), (rf_best, 'RandomForrest'), (et_best, 'ExtraTree')]\n\nfig, ax = plt.subplots(2, 2, figsize=(15, 15), sharex=True)\n\nax = ax.ravel()\n\nfor i in range(len(clfs)):\n\n    important_features = pd.Series(\n        data=clfs[i][0].steps[1][1].feature_importances_, index=X_train.columns)\n\n    important_features.sort_values(ascending=False, inplace=True)\n\n    g = sns.barplot(x=important_features.values,\n                    y=important_features.index, orient='h',\n                    ax=ax[i])\n\n    g.set_xlabel('Relative Importance')\n    g.set_ylabel('Features')\n    g.set_title(clfs[i][1])","e249ea23":"def plot_roc(estimators, X, y, figure_size=(16, 6)):\n    \"\"\"\n    Plot both confusion matrix and ROC curce on the same figure.\n    Parameters:\n    -----------\n    estimators : dict\n        key, value for model name and sklearn.estimator to use for predicting\n        class probabilities.\n    X : array_like\n        data to predict class probabilities.\n    y : array_like\n        true label vector.\n    figure_size : tuple (optional)\n        size of the figure.\n    Returns:\n    --------\n    plot : matplotlib.pyplot\n        plot confusion matrix and ROC curve.\n    \"\"\"\n    plt.figure(figsize=figure_size)\n    for estimator in estimators.keys():\n        # Compute tpr, fpr, auc and confusion matrix\n        fpr, tpr, thresholds = roc_curve(\n            y, estimators[estimator].predict_proba(X)[:, 1])\n        auc = roc_auc_score(y, estimators[estimator].predict_proba(X)[:, 1])\n\n        # Plot ROC curce\n        plt.plot(fpr, tpr, label=\"{}: auc = {:.3f}\".format(estimator, auc))\n        plt.title(\"ROC curve\", y=1, fontdict={\"fontsize\": 20})\n        plt.legend(loc=\"lower right\", fontsize=\"medium\")\n\n    plt.plot([0, 1], [0, 1], \"--\")\n    plt.xlabel(\"False positive rate\", fontdict={\"fontsize\": 16})\n    plt.ylabel(\"True positive rate\", fontdict={\"fontsize\": 16})","02ec951d":"# Plot ROC curves for all classifiers\nestimators = {\"RF\": rf_best,\n              \"SVC\": svc_best,\n              \"BDT\": dtc_best,\n              \"ETC\": et_best}\n\nplot_roc(estimators, X_test, y_test, (12, 8))\n\n# Print out accuracy score on test data\nprint(\"The accuracy rate and f1-score on test data are:\")\nfor estimator in estimators.keys():\n    print(\"{}: {:.2f}%, {:.2f}%.\".format(estimator,\n                                         accuracy_score(\n                                             y_test, estimators[estimator].predict(X_test)) * 100,\n                                         f1_score(y_test, estimators[estimator].predict(X_test)) * 100))","721247d2":"#### RandomForrestClassifier","595a299c":"#### DecisionTreeClassifier","2a68d0e2":"#### General Considerations","391a87a3":"It appears that women earn slightly more than men. Those who left the company earn the same amount across genders and significantly lower than those who stayed, this would make me think that income is an important factor in the decision process and thus worth investigating more.","10db2ae8":"It appears that Over18 only have one value and can be dropped from the dataset.","7fd80a22":"We saw that the Attrition class is imbalanced. As a consequence accuracy is not a good metric anymore and we should use other metrics when comparing models such as recall, f1-score or AUC. Moreover most algorithms will learn patterns that will be biased towards the majority class.\n\nWe can deal with imbalanced classes by:\n\n- Assign a bigger penalty to wrong predictions from the minority class.\n- Upsampling the minority class or downsampling the majority one.\n- Simulate more data.\n\nA penalty for wrong predictios for the minority class can be assigned in some of the algorithms using the `class_weight` parameter.\nWe need to split our dataset before oversampling because in this case, the same observation can be repeated in both the training and test sets causing the model to simply learn some specific datapoints and thus overfit.","79e1260f":"There don't seem to be high correlation between any of the variables and the target one but some features are highly correlated with each other and worth investigating more to see if they can be dropped. In particular:\n\n- JobLevel almost has perfect correlation with MonthlyIncome\n- EmployeeCount and StandardHours have the same number in it and can probably be dropped from the dataset.\n- Age higly correlates with JobLevel, MonthlyIncome and TotalWorkingYears\n- JobLevel highly correlates with TotalWorkingYears and YearsAtCompany\n- MonthlyIncome highly correlates with TotalWorkingYears and YearsAtCompany\n- PercentSalaryHike highly correlates with PerformanceRating\n\nLet us check the categorical features","810ac12a":"The average age for men is 36.65 years while for women is 37.33. Both distributions appear to be quite similar.","dd7ef125":"# Introduction\nHi this is my first Kaggle kernel and actually one of my first data science projects. This kernel is mostly a collection of the things I learned in the past few months and I decided to upload it in order to get suggestions to improve myself. At the moment it is not complete but I'll get back to it when I have time.","e354503b":"#### ExtraTreesClassifier","5d25403b":"Looks like that independently from job satisfaction people who left the company earned significantly less than those who stayed.","94d0c77a":"Seems like SVC, DecisionTreeClassifier, RandomForrestClassifier and ExtraTreesClassifiers have the highest scores. Let us try to fine tune them.","575a9efa":"Since there are some redundant features we can use some algorithm to make a rank of the feature importance and decide which one we should remove.","a8481dec":"# Exploratory Data Analysis (EDA)","af3d9e4b":"# Employee Attrition\nThe aim of this study is to provide companies with information on why employees leave. This may happen for several reasons:\n- Low income\n- Working environment\n- Low level of satisfaction\n- Retirement\n- Health issues","5f85e90c":"It appears that for those who didn't leave the job satisfaction level is the same across genders. On the other hand females who left were more dissatisfied compared to males.","cd3f1123":"Let us now visualize the most important features for the best classifiers chosen.","5146757e":"Since the upsampled dataset yelded the best results we will use it to train the other models.","830ccfad":"It appears that male employee are more likely to leave than females.","dcb39035":"- Quite a few features appear to be ordinal variables (Education, EnvironmentSatisfaction, JobInvolvment, ...)\n- There are only two possible values for the target variable and it is highly imbalanced, will need to balance it before training the model.\n\nLet us transform Attrition into numeric.","a1704f31":"Let us check correlation between variables.","63b899ea":"#### Impact of Gender\nThere are few factors that could impact attrition that are worth exploring:\n\n- Is there any differences between genders? Do male earn more than females? Are females more satisfied than males?","e4783ab2":"Convert BusinessTravel into an ordinal categorical variable since there is intrinsic order between non, rarely and frequently.","b2979041":"- Looks like that people who travel more frequently are more likely to quit compared to those who don't travel or travel rarely.\n- People in the sales department are more likely to quit although HR has a high standard deviation. \n- Male quit more often than women.\n- Sales representatives have the highest probability to quit.\n- Singles are more likely to quit compared to married or divorced employees.\n- People doing overtime have a high probability to quit.\n\nWould be nice to study more the relationship between the features but for time constraints I will come back to it if I have some time left.","1497cede":"#### SVC","83668998":"#### Impact of Income\n- Is income more important than job satisfaction?\n- Is there a huge difference in income between different roles in the company? How does this affect Attrition?\n- Is payrise a factor to consider in Attrition?","77b3d69d":"- There aren't any missing values\n- The target variable appears to be categorical.\n- There are two datatypes: int and objects"}}