{"cell_type":{"f1e8d87c":"code","afc4521f":"code","940fe38b":"code","013b27e4":"code","b461069f":"code","20afb3a7":"code","4ddfa687":"code","26b4b989":"code","1d65c71b":"code","436a4d62":"code","b153594b":"code","0b3b8892":"code","8dfecb77":"code","da974f36":"code","1c9db29d":"code","75a377ac":"code","9fde515b":"code","748a1a04":"code","cf87f4fb":"code","43c81896":"code","1a962305":"code","f9f9ab43":"code","c9046852":"code","e1d9d914":"code","fd72f8ac":"code","0ea2006f":"code","4bc16ca2":"markdown","5b7747c1":"markdown","13fb1bed":"markdown","b2f62f37":"markdown","a182c57c":"markdown","7402bc6c":"markdown","565e3364":"markdown","ca1aca4c":"markdown","0b636108":"markdown"},"source":{"f1e8d87c":"#import basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","afc4521f":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint('train data shape :',train_df.shape)\nprint('test data shape :',test_df.shape)","940fe38b":"#snapshot of training data\ntrain_df.head()","013b27e4":"#check distribution of target classes in training dataset\ny=train_df['Target']\n#y=train_df.query('parentesco1==1')['Target']\ny.value_counts()","b461069f":"#check nulls in train dataset\ntrain_null = train_df.isnull().sum()\ntrain_null[train_null > 0]","20afb3a7":"#check nulls in test dataset\ntest_null = test_df.isnull().sum()\ntest_null[test_null > 0]","4ddfa687":"train_df.info()","26b4b989":"# Groupby the household and figure out the number of unique values\ntrain_grphh = train_df.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nerr_train = train_grphh[train_grphh !=True]\nprint('Number of households with incorrect poverty level :',len(err_train))","1d65c71b":"#let's correct the poverty level in incorrect records\nfor household in err_train.index:\n    #find correct poverty level\n    target = int(train_df[(train_df['idhogar']==household) & (train_df['parentesco1']==1.0)]['Target'])\n    #set correct poverty level\n    train_df.loc[train_df['idhogar']==household,'Target'] = target","436a4d62":"#Align training and test dataset to find common features\ntrain_df,test_df = train_df.align(test_df,join='inner',axis=1)\n\nprint('Training Features shape: ', train_df.shape)\nprint('Testing Features shape: ', test_df.shape)","b153594b":"#let's join train and test data\ndata = pd.concat([train_df,test_df],axis=0)\ndata.head()","0b3b8892":"#run some checks\ndata[data['hhsize'] != data['hogar_total']].shape","8dfecb77":"#get household heads\n#hh = data.loc[data['parentesco1'] == 1]\n#check null values for flag v18q\n#hh.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","da974f36":"#replace null values with 0\n#data['v18q1'].fillna(0,inplace=True)","1c9db29d":"#similarly fill v2al for missing rent payment\n# Fill in households that own the house with 0 rent payment\n#data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\n#data['v2a1-missing'] = data['v2a1'].isnull()\n\n#data['v2a1-missing'].value_counts()\n","75a377ac":"#rez_esc\n# If individual is over 19 or younger than 7 and missing years behind, set it to 0\n# data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n# data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5","9fde515b":"#check distinct values in object(categorical) column\ncat_cols = data.nunique()==2\ncat_cols = list(cat_cols[cat_cols].keys())","748a1a04":"cols = ['edjefe', 'edjefa','dependency']\ndata[cols] = data[cols].replace({'no': 0, 'yes':1}).astype(float)\n# data = pd.get_dummies(data,columns=cat_cols)\n# print('data shape after one hot encoding :',data.shape)\n#interaction features\ndata['hogar_mid'] = data['hogar_adul'] - data['hogar_mayor']\ndata['bedroom%'] =  data['bedrooms']\/data['rooms']\ndata['person\/rooms'] = data['rooms']\/data['hhsize']\ndata['male_ratio'] = data['r4h3']\/data['r4t3']\ndata['female_ratio'] = data['r4m3']\/data['r4t3']\ndata['female_per_room'] = data['r4m3']\/data['rooms']\ndata['female_per_bedroom']  = data['r4m3']\/data['bedrooms']\ndata['hogarmid_per_bedroom']  = data['hogar_mid']\/data['bedrooms']\n#data['hogar_nin%'] = data['hhsize']\/(data['hogar_nin']+1)\ndata['escolari_age'] = data['escolari']\/data['age']\ndata['hogar_nin_per_room'] = data['rooms']\/(data['hogar_nin']+1)\ndata['male_per_bedroom'] = data['r4h3']\/data['bedrooms']\ndata['male_per_room'] = data['r4h3']\/data['rooms']\ndata['dependencyXmale']= data['dependency']*data['r4h3']\ndata['dependencyXhogar_mid'] = data['dependency']*data['hogar_mid']\ndata['dependencyXhogar_adul'] = data['dependency']*data['hogar_adul']\ndata['dependency_prod_sum']=data['dependencyXmale'] + data['dependencyXhogar_mid'] + data['dependencyXhogar_adul']\ndata['overcrowding_room_and_bedroom'] = (data['hacdor'] + data['hacapo'])\/2\n\ndata['no_appliances'] = data['refrig'] + data['computer'] + data['television']\n#data['mobile_per_bedrooms'] = (data['qmobilephone'])\/data['bedrooms']\n#data['mobile_per_person'] = data['r4t3']\/(data['qmobilephone']+1)\n#data['mobile_per_male'] = data['r4h3']\/(data['qmobilephone']+1)\n#data['mobile_per_female'] = data['r4m3']\/(data['qmobilephone']+1)\n#data['escolari_age_diff'] = data['age'] - data['escolari']\n#aggregation features\n# other_list = ['escolari', 'age']\n# for item in other_list:\n#     for function in ['mean','std','min','max','sum']:\n#         group_data = data[item].groupby(data['idhogar']).agg(function)\n#         new_col = item + '_' + function\n#         data[new_col] = group_data","cf87f4fb":"#aggregation columns\ndf_group = pd.DataFrame()\nother_list = ['escolari', 'age', 'escolari_age']\nfor item in other_list:\n    for function in ['mean','std','min','max','sum']:\n        group_data = data[item].groupby(data['idhogar']).agg(function)\n        new_col = item + '_' + function\n        df_group[new_col] = group_data\ndf_group = df_group.reset_index()\ndata = pd.merge(data, df_group, on='idhogar')","43c81896":"#drop columns contains mostly null values\ndata.drop(labels=['v2a1','v18q1','rez_esc','Id','idhogar'],axis=1,inplace=True)\nprint('data shape after dropping null columns :',data.shape)","1a962305":"#impute missing values\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n# Median imputation of missing values\n#imputer = Imputer(strategy = 'median')\n# Fit on the training data\n#imputer.fit(data)\n\n# Transform both training and testing data\n#data = imputer.transform(data)\ndata.fillna(-1,inplace=True)\n# scaler = MinMaxScaler()\n# data = pd.DataFrame(scaler.fit_transform(data[list(datacols-set(cat_cols))]))\n","f9f9ab43":"train_df = data[:len(train_df)]\ntest_df = data[len(train_df):]\n#train_df = train_df.query('parentesco1==1')\nprint('Training data shape: ', train_df.shape)\nprint('Testing data shape: ', test_df.shape)","c9046852":"#modelling\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score,make_scorer\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","e1d9d914":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","fd72f8ac":"X = train_df\nlogR = LogisticRegression(class_weight='balanced',C=0.0005)\ncv_score = cross_val_score(logR, X, y, cv = 10, scoring = scorer)\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')\n","0ea2006f":"logR.fit(X,y)\npreds_log = logR.predict(test_df)\nsub_log = pd.DataFrame({'Id':test_df.index, 'Target':preds_log})\nsub_log.to_csv('sub_log1.csv', index=False)\n","4bc16ca2":"It is clarified in discussion, that correct poverty level is poverty level of head of the family. We can identity it using parentesco1 column with value 1. Let's use this to correct poverty level in errorneous records.","5b7747c1":"### We have seen that majorly columns v2a1,v18q1 and rez_esc have missing values. Let's handle them one by one.\n\n","13fb1bed":"It seems value of v18q1 is null only for records where v18q is 0 i.e. no tablet in house. So, we can replace nan with 0 here.","b2f62f37":"## Handle Missing values","a182c57c":"#### Here, we can see that number of observations in training data set is very less than test dataset (almost 40%).","7402bc6c":"#### v18q1 - denotes number of tables in household. We have another column v18q represents  whether household owns a table or not.","565e3364":"From discussion in [thread](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#358941), poverty level is not consistent throughout the household. As per suggested by organizers, it is a data discrepany. So we will try to handle it here.","ca1aca4c":"This is my first attempt with minimal feature engineering and basic logistic regression model\non Rican Household PovertyCosta model.","0b636108":"#### We can see from above two cells that columns v2a1, v18q1, rez_esc are missing for most of the observations in both training and test dataset."}}