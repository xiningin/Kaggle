{"cell_type":{"0e9b3b7b":"code","1ffd9c37":"code","ef9a0b43":"code","f48778e6":"code","e4239e04":"code","da6af761":"code","7016c4f6":"code","ef7336bb":"code","dc613969":"code","92826945":"code","cb1ce4a4":"code","a737df62":"code","41ec600c":"code","c1929ffc":"code","29a7123b":"code","9e3a70d0":"markdown","e42b765b":"markdown","269f7127":"markdown","94ec0e8e":"markdown","1e86ace1":"markdown","9a652038":"markdown","d5ea9765":"markdown","af5478c7":"markdown","0b587ff7":"markdown","f2a6fbf5":"markdown","93b04c91":"markdown"},"source":{"0e9b3b7b":"!pip install BorutaShap","1ffd9c37":"!pip install scikit-learn -U","ef9a0b43":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifier\/Regressor\nfrom xgboost import XGBRegressor\n\n# Feature selection\nfrom BorutaShap import BorutaShap\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.mixture import GaussianMixture\n\n# Validation\nfrom sklearn.model_selection import StratifiedKFold","f48778e6":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","e4239e04":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","da6af761":"categoricals = [item for item in X_train.columns if 'cat' in item]\nnumerics = [item for item in X_train.columns if 'cont' in item]","7016c4f6":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","ef7336bb":"# Dealing with categorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals[3:]] = ordinal_encoder.fit_transform(X_train[categoricals[3:]])\nX_test[categoricals[3:]] = ordinal_encoder.transform(X_test[categoricals[3:]])\nX_train = X_train.drop(categoricals[:3], axis=\"columns\")\nX_test = X_test.drop(categoricals[:3], axis=\"columns\")","dc613969":"# Stratifying the target for cross-validation\nkm = KMeans(n_clusters=32, random_state=0)\npca = PCA(n_components=16, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\ny_stratified = km.labels_","92826945":"fa = FactorAnalysis(n_components=len(numerics), rotation='varimax', random_state=0)\nfa.fit(X_train[numerics])\n\nextra_feats = [f'fa_{i}'for i in range(len(numerics))][:2]\n\nX_train[extra_feats] = fa.transform(X_train[numerics])[:,:2]\nX_test[extra_feats] = fa.transform(X_test[numerics])[:,:2]","cb1ce4a4":"pca = PCA(n_components=len(numerics), random_state=0)\npca.fit(X_train[numerics])\n\npca_feats = [f'pca_{i}'for i in range(len(numerics))]\n\nX_train[pca_feats] = pca.transform(X_train[numerics])\nX_test[pca_feats] = pca.transform(X_test[numerics])\n\nextra_feats += pca_feats","a737df62":"from matplotlib import pyplot as plt\nnumerics = [f\"cont{i}\" for i in range(14)]\nplt.figure(figsize=(15, 10))\nfor k, num in enumerate(numerics):\n    plt.subplot(5, 3, k+1)\n    X_train[num].plot.density()\nplt.show()","41ec600c":"dists = [30, 32, 32, 29, 28, 31, 22, 32, 13, 27, 32, 30, 27, 25] \ndists += [15, 9, 12, 11, 7, 6, 6, 2, 10, 2, 4, 4, 4, 7, 8]\n\nfor feature, dist in zip(numerics + extra_feats, dists):\n    x = X_train[[feature]].append(X_test[[feature]])[feature].values.reshape(-1, 1)\n    gmm = GaussianMixture(n_components=dist,\n                           max_iter=300,\n                           random_state=0).fit(x)    \n    clus = pd.get_dummies(gmm.predict(x)).values * x\n    clus_train = clus[:len(X_train), :]\n    clus_test = clus[len(X_train):, :]\n    \n    clus_feats = [f'{feature}_gmm_dev_{i}'for i in range(clus_train.shape[1])]\n    X_train[clus_feats] = clus_train\n    X_test[clus_feats] = clus_test","c1929ffc":"params = dict([('colsample_bytree', 0.1),\n               ('learning_rate', 0.347),\n               ('max_depth', 2),\n               ('n_estimators', 1_000),\n               ('reg_alpha', 1e-09),\n               ('reg_lambda', 100.0),\n               ('subsample', 1.0)])\n\nfolds = 3\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\nselected_columns = list()\n    \nfor k, (train_idx, val_idx) in enumerate(skf.split(X_train, y_stratified)):\n    \n    print(f\"FOLD {k+1}\/{folds}\")\n    \n    model = XGBRegressor(random_state=0, objective='reg:squarederror', **params)\n    Feature_Selector = BorutaShap(model=model,\n                                  importance_measure='shap', \n                                  classification=False)\n\n    Feature_Selector.fit(X=X_train.iloc[train_idx, :], y=y_train[train_idx], n_trials=50, random_state=0)\n    \n    Feature_Selector.plot(which_features='all', figsize=(24,12))\n    \n    selected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \n    print(f\"Selected features at fold {k+1} are: {selected_columns[-1]}\")","29a7123b":"final_selection = sorted({item for selection in selected_columns for item in selection})\nprint(final_selection)","9e3a70d0":"# Feature selection with Boruta-SHAP to increase your score","e42b765b":"Gradient Boosting incorporates feature selection, since the trees spit only on significant features (or at least they should). In reality, this is not always true as sometimes noisy, irrelevant splits may appear in the tree. Moreover, working with not useful features will cause your training to go slower.\n\nGenerally, widely recognized benefits of featue selection are:\n\n* simplification of models to make them easier to interpret\n* shorter training times,\n* to avoid the curse of dimensionality,\n* more generalization by reducing overfitting (reduction of variance)","269f7127":"#### Let's start by uploading packages and data","94ec0e8e":"#### As the competition is approaching the end, it is becoming clear that it has become a battle of ensembles. \n\nHow can you win in such a battle?\n\n 1. by training new useful models that others don't have in their ensemble\n 2. by making your models run better","1e86ace1":"Boruta-SHAP is a package combining Boruta (https:\/\/github.com\/scikit-learn-contrib\/boruta_py), a feature selection method based on repeated tests of the importance of a feature in a model, with the interpretability method SHAP (https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html).\n\nBoruta-SHAP, developed by Eoghan Keany (https:\/\/github.com\/Ekeany\/Boruta-Shap), is extremely simple to use: get your best model, let it run some time on Boruta-SHAP and evaluate the results!\n\np.s\np.s. You can read more about Boruta-SHAP on this Medium article by the author: https:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677","9a652038":"## Happy Kaggling!","d5ea9765":"#### Since there is some discussion if the categorical features are ordinal or not, we process them both by one-hot encoding and ordinal encoding. If a categorical feature is ordinal, the ordinal encoding should prevail and be detected as significant by Boruta-SHAP.","af5478c7":"#### Here we finally have the good set of features to be used in this competition (at least using XGBoost - better to test for other algorithms)","0b587ff7":"#### Now we pick our best model and let Boruta-SHAP run a few experiments (usually 50 are enough) before getting the results.\n\n#### We cross-validate our experiments in order to ascertain that we are indeed picking the right variables\n\n#### as the results are prepared and we can plot them to visualize the Z-scores intervals of our features. That will signal us the confidence of the choice made by the algorithm in selecting or rejecting features.\n\n#### Please notice that the last two features are noisy features used by Boruta-SHAP to fgure out the important features. Clearly they are non-significant.\n\n#### Cross-validation takes time. Meanwhile we can grab a cup of coffee and relax as Boruta-SHAP is doing all the heavy-lift job.\n\n![immagine.png](attachment:8530c9d8-3db0-4a8e-b4a0-6d5e5c1cf18f.png)","f2a6fbf5":"![immagine.png](attachment:941cf5aa-8564-475b-ac05-5e2505a85605.png)","93b04c91":"#### Since continuous variables seems a mixture of distributions, we also test how would rank their transformation by factor analysis, pca and gaussian mixtures"}}