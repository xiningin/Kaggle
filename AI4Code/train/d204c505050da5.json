{"cell_type":{"88036bfb":"code","1d5d18e7":"code","bb0c466f":"code","27149e1c":"code","61440124":"code","6c1a3597":"code","e7f9fdf5":"code","868afedc":"code","78df7e7f":"code","fb59262b":"code","96f97433":"code","46478038":"code","27864ec6":"code","57295c2f":"code","ca1b3973":"code","fe695196":"code","df4c1456":"code","2a469368":"code","6e4ac41f":"code","df89dc31":"code","e562cf14":"code","d922991a":"code","950a5c5b":"code","6dc2640e":"code","a1680eea":"markdown","ad553007":"markdown","fab0e1c8":"markdown","2aa5b642":"markdown","7cb2beab":"markdown","4668af03":"markdown","720842b3":"markdown","de95041e":"markdown","c3c034f0":"markdown"},"source":{"88036bfb":"# Check input data\n!ls '..\/input\/review-lapak-sentiment\/'","1d5d18e7":"import pandas as pd\nimport numpy as np","bb0c466f":"raw_data = pd.read_csv('..\/input\/review-lapak-sentiment\/train.csv')\nraw_data.head(10)","27149e1c":"# Count Labels\nraw_data['label'].value_counts()","61440124":"# First make a function to delete repetitive alphabet\nimport itertools\n\ndef remove_repeating_characters(text):\n    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n\n# Check our function\nremove_repeating_characters('oooofel')","6c1a3597":"# Second make a function to remove non alphanumeric\nimport re\n\ndef remove_nonalphanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    return text\n\n# Check our function\nremove_nonalphanumeric('o,,,f!!e;;l')","e7f9fdf5":"# Last make a function to convert string to lower case\n\ndef to_lower_case(text):\n    return text.lower()\n\n# Check our function\nto_lower_case('OFEL')","868afedc":"# Make function that combine them all\n\ndef preprocessing_text(text):\n    text = remove_repeating_characters(text)\n    text = remove_nonalphanumeric(text)\n    text = to_lower_case(text)\n    \n    return text\n\n# Check our function\npreprocessing_text('Bagus\\n\\n\\nNamun Akan Lebih Baik Apabila Lebih')","78df7e7f":"# Apply function to column 'review_sangat_singkat'\n\nraw_data['review_sangat_singkat'] = raw_data['review_sangat_singkat'].apply(lambda x: preprocessing_text(x))\nraw_data.head()","fb59262b":"# Make a vector to contain all unique word in 'review sangat singkat'\n\nunique_string = set()\nfor x in raw_data['review_sangat_singkat']:\n    for y in x.split():\n        unique_string.add(y)\n        \nlen(unique_string)","96f97433":"# Count statistics of number of word in review\n\nlen_data = [len(x.split()) for x in raw_data['review_sangat_singkat']]\nprint(np.mean(len_data))\nprint(np.median(len_data))\nprint(np.std(len_data))\nprint(np.min(len_data))\nprint(np.max(len_data))\nprint(np.percentile(len_data, 98))","46478038":"embed_size = 100 # how big is each word vector\nmax_features = 23000 # how many unique words to use\nmaxlen = 20 # max number of words in a comment to use","27864ec6":"# Example\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = 4)\ntokenizer.fit_on_texts([\"ini sebuah kalimat hehehe\"])\nexamples = tokenizer.texts_to_sequences([\"ini contoh kalimat juga\"])\nprint(examples[0])\n","57295c2f":"# Real one\n\ntokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(raw_data['review_sangat_singkat'])\nlist_tokenized_train = tokenizer.texts_to_sequences(raw_data['review_sangat_singkat'].values)\n","ca1b3973":"list_tokenized_train[0]","fe695196":"# Example\n\nfrom keras.preprocessing.sequence import pad_sequences\npad_sequences(examples, maxlen = maxlen)","df4c1456":"# Real one\n\nX_t = pad_sequences(list_tokenized_train, maxlen= maxlen )","2a469368":"X_t[0]","6e4ac41f":"import gensim\nDIR_DATA_MISC = \"..\/input\/word2vec-100-indonesian\"\npath = '{}\/idwiki_word2vec_100.model'.format(DIR_DATA_MISC)\nid_w2v = gensim.models.word2vec.Word2Vec.load(path)\nprint(id_w2v.most_similar('itb'))","df89dc31":"index2word_set = set(id_w2v.wv.index2word)","e562cf14":"word_index = tokenizer.word_index\nnb_words = max_features\nembedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\nunknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\nfor word, i in word_index.items():\n    cur = word\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        continue\n        \n    embedding_matrix[i] = unknown_vector","d922991a":"# Import needed packages\n# And make needed function\n\n\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import callbacks\n\nfrom keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(32, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(32, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    x = Dense(1, activation=\"sigmoid\")(conc)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    return model","950a5c5b":"from sklearn.model_selection import KFold\ndef get_kfold():\n    return KFold(n_splits=5, shuffle=True, random_state=1)","6dc2640e":"X = X_t\ny = raw_data[\"label\"].values\n\npred_cv = np.zeros(len(y))\ncount = 0\n\nfor train_index, test_index in get_kfold().split(X, y):\n    count += 1\n    print(count, end='')\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    es = callbacks.EarlyStopping(monitor='val_f1', min_delta=0.0001, patience=8,\n                                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    \n    model = get_model()\n    model.fit(X_train, \n             y_train, batch_size=16, epochs=4,\n             validation_data=(X_test, y_test),\n             callbacks=[es, rlr],\n             verbose=1)\n    \n    pred_cv[[test_index]] += model.predict(X_test)[:,0]","a1680eea":"## Extra Preprocessing","ad553007":"## Preprocessing","fab0e1c8":"## Feature Engineering","2aa5b642":"We will use pad sequneces, check this out at: https:\/\/keras.io\/preprocessing\/sequence\/","7cb2beab":"We will use word embedding, check this out at: https:\/\/www.kaggle.com\/ilhamfp31\/word2vec-100-indonesian","4668af03":"## Import Package","720842b3":"We will use Tokenizer, check this out at: https:\/\/keras.io\/preprocessing\/text\/ ","de95041e":"## Model","c3c034f0":"## Input Data"}}