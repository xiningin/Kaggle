{"cell_type":{"24c086d2":"code","2dc8ddd0":"code","a0dce484":"code","9829fdc9":"code","db36174c":"code","750c8862":"code","7ff56eb8":"code","470fd750":"code","98880309":"code","1d4cfd6c":"code","1dfe8d72":"code","e145e9b7":"code","263c91d6":"code","9829b7a6":"code","3aeaabc1":"code","28c26cd6":"code","323f2d72":"code","e83b5de1":"code","3f77d191":"code","57a5b729":"code","2da7a661":"code","ff652f20":"code","c24daad6":"code","57c81e9e":"markdown","11cac78c":"markdown","2b1e659e":"markdown","ef133ddc":"markdown"},"source":{"24c086d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2dc8ddd0":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","a0dce484":"train.head()","9829fdc9":"test.head()","db36174c":"print(train.apply(lambda col: col.unique()))\nprint(train.apply(lambda col: col.nunique()))","750c8862":"!pip install spacy -q\n!python -m spacy download en_core_web_sm -q","7ff56eb8":"import matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport spacy\nimport en_core_web_sm\nimport re","470fd750":"from sklearn.model_selection import train_test_split\n\nX = train['text'] + ' ' +  train['keyword'].astype(str) + ' ' +  train['location'].astype(str) # the features we want to analyze\nylabels = train['target'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)","98880309":"X_train[100:500]\n#type(X_train[1])\n#y_train[:100]","1d4cfd6c":"\npunctuations = string.punctuation \nnlp = spacy.load('en_core_web_sm') #, exclude=[\"tok2vec\", \"parser\", \"ner\", \"attribute_ruler\"]\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\nparser = English() # Load English tokenizer, tagger, parser, NER and word vectors\n\ndef spacy_tokenizer(sentence):\n    mytokens = str(sentence)\n    mytokens = nlp(mytokens)\n    #mytokens = parser(sentence) \n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] \n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]    \n    return mytokens      # return preprocessed list of tokens\n\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\ndef clean_text(text):\n    text =  text.strip().lower()\n    #text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n    return text #.split()\n\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1), stop_words = None)\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer,  stop_words = None) #token_pattern='(?u)\\b\\w\\w+\\b', stop_words = 'english'\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = LogisticRegression()\n# classifier = RandomForestClassifier()\n\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', tfidf_vector),\n                 ('classifier', classifier)])\n\n#clean_text(X_train[1773])\n#spacy_tokenizer(X_train[1773])\n#mytokens = parser(X_train[1773])\n\n# mytokens = str(X_train[1773])\n# #mytokens = re.sub(r'[^A-Za-z0-9 ]+', '', mytokens)\n# #mytokens = parser(mytokens)\n# mytokens = nlp(mytokens)\n# mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n# print(mytokens)","1dfe8d72":"pipe.fit(X_train, y_train)","e145e9b7":"from sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Recall:\",metrics.recall_score(y_test, predicted))","263c91d6":"predicted_df = pd.DataFrame(predicted)\npredicted_df.value_counts()","9829b7a6":"predicted_df.plot.hist()","3aeaabc1":"predicted_df.head()","28c26cd6":"test.head()","323f2d72":"sample_submission.head()","e83b5de1":"predicted","3f77d191":"test","57a5b729":"my_submission_preds = pipe.predict(test['text']+ ' ' +  test['keyword'].astype(str) + ' ' +  test['location'].astype(str))\n\nmy_submission = pd.DataFrame({\"id\":test['id'], 'target':my_submission_preds})","2da7a661":"my_submission.head()","ff652f20":"len(my_submission)","c24daad6":"my_submission.to_csv('submission.csv', index=False)","57c81e9e":"# Imports","11cac78c":"# Custom transformer using spaCy","2b1e659e":"# Generate Submission","ef133ddc":"# spaCy pipeline starter code - https:\/\/www.dataquest.io\/blog\/tutorial-text-classification-in-python-using-spacy\/"}}