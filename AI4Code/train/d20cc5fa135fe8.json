{"cell_type":{"6febae8e":"code","a0667cce":"code","6a01e6b6":"code","caf808ec":"code","818e88a2":"code","6cca702b":"code","31af43d5":"code","5976fa9e":"code","754d6f2d":"code","1d0b1416":"code","62c51b0d":"code","e43aaced":"code","d7e00990":"markdown","0c9bc8e5":"markdown","f02ab5fa":"markdown","163edbf5":"markdown","4cc2eca2":"markdown","01609cdd":"markdown","fd2a04e1":"markdown","6a91fac5":"markdown","a80df0b9":"markdown","2e8db8d3":"markdown","5ce2a521":"markdown","c2aaa0df":"markdown","7894d581":"markdown","c172cbec":"markdown","2dd67633":"markdown","a89118a5":"markdown"},"source":{"6febae8e":"# Setup. Import libraries and load dataframes for Movielens data.\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport random\n\ntf.set_random_seed(1); np.random.seed(1); random.seed(1) # Set random seeds for reproducibility\n\ninput_dir = '..\/input'\nratings_path = os.path.join(input_dir, 'rating.csv')\n\nratings_df = pd.read_csv(ratings_path, usecols=['userId', 'movieId', 'rating', 'y'])\n\nmovies_df = pd.read_csv(os.path.join(input_dir, 'movie.csv'), usecols=['movieId', 'title', 'year'])\n\ndf = ratings_df.merge(movies_df, on='movieId').sort_values(by='userId')\ndf = df.sample(frac=1, random_state=1) # Shuffle\n\ndf.sample(5, random_state=1)","a0667cce":"n_movies = len(df.movieId.unique())\nn_users = len(df.userId.unique())\nprint(\n    \"{1:,} distinct users rated {0:,} different movies (total ratings = {2:,})\".format(\n        n_movies, n_users, len(df),\n    )\n)\n","6a01e6b6":"model = keras.Sequential([\n    # 2 input values: user id and movie id\n    keras.layers.Dense(256, input_dim=2, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    # A single output node, containing the predicted rating\n    keras.layers.Dense(1)\n])","caf808ec":"input_size = n_movies + n_users\nprint(\"Input size = {:,} ({:,} movies + {:,} users)\".format(\n    input_size, n_movies, n_users,\n))\nmodel = keras.Sequential([\n    # One hidden layer with 128 units\n    keras.layers.Dense(128, input_dim=input_size, activation='relu'),\n    # A single output node, containing the predicted rating\n    keras.layers.Dense(1)\n])\nmodel.summary()","818e88a2":"hidden_units = (32,4)\nmovie_embedding_size = 8\nuser_embedding_size = 8\n\n# Each instance will consist of two inputs: a single user id, and a single movie id\nuser_id_input = keras.Input(shape=(1,), name='user_id')\nmovie_id_input = keras.Input(shape=(1,), name='movie_id')\nuser_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size,\n                                      input_length=1, name='user_embedding')(user_id_input)\nmovie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size,\n                                       input_length=1, name='movie_embedding')(movie_id_input)\n# Concatenate the embeddings (and remove the useless extra dimension)\nconcatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\nout = keras.layers.Flatten()(concatenated)\n\n# Add one or more hidden layers\nfor n_hidden in hidden_units:\n    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n    \n# A single output: our predicted rating\nout = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n\nmodel = keras.Model(\n    inputs = [user_id_input, movie_id_input],\n    outputs = out,\n)\nmodel.summary(line_length=88)","6cca702b":"model.compile(\n    # Technical note: when using embedding layers, I highly recommend using one of the optimizers\n    # found  in tf.train: https:\/\/www.tensorflow.org\/api_guides\/python\/train#Optimizers\n    # Passing in a string like 'adam' or 'SGD' will load one of keras's optimizers (found under \n    # tf.keras.optimizers). They seem to be much slower on problems like this, because they\n    # don't efficiently handle sparse gradient updates.\n    tf.train.AdamOptimizer(0.005),\n    loss='MSE',\n    metrics=['MAE'],\n)","31af43d5":"history = model.fit(\n    [df.userId, df.movieId],\n    df.y,\n    batch_size=5000,\n    epochs=20,\n    verbose=0,\n    validation_split=.05,\n);","5976fa9e":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\ndf_train, df_val = train_test_split(df, test_size=.05, random_state=1)\n\ndef get_metrics(y_true, y_pred):\n    return metrics.mean_absolute_error(y_true, y_pred), metrics.mean_squared_error(y_true, y_pred)\n    \nmean_rating = df_train['rating'].mean()\nprint(\"Average rating in training set is {:.2f} stars\".format(mean_rating))\n\ny_true = df_val['rating'].values\nalways_mean = np.full(y_true.shape, mean_rating)\n\nmae, mse = get_metrics(y_true, always_mean)\nprint(\"Always predicting global average rating results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(mae, mse))\n\nmovies = movies_df.copy().set_index('movieId')\nmean_per_movie = df_train.groupby('movieId')['rating'].mean()\nmovies['mean_rating'] = mean_per_movie\nratings_per_movie = df_train.groupby('movieId').size()\nmovies['n_ratings'] = ratings_per_movie\n# There are a few movies in the validation set not present in the training set. We'll just use the global\n# mean rating in their case.\ny_movie_mean = df_val.join(mean_per_movie, on='movieId', rsuffix='mean')['ratingmean'].fillna(mean_rating).values\n\nmae, mse = get_metrics(y_true, y_movie_mean)\nprint(\"Predicting mean per movie results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(mae, mse))","754d6f2d":"\nfig, ax = plt.subplots(figsize=(15, 6))\nax.plot(history.epoch, history.history['val_mean_absolute_error'], label='Validation MAE')\nax.plot(history.epoch, history.history['mean_absolute_error'], label='Training MAE')\nax.set_xlabel('Epoch')\nax.set_ylabel('Mean Absolute Error')\nax.set_xlim(left=0, right=history.epoch[-1])\nbaseline_mae = 0.73\nax.axhline(baseline_mae, ls='--', label='Baseline', color='#002255', alpha=.5)\nax.grid()\nfig.legend();","1d0b1416":"\n# Save training history for later comparison\nhdf = pd.DataFrame(dict(\n    epoch=history.epoch,\n    val_mae=history.history['val_mean_absolute_error'],\n    train_mae=history.history['mean_absolute_error'],\n))\nhdf.to_csv('history-1.csv')","62c51b0d":"\nratings_per_user = df.groupby('userId').size()\nuid = ratings_per_user[ratings_per_user < 30].sample(1, random_state=1).index[0]\nuser_ratings = df[df.userId==uid]\nprint(\"User #{} has rated {} movies (avg. rating = {:.1f}):\".format(\n    uid, len(user_ratings), user_ratings['rating'].mean(),\n))\ncols = ['userId', 'movieId', 'rating', 'title', 'year']\nuser_ratings.sort_values(by='rating', ascending=False)[cols]","e43aaced":"candidate_movies = movies[\n    movies.title.str.contains('Naked Gun')\n    | (movies.title == 'The Sisterhood of the Traveling Pants')\n    | (movies.title == 'Lilo & Stitch')\n].copy()\n\npreds = model.predict([\n    [uid] * len(candidate_movies), # User ids \n    candidate_movies.index, # Movie ids\n])\n# NB: Remember we trained on 'y', which was a version of the rating column centered on 0. To translate\n# our model's output values to the original [0.5, 5] star rating scale, we need to 'uncenter' the\n# values, by adding the mean back\nrow = df.iloc[0] # The difference between rating and y will be the same for all rows, so we can just use the first\ny_delta = row.rating - row.y\ncandidate_movies['predicted_rating'] = preds + y_delta\n# Add a column with the difference between our predicted rating (for this user) and the movie's\n# overall average rating across all users in the dataset.\ncandidate_movies['delta'] = candidate_movies['predicted_rating'] - candidate_movies['mean_rating']\ncandidate_movies.sort_values(by='delta', ascending=False)","d7e00990":"A key thing to note is that this network is not simply a stack of layers from input to output. We're treating the user and the movie as separate inputs, which come together only after each has gone through its own embedding layer.\n\nThis means that the [keras.Sequential](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Sequential) class (which you may be familiar with from our [course on deep learning with image data](https:\/\/www.kaggle.com\/learn\/deep-learning)) won't work. We'll need to turn to the more powerful 'functional API', using the [keras.Model](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model) class. For more detail on the Functional API, check out [Keras's guide, here](https:\/\/keras.io\/getting-started\/functional-api-guide\/).\n\nHere's the code:","0c9bc8e5":"To judge whether our model is any good, it'd be helpful to have a baseline. In the cell below, we calculate the error of a couple dumb baselines: always predicting the global average rating, and predicting the average rating per movie:","f02ab5fa":"Looks pretty reasonable! For each of the movies in the *The Naked Gun* series, our predicted ratings for this user are around a full star above the average rating in the dataset, and our 'out of left field' picks have their predicted ratings downgraded compared to average.","163edbf5":"In the simplest terms, neural nets work by doing math on their inputs. But the actual numerical values of the ids assigned to users and movies are meaningless. *Schindler's List* has id 527 and *The Usual Suspects* has id 50, but that doesn't mean *Schindler's List* is 'ten times bigger' than *The Usual Suspects*.","4cc2eca2":"Here's a plot of our embedding model's absolute error over time. For comparison, our best baseline (predicting the average rating per movie) is marked with a dotted line:","01609cdd":"# Building a rating prediction model in Keras\n\nWe want to build a model that takes a user, $u_i$ and a movie, $m_j$, and outputs a number from 0.5-5, representing how many stars we think this user would give that movie. \n\n> **Aside:** You may have noticed that the [MovieLens dataset](https:\/\/www.kaggle.com\/grouplens\/movielens-20m-dataset) includes information about each movie such as its title, its year of release, a set of genres and user-assigned tags. But for now, we're not going to try to exploit any of that extra information.\n\nI claim we need an embedding layer to handle these inputs. Why? Let's review some alternatives and see why they don't work.\n\n## Bad idea #1: Use user ids and movie ids as numerical inputs\n\nWhy not feed in user ids and movie ids as inputs, then add on some dense layers and call it a day? i.e.:","fd2a04e1":"Let's train the model.\n\n> **Aside**: I'm passing in `df.y` as my target variable rather than `df.rating`. The `y` column is just a 'centered' version of the rating - i.e. the rating column minus its mean over the training set. For example, if the overall average rating in the training set was 3 stars, then we would translate 3 star ratings to 0, 5 star ratings to 2.0, etc. to get `y`. This is a common practice in deep learning, and tends to help achieve better results in fewer epochs. For more details, feel free to check out [this kernel](https:\/\/www.kaggle.com\/colinmorris\/movielens-preprocessing) with all the preprocessing I performed on the MovieLens dataset.","6a91fac5":"Ratings range from 0.5 stars to 5. Our goal will be to predict the rating a given user $u_i$ will give a particular movie $m_j$. (The column `y` is just a copy of the rating column with the mean subtracted - this will be useful later.)\n\n`userId` and `movieId` are both sparse categorical variables. They have many possible values:","a80df0b9":"## Implementing it\n\nI want my model to look something like this:\n\n![Imgur](https:\/\/i.imgur.com\/Z1eVQu9.png)","2e8db8d3":"User 26556 has given out two perfect ratings to the movies *Airplane!* and *Airplane II: The Sequel*. Great choices! Perhaps they'd also enjoy the [*The Naked Gun*](https:\/\/en.wikipedia.org\/wiki\/The_Naked_Gun) series - another series of spoof films starring Leslie Nielsen.\n\nWe don't have as much evidence about what this user hates. Rather than extrapolating from their few low ratings, a better indication of this user's dislikes might be the kinds of movies they haven't even rated. Let's also throw in a couple examples of movies that this user seems unlikely to ever watch, according to their rating history.","5ce2a521":"A basic issue here is scaling and efficiency. A single input to our model is a vector of 165,237 numbers (of which we know that 165,235 will be zeros). The feature data for our whole dataset of 20 million rating instances will require a 2-d array of size 20,000,000 x 165,237, or about 3 **trillion** numbers. Good luck fitting that all into memory at once!\n\nAlso, doing training and inference on our model will be inefficient. To calculate the activations of our first hidden layer, we'll need to multiply our 165k inputs through about 21 million weights - but the vast, vast majority of those products will just be zero.\n\nOne-hot encoding is fine for categorical variables with a small number of possible values, like `{Red, Yellow, Green}`, or `{Monday, Tuesday, Wednesday, Friday, Saturday, Sunday}`. But it's not so great in cases like our movie recommendation problem, where variables have tens or hundreds of thousands of possible values.\n\n## Good idea: Embedding layers\n\nIn short, an **embedding layer** maps each element in a set of discrete things (like words, users, or movies) to a dense vector of real numbers (its **embedding**). \n\n> **Aside:** A key implementation detail is that embedding layers take as input the *index* of the entity being embedded (i.e. we can give it our userIds and movieIds as input). You can think of it as a sort of 'lookup table'. This is much more efficient than taking a one-hot vector and doing a huge matrix multiplication!\n\nAs an example, if we learn embeddings of size 8 for movies, the embedding for *Legally Blonde* (index=4352) might look like:\n\n$$[ 1.624, -0.612, -0.528, -1.073,  0.865, -2.302,  1.745, -0.761]$$\n\n**Where do these come from?** We initialize an embedding for each user and movie using random noise, then we train them as part of the process of training the overall rating-prediction model. \n\n**What do they mean?** An object's embedding, if it's any good, should capture some useful latent properties of that object. But the key word here is *latent* AKA hidden. It's up to the model to discover whatever properties of the entities are useful for the prediction task, and encode them in the embedding space. Sound mysterious? In later lessons I'll show some techniques for interpreting learned embeddings, such as visualizing them with the t-SNE algorithm.","c2aaa0df":"## Training it\n\nWe'll compile our model to minimize squared error ('MSE'). We'll also include absolute error ('MAE') as a metric to report during training, since it's a bit easier to interpret.\n\n> Something to think about: We know that ratings can only take on the values `{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}` - so why not treat this as a multiclass classification problem with 10 classes, one for each possible star rating?","7894d581":"# Your turn!\n\nHead over to [the Exercises notebook](https:\/\/www.kaggle.com\/kernels\/fork\/1598432) to get some hands-on practice working with embedding layers.\n### P.S...\n\nThis course is still in beta, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this lesson](https:\/\/form.jotform.com\/82826168584267), I'd greatly appreciate it. You can also leave public feedback in the comments below, or on the [Learn Forum](https:\/\/www.kaggle.com\/learn-forum).\n","c172cbec":"Welcome to our first lesson on the topic of **embeddings**. In this lesson, I'll show how to implement a model with embedding layers using the `tf.keras` API. Embeddings are a technique that enable deep neural nets to work with **sparse categorical variables**.\n\n# Sparse Categorical Variables\n\nBy this I mean a categorical variable with lots of possible values (high *cardinality*), with a small number of them (often just 1) present in any given observation. One good example is words. There are hundreds of thousands of them in the English language, but a single tweet might only have a dozen. Word embeddings are a crucial technique for applying deep learning to natural language. But other examples abound.\n\nFor example, [this dataset of LA county restaurant inspections](https:\/\/www.kaggle.com\/meganrisdal\/la-county-restaurant-inspections-and-violations) has several sparse categorical variables, including:\n- `employee_id`: which of the health department's employees performed this inspection? (~250 distinct values)\n- `facility_zip`: what zip code is the restaurant located in? (~3,000 distinct values)\n- `owner_name`: who owns the restaurant? (~35,000 distinct values)\n\nAn embedding layer would be a good idea for using any of these variables as inputs to a network.\n\nIn this lesson, I'll be using the [MovieLens dataset](https:\/\/www.kaggle.com\/grouplens\/movielens-20m-dataset) as an example.\n\n# MovieLens\n\nThe MovieLens dataset consists of ratings assigned to movies by users. Here's a sample:","2dd67633":"## Bad idea #2: One-hot encoded user and movie inputs\n\nIf you're not familiar with one-hot encoding, you may want to check out our lesson [Using Categorical Data with One Hot Encoding](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding).\n\nIn that lesson, we claim that one-hot encoding is \"The Standard Approach for Categorical Data\". So why is it a bad idea here? Let's see what a model would look like that took one-hot encoded users and movies.","a89118a5":"Compared to the baseline, we were able to get our average error down by more than .1 stars (or about 15%). Not bad!\n\n## Example predictions\n\nLet's try some example predictions as a sanity check. We'll start by picking out a specific user from the dataset at random."}}