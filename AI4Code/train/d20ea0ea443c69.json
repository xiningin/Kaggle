{"cell_type":{"50ec5159":"code","399f65e2":"code","d7707037":"code","0557d154":"code","88d5352d":"code","992e0e44":"code","7af32e54":"code","21a506e3":"code","adcd82cd":"code","0ef1428a":"code","1f5f1a65":"code","03ac4b10":"code","595da164":"code","cd8d2892":"code","8b5db9e4":"code","4b0873bc":"code","5236c7c5":"code","94d37e4b":"code","df374a1a":"code","72d99e4d":"code","5f4f554b":"code","1f563869":"code","40c1eff1":"code","d134979e":"code","81916ecb":"code","e2ddbfb2":"code","fcc90c6e":"code","9a457b38":"code","89ae92cf":"code","1942a32d":"code","f7f3ae20":"code","3c6b9c4b":"markdown","ea88c8f9":"markdown","e13aa153":"markdown","5e425ab6":"markdown","30f0b3fc":"markdown"},"source":{"50ec5159":"# importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","399f65e2":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","d7707037":"train_data.head()\ndata_test = test_data.copy(deep=True)","0557d154":"test_data.head()","88d5352d":"# checking the null values in dataset\nprint(f\"Null values in train dataset - {train_data.isnull().sum()}\")\nprint(\"------------\")\nprint(f\"Null values in test dataset - {test_data.isnull().sum()}\")","992e0e44":"# Printing the shape of train and test data \nprint(f\"train data shape - {train_data.shape}\")\nprint(f\"test data shape - {test_data.shape}\")\ntrain_data_length = len(train_data)\nprint(train_data_length)","7af32e54":"# temporarily concatenating the train and test dataframes\ndf = pd.concat([train_data.assign(ind=\"train\"), test_data.assign(ind=\"test\")],ignore_index=True)","21a506e3":"df.shape","adcd82cd":"df.head()","0ef1428a":"print(df.isnull().sum())","1f5f1a65":"# Filling the null values of columns in training dataset\nprint(df['Embarked'].value_counts())\nprint(\"----------------------\")\ndf['Age'].fillna(df['Age'].mean(),inplace=True)\ndf['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)\ndf['Fare'].fillna(df['Fare'].mean(),inplace=True)\n\n# Removing the column with most null values and meaningless values\ndf.drop(['Cabin','Name','PassengerId','Ticket'],axis=1, inplace=True)\nprint(df.isnull().sum())","03ac4b10":"df.head()","595da164":"# Printing dataset correlation\ndf.corr()","cd8d2892":"# One hot encoding for Sex, Embarked column\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder_1 = LabelEncoder()\ndf['Sex'] = labelencoder_1.fit_transform(df['Sex'])\nembarked_cols = pd.get_dummies(df['Embarked'],prefix='Embarked',drop_first=True)\ndf.drop(['Embarked'],axis=1,inplace=True)\ndf = df.join(embarked_cols)","8b5db9e4":"df.head()","4b0873bc":"# splitting the combined dataframe into train, test data\ntrain_data, test_data = df[df[\"ind\"].eq(\"train\")].copy(), df[df[\"ind\"].eq(\"test\")].copy().reset_index(drop=True)","5236c7c5":"# Removing the Survived column from test data\ntest_data.drop(['Survived'],axis=1,inplace=True)\ntest_data.drop(['ind'],axis=1,inplace=True)\ntrain_data.drop(['ind'],axis=1,inplace=True)","94d37e4b":"train_data = train_data.astype({'Survived': int})\ntrain_data.head()","df374a1a":"test_data.head()","72d99e4d":"train_data.describe()","5f4f554b":"from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import ShuffleSplit, cross_validate\n\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n\n# fitting training data with classifiers with default parameters\nalgorithm_ob_dict = {\n    'bagging_classifier':BaggingClassifier(), 'extra_trees': ExtraTreeClassifier(), 'random_forest': RandomForestClassifier(),\n    'passive_aggressive': PassiveAggressiveClassifier(),'logistic_regression':LogisticRegression(solver='liblinear'),'gaussian_nb':GaussianNB(), \n    'decision_tree':DecisionTreeClassifier(), 'extra_tree': ExtraTreeClassifier(), 'svc': SVC(),\n    'knn' : KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n}\naccuracies_dict = {}\npredictions_dict = {}\n# print(type(train_data.iloc[:,1].values))\nfor key, obj in algorithm_ob_dict.items():\n    try:\n        obj.fit(train_data.iloc[:,1:9].values, train_data.iloc[:,0].values)\n        cv_results = cross_validate(obj, train_data.iloc[:,1:9].values, train_data.iloc[:,0].values, cv  = cv_split)\n        accuracies_dict[key] = {'mean_test_score' :  cv_results['test_score'].mean()}\n    except Exception as e:\n        print(f\"Exception occured in {key} - {str(e)}\")\n        pass\nprint(f\"Accuracies dictionary containing test_score of each classifier \\n\\n{accuracies_dict}\")","1f563869":"# Using random forest to predict and submit result\nrandom_forest_classifier = algorithm_ob_dict['random_forest']\npredictions = random_forest_classifier.predict(test_data.values)\nprint(predictions)\ntest_data['Survived'] = predictions\nprint(\"-------------\")\ntest_data.head()","40c1eff1":"# Defining parameters for grid search of all algorithms used above to make predictions\nfrom sklearn.model_selection import GridSearchCV\n\nalogrithm_parameter_list = {\n    'bagging_classifier': [\n        {'n_estimators':[10,100,150],'max_samples': [0.6,0.7,0.8],\n         'max_features': [0.6,0.7,0.8,1], 'bootstrap_features': [True,], 'bootstrap':[True,]},\n        {'n_estimators':[10,100,150],'max_samples': [0.6,0.7,0.8],\n         'max_features': [0.6,0.7,0.8,1]}\n    ],\n    'extra_trees': [\n        {'criterion':['gini', 'entropy'],'splitter':['random', 'best'],\n         'max_features':['auto', 'sqrt', 'log2']}\n    ],\n    'random_forest': [\n        {'criterion':['gini', 'entropy'],\n         'max_features':['auto', 'sqrt', 'log2']}\n    ],\n    'passive_aggressive': [\n        {'C':[1,10,100,1000], 'max_iter':[100,500,1000], 'loss': ['hinge','squared_hinge']}\n    ],\n    'logistic_regression': [\n        {'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C':[1,10,100,1000]}\n    ],\n    'extra_tree': [\n        {'criterion':['gini', 'entropy'],'splitter':['random', 'best'],\n         'max_features':['auto', 'sqrt', 'log2']}\n    ],\n    'decision_tree' : [\n        {'criterion':['gini', 'entropy'],'splitter':['random', 'best'],\n         'max_features':['auto', 'sqrt', 'log2']}\n    ],\n    'svc': [\n        {'C':[1,10,100,1000],'kernel':['linear']},\n        {'C':[1,10,100,1000],'kernel':['rbf'],'gamma':[0.5,0.1,0.01,0.001,0.0001]},\n    ],\n    'knn': [\n        {'n_neighbors': [3,4,5,7], 'weights': ['uniform', 'distance'],\n        'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}\n    ]\n }","d134979e":"# Applying Grid Search to find the best model and the best parameters\n# best_score_parameters = {}\n# for key, obj in algorithm_ob_dict.items():\n#     try:\n#         if key in alogrithm_parameter_list.keys():\n#             grid_search = GridSearchCV(estimator=obj,\n#                               param_grid=alogrithm_parameter_list[key],\n#                               scoring='accuracy',\n#                                cv=10,\n#                                n_jobs=-1\n#                               )\n#             grid_search.fit(train_data.iloc[:,1:9].values, train_data.iloc[:,0].values)\n#             best_score_parameters[key] = {'best_score' : grid_search.best_score_, \n#                                           'best_parameters': grid_search.best_params_}\n#     except Exception as e:\n#         print(f\"Exception occured in GridSearch - {key} - {str(e)}\")\n#         pass\n# print(f\"Best score and parameters for each classifier \\n\\n{best_score_parameters}\")","81916ecb":"# Removing the survived column from previous predictions\ntest_data.drop('Survived',axis=1,inplace=True)","e2ddbfb2":"# Using feature selection to select important features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nnp.set_printoptions(suppress=True)\nbestfeatures = SelectKBest(score_func=chi2, k=3)\nfit = bestfeatures.fit(train_data.iloc[:,1:9].values, train_data.iloc[:,0].values)\n# print(fit.scores_)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train_data.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']\nfeatureScores.head(n=8).sort_values(by='Score', ascending=False)","fcc90c6e":"train_data.head()","9a457b38":"test_data.head()","89ae92cf":"train_data.iloc[:,[1,2,4,5]].values","1942a32d":"# Using Random classifier to predict result as it is best in grid search\n# Taking the best columns from train and test set with best scores after using feature selection\nclassifier = RandomForestClassifier()\nclassifier.fit(train_data.iloc[:,[1,2,4,5]].values, train_data.iloc[:,0].values)\npredictions = classifier.predict(test_data.iloc[:,[0,1,3,4]].values)\nprint(predictions)","f7f3ae20":"data_test['Survived'] = predictions\ndata_test.head()\nsubmit = data_test[['PassengerId','Survived']]\nsubmit.head()\nsubmit.to_csv(\"submission.csv\", index=False)","3c6b9c4b":"# Model Fitting and Predictions","ea88c8f9":"# Loading train and test data into dataframes","e13aa153":"# **Tuning models with hyper parameters**","5e425ab6":"# Data Cleaning","30f0b3fc":"**Commented the grid search section as we know the best parameters from previous version**"}}