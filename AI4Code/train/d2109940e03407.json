{"cell_type":{"72ba1db8":"code","ba8cef75":"code","59b53df7":"code","6eaa5990":"code","3589cc69":"code","abf78802":"code","ce106e81":"code","2686b2b4":"code","e31c58fb":"code","5daa2185":"code","ddde504c":"code","9aeca463":"code","19c35ddf":"code","80e4aaeb":"code","05352033":"code","14a93592":"code","b74ff110":"code","2e186d3d":"code","a938c050":"code","0090c5fc":"code","6b0f8f0a":"code","e99d958a":"code","13fb8e3f":"code","178c9bcc":"code","85c07cae":"code","0f07c570":"code","09282332":"code","36c88882":"code","b8f08fb9":"code","60a21e01":"code","d448d313":"code","230cb680":"code","e789cdc5":"code","1ce06efe":"code","05143a7e":"code","43514cad":"markdown","9be42ffe":"markdown","5be55469":"markdown","2204e235":"markdown","64b99ecc":"markdown","143b7010":"markdown","7cedab8d":"markdown","2a6276be":"markdown","3c45565a":"markdown","243e033d":"markdown","cdcda4c2":"markdown","ccf0cc05":"markdown","74d11462":"markdown","fc623f18":"markdown","73bc5709":"markdown","89b89b34":"markdown","27b42057":"markdown","853b57bd":"markdown","344f3f6c":"markdown","d41decb2":"markdown","fb417859":"markdown","f918fc00":"markdown","df675cd2":"markdown","5ff0ebe0":"markdown","c079b93a":"markdown","a3a7309e":"markdown","67f4372d":"markdown","43f6f9b1":"markdown","a660e420":"markdown","226b99b0":"markdown","cd01aab6":"markdown","3469e161":"markdown","fdc011ee":"markdown","8533cd50":"markdown"},"source":{"72ba1db8":"_ = !pip install fuggle","ba8cef75":"from fuggle import setup\nsetup()","59b53df7":"%%fsql spark\nCREATE [[0]] SCHEMA a:int\nPRINT","6eaa5990":"from pyspark.sql import SparkSession, Row\nfrom triad import FileSystem\nfrom time import sleep\nfrom uuid import uuid4\nimport pandas as pd\n\ndef tmpfile(suffix=\".parquet\"):\n    return \"\/tmp\/\" + str(uuid4())+ suffix\n\nsession = SparkSession.builder.getOrCreate()\nctx = session.sparkContext\nctx.setCheckpointDir(\"\/tmp\")\nfs = FileSystem()","3589cc69":"def op1(rows):\n    sleep(5)\n    yield from rows\n    \ndf1 = session.createDataFrame(pd.DataFrame([[0]], columns=[\"a\"]))","abf78802":"%%timeit -r1 -n1\nres = df1.rdd.mapPartitions(op1).toDF()\nres.show()\nprint(res.count())\nres.write.parquet(tmpfile())","ce106e81":"%%timeit -r1 -n1\nres = df1.rdd.mapPartitions(op1).toDF().persist()\nres.show()\nprint(res.count())\nres.write.parquet(tmpfile())","2686b2b4":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()\nres.show()\nprint(res.count())\nres.write.parquet(tmpfile())","e31c58fb":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()\nprint(res.count())\nres.show()\nres.write.parquet(tmpfile())","5daa2185":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()","ddde504c":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()\nprint(res.head(1))\nprint(res.head(1))\nprint(res.head(1))\nprint(res.head(1))","9aeca463":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()\nprint(res.count())\nprint(res.head(1))\nprint(res.head(1))\nprint(res.head(1))\nprint(res.head(1))","19c35ddf":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist()\nprint(res.count())\nres.show()\nres.write.parquet(tmpfile())","80e4aaeb":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").checkpoint()","05352033":"%%timeit -r1 -n1\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").persist().checkpoint()\nres.show()\nres.write.parquet(tmpfile())","14a93592":"%%timeit -r1 -n1\ndef my_checkpoint(df, session):\n    path = tmpfile()\n    df.write.parquet(path)\n    return session.read.parquet(path)\n\nres = session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\")\nres = my_checkpoint(res, session)\nres.show()\nres.write.parquet(tmpfile())","b74ff110":"import numpy as np\n\nnp.random.seed(0)\ntasks1 = session.createDataFrame(pd.DataFrame(dict(a=np.random.randint(0,1000,100))))\ntasks2 = session.createDataFrame(pd.DataFrame(dict(a=list(range(100)))))\ntasks3 = session.createDataFrame(pd.DataFrame(dict(a=[0]*100)))\ntasks1.show()\ntasks2.show()\ntasks3.show()\n\ndef op2(rows):\n    yield len(list(rows))","2e186d3d":"print(tasks1.repartition(100).rdd.mapPartitions(op2).collect())","a938c050":"print(tasks2.repartition(100).rdd.mapPartitions(op2).collect())","0090c5fc":"print(tasks3.repartition(100).rdd.mapPartitions(op2).collect())","6b0f8f0a":"print(tasks2.repartitionByRange(100, \"a\").rdd.mapPartitions(op2).collect())","e99d958a":"print(tasks1.repartitionByRange(100, \"a\").rdd.mapPartitions(op2).collect())\nprint(tasks3.repartitionByRange(100, \"a\").rdd.mapPartitions(op2).collect())","13fb8e3f":"%%timeit -r1 -n1\nprint(session.createDataFrame(df1.rdd.mapPartitions(op1), \"a int\").repartitionByRange(3, \"a\").rdd.mapPartitions(op2).collect())","178c9bcc":"GLOBAL_VAR = []\n\ndef op3(df):\n    if len(GLOBAL_VAR) ==0:\n        sleep(1)\n        GLOBAL_VAR.append(1)\n    sleep(df.b.sum())\n    return pd.DataFrame([[0]], columns=[\"a\"])\n    \nnp.random.seed(0)\ncat, size = 40, 400\ncats = np.random.randint(0,cat,size)\ndf3 = session.createDataFrame(pd.DataFrame(dict(\n    a=cats,\n    b=np.random.rand(size)*0.006*(cats+20),\n)))\ndf3.show()","85c07cae":"%%timeit -r1 -n1\nsession = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 2).config(\"spark.sql.adaptive.enabled\",False).getOrCreate()\n\ndf3.groupby(\"a\").applyInPandas(op3, \"a int\").count()","0f07c570":"%%timeit -r1 -n1\nsession = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 4).config(\"spark.sql.adaptive.enabled\",False).getOrCreate()\n\ndf3.groupby(\"a\").applyInPandas(op3, \"a int\").count()","09282332":"%%timeit -r1 -n1\nsession = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 8).config(\"spark.sql.adaptive.enabled\",False).getOrCreate()\n\ndf3.groupby(\"a\").applyInPandas(op3, \"a int\").count()","36c88882":"%%timeit -r1 -n1\nsession = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 32).config(\"spark.sql.adaptive.enabled\",False).getOrCreate()\n\ndf3.groupby(\"a\").applyInPandas(op3, \"a int\").count()","b8f08fb9":"# schema: *\ndef fop1(df:pd.DataFrame) -> pd.DataFrame:\n    sleep(5)\n    return df","60a21e01":"from fugue import FugueWorkflow, transform\n\npdf1 = pd.DataFrame([[0]], columns=[\"a\"])","d448d313":"dag = FugueWorkflow()\n\nfdf = dag.df(pdf1).transform(fop1).persist()\nfdf.show(show_count=True)\nfdf.save(tmpfile())\n\n%timeit -r1 -n1 dag.run()  # run on pandas\n%timeit -r1 -n1 dag.run(session)  # run on spark","230cb680":"%%timeit -r1 -n1\nwith FugueWorkflow(session) as dag:\n    fdf = dag.df(pdf1).transform(fop1).checkpoint()\n    fdf.show(show_count=True)\n    fdf.save(tmpfile())","e789cdc5":"%%timeit -r1 -n1\nwith FugueWorkflow(session, {\"fugue.workflow.auto_persist\":True}) as dag:\n    fdf = dag.df(pdf1).transform(fop1)\n    fdf.show(show_count=True)\n    fdf.save(tmpfile())","1ce06efe":"tasks4 = pd.DataFrame(dict(a=[0]*100))\n\n# schema: ct:int\ndef fop2(df:pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame([[df.shape[0]]], columns=[\"ct\"])","05143a7e":"print(transform(tasks4, fop2, partition=\"per_row\", engine=\"spark\").toPandas()[\"ct\"].tolist())","43514cad":"Now let's talk about `checkpoint`\n\n[This](https:\/\/jaceklaskowski.gitbooks.io\/mastering-spark-sql\/content\/spark-sql-checkpointing.html) is a very good article about Spark `checkpoint`. Checkpoint in Spark meant to keep the intermediate dataframe on a distributed file system such as HDFS in order to break the lineage (note `persist` does not break the lineage).\n\nIt is a more robust and more statful way of caching. During a Spark job, if an executor died, the cached data can be retrieved from file system without recompute. Cross Spark jobs, the checkpoint can also be recovered to resume the execution (but it's lack of documentation and examples)\n\nNow, let's replace `persist` with `checkpoint` and see what will happen.","9be42ffe":"## Summary\n\nHere are the rule of thumbs we discussed in this notebook:\n\n1. cache when there are multiple actions\n2. DO NOT let Spark do schema inference\n3. be very careful to use show and head, try to avoid them in production\n4. make `persist` eager with an immediate count()\n5. DO NOT use PySpark's checkpoint, use save and read instead\n6. caching on a file system is a great idea, do not over concern about speed\n7. small data big compute problems are very important, Spark does not have an immediate solution to achieve perfect partitioning\n8. set `spark.sql.shuffle.partitions` to 2x of total executor cores","5be55469":"But `tasks2` is special because it contains continuous increasing integers. Let's try the same thing on the other two lists","2204e235":"Partial action is very tricky, `show` is using `head`, so\n\n**Rule of thumb 3: be very careful to use show and head, try to avoid them in production**\n\nOne thing that may be unnecessary for most use cases is the laziness of `persist`. When people add `persist` to Spark code, they know they are going to add multiple actions after that. So Spark doesn't need to be smart and defer the operations. On the other side, as you can see in the previous examples, in certain cases, the laziness could cause more trouble than being useful. And it will be very hard to debug for someone with average knowledge of Spark.\n\n**Rule of thumb 4: make persist eager with an immediate count()**\n\nNow let's make the original example right:","64b99ecc":"We have avoided schema inference and we have persist the dataframe, why there is still duplicated runs?\n\nBefore answering the question, let's make a small change of the last 3 lines","143b7010":"## Topics:\n\n1. Caching\n2. Partitioning\n3. Repartitioning","7cedab8d":"In this solution, the new checkpoint is eager, scalable (file based), and it breaks the lineage. And with slight modification, it can also work cross Spark jobs.\n\nOn the other hand, it doesn't recompute, and it doesn't require `persist`.\n\nPractically, this is good enough for most cases where you need strong checkpoints.\n\nAnother thing worth to mention is speed. As mentioned in [this](https:\/\/medium.com\/@adrianchang\/apache-spark-checkpointing-ebd2ec065371) article. It says checkpoint will be slower than persist. **This is NOT TRUE**, especially for large dataset. Modern Spark has extremely good disk IO capability, and when data size is large, with multiple factors, caching on a file system may be significantly faster than in memory, not to say other benefits.\n\n**Rule of thumb 6: caching on a file system is a great idea, do not over concern about speed**\n\nThe only real thing to concern is clean up. Neither Spark `checkpoint` nor this DIY solution clean up the cache. But this responsibility can be pushed to the file systems. For example S3 can set expiration on certain prefix, that simply solves the problem.","2a6276be":"Here are what we observed:\n\n* The more partitions, the more overhead. Overhead will certainly drive up the total cpu time, and when there are a lot partitions, it will also affect the wall time to finish.\n* The less partitions, the more skewness. If all expensive partitions are on one logical partition, the process time of that partition could determine the entire wall time.\n\nBoth overhead and skewness could affect time and cost. So we should find a number in the middle.\n\n**Rule of thumb 8: set `spark.sql.shuffle.partitions` to 2x of total executor cores**\n\nPractically, this always works very well. And for Spark 3+, this doesn't conflict with adaptive execution, they work together very well too.","3c45565a":"But why?\n\nLet's test one thing","243e033d":"## Example 1: Caching","cdcda4c2":"As you see, it is not true. We still see the sample problem. How about on `tasks3`","ccf0cc05":"When we add a `count` immediately after `persist`, all data must be materialized. So now it takes only 5 sec.","74d11462":"You see, `op1` doesn't really happen. This is because in spark, persist is lazy.\n\nPersist doesn't really happen until there is an action. But although `head` is an action, it is somehow special. This may be because it is a partial action, it does not realize all partitions, so Spark treats it differently.","fc623f18":"As we can see, for PySpark dataframes, `checkpoint` is eager (there is no lazy option), it happens immediately.\n\nHowever, why it duplicated `op1` again?\n\nThis is due to the special implementation of checkpoint process. The [Spark recommended solution](https:\/\/github.com\/apache\/spark\/blob\/35848385ae6518b4b72c2f5c1e9ca5a83a190723\/core\/src\/main\/scala\/org\/apache\/spark\/rdd\/RDD.scala#L1628-L1629) is `persist` before `checkpoint`","73bc5709":"In Fugue it is very easy to do perfect partitioning. You just need to specify `partition=\"per_row\"`","89b89b34":"Now we define a `FugueWorkflow` to do the same in example 1. And it can run on Pandas or Spark with a parameter change.\n\nSee, we don't need `count()`, everything is handled properly. And `show` isn't special.","27b42057":"Now you see the time is right. However, what is the real value of checkpointing using a file system?\n\nActually, the most important value of this idea is to decouple cache and executors. When you have an expensive compute with large output, using file system to cache is better than caching on executors. You want statefulness and scalability from this approach.\n\nIt doesn't make sense to encourage users to `persist` before `checkpoint`. When a process is expensive and has large output, it may not even be able to be persisted. And this requirement is just due to the less optimal design of Spark.\n\n**Rule of thumb 5: DO NOT use PySpark's checkpoint, use save and read instead**\n\nIt is quite simple and intuitive to do this by yourself:","853b57bd":"## Fugue\n\nFugue is a programming model unifying the compute on Spark, Dask, Pandas and Cudf (through blazing SQL).\n\nLet's see how fugue solves some of the issues mentioned above.\n\nFirst of let's define `op1` and `op2` again","344f3f6c":"# PySpark Pitfalls\n\n## Setup","d41decb2":"We are not discussing a solution here, because it's not trivial. But you must understand the importance of this cenario:\n\n| . | Small Data | Big Data |\n| -- | -- | -- |\n| **Small Compute** | 1 | 2 |\n| **Big Compute** | 3 | 4 |\n\nWe are talking about scenario 3 here, small data, big compute. When data is big (scenario 2 and 4), hash partitioning commonly makes sense because it can achieve relatively even partitioning. However, small data big compute is also a very important scenario that people tend to ignore.\n\nIn scenario 3, perfect partitioning is critical to achieve perfect load balance to fully utilize the compute resource. But to achieve it is not trivial and will require some trade-offs in design. Spark doesn't have an immediate solution for that, neither `repartition` nore `repartitionByRange` can work perfectly in every case.\n\n**Rule of thumb 7: small data big compute problems are very important, Spark does not have an immediate solution to achieve perfect partitioning**","fb417859":"Each of the task lists contain 100 tasks. The first list contains random numbers, the second contains ordered numbers from 0 to 99, the third contains all 0s.\n\nSo now let's consider a very simple thing, assume eash task takes 1 hour, and we have unlimited computing resourse, we just want to leverage Spark to run the tasks in parallel to finish in roughly 1 hour. How should we do it?\n\nFirst of all, let's try on `tasks1`","f918fc00":"Now let's change to Fugue's `checkpoint`. Under the hood, it doesn't use Spark's `checkpoint`. And the effect is identical to `persist`, the difference is that `checkpoint` is using a file system to cache data.","df675cd2":"Another thing to note is that `repartitionByRange` is difficult to implement, and it needs to go through the entire dataset twice, so if you don't persist, the previous operations will run twice:","5ff0ebe0":"It has the same issue. ","c079b93a":"Ah, maybe you know that map is a transformation, and the followings are actions. Actions will trigger transformations, so there must be duplicated runs of op1.\n\n**Rule of thumb 1: cache when there are multiple actions**\n\nWe should add a persist in this case. But it seems op1 ran 6 times, that doesn't make sense, because we have onlye 3 actions. But let's add a persist and see what will happen first.","a3a7309e":"It's not worth to figure out the exact hash function Spark is using because they never commit to fix the hash function forever. Also it's Spark specific logic, other frameworks may use other hash functions, or they don't even use hash partitioning.\n\nThe most important thing is to figure out how to reliably solve this problem.\n\n[repartitionByRange](https:\/\/spark.apache.org\/docs\/3.1.1\/api\/python\/reference\/api\/pyspark.sql.DataFrame.repartitionByRange.html) can be helpful here:","67f4372d":"Now let's create a dummy operation `op1` that does nothing but wait 5 seconds and return the original.\n\nLet's also create a spark dataframe with 1 row 1 column","43f6f9b1":"Surprisingly, when we want to repartition `tasks1` to 100 tasks, some partitions contain 2 items, and some contain nothing. The consequence is that the entire process will take 2 hours no matter how much compute resource you have.\n\nActually this is because Spark's default partitioning logic is based on hash code, since `tasks1` contains random numbers, there could be hash collisions. But `tasks2` should be perfect, it contains only one value each row, and they are from 0-99, and if we partition by 100, each partition should contain 1 item, let's see:","a660e420":"Now, let's see the process time difference for different settings.","226b99b0":"So how long do you think the following will take? 5, 10, 15 or more seconds?","cd01aab6":"Weird, where is 20 sec from? Why is op1 still repeated 4 times?\n\nActually, imagine how toDF is working, it taks the rdd without know what op1 is doing and magically knows its schema, how does that happen?\n\nThis is called schema inference, if you want Spark to guess for you, you pay the price.\n\nSchema inference has to run at least 1 partition of data (using op1) to get the result to guess the schema of all partitions. It is neither safe nor efficient.\n\n**Rule of thumb 2: DO NOT let Spark do schema inference**","3469e161":"Fugue can do auto persist. You just trun on the config, fugue will do every necessary persist automatically.","fdc011ee":"## Example 2: Partitioning\n\nWe have 100 tasks, we want them to run distributedly on 100 workers using PySpark.","8533cd50":"## Example 3: Repartitioning\n\nThis is still about partitioning. We want to discuss how to figure out a good number for `spark.sql.shuffle.partitions`.\n\nIn Spark, if you group by key, or join, `spark.sql.shuffle.partitions` will be used to determine the parallelism.\n\nIn the following example, we have we have a dataframe where `a` will be the group key. We want to group by `a` then for each logical partition, we want to sleep for the total sum of `b` and return a dummy dataframe.\n\nNote that\n\n* there is also a `GLOBAL_VAR` we use it to simulate the overhead of initializing a physical partition.\n* for rows with larger `a` the `b` are also larger, we use this to simulate the skewness of process time for each logical partition"}}