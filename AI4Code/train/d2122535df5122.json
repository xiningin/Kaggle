{"cell_type":{"a1efc9d5":"code","d9bc0b1e":"code","ba016ff4":"code","af6bfd82":"code","e95283b1":"code","5a54704a":"code","d23f1bd4":"code","1ae8294a":"code","5ac59abe":"code","ee91efdd":"code","a565fbd5":"code","64205bce":"code","0c6dec43":"code","a8a3460e":"code","542b1ae3":"code","673b5558":"code","65522e63":"code","3d936d9f":"code","65cf0aed":"code","829dfeb9":"code","e48312f8":"code","ffcdca8f":"code","3da9cadf":"code","1cf9b46d":"code","a1c996b8":"code","9cd1653f":"code","dd95ad24":"code","4373da4e":"code","5076e803":"code","7c303a86":"code","9853b627":"code","2a938f16":"code","a8884e12":"code","76f325e2":"code","0578eb80":"code","f89d4d06":"code","4efd6c40":"code","2f82ce21":"code","9a4089d0":"code","940e3320":"markdown","270a032c":"markdown","723ae9be":"markdown","0467e8b9":"markdown","37b2ed11":"markdown","36ceac41":"markdown","33ed3690":"markdown","9fa9d80c":"markdown","3bd9eb0c":"markdown","7c92fb83":"markdown","b3a2911d":"markdown","94342fcd":"markdown"},"source":{"a1efc9d5":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9bc0b1e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna","ba016ff4":"df = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\ndf = df.drop(\"Id\", axis=1)\ndf.head()","af6bfd82":"df.info()","e95283b1":"df.describe()","5a54704a":"# splitting data into train, validation and test sets to avoid data leakage\n\nfrom sklearn.model_selection import train_test_split as tts\ntrain, valid_test = tts(df.copy(), test_size=0.2, random_state=42)\nvalid, test = tts(valid_test.copy(), test_size=0.5, random_state=42)","d23f1bd4":"# creating list of numerical and categorical features\n\nnum_cols = [column for column in train.columns if train.dtypes[column] == \"int64\"]\ncat_cols = [column for column in train.columns if train.dtypes[column] == \"object\"]\n\nprint(\"Numerical Columns : \" + str(num_cols))\nprint(\"Categorical Columns : \" + str(cat_cols))","1ae8294a":"# visualizing the distribution of all numerical features and the label\n\ntrain.hist(figsize=(12, 9))\nplt.show()","5ac59abe":"# plotting the correlation matrix using seaborn to check for correlation between numerical features\n\ncorr_mat = train.corr()\nsns.heatmap(corr_mat, vmin=-1, vmax=1, center=0, annot=True)\nplt.plot()","ee91efdd":"# checking for outliers\n# replace 'Income' with any other numerical feature that you need to inspect and re-run the cell\n\nplt.figure(figsize=(10,2))\nsns.boxplot(data=train['Income'].values, orient='h') \nplt.plot()\n\n# no feature shows any outliers in the boxplot","a565fbd5":"def print_count_cats(df, columns):\n    '''\n    print the number of categories in each categorical feature\n    '''\n    for column in columns:\n        count = len(df[column].value_counts())\n        print(\"{0} : {1}\".format(column, count))\n\nprint_count_cats(train, cat_cols)","64205bce":"# dropping CITY as it has too many categories. \n# Using one hot encoding will add 317 more features and may lead to the curse of dimensionality\n# train = train.drop([\"CITY\"], axis=1)\n# valid = valid.drop([\"CITY\"], axis=1)\n# test = test.drop([\"CITY\"], axis=1)","0c6dec43":"# ordinal encoding on 'Married\/Single' feature\nfrom sklearn.preprocessing import OrdinalEncoder\n\nmarital_enc = OrdinalEncoder(categories=[['single', 'married']])\ntrain['Married\/Single'] = marital_enc.fit_transform(train['Married\/Single'].values.reshape(-1, 1))\nvalid['Married\/Single'] = marital_enc.transform(valid['Married\/Single'].values.reshape(-1, 1))\ntest['Married\/Single'] = marital_enc.transform(test['Married\/Single'].values.reshape(-1, 1))","a8a3460e":"# ordinal encoding on 'House_Ownership'\n# order between the categories may be percieved as norent_noown < rented < owned\nhouse_enc = OrdinalEncoder(categories=[['norent_noown', 'rented', 'owned']])\ntrain['House_Ownership'] = house_enc.fit_transform(train['House_Ownership'].values.reshape(-1, 1))\nvalid['House_Ownership'] = house_enc.transform(valid['House_Ownership'].values.reshape(-1, 1))\ntest['House_Ownership'] = house_enc.transform(test['House_Ownership'].values.reshape(-1, 1))","542b1ae3":"# ordinal encoding on 'Car_Ownership'\ncar_enc = OrdinalEncoder(categories=[['no', 'yes']])\ntrain['Car_Ownership'] = car_enc.fit_transform(train['Car_Ownership'].values.reshape(-1, 1))\nvalid['Car_Ownership'] = car_enc.transform(valid['Car_Ownership'].values.reshape(-1, 1))\ntest['Car_Ownership'] = car_enc.transform(test['Car_Ownership'].values.reshape(-1, 1))","673b5558":"# prof_enc = OrdinalEncoder()\n# train['Profession'] = prof_enc.fit_transform(train['Profession'].values.reshape(-1, 1))\n# test['Profession'] = prof_enc.transform(test['Profession'].values.reshape(-1, 1))","65522e63":"# state_enc = OrdinalEncoder()\n# train['STATE'] = prof_enc.fit_transform(train['STATE'].values.reshape(-1, 1))\n# test['STATE'] = prof_enc.transform(test['STATE'].values.reshape(-1, 1))","3d936d9f":"# printing shapes of dataframes before one hot encoding\nprint(train.shape)\nprint(valid.shape)\nprint(test.shape)","65cf0aed":"# one hot encoding of 'Profession', 'STATE' using pandas\ndf_combined = pd.get_dummies(train.append([valid, test]), columns=['Profession', 'STATE', 'CITY'],\n                            drop_first=True)\ntrain = df_combined.iloc[:train.shape[0], :].copy()\nvalid = df_combined.iloc[train.shape[0]: train.shape[0] + valid.shape[0], :]\ntest = df_combined.iloc[train.shape[0] + valid.shape[0]:, :].copy()","829dfeb9":"# printing shapes of dataframes after one hot encoding to ensure spilts were made correctly\nprint(train.shape)\nprint(valid.shape)\nprint(test.shape)","e48312f8":"# seperating the features and the label\n\ntrain_y = train['Risk_Flag'].copy()\ntrain = train.drop('Risk_Flag', axis=1)\n\nvalid_y = valid['Risk_Flag'].copy()\nvalid = valid.drop('Risk_Flag', axis=1)\n\ntest_y = test['Risk_Flag'].copy()\ntest = test.drop('Risk_Flag', axis=1)","ffcdca8f":"# oversampling\n\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\n\nsampler = RandomOverSampler(sampling_strategy=0.25)\n\nprint(\"Before sampling: \" + str(Counter(train_y)))\ntrain, train_y = sampler.fit_resample(train, train_y)\nprint(\"After sampling: \" + str(Counter(train_y)))","3da9cadf":"# undersampling\n\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\n\nsampler = RandomUnderSampler(sampling_strategy=1.0)\n\nprint(\"Before sampling: \" + str(Counter(train_y)))\ntrain, train_y = sampler.fit_resample(train, train_y)\nprint(\"After sampling: \" + str(Counter(train_y)))","1cf9b46d":"# # SMOTE\n\n# from collections import Counter\n# from imblearn.over_sampling import SMOTE\n\n# smote = SMOTE()\n\n# print(\"Before sampling: \" + str(Counter(train_y)))\n# train, train_y = smote.fit_resample(train, train_y)\n# print(\"After sampling: \" + str(Counter(train_y)))","a1c996b8":"# performing scaling on all the features\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_X = scaler.fit_transform(train)\nvalid_X = scaler.transform(valid)\ntest_X = scaler.transform(test)","9cd1653f":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, roc_auc_score, recall_score\n\n# hyperparameter tuning of xgboost using optuna \n\ndef objective(trial):\n    # setting search space\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 5000)\n    \n    # defining the model\n    clf = XGBClassifier(learning_rate=learning_rate, \n                        reg_lambda=reg_lambda,\n                        subsample=subsample, \n                        colsample_bytree=colsample_bytree, \n                        tree_method='gpu_hist', predictor=\"gpu_predictor\", # using gpu to speed up the process\n                        max_depth=max_depth, \n                        n_estimators=n_estimators, \n                        use_label_encoder=False,\n                        random_state=42)\n    \n    clf.fit(train_X, train_y, eval_metric='logloss')\n    valid_preds = clf.predict(valid_X)\n    score = roc_auc_score(valid_y, valid_preds)\n    \n    return score","dd95ad24":"# # This cell is run only when hypertuning using OPTUNA\n\n# # creating study object\n# study = optuna.create_study(direction=\"maximize\")\n\n# # optimising the study object\n# study.optimize(objective, n_trials=100)\n\n# # Print the result\n# best_params = study.best_params\n# best_score = study.best_value\n# print(f\"Best score: {best_score}\\n\")\n# print(f\"Optimized parameters: {best_params}\\n\")","4373da4e":"# Optuna results\n\n# Best score: 0.7530063367504715\n# scored on roc auc\nparams = {'learning_rate': 0.22657857685769822, \n                       'reg_lambda': 5.8263201980534444e-08, \n                       'reg_alpha': 6.30631361255538e-05, \n                       'subsample': 0.3678211180860871, \n                       'colsample_bytree': 0.8672653738124343, \n                       'max_depth': 7, \n                       'n_estimators': 4973}\n\n\n# performance on test set\n# [[20760  1362]\n#  [ 1405  1673]]\n# recall: 0.5435347628330085\n# precision: 0.5512355848434926\n# f1_score: 0.5473580893178474\n# accuracy_score: 0.8901984126984127\n# ROC AUC score: 0.74098354632022","5076e803":"# measuring performance of our model on the test set\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n\n\nxgb = XGBClassifier(random_state=42, use_label_encoder=False,\n                    tree_method='gpu_hist', **params)\n\nxgb.fit(train_X, train_y, eval_metric='logloss')\ny_pred = xgb.predict(valid_X)\n\ncm = confusion_matrix(valid_y, y_pred)\nprint(cm)\nprint(\"recall: \" + str(recall_score(valid_y, y_pred)))\nprint(\"precision: \" + str(precision_score(valid_y, y_pred)))\nprint(\"f1_score: \" + str(f1_score(valid_y, y_pred)))\nprint(\"accuracy_score: \" + str(accuracy_score(valid_y, y_pred)))\nprint(\"ROC AUC score: \" + str(roc_auc_score(valid_y, y_pred)))","7c303a86":"# using optuna for lightgbm hypertuning","9853b627":"# using lightGBM\n\n# from lightgbm import LGBMClassifier\n\n# lgbm_clf = LGBMClassifier()\n# lgbm_clf.fit(train_X, train_y)\n# y_pred = lgbm_clf.predict(test_X)\n\n# cm = confusion_matrix(test_y, y_pred)\n# print(cm)\n# print(\"recall: \" + str(recall_score(test_y, y_pred)))\n# print(\"precision: \" + str(precision_score(test_y, y_pred)))\n# print(\"f1_score: \" + str(f1_score(test_y, y_pred)))\n# print(\"accuracy_score: \" + str(accuracy_score(test_y, y_pred)))\n# print(\"ROC AUC score: \" + str(roc_auc_score(test_y, y_pred)))","2a938f16":"# saving the model in a pickle file\n\nimport pickle\n\nfilename = 'xgb_classifier.pkl'\nwith open(filename, 'wb') as file:\n    pickle.dump(xgb, file)","a8884e12":"# Creating new dataset splits\nfinal_train, final_test = tts(df.copy(), test_size=0.1, random_state=42)\n\n# seperating the features and the label\nfinal_train_y = final_train['Risk_Flag'].copy()\nfinal_train_X = final_train.drop('Risk_Flag', axis=1)\nfinal_test_y = final_test['Risk_Flag'].copy()\nfinal_test_X = final_test.drop('Risk_Flag', axis=1)\n\n# Sampling the final training set\noversampler = RandomOverSampler(sampling_strategy=0.25)\nprint(\"Before over sampling: \" + str(Counter(final_train_y)))\nfinal_train_X, final_train_y = oversampler.fit_resample(final_train_X, final_train_y)\nprint(\"After over sampling: \" + str(Counter(final_train_y)))\n\nundersampler = RandomUnderSampler(sampling_strategy=1.0)\nprint(\"Before under sampling: \" + str(Counter(final_train_y)))\nfinal_train_X, final_train_y = undersampler.fit_resample(final_train_X, final_train_y)\nprint(\"After under sampling: \" + str(Counter(final_train_y)))","76f325e2":"final_train_X.shape","0578eb80":"final_test_X.shape","f89d4d06":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nmarital_enc = OrdinalEncoder(categories=[['single', 'married']])\n\n# order between the categories may be percieved as norent_noown < rented < owned\nhouse_enc = OrdinalEncoder(categories=[['norent_noown', 'rented', 'owned']])\n\ncar_enc = OrdinalEncoder(categories=[['no', 'yes']])\n\nenc1hot_profession = enc = OneHotEncoder(handle_unknown='error', drop='first', sparse=False)\nenc1hot_state = enc = OneHotEncoder(handle_unknown='error', drop='first', sparse=False)\nenc1hot_city = enc = OneHotEncoder(handle_unknown='error', drop='first', sparse=False)\n\n\nscaler = StandardScaler()\n\ntransformers = [\n    ('marital_enc', marital_enc, ['Married\/Single']),\n    ('house_enc', house_enc, ['House_Ownership']),\n    ('car_enc', car_enc, ['Car_Ownership']),\n    ('enc1hot_profession', enc1hot_profession, ['Profession']),\n    ('enc1hot_state', enc1hot_state, ['STATE']),\n    ('enc1hot_city', enc1hot_city, ['CITY']),\n]\n\nencoding_transformer = ColumnTransformer(transformers=transformers, \n                                        remainder='passthrough', n_jobs=-1)\n\nsteps = [\n    ('encoding_transformer', encoding_transformer),\n    ('scaler', scaler),\n    ('xgb_clf', xgb)\n]\n\nfull_pipeline = Pipeline(steps=steps, verbose=True)","4efd6c40":"# training using the full pipeline\n\nfull_pipeline.fit(final_train_X, final_train_y)","2f82ce21":"# testing the performance of our pipeline\n\nfinal_y_pred = full_pipeline.predict(final_test_X)\n\ncm = confusion_matrix(final_test_y, final_y_pred)\nprint(cm)\nprint(\"Recall: \" + str(recall_score(final_test_y, final_y_pred)))\nprint(\"Precision: \" + str(precision_score(final_test_y, final_y_pred)))\nprint(\"F1 Score: \" + str(f1_score(final_test_y, final_y_pred)))\nprint(\"Accuracy Score: \" + str(accuracy_score(final_test_y, final_y_pred)))\nprint(\"ROC AUC score: \" + str(roc_auc_score(final_test_y, final_y_pred)))","9a4089d0":"# saving pipeline in pickle file\n\nfilename = 'full_pipeline.pkl'\nwith open(filename, 'wb') as file:\n    pickle.dump(full_pipeline, file)","940e3320":"## Creating the final pipeline","270a032c":"### Numerical Features","723ae9be":"## Creating Models","0467e8b9":"### Standardization","37b2ed11":"## Loading the Data","36ceac41":"### Categorical Features","33ed3690":"## Exploratory Data Analysis","9fa9d80c":"### Handling Imbalanced Dataset","3bd9eb0c":"### Encoding","7c92fb83":"### Creating fresh datasets to train and test the pipeline","b3a2911d":"## Feature Engineering - Modifying our features","94342fcd":"## Importing Libraries"}}