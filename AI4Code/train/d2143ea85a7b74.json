{"cell_type":{"88f9663b":"code","1f1dfeae":"code","b3eb5bfb":"code","f012eeae":"code","e8625277":"code","60c69e8b":"code","d9fb5fb8":"code","55f99116":"code","49eec50d":"code","b0259ecf":"code","c0d7f198":"code","3dc607ec":"code","c0d7ca18":"code","de4b7bad":"code","3cf54426":"code","d47238f1":"code","90f6620b":"code","070adc56":"code","b478aca6":"code","599430e1":"code","d2ef3fc7":"code","6378b330":"code","30012107":"code","6ebcb278":"code","8754ae0e":"code","ce6ac9f9":"code","a5b1504e":"code","8717d8e1":"code","7f2df3bf":"code","bf1a302b":"code","43e4e90b":"code","1efd3bce":"code","d3932658":"code","d161abd0":"code","a328ad9a":"code","c3cdba22":"code","e72896ad":"code","f3cd5f70":"code","e3f87490":"code","2b7f37df":"code","29e926cf":"code","bdb7c3a4":"code","1f0f94a5":"code","3a5c587d":"code","2288cbcc":"code","ed5d9ae8":"code","bf1fa8b0":"code","5c722dd1":"code","913e2e40":"code","d3338693":"code","a5d0ada1":"code","7036f454":"code","d801185e":"code","2ee94389":"code","04de04f7":"code","41cf284c":"code","3901c21b":"code","7d02044a":"code","5902e35d":"code","458cbc01":"code","218dbc26":"code","154fe7f4":"code","8734b53b":"markdown","ec1e0d0e":"markdown","a48ed591":"markdown","3d97d755":"markdown"},"source":{"88f9663b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport xgboost as xgb\nfrom time import time\nimport os\n#print(os.listdir(\"..\/input\"))\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1f1dfeae":"train = pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/test_QkPvNLx.csv')\n","b3eb5bfb":"train.head()","f012eeae":"test.head()","e8625277":"train.describe()","60c69e8b":"train.describe(include='object')","d9fb5fb8":"test.describe()","55f99116":"test.describe(include='object')","49eec50d":"# checking for null values\ntrain.isnull().sum()","b0259ecf":"train.fillna(train['Competition_Metric'].median(),inplace=True)","c0d7f198":"test.isnull().sum()","3dc607ec":"test.fillna(train['Competition_Metric'].median(),inplace=True)","c0d7ca18":"train['Day_No'].value_counts()","de4b7bad":"test['Day_No'].value_counts().head()","3cf54426":"train['Course_ID'].value_counts()","d47238f1":"test['Course_ID'].value_counts()","90f6620b":"sns.countplot(train['Course_Domain'])","070adc56":"sns.countplot(train['Course_Type'])","b478aca6":"sns.countplot(test['Course_Domain'])","599430e1":"sns.countplot(test['Course_Type'])","d2ef3fc7":"sns.countplot(train['Short_Promotion']) ","6378b330":"sns.countplot(test['Short_Promotion']) ","30012107":"sns.countplot(train['Public_Holiday'])","6ebcb278":"sns.countplot(test['Public_Holiday'])","8754ae0e":"sns.countplot(train['Long_Promotion'])","ce6ac9f9":"sns.countplot(test['Long_Promotion'])","a5b1504e":"#target variable\nsns.distplot(train['Sales'])","8717d8e1":"# it is skewed hence while performing model,log1p transformation is applied","7f2df3bf":"sns.distplot(train['User_Traffic'])","bf1a302b":"sns.distplot(train['Competition_Metric'])","43e4e90b":"sns.distplot(test['Competition_Metric'])","1efd3bce":"# sales of course_ID 1 full timne\nstrain = train[train.Sales>0]\nstrain.loc[strain['Course_ID']==1 ,['Day_No','Sales']].plot(x='Day_No',y='Sales',title='Course_ID 1',figsize=(16,4))\n","d3932658":"# sales of course_ID 2  full timne\nstrain = train[train.Sales>0]\nstrain.loc[strain['Course_ID']==2 ,['Day_No','Sales']].plot(x='Day_No',y='Sales',title='Course_ID 2',figsize=(16,4))\n","d161abd0":"#sales of course_id 1 ,day no 240 vs 605\nstrain = train[train.Sales>0]\nstrain.loc[strain['Course_ID']==1 ,['Day_No','Sales']]     .plot(x='Day_No',y='Sales',title='Course_ID 1',figsize=(8,2),xlim=[240,605])\nstrain.loc[strain['Course_ID']==1 ,['Day_No','Sales']]     .plot(x='Day_No',y='Sales',title='Course_ID 1',figsize=(8,2),xlim=[240,605])","a328ad9a":"# drop user_traffic variable as it is not present is test data","c3cdba22":"train1=train.copy()","e72896ad":"train1.drop('User_Traffic',axis=1,inplace=True)","f3cd5f70":"# Creating new feature Day_of_week in train and test data","e3f87490":"train1['Day_of_week']=train1['Day_No'].apply(lambda x:x%7)","2b7f37df":"test1=test.copy()","29e926cf":"test1['Day_of_week']=test1['Day_No'].apply(lambda x:x%7)","bdb7c3a4":"# one hot encoding for categorical columns","1f0f94a5":"train1=pd.get_dummies(train1,columns=['Course_Domain','Course_Type'],drop_first=True)","3a5c587d":"test1=pd.get_dummies(test1,columns=['Course_Domain','Course_Type'],drop_first=True)","2288cbcc":"# splitting train data into train and test(for validating)","ed5d9ae8":"ho_test = train1[:6*7*600]\n\nho_train = train1[6*7*600:]","bf1fa8b0":"plt.subplots(figsize=(24,20))\nsns.heatmap(ho_train.corr(),annot=True, vmin=-0.1, vmax=0.1,center=0)","5c722dd1":"# defining RMSLE metric (Root mean squared log error)","913e2e40":"from sklearn.metrics import mean_squared_log_error\ndef rmsle(y, yhat):\n    return (np.sqrt(mean_squared_log_error(y, yhat)))*1000\n\ndef rmsle_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmsle\", rmsle(y,yhat)","d3338693":"# as the target variable is skewed applying log1p transformation","a5d0ada1":"ho_xtrain = ho_train.drop(['Sales','ID'],axis=1 )\nho_ytrain = np.log1p(ho_train.Sales)\nho_xtest = ho_test.drop(['Sales','ID'],axis=1 )\nho_ytest = np.log1p(ho_test.Sales)","7036f454":"params = {\"objective\": \"reg:linear\",\n          \"booster\" : \"gbtree\",\n          \"eta\": 0.03,\n          \"max_depth\": 14,\n          \"subsample\": 0.9,\n          \"colsample_bytree\": 0.7,\n          \"silent\": 1,\n          \"seed\": 10\n          }\nnum_boost_round = 6000\n\n\ndtrain = xgb.DMatrix(ho_xtrain, ho_ytrain)\ndvalid = xgb.DMatrix(ho_xtest, ho_ytest)\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]","d801185e":"print(\"Train a XGBoost model\")\nstart = time()\ngbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \n  early_stopping_rounds=100, feval=rmsle_xg, verbose_eval=True)\nend = time()\nprint('Training time is {:2f} s.'.format(end-start))","2ee94389":"print(\"validating\")\nho_xtest.sort_index(inplace=True) \nho_ytest.sort_index(inplace=True) \nyhat = gbm.predict(xgb.DMatrix(ho_xtest))\nerror = rmsle(np.expm1(ho_ytest), np.expm1(yhat))\n\nprint('RMSPE: {:.6f}'.format(error))","04de04f7":"res = pd.DataFrame(data = ho_ytest)\nres['Prediction']=yhat\nres = pd.merge(ho_xtest,res, left_index= True, right_index=True)\nres['Ratio'] = res.Prediction\/res.Sales\nres['Error'] =abs(res.Ratio-1)\nres['Weight'] = res.Sales\/res.Prediction\nres.head()","41cf284c":"for i in range(1,5):\n    s1 = pd.DataFrame(res[res['Course_ID']==i],columns = ['Sales','Prediction'])\n    s2 = pd.DataFrame(res[res['Course_ID']==i],columns = ['Ratio'])\n    s1.plot(figsize=(12,4))\n    s2.plot(figsize=(12,4))\n    plt.show()","3901c21b":"test2=test1.drop(['ID'],axis=1)","7d02044a":"print(\"Make predictions on the test set\")\n\ndtest = xgb.DMatrix(test2)\ntest_probs = gbm.predict(dtest)\n\n# model1  kaggle private score 0.12647\nresult = pd.DataFrame({\"ID\": test1['ID'], 'Sales': np.expm1(test_probs)})","5902e35d":"result","458cbc01":"print(\"weight correction\")\nW=[(0.995+(i\/1000)) for i in range(30)]\nS =[]\nfor w in W:\n    error = rmsle(np.expm1(ho_ytest), np.expm1(yhat*w))\n    print('RMSPE for {:.3f}:{:.6f}'.format(w,error))\n    S.append(error)\nScore = pd.Series(S,index=W)\nScore.plot()\nBS = Score[Score.values == Score.values.min()]\nprint ('Best weight for Score:{}'.format(BS))","218dbc26":"result_w3 = pd.DataFrame({\"ID\": test1['ID'], 'Sales': np.expm1(test_probs*1.001)})\nresult_w3 #.to_csv('gbmm7.csv',index=False) ","154fe7f4":"# out of the different models XGBoost Worked well and also Random Forest gave relatively good results.\n# The predictions can be more improved provided there are more features\n# Features like Year and Week of year were added and different models are builded but none of them performed better.Hence they are removed and only dy of week feature is added\n# All the features were important and ID feature has much correlation with Course_ID variable hence it is dropped\n","8734b53b":"### Descriptive Statistics","ec1e0d0e":"## Exploratory Data Analysis","a48ed591":"# Model Building","3d97d755":"# Feature Engineering"}}