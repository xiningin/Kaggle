{"cell_type":{"0c72f458":"code","5b07a8e6":"code","b0307e72":"code","834b4f4b":"code","6bbe0f3c":"code","5cef078e":"code","7ef0b099":"code","d9f0b591":"code","9a71b34c":"code","5e533c34":"code","fdb12400":"code","1d7a1499":"code","19e59f3b":"code","5e2a13e5":"code","0ba8996a":"code","91177b91":"code","bf4148f6":"code","4cf45ac6":"code","bd8978ab":"code","256fcb87":"code","211648cb":"code","30e09a05":"code","6436e15d":"code","0faca790":"code","5e7fe4e2":"code","6dd8da0b":"code","b430c9d5":"code","195b0c50":"code","22730b34":"code","3ea7e07f":"code","f1b37ab2":"code","375ba509":"code","c2c93806":"code","1d492ac9":"markdown","9218c454":"markdown","7660770b":"markdown","2282eadc":"markdown","9adfedc7":"markdown","7906aa36":"markdown","720465d2":"markdown","53469c86":"markdown","4496c9af":"markdown","c82246b4":"markdown","ecccb35d":"markdown"},"source":{"0c72f458":"# Import Python modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # ploting statistical graphs\n\nimport warnings\nwarnings.filterwarnings('ignore')","5b07a8e6":"# read heart.csv data into a Pandas DataFrame\ndata = pd.read_csv('..\/input\/heart.csv')\n\n# Returns the dimensionality of the DataFrame (rows, columns).\nprint(data.shape)\n#print(len(data))","b0307e72":"# Explore data\ndata.head()","834b4f4b":"#print last 3 rows\ndata.tail(3)","6bbe0f3c":"# Generate descriptive statistics of DataFrame columns (I am using display to print table).\ndisplay(data.describe())","5cef078e":"#Print a concise summary of a DataFrame\ndata.info()","7ef0b099":"# Check Null values\nprint(data[pd.isnull(data).any(axis=1)])","d9f0b591":"# Another way to check for null values\ndata.isnull().sum()","9a71b34c":"# Print column names\ndata.columns","5e533c34":"# print Pairwise relationships between the features\nsns.pairplot(data)","fdb12400":"plt.figure(figsize=(14,8))\nsns.heatmap(data.corr(), linewidths=.01, annot = True, cmap='Greens')\nplt.show()","1d7a1499":"# Create column list (used by graphviz for ptintin decision tree)\nfeature_cols = ['age','sex','chest_pain','rest_bp','chol','fasting_bloodsugar','rest_ecg','max_heartrate','excercise_angina','oldpeak','slope','n_major_vasel','thal']","19e59f3b":"# Split dataset in features and target variable\nX = data.drop(['target'],axis=1) #features\ny = data.target # response\/target\nprint (\"Feature: \", X.shape)  # metrix\nprint (\"Response: \", y.shape) # series","5e2a13e5":"import sklearn\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Split the dataset into two sets, so that the model can be trained and tested on different data\nfrom sklearn.model_selection import train_test_split\n# Split dataset into training set (80%)) and test set (20%)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5)\nprint (\"Taining data: \", X_train.shape)\nprint (\"Test data: \", X_test.shape)","0ba8996a":"# Model: 1. Create Decision Tree classifer object\nfrom sklearn.tree import DecisionTreeClassifier\ncf = DecisionTreeClassifier()\n# Train the model using the training sets\ncf = cf.fit(X_train,y_train)\n# make Predictions on the test dataset\ncf_predicted = cf.predict(X_test)","91177b91":"# Classification accuracy, how often is the classifier correct (percentage of correct predictions)?\n# Determine the accuracy of the model (compare actual value:y_test with predicted value:cf_predicted)\nprint (\"DECISION TREE:\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, cf_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, cf_predicted))","bf4148f6":"# Null accuracy for binary classification problems\n#Examine class distribution of testing set\ny_test.value_counts()","4cf45ac6":"# calculate percentage of NO heart disease (target: 0)\ny_test.mean()","bd8978ab":"# calculate percentage of heart disease (target: 1)\n1-y_test.mean()","256fcb87":"## generate classification tree for DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\ndot_data = tree.export_graphviz(cf, out_file=None, feature_names=feature_cols,  class_names=['0','1'],  filled=True, rounded=True,  special_characters=True)\ngraph = graphviz.Source(dot_data)\n#graph.render(\"class\") ## print pdf file\ngraph","211648cb":"# Model: 2. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n#Train the model using the training sets\nlr.fit(X_train,y_train)\n#Predict the response for test dataset\nlr_predicted=lr.predict(X_test)\nprint (\"LOGISTIC REGRESSION:\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, lr_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, lr_predicted))","30e09a05":"# Model: 3. Support Vector Machine\nfrom sklearn.svm import SVC\nsm = SVC(gamma='scale')\n#Train the model using the training sets\nsm.fit(X_train,y_train)\n#Predict the response for test dataset\n#sm.score(X_test, y_test)\nsm_predicted=sm.predict(X_test)\nprint (\"SUPPORT VECTOR MACHINE\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, sm_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, sm_predicted))","6436e15d":"# Model: 4. K-Neighrest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\n#Train the model using the training sets\nknn.fit(X_train,y_train)\n#Predict the response for test dataset\nknn_predicted = knn.predict(X_test)\nprint (\"K-NEIGHEST NEIGHBORS\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, knn_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, knn_predicted))","0faca790":"# Model: 5. Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\ngb.fit(X_train,y_train)\ngb_predicted = gb.predict(X_test)\nprint (\"NAIVE BAYES\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, gb_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, gb_predicted))","5e7fe4e2":"# Model: 6. Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nprint (\"RANDOM FOREST\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, rf_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, rf_predicted))","6dd8da0b":"# Model: 7. Neural Network\nfrom sklearn.neural_network import MLPClassifier\nnn = MLPClassifier()\nnn.fit(X_train,y_train)\nnn_predicted = nn.predict(X_test)\nprint (\"NEURAL NETWORK\")\nprint (\"Accuracy Score:\")\nprint (metrics.accuracy_score(y_test, nn_predicted))\n# Compute confusion matrix to evaluate the accuracy of a classification\nprint (\"Confusion metrix:\")\nprint (metrics.confusion_matrix(y_test, nn_predicted))","b430c9d5":"# We have learn how to use different ML classifier. Next we will use Python script to comapre different classifiers.\n# Import classifiers (commented out as we have already imported in previous section)\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.svm import SVC\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.neural_network import MLPClassifier\n\n# Create a dictionary\nclassifier_collection = {\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Logistic Regression\": LogisticRegression(),\n    \"SVM\": SVC(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Naive Bayes\": GaussianNB(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Neural Network\": MLPClassifier()\n}\n\n# Create and evaluate models\n# Evaluation criteria: accuracy_score (help you to choose between models and qualify model performance)\naccuracy_score_dict = {}\nconfusion_matrix_dict = {}\nroc_auc_dict = {}\ncount=0\nfor classifier_model, classifier in (classifier_collection.items()):\n    #print(classifier_name)\n    count +=1\n    classifier.fit(X_train,y_train)\n    predicted = classifier.predict(X_test)\n    accuracy_score_dict[classifier_model] = {'accuracy_score' :  metrics.accuracy_score(y_test, predicted)}\n    confusion_matrix_dict[classifier_model] = {'matrix' : metrics.confusion_matrix(y_test, predicted)}\n    roc_auc_dict[classifier_model] = {'roc-auc' : metrics.roc_auc_score(y_test, predicted)}","195b0c50":"accuracy_score_dict","22730b34":"confusion_matrix_dict","3ea7e07f":"roc_auc_dict","f1b37ab2":"# Model evaluation : Confusion matrix\nplt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion matrix for different models\",fontsize=24)\n# initialize n with zero\nn = 0\nfor classifier, class_score in confusion_matrix_dict.items():\n    for item, score in class_score.items():\n        # excluded confusion matrix for SVM (with lowest accuracy score))\n        if classifier != \"SVM\":\n            pass\n            n +=1\n            plt.subplot(2, 3, n) \n            plt.title(classifier, fontsize=8)\n            sns.heatmap(score,annot=True,cbar=False,cmap=\"Greens\",fmt=\"d\")","375ba509":"# Model evaluation : Accuracy score\nax = pd.DataFrame(accuracy_score_dict).plot(kind='bar', figsize=(16,8), title=\"Accuracy score compared between the models\")\nx_axis = ax.axes.get_xaxis()\nx_axis.set_visible(False)\nax.set_ylabel(\"Accuracy score\")\nax.set_ylim(0,1)","c2c93806":"# Model evaluation : \nax = pd.DataFrame(roc_auc_dict).plot(kind='bar', figsize=(16,8), title=\"ROC AUC score compared between the models\")\nx_axis = ax.axes.get_xaxis()\nx_axis.set_visible(False)\nax.set_ylabel(\"ROC AUC score\")\nax.set_ylim(0,1)","1d492ac9":"###### Dataset features:\n\n1. age in years\n2. sex(1 = male; 0 = female)\n3. cp chest pain type\n4. trestbpsresting blood pressure (in mm Hg on admission to the hospital)\n5. cholserum cholestoral in mg\/dl\n6. fbs(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecgresting electrocardiographic results\n8. thalachmaximum heart rate achieved\n9. exangexercise induced angina (1 = yes; 0 = no)\n10. oldpeakST depression induced by exercise relative to rest\n11. slopethe slope of the peak exercise ST segment\n12. canumber of major vessels (0-3) colored by flourosopy\n13. thal3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target 1 or 0","9218c454":"Further improvements: In this study all the algorithms were tested with default parameters. These parameters influence the outcome of learning process. The classifier's parameters can be tuned to improve the accuracy of the model.","7660770b":"###### Data summary:\n\nTotal number of obsevations (rows): 303 # note that row numbering starts from 0\n\nTotal number of features (columns): 14\n\nResponse (target column): (0,1)\n\n    0: no disease\n    1: presence of disease","2282eadc":"**4. Visualization**","9adfedc7":"**3. Model Evaluation**","7906aa36":"###### Conclusion: Our best performing model is Logistic Regression  with ~ 90% accuray.","720465d2":"###### Confusion matrix explained\n\t       Predicted 0\t\tPredicted 1\n    \n    Actual 0\t  TN(24)             FP(6)\n\n    Actual 1\t  FN(5)\t\t      TP(26)\n\n### Terminology\nTrue negative  (TN): correctly predicted the patient has NO heart disease (actual value: No disease).\n\nTrue positive  (TP): correctly predicted patient has heart disease(actual value: disease).\n\nFalse negative (FN): incorrectly predicted patient has NO heart disease (actual value: disease).\n\nFalse positive (FP): incorrectly predicted patient has heart disease (actual value: No disease).","53469c86":"**1. Explore dataset: Heart Disease dataset from UCI**","4496c9af":"#### Introduction\n\n1. Explore dataset: Heart Disease dataset from UCI\n\n\n2. Model building in Scikit-learn\n\n    - Decision Tree\n    - Logistic Regression\n    - Support Vector Machine\n    - K-Neighbors\n    - Naive Bayes\n    - Random Forest\n    - Neural Network\n    \n\n3. Model Evaluation\n\n    - Confusion Matrix\n    - Accuracy score\n    - ROC AUC score\n    \n4. Visualization\n","c82246b4":"**2. Model building in Scikit-learn**","ecccb35d":"I am new to machine learning and this is my 1st kaggle kernel. Any comments and suggestions are welcome!"}}