{"cell_type":{"b6bc35a3":"code","f33e015d":"code","7167af47":"code","1ac15e1d":"code","f77294a9":"code","256d8b60":"code","4acb6e81":"code","115549ad":"code","14085747":"code","140eaa05":"code","034b9b73":"code","7ccabd14":"code","c8a30a8d":"code","5ee53416":"code","639b1f6a":"code","9face75a":"code","51972801":"code","5b87fd7d":"code","6a2f9372":"code","66d9e138":"code","cbce5de5":"markdown","892cee6e":"markdown","eb0f1f88":"markdown","fd26a76c":"markdown","6d7500e2":"markdown","a07b5361":"markdown","2bfe38f5":"markdown","ebd54a77":"markdown","d64bd83b":"markdown","672ace52":"markdown","85b0407e":"markdown","19b3ac61":"markdown"},"source":{"b6bc35a3":"## HOW WE TRAINED THE MODEL \n\n\n\nimport os\nimport joblib\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport pickle\nimport re\n\n\nimport nltk.corpus\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n","f33e015d":"# nltk.download(\"all\")","7167af47":"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1ac15e1d":"#LOADING DATA\ntrain = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")\n\n\n#PLOTTING HISTOGRAM\ntrain['label'].hist()\nplt.show()\n\n\n#TURNING LABELS INTO CATEGORIES\ntrain['label'] = train['label'].astype('category')\ntrain.drop(columns=['id'], inplace=True)\n","f77294a9":"\ntrain_2 = pd.read_csv(\"\/kaggle\/input\/hatespeechlabeleddata\/labeled_data.csv\")\ntrain_2['class'].hist()\nplt.show()\n\n#create new columns containing \ntrain_2['label'] = 1\n# this dataset contains three levels of hate speech level 0,1,2\n# We have added all hate tweets to our existing data so that we can get a balanced data\n\n","256d8b60":"#Creating dataframe\ntrain_2 = pd.DataFrame(train_2[['label',\"tweet\"]])","4acb6e81":"#Concatinating both dataframes\ntrain = [train, train_2]\nresult = pd.concat(train)\n","115549ad":"# shuffling data and reseting index\nresult=shuffle(result)\nresult = result.reset_index(drop=True)\n\n#Plotting histogram\nresult['label'].hist()","14085747":"#checking for null values\nresult.info()","140eaa05":"# Storing stopwords of english language from nltk library\nsw = set(stopwords.words(\"english\"))\n\n# remove stop words\ndef filter_words(word_list):\n    useful_words = [ w for w in word_list if w not in sw ]\n    return(useful_words)\n\n\n\ndef preprocess_data(dataset):\n    data = dataset.copy()\n\n    #Removing punctuations, special characters and lemmatizing words to their base form\n    data['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]',' ',text)) for text in lis]) for lis in data['tweet']]\n    \n    a=[]\n    for text in data['text_lem']:\n        word_list = word_tokenize(text)\n        text=filter_words(word_list)\n        a.append(text)  \n    \n    train_text = []\n    for i in a:\n        sent=''\n        for  j in i:\n            sent += str(j) + ' '\n        train_text.append(sent)\n\n    data['cleaned_tweets'] = train_text\n    \n    #Using TF-IDF vectorizer\n    vect = TfidfVectorizer(ngram_range = (1,3)).fit(data['cleaned_tweets'])\n    \n    #Transforming our data using the vector trained on training data.  \n    vectorized_tweets = vect.transform(data['cleaned_tweets'])\n    \n    return vectorized_tweets, vect\n\n\n    ","034b9b73":"#storing preprocessed data in data_train and vector in vect\ndata_train,vect  = preprocess_data(result)\ndata_target = np.array(result[\"label\"])","7ccabd14":"\nprint(data_train.shape, data_target.shape)","c8a30a8d":"X_train, X_test, y_train, y_test = train_test_split(data_train, data_target, test_size=0.2, random_state=42)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","5ee53416":"# Helping_Function to show Cross Val Scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n","639b1f6a":"# we are using logistic regression  \nlg_reg_clf = LogisticRegression(C=50, penalty='l2', solver='lbfgs')","9face75a":"#Calculating cross-val score\nscore = cross_val_score(lg_reg_clf, X_train, y_train, cv=7)","51972801":"#Display CV score:\ndisplay_scores(score)","5b87fd7d":"model = lg_reg_clf.fit(X_train, y_train)","6a2f9372":"print(\"Accuracy   :\\t\",lg_reg_clf.score(X_test,y_test))\nsns.heatmap(confusion_matrix(lg_reg_clf.predict(X_test), y_test),annot=True)\nplt.show()","66d9e138":"with open('model','wb') as f:\n    pickle.dump(model,f)\n    \nwith open('vector','wb') as f:\n    pickle.dump(vect,f)\n    ","cbce5de5":"## Train Test Split","892cee6e":"## Our dataset is imbalanced, so just to balance it we loaded many dataset and combined the datasets so that we get a balanced data set containing nearly equal number of both the classes","eb0f1f88":"### If you dont have ntlk then install it!","fd26a76c":"# Accuracy and confusion matrix:","6d7500e2":"## Now training model on our data and checking its accuracy on our test","a07b5361":"## Training the MODEL","2bfe38f5":"# CV score:","ebd54a77":"# Preprocessing","d64bd83b":"# Importing data","672ace52":"## Now we have nearly equal data of both the classes","85b0407e":"# Importing libraries","19b3ac61":"# Saving model and vector "}}