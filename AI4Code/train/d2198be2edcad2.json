{"cell_type":{"2a9a29f2":"code","094af7b8":"code","806803b4":"code","899a0f3e":"code","79998ca5":"code","f83f930b":"code","3aa5e712":"code","819243eb":"code","cd3cbc02":"code","2e5bd41a":"code","a2d645b7":"code","2399eafc":"code","c17e15fb":"code","5948c6f3":"markdown","6859100d":"markdown","e85969b1":"markdown","00cd8270":"markdown","ece577b8":"markdown","7d1b3176":"markdown","8102e552":"markdown","341c542a":"markdown","ab3de73b":"markdown"},"source":{"2a9a29f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","094af7b8":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\norig_X = pd.read_csv('..\/input\/train.csv')\norig_X['Label'] = 'train'\n\norig_X_test = pd.read_csv('..\/input\/test.csv')\norig_X_test['Label'] = 'test'\n\n# Process as full dataset\norig_X.dropna(axis=0, subset=['Survived'], inplace=True) # Drop rows with uknown survival\nX_full = pd.concat([orig_X.drop('Survived', axis = 1), orig_X_test], axis = 0)\nX_full.drop('PassengerId', axis = 1, inplace=True)\n\n# Select categorical columns\nprint(\"Categorical features: \", [cname for cname in X_full.columns if X_full[cname].dtype == \"object\"])\n\n# Select numeric columns\nprint(\"Numeric features: \", [cname for cname in X_full.columns if X_full[cname].dtype in ['int64', 'float64']])\n\nX_full.head()","806803b4":"# Determine the number of missing values in each column of training data\nmissing_val_count_by_column = (X_full.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","899a0f3e":"X_full.dtypes","79998ca5":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Remove unuseful features\nX_full.drop('Name', axis=1, inplace=True)\nX_full.drop('Ticket', axis=1, inplace=True)\nX_full.drop('Cabin', axis=1, inplace=True)\n\n# Setup method for missing data using a median imputer for important numeric features\nnum_simple_imputer = SimpleImputer(strategy='median')\nnumeric_features = ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']\nnum_transformed = pd.DataFrame(num_simple_imputer.fit_transform(X_full[numeric_features]))\nnum_transformed.columns = numeric_features\n\n# Setup one hot enoding for catagorical features\ncat_simple_imputer = SimpleImputer(strategy='constant', fill_value='missing')\ncategorical_features = ['Embarked','Sex', 'Label']\ncat_transformed = pd.DataFrame(cat_simple_imputer.fit_transform(X_full[categorical_features]))\ncat_transformed.columns = categorical_features\nX_dummies = pd.get_dummies(cat_transformed, columns = categorical_features)\nX_full = pd.concat([num_transformed, X_dummies], axis = 1)\n\nprint(X_full.dtypes)\nprint(X_full.head())","f83f930b":"import seaborn as sns\ncorr = X_full.corr()\nsns.heatmap(corr, cmap = sns.color_palette(\"coolwarm\", 10))","3aa5e712":"# Split your data\nX = X_full[X_full['Label_train'] == 1].copy()\nX_test = X_full[X_full['Label_test'] == 1].copy()\n\n# Drop your labels\nX.drop('Label_train', axis=1, inplace=True)\nX.drop('Label_test', axis=1, inplace=True)\nX_test.drop('Label_test', axis=1, inplace=True)\nX_test.drop('Label_train', axis=1, inplace=True)\ny = orig_X.Survived","819243eb":"# Framework via: https:\/\/towardsdatascience.com\/feature-selection-correlation-and-p-value-da8921bfb3cf\nprint(\"Full column list: \", X.columns)\nimport statsmodels.formula.api as sm\ndef backwardElimination(x, Y, sl, columns):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(Y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n                    columns = np.delete(columns, j)\n\n    regressor_OLS.summary()\n    return x, columns\n\nSL = 0.05\ndata_modeled, selected_columns = backwardElimination(X.values, y.values, SL, X.columns)\n\n# Creating a Dataframe with the columns selected using the p-value and correlation\nnew_df = pd.DataFrame(data = data_modeled, columns = selected_columns)\n\nprint(\"Columns to remove: \", set(X.columns).symmetric_difference(new_df.columns))","cd3cbc02":"# Remove columns\nX.drop('Fare', axis=1, inplace=True)\nX.drop('Parch', axis=1, inplace=True)\nX_test.drop('Fare', axis=1, inplace=True)\nX_test.drop('Parch', axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3,\n                                                                random_state=0)","2e5bd41a":"# from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Initial model\nmodel = XGBClassifier(random_state = 18)\n\n# Set up GridSearchCV in order to determine the best parameters for a gradient boosting model\ngrid_param = {  \n    'n_estimators': [12, 25, 50, 75],\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'early_stopping_rounds': [3, 4, 5, 6]\n    }\n\ngd_sr = GridSearchCV(estimator = model, param_grid = grid_param, \n                     cv = 3, n_jobs = -1, verbose = 2)\n\ngd_sr.fit(X_train, y_train)  \nbest_parameters = gd_sr.best_params_\nprint(best_parameters)  ","a2d645b7":"# Create a final model fit to these parameters\nxgb = XGBClassifier(early_stopping_rounds = 3, eval_set = [(X_valid, y_valid)], learning_rate = 0.05, \n                    max_depth = 5, n_estimators = 50)\n\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_valid)\n\n# Get mean absolute error\nprint('MAE:', mean_absolute_error(y_valid, y_pred))","2399eafc":"# Form confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_valid, y_pred)","c17e15fb":"final_preds = xgb.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': orig_X_test.PassengerId,'Survived': final_preds})\noutput\noutput.to_csv('submission.csv', index=False)","5948c6f3":"## Import and Explore Data","6859100d":"## Use of GridSearchCV Search to Determine the Best Model Parameters","e85969b1":"## Examine Attribute Correlations","00cd8270":"It appears we are missing values from four columns. Two of them are categorical, two of them are numeric.","ece577b8":"Examining the column names, we will keep all but two: name and ticket number. These are additional unique identifiers to PassengerId.","7d1b3176":"## Create Submission","8102e552":"## Process the Data\nHere we will impute missing values in these columns and transform relevant categorical data through one-hot encoding.","341c542a":"Our approach is to concatenate train and test data for processing, then separate them later for modelling.","ab3de73b":"# Competition Description\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n## Practice Skills\n+ Binary classification\n+ Python and R basics"}}