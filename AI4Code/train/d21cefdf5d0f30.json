{"cell_type":{"f8318aa3":"code","9f93e06d":"code","8324ac38":"code","93936914":"code","43d0669b":"code","ab7a55f3":"code","ed1f1969":"code","af2407b9":"code","9a393699":"code","812f5e78":"code","f86a387a":"code","5847d5b4":"code","8460b922":"code","c1c893a1":"code","f34dffe5":"code","89357dc9":"code","bf6342ce":"code","53f900f4":"code","0fd5c63d":"code","b28e6f31":"code","fa657de4":"code","7623bc85":"code","d57bbe88":"code","2a56a6f6":"code","a65ae5ea":"code","1db71e7d":"code","e23b77c9":"code","2fa8824d":"code","11d7ab63":"code","51f5352b":"code","8fa1a0a9":"code","38e9060d":"code","58b60c9c":"code","9d78c831":"code","0a1901ce":"code","9436af67":"code","721131b7":"code","bed2845f":"code","d577e06a":"code","4e42e0ea":"code","b2ad7085":"code","61a9ccb1":"code","33bd7e5d":"code","60fcd792":"code","ed1e5ed5":"code","8789ad54":"code","4f2d6804":"code","2ce2aeac":"code","4166d513":"code","77231a92":"code","30e15109":"code","bd9a3362":"code","3ae4084a":"code","edd454a8":"code","ca4cf750":"code","4560fe5b":"code","29ee9f32":"code","c28d17df":"code","b44635e1":"code","edc626e4":"code","7dfc6dc7":"code","3176f1cb":"code","cbf87cd3":"code","feb1ba42":"code","e34c99a0":"code","7de62b24":"code","f6732d21":"markdown","776bade0":"markdown","cb663e6e":"markdown","944448e6":"markdown","f950b4dd":"markdown","bc381f16":"markdown","bd1437ad":"markdown","0486b59c":"markdown","7a6edfbc":"markdown","c4dddf89":"markdown","24a57394":"markdown","7fc1302f":"markdown","240b07ec":"markdown","45288ba2":"markdown","b4a647dc":"markdown","a4c580ae":"markdown","3d7f187a":"markdown","6bc1e34f":"markdown","48147f20":"markdown","0bd0d930":"markdown","0adbb173":"markdown","9914fb1f":"markdown","4ce9e70c":"markdown","70f29bee":"markdown","2e41b7a3":"markdown","f71618b9":"markdown","c4bbf4d3":"markdown","e8970868":"markdown","9b5a90a3":"markdown","5aa1d73c":"markdown","f33f4364":"markdown","98f3e369":"markdown","b5e80e90":"markdown","0c6669cc":"markdown","ebc4c502":"markdown","5ad23731":"markdown","509887aa":"markdown","4415e91d":"markdown","9247eaf7":"markdown","216489d3":"markdown"},"source":{"f8318aa3":"#All imports here.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib nbagg \n#Much larger, clear plots\nimport seaborn as sns\n\nfrom tqdm import tqdm\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","9f93e06d":"class_labels = pd.read_csv(\"..\/input\/trainLabels.csv\")  #This will remain same for both asm or byte file.","8324ac38":"#Statistical study of data.\n\ndist_of_class = class_labels[\"Class\"].value_counts()\ndist_of_class = dist_of_class.apply( lambda x: ((x\/sum(dist_of_class))*(100.0)) )\ndist_of_class","93936914":"total = len(class_labels)*1\n#print(total)\nax = sns.countplot(x = \"Class\", data = class_labels)\n\"\"\"\nDocumentation of sns.countplot here.\n\"\"\"\nfor p in ax.patches:\n    \"\"\"ax.patches gives a list of rectangle object. Each element represent a rectangle in above histogram.\n    Example: Rectangle(xy=(-0.4, 0), width=0.8, height=1541, angle=0)\"\"\"\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\n\n#put 11 ticks (therefore 10 steps), from 0 to the total number of rows in the dataframe\nax.yaxis.set_ticks(np.linspace(0, total, 11))\n#adjust the ticklabel to the desired format, without changing the position of the ticks. \nax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\nplt.show()","43d0669b":"byte_file_size = pd.read_csv(\"..\/input\/byte_file_size.csv\")\nbyte_file_size.head()","ab7a55f3":"ax = sns.boxplot(x = \"Class\", y=\"fsize\", data= byte_file_size)\nplt.title(\"boxplot of .byte file size\")","ed1f1969":"byte_final_data = pd.read_csv(\"..\/input\/byte_final_data.csv\")\nbyte_final_data.head()","af2407b9":"#Let's normalize the data.\ndef normalize(dataframe):\n    #print(\"Here\")\n    test = dataframe.copy()\n    for col in test.columns:\n        if(col != \"Id\" and col !=\"Class\"):\n            max_val = max(dataframe[col])\n            min_val = min(dataframe[col])\n            test[col] = (dataframe[col] - min_val) \/ (max_val-min_val)\n    return test","9a393699":"'''\ntest = byte_final_data\nAbove assignment will only make a new pointer of name test to byte_final_data dataframe object.\nAny change performed will bring changes to both. Hence it is neccessary to create a new copy of df.\nfor col in test.columns:\n    if (col != \"Id\" and col != \"Class\"):\n        test[col] = test[col]+2\n'''","812f5e78":"byte_result = normalize(byte_final_data)\nbyte_result.head()","f86a387a":"data_y = byte_result[\"Class\"]","5847d5b4":"#With perplexity 50.\nmodel = TSNE(perplexity = 50)\nbyte_tsne_data = model.fit_transform(byte_result.drop([\"Id\", \"Class\"], axis=1))","8460b922":"#Byte tsne data will return 2-dimesnions.\nx_ax = byte_tsne_data[:,0]\ny_ax = byte_tsne_data[:,1]\nplt.scatter(x_ax, y_ax, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9)) #cmap argument gives different color coded map.\nplt.colorbar(ticks=range(10))\nplt.clim(0.5, 9)\nplt.show()","c1c893a1":"#With perplexity 30.\nmodel = TSNE(perplexity = 30)\nbyte_tsne_data = model.fit_transform(byte_result.drop([\"Id\", \"Class\"], axis=1))","f34dffe5":"#Byte tsne data will return 2-dimesnions.\nx_ax = byte_tsne_data[:,0]\ny_ax = byte_tsne_data[:,1]\nplt.scatter(x_ax, y_ax, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9)) #cmap argument gives different color coded map.\nplt.colorbar(ticks=range(10))\nplt.clim(0.5, 9)\nplt.show()","89357dc9":"# split the data into test and train by maintaining same distribution of output varaible .[stratify=data_y]\nx_train,x_test,y_train,y_test = train_test_split(byte_result.drop([\"Id\", \"Class\"],axis=1),data_y,stratify=data_y,test_size=0.20)\n# split the data into train and cv by maintaining same distribution of output varaible. [stratify=y_train]\nx_train,x_cv,y_train,y_cv = train_test_split(x_train,y_train,stratify=y_train,test_size=0.20)","bf6342ce":"#Training dataframe.\nprint(\"The shape of training dataframe is \", x_train.shape)\n\n#Here we have y_train a series, and not a dataframe.\nclass_dist = y_train.value_counts()\nclass_dist = class_dist.apply(lambda x: (x\/sum(class_dist))*(100.0))\nclass_dist ","53f900f4":"#Training dataframe.\nprint(\"The shape of training dataframe is \", x_cv.shape)\n\n#Here we have y_train a series, and not a dataframe.\nclass_dist = y_cv.value_counts()\nclass_dist = class_dist.apply(lambda x: (x\/sum(class_dist))*(100.0))\nclass_dist","0fd5c63d":"#Training dataframe.\nprint(\"The shape of training dataframe is \", x_test.shape)\n\n#Here we have y_train a series, and not a dataframe.\nclass_dist = y_test.value_counts()\nclass_dist = class_dist.apply(lambda x: (x\/sum(class_dist))*(100.0))\nclass_dist","b28e6f31":"#sort_index method will sort the series according to index (class labels here).\ntrain_class_distribution = y_train.value_counts().sort_index()\ncv_class_distribution = y_cv.value_counts().sort_index()\ntest_class_distribution = y_test.value_counts().sort_index()\n\n","fa657de4":"train_class_distribution_list = list(train_class_distribution)\nfor i in range (0,9):\n    print(\"Number of points in class\", i+1,\"are\", train_class_distribution_list[i],'(',train_class_distribution_list[i]\/ sum(train_class_distribution_list) * (100.0), '%)')\n\nax = sns.barplot(train_class_distribution.index, train_class_distribution.values)\nax.set_title(\"Distribution of y in train data\")\nax.set_ylabel(\"Data points count\")\nax.set_xlabel(\"Class\")","7623bc85":"cv_class_distribution_list = list(cv_class_distribution)\nfor i in range (0,9):\n    print(\"Number of points in class\", i+1,\"are\", cv_class_distribution_list[i],'(',cv_class_distribution_list[i]\/ sum(cv_class_distribution_list) * (100.0), '%)')\n\nax = sns.barplot(cv_class_distribution.index, cv_class_distribution.values)\nax.set_title(\"Distribution of y in cv data\")\nax.set_ylabel(\"Data points count\")\nax.set_xlabel(\"Class\")","d57bbe88":"test_class_distribution_list = list(test_class_distribution)\nfor i in range (0,9):\n    print(\"Number of points in class\", i+1,\"are\", test_class_distribution_list[i],'(',test_class_distribution_list[i]\/ sum(test_class_distribution_list) * (100.0), '%)')\n\nax = sns.barplot(test_class_distribution.index, test_class_distribution.values)\nax.set_title(\"Distribution of y in test data\")\nax.set_ylabel(\"Data points count\")\nax.set_xlabel(\"Class\")","2a56a6f6":"def plot_confusion_matrix(y_test, predicted_y):\n    '''y_test is our actual labels.\n    predicted_y gives labels predicted by our model.\n    In matrix each column heading is actual label . Each row is predicted label. '''\n    \n    '''Confusion Matrix'''\n    C = confusion_matrix(y_test, predicted_y)\n    print(\"Percentage of misclassified points by model :\" , ((len(y_test) - np.trace(C))\/(len(y_test)))*100 )\n    #C will be a 9*9 matrix whoce each cell (i,j) will represent number of observations known to be in group i but predicted \n    #to be in group j. (This notation is opposite of wikipedia entry, or what we have learnned.)\n    print(C)\n    print(\"\\n\")\n    labels = [1,2,3,4,5,6,7,8,9]\n    cmap = sns.light_palette(\"green\")\n    #Representing Confusion matrix in heatmap format.\n    print(\"-\"*50, \"confusion matrix\", \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    \n    '''Precision Matrix\n    TP\/(TP+FP) How precise we are in our result.\n    Out of all the points which were predicted to be of class 1(TP+FP), how many were actually of class 1(TP). \n    Hence divide each column element from the sum of their respective column.\n    '''\n    A = (C\/C.sum(axis=0))\n    print(\"-\"*50, \"precision matrix\", \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of columns in precision matrix\",A.sum(axis=0))\n    \n    '''Recall Matrix\n    TP\/(FN+TP) How much were we able to remember from original dataframe.\n    Out of all the points which were actually of class 1(TP+FN), how many are detected correctly from our model(TP)\n    Divide each row element from the sum of their respeective rows.\n    By default numpy divide column-wise.'''\n    B = ((C.T\/ C.sum(axis=1)).T)\n    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of columns in precision matrix\",B.sum(axis=1))\n\n    \ndef perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv):\n    cv_log_error_array = []\n    for i in tqdm(list_of_hyperparam):\n        if (model_name == \"knn\"):\n            model =  KNeighborsClassifier(n_neighbors = i)\n            #model.fit(x_train, y_train)\n        elif(model_name == \"lr\"):\n            model = LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n            #model.fit(x_train, y_train)\n        elif(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1, class_weight='balanced')\n            #model.fit(x_train, y_train)\n        elif(model_name == \"xgbc\"):\n            #weights = []\n            #a = y_train.value_counts().sort_index()\n            #sum_a = a.sum()\n            #for i in a:\n            #    weights.append(sum_a\/i)\n            model = XGBClassifier(n_estimators=i,nthread=-1)\n            #model.fit(x_train, y_train, sample_weight = weights)\n        model.fit(x_train, y_train)\n        caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n        caliberated_model.fit(x_train, y_train)\n        predict_y = caliberated_model.predict_proba(x_cv)\n        cv_log_error_array.append(log_loss(y_cv, predict_y))\n    for i in range(len(cv_log_error_array)):\n        print ('log_loss for hyper_parameter = ',list_of_hyperparam[i],'is',cv_log_error_array[i])\n    return cv_log_error_array\n       \ndef get_best_hyperparam(list_of_hyperparam, cv_log_error_array):\n    index = np.argmin(cv_log_error_array)\n    best_hyperparameter = list_of_hyperparam[index]\n    return best_hyperparameter\n\n\ndef perform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test):\n    \n    if (model_name == \"knn\"):\n            model =  KNeighborsClassifier(n_neighbors = best_hyperparameter)\n    elif(model_name == \"lr\"):\n            model = LogisticRegression(penalty = 'l2',C = best_hyperparameter,class_weight = 'balanced')\n    elif(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators = best_hyperparameter,random_state = 42,n_jobs = -1,  class_weight='balanced')\n    elif(model_name == \"xgbc\"):\n            model = XGBClassifier(n_estimators = best_hyperparameter,nthread=-1)\n            \n    model.fit(x_train, y_train)\n    \n    caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n    caliberated_model.fit(x_train, y_train)\n\n    predicted_y = caliberated_model.predict_proba(x_train)\n    print(\"The training log-loss for best hyperparameter is\", log_loss(y_train, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_cv)\n    print(\"The cv log-loss for best hyperparameter is\", log_loss(y_cv, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_test)\n    print(\"The test log-loss for best hyperparameter is\", log_loss(y_test, predicted_y))\n\n    predicted_y = caliberated_model.predict(x_test)\n    plot_confusion_matrix(y_test, predicted_y)\n    \n\ndef plot_cv_error(list_of_hyperparam, cv_log_error_array):\n    fig, ax = plt.subplots()\n    ax.plot(list_of_hyperparam, cv_log_error_array,c='g')\n    for i, txt in enumerate(np.round(cv_log_error_array,3)):\n        ax.annotate((list_of_hyperparam[i],np.round(txt,3)), (list_of_hyperparam[i],cv_log_error_array[i]))\n    plt.grid()\n    plt.title(\"Cross Validation Error for each hyperparameter\")\n    plt.xlabel(\"Hyperparameter\")\n    plt.ylabel(\"Error measure\")\n    plt.show()\n","a65ae5ea":"#Key idea here is we will generate random values for output.\n#One way to generate random value is np.random.rand(1,9) .\n#np.random.rand(1,9) -> array([[0.37220974, 0.22175184, 0.87718086, 0.85841599, 0.4761965 ,\\\n#                              0.65296651, 0.14017462, 0.04881395, 0.04505459]])\n#sum(np.random.rand(1,9)) -> array([0.37220974, 0.22175184, 0.87718086, 0.85841599, 0.4761965 ,\\\n#                              0.65296651, 0.14017462, 0.04881395, 0.04505459]) Performs colummn wise sum.\n#sum(sum(np.random.rand(1,9))) -> 3.6927646 finally a single integer.\n\n#Hence key idea is that as output we will simply generate random values for every class.\ncv_predicted_y = np.zeros((x_cv.shape[0],9))#For each data point in x_cv we will save the probability of each class label.\ntest_predicted_y = np.zeros((x_test.shape[0],9))#Similarly for each point in x_test.","1db71e7d":"#We need to generate 9 numbers and the sum of numbers should be 1.\n#One solution is to genarate 9 numbers and divide each of the numbers by their sum.\nfor i in range(x_cv.shape[0]):\n    random_probs = np.random.rand(1,9) #Explained above.\n    cv_predicted_y[i] = (random_probs\/sum(sum(random_probs)))[0] #random_probs\/sum(sum(random_probs)) will return 2D array.\nprint(\"Log loss on Cross Validation data is \", log_loss(y_cv, cv_predicted_y))\n\nfor i in range(x_cv.shape[0]):\n    random_probs = np.random.rand(1,9) #Explained above.\n    test_predicted_y[i] = (random_probs\/sum(sum(random_probs)))[0] #random_probs\/sum(sum(random_probs)) will return 2D array.\nprint(\"Log loss on Cross Validation data is \", log_loss(y_test, test_predicted_y))\n\n#Plot confusion matrix.\npredicted_y = np.argmax(test_predicted_y, axis=1)\n#It returns an array with indices of maximum value along axis. Here row-wise.\nplot_confusion_matrix(y_test, predicted_y+1)","e23b77c9":"list_of_hyperparam = [x for x in range(1,15,2)]\nmodel_name = \"knn\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","2fa8824d":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","11d7ab63":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","51f5352b":"list_of_hyperparam = [10 ** x for x in range(-5, 4)]\nmodel_name = \"lr\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","8fa1a0a9":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","38e9060d":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","58b60c9c":"list_of_hyperparam = [10,50,100,500,1000,2000,3000]\nmodel_name = \"rf\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","9d78c831":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","0a1901ce":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","9436af67":"list_of_hyperparam = [10,50,100,500,1000,2000]\nmodel_name = \"xgbc\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","721131b7":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","bed2845f":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","d577e06a":"#Let's load our final dataframe where sizes and features are merge.\nasm_final_data = pd.read_csv(\"..\/input\/asm_final_data.csv\")\nasm_final_data.head()","4e42e0ea":"#Let's check columns which have absolutely no value.\nsum_of_cols = asm_final_data.sum(axis = 0)\n(sum_of_cols) ","b2ad7085":"# These columns -> .BSS, .CODE, rtn have absolutely no value drop them.\n#Run it only once\nasm_n = asm_final_data.drop(columns = ['.BSS:', '.CODE', 'rtn'], axis=1)","61a9ccb1":"asm_n.shape","33bd7e5d":"asm_result = normalize(asm_n)\nasm_result.head()","60fcd792":"asm_y = asm_result['Class']\nasm_x = asm_result.drop(['Id','Class'], axis=1)","ed1e5ed5":"x_train_asm, x_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,stratify=asm_y,test_size=0.20)\nx_train_asm, x_cv_asm, y_train_asm, y_cv_asm = train_test_split(x_train_asm, y_train_asm,stratify=y_train_asm,test_size=0.20)","8789ad54":"list_of_hyperparam = [x for x in range(1,21,2)]\nmodel_name = \"knn\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train_asm, y_train_asm, x_cv_asm, y_cv_asm)","4f2d6804":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train_asm,y_train_asm,x_cv_asm,y_cv_asm,x_test_asm,y_test_asm)","2ce2aeac":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","4166d513":"list_of_hyperparam = [10 ** x for x in range(-5, 4)]\nmodel_name = \"lr\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train_asm, y_train_asm, x_cv_asm, y_cv_asm)","77231a92":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train_asm,y_train_asm,x_cv_asm,y_cv_asm,x_test_asm,y_test_asm)","30e15109":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","bd9a3362":"list_of_hyperparam = [10,50,100,500,1000,2000,3000]\nmodel_name = \"rf\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train_asm, y_train_asm, x_cv_asm, y_cv_asm)","3ae4084a":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train_asm,y_train_asm,x_cv_asm,y_cv_asm,x_test_asm,y_test_asm)","edd454a8":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","ca4cf750":"list_of_hyperparam = [10,50,100,500,1000,2000]\nmodel_name = \"xgbc\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train_asm, y_train_asm, x_cv_asm, y_cv_asm)","4560fe5b":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train_asm,y_train_asm,x_cv_asm,y_cv_asm,x_test_asm,y_test_asm)","29ee9f32":"plot_cv_error(list_of_hyperparam,cv_log_error_array)","c28d17df":"asm_result.head()","b44635e1":"byte_result.head()","edc626e4":"result_y = byte_result[\"Class\"] #This will containg classes of all the data files.","7dfc6dc7":"final_byte_result = byte_result.drop([\"Class\"], axis = 1)","3176f1cb":"final_asm_result = asm_result.drop([\"Class\"], axis = 1)","cbf87cd3":"result_x = pd.merge(final_byte_result, final_asm_result, on= \"Id\")\nresult_x.head()","feb1ba42":"result_x = result_x.drop([\"Id\"], axis=1)","e34c99a0":"#result_x and result_y will finally be used . They are already normalized.","7de62b24":"x_train, x_test_merge, y_train, y_test_merge = train_test_split(result_x, result_y,stratify=result_y,test_size=0.20)\nx_train_merge, x_cv_merge, y_train_merge, y_cv_merge = train_test_split(x_train, y_train,stratify=y_train,test_size=0.20)","f6732d21":"**Finally in byte_result dataframe we have noralize data of all the features extracted and size of byte files**","776bade0":"### 2.5.1. K-Nearest-Neighbor Classifier","cb663e6e":"**Using boxplot we can see that file size overlapp too little, and hence can be used as an important feature for modelling.**","944448e6":"## 1.6. Machine Learning Models","f950b4dd":"### 2.5.3. Random Forest","bc381f16":"## 1.4. Byte file multivariate analysis using TSNE","bd1437ad":"### 2.5.2. Logistic Regression","0486b59c":"## 2.2. asm files size","7a6edfbc":"### 3.3.2. XGBoost Classifier","c4dddf89":"## 2.5. Machine Learning Models","24a57394":"**Script used to extract byte file sizes and storing it in csv is:-**\n\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef calculate_byte_file_size(class_labels):\n    #It calculate size of each file in MB.\n\n     fname = [] #Stores the name of each file.\n     fsize = [] #Stores the size of each file respectively.\n     flabels = [] #Stores respective labels of malware\n\n     for file in tqdm(os.listdir(\".\/byte_files\")):\n         file_name = file.split('.')[0]\n         fname.append(file_name)\n         size_in_mb = (os.stat(\".\/byte_files\/\"+file).st_size)\/(1024.0*1024.0)\n         fsize.append(size_in_mb)\n         flabels.append(int(class_labels[class_labels[\"Id\"] == file_name][\"Class\"]))\n \n     file_size_df = pd.DataFrame({\"Class\":flabels, \"Id\": fname, \"fsize\": fsize})\n \n     if not os.path.exists(\".\/byte_file_size.csv\"):\n         file_size_df.to_csv(\".\/byte_file_size.csv\", index = False)\n \n     #print(file_size_df.head())\n\n     #print(\"size calculated and stored in csv\")\n\n#Call the function.\nclass_labels = pd.read_csv(\".\/trainLabels.csv\")\ncalculate_byte_file_size(class_labels)\n","7fc1302f":"### 1.6.1. Random Model","240b07ec":"# 3. Classification on asm and byte files.","45288ba2":"**Observation**","b4a647dc":"### 1.6.2. K-Nearest-Neighbour Classifier","a4c580ae":"## 3.2. Train Test Split","3d7f187a":"**Observation**","6bc1e34f":"### 1.6.4. Random Forest","48147f20":"### 1.6.3. Logistic Regression","0bd0d930":"## 1.5. Train Test Split","0adbb173":"**Observation**","9914fb1f":"## 2.3.Univariate and Multivariate eanalysis","4ce9e70c":"**Finally I have merged both feature file and size file into a common dataframe and stored it in a single csv file.**","70f29bee":"<li>Log Loss can be anywhere between [0, inf).<\/li>\n<li>Here our base model, which is giving log-loss in worst condition, i.e on randomly allocating probabilities is giving 2.47 loss.<\/li>\n<li>Hence we will try to keep our loss between [0, 2.47) .<\/li>","2e41b7a3":"**We can clearly see malware of class 3 are dominating. And also, malware of class 5 are significantly very less.**","f71618b9":"### 3.3.1 Random Forest Classifier","c4bbf4d3":"# 1. Classification on byte files  ","e8970868":"## 1.1. Distribution of Malware Classes\n**And it will be same for asm, byte section.**","9b5a90a3":"## 1.3. Byte files feature extraction","5aa1d73c":"## 3.3. Machine Learning Models","f33f4364":"### 2.5.4. XGBoost Classifier","98f3e369":"## 2.1. Feature extraction on asm files.","b5e80e90":"## 1.2. Byte files size","0c6669cc":"# 2. Classification on asm files","ebc4c502":"### 1.6.5. XGBoost Classifier","5ad23731":"## 3.1. Multivariate analysis","509887aa":"**All Scripts and generic functions here.**<br>\n**And they will be used in bytes, asm and combination of these files too.**","4415e91d":"**Script used for extracting feature is:-**\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\nbyte_feature_file = open(\".\/byte_fe_results.csv\", \"w+\") # w+ idicates it will create a file if it does not exist in library.\n\nchar_list = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f']\nfinal_string = \"Id,\"\nfor i in char_list:\n    for j in char_list:\n        concat = \"\"\n        concat = concat.join((i,j))\n        final_string = final_string + concat + \",\"\n\nfinal_string = final_string+\"??\"\nbyte_feature_file.write(final_string)\nbyte_feature_file.write(\"\\n\")\n\nfiles = os.listdir(\".\/byte_files\")\nfeature_matrix = np.zeros((len(files), 257), dtype = int)\n\nk=0 #Denotes each row. Rows will be total datapoints,here files.\nfor file in tqdm(files):\n    f_name = file.split(\".\")[0]\n    byte_feature_file.write(f_name+\",\") #This goes into byte_fe_results.csv file.\n    with open(\".\/byte_files\/\"+file, \"r\") as each_file:\n        for line in each_file:\n            line = line.rstrip().split(\" \") #At the end of each line there is a new space.\n            line = line[1:] #Ignored the addresses.\n            for hex_word in line:\n                if hex_word == \"??\":\n                    feature_matrix[k][256] += 1\n                else:\n                    feature_matrix[k][int(hex_word, 16)] += 1 #int(hex_word, 16) will return decimal equivalent.\n    string_for_each_file = \"\"\n    for i in feature_matrix[k]:\n        string_for_each_file = string_for_each_file + str(i) + \",\"\n    string_for_each_file = string_for_each_file[:-1]\n    byte_feature_file.write(string_for_each_file)\n    byte_feature_file.write(\"\\n\")\n    each_file.close()\n    k += 1\n\nbyte_feature_file.close()\nprint(\"The code has executed----------------------------------------------------------------------------->>>>>>>>>>>>\")","9247eaf7":"**Let's check for the distribution of labels in each splitted dataframe.**","216489d3":"## 2.4. Train Test Split"}}