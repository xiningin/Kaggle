{"cell_type":{"100f7d48":"code","799566d2":"markdown","db00851f":"markdown","b66a100e":"markdown","a9e351f9":"markdown","b1cdac68":"markdown","f02a4234":"markdown","4ecc4358":"markdown","f98fbd43":"markdown","b0c4abac":"markdown","e0dd15ce":"markdown","3c910375":"markdown"},"source":{"100f7d48":"from IPython.display import Image\nImage(\"..\/input\/tensor-diagram\/tensor-diagram.jpg\")","799566d2":"<a id=\"8\"><\/a> <br>\n\n### Trace\n\nThe trace of an operator is given by connecting the input legs to the corresponding output legs as follow\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_10.PNG\" alt=\"drawing\" width=\"700\"\/><\/center>","db00851f":"**Exemple 3** :\nOne last example (a bit more complex). \nLet $T$ be a $2N$-order tensor with indices $s_1,\\cdots,s_N$ and $r_1,\\cdots,r_N$ emerging from the contraction along $\\alpha$ and $\\beta$ indices of all the $G$ tensors (for simplification purpose all tensors have the same name $G$ but it should be $G_1,\\cdots,G_N$).\n\n\n<br>\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_3.PNG\" alt=\"drawing\" width=\"650\"\/><\/center>","b66a100e":"<a id=\"10\"><\/a> <br>\n\n---\n\n# Conclusion\n\n\nTensor diagrams and Einstein notations are very helpful when you are trying to picture contractions of elaborated tensors. Using a sheet of paper, you can understand in no time how to perform your calculation.\n\nThough tensor diagrams are not exactly theorised and depending on the author things can change a bit but it won't be much of a problem to adapt. \n\n(In **quantum computing**, a quantum circuit is nothing more than a complex tensor diagram.) \n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_12.PNG\" alt=\"drawing\" width=\"600\"\/><\/center>\n\nStay tuned for further notebooks on tensor diagrams and **quantum machine learning**!","a9e351f9":"<center><h1>Tensor Diagram & Einstein Notation<\/h1><\/center>\n\n# Table of contents\n- [Introduction](#1)\n- [Einsum notation](#2)\n- [(Penrose) Tensor Diagram](#3)\n    - [Basic diagram](#4)\n    - [Contraction of tensors](#5)\n    - [Juxtaposition of tensors](#6)\n    - [Transposition](#7)\n    - [Trace](#8)\n    - [Example : Singular Value Decomposition](#9)\n- [Conclusion](#10)\n\n<a id=\"1\"><\/a> <br>\n---\n\n# Introduction\n\nWhen dealing with complex tensors, things can get messy fast as to mathematical notations. Imagine you're somehow unfamiliar with covariants, contravariants tensor and tensor contractions: you are trying to learn new algorithms (especially in the field of quantum computing) and you stumble across a website or paper bombarding you with awful equations. I think that you would give up on the explanation quite fast.\n\nIn this notebook, I present a basic introduction to Einstein notation and tensor diagram that will help you grasp the concept of tensor contraction that you can see in your everyday life as a data scientist (think about a batch of images: that's a 3-order tensor).\n\n**Remark**: we won't focus too much on the covariant\/contravariant aspect of tensors thus this won't be completely rigorous as I will sometimes adopt the up\/down indices to symbolize input and output indices of operators.\n\n**Remark**: All images belong to me.\n\n\nAt the end of this short course, you will be able to understand what is going on in a picture like the following. You will be able to understand Einstein notations that can come in handy when creating your custom layers in Tensorflow or Pytorch.\n<!-- attachment:fea963d9-847a-425d-8a1c-3d431f89fc54.png -->\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_0.PNG\" alt=\"drawing\" width=\"800\"\/><\/center>\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_01.PNG\" alt=\"drawing\" width=\"850\"\/><\/center>\n\n<a id=\"2\"><\/a> <br>\n---\n\n# Einsum notation\n\nEinstein notation is a  notational convention aiming to shorten the length of equations when dealing with tensors. The basic concept is to remove the summation symbol ($\\sum$): when two indices are the same in an expression, it implies a summation. Another point is to express a tensor only by its element for any indices (i.e $v_j$ for a tensor, $M_ij$ for a matrix...).\n\nLet's jump to examples:\n\n**Example 1**:\n\nLet $A \\in \\mathcal{M}_{n \\times p}(\\mathbb{R})$, we know that you can write the matrix $A$ as the set of its elements $(A_{ij})_{i \\in \\{1,\\cdots,n\\}, j \\in \\{1,\\dots,p\\}}$.\nUsing Einstein notation, when refering to the matrix $A$, you just write $A_{ij}$ (assuming it is for any $i \\in \\{1,\\cdots,n\\}$ and $j \\in \\{1,\\dots,p\\}$)\n$$A \\longrightarrow A_{ij}$$\n\nFor a vector $v = (v_i) \\in \\mathbb{R}^n$, the same way as before we represent this vector as $v_i$.\n\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_1.jpg\" alt=\"drawing\" width=\"650\"\/><\/center>\n\n\n","b1cdac68":"<a id=\"9\"><\/a> <br>\n\n### Example : Singular Value Decomposition\n\n\nA diagram that you will often come across is the SVD diagram. It has nothing more exotic than the other diagrams we've just seen but it is very important.\nLet $A$ be a matrix one can show that $A$ can be decomposed as $A=USV^*$ with $U$ and $V$ orthogonal matrices and $S$ a rectangular diagonal matrix with non-negative real numbers on the diagonal corresponding to the singular value of the matrix $A$.\n\nA diagram for this operation can be :\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_11.PNG\" alt=\"drawing\" width=\"700\"\/><\/center>","f02a4234":"<a id=\"7\"><\/a> <br>\n\n### Transposition\n\n\nOne can represent a transposition simply by inverting the legs.\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_9.PNG\" alt=\"drawing\" width=\"700\"\/><\/center>\n","4ecc4358":"<a id=\"4\"><\/a> <br>\n\n### Basic diagram\n\nThe basic elements of linear algebra (vectors, matrix...) can be represented as below :\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_4.PNG\" alt=\"drawing\" width=\"650\"\/><\/center>","f98fbd43":"<a id=\"3\"><\/a> <br>\n\n---\n\n# (Penrose) Tensor Diagram\n\nThe last example of the previous paragraph may seem a bit abstract don't you think! In this section, we will introduce \"tensor diagrams\" which aim at visualizing easily tensor contractions. In the rest of this notebook, we will be using Einstein notation.\n\nA tensor corresponds to a (labelled) shape such as a circle, rectangle, diamond with one or more output legs. Those legs represent the indices of the tensor. For instance, a vector that only has one index will be represented as a circle with one leg (potentially labelled with the name of the index). A matrix has two indices (rows and columns) and thus will be a circle with two output legs. For $N$-order tensor, you follow just the same pattern as before.\n\n**Remark** : In books\/papers using these tensor diagrams, authors tend to represent the covariant tensors with indices below and contravariant with indices above i.e $w_i = A_i^j v_j$ with the example above.","b0c4abac":"<a id=\"5\"><\/a> <br>\n\n### Contraction of tensors\n\nHow to represent a matrix multiplication using those diagrams? And what about the $2N$-order tensor described hereinabove?\n\nThe answer is simple : connecting two tensor legs with a wire implies that the corresponding indices are contracted. \n\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_5.PNG\" alt=\"drawing\" width=\"800\"\/><\/center>\n\nLet's dive into the diagram of the $2N$-order tensor!\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_6.PNG\" alt=\"drawing\" width=\"800\"\/><\/center>\n\n","e0dd15ce":"\n**Example 2**:\n\nLet's remove those summation symbols!\nLet $x$ and $y$ be two vectors in (let's say) $\\mathbb{R}^n$, if we want to compute the inner product (scalar product) of $x$ and $y$, we would write  \n$$\\langle x | y \\rangle = x \\cdot y = x^\\intercal y = \\sum_{i = 1}^n x_i y_i$$\nWe can see that we perform a summation over the index $i$ (and that it is present in both $x$ and $y$ inside the sum. Hence, we can delete this sum (as it is implied that we are summing along the $i$ axis) :\n\n$$\\langle x | y \\rangle = x^\\intercal y = x \\cdot y = x_i y_i$$\n\nThe same way given two matrix $A \\in \\mathcal{M}_{n \\times p}(\\mathbb{R})$ and $B \\in \\mathcal{M}_{p \\times q}(\\mathbb{R})$, the product $C=AB=\\sum_{j=1}^p A_{ij}B_{jk}$ can be written in Einstein notation as \n$$C_{ik} = A_{ij} B_{jk}$$\n\n<br>\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_2.jpg\" alt=\"drawing\" width=\"650\"\/><\/center>\n<br>\n","3c910375":"<a id=\"6\"><\/a> <br>\n\n### Juxtaposition of tensors\n\n\nWhen two or more tensors are drawn disconnected to each other in the same diagram, it means that they are multiplied together with a tensor product.\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_7.PNG\" alt=\"drawing\" width=\"800\"\/><\/center>\n\n<br>\n<br>\nAnother diagram\n<center><img src=\"https:\/\/raw.githubusercontent.com\/antoine311200\/Host-Utils-Data\/master\/tensor_8.PNG\" alt=\"drawing\" width=\"800\"\/><\/center>"}}