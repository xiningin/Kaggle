{"cell_type":{"29ff31f6":"code","0362b80d":"code","518702a0":"code","4746b1b8":"code","fe5b236c":"code","e53f0168":"code","32aaeb62":"code","3966c888":"code","e8005dcc":"code","02fee0d8":"code","01feeeb8":"code","6b06794a":"code","cc438222":"code","938a0c0f":"code","52daf540":"code","9dc5a69b":"code","f7ade8c5":"code","f0a6ba6c":"code","11b75a74":"code","fcb2a335":"code","11218b02":"code","fadceff5":"code","31c2ccc9":"markdown","04e6e608":"markdown","f80533f7":"markdown","ebcc1eb0":"markdown","f058aa3e":"markdown","8342c94b":"markdown","e634410d":"markdown","9053644c":"markdown","548f5287":"markdown","08b3a982":"markdown"},"source":{"29ff31f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0362b80d":"import torch\nimport torchvision\nfrom torchvision import transforms, datasets\nimport matplotlib.pyplot as plt","518702a0":"train_data_path = '..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/'\ntest_data_path = '..\/input\/asl-alphabet\/asl_alphabet_test\/'","4746b1b8":"test_size = 0.2\nbatch_size = 32\nnum_epoch = 10\nlearning_rate = 0.001\nnum_classes = 29","fe5b236c":"train_transforms = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor()\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(224), \n    transforms.ToTensor()\n])","e53f0168":"%%time\ntrain_dataset = datasets.ImageFolder(train_data_path, transform=train_transforms)\nnum_train_samples = len(train_dataset)\ntrain_dataset","32aaeb62":"%%time\ntest_dataset = datasets.ImageFolder(train_data_path, transform=test_transforms)\ntest_dataset","3966c888":"torch.manual_seed(1)\nindices = torch.randperm(num_train_samples)\n\nsplit = int(num_train_samples * test_size)\n\ntrain_dataset = torch.utils.data.Subset(train_dataset, indices[split:])\ntest_dataset = torch.utils.data.Subset(test_dataset, indices[:split])\n\nlen(train_dataset), len(test_dataset)","e8005dcc":"train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                               batch_size=batch_size, \n                                               shuffle=True, \n                                               num_workers=4)\n\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                              batch_size=batch_size,\n                                              shuffle=False,\n                                              num_workers=4)","02fee0d8":"classes = train_dataloader.dataset.dataset.classes","01feeeb8":"for img, label in train_dataloader:\n    print('Ground truth', classes[label[0]])\n    plt.imshow(img[0].permute(1, 2, 0))\n    plt.show()\n    break","6b06794a":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","cc438222":"model = torchvision.models.resnet50(pretrained=True)\nmodel","938a0c0f":"for param in model.parameters():\n    param.requires_grad = False","52daf540":"in_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features, num_classes)\nmodel","9dc5a69b":"!pip install torchsummary","f7ade8c5":"from torchsummary import summary\nmodel.to(device)\nsummary(model, (3, 224, 224), batch_size=2)","f0a6ba6c":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","11b75a74":"from tqdm import tqdm\nfrom time import time\n\n\nprint_every = 1000\nsteps = 0\ntrain_losses, val_losses = [], []\n\nmodel.to(device)\nfor epoch in tqdm(range(num_epoch)):\n    iterations = 0\n    running_loss = 0\n    correct_train = 0\n    total_train = 0\n    start_time = time()\n    \n    model.train()\n    for i, (images, labels) in enumerate(train_dataloader):\n        steps += 1\n        images = images.to(device)\n        labels = labels.to(device)\n            \n        # Forward pass\n        output = model(images)\n        loss = criterion(output, labels)\n    \n        correct_train += (torch.max(output, dim=1)[1] == labels).sum()\n        total_train += labels.size(0)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        iterations += 1\n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            print(f'Epoch [{epoch + 1}]\/[{num_epoch}]. Batch [{i + 1}]\/[{len(train_dataloader)}]. Train loss {running_loss \/ steps}.', end=' ')\n            print(f'Train acc {correct_train \/ total_train * 100}.', end=' ')\n            with torch.no_grad():\n                model.eval()\n                correct_val, total_val = 0, 0\n                val_loss = 0\n                for images, labels in test_dataloader:\n                    images = images.to(device)\n                    labels = labels.to(device)\n                    output = model(images)\n                    loss = criterion(output, labels)\n                    val_loss += loss.item()\n\n                    correct_val += (torch.max(output, dim=1)[1] == labels).sum()\n                    total_val += labels.size(0)\n\n            print(f'Val loss {val_loss \/ len(test_dataloader)}. Val acc {correct_val \/ total_val * 100}')\n\n            train_losses.append(running_loss \/ total_train)\n            val_losses.append(val_loss \/ total_val)\n        \n        \n    print(f'Epoch took {time() - start_time}') \n    torch.save(model, f'checkpoint_{correct_val \/ total_val * 100:.2f}')","fcb2a335":"plt.plot(train_losses, label='Training loss')\nplt.plot(val_losses, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","11218b02":"from pathlib import Path\nfrom PIL import Image\n\n\ntest_data_path = Path('..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\/')\n\n\nclass ASLTestDataset(torch.utils.data.Dataset):\n    def __init__(self, root_path, transforms=None):\n        super().__init__()\n        \n        self.transforms = transforms\n        self.imgs = sorted(list(Path(root_path).glob('*.jpg')))\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        img = Image.open(img_path).convert('RGB')\n        \n        label = img_path.parts[-1].split('_')[0]\n        if self.transforms:\n            img = self.transforms(img)\n        \n        return img, label","fadceff5":"test_dataset = ASLTestDataset(test_data_path, transforms=test_transforms)\n\ncolumns = 7\nrow = round(len(test_dataset) \/ columns)\n\nfig, ax = plt.subplots(row, columns, figsize=(columns * row, row * columns))\nplt.subplots_adjust(wspace=0.1, hspace=0.2)\n\ni, j = 0, 0\nfor img, label in test_dataset:\n    img = torch.Tensor(img)\n    img = img.to(device)\n    model.eval()\n    prediction = model(img[None])\n\n    ax[i][j].imshow(img.cpu().permute(1, 2, 0))\n    ax[i][j].set_title(f'GT {label}. Pred {classes[torch.max(prediction, dim=1)[1]]}')\n    ax[i][j].axis('off')\n    j += 1\n    if j == columns:\n        j = 0\n        i += 1\n        \nplt.show()","31c2ccc9":" And create the loss function and an optimizer.","04e6e608":" Re-define the final fully-connected layer.","f80533f7":"Inference","ebcc1eb0":"Next we\u2019ll define the train and test dataset loader, using the Subset for the split:","f058aa3e":"Define transformations for train and test datasets.","8342c94b":"We have to freeze the pre-trained layers, so we don\u2019t backprop through them during training.","e634410d":"Train the model.","9053644c":"Load datasets and then we'll split it into a training and test set","548f5287":"Display results for all test images.","08b3a982":"We can check how our data loader is working:"}}