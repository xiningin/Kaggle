{"cell_type":{"bc24f092":"code","888c48e1":"code","b431c8e5":"code","29b4eb60":"code","d2c6b2b8":"code","3401d3a1":"code","acb143aa":"code","58a6889b":"code","00ecfbfa":"code","a3348f49":"code","49274d4c":"code","b9327ce0":"code","786cd9b1":"code","56d096e4":"code","a5c8e7ea":"code","eacdc727":"code","5f5c2622":"code","77f6613c":"code","66ce3a33":"code","e7ecc483":"code","0ab87f67":"code","af9ef8f2":"markdown","69a00e24":"markdown","71ab48d0":"markdown","c30ac7fc":"markdown","8ad7d39f":"markdown","f2c5dd4c":"markdown","a021d8df":"markdown","71056f8a":"markdown","663b9a2a":"markdown","f81e649d":"markdown","1b854a3e":"markdown","79fc3201":"markdown","a19a827e":"markdown","23abf335":"markdown","a9e84d79":"markdown","2def3263":"markdown","b3de9c7a":"markdown","f1ed3a07":"markdown","2c4548b2":"markdown","0a217eeb":"markdown","27823a15":"markdown","abf2c816":"markdown","086826cd":"markdown","64587688":"markdown","1cfe7f1e":"markdown","d6a4aad7":"markdown","6d67f914":"markdown","00db7f38":"markdown"},"source":{"bc24f092":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","888c48e1":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","b431c8e5":"sns.catplot(x=\"SibSp\", col = 'Survived', data=train, kind = 'count', palette='pastel')\nsns.catplot(x=\"Parch\", col = 'Survived', data=train, kind = 'count', palette='pastel')\nplt.show()","29b4eb60":"def is_alone(x):\n    if  (x['SibSp'] + x['Parch'])  > 0:\n        return 1\n    else:\n        return 0\n\ntrain['Is_alone'] = train.apply(is_alone, axis = 1)\ntest['Is_alone'] = test.apply(is_alone, axis = 1)\n\ng = sns.catplot(x=\"Is_alone\", col = 'Survived', data=train, kind = 'count', palette='deep')","d2c6b2b8":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","3401d3a1":"f, axes = plt.subplots(2, 1, figsize = (10, 6))\n\ng1 = sns.distplot(train[\"Fare\"], color=\"red\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()), ax=axes[0])\naxes[0].title.set_text('Before \\'log\\' Transformation')\naxes[0].legend()\n\ntrain_fare = train[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ng2 = sns.distplot(train_fare, color=\"blue\", label=\"Skewness : %.2f\"%(train_fare.skew()), ax=axes[1])\naxes[1].title.set_text('After \\'log\\' Transformation')\naxes[1].legend()\n\nplt.tight_layout()","acb143aa":"sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\", data=train, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')\nsns.catplot(x=\"Sex\", y=\"Survived\", col=\"Embarked\", data=train, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')","58a6889b":"train = train.drop(['PassengerId','Name','SibSp','Parch'], axis = 1)\ntest = test.drop(['Name','SibSp','Parch'], axis = 1)","00ecfbfa":"print(\"Train columns:\", ', '.join(map(str, train.columns))) \ndisplay(train.head())\nprint(\"\\nTest columns:\",  ', '.join(map(str, test.columns)))\ndisplay(test.head())","a3348f49":"print(\"TRAIN DATA:\")\ntrain.isnull().sum()","49274d4c":"print(\"TEST DATA:\")\ntest.isnull().sum()","b9327ce0":"train.dtypes","786cd9b1":"numerical = ['Pclass','Age','Is_alone','Fare']\ncategorical = ['Sex','Ticket','Cabin', 'Embarked']","56d096e4":"features = numerical + categorical\ntarget = ['Survived']\nprint('Features:', features, '\\nTarget:', target)","a5c8e7ea":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnumerical_transformer = Pipeline(\n    steps=[('iterative', IterativeImputer(max_iter = 10, random_state=0)),\n           ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(\n    steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n           ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[('num', numerical_transformer, numerical),\n                  ('cat', categorical_transformer, categorical)])","eacdc727":"from sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\n\nobservations = pd.DataFrame()\nclassifiers = ['Linear SVM', 'Radial SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'AdaBoostClassifier', \n               'XGBoostClassifier', 'KNeighborsClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          xgb.XGBClassifier(n_estimators=100),\n          KNeighborsClassifier()\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\n    observations[classifiers[j]] = (cross_val_score(pipe, train[features], np.ravel(train[target]), scoring='accuracy', cv=cv))\n    j = j+1","5f5c2622":"mean = pd.DataFrame(observations.mean(), index= classifiers)\nobservations = pd.concat([observations,mean.T])\nobservations.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nobservations.T.sort_values(by=['Mean'], ascending = False)","77f6613c":"from sklearn.ensemble import VotingClassifier\n\nlinear_svm = svm.SVC(kernel='linear', C=0.1,gamma=10, probability=True)\npipe_linear = Pipeline(steps=[('preprocessor', preprocessor),  ('model', linear_svm)])\n\nrand = RandomForestClassifier(n_estimators=200, random_state=0)\npipe_rand = Pipeline(steps=[('preprocessor', preprocessor),  ('model', rand)])\n\nlog = LogisticRegression()\npipe_log = Pipeline(steps=[('preprocessor', preprocessor),  ('model', log)])\n\nensemble_all = VotingClassifier(estimators=[('Linear_svm', pipe_linear),\n                                                                        ('Random Forest Classifier', pipe_rand),\n                                                                        ('Log', pipe_log)], \n                                                                        voting='soft', weights=[2,3,2])","66ce3a33":"cross_validation_score = cross_val_score(ensemble_all, train[features], np.ravel(train[target]), scoring='accuracy', cv=cv)\nprint(\"K-Fold scores:\", cross_validation_score,\n      \"\\nMean:\", round(cross_validation_score.mean(), 3),\n      \"\\nMax:\", round(cross_validation_score.max(), 3))","e7ecc483":"from sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nmodel = RandomForestClassifier(n_estimators=200, random_state = 0)\n\npipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])","0ab87f67":"cross_validation_score_rand = cross_val_score(pipe, train[features], np.ravel(train[target]), scoring='accuracy', cv=cv)\nprint(\"K-Fold scores:\", cross_validation_score_rand,\n      \"\\nMean:\", round(cross_validation_score_rand.mean(), 3),\n      \"\\nMax:\", round(cross_validation_score_rand.max(), 3))","af9ef8f2":"### 1.2 Voting Ensemble\nWe will select the top 3 models based on their scores.","69a00e24":"## Dealing with missing values\n\n- We have two types of missing values:\n    - Integer\/Float (int64\/float64)\n    - Text (object)\n\n- We will use [IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html) for numerical values and [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) for categorical values.\n- Let's find out which are numerical and categorical columns in our dataset.","71ab48d0":"- This notebook deals with the popular problem statement from Kaggle Competion [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic). \n- Intially we talk about how the features affect the target (prediction about whether the passenger would survive or not).\n- If you are here for the code, feel free to skip the Visualisation part and directly go to [Summary based on visuals](#Summary-based-on-visuals). \n- Table of Contents:\n    1. [Overview](#Overview)\n    2. [Visualisation](#Visualising-the-data)\n    3. [Dealing with missing values.](#Dealing-with-missing-values)\n    4. [Transforming the data.](#Transforming-the-data)\n    5. [Defining Models.](#Defining-Models)\n        1. Ensembling.\n        2. Using the best performing model.\n    6. Evaluation.\n\n- Classifiers used: \n    - For Submission: [RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n    - For Evaluation: \n        1. [Support Vector Machine(SVM)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)\n            1. SVM with Linear kernel.\n            2. SVM with Radial kernel.\n        2. [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n        3. [AdaBoostClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html)\n        4. [XGBoostClassifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn)\n        5. [KNeighborsClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n- Evaluation Metrics: [K-Folds cross-validator](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html)","c30ac7fc":"As you can clearly see, a person has better a chance of surviving if he\/she is not alone. This might prove to be a very good feature in prediction.","8ad7d39f":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with the log function to reduce this skew.\n\nAs we can see, Skewness is clearly reduced after the log transformation.","f2c5dd4c":"We notice that age distributions are not the same in the survived and not survived subpopulations. There is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived.\n\nSo, even if \"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive. It seems that very young passengers have more chance to survive.","a021d8df":"> **UPDATE**: \n- Version 16 gave the best results i.e. Top 4% (0.80622). Later versions are just some of my experiments.\n- So if you are here for the best scoring book, use **version 16**. Otherwise, if you are here to **learn different type of models and experiments** use the later version i.e. **version 17 and ahead.**\n_________________________________________________________________________________________________________________________________________","71056f8a":"### Evaluation of model with the best classifier.","663b9a2a":"### Summary based on visuals\n\n1. Column `PassengerId` won't help us.\n2. I've seen people use column `Name` cleverly like [here](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) but I won't be using in this notebook because:\n    - Not important from prespective of our main objective.\n    - Requires extra efforts.\n    - Might not bring a huge change.\n3. Now that we have created a new feature `Is_alone` using features `SibSp` and `Parch`, we can delete them from our dataset.\n4. Probability of surviving was the most with:\n    1. Passengers who were **young**.\n    2. Passengers who paid more **Fare**.\n    3. Passengers who were **Female**, had passenger class as **1** and boarded at **Cherbourg**.","f81e649d":"## Visualising the data","1b854a3e":"## Titanic Survival Prediction\n<img src = \"https:\/\/mcdn.wallpapersafari.com\/medium\/37\/71\/qtAKe3.jpg\">","79fc3201":"### 1. SibSp and Parch","a19a827e":"#### Observation\n`Pclass, Age, Is_alone, Fare` are numeircal columns.\n\n`Sex, Ticket, Cabin, Embarked` are categorical columns.","23abf335":"## Defining Models\n\nHere, we are going to try two approaches:\n\n1. Ensembling.\n2. Random Forest Classifier (used for submission).","a9e84d79":"### 1.1 Ensembling\n[Ensemble](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html) methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. The models used to create such ensemble models are called \u2018base models\u2019.\n\nWe will do ensembling with the [Voting Ensemble](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html). Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nWe will be using weighted Voting Classifier. We will assign to the classifiers according to their accuracies. So the classifier with single accuracy will be assigned the highest weight and so on.\n\nBut before directly moving to using Voting Classifier, let's take a look at how the above mentioned classification algorithms work individually.\n\nWe will be following a pipeline in the next code. So please pay attention to each and every line.","2def3263":"#### Observations:\n- **177** values missing from `Age` from training data.\n- **687** values missing from `Cabin` from training data.\n- **2** values missing from `Embarked` from training data.\n\n- **86** values missing from `Age` from testing data.\n- **327** values missing from `Cabin` from testing data.","b3de9c7a":"### Explore","f1ed3a07":"### Evaluation of model with 3 classifiers.","2c4548b2":"### Checking for missing values","0a217eeb":"### 2. Random Forest Classifier","27823a15":"### 4. Sex, Pclass and Embarked","abf2c816":"<center>Numerical Imputation.<\/center>\n<center>\u2193<\/center>\n<center>Categorical Imputation.<\/center>\n<center>\u2193<\/center>\n<center>Transforming the dataset after the imputations.<\/center>\n<center>\u2193<\/center>\n<center>Use the models mentioned in `classifiers` list.<\/center>\n<center>\u2193<\/center>\n<center>Use K-Fold cross validation (5 folds).<\/center>","086826cd":"### 3. Fare","64587688":"Basically, the columns `SibSp` and `Parch` tell us whether the corresponding person was accompanied by anyone or not. So we will create a new column `Is_alone` which will tell us whether the person was accompanied (**1**) or not (**0**).","1cfe7f1e":"## Transforming the data\nWe will use combination of [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html) with [Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) to carry out the necessary transformation on our data.\n\nTransformers we are going to use:\n\n|Data type|Transformer|\n|:---|:---|\n|Numerical|[IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html) & [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)|\n|Categorical|[SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html) & [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)|\n\n\nWe will use **most_frequent** strategy for categorical columns.","d6a4aad7":"### 2. Age","6d67f914":"## **Overview**\n* `PassengerId` is the unique id of the row and it doesn't have any effect on target\n* `Survived` is the target variable we are trying to predict (**0** or **1**):\n    - **1 = Survived**\n    - **0 = Not Survived**\n* `Pclass` (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has **3** unique values (**1, 2 or 3**):\n    - **1 = Upper Class**\n    - **2 = Middle Class**\n    - **3 = Lower Class**\n* `Name`, `Sex` and `Age` are self-explanatory\n* `SibSp` is the total number of the passengers' siblings and spouse\n* `Parch` is the total number of the passengers' parents and children\n* `Ticket` is the ticket number of the passenger\n* `Fare` is the passenger fare\n* `Cabin` is the cabin number of the passenger\n* `Embarked` is port of embarkation and it is a categorical feature which has **3** unique values (**C**, **Q** or **S**):\n    - **C = Cherbourg**\n    - **Q = Queenstown**\n    - **S = Southampton**","00db7f38":"### Inference using the above graphs:\n1. **Sex**: There are more than chances of a person suriving if the person is **Female**.\n\n2. **Pclass**: Passengers with passenger class (`Pclass`) as **1** have higher chances of surviving than the others.\n\n3. **Embarked**: Passengers that boarded at **Cherbourg(C)** have survived more than those who boarded at **Queenstown(Q)** and **Southampton(S)**."}}