{"cell_type":{"35ce63a0":"code","cb7611e3":"code","e3f051e2":"code","2123e080":"code","dee5e97b":"code","0cdea521":"code","7fae36fe":"code","e15ca109":"code","37bc5c2e":"markdown","0c925c88":"markdown","0dc51ac8":"markdown","dd85d4ae":"markdown","4be4231b":"markdown","99e0b1c2":"markdown","993c0e5f":"markdown"},"source":{"35ce63a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import optimize # to use minimize function\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))","cb7611e3":"# =============================================================================\n# Function to calculate value of Sigmoid Function of any variable z.\n# z can be a matrix, vector or scalar\n# sigmoid g(z) = 1\/(1 + e^-z)\n# =============================================================================\ndef sigmoid(z):\n    sig = 1.0\/(1.0 + np.exp(-z))\n    \n    # Due to floating point presision related issues, e^-z might return very \n    # small or very large values, resulting in sigmoid = 1 or 0. Since we will\n    # compute log of these values later in cost function, we want to avoid \n    # sig = 1 or 0, and hardcode to following values instead.\n    sig[sig == 1.0] = 0.9999\n    sig[sig == 0.0] = 0.0001\n    \n    return sig","e3f051e2":"# =============================================================================\n# Compute cost of Logistic Regression with multiple features\n# Vectorized implementation\n# Input: data_X = mxn matrix, data_y = m-dim vector, theta = n-dim vector\n# Output: cost = 1-dim vector\n# =============================================================================\ndef computeCost(theta, data_X, data_y, lambda_reg = 0):\n    m = len(data_X) # No of rows\n    n = len(data_X[0]) # No of features\n    theta = theta.reshape(n,1)\n    \n    # h(x) = g(z) = g(theta0 + theta1*X1 + theta2*X2 + .. + thetan*Xn)\n    # h(x) = g(X * theta) = Sigmoid(X * theta) = m-dim vector\n    hx = sigmoid(np.dot(data_X, theta))\n    cost = - np.dot(data_y.T, np.log(hx)) - np.dot((1 - data_y).T, np.log(1 - hx))\n    \n    # This is unregularized cost\n    J = cost\/m\n    \n    # Adding regularization. Setting theta0 to 0, because theta0 will not be \n    # regularized\n    J_reg = (lambda_reg\/(2*m)) * np.dot(theta[1:,:].T, theta[1:,:])\n    J = J + J_reg\n    \n    return J","2123e080":"# =============================================================================\n# Compute gradient or derivative of cost function over parameter, i.e.\n# d J(Theta)\/d Theta\n# =============================================================================\ndef computeGradient(theta, data_X, data_y, lambda_reg = 0):\n    m = len(data_X) # No of rows\n    n = len(data_X[0]) # No of features\n    theta = theta.reshape(n,1)\n    theta_gradient = np.zeros(theta.shape)\n    cost = 0\n    #print(\"==== Inside computeGradient() ====\", data_X.shape, data_y.shape)\n\n    cost = computeCost(theta, data_X, data_y, lambda_reg)\n    \n    hx = sigmoid(np.dot(data_X, theta))\n    error = hx - data_y\n    theta_gradient = (1\/m) * (np.dot(data_X.T, error))\n    \n    # Apply regularization\n    theta_reg = (lambda_reg\/m) * theta[1:,:]\n    theta_gradient[1:,:] = theta_gradient[1:,:] + theta_reg\n    \n    #print(\"==== Inside computeGradient() ====\", cost)\n    return cost.flatten(), theta_gradient.flatten()","dee5e97b":"# =============================================================================\n# One vs All method of logistic regression\n# Used for data with multiple clssification outputs\n# =============================================================================\ndef oneVsAll(data_X, data_y, num_labels, lambda_reg):\n    n = data_X.shape[1] # No of features\n    all_theta = np.zeros([num_labels, n])\n    initial_theta = np.zeros([n, 1])\n    \n    for label in range(num_labels):\n        theta_optimized = optimize.minimize( \\\n            computeGradient, \\\n            initial_theta, \\\n            args=(data_X, data_y == label, lambda_reg), \\\n            method = \"CG\", \\\n            jac=True, options={'disp': True, 'maxiter': 150} \\\n            )\n        #print(\"OneVsAll: Optimization Result =\", theta_optimized)\n        theta = theta_optimized.x.reshape(n, 1)\n        all_theta[label,:] = theta.T\n\n    return all_theta","0cdea521":"# Load training data\ndef loadTrainingData(path):\n    train_data = pd.read_csv(path)\n    #print(train_data.isnull().sum())\n    return train_data","7fae36fe":"# Test my implementation of one-vs-all logistic regression algorithm\ndef test_OneVsAll():\n    # This dataset is downloaded from Kaggle\n    train_data = loadTrainingData('..\/input\/train.csv')\n\n    # Total number of records\n    m = len(train_data)\n    \n    # Populate y data into a m-dim vector\n    # And then drop that column from feature list\n    num_labels = len(train_data.label.unique())\n    data_y = train_data.label.values.reshape(m, 1)\n    train_data = train_data.drop('label', 1)\n    \n    # Setting first feature to 1, this is the bias\/y-intercept or theta0\n    train_data.insert(0, 'first_dummy_feature', 1)\n\n    # Populate X (features) data into a mxn matrix\n    data_X = train_data.values\n    \n    # Call one-vs-all calculation\n    lambda_reg = 0.5\n    all_theta = oneVsAll(data_X, data_y, num_labels, lambda_reg)\n    print(\"OneVsAll: Theta after Advanced Optimization =\", all_theta.shape)\n    \n    # Predict results of test data\n    test_data = loadTrainingData('..\/input\/test.csv')\n    test_data_m = len(test_data)\n    test_data.insert(0, 'first_dummy_feature', 1)\n    test_data_X = test_data.values\n    \n    Z = sigmoid(np.dot(test_data_X, all_theta.T))\n    prediction = np.argmax(Z, axis=1)\n    print(\"OneVsAll: Prediction Result =\", prediction.shape)\n    \n    # Prepare submission file\n    my_submission = pd.DataFrame({ \\\n            'ImageId': np.arange(1, test_data_m+1), \\\n            'Label': prediction.flatten()})\n    my_submission.to_csv('DG_submission.csv', index=False)","e15ca109":"test_OneVsAll()","37bc5c2e":"**The following function is to calculate value of Sigmoid Function of any variable z.\nz can be a matrix, vector or scalar.\n> sigmoid g(z) = 1\/(1 + e^-z)","0c925c88":"This function computes cost of Logistic Regression with multiple features - Vectorized implementation.\n> Input: data_X = mxn matrix, data_y = m-dim vector, theta = n-dim vector <br>\n> Output: cost = 1-dim vector","0dc51ac8":"Testing the implementation.\n1. Load training data\n2. Copy target variable and drop from dataset\n3. Add intercept column with value 1\n4. Call OneVsAll function to find optimized value of parameter\n5. Load test data and add intercept\n6. Predict output on test data using parameter values learnt","dd85d4ae":"This function computes gradient or derivative of cost function over parameter, i.e.\n> d J(Theta)\/d Theta\n\nThis is the objective function which returns both gradient and cost.","4be4231b":"This implementation was for learning purpose only. Please share you thoughts on this can be improved further.","99e0b1c2":"This is the One vs All method of logistic regression. We can use CG or BFGS for optimization. Also, experiment with the regression parameter lambda and number of iterations.","993c0e5f":"**Logistic Regression: One-Vs-All implemented from scratch**\n\nThis is my first implementation of Logistic Regression One-Vs-All or One-Vs-Rest approach, for multiclass classification. This simple implementation uses only numpy for matrix operations and scipy.optimize to find global minima using advanced optimization algorithms (BFGS, CG etc). No other in-built modules or functions have been used.\n\n*NOTE: This implementation is for the sole purpose of validating my theoritical understanding of logistic regression and gradient computation. I understand that in reality more advanced algorithms and optimized pre-built libraries are used for better prediction results. *\n\nFunctions implemented:\n* Sigmoid Function - to calculate Sigmoid value of a scalar, vector or matrix\n* Cost Function - to calcuate cost of regression function for given values of coefficients. Objective is to find optimized coefficients which will minimize cost.\n*  ComputeGradient - to compute the gradient or slope of cost function for given values of coefficients. Objective is to find the global minima, i.e. parameters for which the slope\/derivative is minimum. "}}