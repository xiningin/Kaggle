{"cell_type":{"73a28f45":"code","17f034f4":"code","2890cecd":"code","1ebb9f70":"code","b81993c7":"code","44783498":"code","9975db41":"code","3003f433":"code","b4955427":"code","10aef492":"markdown","d728ddf4":"markdown","dde7eabd":"markdown","cbe10d9a":"markdown","62278c33":"markdown","4a17b0c1":"markdown","05b0e6f6":"markdown","f2012c0c":"markdown","f84e87ea":"markdown","6b4d40ad":"markdown"},"source":{"73a28f45":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n%matplotlib inline","17f034f4":"#Importing iris input features\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\niris.drop(columns=\"Id\",inplace=True)\niris.columns = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\",\"Petal Width\",\"y\"]\nInput = iris.iloc[:,:-1]","2890cecd":"norm_X = MinMaxScaler().fit_transform(Input)","1ebb9f70":"pca = PCA(n_components=3)","b81993c7":"X_pca = pca.fit_transform(norm_X)","44783498":"evr = np.cumsum(pca.explained_variance_ratio_)\nprint(evr)","9975db41":"plt.plot(range(1,len(evr)+1),evr)\nplt.xticks(range(1,len(evr)+1))\nplt.title(\"Explained variance ratio\")\nplt.ylabel(\"Explained variance ratio\")\nplt.xlabel(\"n_components\")\nplt.show()","3003f433":"X_pca=pd.DataFrame(X_pca)\nX_pca.columns=[\"pc1\",\"pc2\",\"pc3\"]\nX_pca[\"y\"]=iris[\"y\"]","b4955427":"fig = px.scatter_3d(X_pca, x='pc1', y='pc2', z='pc3',color='y',title=\"Iris 3D\")\nfig.update_traces(marker_coloraxis=None)\nfig.show()","10aef492":"Scaling \/ normalization improves PCA performance. Hence, we are using min-max normalization technique to normalize the data between 0 and 1","d728ddf4":"explained_variance_ratio_ returns the percentage of variance explained by each component. Calculate the cumulative sum to find the explained_variance_ratio_ of the reduced dataset. The higher the value the lower is the information loss.\n\nBelow we can see the array of 'explained_variance_ratio_'. The first element shows the percentage variance explained if the 'n_components'=1, 2nd element shows 'explained_variance_ratio_' when 'n_components'=2. The third element is the 'explained_variance_ratio_' of the 'n_components' we selected i.e. 3.\n\n0.99361408 shows that 99.4% of the variance of the data is explained by 3 principal components","dde7eabd":"Use fit_transform to fit and transform the PCA on our dataset. The output returned is an array","cbe10d9a":"<h1>Visualising high-dimensional data using PCA and Plotly","62278c33":"Loading the iris dataset which we'll be using for the analysis","4a17b0c1":"PCA takes n_components parameter which is our target dimension 'k' which should be less than the original number of features. Here we choose k as 3","05b0e6f6":"Creating a new dataframe from the reduced components and adding the target variable to the data frame","f2012c0c":"Plotting a 3d scatter plot to visualize the clusters exisiting in the iris dataset.\n\nWe can clearly see there are 3 clusters, each for one target variable","f84e87ea":"Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features with minimal or no information loss.\n\nPCA reduces the dimensionalty 'n' of the original dataset to 'k' dimensions where (k<n).\n\nPCA is mainly used for:\n\n<ul>\n<li>Improving algorithm performance in terms of execution speed<\/li>\n<li>Reducing the dimensionality of data for visualization in 2D or 3D space<\/li>\n    ","6b4d40ad":"PCA in Python can be easily done using the PCA class in sklearn.decomposition"}}