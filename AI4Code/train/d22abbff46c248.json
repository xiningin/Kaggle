{"cell_type":{"05220d4a":"code","5ccd3e3a":"code","4fef157f":"code","63e5eb35":"code","b203a86f":"code","6f2c4d54":"code","06a396f1":"code","e8e55fd6":"code","eac8e453":"code","6ceb525d":"code","03764056":"code","43ecf86a":"code","53851acc":"code","b115a651":"code","f2914869":"code","0cee9a14":"code","4a3856cc":"code","35421362":"code","9e260456":"markdown","8718ebfb":"markdown","0dc6102e":"markdown","fa155463":"markdown","8e54507c":"markdown","d39dee57":"markdown","0a7255f2":"markdown","5c441e20":"markdown","45e26d8f":"markdown","1d7d892e":"markdown","a13cb1c2":"markdown","ce305ad4":"markdown","444c489b":"markdown","e121e461":"markdown","11d22dee":"markdown","4846f7ac":"markdown"},"source":{"05220d4a":"import numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras import regularizers\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nimport itertools\nimport os\nimport time\nprint(os.listdir(\"..\/input\"))","5ccd3e3a":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","4fef157f":"def clean_inputs(train, test, img_shape = (-1,28,28,1), num_classes = 10):\n    t_X = train.drop(\"label\", axis=1)\n    t_Y = train[\"label\"]\n    t_X = t_X \/ 255\n    test_x = test.values \/ 255\n    \n    t_X = np.reshape(t_X.values, img_shape)\n    test_x = np.reshape(test_x, img_shape)\n    \n    t_Y = keras.utils.to_categorical(t_Y, num_classes = num_classes)\n    train_x, dev_x, train_y, dev_y = train_test_split(t_X, t_Y, test_size = 0.15, random_state = 0)\n    \n    return train_x, train_y, dev_x, dev_y, test_x","63e5eb35":"train_x, train_y, dev_x, dev_y, test_x = clean_inputs(train, test)","b203a86f":"def showRandImages(): \n    # show random images from train\n    random_index = np.random.randint(0,train_x.shape[0])\n    plt.imshow(train_x[random_index][:,:,0])\n    print(train_y[random_index])","6f2c4d54":"showRandImages()","06a396f1":"precisiones_globales=[]\ndef plot_model(history):\n    # Plot Accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # Plot Loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","e8e55fd6":"# Propuesta de Modelo A Basado en [1]\ndef model(inp_shape):\n    # (32 conv -> 28x6) -> (pool 14x6) -> (conv -> 10X16) -> (pool -> 5X16) - -> flatte -> FC120 -> FC84 -> softmax10\n    X = Input(inp_shape, name='input')\n    # 32x32x1 -> conv -> 28x28x6 -> pool -> 14x14x6\n    A = Conv2D(6, (7, 7), strides=(1, 1), padding='Same', activation='relu', name='C1')(X)\n    A = MaxPooling2D(pool_size=2, padding='valid')(A)\n    # 14x14x6 -> conv -> 10x10x16 -> pool -> 5x5x16\n    A = Conv2D(16, (5, 5), strides=(1, 1), padding='Same', activation='relu', name='C2')(A)\n    A = MaxPooling2D(pool_size=2, padding='valid')(A)\n    # flatten 5*5*16 = 400\n    A = Flatten()(A)\n    # normalization -> FC 120 -> FC 84 -> softmax 10\n    A = BatchNormalization()(A)\n    A = Dense(120, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='FC1')(A)\n    A = Dense(84, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='FC2')(A)\n    A = Dense(10, activation='softmax', name='Final')(A)\n    model = Model(inputs=X, outputs=A, name='LeNet')\n    return model","eac8e453":"datagen = ImageDataGenerator(       \n        rotation_range=10,  \n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1)        \ndatagen.fit(train_x)","6ceb525d":"# Adding pad to the image\ntrain_x_pad = np.pad(train_x, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0).astype(float)\ndev_x_pad = np.pad(dev_x, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0).astype(float)\ntest_x_pad = np.pad(test_x, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0).astype(float)\n# Learning Rate Decay\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n# model\nmodel = model(train_x_pad.shape[1:])\nmodel.summary()\nmodel.compile('adam', 'categorical_crossentropy', metrics=['accuracy'] )\n# model history\nhistory = model.fit_generator(datagen.flow(train_x_pad, train_y, batch_size=32), validation_data=(dev_x_pad, dev_y), steps_per_epoch=len(train_x_pad)\/\/32, epochs=25, callbacks=[learning_rate_reduction])","03764056":"plot_model(history)","43ecf86a":"# Modelo basado en [2]\ndef model2(num_classes = 10):\n    model = Sequential()\n    # [conv2D -> conv2D -> MaxPooling2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Dense (Out)\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation ='relu', input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    \n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation ='relu'))\n    model.add(BatchNormalization())\n   \n    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation = \"softmax\"))\n    \n    return model;","53851acc":"start = time.time()\n#del model2\nmodel2 = model2(10)\n\n# Learning Rate Decay\nlearning_rate_reduction2 = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n\nmodel2.summary()\nmodel2.compile('adam', 'categorical_crossentropy', metrics=['accuracy'] )\nhistory2 = model2.fit_generator(datagen.flow(train_x, train_y, batch_size=32), validation_data=(dev_x, dev_y), steps_per_epoch=len(train_x)\/\/32, epochs=25, callbacks=[learning_rate_reduction2])\n\ntimeRecord = time.time() - start\nprint(\"--- %s seconds ---\" % (timeRecord))","b115a651":"plot_model(history2)","f2914869":"# Modelo basado en [3]\ndef model3(num_classes = 10):\n    model = Sequential()\n    # [conv2D -> MaxPooling2D -> conv2D ]*3 -> Flatten -> Dense -> Dense (Out)\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation ='relu', input_shape = (28,28,1)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation ='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n \n    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation ='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dense(num_classes, activation = \"softmax\"))\n    \n    return model;","0cee9a14":"start = time.time()\nmodel3 = model3(10)\n\n# Learning Rate Decay\nlearning_rate_reduction3 = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n\nmodel3.summary()\nmodel3.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'] )\nhistory3 = model3.fit_generator(datagen.flow(train_x, train_y, batch_size=64), \n                                validation_data=(dev_x, dev_y), steps_per_epoch=len(train_x)\/\/64, epochs=25, \n                                callbacks=[learning_rate_reduction3])\n\ntimeRecord = time.time() - start\nprint(\"--- %s seconds ---\" % (timeRecord))","4a3856cc":"plot_model(history3)","35421362":"prediction = model2.predict(test_x)\nprediction = np.argmax(prediction, axis=1)\nprediction = pd.Series(prediction, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), prediction],axis = 1)\nsubmission.to_csv('mnist-submission.csv', index = False)\nprint(submission)","9e260456":"### Gr\u00e1fica del modelo 3","8718ebfb":"## Cargar los dataset","0dc6102e":"## Evaluaci\u00f3n del modelo 2","fa155463":"## Definici\u00f3n del Modelo 2","8e54507c":"## Definici\u00f3n del Modelo 1","d39dee57":"## Funciones auxiliares","0a7255f2":"## Definici\u00f3n del modelo 3","5c441e20":"## Filtrados de Datos y Data Augmentation","45e26d8f":"# Referencias\n1. Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient-based learning applied to document recognition,\" in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998.\n2. A. Rosebrock, \"Keras Conv2D and Convolutional Layers - PyImageSearch\", PyImageSearch, 2018. [Online]. Available: [link](https:\/\/www.pyimagesearch.com\/2018\/12\/31\/keras-conv2d-and-convolutional-layers\/). [Accessed: 13- Jun- 2019].\n3. M. Bhob\u00e9, \"MNIST\u200a\u2014\u200aDigits Classification with Keras\", Medium, 2018. [Online]. Available: [link] (https:\/\/medium.com\/@mjbhobe\/mnist-digits-classification-with-keras-ed6c2374bd0e). [Accessed: 13-jun-2019].\n","1d7d892e":"## Evaluaci\u00f3n del modelo 3","a13cb1c2":"## Evaluacion del modelo 1","ce305ad4":"### Gr\u00e1fica del modelo 2","444c489b":"## Predicci\u00f3n del test set","e121e461":"### Gr\u00e1fica del modelo 1","11d22dee":"## Limpieza de Datos","4846f7ac":"# Librerias"}}