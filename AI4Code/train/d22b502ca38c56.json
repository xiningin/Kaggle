{"cell_type":{"cbacb9d2":"code","40e1a735":"code","c99ce4bd":"code","955c9f9e":"code","989bd495":"code","6db6c1a9":"code","1179d41f":"code","ab850834":"code","ccc0c24f":"code","5e9a2fdb":"code","e8b45134":"code","ce7c6fbd":"code","047a7372":"code","6d81a4fc":"code","115ab72d":"code","4712fac8":"code","ce84a1ea":"code","bf770cc7":"code","8d18106c":"code","5604e6a6":"code","187c71c7":"code","39eebf3b":"code","c567cd40":"code","76fd8d93":"code","6247e35e":"code","fb0bd870":"code","3c72f497":"code","e741d898":"code","8f44d3ae":"code","1f5e645f":"code","96cfdb53":"code","7c4aac7d":"code","be61ebea":"code","4eace811":"markdown","54940ced":"markdown","12aed028":"markdown","5a9f1293":"markdown","30e6aa8a":"markdown","166cd888":"markdown","90b6c180":"markdown","ebbf7f60":"markdown","c4735738":"markdown","47bff93b":"markdown","20a2ca77":"markdown","cf78d80a":"markdown","8b46503d":"markdown","039352be":"markdown","7c65afe3":"markdown","cc76c29b":"markdown","716bbf52":"markdown","c9be31a7":"markdown"},"source":{"cbacb9d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\n%matplotlib inline\nfrom scipy.stats import linregress\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","40e1a735":"import pandas as pd\nIris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","c99ce4bd":"Iris.head()","955c9f9e":"Iris.describe()","989bd495":"Iris[\"Species\"].value_counts()","6db6c1a9":"Iris[Iris.isna().any(axis=1)]","1179d41f":"Iris.info()","ab850834":"X = Iris.iloc[:,1:5]    #Dependent variable\nX","ccc0c24f":"y = Iris.iloc[:,5]\ny","5e9a2fdb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n","e8b45134":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ce7c6fbd":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","047a7372":"from sklearn import tree\ntree.plot_tree(classifier,fontsize = 7)","6d81a4fc":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","115ab72d":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","4712fac8":"import matplotlib.pyplot as plt","ce84a1ea":"Iris[\"Species\"].unique()\nIris_Setosa = Iris.loc[Iris[\"Species\"] == \"Iris-setosa\"]\nIris_Versicolor = Iris.loc[Iris[\"Species\"] == \"Iris-versicolor\"]\nIris_Virginica = Iris.loc[Iris[\"Species\"] == \"Iris-virginica\"]\n","bf770cc7":"#Check if the categories of species are well separated by their sepal length\/width or petal length\/width, to judge suitability of clustering\nplt.figure(figsize = (15,7))\nseto = plt.scatter(x = Iris_Setosa[\"SepalLengthCm\"],y = Iris_Setosa[\"SepalWidthCm\"],marker = \"o\",color = \"r\")\nvers = plt.scatter(x = Iris_Versicolor[\"SepalLengthCm\"],y = Iris_Versicolor[\"SepalWidthCm\"],marker = \"o\",color = \"b\")\nvirg = plt.scatter(x = Iris_Virginica[\"SepalLengthCm\"],y = Iris_Virginica[\"SepalWidthCm\"],marker = \"o\",color = \"g\")\nplt.legend((seto,vers,virg),(\"Setosa\",\"Versicolor\",\"Virginica\"),scatterpoints = 1)\nplt.xlabel(\"SepalLengthCm\")\nplt.ylabel(\"SepalWidthCm\")\nplt.title(\"Sepal Length vs Sepal Width\")\n\n\n\n#Difficult to plot legend so use above method\n#colours = np.where(Iris[\"Species\"] == \"Iris-setosa\",'r','-')\n#print(colours)\n#colours[Iris[\"Species\"] == \"Iris-versicolor\"] = 'g'\n#colours[Iris[\"Species\"] == \"Iris-virginica\"] = 'b'\n#print(colours)\n#plt.scatter(x = Iris[\"SepalLengthCm\"],y = Iris[\"SepalWidthCm\"],c = colours)\n#plt.xlabel(\"SepalLengthCm\")\n#plt.ylabel(\"SepalWidthCm\")\n#plt.title(\"Sepal Length vs Sepal Width\")","8d18106c":"plt.figure(figsize = (15,7))\nseto = plt.scatter(x = Iris_Setosa[\"PetalLengthCm\"],y = Iris_Setosa[\"PetalWidthCm\"],marker = \"o\",color = \"r\")\nvers = plt.scatter(x = Iris_Versicolor[\"PetalLengthCm\"],y = Iris_Versicolor[\"PetalWidthCm\"],marker = \"o\",color = \"b\")\nvirg = plt.scatter(x = Iris_Virginica[\"PetalLengthCm\"],y = Iris_Virginica[\"PetalWidthCm\"],marker = \"o\",color = \"g\")\nplt.legend((seto,vers,virg),(\"Setosa\",\"Versicolor\",\"Virginica\"),scatterpoints = 1)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.title(\"Petal Length vs Petal Width\")\n\n#Difficult to include legend so use above method to separate each plot by color.\n#plt.scatter(x = Iris[\"PetalLengthCm\"],y = Iris[\"PetalWidthCm\"],c = colours) \n#plt.xlabel(\"PetalLengthCm\")\n#plt.ylabel(\"PetalWidthCm\")\n#plt.title(\"Petal Length vs Petal Width\")\n#plt.legend(Iris[\"Species\"].unique(),())","5604e6a6":"plt.figure(figsize = (15,7))\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3).fit(Iris[['PetalLengthCm','PetalWidthCm']])\n# Visualise the output labels\nplt.scatter(x=Iris['PetalLengthCm'],y=Iris['PetalWidthCm'], c=kmeans.labels_)\n\n# Visualise the cluster centers (black stars)\nplt.plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],'k*',markersize=20)\nplt.xlabel('Distance_Feature')\nplt.ylabel('Speeding_Feature')\nplt.show()","187c71c7":"print(kmeans.labels_)\nprint(kmeans.cluster_centers_)","39eebf3b":"colours = np.where(Iris[\"Species\"] == \"Iris-setosa\",1,-1)\ncolours[Iris[\"Species\"] == \"Iris-versicolor\"] = 2\ncolours[Iris[\"Species\"] == \"Iris-virginica\"] = 0\n\nfor i in range(len(colours)):\n    colours[i] = int(colours[i])\n","c567cd40":"cm = confusion_matrix(kmeans.labels_, colours)\ncm","76fd8d93":"import seaborn as sns\nsns.set_palette('hls')","6247e35e":"#Reference: https:\/\/www.kaggle.com\/suneelpatel\/learn-ml-from-scratch-with-iris-dataset\nIrisNoID = Iris.drop('Id', axis=1)    #drop the ID column\nsns.pairplot(IrisNoID, hue='Species', markers='+')\nplt.show()","fb0bd870":"IrisNoID.corr()","3c72f497":"#Visualizing the correlation: using heatmap\nplt.figure(figsize=(10,8)) \n#sns.heatmap(IrisNoID.corr(),annot=True,cmap = \"PuBu\")\n#sns.heatmap(IrisNoID.corr(),annot=True,cmap = \"YlOrBr\") #cmap matplotlib colormap name or object, or list of colors, optional\n\n#Diverging color map better for correlation\nsns.heatmap(IrisNoID.corr(),annot=True,cmap = \"PiYG\") #cmap matplotlib colormap name or object, or list of colors, optional\n#sns.heatmap(iris1.corr(),annot=True,cmap='cubehelix_r') \n\nplt.show()\n","e741d898":"#Visualizing the correlation: using diagonal heatmap.\n#Reference: Basic correlation plot.(n.d.).Retrieved from: https:\/\/riptutorial.com\/seaborn\/example\/31922\/basic-correlation-plot\n\n#Compute correlations\ncorr = IrisNoID.corr()\n\n# Exclude duplicate correlations by masking uper right values\nmask = np.zeros_like(corr, dtype=np.bool)    #np.zeros_like return a matrix with same shape except entries = 0 (or boolean\/float\/etc)\nmask[np.triu_indices_from(mask)] = True      #np.triu_indices_from: Return the indices for the upper-triangle of arr.\n\n# Set background color \/ chart style\nsns.set_style(style = 'white')\n\n# Set up  matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Add diverging colormap\ncmap = sns.diverging_palette(10, 250, as_cmap=True)\n\n# Draw correlation plot\nsns.heatmap(corr, mask=mask, cmap=cmap, \n        square=True,\n        linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)","8f44d3ae":"# Importing alll the necessary packages to use the various classification algorithms\n\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import metrics #for checking the model accuracy","1f5e645f":"logr = LogisticRegression()\nlogr.fit(X_train,y_train)\ny_pred = logr.predict(X_test)\nacc_log = metrics.accuracy_score(y_pred,y_test)    #metrics from scikit learn. Can be used instead of manually calculating from confusion matrix\nprint('The accuracy of the Logistic Regression is', acc_log)","96cfdb53":"sv = svm.SVC() #select the algorithm\nsv.fit(X_train,y_train) # we train the algorithm with the training data and the training output\ny_pred = sv.predict(X_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","7c4aac7d":"knc = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nknc.fit(X_train,y_train)\ny_pred = knc.predict(X_test)\nacc_knn = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the KNN is', acc_knn)","be61ebea":"a_index = list(range(1,11))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    kcs = KNeighborsClassifier(n_neighbors=i) \n    kcs.fit(X_train,y_train)\n    y_pred = kcs.predict(X_test)\n    a=a.append(pd.Series(metrics.accuracy_score(y_pred,y_test)))\nplt.plot(a_index, a)\nplt.xticks(x)","4eace811":"This category look better for clustering classification that the previous due to the more obvious separation between classes.","54940ced":"Data Visualization:","12aed028":"Accuracy of Decision Tree: 13+15+9\/13+15+9+1 = 0.9737","5a9f1293":"***Other kernels' methods***","30e6aa8a":"Let's check the accuracy for various values of n for K-Nearest nerighbours","166cd888":"Data auditing\n* Dimensions of data\n* Missing values","90b6c180":"** Using same method as above except using clustering (k-means)**","ebbf7f60":"Another way to check for missing values:","c4735738":"Feature selection: Choose features that are not highly correlated.","47bff93b":"Patel, S. (2019). Learn ML from Scratch with IRIS Dataset. Retrieved from: https:\/\/www.kaggle.com\/suneelpatel\/learn-ml-from-scratch-with-iris-dataset\/comments\n","20a2ca77":"Patel, S. (2019). Learn ML from Scratch with IRIS Dataset. Retrieved from: https:\/\/www.kaggle.com\/suneelpatel\/learn-ml-from-scratch-with-iris-dataset\/comments\n\nBuild Model with Scikit-learn\n\nSteps To Be followed When Applying an Algorithm\n\nStep 1: Split the dataset into training and testing dataset. The testing dataset is generally smaller than training one as it will help in training the model better.\n\nStep2: Select any algorithm based on the problem (classification or regression) whatever you feel may be good.\n\nStep3: Then pass the training dataset to the algorithm to train it. We use the .fit() method\n\nStep4: Then pass the testing data to the trained algorithm to predict the outcome. We use the .predict() method.\n\nStep5: We then check the accuracy by passing the predicted outcome and the actual output to the model.","cf78d80a":"No missing values","8b46503d":"Accuracy of K means: 46+50+48\/46+50+48+2+4 = 0.96","039352be":"Another method to visualize all pair plots of variables using seaborn","7c65afe3":"Support Vector Machine articles for referencing: \n* https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n* https:\/\/medium.com\/machine-learning-101\/chapter-2-svm-support-vector-machine-theory-f0812effc72","cc76c29b":"**Checking missing values**","716bbf52":"Using the pairplots can choose most suitable variables for clustering","c9be31a7":"Splitting data and using Decision Tree to classify"}}