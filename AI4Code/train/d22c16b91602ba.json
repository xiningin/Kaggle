{"cell_type":{"30294438":"code","21a194cb":"code","d4557b95":"code","ac71b622":"code","9635d0b5":"code","b4ac70ab":"code","40b3e502":"code","85860a28":"markdown","0632e027":"markdown","1be240be":"markdown","6fd0fda2":"markdown","5ae4505a":"markdown","b334afe5":"markdown","04c5b27c":"markdown","eff324f7":"markdown","07c742d6":"markdown","bf7e612d":"markdown","1a187fab":"markdown","cf9891d6":"markdown","9d3049cf":"markdown","68cdcd1a":"markdown"},"source":{"30294438":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport sklearn.model_selection as model_selection\nfrom sklearn.neighbors import KNeighborsClassifier","21a194cb":"df = pd.read_csv(\"..\/input\/lecture06risk\/ClassifyRisk.csv\")\ndf","d4557b95":"import sklearn.model_selection as model_selection\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.3,random_state=4)","ac71b622":"#normalisation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nscaler.fit(X_train)\nX_train = scaler.transform(X_train) \nX_test = scaler.transform(X_test)","9635d0b5":"clf.get_params()","b4ac70ab":"#Predicted lables\ny_pred = clf.predict(X_test)  \nfrom sklearn import metrics\n# Model Accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","40b3e502":"clf1 = LogisticRegression(max_iter =1000, penalty ='l1', solver ='liblinear')\nclf2 = LogisticRegression(max_iter =2000, penalty ='l2', solver ='saga')","85860a28":"## Reflect\nBriefly note what you\u2019ve learnt, found easy and found challenging in your Jupyter notebook. Keep these notes safe and maintain a reflective log for each lab session.","0632e027":"## Resources\/references\n1. sklearn.neighbors.KNeighborsClassifier: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n2. Seaborn plotting: https:\/\/seaborn.pydata.org\/ \n","1be240be":"* Adjust the order of Column `risk`","6fd0fda2":"## Activity 1: Predict credit risk  \n\nIn this lab, you will build a logistic regression model for predicting customers' `credit risk`. The task is the same as Labs 6 and 7. \n\n### Download data\n\n* Download `ClassifyRisk.csv` from Lecture 6 Juputer notebook \n\n> Q: how to download a data set from another Kaggle notebook?","5ae4505a":"### Print coefficients of the logistic regression Curve\n$$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n* Coefficients are $\\beta_1, \\cdots, \\beta_n$\n    * weights on input variables (features) \n* Intercept: $\\beta_0$\n> Q: which predictor has a positive coefficient? Which predictor has a negative coefficient?","b334afe5":"> Q. which of classifiers`clf1` and `clf2` performs better in terms of accuracy?","04c5b27c":"### Split data into training and test data\n* Set 70% for training and 30% for test\n* normalisation \n> Q: a scaler must fit both `X_train` and`X_test`?\n> 1. True\n> 2. False","eff324f7":"### Evaluate the model\n* Predict the label of patients in test data set\n* Calculate the accuracy of prediction\n> Q: is the accuracy of your logistic regression model better than that of KNN and Decision Tree models in Lab 6 and Lab 7?","07c742d6":"### Seperate predictors and target\n* Set `X` to predictors\n* Set `y` to target\n> Q: which attributes are predictors? Which attribute is the target?","bf7e612d":"\n## Activity 2 Tune parameters in logistic regression\nTune the following parameters in  logistic regression and find out the best parameters\n* `max_iter`: the maximum number of iterations for a solver  to iterate\n* `penalty`:  Used to specify penalization in regularization\n    * \u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019, \u2018none\u2019\n* `solver`: used for fitting the model in logistic regression\n    * For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.\n    * \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018saga\u2019 handle L2 or no penalty\n    * \u2018liblinear\u2019 and \u2018saga\u2019 also handle L1 penalty\n    * \u2018saga\u2019 also supports \u2018elasticnet\u2019 penalty\n    * \u2018liblinear\u2019 does not support setting penalty='none' ","1a187fab":"## Learning objectives\n* Implement logistic regression for classification \n* Tune parameters in logistic regression","cf9891d6":"# Logistic Regression\n\n\nCOMP20121 Machine Learning for Data Analytics\n\nAuthor: [Jun He](https:\/\/sites.google.com\/site\/hejunhomepage\/) ","9d3049cf":"### Encode categorical features\n* Encode all categorical features\n>  Q: Which feature takes ordinal encoder? Which feature takes one-hot encoder?","68cdcd1a":"### Build a logistic regression model\n* Create a logistic regression object (classifier)\n* Train the classifer on training data `fit(X_train,y_train)`\n* Print parameters used in the classifier\n> Q:  what is the default `solver` used in Sklearn `LogisticRegression` model?"}}