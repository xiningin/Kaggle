{"cell_type":{"578b48b7":"code","4f6426d1":"code","af936335":"code","8efc8c51":"code","870c78ba":"code","a9264caf":"code","88fd91d2":"code","9120cf73":"code","74f22763":"code","77150bbe":"code","018e4619":"code","8a62d054":"code","f31c072d":"code","7bde8ab6":"code","93db54c6":"code","620d6914":"code","168e15c1":"code","ce9ea5f1":"code","a6f09904":"code","a4dc87b7":"code","8eb6becb":"code","a94b995b":"code","b94b7a61":"code","24e1c1ee":"code","98fe0e52":"code","15ce9ffb":"code","b3807a01":"code","521b40de":"code","d4472b5f":"code","7741f39d":"markdown","8f6e512d":"markdown","a7b2d652":"markdown","daf454c1":"markdown","dae6c6bc":"markdown","6489ba6d":"markdown","372c22e7":"markdown","5a484188":"markdown","af7b78e3":"markdown","00d59f53":"markdown","1ae8847d":"markdown"},"source":{"578b48b7":"### Supervised Learning Assignment\n### Google Play Store Dataset\n\n## Importing various packages which can aid us to work on the dataset\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import metrics \nimport xgboost\nfrom xgboost import XGBClassifier\nimport catboost\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom numpy import asarray\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix","4f6426d1":"import os\nprint(os.listdir('..\/input'))\n\n## Reading the google play store dataset in a dataframe\n## The dataset with prelimnary edits have some unnecessary rows and columns removed with the help of SPSS and Excel\ngoogle_data = pd.read_csv('..\/input\/google-play-store-dataset-with-prelimnary-edits\/googleplaystore(edited with excel).csv')\n\n## Understanding the dataset columns, rows and check for missing values and draw out data description\n# Check the total number of rows and ccolumns\nprint(\"Total number of rows and columns in the data set are: \",google_data.shape)\n# Identify the columns present in the dataset\nprint(\"The columns in the dataset are as followed: \\n\", google_data.columns)\n# Check the datatypes of each column in python\nprint(\"THe data types of each columns are:\\n\", google_data.dtypes)\n# Check for null values\nprint(\"Check whether there are missing values in the dataset or not:\\n\", google_data.isnull().sum())\n# Print the dataset to view the data\nprint(google_data)\ndf1 = google_data","af936335":"## Converting the categorical values using one hot encoder\nle_Category = LabelEncoder()\nle_ContentRating = LabelEncoder()\nle_Type = LabelEncoder()\n\ndf1['Category_n'] = le_Category.fit_transform(df1['Category'])\ndf1['Type_n'] = le_Type.fit_transform(df1['Type'])\ndf1['ContentRating_n'] = le_ContentRating.fit_transform(df1['Content Rating'])","8efc8c51":"## Convert the Size variable in thousands\nfor i in range(len(df1)):\n    value = df1['Size'].values[i]\n    ver = value.isnumeric()\n    try:  \n        float(value) \n        ver1 = True\n    except: \n        ver1 = False\n    if ((ver == True) or (ver1 == True)):\n        value = float(value) * 1000\n        df1['Size'].values[i] = value\n    elif value == 'Varies with device':\n        df1['Size'].values[i] = 0\n    else:\n        value = value.replace(\"k\", \"\")\n        df1['Size'].values[i] = value\n\nnew = df1[\"Last Updated\"].str.split(\", \", n = 1, expand = True)\ndf1[\"Last Updated\"] = new[1]","870c78ba":"## Dropping the unnecessary columns\ndf1 = df1.drop(['Genres', 'Current Ver', 'Android Ver', 'Category', 'Type', 'Content Rating'], axis = 'columns')","a9264caf":"## Impute the missing values of the rating\ndf1['Rating'] = df1['Rating'].fillna(df1['Rating'].median())\ndf1['Last Updated'] = df1['Last Updated'].fillna(df1['Last Updated'].mode())\nmedian_size = df1['Size'].median()\ndf1['Size'] = df1['Size'].replace(0, median_size)\n\nfor i in range(len(df1)):\n    value = df1[\"Rating\"].values[i]\n    if (1<=value<=1.9):\n        df1[\"Rating\"].values[i] = 1\n    elif (2<=value<=2.9):\n        df1[\"Rating\"].values[i] = 2\n    elif (3<=value<=3.9):\n        df1[\"Rating\"].values[i] = 3\n    elif (4<=value<=4.9):\n        df1[\"Rating\"].values[i] = 4\n    elif value==5:\n        df1[\"Rating\"].values[i] = 5\n    else:\n        df1[\"Rating\"].values[i] = 0","88fd91d2":"print(\"Total number of rows and columns in the data set are: \",df1.shape)\n# Identify the columns present in the dataset\nprint(\"The columns in the dataset are as followed: \\n\", df1.columns)\n# Check the datatypes of each column in python\nprint(\"THe data types of each columns are:\\n\", df1.dtypes)\n# Check for null values\nprint(\"Check whether there are missing values in the dataset or not:\\n\", df1.isnull().sum())\n# Print the dataset to view the data\nprint(df1)","9120cf73":"google_data.shape","74f22763":"google_data.info()","77150bbe":"# Number of NaN's in every column\ngoogle_data.isnull().sum()","018e4619":"google_data.Category.unique() ","8a62d054":"# Correlation map\nplt.figure(figsize = (12, 10)); # Adding command before the heatmap allows to control the size of a plot\nsns.heatmap(google_data.corr(), annot = True);","f31c072d":"## Identify the predictors and the target variable\nX = df1.drop(['Rating', 'App'], axis = 'columns')\ny = df1['Rating']\n\n## Split the dataset into train and test model\nX_train,X_test,y_train,y_test=train_test_split(X, y, test_size = 0.2, random_state = 3)\n# Print and understand the train test\nprint(\"The dimensions of the Train Dataset\\n   Predictors dataset dimension: {}\\n   Target dataset dimension: {}\"\\\n      .format(X_train.shape, y_train.shape))\nprint(\"The dimensions of the Test Dataset\\n   Predictors dataset dimension: {}\\n   Target dataset dimension: {}\"\\\n      .format(X_test.shape, y_test.shape))","7bde8ab6":"## Apply Logistic Regression on the model\nprint(\"------------Regression------------\")\nlr = LogisticRegression()\n# Fit the model\nlr.fit(X_train, y_train)\ntrain_score = lr.score(X_train, y_train)\ntest_score = lr.score(X_test, y_test)\n# Print the required results\nprint('Coefficients of the logistic regression is: \\n', lr.coef_)\nprint(\"\\n\")\nprint('Variance score of the model is: ',format(lr.score(X_test, y_test)))\nprint (\"Logistic regression train score: \", train_score)\nprint (\"Logistic regression test score: \", test_score)","93db54c6":"## Applying CV on the logistic regression\nlr = LogisticRegressionCV(cv = 100, n_jobs = 1, solver = 'liblinear', random_state = 10)\nlr.fit(X, y)\ntrain_score = lr.score(X_train, y_train)\ntest_score = lr.score(X_test, y_test)\n# Print the required results\nprint('Coefficients of the logistic regression is: \\n', lr.coef_)\nprint(\"\\n\")\nprint('Variance score of the model is: ',format(lr.score(X_test, y_test)))\nprint (\"Logistic regression train score: \", train_score)\nprint (\"Logistic regression test score: \", test_score)","620d6914":"#Applying GridSearchCV on Logistic Regression\ntuned_parameters = [{'C' :[10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4]}]\ngrid = GridSearchCV(LogisticRegression(n_jobs = 1, solver = 'liblinear', random_state = 10, max_iter = 1000), \\\n                    tuned_parameters, scoring = 'f1_weighted')\ngrid.fit(X_train, y_train)\nprint(\"Best Grid Estimators: \",grid.best_estimator_)\nprint('Variance score of the model is: {}'.format(grid.score(X_test, y_test)))\nprint (\"Logistic regression train score: \", grid.score(X_train, y_train))\nprint (\"Logistic regression test score: \", grid.score(X_test, y_test))","168e15c1":"## Decision Tree\nprint(\"------------Decision Tree------------\")\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(\"The Decision Tree model gives score\\n  Train: {}\\n  Test: {}\".format(model.score(X_train, y_train), \\\n                                                                            model.score(X_test, y_test)))\ny_pred = model.predict(X_test)\nprint(\"\\nThe accuracy on prediciton and test\", metrics.accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report of Decision Tress:\\n\",classification_report(y_test, y_pred))\nscores = cross_val_score(estimator = model, X = X, y = y, cv = 7, n_jobs = 4)\nprint(\"Cross Validation Score for Decision Tree: \", scores.mean())","ce9ea5f1":"## Decision Tree\nmodel = tree.DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5)\nmodel.fit(X_train, y_train)\nprint(\"The Decision Tree model gives score\\n  Train: {}\\n  Test: {}\".format(model.score(X_train, y_train), \\\n                                                                            model.score(X_test, y_test)))\ny_pred = model.predict(X_test)\nprint(\"\\nThe accuracy on prediciton and test\", metrics.accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report of Decision Tress:\\n\",classification_report(y_test, y_pred))\nscores = cross_val_score(estimator = model, X = X, y = y, cv = 7, n_jobs = 4)\nprint(\"Cross Validation Score for Decision Tree: \", scores.mean())","a6f09904":"# Fitting the Grid Search on Decision Tree\nparams = {'criterion' : ['gini' , 'entropy'], 'max_depth' : range(1,10), 'min_samples_leaf' : range(1,5)}\ngrid = GridSearchCV(model, params, scoring = 'accuracy', cv = 20)\ngrid.fit(X_train, y_train)\ngrid_pred = grid.predict(X_test)\nprint(\"Best Grid Search Estimators: \", grid.best_estimator_)\nprint(\"The Decision Tree (Grid Search) model gives score\\n  Train: {}\\n  Test: {}\".format(grid.score(X_train, y_train), \\\n                                                                            grid.score(X_test, y_test)))\nprint(\"\\nThe accuracy on prediciton and test\", metrics.accuracy_score(y_test, grid_pred))\nscores = cross_val_score(estimator = grid, X = X, y = y, cv = 7, n_jobs = 4)\nprint(\"Cross Validation Score for Decision Tree (Grid Search): \", scores.mean())","a4dc87b7":"## Random Forrest\nprint(\"------------Random Forrest------------\")\nmodel = RandomForestClassifier(n_estimators = 10)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(\"The random forest model gives score\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\"\\\n      .format(model.score(X_train, y_train), model.score(X_test, y_test), accuracy_score(y_test, y_pred)))\nprint(\"Classification Report of Random Forrest:\\n\",classification_report(y_test, y_pred))\nscores = cross_val_score(model, X, y, cv=5)\nprint(\"Cross Validation Score of random forest: \", scores.mean())","8eb6becb":"cm = confusion_matrix(y_test, y_pred)\nprint(\"The confusion matrix for random forest:\\n\",cm)","a94b995b":"# Fitting the Grid Search\nparams = {'n_estimators':[34,35,65,45]}\ngrid = GridSearchCV(estimator = model,param_grid = params,scoring = 'accuracy', cv = 5)\ngrid.fit(X_train, y_train)\ngrid_pred = grid.predict(X_test)\nprint(\"The random forest model after applying grid search gives the following score\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\".format(grid.score(X_train, y_train), grid.score(X_test, y_test), accuracy_score(y_test, grid_pred)))\nprint(\"Classification Report of Random Forrest:\\n\",classification_report(y_test, grid_pred))","b94b7a61":"## Boosting Methods\nprint(\"------------Gradient Boosting------------\")\n# evaluate the model gradient boost\nmodel = GradientBoostingClassifier()\ncv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\nn_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error',\\\n                           cv = cv, n_jobs = -1, error_score = 'raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\nprint(\"Accuracy Gradient\", model.score(X_train, y_train))","24e1c1ee":"# evaluate the model CatBoost\nprint(\"------------CatBoost Classifier------------\")\nmodel = CatBoostClassifier(verbose = 0, n_estimators = 100)\ncv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\nn_scores = cross_val_score(model, X_train, y_train, scoring = 'neg_mean_absolute_error',\\\n                           cv = cv, n_jobs = -1, error_score = 'raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = CatBoostClassifier(verbose = 0, n_estimators = 100)\nmodel.fit(X_train, y_train)\nprint(\"Accuracy CatBoost\", model.score(X_train, y_train))","98fe0e52":"# evaluate the model Adaboost\nprint(\"------------AdaBoost Classifier------------\")\nregr = AdaBoostClassifier(random_state = 0, n_estimators = 100)\ncv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\nn_scores = cross_val_score(regr, X_train, y_train, scoring='neg_mean_absolute_error',\\\n                           cv = cv, n_jobs = -1, error_score = 'raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nregr = AdaBoostClassifier(random_state = 0, n_estimators = 100)\nregr.fit(X, y)\nprint(\"Accuracy Adaboost\", regr.score(X_train, y_train))","15ce9ffb":"## Naive Bayesian\nprint(\"------------Gausian Naive Bayesian------------\")\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(\"Accuracy of the Naive Bayes\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\"\\\n      .format(model.score(X_train,y_train), model.score(X_test,y_test), accuracy_score(y_test, y_pred)))\nprint(\"Cross Validation of the Naive Bayesian Model: \",(cross_val_score(GaussianNB(),X_train, y_train, cv=5)).mean())\nprint(\"\\nClassification Report of Gausian Naive Bayes:\\n\",classification_report(y_test, y_pred))","b3807a01":"## Naive Bayesian\nprint(\"------------Multinomial Naive Bayesian------------\")\nmodel = MultinomialNB()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(\"Accuracy of the Multinomial Naive Bayes\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\"\\\n      .format(model.score(X_train,y_train), model.score(X_test,y_test), accuracy_score(y_test, y_pred)))\nprint(\"Cross Validation of the Multinomial Naive Bayesian Model: \",(cross_val_score(MultinomialNB(),X_train, y_train, cv=5)).mean())\nprint(\"\\nClassification Report of Multinomial Naive Bayes:\\n\",classification_report(y_test, y_pred))","521b40de":"## Naive Bayesian\nprint(\"------------Bernoulli Naive Bayesian------------\")\nmodel = BernoulliNB(binarize = 0.0)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(\"Accuracy of the Bernoulli Naive Bayes\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\"\\\n      .format(model.score(X_train,y_train), model.score(X_test,y_test), accuracy_score(y_test, y_pred)))\nprint(\"Cross Validation of the Bernoulli Naive Bayesian Model: \",(cross_val_score(BernoulliNB(),X_train, y_train, cv=5)).mean())\nprint(\"\\nClassification Report of Bernoulli Naive Bayes:\\n\",classification_report(y_test, y_pred))","d4472b5f":"# Fitting the Grid Search\nparams = [{'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}]\ngrid = GridSearchCV(BernoulliNB(), params, scoring = 'f1_weighted')\ngrid.fit(X_train, y_train)\ngrid_pred = grid.predict(X_test)\nprint(\"Best Grid Search Estimators\",grid.best_estimator_)\nprint(\"Accuracy of the Bernoulli Naive Bayes\\n  Train: {}\\n  Test: {}\\n  Accuracy of the model: {}\"\\\n      .format(grid.score(X_train,y_train), grid.score(X_test,y_test), accuracy_score(y_test, grid_pred)))","7741f39d":"#### Cleaning and Pre-Processing the dataset for further use","8f6e512d":"* # Google Play Store Review Dataset","a7b2d652":"#### Logistic Regression Model","daf454c1":"## Apply Various Algorithms ot the Dataset","dae6c6bc":"#### Importing theRequired Packages","6489ba6d":"## Processing the Dataset\n\n#### Read the Dataset to be processed and understand the dataset","372c22e7":"Note: The Output Variable here is categorical in nature","5a484188":"#### Decision Tree Model","af7b78e3":"#### Understanding the parameters of the newly created dataset","00d59f53":"#### Naive Bayesian Model","1ae8847d":"#### Random Forest Model"}}