{"cell_type":{"d3bcb631":"code","3d4a9ba2":"code","6a0f3dc0":"code","46a61758":"code","df865d65":"code","ccb14d57":"code","307700fd":"code","6722c37a":"code","66ce5540":"code","4bc6d45f":"code","b62791c6":"code","8a5c0e7c":"code","2ef08d11":"code","2e68a486":"code","2ddaacf9":"code","97ba1536":"code","d2964607":"code","89d938e9":"code","e2587685":"code","f787ebcc":"code","bd1b6515":"code","07b82b14":"code","469cb824":"code","d0516282":"code","29a4a0ae":"code","9f27c3ba":"code","54c79ae7":"code","55fbfd66":"code","ed350f65":"code","0c03a16d":"code","1a8a2d75":"code","b2a18b48":"code","54f5537d":"code","9b9c650d":"code","61c341db":"code","9c44754e":"code","202b5837":"code","e8018456":"code","8c5fd737":"code","f866b9d7":"code","5dde3ebd":"code","6c0a426b":"code","e9e220ac":"code","fc6a7865":"code","c62c4a88":"code","729d3186":"code","0b88d5e3":"code","dfdec817":"code","0f0c075d":"code","06045e0a":"code","9620ccae":"code","3d8ad40c":"code","d0aea1ae":"code","6ed6c7ed":"code","c25fac7c":"code","fdb98b4e":"code","8e21d12c":"code","c0e63f51":"code","43a6f088":"code","3b2a9583":"code","123847f5":"code","298fc3e1":"code","2e8a35b5":"code","b7f38160":"code","3cab1613":"code","cdef5a8e":"markdown","ba3764c5":"markdown","cffa76c9":"markdown","7ceb9c61":"markdown","9eaed0d8":"markdown","97aa57a8":"markdown","a8548a03":"markdown","2aa31d73":"markdown"},"source":{"d3bcb631":"!pip install imutils","3d4a9ba2":"import shutil\nimport tqdm\nimport os\nimport cv2\nfrom imutils import paths\nimport random\nimport shutil","6a0f3dc0":"from keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, BatchNormalization, Flatten, Dense\n\nfrom keras.models import Sequential, Model\n#rom keras.applications.xception import Xception\n#from keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.optimizers import Adam, SGD, RMSprop\n\nimport tensorflow as tf\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","46a61758":"dataset_path = '.\/dataset'","df865d65":"covid_dataset_path = '..\/input\/covid-chest-xray'\n\ncoivd_path='..\/input\/covid-19-x-ray-10000-images\/dataset\/covid\/'","ccb14d57":"%%bash\nrm -rf dataset\nmkdir -p dataset\/covid\nmkdir -p dataset\/normal\nmkdir -p dataset\/pneumonia","307700fd":"# construct the path to the metadata CSV file and load it\ncsvPath = os.path.sep.join([covid_dataset_path, \"metadata.csv\"])\ndf = pd.read_csv(csvPath)\n\n# loop over the rows of the COVID-19 data frame\nfor (i, row) in df.iterrows():\n    # if (1) the current case is not COVID-19 or (2) this is not\n    # a 'PA' view, then ignore the row\n    if row[\"finding\"] != \"COVID-19\" or row[\"view\"] != \"PA\":\n        continue\n\n    # build the path to the input image file\n    imagePath = os.path.sep.join([covid_dataset_path, \"images\", row[\"filename\"]])\n\n    # if the input image file does not exist (there are some errors in\n    # the COVID-19 metadeta file), ignore the row\n    if not os.path.exists(imagePath):\n        continue\n\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = row[\"filename\"].split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/covid\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","6722c37a":"data5 = '..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/'","66ce5540":"print('NORMAL cases ',len(os.listdir(data5+'NORMAL')))\nprint('Covid cases ',len(os.listdir(data5+'COVID-19')))\nprint('Viral Pneumonia cases ',len(os.listdir(data5+'Viral Pneumonia')))","4bc6d45f":"basePath = os.path.sep.join([data5,  \"NORMAL\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:220]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/normal\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","b62791c6":"basePath = os.path.sep.join([data5,  \"COVID-19\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:220]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/covid\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","8a5c0e7c":"basePath = os.path.sep.join([data5, 'Viral Pneumonia'])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:220]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/pneumonia\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","2ef08d11":"print('Covid cases ',len(os.listdir('dataset\/covid')))\nprint('normal cases ',len(os.listdir('dataset\/normal')))\nprint('pneumonia cases ',len(os.listdir('dataset\/pneumonia')))","2e68a486":"!cp -a ..\/input\/covid-19-x-ray-10000-images\/dataset\/normal\/. dataset\/normal","2ddaacf9":"!cp -a ..\/input\/covid-19-x-ray-10000-images\/dataset\/covid\/. dataset\/covid","97ba1536":"pneumonia_dataset_path ='..\/input\/chest-xray-pneumonia\/chest_xray'","d2964607":"basePath = os.path.sep.join([pneumonia_dataset_path, \"train\", \"NORMAL\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:53]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/normal\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","89d938e9":"basePath = os.path.sep.join([pneumonia_dataset_path, \"train\", \"PNEUMONIA\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:53]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/pneumonia\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","e2587685":"basePath = os.path.sep.join([pneumonia_dataset_path, \"test\", \"PNEUMONIA\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:53]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/pneumonia\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","f787ebcc":"data4 = '..\/input\/covid19-xray-dataset-train-test-sets\/xray_dataset_covid19\/'","bd1b6515":"print('NORMAL cases ',len(os.listdir('..\/input\/covid19-xray-dataset-train-test-sets\/xray_dataset_covid19\/train\/NORMAL\/')))","07b82b14":"!cp -a ..\/input\/covid19-xray-dataset-train-test-sets\/xray_dataset_covid19\/train\/NORMAL\/. dataset\/normal","469cb824":"print('Covid cases ',len(os.listdir('dataset\/covid')))\nprint('normal cases ',len(os.listdir('dataset\/normal')))\nprint('pneumonia cases ',len(os.listdir('dataset\/pneumonia')))","d0516282":"# grab the list of images in our dataset directory, then initialize\n# the list of data (i.e., images) and class images\nprint(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(dataset_path))","29a4a0ae":"im = mpimg.imread(img_path)","9f27c3ba":"im.shape[0]","54c79ae7":"import glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nsizes_covid_width = []\nsizes_covid_height = []\n\nsizes_normal_width = []\nsizes_normal_height = []\n\nsizes_pneum_width = []\nsizes_pneum_height = []\n\ncovid_images1 = []\nfor img_path in glob.glob('dataset\/covid\/*'):\n    im = mpimg.imread(img_path)\n    covid_images1.append(im)\n    sizes_covid_width.append(im.shape[0])\n    sizes_covid_width.append(im.shape[1])\n        \n\nfig = plt.figure()\nfig.suptitle('COVID')\nplt.imshow(covid_images1[0], cmap='gray') \n\npneumonia_images = []\nfor img_path in glob.glob('dataset\/pneumonia\/*'):\n    pneumonia_images.append(mpimg.imread(img_path))\n\nfig = plt.figure()\nfig.suptitle('Pneumonoia')\nplt.imshow(pneumonia_images[0], cmap='gray') \n\n\nnormal_images = []\nfor img_path in glob.glob('dataset\/normal\/*'):\n    normal_images.append(mpimg.imread(img_path))\n\nfig = plt.figure()\nfig.suptitle('NORMAL')\nplt.imshow(normal_images[0], cmap='gray')","55fbfd66":"covid_images1[0][0][0][0]\nimport statistics\nstatistics.mean(covid_images1[0][0][0])","ed350f65":"IMG_W = 224\nIMG_H = 224\nCHANNELS = 3\n\nINPUT_SHAPE = (IMG_W, IMG_H, CHANNELS)\nNB_CLASSES = 3\nEPOCHS = 55\nBATCH_SIZE = 16","0c03a16d":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.15)\n\ntrain_generator = train_datagen.flow_from_directory(\n    'dataset',\n    target_size=(IMG_H, IMG_W),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    'dataset',  \n    target_size=(IMG_H, IMG_W),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation')","1a8a2d75":"model = Sequential()\nmodel.add(Conv2D(80, (3, 3), input_shape=INPUT_SHAPE))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\n\n\nmodel.add(Conv2D(64, (3, 3)))\n\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',\n          optimizer='adam',\n          metrics=['accuracy'])","b2a18b48":"\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples \/\/ BATCH_SIZE,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ BATCH_SIZE,\n    epochs = 80)","54f5537d":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train.', 'Valid.'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9b9c650d":"x_test, y_test=next(validation_generator)","61c341db":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","9c44754e":"model.save_weights(\"3classes.h5\")","202b5837":"Classifier = Sequential()\nClassifier.add(Conv2D(80, (3, 3), input_shape=(224,224,3)))\nClassifier.add(Activation('relu'))\nClassifier.add(MaxPooling2D(pool_size=(2, 2)))\nClassifier.add(Dropout(0.2))\n\nClassifier.add(Conv2D(64, (3, 3)))\nClassifier.add(Activation('relu'))\nClassifier.add(MaxPooling2D(pool_size=(2, 2)))\nClassifier.add(Dropout(0.5))\n\nClassifier.add(Conv2D(64, (3, 3)))\nClassifier.add(Activation('relu'))\n\n\nClassifier.add(Conv2D(80, (3, 3)))\n\nClassifier.add(Activation('relu'))\nClassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\nClassifier.add(Flatten())\nClassifier.add(Dense(64))\nClassifier.add(Activation('relu'))\nClassifier.add(Dropout(0.5))\n\nClassifier.add(Dense(3))\nClassifier.add(Activation('softmax'))\nClassifier.compile(loss='categorical_crossentropy',\n          optimizer='rmsprop',\n          metrics=['accuracy'])","e8018456":"BATCH_SIZE=8\nhistory2 = Classifier.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples \/\/ BATCH_SIZE,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ BATCH_SIZE,\n    epochs = 80)","8c5fd737":"plt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train.', 'Valid.'], loc='upper left')\nplt.show()\n\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f866b9d7":"score = Classifier.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","5dde3ebd":"from keras.models import model_from_json\n\nmodel_json = Classifier.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","6c0a426b":"Classifier.save_weights('Classifier.h5')","e9e220ac":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","fc6a7865":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16,InceptionV3\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split","c62c4a88":"baseModel = InceptionV3(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n#construct the head of the model that will be placed on top of the\n#the base model\n\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(4, 4))(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(64, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(3, activation=\"softmax\")(headModel)\n\n\nmodel3 = Model(inputs=baseModel.input, outputs=headModel)\n\nfor layer in baseModel.layers:\n    layer.trainable = False","729d3186":" # compile our model\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=1e-3, decay=1e-3 \/ EPOCHS)\nmodel3.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","0b88d5e3":"BATCH_SIZE=8\nhistory = model3.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples \/\/ BATCH_SIZE,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ BATCH_SIZE,\n    epochs = 50)","dfdec817":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train.', 'Valid.'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","0f0c075d":"model = Sequential()\nmodel.add(Conv2D(80, (3, 3), input_shape=INPUT_SHAPE))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(BatchNormalization())\n\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))","06045e0a":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n#model.compile(Adam(lr=0.0001),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","9620ccae":"\nhistory=model.fit(X_train, train_y,\n                  batch_size=8, \n                  epochs=20,\n                  validation_data=(X_test, y_test))","3d8ad40c":"from keras.utils.np.utils import to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","d0aea1ae":"train_y=tf.keras.utils.to_categorical(y_train)","6ed6c7ed":"ytest=tf.keras.utils.to_categorical(y_test)","c25fac7c":"X_test,ytest=next(validation_generator)","fdb98b4e":"import warnings\nwarnings.filterwarnings(\"ignore\")","8e21d12c":"import pandas as pd","c0e63f51":"import random","43a6f088":"y_test1","3b2a9583":"import numpy","123847f5":"x_test.shape","298fc3e1":"labelsFaces =['COVID-19', 'Normal',\"pneumonia\"]\n\n \n\npredictedExpression = model.predict(x_test)\n\nfigure = plt.figure(figsize=(20, 8))\n\nfor i, index in enumerate(numpy.random.choice(x_test.shape[0], size=16, replace=False)):\n    ax = figure.add_subplot(5, 10, i + 1, xticks=[], yticks=[])\n    # Display each image\n    ax.imshow(numpy.squeeze(x_test[index]))\n    predict_index = numpy.argmax(predictedExpression[index])\n    true_index = numpy.argmax(y_test[index])\n    # Set the title for each image\n    ax.set_title(\"{} ({})\".format(labelsFaces[predict_index], \n                                  labelsFaces[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))\nplt.show()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2e8a35b5":" labelsFaces =['COVID-19', 'Normal',\"pneumonia\"]\n\n \n\npredictedExpression = Classifier.predict(x_test)\n\nfigure = plt.figure(figsize=(20, 8))\n\nfor i, index in enumerate(numpy.random.choice(x_test.shape[0], size=16, replace=False)):\n    ax = figure.add_subplot(5, 10, i + 1, xticks=[], yticks=[])\n    # Display each image\n    ax.imshow(numpy.squeeze(x_test[index]))\n    predict_index = numpy.argmax(predictedExpression[index])\n    true_index = numpy.argmax(y_test[index])\n    # Set the title for each image\n    ax.set_title(\"{} ({})\".format(labelsFaces[predict_index], \n                                  labelsFaces[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))\nplt.show()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b7f38160":"predictedExpression","3cab1613":"path = \"..\/input\/covid-19-x-ray-10000-images\/dataset\"","cdef5a8e":"# Preparing the dataset 2","ba3764c5":"LAST","cffa76c9":"# This kernel uses All COVID-19 images availble on Kaggle # Multi image source\n**This is a kernel that shows you how to Apply Conv Nets to classify COVID-19 scans**\n","7ceb9c61":"# Preparing dataset  3  chest-xray-pneumonia**","9eaed0d8":"# Transfer learning","97aa57a8":"Copying the files from dataset 2 to dataset\/covid and to Normal","a8548a03":"# Preparing the dataset 1","2aa31d73":"# Preparing dataset  4"}}