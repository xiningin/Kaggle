{"cell_type":{"a9938e39":"code","0f17de78":"code","3ba33258":"code","e81e192f":"code","9897d27b":"code","6a74f8e3":"code","2fac22a8":"code","827d02c8":"code","63afa3ac":"code","ac40b2a5":"code","504f4988":"code","cd54d875":"code","e1352aec":"code","bcf7457f":"code","d6f412cd":"code","7f39adb7":"code","00abb5ad":"code","fcfd3a47":"code","0aaece87":"code","cd28843b":"code","18eb0c99":"markdown","b071f8a4":"markdown","26b318f2":"markdown","fc5bf26f":"markdown","5e749a3f":"markdown","63d4a9ad":"markdown","f0af8839":"markdown","69f44a8b":"markdown","8a99acd2":"markdown","ba71bb62":"markdown","02f35f31":"markdown"},"source":{"a9938e39":"import tensorflow as tf\nprint(tf.test.gpu_device_name())\n\nimport os\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.image import extract_patches_2d\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('Agg')     \n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np","0f17de78":"(X_train, y_train), (X_ttest, y_ttest) = mnist.load_data()","3ba33258":"'''\nLet's log the training set shape.\n\nThe first column indicates that we have 60 000 training samples\nand the two last columns indicate that the dimension of each sample is (h, w) <=> (28, 28).\n'''\n\nX_train.shape","e81e192f":"'''\nAs we can see, we've got 60000 samples in our training set and 10000 samples in our testing set.\nEach image, or sample, or datapoint has a dimension of (28, 28) and belongs\nto the gray-scale color-space, so their depth equals 1.\n'''\n\nplt.imshow(X_train[0])\nplt.title(label='Image at position 0')\nplt.show()","9897d27b":"'''\nAs you can see, images in the MNIST dataset are heavily pre-processed.\nThat's why this is used as a benchmark dataset and getting very high accuracy is common.\nIn a real-world problem, we'll need to do some image pre-processing to enchance images\nand extract meaningful features.\n'''\n\nfig, ax = plt.subplots(\n    nrows=6,\n    ncols=5,\n    figsize=[6, 8]\n)\n\nfor index, axi in enumerate(ax.flat):\n    axi.imshow(X_train[index])\n    axi.set_title(f'Image #{index}')\n\nplt.tight_layout(True)\nplt.show()","6a74f8e3":"'''\nNormalization of pixel intensities is adjusting values measured on different scales to a notionally common scale.\nThat's a best practice you have to follow because weights reach optimum values faster.\nTherefore, the network converges faster.\n\nSo, instead of having pixel intensities in the range [0, 255] in the gray-scale color-space,\nwe're going to scale them into the range [0, 1].\nThere are many normalization techniques and this is one of them.\n'''\n\n(X_train, X_ttest) = (child.astype('float32') \/ 255.0 for child in [X_train, X_ttest])","2fac22a8":"'''\nReshape the both training and testing set that way \nthe number of samples is the first entry in the matrix,\nthe single channel as the second entry,\nfollowed by the number of rows and columns.\n\n(num_samples, rows, columns, channel)\n'''\n\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_ttest = X_ttest.reshape(-1, 28, 28, 1)\n\n# Let's log the new shape.\nX_train.shape","827d02c8":"'''\nThere are 60000 integers labels for the training set,\neach one corresponding to one single datapoint.\nThat means, for a given Xi datapoint we got a Yi label in the range [0, 9].\nFor instance, the datapoint at the index 59995 of our training set has the label 8.\n'''\n\npd.DataFrame(y_train)","63afa3ac":"'''\nNow, our previous integer labels are converted to vector labels.\nThis process is called one-hot encoding and most of the machine learning algorithms\nbenefit from this label representation. 2 = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n8 = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0].\n\nWe could also use the to_categorical() function from Keras which yields the exact same values.\n'''\n\nlb = LabelBinarizer()\n(y_train, y_ttest) = (lb.fit_transform(labels) for labels in [y_train, y_ttest])\npd.DataFrame(y_train)","ac40b2a5":"'''\nLet's apply some data augmentation.\n\nData augmentation is a set of techniques used to generate new training samples from the original ones\nby applying jitters and perturbations such that the classes labels are not changed.\nIn the context of computer vision, these random transformations can be translating,\nrotating, scaling, shearing, flipping etc.\n\nData augmentation is a form of regularization because the training algorithm is being\nconstantly presented with new training samples,\nallowing it to learn more robust and discriminative patterns\nand reducing overfitting.\n'''\n\ndaug = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range = 0.1, \n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False\n)","504f4988":"'''\nHere, we've imported our CNN.\n\nThat's a small VGG-like net, with two stacks of (Conv => ReLU => BN) * 2 => POOL => DO\nand a Fully-Connected layer at the end.\nPay attention to Batch Normalization and Dropout layers\nwhich help to reduce overfitting.\n'''\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n\nclass CustomNet(object):\n    @staticmethod\n    def build(width, height, num_classes, depth=3):\n        model = Sequential()\n        input_shape = (height, width, depth)\n        chan_dim = -1\n        \n        # (Conv => ReLU => BN) * 3 => POOL\n        model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        \n         # (Conv => ReLU => BN) * 3 => POOL => DO\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(Conv2D(128, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization(axis=chan_dim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # FC => ReLU => BN => DO\n        model.add(Flatten())\n        model.add(Dense(256))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        \n        # Softmax\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n        \n        print(model.summary())\n        \n        return model","cd54d875":"net = CustomNet()\n\nmodel = net.build(\n    width=28,\n    height=28,\n    num_classes=10,\n    depth=1)","e1352aec":"'''\nWhen the model has seen all of your training samples, we say that one epoch has passed.\nWe're going to train the model for 50 epochs.\n'''\n\nnum_epochs = 50","bcf7457f":"'''\nLet's use an optimization method.\n\nOptimization algorithms are the engines that power neural networks and\nenable them to learn patterns from data by tweaking and seeking for optimal weights values.\nMost common one is the (Stochastic) Gradient Descent, but I'll use Adam here.\n\nAs you can see, the first param is lr, or learning rate.\nThis is one of the most important hyperparameters we have to tune.\nA learning rate is the step your optimization algorithm is going to make toward\nthe direction that leads to a lower loss function (and a higher accuracy).\n\nIf the learning rate is too small, the algorithm is going to make tiny steps slowing down the process.\nBut on the other hand, if the learning rate is too high,\nthe algorithm risks to bounce around the loss landscape and not actually \u201clearn\u201d any patterns from your data.\n'''\n\n# Initial learning rate\ninit_lr = 0.001\n\nadam_opt = Adam(\n    lr=init_lr,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-08,\n    decay=0.0\n)\n\n'''\nLet's now define a learn-rate scheduler.\n\nThe decay is used to slowly reduce the learning rate over time.\nDecaying the learning rate is helpful in reducing overfitting \nand obtaining higher classification accuracy \u2013 the smaller the learning rate is, \nthe smaller the weight updates will be. \nWe're going to use a polynomial decay. \nAlthough there are many way to do that.\n'''\n\ndef polynomial_decay(epoch):\n    max_epochs = num_epochs\n    base_lr = init_lr\n    power = 2.0\n    \n    return base_lr * (1 - (epoch \/ float(max_epochs))) ** power","d6f412cd":"# Let's plot it.\n\nx = np.linspace(0, num_epochs)\nfx = [init_lr * (1 - (i \/ float(num_epochs))) ** 2.0 for i in range(len(x))]\nplt.plot(x, fx)\nplt.title(label='Polynomial decay, power 2')\nplt.show()","7f39adb7":"'''\nHere, we define two callbacks.\n\nCallbacks are functions executed at the end of an epoch.\nThe first one save our model (checkpoint) whenever the loss decreases (therefore our accuracy improves).\nThat way we keep the best model. The last one is our learning rate scheduler using the polynomial decay.\n'''\n\n'''\ncheckpointHandler = ModelCheckpoint(\n    os.path.join(base_dir, 'best_c10_weights.hdf5'),\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n'''\n\ncallbacks = [\n    LearningRateScheduler(polynomial_decay)\n    # checkpointHandler\n]","00abb5ad":"batch_size = 128\n\nprint('# Compiling the model...')\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=adam_opt,\n    metrics=['accuracy']\n)\n\nprint('# Training the network...')\nh = model.fit_generator(\n    daug.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_ttest, y_ttest),\n    epochs=num_epochs,\n    steps_per_epoch=len(X_train) \/\/ batch_size,\n    callbacks=callbacks,\n    verbose=1\n)","fcfd3a47":"label_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nprint('Confusion matrix:')\npreds = model.predict(X_ttest, batch_size=batch_size)\nprint(classification_report(y_ttest.argmax(axis=1),\npreds.argmax(axis=1), target_names=label_names))","0aaece87":"# Loss\nplt.figure(figsize=(8, 5))\nplt.plot(np.arange(0, num_epochs), h.history['loss'], label='train_loss')\nplt.plot(np.arange(0, num_epochs), h.history['val_loss'], label='val_loss')\nplt.title('Loss')\nplt.legend()\nplt.show()\n\n# Accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(np.arange(0, num_epochs), h.history['accuracy'], label='train_acc')\nplt.plot(np.arange(0, num_epochs), h.history['val_accuracy'], label='val_acc')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","cd28843b":"'''\nPushing a submission to Kaggle.\n\nFirst of, load the test set, normalize it and reshape it.\nThen, make predictions via the trained model and build the\nsubmission csv file.\n'''\n\nsub_X_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')   # Load CSV\nsub_X_test = sub_X_test.iloc[:,:].values                         # Get raw pixel intensities\nsub_X_test = sub_X_test.reshape(sub_X_test.shape[0], 28, 28, 1)  # Reshape to meet Keras requirements\nsub_X_test = sub_X_test \/ 255.0                                  # Normalize to range [0, 1]\n# Get predictions\npreds = model.predict_classes(sub_X_test, batch_size=batch_size)\n# Generate a submission file\nid_col = np.arange(1, preds.shape[0] + 1)\nsubmission = pd.DataFrame({'ImageId': id_col, 'Label': preds})                                         # Shift index\nprint(pd.DataFrame(submission))\nsubmission.to_csv('submission.csv', index = False)","18eb0c99":"# Build the model","b071f8a4":"# Load the MNIST dataset from Keras","26b318f2":"# Evaluate","fc5bf26f":"# Training time!","5e749a3f":"# Normalize pixel intensities to range [0, 1]","63d4a9ad":"# Data Augmentation","f0af8839":"# Visualize curves","69f44a8b":"# One-Hot Encode labels\n","8a99acd2":"# Reshape datapoints","ba71bb62":"# Kaggle","02f35f31":"# Set the optimizer and hyperparameters"}}