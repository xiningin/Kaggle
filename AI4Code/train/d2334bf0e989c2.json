{"cell_type":{"1a8619c1":"code","aec3a6b5":"code","e35dbea6":"code","42518977":"code","ff1c22ee":"code","1d449ee1":"code","c34b9eb2":"code","55b7b865":"code","34e1010f":"code","debabe3c":"code","524a9be7":"code","dbaa2309":"code","412fb776":"code","aced4890":"code","553f9dfe":"code","1e4cf76e":"code","059bf606":"code","0dc91733":"code","fe955587":"code","205cedf9":"code","dcc41a92":"code","559cedc2":"code","6b5b5ce1":"code","76811c80":"code","3cb10134":"code","7726ddc4":"code","3f5fef37":"code","3405cd83":"code","661faa4b":"code","02115dd4":"code","6f8daaae":"code","1e8b9efc":"code","f73276eb":"code","6a6cf957":"code","e7092fb6":"code","e66f45ee":"code","ba7cfd5c":"code","28c4832b":"code","2025509b":"code","15e84e9e":"code","24410d47":"code","35784876":"code","d2663c65":"code","f19d78f4":"code","05fb0d05":"code","1dc692ae":"code","825de37e":"code","d0337195":"code","6ddb5952":"code","b8b0530c":"code","79bfe2c6":"code","5bff2470":"code","b3985725":"markdown","9d90b0bf":"markdown","9bc8589e":"markdown","0ba2696b":"markdown","4e2b8358":"markdown","228f14bc":"markdown","860760cf":"markdown","9096bfa5":"markdown","54d45bb6":"markdown","e4f19910":"markdown"},"source":{"1a8619c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aec3a6b5":"import warnings\nwarnings.filterwarnings('ignore')","e35dbea6":"df=pd.read_csv('\/kaggle\/input\/top-beer-information\/beer_data_set.csv')","42518977":"df","ff1c22ee":"import matplotlib.pyplot as plt\nimport seaborn as sns","1d449ee1":"df.isnull().sum()","c34b9eb2":"sns.heatmap(df.corr())","55b7b865":"df.describe()","34e1010f":"df.hist(figsize=(15,15))","debabe3c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge,LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","524a9be7":"x=df.drop(['Name', 'key', 'Style','Brewery','Description','Ave Rating'],axis=1)\ny=df['Ave Rating']","dbaa2309":"x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","412fb776":"model_lr=LinearRegression()\nmodel_lr.fit(x_train,y_train)\npred_train=model_lr.predict(x_train)\npred_test=model_lr.predict(x_test)\nprint(\"train_RMSE:\",np.sqrt(mean_squared_error(y_train, pred_train)))\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred_test)))\nprint(\"train_MAE:\",mean_absolute_error(y_train, pred_train))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred_test))\nprint(\"R^2:{}\".format(model_lr.score(x_test, y_test)))","aced4890":"from scipy.stats import skew","553f9dfe":"AP=pd.DataFrame({'ap':y,'log(ap+1)':np.log1p(y)})\nprint(AP, '\u00a5n')\n\nprint('ap skew        :',skew(AP['ap']))\nprint('log(ap+1) skew:', skew(AP['log(ap+1)']))\n\nAP.hist()","1e4cf76e":"x_skew=x.apply(lambda x:skew(x))\nprint(x_skew)","059bf606":"x_skew = x_skew[x_skew > 0.75]\nprint('-----Skewness greater than 0.75-----')\nprint(x_skew)\nx_skew = x_skew.index\n\nx[x_skew] = np.log1p(x[x_skew])\nx[x_skew]","0dc91733":"x","fe955587":"x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","205cedf9":"model_lr=LinearRegression()","dcc41a92":"model_lr.fit(x_train,y_train)\npred_train=model_lr.predict(x_train)\npred_test=model_lr.predict(x_test)\nprint(\"train_RMSE:\",np.sqrt(mean_squared_error(y_train, pred_train)))\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred_test)))\nprint(\"train_MAE:\",mean_absolute_error(y_train, pred_train))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred_test))\nprint(\"R^2:{}\".format(model_lr.score(x_test, y_test)))","559cedc2":"x=df.drop(['Name', 'key', 'Style','Brewery','Description','Ave Rating'],axis=1)\ny=df['Ave Rating']\nx_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","6b5b5ce1":"def rmse_cv(model):\n    rmse = np.sqrt(\n        -cross_val_score(\n            model, x_train, y_train,\n            scoring=\"neg_mean_squared_error\", \n            cv = 5))\n    return(rmse)","76811c80":"model_rg = Ridge()\n\nalphas = [3.2,3.4, 3.6, 3.8,4.0,4.2]\ncv_rg = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_rg = pd.Series(cv_rg, index = alphas)\n\nprint('Ridge RMSE loss:')\nprint(cv_rg, '\\n')\n\nprint('Ridge RMSE loss Mean:')\nprint(cv_rg.mean())\n\n\nplt.\ufb01gure(\ufb01gsize=(10, 5))\nplt.plot(cv_rg)\nplt.grid()\nplt.title('Validation - by regularization strength')\nplt.xlabel('Alpha')\nplt.ylabel('RMSE')\nplt.show()","3cb10134":"model_rg.fit(x_train,y_train)\npred1=model_rg.predict(x_test)\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred1)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred1))\nprint(\"R^2:{}\".format(model_rg.score(x_test, y_test)))","7726ddc4":"model_ls = LassoCV(\n    alphas = [1, 0.1, 0.001, 0.0005]).fit(x_train, y_train)\n\nprint('Lasso regression RMSE loss:')\nprint(rmse_cv(model_ls))\n\nprint('Average loss:', rmse_cv(model_ls).mean())\nprint('Minimum loss:', rmse_cv(model_ls).min())\nprint('Best alpha  :', model_ls.alpha_) \n","3f5fef37":"model_ls.fit(x_train,y_train)\npred2=model_ls.predict(x_test)\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred2)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred2))\nprint(\"R^2:{}\".format(model_ls.score(x_test, y_test)))","3405cd83":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(x_train, label = y_train)\n\nparams = {\"max_depth\":3, \"eta\":0.1}\n\ncross_val = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    early_stopping_rounds=50)\ncross_val","661faa4b":"plt.\ufb01gure(\ufb01gsize=(8, 6))\nplt.plot(cross_val.loc[0:,[\"test-rmse-mean\", \"train-rmse-mean\"]])\nplt.grid()\nplt.xlabel('num_boost_round')\nplt.ylabel('RMSE')\nplt.show()","02115dd4":"model_xgb = xgb.XGBRegressor(\n    n_estimators=325,\n    max_depth=3,\n    learning_rate=0.1)\nmodel_xgb.fit(x_train, y_train)\npred3=model_xgb.predict(x_test)\n\nprint('xgboost RMSE loss:')\nprint(rmse_cv(model_xgb).mean())\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred3)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred3))\nprint(\"R^2:{}\".format(model_xgb.score(x_test, y_test)))","6f8daaae":"dtrain = xgb.DMatrix(x_train, label = y_train)\ndtest=xgb.DMatrix(x_test, label = y_test)","1e8b9efc":"base_params = {\n    'booster': 'gbtree',\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n}\n\nwatchlist = [(dtrain, 'train'), (dtest, 'eval')]","f73276eb":"import optuna\nfrom sklearn.metrics import r2_score\nimport copy\n\ntmp_params = copy.deepcopy(base_params)\n\ndef optimizer(trial):\n    eta = trial.suggest_uniform('eta', 0.01, 0.3)\n    max_depth = trial.suggest_int('max_depth', 4, 15)\n    __lambda = trial.suggest_uniform('lambda', 0.7, 2)\n\n    tmp_params['eta'] = eta\n    tmp_params['max_depth'] = max_depth\n    tmp_params['lambda'] = __lambda\n\n    model = xgb.train(tmp_params, dtrain, num_boost_round=50)\n    predicts = model.predict(dtest)\n\n    r2 = r2_score(y_test, predicts)\n    print(f'#{trial.number}, Result: {r2}, {trial.params}')\n\n    return r2","6a6cf957":"study = optuna.create_study(direction='maximize')\nstudy.optimize(optimizer, n_trials=500)","e7092fb6":"study.best_params","e66f45ee":"paramas=study.best_params","ba7cfd5c":"from sklearn.metrics import r2_score\n\ndef eval_model(params, dtrain, dtest):\n    model = xgb.train(params, dtrain, num_boost_round=100, verbose_eval=False, evals=watchlist)\n    predicts = model.predict(dtest)\n    r2 = r2_score(y_test, predicts)\n\n    return r2","28c4832b":"base_r2 = eval_model(base_params, dtrain, dtest)\n\nmerged_params = dict(base_params, **study.best_params)\nbest_r2 = eval_model(merged_params, dtrain, dtest)\n\nprint(f'Base params: {base_params}')\nprint(f'Best params: {merged_params}')\nprint(f'Base: {base_r2}, Best: {best_r2}, Diff: {best_r2 - base_r2}')","2025509b":"from catboost import CatBoostRegressor, FeaturesData, Pool","15e84e9e":"categorical_features_indices = np.where(x.dtypes != np.float)[0]","24410d47":"model_cat = CatBoostRegressor(iterations=2000, learning_rate=0.05, depth=5)\nmodel_cat.fit(x_train, y_train)\npred4=model_cat.predict(x_test)\n\nprint('catboost RMSE loss:')\nprint(rmse_cv(model_cat).mean())\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred4)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred4))\nprint(\"R^2:{}\".format(model_cat.score(x_test, y_test)))","35784876":"df1=df.drop(['Name', 'Style','Brewery','Description','key'], axis=1)","d2663c65":"from sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(df1)","f19d78f4":"df1_2d = pca.transform(df1)\n\ndf1_2d = pd.DataFrame(df1_2d)\ndf1_2d.index = df1.index\ndf1_2d.columns = ['PC1','PC2','PC3','PC4']\ndf1_2d.head()","05fb0d05":"pca.explained_variance_ratio_","1dc692ae":"components=pd.DataFrame(pca.components_)\ncomponents.columns=df1.columns\ncomponents.index=['PCA1','PCA2','PCA3','PCA4']\ncomponents","825de37e":"components.plot.bar(figsize=(15,8))\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')","d0337195":"from sklearn.cluster import KMeans   \nkmeans = KMeans(n_clusters=4)\nclusters = kmeans.fit(df1)\n\ndf1_2d['cluster'] = pd.Series(clusters.labels_, index=df1_2d.index)","6ddb5952":"df1_2d.plot(\n        kind='scatter',\n        x='PC2',y='PC1',\n        c=df1_2d.cluster.astype(np.float),\n        figsize=(16,8))","b8b0530c":"df1['cluster']=df1_2d['cluster']\ndf1","79bfe2c6":"df1.groupby('cluster')['cluster'].count().plot.pie(figsize=(8,8),autopct=\"%.1f%%\")","5bff2470":"df1.groupby('cluster').mean()","b3985725":"Last one is PCA and Clustering.","9d90b0bf":"Second, I tried Ridge and LassoCV.","9bc8589e":"Third, I tried XGBoost.","0ba2696b":"The Logarithmic is not usefull to improve in LinerRegression.","4e2b8358":"First, I tried LinerRegression and also tried Logarithmic of features.","228f14bc":"I tried to tune the parameters, but I could not improve the results.\nBut by Catboost, I got MSE 0.26, MAE 0.19 and R2 0.66.","860760cf":"Ridge and LassoCV were not usefull to improve.","9096bfa5":"Optimization of Pamaeterss.","54d45bb6":"The features which skews are over 0.75 should be logarithmic.","e4f19910":"Next, I tried  CatBoost."}}