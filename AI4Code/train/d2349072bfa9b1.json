{"cell_type":{"ac7ea0b5":"code","5d3347f9":"code","1a874f12":"code","8263bda6":"code","6e891d6a":"code","a37feed1":"code","92c4f772":"code","3e87df93":"code","7ee82a0a":"code","1a930727":"code","3ffebad7":"code","f79d27a1":"code","b02fc234":"code","87038f63":"code","cdfbb06f":"code","b633b4e3":"code","013e19fa":"code","4f4495e5":"code","8f346690":"code","13e5504d":"code","59b3b732":"code","7fd983ff":"code","668681ff":"code","0d12dbeb":"code","190a3117":"code","71150fcd":"code","d84ea96a":"code","ff305e74":"code","df80344c":"code","0aaf45c5":"code","725b7fda":"code","1238634e":"code","0a9e1c8b":"markdown","13d671f4":"markdown","b07cbfed":"markdown","a56852a0":"markdown","906afc9b":"markdown"},"source":{"ac7ea0b5":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import patches\nimport imageio\n\n#from tqdm import tqdm_notebook as tqdm\n#from tqdm import tqdm \nfrom tqdm.notebook import tqdm as tqdm\n\nimport cv2\nimport os\nimport re\n\nimport random\nimport subprocess\n\nfrom PIL import Image\nfrom IPython.display import Video, display\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nimport ast\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler","5d3347f9":"DATA_PATH = '..\/input\/nfl-impact-detection'\nim_path = '..\/input\/nfl-impact-detection\/images'\n\ntrain_label = pd.read_csv(\"..\/input\/nfl-impact-detection\/train_labels.csv\")","1a874f12":"img_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/image_labels.csv')\nimg_labels.head()","8263bda6":"img_name = img_labels['image'][50]\nimg_name","6e891d6a":"img_path = f\"\/kaggle\/input\/nfl-impact-detection\/images\/{img_name}\"","a37feed1":"# Read it and plot the image\nimg = imageio.imread(img_path)\nplt.figure(figsize=(20,10))\nplt.imshow(img)\nplt.show()\n","92c4f772":"def add_img_boxes(image_name, image_labels):\n    # Label colors for bounding boxes\n    HELMET_COLOR = (255,0,0)  # Red\n    \n    boxes = img_labels.loc[img_labels['image'] == img_name]\n    for j, box in boxes.iterrows():\n        color = HELMET_COLOR\n        cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness = 2)\n        \n    plt.figure(figsize=(20,10))\n    plt.imshow(img)\n    plt.show()\n    ","3e87df93":"add_img_boxes(img_name, img_labels)","7ee82a0a":"# Read the data from the video label file\nvideo_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/train_labels.csv')\nvideo_labels.head()","1a930727":"video_name = video_labels['video'][100]\nvideo_name","3ffebad7":"video_path = f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","f79d27a1":"def annotate_video(video_path: str, video_labels: pd.DataFrame) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (155, 0, 0)    #Red\n    IMPACT_COLOR = (0, 0, 0)  # Black\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        \n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    \n                color, thickness = IMPACT_COLOR, 2\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=2)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","b02fc234":"labeled_video = annotate_video(f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\", video_labels)\ndisplay(Video(data=labeled_video, embed=True))","87038f63":"class NFLDataset(object):\n    \n    def __init__(self, root_path):\n        self.root_path = root_path\n        self.images_list = os.listdir(os.path.join(root_path, 'images'))\n        self.images_df = pd.read_csv(os.path.join(root_path, 'image_labels.csv'))\n        self.labels_dict = {'Helmet': 1,\n                           'Helmet-Blurred': 2,\n                           'Helmet-Difficult': 3,\n                           'Helmet-Sideline': 4,\n                           'Helmet-Partial': 5}\n        \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_path, 'images', self.images_list[idx])\n        img = np.array(Image.open(img_path)) \/ 255\n        img = np.moveaxis(img, 2, 0) # to [C, H, W]\n        \n        # Collect data about boxes and helmet labels from `image_labels.csv`\n        img_data_df = self.images_df[self.images_df['image'] == self.images_list[idx]]     \n        n_bboxes = img_data_df.shape[0]\n        bboxes = []\n        labels = []\n        for i in range(n_bboxes):\n            img_data = img_data_df.iloc[i]\n            x_min = img_data.left\n            x_max = img_data.left + img_data.width\n            y_min = img_data.top\n            y_max = img_data.top + img_data.height\n            bboxes.append([x_min, y_min, x_max, y_max])\n            label = self.labels_dict[img_data.label]\n            labels.append(label)\n         \n        # Convert data to tensors\n        img = torch.as_tensor(img, dtype=torch.float32)    \n        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        \n        target = {}\n        target['boxes'] = bboxes\n        target['labels'] = labels\n        target['image_id'] = image_id\n        \n        return img, target\n    \n    def __len__(self):\n        return len(self.images_list)","cdfbb06f":"#albumentation \ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","b633b4e3":"# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 6  \n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nmodel","013e19fa":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndataset = NFLDataset(DATA_PATH)\n\nindices = torch.randperm(len(dataset)).tolist()\ntrain_cnt = int(0.9*len(indices))\n\ntrain_dataset = torch.utils.data.Subset(dataset, indices[:train_cnt])\nvalid_dataset = torch.utils.data.Subset(dataset, indices[train_cnt:])\n\ntrain_data_loader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size = 8,\n                shuffle = False,\n                collate_fn = collate_fn)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n                valid_dataset,\n                batch_size = 8,\n                shuffle = False,\n                collate_fn = collate_fn)","4f4495e5":"train_dataset","8f346690":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","13e5504d":"torch.cuda.empty_cache()\ntorch.cuda.memory_summary(device=None, abbreviated=False)","59b3b732":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\n","7fd983ff":"def forward_train(model, data, device):\n    imgs, targets = data\n    imgs = [image.to(device) for image in imgs]\n    targets = [{k: v.to(device) for k, v in tgt.items()} for tgt in targets]\n    \n    loss_dict = model(imgs, targets) \n    losses = sum(loss for loss in loss_dict.values())\n    \n    return losses","668681ff":"N_ITERS = 100\n\n\nprogress_bar = tqdm(range(N_ITERS))\ntr_it = iter(train_data_loader)\nloss_log = []\niterations = []\n\nfor i in progress_bar:\n    try:\n        data = next(tr_it)\n    except StopIteration:\n        tr_it = iter(train_data_loader)\n        data = next(tr_it)\n    model.train()\n    torch.set_grad_enabled(True)\n    imgs, targets = data\n    imgs = [image.to(device) for image in imgs]\n    targets = [{k: v.to(device) for k, v in tgt.items()} for tgt in targets]\n    loss_dict = model(imgs, targets)\n    losses = sum(loss for loss in loss_dict.values())\n    \n    \n    optimizer.zero_grad()\n    losses.backward()\n    optimizer.step()\n        \n    loss_log.append(losses.item())\n    iterations.append(i)\n    progress_bar.set_description(f'batch loss: {losses.item()}, average loss: {np.mean(loss_log)}.')\n    ","0d12dbeb":"plt.plot(iterations, loss_log)\nplt.show()","190a3117":"valid_data_loader = torch.utils.data.DataLoader(\n                valid_dataset,\n                batch_size = 8,\n                shuffle = False,\n                collate_fn = collate_fn)","71150fcd":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for s, b in zip(scores, boxes.astype(int)):\n        pred_strings.append(f'{s:.4f} {b[0]} {b[1]} {b[2] - b[0]} {b[3] - b[1]}')\n\n    return \" \".join(pred_strings)","d84ea96a":"detection_threshold = 0.5\nresults = []\ndevice = 'cuda'\nmodel.eval()\nfor images, image_ids in valid_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)","ff305e74":"def plot_detected_bboxes(test_img, predictions, n_to_plot=2, score_threshold=0.6):\n    \n    n = min(len(test_img), n_to_plot)\n    \n    fig, ax = plt.subplots(1, n, figsize=(20, 8))\n    \n    for i in range(n):\n        img = np.asarray(test_img[i].cpu().numpy() * 255, dtype=np.int64)\n        img = np.moveaxis(img, 0, 2)\n        img = Image.fromarray(np.uint8(img)).convert('RGB')\n        ax[i].imshow(img)\n        ax[i].set_axis_off()\n\n        bboxes = predictions[i]['boxes'].cpu().numpy()\n        scores = predictions[i]['scores'].cpu().numpy()\n        scores_mask = scores > score_threshold\n        for bbox in bboxes[scores_mask]:\n            patch = patches.Rectangle(\n                (bbox[0], bbox[1]),\n                bbox[2] - bbox[0], bbox[3] - bbox[1],\n                linewidth=2,\n                edgecolor='r',\n                facecolor='None',\n                alpha=0.8)\n            ax[i].add_patch(patch)  \n        \n    fig.tight_layout()\n    return ","df80344c":"model.eval()\ntorch.set_grad_enabled(False)\n\ntest_it = iter(valid_data_loader)","0aaf45c5":"test_img, test_gt  = next(test_it)\ntest_img = [image.to(device) for image in test_img]\n\npredictions = model(test_img)\n\nplot_detected_bboxes(test_img, predictions,\n                     n_to_plot=2,\n                     score_threshold=0.6)","725b7fda":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","1238634e":"test_df.to_csv('submission.csv', index=False)","0a9e1c8b":"# Video Data","13d671f4":"# Image Data Overview","b07cbfed":"# Model Faster RCNN","a56852a0":"**Develope a function to add bounding boxes to every frame in the video**","906afc9b":"**Function for adding the bounding boxes from label to image**"}}