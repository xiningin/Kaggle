{"cell_type":{"eafeaae4":"code","48c653a2":"code","49d0e35d":"code","acd62f63":"code","2cdbaca2":"code","15ba7e86":"code","93750961":"code","686fd4a2":"code","f38ec81c":"code","d6e1c34e":"code","e6949679":"code","2f64bf97":"code","ca403ff0":"code","07b22ebb":"code","57a731a7":"code","51e127d1":"code","9ccec180":"code","4c790c36":"code","3754e258":"code","20fdb2c6":"code","7f515c00":"code","9316a792":"code","1d4fb5b6":"code","0008f49c":"code","87a0e3ba":"code","275d113b":"code","64bf9c75":"code","e906c7a2":"code","8416bd46":"code","3666f33e":"code","ae83673f":"code","a763c0ff":"code","49347f22":"code","8fbcd571":"code","5b65cbb4":"code","d85da269":"code","ce44ad93":"code","e1cf2e74":"code","b9a07a6d":"code","9e62719d":"code","cc2e5917":"code","dc1d2d30":"code","d8887fe5":"code","79e92e5e":"code","41d74e94":"code","46a69aab":"code","98a61fa2":"code","66741b39":"code","1c7adb7d":"code","9ec39e31":"code","e7d49254":"code","86ed5e20":"code","c11970ec":"code","233c4a62":"code","728b6974":"code","94c7ba08":"code","b34bba49":"code","56d22c64":"code","31aa5ee7":"code","136682a7":"code","c548c223":"code","50c29031":"code","40a49d5f":"code","3c666601":"code","e8922226":"code","bc5e6785":"code","9624faec":"code","6ccea86d":"code","05a61885":"code","78a5a925":"code","eb59b485":"markdown","4fddd08c":"markdown","9e5bb7c0":"markdown","e91488eb":"markdown","30e64d2e":"markdown","a4bac9f3":"markdown","35355e97":"markdown","68281cca":"markdown","3567ded5":"markdown","d7263214":"markdown","4612eef5":"markdown","74e4beb7":"markdown","3da12b27":"markdown"},"source":{"eafeaae4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48c653a2":"#here we are combining three sets of data, to make one dataset containing all the UK accidents between 2005 and 2012\n\ndata_2005 = pd.read_csv(\"..\/input\/uk-data-three-files\/accidents_2005_to_2007.csv\/accidents_2005_to_2007.csv\")\ndata_2009 = pd.read_csv(\"..\/input\/uk-data-three-files\/accidents_2009_to_2011.csv\/accidents_2009_to_2011.csv\")\ndata_2012 = pd.read_csv(\"..\/input\/uk-data-three-files\/accidents_2012_to_2014.csv\/accidents_2012_to_2014.csv\")\n\nframes = [data_2005, data_2009, data_2012]\ndf_complete = pd.concat(frames)\ndf_complete.head()\n\n# lets take an inital looka the data","49d0e35d":"df_complete[\"Urban_or_Rural_Area\"].value_counts()","acd62f63":"#lets make a new DF that contains just contain columns that could indicate the severity result or could \n#influcence the severity of an accident\n\n\ndf_select = df_complete[['Longitude', 'Latitude', 'Accident_Severity',\n       'Number_of_Vehicles', 'Number_of_Casualties', 'Date', 'Day_of_Week',\n       'Time','Road_Type', 'Speed_limit','Light_Conditions',\n       'Weather_Conditions', 'Road_Surface_Conditions',\n       'Special_Conditions_at_Site', 'Carriageway_Hazards',\n       'Urban_or_Rural_Area']]\ndf_select.head()\n","2cdbaca2":"#lets have a look at the data in each column\n\ndef list_values (columns, df):\n    for i in columns:\n        print(i)\n        print(df[i].value_counts())\n        print(\"number of NaN's \", df[i].isnull().sum(axis = 0))\n        print()\n        \n\n    \nlist_values(df_select.columns, df_select)\n","15ba7e86":"df_select[\"Weather_Conditions\"] = df_select[\"Weather_Conditions\"].replace({'Unknown' : np.NaN, 'Other': np.NaN})\ndf_select[\"Road_Type\"] = df_select[\"Road_Type\"].replace({'Unknown' : np.NaN})","93750961":"# i think we can tidy up weather conditions to have more bollean options and make the details more precise\n# fine | yes or no | 1 is \n# high winds | 1 is high windes\n# rain: yes or no 1 is rain\n# snow: yes or no 1 is snow 0 no snow\n\n#so lets make three new columss that we will populare with the new values\n\ndf_select[\"rain\"] = df_select[\"Weather_Conditions\"]\ndf_select[\"high_winds\"] = df_select[\"Weather_Conditions\"]\ndf_select[\"fine\"] = df_select[\"Weather_Conditions\"]\ndf_select[\"snow\"] = df_select[\"Weather_Conditions\"]\n\n","686fd4a2":"df_select[\"rain\"].value_counts()","f38ec81c":"# changing the vlaue of the columns clumn to be boolean representing their state\n# its a bit like manual one hot encoding\n\n\ndf_select[\"rain\"] = df_select[\"rain\"].replace(\n    {'Fine without high winds' : 0,\n     'Raining without high winds' : 1,\n     'Raining with high winds' : 1,\n     'Fine with high winds': 0,\n     'Snowing without high winds': 0,\n     'Fog or mist' : 0,\n     'Snowing with high winds' : 0}\n\n) \n\ndf_select[\"fine\"] = df_select[\"fine\"].replace(\n    {'Fine without high winds' : 1,\n     'Raining without high winds' : 0,\n     'Raining with high winds' : 0,\n     'Fine with high winds': 1,\n     'Snowing without high winds': 0,\n     'Fog or mist' : 0,\n     'Snowing with high winds' : 0}\n\n) \n\ndf_select[\"high_winds\"] = df_select[\"high_winds\"].replace(\n    {'Fine without high winds' : 0,\n     'Raining without high winds' : 0,\n     'Raining with high winds' : 1,\n     'Fine with high winds': 1,\n     'Snowing without high winds': 0,\n     'Fog or mist' : 0,\n     'Snowing with high winds' : 1}\n\n) \n\ndf_select[\"snow\"] = df_select[\"snow\"].replace(\n    {'Fine without high winds' : 0,\n     'Raining without high winds' : 0,\n     'Raining with high winds' : 0,\n     'Fine with high winds': 0,\n     'Snowing without high winds': 0,\n     'Fog or mist' : 0,\n     'Snowing with high winds' : 1}\n\n) \n","d6e1c34e":"#now we will drop the weatahter_conditions column\ndf_select = df_select.drop([\"Weather_Conditions\"], axis = 'columns')","e6949679":"# now we need to look at road type, according to the metadata of the dataset the data should just contain 1 or 2 values, but we have 132 insrances of 3\n#we will convert all the 3's to NaN\n\ndf_select[\"Urban_or_Rural_Area\"] = df_select[\"Urban_or_Rural_Area\"].replace({3 : np.NaN})","2f64bf97":"#we will now drop all the NaN's\n\ndf_select.dropna(inplace = True)\ndf_select.info()\n","ca403ff0":"# now we need to balance the data\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nX = df_select[['Longitude', 'Latitude', 'Day_of_Week', 'Road_Type', 'Speed_limit',\n       'Light_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area',\n       'rain', 'high_winds', 'fine','snow']]\ny = df_select[['Accident_Severity']]\n\nsampler = RandomUnderSampler(random_state=5555)\nX_bal, y_bal = sampler.fit_resample(X, y)\n\n\ndf_select = X_bal\ndf_select['Accident_Severity'] = y_bal\n\nsns.countplot(x=\"Accident_Severity\", data=df_select)\n","07b22ebb":"fig, axs = plt.subplots(ncols=3, nrows=4, figsize=(50,24))\nplt.subplots_adjust(hspace = 0.8, wspace=0.8)\n\ncolData = df_select\n\n\nsns.countplot(y=\"Day_of_Week\", hue=\"Accident_Severity\", data=colData, ax=axs[0, 0])\nsns.countplot(y=\"Road_Type\", hue=\"Accident_Severity\", data=colData, ax=axs[0, 1])\nsns.countplot(y=\"Speed_limit\", hue=\"Accident_Severity\", data=colData, ax=axs[0, 2])\nsns.countplot(y=\"Light_Conditions\", hue=\"Accident_Severity\", data=colData, ax=axs[1, 0])\nsns.countplot(y=\"Road_Surface_Conditions\", hue=\"Accident_Severity\", data=colData, ax=axs[1, 1])\n#sns.countplot(y=\"Special_Conditions_at_Site\", hue=\"Accident_Severity\", data=colData, ax=axs[1, 2])\nsns.countplot(y=\"Carriageway_Hazards\", hue=\"Accident_Severity\", data=colData, ax=axs[2, 0])\nsns.countplot(y=\"rain\", hue=\"Accident_Severity\", data=colData, ax=axs[2,1])\nsns.countplot(y=\"snow\", hue=\"Accident_Severity\", data=colData, ax=axs[2, 2])\nsns.countplot(y=\"high_winds\", hue=\"Accident_Severity\", data=colData, ax=axs[3, 0])\nsns.countplot(y=\"fine\", hue=\"Accident_Severity\", data=colData, ax=axs[3, 1])","57a731a7":"#from the above charts we can see that right away we can drop Special_Conditions_at_Site, Carriageway_Hazards, snow\n#we can also drop rain as it is just the inverse of fine and their correlation is almost perfect negative\n\ndf_select.drop(['Special_Conditions_at_Site', 'Carriageway_Hazards', 'snow','rain'], axis = 'columns', inplace = True)\n\n#we will also drop the above from our X data set\n\nX.drop(['Special_Conditions_at_Site', 'Carriageway_Hazards', 'snow','rain', 'Longitude', 'Latitude'], axis = 'columns', inplace = True)\n","51e127d1":"X.head()","9ccec180":"# example of mutual information feature selection for categorical data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\n# prepare input data\ndef prepare_inputs(X_train, X_test):\n    oe = OrdinalEncoder()\n    oe.fit(X_train)\n    X_train_enc = oe.transform(X_train)\n    X_test_enc = oe.transform(X_test)\n    return X_train_enc, X_test_enc\n \n# prepare target\ndef prepare_targets(y_train, y_test):\n    le = LabelEncoder()\n    le.fit(y_train)\n    y_train_enc = le.transform(y_train)\n    y_test_enc = le.transform(y_test)\n    return y_train_enc, y_test_enc\n \n# feature selection\ndef select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=mutual_info_classif, k='all')\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\ndef calc_entropy(X, y, n):\n    scores = []\n    result = np.zeros(len(X.columns))\n    \n    for i in range(n):\n        # split into train and test sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n        # prepare input data\n        X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n        # prepare output data\n        y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n        # feature selection\n        X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n        # what are scores for the features\n        result = fs.scores_ + result\n        \n    return result \/ n\n\n\nX = X.astype(str)\n\n\n# Change the iteration number if you want to average the results\niterations = 1\n\nresult = calc_entropy(X, y, iterations)\n\nfor i in range(len(result)):\n    print('Feature %s: %f' % (X.columns[i], result[i]))\n# Plot the scores\n\nsns.barplot(y=X.columns, x=result)\n#plt.savefig('fig\/feature_extraction.png',dpi=300, bbox_inches = \"tight\")","4c790c36":"df_select.head()\ndf_select.dropna(inplace = True)","3754e258":"print(df_select.shape)\nprint(feature_set.shape)","20fdb2c6":"# import folium\n# from folium import plugins\n\n# latitude = df_select.Latitude.mean()\n# longitude = df_select.Longitude.mean()\n\n# # create map and display it\n# map_uk = folium.Map(location=[latitude, longitude], zoom_start=12)\n\n# # instantiate a mark cluster object for the incidents in the dataframe\n# collisions = plugins.MarkerCluster().add_to(map_uk)\n\n# # loop through the dataframe and add each data point to the mark cluster\n# for lat, lng, label, in zip(df_select.Latitude, df_select.Longitude, df_select.Accident_Severity):\n#     folium.Marker(\n#         location=[lat, lng],\n#         icon=None,\n#         popup=label,\n#     ).add_to(collisions)\n\n\n# map_uk","7f515c00":"#define the feature set\nfeature_set = df_select[['Speed_limit', 'Urban_or_Rural_Area', 'Light_Conditions', 'Road_Type', 'fine','Road_Surface_Conditions']]\nfeature_set\nfeature_set.head()\n","9316a792":"\n# generate binary values using get_dummies\ndum_df = pd.get_dummies(feature_set[[\"Urban_or_Rural_Area\",\"fine\", \"Light_Conditions\",'Road_Type','Road_Surface_Conditions']])\n# merge with main df bridge_df on key values\n\nX1 = feature_set[['Speed_limit']].join(dum_df)\nX1.info()\n","1d4fb5b6":"#now we normalise the data\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n# fit and transform the data\nX1_norm = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)\n\nX1_norm","0008f49c":"X1_norm.info()","87a0e3ba":"y1 = df_select['Accident_Severity']\ny1.shape","275d113b":"X = X1\ny = y1\nprint(X.shape)\nprint(y.shape)","64bf9c75":"#lets split the data into test and train\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=6665)\nprint ('Train Set:', X_train.shape,  y_train.shape)\nprint ('Test Set:', X_test.shape,  y_test.shape)","e906c7a2":"X = X.astype('Int64')","8416bd46":"X.info()","3666f33e":"#KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nKs = 12\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat = neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\n\nprint(\"k values\", mean_acc)\nprint(\"best k accuracy value\", mean_acc.max())\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()\n\n","ae83673f":"#testing KNN model on the train set data\nyhat_train = neigh.predict(X_train)","a763c0ff":"#lets do some nice plotting of KNN points\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import datasets, neighbors\nfrom mlxtend.plotting import plot_decision_regions\n\n\n\n\ndef knn_comparison(data_knn, k):\n    x = data_knn[['weather_cat', 'roadCond_cat']].values\n    y_ = data_knn['severity'].astype(int).values\n    clf = neighbors.KNeighborsClassifier(n_neighbors=k)\n    clf.fit(x, y_)\n    \n# Plotting decision region\n    plot_decision_regions(x, y_, clf=clf, legend=2)\n    \n# Adding axis annotations\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.title(\"Knn with K=\"+ str(k))\n    plt.show()\n    \n\n\ndata_knn = df_bal_cat\nfor i in [2,8,12]:\n    knn_comparison(data_knn, i)\n\n\n","49347f22":"#decision trees\nfrom sklearn.tree import DecisionTreeClassifier\n","8fbcd571":"crashTreeEntrophy = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\ncrashTreeGini = DecisionTreeClassifier(criterion=\"gini\", max_depth = 4)\n\n\nprint(\"crashTreeEntrophy\",crashTreeEntrophy) # it shows the default parameters\nprint(\"crashTreeGini\", crashTreeGini)","5b65cbb4":"crashTreeEntrophy.fit(X_train,y_train)\ncrashTreeGini.fit(X_train,y_train)","d85da269":"predictionCrashEntrophy = crashTreeEntrophy.predict(X_test)\npredictionCrashGini = crashTreeGini.predict(X_test)\n\nDT_yhat_train = crashTreeEntrophy.predict(X_train)","ce44ad93":"print(\"entrophy model\")\nprint(\"\")\nprint (\"predicted value\",predictionCrashEntrophy [0:100])\nprint (\"actual value\", y_test [0:100])\nprint(\"\")\nprint(\"\")\n\n\nprint(\"gini model\")\nprint(\"\")\nprint (\"predicted value\",predictionCrashGini [0:10])\nprint (\"actual value\", y_test [0:10])\n\n\n","e1cf2e74":"#evaluating the decision tree\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nprint(\"DecisionTrees's Accuracy Entrophy Model: \", metrics.accuracy_score(y_test, predictionCrashEntrophy))\nprint(\"DecisionTrees's Accuracy Entrophy Model: \", metrics.accuracy_score(y_test, predictionCrashGini))","b9a07a6d":"# !conda install -c conda-forge pydotplus -y\n# !conda install -c conda-forge python-graphviz -y","9e62719d":"#we will reduce the size of the data  now so it is faster to run in SVM\n\nfrom sklearn.utils import resample\ndf_majority = df[df.severity==1]\ndf_minority = df[df.severity==2]\n\n \n#Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=20000,  # to match minority class\n                                 random_state=5555) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_balanced_small = pd.concat([df_majority_downsampled, df_minority])\n \n# Display new class counts\ndf_balanced_small[\"severity\"].value_counts()","cc2e5917":"#we will reduce the size of the data now so it is faster to run in SVM\n\nfrom sklearn.utils import resample\n\ndf_majority = df_balanced_small[df_balanced_small.severity==2]\ndf_minority = df_balanced_small[df_balanced_small.severity==1]\n\n \n#Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=20000,  # to match minority class\n                                 random_state=5555) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_balanced_small = pd.concat([df_majority_downsampled, df_minority])\n \n# Display new class counts\ndf_balanced_small[\"severity\"].value_counts()","dc1d2d30":"df_balanced_small.columns\ndf_balanced_small.head()","d8887fe5":"# encode the small data\n\n\ndf_balanced_small[['weather', \"roadCond\",\"lightCond\", \"speeding\", \"underInfluence\", \"inattention\"]] = df_balanced[['weather', \"roadCond\",\"lightCond\", \"speeding\", \"underInfluence\", \"inattention\"]].astype('category')\n\ndf_balanced_small['weather_cat'] = df_balanced_small['weather'].cat.codes\ndf_balanced_small['roadCond_cat'] = df_balanced_small['roadCond'].cat.codes\ndf_balanced_small['lightCond_cat'] = df_balanced_small['lightCond'].cat.codes\ndf_balanced_small['speeding_cat'] = df_balanced_small['speeding'].cat.codes\n\ndf_balanced_small['underInfluence_cat'] = df_balanced_small['underInfluence'].cat.codes\ndf_balanced_small['inattention_cat'] = df_balanced_small['inattention'].cat.codes\ndf_balanced_small.head()\n","79e92e5e":"# lets create a new df with just the categorical values in and just the fieleds we wiull use\n# we will remove the columns for other severity indicators. \n\n\ndf_bal_cat_small = df_balanced_small[['severity', 'weather_cat', 'roadCond_cat', 'lightCond_cat',\n       'speeding_cat', 'underInfluence_cat', 'inattention_cat']]\n\ndf_bal_cat_small.head()","41d74e94":"df_bal_cat_small[\"severity\"].value_counts()","46a69aab":"#now we have a much smaller Dataset 20000 for each severity\n#now we will creatre our x and y sets\n\nX_small = df_bal_cat_small[['inattention_cat', 'underInfluence_cat', 'speeding_cat', 'weather_cat', 'roadCond_cat','lightCond_cat']].values\ny_small = df_bal_cat_small[\"severity\"].values\n\nX_small = preprocessing.StandardScaler().fit(X_small).transform(X_small.astype(float))\n","98a61fa2":"#lets split the data into test and train\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_small, X_test_small, y_train_small, y_test_small = train_test_split( X_small, y_small, test_size=0.35, random_state=6665)\nprint ('Small Train Set:', X_train_small.shape,  y_train_small.shape)\nprint ('Small Test Set:', X_test_small.shape,  y_test_small.shape)","66741b39":"from sklearn import svm\n\n#adding the linear kernels\nclf_lin= svm.SVC(kernel=\"linear\")\n# training the data using linear kernels\nclf_lin.fit(X_train_small, y_train_small)\n\n\n","1c7adb7d":"#adding polynomial svm kernel\nclf_pol= svm.SVC(kernel='poly')\n# training the polynomial kernel\nclf_pol.fit(X_train_small, y_train_small)","9ec39e31":"#adding the sigmod svm kernel\nclf_sig= svm.SVC(kernel='sigmoid')\n#training the sigmod kernel\nclf_sig.fit(X_train_small, y_train_small)\n\n","e7d49254":"#adding the rbf svm kernel\nclf_rbf= svm.SVC(kernel='rbf')\n#training the rbf svm kernel\nclf_rbf.fit(X_train_small, y_train_small)\n","86ed5e20":"#making preductions based on the above fitted models\nyhat_svm_lin = clf_lin.predict(X_test_small)\nyhat_svm_pol = clf_pol.predict(X_test_small)\nyhat_svm_rbf = clf_rbf.predict(X_test_small)\nyhat_svm_sig = clf_sig.predict(X_test_small)\n\n\nrange_ = \"0:10\"\nprint(\"actual y values\", y_test_small[0:10])\n\nprint('yhat_svm_lin', yhat_svm_lin[0:10])\nprint(\"yhat_svm_pol\", yhat_svm_pol[0:10])\nprint(\"yhat_svm_rbf\", yhat_svm_rbf[0:10])\nprint(\"yhat_svm_sig\", yhat_svm_sig[0:10])\n\n","c11970ec":"#lets show the diffetnt values of yhat based on the type of SCVM kernel\npandas_df = [[]]\npandas_df[\"actual\"] = pd.DataFrame(y_test_small)\npandas_df[\"yhat_lin\"] = pd.DataFrame(yhat_svm_lin)\npandas_df[\"yhat_pol\"] = pd.DataFrame(yhat_svm_lin)\npandas_df[\"yhat_rbf\"] = pd.DataFrame(yhat_svm_lin)\npandas_df[\"yhat_sig\"] = pd.DataFrame(yhat_svm_lin)\n\npandas_df_of_svm = pandas_df[['actual', 'yhat_lin','yhat_pol', 'yhat_rbf', 'yhat_sig']]\npandas_df_of_svm.head(50)","233c4a62":"yhat_svm_pol_train = clf_pol.predict(X_train_small)","728b6974":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools","94c7ba08":"#the code for confusion matrix\n\ndef plot_confusion_matrix_svm(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n        print(\"\")\n        print(\"\")\n    else:\n        print('Confusion matrix, without normalization')\n        print(\"\")\n        print(\"\")\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual Value')\n    plt.xlabel('Predicted Value')","b34bba49":"# Compute confusion matrix\n\n\ncnf_matrix = confusion_matrix(y_test_small, yhat_svm_lin, labels=[1,2])\nnp.set_printoptions(precision=5)\n\nprint (\"classification_report LIN\")\nprint(classification_report(y_test_small, yhat_svm_lin))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix_svm(cnf_matrix, classes=['serious(2)','less serious(1)'],normalize= False,  title='Confusion matrix LIN')\nprint(\"\")\nprint(\"\")\n\n#_________________________-_-_-_-_-_----\n\ncnf_matrix = confusion_matrix(y_test_small, yhat_svm_pol, labels=[1,2])\nnp.set_printoptions(precision=5)\n\nprint (\"classification_report Polly\")\nprint(classification_report(y_test_small, yhat_svm_pol))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix_svm(cnf_matrix, classes=['serious(2)','less serious(1)'],normalize= False,  title='Confusion matrix Polly')\nprint(\"\")\nprint(\"\")\n\n#_________________________-_-_-_-_-_----\n\n\ncnf_matrix = confusion_matrix(y_test_small, yhat_svm_rbf, labels=[1,2])\nnp.set_printoptions(precision=5)\n\nprint (\"classification_report RBF\")\nprint(classification_report(y_test_small, yhat_svm_rbf))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix_svm(cnf_matrix, classes=['serious(2)','less serious(1)'],normalize= False,  title='Confusion matrix RBF')\nprint(\"\")\nprint(\"\")\n\n#_________________________-_-_-_-_-_----\n\n\n\ncnf_matrix = confusion_matrix(y_test_small, yhat_svm_sig, labels=[1,2])\nnp.set_printoptions(precision=5)\n\nprint (\"classification_report SIG\")\nprint(classification_report(y_test_small, yhat_svm_sig))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix_svm(cnf_matrix, classes=['severity = 1','severity =2'],normalize= False,  title='Confusion matrix SIG')\nprint(\"\")\nprint(\"\")","56d22c64":"\n#lets take the best version of SVM and include the accuracy here\n\n#RBF seems to be the most accurate\n\nfrom sklearn.metrics import f1_score\n\nprint(\"f1 score for RBF model\")\nf1_score(y_test_small, yhat_svm_rbf, average='weighted') \n","31aa5ee7":"\nfrom sklearn.metrics import jaccard_score as jss\nprint(\"Jaccard Similarity Score\")\njss(y_test_small, yhat_svm_rbf)","136682a7":"import scipy.optimize as opt\nimport pylab as pl","c548c223":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","50c29031":"yhat_LR = LR.predict(X_test)\nyhat_LR_train = LR.predict(X_train)\nyhat_LR","40a49d5f":"yhat_prob_LR = LR.predict_proba(X_test)\nyhat_prob_LR","3c666601":"#EVALUATION OF LR\nfrom sklearn.metrics import jaccard_score\n\nLR_jac_score = jaccard_score(y_test, yhat_prob_LR)\nprint(\"LR jaccard score :\", LR_jac_score)","e8922226":"from sklearn.metrics import jaccard_score\nprint(\"LR jaccard score :\")\njaccard_score(y_test, yhat_LR)","bc5e6785":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_LR, labels=[1,2])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix_svm(cnf_matrix, classes=['severity = 1','severity =2'],normalize= False,  title='Confusion matrix')\nprint (classification_report(y_test, yhat_LR))","9624faec":"yhat_prob_LR_train = LR.predict_proba(X_train)","6ccea86d":"#Summary report\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\n# getting a summaty of the accuraccy on test data\nknn_prec = precision_score(y_test, yhat, average='weighted')\nDT_prec = precision_score(y_test, predictionCrashGini, average='weighted')\nLR_prec = precision_score(y_test, yhat_LR, average='weighted')\nSVM_prec = precision_score(y_test_small, yhat_svm_pol, average='weighted')\n\nknn_rec = recall_score(y_test, yhat, average='weighted')\nDT_rec = recall_score(y_test, predictionCrashGini, average='weighted')\nLR_rec = recall_score(y_test, yhat_LR, average='weighted')\nSVM_rec = recall_score(y_test_small, yhat_svm_pol, average='weighted')\n\nknn_f1 = f1_score(y_test, yhat, average='weighted')\nDT_f1 = f1_score(y_test, predictionCrashGini, average='weighted')\nLR_f1 = f1_score(y_test, yhat_LR, average='weighted')\nSVM_f1 = f1_score(y_test_small, yhat_svm_pol, average='weighted')\n\nknn_acc = metrics.accuracy_score(y_test, yhat)\nDT_acc = metrics.accuracy_score(y_test, predictionCrashGini)\nLR_acc = metrics.accuracy_score(y_test, yhat_LR)\nSVM_acc = metrics.accuracy_score(y_test_small, yhat_svm_pol)\n\n# getting a summaty of the accuraccy on the train data\n\nknn_prec_train = precision_score(y_train, yhat_train, average='weighted')\nDT_prec_train = precision_score(y_train, DT_yhat_train, average='weighted')\nLR_prec_train = precision_score(y_train, yhat_LR_train, average='weighted')\nSVM_prec_train = precision_score(y_train_small, yhat_svm_pol_train, average='weighted')\n\nknn_rec_train = recall_score(y_train, yhat_train, average='weighted')\nDT_rec_train = recall_score(y_train, DT_yhat_train, average='weighted')\nLR_rec_train = recall_score(y_train, yhat_LR_train, average='weighted')\nSVM_rec_train = recall_score(y_train_small, yhat_svm_pol_train, average='weighted')\n\nknn_f1_train = f1_score(y_train, yhat_train, average='weighted')\nDT_f1_train = f1_score(y_train, DT_yhat_train, average='weighted')\nLR_f1_train = f1_score(y_train, yhat_LR_train, average='weighted')\nSVM_f1_train = f1_score(y_train_small, yhat_svm_pol_train, average='weighted')\n\nknn_acc_train = metrics.accuracy_score(y_train, yhat_train)\nDT_acc_train = metrics.accuracy_score(y_train, DT_yhat_train)\nLR_acc_train = metrics.accuracy_score(y_train, yhat_LR_train)\nSVM_acc_train = metrics.accuracy_score(y_train_small, yhat_svm_pol_train)\n\n\n\n\n","05a61885":"import pandas as pd\n\n# initialize list of lists \ndata = [['KNN', knn_prec,knn_prec, knn_f1, knn_acc], \n        ['Decision Tree', DT_prec,DT_rec,DT_f1, DT_acc], \n        ['Log Reg', LR_prec, LR_rec,LR_f1,LR_acc], \n        ['SVM (Polly)', SVM_prec,SVM_rec,SVM_f1,SVM_acc]] \n\n# initialize list of lists \ndata_train = [['KNN', knn_prec_train,knn_prec_train, knn_f1_train, knn_acc_train], \n        ['Decision Tree', DT_prec_train,DT_rec_train,DT_f1_train, DT_acc_train], \n        ['Log Reg', LR_prec_train, LR_rec_train,LR_f1_train,LR_acc_train], \n        ['SVM (Polly)', SVM_prec_train,SVM_rec_train,SVM_f1_train,SVM_acc_train]] \n\n  \n# Create the pandas DataFrame \nresults_df = pd.DataFrame(data, columns = ['model','Precision', 'Recall', 'f1 score', 'accuracy']) \nresults_df_train = pd.DataFrame(data_train, columns = ['model','Precision', 'Recall', 'f1 score', 'accuracy']) \n\n\nresults_df = results_df.set_index('model')\nresults_df_train = results_df_train.set_index('model')\n\nprint(\"results on test data\")\nprint(results_df.head(10))\nprint(\"\")\nprint(\"\")\n\n\nprint(\"results on train data\")\nprint(results_df_train.head(10))","78a5a925":"\nresults_plot = results_df.plot\nresults_plot()\nx = ('model')\n\n\nresults_plot_train_data = results_df_train.plot\nresults_plot_train_data()\nx = ('model')\n","eb59b485":"# feature selection","4fddd08c":"> from the above we can see that there are not many current null cells. However in weather conditions and light conditions we have quite a bit of unknown data. We will remove from Weather_Conditions = Other and Unknown, we will remove from Light_Conditions","9e5bb7c0":"# GOT TO HERE","e91488eb":"# we have finished preparing a small data set for SVM\n# now we will run the SVM with small data","30e64d2e":"# remember to go back and remove unknown values and other values\n\n# and to also run the models on all the data\n# run the model on the train data as well to see its accuracy\n# to try the model with just the weather data****\n# understrand precision, recll and f1","a4bac9f3":"<div href=\"pre-processing\">\n    <h2>DECISION TREE<\/h2>\n<\/div>","35355e97":"# Now we need to prepare the data for the models \n\n- chose our final features\n- do one hot encoding on the data\n- normaise\n- split the data into test and train","68281cca":"# LOGISTICAL REGRESSION","3567ded5":"<div href=\"pre-processing\">\n    <h2>SVM<\/h2>\n<\/div>","d7263214":"# KNN\n****","4612eef5":"1. # Now we must evaluate the SVM model","74e4beb7":"# summary report|","3da12b27":"# Business Understanding\n\n\nMy objective is to find what factors cause accidents and see if we can offer any insights to predict "}}