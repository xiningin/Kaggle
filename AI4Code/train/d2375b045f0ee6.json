{"cell_type":{"780b5860":"code","e2868034":"code","77a442dc":"code","6e58fb85":"code","a1ec0cda":"code","21855436":"code","dd21289e":"code","5e52c361":"code","af037125":"code","8ea69a5b":"code","699d8cef":"code","09d778b9":"code","3e330a0a":"code","092ef6ed":"code","c07500e6":"code","f1913f1c":"code","7c633681":"code","921fa081":"code","2bf8e408":"code","72cfdf1e":"code","13a7b4af":"code","bce8bd7f":"code","37c6ed52":"code","f87c122d":"code","aceed7ec":"code","60d3923f":"code","2596336c":"code","71b53c13":"code","b3e715fb":"code","aabd4ba1":"code","d9890c22":"code","684707d3":"code","1ed483d4":"code","9d6375d0":"code","26cc7a94":"code","8a3d0793":"markdown","016cc8ed":"markdown","2236399e":"markdown","f0f24903":"markdown","c7bb958c":"markdown","4837fcaf":"markdown","f21ad77b":"markdown","0421bbff":"markdown","e8a76160":"markdown","8ea61b62":"markdown","2e2e9152":"markdown","2102cf33":"markdown","4443a5e7":"markdown","8f8acba4":"markdown","1dea9533":"markdown","b050327b":"markdown","0c89e183":"markdown"},"source":{"780b5860":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2868034":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pandas_profiling\n\n","77a442dc":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6e58fb85":"train.head(3)\n","a1ec0cda":"test.head(3)","21855436":"train.shape, test.shape","dd21289e":"train.info()","5e52c361":"train.describe()","af037125":"# lets understand our target variable sale price\n# descrptive stats of price\n\ntrain ['SalePrice'].describe()","8ea69a5b":"sns.distplot(train['SalePrice']);","699d8cef":"# what is the relationship between the price and the rest of the features\n# Is there a correlation\n# Perform correlation\n\ncorr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)\n","09d778b9":"#scatterplot of the above identified top corr features\n\nsns.set()\ncols = [ 'SalePrice','OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt']\nsns.pairplot(train[cols], height = 2.5)\nplt.show();\n\n# after viewing the scatter plots, I have adjusted the code to remove FullBath and GarageCars - they seem not to give much info","3e330a0a":"# We have identified missing values in various columns in our previous codes\n# Lets see missing values in an assending order\n# adjust count to only show values with missing data\n# are the features among the most corr or least corr\n\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/1460).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n","092ef6ed":"# drop the null observation in Electrical \ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)","c07500e6":"# drop all columns with null\n# Note for better performance, we will review this section\n\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index,1)","f1913f1c":"train.shape","7c633681":"# 2. check for duplicates entries, in Id\n# If duplicates. . delete\n# drop Id column since we will not need to use it in our analysis\n\nid_unique = len(set(train.Id))\nid_total = train.shape[0]\nid_dup = id_total - id_unique\nprint(id_dup)\n\n#drop id column\ntrain.drop(['Id'],axis =1,inplace=True)\n\n# no duplicates :-)\n\n","921fa081":"# check the shape of the train dataset at this point\ntrain.shape","2bf8e408":"# Outliers\n# data values 1.5 times the interquartile range above the third quartile or below the first quartile - IQR rule\n\n# outliers in price\n","72cfdf1e":"stat = train.SalePrice.describe()\n\nIQR = stat ['75%'] - stat ['25%']\nupper = stat ['75%'] + 1.5 * IQR\nlower = stat ['25%'] - 1.5 * IQR\nprint ('The lower and upper bound for suspected outliers in SalePrice are {} and {},'. format (upper, lower))","13a7b4af":"train[train.SalePrice == 3875.0]","bce8bd7f":"train[train.SalePrice > 340075]","37c6ed52":"train.duplicated().sum()","f87c122d":"#get dummies for categorical data\ntrain = pd.get_dummies(train)","aceed7ec":"# Display the first 5 rows of the last 12 columns to confirm that categorical features been converted to numerical 0 & 1\ntrain.iloc[:,5:].head(5)","60d3923f":"train.shape","2596336c":"# define X and Y axis\n\nX_train = train.drop(['SalePrice'], axis=1)\nY_train = train['SalePrice']","71b53c13":"#Use numpy to convert to array\nXtrain = np.array(X_train)\nYtrain = np.array(Y_train)","b3e715fb":"# use decision tree\n\nfrom sklearn.tree import DecisionTreeRegressor\ndecision_tree = DecisionTreeRegressor()\ndecision_tree.fit(Xtrain, Ytrain)\nacc_decision_tree = round(decision_tree.score(Xtrain, Ytrain) * 100, 2)\nacc_decision_tree","aabd4ba1":"# use random forest\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(Xtrain, Ytrain);\nrf","d9890c22":"from sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","684707d3":"# drop all features dropped during training\n\ntest = test.drop (['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage','GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrArea','MasVnrType'], axis =1)","1ed483d4":"test.shape","9d6375d0":"#get dummies for categorical data\ntest = pd.get_dummies(test)\n","26cc7a94":"test.shape","8a3d0793":"# 6. Feature Engineering","016cc8ed":"# 7. Train The Model","2236399e":"# 2. Import libraries\n","f0f24903":"## Prepare the test set","c7bb958c":"# Outline\n1. Problem description\n1. Import libraries \n1. Upload Datasets\n1. Understanding the Data\n1. Data Cleaning\n1. feature Engineering\n1. Train The Model\n\n","4837fcaf":"Lets reveiw all the variables with missing data\n\nWe should consider dropping variables with more than 50% missing values. \nLooking at the first 5 variables with close to 50% and above,and compare it with the corr table we can conclude that these varibles are of no much value to our analysis : PoolQC,MiscFeature, Alley, Fence, FireplaceQu. Hence decision to delete.\n\nThe garage variables have the same # of missing data, possibly the same observations. .\n\nFor the rest of the variables lets replace with a median apart from electrical which has only one missing observation which we are going to delete","f21ad77b":"Some of of the top corr features: OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd Least corr includes: YrSold\n\nLets do a scatter plot of the features most corr to price for a better fiew","0421bbff":"# 3. Upload The Datasets","e8a76160":"No missing price, max 755,000 min 43,900 In real life we can say that its a great difference\n","8ea61b62":"We can confirm the strong correlation. Some of what we are observing is common knowledge, what is interesting is the detail of the relationship\nThe price increase with newer houses, we also see increase as features increase upto to the highest value such as the TotalBsmtSF","2e2e9152":"# **1. Problem Description**\n\nWe need to predict the final price of a residential homes in Ames, Iowa given its attributes.\n\nPeople buy houses to satisfy a certain need or a set of needs. It could be a combination of space for kids, procimity to schools, prestige etc.  The more people are looking for a certain feature to satisfy their need the more the demand, that pushes the price to go high and vice versa. For every house that was sold we want to determine how important each feature was and how much did it impact the price.","2102cf33":"# 5. Data Cleaning\n* Are there any irregularities in this data set and how do we deal with it?\n* Is there missing data and how do we deal with it?\n* Do we have any duplicate entries that we need to delete?\n* Do we have columns that will be irrelevant to our analysis that we may need to delete such as Id?\n* Do we have outliers that can affect our analysis, and how do we deal with it?","4443a5e7":"lets not drop these observations since we may loose alot of information. Also, looking at the price corr to other feature graphs, the expensive house also rate high on other features hence may not be outliers as such.\nNote: This is an area we can review for better outcome","8f8acba4":"The price is skewed to the left, with most home prices being btwn 50000 and 300,000, 500,000 and above are replecented by a smaller population.","1dea9533":"**To Continue...**...","b050327b":"# 8. Make Prediction On The Test Set","0c89e183":"# 4. Understanding our Data"}}