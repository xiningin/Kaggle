{"cell_type":{"48c46bdb":"code","11af56fa":"code","b369e7b5":"code","c8f09046":"code","3c5d7115":"code","efb297c7":"code","bd1305c1":"code","54b04523":"code","5684050a":"code","a31ae957":"code","adbf0f13":"code","de47583e":"code","bcd1d755":"code","3560b6a0":"code","ef479d42":"code","0312c035":"code","f523fce3":"code","bcf5b1fe":"code","a9406bfa":"code","f15c3a9d":"code","c6ebc746":"markdown","0a067cdb":"markdown","c4e8d13a":"markdown","a3599fc4":"markdown","a2d5d852":"markdown","4475f9c8":"markdown","22c7bc03":"markdown","f59505ee":"markdown","4edeed1d":"markdown","48a5d6ee":"markdown","7a203319":"markdown","99ed6306":"markdown","7c53eb93":"markdown","2d465884":"markdown","99afaf49":"markdown"},"source":{"48c46bdb":"import pathlib\nimport pickle\nimport itertools\nimport time\nimport datetime\nimport re\nimport random\n\nimport nltk\nimport torch\nimport transformers\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import metrics, model_selection\nfrom scipy import special as s_special\nfrom tqdm import tqdm\nfrom torch.utils import data\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","11af56fa":"!mkdir -p '..\/working\/cache\/encodings'\nRANDOM_SEED = 42\nCACHE_OUTPUT_FOLDER = pathlib.Path('..\/working\/cache')\nCACHE_INPUT_FOLDER = pathlib.Path('..\/input\/rubert-short-v1\/cache')\nMODEL_NAME = 'DrMatters\/rubert_cased'\n\n\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\n\nUSE_CACHED_ENCODINGS = True\nTRAIN_ON_FULL_DATASET = False\nSPLIT_ARTICLES = True\n\nAPPLY_PROCESSOR = True\n\nTEST_SIZE = 0.2\nif SPLIT_ARTICLES:\n    PAD_LEN = 64\nelse:\n    PAD_LEN = 500\n\nif PAD_LEN > 100:\n    BATCH_SIZE = 8\nelse:\n    BATCH_SIZE = 32","b369e7b5":"if torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","c8f09046":"def load_dataset(folder: str, load_test=True, info=True):\n    core: pd.DataFrame = pd.read_csv(folder + '\/core.csv')\n    if load_test:\n        test: pd.DataFrame = pd.read_csv(folder + '\/test_data.csv')\n\n    if info:\n        if load_test:\n            print(test.info())\n        print(core.info())\n\n    print('Dataset loaded')\n    print('______________')\n\n    if load_test:\n        return core, test\n    else:\n        return core","3c5d7115":"df, test = load_dataset('..\/input\/nlp-task\/', info=False)\nlabels = df['target'].values","efb297c7":"print('Loading BERT tokenizer...')\ntokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n\ndef apply_processors(processors, document):\n    for processor in processors:\n        document = processor(document)\n    return document\n\ndef add_space_after_dot(document):\n    return re.sub(r'(?<=[.])(?=[^\\s])', r' ', document)\n\nprocessors = [add_space_after_dot]","bd1305c1":"out_path_folder = CACHE_OUTPUT_FOLDER \/ 'encodings\/'\nsuffix = ''\nif not SPLIT_ARTICLES:\n    suffix += '_full'\nif APPLY_PROCESSOR:\n    suffix +=  '_processor'\n\nif USE_CACHED_ENCODINGS:\n    print(\"Using cached encodings\")\n    in_path_folder = CACHE_INPUT_FOLDER \/ 'encodings\/'\n    print(\"Reading encoded_titles\")\n    encoded_titles = pickle.load(open(in_path_folder \/ 'encoded_titles.pkl', 'rb'))\n    print(\"Reading encoded_articles\")\n    encoded_articles = pickle.load(open(in_path_folder \/ f'encoded_articles{suffix}.pkl', 'rb'))\n    print(\"Reading encoded_labeled\")\n    encoded_labeled = pickle.load(open(in_path_folder \/ 'encoded_labeled.pkl', 'rb'))\nelse:\n    print('Start encoding labeled data')\n    encoded_labeled = [tokenizer.encode(sent, add_special_tokens=True) for sent in df['text']]\n    \n    print('Start encoding titles')\n    encoded_titles = [tokenizer.encode(sent, add_special_tokens=True) for sent in tqdm(test['title'])]\n    \n    print('Start encoding texts')\n    encoded_articles = []\n    if SPLIT_ARTICLES:\n        print('With splitting articles into sentences')\n        sent_tokenizer = nltk.data.load('tokenizers\/punkt\/russian.pickle')\n        for article in tqdm(test['text']):\n            if APPLY_PROCESSOR:\n                article = apply_processors(processors, article)\n            articles = sent_tokenizer.tokenize(article)\n            encoded_sentences = []\n            for sent in articles:\n                encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n                encoded_sentences.append(encoded_sent)\n            encoded_articles.append(encoded_sentences)\n    else:\n        encoded_articles = [tokenizer.encode(sent, add_special_tokens=True) for sent in tqdm(test['text'])]\n            \n\npickle.dump(encoded_titles, open(out_path_folder \/ 'encoded_titles.pkl', 'wb'))\npickle.dump(encoded_articles, open(out_path_folder \/ f'encoded_articles{suffix}.pkl', 'wb'))\npickle.dump(encoded_labeled, open(out_path_folder \/ 'encoded_labeled.pkl', 'wb'))","54b04523":"# find out the 95 percentile of lens of sentences\nlens = [len(sen) for sen in encoded_labeled]\nif SPLIT_ARTICLES:\n    lens1 = []\n    for article in encoded_articles:\n        for sentence in article:\n            lens1.append(len(sentence))\nelse:\n    lens1 = [len(sen) for sen in encoded_articles]\nlens.extend(lens1)    \n\nprint(f'95 percent of sequences is less or equal than {np.percentile(lens, 95)}')\nprint(f'80 percent of sequences is less or equal than {np.percentile(lens, 80)}')","5684050a":"from keras.preprocessing.sequence import pad_sequences\n\nprint('\\nPadding\/truncating all sentences to %d values...' % PAD_LEN)\nprint('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n\nencoded_labeled = pad_sequences(encoded_labeled, maxlen=PAD_LEN, dtype=\"long\",\n                                value=0, truncating=\"post\", padding=\"post\")\nencoded_titles = pad_sequences(encoded_titles, maxlen=PAD_LEN, dtype=\"long\",\n                                value=0, truncating=\"post\", padding=\"post\")\n\nif SPLIT_ARTICLES:\n    encoded_articles = [pad_sequences(article_sentences, maxlen=PAD_LEN, dtype=\"long\",\n                                      value=0, truncating=\"post\", padding=\"post\") for article_sentences in encoded_articles]\nelse:\n    encoded_articles = pad_sequences(encoded_articles, maxlen=PAD_LEN, dtype=\"long\",\n                                     value=0, truncating=\"post\", padding=\"post\")\n\nprint('\\nDone.')","a31ae957":"labeled_masks = [(enc_sent > 0).astype(int) for enc_sent in encoded_labeled]\ntitles_masks = [(enc_sent > 0).astype(int) for enc_sent in encoded_titles]\n\nif SPLIT_ARTICLES:\n    articles_masks = []\n    for article in encoded_articles:\n        article_masks = [(enc_sent > 0).astype(int) for enc_sent in article]\n        articles_masks.append(article_masks)\nelse:\n    articles_masks = [(enc_sent > 0).astype(int) for enc_sent in encoded_articles]\nprint('Masks created')","adbf0f13":"if TRAIN_ON_FULL_DATASET:\n    train_inputs = encoded_labeled\n    train_labels = labels\n    train_masks = labeled_masks\n\nelse:\n    train_inputs, validation_inputs, train_labels, validation_labels = model_selection.train_test_split(\n        encoded_labeled, labels, stratify=labels, random_state=RANDOM_SEED, test_size=TEST_SIZE\n    )\n    train_masks, validation_masks, _, _ = model_selection.train_test_split(\n        labeled_masks, labels, stratify=labels, random_state=RANDOM_SEED, test_size=TEST_SIZE)","de47583e":"train_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\n\nif not TRAIN_ON_FULL_DATASET:\n    validation_inputs = torch.tensor(validation_inputs)\n    validation_labels = torch.tensor(validation_labels)\n    validation_masks = torch.tensor(validation_masks)\n\n# Create the DataLoader for our training set.\ntrain_data = data.TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = data.RandomSampler(train_data)\ntrain_dataloader = data.DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n\nif not TRAIN_ON_FULL_DATASET:\n    # Create the DataLoader for our validation set.\n    validation_data = data.TensorDataset(validation_inputs, validation_masks, validation_labels)\n    validation_sampler = data.SequentialSampler(validation_data)\n    validation_dataloader = data.DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)","bcd1d755":"model = transformers.BertForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels = 2,\n    output_attentions = False,\n    output_hidden_states = False,\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()","3560b6a0":"# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = transformers.AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\nepochs = 4\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","ef479d42":"def format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef flat_f1(preds, labels):\n    # code to get probabilites:\n    # from scipy import special as s_special\n    # s_special.softmax(preds, axis=1)[:, 1]\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return metrics.f1_score(labels_flat, pred_flat, average='binary')\n\ndef roc_auc(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    probas = s_special.softmax(pred_flat, axis=1)[:, 1]\n    metrics.roc_auc_score(labels_flat, pred_flat)","0312c035":"# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\nrandom.seed(RANDOM_SEED + 1)\nnp.random.seed(RANDOM_SEED + 1)\ntorch.manual_seed(RANDOM_SEED + 1)\ntorch.cuda.manual_seed_all(RANDOM_SEED + 1)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\neval_hist = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    t0 = time.time()\n\n    total_loss = 0\n    \n    # Put the model into training mode.\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 10 batches.\n        if step % 10 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass.\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end.\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print()\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n    \n    if not TRAIN_ON_FULL_DATASET:\n        print()\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n\n        all_logits = []\n        all_labels = []\n        for batch in validation_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n\n            b_input_ids, b_input_mask, b_labels = batch\n            with torch.no_grad():        \n                outputs = model(b_input_ids, \n                                token_type_ids=None, \n                                attention_mask=b_input_mask)\n            logits = outputs[0]\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            \n            all_logits.extend(logits)\n            all_labels.extend(label_ids)\n\n        eval_metric = flat_f1(np.asarray(all_logits), np.asarray(all_labels))\n\n        # Report the final accuracy for this validation run.\n        print(\"  Metric: {0:.2f}\".format(eval_metric))\n        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n        eval_hist.append(eval_metric)\n\nprint(\"\")\nprint(\"Training complete!\")","f523fce3":"if not TRAIN_ON_FULL_DATASET:\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import seaborn as sns\n\n    # Use plot styling from seaborn.\n    sns.set(style='darkgrid')\n    sns.set(font_scale=1.5)\n    plt.rcParams[\"figure.figsize\"] = (12,6)\n\n    # Plot the learning curve.\n    plt.plot(eval_hist, 'b-o')\n    plt.title(\"Evaluation results\")\n    plt.xlabel(\"Epoch\")\n\n    plt.show()","bcf5b1fe":"def get_simple_loader(encoded_seq, seq_masks, batch_size=BATCH_SIZE):\n    if not isinstance(encoded_seq, list):\n        encoded_seq = torch.tensor(encoded_seq)\n        seq_masks = torch.tensor(seq_masks)\n        ids = torch.tensor(list(range(encoded_seq.shape[0])))\n\n    else:\n        stretched_articles, stretched_masks, stretched_ids = [], [], []\n        for idx, (encoded_article, article_masks) in enumerate(zip(encoded_seq, seq_masks)):\n            stretched_articles.extend(encoded_article)\n            stretched_masks.extend(article_masks)\n            stretched_ids.extend([idx] * len(encoded_article))\n        \n        encoded_seq = torch.tensor(stretched_articles)\n        seq_masks = torch.tensor(stretched_masks)\n        ids = torch.tensor(stretched_ids)\n        \n    dataset = data.TensorDataset(encoded_seq, seq_masks, ids)\n    sampler = data.SequentialSampler(dataset)\n    dataloader = data.DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n    return dataloader\n\n\ndef make_prediction(model, dataloader):\n    print(f'Predicting labels for {len(dataloader)} test batches each of {dataloader.batch_size} elements')\n    model.eval()\n    predictions = []\n    ids = []\n    \n    for batch in tqdm(dataloader):\n        b_input, b_mask, b_ids = batch\n        b_input = b_input.to(device)\n        b_mask = b_mask.to(device)\n        \n        with torch.no_grad():\n            outputs = model(b_input, token_type_ids=None,\n                            attention_mask=b_mask)\n            logits = outputs[0].detach().cpu().numpy()\n            probas = s_special.softmax(logits, axis=1)[:, 1]\n            \n            predictions.append(probas)\n            ids.append(b_ids.numpy())\n    print('    DONE.')\n    return predictions, ids\n\n\ndef convert_predictions(probas, ids):\n    probas = itertools.chain.from_iterable(probas)\n    probas = map(lambda elem: int(elem * 1000) \/ 1000, probas)\n    ids = itertools.chain.from_iterable(ids)\n    df = pd.DataFrame({'probas': probas, 'ids': ids})\n    df = df.groupby('ids').agg({'probas': lambda x: list(x)})\n    return df","a9406bfa":"titles_loader = get_simple_loader(encoded_titles, titles_masks)\narticles_loader = get_simple_loader(encoded_articles, articles_masks)\n\n!mkdir -p '..\/working\/info'\nwith open('..\/working\/info\/reached_predictions_stage.txt', 'w') as f:\n    f.write('success')\n\nprint('Making predictions for titles')\ntitles_probas, titles_ids = make_prediction(model, titles_loader)\nprint('Making predictions for texts')\narticles_probas, articles_ids = make_prediction(model, articles_loader)","f15c3a9d":"titles_pred = convert_predictions(titles_probas, titles_ids).rename(columns={'probas': 'probas_title'})\narticles_pred = convert_predictions(articles_probas, articles_ids).rename(columns={'probas': 'probas_text'})\n\ntest_res = pd.concat([test, titles_pred, articles_pred], axis=1)\ntest_res.head()\n\nif TRAIN_ON_FULL_DATASET:\n    data_split = 'full'\n    last_metric = 'none'\nelse:\n    data_split = 'split'\n    last_metric = f'{int(eval_hist[-1] * 100)}'\nadditional_info = 'space_after_dot'\nmodel_name = MODEL_NAME.translate(str.maketrans('\/', '|'))\nsubmission_name = f'sub__model({model_name})_data({data_split})_metric({last_metric})' + \\\n    f'_SplitArticles({SPLIT_ARTICLES})_additional({additional_info})'\n\ntest_res.to_csv(f'..\/working\/{submission_name}.csv')","c6ebc746":"## 2.2. Load dataset","0a067cdb":"## 4.3. Training Loop","c4e8d13a":"Let's take a look at our training metric over all batches:","a3599fc4":"## 3.5. Training & Validation Split\n","a2d5d852":"Given that, let's choose PAD_LEN = 64 and apply the padding.","4475f9c8":"# 4. Train Our Classification Model","22c7bc03":"## 3.3. Padding & Truncating","f59505ee":"# 5. Evaluation of the test set","4edeed1d":"## 3.6. Converting to PyTorch Data Types","48a5d6ee":"In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. ","7a203319":"## 3.2. Sentences to IDs","99ed6306":"Original blog post [here](http:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/) and as a Colab Notebook [here](https:\/\/colab.research.google.com\/drive\/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP). \n","7c53eb93":"## 4.2. Optimizer & Learning Rate Scheduler\nFor the purposes of fine-tuning, the authors recommend choosing from the following values:\n- Batch size: 16, 32  (We chose 32 when creating our DataLoaders).\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (We'll use 2e-5).\n- Number of epochs: 2, 3, 4  (We'll use 4).","2d465884":"## 3.4. Attention Masks","99afaf49":"Helper for loading"}}