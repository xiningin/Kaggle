{"cell_type":{"9cb12b5f":"code","d95a7f20":"code","ab110973":"code","39dea506":"code","4fb7cebc":"code","962ea1fb":"code","73ede37c":"code","f53ca829":"code","8e27cd3e":"code","7164c0eb":"code","fce14000":"code","b46ad486":"code","7f4a2bef":"code","1bd904db":"code","d3bb86d1":"code","c2f2c7c4":"code","d65cdbea":"code","e157b539":"code","412d0126":"code","6f7ba3b3":"code","8166329d":"code","c206c8df":"code","b74b54ce":"code","52c2ee39":"code","299f3ee3":"code","6171b6be":"code","55cd84c8":"code","a5a163e7":"code","b77dbfc0":"code","7870aba8":"code","f583bb8b":"code","5937cec7":"code","3c07d687":"code","ae2e3aaf":"code","d98d2c19":"code","3db9ae29":"code","ff86b361":"code","c1486480":"code","05eef96a":"code","17acbdec":"code","43f61b6e":"code","de8b2b86":"code","471fe9dc":"code","c7044fa1":"code","89fa7d31":"markdown","bbb60544":"markdown"},"source":{"9cb12b5f":"#optiver volatility prediction\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", 50)","d95a7f20":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\norder_book = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_book = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')","ab110973":"#train first few rows\ntrain.head(10)","39dea506":"#first few rows of order book stock = 0\norder_book.head(3)","4fb7cebc":"#first few rows of trade book stock = 0\ntrade_book.head(3)","962ea1fb":"#create log diffs\ndef logDiff(stock_prices):\n    return np.log(stock_prices).diff()","73ede37c":"#create realized vols for each time \/ stock price\ndef realized_vol(log_diffs):\n    return np.sqrt(np.sum(log_diffs ** 2))","f53ca829":"#process the order book file\ndef preprocess_order(orderPath):\n    stock = pd.read_parquet(orderPath)\n    stock_id = orderPath.split('=')[1]\n    stock['stock_id'] = stock_id\n    stock['wap'] = (stock['bid_price1'] * stock['ask_size1'] + stock['ask_price1'] * stock['bid_size1']) \/ (stock['bid_size1'] + stock['ask_size1'])\n    stock['wap2'] = (stock['bid_price2'] * stock['ask_size2'] + stock['ask_price2'] * stock['bid_size2']) \/ (stock['bid_size2'] + stock['ask_size2'])\n    stock['logDifferences'] = stock.groupby(['time_id'])['wap'].apply(logDiff)\n    stock['logDifferences2'] = stock.groupby(['time_id'])['wap2'].apply(logDiff)    \n    stock['volume_imbalance1'] = stock['bid_size1'] \/ stock['ask_size1']\n    stock['volume_imbalance2'] = stock['bid_size2'] \/ stock['ask_size2']\n    stock['spread'] = stock['ask_price1'] - stock['bid_price1']\n    stock['bid_spread'] = stock['bid_price1'] - stock['bid_price2']\n    stock['ask_spread'] = stock['ask_price2'] - stock['ask_price1']\n    \n    \n    return stock","8e27cd3e":"#glob glob the two file paths with all the trade and order files\norderPath = glob.glob('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')\ntradePath = glob.glob('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/*')\nstock = preprocess_order(orderPath[0])\nstock.head()\n#let's see first few rows of stock = 0 after preprocessing","7164c0eb":"#preprocess the orderbook with aggregate stats\ndef preprocess_order_agg(stk):\n    \n    agg_stats = {\n        'logDifferences':[realized_vol],\n        'logDifferences2':[realized_vol],\n        'wap': [np.mean, np.std],\n        'wap2':[np.mean, np.std],\n        'volume_imbalance1':[np.mean, np.std],\n        'spread':[np.mean, np.std, np.min, np.max],\n        'bid_spread':[np.mean, np.std],\n        'ask_spread':[np.mean, np.std]\n    }\n    \n    df_agg = pd.DataFrame(stk.groupby(['time_id']).agg(agg_stats)).reset_index()\n    df_agg.columns = ['_'.join(col) for col in df_agg.columns]\n    \n    return df_agg","fce14000":"#create agg stats for stock\nagg_stats = preprocess_order_agg(stock)","b46ad486":"agg_stats.head()","7f4a2bef":"def preprocess_trade(tradePath):\n    stk = pd.read_parquet(tradePath)\n    stock_id = tradePath.split('=')[1]\n    stk['stock_id'] = stock_id\n    \n    agg_stats = {\n        'price': [np.mean, np.std, np.min, np.max],\n        'size':[np.sum],\n        'order_count':[np.sum]\n    }\n    \n    df_agg = pd.DataFrame(stk.groupby(['time_id']).agg(agg_stats)).reset_index()\n    df_agg.columns = ['_'.join(col) for col in df_agg.columns]\n   \n    return df_agg","1bd904db":"#create agg stats for trade book\nagg_stats2 = preprocess_trade(tradePath[0])","d3bb86d1":"agg_stats2.head()","c2f2c7c4":"#time stats\n\ndef time_stats_agg(stk, time_in_seconds):\n    df = pd.DataFrame()\n    \n    agg_stats = {\n        'logDifferences':[realized_vol],\n        'logDifferences2':[realized_vol],\n        'wap': [np.mean, np.std],\n        'wap2':[np.mean, np.std],\n        'volume_imbalance1':[np.mean, np.std],\n        'spread':[np.mean, np.std, np.min, np.max],\n        'bid_spread':[np.mean, np.std],\n        'ask_spread':[np.mean, np.std]\n    }\n    \n    time_df = pd.DataFrame(stk.query(f'seconds_in_bucket > {time_in_seconds}').groupby(['time_id']).agg(agg_stats)).reset_index()\n    time_df.columns = ['_'.join(col) for col in time_df.columns]\n    time_df = time_df.add_suffix('_' + str(time_in_seconds))\n    \n    return time_df","d65cdbea":"#create time stats by different time amts\ntime_stats_0 = time_stats_agg(stock, time_in_seconds = 0)\ntime_stats_150 = time_stats_agg(stock, time_in_seconds = 150)\ntime_stats_300 = time_stats_agg(stock, time_in_seconds = 300)\ntime_stats_450 = time_stats_agg(stock, time_in_seconds = 450)","e157b539":"time_stats_0.head()","412d0126":"#merge all dfs\ntime_stats = time_stats_0.merge(time_stats_150, how = 'left', left_on = 'time_id__0', right_on = 'time_id__150')\ntime_stats = time_stats.merge(time_stats_300, how = 'left', left_on = 'time_id__0', right_on = 'time_id__300')\ntime_stats = time_stats.merge(time_stats_450, how = 'left', left_on = 'time_id__0', right_on = 'time_id__450')","6f7ba3b3":"#see columns created\ntime_stats.columns","8166329d":"#drop unneccessary time columns and add row_id\nstock_id = orderPath[0].split('=')[1]\ntime_stats['row_id'] = time_stats['time_id__0'].apply(lambda x: f'{stock_id}-{x}')\ntime_stats.drop(['time_id__0','time_id__150', 'time_id__300', 'time_id__450'], axis = 1, inplace = True)","c206c8df":"time_stats.columns","b74b54ce":"time_stats.head()","52c2ee39":"#check the shape of df to see that it's correct\ntime_stats.shape","299f3ee3":"#loop through all stocks\ni = 1\ndf_final = pd.DataFrame()\n\nfor (order, trade) in zip(orderPath, tradePath):\n    \n    stock = preprocess_order(order)\n    trade_agg = preprocess_trade(trade)\n    time_stats_0 = time_stats_agg(stock, time_in_seconds = 0)\n    time_stats_150 = time_stats_agg(stock, time_in_seconds = 150)\n    time_stats_300 = time_stats_agg(stock, time_in_seconds = 300)\n    time_stats_450 = time_stats_agg(stock, time_in_seconds = 450)\n    \n    #merge all dfs\n    time_stats = time_stats_0.merge(time_stats_150, how = 'left', left_on = 'time_id__0', right_on = 'time_id__150')\n    time_stats = time_stats.merge(time_stats_300, how = 'left', left_on = 'time_id__0', right_on = 'time_id__300')\n    time_stats = time_stats.merge(time_stats_450, how = 'left', left_on = 'time_id__0', right_on = 'time_id__450')\n    \n    df = time_stats.merge(trade_agg, how = 'left', left_on = 'time_id__0', right_on = 'time_id_')\n    \n    stock_id = order.split('=')[1]\n    df['stock_id'] = int(stock_id)\n    df['row_id'] = df['time_id__0'].apply(lambda x: f'{stock_id}-{x}')\n    \n    df.drop(['time_id__0','time_id__150', 'time_id__300', 'time_id__450'], axis = 1, inplace = True)\n    \n    df_final = pd.concat([df, df_final], axis = 0)\n    \n    if i%10 == 0:\n        print (i)\n    i += 1","6171b6be":"#check shape of final df\ndf_final.shape","55cd84c8":"#check first few rows of df_final\ndf_final.head(5)","a5a163e7":"df_final['stock_id'].unique()","b77dbfc0":"df_final.isnull().any().sum()","7870aba8":"#lgb model\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\n\nX = df_final.drop(['logDifferences_realized_vol_0', 'row_id'], axis = 1)\ny = df_final['logDifferences_realized_vol_0']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y)\nprint('Shape of X_test is {}'.format(X_test.shape))\nprint('Shape of X_train is {}'.format(X_train.shape))\nprint('Shape of y_test is {}'.format(y_test.shape))\nprint('Shape of y_train is {}'.format(y_train.shape))","f583bb8b":"#build lgb model. fit to train data\nmodel_lgb = lgb.LGBMRegressor()\nmodel_lgb.fit(X_train, y_train)","5937cec7":"#make predictions\nypreds = model_lgb.predict(X_test)","3c07d687":"#create RMSPE metric\ndef RMSPE(vols, truth):         \n    return np.sqrt(np.sum(np.mean(np.square((vols - truth)\/truth))))","ae2e3aaf":"#check preds against values\nRMSPE(ypreds, y_test)","d98d2c19":"#params from gridsearch\nparams = {\n    'learning_rate': 0.1, \n    'max_depth': 5, \n    'n_estimators': 500, \n    'num_leaves': 28\n}","3db9ae29":"#rerun with new params\nmodel_lgb = lgb.LGBMRegressor(**params)\nmodel_lgb.fit(X_train, y_train)","ff86b361":"#create new preds\nypreds = model_lgb.predict(X_test)","c1486480":"RMSPE(ypreds, y_test)","05eef96a":"#create preds for test set\n#loop through all stocks\n#glob glob the two file paths with all the trade and order files\norderTest = glob.glob('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\ntradeTest = glob.glob('..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/*')\n\ndf_final = pd.DataFrame()\n\nfor (order, trade) in zip(orderTest, tradeTest):\n    stock = preprocess_order(order)\n    trade_agg = preprocess_trade(trade)\n    time_stats_0 = time_stats_agg(stock, time_in_seconds = 0)\n    time_stats_150 = time_stats_agg(stock, time_in_seconds = 150)\n    time_stats_300 = time_stats_agg(stock, time_in_seconds = 300)\n    time_stats_450 = time_stats_agg(stock, time_in_seconds = 450)\n    \n    #merge all dfs\n    time_stats = time_stats_0.merge(time_stats_150, how = 'left', left_on = 'time_id__0', right_on = 'time_id__150')\n    time_stats = time_stats.merge(time_stats_300, how = 'left', left_on = 'time_id__0', right_on = 'time_id__300')\n    time_stats = time_stats.merge(time_stats_450, how = 'left', left_on = 'time_id__0', right_on = 'time_id__450')\n    \n    df = time_stats.merge(trade_agg, how = 'left', left_on = 'time_id__0', right_on = 'time_id_')\n    df['stock_id'] = int(trade.split('=')[1])\n    df['row_id'] = df['time_id__0'].apply(lambda x: f'{stock_id}-{x}')\n    \n    df.drop(['time_id__0','time_id__150', 'time_id__300', 'time_id__450'], axis = 1, inplace = True)\n    \n    df_final = pd.concat([df, df_final], axis = 0)\n","17acbdec":"#create model on test data\nX = df_final.drop(['logDifferences_realized_vol_0', 'row_id'], axis = 1)\nX.head()","43f61b6e":"#LGB model\nX['target'] = model_lgb.predict(X)\nX['row_id'] = X['stock_id'].apply(str) + '-' + (X['time_id_']).apply(str)","de8b2b86":"submission = X[['row_id', 'target']]","471fe9dc":"submission.head(3)","c7044fa1":"submission.to_csv('submission.csv', index = False)","89fa7d31":"params = grid.best_params_\n\n{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'num_leaves': 28}","bbb60544":"from sklearn.model_selection import GridSearchCV\nparams = {\n    'num_leaves': [7, 14, 21, 28, 31, 50],\n    'learning_rate': [0.1, 0.03, 0.003],\n    'max_depth': [-1, 3, 5],\n    'n_estimators': [50, 100, 200, 500],\n}\n\ngrid = GridSearchCV(model_lgb, params, scoring='r2', cv = 5)\ngrid.fit(X_train, y_train)"}}