{"cell_type":{"25bf0979":"code","bb267474":"code","033cb262":"code","cb36b12f":"code","80f40440":"code","d921e3ce":"code","f27d3b7e":"code","2cd3e3ed":"code","f4710a9a":"code","c79e6803":"code","0da0eba4":"code","3767580a":"code","b0bf8d69":"code","806b964d":"code","30882d63":"code","e848d4f0":"code","7cdfab9a":"code","a13dee09":"code","9f0cf029":"code","729f717b":"code","84ddde23":"code","a70d3f9b":"code","270b98db":"code","17ea3a57":"code","9df14dba":"code","736daff3":"code","45af99e0":"code","9bd0278d":"code","03121ea8":"code","b1e95e43":"code","51425284":"code","c0f0b03e":"code","caa4b2e3":"code","f83a2767":"code","6950a957":"code","567b969f":"code","41ae3968":"code","f095fcba":"code","af0da57a":"code","2bf5e18d":"code","24e92207":"code","fad5d82c":"code","47ba1671":"code","cb6582d5":"code","f90c13b3":"code","20914051":"code","e3eb0c97":"code","1b0aeec1":"code","8ca141d5":"code","45bce6f5":"code","e61ad81f":"code","a6b21209":"code","9547cc61":"code","e7648349":"code","652f9bbe":"code","a95b62bb":"code","fb45ac31":"code","6713b98d":"code","82ee4a2d":"code","306b6eeb":"code","c2efe82b":"code","577b4e2f":"code","f1dfd3b0":"code","09aa70e0":"code","4ba6d15c":"code","1ccbe822":"code","522d95d1":"code","7afe49db":"code","d994f88e":"code","456224e9":"code","9d124bcb":"code","f02381de":"code","b6435912":"code","29010e2e":"code","888151d4":"code","0c26207e":"code","8e55625b":"code","90919b27":"code","1b2f62b2":"code","aaa2dd6a":"code","469cf412":"code","d2eedce4":"code","49f006f7":"code","709acf5b":"code","998ab004":"code","f2ebdda2":"code","cf0fff8f":"code","cb960ac5":"code","f59ec85b":"code","8b763a24":"code","9d693a77":"code","0a1e643f":"code","db0fb6e1":"code","1300215f":"code","e82bb408":"code","999f66cb":"code","de4b834e":"code","21c09cf1":"code","6a5b677f":"code","b0ca5532":"code","509ec7a2":"code","49800956":"code","a4948c71":"code","26b80bd0":"code","0ca5c879":"code","0e99f364":"code","163d2ff5":"code","942c3388":"code","dd025f7b":"code","4914052b":"code","1cc2c62c":"code","28446cfd":"code","6ddad065":"code","8e24a271":"code","1ed7687b":"code","482ab451":"code","d8d68adf":"code","4a38851f":"code","9919f689":"code","0d326496":"code","99c699b9":"code","8dc5d7c4":"code","3b6bb74f":"code","503581ea":"code","e8fe8be2":"code","52a837b5":"code","b6828d80":"code","e8ddda96":"code","7bdbe967":"code","cbed060a":"code","1cb14fa0":"code","d124b4ce":"code","48fc9f9a":"code","e9f5933e":"code","ff588f3e":"code","76fc6b49":"code","fb8b6e0c":"code","e939d013":"code","fb7d0283":"code","8de04cb3":"code","755b820a":"code","04ac75f0":"code","30ef4bff":"markdown","d33c5e35":"markdown","75bdb077":"markdown","0ee54ea0":"markdown","4bb63424":"markdown","5d364aa8":"markdown","cac46570":"markdown","f5fb75b0":"markdown","4c8a9f9e":"markdown","8766937b":"markdown","96f2125a":"markdown","0afce0f6":"markdown","9b12717d":"markdown","969a96ef":"markdown","a56a4e83":"markdown","48a085b0":"markdown","fb9fb5a3":"markdown","7b07bcdf":"markdown","0e465699":"markdown","8abf68d1":"markdown","c6826b1d":"markdown","4d97dd4e":"markdown","30bb1f20":"markdown","d5d6b032":"markdown","6fd88465":"markdown","c33eb59a":"markdown","017a2d8d":"markdown","5dd2c17e":"markdown","dc3a9f5e":"markdown","258ca28a":"markdown","76328bb4":"markdown","9e76febe":"markdown","6995bd9a":"markdown","2d273c63":"markdown","62715d4a":"markdown","32913643":"markdown","44fe83f4":"markdown","ed148c5c":"markdown","b8abd37a":"markdown","6e374476":"markdown","d3647124":"markdown","80ba21a7":"markdown","dc517cd6":"markdown","32238eb8":"markdown","ba84fe02":"markdown","7d7e7b03":"markdown","eff83678":"markdown","ea592f96":"markdown","6e0f3b7f":"markdown","7ed6e767":"markdown","f2a7b956":"markdown","0ed58532":"markdown","82e22af8":"markdown","d5bf3237":"markdown","c14793ca":"markdown","1b99aaf4":"markdown","906ef786":"markdown","239b834d":"markdown","e57ea380":"markdown","c3111438":"markdown","807ac349":"markdown","b984580c":"markdown","91ef0882":"markdown","16c11c08":"markdown","1aca71da":"markdown","6a64b716":"markdown","0e400256":"markdown","66fc1f73":"markdown","6dc99fa8":"markdown","4982ca4e":"markdown","6319ddc4":"markdown","47e554e9":"markdown","b7315e42":"markdown","61c36f40":"markdown","5aec953a":"markdown","9b512524":"markdown","0cd56022":"markdown","fd204a7e":"markdown","97177ae5":"markdown","0b8c049d":"markdown","d8b326c2":"markdown","0688237f":"markdown","8cb23cfb":"markdown","f99d0619":"markdown","5263f159":"markdown","90b14a2f":"markdown","fb899ec3":"markdown","41825d27":"markdown","d42abc4f":"markdown","503c1bc4":"markdown","2851d725":"markdown","77dfe6c8":"markdown","997723d5":"markdown","e8ff045c":"markdown","b8a0a935":"markdown","fa90666e":"markdown","2ec6c890":"markdown","527bc01f":"markdown","779324e7":"markdown","5f708c36":"markdown","ec9cd664":"markdown","901dbc75":"markdown","acb9dafb":"markdown","ea4aeb9b":"markdown","d2fd131e":"markdown","7d91bf75":"markdown","e866e73b":"markdown","5ce4c8b3":"markdown","0607546c":"markdown","7ec28b6e":"markdown","2ff3d34c":"markdown","187abb6b":"markdown","208e32d7":"markdown","4cd6a9ae":"markdown","ecd715a2":"markdown","5efe5764":"markdown","6ed93ec3":"markdown","82b49d61":"markdown","9e02fe58":"markdown","d570f4e5":"markdown","ed4522bd":"markdown","408165f3":"markdown","ac1af216":"markdown","b80a914a":"markdown","d84c5d16":"markdown","f3b3a1dc":"markdown","140bde44":"markdown","0adb3c06":"markdown","c4dc8582":"markdown","f8f0acef":"markdown","3b0ed6bd":"markdown","33a91ba2":"markdown","843f7bdd":"markdown","9a0cdfc0":"markdown","a9fc8b63":"markdown","4583ae80":"markdown","b7d73659":"markdown","eec3dbe7":"markdown","e05533b7":"markdown","d290f01c":"markdown"},"source":{"25bf0979":"import pandas as pd\nimport numpy as np\nfrom scipy import stats \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","bb267474":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ny_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","033cb262":"train.head()","cb36b12f":"train.info()","80f40440":"plt.figure(figsize=(15,10))\nsns.heatmap(train.isnull());","d921e3ce":"print(\n    'Alley:',train['Alley'].unique(), round(train['Alley'].count()\/train.shape[0],3),'%',\n    '\\nFireplaceQu:',train['FireplaceQu'].unique(), round(train['FireplaceQu'].count()\/train.shape[0],3),'%',\n    '\\nPoolQC:',train['PoolQC'].unique(), round(train['PoolQC'].count()\/train.shape[0],3),'%',\n    '\\nFence:',train['Fence'].unique(), round(train['Fence'].count()\/train.shape[0],3),'%',\n    '\\nMiscFeature:',train['MiscFeature'].unique(), round(train['MiscFeature'].count()\/train.shape[0],3),'%',\n    )\n","f27d3b7e":"train['Alley'].fillna('No', inplace=True)\ntrain['FireplaceQu'].fillna('No', inplace=True)\ntrain['PoolQC'].fillna('No', inplace=True)\ntrain['Fence'].fillna('No', inplace=True)\ntrain['MiscFeature'].fillna('No', inplace=True)\n\ntest['Alley'].fillna('No', inplace=True)\ntest['FireplaceQu'].fillna('No', inplace=True)\ntest['PoolQC'].fillna('No', inplace=True)\ntest['Fence'].fillna('No', inplace=True)\ntest['MiscFeature'].fillna('No', inplace=True)","2cd3e3ed":"train.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","f4710a9a":"train['SalePrice'].describe()","c79e6803":"plt.figure(figsize=(15,10))\nsns.histplot(train['SalePrice'],kde=True);","0da0eba4":"fig = plt.figure(figsize = (15,10))\nfig.add_subplot(1,2,1)\nres = stats.probplot(train['SalePrice'], plot=plt)\nfig.add_subplot(1,2,2)\nres = stats.probplot(np.log(train['SalePrice']), plot=plt)","3767580a":"plt.figure(figsize=(15,10))\nsns.histplot(np.log(train['SalePrice']),kde=True);","b0bf8d69":"train['SalePrice'] = np.log(train['SalePrice'])","806b964d":"main_corr_map = abs(train.corr())>0.5","30882d63":"plt.figure(figsize=(15,10))\nsns.heatmap(main_corr_map, linewidths=3, linecolor='black');","e848d4f0":"main_corr = train.corr()['SalePrice'][abs(train.corr())['SalePrice']>0.5].head(11).sort_values(ascending=False)\nmain_corr","7cdfab9a":"main_corr_name = list(main_corr.index)","a13dee09":"fig, ax =plt.subplots(3,4, figsize=(17,10))\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[0], ax=ax[0,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[1], ax=ax[0,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[2], ax=ax[0,2])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[3], ax=ax[0,3])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[4], ax=ax[1,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[5], ax=ax[1,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[6], ax=ax[1,2])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[7], ax=ax[1,3])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[8], ax=ax[2,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[9], ax=ax[2,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[10], ax=ax[2,2]);","9f0cf029":"train.drop(train[train['TotalBsmtSF']>6000].index, inplace=True)\ntrain.drop(train[train['GrLivArea']>4500].index, inplace=True)\ntrain.drop(train[train['TotRmsAbvGrd']>12.5].index, inplace=True)\ntrain.drop(train[train['GarageArea']>1220].index, inplace=True)","729f717b":"after_main_corr = train.corr()['SalePrice'][abs(train.corr())['SalePrice']>0.5].head(11).sort_values(ascending=False)\nafter_main_corr","84ddde23":"after_main_corr - main_corr","a70d3f9b":"fig, ax =plt.subplots(3,4, figsize=(20,10))\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[0], ax=ax[0,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[1], ax=ax[0,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[2], ax=ax[0,2])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[3], ax=ax[0,3])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[4], ax=ax[1,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[5], ax=ax[1,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[6], ax=ax[1,2])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[7], ax=ax[1,3])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[8], ax=ax[2,0])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[9], ax=ax[2,1])\nsns.scatterplot(data=train, y='SalePrice', x=main_corr_name[10], ax=ax[2,2]);","270b98db":"plt.figure(figsize=(15,10))\nsns.heatmap(abs(train.drop(['SalePrice'], axis=1).corr())>0.6, linewidths=3, linecolor='black');","17ea3a57":"train.select_dtypes(exclude=['object']).columns","9df14dba":"sns.heatmap(train[['TotalBsmtSF','1stFlrSF']].corr(), annot=True);","736daff3":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.histplot(data= train, x='1stFlrSF', ax=ax[0])\nsns.histplot(data= train, x='TotalBsmtSF', ax=ax[1]);","45af99e0":"plt.figure(figsize=(15,10))\nsns.scatterplot(data=train, x='1stFlrSF', y='TotalBsmtSF');","9bd0278d":"train['CheckBsmt'] = train['TotalBsmtSF'].apply(lambda x: 0 if x==0  else 1)\ntest['CheckBsmt'] = test['TotalBsmtSF'].apply(lambda x: 0 if x==0  else 1)","03121ea8":"train['1stFlrSF'] = np.log(train['1stFlrSF'] )","b1e95e43":"sns.heatmap(train[['2ndFlrSF','GrLivArea','HalfBath', 'TotRmsAbvGrd']].corr(), annot=True);","51425284":"fig, ax =plt.subplots(1,4, figsize=(15,5))\nsns.histplot(data= train, x='2ndFlrSF', ax=ax[0])\nsns.histplot(data= train, x='GrLivArea', ax=ax[1]);\nsns.histplot(data= train, x='TotRmsAbvGrd', ax=ax[2]);\nsns.histplot(data= train, x='HalfBath', ax=ax[3]);","c0f0b03e":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.scatterplot(data=train, x='2ndFlrSF', y='GrLivArea', ax=ax[0]);\nsns.scatterplot(data=train, x='GrLivArea', y='TotRmsAbvGrd', ax=ax[1]);","caa4b2e3":"train['Check2ndFlr'] = train['2ndFlrSF'].apply(lambda x: 0 if x==0  else 1)\ntest['Check2ndFlr'] = test['2ndFlrSF'].apply(lambda x: 0 if x==0  else 1)\ntrain['GrLivArea'] = np.log(train['GrLivArea'] )","f83a2767":"sns.heatmap(train[['BsmtFullBath','BsmtFinSF1','BedroomAbvGr', 'GarageYrBlt','YearBuilt','YearRemodAdd']].corr(), annot=True);","6950a957":"fig, ax =plt.subplots(2,3, figsize=(15,7))\nsns.histplot(data= train, x='BsmtFullBath', ax=ax[0,0])\nsns.histplot(data= train, x='BsmtFinSF1', ax=ax[0,1]);\nsns.histplot(data= train, x='BedroomAbvGr', ax=ax[0,2]);\nsns.histplot(data= train, x='GarageYrBlt', ax=ax[1,0]);\nsns.histplot(data= train, x='YearBuilt', ax=ax[1,1])\nsns.histplot(data= train, x='YearRemodAdd', ax=ax[1,2]);\n","567b969f":"train['CheckBsmtFin'] = train['BsmtFinSF1'].apply(lambda x: 0 if x==0  else 1)\ntest['CheckBsmtFin'] = test['BsmtFinSF1'].apply(lambda x: 0 if x==0  else 1)","41ae3968":"train['YearDifference'] = train['YearRemodAdd'] - train['YearBuilt']\ntest['YearDifference'] = test['YearRemodAdd'] - test['YearBuilt']","f095fcba":"train['YearDifference'].unique()","af0da57a":"train['CheckGarage'] = train['GarageArea'].apply(lambda x: 0 if x==0 else 1)\ntest['CheckGarage'] = test['GarageArea'].apply(lambda x: 0 if x==0 else 1)\n\ntrain.drop(['GarageCars'],axis=1, inplace=True)\ntest.drop(['GarageCars'],axis=1, inplace=True)","2bf5e18d":"train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']","24e92207":"train_object = train.select_dtypes(include=['object'])","fad5d82c":"train_object.head()","47ba1671":"train_object.describe().T","cb6582d5":"train_object.shape[1]","f90c13b3":"check_object = []\nfor col in train_object.columns:\n    if train_object[col].value_counts().values[0]>train_object.shape[0]*0.9:\n        check_object.append(col)\ntrain.drop(check_object, axis=1, inplace=True)\ntest.drop(check_object, axis=1, inplace=True)","20914051":"train_object = train.select_dtypes(include=['object'])\ntrain_object.shape[1]","e3eb0c97":"train_object.describe().T","1b0aeec1":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.countplot(data=train, x='Exterior1st', palette='coolwarm', ax=ax[0])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\nsns.countplot(data=train, x='Exterior2nd',  palette='coolwarm', ax=ax[1])\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right');","8ca141d5":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.boxplot(data= train, y='Exterior1st', x='SalePrice', palette='coolwarm', ax=ax[0])\nsns.boxplot(data= train, y='Exterior2nd', x='SalePrice', palette='coolwarm', ax=ax[1]);","45bce6f5":"plt.figure(figsize=(15,10))\nsns.countplot(data=train, x='Neighborhood', palette='rainbow')\nplt.xticks(rotation=45);","e61ad81f":"plt.figure(figsize=(15,10))\nsns.boxplot(data= train, y='Neighborhood', palette='rainbow', x='SalePrice');","a6b21209":"plt.figure(figsize=(15,5))\nsns.countplot(data=train, x='Condition1')\nplt.xticks(rotation=45);","9547cc61":"plt.figure(figsize=(15,10))\nsns.boxplot(data= train, y='Condition1', x='SalePrice');","e7648349":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.countplot(data=train, x='SaleType', palette='Set1', ax=ax[0])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\nsns.countplot(data=train, x='HouseStyle', palette='Set1', ax=ax[1])\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right');","652f9bbe":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.boxplot(data= train, y='SaleType', x='SalePrice', palette='Set1', ax=ax[0])\nsns.boxplot(data= train, y='HouseStyle', x='SalePrice', palette='Set1', ax=ax[1]);","a95b62bb":"train_object = train_object.drop(\n    ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'Condition1', 'SaleType', 'HouseStyle'],\n    axis = 1)","fb45ac31":"object_part_1 = train_object.columns.to_list()[0:5]\nobject_part_2 = train_object.columns.to_list()[5:9]\nobject_part_3 = train_object.columns.to_list()[9:13]\nobject_part_4 = train_object.columns.to_list()[13:17]\nobject_part_5 = train_object.columns.to_list()[17:]","6713b98d":"fig, ax =plt.subplots(2,3, figsize=(15,10))\nsns.countplot(data=train, x=object_part_1[0], palette='Set2', ax=ax[0,0])\nsns.countplot(data=train, x=object_part_1[1], palette='Set2', ax=ax[0,1])\nsns.countplot(data=train, x=object_part_1[2], palette='Set2', ax=ax[0,2])\nsns.countplot(data=train, x=object_part_1[3], palette='Set2', ax=ax[1,0])\nsns.countplot(data=train, x=object_part_1[4], palette='Set2', ax=ax[1,1]);","82ee4a2d":"fig, ax =plt.subplots(2,3, figsize=(15,10))\nsns.boxplot(data= train, y=object_part_1[0], x='SalePrice', palette='Set2', ax=ax[0,0])\nsns.boxplot(data= train, y=object_part_1[1], x='SalePrice', palette='Set2', ax=ax[0,1])\nsns.boxplot(data= train, y=object_part_1[2], x='SalePrice', palette='Set2', ax=ax[0,2])\nsns.boxplot(data= train, y=object_part_1[3], x='SalePrice', palette='Set2', ax=ax[1,0])\nsns.boxplot(data= train, y=object_part_1[4], x='SalePrice', palette='Set2', ax=ax[1,1]);","306b6eeb":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.countplot(data=train, x=object_part_2[0], palette='Set3', ax=ax[0,0])\nsns.countplot(data=train, x=object_part_2[1], palette='Set3', ax=ax[0,1])\nsns.countplot(data=train, x=object_part_2[2], palette='Set3', ax=ax[1,0])\nsns.countplot(data=train, x=object_part_2[3], palette='Set3', ax=ax[1,1]);","c2efe82b":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.boxplot(data= train, y=object_part_2[0], x='SalePrice', palette='Set3', ax=ax[0,0])\nsns.boxplot(data= train, y=object_part_2[1], x='SalePrice', palette='Set3', ax=ax[0,1])\nsns.boxplot(data= train, y=object_part_2[2], x='SalePrice', palette='Set3', ax=ax[1,0])\nsns.boxplot(data= train, y=object_part_2[3], x='SalePrice', palette='Set3', ax=ax[1,1]);","577b4e2f":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.countplot(data=train, x=object_part_3[0], palette='pastel', ax=ax[0,0])\nsns.countplot(data=train, x=object_part_3[1], palette='pastel', ax=ax[0,1])\nsns.countplot(data=train, x=object_part_3[2], palette='pastel', ax=ax[1,0])\nsns.countplot(data=train, x=object_part_3[3], palette='pastel', ax=ax[1,1]);","f1dfd3b0":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.boxplot(data= train, y=object_part_3[0], x='SalePrice', palette='pastel', ax=ax[0,0])\nsns.boxplot(data= train, y=object_part_3[1], x='SalePrice', palette='pastel', ax=ax[0,1])\nsns.boxplot(data= train, y=object_part_3[2], x='SalePrice', palette='pastel', ax=ax[1,0])\nsns.boxplot(data= train, y=object_part_3[3], x='SalePrice', palette='pastel', ax=ax[1,1]);","09aa70e0":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.countplot(data=train, x=object_part_4[0], palette='rainbow', ax=ax[0,0])\nsns.countplot(data=train, x=object_part_4[1], palette='rainbow', ax=ax[0,1])\nsns.countplot(data=train, x=object_part_4[2], palette='rainbow', ax=ax[1,0])\nsns.countplot(data=train, x=object_part_4[3], palette='rainbow', ax=ax[1,1]);","4ba6d15c":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.boxplot(data= train, y=object_part_4[0], x='SalePrice', palette='rainbow', ax=ax[0,0])\nsns.boxplot(data= train, y=object_part_4[1], x='SalePrice', palette='rainbow', ax=ax[0,1])\nsns.boxplot(data= train, y=object_part_4[2], x='SalePrice', palette='rainbow', ax=ax[1,0])\nsns.boxplot(data= train, y=object_part_4[3], x='SalePrice', palette='rainbow', ax=ax[1,1]);","1ccbe822":"fig, ax =plt.subplots(3,2, figsize=(15,10))\nsns.countplot(data=train, x=object_part_5[0], palette='coolwarm', ax=ax[0,0])\nsns.countplot(data=train, x=object_part_5[1], palette='coolwarm', ax=ax[0,1])\nsns.countplot(data=train, x=object_part_5[2], palette='coolwarm', ax=ax[1,0])\nsns.countplot(data=train, x=object_part_5[3], palette='coolwarm', ax=ax[1,1]);\nsns.countplot(data=train, x=object_part_5[4], palette='coolwarm', ax=ax[2,0]);","522d95d1":"fig, ax =plt.subplots(3,2, figsize=(15,10))\nsns.boxplot(data= train, y=object_part_5[0], x='SalePrice', palette='coolwarm', ax=ax[0,0])\nsns.boxplot(data= train, y=object_part_5[1], x='SalePrice', palette='coolwarm', ax=ax[0,1])\nsns.boxplot(data= train, y=object_part_5[2], x='SalePrice', palette='coolwarm', ax=ax[1,0])\nsns.boxplot(data= train, y=object_part_5[3], x='SalePrice', palette='coolwarm', ax=ax[1,1]);\nsns.boxplot(data= train, y=object_part_5[4], x='SalePrice', palette='coolwarm', ax=ax[2,0]);","7afe49db":"plt.figure(figsize=(15,10))\nsns.heatmap(train.isnull());","d994f88e":"train.isnull().sum().head(60)","456224e9":"Bsmt = train.filter(like=\"Bsmt\")\n\nprint(\n     'BsmtQual:', Bsmt.BsmtQual.unique(),\n     '\\nBsmtCond:', Bsmt.BsmtCond.unique(),\n     '\\nBsmtExposure:', Bsmt.BsmtExposure.unique(),\n     '\\nBsmtFinType1:', Bsmt.BsmtFinType1.unique(),\n     '\\nBsmtFinType2:', Bsmt.BsmtFinType2.unique()\n     )","9d124bcb":"Bsmt[Bsmt['CheckBsmt']==0].isnull().sum()","f02381de":"train['BsmtQual'].fillna('No', inplace=True)\ntrain['BsmtCond'].fillna('No', inplace=True)\ntrain['BsmtExposure'].fillna('No', inplace=True)\ntrain['BsmtFinType1'].fillna('No', inplace=True)\ntrain['BsmtFinType2'].fillna('No', inplace=True)\n\ntest['BsmtQual'].fillna('No', inplace=True)\ntest['BsmtCond'].fillna('No', inplace=True)\ntest['BsmtExposure'].fillna('No', inplace=True)\ntest['BsmtFinType1'].fillna('No', inplace=True)\ntest['BsmtFinType2'].fillna('No', inplace=True)\n\nBsmt = train.filter(like=\"Bsmt\")\nBsmt.isnull().sum()","b6435912":"Garage = train.filter(like=\"Garage\")","29010e2e":"print(\n     'GarageType:', Garage.GarageType.unique(),\n     '\\nGarageFinish:', Garage.GarageFinish.unique(),\n     '\\nGarageQual:', Garage.GarageQual.unique(),\n     '\\nGarageYrBlt:', Garage.GarageYrBlt.unique()\n     )","888151d4":"Garage[Garage['CheckGarage']==0].isnull().sum()","0c26207e":"train['GarageType'].fillna('No', inplace=True)\ntrain['GarageFinish'].fillna('No', inplace=True)\ntrain['GarageQual'].fillna('No', inplace=True)\n\ntest['GarageType'].fillna('No', inplace=True)\ntest['GarageFinish'].fillna('No', inplace=True)\ntest['GarageQual'].fillna('No', inplace=True)\n\ntrain.drop(['GarageYrBlt'], axis=1, inplace=True)\ntest.drop(['GarageYrBlt'], axis=1, inplace=True)\n\nGarage = train.filter(like=\"Garage\")\nGarage.isnull().sum()","8e55625b":"Mas = train.filter(like=\"Mas\")\nMas[Mas['MasVnrArea']==0].isnull().sum()","90919b27":"train['MasVnrType'].fillna('No', inplace=True)\ntrain['MasVnrArea'].fillna(0, inplace=True)\n\ntest['MasVnrType'].fillna('No', inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)","1b2f62b2":"train['LotFrontage'].unique()","aaa2dd6a":"train['LotFrontage'][train['LotFrontage']==0].sum()","469cf412":"train['LotFrontage'].fillna(0, inplace=True)\n\ntest['LotFrontage'].fillna(0, inplace=True)","d2eedce4":"train.isnull().sum().any()","49f006f7":"test.isnull().sum().tail(10)","709acf5b":"print(\n     'MSZoning:', test.MSZoning.unique(),\n     '\\nExterior1st:', test.Exterior1st.unique(),\n     '\\nExterior2nd:', test.Exterior2nd.unique(),\n     '\\nTotalBsmtSF count:', test.TotalBsmtSF.nunique(),\n     '\\nBsmtFinSF1 count:', test.BsmtFinSF1.nunique(),\n     '\\nBsmtFinSF2 count:', test.BsmtFinSF2.nunique(),\n     '\\nBsmtUnfSF count:', test.BsmtUnfSF.nunique(),\n     '\\nBsmtFullBath:', test.BsmtFullBath.unique(),\n     '\\nBsmtHalfBath:', test.BsmtHalfBath.unique(),\n     '\\nKitchenQual:', test.KitchenQual.unique(),\n     '\\nGarageArea count:', test.GarageArea.nunique(),\n     '\\nSaleType:', test.SaleType.unique(),\n     '\\nTotalSF count:', test.TotalSF.nunique(),\n     )","998ab004":"test['MSZoning'].fillna(method='ffill', inplace=True)\ntest['Exterior1st'].fillna(method='ffill', inplace=True)\ntest['Exterior2nd'].fillna(method='ffill', inplace=True)\ntest['TotalBsmtSF'].fillna(method='ffill', inplace=True)\ntest['BsmtFinSF1'].fillna(method='ffill', inplace=True)\ntest['BsmtFinSF2'].fillna(method='ffill', inplace=True)\ntest['BsmtUnfSF'].fillna(method='ffill', inplace=True)\ntest['BsmtFullBath'].fillna(method='ffill', inplace=True)\ntest['BsmtHalfBath'].fillna(method='ffill', inplace=True)\ntest['KitchenQual'].fillna(method='ffill', inplace=True)\ntest['GarageArea'].fillna(method='ffill', inplace=True)\ntest['SaleType'].fillna(method='ffill', inplace=True)\ntest['TotalSF'].fillna(method='ffill', inplace=True)","f2ebdda2":"test.isnull().sum().any()","cf0fff8f":"train.reset_index(inplace=True)\ny_train = train['SalePrice']\ntrain.drop(['SalePrice'],axis=1, inplace=True)","cb960ac5":"train_size = train.shape[0]\ntest_size = test.shape[0]\ndf = pd.concat((train, test)).reset_index(drop=True)\ndf.shape","f59ec85b":"df_object = df.select_dtypes(include=['object'])\ndf_num = df.select_dtypes(exclude=['object'])","8b763a24":"df_object.shape, df_num.shape","9d693a77":"df_object = pd.get_dummies(df_object)","0a1e643f":"df_object.shape","db0fb6e1":"from sklearn.preprocessing import StandardScaler","1300215f":"scal = StandardScaler()\ndf_num_scal = scal.fit_transform(df_num)\ndf_num_scal = pd.DataFrame(df_num_scal, columns=df_num.columns, index= df_num.index)","e82bb408":"X_df = df_num_scal.join(df_object)\nX_df.shape","999f66cb":"X_df.drop(['index'],axis=1, inplace=True)","de4b834e":"X_train = X_df[:train_size]\nX_test = X_df[train_size:]","21c09cf1":"X_train.shape, X_test.shape","6a5b677f":"from sklearn.decomposition import PCA","b0ca5532":"pca = PCA(random_state=17)\npca.fit_transform(X_train)\npca.transform(X_test);","509ec7a2":"plt.figure(figsize=(15, 10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 224)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.axvline(66, c='b')\nplt.axhline(0.95, c='r')\nplt.show();","49800956":"X_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","a4948c71":"from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge","26b80bd0":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, KFold","0ca5c879":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import learning_curve","0e99f364":"from sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import StackingRegressor","163d2ff5":"kf = KFold(n_splits=5 , shuffle=True, random_state=17)","942c3388":"def print_metrics(estimator):  \n    mae = -cross_val_score(estimator, X_train, y_train, cv = kf, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n    mse = -cross_val_score(estimator, X_train, y_train, cv = kf, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n    rmse = np.sqrt(-cross_val_score(estimator, X_train, y_train, cv = kf, scoring=\"neg_mean_squared_error\", n_jobs=-1))\n    r2 = cross_val_score(estimator, X_train, y_train, cv = kf, scoring=\"r2\", error_score='raise', n_jobs=-1)\n    print(estimator,'\\n--------------------------------')\n    print('MAE:', np.mean(mae))\n    print('MSE:', np.mean(mse))\n    print('RMSE:', np.mean(rmse))\n    print('R2:', np.mean(r2))\n    print('--------------------------------\\n')","dd025f7b":"#def predict_metrics(y_train, pred):  \n#    mae = mean_absolute_error(y_train, pred)\n#    mse = mean_squared_error(y_train, pred)\n#    rmse = np.sqrt(mean_squared_error(y_train, pred))\n#    r2 = r2_score(y_train, pred)\n#    print('MAE:', mae)\n#    print('MSE:', mse)\n#    print('RMSE:', rmse)\n#    print('R2:', r2)\n#    print('')","4914052b":"def check_rmse(estimator):  \n    rmse = np.sqrt(-cross_val_score(estimator, X_train, y_train, cv = kf, scoring=\"neg_mean_squared_error\", n_jobs=-1))\n    return np.mean(rmse)","1cc2c62c":"def predict_rmse(y_train, pred):\n    rmse = np.sqrt(mean_squared_error(y_train, pred))\n    return np.mean(rmse)","28446cfd":"%%time\nlr = LinearRegression().fit(X_train, y_train)\nlasso = Lasso(random_state=17).fit(X_train, y_train)\nridge = Ridge(random_state=17).fit(X_train, y_train)\nelastic = ElasticNet(random_state=17).fit(X_train, y_train)\nsvr = SVR().fit(X_train, y_train)\nkr = KernelRidge().fit(X_train, y_train)\nsgd = SGDRegressor(random_state=17).fit(X_train, y_train)\nrs = RandomForestRegressor(random_state=17, n_jobs=-1).fit(X_train, y_train)","6ddad065":"print_metrics(lr), \nprint_metrics(lasso), \nprint_metrics(ridge),\nprint_metrics(elastic),\nprint_metrics(svr),\nprint_metrics(kr),\nprint_metrics(sgd), \nprint_metrics(rs)\n","8e24a271":"def grid_search (estimator, param):\n    gscv = GridSearchCV(estimator, param, cv=kf, n_jobs=-1, scoring='neg_mean_squared_error')\n    gscv.fit(X_train, y_train)\n    return gscv.best_estimator_\n\ndef random_search (estimator, param):\n    rscv = RandomizedSearchCV(estimator, param, cv=kf, n_jobs=-1, scoring='neg_mean_squared_error')\n    rscv.fit(X_train, y_train)\n    return rscv.best_estimator_","1ed7687b":"lasso_params = {'alpha':[.0005, .001, .0015, .01, .015, .1, 1]}\nridge_params = {'alpha':[.01, .015, .1, 1, 10, 100]}\nsgd_params = {'alpha':[.001, .01, .1, 1], 'eta0':[.001, .003, .01, .03, .1, .3, 1]}\nelastic_params ={'alpha':[.0005, .001, .0015, .01, .015, .1, 1], 'l1_ratio':[.1, .3, .5, .7, .9]}\nsvr_params = {'C': [.001, .015, .01, .1], 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\nkr_params = {'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\nrs_params = {'n_estimators': range(100, 551, 50)}\n\ngscv_param = [lasso_params, ridge_params, elastic_params, sgd_params, kr_params]\nrscv_param = [svr_params, rs_params]","482ab451":"%%time\nbest_est = []\nn=0\nfor model in [lasso, ridge, elastic, sgd, kr]:\n    best_est.append(grid_search(model, gscv_param[n]))\n    n+=1\nn=0\nfor model in [svr, rs]:\n    best_est.append(random_search(model, rscv_param[n]))\n    n+=1","d8d68adf":"best_est","4a38851f":"for model in best_est:\n    print_metrics(model)","9919f689":"best_est","0d326496":"best_est = best_est[:6]\n\nlasso = best_est[0]\nridge = best_est[1]\nelastic = best_est[2]\nsgd = best_est[3]\nkr = best_est[4]\nsvr = best_est[5]\n\nlasso_rmse = predict_rmse(y_train, lasso.predict(X_train))\nridge_rmse = predict_rmse(y_train, ridge.predict(X_train))\nelastic_rmse = predict_rmse(y_train, elastic.predict(X_train))\nsgd_rmse = predict_rmse(y_train, sgd.predict(X_train))\nkr_rmse = predict_rmse(y_train, kr.predict(X_train))\nsvr_rmse = predict_rmse(y_train, svr.predict(X_train))","99c699b9":"def plot_learning_curve(estimator, X, y):\n    \n    train_sizes, train_score, test_score = learning_curve(\n    estimator =  estimator,\n    X = X,\n    y = y, \n    train_sizes = np.linspace(0.01, 1.0, 50), \n    cv = kf,\n    n_jobs=-1,\n    shuffle=True,\n    scoring='neg_mean_squared_error',\n    random_state=17) \n    \n    mean_train = -np.mean(train_score, axis=1)\n    mean_test = -np.mean(test_score, axis=1)\n\n    plt.plot(train_sizes, mean_train, '--', color=\"b\",  label=\"Training score\")\n    plt.plot(train_sizes, mean_test ,color=\"g\", label=\"Cross-validation score\")\n\n    plt.title('Learning curve')\n    plt.xlabel(\"Size\"),\n    plt.ylabel(\"Rmse\"),\n    plt.legend(loc=\"best\")\n    plt.show()","8dc5d7c4":"plt.figure(figsize=(15,10))\nplot_learning_curve(svr, X_train, y_train)","3b6bb74f":"%%time\nvoting = VotingRegressor([('lasso',lasso),('ridge',ridge),('elastic', elastic),('svr',svr),('kr',kr),('sgd',sgd)], n_jobs=-1)\nvoting.fit(X_train, y_train)","503581ea":"print_metrics(voting)","e8fe8be2":"voting_rmse = predict_rmse(y_train, voting.predict(X_train))","52a837b5":"scores = {}\nfor model in best_est:\n    scores[str(model)[:(str(model).find('('))]] = check_rmse(model)","b6828d80":"plt.figure(figsize=(15,8))\nsns.pointplot(y = list(scores.values()), x=list(scores.keys()),markers='X');","e8ddda96":"w1, w2, w3, w4, w5, w6 = 0.16, 0.18, 0.2, 0.03, 0.35, 0.08","7bdbe967":"weight_voting = VotingRegressor([('lasso',lasso),('ridge',ridge),('elastic', elastic),('sgd',sgd),('kr',kr),('svr',svr)],\n                                weights=[w1, w2, w3, w4, w5, w6],\n                                n_jobs=-1)\nweight_voting.fit(X_train, y_train)","cbed060a":"print_metrics(weight_voting)","1cb14fa0":"weight_voting_rmse = predict_rmse(y_train, weight_voting.predict(X_train))","d124b4ce":"%%time\nstack = StackingRegressor([('lasso',lasso),('ridge',ridge),('svr',svr),('sgd',sgd),('kr',kr),('elastic',elastic)], kr, cv=kf, n_jobs=-1)\nstack.fit(X_train, y_train)","48fc9f9a":"print_metrics(stack)","e9f5933e":"stack_rmse = predict_rmse(y_train, stack.predict(X_train))","ff588f3e":"result =[lasso_rmse, ridge_rmse, elastic_rmse, sgd_rmse, kr_rmse, svr_rmse, voting_rmse, weight_voting_rmse, stack_rmse]\nmodel_name =['Lasso','Ridge','Elastic Net','SGDRegressor','Kernel Ridge','SVR','Voting','Weight voting','Stacking']","76fc6b49":"result_rmse = pd.DataFrame(result, index=model_name, columns=['RMSE']).sort_values(['RMSE']).T\nresult_rmse","fb8b6e0c":"plt.figure(figsize=(15,10))\nsns.pointplot(y = result, x=model_name,markers='X');","e939d013":"final_model = VotingRegressor([('ridge',ridge),('elastic', elastic),('kr',kr),('voting', voting),('weight_voting',weight_voting),('stack',stack)],\n                                weights=[0.1, 0.1, 0.2, 0.1, 0.15, 0.35],\n                                n_jobs=-1).fit(X_train, y_train)","fb7d0283":"predict = np.exp(final_model.predict(X_test))","8de04cb3":"submission = pd.DataFrame({'Id':y_test.Id, 'SalePrice': predict})","755b820a":"submission.to_csv('submission_house_price_final.csv', index=False)","04ac75f0":"submission.head()","30ef4bff":"Looks like these 37-38 houses just don't have a basement\n\nWe have already introduced a variable that is responsible for the presence of a basement in the house, let's look at houses that do not have a basement","d33c5e35":"In this case, we do not have a large **bias** or **variance**, and reducing these metrics is not the main goal of this notebook","75bdb077":"It is logical to assume that the relationship between these variables is **natural**, because the size of the basement will obviously depend on the size of the 1st floor of the house, but let's make sure of this and see the plots\n\n-------------\n\nFirst, let's build a boxplot for each variable","0ee54ea0":"In our notebook, the best model was **Stacking Model**, but I want to present the average of the best models, this should improve accuracy even more.","4bb63424":"--------------------\n\nWe will at once create kfold, as well as create the necessary functions for quickly displaying the necessary information","5d364aa8":"### LotFrontage missing values","cac46570":"Good class distribution at **GarageFinish**","f5fb75b0":"Indeed, these are the same values responsible for the absence of a garage in the house, let's convert them to **No**\n\nBut we also have **GarageYrBlt**, which is responsible for the year of construction of the garage. We do not want to write **No** for this variable, because we will automatically make it an object type, but we do not want to fill **0**, because this can worsen the understanding of this feature, so there is nothing left but to remove this feature","4c8a9f9e":"# Principal component analysis","8766937b":"And also apply the log transformation to **1stFlrSF**","96f2125a":"There is a clear influence on the price in the **MSZoning** variable, but it should be borne in mind that the **RL** class predominates\n\nThere is also a dependency in the **Lot Shape** variable","0afce0f6":"#### part_3","9b12717d":"The situation is much better, the variables have an imbalance, but they have at least 2 main classes","969a96ef":"\nwe have dtypes: **float-3, int-34, object-43** \n\nWe also see that there are columns with missing values, some of them are almost completely empty, let's study them first","a56a4e83":"Indeed, there is an almost linear positive relationship with many variables\n\nOn several scatterplots, such as **TotalBsmtSF**, **GrLivArea**, **TotRmsAbvGrd**, **GarageArea**, outliers are visible that can greatly affect the correlation with the target variable, you need to remove them","48a085b0":"Great, now let's look at the **Garage**, but I'm sure the missing values are due to the fact that the garage is missing","fb9fb5a3":"The need to combine data was exactly because of **get_dummies**, if we used **get_dummies** on different data, we could get a different number of columns in train and test, as some classes in features might not be in test, but they were in train, because of this we would have errors in the future","7b07bcdf":"**GarageCars**: Size of garage in car capacity\n\n**GarageArea**: Size of garage in square feet","0e465699":"# Analysis of nominative variables","8abf68d1":"Now let's look at the target variable and its correlation with the independent variables","c6826b1d":"I decided not to write a huge number of different parameters, but limited myself to the most important ones for each model.\n\nAlso, for faster parameter finding, for long-learning models such as **RandomForest** or **SVR**, I will use **RandomSearch**\n","4d97dd4e":"#### part_2","30bb1f20":"#### part_4","d5d6b032":"### MasVnrType and MasVnrArea  missing values","6fd88465":"We can see the effect on the target variable depending on the classes, this is well expressed in **ExterQual** and **Foundation**","c33eb59a":"We can see feture in which one of the classes of the feture strongly predominates\n\nFor example **Utilities** has a ratio of 1453:1\n\nSuch feature will not help us much, so we will remove those in which more than 90% is dominated by one class","017a2d8d":"**GarageFinish** has a good dependency","5dd2c17e":"Both variables almost repeat each other, the logic of using two such variables at once is not entirely clear, let's delete one of them\n\nWe will also **create a new feature** responsible for the presence of a garage in the house","dc3a9f5e":"# Target Variable Analysis","258ca28a":"We have selected **11** variables that are most correlated with the target, let's study them","76328bb4":"As we saw earlier, all this data cannot but have its own values, most likely these are really missings in the data.\n\nThere are not too many missings, so it makes no sense to apply exact methods to fill in missing values\n\nLet's fill them with the first non-missing value that occurs before it.","9e76febe":"## Hyperparameter tuning","6995bd9a":"We see a direct linear relationship between these features, not counting the values of 0 and a few outliers\n\n----------------------------\n\nTo **generate new features**, you can add a new categorical feature responsible for the presence of a basement in the house, becouse see a lot of values equal to 0","2d273c63":"Let's look at the **correlation matrix**\n\nI won't use annot=True in the heatmap settings as the numbers will overlap and the visualization won't be readable, instead I'll just set the threshold by which we look at the correlation, for example 0.5.\n\nIn order not to miss the negative correlation, we apply abs to matrix\n\nNext, I will numerically derive the most important relationships with the target variable and their true values.","62715d4a":"#### part_1","32913643":"Each of these variables is quite logically correlated with others, let's look at them\n\nTotRmsAbvGrd we analyzed earlier","44fe83f4":"## Base models","ed148c5c":"**Condition1**: Proximity to main road or railroad","b8abd37a":"A variable that correlates with all the others in the same way as **TotalBsmtSF**, which we analyzed earlier, is responsible for the area. In this case, for the area of 2 floors, **we can generate another new feature** that is responsible for the presence of 2 floors in the house\n\n**GrLivArea**, you can see that the feature is not normally distributed, but you can **apply a log transformation**\n\n**TotRmsAbvGrd** seems to be quite an important variable for the number of rooms in the entire house, not including bathrooms.\n\nWhile the **correlation between 2ndFlrSF and HalfBath is not clear**, it is possible that the presence of 2 floors directly affects the class of the bath in the house\n\n----------------------\n\nIt makes sense to look at the scatterplot only for **2ndFlrSF** and **GrLivArea**, the rest of the signs are categorical and the scatterplot will not bring much information\n\nLet's also look at **GrLivArea** and **TotRmsAbvGrd**, as there is a very strong correlation","6e374476":"Each of these classes has one of the most popular **Vinyls** class and about 3-4 equally distributed classes which are half  smaller than the most popular class, the other classes are rare","d3647124":"The logic here is the same, there is no masonry cladding, there is no type of this masonry, fill in **No** and **0**","80ba21a7":"The **NAmes** class is more common than the others, but there is no strong imbalance, the rest of the classes are about the same","dc517cd6":"In this work, I will practice ensembles of models without affecting boosting, for this method, I will make another notebook\n\nThis project will use models such as:\n\nLinearRegression\n\nLasso\n\nRidge\n\nelasticnet\n\nSVR\n\nKernel Ridge\n\nSGDRegressor\n\nRandomForestRegressor\n\nFirst, import all the necessary libraries\n","32238eb8":"### Nominative features","ba84fe02":"# Result and submission","7d7e7b03":"Great! After hyper-tuning, all our models performed much better, with the exception of **Random Forest**: hyper-tuning improved the results, but not significantly compared to the base model\n\nFor further work speed up, I will not use Random Forest, because training this model takes a lot of time, and the result is worse than other algorithms one\n\nLet's reassign the model data as the main model and save the results\n\nAs the data on Kaggle imply checking for **RMSE**, in order to reduce the training time and calculate metrics, in the future I will only consider **RMSE**","eff83678":"There are no data values, so fill in the missings how **0**","ea592f96":"## Train","6e0f3b7f":"Strong imbalance of all classes, not counting **LotShape**","7ed6e767":"Great, after analyzing all the selected variables, it can be said that the **Neighborhood** has the greatest influence on the price of a house.\n\nNow let's look at the rest of the variables, but let's make it faster and more compact.\n\nAs I said above, we will divide the remaining variables into 5 parts and look at them","f2a7b956":"We have several outliers in the target variable, we can use the **log transformation** to normalize, but first let's look at **probplot**","0ed58532":"#### Exterior1st, Exterior2nd","82e22af8":"For BsmtFinSF1, we will do the same as with **TotalBsmtSF** and **2ndFlrSF**, we **will create the CheckBsmtFin feature**\n\nYearBuilt seems to be the most important variable of the 3, but **we can create a new feature** that counts the difference between the time the house was built and when it was repaired.","d5bf3237":"There is a strong class imbalance, **Norm** is more common than the rest","c14793ca":"Great, now let's take a look at the **LotFrontage**","1b99aaf4":"# Filling in Missing Values","906ef786":"Now let's return our data to the form test and train, which was before the concatenating","239b834d":"**SaleType** has an imbalance of classes, **WD** prevails\n\n**HouseStyle** has two classes 1-2 Story, more often found above the rest","e57ea380":"## Encoding","c3111438":"## Test","807ac349":"For categorical variables, we write their unique values, for nominative variables, the number of unique values","b984580c":"# Preprocessing","91ef0882":"To scale the nominative features, we will use **StandardScaler**, since in the future we will use **PCA**, which requires scaling with a center at 0 and a variance of 1","16c11c08":"Scatteplot shows a direct linear relationship, ignoring 0 values\n\nAs mentioned above, let's create a new feature that is responsible for the presence of 2 floors in the house","1aca71da":"#### Condition1","6a64b716":"LotFrontage: Linear feet of street connected to property","0e400256":"The target variable is now **normally distributed**","66fc1f73":"# Data Analysys","6dc99fa8":"In this method, we must give our models **weights** that will decide the overall contribution of a particular model to the prediction.\n\nLet's look at the results of our models on the chart and decide what **weights** to assign to each model.","4982ca4e":"Now let's start trying ensemble methods, ranging from simple ones to more complex ones.\n\nAlthough we have already used Random Forest. This model belongs to the **bagging method** and implements its algorithm, so I will not try to apply **BaggingRegressor** in this notebook, especially Random Forest shows itself worse than other regression models.\n\n**Bagging** and **boosting** are needed to reduce one of the trade-off metrics: **bias** and **variance**.\n**Staking** does not try to minimize any of these errors.\nSuccessfully stacking simply reduces the error, and as a result, its components will also decrease.","6319ddc4":"# House Prices - Advanced Regression Techniques","47e554e9":"Use **PCA** to eliminate multicollinearity between features and get new **X_train** and **X_test** consisting of principal components\n\nIt should be understood that the main components do not mean the signs that we analyzed above.\n\nThe main component is a new features, a mixture of the original ones.\n\nThese combinations are performed in such a way that the principal components are not correlated, and most of the information in the original features is placed in the first components.","b7315e42":"#### SaleType, HouseStyle","61c36f40":"# Ensemble methods","5aec953a":"### Categorical features","9b512524":"Now let's get into the analysis of categorical features","0cd56022":"**MasVnrType**: Masonry veneer type\n\n**MasVnrArea**: Masonry veneer area in square feet","fd204a7e":"We will also remove the **Id** column, which does not carry any meaning.\n\nWe'll deal with the rest of the missings values later.","97177ae5":"Great, but we still have a lot of categorical variables\n\nLet's look at the boxplot for each and analyze the effect on the target variable\n\nTo make the visualization process more convenient, we divide the object variables into 5 equal parts\n\n**Neighborhood**, **Exterior1st**, **Exterior2nd**, **Condition1**, **SaleType**, **HouseStyle** visualize separately as they have many classes","0b8c049d":"### 1stFlrSF :TotalBsmtSF","d8b326c2":"This method is more often used in classification problems, but you can also try it for regression.\n\nThe method is that we take the arithmetic mean of our predictions","0688237f":"There is a relationship between missings and variables\n\n**Bsmt** have 37-38 missing values\n\n**Garage**  have 80 missing values\n\n**MasVnrType** and **MasVnrArea** each have 8 missing values\n\nLet's take a look at them individually and decide what to do with them, after that we'll deal with **LotFrontage**\n\nWe will carry out all operations on train and test, if there are missings in test, we will analyze them separately","8cb23cfb":"### BsmtFullBath : BsmtFinSF1\n### TotRmsAbvGrd : BedroomAbvGr\n### GarageYrBlt: YearBuilt, YearRemodAdd","f99d0619":"Good distribution in all variables except **BsmtFinType2**","5263f159":"## Weight voting Ensembles","90b14a2f":"Great, we have looked at all the significant relationships between the nominal variables, it may be worth looking into the remaining nominative features in more detail and trying to generate even more new features, but we will do this another time\n\nHowever, I **will add one feature that immediately catches your eye after studying and generating the 3 previous ones** - the total area of the house, add it and deal with categorical features","fb899ec3":"### Data aggregation","41825d27":"### Other categorical columns","d42abc4f":"## Thank you for watching it's my project, I will be grateful if you give feedback on my work in comments. I want to improve my skills and if you find mistakes in work, tell me about it, please.","503c1bc4":"**1stFlrSF**: First Floor square feet\n\n**TotalBsmtSF**: Total square feet of basement area","2851d725":"## Stacking ","77dfe6c8":"Perhaps **NaN** is also responsible for the absence, let's see if there are 0 values in this variable","997723d5":"To begin with, let's temporarily combine our datasets so that our transformations are done the same for **test** and **train**, this could have been done at the stage of filling in the missing values, but for some reason I did not think about it before\n\nNext, we divide our variables into **categorical** and **nominative** feature.\n\nAfter all the transformations, we use **join** to combine them and again **split** the dataset into **train** and **test**\n\nWe will also straightaway create **y_train** as a target variable, before that we **reset the indexes**, since we removed outliers in our train data","e8ff045c":"## Voting Ensembles","b8a0a935":"# Analysis of categorical variables","fa90666e":"Let's analyze each relationship separately, in the future I use PCA to eliminate multicollinearity, but for analysis it is worth looking at them in more detail","2ec6c890":"The correlation has not increased much, but in any case, outliers are not desirable and they need to be removed\n\nTake another look at the scatterplots without outliers","527bc01f":"Now let's look again at how the correlation with the target variable has changed","779324e7":"We introduced a variable for the presence of a garage in the house, which will make it clear that there is no garage\n","5f708c36":"**BsmtFullBath**: Basement full bathrooms\n\n**BsmtFinSF1**: Type 1 finished square feet\n\n-----------\n\n**TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n\n**Bedroom**: Number of bedrooms above basement level\n\n\n-------------\n\n**GarageYrBlt**: Year garage was built\n\n**YearBuilt**: Original construction date\n\n**YearRemodAdd**: Remodel date\n\n\n","ec9cd664":"This variable correlates with three at once, it may be worth removing it, but first, let's look at the relationships","901dbc75":"Join our processed data with **join**\n\nSince the default is the inner setting, we will not change anything","acb9dafb":"### GarageCars : GarageArea","ea4aeb9b":"**Linear Regression** is badly trained, large **MAE** and **MSE**, as well as negative **R2** indicates that the model performs worse than random distribution\n\n**Kernel ridge** is very badly trained, perhaps this is due to the fact that the standard **Kernel ridge** uses **kernel = linear**, which is not suitable for the data\n\nStandard **Lasso** and **ElasticNet** output bad results, this may mean that **L1** regularization or standard **alpha for L1** is not suitable for this goal. Since standart **ElasticNet** uses **0.5 L1** when training the model, this could affect the result\n\n**Ridge**, **SGDRegressor**, **SVR** and **Random Forest** showed good results\n\n\n--------------------\n\nExplicitly specified hyperparameters will improve the situation, let's find the best parameters for each model \n\nWe will no longer use the standard Linear Regression in this work\n","d2fd131e":"Great, we see that for 95% we need 66 main components, but we will keep all the components, since in this case we are not trying to reduce the dimension.\n\nOur goal is to eliminate **possible multicollinearity**","7d91bf75":"**2ndFlrSF**: Second floor square feet\n\n**GrLivArea**: Above grade (ground) living area square feet\n\n**HalfBath**: Half baths above grade\n\n**TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n","e866e73b":"The **WD** and **New** classes in the **SaleType** class have different effects on the target variable, although the **New** class is less common in the data, but it definitely has a dependency on **SalePrice**\n\nIn **HouseStyle**, the 3 most popular classes describe the **Sale Price** well","5ce4c8b3":"In this method, we use cross-validation to get the predictions of the **base model**, then these predictions are passed to the **meta-model**, which makes the final prediction","0607546c":"### 2ndFlrSF : GrLivArea, HalfBath, TotRmsAbvGrd","7ec28b6e":"#### Neighborhood","2ff3d34c":"### Garage missing values ","187abb6b":"Looks better than it was\n\nAs we observed in the correlation matrix, many variables correlate not only with the target variable, but also with each other, such relationships may well be multicollinearity, which will damage our future model.","208e32d7":"**Exterior1st**: Exterior covering on house\n\n**Exterior2nd**: Exterior covering on house (if more than one material)","4cd6a9ae":"## Data fields\n\n**SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n\n**MSSubClass**: The building class\n\n**MSZoning**: The general zoning classification\n\n**LotFrontage**: Linear feet of street connected to property\n\n**LotArea**: Lot size in square feet\n\n**Street**: Type of road access\n\n**Alley**: Type of alley access\n\n**LotShape**: General shape of property\n\n**LandContour**: Flatness of the property\n\n**Utilities**: Type of utilities available\n\n**LotConfig**: Lot configuration\n\n**LandSlope**: Slope of property\n\n**Neighborhood**: Physical locations within Ames city limits\n\n**Condition1**: Proximity to main road or railroad\n\n**Condition2**: Proximity to main road or railroad (if a second is present)\n\n**BldgType**: Type of dwelling\n\n**HouseStyle**: Style of dwelling\n\n**OverallQual**: Overall material and finish quality\n\n**OverallCond**: Overall condition rating\n\n**YearBuilt**: Original construction date\n\n**YearRemodAdd**: Remodel date\n\n**RoofStyle**: Type of roof\n\n**RoofMatl**: Roof material\n\n**Exterior1st**: Exterior covering on house\n\n**Exterior2nd**: Exterior covering on house (if more than one material)\n\n**MasVnrType**: Masonry veneer type\n\n**MasVnrArea**: Masonry veneer area in square feet\n\n**ExterQual**: Exterior material quality\n\n**ExterCond**: Present condition of the material on the exterior\n\n**Foundation**: Type of foundation\n\n**BsmtQual**: Height of the basement\n\n**BsmtCond**: General condition of the basement\n\n**BsmtExposure**: Walkout or garden level basement walls\n\n**BsmtFinType1**: Quality of basement finished area\n\n**BsmtFinSF1**: Type 1 finished square feet\n\n**BsmtFinType2**: Quality of second finished area (if present)\n\n**BsmtFinSF2**: Type 2 finished square feet\n\n**BsmtUnfSF**: Unfinished square feet of basement area\n\n**TotalBsmtSF**: Total square feet of basement area\n\n**Heating**: Type of heating\n\n**HeatingQC**: Heating quality and condition\n\n**CentralAir**: Central air conditioning\n\n**Electrical**: Electrical system\n\n**1stFlrSF**: First Floor square feet\n\n**2ndFlrSF**: Second floor square feet\n\n**LowQualFinSF**: Low quality finished square feet (all floors)\n\n**GrLivArea**: Above grade (ground) living area square feet\n\n**BsmtFullBath**: Basement full bathrooms\n\n**BsmtHalfBath**: Basement half bathrooms\n\n**FullBath**: Full bathrooms above grade\n\n**HalfBath**: Half baths above grade\n\n**Bedroom**: Number of bedrooms above basement level\n\n**Kitchen**: Number of kitchens\n\n**KitchenQual**: Kitchen quality\n\n**TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n\n**Functional**: Home functionality rating\n\n**Fireplaces**: Number of fireplaces\n\n**FireplaceQu**: Fireplace quality\n\n**GarageType**: Garage location\n\n**GarageYrBlt**: Year garage was built\n\n**GarageFinish**: Interior finish of the garage\n\n**GarageCars**: Size of garage in car capacity\n\n**GarageArea**: Size of garage in square feet\n\n**GarageQual**: Garage quality\n\n**GarageCond**: Garage condition\n\n**PavedDrive**: Paved driveway\n\n**WoodDeckSF**: Wood deck area in square feet\n\n**OpenPorchSF**: Open porch area in square feet\n\n**EnclosedPorch**: Enclosed porch area in square feet\n\n**3SsnPorch**: Three season porch area in square feet\n\n**ScreenPorch**: Screen porch area in square feet\n\n**PoolArea**: Pool area in square feet\n\n**PoolQC**: Pool quality\n\n**Fence**: Fence quality\n\n**MiscFeature**: Miscellaneous feature not covered in other categories\n\n**MiscVal**: $Value of miscellaneous feature\n\n**MoSold**: Month Sold\n\n**YrSold**: Year Sold\n\n**SaleType**: Type of sale\n\n**SaleCondition**: Condition of sale","ecd715a2":"--------------------------\nIf we analyze the variables in more detail, we can assume that\n\nthat NaN in this case is not a missing, but an identifier that there is no presence of a certain object in the house.\n\nFor example, **Alley** NaN may mean that there is no access through the alley to the house\n\n**FireplaceQu** NaN means that there is no fireplace in the house at all\n\n**PoolQC** NaN means no pool\n\n**Fence** NaN means no fence\n\n**MiscFeature** NaN means the absence of various features\n\n--------------------\n\nBut this assumption does not have to be true or it can be half true, in which case we will mark the real NaNs as an absence target, which is also not a good solution.\n\n\nI'll still assume that NaN is indeed the absence factor and re-encode to class **'No'**","5efe5764":"**Neighborhood**: Physical locations within Ames city limits","6ed93ec3":"All other classes fall into the **Norm** whisker zone","82b49d61":"**GarageType**: Garage location\n\n**GarageYrBlt**: Year garage was built\n\n**GarageFinish**: Interior finish of the garage\n\n**GarageQual**: Garage quality\n","9e02fe58":"**Alley:** Type of alley access\n\n**FireplaceQu:** Fireplace quality\n\n**PoolQC:** Pool quality\n\n**Fence:** Fence quality\n\n**MiscFeature:** Miscellaneous feature not covered in other categories","d570f4e5":"#### part_5","ed4522bd":"This variable strongly affects the target one, for example, **NoRidge** and **Mitchell** have approximately the same class distribution, but **SalePrice** is very different for them","408165f3":"## Check columns with the most missing values","ac1af216":"Remove the index column that appeared after concatenating","b80a914a":"Great, now we will train the basic models and look at the results, then we will find the optimal parameters for them","d84c5d16":"### Bsmt missing values","f3b3a1dc":"Let's have a look at the correlation matrix again, but increase the correlation selection threshold to 0.65 to catch linear relationships between variables","140bde44":"Great, these are exactly the values we were looking for, let's convert **NaN to No**, which will indicate that this parameter is missing","0adb3c06":"Let me remind you that **boosting** is also not intentionally used in this work, the goal of this notebook is not to get the best result, but to learn how to apply only some **ensemble methods** of machine learning.","c4dc8582":"**SaleType**: Type of sale\n\n**HouseStyle**: Style of dwelling","f8f0acef":"Great, all missing values have been restored\n","3b0ed6bd":"# Machine Learning","33a91ba2":"**BsmtQual** has a strong effect on the target variable\n\nYou can also see that with a light class balance in **BsmtFinType1**, you can see how the **GLQ** class affects the target variable","843f7bdd":"-----------------\n\nAfter analyzing all the categorical variables, it can be concluded that many of them have a strong influence on the target variable.\n\nMany variables after cleaning still have a strong imbalance, and some of them have an almost linear relationship with the target variable\n\n\nNow you need to deal with the missing values in the data","9a0cdfc0":"All variables except **BsmtFinType2** have an effect on **SalePrice**","a9fc8b63":"It can be seen that the type of class affects the Sale Price, for example, the **VinylSd** box expresses the high cost of the house more strongly than **MetalSD** or **Wd Shng**, even though they fall into the VinylSd whiskers zone","4583ae80":"Good class distribution in **BsmtFinType1** ","b7d73659":"We see that the distribution of these variables is not normal due to values more than 2500, as well as due to the high number of values where there is no basement in the houses\n\nHowever, you can apply a **log transformation to 1stFlrSF**, thereby possibly improving the linear relationship with the target variable and trying to get rid of it between these two features manually, but before that let's look at the scatterplot","eec3dbe7":"### Neighborhood, Exterior1st, Exterior2nd, Condition1, SaleType, HouseStyle","e05533b7":"**BsmtQual**: Height of the basement\n\n**BsmtCond**: General condition of the basement\n\n**BsmtExposure**: Walkout or garden level basement walls\n\n**BsmtFinType1**: Quality of basement finished area\n\n**BsmtFinType2**: Quality of second finished area (if present)","d290f01c":"Here I will try ensemble algorithms like:\n\n1. **Voting Ensembles**\n2. **Weight voting Ensembles**\n3. **Stacking**"}}