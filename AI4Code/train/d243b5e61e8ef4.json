{"cell_type":{"ced798e3":"code","f9805c8a":"code","cc548e2d":"code","f46bd30f":"code","13cfd2e8":"code","a09dbb0b":"code","f465d0ed":"code","24e05c89":"code","c93a7947":"code","c693ec30":"code","2d3a9a4b":"code","b8a2124a":"code","f8a3ab98":"code","6ad8b076":"code","97ca2578":"code","40a83ddc":"code","28b38274":"code","6103094f":"code","eedd73a5":"code","9aee1379":"code","45e40f71":"code","765db7f7":"code","a88f91d9":"code","ac29873a":"code","b5722980":"code","db1b621b":"code","d6e06bed":"code","604deeb4":"code","8642d6ca":"code","b55fa019":"code","fc09ef48":"code","899faaa3":"code","4bda9961":"code","9879c023":"code","028dc481":"code","bf7c3481":"code","84fe13d8":"code","bd962543":"code","55fb668d":"code","40d2fbbd":"code","0db8e93c":"code","a01d4efc":"code","9e6e9145":"code","b9bc0582":"code","b9728dee":"code","a07a3a64":"code","2c44c789":"code","604a5bc0":"code","146e2c2d":"code","d1ba009f":"code","bca449c1":"code","d1887c6f":"code","d186ad4d":"code","2fd13004":"code","8898b312":"code","df6c8d33":"code","fcf833e9":"code","9dad3fd2":"code","35158f06":"code","bfbadb19":"code","54857448":"code","35643dc1":"code","70f63e53":"code","14f25df4":"code","a9c0b54c":"code","3272b3f0":"code","e3184979":"code","644f6092":"code","13a8b02c":"code","fb56b681":"code","db2cebbf":"markdown","2b938e2e":"markdown","1218c163":"markdown","59fc755d":"markdown","0db0f6a4":"markdown","0c02acd0":"markdown","a6726df7":"markdown","9bd4c0a9":"markdown","b9448ca3":"markdown","85537c22":"markdown","aab8e3b0":"markdown","247c1739":"markdown","7a6c3019":"markdown","a5722325":"markdown","89699fb3":"markdown","a7d79b01":"markdown","e13c64e3":"markdown","f84f66c9":"markdown","48d07765":"markdown","dc0d0067":"markdown","fe6b9bab":"markdown","4600f6a9":"markdown","8060a447":"markdown","2f07b1b4":"markdown","5aaab2e8":"markdown","6003b000":"markdown","ef041e5e":"markdown","1f800650":"markdown","e39efb15":"markdown","70ed8bf5":"markdown","7fbbdc26":"markdown","c47028ca":"markdown","dc5eaa60":"markdown","e3ab49f6":"markdown","eb1ebe4b":"markdown","adf32263":"markdown","8ca0f1f0":"markdown","6ca140bf":"markdown","e5ba9b98":"markdown","9bc7d318":"markdown","61eae5f9":"markdown","7c9ad3be":"markdown","40242f99":"markdown","2883b783":"markdown","a12770e6":"markdown","47f667f3":"markdown","3e62bab1":"markdown","bd728f34":"markdown","6f199698":"markdown","fde21ce8":"markdown","92c30100":"markdown","50cec429":"markdown","acae3a07":"markdown","5fc41489":"markdown","1ec1e4ac":"markdown","84c38304":"markdown","14828e45":"markdown","29c1f56f":"markdown","435dca42":"markdown","7334ca33":"markdown","4e71a6fd":"markdown","ea41a165":"markdown","a9276bd2":"markdown","6eb66a45":"markdown","09627b3e":"markdown","6184d945":"markdown","c9755d3d":"markdown","6bef77db":"markdown","8ac8161a":"markdown","9d9ff0ad":"markdown","f03e863f":"markdown","a2e2b8ee":"markdown","aad3ca59":"markdown","d746116f":"markdown","3b1a648f":"markdown"},"source":{"ced798e3":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nimport plotly.figure_factory as ff","f9805c8a":"df = pd.read_csv('..\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv')\nethnicity_list = [\"White\", \"Black\", \"Asian\", \"Indian\", \"Hispanic\"]\ngender_list = ['Male', 'Female']\nage_list = ['Baby','Child','Adult','Elderly']\ndf.drop (columns={'img_name'},inplace=True)\ndf.head()","cc548e2d":"print('Total rows: {}'.format(df.shape[0]))\nprint('Total columns: {}'.format(df.shape[1]))","f46bd30f":"# Converting pixels into numpy array\ndf['pixels']=df['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\n\n# normalizing pixels data\ndf['pixels'] = df['pixels']\/255\n\nX = np.array(df['pixels'].tolist())\nX = X.reshape(-1,1,48,48)\nX.shape","13cfd2e8":"df['age_bins'] = pd.cut(df['age'],bins=[0,2,17,65,150],labels=range(4))","a09dbb0b":"df.head()","f465d0ed":"# calculating distributions\nage_dist = df['age_bins'].value_counts()\nage_dist.index = age_list\nethnicity_dist = df['ethnicity'].value_counts()\nethnicity_dist.index = ethnicity_list\ngender_dist = df['gender'].value_counts()\ngender_dist.index = gender_list","24e05c89":"px.bar(x=age_dist.index, y=age_dist.values, title='Age Distribution', labels={'x':'age','y':'count'}, width=800, height=400)","c93a7947":"px.bar(x=ethnicity_dist.index, y=ethnicity_dist.values, title='Ethnicity Distribution', labels={'x':'Ethnicity','y':'count'}, width=800, height=400)","c693ec30":"px.bar(x=gender_dist.index, y=gender_dist.values, title='Gender Distribution', labels={'x':'Gender','y':'count'}, width=800, height=400)","2d3a9a4b":"plt.figure(figsize=(14,14))\nfor i in range(3720,3736):\n  plt.subplot(4,4,(i%16)+1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.imshow(X[i].squeeze(),cmap='gray')\n  plt.title(ethnicity_list[df['ethnicity'].iloc[i]] + \" \" + gender_list[df['gender'].iloc[i]] + \" Age:\" + str(df['age'].iloc[i]))\n  ","b8a2124a":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n","f8a3ab98":"device = get_default_device()\ndevice","6ad8b076":"random_seed = 42\ntorch.manual_seed(random_seed)\ndef train_validation_test(dataset, val_size: int = 0, test_size: int = 0):\n  train_size = len(dataset) - val_size - test_size \n  if (test_size == 0 or val_size == 0 ):\n    return random_split(dataset, [train_size, (val_size + test_size)])\n  else:\n    return random_split(dataset, [train_size, val_size, test_size])","97ca2578":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","40a83ddc":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\ndef plot_accuracies(history):\n  accuracies = [x['val_acc'] for x in history]\n  fig = px.line(x = range(len(accuracies)), y = accuracies, title='Accuracy vs. No. of epochs', labels={'x': 'Epoch', 'y':'Accuracy'},width=800, height=400)\n  fig.show()\n\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x = list(range(len(train_losses))), y = train_losses,\n                        mode='lines+markers',\n                        name='Training'))\n    fig.add_trace(go.Scatter(x = list(range(len(val_losses))), y = val_losses,\n                        mode='lines+markers',\n                        name='Validation'))\n    fig.update_layout(title='Loss vs. No. of epochs',\n                   xaxis_title='Epoch',\n                   yaxis_title='Loss', width=800, height=400)\n    fig.show()\n\n# Helper function which returns the predicted label for a single image tensor\ndef predict_label(img, model, label_classes):\n  # Convert to a batch of 1\n  xb = to_device(img.unsqueeze(0), device)\n  # Get predictions from model\n  yb = model(xb)\n  # Pick index with highest probability\n  _, preds  = torch.max(yb, dim=1)\n  # Retrieve the class label\n  if label_classes:\n    return label_classes[preds[0].item()]\n  else: \n    return preds[0].item()\n","28b38274":"def draw_confusion_matrix(class_labels,model, batch_loader):\n  nb_classes = len(class_labels)\n\n  confusion_matrix = torch.zeros(nb_classes, nb_classes)\n  with torch.no_grad():\n      for (images, classes) in batch_loader:\n        outputs = model(images)\n        _, preds = torch.max(outputs, dim=1)\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n          confusion_matrix[t.long(), p.long()] += 1\n\n\n  # set up figure \n  fig = ff.create_annotated_heatmap(confusion_matrix.tolist(), x=class_labels, y=class_labels, colorscale='Viridis')\n\n\n  # add custom xaxis title\n  fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                          x=0.5,\n                          y=-0.15,\n                          showarrow=False,\n                          text=\"Predicted value\",\n                          xref=\"paper\",\n                          yref=\"paper\"))\n\n  # add custom yaxis title\n  fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n                          x=-0.15,\n                          y=0.5,\n                          showarrow=False,\n                          text=\"Real value\",\n                          textangle=-90,\n                          xref=\"paper\",\n                          yref=\"paper\"))\n\n  # adjust margins to make room for yaxis title\n  fig.update_layout(margin=dict(t=50, l=200))\n\n  # add colorbar\n  fig['data'][0]['showscale'] = True\n  fig.show()","6103094f":"from sklearn.metrics import f1_score, precision_score, recall_score\n\ndef get_metrics(batch_loader, model, average_setting = 'binary'):\n  preds=[];truth=[]\n  with torch.no_grad():\n      for (images, classes) in batch_loader:\n        outputs = model(images)\n        _, pred = torch.max(outputs, dim=1)\n        preds = preds + pred.cpu().tolist()\n        truth = truth + classes.cpu().tolist()\n  print('F1: {}'.format(f1_score(truth, preds,average=average_setting)))\n  print('Precision: {}'.format(precision_score(truth, preds, average=average_setting)))\n  print('Recall: {}'.format(recall_score(truth, preds, average=average_setting)))","eedd73a5":"X_tensor = torch.from_numpy(X)\ny = torch.from_numpy(np.array(df['gender']))\ndataset = TensorDataset(X_tensor,y)","9aee1379":"dataset.classes = gender_list\nprint(dataset.classes)","45e40f71":"# split dataset train\/validation\/test\ntrain_ds, val_ds, test_ds = train_validation_test(dataset=dataset,val_size=2500,test_size=1000)\n\nbatch_size=128\n# create batchs using dataloader\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)","765db7f7":"class GenderModel(ImageClassificationBase):\n  def __init__(self):\n    super().__init__()\n    self.network = nn.Sequential(\n        # 1st Convolution Layer\n        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),      # 1 x 48 x 48 -> 16 x 48 x 48\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),                                              # 16 x 48 x 48 -> 16 x 24 x 24\n        nn.BatchNorm2d(16),\n        nn.Dropout(0.2),\n\n        # 2nd Convolution Layer\n        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),                # 16 x 24 x 24 -> 32 x 22 x 22\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),                                              # 16 x 24 x 24 -> 32 x 22 x 22\n        nn.BatchNorm2d(32),\n        nn.Dropout(0.2),\n\n        # 3rd Convolution Layer\n        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(64),\n        nn.Dropout(0.2),\n\n         # 4th Convolution Layer\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(128),\n        nn.Dropout(0.2),\n\n        # fully connected layer\n        nn.Flatten(),\n        nn.Linear(128 * 1 * 1,128),\n        nn.Dropout(0.5),\n        nn.Linear(128,256),\n        nn.Dropout(0.5),\n        nn.Linear(256,2)\n    )\n  def forward(self, images):\n    return self.network(images)","a88f91d9":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nmodel = to_device(GenderModel(), device)","ac29873a":"history = evaluate(model, val_dl)\nprint(\"Untrained Accuracy: \", history['val_acc'])\nprint(\"Untrained Loss: \", history['val_loss'])","b5722980":"num_epochs = 15\nopt_func = torch.optim.Adam\nlr = 0.001","db1b621b":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","d6e06bed":"plot_accuracies(history)","604deeb4":"plot_losses(history)","8642d6ca":"get_metrics(train_dl,model)","b55fa019":"index=np.random.randint(0,712,25) # predicted label and their images\n\nplt.figure(figsize=(16,16))\n\nfor i in range(len(index)):\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test_ds[index[i]][0].squeeze())\n    plt.xlabel(\"Predicted :\" + predict_label(test_ds[index[i]][0],model,dataset.classes) \n    )\n    \nplt.show()","fc09ef48":"test_dl = DeviceDataLoader(DataLoader(test_ds, batch_size*2), device)\nresult = evaluate(model, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])\nget_metrics(test_dl,model)","899faaa3":"draw_confusion_matrix(gender_list,model,test_dl)","4bda9961":"folder_path = '.\/'\nmodel_pth = 'GenderModel1.pth'","9879c023":"torch.save(model.state_dict(), folder_path + model_pth)","028dc481":"model2 = to_device(GenderModel(), device)","bf7c3481":"model2.load_state_dict(torch.load(folder_path+ model_pth,map_location=torch.device('cpu')))\nresult = evaluate(model2, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])","84fe13d8":"X_tensor = torch.from_numpy(X)\ny = torch.from_numpy(np.array(df['ethnicity']))\ndataset = TensorDataset(X_tensor,y)","bd962543":"dataset.classes = ethnicity_list\nprint(dataset.classes)","55fb668d":"# split dataset train\/validation\/test\ntrain_ds, val_ds, test_ds = train_validation_test(dataset=dataset,val_size=2500,test_size=1000)\n\nbatch_size=128\n# create batchs using dataloader\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)","40d2fbbd":"class EthnicityModel(ImageClassificationBase):\n  def __init__(self):\n    super().__init__()\n    self.network = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(16),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(32),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(64),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(128),\n        nn.Dropout(0.2),\n\n        nn.Flatten(),\n        nn.Linear(128 * 1 * 1,128),\n        nn.Dropout(0.5),\n        nn.Linear(128,256),\n        nn.Dropout(0.5),\n        nn.Linear(256,5)\n    )\n  def forward(self, images):\n    return self.network(images)","0db8e93c":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nmodel = to_device(EthnicityModel(), device)\nmodel","a01d4efc":"history = evaluate(model, val_dl)\nprint(\"Untrained Accuracy: \", history['val_acc'])\nprint(\"Untrained Loss: \", history['val_loss'])","9e6e9145":"num_epochs = 20\nopt_func = torch.optim.Adam\nlr = 0.001","b9bc0582":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","b9728dee":"plot_accuracies(history)","a07a3a64":"plot_losses(history)","2c44c789":"get_metrics(train_dl,model, average_setting='weighted')","604a5bc0":"index=np.random.randint(0,712,25) # predicted label and their images\n\nplt.figure(figsize=(16,16))\n\nfor i in range(len(index)):\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test_ds[index[i]][0].squeeze())\n    plt.xlabel(\"Predicted :\" + predict_label(test_ds[index[i]][0],model,dataset.classes) \n    )\n    \nplt.show()","146e2c2d":"test_dl = DeviceDataLoader(DataLoader(test_ds, batch_size*2), device)\nresult = evaluate(model, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])\nget_metrics(test_dl,model,'weighted')","d1ba009f":"draw_confusion_matrix(ethnicity_list,model,test_dl)","bca449c1":"folder_path = '.\/'\nmodel_pth = 'EthnicityModel1.pth'\ntorch.save(model.state_dict(), folder_path + model_pth)","d1887c6f":" model2 = to_device(EthnicityModel(), device)","d186ad4d":"model2.load_state_dict(torch.load(folder_path+ model_pth))\nresult = evaluate(model2, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])","2fd13004":"X_tensor = torch.from_numpy(X)\ny = torch.from_numpy(np.array(df['age_bins']))\ndataset = TensorDataset(X_tensor,y)","8898b312":"dataset.classes = age_list\nprint(dataset.classes)","df6c8d33":"# split dataset train\/validation\/test\ntrain_ds, val_ds, test_ds = train_validation_test(dataset=dataset,val_size=1500,test_size=500)\n\nbatch_size=128\n# create batchs using dataloader\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)","fcf833e9":"class AgeModel(ImageClassificationBase):\n  def __init__(self):\n    super().__init__()\n    self.network = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(16),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(32),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(64),\n        nn.Dropout(0.2),\n\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.BatchNorm2d(128),\n        nn.Dropout(0.2),\n\n        nn.Flatten(),\n        nn.Linear(128 * 1 * 1,128),\n        nn.Dropout(0.5),\n        nn.Linear(128,256),\n        nn.Dropout(0.5),\n        nn.Linear(256,4)\n    )\n  def forward(self, images):\n    return self.network(images)","9dad3fd2":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nmodel = to_device(AgeModel(), device)\nmodel","35158f06":"history = evaluate(model, val_dl)\nprint(\"Untrained Accuracy: \", history['val_acc'])\nprint(\"Untrained Loss: \", history['val_loss'])","bfbadb19":"num_epochs = 20\nopt_func = torch.optim.Adam\nlr = 0.001","54857448":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","35643dc1":"plot_accuracies(history)","70f63e53":"plot_losses(history)","14f25df4":"get_metrics(train_dl,model, 'weighted')","a9c0b54c":"index=np.random.randint(0,500,25) # predicted label and their images\n\nplt.figure(figsize=(16,16))\n\nfor i in range(len(index)):\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test_ds[index[i]][0].squeeze())\n    plt.xlabel(\"(\"+age_list[test_ds[index[i]][1].item()]+\") Predicted: \" + predict_label(test_ds[index[i]][0],model,dataset.classes) \n    )\n    \nplt.show()","3272b3f0":"test_dl = DeviceDataLoader(DataLoader(test_ds, batch_size*2), device)\nresult = evaluate(model, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])\nget_metrics(test_dl,model, 'weighted')","e3184979":"draw_confusion_matrix(age_list,model,test_dl)","644f6092":"folder_path = '.\/'\nmodel_pth = 'AgeModel1.pth'\ntorch.save(model.state_dict(), folder_path + model_pth)","13a8b02c":"model2 = to_device(AgeModel(), device)","fb56b681":"model2.load_state_dict(torch.load(folder_path+ model_pth))\nresult = evaluate(model2, test_dl)\nprint(\"Test Accuracy: \",result['val_acc'])\nprint(\"Test Loss: \",result['val_loss'])","db2cebbf":"We can also plot the training and validation losses to study the trend.","2b938e2e":"### Evaluating training history <a id=\"5.4\"><\/a>","1218c163":"We can also plot the valdation set accuracies to study how the model improves over time.","59fc755d":"Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch.","0db0f6a4":"### Saving model <a id=\"7.6\"><\/a>","0c02acd0":"We can also plot the valdation set accuracies to study how the model improves over time.","a6726df7":"The numeric label for each element corresponds to index of the element's label in the list of classes.","9bd4c0a9":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","b9448ca3":"### For evaluation metrics <a id=\"4.5\"><\/a>","85537c22":"We can also plot the valdation set accuracies to study how the model improves over time.","aab8e3b0":"Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch.","247c1739":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","7a6c3019":"We can also plot the training and validation losses to study the trend.","a5722325":"We can also plot the training and validation losses to study the trend.","89699fb3":"As a final step, let's also look at the overall loss and accuracy of the model on the test set. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data).","a7d79b01":"### Split dataset and create dataloader <a id=\"7.2\"><\/a>","e13c64e3":"### Split dataset and create dataloader <a id=\"6.2\"><\/a>","f84f66c9":"As a final step, let's also look at the overall loss and accuracy of the model on the test set. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data).","48d07765":"### Build and train model <a id=\"7.3\"><\/a>","dc0d0067":"## Model for Age Prediction <a id=\"07\"><\/a>","fe6b9bab":"While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset.","4600f6a9":"### Build and train model <a id=\"5.3\"><\/a>","8060a447":"From the code above you can see that the bins are:\n\n- 0 to 2 = \u2018Toddler\/Baby\u2019\n- 3 to 17 = \u2018Child\u2019\n- 18 to 65 = \u2018Adult\u2019\n- above 65 =\u2019Elderly\u2019\n","2f07b1b4":"### Evaluating training history <a id=\"6.4\"><\/a>","5aaab2e8":"### Ethnicity Distribution <a id=\"3.2\"><\/a>","6003b000":"### Saving model <a id=\"6.6\"><\/a>","ef041e5e":"### For training and validation <a id=\"4.4\"><\/a>","1f800650":"### Performance on Test Data <a id=\"7.5\"><\/a>","e39efb15":"The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model.","70ed8bf5":"### Gender Distribution <a id=\"3.3\"><\/a>","7fbbdc26":"The numeric label for each element corresponds to index of the element's label in the list of classes.","c47028ca":"### Create TensorDataset <a id=\"5.1\"><\/a>","dc5eaa60":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","e3ab49f6":"## Distributions <a id=\"03\"><\/a>","eb1ebe4b":"### Performance on Test Data <a id=\"6.5\"><\/a>","adf32263":"## Table of content","8ca0f1f0":"### Build and train model <a id=\"6.3\"><\/a>","6ca140bf":"## Further Work <a id=\"8\"><\/a>","e5ba9b98":"The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method.","9bc7d318":"As a final step, let's also look at the overall loss and accuracy of the model on the test set. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data).","61eae5f9":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","7c9ad3be":"### Create TensorDataset <a id=\"7.1\"><\/a>","40242f99":"Based on where you're running this notebook, your default device could be a CPU (torch.device('cpu')) or a GPU (torch.device('cuda'))","2883b783":"While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset.","a12770e6":"The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method.","47f667f3":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU as required.","3e62bab1":"While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.","bd728f34":"There's a lot of scope to experiment here, and we can use the interactive nature of Jupyter to play around with the various parameters. Here are a few ideas:\n* Try chaging the hyperparameters to achieve a higher accuracy within fewer epochs.\n* Try adding more convolutional layers, or increasing the number of channels in each convolutional layer\n\nIn Future, we can try to combine these three models into a singular model branched such that it's becomes capable of predicting each of these three targets simulatenously. We can also imply transfer learning algorithms for possible improvement. in accuracy.\n\nAnother possible addition to this note book would be face detection from images that can then be passed to these trained models for age, gender and ethnicity predictions.","6f199698":"### Sample Images <a id=\"3.4\"><\/a>","fde21ce8":"The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model.","92c30100":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","50cec429":"The numeric label for each element corresponds to index of the element's label in the list of classes.","acae3a07":"## Model for Ethnicity Prediction <a id=\"06\"><\/a>","5fc41489":"## Acknowledgments <a id=\"9\"><\/a>\nFor Creating Helper Functions [PyTorch for Deep Learning - Full Course \/ Tutorial](https:\/\/www.youtube.com\/watch?v=GIsg-ZUy0MY&t=24544s&ab_channel=freeCodeCamp.org)","1ec1e4ac":"## Model for Gender Prediction <a id=\"05\"><\/a>","84c38304":"The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method.","14828e45":"While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset.","29c1f56f":"## Loading Dataset <a id=\"02\"><\/a>","435dca42":"1. [Importing relevent libraries](#01)\n\n2. [Loading Dataset](#02)\n\n3. [Distributions](#03)\n    - [Age Distribution](#3.1)\n    - [Ethnicity Distribution](#3.2)\n    - [Gender Distribution](#3.3)\n    - [Sample Images](#3.4)\n\n4. [Helper Functions and Classes](#04)\n    - [For GPU utilization](#4.1)\n    - [For splitting dataset](#4.2)\n    - [For image classification](#4.3)\n    - [For training and validation](#4.4)\n    - [For evaluation metrics](#4.5)\n\n5. [Model for Gender Prediction](#05)\n    - [Create TensorDataset](#5.1)\n    - [Split dataset and create dataloader](#5.2)\n    - [Build and train model](#5.3)\n    - [Evaluating training history](#5.4)\n    - [Performance on test data](#5.5)\n    - [Saving model](#5.6)\n\n6. [Model for Ethnicity Prediction](#06)\n    - [Create TensorDataset](#6.1)\n    - [Split dataset and create dataloader](#6.2)\n    - [Build and train model](#6.3)\n    - [Evaluating training history](#6.4)\n    - [Performance on test data](#6.5)\n    - [Saving model](#6.6)\n\n7. [Model for Age Prediction](#07)\n    - [Create TensorDataset](#7.1)\n    - [Split dataset and create dataloader](#7.2)\n    - [Build and train model](#7.3)\n    - [Evaluating training history](#7.4)\n    - [Performance on test data](#7.5)\n    - [Saving model](#7.6)\n\n8. [Future Work](#08)\n  ","7334ca33":"Let's look at element from the dataframe. the Pixel column consist of pixel values as strings. In order to apply any deep learning Algorithm we need to first convert these pixel values to a desirable format i.e. each image in dataset to be of the shape `(1, 48, 48)` ","4e71a6fd":"We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set.","ea41a165":"## Helper Functions and Classes <a id=\"04\"><\/a>","a9276bd2":"### Performance on Test Data <a id=\"5.5\"><\/a>","6eb66a45":"### Evaluating training history <a id=\"7.4\"><\/a>","09627b3e":"Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch.","6184d945":"### Age Distribtion <a id=\"3.1\"><\/a>","c9755d3d":"### Saving model <a id=\"5.6\"><\/a>","6bef77db":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","8ac8161a":"### For GPU utilization <a id=\"4.1\"><\/a>\n","9d9ff0ad":"The initial accuracy is around 50%, which is what one might expect from a randomly intialized model (since it has a 1 in 2 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model.","f03e863f":"### Split dataset and create dataloader <a id=\"5.2\"><\/a>","a2e2b8ee":"## Importing relevent libraries <a id=\"01\"><\/a>","aad3ca59":"### For splitting dataset <a id=\"4.2\"><\/a>\n","d746116f":"### For image classification <a id=\"4.3\"><\/a>","3b1a648f":"### Create TensorDataset <a id=\"6.1\"><\/a>"}}