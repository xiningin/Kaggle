{"cell_type":{"b88aefee":"code","f7690feb":"code","808d10d7":"code","41b9465a":"code","d518189d":"code","62651080":"code","20a5bc24":"code","fed9ab49":"code","454ba136":"code","22c05562":"code","f4ba1547":"code","cc4b7f15":"code","d87886f1":"code","6d987c4e":"code","8957a4ae":"code","3c59f36d":"code","d08c0a60":"code","a5661d32":"markdown","7ade4271":"markdown","6f466a55":"markdown","a518195a":"markdown","b64104e3":"markdown","6c765cb4":"markdown","3a317a54":"markdown","2badb43b":"markdown","f36854e3":"markdown","2231080d":"markdown","d9eb1f75":"markdown","a043d90a":"markdown","d7ddd14c":"markdown","61950527":"markdown","6ea9a08a":"markdown","3c4d2899":"markdown","9b4ff15d":"markdown","a5a53154":"markdown"},"source":{"b88aefee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plotting\n\n# Read data and make Date column index\ndjia = pd.read_csv(\"..\/input\/DJIA_table.csv\", parse_dates=['Date'], index_col='Date')\nprint(djia.head())","f7690feb":"ts = djia['Open']\nts = ts.head(100)\nplt.plot(ts)","808d10d7":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(tieseries):\n\n    rollmean = tieseries.rolling(window=12).mean()\n    rollstd = tieseries.rolling(window=12).std()\n\n    plt.plot(tieseries, color=\"blue\", label=\"Original\")\n    plt.plot(rollmean, color=\"red\", label=\"Rolling Mean\")\n    plt.plot(rollstd, color=\"black\", label=\"Rolling Std\")    \n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(tieseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput,'%f' % (1\/10**8))","41b9465a":"test_stationarity(ts)","d518189d":"import matplotlib.gridspec as gridspec\n\nts_log = np.log(ts)\nfig = plt.figure(constrained_layout = True)\ngs_1 = gridspec.GridSpec(2, 3, figure = fig)\nax_1 = fig.add_subplot(gs_1[0, :])\nax_1.plot(ts_log)\nax_1.set_xlabel('time')\nax_1.set_ylabel('data')\nplt.title('Logged time serie')\n\nax_2 = fig.add_subplot(gs_1[1, :])\nax_2.plot(ts)\nax_1.set_xlabel('time')\nax_1.set_ylabel('data')\nplt.title('Original time serie')","62651080":"from sklearn import datasets, linear_model\n\nts_wi = ts_log.reset_index()\ndf_values = ts_wi.values\ntrain_y = df_values[:,1]\ntrain_y = train_y[:, np.newaxis]\ntrain_x = ts_wi.index\ntrain_x = train_x[:, np.newaxis]\nregr = linear_model.LinearRegression()\nregr.fit(train_x, train_y)\npred = regr.predict(train_x)\nplt.plot(ts_wi.Date, pred)\nplt.plot(ts_log)","20a5bc24":"mov_average = ts_log.rolling(12).mean()\nplt.plot(mov_average)\nplt.plot(ts_log)","fed9ab49":"ts_log_mov_av_diff = ts_log - mov_average\n#ts_log_mov_av_diff.head(12)\nts_log_mov_av_diff.dropna(inplace=True)\n\ntest_stationarity(ts_log_mov_av_diff)","454ba136":"ts_log_mov_reg_diff = ts_log - pred[:,0]\n#ts_log_mov_av_diff.head(12)\nts_log_mov_reg_diff.dropna(inplace=True)\n\ntest_stationarity(ts_log_mov_reg_diff)","22c05562":"from statsmodels.graphics import tsaplots as tsa\nts_log_diff = ts_log - ts_log.shift(1)\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = tsa.plot_acf(ts_log_diff.iloc[13:], lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = tsa.plot_pacf(ts_log_diff.iloc[13:], lags=40, ax=ax2)","f4ba1547":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff)","cc4b7f15":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log, freq=4, model='additive')\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","d87886f1":"#ts_decompose = residual\nts_decompose = ts_log_diff\nts_decompose.dropna(inplace=True)\ntest_stationarity(ts_decompose)","6d987c4e":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = tsa.plot_acf(ts_decompose, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = tsa.plot_pacf(ts_decompose, lags=40, ax=ax2)","8957a4ae":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Raw time serie\nfig = plt.figure(constrained_layout=True) \ngs = gridspec.GridSpec(2, 1, figure=fig)\nax = fig.add_subplot(gs[0, :])\nax.plot(ts_decompose)\nax.set_xlabel('time [months]')\nax.set_ylabel('data')\nax.set_title('Logged data')\n\nmodel = ARIMA(ts_decompose, order=(1, 0, 0))\nres = model.fit(disp=-2)\n\nax2 = fig.add_subplot(gs[1, :])\nax2.plot(res.fittedvalues)\nax2.set_xlabel('time [months]')\nax2.set_ylabel('data')\nax2.set_title('ARIMA model')\n\nprint('ARIMA RMSE: %.6f'% np.sqrt(sum((res.fittedvalues-ts_decompose)**2)\/len(ts)))","3c59f36d":"mod = SARIMAX(ts_decompose, trend='n', order=(1,1,0), seasonal_order=(3,0,3,4))\nresSARIMAX = mod.fit()\npred = resSARIMAX.predict()\n\n# Raw time serie\nfig = plt.figure(constrained_layout=True) \ngs = gridspec.GridSpec(2, 1, figure=fig)\nax = fig.add_subplot(gs[0, :])\nax.plot(ts_decompose)\nax.set_xlabel('time [months]')\nax.set_ylabel('data')\nax.set_title('Logged data')\n\nax2 = fig.add_subplot(gs[1, :])\nax2.plot(pred)\nax2.set_xlabel('time [months]')\nax2.set_ylabel('data')\nax2.set_title('SARIMAX model')\nprint('SARIMAX RMSE: %.6f'% np.sqrt(sum((pred-ts_decompose)**2)\/len(ts)))","d08c0a60":"predictions_SARIMA_diff = pd.Series(pred, copy=True)\npredictions_SARIMA_diff_cumsum = predictions_SARIMA_diff.cumsum()\npredictions_SARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\npredictions_SARIMA_log = predictions_SARIMA_log.add(predictions_SARIMA_diff_cumsum,fill_value=0)\npredictions_SARIMA = np.exp(predictions_SARIMA_log)\n\npredictions_ARIMA_diff = pd.Series(res.fittedvalues, copy=True)\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(predictions_ARIMA)\nprint('ARIMA RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)\/len(ts)))\n\nplt.plot(ts)\nplt.plot(predictions_SARIMA)\nprint('SARIMA RMSE: %.4f'% np.sqrt(sum((predictions_SARIMA-ts)**2)\/len(ts)))","a5661d32":"<a id=\"3\"><\/a> <br>\n# 3. Time Series Trend #\nConsidering the trend as an indicator telling if series goes up or down, we can realize that it's a component affecting directly the serie. So, as a component, we can deal with it getting important information of it such as the direction of the serie. In this case, we want to make predictions that implies making it stationary and for this trend has to be removed.\n<a id=\"3-1\"><\/a> <br>\n## 3.1. Dealing with trends ##\nThis section involves various methods to estimate trend and how to remove it. It includes linear regression, moving average and decomposition, these are evaluated and compared to find out the best result.\n<a id=\"3-1-1\"><\/a> <br>\n### 3.1.1. Estimating trend ###\nTrend estimating is required to approach the behaviour of time series. However, for forecasting trend is not necessary as it represents a non-stationary serie.\n\nThen, we're going to compare methods for trend estimating and decide the one gives better results. Also, it's important to notice log transformation for simplicity.","7ade4271":"Finally we've got another method that is differencing, for this one we substract a previous instant of time to original one. As well, it's possible to increse the order of the differencing by getting the previous point of first difference and we can increse the order by doing the same process. Incresing the order of the difference depends on the following rules (you can se the hole rules in this [page](http:\/\/people.duke.edu\/~rnau\/arimrule.htm))\n* If lag-1 term in ACF is zero or negative, or autocorrelations are small or partnerless, then doesn't need a higer order of differencing.\n* The optimal order of differencing is at which the standard deviation is lowest.","6f466a55":"<a id=\"5\"><\/a> <br>\n# 5. Forecasting Time Series #\n\nOnce the serie is stationary, forecasting can be done. The ARIMA model has non-seasonal part, but it can handle seasonal data. Selecting the best model implies analize autocorrelation function and partial autocorrelation function. Also, please note the following rules. The complete version of these rules can be consulted [here](http:\/\/people.duke.edu\/~rnau\/arimrule.htm)\n* If PACF shows *\"a sharp cutoff and\/or the lag-1 autocorrelation is positive, (...) then consider adding one or more AR terms to the model\"*.\n* If ACF shows *\"a sharp cutoff and\/or the lag-1 autocorrelation is negative, (...)then consider adding an MA term to the model.\"*","a518195a":"<a id=\"7\"><\/a> <br>\n# 7. References #\n* Jain, A. (2016) A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python). Consulted from: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n\n* Hyndman, R. and Athanasopoulos, G. (2018) Forecasting: Principles and Practice. Consulted from: https:\/\/otexts.org\/fpp2\/seasonal-arima.html#fig:euretail3\n\n* Abu, S. (2016) Seasonal ARIMA with Python. Consulted from: http:\/\/www.seanabu.com\/2016\/03\/22\/time-series-seasonal-ARIMA-model-in-python\/\n\n* Nau, R. (2018) Statistical forecasting: notes on regression and time series analysis. Consulted from: https:\/\/people.duke.edu\/~rnau\/arimrule.htm","b64104e3":"#### Linear regression ####\nIt'll give us a linear trend estimation, python implementation requires importing sklearn library. To do a linear regression separate X and Y axis and add a second dimension to Y axis.","6c765cb4":"In the other hand, we have the linear regression with a value over 10%, meaning that we can improve the results.","3a317a54":"<a id=\"3-1-2\"><\/a> <br>\n### 3.1.2. Eliminating trend ###\nRemoving trend is an important part of time series forecasting since time series are time dependant and stationary is necessary to make regression works.\n\nCompleting this task requires a method to estimate trend, as done it before, and substract this component from time series as a first step to reach stationarity.\n\nUsing moving averaga we get the following results.\n\nAs Dickey-Fuller test says, our data is under 5% critical value, giving a 90% of certainty that our data is stationarity","2badb43b":"The results of the test proof that the residual of the serie is stationary by comparing the test statistic value it's under 1% critical value, meaning we have 99% of confidence that the serie is stationary.","f36854e3":"<a id=\"2\"><\/a> <br>\n# 2. Time serie Stationarity #\n\nIn this section it's going to be mentioned a method to perform a test to proof stationarity. The following code is taken from [here](https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/)\n<a id=\"2-1\"><\/a> <br>\n## 2.1. Augmented Dickey-Fuller test ##\nThis test part of the assumption of the null hypotesis that a unit root exists in a autoregressive model; otherwise, it doesn't. The importance of this test lies on abvoiding spurious regressions, so the model fits the data as much as possible.\nIn the next graph, we can appreciate two different ways to evaluate if a model has unit roots or not. The graph shows how mean and standard deviation variate over time, in fact the standard deviation varies a little it's enough to reject he null hypotesis.\n\nThe Augmented Dickey-Fuller test says that if the p-value is below of one critical values, then we can't reject the null hypotesis, meaning that the model has a unit root.","2231080d":"<a id=\"4-1\"><\/a> <br>\n## 4.1. Decomposing ##\nAs mentioned, seasonal decompose is the fastest way to remove trend and seasonality components from a time serie to becoming it stationary.","d9eb1f75":"<a id=\"5-1\"><\/a> <br>\n## 5.1. ARIMA Models ##\n\nARIMA models can be separated as nonseasonal and seasonal. The noneseasonals are classified as $ARIMA(p,d,q)$ model, where\n* $p$ is the number of autoregressive terms,\n* $d$ is the number of noneseasonal differences needed for stationarity\n* $q$ is the number of lagged forecast errors in the prediction equation.\n\nIn a general way, the forecast equation can be written as $\\hat{y}_t=\\mu+{\\phi_1}{y_t}_{-1}+\\cdots+{\\phi_t}{y_t}_{-p}{-\\theta_1}{e_t}_{-1}-\\cdots-{\\theta}_q{e_t}_{-q}$. The moving average parameters are represented as $\\theta$, $\\phi$ expresses coeficients for difference parameters and $\\mu$ represents a constant value.\n\nSelecting the correct number of AR and MA terms for ARIMA model can be performed in two different ways. One is by trying all kind of combinations using a computer software until it finds which fit correctly the time serie, or by looking at the ACF and PACF to get an aproximation. Note that exists more sophisticated alternatives.\n\nThen, AR and MA terms are going to be selected based on the previous ACF and PACF plots. It's possible to determine the AR terms if lag *k* is more significant than the higher orders lags, that is PACF cuts off at lag *k*, thus would be *k* AR components. The same occurs for MA terms and ACF, if lag *k* is more significant than the higher orders lags, then would be *k* components for MA.\n\nBy performing lots of test, the best results given by ARIMA model was with $p = 1$, $d = 0$ and $q = 0$.","a043d90a":"# Content #\n## [1. Introduction](#1) ##\n### [1.1. Time series](#1-1) ###\n### [1.2. Stationarity vs Stationality](#1-2) ###\n## [2. Time Serie Stationarity](#2) ##\n### [2.1. Augmented Dickey-Fuller test](#2-1) ###\n## [3. Time series Trend](#3) ##\n### [3.1. Dealing with trend](#3-1) ###\n#### [3.1.1. Estimating trend](#3-1-1) ####\n#### [3.1.2. Eliminating trend](#3-1-2) ####\n## [4. Trend and seasonality](#4) ##\n### [4.1. Decomposing](#4-1) ###\n## [5. Forecasting Time Series](#5) ##\n### [5.1. ARIMA Models](#5-1) ###\n### [5.2 SARIMAX Models](#5-2) ###\n## [6. Conclusions](#6) ##\n## [7. References](#7) ##","d7ddd14c":"<a id=\"4\"><\/a> <br>\n# 4. Trend and seasonality #\n\nAs did when removing trend, seasonality will be done in the same way. Decomposing is a technique referered to remove trend and sesonal components of time serie.\n\nFor practicity, the time serie wouldn't be differencing, so graphics will show all components.","61950527":"<a id=\"5-2\"><\/a> <br>\n## 5.2. SARIMAX Models ##\nSARIMAX model is an extended version of ARIMA considering a seasonal component. It's compound of $p$, $d$ and $q$ parameters, as ARIMA model, but it includes a sesonal order $P$, $D$, $Q$ and $X$ component which is explanatory and may be used when the residual is expected to exhibit a seasonal trend or pattern.","6ea9a08a":"<a id=\"6\"><\/a> <br>\n# 6. Conclusions #\nAs noted, the ARIMA model has it advantage over SARIMAX, it easier to implement and finding out its parameter that fits better the data, but it can't deal with seasonal data as well as SARIMAX. While SARIMAX fits better, it's more complicated to spot the correct parametrs. Although there are lot of algorithms that help out to reach the best results such as Kalman filters, likelihood of ARMA process and Hyndman-Khandakar algorithm (used by R to perform the automatic ARIMA function).\n\nIn the graph bellow, it's possible to observe the comparasion between ARIMA and SARIMAX models. Where SARIMAX gives better approach to the serie without considering the RMSE of 215.1053. ","3c4d2899":"#### Moving Average ####\nThe moving average works, also, for trend estimation. This was achieved by smoothing the series and getting just the reresentative data.\n\nThe period was selected afeter making repeted observation and concluding that, for this case, 12 gives better results.","9b4ff15d":" <a id=\"1\"><\/a> <br>\n# 1. Introduction #\n <a id=\"1-1\"><\/a> <br>\n   ## 1.1. Time series ##\nTime series are chronologic secuence of data, it has four different components listed below\n   * Level: Average value in the series\n   * Trend: Defined as the trend that the serie follows, it represents if it goes up, down or remain constant\n   * Sesonality: Shows if exists any patern present through seasons (periods).\n   * Noise: White noise present during sampling or random factors such as politics, natural disasters, strikes, etc.\n   \nIt's important to note that trend and sesonality components are optional since series can be stationary.\n   <a id=\"1-2\"><\/a> <br>\n   ## 1.2. Stationarity vs Stationality ##\nAs mentioned, stationarity is a property of time series that represents trend and seasonality constant while seasonality shows periodic fluctuations. In order to perform an time series analyzing and avoid spurious regressions, its necessary to remove trend and stationality of our data.","a5a53154":"Once we decide the differencing that better fits our data, perform the stationary test to validate it. As the results describe, the parameter related to stationarity, i.e. test statistic, is under 1% critial value, so we're 99% confident that the time serie is stationary."}}