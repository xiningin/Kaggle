{"cell_type":{"9553aa2e":"code","ff98b5d3":"code","3b310d1f":"code","022e0c91":"code","e230ee79":"code","7efafd17":"code","6f39335b":"code","a722a5c7":"code","9a0635f7":"code","6fc80f0d":"code","066ddb58":"code","d7c431a2":"code","bf456608":"code","542d26cf":"code","e7b0fe76":"code","d53e01bf":"code","962a412c":"code","7b49c48f":"code","556c9b21":"code","78465a62":"code","e3bcce2b":"code","81a218ef":"code","9a63b746":"code","99005d21":"markdown","cadff9f1":"markdown","7d372037":"markdown","9e850c7c":"markdown","684c7fe3":"markdown","fcefcb8c":"markdown","5388d5b0":"markdown","325ef118":"markdown"},"source":{"9553aa2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nprint(os.getcwd())\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff98b5d3":"# !kaggle competitions download -c tabular-playground-series-jun-2021\n# !unzip .\/tabular-playground-series-jun-2021.zip -d \/kaggle\/working\/input\/tabular_202106data","3b310d1f":"file_path = '\/kaggle\/input\/tabular-playground-series-jun-2021'\nos.listdir(file_path)","022e0c91":"import missingno as missno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom  contextlib import contextmanager\nimport time\nfrom datetime import datetime\nfrom functools import wraps\npd.set_option('display.max_rows', 200)\n\n@contextmanager\ndef timer(msg):\n    st = datetime.now()\n    yield\n    cost = datetime.now() - st\n    print(f'{msg} Done. It cost {cost}')\n\n    \ndef clock(func):\n    @wraps(func)\n    def clocked(*args, **kwargs):\n        st = datetime.now()\n        res = func(*args, **kwargs)\n        cost_ = datetime.now() - st\n        print(f'{func.__name__} cost {cost_}')\n        return res\n    return clocked","e230ee79":"train_df = pd.read_csv(os.path.join(file_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(file_path, 'test.csv'))\nsub_df = pd.read_csv(os.path.join(file_path, 'sample_submission.csv'))\n\nneed_columns = [col for col in train_df.columns if 'feature' in col] + ['target']","7efafd17":"## missing\nmissno.matrix(train_df[need_columns], figsize=(12, 6))\nplt.show()","6f39335b":"## target \nplt.figure(figsize=(16,8))\ntrain_df['target'].value_counts().plot.pie(autopct='%3.1f%%', pctdistance=0.7, wedgeprops=dict(linewidth=2,width=0.5,edgecolor='w'))\n# help(train_df[need_columns].describe().T.style)","a722a5c7":"## data overview\ndef zero_rate(series_):\n    return np.mean(series_ == 0)\n\nwith timer('data_over_view'):\n    overview_columns = [i for i in need_columns if 'feat' in i]\n    desc_df = train_df[overview_columns].describe().T\n    nunique_df = pd.DataFrame(train_df[overview_columns].nunique()).rename(columns = {0: 'nunique'})\n    \n    train_df['gp_fake'] = '1'\n    zero_df = train_df[overview_columns+['gp_fake']].groupby('gp_fake').agg(zero_rate).T.rename(columns = {'1': 'zero_rate'})\n    range_df = train_df[overview_columns+['gp_fake']].groupby('gp_fake').agg(np.ptp).T.rename(columns = {'1': 'max_min_range'})\n    train_df.drop(columns='gp_fake', inplace=True)\n    exp_df = desc_df.merge(nunique_df, left_index=True, right_index=True)\\\n                    .merge(range_df, left_index=True, right_index=True)\\\n                    .merge(zero_df, left_index=True, right_index=True)\n    \n    del nunique_df, desc_df, range_df, zero_df\n    display(exp_df.style.format('{:.2f}',subset=['mean', 'std', 'zero_rate']).bar('std',vmin=0)\\\n                    .highlight_max('nunique').highlight_min('nunique')\\\n                    .background_gradient('Greens',subset='max_min_range')\\\n                    .bar('zero_rate', vmin=0, vmax=1).highlight_max('zero_rate')\n           )","9a0635f7":"# corr\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ntrain_df['target_encode'] = lb.fit_transform(train_df['target'])\n\nneed_columns = overview_columns + ['target_encode']\n\nwith timer('Get corr df'):\n    corr_df = train_df[need_columns+['target']].corr(method='spearman')\n\nwith timer('Get mask matrix'):\n    mask = np.triu(np.ones_like(corr_df))\n    mask_small = (mask + (np.abs(corr_df.values) > 0.01)).astype(bool)\n    mask_big = (mask + (np.abs(corr_df.values) < 0.2)).astype(bool)","6fc80f0d":"plt.figure(figsize=(16, 12))\nsns.heatmap(corr_df, mask=mask_small) #, annot=True, fmt='.3f')\nplt.show()\n\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_df, mask=mask_big) #, annot=True, fmt='.2f')\nplt.show()\n\n\nplt.figure(figsize=(16, 8))\ncorr_df.iloc[:-1, -1].sort_values().plot.bar()\nplt.title('target & featues')","066ddb58":"def class_unique_plot(df, columns, traget, axe):\n    if len(traget) == 0:\n        df[columns].nunique().plot(ax=axe)\n        plt.show()\n        return None\n\n    gp = df[[traget]+columns].groupby(traget)\n    for name, tmp_df in gp:\n        tmp_df.nunique().plot(ax=axe, label=name)\n    \n    plt.legend()\n    plt.show()\n    \n    \nfig, axes = plt.subplots(figsize=(18, 4))\nclass_unique_plot(train_df, overview_columns, 'target', axe=axes)\n\nfig, axes = plt.subplots(figsize=(18, 4))\nclass_unique_plot(train_df, overview_columns, '', axe=axes)\n\nfig, axes = plt.subplots(figsize=(18, 4))\nclass_unique_plot(test_df, overview_columns, '', axe=axes)","d7c431a2":"fig, axes = plt.subplots(figsize=(18, 4))\nclass_unique_plot(train_df.loc[train_df.target_encode.isin([5, 7]), :], overview_columns, 'target', axe=axes)\n","bf456608":"a = pd.DataFrame(train_df.loc[train_df.target_encode.isin([5]), :].nunique())\\\n.merge(pd.DataFrame(train_df.loc[train_df.target_encode.isin([7]), :].nunique()), left_index=True, right_index=True)\na['diff'] =  a['0_y'] - a['0_x']\ndisplay(a.style.bar('diff',vmin=0))\n\nfrequnce_add_col = ['feature_60', 'feature_15', 'feature_28', 'feature_61', 'feature_62']","542d26cf":"from tqdm import tqdm\n@clock\ndef low_freqence_detector(df, vars_to_agg, agg_threshold=0.999):\n    \"\"\"\n    \u5c06\u4f4e\u9891\u503c\u5f52\u4e3a\u4e00\u7c7b\n    \"\"\"\n    replace_dict = {}\n    for col in tqdm(vars_to_agg):\n        a_cumsum = df[col].value_counts(normalize=True).cumsum()\n        value_count_series = df[col].value_counts()\n        will_be_replaced_values = value_count_series[a_cumsum >= agg_threshold].index.tolist()\n        n = len(will_be_replaced_values)\n        replace_value = min(will_be_replaced_values)\n        tmp_dict = (will_be_replaced_values, replace_value)\n        replace_dict[col] = tmp_dict\n    return replace_dict\n\ndef aggregate_low_freq_values(df, replace_dict):\n    df_out = df.copy(deep=True)\n    for replace_feat in tqdm(replace_dict):\n        need_replaced_values = replace_dict[replace_feat][0]\n        replace_value = replace_dict[replace_feat][1]\n        df_out.loc[df[replace_feat].isin(need_replaced_values), replace_feat] = replace_value\n    return df_out\n\ndef quick_agg_low_freq_values(tr_df, te_df, vars_to_agg, agg_threshold=0.999):\n    c = pd.concat([tr_df[vars_to_agg], te_df[vars_to_agg]], ignore_index=True)\n    replace_dict = low_freqence_detector(c, vars_to_agg, agg_threshold)\n    return aggregate_low_freq_values(tr_df, replace_dict), aggregate_low_freq_values(te_df, replace_dict)","e7b0fe76":"overview_columns = [i for i in train_df.columns if 'feat' in i]\ntrain_df, test_df = quick_agg_low_freq_values(\n    train_df, test_df,\n    overview_columns\n)\n\n# for col_ in tqdm(frequnce_add_col):\n#     dict_ = train_df[col_].value_counts(normalize=True).to_dict()\n#     train_df[f'{col_}_freq'] = train_df[col_].map(dict_)\n#     test_df[f'{col_}_freq'] = test_df[col_].map(dict_)\n\n\n","d53e01bf":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom lightgbm import LGBMClassifier\n\n\ndef plot_heatmap(y_true, y_pred_prob):\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    conf = confusion_matrix(y_true, y_pred)\n    conf = conf\/ conf.sum(axis=1)\n    sns.heatmap(conf, annot=True, fmt='.2f')\n    plt.show()\n\n@clock\ndef cross_val(X, y, model, params, folds=5):\n    models = []\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=50)\n\n        pred = alg.predict_proba(x_test)\n        loss = log_loss(y_test, pred)\n        models.append((alg, loss))\n        plot_heatmap(y_test, pred)\n        print(f\"Log loss: {loss}\")\n        print(\"-\"*50)\n    return models","962a412c":"# lgb_params = {\n#     'n_estimators': 1500, \n#      'max_depth': 8, \n#      'num_leaves': 25, \n#      'learning_rate': 0.05, \n#      'reg_lambda': 28,\n#      'subsample': 0.85, \n#      'colsample_bytree': 0.75, \n#       'n_jobs':-1\n# }\n# def f1_metric(preds, train_data):\n#     labels = train_data.get_label()\n#     return 'f1', f1_score(labels, preds, average='marco'), True\n\n# lgb_params = { # 1.75060\n#         'num_leaves': 14,\n#         'min_data_in_leaf': 52,\n#         'learning_rate': 0.04,\n#         'min_sum_hessian_in_leaf': 10.090025079493055,\n#         'bagging_fraction': 1.0,\n#         'bagging_freq': 5,\n#         'boost_from_average':'false',\n# #         'subsample': 0.6798695128633439,\n#         'colsample_bytree': 0.7727780074568463,\n#         'reg_alpha': 0.45606998421703593,\n#         'reg_lambda': 78.51759613930136,\n#         'min_gain_to_split': 0.13949386065204183,\n#         'max_depth': 6, \n#         'n_jobs': -1,\n#         'boosting_type': 'gbdt',\n#         'metric':'multi_logloss',\n#         'early_stopping_round' : 100,\n#         'n_estimators': 500,\n#         'tree_learner': 'serial',\n#     }\n\nlgb_params = {\n        'num_leaves': 10,\n        'min_data_in_leaf': 63,\n        'learning_rate': 0.05,\n        'min_sum_hessian_in_leaf': 8.140308692805194,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'boost_from_average':'false',\n        'subsample': 0.749948437333368,\n        'colsample_bytree': 0.6168504947710284,\n         'reg_alpha': 0.227796749807186,\n         'reg_lambda': 70.2792417704872,\n        'min_gain_to_split': 0.4758826409257615,\n        'max_depth': 14, \n        'n_jobs': -1,\n        'boosting_type': 'gbdt',\n        'metric':'multi_logloss',\n        'early_stopping_round' : 100,\n        'n_estimators': 500,\n        'tree_learner': 'serial',\n    }\n\n\nX = train_df[overview_columns]\ny = train_df['target_encode']\n\nlgb_models = cross_val(X, y, LGBMClassifier, lgb_params)","7b49c48f":"# os.makedirs('.\/submit_data')\nos.listdir(); os.getcwd()","556c9b21":"def models_merge(models_loss, pred_df):\n    n = 0\n    loss_d_sum = 0\n    for m, l in tqdm(models_loss, desc='MODEL PREDICT\uff1a'):\n        loss_d_sum += 1 \/ l\n        if n == 0:\n            pred_res = m.predict_proba(pred_df) * 1\/l\n            n += 1\n            continue\n        n += 1\n        pred_res += m.predict_proba(pred_df) * 1\/l\n    return pred_res \/ loss_d_sum","78465a62":"with timer('submit data'):\n    pred = models_merge(lgb_models[1:], test_df[overview_columns])\n    now_ = datetime.now().strftime('%Y%m%d_%H_%M')\n    sub_df.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = np.clip(pred, 10**-15, 1-10**-15)\n    display(sub_df.head())\n    sub_df.to_csv(f'lgb_model_{now_}.csv',index=False)","e3bcce2b":"os.listdir(), os.getcwd(), now_","81a218ef":"# !rm .\/submit_data\/lgb_model_20210607_20_03.csv","9a63b746":"# os.environ['KAGGLE_USERNAME'] = \"scchuy\" # username from the json file \n# os.environ['KAGGLE_KEY'] = \"..\" #\n# !kaggle competitions submit -c tabular-playground-series-jun-2021 -f .\/lgb_model_20210609_18_18.csv -m \"Message\"","99005d21":"# Loading Data","cadff9f1":"# 5fold - LGB","7d372037":"# Download Data And Basic Setting ","9e850c7c":"# Data Explore (simple) ","684c7fe3":"feature 17 low corr & min nunique & max zero rate\n\nwe may drop the features","fcefcb8c":"## test vs train","5388d5b0":"# Generator Features","325ef118":"# Submit"}}