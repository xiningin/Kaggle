{"cell_type":{"33f7ee9c":"code","e5cf1c5b":"code","f2557e00":"code","eac07aeb":"code","33737923":"code","afc9877f":"code","56e33b88":"code","cd26cbec":"code","75a35680":"code","4a985872":"code","7b5bfd5c":"code","4eae5837":"code","585e9f4d":"code","c0df72a4":"code","76485c19":"code","23794361":"code","39f8cfe4":"code","6753784d":"code","5a5d22e5":"code","ee578a87":"code","9ade5ff1":"code","159f21f7":"code","5378fab0":"code","7f8a71f6":"code","51de8e76":"code","dd0f5120":"code","991fb6cc":"code","a90a7734":"code","141e2520":"code","8060b98e":"code","c50872ea":"code","0b127295":"code","2f30a218":"code","34929e89":"code","e0b75b0e":"code","3c04da4d":"code","94f7c54d":"code","b2a97f00":"code","0e692158":"code","fd0f887a":"code","13860a3d":"code","b25769a6":"code","dd6d2dbf":"code","c7d24594":"code","a0aba1c0":"code","6068ac50":"code","5e748936":"code","54c5bb27":"code","b63fc5a0":"code","684ca5b7":"code","db8d6c66":"code","9ab90015":"code","505a7d56":"code","45ccbc69":"code","0d35c066":"code","125f76bd":"code","178efabc":"code","93dc1ead":"code","f9d29677":"code","82c1b478":"code","53ab2310":"code","c1a2be49":"code","84ba6e36":"markdown","42171926":"markdown","937cd031":"markdown","714bb0ec":"markdown","abd4a0e3":"markdown","05c916ec":"markdown","9b15aad7":"markdown","59dc7836":"markdown","2f2f5ad1":"markdown","0dbe85bb":"markdown","3ad1a8c7":"markdown","6379d562":"markdown","dc988520":"markdown","4653fc1c":"markdown","b742a055":"markdown","91020350":"markdown","daaff933":"markdown","f060585a":"markdown","41165290":"markdown","c013f991":"markdown","1fb328bd":"markdown","7eaff252":"markdown"},"source":{"33f7ee9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5cf1c5b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n## imported library for visualizations\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f2557e00":"cf_data = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')\n## load data and check head of the data\ncf_data.head()","eac07aeb":"## check shape of the data\n\ncf_data.shape","33737923":"## check info of the dataset\n\ncf_data.info()","afc9877f":"cf_data.describe(include=\"all\")","56e33b88":"## check the missing values percentage\n\nround(100*(cf_data.isnull().sum()\/cf_data.shape[0]),3)","cd26cbec":"## check my only one categorical variable\n\ncf_data['ocean_proximity'].value_counts(normalize=True)\n\n","75a35680":"## visualize ocean proximity\n\nplt.figure(figsize=(12,6))\nsns.countplot(x='ocean_proximity',data=cf_data)\nplt.show()","4a985872":"## create one copy for visualization\n\ncf_data_copy = cf_data.copy()","7b5bfd5c":"## convert categorical variable into numerical variables using dummy encoding\n\nstat = pd.get_dummies(cf_data['ocean_proximity'],drop_first=True)","4eae5837":"## concat stat and dro original categorical variable\n\ncf_data = pd.concat([cf_data,stat],axis=True)\ncf_data.drop('ocean_proximity',axis=1,inplace=True)","585e9f4d":"import sklearn \nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\n\niterimp = IterativeImputer(estimator=LinearRegression(),random_state=100)\ncf_data_clean = pd.DataFrame(iterimp.fit_transform(cf_data))\n","c0df72a4":"cf_data_clean.columns = cf_data.columns.tolist() ## rename our new data frame ","76485c19":"total = 0\nfor i in cf_data_clean.columns.tolist():\n    total = total + cf_data_clean[i].isnull().sum()\nprint(\"Total Number Of Null Values : \",total)","23794361":"## check before imputation stats\n\ncf_data['total_bedrooms'].describe()","39f8cfe4":"## check after imputation stats\n\ncf_data_clean['total_bedrooms'].describe()","6753784d":"import scipy \nfrom scipy.stats import ttest_ind\nttest_ind(cf_data['total_bedrooms'],cf_data_clean['total_bedrooms'],nan_policy='omit')","5a5d22e5":"plots_var = ['housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']\n\nplt.figure(figsize=(30,15))\n\nfor i in enumerate(plots_var):\n    plt.subplot(2,4,i[0]+1)\n    ax = sns.distplot(cf_data_clean[i[1]])\n    ax.set_xlabel(i[1],fontsize=15)\nplt.tight_layout()\nplt.show()","ee578a87":"## visualize house value with respect to occen proximity\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='ocean_proximity',y='median_house_value',data=cf_data_copy)\nplt.show()","9ade5ff1":"## visualize household with respect to occen proximity\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='ocean_proximity',y='median_house_value',data=cf_data_copy)\nplt.show()","159f21f7":"## visualize house value with respect to occen proximity\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='ocean_proximity',y='households',data=cf_data_copy)\nplt.show()","5378fab0":"## visualize population value with respect to occen proximity\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='ocean_proximity',y='population',data=cf_data_copy)\nplt.show()","7f8a71f6":"## visualize housing median age value with respect to occen proximity\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='ocean_proximity',y='housing_median_age',data=cf_data_copy)\nplt.show()","51de8e76":"BBox = ((cf_data_clean.longitude.min(),   cf_data_clean.longitude.max(),      \n         cf_data_clean.latitude.min(), cf_data_clean.latitude.max()))\n        \n        ","dd0f5120":"mymap = plt.imread('..\/input\/california-map\/map.png') ","991fb6cc":"plt.figure(figsize = (30,15))\nax=sns.scatterplot(x='longitude', y='latitude',data = cf_data_copy ,hue='ocean_proximity' ,alpha = 0.5)\nax.set_title('Plotting Spatial Data on California Map')\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(mymap, zorder=0, extent = BBox, aspect= 'equal')\n\nplt.show()","a90a7734":"## check the pairplots\n\nsns.pairplot(cf_data_copy)\nplt.show()","141e2520":"## first make train test split \n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(cf_data_clean,train_size=0.7,random_state=42)","8060b98e":"## use power transform to make data outlier robust and less skewed\n\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer()\n\ntrain = pd.DataFrame(pt.fit_transform(train))\ntest = pd.DataFrame(pt.transform(test))\n\n ","c50872ea":"## use scaler transform for linear regression\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\ntrain = pd.DataFrame(sc.fit_transform(train))\ntest = pd.DataFrame(sc.transform(test))","0b127295":"## renaming train and test\n\ntrain.columns = cf_data_clean.columns.tolist()\ntest.columns = cf_data_clean.columns.tolist()","2f30a218":"y_train = train.pop('median_house_value') ## train split\nX_train = train","34929e89":"y_test = test.pop('median_house_value') ## test split\nX_test = test","e0b75b0e":"## use linear regression model to predict\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression().fit(X_train,y_train)\n","3c04da4d":"## let's predict the test data and check r2 score \n\nfrom sklearn.metrics import r2_score\n\nscore1 = r2_score(y_test,lr.predict(X_test))\n","94f7c54d":"## score of our first model\n\nscore1","b2a97f00":"## use of stats models to check the multi colinearity \n\nimport statsmodels.api as sm\n\nlr2 = sm.OLS(y_train,sm.add_constant(X_train)).fit()","0e692158":"print(lr2.summary())","fd0f887a":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['features'] = X_train.columns\nvif['vif'] = [variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])]\nvif['vif'] = round(vif['vif'],2)\nvif = vif.sort_values(by='vif',ascending=False)\n#vif.drop(vif.index[0],inplace=True)\nvif","13860a3d":"## hence remove total_bedrooms\n\nX_train_sm = X_train.copy()\nX_train_sm.drop('total_bedrooms',inplace=True,axis=1)","b25769a6":"lr3 = sm.OLS(y_train,sm.add_constant(X_train_sm)).fit()\nprint(lr3.summary())","dd6d2dbf":"vif = pd.DataFrame()\nvif['features'] = X_train_sm.columns\nvif['vif'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nvif['vif'] = round(vif['vif'],2)\nvif = vif.sort_values(by='vif',ascending=False)\n#vif.drop(vif.index[0],inplace=True)\nvif","c7d24594":"## hence remove latitude\n\nX_train_sm.drop('latitude',inplace=True,axis=1)","a0aba1c0":"lr4 = sm.OLS(y_train,sm.add_constant(X_train_sm)).fit()\nprint(lr4.summary())","6068ac50":"vif = pd.DataFrame()\nvif['features'] = X_train_sm.columns\nvif['vif'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nvif['vif'] = round(vif['vif'],2)\nvif = vif.sort_values(by='vif',ascending=False)\n#vif.drop(vif.index[0],inplace=True)\nvif","5e748936":"## remove near occean\n\nX_train_sm.drop('NEAR OCEAN',inplace=True,axis=1)","54c5bb27":"lr5 = sm.OLS(y_train,sm.add_constant(X_train_sm)).fit()\nprint(lr5.summary())","b63fc5a0":"vif = pd.DataFrame()\nvif['features'] = X_train_sm.columns\nvif['vif'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nvif['vif'] = round(vif['vif'],2)\nvif = vif.sort_values(by='vif',ascending=False)\n#vif.drop(vif.index[0],inplace=True)\nvif","684ca5b7":"## remove households \nX_train_sm.drop('households',inplace=True,axis=1)","db8d6c66":"lr6 = sm.OLS(y_train,sm.add_constant(X_train_sm)).fit()\nprint(lr6.summary())","9ab90015":"vif = pd.DataFrame()\nvif['features'] = X_train_sm.columns\nvif['vif'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nvif['vif'] = round(vif['vif'],2)\nvif = vif.sort_values(by='vif',ascending=False)\n#vif.drop(vif.index[0],inplace=True)\nvif","505a7d56":"## r2 score of new created model\nscore2 = r2_score(y_test,lr6.predict(sm.add_constant(X_test[X_train_sm.columns.tolist()])))\nscore2","45ccbc69":"## implement ridge regression \n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\n\nridge = Ridge()\nparams = {'alpha' :np.arange(0,100,0.01).reshape(10000,1).tolist()}\nridge_grd = GridSearchCV(estimator = ridge,param_grid = params,scoring = 'neg_mean_absolute_error',return_train_score=True).fit(X_train,y_train)","0d35c066":"## check the cv results\n\ncv_results_ridge = pd.DataFrame(ridge_grd.cv_results_)","125f76bd":"## check results for different fit\n\ncv_results_ridge.head(10)","178efabc":"# plotting mean test and train scoes with alpha \n#cv_results_ridge['param_alpha'] = cv_results_ridge['param_alpha'].astype('int')\n\n# plotting\nplt.figure(figsize=(20,10))\nplt.plot( cv_results_ridge['mean_train_score'])\nplt.plot( cv_results_ridge['mean_test_score'])\nplt.grid()\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","93dc1ead":"## check best params \n\nridge_grd.best_score_","f9d29677":"ridge_grd.best_estimator_","82c1b478":"## train my model with best estimator \n\nfinal_ridge = ridge_grd.best_estimator_\nfinal_ridge = final_ridge.fit(X_train,y_train)","53ab2310":"## check test score on test data set\n\nscore_ridge = r2_score(y_test,final_ridge.predict(X_test))","c1a2be49":"score_ridge","84ba6e36":"* We will use the iterative imputer to impute missing values . We will use linear regression model to predict the missing value using other columns.","42171926":"# Data Preproccessing","937cd031":"* Total Bedrooms and Household , Total Rooms and Total bedrooms are having a linear relationhip.","714bb0ec":"* From the p value we fail to reject the null hypothesis.","abd4a0e3":"* Only total bedrooms has missig values.","05c916ec":"# EDA","9b15aad7":"* Those houses which are in island areas are all costly compared to other houses\n* Some houses are very costly which are inland areas also.","59dc7836":"# Ridge Regression","2f2f5ad1":"* This linear model is slightly better than before .","0dbe85bb":"* Maximum population stays at less than 1h ocean than inland area then near ocean and after that near bay and at last island areas.","3ad1a8c7":"* Maximum house holds stays at less than 1h ocean than inland area then near ocean and after that near bay and at last island areas.","6379d562":"* To check more clearly we will do a hypothesis test on it \n\nNull Hypothesis : mean before imputation = mean after imputation\nAlternative Hypothesis : mean before imputation != mean after imputation\n\nwe will use scipy to do it","dc988520":"# Model Build(Linear Regression using Stats model)","4653fc1c":"* There is a very minor difference between before mean and after mean of total bedrooms variable.","b742a055":"* Near bay and island houses are older than other area houses.","91020350":"* I will add more models one by one to improve prediction. To be continued...","daaff933":"* Let's now check our column whose null value is imputed. First we will check mean of that column before imputation and after imputation statistically.","f060585a":"* Import the map from  opestreetmap.org website and export the desired map as an image by first entering the bounding box data.\n","41165290":"# Model Build (Linear Regression)","c013f991":"* create one boundary box to define our range of geo spatial data","1fb328bd":"* Save the image as mymap.","7eaff252":"* Clusters are clearly visible on the above data."}}