{"cell_type":{"f7ce56fa":"code","1d427f19":"code","25efeaa9":"code","a7038561":"code","5c3faacc":"code","e747252a":"code","f4d846e1":"code","0ab1565d":"code","f150d063":"code","37c33e51":"code","07d0921e":"code","e627b884":"code","afb80f2a":"code","c5e73698":"code","61a2ac46":"code","20665de1":"code","a0364059":"code","75b99995":"code","f63fb7a9":"code","2994bf4c":"code","4cd94acd":"code","62ab8f08":"code","f79168f6":"code","51a0200d":"code","8f376dd6":"code","1e70af9f":"code","320f1983":"code","b9d460be":"code","75e083ad":"code","cbb5c336":"code","b73a5d1c":"code","a64cab55":"code","9956a5b9":"code","c43301da":"code","9cdb8cbf":"code","ae1cac52":"code","c7867d24":"code","12c67867":"markdown","ef927271":"markdown","41300b8b":"markdown","5c7a2f3a":"markdown","671128e5":"markdown","e01140c1":"markdown","d6600d24":"markdown","88843e2d":"markdown","f7a77414":"markdown","4d005068":"markdown","1d81e48b":"markdown","8b981079":"markdown","e4ff5033":"markdown","06cb5d32":"markdown","54eb9da9":"markdown","4f131f37":"markdown","c0b54a1f":"markdown","6fbff0d1":"markdown"},"source":{"f7ce56fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pandas_profiling import ProfileReport as pr\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d427f19":"df1 = pd.read_csv('\/kaggle\/input\/flight-delay-prediction\/Jan_2019_ontime.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/flight-delay-prediction\/Jan_2020_ontime.csv')","25efeaa9":"set(df1.columns) == set(df2.columns)","a7038561":"df = pd.concat([df1,df2])","5c3faacc":"df.info()","e747252a":"pd.set_option('display.max_columns', None)\ndf.head()","f4d846e1":"pr(df)","0ab1565d":"df = df[df['DIVERTED'] == 0]\ndf = df[df['CANCELLED'] == 0]","f150d063":"df = df[['DAY_OF_WEEK', 'OP_UNIQUE_CARRIER', 'DEP_TIME_BLK', 'ORIGIN', 'DEST', 'DISTANCE', 'ARR_DEL15']]","37c33e51":"df.dropna()\ndf = df.reset_index(drop=True)","07d0921e":"df = df.rename(columns = {'DAY_OF_WEEK' : 'ON_WEEKEND'})\ndf['ON_WEEKEND'] = (df['ON_WEEKEND'] > 5).astype(int)","e627b884":"df['OP_UNIQUE_CARRIER'].nunique()","afb80f2a":"carrier_df = df[['OP_UNIQUE_CARRIER','ARR_DEL15']].groupby('OP_UNIQUE_CARRIER').sum().sort_values(by='ARR_DEL15',ascending=False)\ncarrier_df['CARRIER_cat'] = pd.qcut(carrier_df['ARR_DEL15'], 17, labels = False)\ncarrier_df","c5e73698":"data_carrier = carrier_df.loc[df['OP_UNIQUE_CARRIER']].reset_index()\ndf['CARRIER_cat'] = data_carrier['CARRIER_cat']","61a2ac46":"df['DEP_TIME_BLK'].nunique()","20665de1":"time_blk_df = df[['DEP_TIME_BLK','ARR_DEL15']].groupby('DEP_TIME_BLK').sum().sort_values(by='ARR_DEL15',ascending=False)\ntime_blk_df['TIME_cat'] = pd.qcut(time_blk_df['ARR_DEL15'], 19, labels = False)\ntime_blk_df","a0364059":"data_time = time_blk_df.loc[df['DEP_TIME_BLK']].reset_index()\ndf['DEP_TIME_cat'] = data_time['TIME_cat']","75b99995":"df['ORIGIN'].nunique()","f63fb7a9":"origin_df = df[['ORIGIN','ARR_DEL15']].groupby('ORIGIN').sum().sort_values(by='ARR_DEL15',ascending=False)\norigin_df['ORIGIN_cat'] = pd.qcut(origin_df['ARR_DEL15'], 25, labels = False)\norigin_df","2994bf4c":"data_origin = origin_df.loc[df['ORIGIN']].reset_index()\ndf['ORIGIN_cat'] = data_origin['ORIGIN_cat']","4cd94acd":"df['DEST'].nunique()","62ab8f08":"dest_df = df[['DEST','ARR_DEL15']].groupby('DEST').sum().sort_values(by='ARR_DEL15',ascending=False)\ndest_df['DEST_cat'] = pd.qcut(dest_df['ARR_DEL15'], 25, labels = False)\ndest_df","f79168f6":"data_dest = dest_df.loc[df['DEST']].reset_index()\ndf['DEST_cat'] = data_dest['DEST_cat']","51a0200d":"df = df[['ON_WEEKEND', 'CARRIER_cat','DEP_TIME_cat', 'ORIGIN_cat', 'DEST_cat', 'DISTANCE', 'ARR_DEL15']]\ndf","8f376dd6":"df_X = df.drop('ARR_DEL15', axis=1)\ndf_y =  df[['ARR_DEL15']]","1e70af9f":"X = df_X.values\ny = df_y.values","320f1983":"y = LabelBinarizer().fit_transform(y)","b9d460be":"df_y.value_counts()","75e083ad":"oversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","cbb5c336":"X = StandardScaler().fit_transform(X)\nX = MinMaxScaler().fit_transform(X)","b73a5d1c":"X","a64cab55":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","9956a5b9":"model = tree.DecisionTreeClassifier()\nmodel = model.fit(X_train, y_train)","c43301da":"y_pred_test = model.predict(X_test)","9cdb8cbf":"print(confusion_matrix(y_test, y_pred_test))","ae1cac52":"print(classification_report(y_test, y_pred_test))","c7867d24":"plot_roc_curve(model, X_test, y_test)","12c67867":"Let's start with the carriers. There are 17 carriers, let's rank them in quantiles (# of delays generated):","ef927271":"As our data is significantly imbalanced (977724 rows for class '0=on time' and '187507 rows for class '1= delayed'), let's use the SMOTE method (Synthetic Minority Oversampling TEchnique) to generate more examples of class 1:","41300b8b":"Let's have a look at the newly generated dataframe, fully numerical with ordinal values and a target feature:","5c7a2f3a":"This project is about predicting if a flight will be delayed by over 15 minutes upon arrival, with Scikit-learn Decision Tree Classifier, using US flight data from January 2019 and January 2020. Here is the URL of the dataset and variables description:\n\nhttps:\/\/www.kaggle.com\/divyansh22\/flight-delay-prediction","671128e5":"The challenge here is that our data is significantly imbalanced, as flights are way more often on time than delayed. Therefore we need to build a model capable of effectively separating classes 'on time' or 'delayed'. It is a binary classification problem. The AUC (Area Under the Curve) will be the most relevant metric to evaluate our model.","e01140c1":"Let's generate the feature matrix and the binary target vector to feed our model:","d6600d24":"To feed our model, let's normalize and standardize our feature matrix so that each feature is of equal importance and equal range before splitting the data into train and test datasets (80% & 20%). \n\nImportant note: as per sklearn documentation all decision tree models use float32, preferably with a gaussian distribution as input, which is exactly what we get here (all features in the 0-1 range).","88843e2d":"When data is highly skewed, any model can reach good accuracy by always predicting the same class for example. In our particular case, we are trying to predict the minority class well for the model to be useful. \n\nTherefore the most relevant metric is AUC, or Area Under the Curve:\n\n    If AUC=50% the model is useless as it is wrong 50% of the time.\n    If AUC=100% the model is perfect, it identifies both classes right every time.\n\nIn conclusion, we get an AUC of 83% on the testing data, meaning that our model performs well at separating classes on unseen data and can predict flight delays effectively (75% accuracy and recall on both classes).","f7a77414":"Now let's have a look at time blocks. There are 19 of them, let's apply the same quantile indexing method: rush hours (generating more delays) will get penalized more with a higher weight.","4d005068":"As per our Pandas profiling report, we can see that the data is mostly categorical with large amounts of possible values in some categories such as 'ORIGIN' and 'DEST' (353 values each). \n\nOur strategy consists in defining quantiles correlated to the target ARR_DEL15 for each categorical feature, and then assigning a weight for each quantile by order of importance: higher quantile = higher weight. Our model will eventually be trained on the resulting ordinal data. ","1d81e48b":"The above data profile report allows us to establish relevant information such as column redundancy, feature correlation and missing values at a glance. To solve our problem, we will choose 'ARR_DEL15' as the binary target label (0= on time, 1= late) and select the following features:\n\n    'DAY_OF_WEEK': day # starting from monday. Will be set to 0 if week day and 1 if weekend.\n    'OP_UNIQUE_CARRIER': carrier identifier.\n    'DEP_TIME_BLK': 24h time chunks.\n    'ORIGIN': departure airport identifier.\n    'DEST': destination airport identfier.\n    'DISTANCE': flight length.\n\nWe will drop all flights that were diverted or cancelled, for which ARR_DEL15 is NaN. \nWe will also drop all rows that contain NaN values.","8b981079":"Now let's have a look at the departure airports feature 'ORIGIN':","e4ff5033":"Now let's replace the carrier identifier with its quantile number\/weight in the main dataframe 'df':","06cb5d32":"Our main dataframe now consists of 22 columns and 1191331 rows:","54eb9da9":"To explore the data, let's use the amazing Pandas profile report capabilities:","4f131f37":"Checking column structures before concatenating dataframes:","c0b54a1f":"This time around, let's generate 25 quantiles, each containing a group of departure airports that tend to generate the same amount of delays. We have observed that generating more quantile bins does not improve our model performance.","6fbff0d1":"Same approach with the destination airports feature 'DEST':"}}