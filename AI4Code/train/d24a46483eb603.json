{"cell_type":{"507ea43b":"code","79bc8ff8":"code","7756b6de":"code","e8584c7a":"code","e6415a08":"code","02330ed8":"code","1c4321da":"code","0a37e635":"code","d3b45a70":"code","85d6250f":"code","34dba931":"code","3707fdb5":"code","d78ae4e9":"code","bf63fd3b":"code","2f62d061":"code","ae103332":"code","9ff45ea5":"code","e517c20e":"code","e5bc5a8b":"code","5295d236":"code","4bbe42ad":"code","4304d5e4":"code","72b8c589":"code","84b1a6cf":"code","e73b8445":"code","74810a6b":"code","fbe99455":"markdown","1ce96192":"markdown"},"source":{"507ea43b":"## Because Kaggle has stopped internet to fight with bad bad guys . I am installing them from local downloads .\nimport os\n#os.system('pip install ..\/input\/sacremoses\/sacremoses-master\/')\n#os.system('pip install ..\/input\/transformers-2-3-0\/')","79bc8ff8":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers-2-3-0\/","7756b6de":"## Loading Libraries  -No they are not for books\nimport transformers, sys, os, gc\nimport numpy as np, pandas as pd, math\nimport torch, random, os, multiprocessing, glob\nimport torch.nn.functional as F\nimport torch, time\n\nfrom ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\nfrom scipy.stats import spearmanr\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup,    GPT2Config,GPT2Model,\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n)\nfrom transformers.modeling_gpt2 import GPT2PreTrainedModel\nfrom tqdm import tqdm\nprint(transformers.__version__)","e8584c7a":"## Common Variables for Notebook \nROOT = '..\/input\/google-quest-challenge\/' ## This is the root of all evil.","e6415a08":"pretrained_bert = '..\/input\/gpt2-models\/'","02330ed8":"## Make results reproducible .Else noone will believe you .\nimport random\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","1c4321da":"## This is for fast experiements , but we dont have enough GPU anyway, so thats a bummer!\nclass PipeLineConfig:\n    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n        self.lr = lr\n        self.warmup = warmup\n        self.accum_steps = accum_steps\n        self.epochs = epochs\n        self.seed = seed\n        self.expname = expname\n        self.head_tail = head_tail\n        self.freeze = freeze\n        self.question_weight = question_weight\n        self.answer_weight =answer_weight\n        self.fold = fold\n        self.train = train\n","0a37e635":"config_1 = PipeLineConfig(3e-5,0.05,4,0,42,'uncased_1',True,False,0.7,0.3,8,False) ## These are experiments . You can do as much as you want as long as inference is faster \nconfig_2 = PipeLineConfig(4e-5,0.03,4,6,2019,'uncased_2',True,False,0.8,0.2,5,False)## Adding various different seeds , folds and learning rate and mixing them and then doing inference .\nconfig_3 = PipeLineConfig(4e-5,0.03,4,4,2019,'small_test_3',True ,False, 0.8,0.2,3,True) ## For Small tests in Kaggle , less number of fold , less number of epochs.\nconfig_4 = PipeLineConfig(4e-5,0.05,1,4,2019,'small_test_4',True ,False, 0.8,0.2,3,True)\nconfig_5 = PipeLineConfig(3e-5,0.05,1,6,42,'gpt2class',True ,False, 0.8,0.2,8,True)\nconfig_6 = PipeLineConfig(3e-5,0.05,1,6,42,'gpt2cnn',True ,False, 0.8,0.2,8,True)\n## I am doing first experiement\nconfig = config_5\n## Note : If you want to train and just not copy this and submit then change the last parameter above to \"True\" it will kick off the training process.","d3b45a70":"seed_everything(config.seed)","85d6250f":"## load the data \ntrain = pd.read_csv(ROOT+'train.csv')\ntest = pd.read_csv(ROOT+'test.csv')\nsub = pd.read_csv(ROOT+'sample_submission.csv')","34dba931":"## Get the shape of the data\ntrain_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\nprint(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')","3707fdb5":"## These are those target data , many of which has created so much controversy . Mr Spearman has also commented NaN for many of them .\ntarget_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']\n\n","d78ae4e9":"# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\": ## We dont need it for gpt2\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=512, t_max_len=30, q_max_len=241, a_max_len=241):\n    \n    #350+128+30 = 508 + 4 = 512\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len)))\n        q_len_head = round(q_new_len\/2)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len\/2)\n        a_len_tail = -1* (a_new_len -a_len_head)        \n        t = t[:t_new_len]\n       # q = q[:q_new_len]\n      \n        q = q[:q_len_head]+q[q_len_tail:]\n      #  print(len(q1))\n        #a = a[:a_new_len]\n        a = a[:a_len_head]+a[a_len_tail:]\n    \n    return t, q, a\n\ndef _convert_to_gpt2_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for GPT2\"\"\"\n    \n    stoken = title +  question + answer ## No CLS or SEP tokens for GPT2\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in df[columns].iterrows():\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        ids, masks, segments = _convert_to_gpt2_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","bf63fd3b":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, lengths, labels = None):\n        \n        self.inputs = inputs\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.lengths = lengths\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        lengths         = self.lengths[idx]\n        if self.labels is not None: # targets\n            labels = self.labels[idx]\n            return input_ids, input_masks, input_segments, labels, lengths\n        return input_ids, input_masks, input_segments, lengths\n\n    def __len__(self):\n        return len(self.inputs[0])\n","2f62d061":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\n##Sakami gave this \nclass GPT2ClassificationHeadModel(GPT2Model):\n\n    def __init__(self, config, clf_dropout=0.1, n_class=30):\n        super(GPT2ClassificationHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.dropout = nn.Dropout(clf_dropout)\n        self.linear = nn.Linear(config.n_embd*3 , n_class)\n\n       # nn.init.normal_(self.linear.weight, std=0.02)\n       # nn.init.normal_(self.linear.bias, 0)\n        \n        self.init_weights()\n\n    def forward(self, input_ids, position_ids=None,attention_mask=None, token_type_ids=None, labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids,attention_mask, token_type_ids, past)\n        avg_pool = torch.mean(hidden_states, 1)\n        max_pool, _ = torch.max(hidden_states, 1)\n        h_conc = torch.cat((avg_pool, max_pool, hidden_states[:, -1, :]), 1)\n        logits = self.linear(self.dropout(h_conc))\n        return logits","ae103332":"class NeuralNet3(nn.Module):\n    def __init__(self,pretrained_bert):\n        super(NeuralNet3, self).__init__()\n        self.config = GPT2Config.from_pretrained(pretrained_bert, output_hidden_states=False)\n        self.gptmodel = GPT2Model(self.config)\n        self.gpt = self.gptmodel.from_pretrained( pretrained_bert,config =self.config)\n         \n        self.encoded_dropout = SpatialDropout( 0.2 )\n        self.pooled_dropout = nn.Dropout( 0.2 )            \n\n        \n        dense_hidden_units = self.gpt.config.hidden_size * 3\n        \n        self.linear1 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        self.linear2 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear( dense_hidden_units, 30 ),\n        )\n        \n    def forward( self, ids, masks, segments ):  \n        \n        outputs = self.gpt( input_ids=ids, attention_mask=masks, token_type_ids=segments)\n\n        last_hidden_state,present = self.gpt( input_ids=ids, attention_mask=masks, token_type_ids=segments)\n\n        last_hidden_state = self.encoded_dropout(last_hidden_state)\n\n        avg_pool = torch.mean( last_hidden_state, 1 )\n        max_pool, _ = torch.max( last_hidden_state, 1 )\n\n        h_conc = torch.cat( ( avg_pool, max_pool, last_hidden_state[:, -1, :] ), 1 )\n        h_conc_linear  = F.relu(self.linear1(h_conc))\n        hidden = h_conc + h_conc_linear \n            \n        h_conc_linear  = F.relu(self.linear2(hidden))\n        hidden = hidden + h_conc_linear      \n        \n        return self.classifier( hidden )\n","9ff45ea5":"##From combat-wombat\nclass GPT2CNN(GPT2PreTrainedModel):\n    def __init__(self, config, num_labels):\n        super().__init__(config)\n        self.transformer = GPT2Model(config)\n        self.cnn1 = nn.Conv1d(768, 256, kernel_size=3, padding=1)\n        self.cnn2 = nn.Conv1d(256, num_labels, kernel_size=3, padding=1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids,\n        position_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None,\n        past=None,\n    ):\n        x, _ = self.transformer(input_ids, position_ids,attention_mask, token_type_ids,labels, past)\n        x = x.permute(0, 2, 1)\n        x = F.relu(self.cnn1(x))\n        x = self.cnn2(x)\n        output, _ = torch.max(x, 2)\n        return output","e517c20e":"from tqdm.notebook import tqdm","e5bc5a8b":"def train_model(train_loader, optimizer, criterion, scheduler,config):\n    \n    model.train()\n    avg_loss = 0.\n    \n    tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n    \n    for idx, batch in tk0:\n\n        input_ids, input_masks, input_segments, labels, _ = batch\n        input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n        \n        output_train = model(ids = input_ids.long(),\n                             masks = input_masks,\n                             segments = input_segments,\n                            )\n        logits = output_train #output preds\n        loss = criterion(logits,labels)\n        #loss = config.question_weight*criterion(logits[:,0:21], labels[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], labels[:,21:30])\n        loss.backward()\n        \n        optimizer.step()\n       # scheduler.step()\n        optimizer.zero_grad()\n        \n        avg_loss += loss.item() \/ len(train_loader)\n        del input_ids, input_masks, input_segments, labels\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return avg_loss\n\ndef val_model(val_loader, val_shape, batch_size=4):\n\n    avg_val_loss = 0.\n    model.eval() # eval mode\n    \n    valid_preds = np.zeros((val_shape, len(target_cols)))\n    original = np.zeros((val_shape, len(target_cols)))\n    \n    tk0 = tqdm(enumerate(val_loader))\n    with torch.no_grad():\n        \n        for idx, batch in (tk0):\n            input_ids, input_masks, input_segments, labels, _ = batch\n            input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n            \n            output_val = model(ids = input_ids.long(),\n                             masks = input_masks,\n                             segments = input_segments,\n                            )\n            logits = output_val #output preds\n            avg_val_loss += criterion(logits, labels).item() \/ len(val_loader)\n            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n        \n        score = 0\n        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n        \n        # np.save(\"preds.npy\", preds)\n        # np.save(\"actuals.npy\", original)\n        \n        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n        \n        for i in range(len(target_cols)):\n            print(i, spearmanr(original[:,i], preds[:,i]))\n            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n    return avg_val_loss, score*1.0\/len(target_cols)","5295d236":"def predict_result(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), len(target_cols)))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader))\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(ids = x_batch[0].to(device), \n                            masks = x_batch[1].to(device),\n                            segments = x_batch[2].to(device),\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output","4bbe42ad":"!ls ..\/input","4304d5e4":"### Here is the GPT2 part \ntokenizer = GPT2Tokenizer.from_pretrained('..\/input\/gpt2-models\/', cache_dir=None)\ninput_categories = list(train.columns[[1,2,5]]); input_categories\n\n#bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\ngpt2_config =  GPT2Config.from_pretrained('..\/input\/gpt2-models\/')\n#gpt2_config.n_embd = 30\n\ngpt2_model = 'gpt2'\n#do_lower_case = 'uncased' in bert_model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n#output_model_file = 'bert_pytorch.bin'\n\nfrom sklearn.model_selection import GroupKFold\n\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512)\nlengths_test = np.argmax(test_inputs[0] == 0, axis=1)\nlengths_test[lengths_test == 0] = test_inputs[0].shape[1]","72b8c589":"# 'Cyclical Learning Rates for Training Neural Networks'- Leslie N. Smith, arxiv 2017\n#       https:\/\/arxiv.org\/abs\/1506.01186\n#       https:\/\/github.com\/bckenstler\/CLR\n\nclass CyclicScheduler1():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10 ):\n        super(CyclicScheduler1, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n\n    def __call__(self, time):\n\n        #sawtooth\n        #r = (1-(time%self.period)\/self.period)\n\n        #cosine\n        time= time%self.period\n        r = (np.cos(time\/self.period *np.pi)+1)\/2\n\n        lr = self.min_lr + r*(self.max_lr-self.min_lr)\n        return lr\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass CyclicScheduler2():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler2, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)\/\/self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n\n        r   = (np.cos(time\/period *np.pi)+1)\/2\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.4f, max_lr=%0.4f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\n#tanh curve\nclass CyclicScheduler3():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler3, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)\/\/self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n        r   = (np.tanh(-time\/period *16 +8)+1)*0.5\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string\n\n\n# net ------------------------------------\n# https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py ###############\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n        lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr","84b1a6cf":"class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\n# In[8]:\n\nNUM_FOLDS = 6  # change this\nSEED = config.seed\nBATCH_SIZE = 4\nepochs = config.epochs   # change this\nACCUM_STEPS = 1\n\nkf = GroupKFold(n_splits = NUM_FOLDS)\n\ntest_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\ntest_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\nresult = np.zeros((len(test), 30))\n\ny_train = train[target_cols].values # dummy\n\nprint(bcolors.FAIL, f\"For Every Fold, Train {epochs} Epochs\", bcolors.ENDC)\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X=train.values, groups=train.question_body)):\n    if fold > 2:\n        break\n    print(bcolors.HEADER, \"Current Fold:\", fold, bcolors.ENDC)\n\n    train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n    print(\"Train and Valid Shapes are\", train_df.shape, val_df.shape)\n    \n    print(bcolors.HEADER, \"Preparing train datasets....\", bcolors.ENDC)\n    \n    inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=512)\n    outputs_train = compute_output_arrays(train_df, columns = target_cols)\n    outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n    lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n    lengths_train[lengths_train == 0] = inputs_train[0].shape[1]\n    \n    print(bcolors.HEADER, \"Preparing Valid datasets....\", bcolors.ENDC)\n    \n    inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=512)\n    outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n    outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n    lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n    lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n    \n    print(bcolors.HEADER, \"Preparing Dataloaders Datasets....\", bcolors.ENDC)\n\n    train_set    = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n    \n    valid_set    = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n    \n    model = NeuralNet3(pretrained_bert)\n    model.zero_grad();\n    model.to(device);\n    torch.cuda.empty_cache()\n    model.train();\n    \n    i = 0\n    best_avg_loss   = 100.0\n    best_score      = -1.\n    best_param_loss = None\n    best_param_score = None\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n    criterion = nn.BCEWithLogitsLoss()\n    #scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= epochs*len(train_loader)\/\/ACCUM_STEPS)\n    scheduler = CyclicScheduler2( min_lr=0.000002, max_lr=0.000035, period=4, warm_start=0, max_decay=0.9 )\n\n    print(\"Training....\")\n    \n    for epoch in range(epochs):\n        lr = scheduler( epoch )\n        adjust_learning_rate(optimizer, lr )\n        torch.cuda.empty_cache()\n        \n        start_time   = time.time()\n        avg_loss     = train_model(train_loader, optimizer, criterion, scheduler,config)\n        avg_val_loss, score = val_model(valid_loader, val_shape=val_df.shape[0],batch_size =BATCH_SIZE)\n        elapsed_time = time.time() - start_time\n\n        print(bcolors.WARNING, 'Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n            epoch + 1, epochs, avg_loss, avg_val_loss, score, elapsed_time),\n        bcolors.ENDC\n        )\n\n        if best_avg_loss > avg_val_loss:\n            i = 0\n            best_avg_loss = avg_val_loss \n            best_param_loss = model.state_dict()\n\n        if best_score < score:\n            best_score = score\n            best_param_score = model.state_dict()\n            torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n        else:\n            i += 1\n\n    model.load_state_dict(best_param_score)\n    result += predict_result(model, test_loader)\n    torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n\n    del train_df, val_df, model, optimizer, criterion, scheduler\n    torch.cuda.empty_cache()\n    del valid_loader, train_loader, valid_set, train_set\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    gc.collect()\n\nNUM_FOLDS =1\nresult \/= NUM_FOLDS\nprint(result)\n","e73b8445":"submission = pd.read_csv(r'..\/input\/google-quest-challenge\/sample_submission.csv')\nsubmission.loc[:, 'question_asker_intent_understanding':] = result\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","74810a6b":"for i in range(30):\n    print(submission[target_cols[i]].value_counts())\n    ","fbe99455":"# Data Understanding","1ce96192":"## Just wanted to build a baseline GPT2 kernel even if the results are not promising\n### May we will use it in blend of blend of blend at the end !"}}