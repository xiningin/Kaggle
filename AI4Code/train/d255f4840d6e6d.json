{"cell_type":{"b1caf29d":"code","fd779aa8":"code","532eff20":"code","c5b24e22":"code","3f125e5d":"code","8b2f0433":"code","afb1dafa":"code","dfd78d13":"code","ff8d9c49":"code","a1cf35df":"code","84d7e773":"code","51f6ca4b":"code","611f6f19":"code","707cf76c":"code","964fb94f":"code","236830b2":"code","9e7358a6":"code","3995cd1f":"code","51d1abb8":"code","df8e478d":"code","6f5fb628":"code","5b8cb4be":"code","9c2db7fb":"code","c726624e":"code","23925251":"code","5d717389":"code","6e7b214b":"code","58ab29a6":"code","aeeef47c":"code","8d000cd8":"code","96d5af3c":"markdown","5a383b14":"markdown","333c7d48":"markdown","4077420b":"markdown","00116c86":"markdown","a35396cb":"markdown","d389c601":"markdown","d2a1ddb6":"markdown","0b2c67de":"markdown"},"source":{"b1caf29d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport random","fd779aa8":"train = pd.read_csv('\/kaggle\/input\/tutors-expected-math-exam-results\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tutors-expected-math-exam-results\/test.csv')\ntrain.head()","532eff20":"train.describe().T","c5b24e22":"train.hist(figsize =(12,14),bins = 15, grid = True)\nplt.show()","3f125e5d":"test.hist(figsize =(12,14),bins = 15, grid = True)\nplt.show()","8b2f0433":"plt.rcParams['figure.figsize'] = (14.0, 12.0)\nsns.heatmap(train.corr(), annot = True,fmt='.2g', vmin=-1, vmax=1, center= 0, linewidths = 2, cmap = 'GnBu')\nplt.title('\u0422\u0435\u043f\u043b\u043e\u0432\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0439')\nplt.show()","afb1dafa":"train['qualification * years_of_experience'] = train['qualification'] * train['years_of_experience']\ntest['qualification * years_of_experience'] = test['qualification'] * test['years_of_experience']\n\ntemp = test.drop(columns='Id')","dfd78d13":"# \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a \u0444\u043e\u0440\u043c\u0430\u0442\u0443 array\n# convert data to array\nfeatures = temp.columns\ntarget = 'mean_exam_points'\nX = train[features].values\ny = train[target].values\ntest_set = test[features].values","ff8d9c49":"class PCA():\n    def __init__(self, n_components):\n        self.n_components = n_components\n\n    def fit_transform(self, data):\n        data_ = data.astype(float)\n        cov_matrix = data_.T.dot(data_) # \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0439\n        # Eigenvalues, eigenvectors \/ (\u0421\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430)\n        eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n        # list of tuples (value, vector) \/ \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439 (\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u0432\u0435\u043a\u0442\u043e\u0440)\n        eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:,i]) for i in range(len(eig_values))]\n        # sorting the list in descending order eigenvalues \/ \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e \u0443\u0431\u044b\u0432\u0430\u043d\u0438\u044e eig_values\n        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n        eig_sum = sum(eig_values)\n        var_exp = [(i \/ eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n        cum_var_exp = np.cumsum(var_exp)\n\n        w = np.hstack(([eig_pairs[i][1].reshape(data.shape[1], 1) for i in range(self.n_components)]))\n        data_pca = data_.dot(w)\n        return data_pca, print(f'\u0414\u043e\u043b\u044f \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c\u0430\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u043e\u0439 \\n'\n        f'Proportion of variance described by each component \\n{var_exp}'),\\\n    print(f'\u041a\u0443\u043c\u0443\u043b\u044f\u0442\u0438\u0432\u043d\u0430\u044f \u0434\u043e\u043b\u044f \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \\n'\n    f'Cumulative proportion of variance \\n{cum_var_exp}')","a1cf35df":"imp = ['years_of_experience','lesson_price','qualification']\npca = PCA(n_components=1)\ntrain_components = pca.fit_transform(train[imp])","84d7e773":"pca_1 = train_components[0][:].values\npca_1","51f6ca4b":"#X = np.hstack((X,pca_1))\nX","611f6f19":"test_components = pca.fit_transform(test[imp])\npca_1 = test_components[0][:].values\npca_1","707cf76c":"#test_set = np.hstack((test_set,pca_1))\ntest_set","964fb94f":"def normalin(data):\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            data[i][j] = (data[i][j] - means[j])\/stds[j]\n    return data","236830b2":"#normalin(X)\n#normalin(test_set)","9e7358a6":"class Node:\n\n    def __init__(self, index, t, true_branch, false_branch):\n         # index of the attribute to compare with the threshold in the node \/ \u0438\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u0440\u043e\u0433\u043e\u043c \u0432 \u0443\u0437\u043b\u0435\n        self.index = index\n        self.t = t  # treshold\n        # branch satisfies condition in node \/ \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u043e, \u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u044f\u044e\u0449\u0435\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044e \u0432 \u0443\u0437\u043b\u0435\n        self.true_branch = true_branch \n        self.false_branch = false_branch # not\n\nclass Leaf:\n\n    def __init__(self, data, values):\n        self.data = data\n        self.values = values\n        self.prediction = self.predict()\n\n    def predict(self):\n        prediction = np.mean(self.values)\n        return prediction\n\nclass RegressionTreeStG():\n\n    def __init__(self, max_depth = 50):\n        self.max_depth = max_depth\n        self.tree = None\n\n    # iform criterion \/  \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438\n    # In regression, the spread of values will be characterized by variance\n    # \u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0440\u0430\u0437\u0431\u0440\u043e\u0441 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0431\u0443\u0434\u0435\u0442 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\n    def inf_criterion(self, values):\n        return np.var(values)\n\n    @staticmethod\n    def get_subsample(len_sample):\n        # saving the feature indexes \/ \u0431\u0443\u0434\u0435\u043c \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043d\u0435 \u0441\u0430\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0430 \u0438\u0445 \u0438\u043d\u0434\u0435\u043a\u0441\u044b\n        sample_indexes = [i for i in range(len_sample)]\n\n        len_subsample = int(np.sqrt(len_sample))\n        subsample = []\n\n        random.shuffle(sample_indexes)\n        for _ in range(len_subsample):\n            subsample.append(sample_indexes.pop())\n\n        return subsample\n\n    # Quality calculation function \/ \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0441\u0441\u0447\u0435\u0442\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\n    def quality(self, left_labels, right_labels, current_dispersion):\n        # part that went to left subtree \/ \u0434\u043e\u043b\u044f \u0432\u044b\u0431\u043e\u043a\u0438, \u0443\u0448\u0435\u0434\u0448\u0430\u044f \u0432 \u043b\u0435\u0432\u043e\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u043e\n        p = float(left_labels.shape[0]) \/ (left_labels.shape[0] + right_labels.shape[0])\n        return current_dispersion - p * self.inf_criterion(left_labels) - (1 - p) * self.inf_criterion(right_labels)\n\n    # Splitting data function \/ \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0443\u0437\u043b\u0435\n    def split(self, data, values, index, t):\n\n        left = np.where(data[:, index] <= t)\n        right = np.where(data[:, index] > t)\n\n        true_data = data[left]\n        false_data = data[right]\n        true_values = values[left]\n        false_values = values[right]\n\n        return true_data, false_data, true_values, false_values\n\n    # Finding the best split \/ \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\n    def find_best_split(self, data, values):\n\n        # minimum number of objects in node \/ \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0438\u043c \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u0443\u0437\u043b\u0435\n        min_leaf = 5\n        current_inf = self.inf_criterion(values)\n        best_quality = 0\n        best_t = None\n        best_index = None\n\n        n_features = data.shape[1]\n        subsample = self.get_subsample(n_features)\n\n        for index in subsample:\n            # check only unique values \/ \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\n            t_values = np.unique([row[index] for row in data])\n\n            for t in t_values:\n                true_data, false_data, true_labels, false_labels = self.split(data, values, index, t)\n                # If there < 5 objects left in node, we skip node\n                #  \u0415\u0441\u043b\u0438 \u0432 \u0443\u0437\u043b\u0435 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f < 5 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 - \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0443\u0437\u0435\u043b\n                if len(true_data) < min_leaf or len(false_data) < min_leaf:\n                    continue\n\n                current_quality = self.quality(true_labels, false_labels, current_inf)\n                # selecting threshold with maximum quality\n                #  \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0440\u043e\u0433 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u043e\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\n                if current_quality > best_quality:\n                    best_quality, best_t, best_index = current_quality, t, index\n\n        return best_quality, best_t, best_index\n\n    # building a tree \/ \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u0440\u0435\u0432\u0430\n    def build_tree(self, data, values, current_deep, max_depth):\n\n        quality, t, index = self.find_best_split(data, values)\n        # Stop recursion if there is no quality gain\n        # \u041f\u0435\u0440\u0432\u044b\u0439 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 (\u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u044e, \u0435\u0441\u043b\u0438 \u043d\u0435\u0442 \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430)\n        if quality == 0:\n            return Leaf(data, values)\n        # Stop recursion if the tree depth is exceeded\n        # \u0412\u0442\u043e\u0440\u043e\u0439 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 (\u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u044e, \u0435\u0441\u043b\u0438 \u043f\u0440\u0435\u0432\u044b\u0448\u0435\u043d\u0430 \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u0430)\n        #(\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0433\u0440\u0443\u0431\u044b\u043c, \u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0437\u0430\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043b \u0441\u0435\u0431\u044f \u0432 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432)\n        if current_deep > max_depth:\n            return Leaf(data, values)\n\n        current_deep += 1\n\n        true_data, false_data, true_values, false_values = self.split(data, values, index, t)\n        # Building trees \/ \u0420\u0435\u043a\u0443\u0440\u0441\u0438\u0432\u043d\u043e \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u044f\n        true_branch = self.build_tree(true_data, true_values, current_deep, max_depth)\n        false_branch = self.build_tree(false_data, false_values, current_deep, max_depth)\n\n        return Node(index, t, true_branch, false_branch)\n\n    # Predicting objects function\n    def predict_value(self, obj, node):\n\n        # If have reached the leaf - stop recursion\n        if isinstance(node, Leaf):\n            answer = node.prediction\n            return answer\n\n        if obj[node.index] <= node.t:\n            return self.predict_value(obj, node.true_branch)\n        else:\n            return self.predict_value(obj, node.false_branch)\n\n    def predict(self, data):\n\n        pred_values = []\n        for obj in data:\n            prediction = self.predict_value(obj, self.tree)\n            pred_values.append(prediction)\n        return pred_values\n\n    def fit(self, data, values):\n        self.tree = self.build_tree(data, values, 0, self.max_depth)","3995cd1f":"class StGradBoost:\n    \n    def __init__(self, max_depth, n_trees, alpha, coef, iter_num):\n        self.max_depth = max_depth\n        self.n_trees = n_trees\n        self.alpha = alpha\n        self.coef = coef\n        self.iter_num = iter_num\n        self.trees = []\n\n    def bias(self, y, z):\n        return 2*(y - z)\n\n\n    def get_bootstrap(self, X, Y, N):\n        n_samples = X.shape[0]\n        bootstrap = []\n\n        for i in range(N):\n            b_data = np.zeros(X.shape)\n            b_labels = np.zeros(Y.shape)\n\n            for j in range(n_samples):\n                sample_index = random.randint(0, n_samples-1)\n                b_data[j] = X[sample_index]\n                b_labels[j] = Y[sample_index]\n            bootstrap.append((b_data, b_labels))\n\n        return bootstrap\n\n    def fit(self, X, Y):\n        trees = []\n        bootstrap = self.get_bootstrap(X, Y, self.n_trees)\n        for i in range(self.n_trees):\n            tree = RegressionTreeStG(max_depth=self.max_depth)\n            X_temp = bootstrap[i][0]\n            y_temp = bootstrap[i][1]\n\n            if len(trees) == 0:\n                tree.fit(X_temp, y_temp)\n\n            else:\n                # \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u043e\u0442\u0432\u0435\u0442\u044b \u043d\u0430 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438\n                # Responses to the current composition\n                values = self.predict(X_temp)\n                # \u041d\u0430\u0447\u0438\u043d\u0430\u044f \u0441\u043e \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u043d\u0430 \u0441\u0434\u0432\u0438\u0433\n                # train algorithms for the shift\n                tree.fit(X_temp, self.bias(y_temp, values))\n\n            self.trees.append(tree)\n        return self.trees\n\n    def predict(self, X):\n        return np.array([sum([self.alpha * coef * alg.predict([x])[0] for alg,\\\n                        coef in zip(self.trees, self.coef)]) for x in X])","51d1abb8":"# R2_score\ndef r2_score(y, y_real):\n    ss_total=np.sum((y_real-np.mean(y_real))**2)\n    ss_res=np.sum((y_real-y)**2)\n    return 1-ss_res\/ss_total\n\n# MSE\ndef mean_squared_error(y_real, prediction):\n    return (sum((y_real - prediction)**2)) \/ len(y_real)","df8e478d":"# train_test_split\ndef tts(X, y, test_size, random_state):\n    np.random.seed(random_state)\n    \n    train_test_cut = int(len(X) * (1 - test_size))\n    \n    shuffle_index = np.random.permutation(X.shape[0])\n    X_shuffled, y_shuffled = X[shuffle_index], y[shuffle_index]\n    \n    X_train, X_test, y_train, y_test = \\\n    X_shuffled[:train_test_cut], \\\n    X_shuffled[train_test_cut:], \\\n    y_shuffled[:train_test_cut], \\\n    y_shuffled[train_test_cut:]\n    \n    return X_train, X_test, y_train, y_test","6f5fb628":"X_train, X_test, y_train, y_test = tts(X,y,test_size = 0.3, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","5b8cb4be":"# \u043f\u0440\u0438\u043c\u0435\u043c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0440\u0430\u0432\u043d\u044b\u043c\u0438 1\n# take coefficients equal to 1\nn_trees = 10\ncoefs = [1] * n_trees","9c2db7fb":"# make model\nmodel = StGradBoost(max_depth=16, n_trees=n_trees, alpha=0.1, coef = coefs, iter_num = 1500)","c726624e":"# fit model on train set\nmodel.fit(X_train,y_train)","23925251":"y_train_pred = model.predict(X_train)\nr2_score(y_train_pred, y_train)","5d717389":"y_test_pred = model.predict(X_test)\nr2_score(y_test_pred, y_test)","6e7b214b":"# train model on all set\nmodel.fit(X,y)","58ab29a6":"test_pred = model.predict(test_set)","aeeef47c":"sub = pd.concat([test['Id'], pd.Series(test_pred)], axis = 1)\nsub = sub.rename(columns = {0 : 'mean_exam_points'})\nsub","8d000cd8":"sub.to_csv('StGBoost_predict.csv', index=None)","96d5af3c":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\n\nLet's look at distribution","5a383b14":"### RegressionTree for Stochastic Gradient Descent","333c7d48":"## Stochastic(batch) Gradient Boosting","4077420b":"### Lets make one feature (\u0434\u043b\u044f \u043f\u0440\u0438\u043b\u0438\u0447\u0438\u044f \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043e\u0434\u0438\u043d \u043f\u0440\u0438\u0437\u043d\u0430\u043a)","00116c86":"### \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 ( Standard scaler )","a35396cb":"### Batch Gradient Descent","d389c601":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u0435\u043f\u043b\u043e\u0432\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0439\n\nCorr matrix","d2a1ddb6":"\u0414\u0430\u043d\u043d\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043d\u0435 \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u043b\u043e\u0441\u044c \u0441 \u0446\u0435\u043b\u044c\u044e \u0432\u044b\u0431\u0438\u0442\u044c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0441\u043a\u043e\u0440(\u0432 \u043d\u0435\u043c \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435\u0442 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432). \u041e\u043d\u043e \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0443 \u041f\u0430\u043a\u0435\u0442\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438. \u0422\u0430\u043a\u0436\u0435 \u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0435, \u0441\u0443\u0433\u0443\u0431\u043e \u0432 \u043f\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0446\u0435\u043b\u044f\u0445, \u0432\u043a\u043b\u044e\u0447\u0435\u043d \u043c\u0435\u0442\u043e\u0434 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442 \u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438. \n\nThis solution was not created for the purpose of knocking out the maximum score. It demonstrates how Batch Gradient Descent works on regression trees. Also included in the solution, for cognitive purposes, is the method of principal components and normalization.","0b2c67de":"### PCA"}}