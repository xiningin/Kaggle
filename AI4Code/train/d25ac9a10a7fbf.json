{"cell_type":{"c5d5761e":"code","cad09df8":"code","4486e0fe":"code","9ff55c78":"code","2001fb01":"code","e52da86c":"code","d80e9228":"code","8504b797":"code","2cbb0a8c":"code","27cfd62e":"code","e9a9baab":"code","18d9e95d":"code","09ce008a":"code","2525a186":"code","967ab627":"code","5f925887":"code","71896f87":"code","dc2d9c02":"code","52427c64":"code","b979643a":"code","7b8fe3cd":"code","40406fa4":"code","115a0bb8":"code","89a30774":"code","f353ef38":"code","b42d5ea9":"code","d514bc07":"code","2443ac8b":"code","56613fd4":"code","6b57ae1f":"code","857feb86":"code","73b42a00":"code","b8586564":"code","43e26ef6":"code","75afb6bf":"code","7bb85d38":"code","25d6fc73":"code","059507bd":"code","67d826e6":"code","13045de1":"code","5b6cb905":"code","5499e021":"code","3fa07c0f":"code","4efac0d5":"code","e31f6ef5":"code","9e13525d":"code","7d66a2a3":"code","c2ac4d43":"code","c22a99c2":"code","a0845ebe":"code","f2433e30":"code","c541c189":"code","b9dc7fba":"code","72df44a9":"code","f83d4ea5":"code","5a06d8fa":"code","9cb9e41e":"code","0a5cb737":"code","7f1f6c2e":"code","d6f0a863":"code","e1d5fc34":"code","512ddc87":"code","a6f17631":"code","c66c0d9e":"code","7d7f22ef":"code","96a49fe3":"code","cd136510":"code","712a92ef":"code","17e8eef5":"code","12a2e670":"code","565a14d5":"code","48d56b3c":"code","090b91bb":"code","4e6d5fd1":"code","67382140":"code","864bd089":"code","61ecb3f5":"code","0bac40d1":"code","bb8f8bb7":"markdown","a47af295":"markdown","f5d7a7d7":"markdown","86242e9d":"markdown","8b33f175":"markdown","0d1aa9f9":"markdown","9eaa18b3":"markdown","67258eba":"markdown","6ad8a9c6":"markdown","4a8232c2":"markdown","14e2fe7a":"markdown","6bb0e184":"markdown","52284234":"markdown","4a58da91":"markdown","a19f99c3":"markdown","7af331dd":"markdown","a5b79fff":"markdown","b730b709":"markdown","fc0548a2":"markdown","9dcd7b1c":"markdown","5d176550":"markdown","86ef0c62":"markdown","b935d5f5":"markdown","cc4161c0":"markdown","064d582b":"markdown","fb79c129":"markdown","2a9d1bee":"markdown","f37c4412":"markdown","e3d64a4d":"markdown"},"source":{"c5d5761e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cad09df8":"import os\nimport gc\nimport random\nimport math\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import log_loss\n\nimport category_encoders as ce\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","4486e0fe":"from sklearn.decomposition import PCA","9ff55c78":"!pip  install iterative-stratification","2001fb01":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","e52da86c":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)","d80e9228":"import plotly.express as px","8504b797":"train_drug=pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\ntrain_targets_scored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_features=pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features=pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","2cbb0a8c":"train_features.head(5)","27cfd62e":"train_targets_scored.head(5)","e9a9baab":"train_targets_nonscored.head(5)","18d9e95d":"sample_submission=pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n","09ce008a":"train_features.isnull().any().sum()\ntest_features.isnull().any().sum()","2525a186":"print('Train dataset',train_features.shape)\nprint('Test dataset',test_features.shape)","967ab627":"cat_features=train_features.select_dtypes(include=[\"object\"])\n","5f925887":"len(cat_features.columns)","71896f87":"cat_features=train_features.select_dtypes(include=[\"object\"])\nnum_features=train_features.select_dtypes(exclude=[\"object\"])\nprint(f'Categorical features {len(cat_features.columns)}, Number features {len(num_features.columns)} ' )","dc2d9c02":"import seaborn as sns\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt","52427c64":"fig,ax=plt.subplots(2,3, figsize=(8,4), dpi=100) \nsns.countplot(train_features[\"cp_dose\"],ax=ax[0,0])\nsns.countplot(train_features[\"cp_type\"],ax=ax[0,1])\nsns.countplot(train_features[\"cp_time\"],ax=ax[0,2])\nsns.countplot(test_features[\"cp_dose\"],ax=ax[1,0])\nsns.countplot(test_features[\"cp_type\"],ax=ax[1,1])\nsns.countplot(test_features[\"cp_time\"],ax=ax[1,2])\nax[0,0].set_title('Train Doses Low\/Hight')\nax[1,0].set_title('Test Doses Low\/Hight')\nax[0,1].set_title('Train Compound \/ control treatment')\nax[1,1].set_title('Test Compound \/ control treatment')\nax[0,2].set_title('Train duration(hour)')\nax[1,2].set_title('Test duration(hour)')\n\nplt.tight_layout()","b979643a":"train_features.groupby([\"cp_type\"])[\"sig_id\"].count()","7b8fe3cd":"gens = list(filter(lambda x: x.startswith('g-'),train_features.columns))\ncells = list(filter(lambda x: x.startswith('c-'),train_features.columns))\nprint(f'gen len {len(gens)}, cells len {len(cells)}')","40406fa4":"def plot_list(plot_lists,name):\n    fig,ax=plt.subplots(1,4, figsize=(10,4), dpi=100) \n    fig.suptitle(name, fontsize=16)\n    for i,gen in enumerate(plot_lists):\n        train_features.hist(gen,ax=ax[i])\n        plt.tight_layout()","115a0bb8":"plot_list( [gens[np.random.randint(0, 772)] for i in range(4)],'GENE DISTRIBUTION')","89a30774":"plot_list( [cells[np.random.randint(0, 99)] for i in range(4)],'CELLS DISTRIBUTION')","f353ef38":"plt.figure(figsize=(10,8))\nsns.heatmap(train_features[cells].corr(),cmap='viridis')","b42d5ea9":"all_list=cells+gens\nplt.figure(figsize=(8,6))\nsns.heatmap(train_features[list([all_list[np.random.randint(1,len(all_list)-1)] for i in range(90)])].corr(),cmap='viridis')","d514bc07":"target_non_zero=pd.DataFrame(train_targets_scored.drop([\"sig_id\"],axis=1).sum(axis=0).sort_values().reset_index())\ntarget_non_zero.columns=['name','count_z']","2443ac8b":"plt.figure(figsize=(7,8))\nsns.barplot(data=target_non_zero[-50:],x='count_z',y='name')","56613fd4":"plt.figure(figsize=(7,8))\nsns.barplot(data=target_non_zero[:50],x='count_z',y='name')","6b57ae1f":"print(f'it is {target_non_zero[target_non_zero.count_z.values<20].count().values[0]} value that number of positive sample <20 , it is less than 0.1%')\n","857feb86":"plt.figure(figsize=(17,8))\nsns.barplot(y=target_non_zero.name[-20:],x=(target_non_zero.count_z.values\/train_targets_scored.shape[0]*100)[-20:])\n","73b42a00":"data=train_targets_scored.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns=['row','count']\ndata=data.groupby(\"count\")['row'].count().reset_index()\nplt.figure(figsize=(10,5))\nsns.barplot(data=data,x='count',y='row')","b8586564":"data['count'].values","43e26ef6":"\nplt.figure(figsize=(10,10))\nlabels = data['count'].values\nexplode = (0, 0.1, 0, 0, 0, 0.1, 0)  \n\nfracs = data['row'].values\/train_targets_scored.shape[0]*100\n# Make figure and axes\nfig, axs = plt.subplots()\n# A standard pie plot\naxs.pie(fracs,  autopct='%1.1f%%', shadow=True,explode=explode)\n\naxs.legend( loc=\"left\",labels=labels)\nplt.show()","75afb6bf":"train_targets_scored.describe()","7bb85d38":"columns=gens+cells","25d6fc73":"correlation_matrix = pd.DataFrame()\n\nfor t_col in train_targets_scored.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train_features[col].corr(train_targets_scored[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list","059507bd":"correlation_matrix['train_features']=columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\ncorrelation_matrix\n","67d826e6":"maxCol=lambda x: max(x.min(), x.max(), key=abs)\nhigh_scores = correlation_matrix.apply(maxCol, axis=0).reset_index()\nhigh_scores.columns=[\"column\",\"corr\"]","13045de1":"fig = px.bar(\n    high_scores, \n    x='column', \n    y=\"corr\", \n    orientation='v', \n    title='Best correlation with train columns for every target column', \n    width=1200,\n    height=800\n)\n\nfig.show()","5b6cb905":"col_df = pd.DataFrame()\ntr_cols = list()\ntar_cols = list()\n\nfor col in correlation_matrix.columns:\n    tar_cols.append(col)\n    tr_cols.append(\n        correlation_matrix[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(1).values[0]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_best_column'] = tr_cols\n\ntotal_scores = pd.merge(high_scores, col_df)\n\ntotal_scores","5499e021":"target_columns = train_targets_scored.columns.tolist()\ntarget_columns.remove('sig_id')\nfor_analysis = [\n    target_columns[\n        np.random.randint(0, len(target_columns)-1)\n    ] for i in range(5)\n]\n\ncurrent_corr = correlation_matrix[for_analysis]","3fa07c0f":"current_corr","4efac0d5":"col_df=pd.DataFrame()\nfirst_col=list()\nsecond_col=list()\ntar_cols = list()\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    first_col.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].values[0])\n    second_col.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].values[1])\ncol_df['column']=tar_cols\ncol_df['train_1_column']=first_col\ncol_df['train_2_column']=second_col\ncol_df","e31f6ef5":"for i in range(col_df.shape[0]):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_targets_scored[col_df.iloc[i]['column']]\n    analysis['x'] = train_features[col_df.iloc[i]['train_1_column']]\n    analysis['y'] = train_features[col_df.iloc[i]['train_2_column']]\n    analysis.columns = [\n        'color', \n        col_df.iloc[i]['train_1_column'], \n        col_df.iloc[i]['train_2_column']\n    ]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 12\n    plt.figure(figsize=(8,7))\n    plt.title(col_df.iloc[i]['column'])\n    sns.scatterplot(x=col_df.iloc[i]['train_1_column'],y=col_df.iloc[i]['train_2_column'], data=analysis,hue='color',size='size')","9e13525d":"target_columns","7d66a2a3":"last_term={}\nfor col in target_columns:\n    try:\n        last_term[col.split('_')[-1]] += 1\n    except:\n        last_term[col.split('_')[-1]] = 1\nlast_term=pd.DataFrame(last_term.items(),columns=['name','count'])\nlast_term=last_term[last_term['count']>1].sort_values('count')\nfig = px.bar(\n    last_term, \n    x='name', \n    y=\"count\", \n    orientation='v', \n    title='Group of target columns', \n    width=800,\n    height=500\n)\n\nfig.show()\n    ","c2ac4d43":"answer = list()\n\nfor group in last_term.name.tolist():\n    agent_list = list()\n    for item in target_columns:\n        if item.split('_')[-1] == group:\n            \n            agent_list.append(item)\n    agent_df = train_targets_scored[agent_list]\n    data = agent_df.astype(bool).sum(axis=1).reset_index()\n    answer.append(data[0].max())\nanswer_df=pd.DataFrame({'columns':last_term.name.tolist(),'value':answer})\nfig = px.bar(\n    answer_df, \n    x='columns', \n    y=\"value\", \n    orientation='v', \n    title='Maximum number of active columns in one sample in every group', \n    width=800,\n    height=500\n)\n\nfig.show()\n    ","c22a99c2":"cat_var=['cp_type','cp_time','cp_dose']\ncat_df=pd.concat([train_features[cat_var],train_targets_scored.drop('sig_id',axis=1)],axis=1)\ncat_df","a0845ebe":"def find_targets_zero(cat_var):\n    dict_cat={}\n    for i in cat_var:\n        for cat in np.unique(cat_df[i]):\n            name_columns=[]\n            for col in cat_df.columns:\n                if col in cat_var:\n                    continue\n                else: \n                    if len(cat_df[cat_df[i]== cat][col].value_counts())==1:\n                        name_columns.append(col)\n            dict_cat[cat]=name_columns\n    return dict_cat\n                    \n    ","f2433e30":"result=find_targets_zero(cat_var)\n","c541c189":"for key,value in result.items():\n    if len(value) >10 :\n        print(f'the number of zero target for {key} is {len(value)}')\n    else:\n        print(f'{key}:{value}')\n","b9dc7fba":"from sklearn.preprocessing import QuantileTransformer","72df44a9":"SEED_VALUE = 42","f83d4ea5":"vec_len = train_features.shape[0]\nvec_len_test =test_features.shape[0]\nfor col in (gens + cells):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n   \n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","5a06d8fa":"# GENES\nn_comp = 600\n\ndata = pd.concat([pd.DataFrame(train_features[gens]), pd.DataFrame(test_features[gens])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[gens]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))\n","9cb9e41e":"# CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[cells]), pd.DataFrame(test_features[cells])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[cells]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","0a5cb737":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","7f1f6c2e":"train_features.shape","d6f0a863":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","e1d5fc34":"train = train.drop('cp_type',axis=1)\ntest = test.drop('cp_type',axis=1)","512ddc87":"train.head(5)","a6f17631":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","c66c0d9e":"print(train.shape)\nprint(test.shape)\nprint(sample_submission.shape)","7d7f22ef":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","96a49fe3":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n","cd136510":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","712a92ef":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))  ","17e8eef5":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","12a2e670":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","565a14d5":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","48d56b3c":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","090b91bb":"# Show model architecture\nmodel = Model(num_features, num_all_targets)\nmodel","4e6d5fd1":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","67382140":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions\ndef run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","864bd089":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","61ecb3f5":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score \/ y_pred.shape[1])","0bac40d1":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","bb8f8bb7":"# Data loading","a47af295":"# QuantileTransformer","f5d7a7d7":"# Single fold training","86242e9d":"The highest number of positive sample is 3.5% ","8b33f175":"There are three categorical features : cp_type, cp_dose, cp_time.\nLet's see there distribution","0d1aa9f9":"There is a clear high correlation between cell viabilities that has to be examined.","9eaa18b3":"We see that our distribution (test and train) are very similar to each other. Samples  control perturbation( ctl_vehicle) is less than 8%.","67258eba":"Let's see what is the higher value (absolute) of correlation for target columns with every column from train set","6ad8a9c6":"We can see that our distributions of genes and cells lool like normal,with mean in zero as random following plots show.","4a8232c2":"Let's take some random columns from target and see their correlation","14e2fe7a":"# TRAIN & TARGET CORRELATION","6bb0e184":"We see :\n1. that for column cp_type all records are zero where cp_type=ctl_vehicle \n2. for column cp_time=24 ,records 'atp-sensitive_potassium_channel_antagonist', 'erbb2_inhibitor' is zero\n3. for column cp_time=72 ,records 'atp-sensitive_potassium_channel_antagonist', 'erbb2_inhibitor' is zero\n4. for column cp_dose=D2 ,records 'atp-sensitive_potassium_channel_antagonist', 'erbb2_inhibitor' is zero\n ","52284234":"# Target categorical column(dependecies)","4a58da91":"# Training features correlation","a19f99c3":"# About this Competition\nIn this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.\n\nTwo notes:\n\n* the training data has an additional (optional) set of MoA labels that are not included in the test data and not used for scoring.\n* the re-run dataset has approximately 4x the number of examples seen in the Public test.\n# Files\n* train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n* train_drug.csv - This file contains an anonymous drug_id for the training set only.\n* train_targets_scored.csv - The binary MoA targets that are scored.\n* train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n* test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n* sample_submission.csv - A submission file in the correct format.","7af331dd":"# ___________________________________","a5b79fff":"Check for empty values","b730b709":"# Variance Encoding","fc0548a2":"We see that 40 % of sample have zeros in all columns, and only 53% have only one active target column","9dcd7b1c":"# PCA","5d176550":"# Preprocessing steps","86ef0c62":"Lets see zero targets.","b935d5f5":"# Categorical Features","cc4161c0":"Some distribution of randomly selected columns.","064d582b":"# Target analysis","fb79c129":"# EDA","2a9d1bee":"# Dataset Classes","f37c4412":"# Model","e3d64a4d":"If we look to the name of target columns, it is look like the last term in the columns name is definition of a group."}}