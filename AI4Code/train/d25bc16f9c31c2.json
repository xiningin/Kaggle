{"cell_type":{"d0b093af":"code","ee4da867":"code","f20ba14e":"code","855827bf":"code","33909c9d":"code","b61ec6ea":"code","8fb0eedb":"code","af314a35":"code","42e499c9":"code","99ee0a51":"code","7bf207eb":"code","8cb650e1":"code","f43af7c1":"code","f7491305":"code","cabe6eee":"code","8f5a940c":"code","dfe3e9df":"code","086afcaa":"code","1d08b9f6":"code","ef637c6e":"code","952bfc00":"code","5edcfaca":"code","e111c4ab":"code","2f9cb5ab":"code","e108a224":"code","2f973197":"code","6ba28496":"code","507210df":"code","01b809a7":"code","6a6c1b01":"code","66a67546":"markdown","f1c65c55":"markdown","67247eda":"markdown","48807ea1":"markdown","3aec83f3":"markdown","89791360":"markdown","75e6caa2":"markdown","6605607d":"markdown","771c64dd":"markdown","619bfa87":"markdown","d6727469":"markdown","e0de3ccd":"markdown","933d19c8":"markdown"},"source":{"d0b093af":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","ee4da867":"import numpy as np\nimport pandas as pd\nfrom fastai.vision import *","f20ba14e":"path = Path(\"..\/input\/\")\npath.ls()","855827bf":"# look at the image-label file\ndf = pd.read_csv(path\/\"train_v2.csv\")\ndf.head()","33909c9d":"# image transformation\ntfms = get_transforms(flip_vert = True, max_lighting = 0.1, max_zoom = 1.05, max_warp = 0.)","b61ec6ea":"# create ImageList\nnp.random.seed(7)\nsource = (ImageList.from_csv(path, \"train_v2.csv\", folder = \"train-jpg\", suffix = \".jpg\")\n         .split_by_rand_pct(0.2)\n         .label_from_df(cols = \"tags\", label_delim = \" \"))\n# label_delim to separate the words in \"tags\" column so as to generate multiple labels ","8fb0eedb":"# data with size 128 (default 256)\ndata = (source.transform(tfms, size = 128)\n       .databunch()\n       .normalize(imagenet_stats))","af314a35":"# show the data\ndata.show_batch(rows = 4, figsize = (15,15))","42e499c9":"# metrics\nacc_thresh = partial(accuracy_thresh, thresh = 0.2) # choose threshold = 0.2\nf2_score = partial(fbeta, beta = 2, thresh = 0.2) # fbeta where beta = 2 (F2) and threshold = 0.2","99ee0a51":"# download model\nlearn = cnn_learner(data, models.resnet50, metrics = [acc_thresh, f2_score], model_dir = \"\/tmp\/models\")","7bf207eb":"# find good learning rate\nlearn.lr_find()\nlearn.recorder.plot()","8cb650e1":"# baseline model with image size=128\nlearn.fit_one_cycle(cyc_len = 5, max_lr = slice(1e-2))","f43af7c1":"# save this model\nlearn.save(\"baseline-rn50-128\")","f7491305":"# create 2nd data with original size\nnp.random.seed(7)\ndata2 = (source.transform(tfms, size = 256)\n        .databunch()\n        .normalize(imagenet_stats))","cabe6eee":"# create another CNN model\nlearn2 = cnn_learner(data2, models.resnet50, metrics = [acc_thresh, f2_score], model_dir = \"\/tmp\/models\")","8f5a940c":"# plot the learning rate of this model\nlearn2.lr_find()\nlearn2.recorder.plot()","dfe3e9df":"# baseline model with image size = 256\nlr = 3e-2\nlearn2.fit_one_cycle(cyc_len = 5, max_lr = slice(lr))","086afcaa":"# save baseline model for size=256\nlearn2.save(\"baseline-rn50-256\")","1d08b9f6":"# plot the learning rate\nlearn2.unfreeze()\nlearn2.lr_find()\nlearn2.recorder.plot()","ef637c6e":"# model 2 for image size = 256\nlearn2.fit_one_cycle(cyc_len = 10, max_lr = slice(1e-5, lr\/10))","952bfc00":"# plot the training and validation loss of the model\nlearn2.recorder.plot_losses()","5edcfaca":"# save the unfreezed model\nlearn2.save(\"stage-2-rn50-256\")","e111c4ab":"# export the model\nlearn2.export(file = \"..\/working\/export.pkl\")","2f9cb5ab":"#test = ImageList.from_folder(path\/\"test-jpg\").add(ImageList.from_folder(path\/\"test-jpg-additional\"))\ntest = ImageList.from_folder(path\/\"test-jpg-v2\")\nlen(test)","e108a224":"load_path = Path(\"..\/working\/\")\n\nlearn = load_learner(load_path, test=test)\npredicts, _ = learn.get_preds(ds_type = DatasetType.Test)","2f973197":"# pick the labels as long as the probability is more than 0.2\nlabels_pred = [\" \".join([learn.data.classes[i] for i,p in enumerate(pred) if p > 0.2]) for pred in predicts]\n\nlabels_pred[:5]","6ba28496":"for img in learn.data.test_ds.items[:10]:\n    print(img.name)","507210df":"# pick up the images' names\nimage_names = [img.name[:-4] for img in learn.data.test_ds.items] # img.name[:-4] because I want to remove '.jpg' from the name","01b809a7":"# create the dataframe of images' names and their tags (the format we have seen in train_v2.csv)\ndf2 = pd.DataFrame({\"image_name\":image_names, \"tags\":labels_pred})\ndf2.head()","6a6c1b01":"# create the csv file for submission\ndf2.to_csv(\"submission.csv\", index = False)","66a67546":"The loss-line drops steepest around 0.01 (1e-02), so this will be used as the learning rate.","f1c65c55":"Same as above, the loss-line drops steepest around 0.03 (3e-02) and I will use that as learning rate.","67247eda":"The model is now ready for inference, `learn.export` is called to save all the information of the Learner object for inference: the stuff we need in the DataBunch (transforms, classes, normalization...), the model with its weights and all the callbacks the Learner was using.","48807ea1":"0.0001 (1e-04) seems to be a good turning point before the loss-line shoots up. I will use 0.0001\/10 = 0.00001 as one of the learning rate.","3aec83f3":"## Predictions and Submission\n\nThere are 2 set of images for testing provided by Kaggle, in the folders `test-jpg.tar.7z` and `test-jpg-additional.tar.7z`. All of these test images are compiled in the folder `test-jpg-v2` in the path. ","89791360":"### Model Fine-tuning\n\nI will now fit the ResNet50 CNN with original image size, expect it to perform better.","75e6caa2":"I will create data with rundown resolution first (128x128) so that I can fit the model faster and see how that model performs with lower quality images, as a baseline benchmark. (default image size provided by Kaggle is 256x256)","6605607d":"# Multiple-label Classification with Planet Amazon Dataset\n\n`fastai` package will be used for this work.","771c64dd":"## DataBunch Preparation\n\n\"[data block API](https:\/\/docs.fast.ai\/data_block.html)\" procedure will be used to convert the raw images into model-readable DataBunch object.","619bfa87":"What an interesting Amazon's satellite images!","d6727469":"## Multiclassification\n\nPre-trained ResNet50 CNN will be used to fit the data. As for the metrics, `accuracy_thresh` will be used instead of `accuracy` because this is a multiclassification problem, the model should return multiple labels for each images as long as the probabilities of those labels are above a certain threshold. Apart from that, **F2-score** will also be used since it is the metric used by Kaggle in this competition. `fbeta` function from `fastai` will generates this metric.","e0de3ccd":"The \"tags\" column consists of the labels for each training images. Each tags consists of multi-labels which describe the weather (clear, cloudy, partly_cloudy, haze, etc.) and the geographical substances (primary forest, agriculture, road, water, habitation, etc.) that can be found in each of the Amazon's satellite images.","933d19c8":"This baseline model achieves an accuracy of 95.6% and F2-score of 0.924 which is pretty good."}}