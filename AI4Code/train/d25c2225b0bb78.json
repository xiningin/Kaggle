{"cell_type":{"94ea4013":"code","69da3ef4":"code","5ebcf5ca":"code","3a30b982":"code","951799bf":"code","7d18de7f":"code","e1dbc1d7":"code","3c7d62d4":"code","d8dc94c7":"code","462b1948":"code","aab1a517":"code","53561a01":"code","45a59b2c":"code","29dc932b":"code","3617607b":"code","d07343d1":"code","5be07e49":"markdown","58a6c9d3":"markdown","7af01b62":"markdown","78314d9b":"markdown"},"source":{"94ea4013":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69da3ef4":"'''\nReference:\n1. https:\/\/www.kaggle.com\/pawan2905\/commonlit-readability-prize\n2. https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\n3. https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system\n\n'''\nimport numpy as np\nimport pandas as pd\n\nimport os\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_squared_error as mse\n\n\ntrain = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')\n","5ebcf5ca":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\ndf2['overview'] = df2['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape\n#Out[12]:\n#(4803, 20978)","3a30b982":"#print(train)\n\nprint(train.head(20))","951799bf":"print('df.shape .......', train.shape)","7d18de7f":"# Removing unnecessary columns\n#train.drop(['url_legal', 'license'], axis=1, inplace=True)","e1dbc1d7":"\n# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n","3c7d62d4":"\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","d8dc94c7":"print(train['excerpt'])","462b1948":"train['excerpt_clean'] = train['excerpt'].apply(str).apply(lambda x: text_preprocessing(x))\nprint(train['excerpt_clean'])","aab1a517":"test['excerpt_clean'] =test['excerpt'].apply(str).apply(lambda x: text_preprocessing(x))\ntrain['excerpt_len'] = train['excerpt_clean'].astype(str).apply(len)\ntrain['excerpt_count'] = train['excerpt_clean'].apply(lambda x: len(str(x).split()))","53561a01":"print(train['excerpt_len'], train['excerpt_count'])","45a59b2c":"vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(1,6))\nX = vectorizer.fit_transform(train.excerpt_clean)\nX.shape\nprint('X.shape .......', X.shape)","29dc932b":"lr = LinearRegression()\n\nlr.fit(X, train.target)\n","3617607b":"sub = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv',index_col='id')\nprint(sub)\nx = vectorizer.transform(sub.excerpt)","d07343d1":"p = lr.predict(x)\nsub['target'] = p\nsub[['target']].to_csv('submission.csv')","5be07e49":"The two main processes involved are: a. Text cleaning b. Text preprocessing. Then, we are passing the article(excerpt column) to the TfIdfVectorizer that converts the words in the article into features. These features are then fed to a linear regressor to predict the target column which shows the level difficulty of the article.","58a6c9d3":"Here, the dataset contains articles(excerpt column) which are evaluated based on the level of difficulty of the articles and shown in the target column which we need to predict for the test.csv file as well as the submission.csv file ","7af01b62":"We see that over 20,000 different words were used to describe the 4800 movies in their dataset from the value of the tfidf_matrix.shape. Now,we continue with our code below.","78314d9b":"From reference 3, we will learn how to use the TfidfVectorizer to convert the words into the features as shown below:"}}