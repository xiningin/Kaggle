{"cell_type":{"0460ae52":"code","688f4fb8":"code","5633bced":"code","4c4a688a":"code","ceb1a46f":"code","4171a65c":"code","0bc8aa57":"code","f77e44dc":"code","8e91177d":"code","cc70dea1":"code","6e982010":"code","5812117a":"code","7fa5ec32":"code","b339f4ed":"code","4157e267":"markdown","42d71496":"markdown","186c674c":"markdown","99ccb71c":"markdown","b4fe0049":"markdown","748d74d2":"markdown","bd7790b3":"markdown","645d4b5c":"markdown","801c85da":"markdown","2243bc67":"markdown","0c36a84d":"markdown","35f80aad":"markdown","b4e7fc09":"markdown","18803996":"markdown","7c37de5d":"markdown"},"source":{"0460ae52":"import tensorflow as tf\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.manifold import TSNE\nimport os\n\n%matplotlib inline\nnp.random.seed(0)\ntf.random.set_random_seed(123456)","688f4fb8":"### Utility Functions\n## Plots\n# Plot Feature Projection [credit: https:\/\/www.kaggle.com\/shivamb\/semi-supervised-classification-using-autoencoders]\ndef tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2, random_state=0)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud')\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \n# Plot Keras training history\ndef plot_loss(hist):\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n## Util methods copied from OCAN package due to failure to install as custom package [credit:https:\/\/github.com\/PanpanZheng\/OCAN]\ndef xavier_init(size): # initialize the weight-matrix W.\n    in_dim = size[0]\n    xavier_stddev = 1. \/ tf.sqrt(in_dim \/ 2.)\n    return tf.random_normal(shape=size, stddev=xavier_stddev)\n\ndef sample_shuffle_uspv(X):\n    n_samples = len(X)\n    s = np.arange(n_samples)\n    np.random.shuffle(s)\n    return np.array(X[s])\n\ndef pull_away_loss(g):\n\n    Nor = tf.norm(g, axis=1)\n    Nor_mat = tf.tile(tf.expand_dims(Nor, axis=1),\n                      [1, tf.shape(g)[1]])\n    X = tf.divide(g, Nor_mat)\n    X_X = tf.square(tf.matmul(X, tf.transpose(X)))\n    mask = tf.subtract(tf.ones_like(X_X),\n                       tf.diag(\n                           tf.ones([tf.shape(X_X)[0]]))\n                       )\n    pt_loss = tf.divide(tf.reduce_sum(tf.multiply(X_X, mask)),\n                        tf.multiply(\n                            tf.cast(tf.shape(X_X)[0], tf.float32),\n                            tf.cast(tf.shape(X_X)[0]-1, tf.float32)))\n\n    return pt_loss\n\ndef one_hot(x, depth):\n    x_one_hot = np.zeros((len(x), depth), dtype=np.int32)\n    x = x.astype(int)\n    for i in range(x_one_hot.shape[0]):\n        x_one_hot[i, x[i]] = 1\n    return x_one_hot\n\ndef sample_Z(m, n):   # generating the input for G.\n    return np.random.uniform(-1., 1., size=[m, n])\n\ndef draw_trend(D_real_prob, D_fake_prob, D_val_prob, fm_loss, f1):\n\n    fig = plt.figure()\n    fig.patch.set_facecolor('w')\n    # plt.subplot(311)\n    p1, = plt.plot(D_real_prob, \"-g\")\n    p2, = plt.plot(D_fake_prob, \"--r\")\n    p3, = plt.plot(D_val_prob, \":c\")\n    plt.xlabel(\"# of epoch\")\n    plt.ylabel(\"probability\")\n    leg = plt.legend([p1, p2, p3], [r'$p(y|V_B)$', r'$p(y|\\~{V})$', r'$p(y|V_M)$'], loc=1, bbox_to_anchor=(1, 1), borderaxespad=0.)\n    leg.draw_frame(False)\n    # plt.legend(frameon=False)\n\n    fig = plt.figure()\n    fig.patch.set_facecolor('w')\n    # plt.subplot(312)\n    p4, = plt.plot(fm_loss, \"-b\")\n    plt.xlabel(\"# of epoch\")\n    plt.ylabel(\"feature matching loss\")\n    # plt.legend([p4], [\"d_real_prob\", \"d_fake_prob\", \"d_val_prob\"], loc=1, bbox_to_anchor=(1, 1), borderaxespad=0.)\n\n    fig = plt.figure()\n    fig.patch.set_facecolor('w')\n    # plt.subplot(313)\n    p5, = plt.plot(f1, \"-y\")\n    plt.xlabel(\"# of epoch\")\n    plt.ylabel(\"F1\")\n    # plt.legend([p1, p2, p3, p4, p5], [\"d_real_prob\", \"d_fake_prob\", \"d_val_prob\", \"fm_loss\",\"f1\"], loc=1, bbox_to_anchor=(1, 3.5), borderaxespad=0.)\n    plt.show()\n\n## OCAN TF Training Utils\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_logit = tf.nn.tanh(tf.matmul(G_h1, G_W2) + G_b2)\n    return G_logit\n\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n    D_prob = tf.nn.softmax(D_logit)\n    return D_prob, D_logit, D_h2\n\n\n# pre-train net for density estimation.\ndef discriminator_tar(x):\n    T_h1 = tf.nn.relu(tf.matmul(x, T_W1) + T_b1)\n    T_h2 = tf.nn.relu(tf.matmul(T_h1, T_W2) + T_b2)\n    T_logit = tf.matmul(T_h2, T_W3) + T_b3\n    T_prob = tf.nn.softmax(T_logit)\n    return T_prob, T_logit, T_h2","5633bced":"raw_data = pd.read_csv(\"..\/input\/creditcard.csv\")\ndata, data_test = train_test_split(raw_data, test_size=0.25)","4c4a688a":"raw_data_sample = data[data['Class'] == 0].sample(1000).append(data[data['Class'] == 1]).sample(frac=1).reset_index(drop=True)\nraw_data_x = raw_data_sample.drop(['Class'], axis = 1)\nraw_data_x[['Time']]=MinMaxScaler().fit_transform(raw_data_x[['Time']])\nraw_data_x[['Amount']]=MinMaxScaler().fit_transform(raw_data_x[['Amount']])\ntsne_plot(raw_data_x, raw_data_sample[\"Class\"].values, \"raw.png\")","ceb1a46f":"data.loc[:,\"Time\"] = data[\"Time\"].apply(lambda x : x \/ 3600 % 24)\ndata.loc[:,'Amount'] = np.log(data['Amount']+1)\n\ndata_test.loc[:,\"Time\"] = data_test[\"Time\"].apply(lambda x : x \/ 3600 % 24)\ndata_test.loc[:,'Amount'] = np.log(data_test['Amount']+1)\n# data = data.drop(['Amount'], axis = 1)\nprint(data.shape)\ndata.head()","4171a65c":"non_fraud = data[data['Class'] == 0].sample(1000)\nfraud = data[data['Class'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'], axis = 1).values\nY = df[\"Class\"].values\n\ntsne_plot(X, Y, \"original.png\")","0bc8aa57":"## input layer \ninput_layer = Input(shape=(X.shape[1],))\n\n## encoding part\nencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50, activation='sigmoid')(encoded) \n\n## decoding part\ndecoded = Dense(50, activation='tanh')(encoded)\n\n## output layer\noutput_layer = Dense(X.shape[1], activation='relu')(decoded) \n\n# Autoencoder model\nautoencoder = Model(input_layer, output_layer)\n\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n\n# Min-max scaling \nx = data.drop([\"Class\"], axis=1)\ny = data[\"Class\"].values\n\n# x_scale = MinMaxScaler(feature_range=(-1, 1)).fit_transform(x)\nx_norm, x_fraud = x.values[y == 0], x.values[y == 1]","f77e44dc":"checkpointer = ModelCheckpoint(filepath='bestmodel.hdf5', verbose=0, save_best_only=True)\nearlystopper = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\nx_norm_train_sample = x_norm[np.random.randint(x_norm.shape[0], size=10000),:]\nhist = autoencoder.fit(x_norm_train_sample, x_norm_train_sample, \n                batch_size = 256, epochs = 400, \n                shuffle = True, validation_split = 0.05, verbose=0, callbacks=[checkpointer, earlystopper])\nplot_loss(hist)","8e91177d":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])\n\nnorm_hid_rep = hidden_representation.predict(x_norm[np.random.randint(x_norm.shape[0], size=700),:])\nfraud_hid_rep = hidden_representation.predict(x_fraud)\n\n# norm_hid_rep = MinMaxScaler(feature_range=(-1, 1)).fit_transform(norm_hid_rep)\n# fraud_hid_rep = MinMaxScaler(feature_range=(-1, 1)).fit_transform(fraud_hid_rep)","cc70dea1":"rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)\ntsne_plot(rep_x, rep_y, \"latent_representation.png\")","6e982010":"from IPython.display import display, Image, HTML\ndisplay(HTML(\"\"\"<table align=\"center\">\n<tr ><td><b>Actual Representation (Before) <\/b><\/td><td><b>Latent Representation (Actual)<\/b><\/td><\/tr>\n<tr><td><img src='original.png'><\/td><td>\n             <img src='latent_representation.png'><\/td><\/tr><\/table>\"\"\"))","5812117a":"dim_input = norm_hid_rep.shape[1]\nmb_size = 70\n\nD_dim = [dim_input, 100, 50, 2]\nG_dim = [50, 100, dim_input]\nZ_dim = G_dim[0]\n\nX_oc = tf.placeholder(tf.float32, shape=[None, dim_input])\nZ = tf.placeholder(tf.float32, shape=[None, Z_dim])\nX_tar = tf.placeholder(tf.float32, shape=[None, dim_input])\n\n# define placeholders for labeled-data, unlabeled-data, noise-data and target-data.\n\nX_oc = tf.placeholder(tf.float32, shape=[None, dim_input])\nZ = tf.placeholder(tf.float32, shape=[None, Z_dim])\nX_tar = tf.placeholder(tf.float32, shape=[None, dim_input])\n# X_val = tf.placeholder(tf.float32, shape=[None, dim_input])\n\n\n# declare weights and biases of discriminator.\n\nD_W1 = tf.Variable(xavier_init([D_dim[0], D_dim[1]]))\nD_b1 = tf.Variable(tf.zeros(shape=[D_dim[1]]))\n\nD_W2 = tf.Variable(xavier_init([D_dim[1], D_dim[2]]))\nD_b2 = tf.Variable(tf.zeros(shape=[D_dim[2]]))\n\nD_W3 = tf.Variable(xavier_init([D_dim[2], D_dim[3]]))\nD_b3 = tf.Variable(tf.zeros(shape=[D_dim[3]]))\n\ntheta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n\n\n\n# declare weights and biases of generator.\n\nG_W1 = tf.Variable(xavier_init([G_dim[0], G_dim[1]]))\nG_b1 = tf.Variable(tf.zeros(shape=[G_dim[1]]))\n\nG_W2 = tf.Variable(xavier_init([G_dim[1], G_dim[2]]))\nG_b2 = tf.Variable(tf.zeros(shape=[G_dim[2]]))\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n\n# declare weights and biases of pre-train net for density estimation.\n\nT_W1 = tf.Variable(xavier_init([D_dim[0], D_dim[1]]))\nT_b1 = tf.Variable(tf.zeros(shape=[D_dim[1]]))\n\nT_W2 = tf.Variable(xavier_init([D_dim[1], D_dim[2]]))\nT_b2 = tf.Variable(tf.zeros(shape=[D_dim[2]]))\n\nT_W3 = tf.Variable(xavier_init([D_dim[2], D_dim[3]]))\nT_b3 = tf.Variable(tf.zeros(shape=[D_dim[3]]))\n\ntheta_T = [T_W1, T_W2, T_W3, T_b1, T_b2, T_b3]\n\nD_prob_real, D_logit_real, D_h2_real = discriminator(X_oc)\n\nG_sample = generator(Z)\nD_prob_gen, D_logit_gen, D_h2_gen = discriminator(G_sample)\n\nD_prob_tar, D_logit_tar, D_h2_tar = discriminator_tar(X_tar)\nD_prob_tar_gen, D_logit_tar_gen, D_h2_tar_gen = discriminator_tar(G_sample)\n# D_prob_val, _, D_h1_val = discriminator(X_val)\n\n# disc. loss\ny_real= tf.placeholder(tf.int32, shape=[None, D_dim[3]])\ny_gen = tf.placeholder(tf.int32, shape=[None, D_dim[3]])\n\nD_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_real,labels=y_real))\nD_loss_gen = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_gen, labels=y_gen))\n\nent_real_loss = -tf.reduce_mean(\n                        tf.reduce_sum(\n                            tf.multiply(D_prob_real, tf.log(D_prob_real)), 1\n                        )\n                    )\n\nent_gen_loss = -tf.reduce_mean(\n                        tf.reduce_sum(\n                            tf.multiply(D_prob_gen, tf.log(D_prob_gen)), 1\n                        )\n                    )\n\nD_loss = D_loss_real + D_loss_gen + 1.85 * ent_real_loss\n\n\n# gene. loss\npt_loss = pull_away_loss(D_h2_tar_gen)\n\ny_tar= tf.placeholder(tf.int32, shape=[None, D_dim[3]])\nT_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_tar, labels=y_tar))\ntar_thrld = tf.divide(tf.reduce_max(D_prob_tar_gen[:,-1]) +\n                      tf.reduce_min(D_prob_tar_gen[:,-1]), 2)\n\nindicator = tf.sign(\n              tf.subtract(D_prob_tar_gen[:,-1],\n                          tar_thrld))\ncondition = tf.greater(tf.zeros_like(indicator), indicator)\nmask_tar = tf.where(condition, tf.zeros_like(indicator), indicator)\nG_ent_loss = tf.reduce_mean(tf.multiply(tf.log(D_prob_tar_gen[:,-1]), mask_tar))\n\nfm_loss = tf.reduce_mean(\n            tf.sqrt(\n                tf.reduce_sum(\n                    tf.square(D_logit_real - D_logit_gen), 1\n                    )\n                )\n            )\n\nG_loss = pt_loss + G_ent_loss + fm_loss\n\nD_solver = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(D_loss, var_list=theta_D)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\nT_solver = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(T_loss, var_list=theta_T)\n\n\n# Load data\n# min_max_scaler = MinMaxScaler()\n\nx_benign = norm_hid_rep # min_max_scaler.fit_transform(norm_hid_rep)\nx_vandal = fraud_hid_rep # min_max_scaler.transform(fraud_hid_rep)\n\nx_benign = sample_shuffle_uspv(x_benign)\nx_vandal = sample_shuffle_uspv(x_vandal)\n\nx_pre = x_benign\n\ny_pre = np.zeros(len(x_pre))\ny_pre = one_hot(y_pre, 2)\n\nx_train = x_pre\n\ny_real_mb = one_hot(np.zeros(mb_size), 2)\ny_fake_mb = one_hot(np.ones(mb_size), 2)\n\nx_test = x_benign.tolist() + x_vandal.tolist()\nx_test = np.array(x_test)\n\n\ny_test = np.zeros(len(x_test))\n\ny_test[len(x_benign):] = 1\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# pre-training for target distribution\n_ = sess.run(T_solver,\n             feed_dict={\n                X_tar:x_pre,\n                y_tar:y_pre\n                })\n\nq = np.divide(len(x_train), mb_size)\n\nd_ben_pro, d_fake_pro, fm_loss_coll = list(), list(), list()\nf1_score  = list()\nd_val_pro = list()\n\n\nn_round = 200\n\nfor n_epoch in range(n_round):\n\n    X_mb_oc = sample_shuffle_uspv(x_train)\n\n    for n_batch in range(int(q)):\n\n        _, D_loss_curr, ent_real_curr = sess.run([D_solver, D_loss, ent_real_loss],\n                                          feed_dict={\n                                                     X_oc: X_mb_oc[n_batch*mb_size:(n_batch+1)*mb_size],\n                                                     Z: sample_Z(mb_size, Z_dim),\n                                                     y_real: y_real_mb,\n                                                     y_gen: y_fake_mb\n                                                     })\n\n        _, G_loss_curr, fm_loss_curr = sess.run([G_solver, G_loss, fm_loss],\n                                           feed_dict={Z: sample_Z(mb_size, Z_dim),\n                                                      X_oc: X_mb_oc[n_batch*mb_size:(n_batch+1)*mb_size],\n                                                      })\n\n    D_prob_real_, D_prob_gen_ = sess.run([D_prob_real, D_prob_gen],\n                                         feed_dict={X_oc: x_train,\n                                                    Z: sample_Z(len(x_train), Z_dim)})\n\n\n    D_prob_vandal_ = sess.run(D_prob_real,\n                              feed_dict={X_oc:x_vandal})\n\n    d_ben_pro.append(np.mean(D_prob_real_[:, 0]))\n    d_fake_pro.append(np.mean(D_prob_gen_[:, 0]))\n    d_val_pro.append(np.mean(D_prob_vandal_[:, 0]))\n    fm_loss_coll.append(fm_loss_curr)\n\n    prob, _ = sess.run([D_prob_real, D_logit_real], feed_dict={X_oc: x_test})\n    y_pred = np.argmax(prob, axis=1)\n    y_pred_prob = prob[:,1]\n    conf_mat = classification_report(y_test, y_pred, target_names=['genuine', 'fraud'], digits=4)\n    f1_score.append(float(list(filter(None, conf_mat.strip().split(\" \")))[12]))\n    # print conf_mat\n\n\ndraw_trend(d_ben_pro, d_fake_pro, d_val_pro, fm_loss_coll, f1_score)","7fa5ec32":"print (\"OCAN: \")\nprint(conf_mat)\nprint (\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n\ntrain_x, val_x, train_y, val_y = train_test_split(x_test, y_test, test_size=0.4)\n\nclf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\npred_y = clf.predict(val_x)\npred_y_prob = clf.predict_proba(val_x)[:,1]\n\nprint (\"\")\nprint (\"Linear Classifier: \")\nprint (classification_report(val_y, pred_y, target_names=['genuine', 'fraud'], digits=4))\nprint (\"Accuracy Score: \", accuracy_score(val_y, pred_y))\n\nfpr, tpr, thresh = roc_curve(val_y, pred_y_prob)\nauc = roc_auc_score(val_y, pred_y_prob)\nfpr2, tpr2, thresh2 = roc_curve(y_test, y_pred_prob)\nauc2 = roc_auc_score(y_test, y_pred_prob)\nplt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\nplt.plot(fpr2,tpr2,label=\"OCAN in-sample, auc=\"+str(auc2))\nplt.legend(loc='best')\nplt.show()","b339f4ed":"test_hid_rep = hidden_representation.predict(data_test.drop(['Class'], axis = 1).values)\ntest_y = data_test[\"Class\"].values\n\nprob_test, _ = sess.run([D_prob_real, D_logit_real], feed_dict={X_oc: test_hid_rep})\ny_pred_test = np.argmax(prob_test, axis=1)\ny_pred_prob_test = prob_test[:,1]\n\nconf_mat_test = classification_report(test_y, y_pred_test, target_names=['genuine', 'fraud'], digits=4)\nprint (\"OCAN: \")\nprint(conf_mat_test)\nprint (\"Accuracy Score: \", accuracy_score(test_y, y_pred_test))\n\npred_y_test = clf.predict(test_hid_rep)\npred_y_prob_test = clf.predict_proba(test_hid_rep)[:,1]\n\nprint (\"\")\nprint (\"Linear Classifier: \")\nprint (classification_report(test_y, pred_y_test, target_names=['genuine', 'fraud'], digits=4))\nprint (\"Accuracy Score: \", accuracy_score(test_y, pred_y_test))\n\nfpr, tpr, thresh = roc_curve(test_y, pred_y_prob_test)\nauc = roc_auc_score(test_y, pred_y_prob_test)\nfpr2, tpr2, thresh2 = roc_curve(test_y, y_pred_prob_test)\nauc2 = roc_auc_score(test_y, y_pred_prob_test)\nplt.plot(fpr,tpr,label=\"linear out-of-sample, auc=\"+str(auc))\nplt.plot(fpr2,tpr2,label=\"OCAN out-of-sample, auc=\"+str(auc2))\nplt.legend(loc='best')\nplt.show()","4157e267":"## 5. Training of One-Class Adversarial Nets Classifier\nThe One-Class Adversarial Nets (OCAN) proposed by [One-Class Adversarial Nets for Fraud Detection (Zheng et al., 2018)](https:\/\/arxiv.org\/abs\/1803.01798) is a state-of-the-art one-class classification model that can be trained using samples of only one class through a generative training process, hence a compelling candidate for fraud detection problems where positive labeled samples are rare and diverse in features.\n\nThe following are tensorflow training code (hidden for brevity, click to expand) adopted from the original paper's repository. The model is trained for 200 epochs.","42d71496":"You can see from above that using naive preprocessing, fraud transactions (in red) are mixed with genuine transactions (in green) with no clear distinctions.\n\nNow as an alternative, we transform the Time field to time-of-day to account for intraday seasonality, with the intuition that transactions that happen at certain time-of-day such as late night could be fraudulent. The Amount field is transformed to log scale for normalization, with the intuition that the scale of magnitute of a transaction could be a more relevant feature for fraud than linear amounts.","186c674c":"First let us visualize the raw training data before preprocessing using the T-SNE (t-Distributed Stochastic Neighbor Embedding) plot. T-SNE visualizes the dissimilarities among data points in the feature space. The Time and Amount field are scaled to [0,1]. We will show how other ways of preprocessing can help.","99ccb71c":"## 3. Extraction Latent Representation with Autoencoders  \nWe train an autoencoder with 10k in-sample non-fraud transactions to extract latent representations (of 50 dimensions, in order to be consistent with the original paper).\n\nNotice that the encoder and decoder layers are not symetric, and that the hidden layer sizes are non-increasing. This is because the goal here is not to learn the identity function nor sparse coding, but rather the abstract representations of the transactions. As we shall see the learnt latent representations did a good job at clustering. ","b4fe0049":"With the [Credit Card Fraud Dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) by ULB machine learning group, I attempt to validate the results reported by the paper [One-Class Adversarial Nets for Fraud Detection (Zheng et al., 2018)](https:\/\/arxiv.org\/abs\/1803.01798)<sup>1<\/sup> and benchmark against a simple linear logistic classifier. I adopted a similar procedure for learning latent transaction representation using Autoencoders as documented in the paper which reported better results than using the raw (PCA-transformed) features. I reserved test samples at the beginning of the code, and these test samples were not used for model tuning until the final evaluation of the classifiers before this kernel was committed. \n\nThe following report (casually) shows that given well-formed feature representations, One-Class Adversarial Nets (OCAN) fails to outperform a baseline linear classifier used in the kernel [Semi Supervised Classification using AutoEncoders kernel by Shivam Bansa](https:\/\/www.kaggle.com\/shivamb\/semi-supervised-classification-using-autoencoders). In addition, the superior performance by the simple linear classifier could be due to the inclusion of intraday seasonality and log-normalization of amounts, which together with latent representation learning by autoencoders resulted in well-formed feature space.\n\nThis is my first kernel on Kaggle, I humbly welcome your feedbacks and criticism. Thank you.\n\n<sub><sup>1<\/sup> Code for the original paper can be found at [https:\/\/github.com\/PanpanZheng\/OCAN](https:\/\/github.com\/PanpanZheng\/OCAN)<\/sub>  \n<sub><sup>2<\/sup> Plotting code adopted from [Semi Supervised Classification using AutoEncoders kernel by Shivam Bansa](https:\/\/www.kaggle.com\/shivamb\/semi-supervised-classification-using-autoencoders)<\/sub>\n\n### Contents \n\n1. Raw Dataset\n2. Visualize Preprocessed Transaction Features\n3. Extraction Latent Representation with Autoencoders  \n4. Visualize Latent Representations \n5. Training of One-Class Adversarial Nets Classifier\n6. OCAN vs. Simple Linear Classifier\n7. Evaluation of Classifiers on Test Set\n8. Limitations ","748d74d2":"Instead of training for a fixed number of epochs as did in the paper and Shivam's kernel, this autoencoder is trained with early-stopping. The training stops when the validation losses fail to decrease for 20 consecutive epochs.","bd7790b3":"## 4. Visualize Latent Representations \nT-SNE plot with latent representations for the (700 non-fraud + fraud) samples. Notice how the clustering becomes even more pronounced using the latent representations of the transactions.","645d4b5c":"## 2. Visualize Preprocessed Transaction Features\nSample another 1000 non-fraud transactions from the training set and plot with all fraudulent transactions in the training set. The T-SNE plot shows that after Time and Amount preprocessing, fraud transactions (in red) are adequately seperated from non-fraud transactions (in green), dispite the clusters being close to each other.","801c85da":"## 7. Evaluation of Classifiers on Test Set\nEvaluate the two classifier on the previously reserved test set, the conclusion holds if we look at the ROC curve: OCAN did not outperform linear classifier significantly.","2243bc67":"## 8. Limitations \n* This is a rather casual attempt. The motivation for trying out OCAN was pure curiosity.\n* OCAN's in-sample F1-score was slightly better than the one reported in the original paper. The reason for poor out-of-sample performance could be it's complexity. When the feature representations are well clustered, it might reduce the benefit brought by an advanced classifier where a simple linear one can do an excellent job. In addition, the linear classifier was able to take advantage of two-class labeled training set and the samples provided happen to be enough for it to perform well.\n* Early stopping can be implemented for OCAN\n* Numerical metrics can be used to measure exactly the improvement of segregation\/clustering brought by using the latent representations vs. raw (preprocessed) features.\n* The autoencoder latent representation learning still has room for improvement.\n* I did not have enough time to dwell further into hyperparameter tuning. The hyperparams in this kernel were mostly trial-and-error based on the training set. \n* There might be bugs in this kernel. Please also excuse the variable namings and the tensorflow code, I will make them more consistent and readable when I have more time.","0c36a84d":"## 1. Raw Dataset\n\nRead the data, split the data into training set (for learning latent representation and training the classifier) and test set. The \"data_test\" set was never used until the final evaluation of classifiers before the initial commit of this report.","35f80aad":"<h1 align=\"center\">Fraud Detection with One-Class Adversarial Nets on Latent Representation<\/h1>","b4e7fc09":"Extract latent representation by running transactions through the trained encoding layers of the autoencoder. We do so for another sample of 700 non-fraud transactions plus all fraud transactions from the training set. 700 is chosen to be consistent with the original paper.","18803996":"## 6. OCAN vs. Simple Linear Classifier\nThis section apply the trained OCAN classifier and a simple linear logistic classifier on the random samples drew from the training set previously. The linear classifier is trained on a different split of supervised train-val set drew from the overall training set, due to the fact that OCAN was trained with one-class data only.\n\nThe results below show that for fraud case OCAN did not outperform the linear classifier significantly in-sample.","7c37de5d":"Following Shivam's kernel, we show a comparison of the two T-SNE plots."}}