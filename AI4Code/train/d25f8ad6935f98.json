{"cell_type":{"85507759":"code","d211e52c":"code","5f7f21eb":"code","1b0920ac":"code","45d0e92a":"code","d44be04b":"code","0f351583":"code","c09781e1":"code","b3a2ba1d":"code","2194763d":"code","b8e1eaba":"code","8c0e033a":"code","049dc0e9":"code","cc52a482":"code","5d0a61e1":"code","06597be0":"code","df657da0":"code","e6fd3d04":"code","224f544c":"code","15bf0df4":"code","fcbdb40d":"code","cd00c554":"code","f3a6356f":"code","d84f8250":"code","71b3f3c3":"code","12a1e40e":"code","61b2f067":"code","2f37419d":"code","ff8681f0":"code","aeb125e8":"code","c34a5e33":"code","60cd01b2":"code","32b9c6ac":"code","dbc278ea":"code","df4cb9c4":"code","e41ddb21":"markdown","3fdc080f":"markdown","61f037db":"markdown","09a3d9e3":"markdown","81296b8a":"markdown","3ca4809b":"markdown","93efcae3":"markdown","76491b5f":"markdown","cf5f81eb":"markdown","ccc989dc":"markdown","a59d38dd":"markdown","54ec26de":"markdown","21b13053":"markdown","ba1d7381":"markdown","2ce82779":"markdown","d716e324":"markdown","a997153b":"markdown","4fc6e31c":"markdown","ec231fec":"markdown","e8d23ab6":"markdown","e42b9a01":"markdown","d54c64b8":"markdown","896b9608":"markdown","f23bb2d1":"markdown","4fc8103a":"markdown","0083fdea":"markdown","d2ad974b":"markdown","ced8ed30":"markdown","946a4703":"markdown","e8d2e743":"markdown"},"source":{"85507759":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (Rescaling, RandomFlip, RandomRotation, RandomZoom,\n                                    Dense, Flatten, Dropout, Conv2D, MaxPooling2D)\nfrom tensorflow.keras.utils import image_dataset_from_directory, plot_model\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.applications import VGG16","d211e52c":"# Define some basic parameters\n\nbatch_size = 32\nimg_height = 150\nimg_width = 150","5f7f21eb":"# Define the path to the datasets directories\n\ntrain_path = '\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/'\ntest_path = '\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/'\npred_path = '\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred\/'","1b0920ac":"# Define data loading function\n\ndef load_data(path, labels):\n    dataset = image_dataset_from_directory(\n        directory=path,\n        labels=labels,\n        seed=123,\n        image_size=(img_height, img_width),\n        batch_size=batch_size\n    )\n    return dataset","45d0e92a":"# Load the datasets\n\ntrain_ds = load_data(train_path, labels='inferred')\ntest_ds = load_data(test_path, labels='inferred')\npred_ds = load_data(pred_path, labels=None)","d44be04b":"# Explore the image labels\n\nclass_names = train_ds.class_names\nclass_names","0f351583":"# Show the first nine images from the training dataset\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        plt.title(class_names[labels[i]])\n        plt.axis('off')","c09781e1":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\npred_ds = pred_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)","b3a2ba1d":"scaling = Rescaling(1. \/ 255)\n\ntrain_ds = train_ds.map(lambda x, y: (scaling(x), y))\ntest_ds = test_ds.map(lambda x, y: (scaling(x), y))\npred_ds = pred_ds.map(lambda x: scaling(x))","2194763d":"sCNN = Sequential([\n    Conv2D(16, 3, padding='same', activation='relu', input_shape=(img_height, img_width, 3)),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(6, activation='softmax')\n])","b8e1eaba":"sCNN.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","8c0e033a":"sCNN.summary()","049dc0e9":"epochs = 10\n\nhistory = sCNN.fit(train_ds, validation_data=test_ds, epochs=epochs)","cc52a482":"def visualize_results(history, epochs):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(epochs)\n\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower center')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper center')\n    plt.title('Training and Validation Loss')\n    plt.show();","5d0a61e1":"visualize_results(history, epochs)","06597be0":"sCNN_loss, sCNN_acc = sCNN.evaluate(test_ds, verbose=2)","df657da0":"# Implement data augmentation using the RandomFlip, RandomRotation and RandomZoom\n\ndata_augmentation = Sequential([\n    RandomFlip('horizontal', input_shape=(img_height, img_width, 3)),\n    RandomRotation(0.1),\n    RandomZoom(0.1),\n])","e6fd3d04":"# Show a few augmented examples applied to the same image several times\n\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0])\n        plt.axis('off')","224f544c":"aCNN = Sequential([\n    data_augmentation,\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(6, activation='softmax')\n])","15bf0df4":"# Compile the model\n\naCNN.compile(optimizer='adam',\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])","fcbdb40d":"# View all levels of the model\n\naCNN.summary()","cd00c554":"# Train the model using 15 epochs and test_ds as the validation data\n\nepochs = 15\n\nhistory = aCNN.fit(train_ds, validation_data=test_ds, epochs=epochs)","f3a6356f":"visualize_results(history, epochs)","d84f8250":"aCNN_loss, aCNN_acc = aCNN.evaluate(test_ds, verbose=2)","71b3f3c3":"# Load pre-trained VGG16 model and define its layers as untrainable\n\npretrained_model=VGG16(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n\nfor layer in pretrained_model.layers:\n    layer.trainable = False","12a1e40e":"# Explore the last layer of pre-trained model\n\npretrained_model.summary()","61b2f067":"# Create the complex model\n\nlast_layer = pretrained_model.get_layer('block5_pool').output\n\nx = Flatten()(last_layer)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(6, activation='softmax')(x)\n\nvgg16 = Model(pretrained_model.input, x)","2f37419d":"# Compile the model\n\nvgg16.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])","ff8681f0":"# View all levels of the model\n\nvgg16.summary()","aeb125e8":"# Train the model using 10 epochs and test_ds as the validation data\n\nepochs = 10\n\nhistory = vgg16.fit(train_ds, validation_data=test_ds, epochs=epochs)","c34a5e33":"visualize_results(history, epochs)","60cd01b2":"vgg16_loss, vgg16_acc = vgg16.evaluate(test_ds, verbose=2)","32b9c6ac":"# Create a dataframe to compare the model results\n\ndata = {\n    'Loss': [sCNN_loss, aCNN_loss, vgg16_loss],\n    'Accuracy': [sCNN_acc, aCNN_acc, vgg16_acc]\n}\n\nmodels = ['sCNN', 'aCNN', 'VGG16']\n\nevaluation = pd.DataFrame(data, index=models)\nevaluation","dbc278ea":"# Perform predictions on the new unlabeled data using VGG16 model\n\npredictions = vgg16.predict(pred_ds)","df4cb9c4":"# Show a sample of predicted images\n\nplt.figure(figsize=(10, 10))\nfor images in pred_ds.take(1):\n    prediction = vgg16.predict(images)\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        score = tf.nn.softmax(prediction[i])\n        plt.imshow(array_to_img(images[i]))\n        plt.title(class_names[np.argmax(score)])\n        plt.axis('off')","e41ddb21":"## Conclusion\n\nThe plot above shows that using the pre-trained complex model significantly improves the result - the accuracy increased by 10%, and the loss decreased almost 4 times, compared to the baseline model.\n\nUnfortunately, there is also a sign of overfitting this time. This indicates that the model needs fine-tuning of the parameters to deal with overfitting and obtain higher accuracy.","3fdc080f":"## Model Comparison","61f037db":"## Compile and Train the Tuned Model","09a3d9e3":"## Visualize Training Results","81296b8a":"## Create Simple CNN Model (sCNN)\n\nThe Sequential model consists of three convolution blocks with a max pooling layer in each of them.\n\nThere's a fully-connected layer with 128 units on top of it that is activated by a ReLU activation function ('relu').\n\nThis model is a simple model and has not been tuned for high accuracy.","3ca4809b":"## Visualize the Data","93efcae3":"## Configure the Datasets for Performance\n\nTwo important methods should be used when loading data:\n\n- **Dataset.cache** keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training the model. If the dataset is too large to fit into memory, this method also can be used to create a performant on-disk cache.\n- **Dataset.prefetch** overlaps data preprocessing and model execution while training.","76491b5f":"## Create Augmented CNN Model (aCNN)\n\nCreate a new model using the **Data Augmentation** approach and **Dropout** layer based on the previous baseline model.","cf5f81eb":"## Evaluate the Model","ccc989dc":"## Next Steps\n\nOne way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n\nIn most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n\nAnother way is to use other models to train the data, such as InceptionV3 and ResNet50, before fine-tuning the VGG16 model. This will allow to select the best model for further fine-tuning and more accurate prediction on the new data.","a59d38dd":"## Conclusion\n\nIn the plots above, the training accuracy is increasing linearly over time, whereas validation accuracy stalls around 80% in the training process. Also, the difference in accuracy between training and validation accuracy is noticeable \u2014 a sign of overfitting.\n\nWhen there are a small number of training examples, the model sometimes learns from noises or unwanted details from training examples \u2014 to an extent that it negatively impacts the performance of the model on new examples. This phenomenon is known as overfitting. It means that the model will have a difficult time generalizing on a new dataset.\n\nTo deal with overfitting in the training process, the base model will be tuned with data augmentation, and Dropout layer will also be added to the model.","54ec26de":"## Visualize Training Results\n\nCreate plots of loss and accuracy on the training and validation sets.","21b13053":"## Load and Explore the Datasets","ba1d7381":"## Compile and Train the Complex Model","2ce82779":"## Key Findings and Insights\n\nAnalyzing the images from the training dataset, it can be noticed that many of them are labeled incorrectly.\n\nThe classes that are mostly difficult to classify are **streets** and **buildings**, and **mountains** and **glaciers**.\n\nA basic model with no tuning or optimization is able to provide good accuracy rate (~80%).\n\nImplementing the data augmentation approach and adding the Dropout layer provide slightly better results than the basic model, and also deal with overfitting.\n\nOn the other side, the tuned model with the data augmentation approach and Dropout layer provides the same results as the baseline model trained in 3 epochs.\n\nThe complex pre-trained VGG16 model provides the best results, but it also needs additional tuning.","d716e324":"## Prediction\n\nPrediction will be performed on new unlabeled data using VGG16 model.","a997153b":"## Model summary\n\nView all levels of the neural network.","4fc6e31c":"## Standardize the Data\n\nThe RGB channel values are in the **[0, 255]** range. This is not ideal for a neural network.\n\nThe values should be standardized to be in the **[0, 1]** range.","ec231fec":"## Conclusion\n\nThe plot above shows that implementing the data augmentation approach and adding the Dropout layer to the model increased accuracy by almost 5% and reduced the loss by more than half.\n\nHowever, the accuracy is still below 85%. This result might be due to the fact that a very simple model was used in this workflow.\n\nAnother, more complex model based on the pre-trained VGG16 model will be created to improve accuracy.","e8d23ab6":"## Data Summary\n\nThe data contains around 25k images of Natural Scenes around the world.\n\nAll training and testing images with a size of 150x150 are classified into 6 categories:\n- buildings = 0\n- forest = 1\n- glacier = 2\n- mountains = 3\n- sea = 4\n- street = 5\n\nThe data consists of 3 separated datasets:\n- Train with 14034 images\n- Test  with 3000 images\n- Prediction with 7301 images\n\nThis data was originally published on https:\/\/datahack.analyticsvidhya.com by Intel for the Image Classification Competition.","e42b9a01":"## Import TensorFlow and Other Libraries","d54c64b8":"# Intel Image Classification","896b9608":"## Evaluate the Model","f23bb2d1":"## Compile the Model\n\nThe Compile method configures the model for training and validation using the optimizer, loss function, and evaluation metrics.\n\nThis workflow will use the **Adam** optimizer, the **Sparse Categorical Crossentropy** loss function, and the **Accuracy** evaluation metric.","4fc8103a":"## Evaluate the Model\n\nExplore the loss and accuracy of validation data after training with 10 epochs.","0083fdea":"## Project Workflow:\n    1. Examine and understand data\n    2. Build an input pipeline\n    3. Build the model\n    4. Train the model\n    5. Test the model\n    6. Improve the model and repeat the process","d2ad974b":"## Create a Complex Model based on Pre-trained VGG16 Model (VGG16)","ced8ed30":"## Train the Model\n\nThis workflow will train the model using 10 epochs, and **test_ds** as the validation data.","946a4703":"## Visualize Training Results","e8d2e743":"## Data Augmentation\n\nData augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better."}}