{"cell_type":{"0c30bc7e":"code","36f43b4b":"code","cd55993d":"code","b3189608":"code","9e232914":"code","3924a218":"code","b7a739d8":"code","65f07406":"code","b62a120c":"code","f2da3e3f":"code","bcb4f55f":"code","066ef7d3":"code","23c88d90":"code","ad28f42e":"code","16ebbb62":"code","0c3e207d":"code","85c0931a":"code","a593b2c1":"code","24901e5e":"code","aa4290d9":"code","13f77a4d":"code","768b3348":"code","eb585a25":"code","d91da2d0":"code","b933c2bf":"code","9eefc279":"code","5222a9ac":"code","01edc286":"code","cf7b9ac3":"code","75212166":"code","ad8cdcdd":"code","20816a0e":"code","a5bd059e":"code","273ad700":"code","f53db1bc":"code","349f733c":"code","7aec5a10":"code","4386f0a2":"code","d31fedba":"code","28596740":"code","77d39db7":"code","9be534f3":"code","5e7b2dfc":"code","fd887d93":"code","0c0bba9a":"code","8c95ced6":"code","2860c15d":"code","79b32030":"code","ed5ef59b":"code","3b9be7a6":"code","a634a956":"code","ce0c35cf":"code","1882ecf3":"code","e2f3c1b8":"code","c4fd4cc6":"code","562ce700":"code","450c5c8b":"code","6fcbf633":"markdown","bad53f7e":"markdown","36d19cb5":"markdown","2a57b370":"markdown","38cb4c29":"markdown","3a94d5b2":"markdown","3cd5bb03":"markdown","cf3f4f13":"markdown","5cb62a51":"markdown","9299c18b":"markdown","448e0e97":"markdown","9dd07dd5":"markdown","8e3511f8":"markdown","146471cf":"markdown","880f612e":"markdown","fabec793":"markdown","8d3c2a94":"markdown","29f6337e":"markdown","7143594f":"markdown","03eb59ec":"markdown","d980a116":"markdown","3ebcbb2a":"markdown","acd91a83":"markdown","e1ce3520":"markdown","34762000":"markdown","823621d1":"markdown","5805e325":"markdown","0a05a3e2":"markdown","63110a84":"markdown","79a82b8e":"markdown","91f55396":"markdown","604a3a14":"markdown","f24b2d07":"markdown","cb65be7d":"markdown","d91852fc":"markdown","41e4d1fc":"markdown","bbc0d409":"markdown","9c708ecd":"markdown","cb2c056c":"markdown"},"source":{"0c30bc7e":"!pip install tubesml==0.5.2","36f43b4b":"import pandas as pd\nimport numpy as np\n\nimport tubesml as tml\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npd.set_option('max_columns', 50)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")","cd55993d":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\ndf.head()","b3189608":"df.info()","9e232914":"train, test = tml.make_test(df, 0.2, random_state=43, strat_feat=['Sex', 'Pclass'])\nprint(round(len(train) \/ len(df) * 100, 2))\nprint(round(len(test) \/ len(df) * 100, 2))\ntrain.head()","3924a218":"tml.list_missing(train)\nprint('_'*40)\ntml.list_missing(test)","b7a739d8":"train.groupby('Pclass').Survived.mean()","65f07406":"train.groupby('Sex').Survived.mean()","b62a120c":"train.groupby(['Pclass', 'Sex']).Survived.mean().unstack()","f2da3e3f":"def check_missing(data, feature):\n    print(data[data[feature].isna()].Survived.mean())\n    print(data[data[feature].notna()].Survived.mean())\n\nfor cat in ['Age', 'Cabin', 'Embarked']:\n    print(cat)\n    check_missing(train, cat)\n    print('_'*40)","bcb4f55f":"print(round(accuracy_score(y_pred=[0]*len(train), y_true=train.Survived)*100, 3))\nprint(round(accuracy_score(y_pred=[0]*len(test), y_true=test.Survived)*100, 3))","066ef7d3":"tr_pred = np.where(train.Sex == 'female', 1, 0)\nte_pred = np.where(test.Sex == 'female', 1, 0)\n\nprint(round(accuracy_score(y_pred=tr_pred, y_true=train.Survived)*100, 3))\nprint(round(accuracy_score(y_pred=te_pred, y_true=test.Survived)*100, 3))","23c88d90":"tr_pred = np.where(train.Pclass < 3, 1, 0)\nte_pred = np.where(test.Pclass < 3, 1, 0)\n\nprint(round(accuracy_score(y_pred=tr_pred, y_true=train.Survived)*100, 3))\nprint(round(accuracy_score(y_pred=te_pred, y_true=test.Survived)*100, 3))","ad28f42e":"def compare_models(data, target, models, cv, processing=None):\n    for model in models:\n        print(model[0])  #model is going to be a tuple (name, model)\n        \n        if processing is not None:\n            model_pipe = Pipeline([('processing', processing), model])\n        else:\n            model_pipe = model[1]\n        \n        if not model[0] in ['xgb', 'lgb']: \n            oof, _ = tml.cv_score(data=data, target=target, estimator=model_pipe, cv=cv, predict_proba=True)\n        else:\n            oof, _ = tml.cv_score(data=data, target=target, estimator=model_pipe, cv=cv, predict_proba=True, \n                               early_stopping=50, eval_metric='auc')\n        \n        tml.eval_classification(data, target, oof, proba=True, thrs=0.5, plot=1, hue_feat='Sex', feat='Age')\n        \n        print('_'*50)\n        print('_'*50)","16ebbb62":"train_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']\n\nkfolds = KFold(n_splits=5, random_state=409, shuffle=True)\n\ny = train.Survived\n\n\nmodels = [('logit', LogisticRegression()),\n          ('tree', DecisionTreeClassifier(max_depth=5)), \n          ('forest', RandomForestClassifier(n_estimators=200)), \n          ('xgb', xgb.XGBClassifier(n_estimators=2000, use_label_encoder=False)), \n          ('lgb', lgb.LGBMClassifier(n_estimators=2000))]  # they both are going to use early stopping\n\nproc = Pipeline([('dummy', tml.Dummify(drop_first=True))])  # tubesML class to generate dummies\n\ncompare_models(train[train_feats], y, models, kfolds, processing=proc)","0c3e207d":"sel = tml.DtypeSel(dtype='category')  # define our object\n\nsel.transform(train).head()  # the fit method does nothing, calling the transform one is enough","85c0931a":"imp = tml.DfImputer(strategy='median', add_indicator=True)\n\nimp.fit_transform(train[['Age', 'Fare']]).head(10)","a593b2c1":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n# we add the indicator because in data exploration it looked like the fact a missing value exist matters a lot\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])","24901e5e":"train_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age']\n\nmodels = [('logit', LogisticRegression(max_iter=5000)), \n          ('tree', DecisionTreeClassifier(max_depth=5)), \n          ('forest', RandomForestClassifier(n_estimators=200)), \n          ('xgb', xgb.XGBClassifier(n_estimators=2000, use_label_encoder=False)), \n          ('lgb', lgb.LGBMClassifier(n_estimators=2000))]\n\ncompare_models(train[train_feats], y, models, kfolds, processing=proc)","aa4290d9":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n# we add the indicator because in data exploration it looked like the fact a missing value exist matters a lot\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\nforest_pipe = Pipeline([('processing', proc), ('model', RandomForestClassifier(n_estimators=500, \n                                                                               random_state=43, \n                                                                               n_jobs=-1))])","13f77a4d":"train_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age']\n\noof, res = tml.cv_score(data=train[train_feats], target=y, estimator=forest_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True, pdp=['Fare', 'Age', 'Pclass', 'Sex_male'])","768b3348":"tml.plot_feat_imp(res['feat_imp'])","eb585a25":"tml.plot_partial_dependence(res['pdp'])","d91da2d0":"train.groupby('Pclass').Fare.agg(['min', 'max', 'mean', 'median', 'std'])","b933c2bf":"tmp = train.copy()\ntmp['FamSize'] = 1 + tmp['SibSp'] + tmp['Parch']\n\nfig, ax = plt.subplots(1,1, figsize=(12,5))\ntmp.groupby(['FamSize', 'Pclass']).Fare.median().unstack().plot(ax=ax)\nax.set_title('Median Fare per Family Size, by Class', fontsize=14)\nax.set_ylabel('Fare')\nplt.show()","9eefc279":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n# we add the indicator because in data exploration it looked like the fact a missing value exist matters a lot\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\nxgb_pipe = Pipeline([('processing', proc), ('model', xgb.XGBClassifier(n_estimators=2000, use_label_encoder=False, \n                                                                       random_state=23, n_jobs=-1))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age']\n\noof, res = tml.cv_score(data=train[train_feats], target=y, estimator=xgb_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True, \n                        pdp=['Sex_male', 'Pclass', 'Age', 'Fare', 'SibSp', 'Parch'],\n                        early_stopping=100, eval_metric='logloss')\n\nprint(f'Number of iterations by fold: {res[\"iterations\"]}')\n\ntml.plot_feat_imp(res['feat_imp'])\n\ntml.plot_partial_dependence(res['pdp'])","5222a9ac":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n# we add the indicator because in data exploration it looked like the fact a missing value exist matters a lot\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\nlgb_pipe = Pipeline([('processing', proc), ('model', lgb.LGBMClassifier(n_estimators=2000, \n                                                                       random_state=23, n_jobs=-1))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age']\n\noof, res = tml.cv_score(data=train[train_feats], target=y, estimator=lgb_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True, \n                        pdp=['Sex_male', 'Pclass', 'Age', 'Fare'],\n                        early_stopping=100, eval_metric='logloss')\n\nprint(f'Number of iterations by fold: {res[\"iterations\"]}')\n\ntml.plot_feat_imp(res['feat_imp'])\n\ntml.plot_partial_dependence(res['pdp'])","01edc286":"tmp = train[['Ticket']].copy()\nprint(f'Unique values: {tmp.Ticket.nunique()}')\ntmp.head()","cf7b9ac3":"te = tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)\n\nte.fit_transform(tmp, y).head(10)","75212166":"tmp = train[['Sex', 'Pclass', 'Age']].copy()\ntmp.head()","ad8cdcdd":"pol = tml.DfPolynomial(degree=2, interaction_only=True, to_interact=['Sex_male', 'Pclass'])\n\npipe = Pipeline([('dummy', tml.Dummify(drop_first=True)), ('poly', pol)])\n\npipe.fit_transform(tmp).head()","20816a0e":"num_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n# we add the indicator because in data exploration it looked like the fact a missing value exist matters a lot\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age']\n\ntmp = proc.fit_transform(train[train_feats])\n\ntmp.head()","a5bd059e":"pca = tml.DfPCA(n_components=4)\n\npca.fit_transform(tmp).head()","273ad700":"pca = tml.DfPCA(n_components=4, compress=True)\n\npca.fit_transform(tmp).head()","f53db1bc":"from tubesml.base import BaseTransformer, reset_columns, self_columns","349f733c":"class NewFeatures(BaseTransformer):\n    def __init__(self, fam_size=True, scl_fare=True):\n        super().__init__() \n        self.fam_size = fam_size  # boolean to control if doing or not a transformation\n        self.scl_fare = scl_fare  # these will be useful to tune our model in the next section\n        \n    def make_famsize(self, X, y=None):  # A function that makes the transformation we need\n        X_tr = X.copy()\n        \n        if self.fam_size:\n            X_tr['family_size'] = 1 + X_tr['SibSp'] + X_tr['Parch']\n            del X_tr['SibSp']\n            del X_tr['Parch']\n            \n            if self.scl_fare:\n                X_tr['Fare'] = X_tr['Fare'] \/ X_tr['family_size']\n                \n        return X_tr\n            \n    @self_columns  # This is convenient to maintain the column names    \n    def transform(self, X, y=None):  # as everything in sklearn, a transform method is expected\n        # here you just call all the functions you need\n        X_tr = X.copy()\n        \n        X_tr = self.make_famsize(X_tr)\n        \n        return X_tr","7aec5a10":"trsf = NewFeatures()\ntmp = train[['SibSp', 'Parch', 'Fare']].copy()\n\ntrsf.fit_transform(tmp).head()","4386f0a2":"class NewFeatures(BaseTransformer):\n    def __init__(self, fam_size=True, scl_fare=True, title=True, cl_sex=True):\n        super().__init__() \n        self.fam_size = fam_size  \n        self.scl_fare = scl_fare\n        self.title = title\n        self.cl_sex = cl_sex\n        \n    def make_famsize(self, X, y=None):  \n        X_tr = X.copy()\n        if self.fam_size:\n            X_tr['family_size'] = 1 + X_tr['SibSp'] + X_tr['Parch']\n            del X_tr['SibSp']\n            del X_tr['Parch']\n            if self.scl_fare:\n                X_tr['Fare'] = X_tr['Fare'] \/ X_tr['family_size']\n        return X_tr\n    \n    def make_title(self, X, y=None):\n        X_tr = X.copy()\n        if self.title:\n            X_tr['Title'] = X_tr['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n            X_tr['Title'] = X_tr['Title'].replace(['Mme', 'Countess','Dona'], 'Mrs')\n            X_tr['Title'] = X_tr['Title'].replace(['Capt', 'Col','Don', 'Jonkheer', 'Rev', \n                                                         'Major', 'Sir'], 'Mr')\n            X_tr['Title'] = X_tr['Title'].replace(['Mlle', 'Lady','Ms'], 'Miss')\n            X_tr.loc[(X_tr['Sex'] == 'male') & (X_tr['Title'] == 'Dr') , 'Title'] = 'Mr'\n            X_tr.loc[(X_tr['Sex'] == 'female') & (X_tr['Title'] == 'Dr') , 'Title'] =  'Mrs'\n        del X_tr['Name']  # delete regardless\n        return X_tr\n    \n    def class_sex(self, X, y=None):\n        # I like a better interaction between these two\n        X_tr = X.copy()\n        if self.cl_sex:\n            X_tr['class_sex'] = X_tr['Sex'] + '_' + X_tr['Pclass'].astype(str) \n        return X_tr\n  \n    @self_columns    \n    def transform(self, X, y=None):  \n        X_tr = X.copy()\n        X_tr = self.make_famsize(X_tr)\n        X_tr = self.make_title(X_tr)\n        X_tr = self.class_sex(X_tr)\n        \n        return X_tr","d31fedba":"class ImputeAge(BaseTransformer):\n    def __init__(self, imp_age=True):\n        super().__init__()\n        self.imp_age = imp_age\n        self.age_map = {}\n    \n    @reset_columns\n    def fit(self, X, y=None):  # this method learns the data\n        if 'Title' in X.columns:\n            self.age_map = X.groupby('Title').Age.median().to_dict()\n        return self\n    \n    @self_columns\n    def transform(self, X, y=None): # this method changes the data\n        X_tr = X.copy()\n        if self.imp_age and 'Title' in X.columns:\n            X_tr.loc[X_tr['Age'].isna(), 'Age'] = X_tr['Title'].map(self.age_map)\n        \n        return X_tr\n    \n    \nclass DeleteFeats(BaseTransformer):\n    def __init__(self, to_delete=None):\n        super().__init__()\n        self.to_delete = to_delete\n    \n    @self_columns\n    def transform(self, X, y=None):\n        X_tr = X.copy()\n        to_keep = X_tr.columns\n        if self.to_delete is not None:\n            for feat in self.to_delete:  # this will delete all the columns that contain the given name\n                to_keep = [col for col in to_keep if feat not in col]\n            X_tr = X_tr[to_keep]\n        return X_tr","28596740":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')),\n                     ('tar_enc', tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)),\n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc_nc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\n# Now we create the processing pipeline\nproc = Pipeline([('new_feats', NewFeatures(fam_size=True, scl_fare=True, title=True, cl_sex=True)), \n                 ('imp_age', ImputeAge(imp_age=True)), \n                 ('proc_nc', proc_nc), \n                 ('clean', DeleteFeats(to_delete=['Embarked', 'Title', 'Sex_']))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age', 'Name', 'Ticket']\n\ntmp = train[train_feats].copy()\nproc.fit_transform(tmp, y).head()","77d39db7":"forest_pipe = Pipeline([('processing', proc), ('model', RandomForestClassifier(n_estimators=500, \n                                                                               random_state=43, \n                                                                               n_jobs=-1))])\n\nparam_grid = {'processing__new_feats__fam_size': [True, False],   # processing parameters\n              'processing__new_feats__scl_fare': [True, False],  # whether or not taking a processing step\n              'processing__new_feats__title': [True, False], \n              'processing__new_feats__cl_sex': [True, False], \n              'processing__imp_age__imp_age': [True, False], \n              'processing__clean__to_delete': [[], ['Embarked'], ['Title'], ['Embarked','Title']], \n              'model__max_depth': [3, 5, 8, 20],   # modeling hyperparameters\n              'model__max_features': ['sqrt', 'log2']}","9be534f3":"res, bp, best_forest = tml.grid_search(data=train[train_feats], target=y, \n                                       estimator=forest_pipe, param_grid=param_grid, \n                                       scoring='roc_auc', cv=kfolds, random=100)\n\nres.head(10)","5e7b2dfc":"bp","fd887d93":"test_pred = best_forest.predict_proba(test[train_feats])[:,1]\n\ntml.eval_classification(data=test, target=test['Survived'], preds=test_pred, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","0c0bba9a":"xgb_pipe = Pipeline([('processing', proc), ('model', xgb.XGBClassifier(n_estimators=50, learning_rate=0.05,\n                                                                       use_label_encoder=False, \n                                                                       random_state=23, n_jobs=-1))])\n\nparam_grid = {'processing__new_feats__fam_size': [True, False],   # processing parameters\n              'processing__new_feats__scl_fare': [True, False],  # whether or not taking a processing step\n              'processing__new_feats__title': [True, False], \n              'processing__new_feats__cl_sex': [True, False], \n              'processing__imp_age__imp_age': [True, False], \n              'processing__clean__to_delete': [[], ['Embarked'], ['Title'], ['Embarked','Title']], \n              'model__max_depth': [3, 5, 8, 20],   # modeling hyperparameters\n              'model__subsample': [0.5, 0.7, 0.9, 1], \n              'model__reg_alpha': [0, 0.05, 0.5, 1, 3], \n              'model__reg_lambda': [0, 0.05, 0.5, 1, 3]}","8c95ced6":"res, bp, best_xgb = tml.grid_search(data=train[train_feats], target=y, \n                                       estimator=xgb_pipe, param_grid=param_grid, \n                                       scoring='roc_auc', cv=kfolds, random=100)\n\n\nres.head(10)","2860c15d":"bp","79b32030":"test_pred = best_xgb.predict_proba(test[train_feats])[:,1]\n\ntml.eval_classification(data=test, target=test['Survived'], preds=test_pred, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","ed5ef59b":"lgb_pipe = Pipeline([('processing', proc), ('model', lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05,\n                                                                       random_state=23, n_jobs=-1))])\n\nparam_grid = {'processing__new_feats__fam_size': [True, False],   # processing parameters\n              'processing__new_feats__scl_fare': [True, False],  # whether or not taking a processing step\n              'processing__new_feats__title': [True, False], \n              'processing__new_feats__cl_sex': [True, False], \n              'processing__imp_age__imp_age': [True, False], \n              'processing__clean__to_delete': [[], ['Embarked'], ['Title'], ['Embarked','Title']], \n              'model__num_leaves': [10, 31, 61, 120],   # modeling hyperparameters\n              'model__subsample': [0.5, 0.7, 0.9, 1], \n              'model__reg_alpha': [0, 0.05, 0.5, 1, 3], \n              'model__reg_lambda': [0, 0.05, 0.5, 1, 3]}","3b9be7a6":"res, bp, best_lgb = tml.grid_search(data=train[train_feats], target=y, \n                                       estimator=lgb_pipe, param_grid=param_grid, \n                                       scoring='roc_auc', cv=kfolds, random=200)\n\n\nres.head(10)","a634a956":"bp","ce0c35cf":"test_pred = best_lgb.predict_proba(test[train_feats])[:,1]\n\ntml.eval_classification(data=test, target=test['Survived'], preds=test_pred, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","1882ecf3":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')),\n                     ('tar_enc', tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)),\n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc_nc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\n# Now we create the processing pipeline\nproc = Pipeline([('new_feats', NewFeatures(fam_size=False, scl_fare=False, title=True, cl_sex=True)), \n                 ('imp_age', ImputeAge(imp_age=True)), \n                 ('proc_nc', proc_nc), \n                 ('clean', DeleteFeats(to_delete=['Embarked']))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age', 'Name', 'Ticket']\n\n\nforest_pipe = Pipeline([('processing', proc), ('model', RandomForestClassifier(n_estimators=500, \n                                                                               max_features='sqrt', max_depth=5,\n                                                                               random_state=43, \n                                                                               n_jobs=-1))])\n\nprint('Random Forest')\n\noof_forest, res = tml.cv_score(data=train[train_feats], target=y, estimator=forest_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True,\n                        pdp=['Fare', 'Age', 'Pclass', 'Sex_male'])\n\ntml.plot_feat_imp(res['feat_imp'])\n\ntml.plot_partial_dependence(res['pdp'])\n\ntml.eval_classification(data=train, target=y, preds=oof_forest, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","e2f3c1b8":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')),\n                     ('tar_enc', tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)),\n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc_nc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\n# Now we create the processing pipeline\nproc = Pipeline([('new_feats', NewFeatures(fam_size=True, scl_fare=True, title=True, cl_sex=True)), \n                 ('imp_age', ImputeAge(imp_age=False)), \n                 ('proc_nc', proc_nc), \n                 ('clean', DeleteFeats(to_delete=['Embarked']))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age', 'Name', 'Ticket']\n\nxgb_pipe = Pipeline([('processing', proc), ('model', xgb.XGBClassifier(n_estimators=2000, learning_rate=0.05,\n                                                                       subsample=0.7, max_depth=3,\n                                                                       reg_lambda=0.05, reg_alpha=0.5,\n                                                                       use_label_encoder=False, \n                                                                       random_state=23, n_jobs=-1))])\n\nprint('XGBoost')\n\noof_xgb, res = tml.cv_score(data=train[train_feats], target=y, estimator=xgb_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True,\n                        pdp=['Fare', 'Age', 'Pclass', 'Sex_male'],\n                        early_stopping=100, eval_metric='logloss')\n\nprint(f'Number of iterations by fold: {res[\"iterations\"]}')\n\ntml.plot_feat_imp(res['feat_imp'])\n\ntml.plot_partial_dependence(res['pdp'])\n\ntml.eval_classification(data=train, target=y, preds=oof_xgb, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","c4fd4cc6":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')),\n                     ('tar_enc', tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)),\n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc_nc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\n# Now we create the processing pipeline\nproc = Pipeline([('new_feats', NewFeatures(fam_size=True, scl_fare=True, title=True, cl_sex=False)), \n                 ('imp_age', ImputeAge(imp_age=True)), \n                 ('proc_nc', proc_nc), \n                 ('clean', DeleteFeats(to_delete=['Embarked']))])\n\ntrain_feats = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Age', 'Name', 'Ticket']\n\nlgb_pipe = Pipeline([('processing', proc), ('model', lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.05,\n                                                                        subsample=0.9, reg_lambda=1, reg_alpha=1,\n                                                                        num_leaves=61,\n                                                                       random_state=23, n_jobs=-1))])\n\nprint('LGBoost')\n\noof_lgb, res = tml.cv_score(data=train[train_feats], target=y, estimator=lgb_pipe, \n                        cv=kfolds, predict_proba=True, imp_coef=True,\n                        pdp=['Fare', 'Age', 'Pclass', 'Sex_male'],\n                        early_stopping=100, eval_metric='logloss')\n\nprint(f'Number of iterations by fold: {res[\"iterations\"]}')\n\ntml.plot_feat_imp(res['feat_imp'])\n\ntml.plot_partial_dependence(res['pdp'])\n\ntml.eval_classification(data=train, target=y, preds=oof_lgb, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","562ce700":"avg_pred = (oof_forest + oof_xgb + oof_lgb) \/ 3\n\ntml.eval_classification(data=train, target=y, preds=avg_pred, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","450c5c8b":"# A pipeline for numerical features\nnum_pipe = Pipeline([('fs', tml.DtypeSel(dtype='numeric')), \n                     ('imp', tml.DfImputer(strategy='median', add_indicator=True))])\n\n# A pipeline for categorical features\ncat_pipe = Pipeline([('fs', tml.DtypeSel(dtype='category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')),\n                     ('tar_enc', tml.TargetEncoder(to_encode='Ticket', agg_func='mean', prior_weight=100)),\n                     ('dummy', tml.Dummify(drop_first=True))])\n\n# Utility class to put them together\nproc_nc = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\n\n# Now we create the processing pipeline\nproc = Pipeline([('new_feats', NewFeatures(fam_size=True, scl_fare=False, title=True, cl_sex=True)), \n                 ('imp_age', ImputeAge(imp_age=True)), \n                 ('proc_nc', proc_nc), \n                 ('clean', DeleteFeats(to_delete=['Embarked']))])\n\n# list of estimators for the first predictions\n# they must be tuples (name, model)\nbase_estimators = [('forest', RandomForestClassifier(n_estimators=500,\n                                                     max_features='sqrt', \n                                                     max_depth=5,\n                                                     random_state=43,\n                                                     n_jobs=-1)), \n                   ('xgb', xgb.XGBClassifier(n_estimators=2000, learning_rate=0.05,\n                                             subsample=0.7, max_depth=3,\n                                             reg_lambda=0.05, reg_alpha=0.5,\n                                             use_label_encoder=False, \n                                             random_state=23, n_jobs=-1)), \n                   ('lgb', lgb.LGBMClassifier(n_estimators=2000, \n                                              learning_rate=0.05,\n                                              subsample=0.9, reg_lambda=1,\n                                              reg_alpha=1,\n                                              num_leaves=61,\n                                              random_state=23, \n                                              n_jobs=-1))]\n\n# arguments for the training of each base model, if any\nlay1_kwargs={'forest': {'predict_proba': True}, \n             'xgb': {'predict_proba': True, \n                     'early_stopping': 100, \n                     'eval_metric': 'logloss'},\n             'lgb': {'predict_proba': True,\n                     'early_stopping': 100, \n                     'eval_metric': 'logloss'} }\n\n# list of features in original dataset that we want to use to train\n# the final estimator\npass_feat = ['Sex_male', 'Pclass', 'Age']\n\n# estimator to train on the predictions of the base_estimators\nfinal_est = RandomForestClassifier(n_estimators=1000, max_depth=3,\n                                   random_state=43,n_jobs=-1)\n\nstk = tml.Stacker(estimators=base_estimators,\n                  final_estimator=final_est, \n                  cv=kfolds, \n                  passthrough=pass_feat,\n                  lay1_kwargs=lay1_kwargs)\n\nstacked_pipe = Pipeline([('processing', proc), ('model', stk)])\n\noof_stacker, res = tml.cv_score(train[train_feats], y, stacked_pipe, cv=kfolds, \n                                predict_proba=True)  # unfortunately the feature importance does not work from this method yet\n\ntml.eval_classification(data=train, target=y, preds=oof_stacker, proba=True, \n                        thrs=0.5, plot=1, hue_feat='Sex', feat='Age')","6fcbf633":"If no column (or list of columns) is given as `to_encode`, it will pick every non numeric column and encode them. Another advantage is that if the encoder encounters a new category, for example if some value was not present in the training set, it will encode it with the prior. The prior is simply the mean of the target, irregardless of the Ticket number (or of the categorical features you are using). Moreover, if a category has very few values in the training set, the weight of the prior will be higher, this is done with the intent of **avoiding overfitting**. In fact, the entries at index 144 and 40 have the same encoding even though they have 2 different ticket numbers and it is because they are both present only once in the dataset.\n\nAnother transformer to generate new features is [`DfPolynomial`](https:\/\/tubesml.readthedocs.io\/en\/latest\/feat_en.html#tubesml.poly.DfPolynomial), which creates all possible combinations of a given number of features. For example, let's take a simple dataframe","bad53f7e":"We can then test our transformer","36d19cb5":"For simplicity, we have first processed the data and then trained the models, everything is still happening by fold thanks to the use of the pipeline. Nothing stops us from having each of the base models being a pipeline, with specific transformations of the data for each model. \n\nNotice also how the base models are still trained with the early stopping mechanism (when necessary), while the predictions are then made by using an average number of trees as we did before. This should prevent the model both to overtrain on the train set and to be too specific in the number of trees to use when predicting on unseen data.\n\nThis model can be tuned again in a grid search, but only the final estimators hyperparameters can be tuned for now. The grid can be passed, for example, by doing `param_grid = {'model__final_estimator__max_depth': [3,4,5]}`.\n\nOne can spend more time finding the right combinations of models to get the best possible performance but we will move to the next step: give a final assessment of the model performance.\n\n# Final tests and predictions","2a57b370":"Where the partial dependence plot focuses on those featurs since we know from a previous run they are the most important.","38cb4c29":"# Feature Engineering, available transformers\n\nTubesML comes with a number of transformers to generate new features. We have already seen how the [`tml.Dummify`](https:\/\/tubesml.readthedocs.io\/en\/latest\/process.html#tubesml.dummy.Dummify) works in the previous iterations, but this is not the only way to deal with categorical features.\n\nTubesML comes with a handy [**target encoder**](https:\/\/tubesml.readthedocs.io\/en\/latest\/feat_en.html#tubesml.encoders.TargetEncoder) which allows encoding a categorical variable with some statistic of the target variable. Furthermore, it accounts for categories that have too little presence in the training set and risk of being the source of overfitting.\n\nBefore proceeding, it is important to remember putting this transformer inside the pipeline that is then evaluated by the cross validation strategy of choice. In this way, there is no risk of training your model with data that has information of the target variable of the test set, which will only inflate your model evaluation and not make your model better.\n\nTo demonstrate how it works, let's pick a dataframe with only the Ticket number","3a94d5b2":"## Tuning LightGBM\n\nThis is similar to what we did above with XGBoost.","3cd5bb03":"For starter, we are going to load the data and have a first look","cf3f4f13":"We have a mix of categorical and numerical features and a binary target variable. Before proceeding further, we create a train and test sets. We are going to use only the train set to take modeling decisions and the test set is purely for evaluation porpuses. Before training the final model and predict on the test set provided by the competition, we can always use the entire dataset that we have now loaded.\n\nTubesML has a funtion to split the data, it is a wrapper around sklearn's StratifiedShuffleSplit. Given we expect the Sex and the class of each passenger to matter a lot in their chance of survival, we will use these 2 features to stratify our split.","5cb62a51":"We see that the model interrupts the training fairly soon but the pattern seen above are confirmed here.\n\n## LightGBM inspection","9299c18b":"Alright, it seems the best baseline is the one counting on females surviving the accident. Intersting to notice how there seems to be quite the disparity in the survival rate of females in our train and test sets which is something to consider as this feature seems to be important and also lead to very different outcomes.\n\n# Model selection and pipelines\n\nTime to begin our model iterations. It is also going to be the first time we use tubesML to prepare the data for the models. For now, we are not going to worry about missing values as we are not going to use those columns, but we still need to make our categorical columns numeric in one way or the other. \n\nGiven the simplicity of the data, we are just going to make dummies. The tubesML transformer for that is basically a wrapper around pandas get_dummies but it also takes care of the fact that on unseen data new categories can be observed (or some categories can be missing), making sure the pipeline will not break. [See the documentation for more details](https:\/\/tubesml.readthedocs.io\/en\/latest\/process.html#tubesml.dummy.Dummify)\n\nWe are going to test a bunch of different algorithms and use the same tubesML function to generate out of fold predictions on the train set and evaluate those predictions against the true label.\n\nTo achieve that, we are goint to use [tubesml.cv_score](https:\/\/tubesml.readthedocs.io\/en\/latest\/model_selection.html#tubesml.model_selection.cv_score), which does the following:\n* Given a model or a pipeline, it generates out of fold predictions using Kfold\n* It accounts for algorithms that work best when early stopping is used (for example Xgboost and LightGbm)\n* It predicts directly a target or a probability for that target (for binary classification only)\n* If needed, it returns the feature importance and the partial dependency plots averaged across the folds with an estimated uncertainty. (we are going to use these options at a later stage).\n\nWe also use another [tubesML function](https:\/\/tubesml.readthedocs.io\/en\/latest\/report.html#tubesml.report.plot_classification_probs) for plotting the predictions.\n\nSince we don't know what algorithm we are going to use, let's use a few of them and create a utility function to quicly iterate over the various options.","448e0e97":"We now use these components to create a transformer that will work in our pipeline. For example, let's create a transformer that creates the family size and corrects the `Fare` feature. By inheriting the `BaseTransformer` we are sure to have the core functionalities for a transformer to work well with other TubesML components.","9dd07dd5":"But it can also be used to **remove noise** from the data by using the `compress` parameter. It will perfom a PCA to the number of components specified and then perfom an inverse transformation. Some information will be lost in the process and the underlying assumption is that PCA is already capturing the most important part of the information, thus losing the noisier part. [More info can be found in this notebook](https:\/\/www.kaggle.com\/lucabasa\/decision-trees-pca-and-data-compression)","8e3511f8":"Notice that the method returns a dataset, as every other tubesML transformer. This is why it becomes easier to build flexible pipelines: **the data structure remains the same and thus it is simpler to perform any sort of operation on your data** (we are going to see some examples later).\n\nSecondly, we need to impute the missing values. We use [DfImputer](https:\/\/tubesml.readthedocs.io\/en\/latest\/clean.html#tubesml.clean.DfImputer), which can be used as follows","146471cf":"In other words, we tell the transformer what feature to consider (the default is to consider them all) and it will **create interactions** of those features, leaving the rest of the dataframe intact. It requires the features to be numerical, hence the use of dummify in an earlier stage of the pipeline.\n\nAt last, we have the option of creating features using PCA, thanks to [`tml.DfPCA`](https:\/\/tubesml.readthedocs.io\/en\/latest\/feat_en.html#tubesml.pca.DfPCA). This is another transformer that is merely a wrapper around the sklearn's methods but it has a few useful features. For starters, it returns a dataframe","880f612e":"We are lucky enough that the same columns have missing values in our train and test sets but the pipelines that we are going to build are going to be robust against previously unseed missing values (although too bad we can't test that here).\n\nWhat we see is that age could be an important feature and it is missing a significant amount of time. Cabin is missing most of the times and might be then tricky to use, while Embarked is barely missing and however we impute the missing values should not matter too much.\n\nNext, the initial assumption that Sex and Class will be important features can be put to a simple test","fabec793":"# Data Exploration\n\nThis section will not contain fancy graphs as there are many notebooks that will give better ideas on how to make your data look pretty and useful for various applications. We will rather focus on a few aspects that got my attention. On a personal note, I find this process makes more sense when done iteratively, meaning that earlier versions of this notebook will explore the role of some features and, over time and after some model iterations, I will use the newly acquired knowledge to further explore the data. \n\nThe advantage of this approach is that I feel I have more control on how the model evolves and reacts to my intuitions. The downside is that you might go into the modeling too soon with incomplete understanding of the data.\n\nTubesML has a few methods for data exploration, that can be found [in the documentation](https:\/\/tubesml.readthedocs.io\/en\/latest\/data_exploration.html), but they were mainly developed for continuous target variables and therefore there might be a future update to better support binary classification problems.\n\nFirst of all, we noticed there are missing values, tubesML has a simple function to have a list of those.","8d3c2a94":"At last, **everyone in the third class die**","29f6337e":"570 values in our train set, if we were making dummies out of that we would have a very large and very sparse dataframe. With the target encoder we can replace each value with the mean of the survival column.","7143594f":"Interestingly, it seems that when a data point is missing, the survival rate goes down. I would intepret this as the data is missing *because* they didn't survive and therefore there was no record of their age, for example. This opens also the question of whether or not their family was on the boat and also didn't survive but I will pospone this exploration for later.\n\n# Baseline models\n\nThis section is meant to create simple predictions to generate a baseline score. We do that because soon enough we will use a machine learning model with a certain accuracy and these scores are going to give us context to assess if that accuracy is good or not.\n\nFirst, **everybody die**","03eb59ec":"This generates a grid of 1024 configurations. To speed up, we will just focus on 100 random of them (the more, the slowest, the better the result).","d980a116":"We essentially see that females are very likely to survive, and rich females more than most. Men in the second and third class seems to be that enjoyed the trip the least.\n\nOne thing I wonder is if there is any relation between a data point being missing and the survival rate","3ebcbb2a":"This is a small improvement and we could spend time to fine tune this mean until we get the best combination of the the three models.\n\nAnother option is to **let a model decide how to combine the results of the 3 models**. To do so, TubesML has a [**Stacker**](https:\/\/tubesml.readthedocs.io\/en\/stable\/models.html#tubesml.stacker.Stacker) class. The class uses a list of estimators to generate predictions via `tml.cv_score`, hence they are out of fold predictions. After that, it uses the generated values from each model (plus optionally some or all the original features) to train a final model.\n\nThis is how to call it.","acd91a83":"We see indeed how the Fare appeare is, as the intuition suggests, related to the class where a passenger is traveling. Another aspect that can influence the cost of the ticket is the amount of people traveling together, the quickest proxy for that is the family size.","e1ce3520":"The plots above shows an average taken across folds and the corresponding standard deviation of the mean. This is not accurate estimation of the uncertainty around the feature importances or the partial dependence plot, but it gives some indication of it.\n\nThe patterns above make sense overall but there might be something interesting going on with `Fare`. Let's see if our training set can tell us more.","34762000":"# Model Stacking\n\nWe now have a few models that seem promising and achieve results visibly better than our initial baselines. What we can do next is to find a good way of combining the results to obtain a better prediction. This stage has to be done with care to make sure that we are not leaking target information into the model.\n\nFirst, let's inspect again the best configurations of each model.","823621d1":"This performance on unseen data is now about 0.05 higher than the best baseline created at the beginning of this notebook. All the modeling decisions were taken by working on our train set (which is a subset of the provided training set) and every operation on the data is happening in a pipeline, guaranteing there is no information leaking from our test set to the model. In other words, this is a fair estimation of the model perfomance on unseen data. It is also comforting that the result is somewhat in line with the one on truly unseen data.\n\nThe result of the search is also pretty clear\n* The feature `Title` as defined above helps the model, probably because it includes information about Sex and Age at the same time\n* Family size does not help and, consequently, scaling the `Fare` feature is not helpful either\n* The interaction between `Pclass` and `Sex` is helpful as anticipated\n* Imputing age differently brings a marginal improvement in the model performance\n* The model still overestimates the survival rate for Men\n\n## Tuning XGBoost\n\nTuning XGBoost is somewhat more complicated. The main obstacle is that there is no implementation in TubesML that accounts for early stopping in a grid search yet. So we are going to make a compromise on the number of estimators for the grid search in the following way \n\n* We pick a learning rate and see when the early stopping kicks in\n* We take an average across folds of the early stopping round and use that number for the model we feed to the grid search\n* Once we know the best parameters, we go back using early stopping as before.\n\nIt is not a perfect technique but it is efficient.","5805e325":"The method also returns the best configuration","0a05a3e2":"This notebook aims to show how to use the **[TubesML library](https:\/\/pypi.org\/project\/tubesml\/)**, which is designed to make **flexible data and modeling pipelines, model inspection, and data exploration**. \n\nThe frequent goal of a data scientist is to create a model that will fit in some sort of production environment and the common request before deploying such model is to have a faithful evaluation of its performance, with the expectation that the model will still work as intendend with previously unseen data. TubesML does not have the computing performance edge that might be required for the deployment of a model, but it provides **a framework that encourages the use of good modeling practices** and it is a useful tool to get a data scientist to a working proof of concept that can survive working with data that were not use for training. To prove this, we are not going to load the test set at all until we are at the final stages of the model development.\n\nTubesML is maintained by me, hence such notebooks are also a stress test for its components. Some of the *mispractices* that we are going to avoid can be useful in a competition environment, therefore I expect the final score to not be particularly on top of the LB but rather a decent result given the time investment that I am willing to put into predicting survival on the Titanic.","63110a84":"# Feature Engineering, custom transformers\n\nOn top of the transformers above, which provide a very quick way of having new features ready for modeling but arguably the biggest improvements to your models will come from features that you will make yourself. This section will show you how to make such transformer so that they will work inside your pipeline without further effort.\n\nTo do so, just import a [few base components of TubesML](https:\/\/tubesml.readthedocs.io\/en\/latest\/base.html)","79a82b8e":"We see finally some result better than our baseline and some interesting patterns in how each model is classifying (or misclassifying) the survival probability of each passenger in our train set. Given these performances, we can safely drop the logit and the decision tree and just focus on the random forest, xgboost, and lightgbm.\n\n# Model inspection\n\nSo far we didn't do much: clean the data enough to make the model work and run the model. This was enough to beat our baseline and have some first insight on how the model is predicting the survival of our passengers. In this section, we are going to see some [functionalities of tubesML](https:\/\/tubesml.readthedocs.io\/en\/latest\/report.html) for model inspection and discuss how can we use them for better model decisions.\n\n## Random Forest inspection\n\nLet's use `cv_score` as before but also return **more information** about the model training in each fold. The reason why we care about each fold and not just generically the learning of the full train set is that it should be easier to spot when a noisy feature starts to be important for the model, which is going to be a problem down the road when the model is deployed and the noise is different. In other words, great variation in the model behavior across the folds might mean overfitting and we don't want that.\n\nWe still have our simple pipeline","91f55396":"Depending on the pipeline step you want to insert, you can get as creative as you want with your custom transformer, provided that there is a `transform` method and the decorators are called appropriately. \n\nYou can make several transformers or one transformer that does several things. The one that I started above is meant to be at the beginning of the pipeline and it can also take care of some different imputations of the missing values.","604a3a14":"Or, **all the men die**","f24b2d07":"It seems indeed relevant and information to use in future iterations. For now, let's see the other models\n\n## XGboost inspection\n\nTubesML methods allow to have the model training with early stopping and, in that case, the number of iterations by fold is returned. This can be a useful information on how much the model is learning and how many estimators to use if we want a little bit less fine-tuned model (which can guarantee a better generalizability)","cb65be7d":"Now, each model is performing in a slightly different way and an easy way to combine the results is just to take an average of the predictions. This is a safe operations as the TubesML function we used is returning out of fold predictions and we are using the same fold splits for the 3 models. Therefore, each prediction of the 3 models was made after training on exactly the same dataset, hence preventing that some target information is leaking into the training set when we take an average.","d91852fc":"The transformer allows for several strategies, being a wrapper around the sklearn's SimpleImputer, and optionally allows for automatically create flags for data was originally missing.\n\nAt last, we are going to use [another utility class](https:\/\/tubesml.readthedocs.io\/en\/latest\/utility.html#tubesml.utility.FeatureUnionDf) to simply put multiple pipelines together and it is designed to be compatible with all the methods in sklearn and in tubesML.\n\nTherefore, our pipeline will look like this","41e4d1fc":"All the models perform worst that the baseline and all of them overestimate the survival rate of males. We need to do better.\n\n## Imputing missing values and building a pipeline\n\nThe cabin features seems to require more work, so we ignore it for now but it makes sense to impute the missing Age and the missing Embarked columns. TubesML provides a transformer that can be configured to impute both numerical and categorical data. Given it is the first pipeline with some sort of complexity, let's proceed step by step.\n\nThe pipeline we are going to build is going to\n* split the numerical and categorical data, this is for imputing purposes only but we can also do some data processing if we want\n* impute the missing values with the appropriate strategies for numerical and categorical variables\n* put the data back together\n* run the model\n\nLater on, we are going to add more steps in this pipeline so it is worth to see now how it is going to work.\n\nFirst, we need to split the data. We are thus going to use a [utility class from tubesML](https:\/\/tubesml.readthedocs.io\/en\/latest\/utility.html#tubesml.utility.DtypeSel) that works as follows, let's say we want the categorical features in the dataset.","bbc0d409":"# Model Tuning\n\nIt is now time to put all the transformers and models together to tune our 3 models in order to obtain the best possible combination of processing and hyperparameters. The power of flexible pipelines comes into play once again.\n\nThe goal is to both finding the best hyperparameter, let's say the best `max_features` for the RandomForest while taking into account that the choice might change if we process the data differently. We achieve this goal by **running a grid search (or a random one) that covers both the hyperparameters of the model and the processing at the same time**.\n\nFirst, let's define the pipeline.","9c708ecd":"And, most importantly, the best fitted estimator ready for use","cb2c056c":"Alright, the pipeline works and gives us quite the freedom of trying several combinations, let's tune our models.\n\nTo do so, we use [TubesML's `grid_search`](https:\/\/tubesml.readthedocs.io\/en\/stable\/model_selection.html#tubesml.model_selection.grid_search). There are other ways of tuning the model, just make sure you can pass the options for the parameters of your **full** pipeline\n\n## Tuning the Random Forest\n\nWe want to check if the features created above are helping the random forest to learn the data better and if this is influencing the forest hyperparameters. We just focus on the max depth and the max features of each tree as typically these 2 are the hyperparameters that influence the perfomance the most and we are not particularly interested in fine tuning the model."}}