{"cell_type":{"9d0ba600":"code","ec05bc1e":"code","ad218a63":"code","e43d51a2":"code","58b7f51d":"code","4526fb7f":"code","41155311":"code","8983f503":"code","cb93e826":"code","e8f510af":"code","c25ca7bf":"code","c97aff25":"code","80dd7c05":"code","6adfb6c1":"code","f6a144e1":"code","e761d219":"code","6fcb1112":"code","60298686":"markdown","d7de5969":"markdown","f9539833":"markdown","563a58a6":"markdown","b735392c":"markdown"},"source":{"9d0ba600":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec05bc1e":"# df = pd.read_csv(\"..\/input\/allcities-jan5\/allcities_Jan5.csv\", encoding='latin-1')\n\n# df.loc[:,\"sub_brand\"]=df.variant\n# df.loc[(df.brand==\"NUTRALITE\") &( df.sscat ==\"CHOCOLATE SPREAD\"),\"sub_brand\"] = \"CHOCO SPREAD\"\ndf  = pd.read_csv(\"..\/input\/allcities-jan17\/allcities_jan17_RTEC.csv\", encoding='latin-1')","ad218a63":"# df_butter = pd.read_csv(\"..\/input\/allcities-jan5-butter\/allcities_Jan5_butter.csv\")","e43d51a2":"# df = df.loc[df.sscat!=\"BUTTER\"]\n# df = pd.concat([df,df_butter])\n# df.shape","58b7f51d":"# df.loc[df.brand==\"PATANJALI\",\"cat\"]=\"BAKERY, CAKES & DAIRY\"","4526fb7f":"# df_butter.loc[(df_butter.sscat==\"BUTTER\")&(df_butter.brand==\"NUTRALITE\")]","41155311":"def raw_data_filtering_and_factor_calculation(input_file,city_id_list,category_dict,p1_start_month,p1_end_month,p2_start_month,p2_end_month):#format of category [\"sscat\",\"KIDS FLAKES\"]\n    p1 = list(range(p1_start_month,p1_end_month+1))\n    p2 = list(range(p2_start_month,p2_end_month+1))\n    all_months_in_total_period = p1 + p2\n    city_filtered = input_file.loc[input_file.city_id.isin(city_id_list)] \n    city_time_filtered = city_filtered.loc[city_filtered.month_adj.isin(all_months_in_total_period)]\n\n\n    tghh = 0\n    for i in range(len(city_id_list)):\n        tghh += city_time_filtered.loc[city_time_filtered.city_id == city_id_list[i]].groupby(\"month_adj\").tghh.first().mean()\n    \n#     sample_bintix_uid = set(city_time_filtered.loc[city_time_filtered.month_adj.isin(all_months_in_total_period[:2])].bintix_uid.unique()).intersection(set(city_time_filtered.loc[city_time_filtered.month_adj.isin(all_months_in_total_period[-3:])].bintix_uid.unique()))\n#     for a in all_months_in_total_period[1:]:\n#         sample_bintix_uid = sample_bintix_uid.intersection(set(city_time_filtered.loc[city_time_filtered.month_adj==a].bintix_uid.unique())\n    p1_month_presesnt_for_sample = city_time_filtered.loc[city_time_filtered.month_adj.isin(p1)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n    p2_month_presesnt_for_sample = city_time_filtered.loc[city_time_filtered.month_adj.isin(p2)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n    p1_month_present_filter = p1_month_presesnt_for_sample.loc[p1_month_presesnt_for_sample.month_adj>=1]\n    p2_month_present_filter =  p2_month_presesnt_for_sample.loc[p2_month_presesnt_for_sample.month_adj>=1]\n    sample_bintix_uid = set(p1_month_present_filter.bintix_uid.unique()).intersection(set(p2_month_present_filter.bintix_uid.unique()))\n    \n    factor = tghh\/len(sample_bintix_uid)\n\n    city_time_category_filtered = city_time_filtered\n    for a in category_dict:\n        city_time_category_filtered = city_time_category_filtered.loc[eval(\"city_time_category_filtered.\"+ a + category_dict[a])]\n    city_time_category_filtered_trimmed_sample = city_time_category_filtered.loc[city_time_category_filtered.bintix_uid.isin(list(sample_bintix_uid))]\n   #     return [city_time_category_filtered_trimmed_sample,factor]\n    return [city_time_category_filtered_trimmed_sample,factor]","8983f503":"def multi_city_gnl_helper(p1_start_month,p1_end_month,p2_start_month,p2_end_month,ref_brand,city_id_list,category_dict,raw_data): \n    #ref_brand is a list which contains name of brand,sub_brand,variant but only upto the level which we want.\n    \n    input_file,factor_p1_p2 = raw_data_filtering_and_factor_calculation(raw_data,city_id_list,category_dict,p1_start_month,p1_end_month,p2_start_month,p2_end_month)\n    \n    #basic filtering,tagging,grouping\n    p1 = list(range(p1_start_month,p1_end_month+1))\n    p2 = list(range(p2_start_month,p2_end_month+1))\n    relevant_columns = [\"bintix_uid\",\"brand\",\"sub_brand\",\"variant\",\"std_unit_value\",\"volume\"]\n    p1_purchases = input_file.loc[input_file.month_adj.isin(p1),relevant_columns]\n    p2_purchases = input_file.loc[input_file.month_adj.isin(p2),relevant_columns]\n    \n    for a in [p1_purchases,p2_purchases] : \n        if len(ref_brand) == 1:\n            a.loc[:,[\"brand_other\"]] = np.where(a.brand==ref_brand[0],\"brand\",\"other\")\n        if len(ref_brand) == 2:\n            a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1]),\"brand\",\"other\")\n        if len(ref_brand) == 3:\n            a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2]),\"brand\",\"other\")\n#         if len(ref_brand) == 4:\n#             a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2])&(eval(\"a.std_unit_value\" +ref_brand[3])),\"brand\",\"other\")\n        \n    grouped_p1 = p1_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n    grouped_p2 = p2_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n    \n    \n    \n    #removing unwanted uids - nonbrand buyers for both period\n    other_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"other\"].bintix_uid.unique()\n    brand_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"brand\"].bintix_uid.unique()\n    other_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"other\"].bintix_uid.unique()\n    brand_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"brand\"].bintix_uid.unique()\n    p1_uid_only_others = grouped_p1.loc[grouped_p1.bintix_uid.isin(other_uid_p1)&(~grouped_p1.bintix_uid.isin(brand_uid_p1))].bintix_uid.unique()\n    p2_uid_only_others = grouped_p2.loc[grouped_p2.bintix_uid.isin(other_uid_p2)&(~grouped_p2.bintix_uid.isin(brand_uid_p2))].bintix_uid.unique()\n    p1_unwanted_uid = set(p1_uid_only_others)-set(brand_uid_p2)\n    p2_unwanted_uid = set(p2_uid_only_others)-set(brand_uid_p1)\n    grouped_p1_filtered = grouped_p1.loc[~grouped_p1.bintix_uid.isin(p1_unwanted_uid)]\n    grouped_p2_filtered = grouped_p2.loc[~grouped_p2.bintix_uid.isin(p2_unwanted_uid)]\n    merged = pd.merge(grouped_p1_filtered,grouped_p2_filtered,on=[\"bintix_uid\",\"brand_other\"],how=\"outer\").sort_values(by=\"bintix_uid\")\n    \n    #Segmenting\n    new_uid = set(brand_uid_p2) - set(brand_uid_p1) - set(other_uid_p1)\n    lapsed_uid = set(brand_uid_p1) - set(brand_uid_p2) - set(other_uid_p2)\n    volume_change_uid = set(set(brand_uid_p1)-set(other_uid_p1)).intersection(set(brand_uid_p2)-set(other_uid_p2))\n    switch_and_vol_change_uid = set(merged.bintix_uid) - new_uid - lapsed_uid - volume_change_uid\n    possible_dropped_repertoire_uid = set(brand_uid_p1) - set(brand_uid_p2)\n    possible_added_repertoire_uid = set(brand_uid_p2) - set(brand_uid_p1)\n    \n    \n    #tagging\n    merged.loc[merged.bintix_uid.isin(new_uid),[\"type_of_inc_dec\"]] = \"new\"\n    merged.loc[merged.bintix_uid.isin(lapsed_uid),[\"type_of_inc_dec\"]] = \"lapsed\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y>merged.volume_x),[\"type_of_inc_dec\"]] = \"increase_buying\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y<merged.volume_x),[\"type_of_inc_dec\"]] = \"decrease_buying\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y==merged.volume_x),[\"type_of_inc_dec\"]] = \"same_buying\"\n    merged.loc[merged.bintix_uid.isin(switch_and_vol_change_uid),[\"type_of_inc_dec\"]]=\"switching and volume change\"\n    \n    \n    #calculating gain\/loss\n    gain_new = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"new\")].volume_y.sum()\/1000\n    loss_lapsed = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"lapsed\")].volume_x.sum()\/1000\n    gain_inc_buying_solus = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_x.sum())\/1000\n    loss_dec_buying_solus = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_x.sum())\/1000\n    merged.volume_x.fillna(0,inplace=True)\n    merged.volume_y.fillna(0,inplace=True)\n    gain_switch_and_vol_change = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_x.sum())\/1000\n    loss_switch_and_vol_change = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_x.sum())\/1000\n    \n    \n    #switching_pattern\n    all_purchases = pd.concat([p1_purchases,p2_purchases])\n    all_wanted_uid_purchases = all_purchases.loc[~all_purchases.bintix_uid.isin(p1_unwanted_uid.union(p2_unwanted_uid))]\n    if len(ref_brand)==1:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand\n    if len(ref_brand)==2:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand\n    if len(ref_brand)==3:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand + \"-\" + all_wanted_uid_purchases.variant\n#     if len(ref_brand)==4\n    unique_brand_sub_brand = all_wanted_uid_purchases.brand_sub_brand.unique()\n    gain_due_to = dict()\n    loss_due_to = dict()\n    gain_due_to[\"increased_buying\"] = 0\n    loss_due_to[\"decreased_buying\"] = 0\n    added_to_repertoire = 0\n    dropped_from_repertoire = 0\n    \n    for a in unique_brand_sub_brand:\n        gain_due_to[a] = 0\n        loss_due_to[a] = 0\n    for uid in list(switch_and_vol_change_uid):\n        p1_uid_purchase = p1_purchases.loc[p1_purchases.bintix_uid == uid]\n        if len(ref_brand)==1:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand\n        if len(ref_brand)==2:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand\n        if len(ref_brand)==3:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand + \"-\" + p1_uid_purchase.variant\n#         if len(ref_brand)==4\n        p1_uid_grouped = p1_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n        \n        p2_uid_purchase = p2_purchases.loc[p2_purchases.bintix_uid == uid]\n        if len(ref_brand)==1:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand\n        if len(ref_brand)==2:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand\n        if len(ref_brand)==3:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand + \"-\" + p2_uid_purchase.variant\n#         if len(ref_brand)==4\n        p2_uid_grouped = p2_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n        \n        merged_switching = p1_uid_grouped.merge(p2_uid_grouped, on = \"brand_sub_brand\", how = \"outer\")\n        merged_switching.fillna(0,inplace=True)\n        merged_switching.loc[:,[\"volume_diff\"]] = merged_switching.volume_y - merged_switching.volume_x\n        vol_diff_uid = merged_switching.volume_diff.sum()\n        \n        if vol_diff_uid<0:\n            merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"decreased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n        elif vol_diff_uid>0:\n            merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"increased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n        merged_switching.loc[:,[\"sign\"]] = np.where(merged_switching.volume_diff<0,\"negative\",\"non_negative\") \n        sign_desired_brand = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"sign\"]\n        factor_for_assigning_gain_loss = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"volume_diff\"]\/merged_switching.loc[merged_switching.sign == sign_desired_brand].volume_diff.sum()\n        if sign_desired_brand == \"negative\":\n            for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                if (a==\"decreased_buying\") and (uid in possible_dropped_repertoire_uid):\n                    dropped_from_repertoire += merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                else:\n                    loss_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n        else:\n            for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                if (a==\"increased_buying\") and (uid in possible_added_repertoire_uid):\n                    added_to_repertoire +=  merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                else:\n                    gain_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n\n    for a in unique_brand_sub_brand:\n        if gain_due_to[a]==0 and loss_due_to[a]==0:\n            del gain_due_to[a]\n            del loss_due_to[a]\n    \n    #adjustment. gain\/loss due to volume change seperated from switching\n    dropped_from_repertoire = abs(dropped_from_repertoire)\/1000\n    added_to_repertoire = abs(added_to_repertoire)\/1000\n    gain_switch = gain_switch_and_vol_change - abs(gain_due_to[\"increased_buying\"])\/1000 - added_to_repertoire\n    loss_switch = loss_switch_and_vol_change - abs(loss_due_to[\"decreased_buying\"])\/1000 - dropped_from_repertoire\n    gain_inc_buying = gain_inc_buying_solus + abs(gain_due_to[\"increased_buying\"])\/1000\n    loss_dec_buying = loss_dec_buying_solus + abs(loss_due_to[\"decreased_buying\"])\/1000\n\n    \n    del gain_due_to[\"increased_buying\"]\n    del loss_due_to[\"decreased_buying\"]\n    \n    #results\n    gain_brand_switch_list = [x*factor_p1_p2\/1000 for x in gain_due_to.values()]\n    loss_brand_switch_list = [x*factor_p1_p2\/1000 for x in loss_due_to.values()]\n    gain = [x*factor_p1_p2 for x in [gain_new,gain_inc_buying,gain_switch,added_to_repertoire]]\n    loss= [x*factor_p1_p2 for x in [-loss_lapsed,-loss_dec_buying,-loss_switch,-dropped_from_repertoire]]\n    net_gain = factor_p1_p2*(p2_purchases.loc[p2_purchases.brand_other==\"brand\"].volume.sum() - p1_purchases.loc[p1_purchases.brand_other==\"brand\"].volume.sum())\n    \n    \n    #Intercation Index\n    interaction_df = pd.DataFrame({\"Brands\":gain_due_to.keys(),\"abs_gain\":gain_brand_switch_list,\"abs_loss\":[abs(x) for x in loss_brand_switch_list]})\n    interaction_df.loc[:,[\"total_gain_loss_brand\"]] = interaction_df.abs_gain + interaction_df.abs_loss\n    interaction_df.loc[:,\"net vol in 000Kgs\"] =interaction_df.abs_gain - interaction_df.abs_loss\n\n#     interaction_df.loc[:,[\"percent_gain_loss\"]] = (interaction_df.total_gain_loss_brand\/interaction_df.total_gain_loss_brand.sum())*100\n#     volume_share_list=[]\n    if len(ref_brand)==1:\n        all_purchases_without_brand = all_purchases.loc[~(all_purchases.brand==ref_brand[0])]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand \n    if len(ref_brand)==2:\n        all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1]))]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand\n    if len(ref_brand)==3:\n        all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1])&(all_purchases.variant==ref_brand[2]))]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand + \"-\" + all_purchases_without_brand.variant\n#     if len(ref_brand)==4\n\n#     for a in gain_due_to.keys():\n#         volume_share_list+=[(all_purchases_without_brand.loc[all_purchases_without_brand.brand_sub_brand==a].volume.sum()\/all_purchases_without_brand.volume.sum())*100]\n#     interaction_df.loc[:,[\"percent_volume_share_adjusted\"]] = volume_share_list\n#     interaction_df.loc[:,[\"interaction_index\"]] = (interaction_df.percent_gain_loss\/interaction_df.percent_volume_share_adjusted)*100\n    \n    #presenting final numbers in the format used by Kelloggs's \n    gain_loss_segments = [\"Entry To\/Lapse From Category\",\"Incr\/Decr in Cons.of Ref.Brand\",\"Total Shift To\/From Ref. Brand\",\"Addn.\/Deletion from Repertoire\"]\n    gain_loss_segments_net = [(gain[i]+loss[i])\/1000 for i in range(4)]\n    gain_loss_segments_net_df = pd.DataFrame(data=[[gain_loss_segments[i],gain_loss_segments_net[i]] for i in range(4)],columns=[\"Type\/Brand\",\"net vol in 000Kgs\"])\n    \n#     net_df = pd.DataFrame(data=[[\"Brand Net Shift\",net_gain\/1000\/1000]],columns= [\"Type\/Brand\",\"net vol in 000Kgs\"])\n    \n#     interaction_df_trimmed = interaction_df.loc[:,[\"Brands\",\"interaction_index\"]]\n#     interaction_df_trimmed.rename(columns = {'Brands':'Type\/Brand'}, inplace = True)\n#     interaction_df_trimmed.loc[:,[\"net vol in 000Kgs\"]] = [(gain_brand_switch_list[i] + loss_brand_switch_list[i])\/1000 for i in range(len(gain_brand_switch_list))]\n#     interaction_df_trimmed = interaction_df_trimmed[[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"]]\n    \n#     top_info_df = pd.DataFrame(data = [[\"Reference_Brand: \" + \"-\".join(ref_brand)],[\"Time_Period: \"+ time_period_string]],columns=[\"Type\/Brand\"])\n\n#     return pd.concat([top_info_df,net_df,gain_loss_segments_net_df,interaction_df_trimmed])\n    \n    return [net_gain\/1000\/1000,gain_loss_segments_net_df,all_purchases_without_brand,interaction_df[[\"Brands\",\"total_gain_loss_brand\",\"net vol in 000Kgs\"]]]\n    \n    ","cb93e826":"def GNL_analysis(p1_start_month,p1_end_month,p2_start_month,p2_end_month,ref_brand,city_id_list,category_dict,raw_data): \n    #ref_brand is a list which contains name of brand,sub_brand,variant but only upto the level which we want.\n    month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",0:\"DEC\"}\n    p1_start_string = month_num_dict[p1_start_month%12]+str(19+(p1_start_month-1)\/\/12)\n    p1_end_string = month_num_dict[p1_end_month%12]+str(19+(p1_end_month-1)\/\/12)\n    p2_start_string = month_num_dict[p2_start_month%12]+str(19+(p2_start_month-1)\/\/12)\n    p2_end_string = month_num_dict[p2_end_month%12]+str(19+(p2_end_month-1)\/\/12)\n    time_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n\n    if len(city_id_list)==1:\n        input_file,factor_p1_p2 = raw_data_filtering_and_factor_calculation(raw_data,city_id_list,category_dict,p1_start_month,p1_end_month,p2_start_month,p2_end_month)\n\n        #basic filtering,tagging,grouping\n        p1 = list(range(p1_start_month,p1_end_month+1))\n        p2 = list(range(p2_start_month,p2_end_month+1))\n        relevant_columns = [\"bintix_uid\",\"brand\",\"sub_brand\",\"variant\",\"std_unit_value\",\"volume\"]\n        p1_purchases = input_file.loc[input_file.month_adj.isin(p1),relevant_columns]\n        p2_purchases = input_file.loc[input_file.month_adj.isin(p2),relevant_columns]\n\n        for a in [p1_purchases,p2_purchases] : \n            if len(ref_brand) == 1:\n                a.loc[:,[\"brand_other\"]] = np.where(a.brand==ref_brand[0],\"brand\",\"other\")\n            if len(ref_brand) == 2:\n                a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1]),\"brand\",\"other\")\n            if len(ref_brand) == 3:\n                a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2]),\"brand\",\"other\")\n    #         if len(ref_brand) == 4:\n    #             a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2])&(eval(\"a.std_unit_value\" +ref_brand[3])),\"brand\",\"other\")\n\n        grouped_p1 = p1_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n        grouped_p2 = p2_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n\n\n\n        #removing unwanted uids - nonbrand buyers for both period\n        other_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"other\"].bintix_uid.unique()\n        brand_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"brand\"].bintix_uid.unique()\n        other_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"other\"].bintix_uid.unique()\n        brand_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"brand\"].bintix_uid.unique()\n        p1_uid_only_others = grouped_p1.loc[grouped_p1.bintix_uid.isin(other_uid_p1)&(~grouped_p1.bintix_uid.isin(brand_uid_p1))].bintix_uid.unique()\n        p2_uid_only_others = grouped_p2.loc[grouped_p2.bintix_uid.isin(other_uid_p2)&(~grouped_p2.bintix_uid.isin(brand_uid_p2))].bintix_uid.unique()\n        p1_unwanted_uid = set(p1_uid_only_others)-set(brand_uid_p2)\n        p2_unwanted_uid = set(p2_uid_only_others)-set(brand_uid_p1)\n        grouped_p1_filtered = grouped_p1.loc[~grouped_p1.bintix_uid.isin(p1_unwanted_uid)]\n        grouped_p2_filtered = grouped_p2.loc[~grouped_p2.bintix_uid.isin(p2_unwanted_uid)]\n        merged = pd.merge(grouped_p1_filtered,grouped_p2_filtered,on=[\"bintix_uid\",\"brand_other\"],how=\"outer\").sort_values(by=\"bintix_uid\")\n\n        #Segmenting\n        new_uid = set(brand_uid_p2) - set(brand_uid_p1) - set(other_uid_p1)\n        lapsed_uid = set(brand_uid_p1) - set(brand_uid_p2) - set(other_uid_p2)\n        volume_change_uid = set(set(brand_uid_p1)-set(other_uid_p1)).intersection(set(brand_uid_p2)-set(other_uid_p2))\n        switch_and_vol_change_uid = set(merged.bintix_uid) - new_uid - lapsed_uid - volume_change_uid\n        possible_dropped_repertoire_uid = set(brand_uid_p1) - set(brand_uid_p2)\n        possible_added_repertoire_uid = set(brand_uid_p2) - set(brand_uid_p1)\n\n\n        #tagging\n        merged.loc[merged.bintix_uid.isin(new_uid),[\"type_of_inc_dec\"]] = \"new\"\n        merged.loc[merged.bintix_uid.isin(lapsed_uid),[\"type_of_inc_dec\"]] = \"lapsed\"\n        merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y>merged.volume_x),[\"type_of_inc_dec\"]] = \"increase_buying\"\n        merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y<merged.volume_x),[\"type_of_inc_dec\"]] = \"decrease_buying\"\n        merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y==merged.volume_x),[\"type_of_inc_dec\"]] = \"same_buying\"\n        merged.loc[merged.bintix_uid.isin(switch_and_vol_change_uid),[\"type_of_inc_dec\"]]=\"switching and volume change\"\n\n\n        #calculating gain\/loss\n        gain_new = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"new\")].volume_y.sum()\/1000\n        loss_lapsed = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"lapsed\")].volume_x.sum()\/1000\n        gain_inc_buying_solus = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_x.sum())\/1000\n        loss_dec_buying_solus = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_x.sum())\/1000\n        merged.volume_x.fillna(0,inplace=True)\n        merged.volume_y.fillna(0,inplace=True)\n        gain_switch_and_vol_change = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_x.sum())\/1000\n        loss_switch_and_vol_change = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_x.sum())\/1000\n\n\n        #switching_pattern\n        all_purchases = pd.concat([p1_purchases,p2_purchases])\n        all_wanted_uid_purchases = all_purchases.loc[~all_purchases.bintix_uid.isin(p1_unwanted_uid.union(p2_unwanted_uid))]\n        if len(ref_brand)==1:\n            all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand\n        if len(ref_brand)==2:\n            all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand\n        if len(ref_brand)==3:\n            all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand + \"-\" + all_wanted_uid_purchases.variant\n    #     if len(ref_brand)==4\n        unique_brand_sub_brand = all_wanted_uid_purchases.brand_sub_brand.unique()\n        gain_due_to = dict()\n        loss_due_to = dict()\n        gain_due_to[\"increased_buying\"] = 0\n        loss_due_to[\"decreased_buying\"] = 0\n        added_to_repertoire = 0\n        dropped_from_repertoire = 0\n\n        for a in unique_brand_sub_brand:\n            gain_due_to[a] = 0\n            loss_due_to[a] = 0\n        for uid in list(switch_and_vol_change_uid):\n            p1_uid_purchase = p1_purchases.loc[p1_purchases.bintix_uid == uid]\n            if len(ref_brand)==1:\n                p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand\n            if len(ref_brand)==2:\n                p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand\n            if len(ref_brand)==3:\n                p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand + \"-\" + p1_uid_purchase.variant\n    #         if len(ref_brand)==4\n            p1_uid_grouped = p1_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n\n            p2_uid_purchase = p2_purchases.loc[p2_purchases.bintix_uid == uid]\n            if len(ref_brand)==1:\n                p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand\n            if len(ref_brand)==2:\n                p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand\n            if len(ref_brand)==3:\n                p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand + \"-\" + p2_uid_purchase.variant\n    #         if len(ref_brand)==4\n            p2_uid_grouped = p2_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n\n            merged_switching = p1_uid_grouped.merge(p2_uid_grouped, on = \"brand_sub_brand\", how = \"outer\")\n            merged_switching.fillna(0,inplace=True)\n            merged_switching.loc[:,[\"volume_diff\"]] = merged_switching.volume_y - merged_switching.volume_x\n            vol_diff_uid = merged_switching.volume_diff.sum()\n\n            if vol_diff_uid<0:\n                merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"decreased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n            elif vol_diff_uid>0:\n                merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"increased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n            merged_switching.loc[:,[\"sign\"]] = np.where(merged_switching.volume_diff<0,\"negative\",\"non_negative\") \n            sign_desired_brand = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"sign\"]\n            factor_for_assigning_gain_loss = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"volume_diff\"]\/merged_switching.loc[merged_switching.sign == sign_desired_brand].volume_diff.sum()\n            if sign_desired_brand == \"negative\":\n                for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                    if (a==\"decreased_buying\") and (uid in possible_dropped_repertoire_uid):\n                        dropped_from_repertoire += merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                    else:\n                        loss_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n            else:\n                for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                    if (a==\"increased_buying\") and (uid in possible_added_repertoire_uid):\n                        added_to_repertoire +=  merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                    else:\n                        gain_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n\n        for a in unique_brand_sub_brand:\n            if gain_due_to[a]==0 and loss_due_to[a]==0:\n                del gain_due_to[a]\n                del loss_due_to[a]\n\n        #adjustment. gain\/loss due to volume change seperated from switching\n        dropped_from_repertoire = abs(dropped_from_repertoire)\/1000\n        added_to_repertoire = abs(added_to_repertoire)\/1000\n        gain_switch = gain_switch_and_vol_change - abs(gain_due_to[\"increased_buying\"])\/1000 - added_to_repertoire\n        loss_switch = loss_switch_and_vol_change - abs(loss_due_to[\"decreased_buying\"])\/1000 - dropped_from_repertoire\n        gain_inc_buying = gain_inc_buying_solus + abs(gain_due_to[\"increased_buying\"])\/1000\n        loss_dec_buying = loss_dec_buying_solus + abs(loss_due_to[\"decreased_buying\"])\/1000\n\n\n        del gain_due_to[\"increased_buying\"]\n        del loss_due_to[\"decreased_buying\"]\n\n        #results\n        gain_brand_switch_list = [x*factor_p1_p2\/1000 for x in gain_due_to.values()]\n        loss_brand_switch_list = [x*factor_p1_p2\/1000 for x in loss_due_to.values()]\n        gain = [x*factor_p1_p2 for x in [gain_new,gain_inc_buying,gain_switch,added_to_repertoire]]\n        loss= [x*factor_p1_p2 for x in [-loss_lapsed,-loss_dec_buying,-loss_switch,-dropped_from_repertoire]]\n        net_gain = factor_p1_p2*(p2_purchases.loc[p2_purchases.brand_other==\"brand\"].volume.sum() - p1_purchases.loc[p1_purchases.brand_other==\"brand\"].volume.sum())\n\n\n        #Intercation Index\n        interaction_df = pd.DataFrame({\"Brands\":gain_due_to.keys(),\"abs_gain\":gain_brand_switch_list,\"abs_loss\":[abs(x) for x in loss_brand_switch_list]})\n        interaction_df.loc[:,[\"total_gain_loss_brand\"]] = interaction_df.abs_gain + interaction_df.abs_loss\n        interaction_df.loc[:,[\"percent_gain_loss\"]] = (interaction_df.total_gain_loss_brand\/interaction_df.total_gain_loss_brand.sum())*100\n        volume_share_list=[]\n        if len(ref_brand)==1:\n            all_purchases_without_brand = all_purchases.loc[~(all_purchases.brand==ref_brand[0])]\n            all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand \n        if len(ref_brand)==2:\n            all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1]))]\n            all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand\n        if len(ref_brand)==3:\n            all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1])&(all_purchases.variant==ref_brand[2]))]\n            all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand + \"-\" + all_purchases_without_brand.variant\n    #     if len(ref_brand)==4\n\n        for a in gain_due_to.keys():\n            volume_share_list+=[(all_purchases_without_brand.loc[all_purchases_without_brand.brand_sub_brand==a].volume.sum()\/all_purchases_without_brand.volume.sum())*100]\n        interaction_df.loc[:,[\"percent_volume_share_adjusted\"]] = volume_share_list\n        interaction_df.loc[:,[\"interaction_index\"]] = round((interaction_df.percent_gain_loss\/interaction_df.percent_volume_share_adjusted)*100)\n\n        #presenting final numbers in the format used by Kelloggs's \n        gain_loss_segments = [\"Entry To\/Lapse From Category\",\"Incr\/Decr in Cons.of Ref.Brand\",\"Total Shift To\/From Ref. Brand\",\"Addn.\/Deletion from Repertoire\"]\n        gain_loss_segments_net = [(gain[i]+loss[i])\/1000 for i in range(4)]\n        gain_loss_segments_net_df = pd.DataFrame(data=[[gain_loss_segments[i],gain_loss_segments_net[i]] for i in range(4)],columns=[\"Type\/Brand\",\"net vol in 000Kgs\"])\n\n        net_df = pd.DataFrame(data=[[\"Brand Net Shift\",net_gain\/1000\/1000]],columns= [\"Type\/Brand\",\"net vol in 000Kgs\"])\n\n        interaction_df_trimmed = interaction_df.loc[:,[\"Brands\",\"interaction_index\"]]\n        interaction_df_trimmed.rename(columns = {'Brands':'Type\/Brand'}, inplace = True)\n        interaction_df_trimmed.loc[:,[\"net vol in 000Kgs\"]] = [(gain_brand_switch_list[i] + loss_brand_switch_list[i])\/1000 for i in range(len(gain_brand_switch_list))]\n        interaction_df_trimmed = interaction_df_trimmed[[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"]]\n        interaction_df_trimmed = interaction_df_trimmed.sort_values(by=\"net vol in 000Kgs\")\n        \n        interaction_df_trimmed.loc[:,\"net vol in 000Kgs\"] = interaction_df_trimmed[\"net vol in 000Kgs\"]\n        gain_loss_segments_net_df.loc[:,\"net vol in 000Kgs\"] = gain_loss_segments_net_df[\"net vol in 000Kgs\"]\n        net_df.loc[:,\"net vol in 000Kgs\"] = net_df[\"net vol in 000Kgs\"]\n        \n        \n        top_info_df = pd.DataFrame(data = [[\"Reference_Brand: \" , \"-\".join(ref_brand)],[\"Time_Period: \", time_period_string]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\"])\n        middle_info_df_2 = pd.DataFrame(data = [[\"Brands\",\"net vol in 000Kgs\",\"interaction_index\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        middle_info_df_1 = pd.DataFrame(data = [[\"Type\",\"net vol in 000Kgs\",\"\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        empty_df = middle_info_df = pd.DataFrame(data = [[\"\",\"\",\"\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        \n        return pd.concat([top_info_df,empty_df,middle_info_df_1,net_df,gain_loss_segments_net_df,empty_df,middle_info_df_2,interaction_df_trimmed]).rename(columns = {'interaction_index': '',\"Type\/Brand\": \"Type\"}).set_index('Type')\n    \n    \n    \n    \n    \n    else:\n        net_df = pd.DataFrame(data=[[\"Brand Net Shift\",0]],columns= [\"Type\/Brand\",\"net vol in 000Kgs\"])\n\n        gain_loss_segments_net_df = pd.DataFrame()\n        interaction_df = pd.DataFrame()\n        all_purchases_without_brand = pd.DataFrame()\n        \n        for i in range(len(city_id_list)): \n            city_wise_result = multi_city_gnl_helper(p1_start_month,p1_end_month,p2_start_month,p2_end_month,ref_brand,[city_id_list[i]],category_dict,raw_data)\n            net_df.loc[:,\"net vol in 000Kgs\"].iloc[0]+=city_wise_result[0]\n           \n            if i == 0:\n                gain_loss_segments_net_df = city_wise_result[1]\n            else:\n                gain_loss_segments_net_df = pd.merge(gain_loss_segments_net_df,city_wise_result[1], on = \"Type\/Brand\")\n                gain_loss_segments_net_df.loc[:,\"net vol in 000Kgs\"] = gain_loss_segments_net_df[\"net vol in 000Kgs_x\"] + gain_loss_segments_net_df[\"net vol in 000Kgs_y\"]\n                gain_loss_segments_net_df = gain_loss_segments_net_df [[\"Type\/Brand\",\"net vol in 000Kgs\"]]\n                \n            if i==0:\n                all_purchases_without_brand = city_wise_result[2]\n            else:\n                all_purchases_without_brand = pd.concat([all_purchases_without_brand,city_wise_result[2]])\n            \n            if i == 0:\n                interaction_df = city_wise_result[3]\n            else:\n                interaction_df = pd.merge(interaction_df,city_wise_result[3], on = \"Brands\", how = \"outer\")\n                interaction_df = interaction_df.fillna(0)\n                interaction_df.loc[:,\"net vol in 000Kgs\"] = interaction_df[\"net vol in 000Kgs_x\"] + interaction_df[\"net vol in 000Kgs_y\"]\n                interaction_df.loc[:,\"total_gain_loss_brand\"] = interaction_df[\"total_gain_loss_brand_x\"] + interaction_df[\"total_gain_loss_brand_y\"]\n                interaction_df = interaction_df [[\"Brands\",\"net vol in 000Kgs\",\"total_gain_loss_brand\"]]\n         \n        all_purchases_without_brand = all_purchases_without_brand[[\"brand_sub_brand\",\"volume\"]].groupby(\"brand_sub_brand\").volume.sum()\n        interaction_df = pd.merge(interaction_df,all_purchases_without_brand,left_on = \"Brands\", right_on = \"brand_sub_brand\", how=\"left\")\n        interaction_df.loc[:,[\"percent_gain_loss\"]] = (interaction_df.total_gain_loss_brand\/interaction_df.total_gain_loss_brand.sum())*100\n        interaction_df.loc[:,[\"percent_volume_share_adjusted\"]] = (interaction_df.volume\/interaction_df.volume.sum())*100   \n        interaction_df.loc[:,[\"interaction_index\"]] = round((interaction_df.percent_gain_loss\/interaction_df.percent_volume_share_adjusted)*100)\n        interaction_df.loc[:,\"net vol in 000Kgs\"] = interaction_df[\"net vol in 000Kgs\"]\/1000\n        \n        interaction_df_trimmed = interaction_df[[\"Brands\",\"net vol in 000Kgs\",\"interaction_index\"]]\n        interaction_df_trimmed.rename(columns = {'Brands':'Type\/Brand'}, inplace = True)\n        interaction_df_trimmed = interaction_df_trimmed.sort_values(by=\"net vol in 000Kgs\")\n        \n        interaction_df_trimmed.loc[:,\"net vol in 000Kgs\"] = interaction_df_trimmed[\"net vol in 000Kgs\"]\n        gain_loss_segments_net_df.loc[:,\"net vol in 000Kgs\"] = gain_loss_segments_net_df[\"net vol in 000Kgs\"]\n        net_df.loc[:,\"net vol in 000Kgs\"] = net_df[\"net vol in 000Kgs\"]\n        \n        top_info_df = pd.DataFrame(data = [[\"Reference_Brand: \" , \"-\".join(ref_brand)],[\"Time_Period: \", time_period_string]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\"])\n        middle_info_df_2 = pd.DataFrame(data = [[\"Brands\",\"net vol in 000Kgs\",\"interaction_index\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        middle_info_df_1 = pd.DataFrame(data = [[\"Type\",\"net vol in 000Kgs\",\"\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        empty_df = middle_info_df = pd.DataFrame(data = [[\"\",\"\",\"\"]],columns=[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"])\n        \n        return pd.concat([top_info_df,empty_df,middle_info_df_1,net_df,gain_loss_segments_net_df,empty_df,middle_info_df_2,interaction_df_trimmed]).rename(columns = {'interaction_index': '',\"Type\/Brand\": \"Type\"}).set_index('Type')\n        \n    ","e8f510af":"#new code for kellogg's \nkelloggs_template_gnl = [\n# [[\"brand\",\"KELLOGG'S\"],[\"scat\",\"READY TO EAT CEREALS\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"],[\"variant\",\"REAL ALMOND & HONEY\"]],\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"],[\"variant\",\"ORIGINAL\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\",\"FROOT LOOPS INDIA\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\"],[\"variant\",False,\"FILLS\"]], #for excluding use False\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"FROOT LOOPS INDIA\"]] ,  #too little data. \n# [[\"brand\",\"KELLOGG'S\"],[\"attribute1\",\"FRIENDS OF CHOCOS\"]], \n# [[\"brand\",\"KELLOGG'S\"],[\"sscat\",\"MUESLI & GRANOLA\"]] ,\n[[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"MUESLI\"]] #,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"GRANOLA\"]]# , # use new data. older data has the name - CRUNCHY GRANOLA\n# [[\"sscat\",\"OATS & PORRIDGE\"],[\"sub_brand\",\"OATS\"]] ,#if no brand then keep first indicator which you want to use as a brand\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"OATS\"]] ,\n# [[\"brand\",\"SOULFULL\"], [\"sub_brand\",\"MUESLI\"]] ,\n# [[\"brand\",\"SOULFULL\"],[\"sub_brand\",\"RAGI BITES\"]],\n# [[\"brand\",\"YOGA BAR\"],[\"sub_brand\",\"MUESLI\"]],\n# [[\"brand\",\"BAGRRY'S\"],[\"sub_brand\",\"MUESLI\"]] ,\n# [[\"brand\",\"BAGRRY'S\"],[\"sub_brand\",\"CORN FLAKES\"]]\n]\n# kelloggs_template_gnl =[ [[\"brand\",\"NUTRALITE\"],[\"sub_brand\",\"DOODH SHAKTI\"]] ]\ndef only_strings_list_from_1(any_list):\n    return [x for x in any_list[1:] if type(x) == str ]\n\ndef replace_false_with_ex(any_list):\n    return [\"excluding\" if x==False else x for x in any_list]\n\ndef adding_quotes_to_string(string):\n    return \"\\\"\" + string + \"\\\"\"\n\ndef new_sub_brand_name(indicator):\n    indicator_with_excl = [replace_false_with_ex(a) for a in indicator ]\n    return \"-\".join([\"_\".join([indicator_with_excl[j][i] for i in range(1,len(indicator_with_excl[j]))]) for j in range(1,len(indicator_with_excl)) ])\n\ndef new_brand_name(indicator):\n    return \"_\".join([indicator[0][i] for i in range(1,len(indicator[0]))])\n\ndef filters_for_loc(indicator):\n    fl=[]\n    for condition in indicator:\n        if condition[1]:\n            fl += [ \"(data.\" + condition[0] + \".isin([\" + \",\".join([adding_quotes_to_string(x) for x in only_strings_list_from_1(condition)]) +\"]))\" ]\n        else:\n            fl+= [\"~(data.\" + condition[0] + \".isin([\" + \",\".join([adding_quotes_to_string(x) for x in only_strings_list_from_1(condition)]) +\"]))\"]\n    return \"&\".join(fl)\n\ndef new_names_data_frame(indicator,data):\n    #don't change the name data\n    data.loc[eval(filters_for_loc(indicator)),\"brand\"] = new_brand_name(indicator)\n    data.loc[eval(filters_for_loc(indicator)),\"sub_brand\"] = new_sub_brand_name(indicator)\n    return data\n","c25ca7bf":"# brand_for_GNL = [[\"KELLOGG'S\",\"CORN FLAKES\"],[\"KELLOGG'S\",\"CHOCOS\"]] #[brand] or [brand,sub_brand] or [brand,sub_brand,variant]\np1_start = 31 # month_adj\np1_end = 33 # month_adj\np2_start = 34 # month_adj\np2_end = 36 # month_adj\n\ncategory_info = {\"scat\":\".isin(['READY TO EAT CEREALS','OATS & PORRIDGE'])\"} # specify categories, scat, sscat to choose for which GNL to be analysed\ncomplete_data = df #complete data (all cities,cat\/scat\/sscat\/ time - p1 and p2 atleast) is required. Not filtered one. \ncity_name_list =[([1,2,3,4,5,6],\"6Metros\"),([1],\"Hyd\")] #city\/city combination for sheet generation\n# ([1,2,3,4,5,6],\"6Metros\"),([1],\"Hyd\"),([2],\"Blr\"),([3],\"Del\"),([4],\"Mum\"),,([6],\"Che\"),([5],\"Kol\")","c97aff25":"pip install openpyxl","80dd7c05":"pip install xlsxwriter","6adfb6c1":"def style_sheet(kelloggs_template_gnl):\n    \n    last_col = (len(kelloggs_template_gnl)*4)-2\n    \n    align_format = workbook.add_format({'align':'center','valign':'vcenter','text_wrap':False})\n    worksheet.set_column(0,last_col,None,align_format)\n    \n    worksheet.set_default_row(17)\n    worksheet.set_column(0,last_col,42)\n    \n    worksheet.freeze_panes(2,0) \n    \ndef style_table(i,last_row):\n    \n    format_header = workbook.add_format({'bg_color':'#FDE9D9','bold':True,'border':2})\n    worksheet.conditional_format(0, 4*i, 1, 4*i+1, {'type':'no_errors','format':format_header})\n    \n    format_header_2 = workbook.add_format({'bg_color':'#DAEEF3','bold':True,'border':1})\n    worksheet.conditional_format(3, 4*i, 3, 4*i+1, {'type':'no_errors','format':format_header_2})\n    worksheet.conditional_format(10, 4*i, 10, 4*i+2, {'type':'no_errors','format':format_header_2})\n    \n    format_index_1 = workbook.add_format({'bg_color':'#EBF1DE','bold':True,'border':1})\n    worksheet.conditional_format(4, 4*i, 8, 4*i, {'type':'no_errors','format':format_index_1})\n    \n    format_index_2 = workbook.add_format({'bg_color':'#F2DCDB','bold':True,'border':1})\n    worksheet.conditional_format(11, 4*i, last_row, 4*i, {'type':'no_errors','format':format_index_2})\n    \n    format_entry_1 = workbook.add_format({'bg_color':'#EBF1DE','bold':False,'border':1,'num_format':'0.00'})\n    worksheet.conditional_format(4, 4*i+1, 8, 4*i+1, {'type':'no_errors','format':format_entry_1})    \n    \n    format_entry_2 = workbook.add_format({'bg_color':'#F2DCDB','bold':False,'border':1,'num_format':'0.00'})\n    worksheet.conditional_format(11, 4*i+1, last_row, 4*i+1, {'type':'no_errors','format':format_entry_2})\n    \n    format_entry_3 = workbook.add_format({'bg_color':'#F2DCDB','bold':False,'border':1,'num_format':'0'})\n    worksheet.conditional_format(11, 4*i+2, last_row, 4*i+2, {'type':'no_errors','format':format_entry_3})","f6a144e1":"month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",0:\"DEC\"}\np1_start_string = month_num_dict[p1_start%12]+str(19+(p1_start-1)\/\/12)\np1_end_string = month_num_dict[p1_end%12]+str(19+(p1_end-1)\/\/12)\np2_start_string = month_num_dict[p2_start%12]+str(19+(p2_start-1)\/\/12)\np2_end_string = month_num_dict[p2_end%12]+str(19+(p2_end-1)\/\/12)\ntime_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n\nwith pd.ExcelWriter(\"GNL_\"+\"KELLOGG'S_\"+time_period_string+\".xlsx\") as writer: # remove kellogg's later                                                                                                   \n    for c in city_name_list:\n        df_final_city = pd.DataFrame()\n        for i in range(len(kelloggs_template_gnl)):\n            complete_data_copy = complete_data.copy()\n            changed_brand_sub_brand_df = new_names_data_frame(kelloggs_template_gnl[i],complete_data_copy)\n            version_1_df = GNL_analysis(p1_start,p1_end,p2_start,p2_end,[new_brand_name(kelloggs_template_gnl[i]),new_sub_brand_name(kelloggs_template_gnl[i])],c[0],category_info,changed_brand_sub_brand_df)\n            version_1_df.reset_index(level=0,inplace=True)\n            version_1_df.to_excel(writer, sheet_name=c[1],startcol=i*4,header=False,index=False)\n            workbook = writer.book #extra\n            # Get Sheet1\n            worksheet = writer.sheets[c[1]] #extra           \n            style_table(i,version_1_df.shape[0]-1)\n\n        workbook = writer.book #extra\n        # Get Sheet1\n        worksheet = writer.sheets[c[1]] #extra\n        style_sheet(kelloggs_template_gnl) #extra\n            \n            \n            \n            #             if i==0:                                                  \n#                 df_final_city = EER_analysis(changed_brand_sub_brand_df,p1_start,p1_end,p2_start,p2_end,new_brand_name(kelloggs_template_gnl[i]),new_sub_brand_name(kelloggs_template_gnl[i]),c[0])\n#             else:\n#                 df_final_city = pd.concat([df_final_city,EER_analysis(changed_brand_sub_brand_df,p1_start,p1_end,p2_start,p2_end,new_brand_name(kelloggs_template_gnl[i]),new_sub_brand_name(kelloggs_template_gnl[i]),c[0])],axis=1)\n#         df_final_city.to_excel(writer, sheet_name=c[1])","e761d219":"# city_time_filtered = complete_data.loc[complete_data.month_adj.isin([30,31,32,33,34,35])]\n# all_months_in_total_period = [30,31,32,33,34,35]\n# sample_bintix_uid = set(city_time_filtered.loc[city_time_filtered.month_adj.isin(all_months_in_total_period[:2])].bintix_uid.unique()).intersection(set(city_time_filtered.loc[city_time_filtered.month_adj.isin(all_months_in_total_period[3:6])].bintix_uid.unique()))\n# # complete_data_fil1 = city_time_filtered.loc[city_time_filtered.bintix_uid.isin(sample_bintix_uid)]\n# complete_data_fil1 = city_time_filtered\n# p1_pur_rtec = complete_data_fil1.loc[(complete_data_fil1.scat.isin(['READY TO EAT CEREALS','OATS & PORRIDGE']))&(complete_data_fil1.month_adj.isin([30,31,32]))]\n# p2_pur_rtec = complete_data_fil1.loc[(complete_data_fil1.scat.isin(['READY TO EAT CEREALS','OATS & PORRIDGE']))&(complete_data_fil1.month_adj.isin([33,34,35]))]\n# p1_kel_mues_id = p1_pur_rtec.loc[(p1_pur_rtec.brand==\"KELLOGG'S\") & (p1_pur_rtec.sub_brand==\"MUESLI\")].bintix_uid.unique()\n# p2_kel_mues_id = p2_pur_rtec.loc[(p2_pur_rtec.brand==\"KELLOGG'S\") & (p2_pur_rtec.sub_brand==\"MUESLI\")].bintix_uid.unique()\n# kel_mues_lapsers = set(p1_kel_mues_id) - set(p2_kel_mues_id)\n# p1_pur_rtec_lapsers = p1_pur_rtec.loc[p1_pur_rtec.bintix_uid.isin(kel_mues_lapsers)]\n# p2_pur_rtec_lapsers =  p2_pur_rtec.loc[p2_pur_rtec.bintix_uid.isin(kel_mues_lapsers)]\n# # p2_pur_rtec_lapsers.loc[p2_pur_rtec_lapsers.city_id==1].volume.sum() - p1_pur_rtec_lapsers.loc[p1_pur_rtec_lapsers.city_id==1].volume.sum()\n# # p1_pur_rtec_lapsers.loc[(p1_pur_rtec_lapsers.city_id==1)&(p1_pur_rtec_lapsers.brand==\"KELLOGG'S\") & (p1_pur_rtec_lapsers.sub_brand==\"MUESLI\")].groupby(\"bintix_uid\").volume.sum().mean()\/3\n# pd.concat([p1_pur_rtec_lapsers,p2_pur_rtec_lapsers]).to_csv(\"without_condition_lapsers_check_skew.csv\")","6fcb1112":"list({1:2}.keys())[0]","60298686":"# Client specific processing","d7de5969":"# Enter the input","f9539833":"# Supporting function\n* For filtering data based on continous bintix uids\n* For calculating new factor","563a58a6":"# Main function","b735392c":"# Output generation"}}