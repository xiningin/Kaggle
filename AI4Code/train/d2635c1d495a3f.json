{"cell_type":{"45504bd5":"code","335f15b3":"code","6c316827":"code","fe4df929":"code","fec115b5":"code","43710b49":"code","8644f284":"code","879e5eb9":"code","1a7cbef0":"code","e14be493":"code","86e47df5":"code","ef0a5e38":"code","b468ab62":"code","6ae4a657":"code","9a5dc537":"code","ee37c7e6":"code","a71ae61e":"code","adc9ebaf":"code","15c95494":"code","c30dab3d":"code","e6c61ce4":"code","fb959a97":"code","c05cd47f":"code","9afc1b22":"code","523d1503":"code","06e7acd5":"code","47c24d0b":"code","a0b1e842":"markdown","fa600730":"markdown","8bf079e2":"markdown","5c5c4c13":"markdown","f09ee0cd":"markdown","9959cdbf":"markdown","a761703e":"markdown","3e8dac48":"markdown"},"source":{"45504bd5":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score","335f15b3":"data=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","6c316827":"data","fe4df929":"#removing the data\ny=data['diagnosis']\ndata.drop(['id','Unnamed: 32','diagnosis'],axis=1,inplace=True)\nx=data\ny = y.replace({'M':0,'B':1})","fec115b5":"print(x.shape,y.shape)","43710b49":"#### Train Test Split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=0, stratify=y)","8644f284":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","879e5eb9":"print(y_train.value_counts())\nprint(y_test.value_counts())","1a7cbef0":"from sklearn.model_selection import GridSearchCV","e14be493":"%%time\nnEstimator = [10,11,12,13,14,15,16]\ndepth = [5,10,20,30,40,50,60]\ncriterion=['entropy', 'gini']\nmin_samples_leaf=[1, 2, 5, 10]\nmin_samples_split=[2, 5, 10, 15]\nmax_features = ['auto', 'sqrt','log2']\n\nRF = RandomForestClassifier()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth,'criterion':criterion,'max_features': max_features,'min_samples_leaf':min_samples_leaf,'min_samples_split':min_samples_split}]\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='f1_weighted',n_jobs=-1)\ngsv.fit(x_train,y_train)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)","86e47df5":"y_pred=gsv.predict(x_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(8,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","ef0a5e38":"from sklearn.model_selection import RandomizedSearchCV","b468ab62":"%%time\nnEstimator = [10,11,12,13,14,15,16]\ndepth = [5,10,20,30,40,50,60]\ncriterion=['entropy', 'gini']\nmin_samples_leaf=[1, 2, 5, 10]\nmin_samples_split=[2, 5, 10, 15]\nmax_features = ['auto', 'sqrt','log2']\n\nRF = RandomForestClassifier()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth,'criterion':criterion,'max_features': max_features,'min_samples_leaf':min_samples_leaf,'min_samples_split':min_samples_split}]\nrsv = RandomizedSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='f1_weighted',n_jobs=-1)\nrsv.fit(x_train,y_train)\nprint(\"Best HyperParameter: \",rsv.best_params_)\nprint(rsv.best_score_)","6ae4a657":"y_pred=rsv.predict(x_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(8,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","9a5dc537":"%%time\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nspace = {\n    \"n_estimators\" : hp.choice('n_estimators',[10,11,12,13,14,15,16]),\n        \"max_depth\" : hp.choice('max_depth',[5,10,20,30,40,50,60]),\n        \"criterion\" : hp.choice(\"criterion\",['entropy', 'gini']),\n        \"min_samples_leaf\" : hp.choice(\"min_samples_leaf\",[1, 2, 5, 10]),\n        \"min_samples_split\" : hp.choice(\"min_samples_split\",[2, 5, 10, 15]),\n        \"max_features\" : hp.choice(\"max_features\",['auto', 'sqrt','log2'])\n    }\n\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], \n                                   max_depth = space['max_depth'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, x_train, y_train, cv =4).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }\n    \ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 20,\n            trials= trials)\nbest","ee37c7e6":"criterion = {0: 'entropy', 1: 'gini'}\n#feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nestimator = {0: '10', 1: '11', 2: '12', 3: '13', 4: '14',5:'15',6:'16'}\nmax_depth={0:5,1:10,2:20,3:30,4:40,5:50,6:60}\nmin_samples_leaf={0:1, 1:2, 2:5, 3:10}\nmin_samples_split={0:2, 1:5, 2:10, 3:15}\nmax_features={0:'auto', 1:'sqrt',2:'log2'}","a71ae61e":"print(\"criterion\",criterion[best['criterion']])\nprint(\"n_estimators\",estimator[best['n_estimators']])\nprint(\"max_depth\",max_depth[best['max_depth']])\nprint(\"min_samples_leaf\",min_samples_leaf[best['min_samples_leaf']])\nprint(\"min_samples_split\",min_samples_split[best['min_samples_split']])\nprint(\"max_features\",max_features[best['max_features']])","adc9ebaf":"random_forest = RandomForestClassifier(criterion ='entropy',\n                                       n_estimators = 15,\n                                       max_depth = 60, \n                                       min_samples_leaf =1 , \n                                       min_samples_split =5,\n                                       max_features='auto'\n                                       )\nrandom_forest.fit(x_train,y_train)","15c95494":"#predictionforest = random_forest.predict(x_train)\ny_pred=random_forest.predict(x_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(8,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","c30dab3d":"%%time\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [10,11,12,13,14,15,16]\nmax_depth = [5,10,20,30,40,50,60]\ncriterion=['entropy', 'gini']\nmin_samples_leaf=[1, 2, 5, 10]\nmin_samples_split=[2, 5, 10, 15]\nmax_features = ['auto', 'sqrt','log2']\n\n\nparam = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(param)","e6c61ce4":"from tpot import TPOTClassifier\n\n\ntpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(x_train,y_train)","fb959a97":"y_pred=tpot_classifier.predict(x_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(8,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","c05cd47f":"\nn_estimators = [10,11,12,13,14,15,16]\nmax_depth = [5,10,20,30,40,50,60]\ncriterion=['entropy', 'gini']\nmin_samples_leaf=[1, 2, 5, 10]\nmin_samples_split=[2, 5, 10, 15]\nmax_features = ['auto', 'sqrt','log2']\n\n\n\nimport optuna\nimport sklearn.svm\ndef objective(trial):\n\n    classifier = 'RandomForest'\n    \n    n_estimators = trial.suggest_int('n_estimators', 10, 17,1)\n    max_depth = int(trial.suggest_categorical('max_depth', [5,10,20,30,40,50,60]))\n    criterion = trial.suggest_categorical('criterion', ['entropy', 'gini'])\n    min_samples_leaf = int(trial.suggest_categorical('min_samples_leaf',[1, 2, 5, 10]))\n    min_samples_split = int(trial.suggest_categorical('min_samples_split', [2, 5, 10, 15]))\n    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt','log2'])\n\n    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, \n                                                  max_depth=max_depth,\n                                                 criterion=criterion,\n                                                  min_samples_leaf=min_samples_leaf,\n                                                  min_samples_split=min_samples_split,\n                                                  max_features=max_features\n                                                 )\n\n    return sklearn.model_selection.cross_val_score(\n        clf,x_train,y_train, n_jobs=-1, cv=3).mean()","9afc1b22":"%%time\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","523d1503":"study.best_params","06e7acd5":"rf=RandomForestClassifier(n_estimators= 10,\n                         max_depth= 30,\n                        criterion= 'entropy',\n                         min_samples_leaf= 2,\n                         min_samples_split= 2,\n                         max_features= 'log2')\nrf.fit(x_train,y_train)","47c24d0b":"y_pred=rf.predict(x_test)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(8,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","a0b1e842":"## 3.RandomSearchCV\n<p style=\"font-size:120%;\">\nRandomized search on hyper parameters.Train and evaluating each model and selecting the hyper parameters which produces the best results\n<p>\n<p style=\"font-size:120%;\">\n\n<b style=\"font-size:120%;\">Important GridSearchCV argument<\/b><br>\n<ul style=\"font-size:120%;\">\n    <li><b>Model-<\/b>Choose the model which you want to pass like-Random forest,decision tree etc<br><\/li>\n    <li><b>param_grid-<\/b>Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries<br><\/li>\n    <li><b>scoring<\/b>-Strategy to evaluate the performance of the cross-validated model on the test set<br><\/li>\n    <li><b>n_jobs<\/b>-Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.<br><\/li>\n    <li><b>cv<\/b>-Determines the cross-validation splitting strategy. Possible inputs for cv are:<\/li>\n        <ul><li>None, to use the default 5-fold cross validation<\/li>\n        <li>integer, to specify the number of folds in a (Stratified)KFold<\/li>\n        <li>CV splitter<\/li>\n        <li>An iterable yielding (train, test) splits as arrays of indices.<\/li>\n            <\/ul>\n    \n<\/ul>\n\n<p>\n","fa600730":"## 4. Bayesian Optimization\n<p style=\"font-size:120%;\">\nBayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time. In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin.\n<ul style=\"font-size:120%;\">\n    <li> <b>Objective Function<\/b>-defines the loss function to minimize.<\/li>\n    <li><b>Domain Space<\/b> - defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).<\/li>\n    <li><b>Optimization Algorithm<\/b> - defines the search algorithm to use to select the best input values to use in each new iteration.<\/li>\n\n<\/ul>\n<b>Library use for hyperopt<\/b> <br>\nhttps:\/\/hyperopt.github.io\/hyperopt\/\n<\/p>\n","8bf079e2":"## 5.Genetic Algorithm\n<p style=\"font-size:120%;\">\nGenetic algorithm use the concept of  process of natural selection,mutation, crossover and selection.<br><br>\nIn genetic algorithm we take the offspring of best performing models\nLet's imagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again calculate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.\nuse tpot\n<\/p>\n\n![genetic2.gif](attachment:2ac2eda7-a781-491e-8bae-62b62227880b.gif)\n\n<i>image source-https:\/\/mctrans.ce.ufl.edu\/featured\/TRANSYT-7F\/release9\/genetic2.gif<\/i>","5c5c4c13":"## 6.Optuna\n<p style=\"font-size:120%;\">\nOptuna is an automatic hyperparameter optimization framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n\n<\/p>","f09ee0cd":"<b style=\"font-size:120%;\"> Splitting data into test train<\/b>","9959cdbf":"### Data Preparation\n<b style=\"font-size:120%;\">importing data<\/b>","a761703e":"## 2.GridSearchCV\n<p style=\"font-size:120%;\">\nThis is a brute-force Mehod.In this techinque we try all the possible hyper-parameter provided.Train and evaluating each model\nand selecting the hyper-parameter which produces the best results <br>\n<p>\n<p style=\"font-size:120%;\">\n\n<b style=\"font-size:120%;\">Important GridSearchCV argument<\/b><br>\n<ul style=\"font-size:120%;\">\n    <li><b>Model-<\/b>Choose the model which you want to pass like-Random forest,decision tree etc<br><\/li>\n    <li><b>param_grid-<\/b>Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries<br><\/li>\n    <li><b>scoring<\/b>-Strategy to evaluate the performance of the cross-validated model on the test set<br><\/li>\n    <li><b>n_jobs<\/b>-Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.<br><\/li>\n    <li><b>cv<\/b>-Determines the cross-validation splitting strategy. Possible inputs for cv are:<\/li>\n        <ul><li>None, to use the default 5-fold cross validation<\/li>\n        <li>integer, to specify the number of folds in a (Stratified)KFold<\/li>\n        <li>CV splitter<\/li>\n        <li>An iterable yielding (train, test) splits as arrays of indices.<\/li>\n            <\/ul>\n    \n<\/ul>\n\n<p>","3e8dac48":"> ![introduction_image.png](attachment:e66348d3-6d58-4ea1-9425-0698d2c4588e.png)\n# Table of Content\n<ul style=\"font-size:120%;\">\n    <li>1. Introduction<\/li>\n    <ul>\n   <li>1.1. what is hyperparameter tuning <\/li>\n    <li> 1.2. Need of hyperparameter tuning <\/li>\n    <li> 1.3. Process of hyperparameter tuning <\/li>\n    <\/ul>\n<li>2. Grid search<\/li>\n<li>3. Random search<\/li>\n<li> 4. Bayesian Optimization<\/li>\n<li>5. Genetic Algorithms<\/li>\n<li>6. using Optuna<\/li>\n<\/ul>\n\n<b>dataset used<\/b> <br>\nbreast-cancer-wisconsin-data\n*  link-https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n* No of rows-569 \n* No of columns-33 \n\n## 1.Introduction\n### 1.1 What is hyperparameter tuning\n<p style=\"font-size:120%;\">\nwhen we are working on Machine learning problem most often when it comes to Model we don't known the optimal hyperparameter.we should choose the parameter based upon dataset<br>\n<b>hyperparameter<\/b>-whose value is set before the learning process begins.\n<br>\n<br>\n<b>Example<\/b><br>\n\u2022 What should be the maximum depth of Decision Tree? <br>\n\u2022 What should be the number of tree in Random Forest? <br>\n\u2022 What should be the minimum number of samples at a leaf node of decision tree? <br>\n\n<br>\nModel hyperparameter are learned from data and hyper-parameters are tuned to get the best fit.Manual Searching or hit-trail method can be tedious, hence search algorithms like grid searchm,random search & Bayesian Optimization are used.\n<p>\n \n### 1.2 Need of hyperparameter tuning\n<p style=\"font-size:120%;\"> \n\u2022 To avoid over-fitting <br>\n\u2022 To avoid under-fitting <br>\n\n### 1.3 Process of hyperparameter tuning\n<p style=\"font-size:120%;\"> \nIn model optimization we split the data into three part<br>\n\u2022 train-train the model on the given parameter<br>\n\u2022 cv-optimize the model's parameter values<br>\n\u2022 test-evaluate the optimized model<br>\n\n<br><br>\n![flow-hyperparameter.png](attachment:10b415cf-6a39-409f-9a64-a32ee2e14ae4.png)"}}