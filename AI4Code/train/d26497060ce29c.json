{"cell_type":{"05a86707":"code","12da3030":"code","86771f1d":"code","600ce740":"code","6d1ef377":"code","8ad4c520":"code","f55b858f":"code","86ab3605":"markdown","2e5f5a0b":"markdown","23afe372":"markdown"},"source":{"05a86707":"import tensorflow as tf\n\nimport sys\nsys.path.append('..\/input\/swintransformertf')\nfrom swintransformer import SwinTransformer","12da3030":"class CFG:\n    img_size = [384, 384]\n    model_name = 'swin_large_384'","86771f1d":"#custom function\nclass Mish(tf.keras.layers.Activation):\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\ntf.keras.utils.get_custom_objects().update({'Mish': Mish(mish)})","600ce740":"def get_centralized_gradients(optimizer, loss, params):\n    grads = []\n    for grad in K.gradients(loss, params):\n        grad_len = len(grad.shape)\n        if grad_len > 1:\n            axis = list(range(grad_len - 1))\n            grad -= tf.reduce_mean(grad,\n                                   axis=axis,\n                                   keep_dims=True)\n        grads.append(grad)\n\n    if None in grads:\n        raise ValueError('An operation has `None` for gradient. '\n                         'Please make sure that all of your ops have a '\n                         'gradient defined (i.e. are differentiable). '\n                         'Common ops without gradient: '\n                         'K.argmax, K.round, K.eval.')\n    if hasattr(optimizer, 'clipnorm') and optimizer.clipnorm > 0:\n        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n        grads = [\n            tf.keras.optimizers.clip_norm(\n                g,\n                optimizer.clipnorm,\n                norm) for g in grads]\n    if hasattr(optimizer, 'clipvalue') and optimizer.clipvalue > 0:\n        grads = [K.clip(g, -optimizer.clipvalue, optimizer.clipvalue)\n                 for g in grads]\n    return grads\n\n\ndef centralized_gradients_for_optimizer(optimizer):\n    def get_centralized_gradients_for_optimizer(loss, params):\n        return get_centralized_gradients(optimizer, loss, params)\n\n    return get_centralized_gradients_for_optimizer","6d1ef377":"def build_model(model_name=CFG.model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n    image_input = tf.keras.layers.Input(shape=(DIM,DIM,3))\n    #img_adjust_layer = tf.keras.layers.experimental.preprocessing.Resizing(DIM, DIM)\n    pretrained_model = SwinTransformer(CFG.model_name,include_top=False, pretrained=True, use_tpu=True)\n    base = tf.keras.Sequential([pretrained_model,\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense(64, activation='Mish'),\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense(1, activation='sigmoid')])\n    #x = img_adjust_layer(image_input)\n    x = base(image_input)\n    model = tf.keras.Model(inputs = image_input, outputs = x)\n    if compile_model:\n        #optimizer\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n        optimizer.get_gradients=centralized_gradients_for_optimizer(optimizer) \n        #loss\n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n        #metric\n        rmse = RMSE\n        model.compile(optimizer=optimizer,\n                      loss=loss,\n                      metrics=[rmse])\n    return model","8ad4c520":"tmp = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=False)\ntmp.summary()","f55b858f":"tmp.layers[1].summary()","86ab3605":"**The learning code I used is the same as below.**\n\nhttps:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-vit-cls-tpu-train\n\nHis notebook is very useful. Please keep that in mind.\n\nI modified only the build_model function.\n\nSeveral custom functions were used. -> Mish, Gradient Centralization\n\n**tf swin transformer**\n\nhttps:\/\/github.com\/rishigami\/Swin-Transformer-TF\n\nhttps:\/\/www.kaggle.com\/rishigami\/tpu-swin-transformer-tensorflow","2e5f5a0b":"**If there is an error in the input layer, add the resize layer.**","23afe372":"# define custom function\n**Gradient Centralization TensorFlow**\n\nhttps:\/\/github.com\/Rishit-dagli\/Gradient-Centralization-TensorFlow\n\nhttps:\/\/keras.io\/examples\/vision\/gradient_centralization\/\n\n**custom mish**\n\nhttps:\/\/www.kaggle.com\/imokuri\/mish-activation-function"}}