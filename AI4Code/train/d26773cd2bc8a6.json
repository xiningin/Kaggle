{"cell_type":{"fe603532":"code","3e826f30":"code","7fdf8b1d":"code","8be0dd1e":"code","fb71cf47":"code","14064a5b":"code","39e52461":"code","82c767af":"code","7356d26d":"code","3d248a0b":"code","82cb39dc":"code","1aaef71f":"code","252f9107":"code","7ba09439":"code","13475d3c":"code","4519f665":"code","a88fb9b0":"code","76b0cd0d":"code","199f7578":"code","4d997826":"code","f1a52942":"code","fd5c47d6":"code","29d75a9c":"code","b0910fcf":"code","0ff6bcf7":"markdown","d41da99d":"markdown","e80293cf":"markdown","52d451c9":"markdown","6a4abda5":"markdown","c99519c5":"markdown","f45ed309":"markdown","0c6149b6":"markdown","ab2516c2":"markdown"},"source":{"fe603532":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e826f30":"#import libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","7fdf8b1d":"df = pd.read_csv('..\/input\/fifa-18-sampledata\/fifa_18_sample_data.csv') #read data\ndf.head() # sample of the data","8be0dd1e":"df.info #info of the data (names, values, nulls, dtypes)","fb71cf47":"train_data = df[['age', 'special']] #get the data we need (only 2 columns)\ntrain_data ","14064a5b":"train_data.isnull().sum() #check the null","39e52461":"train_data.drop_duplicates() #drop any duplicates","82c767af":"from sklearn.cluster import KMeans #import kmeans cluster from sklearn","7356d26d":"cost = [] # array to hold the cost value for each number of cluster \n\n#try form 1 cluster to 15 cluster and calculate cost in each time\nfor i in range(1,16):\n    kmeans = KMeans(n_clusters = i, init = \"k-means++\") #make cluster using k mean each iteration with new number of clusters\n    kmeans.fit(train_data) #fit the data\n    print('Cost_Function=',kmeans.inertia_,'with', i, 'Clusters') #get the cost in this iteration\n    cost.append(kmeans.inertia_) #add the cost to the array\n    \n#plot the elbow plot    \nplt.plot(range(1, 16), cost) \nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('cost')\nplt.show()","3d248a0b":"kmeans = KMeans(n_clusters = 5, init = \"k-means++\", random_state = 42) #train the data using k-mean\ny_cls = kmeans.fit_predict(train_data) #fit the data and predict which cluster","82cb39dc":"#plot the clusters\nplt.figure(figsize=(15, 8)) #size of the plot\nplt.scatter(train_data['age'][y_cls == 0], train_data['special'][y_cls == 0], s = 100, c = 'red', label = 'Cluster 1') #cluster 1 y = 0\nplt.scatter(train_data['age'][y_cls == 1], train_data['special'][y_cls == 1], s = 100, c = 'blue', label = 'Cluster 2') #cluster 2 y = 1\nplt.scatter(train_data['age'][y_cls == 2], train_data['special'][y_cls == 2], s = 100, c = 'green', label = 'Cluster 3') #cluster 3 y = 2\nplt.scatter(train_data['age'][y_cls == 3], train_data['special'][y_cls == 3], s = 100, c = 'yellow', label = 'Cluster 4') #cluster 4 y = 3\nplt.scatter(train_data['age'][y_cls == 4], train_data['special'][y_cls == 4], s = 100, c = 'cyan', label = 'Cluster 5') #cluster 5 y = 4\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 250, c = 'purple', label = 'Centroids') #plot the centroid for each cluster\nplt.title('Clusters of players')\nplt.xlabel('age')\nplt.ylabel('special')\nplt.legend()\nplt.show()","1aaef71f":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch","252f9107":"plot1 = sch.dendrogram(sch.linkage(train_data, method = 'average'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distance')\nplt.show()","7ba09439":"model = AgglomerativeClustering(n_clusters = 5, linkage = 'average')\ncls_h1 = model.fit_predict(train_data)","13475d3c":"plt.figure(figsize=(15, 8)) #size of the plot\nplt.scatter(train_data['age'][cls_h1 == 0], train_data['special'][cls_h1 == 0], s = 100, c = 'red', label = 'Cluster 1') #cluster 1 y = 0\nplt.scatter(train_data['age'][cls_h1 == 1], train_data['special'][cls_h1 == 1], s = 100, c = 'blue', label = 'Cluster 2') #cluster 2 y = 1\nplt.scatter(train_data['age'][cls_h1 == 2], train_data['special'][cls_h1 == 2], s = 100, c = 'green', label = 'Cluster 3') #cluster 3 y = 2\nplt.scatter(train_data['age'][cls_h1 == 3], train_data['special'][cls_h1 == 3], s = 100, c = 'yellow', label = 'Cluster 4') #cluster 4 y = 3\nplt.scatter(train_data['age'][cls_h1 == 4], train_data['special'][cls_h1 == 4], s = 100, c = 'cyan', label = 'Cluster 5') #cluster 5 y = 4","4519f665":"plot1 = sch.dendrogram(sch.linkage(train_data, method = 'single'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distance')\nplt.show()","a88fb9b0":"model = AgglomerativeClustering(n_clusters = 5, linkage = 'single')\ncls_h2 = model.fit_predict(train_data)","76b0cd0d":"plt.figure(figsize=(15, 8)) #size of the plot\nplt.scatter(train_data['age'][cls_h2 == 0], train_data['special'][cls_h2 == 0], s = 100, c = 'red', label = 'Cluster 1') #cluster 1 y = 0\nplt.scatter(train_data['age'][cls_h2 == 1], train_data['special'][cls_h2 == 1], s = 100, c = 'blue', label = 'Cluster 2') #cluster 2 y = 1\nplt.scatter(train_data['age'][cls_h2 == 2], train_data['special'][cls_h2 == 2], s = 100, c = 'green', label = 'Cluster 3') #cluster 3 y = 2\nplt.scatter(train_data['age'][cls_h2 == 3], train_data['special'][cls_h2 == 3], s = 100, c = 'yellow', label = 'Cluster 4') #cluster 4 y = 3\nplt.scatter(train_data['age'][cls_h2 == 4], train_data['special'][cls_h2 == 4], s = 100, c = 'cyan', label = 'Cluster 5') #cluster 5 y = 4","199f7578":"plot1 = sch.dendrogram(sch.linkage(train_data, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distance')\nplt.show()","4d997826":"model = AgglomerativeClustering(n_clusters = 5, linkage = 'ward')\ncls_h3 = model.fit_predict(train_data)","f1a52942":"plt.figure(figsize=(15, 8)) #size of the plot\nplt.scatter(train_data['age'][cls_h3 == 0], train_data['special'][cls_h3 == 0], s = 100, c = 'red', label = 'Cluster 1') #cluster 1 y = 0\nplt.scatter(train_data['age'][cls_h3 == 1], train_data['special'][cls_h3 == 1], s = 100, c = 'blue', label = 'Cluster 2') #cluster 2 y = 1\nplt.scatter(train_data['age'][cls_h3 == 2], train_data['special'][cls_h3 == 2], s = 100, c = 'green', label = 'Cluster 3') #cluster 3 y = 2\nplt.scatter(train_data['age'][cls_h3 == 3], train_data['special'][cls_h3 == 3], s = 100, c = 'yellow', label = 'Cluster 4') #cluster 4 y = 3\nplt.scatter(train_data['age'][cls_h3 == 4], train_data['special'][cls_h3 == 4], s = 100, c = 'cyan', label = 'Cluster 5') #cluster 5 y = 4","fd5c47d6":"plot1 = sch.dendrogram(sch.linkage(train_data, method = 'complete'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distance')\nplt.show()","29d75a9c":"model = AgglomerativeClustering(n_clusters = 5, linkage = 'complete')\ncls_h4 = model.fit_predict(train_data)","b0910fcf":"plt.figure(figsize=(15, 8)) #size of the plot\nplt.scatter(train_data['age'][cls_h4 == 0], train_data['special'][cls_h4 == 0], s = 100, c = 'red', label = 'Cluster 1') #cluster 1 y = 0\nplt.scatter(train_data['age'][cls_h4 == 1], train_data['special'][cls_h4 == 1], s = 100, c = 'blue', label = 'Cluster 2') #cluster 2 y = 1\nplt.scatter(train_data['age'][cls_h4 == 2], train_data['special'][cls_h4 == 2], s = 100, c = 'green', label = 'Cluster 3') #cluster 3 y = 2\nplt.scatter(train_data['age'][cls_h4 == 3], train_data['special'][cls_h4 == 3], s = 100, c = 'yellow', label = 'Cluster 4') #cluster 4 y = 3\nplt.scatter(train_data['age'][cls_h4 == 4], train_data['special'][cls_h4 == 4], s = 100, c = 'cyan', label = 'Cluster 5') #cluster 5 y = 4","0ff6bcf7":"### from the last plots we can see that from the linkage methods the shape of the clusters will change and it may be uot existent at all ","d41da99d":"### Now the data is clean, has no null, no duplicates and all of it is numerical. so we can start modeling now","e80293cf":"### use elbow method","52d451c9":"## modeling using H cluster","6a4abda5":"the model is trained let's visualize it","c99519c5":"### average linkage","f45ed309":"### **the data is clean already**","0c6149b6":"as shown in the plot the best number of cluster is 4 clusters","ab2516c2":"# start modeling\n"}}