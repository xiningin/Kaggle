{"cell_type":{"f6be0a95":"code","34f144bc":"code","510bbb69":"code","3bf20c5d":"code","0e0fb103":"code","293ab092":"code","897d5b88":"code","2441a3f3":"code","cb334a8e":"code","aca61d7f":"code","f3932fa4":"code","fd0486f1":"code","d16633ba":"code","1f6c80c0":"code","997a5fa6":"code","b81dcf13":"code","6fecf6ee":"code","f7a56f7d":"code","79e316d8":"code","ca529769":"markdown","c802ac33":"markdown","74689dc5":"markdown","8a301fb6":"markdown","d1847aa8":"markdown","1b4f45cc":"markdown","f0ed3808":"markdown"},"source":{"f6be0a95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%pylab inline\n#%pylab\nimport numpy as np\nfrom sklearn import datasets, svm, metrics\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, recall_score, accuracy_score,precision_score, f1_score, roc_curve, auc, make_scorer,roc_auc_score \nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy import stats\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import AdaBoostClassifier\n# import warnings filter\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=DeprecationWarning)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","34f144bc":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","510bbb69":"X_train = train.loc[:,\"pixel0\":\"pixel783\"]\nY_train = train.loc[:, \"label\"]","3bf20c5d":"X_test = test","0e0fb103":"RANDOM_STATE = 42\npca = PCA(n_components=0.95)\nkfold = StratifiedKFold(n_splits=2, shuffle = True)\nkfold","293ab092":"scoring = {'acc': 'accuracy'}","897d5b88":"#KNN\npipe_knn = Pipeline([('StdScaler', StandardScaler()),\n                     ('minmax', MinMaxScaler()),\n                     ('pca', pca),\n                     ('clf_knn', KNeighborsClassifier())])\n\n\n# KNN\nrand_list_KNN = {'clf_knn__n_neighbors': sp_randint(1, 11),\n                 'clf_knn__algorithm': ['auto']}\n\ngs_KNN = RandomizedSearchCV(estimator = pipe_knn,\n                    param_distributions = rand_list_KNN,\n                    cv = kfold,\n                    verbose = 2,\n                    n_jobs = -1,\n                    n_iter = 5)\n\n\nscores_knn = cross_validate(gs_KNN, X_train, Y_train, cv = 5, scoring = scoring)\n\nprint('Cross Validation Scores:')\nprint('Accuracy: %.4f +\/- %.4f' % (np.mean(scores_knn['test_acc']), np.std(scores_knn['test_acc'])))","2441a3f3":"scores_knn","cb334a8e":"# Naive Bayes\n\npipe_nb = Pipeline([('pca', pca),\n                    ('clf_nb', GaussianNB())])\n\n\nrand_list_nb = {\n                \"clf_nb__var_smoothing\": np.logspace(-9, 0, 5)\n               }\n\n\n\ngs_NB = RandomizedSearchCV(estimator = pipe_nb,\n                    param_distributions = rand_list_nb,\n                    cv = kfold,\n                    verbose = 2,\n                    n_jobs = -1,\n                    n_iter = 5)\n\n\n\nscores_nb = cross_validate(gs_NB, X_train, Y_train, cv = 5, scoring = scoring)\n\nprint('Cross Validation Scores:')\nprint('Accuracy: %.4f +\/- %.4f' % (np.mean(scores_nb['test_acc']), np.std(scores_nb['test_acc'])))","aca61d7f":"scores_nb","f3932fa4":"# random forest\n\npipe_rf = Pipeline([('pca', pca),\n                    ('clf', RandomForestClassifier(n_estimators=100,\n                      class_weight = 'balanced'))])\n\n\nrand_list_rf = {\"clf__max_depth\": [5, None],\n              \"clf__max_features\": sp_randint(1, 11),\n              \"clf__min_samples_split\": sp_randint(2, 11),\n              \"clf__min_samples_leaf\": sp_randint(1, 11),\n              \"clf__bootstrap\": [True, False],\n              \"clf__criterion\": [\"gini\", \"entropy\"]}\n\n\n\ngs_RF = RandomizedSearchCV(estimator = pipe_rf,\n                    param_distributions = rand_list_rf,\n                    cv = kfold,\n                    verbose = 2,\n                    n_jobs = -1,\n                    n_iter = 5)\n\n\n\nscores_rf = cross_validate(gs_RF, X_train, Y_train, cv = 5, scoring = scoring)\n\nprint('Cross Validation Scores:')\nprint('Accuracy: %.4f +\/- %.4f' % (np.mean(scores_rf['test_acc']), np.std(scores_rf['test_acc'])))","fd0486f1":"scores_rf","d16633ba":"# AdaBoost\npipe_ada = Pipeline([('pca', pca),\n                    ('clf_ada', AdaBoostClassifier())])\n\n\nrand_list_ADA = {'clf_ada__learning_rate': np.logspace(-2, 2, 10),\n                 'clf_ada__n_estimators': [100]\n                 }\n\ngs_ADA = RandomizedSearchCV(estimator = pipe_ada,\n                    param_distributions = rand_list_ADA,\n                    cv = kfold,\n                    verbose = 2,\n                    n_jobs = -1,\n                    n_iter = 5)\n\nscores_ada = cross_validate(gs_ADA, X_train, Y_train, cv = 5, scoring = scoring)\n\nprint('Cross Validation Scores:')\nprint('Accuracy: %.4f +\/- %.4f' % (np.mean(scores_ada['test_acc']), np.std(scores_ada['test_acc'])))","1f6c80c0":"scores_ada","997a5fa6":"gs_KNN.fit(X_train.values, Y_train.values.ravel())\ny_pred = gs_KNN.predict(X_test)","b81dcf13":"y_pred","6fecf6ee":"gs_KNN.best_params_","f7a56f7d":"submissions = pd.DataFrame({\"ImageId\": range(1,len(y_pred)+1), \"Label\": y_pred})\nfilename = 'MNIST_digits_predictions.csv'\nsubmissions.to_csv(filename, index=False, header=True)\nprint('Saved file: ' + filename)","79e316d8":"submissions.head","ca529769":"## Random Forest","c802ac33":"## KNN","74689dc5":"**Algorithms Comparison**\n\nWe compare 4 algorithms:\n\n* KNN\n* Random forests\n* Naive Bayes\n* Adaboost ensemble\n\nThe training set is used for nested cross validation, in order to estimate the performance of each algorithm on unseen data, based on the average cross validation scores of accuracy.\n\nThe nested cross validation procedure consists of two loops [1]:\n\n*  an inner k-fold loop (k=2, stratified folding) that splits the data into training and validation sets and selects the best model parameters\n*  an outer k-fold loop (k = 5) that splits the data into training and test folds\n\nThe outer loop employs Scikit's method *cross_validate*, providing the metric scores per fold. We calculate the average and standard deviation of the results for each algorithm.<br>\n\nThe inner loop employs Scikit's *RandomizedSearchCV* to select the best model parameters drawn from specified distributions. In this exercise, *RandomizedSearchCV* is preferred over *GridSearchCV*, in order to speed up the tuning process.<br>\n\nStratified folding is used on the outer loop to preserve the ratio of the classes in each fold.<br>\n\nRegarding data processing, we use Scikit's *Pipelines()* that include both preprocessing steps, such as scaling (whenever appropriate) and dimensionality reduction, as well as the final model fitting. Pipelines ensure that data processing takes place per training fold during cross validation, avoiding any data leakage to the whole training set.<br>\n\nPCA is used for reducing the number of features. Principal components selection preserves 95% variance of the data, using Scikit's *'PCA'* method [PCA(n_components = 0,95)]. We do not select the components manually.<br>\n\n\n**Algorithm Selection and Final Evaluation**\n\nKNN algorithm achieved the best CV score.\nTherefore, we train the model on the whole training set using RandomizedSearchCV. The optimal hyperparameter is *number of nearest neighbors = 4*.\n\nReferences:\n[1]: Python Machine Learning - Third Edition by Vahid Mirjalili, Sebastian Raschka, Publisher: Packt Publishing, Release Date: December 2019","8a301fb6":"## Winning algorithm: KNN","d1847aa8":"## Submission of predictions","1b4f45cc":"## Naive Bayes","f0ed3808":"## AdaBoost Classifier"}}