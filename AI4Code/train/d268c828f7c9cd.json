{"cell_type":{"fb3e93ba":"code","e5b7665d":"code","e2f8ff31":"code","147cd88b":"code","be5c4efb":"code","98e0716b":"code","9daba44a":"code","07b15912":"code","51f1d3e2":"code","683db607":"code","b1bea465":"code","bdc3e850":"code","e54187f2":"code","7a08c2b1":"code","6bcf8f66":"code","7a8d9bb3":"code","ac03d17c":"code","e33fb753":"code","ff719c9f":"code","0b7905d2":"code","76a56c0f":"code","94a81f67":"code","addecdcf":"code","8d0c51e8":"code","d2c3bc06":"code","2ec9c980":"code","df9d7bd1":"code","7c51e9c0":"code","379cd68c":"code","fe2c3c4a":"code","062aa742":"code","9085928e":"code","37c62064":"code","a9ea8dd4":"code","ca9f592c":"code","a4c538a9":"markdown","44830303":"markdown","53772cf3":"markdown","e0bebdc0":"markdown","f41dfeff":"markdown","841cef8b":"markdown","bd04cc5f":"markdown"},"source":{"fb3e93ba":"\nfrom sklearn.datasets import load_boston\n\nboston_dataset = load_boston()\n\nimport numpy as np \nimport pandas as pd \n\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nimport pandas as pd  \nimport seaborn as sns \n\n%matplotlib inline\n","e5b7665d":"\nboston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()","e2f8ff31":"#The target values is missing from the data. Create a new column of target values and add it to dataframe\n\n\nboston['MEDV'] = boston_dataset.target","147cd88b":"boston.head()","be5c4efb":"boston.info()","98e0716b":"boston.describe()","9daba44a":"boston.nunique()","07b15912":"# to check null values\n\nboston.isnull().sum()","51f1d3e2":"boston['MEDV'].plot()","683db607":"# to check skewnes\nfrom scipy.stats import skew\nboston['MEDV'].skew()","b1bea465":"100\nnp.log1p(100)","bdc3e850":"(np.log1p(boston['MEDV'])).skew()","e54187f2":"# compute the pair wise correlation for all columns  \ncorrelation_matrix = boston.corr().round(2)","7a08c2b1":"plt.figure(figsize=(10,6))\nsns.heatmap(data=correlation_matrix, annot=True)","6bcf8f66":"\na = [1,2,3,]\n\nplt.figure(figsize=(20, 5))\n\nfeatures = ['LSTAT', 'RM']\ntarget = boston['MEDV']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = boston[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('MEDV')","7a8d9bb3":"X = boston.drop(columns=['RAD','MEDV'])\nY = boston['MEDV']","ac03d17c":"\nfrom sklearn.model_selection import train_test_split\n\n# splits the training and test data set in 80% : 20%\n# assign random_state to any value.This ensures consistency.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","e33fb753":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)","ff719c9f":"# model evaluation for testing set\n\ny_test_predict = lin_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","0b7905d2":"from  sklearn.linear_model import Lasso","76a56c0f":"L_model = Lasso(alpha=0.1)","94a81f67":"L_model.fit(X_train, Y_train)","addecdcf":"# model evaluation for testing set\n\nL = L_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","8d0c51e8":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nlasso_reg = Lasso()\nparameters = {\"alpha\": [0.001,0.01,0.1,0.3,0.5,0.8,1,4,9,10,30],\n              \"fit_intercept\": [True, False],\n             }\ngrid = GridSearchCV(estimator=lasso_reg, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, Y_train)","d2c3bc06":"grid.best_params_","2ec9c980":"grid.best_score_","df9d7bd1":"L_model = Lasso(alpha=0.3)\nL_model.fit(X_train, Y_train)\n# model evaluation for testing set\n\ny_test_predict = L_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","7c51e9c0":"from sklearn.linear_model import Ridge\nR_model = Ridge()\nR_model.fit(X_train, Y_train)\n# model evaluation for testing set\n\nr = R_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","379cd68c":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nlasso_reg = Ridge()\nparameters = {\"alpha\": [0.001,0.01,0.1,0.3,0.5,0.8,1,4,9,10,30],\n              \"fit_intercept\": [True, False],\n             }\ngrid = GridSearchCV(estimator=lasso_reg, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, Y_train)","fe2c3c4a":"grid.best_params_","062aa742":"R_model = Ridge(alpha=10)\nR_model.fit(X_train, Y_train)\n# model evaluation for testing set\n\ny_test_predict = R_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","9085928e":"from sklearn.linear_model import ElasticNet\nE_model = Ridge()\nE_model.fit(X_train, Y_train)\n# model evaluation for testing set\n\ne = E_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","37c62064":"final = L*0.1 + r*0.6 + e*0.3","a9ea8dd4":"# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, final)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, final)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","ca9f592c":"# Difference between L1 and L2 regularization\n# L1 Regularization\n# L1 penalizes sum of absolute value of weights.\n# L1 has a sparse solution\n# L1 has multiple solutions\n# L1 has built in feature selection\n# L1 is robust to outliers\n# L1 generates model that are simple and interpretable but cannot learn complex patterns\n\n\n# L2 Regularization\n# L2 regularization penalizes sum of square weights.\n# L2 has a non sparse solution\n# L2 has one solution\n# L2 has no feature selection\n# L2 is not robust to outliers\n# L2 gives better prediction when output variable is a function of all input features\n# L2 regularization is able to learn complex data patterns","a4c538a9":"**\nObservations\n\nFrom the above coorelation plot we can see that MEDV is strongly correlated to LSTAT, RM\n\nRAD and TAX are stronly correlated, so we don't include this in our features together to avoid multi-colinearity**","44830303":"**Limitations of R-Squared\nR-squared will give you an estimate of the relationship between movements of a dependent variable based on an independent variable's movements. It doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-square isn't necessarily good or bad, as it doesn't convey the reliability of the model, nor whether you've chosen the right regression. You can get a low R-squared for a good model, or a high R-square for a poorly fitted model, and vice versa.**","53772cf3":"**The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data\u2013how close the observed data points are to the model's predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. ... Lower values of RMSE indicate better fit.**","e0bebdc0":"**Lets use Grid Search **","f41dfeff":"# Ridge","841cef8b":"# LASSO","bd04cc5f":"# Elastic-Net"}}