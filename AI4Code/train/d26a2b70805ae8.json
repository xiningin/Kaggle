{"cell_type":{"2922ca48":"code","6337ba57":"code","bc7d82c2":"code","19038dc4":"code","a0bb13fc":"code","7d45a8c6":"code","69b0554b":"code","49ff66f0":"code","505c2b2f":"code","c9738580":"code","d53f364d":"code","0b437271":"code","783e358d":"code","cf36908f":"code","eb41406b":"code","90dcd5f5":"code","a5abfe0c":"code","ed59d735":"code","84674138":"code","d89887f6":"code","dd76c30c":"code","395e52e2":"code","4c416275":"code","6294a7cb":"code","ae453b76":"code","6556ce57":"code","bde756ac":"code","f822223c":"code","79d76a67":"code","7e9ef335":"code","b8e16ea0":"code","c04ee437":"code","8aada106":"code","0696612b":"code","0cfa4cd5":"code","60e6cbf5":"code","762237af":"code","3b42117a":"markdown","55e43e27":"markdown","d5361672":"markdown","a106dec5":"markdown","f7456d85":"markdown","799e5c6f":"markdown","5e0f983f":"markdown","5aec79dc":"markdown","20458f6e":"markdown","a5baf202":"markdown","52c9de51":"markdown","8836e6f6":"markdown","ad1b9889":"markdown","13dc1a13":"markdown","b3e34905":"markdown","e8e3e7e3":"markdown","ba48a045":"markdown","3a168091":"markdown","e848dc35":"markdown","65fbcd32":"markdown","a8af7573":"markdown","6e492e27":"markdown","9ddca91e":"markdown"},"source":{"2922ca48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6337ba57":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndataset=pd.read_csv('..\/input\/BlackFriday.csv')\ndataset.head()","bc7d82c2":"dataset.info()","19038dc4":"dataset.describe()","a0bb13fc":"from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nimputer = imputer.fit(dataset.iloc[:, 9:11].values)\ndataset.iloc[:,9:11] = imputer.transform(dataset.iloc[:, 9:11].values)\ndataset.info() ","7d45a8c6":"dataset.drop(['User_ID','Product_ID'], axis=1, inplace=True)\ndataset.info()","69b0554b":"dataset.head()","49ff66f0":"dataset['Age']=(dataset['Age'].str.strip('+'))\n","505c2b2f":"dataset['Stay_In_Current_City_Years']=(dataset['Stay_In_Current_City_Years'].str.strip('+').astype('float'))","c9738580":"dataset.info()\ndataset.head()","d53f364d":"sns.heatmap(\n    dataset.corr(),\n    annot=True\n)\n","0b437271":"g = sns.FacetGrid(dataset,col=\"Stay_In_Current_City_Years\")\ng.map(sns.barplot, \"Marital_Status\", \"Purchase\");","783e358d":"sns.jointplot(x='Occupation',y='Purchase',\n              data=dataset, kind='hex'\n             )","cf36908f":"g = sns.FacetGrid(dataset,col=\"City_Category\")\ng.map(sns.barplot, \"Gender\", \"Purchase\");","eb41406b":"g = sns.FacetGrid(dataset,col=\"Age\",row=\"City_Category\")\ng.map(sns.barplot, \"Gender\", \"Purchase\");","90dcd5f5":"sns.violinplot(x='City_Category',y='Purchase',hue='Marital_Status',\n               data=dataset)","a5abfe0c":"fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(18,14))\nax = sns.pointplot(y='Product_Category_1', x='City_Category',hue='Age',\n                 data=dataset,\n                 ax=axes[0,0]\n                )\nax = sns.pointplot(y='Product_Category_2', x='City_Category',hue='Age',\n                 data=dataset,\n                 ax=axes[0,1]\n                )\nax = sns.pointplot(y='Product_Category_3', x='City_Category', hue='Age',\n                 data=dataset,\n                 ax=axes[1,0]\n                )\nax = sns.pointplot(y='Purchase', x='City_Category', hue='Age',\n                 data=dataset,\n                 ax=axes[1,1]\n                )\n","ed59d735":"#Dividing the data into test and train datasets\nX = dataset.iloc[:, 0:9].values\ny = dataset.iloc[:, 9].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","84674138":"X_train\n","d89887f6":"y_train","dd76c30c":"X_test\n","395e52e2":"y_test","4c416275":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_X_train = LabelEncoder()\nX_train\n\n","6294a7cb":"X_train[:, 0] = labelencoder_X_train.fit_transform(X_train[:, 0])\nX_train","ae453b76":"X_train[:, 1] = labelencoder_X_train.fit_transform(X_train[:, 1])\nX_train","6556ce57":"X_train[:, 3] = labelencoder_X_train.fit_transform(X_train[:, 3])\nX_train","bde756ac":"labelencoder_X_test = LabelEncoder()\nX_test","f822223c":"X_test[:, 0] = labelencoder_X_test.fit_transform(X_test[:, 0])\nX_test","79d76a67":"X_test[:, 1] = labelencoder_X_test.fit_transform(X_test[:, 1])\nX_test","7e9ef335":"X_test[:, 3] = labelencoder_X_test.fit_transform(X_test[:, 3])\nX_test","b8e16ea0":"# Feature Scaling of training and test set\nfrom sklearn.preprocessing import StandardScaler\nsc_X_train = StandardScaler()\nX_train = sc_X_train.fit_transform(X_train)\n\nsc_X_test = StandardScaler()\nX_test = sc_X_test.fit_transform(X_test)\n","c04ee437":"#Fitting the model\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_absolute_error\n","8aada106":"# compare MAE with differing values of max_leaf_nodes\ndef get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    mae = mean_absolute_error(y_test,y_pred)\n    return(mae)","0696612b":"for max_leaf_nodes in [5, 50, 100, 300, 500, 700, 800, 850]:\n    my_mae = get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n    ","0cfa4cd5":"y_test","60e6cbf5":"regressor = RandomForestRegressor(n_estimators=700, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","762237af":"y_pred","3b42117a":"Having encoded the features, in the next step we will scale all the features to avoid issues due to different measurement scales.","55e43e27":"* So, we focus on the first row of the visuaisation, i.e. City_Category_A and then on the bar for females. \n> There are two age groups that can be identified with higher purchase, 26-35 and 18-25. \nTherefore, apart from the male population of all the three city categories, females of City Category A in the above two identifies age groups can be identified as potential buyers for next time around.","d5361672":"> There are still some special characters, like (+) in the columns 'Age' and 'stay in Current City_Years, which need to be removed, before machine learning algorithms can be run later.","a106dec5":"Having listed down the insights from each step above, let us now move to the next stage of the project, i.e data modelling and predication of sales.","f7456d85":"Let us list down some points that can be addressed with the analsysis.\n1. Understanding the cutomers on the basis of their purchasing habits.\n2. Understanding the purchasing habits according to Age groups, Occuptation, City_Categories.\n3. The above segmented group of users can be then used to model the data and use to predict the purchase spend for each customer. \nLets dive in by understanding the data.\n\n","799e5c6f":"* As can be seen, we have managed to clean the columns to our reuirement and removed the '+' sign from the two columns. \n> At this stage, I will exploratory data analysis by visualising the data, in particular, by visualising the statistical relationship between the different variables.","5e0f983f":"**** There are null values in Product_category_2, Product_Category_3","5aec79dc":"Let us inspet each of the split datasets","20458f6e":"> It is difficult to conclude anything from the above visulaisation, but it might be useful to analyse if the trend shows something different across the different cities. ","a5baf202":"1.  First insight would be that most of the purchase is done between 5000-10000. \n2. Next important insight, would be the occupations that lead to highest purchases. In this case, it would be occupation 4, listed in the dataset, closely followed that by 0 and 7.\n> One can imagine that the store can run targeted advertiements next time around to people with above listed occupations as they more likely to spend within the above purchase range.\n\n**To get a better understanding, we will now analyse the purchase habits across the different city categories.**","52c9de51":"* Dropping the columns that intuitiey should not imapact the purchase outcome, i.e. User_ID and Product ID. ","8836e6f6":"* Ananlysis of Purchase capacity as a function of Marital Status across city categories does not show a definitive trend. It would lead to a lot of assumptions and might lead to wrong conlcusions.","ad1b9889":"> The key take aways from the above plot are the positive correlation coefficients of three features as a function of Purchase:\n* Occupation\n* Stay_In_Current_City_Years\n* Marital Status\n\n> Increase in any of the values for the above three features is likey to result in a higher purchase from the customer.","13dc1a13":"Picking one key highlight from the above visualisation:\n> The stark difference in the purchase acoss City_Categories for the Age Group of 55 and above. It is highest in City_Category_B, as compared to the other age groups which tend to show high purchase in City_Category_C. ","b3e34905":"> Doing the same steps for the X_test dataset","e8e3e7e3":"> Exploratory data analysis supported by data visualisations.","ba48a045":"Lets us see what our prdicted results look like.","3a168091":"* Let us inspect the data now.","e848dc35":"For X_rrain and X_test, there are categorical variables, which need to be encoded before they can be incorporated into the data model. We will convert each of the variable step by step and cross check our results.","65fbcd32":"Mean value of Product_Category_2 is 9.8 and that for Product_Category_3 is 12.6, which we will\nuse to fill the missing values in these two columns.","a8af7573":"1. Clearly people from City_Category C are showing higher purchase capacity as compared to the other two cities on average.\n2. For City_categories B and C, Males tend to dominate the purchasing, whereas it is the opposite for City Category_C, where Females tend to puchase more than men. It is a useful insight, and it be useful to oserve which age group of females does higher purchasing.","6e492e27":"Fitting the model with the best number of n_estimators, to avoid underfitting and overfitting.","9ddca91e":"* The dataset here is a sample of the transactions made in a retail store. \n* The store wants to know better the customer purchase behaviour against different products. \n* Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.\n* There are seven categorical variables to analyse.\n\n"}}