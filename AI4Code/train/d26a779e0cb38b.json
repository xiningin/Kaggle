{"cell_type":{"d07e778a":"code","730a3dc9":"code","d64718f6":"code","79394d7e":"code","869c003e":"code","c7479ddb":"code","7ebd040c":"code","4dd84cae":"code","1e33a24a":"code","906fce8a":"code","790fa621":"code","9394ee40":"code","acc608a1":"code","88cd1026":"code","bed7ce04":"code","53f31526":"code","381f707c":"code","0b89bdae":"code","6327c6eb":"code","b9684b54":"code","ed6b027f":"code","98d358fc":"code","a96b91fb":"code","9d6f1bdc":"code","6a6a8f7c":"code","902879a7":"code","414b062f":"code","24cea357":"code","eb4788fb":"code","7c9de16c":"code","73fa0bab":"code","d8d38441":"code","0c4bf7ea":"code","4cc838aa":"code","512727cf":"code","d1d35c51":"code","15e8b18f":"code","083fa88d":"code","ea1d6276":"code","6a95fa80":"code","181c6191":"code","79cc48c0":"code","0b3cf77f":"code","438fa4f5":"code","489b62f8":"code","f0c7ee3e":"code","dd19dcd1":"code","2e12c1e3":"code","1d978d42":"code","b6e00c48":"code","46cd5448":"code","aef69b91":"code","49e22513":"code","34781330":"code","81bdc50e":"code","68b92010":"code","13860ea2":"code","ff362f65":"code","78463813":"code","d3b59ab1":"code","4dddbbee":"code","64a0ea48":"code","60315dc5":"code","e144a629":"code","1b05aa14":"code","25086def":"code","4a69cbde":"code","ff7bb524":"code","f2c5321c":"code","4718cb34":"markdown","465e1c64":"markdown","a3920b3a":"markdown","5185c795":"markdown","135bff10":"markdown","a59c9dc4":"markdown","2b3437ac":"markdown","52edd60a":"markdown","75cec756":"markdown","bc08358a":"markdown","be214051":"markdown","01376e74":"markdown","5881408f":"markdown","93c759b1":"markdown","5009b404":"markdown","2f98ceff":"markdown","327d6fc4":"markdown","c6304ec1":"markdown","d5982ccd":"markdown","a8c4788c":"markdown","0db5d1b4":"markdown","e7b13b2c":"markdown","14d9be08":"markdown","4869d9d2":"markdown","2ba693cd":"markdown","a123261b":"markdown","074bdd4a":"markdown","3b1c92d3":"markdown","7599d724":"markdown","e46e9e91":"markdown","e0d800e4":"markdown","a0d82561":"markdown","6354f20c":"markdown","34d92d8a":"markdown","eee89258":"markdown","0e8b73d4":"markdown","7a6cd546":"markdown","bb2b45e0":"markdown","fb85b7b3":"markdown","85e3718d":"markdown","82875c7e":"markdown","8570b46e":"markdown","dd5ece80":"markdown","0087328d":"markdown","7a1a6da4":"markdown","cff61774":"markdown","010cec18":"markdown"},"source":{"d07e778a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer","730a3dc9":"df = pd.read_csv('..\/input\/bank-personal-loan-data\/Bank_Personal_Loan_Modelling_TB.csv')\ndf.head()","d64718f6":"df.info()\n# df.dtypes","79394d7e":"df.shape","869c003e":"df.describe()","c7479ddb":"dupes = df.duplicated()\nsum(dupes)\n","7ebd040c":"df.isnull().values.any()","4dd84cae":"df.Experience.median()","1e33a24a":"df['Experience'] = df['Experience'].replace([-1, -2, -3],  df.Experience.median())\n","906fce8a":"df.iloc[2618]  ## here earlier value was -3","790fa621":"titles = list(df.columns)\ntitles","9394ee40":"titles[9],titles[13] = titles[13],titles[9]\ntitles","acc608a1":"df = df[titles]\ndf","88cd1026":"columns = list(df)[1:-1] # Excluding Outcome column \ndf[columns].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2)); ","bed7ce04":"categorical_variables=[col for col in df.columns if df[col].nunique()<=5]\nprint(categorical_variables)\ncontinuous_variables=[col for col in df.columns if df[col].nunique()>5]\nprint(continuous_variables)","53f31526":"categorical_variables.remove(\"Personal Loan\")\nprint(categorical_variables)\ncontinuous_variables.remove(\"ID\")\nprint(continuous_variables)","381f707c":"fig=plt.figure(figsize=(20,10))\n#fig.subplots_adjust(wspace=0.4,hspace=0.4)\nfor i,col in enumerate(continuous_variables):\n    ax=fig.add_subplot(2,3, i+1)\n    sns.distplot(df[col])","0b89bdae":"fig=plt.figure(figsize=(20,10))\n#fig.subplots_adjust(wspace=0.4,hspace=0.4)\nfor i,col in enumerate(categorical_variables):\n    ax=fig.add_subplot(2,3, i+1)\n    sns.countplot(df[col])","6327c6eb":"df.corr()\n","b9684b54":"def plot_corr(df1, size=14):\n    corr = df1.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)","ed6b027f":"plot_corr(df)","98d358fc":"df[['Age','Experience','Personal Loan']].corr()\n","a96b91fb":"df[['ZIP Code','Personal Loan']].corr()\n","9d6f1bdc":"df.drop('ID',axis=1,inplace=True)","6a6a8f7c":"df.drop('Experience', axis=1, inplace=True)\n","902879a7":"df.drop('ZIP Code', axis=1, inplace=True)\n","414b062f":"df.head()\n","24cea357":"n_true = len(df.loc[df['Personal Loan'] == True])\nn_false = len(df.loc[df['Personal Loan'] == False])\nprint(\"Number of true cases: {0} ({1:2.2f}%)\".format(n_true, (n_true \/ (n_true + n_false)) * 100 ))\nprint(\"Number of false cases: {0} ({1:2.2f}%)\".format(n_false, (n_false \/ (n_true + n_false)) * 100))","eb4788fb":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Personal Loan', axis =1)  # Predictor feature columns (10 X m)\nY = df['Personal Loan'] # # Predicted varible (1=True, 0=False) (1 X m)\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=1)\n# random state is just for any random seed\n\nx_train.head()","7c9de16c":"print(\"{0:0.2f}% data is in training set\".format((len(x_train)\/len(df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(x_test)\/len(df.index)) * 100))","73fa0bab":"or_T = len(df.loc[df['Personal Loan']==1])\nor_F = len(df.loc[df['Personal Loan']==0])\n\ntr_T = len(y_train[y_train[:] == 1])\ntr_F = len(y_train[y_train[:] == 0])\n\nts_T = len(y_test[y_test[:] == 1])\nts_F = len(y_test[y_test[:] == 0])\n\nprint(\"Original Personal Loan True Values    : {0} ({1:0.2f}%)\".format(or_T, (or_T\/len(df.index))*100))\nprint(\"Original Personal Loan False Values   : {0} ({1:0.2f}%)\".format(or_F, (or_F\/len(df.index)) * 100))\nprint(\" \")\nprint(\"Training Data Personal Loan True Values : {0} ({1:0.2f}%)\".format(tr_T, (tr_T\/len(y_train))*100))\nprint(\"Taining Data Personal Loan False Values : {0} ({1:0.2f}%)\".format(tr_F, (tr_F\/len(y_train))*100))\nprint(\" \")\nprint(\"Testing Data Personal Loan True Values : {0} ({1:0.2f}%)\".format(ts_T, (ts_T\/len(y_test))*100))\nprint(\"Testing Data Personal Loan False Values : {0} ({1:0.2f}%)\".format(ts_F, (ts_F\/len(y_test))*100))","d8d38441":"x_train.head()\n","0c4bf7ea":"from sklearn.impute import SimpleImputer\n\ncols=x_train.columns\n\nx_train = pd.DataFrame(x_train)\nx_test = pd.DataFrame(x_test)\n\nx_train.columns = cols\nx_test.columns = cols\n\nx_train.head()\n","4cc838aa":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\n# fit the model on train\nmodel = LogisticRegression(solver = 'liblinear')\nmodel.fit(x_train, y_train)\n# prediction on test data\ny_predict = model.predict(x_test)\n\ncoef_df = pd.DataFrame(model.coef_)\ncoef_df['intercept'] = model.intercept_\nprint(coef_df)","512727cf":"model_score = model.score(x_test, y_test)\nprint(model_score)","d1d35c51":"from sklearn.metrics import accuracy_score","15e8b18f":"accuracy_reg = accuracy_score(y_test, y_predict)\naccuracy_reg","083fa88d":"from sklearn.metrics import f1_score","ea1d6276":"f1_score_reg = f1_score(y_test, y_predict)\nf1_score_reg","6a95fa80":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predict))","181c6191":"cm = metrics.confusion_matrix(y_test, y_predict, labels=[1,0])\nprint(cm)","79cc48c0":"cm = metrics.confusion_matrix(y_test, y_predict, labels=[1,0])\ndf_cm = pd.DataFrame(cm, index= [i for i in[\"1\",\"0\"]],\n                    columns = [i for i in[\"Predict 1\", \"Predict 0\"]])\nplt.figure(figsize=(7,5))\nsns.heatmap(df_cm, annot=True, fmt='g')","0b3cf77f":"from sklearn.naive_bayes import GaussianNB\npl_model = GaussianNB()\npl_model.fit(x_train, y_train.ravel())","438fa4f5":"pl_train_predict = pl_model.predict(x_train)\n\nprint(\"Model Accuracy : {0:0.2f}\".format(accuracy_score(y_train, pl_train_predict)))","489b62f8":"pl_test_predict = pl_model.predict(x_test)\n\nprint(\"Model Accuracy : {0:0.2}\".format(accuracy_score(y_test, pl_test_predict)))","f0c7ee3e":"accuracy_nb = accuracy_score(y_test, pl_test_predict)\naccuracy_nb","dd19dcd1":"f1_score_nb = f1_score(y_test, pl_test_predict)\nf1_score_nb","2e12c1e3":"print(classification_report(y_test, pl_test_predict))","1d978d42":"cm = metrics.confusion_matrix(y_test, pl_test_predict, labels=[1,0])\nprint(cm)","b6e00c48":"df_cm = pd.DataFrame(cm, index= [i for i in[\"1\",\"0\"]],\n                    columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize=(7,5))\nsns.heatmap(df_cm, annot=True, fmt='g')","46cd5448":"\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\n\n","aef69b91":"df.head()","49e22513":"df.groupby(['Personal Loan']).count()\n","34781330":"X = df.drop('Personal Loan', axis =1)  # Predictor feature columns (10 X m)\nY = df['Personal Loan'] # # Predicted varible (1=True, 0=False) (1 X m)","81bdc50e":"xScaled = X.apply(zscore)\nxScaled.describe()","68b92010":"x_train, x_test, y_train, y_test = train_test_split(xScaled, Y, test_size = 0.3, random_state=1)\n# random state is just for any random seed","13860ea2":"NNH = KNeighborsClassifier(n_neighbors=5, weights='distance')\nNNH.fit(x_train,y_train)","ff362f65":"predicted_labels = NNH.predict(x_test)\nNNH.score(x_test, y_test)","78463813":"accuracy_knn = accuracy_score(y_test, predicted_labels)\naccuracy_knn","d3b59ab1":"f1_score_knn = f1_score(y_test, predicted_labels)\nf1_score_knn","4dddbbee":"print(classification_report(y_test, predicted_labels))\n","64a0ea48":"cm = metrics.confusion_matrix(y_test, predicted_labels, labels=[1,0])\nprint(cm)","60315dc5":"df_cm = pd.DataFrame(cm, index= [i for i in[\"1\",\"0\"]],\n                    columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize=(7,5))\nsns.heatmap(df_cm, annot=True, fmt='g')","e144a629":"print(\"               Logistic Regression Classification Report\")\nprint(\"----------------------------------------------------------------------\")\nprint(classification_report(y_test, y_predict))\nprint(\" \")\nprint(\"               Naive Bayes Classification Report\")\nprint(\"----------------------------------------------------------------------\")\nprint(classification_report(y_test, pl_test_predict))\nprint(\" \")\nprint(\"               K-NN Classification Report\")\nprint(\"----------------------------------------------------------------------\")\nprint(classification_report(y_test, predicted_labels))","1b05aa14":"model_list = ['Regression', 'Naive Bayes', 'K-NN']\nmodel_list","25086def":"model_accuracy_score = [accuracy_reg, accuracy_nb, accuracy_knn]\nmodel_accuracy_score","4a69cbde":"model_f1_score = [f1_score_reg, f1_score_nb, f1_score_knn]\nmodel_f1_score","ff7bb524":"fig,ax=plt.subplots(figsize=(10,8))\nsns.barplot(model_list,model_f1_score)\nax.set_title(\"F1 Score of  Test Data\")\nax.set_xlabel(\"Models\")\nax.set_ylabel(\"F1_Score\")\nplt.xticks(rotation=90)\n\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x+0.20, y + height + 0.01))","f2c5321c":"fig,ax=plt.subplots(figsize=(10,8))\nsns.barplot(model_list, model_accuracy_score)\nax.set_title(\"Accuracy Score of  Test Data\")\nax.set_xlabel(\"Models\")\nax.set_ylabel(\"Accuracy Score\")\nplt.xticks(rotation=90)\n\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x+0.20, y + height + 0.01))","4718cb34":"# Conclusion of Logistic Regression\nTrue Positive (TP) We correctly Predicted Customer buying Personal Loan : 76\n\nTrue Negative (TN) We correctly Predicted Customer not buying Personla Loan : 1338\n\nFasle Positive (FP) We incorrectly predicted that Customer buying Personal Loan : 13 (Type I error)\n\nFalse Negative (FN) We incorrectly predicted that customer not buying Personal Loan : 73 (Type II error)","465e1c64":"# Loading File and performing Basic EDA & Data Cleansing","a3920b3a":"We will use 2 varibles to stroe the accuracy score and f1- score. later we will use these variable in visualiztion in consoliadtion","5185c795":"# 2) Target Variable & Data Distribution in each attributes. (Visualiztion)\nWe will consider Personal Loan as Target variable as it is given in the problem deifnition. So this is the one which we need to predict. it is depndent variable.so we will remove it.\n\nAlso We will remove the coloumn ID as it is just as simple numbering for the customer identification and does not take part in the modelling\n\nThe Education column can be considered as Categorical variable as it has few values whch are given as\n\nUndergrad\nGraduate\nAdvanced\/Professional.\nAlso in dataset there are few columns which are having few unique values that can be considerd as categorical variable\n\nSwaping coloumns.\n\nAs Personal Loan is the dependent variable that we need to predict. We will simply move it to the last cloumns of datframe\n\n","135bff10":"# Identifying Missing Values","a59c9dc4":"True Positive (TP) We correctly Predicted Customer buying Personal Loan : 92\n\nTrue Negative (TN) We correctly Predicted Customer not buying Personla Loan : 1346\n\nFasle Positive (FP) We incorrectly predicted that Customer buying Personal Loan : 5 (Type I error)\n\nFalse Negative (FN) We incorrectly predicted that customer not buying Personal Loan : 57 (Type II error)\n\n# Selecting Best Model among the Regression, Naive Bayes and K-NN\nWe will check the each models Classification report. Also the variable which we used to capture the accuarcy scor and f1-score, we will use them for comparison and visualiztion. Then based on the comparision we can select the best model","2b3437ac":"# Performance of our model with testing data","52edd60a":"# Build KNN","75cec756":"There are no duplicate values in dataset","bc08358a":"there are 5000 rows and 14 columns in dataset","be214051":"# Supervised Learning Project","01376e74":"# confusion matrix","5881408f":"We swaped the coloumn poistion of Personal Loan with column CreditCard.","93c759b1":"# Import Libraries","5009b404":"# Identifying Invalid records or Non Standard missing Values\nAs we have already noted above the column Exprience is having neagtive entries\n\nWe can deal with this by Hard solution by simply droping those records.\n\nHowever if we do so then we can miss addtional information against these records like motrgare, income, ccredi card. so we will treat this coloumn as Non Standard missing values and deal it with the replacing missing values\n\nWe will replace the nagteive entires -1, -2, -3 with the median values","2f98ceff":"# Identifying Duplicates","327d6fc4":"# Checking rows and columns of data","c6304ec1":"# Conclusion: Selecting Best Model\nBest model is is K-NN with f1-score of 75% & Accuracy of 96%\n# We tried 3 diffrent classifications models\n\namongs which Regression models is having f1-score 64% & accuracy 94% and the Naive Bayes is with f1-score 47% & accuracy 87%\n\n# Clear Winner is K-NN","d5982ccd":"So we have in current data set, Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign\n\n# Splitting the Data\nWe will use 70% of data for training and 30% for testing","a8c4788c":"we can see Values for coloumns Experience are replced with median values (value with 20)","0db5d1b4":"Though we can see there are multiple columns with zero's but these are the No. i.e the records where customers not using Credit card , securities account, CD account, & online\n\n# A) Logistic Regression","e7b13b2c":"# Identify Correlation in data","14d9be08":"Model Tells us that 94.27% time prediction is right\n\n# Confusion Matrix","4869d9d2":"Age & Personal Loan is having some better corelation than Experience so we will drop Experience column","2ba693cd":"There are no missing values in file","a123261b":"# Performance of our model with training data","074bdd4a":"Here yellows are the maximum corelation","3b1c92d3":"Now lets check Personal Loan True\/False ratio in split data","7599d724":"The classification goal is to predict the likelihood of a liability customer buying personal loans By using classification models (Logistic, K-NN and Na\u00efve Bayes)","e46e9e91":"We will use 2 varibles to stroe the accuracy score and f1- score. later we will use these variable in visualiztion in consoliadtion","e0d800e4":"# Dropping Columns\nWe will drop the columns which does not take participation in modelling","a0d82561":"# Visualization for selecting best model","6354f20c":"# Data Prepartion\n# Check hidden missing values\nAs we checked missing values earlier but haven't got any. we will take one more glance of it","34d92d8a":"This will be new look of DataFrame which we will be using for Modelling. There are 11 coloumns\n\n# 3) Model Designing\nCalculate the Personal Loan ratio True\/False from Outcome Variable","eee89258":"# B) Naive Bayes\nTrain the Naive Bayes Algorithm","0e8b73d4":"Excluding coloumns ID & Personal Loan\n\nWe will create two variable Categorical & continous. As in dataset there are few columns which are having few unique values.\n\nThis will help us to simply further analysis of dataset and plotting graphs","7a6cd546":"By Loading file as datafarme and looking top five rows we get little idea about the data\n\nLet's Check each coloumn description and its imporatance\n\nAttribute Information:\n\nID : Customer ID\n\nAge : Customer's age in completed years\n\nExperience : Number of years of professional experience\n\nIncome : Annual income of the customer ($000)\n\nZIP Code : Home Address ZIP code\n\nFamily : Family size of the customer\n\nCCAvg : Avg. spending on credit cards per month ($000)\n\nEducation : Education Level\n\n        1. Undergrad\n        2. Graduate\n        3. Advanced\/Professional\n\nMortgage : Value of house mortgage if any. ($000)\n\nPersonal Loan : Did this customer accept the personal loan offered in the last campaign?\n\nSecurities Account : Does the customer have a securities account with the bank?\n\nCD Account : Does the customer have a certificate of deposit (CD) account with the bank?\n\nOnline : Does the customer use internet banking facilities?\n\nCredit card : Does the customer use a credit card issued by Thera Bank?\n\nThere are 480 records of customer who has accpeted the Personal Loan. ( it denoted in data file as 1)\n\nThere are 1469 customers who ar holding Thera Bank's Credit Card ( we are ssuming this is denoted by 1)\n\nThere 28 customers who posses the Thera bank Credit card but have Avarge spending is Zero\n\nAdditionally we can see there 100 customers whose spending over credit card is zero (included both who holds the Thera banks Credit card and other banks credit card)\n\nWe can also see there are negative entries in Datafile for the column Experience","bb2b45e0":"# Checking info","fb85b7b3":"There are more customers who are having family members 1\n\nLagre number of customers who are Undergraduate\n\nVery few customers are having securities acconut & CD account\n\nMost of the customers are using online banking\n\nMost of the customers does not posses the Thera bank Credit card","85e3718d":"# Evaluate Performance of K-NN Model","82875c7e":"Verifying values are replaced or not. We can pass the index location where earlier value was -1,-2, -3","8570b46e":"We can see that Age and Experience are uniformaly distributed and show a good similarities in distribution.\n\nIncome, CCAvg, Mortgage are positive Skew\n\nZIP code is negative Skew or it contain values from single region.\n\nMortgage contain most of the values as 0","dd5ece80":"True Positive (TP) We correctly Predicted Customer buying Personal Loan : 85\n\nTrue Negative (TN) We correctly Predicted Customer not buying Personla Loan : 1227\n\nFasle Positive (FP) We incorrectly predicted that Customer buying Personal Loan : 124 (Type I error)\n\nFalse Negative (FN) We incorrectly predicted that customer not buying Personal Loan : 64 (Type II error)\n\n# c) K-NN","0087328d":"We can see there is very low corelation of ZIP code with Personal Loan\n\nWe will drop below columns based on our corelation analysis and by looking its visualiztion\n\nID :- This is just the number sequnece given to the customers and loan can not provided just looking customer id\n\nExperience :- As per our analysis there is very low corelation between Experience and personal Loan\n\nZIP code :- As per the analysis there is very low corelation between ZIP code and personal Loan","7a1a6da4":"# Confusion Matrix","cff61774":"# 1) Understanding Data & Each column description and its importance\nFile Bank_Personal_Loan_Modelling.csv contains the 5000 customers data.\n\nThe data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan).\n\nIn past (historical records) states that Among these 5000 customers, only 480 (= 9.6%) customers accepted the personal loan that was offered to them in the earlier campaign.","010cec18":"Experience contains some negative values and experience can't be negative\n\nThe max of Income, Experience, CCAvg, Mortgage, Security Account, CD Accounts, CreditCard is much high than their mean. It means they contains some extream values\n\n"}}