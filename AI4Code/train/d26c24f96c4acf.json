{"cell_type":{"abcb7841":"code","0a6fedc7":"code","c4c6112e":"code","f20207e8":"code","93fbba5c":"code","ba047bf3":"code","3b98335d":"code","bea9def4":"code","9ff3084f":"code","57d8aeeb":"code","9537e4e0":"code","3865b999":"code","56609a97":"code","e55e6613":"code","da61ad28":"code","dadc3a49":"code","423e8329":"code","48d46083":"code","21747fc4":"code","58c7ead6":"code","cf42e6dc":"code","235a9118":"code","d4f56f79":"code","ed4b8b81":"code","737469e0":"code","6671935b":"code","5ac286cf":"code","7e388e71":"code","9f4787fe":"code","2feee19f":"code","0647dbc3":"code","4928cc1e":"code","e97a32ca":"code","6a2550e1":"code","6bb3873f":"code","f1b3bfd1":"code","fa97cd7e":"code","e247f797":"code","92e02229":"code","987aa18e":"code","e27fadf7":"code","f42c9a93":"code","542f6a8e":"code","c813db1b":"code","7ee1e776":"code","8daafc74":"code","10915c49":"code","7bc31266":"code","ecfdf135":"code","b97caa56":"code","0b8b7664":"code","cb232f09":"code","b15b13f9":"code","e829bc55":"code","fb60e6fe":"code","9c3115c2":"code","0fd4ec17":"code","bcedf14d":"code","282f73d6":"code","20e3ace9":"markdown","91d2128c":"markdown","c5cd98f5":"markdown","b9d53667":"markdown","8ef5403d":"markdown","c03112f2":"markdown","f0384ac0":"markdown","8b369244":"markdown","921d0162":"markdown","34497c1b":"markdown","97181876":"markdown","57957f2b":"markdown","9e085247":"markdown","8515de55":"markdown","6eca2e14":"markdown","7100fcce":"markdown","752de3e4":"markdown","96ca7f35":"markdown","72ccf531":"markdown","1c3fced1":"markdown","f8f78c4d":"markdown","6f60204d":"markdown","2556a5d1":"markdown","122379e3":"markdown","102ab233":"markdown","1c815d46":"markdown","aa7f7623":"markdown","14ac742e":"markdown","0c78d2ee":"markdown","4acdc6fa":"markdown","59d94dcd":"markdown","01d127b0":"markdown","673d17e0":"markdown","016af170":"markdown","3f926d70":"markdown","44932b6b":"markdown","bc226e59":"markdown","aac6e7d7":"markdown","60280553":"markdown","5b16e55a":"markdown","18f9858f":"markdown","b0ca9d04":"markdown","0518318a":"markdown","61ca8369":"markdown","0cdd5391":"markdown","3df5eb97":"markdown","00b8b8b6":"markdown","7d71b214":"markdown","320f6717":"markdown","a6fe6cf7":"markdown"},"source":{"abcb7841":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore') # warnings \ubb34\uc2dc\n%matplotlib inline\n\n# sns Theme \nsns.set_style('darkgrid') \n\n# \uc18c\uc218\uc810 \ud45c\ud604 \uc81c\ud55c\npd.set_option('display.float_format', lambda x : '{:.3f}'.format(x))\n\n# \ub514\ub809\ud1a0\ub9ac \ub0b4, \uc0ac\uc6a9\uac00\ub2a5 \ud30c\uc77c \uccb4\ud06c \nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","0a6fedc7":"# \ub370\uc774\ud130 \uc77d\uae30\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","c4c6112e":"# \ub370\uc774\ud130\uccb4\ud06c\nprint(train_df.shape, test_df.shape)\ntrain_df.head(5)","f20207e8":"# Save the 'Id' comlumn\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\n\n# drop the 'Id' column since it's unnecessary for the prediction process\ntrain_df.drop('Id', axis=1, inplace=True)\ntest_df.drop('Id', axis=1, inplace=True)","93fbba5c":"fig, ax = plt.subplots()\n\nax.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","ba047bf3":"#Deleting outlier\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index) \n\n#Check the Graph again\nfig, ax = plt.subplots()\nax.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","3b98335d":"sns.distplot(train_df['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint(mu, sigma)\n\n# \ubd84\ud3ec\ub97c \uadf8\ub798\ud504\uc5d0 \uadf8\ub824\ubd05\uc2dc\ub2e4\nplt.legend(['Normal dist. ($\\mu$={:.2f} and $\\sigma$={:.2f})'.format(mu,sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot\uc744 \uadf8\ub824\ubd05\uc2dc\ub2e4.\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","bea9def4":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\n# \uc704\uc5d0\uc11c\uc640 \uac19\uc740 \ucf54\ub4dc\ub85c \ub611\uac19\uc774 \ubd84\ud3ec\ub97c \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.\nsns.distplot(train_df['SalePrice'], fit=norm)\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint(mu, sigma)\nplt.legend(['Normal dist. ($\\mu$={:.2f} and $\\sigma$={:.2f})'.format(mu,sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","9ff3084f":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\n\ny_train = train_df.SalePrice.values\n\nall_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","57d8aeeb":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({\"Missing Ratio\" : all_data_na})\nmissing_data.head(20)","9537e4e0":"f, ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Feature', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","3865b999":"corrmat = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","56609a97":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","e55e6613":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","da61ad28":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x : x.fillna(x.median()))","dadc3a49":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","423e8329":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","48d46083":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","21747fc4":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","58c7ead6":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","cf42e6dc":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","235a9118":"all_data = all_data.drop(['Utilities'], axis=1)","d4f56f79":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","ed4b8b81":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","737469e0":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","6671935b":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","5ac286cf":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","7e388e71":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","9f4787fe":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","2feee19f":"# MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n# Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].apply(str)\n\n# Year and Month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","0647dbc3":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","4928cc1e":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","e97a32ca":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# \uc218\uce58\ud615 \ub370\uc774\ud130\uc5d0\uc11c skewness \uccb4\ud06c\nskewed_feats = all_data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending=False)\n\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","6a2550e1":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","6bb3873f":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\ntrain_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]","f1b3bfd1":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","fa97cd7e":"# Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n    rmse = np.sqrt(-cross_val_score(model, train_df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return (rmse)","e247f797":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","92e02229":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","987aa18e":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4,\n                                   max_features='sqrt', min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)","e27fadf7":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","f42c9a93":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","542f6a8e":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c813db1b":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7ee1e776":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8daafc74":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","10915c49":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","7bc31266":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","ecfdf135":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","b97caa56":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0b8b7664":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # base_models_\ub294 2\ucc28\uc6d0 \ubc30\uc5f4\uc785\ub2c8\ub2e4.\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # \uac01 \ubaa8\ub378\ub4e4\uc758 \ud3c9\uade0\uac12\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","cb232f09":"stacked_averaged_models = StackingAveragedModels(\n    base_models=(ENet, GBoost, KRR),\n    meta_model=(lasso)\n)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","b15b13f9":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","e829bc55":"stacked_averaged_models.fit(train_df.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train_df.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df.values))\nprint(rmsle(y_train, stacked_train_pred))","fb60e6fe":"model_xgb.fit(train_df, y_train)\nxgb_train_pred = model_xgb.predict(train_df)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","9c3115c2":"model_lgb.fit(train_df, y_train)\nlgb_train_pred = model_lgb.predict(train_df)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","0fd4ec17":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","bcedf14d":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","282f73d6":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","20e3ace9":"# [\uc218\ube44\ub2c8\uc6c0 \uce90\uae00 \ub530\ub77c\ud558\uae30] \uc8fc\ud0dd \uac00\uaca9 \uc608\uce21 : Advanced\n\n\ubcf8 \ucee4\ub110\uc740 \ub2e4\uc74c\uacfc \uac19\uc740 \ucee4\ub110\uc744 \ucd94\uac00\uc801\uc73c\ub85c \uacf5\ubd80\ud558\uba70, Stacking \ud14c\ud06c\ub2c9\uc744 \uc774\uc6a9\ud574\ubd05\ub2c8\ub2e4.\n\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n\nThanks to @serigne for awesome kernel.\n\n\uc774\ubc88\uc5d4  \uc8fc\ud0dd \uac00\uaca9 \uc608\uce21 \ubb38\uc81c\ub97c stacking \ud14c\ud06c\ub2c9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc88b\uc740 \uacb0\uacfc\ub97c \ub0b4\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4.\n\nStacking\uacfc  \uc559\uc0c1\ube14, \uc2a4\ud0dc\ud0b9\uc744 \ud558\ub294 \ubd80\ubd84\uc5d0 \uc788\uc5b4\uc11c class\ub85c \ub9cc\ub4e4\uc5b4 \uc0ac\uc6a9\ud558\ub294 \uac83\uacfc \ub370\uc774\ud130\uc758 \ubd84\ud3ec\ub97c \ub9de\ucd94\ub294 \uacfc\uc815 \ub4f1\uc744 \ubc30\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub610\ud55c \uad50\ucc28 \uac80\uc99d\ub3c4 \ub530\ub85c \ud568\uc218\ub85c \ub9cc\ub4e4\uc5b4 \uc0ac\uc6a9\ud558\ub294 \ub4f1 \uc800\uc5d0\uac8c\ub294 \ubc30\uc6b8 \uc810\uc774 \ub9ce\uc740 \ucee4\ub110\uc774 \uc5c8\uc2b5\ub2c8\ub2e4.\n\n\ud558\uc9c0\ub9cc \ube44\uad50\uc801 \uc0ac\uc804 \uc9c0\uc2dd\uc774 \ub9ce\uc774 \ud544\uc694\ud55c \ucee4\ub110\uc785\ub2c8\ub2e4. \ub0b4\uc6a9 \uc790\uccb4\uac00 \uce5c\uc808\ud55c \ucee4\ub110\uc740 \uc544\ub2d9\ub2c8\ub2e4.\n\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc758 \uc124\uc815\uc774 \uc65c \uadf8\ub807\uac8c \ub42c\ub294\uc9c0 \uc124\uba85\uc774 \ubd80\uc871\ud558\uace0, \ucf54\ub4dc\uc5d0 \ub300\ud55c \uc124\uba85 \ub610\ud55c \ubd80\uc871\ud569\ub2c8\ub2e4.\n\n\uadf8\ub807\uae30\uc5d0 \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc811\ud55c\uc9c0 \uc5bc\ub9c8 \uc548\ub41c \ubd84\uc5d0\uac8c\ub294 \ucd94\ucc9c\ud558\uc9c0 \uc54a\uc744 \uac83 \uac19\uc9c0\ub9cc, \uc2a4\ud0dc\ud0b9\uacfc \uc559\uc0c1\ube14 \uacfc\uc815\uc744 \ub530\ub77c\ud558\uae30\uc5d0\ub294 \uc88b\uc740 \ucee4\ub110\uc774\uc5c8\uc2b5\ub2c8\ub2e4.\n\n> \uc774 \ubb38\uc81c\uc758 \uc559\uc0c1\ube14\uacfc \uc2a4\ud0dc\ud0b9 \ub4f1\uc758 \uae30\ubc95\uc5d0 \uc788\uc5b4\uc11c\ub294 [\ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \uc644\ubcbd \uac00\uc774\ub4dc] \ucc45\uc5d0\ub3c4 \uc0c1\uc138\ud558\uac8c \uae30\uc220\ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4. \n\n\ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc5c5\ub85c\ub4dc\ud558\uace0 \uc788\uc73c\ub2c8 \ub9ce\uc740 \uc88b\uc544\uc694\uc640 \uad6c\ub3c5 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.\n\n- **\ube14\ub85c\uadf8** : [\uc548\uc218\ube48\uc758 \ube14\ub85c\uadf8](https:\/\/subinium.github.io)\n- **\ud398\uc774\uc2a4\ubd81** : [\uc5b4\uc378\ub108\ub4dc \uc218\ube44\ub2c8\uc6c0](https:\/\/www.facebook.com\/ANsubinium)\n- **\uc720\ud29c\ube0c** : [\uc218\ube44\ub2c8\uc6c0\uc758 \ucf54\ub529\uc77c\uc9c0](https:\/\/www.youtube.com\/channel\/UC8cvg1_oB-IDtWT2bfBC2OQ)","91d2128c":"#### \ud2b9\uc131 \uacf5\ud559\n\n- Inputing missing values\n- Transforming : \uc218\uce58\ud615 -> \ubc94\uc8fc\ud615\n- Label Encoding : \ubc94\uc8fc\ud615 -> \uc11c\uc218\ud615\n- Box Cox Transformation : \ube44\ub300\uce6d \ubd84\ud3ec \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc880 \ub354 \uc88b\uc740 \uacb0\uacfc(\ub9ac\ub354\ubcf4\ub4dc\uc640 cross-validation\uc5d0\uc11c)\ub97c \ub0bc \uc218 \uc788\uc74c\n- Getting dummy variables : \ubc94\uc8fc\ud615 \ud2b9\uc131\n\n#### \uc54c\uace0\ub9ac\uc998\n\n\ubcf8 \ucee4\ub110\uc5d0\uc11c\ub294 stacking\/emsemble\uc744 \ud558\uae30 \uc804 \ub2e4\uc74c\uacfc \uac19\uc740 \ubaa8\ub378\uc744 \uc120\ud0dd\ud558\uace0, cross-validate\ud569\ub2c8\ub2e4..\n\n- sklearn based model\n- DMLS's XGBoost\n- Microsoft's LightGBM\n\n\ub610\ud55c stacking \ubc0f \uc559\uc0c1\ube14 \ubc29\ubc95\uc73c\ub85c \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95(\ud3c9\uade0)\uacfc \ube44\uad50\uc801 \uc5b4\ub824\uc6b4 \ubc29\ubc95\uc73c\ub85c \ucd1d 2\uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc608\uce21\uac12\uc744 \ub192\uc785\ub2c8\ub2e4.\n","c5cd98f5":"### Data Correlation\n\nheatmap\uc744 \uc774\uc6a9\ud558\uba74 \uac01 \uc694\uc18c\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc2dc\uac01\ud654\ud558\uc5ec, \ub354 \uc9c1\uad00\uc801\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc5ec\uae30\uc11c \ucd08\uc810\uc73c\ub85c \ubd10\uc57c\ud558\ub294 \uc694\uc18c\ub294 \uc9c0\uae08 \uad6c\ud558\uace0\uc790\ud558\ub294 \uac12\uc778 SalePrice\uc640 \ub2e4\ub978 \uc694\uc18c\uac04\uc758 \uc0c1\uad00\uad00\uacc4\uc785\ub2c8\ub2e4.","b9d53667":"### Base models scores\n\n\uc774\uc81c \uc774 \ubaa8\ub378\ub4e4\uc744 \uc774\uc6a9\ud574 \uad50\ucc28 \uac80\uc99d\uc744 \ud1b5\ud574 score\ub97c \uad6c\ud574\ubd05\uc2dc\ub2e4.","8ef5403d":"\uc774\uc81c \uc774 \ub370\uc774\ud130\ub97c \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4.","c03112f2":" - **MSZoning (The general zoning classification)** : RL\uc774 \ucd5c\ube48\uac12\uc73c\ub85c \ube48 \ubd80\ubd84\uc740 RL\ub85c \ucc44\uc6c1\ub2c8\ub2e4. mode \uba54\uc11c\ub4dc\ub294 \uac00\uc7a5 \ub9ce\uc774 \ub098\ud0c0\ub098\ub294 \uac12\uc744 \uc790\ub3d9\uc73c\ub85c \uc120\ud0dd\ud574\uc90d\ub2c8\ub2e4.","f0384ac0":"meta learner\uc744 \uc774\uc6a9\ud558\uc5ec \ub354 \ub098\uc740 \uc810\uc218\ub97c \ubc1b\uc744 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.","8b369244":"### Define a cross validation strategy\n\n\uc774 \ucee4\ub110\uc5d0\uc11c\ub294 **cross_val_score** \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ud568\uc218\ub294 \uc21c\uc11c\ub97c \uc11e\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \uac80\uc99d\uc758 \uc815\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574 K-fold\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac80\uc99d\uc758 \uc815\ud655\ub3c4\ub97c \ub354 \ub192\uc785\ub2c8\ub2e4.","921d0162":"## Data Processing\n\n### Outliers\n\n\uacf5\uc2dd [Documetation](http:\/\/jse.amstat.org\/v19n3\/decock.pdf)\uc5d0\uc11c\ub294 \ub370\uc774\ud130\uc5d0 \uc774\uc0c1 \uce58\uac00 \uc788\ub2e4\uace0 \uc5b8\uae09\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8 \uc774\uc0c1\uce58\ub97c \ucc3e\uc544 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ud560 \uac83\uc778\uac00\uac00 \uc774 \ubb38\uc81c\uc758 \ud575\uc2ec\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","34497c1b":"### Final Training and Prediction\n\n `expm1` \ud568\uc218\ub294 \ucd08\uae30\uc5d0 \uc815\uaddc\ud654\ub97c \uc704\ud574 \uc0ac\uc6a9\ud55c `log1p`\ud568\uc218\uc758 \uc5ed\ud568\uc218\uc785\ub2c8\ub2e4.\n\n**StackedRegressor**:","97181876":"##  Modeling\n\n###  Import Libraries \n\n\uc774\uc81c \ubaa8\ub378\ub4e4\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c import\ud569\uc2dc\ub2e4.","57957f2b":"### Missing Data\n\n\uc774\uc81c \uc804\uccb4 \ub370\uc774\ud130\uc5d0\uc11c \ube48 \ubd80\ubd84\uc744 \ud655\uc778\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","9e085247":"### Inputing missing values\n\n\uc774\uc81c \ub204\ub77d\ub41c \uac12\uc744 \ucc44\uc6cc\ub123\ub3c4\ub85d \ud569\uc2dc\ub2e4. \uc774\uc81c\ubd80\ud130 \ub204\ub77d\ub41c \uac12\uc774 \uc788\ub294 \ud2b9\uc131\ub4e4\uc744 \ud558\ub098\uc529 \ucc28\ub840\ub85c \ubcfc \ucc28\ub840\uc785\ub2c8\ub2e4.\n\n- **PoolQC** : \ub514\uc2a4\ud06c\ub9bd\uc158\uc5d0 \ub530\ub974\uc790\uba74  NA\uac12\uc740 \"No Pool\"\uc744 \uc758\ubbf8\ud55c\ub2e4\uace0 \ud569\ub2c8\ub2e4. 99%\uc758 \uac12\uc744 \ucc44\uc6cc\uc90d\uc2dc\ub2e4.","8515de55":"\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc544\uc9c1\uae4c\uc9c0 \ucc44\uc6b0\uc9c0 \ubabb\ud55c \ub370\uc774\ud130\uac00 \uc788\ub294\uc9c0 \uccb4\ud06c\ud574\ubd05\uc2dc\ub2e4.","6eca2e14":"- **Utilities** : \uc774 \ub370\uc774\ud130\ub294 \ubaa8\ub4e0 \uac12\uc774 \"AllPub\"\uc73c\ub85c \ub418\uc5b4\uc788\uace0, \ud55c\uac1c\uac00 \"NoSeWa\" \uadf8\ub9ac\uace0 2\uac1c\uc758 NA\uac12\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc608\uce21\ud558\ub294 \ub370\uc5d0 \uc788\uc5b4 \uc804\ud600 \uc720\uc6a9\ud558\uc9c0 \uc54a\uc744 \uac83 \uac19\uc73c\ub2c8 drop\ud569\uc2dc\ub2e4.","7100fcce":"**Ensemble prediction** :\n\n\uc559\uc0c1\ube14\uc5d0 \uc0ac\uc6a9\ud55c \uac00\uc911\uce58\uc5d0 \ub300\ud574\uc11c \uc6d0 \uc800\uc790\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ub2f5\ubcc0\uc744 \ud588\uc2b5\ub2c8\ub2e4. \uac00\uc911\uce58\ub97c Stacked Regressor\uc5d0 \ud06c\uac8c \ud55c \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\nBased on their cross-validation scores (and a bit of trial and errors ^^ )\n\nYou can see for instance in this version of the notebook the following CV mean scores :\n\n- StackedRegressor score : 0.1085\n- Xgboost score: 0.1196\n- LGBM score: 0.1159\n\nThis helps to define the weights. However you may also want to define an optimization function in order to find more optimal weights.","752de3e4":"- **LotFrontage** : \uac70\ub9ac\uc640 \uc9d1\uc758 \uac70\ub9ac \uc694\uc18c\ub85c, \uc774 \uac12\ub4e4\uc740 \uc774\uc6c3\ub4e4\uc758 \uac70\ub9ac\uc640 \uc720\uc0ac\ud55c \uac12\uc744 \uac00\uc9c8 \uac83\uc785\ub2c8\ub2e4. \uc190\uc2e4\ub41c \uac12\ub4e4\uc740 \uc774\uc6c3\ub4e4\uc758 \uc911\uc559\uac12\uc73c\ub85c \ucc44\uc6cc\ub123\uaca0\uc2b5\ub2c8\ub2e4.","96ca7f35":"#### Adding one more important feature\n\n\uc8fc\ud0dd \uac00\uaca9\uc5d0\uc11c \uc911\uc694\ud55c \uc694\uc18c \uc911 \ud558\ub098\ub294 \uc9d1\uc758 \uac00\uc6a9 \ud3c9\uc218\uc785\ub2c8\ub2e4. \uadf8\ub807\uae30\uc5d0 \uc774\ub7f0 \ud2b9\uc131\uc744 basement + 1\uce35 + 2\uce35 \uacf5\uac04\uc73c\ub85c \uc0c8\ub85c\uc6b4 \ud2b9\uc131\uc744 \ud558\ub098 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.","72ccf531":"## Base models\n\n- **LASSO Regression**\n\uc774 \ubaa8\ub378\uc740 \uc774\uc0c1\uce58\uc5d0 \ub9e4\uc6b0 \ubbfc\uac10\ud569\ub2c8\ub2e4. \uadf8\ub807\uae30\uc5d0 \uc774\ub7f0 \uc774\uc0c1\uce58\ub97c \uc880 \ub354 \uaddc\uc81c\ud558\uae30 \uc704\ud574 pipeline\uc5d0  `RobustScaler()` \uba54\uc11c\ub4dc\ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4.\n\n- **Elastic Net Regression**\n\uc774 \ubaa8\ub378 \ub610\ud55c \uc774\uc0c1\uce58\ub97c \uc704\ud574 \ub611\uac19\uc774 \ud569\ub2c8\ub2e4.","1c3fced1":"- **LightGBM**","f8f78c4d":"\uc774\uc81c \uc815\uaddc\ubd84\ud3ec\uc5d0 \ub9e4\uc6b0 \uadfc\uc811\ud558\uac8c \uac12\ub4e4\uc774 \ubc14\ub010 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","6f60204d":"\ub2e4\uc2dc \ube48\ub370\uc774\ud130\ub4e4\uc744 'None' \ub610\ub294 0\uc73c\ub85c \ucc98\ub9ac\ud569\ub2c8\ub2e4.","2556a5d1":"\uc774 \uadf8\ub798\ud504\ub97c \ubcf4\uba74 \ub0ae\uc740 SalePrice\uc5d0\uc11c \uadf8\ub798\ud504\uc640 \ub2e4\ub974\uac8c \uc774\uc0c1\ud55c 2\uac1c\uc758 \ub370\uc774\ud130\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774 \ub370\uc774\ud130\ub97c \ub354 \ub098\uc740 \ud6c8\ub828\uc744 \uc704\ud574 \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4.","122379e3":"#### Box Cox Transformation of (highly) skewed features\n\nBox-Cox Transformation\uc740 \uc815\uaddc \ubd84\ud3ec\uac00 \uc544\ub2cc \ub370\uc774\ud130\ub97c \uc815\uaddc \ubd84\ud3ec \ud615\ud0dc\ub85c \ubcc0\ud658\ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.\n\n\ubcf4\ub2e4 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub2e4\uc74c\uc744 \ucc38\uace0\ud558\uae38 \ubc14\ub78d\ub2c8\ub2e4.\n\n**Reference**\n\n- [Box Cox Transformation](https:\/\/en.wikipedia.org\/wiki\/Power_transform)","102ab233":"- **Functional** : \ub514\uc2a4\ud06c\ub9bd\uc158\uc5d0 \uc758\ud558\uba74  NA\ub294 typical\uc744 \uc758\ubbf8\ud55c\ub2e4\uace0 \ud569\ub2c8\ub2e4.","1c815d46":"### More features engineering\n\n#### Transforming some numerical variable that are really categorical\n\n\uc218\uce58\ud615 \uac12\ub4e4 \uc911 \ubc94\uc8fc\ud615\uc778 \ud2b9\uc131\ub4e4\uc744 \ubcc0\ud658\ud569\uc2dc\ub2e4.","aa7f7623":"## Stacking models\n\n### Simplest Stacking approach : Averaging base models\n\n\uc6b0\uc120 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ud3c9\uade0\ud558\uc5ec \uc0ac\uc6a9\ud558\ub294 \uac83\uc73c\ub85c \uc2dc\uc791\ud574\ubd05\uc2dc\ub2e4.\n**class**\ub97c \ub9cc\ub4e4\uc5b4 \ucea1\uc290\ud654\ud558\uace0, \ucf54\ub4dc\ub97c \uc7ac\uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.\n","14ac742e":"## Emsembling StackedRegressor, XGBoost and LightGBM\n\n\uc704\uc5d0\uc11c \ub9cc\ub4e0 XGBoost\uc640 LightBGM\uc744 \uc774\uc6a9\ud558\uc5ec \ucd5c\uc885 \uacb0\uacfc\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\uc120 rmsle \ud568\uc218\ub97c \uba3c\uc800 \uc815\uc758\ud569\ub2c8\ub2e4.","0c78d2ee":"#### Label Encoding some categorical variables that may contain information in their ordering set\n\n\ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub97c label encoding\uc73c\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4.","4acdc6fa":"### Averaged base models score\n\n\uc774\uc81c **ENet, GBoost, KRR and lasso**\ub97c \uc774\uc6a9\ud574 score\ub97c \ub0b4\ubd05\uc2dc\ub2e4. \ub2e4\ub978 \ubaa8\ub378\uc744 \ucd94\uac00\ud574\ub3c4 \ub429\ub2c8\ub2e4.","59d94dcd":"**Submission**\n\n\uc774\uc81c \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc81c\ucd9c\ub9cc \ub0a8\uc558\uc2b5\ub2c8\ub2e4.","01d127b0":"### Target Variable\n\n**SalePrice**\ub294 \uc6b0\ub9ac\uac00 \uc608\uce21\ud574\uc57c \ud558\ub294 \ud0c0\uac9f \uac12\uc785\ub2c8\ub2e4. \uc774\uc81c \uc774 \ub370\uc774\ud130\ubd80\ud130 \uba3c\uc800 \ubd84\uc11d\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\uc74c \uadf8\ub798\ud504\ub97c \uc774\ud574\ud558\uae30 \uc704\ud574\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uc120\uc9c0\uc2dd\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n- Q- Q Plot : \uac04\ub2e8\ud558\uac8c \ub450 \ub370\uc774\ud130 \uc9d1\ub2e8 \uac04\uc758 \ubd84\ud3ec\ub97c \uccb4\ud06c\n\n**reference** \n\n\ub354 \ub9ce\uc740 \uc815\ubcf4\ub294 \uc544\ub798\uc758 \ub9c1\ud06c\ub97c \ucc38\uace0\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n\n- [Q-Q wiki](https:\/\/en.wikipedia.org\/wiki\/Q%E2%80%93Q_plot)\n- [Q-Q plot \ud55c\uae00 \ube14\ub85c\uadf8 \uc790\ub8cc : sw4r\ub2d8](http:\/\/blog.naver.com\/PostView.nhn?blogId=sw4r&logNo=221026102874&parentCategoryNo=&categoryNo=43&viewDate=&isShowPopularPosts=true&from=search)","673d17e0":"### Less simple Stacking : Adding a Meta-model\n\n\uc774 \ubc29\ubc95\uc740 meta model\uc744 \ucd94\uac00\ud558\uace0, base model\ub4e4\uc758 \ud3c9\uade0\uacfc \uc774 out-of-folds \uc608\uce21\uc744 \uc774\uc6a9\ud558\uc5ec meta-model\uc744 \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\uae30\ubcf8\uc801\uc778 \ud750\ub984\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n1. \ud6c8\ub828 \ub370\uc774\ud130\ub97c \ubd84\ub9ac\ub41c \ub370\uc774\ud130\uc14b train, holdout\uc73c\ub85c \ub098\ub215\ub2c8\ub2e4.\n2. train \ub370\uc774\ud130\ub85c \ud6c8\ub828\uc744 \ud558\uace0\n3. holdout \ub370\uc774\ud130\ub85c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4.\n4. 3)\uc744 \ud1b5\ud574 \uc608\uce21\uac12\uc744 \uad6c\ud558\uace0, **meta model**\uc744 \ud1b5\ud574 \uadf8 \uc608\uce21 \uac12\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4.\n\n\uccab \uc138 \ub2e8\uacc4\ub294 \uc21c\uc11c\ub300\ub85c \uc9c4\ud589\ud558\uba74 \ub429\ub2c8\ub2e4. \ub9cc\uc57d 5-fold stacking\uc744 \ud55c\ub2e4\uba74, 5-folds\ub97c \uc608\uc2dc\ub85c \ub4e4\uc5b4\ubd05\ub2c8\ub2e4.\n\n\uadf8\ub807\ub2e4\uba74 \ud6c8\ub828 \ub370\uc774\ud130\ub97c 5\uac1c\ub85c \ub098\ub204\uace0, \ucd1d 5\ubc88\uc758 \ubc18\ubcf5\ubb38\uc744 \uc9c4\ud589\ud558\uba74 \ub429\ub2c8\ub2e4. \uac01 \ubc18\ubcf5\ubb38\uc740 4 folds\ub85c \ud6c8\ub828\uc744 \uc9c4\ud589\ud558\uace0, \ub098\uba38\uc9c0 1 fold\ub97c \uc608\uce21\ud569\ub2c8\ub2e4.\n\n\uadf8\ub807\ub2e4\uba74 5\ubc88\uc758 \ubc18\ubcf5\ubb38\uc774 \ub05d\ub098\uba74 \ubaa8\ub4e0 \ub370\uc774\ud130\ub294 out-of-folds \uc608\uce21\uac12\uc744 \uac00\uc9c0\uac8c \ub418\uace0, \uc774\uc81c \uc774 \uac12\ub4e4\uc744 \uc774\uc6a9\ud574 meta model\uc758 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. (4)\n\n\uc608\uce21 \ubd80\ubd84\uc5d0 \uc788\uc5b4 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\uc11c \ubaa8\ub4e0 \ubaa8\ub378\uc758 \uc608\uce21\uac12\uc744 \ud3c9\uade0\ub0b4\uace0, \uc774\ub97c **meta-features**\ub85c \uc0ac\uc6a9\ud558\uc5ec meta-model\ub85c \ub9c8\uc9c0\ub9c9 \uc608\uce21\uac12\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n\n![Faron's image](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\nimage taken from [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381)\n","016af170":"\ubd84\ud3ec\uac00 \uc624\ub978\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc77c\ubc18\uc801\uc73c\ub85c \uc120\ud615 \ubaa8\ub378\uc740 \ubd84\ud3ec\uac00 \uade0\ud615\uc7a1\ud78c \uc0c1\ud0dc\uc5d0 \ub354 \uc6a9\uc774\ud558\ubbc0\ub85c \ub370\uc774\ud130\ub97c \uc880 \ub354 \uc190\ubcf4\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\n#### Log-transformation of the target variable \n\n\ub370\uc774\ud130\uc758 \uc815\uaddc\ud654\ub97c \uc704\ud574 numpy\uc758 `log1p` (\ubaa8\ub4e0 column\uc758 \uc6d0\uc18c\uc5d0  $log(1+x)$) \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","3f926d70":"- **Gradient Boosting Regression**\n\n**huber** \uc190\uc2e4 \ud568\uc218\ub85c \uc774\uc0c1\uce58\ub97c \uad00\ub9ac\ud569\ub2c8\ub2e4. \uc774 \uc190\uc2e4\ud568\uc218\ub294 \ub2e4\ub978 \uc190\uc2e4\ud568\uc218\uc5d0 \ube44\ud574 \uc774\uc0c1\uce58\uc5d0 \ub300\ud574 \ubbfc\uac10\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\nReference\n\n- [huber wiki](https:\/\/en.wikipedia.org\/wiki\/Huber_loss)\n\n","44932b6b":"### Stacking Averaged models Score\n\n\uc131\ub2a5\uc744 \ube44\uad50\ud558\uae30 \uc704\ud574 \uac19\uc740 \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec score\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","bc226e59":"## Feature Engineering\n\n\uc6b0\uc120 \ucc98\ub9ac\ud558\uae30\uc5d0 \uc55e\uc11c \ub370\uc774\ud130\ub97c \ud558\ub098\ub85c \ubb36\uc5b4\uc11c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4.","aac6e7d7":"\uac00\uaca9 \ud2b9\uc131\uc744 \uc81c\uc678\ud558\uba74 80\uac1c\uc758 \ud2b9\uc131\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8 \uc678\uc5d0\ub294 \ub370\uc774\ud130 \ubd84\uc11d\uacfc \uc0c1\uad00 \uc5c6\ub294 Id  \ud2b9\uc131\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.","60280553":"- **Electrical, KitchenQual, Exterior1st and Exterior2nd, SaleType** :  \uc774 \ub370\uc774\ud130 \ubaa8\ub450 \ucd5c\ube48\uac12\uc73c\ub85c \ucc44\uc6cc\uc90d\uc2dc\ub2e4.","5b16e55a":"#### Skewed feature","18f9858f":"\uc774\uc0c1\uce58 \uc81c\uac70\ub294 \ud6c8\ub828\uc5d0 \uc788\uc5b4 \ub9e4\uc6b0 \uc88b\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\n\uc774\uc0c1\uce58\ub97c \uc77c\uc77c\ud788 \ucc3e\uc544 \uc81c\uac70\ud558\uc9c0 \uc54a\ub294 \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n- \uc774 \uc678\uc5d0\ub3c4 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc774\uc0c1\uce58\uac00 \ub9ce\uc744 \uc218 \uc788\uc9c0\ub9cc, \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\ub3c4 \uc774\uc0c1\uce58\uac00 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- \uadf8\ub807\ub2e4\uba74 \uc624\ud788\ub824 \ubaa8\ub378\uc5d0 \uc548\uc88b\uc740 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub7f0 \uc774\uc0c1\uce58\ub97c \ubaa8\ub450 \uc81c\uac70\ud558\ub294 \ub300\uc2e0\uc5d0 \ud6c4\uc5d0 \ubaa8\ub378\uc5d0\uc11c \uc774\ub7f0 \ub370\uc774\ud130\ub97c \uc81c\uc5b4\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6b8 \uac83\uc785\ub2c8\ub2e4.","b0ca9d04":"**XGBoost** :","0518318a":"## \ub77c\uc774\ube0c\ub7ec\ub9ac, \ub370\uc774\ud130 \ud655\uc778\ud558\uae30\n\n\uc6b0\uc120 \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \ubd88\ub7ec\uc624\uace0, \ub370\uc774\ud130\uc758 \ud615\ud0dc\ub97c \uac00\ubccd\uac8c \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.","61ca8369":"#### Getting dummy categorical features\n\n\ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub97c get_dummies\ub97c \uc774\uc6a9\ud558\uc5ec \ubcc0\ud658\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub2e4\uc2dc train_df\uc640 test_df\ub85c \ub098\ub204\uaca0\uc2b5\ub2c8\ub2e4.","0cdd5391":"- **XGBoost** \n\n\uac01 \ub9e4\uac1c\ubcc0\uc218, \uc989 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc740 bayesian optimization\uc744 \uc0ac\uc6a9\ud588\ub2e4\uace0 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uc800\ub294 \uc544\uc9c1 \uadf8 \uc0ac\uc6a9\uc744 \uc798 \ubaa8\ub974\uaca0\uc73c\ub098 \ub2e4\uc74c\uc744 \ucc38\uace0\ud558\uba74 \ub429\ub2c8\ub2e4. [link](https:\/\/github.com\/fmfn\/BayesianOptimization)\n","3df5eb97":"**LightGBM** :","00b8b8b6":"### Stacking averaged Models Class\n\n\ub2e4\uc74c\uacfc \uac19\uc740 pseudo \ucf54\ub4dc\ub97c \uad6c\ud604\ud588\ub2e4\uace0 \uc0dd\uac01\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n\ubca0\uc774\uc2a4 \ubaa8\ub378\uc758 \uc608\uce21\uac12\uc744 \ud558\ub098\uc758 \ud2b9\uc131\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ucd5c\uc885 \ubd84\ub958\ub97c \ub9cc\ub4e0\ub2e4\uace0 \uc774\ud574\ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc2b5\ub2c8\ub2e4.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*GXMZ7SIXHyVzGCE_.)","7d71b214":"- **MiscFeature** : \ub514\uc2a4\ud06c\ub9bd\uc158\uc5d0 \ub530\ub974\uba74 NA\ub294 \"no misc feature\"\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.\n\uc774 \uc678\uc5d0\ub3c4 **Alley, Fence, FireplaceQu**\ub3c4 \ube44\uc2b7\ud558\uac8c ~\uc5c6\ub2e4 \uc774\ubbc0\ub85c \ud568\uaed8 \ucc98\ub9ac\ud569\ub2c8\ub2e4.","320f6717":"- **Kernel Ridge Regression**","a6fe6cf7":"- **GarageType, GarageFinish, GarageQual and GarageCond** : \uc774 \ubd80\ubd84\ub3c4 None\uc73c\ub85c \ucc98\ub9ac\ud558\uaca0\uc2b5\ub2c8\ub2e4."}}