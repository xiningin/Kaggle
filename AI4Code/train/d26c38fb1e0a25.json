{"cell_type":{"c8b5a1fa":"code","246e2651":"code","4b8c9678":"code","261621e7":"code","effd1bca":"code","490b8f80":"code","c2c1c2d6":"code","31d6d934":"code","685f7c06":"code","5e44dc0e":"code","81a61a07":"code","5b37af05":"code","d9ef790e":"code","c66712b0":"code","04cf1ad2":"markdown","d7d9ed3b":"markdown","466f6d07":"markdown","365ff0a4":"markdown","9d183411":"markdown","47377072":"markdown","be6bf1fa":"markdown","252dbbf4":"markdown"},"source":{"c8b5a1fa":"import pandas as pd\nimport numpy as np","246e2651":"from pandas.api.types import CategoricalDtype\n\ndf_favorites = pd.read_csv('..\/input\/meta_favorites.csv')#, chunksize = 2000000)\n#for df_favorites in df_favorites:\n#    break\ndf_tracks = pd.read_csv('..\/input\/meta_tracks.csv')\n\nc_track = CategoricalDtype(sorted(df_favorites['track_id'].unique()), ordered=True)\nc_user = CategoricalDtype(sorted(df_favorites['username'].unique()), ordered=True)\n\ndf_favorites['track_id'] = df_favorites['track_id'].astype(c_track)\ndf_favorites['username'] = df_favorites['username'].astype(c_user)","4b8c9678":"print(len(c_track.categories))\nprint(len(c_user.categories))","261621e7":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_favorites, test_size=0.15, random_state=420)\nprint(train.shape)\nprint(test.shape)","effd1bca":"from scipy.sparse import  csr_matrix\ntrain_data = csr_matrix((np.ones(len(train)), (train['username'].cat.codes, train['track_id'].cat.codes)), \n                  shape=( len(c_user.categories), len(c_track.categories)))\nprint(f'{100*train_data.sum()\/(train_data.shape[0]*train_data.shape[1])}% Sparsity')","490b8f80":"# Simple Cosine Similarity\nfrom scipy.stats import percentileofscore\ndef model_simple(data, pred):\n    from sklearn.metrics.pairwise import cosine_similarity\n    # Model Simple: Cosine Similarity to obtain a matrix and then find closest based on the similarity matrix\n    \n    # normalize the matrix for each user (% importance to user)\n    norm_data = data.multiply(1\/data.sum(axis=1)).tocsr()\n    # apply cosine similarity\n    sim = cosine_similarity(norm_data.transpose(), dense_output=False)\n    sim[np.diag_indices(sim.shape[0])] = 0\n    denom = np.asarray(sim.sum(axis=1)).reshape(-1)\n\n    # do it in chunks else we get memory error\n    u_idx = pred['username'].cat.codes.values\n    i_idx = pred['track_id'].cat.codes.values\n\n    n_chunks = 30\n    chunks = np.array_split(np.arange(norm_data.shape[0]), n_chunks)\n    res = []\n    previous_max = 0\n    for i,idx in enumerate(chunks):\n        print(f'Doing Chunk {i+1}\/{n_chunks}')\n        score = (norm_data[idx].dot(sim)) \/ denom\n        score = (-score).argsort() \/ denom.shape[0]\n        sel = (u_idx >= idx.min()) & (u_idx <= idx.max())\n        chunk_score = np.asarray(score[u_idx[sel] - previous_max, i_idx[sel]]).reshape(-1)\n        res.append(chunk_score)\n        previous_max = idx.max() + 1\n    return np.concatenate(res)\n\n# Evaluate model\nmodel_pred = model_simple(train_data, test)\nmpr = model_pred.sum()\/len(test)\nprint(f'MPR Score: {mpr:.5f}')\nrec = (model_pred < 0.5).sum()\/len(test)\nprint(f'Recall Score: {rec:.5f}')","c2c1c2d6":"from sklearn.decomposition import NMF, TruncatedSVD\n\ndef model_mf(data, pred, model):\n    # normalize the matrix for each user (% importance to user)\n    #norm_data = data.multiply(1\/data.sum(axis=1)).tocsr()\n    W = model.fit_transform(data)\n    H = model.components_\n\n    # do it in chunks else we get memory error\n    u_idx = pred['username'].cat.codes.values\n    i_idx = pred['track_id'].cat.codes.values\n    n_chunks = 25\n    chunks = np.array_split(np.arange(W.shape[0]), n_chunks)\n    res = []\n    previous_max = 0\n    for i,idx in enumerate(chunks):\n        print(f'Doing Chunk {i+1}\/{n_chunks}')\n        score = (W[idx].dot(H))\n        score = (-score).argsort() \/ score.shape[1]\n        sel = (u_idx >= idx.min()) & (u_idx <= idx.max())\n        chunk_score = np.asarray(score[u_idx[sel] - previous_max, i_idx[sel]]).reshape(-1)\n        res.append(chunk_score)\n        previous_max = idx.max() + 1\n    return np.concatenate(res)\n\nK = 20\n\n# NMF\nmodel = NMF(n_components = K, init = 'nndsvd')\nmodel_pred = model_mf(train_data, test, model)\nmpr = model_pred.sum()\/len(test)\nprint(f'NMF MPR Score: {mpr:.5f}')\nrec = (model_pred < 0.5).sum()\/len(test)\nprint(f'Recall Score: {rec:.5f}')\n\n# TruncatedSVD\nmodel = TruncatedSVD(n_components = K)\nmodel_pred = model_mf(train_data, test, model)\nmpr = model_pred.sum()\/len(test)\nprint(f'SVD MPR Score: {mpr:.5f}')\nrec = (model_pred < 0.5).sum()\/len(test)\nprint(f'Recall Score: {rec:.5f}')","31d6d934":"# Create the Training Set\nAPPROX_NEGATIVE_SAMPLE_SIZE = int(len(train)*1.2)\nn_users = c_user.categories.shape[0]\nn_tracks = c_track.categories.shape[0]\n# Create Training Set\ntrain_users = train['username'].cat.codes.values\ntrain_tracks = train['track_id'].cat.codes.values\ntrain_labels = np.ones(len(train_users))\n# insert negative samples\nu = np.random.randint(n_users, size=APPROX_NEGATIVE_SAMPLE_SIZE)\ni = np.random.randint(n_tracks, size=APPROX_NEGATIVE_SAMPLE_SIZE)\nnon_neg_idx = np.where(train_data[u,i] == 0)\ntrain_users = np.concatenate([train_users, u[non_neg_idx[1]]])\ntrain_tracks = np.concatenate([train_tracks, i[non_neg_idx[1]]])\ntrain_labels = np.concatenate([train_labels, np.zeros(u[non_neg_idx[1]].shape[0])])\nprint((train_users.shape, train_tracks.shape, train_labels.shape))\n\n# random shuffle the data (because Keras takes last 10% as validation split)\nX = np.stack([train_users, train_tracks, train_labels], axis=1)\nnp.random.shuffle(X)","685f7c06":"import keras\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers.core import Dense, Lambda, Activation\nfrom keras.layers import Embedding, Input, Dense, Reshape, Flatten, merge\nfrom keras.optimizers import RMSprop\nfrom keras.regularizers import l2\n\ndef ncf_model(num_users, num_items, latent_dim, regs=[0,0]):\n    # Input variables\n    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n\n    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', embeddings_regularizer = l2(regs[0]), input_length=1)\n    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', embeddings_regularizer = l2(regs[1]), input_length=1)   \n    \n    # Crucial to flatten an embedding vector!\n    user_latent = Flatten()(MF_Embedding_User(user_input))\n    item_latent = Flatten()(MF_Embedding_Item(item_input))\n    \n    # Element-wise product of user and item embeddings \n    predict_vector = merge.Multiply()([user_latent, item_latent])\n    \n    # Final prediction layer\n    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n    \n    model = Model(inputs=[user_input, item_input], outputs=prediction)\n\n    return model\n\n# create the model\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\nmodel = ncf_model(n_users, n_tracks, 50, regs = [0,0])\nmodel.compile(optimizer=RMSprop(lr=0.001), metrics = ['accuracy', recall],  loss='binary_crossentropy')\n\n# Train Model\nES = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=1, verbose=1)\nhist = model.fit([X[:,0], X[:,1]], X[:,2], batch_size=150000, epochs=20, validation_split = 0.1, verbose=1, callbacks = [ES])\nscore = model.evaluate([test['username'].cat.codes.values, test['track_id'].cat.codes.values], np.ones(test.shape[0]), verbose=1, batch_size=100000)\nprint(f'Test Loss: {score[0]}   |   Test Recall: {score[1]}')","5e44dc0e":"# Plot it\nimport matplotlib.pyplot as plt\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,4))\nax1.plot(hist.history['loss'])\nax1.plot(hist.history['val_loss'])\nax1.set_title('Loss')\nax1.legend(['Train', 'Test'], loc='upper left')\nax2.plot(hist.history['recall'])\nax2.plot(hist.history['val_recall'])\nax2.set_title('Recall')\nax2.legend(['Train', 'Test'], loc='upper left')\nplt.show()","81a61a07":"del X,train_users,train_tracks,train_labels\n# Calculate MPR like before\n# do it in chunks else we get memory error\nu_idx = test['username'].cat.codes.values\ni_idx = test['track_id'].cat.codes.values\nn_chunks = 100\nchunks = np.array_split(np.arange(n_users), n_chunks)\nres = []\nprevious_max = 0\nfor i,idx in enumerate(chunks):\n    print(f'Doing Chunk {i+1}\/{n_chunks}')\n    cross_product = np.transpose([np.tile(np.arange(n_tracks), len(idx)), np.repeat(idx, len(np.arange(n_tracks)))])\n    score = model.predict([cross_product[:,1], cross_product[:,0]], batch_size=500000, verbose=1).reshape(idx.shape[0], n_tracks)\n    score = (-score).argsort() \/ score.shape[1]\n    sel = (u_idx >= idx.min()) & (u_idx <= idx.max())\n    chunk_score = np.asarray(score[u_idx[sel] - previous_max, i_idx[sel]]).reshape(-1)\n    res.append(chunk_score)\n    previous_max = idx.max() + 1\nmpr = np.concatenate(res).sum()\/len(test)\nprint(f'NCF MPR Score: {mpr:.5f}')\nrec = (np.concatenate(res) < 0.5).sum()\/len(test)\nprint(f'NCF Recall Score: {rec:.5f}')","5b37af05":"import implicit # doesn't work with GPU active","d9ef790e":"def model_als(data, pred, n_factors=50):\n    # initialize a model\n    CONFIDENCE = 130\n    model = implicit.als.AlternatingLeastSquares(factors=n_factors, calculate_training_loss=True)\n    model.fit(CONFIDENCE*data.transpose())\n    W = model.user_factors\n    H = model.item_factors.transpose()\n    \n    # do it in chunks else we get memory error\n    u_idx = pred['username'].cat.codes.values\n    i_idx = pred['track_id'].cat.codes.values\n    n_chunks = 30\n    chunks = np.array_split(np.arange(W.shape[0]), n_chunks)\n    res = []\n    previous_max = 0\n    for i,idx in enumerate(chunks):\n        print(f'Doing Chunk {i+1}\/{n_chunks}')\n        score = (W[idx].dot(H))\n        score = (-score).argsort() \/ score.shape[1]\n        sel = (u_idx >= idx.min()) & (u_idx <= idx.max())\n        chunk_score = np.asarray(score[u_idx[sel] - previous_max, i_idx[sel]]).reshape(-1)\n        res.append(chunk_score)\n        previous_max = idx.max() + 1\n    return np.concatenate(res)\n\nK = 50\nmodel_pred = model_als(train_data, test, K)\nmpr = model_pred.sum()\/len(test)\nprint(f'ALS MPR Score: {mpr:.5f}')\nrec = (model_pred < 0.5).sum()\/len(test)\nprint(f'Recall Score: {rec:.5f}')","c66712b0":"def model_pbr(data, pred, n_factors=50):\n    # initialize a model\n    model = implicit.bpr.BayesianPersonalizedRanking(factors=n_factors, iterations = 50)\n    model.fit(data.transpose())\n    W = model.user_factors\n    H = model.item_factors.transpose()\n    \n    # do it in chunks else we get memory error\n    u_idx = pred['username'].cat.codes.values\n    i_idx = pred['track_id'].cat.codes.values\n    n_chunks = 30\n    chunks = np.array_split(np.arange(W.shape[0]), n_chunks)\n    res = []\n    previous_max = 0\n    for i,idx in enumerate(chunks):\n        print(f'Doing Chunk {i+1}\/{n_chunks}')\n        score = (W[idx].dot(H))\n        score = (-score).argsort() \/ score.shape[1]\n        sel = (u_idx >= idx.min()) & (u_idx <= idx.max())\n        chunk_score = np.asarray(score[u_idx[sel] - previous_max, i_idx[sel]]).reshape(-1)\n        res.append(chunk_score)\n        previous_max = idx.max() + 1\n    return np.concatenate(res)\n\nK = 50\nmodel_pred = model_pbr(train_data, test, K)\nmpr = model_pred.sum()\/len(test)\nprint(f'PBR MPR Score: {mpr:.5f}')\nrec = (model_pred < 0.5).sum()\/len(test)\nprint(f'Recall Score: {rec:.5f}')","04cf1ad2":"I implement some recommender systems using some manual work and some python packages.  \n\n**Steps**  \n\n1. Create a index vector for all unique users and tracks\n2.  85\/15 Split the favorites data into test and train set\n3. Create a big 1\/0 binary sparse matrix of User x Tracks for training set\n4. Run the model on the training set\n5. Calculate Mean Percentile Score on the test set prediction\n\n**Why MPR**  \nOur hypem data is in form of implicit feedback from users, meaning that if they favorited a track, it's an explicit action we can observe but the abscence of a favorite can imply they either did not like the song or that they just have yet to listen to the song.  There is no way to know, therefore our test data will only consist of positive cases where the user favorited the track.  MPR will calculate the average recommendation score (measured by percentile ranking) from each model.  0% is the best, 100% is the worst, anything over 50% is worse than random guessing.","d7d9ed3b":"### Step 2.  85\/15 Split the favorites data into test and train set","466f6d07":"### Step 4. Run the model on the training set\n\nModels were testing:\n\n1. Simple Cosine Similarity on tracks\n2. Vanilla Matrix Factorization\n    1. NMF\n    2. TruncatedSVD\n3. Simple Neural Collaborative Filtering\n4. Implicit ALS\n5. Implicit BPR\n\nEvaluation:\n- Recall: % of 1s in the Test set\n- MPR: Mean Percentile Rank\n\n#### Simple Cosine Similarity\nDoing this in chunks for the test set so that we don't run into memory issues.   For the recall measure, if the track's relative percentile is < 0.5 then its a 1","365ff0a4":"### Step 1. Create a index vector for all unique users and tracks","9d183411":"#### Neural Collaborative Filtering\nUsing the model from here: https:\/\/nanx.me\/blog\/post\/recsys-binary-implicit-feedback-r-keras\/ and https:\/\/github.com\/hexiangnan\/neural_collaborative_filtering\/blob\/master\/GMF.py","47377072":"#### Implicit ALS\n\nUsing the Implicit Library here.  For the confidence level number, see [here](https:\/\/github.com\/benfred\/implicit\/issues\/74#issuecomment-358857517)","be6bf1fa":"#### Vanilla Matrix Factorization\n\nLet's test using some vanilla sklearn decomp model.","252dbbf4":"### Step 3. Create a big 1\/0 binary sparse matrix of User x Tracks for training set"}}