{"cell_type":{"ddacc0cc":"code","26223301":"code","59ee6ead":"code","2630ec36":"code","99ee03a6":"code","109333aa":"code","e937fca0":"code","d3dc0269":"code","0b36f4e5":"code","ff8e3ccb":"code","adf7453c":"code","89de408c":"code","c315270c":"code","4901d0bb":"code","980a2528":"code","2fb4051b":"code","5cb1a631":"code","eed8a3fc":"code","265a8845":"code","7b453036":"code","995255d9":"code","9ed7ca39":"code","506c332b":"code","1c68110e":"code","bf9c7280":"code","74e4e8f0":"code","38d914f1":"code","c5022e33":"code","e9d72ecc":"code","b85eae00":"code","32aa5e2c":"code","556c3d5d":"code","9fce881e":"code","08feb9ff":"code","e62b3c13":"code","ab793742":"code","b0411e98":"code","aa00b579":"code","1c06c242":"code","7fd1d140":"code","05f22c2d":"code","c9929934":"code","97928cea":"code","98871cc1":"code","e4a32c9c":"code","b506b7b4":"markdown","b5bb6bb2":"markdown","2c4d8d05":"markdown","3230ac7c":"markdown","c44f127f":"markdown","d6d368a7":"markdown","ff9d522c":"markdown","78cb9f1f":"markdown","5fba9389":"markdown","5f80606a":"markdown","f7cf8ba4":"markdown","781f40af":"markdown","7660d9b0":"markdown"},"source":{"ddacc0cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26223301":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nimport seaborn as sns\n\nnp.set_printoptions(suppress=True)","59ee6ead":"path='\/kaggle\/input\/google-quest-challenge\/'","2630ec36":"#train dataset\ntrain_df=pd.read_csv(path+'train.csv')\ntrain_df.head()","99ee03a6":"#test data\ntest_df=pd.read_csv(path+'test.csv')\ntest_df.head()","109333aa":"#submission file\nsubmission=pd.read_csv(path+'sample_submission.csv')\nsubmission.head()","e937fca0":"#columns in our train_data\ntrain_columns=train_df.columns\ntest_columns=test_df.columns\nprint('-'*15+'Train Columns'+'-'*15)\nprint(train_columns)\nprint('-'*15+'Train data Shape'+'-'*15)\nprint(train_df.shape)\nprint('-'*15+'Test Columns'+'-'*15)\nprint(test_columns)\nprint('-'*15+'Test data Shape'+'-'*15)\nprint(test_df.shape)","d3dc0269":"target_values=train_columns[11:]\nprint('Target columns are: ')\ntarget_values","0b36f4e5":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"couldn\\'t\", \" could not\", phrase)\n    phrase = re.sub(r\"couldnt\", \" could not\", phrase)\n    phrase = re.sub(r\"didn\\'t\", \" did not\", phrase)\n    phrase = re.sub(r\"doesn\\'t\", \" does not\", phrase)\n    phrase = re.sub(r\"don\\'t\", \" do not\", phrase)\n    phrase = re.sub(r\"hadn\\'t\", \" had not\", phrase)\n    phrase = re.sub(r\"hasn\\'t\", \" has not\", phrase)\n    phrase = re.sub(r\"haven\\'t\", \" have not\", phrase)\n    phrase = re.sub(r\"he\\'ll\", \" he will\", phrase)\n    phrase = re.sub(r\"he\\'d\", \" he would\", phrase)\n    phrase = re.sub(r\"didn\\'t\", \" did not\", phrase)\n    phrase = re.sub(r\"wasn\\'t\", \" was not\", phrase)\n    phrase = re.sub(r\"you\\'re\", \" you are\", phrase)\n    return phrase","ff8e3ccb":"#code reference: https:\/\/www.kaggle.com\/codename007\/start-from-here-quest-complete-eda-fe\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = text.lower().split()   \n    text = \" \".join(text)\n    return(text)","adf7453c":"#cleaning of train data\ntrain_df['question_body']=train_df['question_body'].apply(clean_text)\ntrain_df['question_body']=train_df['question_body'].apply(decontracted)\ntrain_df['question_title']=train_df['question_body'].apply(clean_text)\ntrain_df['question_title']=train_df['question_body'].apply(decontracted)\ntrain_df['answer']=train_df['question_body'].apply(clean_text)\ntrain_df['answer']=train_df['question_body'].apply(decontracted)\n\n#cleaning of test data\ntest_df['question_body']=test_df['question_body'].apply(clean_text)\ntest_df['question_body']=test_df['question_body'].apply(decontracted)\ntest_df['question_title']=test_df['question_body'].apply(clean_text)\ntest_df['question_title']=test_df['question_body'].apply(decontracted)\ntest_df['answer']=test_df['question_body'].apply(clean_text)\ntest_df['answer']=test_df['question_body'].apply(decontracted)\n","89de408c":"train_df.head()","c315270c":"test_df.head()","4901d0bb":"random.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n\nrandom.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n\nrandom.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n","980a2528":"import random\n\nrandom.seed(42)\nvalid_n_test_indexes=random.sample(list(range(1,len(train_df))),int(len(train_df)*0.3))\ntrain_indexes = list(set(list(range(1,len(train_df))))-set(valid_n_test_indexes))\nvalid_indexes=random.sample(valid_n_test_indexes,int(len(train_df)*0.2))\ntest_indexes=list(set(valid_n_test_indexes)-set(valid_indexes))\n                        ","2fb4051b":"X_train=train_df.iloc[train_indexes]\nX_valid=train_df.iloc[valid_indexes]\nX_test=train_df.iloc[test_indexes]\n\nprint('Train data shape ='+str(X_train.shape))\nprint('Valid data shape ='+str(X_valid.shape))\nprint('Test data shape ='+str(X_test.shape))","5cb1a631":"all_words=X_train['question_title']+X_train['question_body']+X_train['answer']","eed8a3fc":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(all_words)","265a8845":"#for train data\nquest_title = tokenizer.texts_to_sequences(X_train['question_title'])\nquest_body = tokenizer.texts_to_sequences(X_train['question_body'])\nanswer = tokenizer.texts_to_sequences(X_train['answer'])","7b453036":"#for valid data\n\nquest_title_valid = tokenizer.texts_to_sequences(X_valid['question_title'])\nquest_body_valid = tokenizer.texts_to_sequences(X_valid['question_body'])\nanswer_valid = tokenizer.texts_to_sequences(X_valid['answer'])","995255d9":"#for test data taken from train dataset\n\nquest_title_ts = tokenizer.texts_to_sequences(X_test['question_title'])\nquest_body_ts = tokenizer.texts_to_sequences(X_test['question_body'])\nanswer_ts = tokenizer.texts_to_sequences(X_test['answer'])","9ed7ca39":"#for test data\n\nquest_title_test = tokenizer.texts_to_sequences(test_df['question_title'])\nquest_body_test = tokenizer.texts_to_sequences(test_df['question_body'])\nanswer_test = tokenizer.texts_to_sequences(test_df['answer'])","506c332b":"title_lens=[len(i) for i in quest_title]\nquest_lens=[len(i) for i in quest_body]\nans_lens=[len(i) for i in answer]","1c68110e":"#len for question body\npercentile_train_len = sorted(np.percentile((quest_lens),np.array(range(0,110,10))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","bf9c7280":"percentile_train_len = sorted(np.percentile((quest_lens),np.array(range(90,101))))\nk=90\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=1","74e4e8f0":"#percentiles for answers\npercentile_train_len = sorted(np.percentile((ans_lens),np.array(range(0,110,10))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","38d914f1":"percentile_train_len = sorted(np.percentile((ans_lens),np.array(range(91,101))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","c5022e33":"#for train data\n\npadded_question_title_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title, maxlen=800, padding='post'))\npadded_question_body_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body, maxlen=800, padding='post'))\npadded_question_answer_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_train.shape))\nprint('shape of question body: '+str(padded_question_body_train.shape))\nprint('shape of answer: '+str(padded_question_answer_train.shape))","e9d72ecc":"#for valid data\n\npadded_question_title_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_valid, maxlen=800, padding='post'))\npadded_question_body_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_valid, maxlen=800, padding='post'))\npadded_question_answer_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_valid, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_valid.shape))\nprint('shape of question body: '+str(padded_question_body_valid.shape))\nprint('shape of answer: '+str(padded_question_answer_valid.shape))","b85eae00":"#for test data taken from train_dataset\n\npadded_question_title_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_ts, maxlen=800, padding='post'))\npadded_question_body_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_ts, maxlen=800, padding='post'))\npadded_question_answer_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_ts, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_ts.shape))\nprint('shape of question body: '+str(padded_question_body_ts.shape))\nprint('shape of answer: '+str(padded_question_answer_ts.shape))","32aa5e2c":"#for test data\n\npadded_question_title_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_test, maxlen=800, padding='post'))\npadded_question_body_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_test, maxlen=800, padding='post'))\npadded_question_answer_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_test, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_test.shape))\nprint('shape of question body: '+str(padded_question_body_test.shape))\nprint('shape of answer: '+str(padded_question_answer_test.shape))","556c3d5d":"vocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","9fce881e":"import pickle\n\nwith open(r'\/kaggle\/input\/glove-vectors\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","08feb9ff":"embedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = model.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","e62b3c13":"import tensorflow.keras.backend as T\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n \n    for col_trues, col_pred in zip(trues.T, preds.T):\n     \n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        print(len(self.valid_predictions))\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n      \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )","ab793742":"from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,TimeDistributed,Concatenate\n\n\n\ninput_quest=Input(shape=(800,))\nembedding_layer1=Embedding(vocab_size,output_dim=300,input_length=800)(input_quest)\nlstm_q=LSTM(100, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform',return_sequences = True)(embedding_layer1)\nflatten_question=tf.keras.layers.Flatten()(lstm_q)\n\n\ninput_ans=Input(shape=(800,))\nembedding_layer2=Embedding(vocab_size,output_dim=300, weights = [embedding_matrix],input_length=800)(input_ans)\nlstm_a=LSTM(100, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform',return_sequences = True)(embedding_layer2)\nflatten_answer=tf.keras.layers.Flatten()(lstm_a)\n\nconcatenate_layer = tf.keras.layers.Concatenate(axis=1)(inputs = [flatten_question, flatten_answer])\n\ndense_layer1 = Dense(512,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(concatenate_layer)\ndense_layer2 = Dense(256,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(dense_layer1)\ndense_layer3 = Dense(128,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(dense_layer2)\noutput =  tf.keras.layers.Dense(30,activation='sigmoid',kernel_initializer=tf.keras.initializers.glorot_normal(seed=0))(dense_layer3)\nmodel = tf.keras.Model(inputs=[input_quest,input_ans],outputs=output)\nmodel.summary()","b0411e98":"adam = tf.keras.optimizers.Adam(0.0003)\n#checkpoint,earlystop,\nmodel.compile(loss='mse', optimizer=adam)","aa00b579":"inputs_train=[padded_question_body_train,padded_question_answer_train]\ninputs_valid=[padded_question_body_valid,padded_question_answer_valid]\ninputs_test=[padded_question_body_test,padded_question_answer_test]\ntrain_output=np.array(X_train[target_values])\nvalid_output=np.array(X_valid[target_values])\n","1c06c242":"  custom_callback = CustomCallback(\n        valid_data=(inputs_valid,valid_output), \n        test_data=inputs_test,\n        batch_size=16,\n        fold=None)\n","7fd1d140":"model.fit(inputs_train,train_output,batch_size=16,epochs =30,validation_data=(inputs_valid,valid_output),callbacks=[custom_callback])","05f22c2d":"test_output=model.predict(inputs_test)","c9929934":"print(len(test_output[:,0]))\n\nfor i in range(len(target_values)):\n  submission[target_values[i]]=test_output[:,i]","97928cea":"submission.shape","98871cc1":"submission.to_csv('submission.csv', index=False)","e4a32c9c":"os.listdir(path)","b506b7b4":"**Data Cleaning & Featurization**\n\nNow as we have explored our data it is time to text data in our dataset and obtain new features depending on our dataset","b5bb6bb2":"Overview\n*** Write an overview of the case study that you are working on. (MINIMUM 200 words) ***\n1. When it comes to questions and answers humans have a different way of\nunderstanding and answering the question asked. In this case study we are going\nto understand the same with the help of (Question-Answer) dataset from top\nwebsites.\n2. Humans have a unique way of answering any question asked. We first understand\nthe question, the intuition behind it, the significance of the question etc. With\nmachine it is different.\n3. In this problem, we will help the machine understand these aspects of question\nwith the help of a question-answer dataset from different websites.\n4. In our dataset we have 30 different aspects of question-answer like\n\u2018question_conversational\u2019, \u2018question_body_critical\u2019, \u2018answer_helpful\u2019 etc.\n5. Our goal will be to find the scores for each of these aspects with the help of our\ntraining dataset. In our training dataset we have scores associated with each of\nthese labels in the range of [0,1]. We will make our model learn these aspects and\ntry to evaluate the same for our test dataset.\n","2c4d8d05":"We thus have 6079 training points and 476 test points\/ We have 41 train columns and 11 test columns. For the 30 columns we have to predict value for the test dataset. So, let us have a look at these 30 values","3230ac7c":"Now we will see the percentilte of lenghts to decide our max_tensor_len","c44f127f":"**Glove Vector Words**","d6d368a7":"**Reading Our Datsets**","ff9d522c":"**Printing Our Cleaned Data**","78cb9f1f":"For these 30 columns we need to find value for each point in the test dataset in the range of [0,1]","5fba9389":"#Training with a Baseline Model","5f80606a":"As we see that more than 99% values have lenght of question or answer not more than 750. We take it as our max len.","f7cf8ba4":"**Remove Punctuations & Decontracted Words**","781f40af":"Splitting our train data into train and test dataset for our validating our model","7660d9b0":"Now let us have a look at how these target_values are distributed across our training dataset"}}