{"cell_type":{"5330c3dd":"code","a49b532f":"code","217e7c20":"code","cf606cce":"code","2d63b5b2":"code","03fb0c6c":"code","9ae8dd72":"code","1c1fb29f":"code","c83837e9":"code","5b33143a":"code","312f4bc2":"code","3856c04e":"code","22babe0b":"code","4e00f17a":"code","8d63d67f":"code","a080466b":"code","aa17bd42":"code","fc4afbe5":"code","15e00ade":"code","04b91118":"markdown","6e8088f5":"markdown","4ddd65be":"markdown","2873bc49":"markdown","b7bfaf1b":"markdown","c8133f42":"markdown","6bc364eb":"markdown","4d4b4b82":"markdown","80bf9d40":"markdown","c5e63696":"markdown","a1c39368":"markdown","bf1f48d8":"markdown","959545f9":"markdown"},"source":{"5330c3dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\n\nimport random\nimport math\nfrom scipy import stats\nfrom sklearn import linear_model, model_selection, preprocessing, datasets\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a49b532f":"full_data = pd.read_csv('\/kaggle\/input\/pga-tour-20102018-data\/PGA_Data_Historical.csv')","217e7c20":"full_data.head()","cf606cce":"#Checking which statistics have null values in the original data. I don't plan on using any of these statistics to predict driving distance, so no need to drop null values as they won't be used anyway. \nfull_data[full_data['Value'].isnull()]['Statistic'].unique()","2d63b5b2":"full_data.info()","03fb0c6c":"full_data['Statistic'].unique()[1:20]#Only showing the first 20 so that this does not take up too much space in the final write-up. I combed through all of these originally .","9ae8dd72":"data = full_data[full_data['Statistic'].isin(['Driving Distance', 'Driving Accuracy Percentage', 'Club Head Speed', 'Ball Speed', 'Smash Factor', 'Launch Angle', 'Spin Rate', 'Distance to Apex', 'Apex Height', 'Hang Time', 'Carry Distance', 'Carry Efficiency', 'Total Distance Efficiency', 'Total Driving Efficiency'])]\ndata = data[data['Variable'].isin(['Driving Distance - (AVG.)', 'Driving Accuracy Percentage - (%)', 'Club Head Speed - (AVG.)', 'Ball Speed - (AVG.)', 'Smash Factor - (AVG.)', 'Launch Angle - (AVG.)', 'Spin Rate - (AVG.)', 'Distance to Apex - (AVG.)', 'Apex Height - (AVG.)', 'Hang Time - (AVG.)', 'Carry Distance - (AVG.)', 'Carry Efficiency - (AVG.)', 'Total Distance Efficiency - (AVG.)'])]","1c1fb29f":"#Replacing the \"'\" with \"f\" so it will be properly interpreted by my conversion function\ndata['Value'] = data['Value'].map(lambda x:x.replace('\\'', 'f'))\n\n#Removing Commas\ndata['Value'] = data['Value'].map(lambda x:x.replace(',', ''))\n\n#Function to convert feet and inches to metric\ndef to_meters(string):\n    if 'f' not in string:\n        return string\n    \n    else:    \n        feet = int(string.split('f')[0])\n        inches = int(string.split('f')[1].split('\"')[0].strip())\n\n        return 0.3048*feet + 0.0254*inches\n\n#Converting imperial units to metric\ndata['Value'] = data['Value'].map(lambda x:to_meters(x))\n\n#Finally, converting all values from string to numerical\ndata['Value'] = pd.to_numeric(data['Value'])\n\n","c83837e9":"data.info()","5b33143a":"pivot = pd.pivot_table(data = data, index = 'Player Name', columns = 'Statistic', values = 'Value')\npivot.head()","312f4bc2":"fig, ax = plt.subplots(4,3, figsize=(12,12))\ny_index=0\nx_index=0\nfor variable in pivot.drop(columns = 'Driving Distance').columns:\n    sns.scatterplot(data = pivot, x=variable, y='Driving Distance', ax=ax[x_index, y_index]).set(ylabel = '')\n    x_index += 1\n    if x_index == 4:\n        y_index += 1\n        x_index = 0\n\nplt.subplots_adjust(hspace = 0.3)\n\nfig, ax = plt.subplots(4,3, figsize=(12,12))\ny_index=0\nx_index=0\nfor variable in pivot.drop(columns = 'Driving Distance').columns:\n    sns.distplot(a=pivot[variable], ax=ax[x_index, y_index]).set(ylabel = '')\n    x_index += 1\n    if x_index == 4:\n        y_index += 1\n        x_index = 0\n\nplt.subplots_adjust(hspace = 0.3)","3856c04e":"X = pivot.drop(columns = ['Driving Distance', 'Launch Angle', 'Smash Factor', 'Spin Rate', 'Total Distance Efficiency'])\ny = pivot['Driving Distance']\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state = 10)\n","22babe0b":"#Normalizing data so that we all indendent variables will have equal weight in the model\nscaler = preprocessing.StandardScaler()\n\n#Fitting scaler model to my training data only. Fitting to train and test data assumes we know what our test data will be, and therefore is not an accurate test\nscaler.fit(X_train)\n\n#Transforming all data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n","4e00f17a":"#Creating a linear model using the training data\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X_train,y_train)\n\n#Evaluating the model against our test data\npredictions = lm.predict(X_test)\n\n#See how well the model performed\nlm.score(X_test,y_test)\n","8d63d67f":"coef = pd.DataFrame(lm.coef_, X.columns, columns = ['Coefficient'])\ncoef","a080466b":"#Checking correlation matrix to see which features are heavily correlated\npivot.corr()\nsns.heatmap(data=pivot.corr(), cmap = 'coolwarm')","aa17bd42":"#Creating variables for the predictor and target variables\nX = pivot.drop(columns = ['Driving Distance', 'Launch Angle', 'Smash Factor', 'Spin Rate', 'Total Distance Efficiency'])\ny = pivot['Driving Distance']\n\n#Removing some additional predictor variables to reduce multicolinearity\nX = X.drop(columns = 'Ball Speed') #'Club Head Speed' and 'Ball Speed' are likely colinear, as the club head speed directly affects the ball speed.\n\n#'Carry Distance' in golf refers to the total distance the ball travels. Because this takes the height of the ball into account, we can drop \"Distance to Apex\", \n#\"Apex Height\", and 'Hang Time', which are also directly related to the height of the ball\nX = X.drop(columns = ['Distance to Apex', 'Apex Height', 'Hang Time'])\n\n#Carry Efficiency = Carry Distance \/ Club Head Speed. Carry Distance and Club Head Speed are part of our model, so we can remove this\n#as it is essentially already accounted for.\nX = X.drop(columns = 'Carry Efficiency')\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state = 10)\n\n#Normalizing data so that we can gain more useful insights from our model coefficients\nscaler = preprocessing.StandardScaler()\n\n#Fitting scaler model to my training data only. Fitting to train and test data assumes we know what our test data will be, and therefore is not an accurate test\nscaler.fit(X_train)\n\n#Transforming all data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n#Creating a linear model using the training data\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X_train,y_train)\n\n#Evaluating the model against our test data\npredictions = lm.predict(X_test)\n\n#See how well our model performed\nlm.score(X_test,y_test)\n\ncoef = pd.DataFrame(lm.coef_, X.columns, columns = ['Coefficient'])\ncoef\n\nprint(coef)\nprint('Model score: ', lm.score(X_test,y_test))","fc4afbe5":"sns.heatmap(data = X.corr(), cmap='coolwarm')","15e00ade":"variables = pivot.drop(columns = 'Driving Distance').columns\nresults = pd.DataFrame(columns = ['Model score'])\n\nfor variable in variables:\n    X = pivot[variable].values.reshape(-1,1)\n    y = pivot['Driving Distance']\n\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state = 10)\n\n    scaler = preprocessing.StandardScaler()\n    scaler.fit(X_train)\n\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    lm = linear_model.LinearRegression()\n    model = lm.fit(X_train,y_train)\n\n    predictions = lm.predict(X_test)\n\n    lm.score(X_test,y_test)\n\n    coef = pd.DataFrame(lm.coef_)\n    \n    results.loc[variable] = lm.score(X_test,y_test)\n    \nresults","04b91118":"In the table above, we see that Carry Distance is the greatest predictor of Driving Distance. It predicts almost 92% of the variablity in Driving Distance alone, which was only 1% worse than any multiple regression model we were able to produce. Other good predictors are Ball Speed (88%), Club Head Speed (81%), and Distance to Apex (83%).\n\nIn conclusion, I was able to determine that the greatest predictor of a PGA pro's Driving Distance is the Carry Distance (i.e. the total distance the ball travelled in the air). Adding more independent variables such as ball speed, club head speed, and distance to apex did not significantly improve the performance of the model. ","6e8088f5":"Now finally I will create a pivot table with \"Statistic\" and \"Player Name\" as the columns and rows, respectively.\n\nNote: The original data is measured from the 2011-2018 season. The numbers you see corresponding to each player from each season are averages over this time frame. ","4ddd65be":"The goal is is to create a model that will predict a player's driving distance based on a number of other predicting factors. I will be examining and cleaning the data first","2873bc49":"We can see above in this correlation matrix that many of the independent variables we chose are highly correlated with eachother. This confirms the multicolinearity. I will try to trim the number of independent variables down to a more reasonable number. ","b7bfaf1b":"Now I will create the model, and evaluate its performance.","c8133f42":"As is, the model achieved a R^2 value of 0.92. This means that the modeal can explain 92% of the variability in a player's driving distance. Now we go further and see which of these variables explains the most of the variablity i.e. which variable is doing the most \"work\" in the model. ","6bc364eb":"It appears that the coefficients are not providing meaningful information. Normally, coefficients will describe the effect of one predictor variable on the target variable e.g. \u0394y = (coef)\u0394X. I can tell these are not correct because many coefficients are negative, and nearly all of the chosen predictor variables have positive relationships with the target variable. This indicates that the data has a high degree of multicolinearity, which means that the values of a given predictor variable in our model can be more-or-less predicted by one or more other variables in the model. This is not surprising, the scatterplots above have very similar shapes, and therefor very similar relationships with the target variable. Multicolinearity does not affect the predictive power of the model, but it does limit our ability to see which predictor variables are doing the most \"work\" in the model. \n\nWe can remedy this by removing some predictor variables from the model. It's possible that this decreases the model performance because we are omitting information. However, these variables are likely not adding much to the model because of multicolinearity. Our model performance might also increase by removing predictor variables, as it's possible that our model was \"overfit\" to our training data. \n\nI will create another model, this time with fewer predictor variables.","4d4b4b82":"The data here is in long form which is not very useful to us. I'm going to create a pivot table to make the data more useful once I've narrowed it down to the predicting variables of interest. ","80bf9d40":"I read through the list of different types of statistics. I picked a few that I think will help predict driving distance accurately, I'll create a new df called \"data\" to hold these. ","c5e63696":"Now to convert the values. The only tricky thing here is that the \"Distance to Apex - (AVG.)\" variable is in the format X' Y'' instead of in meters like all the rest. I convert those to metric, and then I convert all the variables in the' \"Value\" columns to numeric data types that can be interpreted by the linear model.","a1c39368":"This model's performance is very similar to the other model, and the coefficients are a bit more normal looking but are still not reflecting the data. The correlation matrix above shows that Carry Distance and Club Head Speed are still highly correlated. It appears that the target variable, \"Driving Distance\" can be accurately predicted by almost any of the independent variables which are all highly correlated with eachother. Just for fun, I want to see how each of these independent variables perform alone in a linear model. ","bf1f48d8":"I notice here that the \"Value\" column is not numeric, which they will need to be in order to be able to do any sort of analysis with them. I'm going to first just narrow down the data to only the predictor variables of interest before I do the conversion, as I do not want to unncecessarily convert data that won't end up being used. ","959545f9":"Quickly checking for relationships between any of the independent variables and \"Driving Distance\", the target variable. I can see here that all variables are normally distributed, and that \"Ball Speed\", \"Carry Distance\", \"Club Head Speed\", and \"Distance to Apex\" all seem to have a strong, positivly corelated relationship with \"Driving Distance\". I will now start building the linear model, which will allow me to confirm that this is the case. I will start by removing columns with no visual correlation (Launch Angle, Smash Factor, Spin Rate, Total Distance Efficiency) and then doing a train\/test split on the data. The training data will be used to create the model. Once the model is created, it will be evaluated using the test data. This ensures that we do not over-fit the model to the data i.e. that the model will be accurate on *all* data, not just ours"}}