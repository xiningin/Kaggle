{"cell_type":{"51d54a4e":"code","24950678":"code","09f56769":"code","de88fd5f":"code","2e059ec1":"code","1ccb499c":"code","12f3448b":"code","069044db":"code","96d6ef21":"code","9d799a4b":"code","51795836":"code","2bb99fb2":"code","fde376bd":"code","bbb2e877":"code","df38578d":"code","fbecc7ce":"code","ff4c22c9":"code","7c661d63":"code","1cb90e0d":"code","75307f7d":"code","da24f785":"code","7ecf19c2":"code","99940390":"code","0a0f00d2":"code","544fdbcb":"code","0c8eb442":"code","2819a115":"code","115cca1d":"code","9514b6f6":"markdown","ca39e754":"markdown","d6a2217c":"markdown","8333b7ac":"markdown","75bc779b":"markdown","69b5e729":"markdown","19c617b1":"markdown","ba8a1c20":"markdown","f04cf246":"markdown"},"source":{"51d54a4e":"!pip install -qq transformers","24950678":"import torch \nimport transformers\nfrom transformers import BertTokenizer, AdamW, BertModel, get_linear_schedule_with_warmup\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\n%matplotlib inline\n\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","09f56769":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","de88fd5f":"# Remove the mislabelled tweets\nincorrect_labels_df = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\nincorrect_labels_df = incorrect_labels_df[incorrect_labels_df['target'] > 1]\nincorrect_texts = incorrect_labels_df.index.tolist()\ntrain = train[~train.text.isin(incorrect_texts)]\n\n# Add the keyword column to the text column\ntrain['keyword'].fillna('', inplace=True)\ntrain['final_text'] = train['keyword'] + ' ' + train['text'] \ntest['keyword'].fillna('', inplace=True)\ntest['final_text'] = test['keyword'] + ' ' + test['text'] ","2e059ec1":"train.head()","1ccb499c":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","12f3448b":"token_len = []\n\nfor txt in test.final_text:\n    tokens = tokenizer.encode(txt, max_length=512)\n    token_len.append(len(tokens))","069044db":"sns.distplot(token_len)\nplt.xlim([0,160])\nplt.xlabel('tokencount')","96d6ef21":"MAX_LEN=160\n","9d799a4b":"df_train, df_val = train_test_split(\n  train,\n  test_size=0.2,\n  random_state=RANDOM_SEED\n)","51795836":"class GPReviewDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        \n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self): \n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n                      review,\n                      add_special_tokens=True,\n                      max_length=self.max_len,\n                      return_token_type_ids=False,\n                      pad_to_max_length=True,\n                      return_attention_mask=True,\n                      return_tensors='pt',\n    )\n        return {\n              'review_text': review,\n              'input_ids': encoding['input_ids'].flatten(),\n              'attention_mask': encoding['attention_mask'].flatten(),\n              'targets': torch.tensor(target, dtype=torch.long)\n    }\n","2bb99fb2":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPReviewDataset(\n    reviews=df.text.to_numpy(),\n    targets=df.target.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n    return DataLoader(\n            ds,\n            batch_size=batch_size,\n            num_workers=4\n          )\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)","fde376bd":"data = next(iter(val_data_loader))\ndata.keys()\n\nprint(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)\n","bbb2e877":"class DisasterClassifier(nn.Module):\n    \n    def __init__(self,n_classes):\n        super(DisasterClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n            \n    def forward(self, input_ids, attention_mask):\n   \n        _,pooled_output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask\n        )\n        \n        output = self.drop(pooled_output)\n        return  self.out(output)","df38578d":"model = DisasterClassifier(2)\nmodel = model.to(device)","fbecc7ce":"\nEPOCHS = 3\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias = False)\ntotal_steps = len(train_data_loader)* EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)\n","ff4c22c9":"def train_epoch( model, dataloader, loss_fn, optimizer, device, scheduler,n_examples):\n    \n    model = model.train()\n    \n    losses =[]\n    correct_pred = 0\n    \n    for d in dataloader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        \n        \n        outputs = model(\n            input_ids =input_ids,\n            attention_mask = attention_mask\n        )\n        \n        _,pred = torch.max(outputs , dim=1)\n        \n        loss = loss_fn(outputs, targets)\n        \n        correct_pred += torch.sum(pred == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n       \n    return correct_pred.double() \/ n_examples, np.mean(losses)","7c661d63":"def train_eval( model, dataloader, loss_fn, device,n_examples):\n    \n    model = model.eval()\n    \n    losses =[]\n    correct_pred = 0\n    \n    with torch.no_grad():\n        \n        for d in dataloader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n        \n        \n            outputs = model(\n                input_ids =input_ids,\n                attention_mask = attention_mask\n                )\n        \n            _,pred = torch.max(outputs , dim=1)\n        \n            loss = loss_fn(outputs, targets)\n        \n            correct_pred += torch.sum(pred == targets)\n            \n            losses.append(loss.item())\n            \n    return correct_pred.double()\/ n_examples, np.mean(losses)","1cb90e0d":"%%time\n\nhistory = defaultdict(list)\nbest_acc =0\n\nfor epoch in range(EPOCHS):\n    \n    \n    print(f'epochs {epoch+1}\/{EPOCHS}')\n          \n    \n    train_acc, train_loss = train_epoch(model,train_data_loader,loss_fn, optimizer, device,scheduler, len(df_train))      \n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n          \n          \n    val_acc, val_loss = train_eval(model,val_data_loader,loss_fn,device,len(df_val)  )\n          \n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n          \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n          \n    if val_acc > best_acc:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_acc = val_acc\n","75307f7d":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","da24f785":"class DisasterTestDataset(Dataset):\n    def __init__(self, tweets, tokenizer, max_len):\n        self.tweets = tweets\n        self.tokenizer= tokenizer\n        self.max_len =max_len\n        \n    def __len__(self):\n        return len(self.tweets)\n    \n    def __getitem__(self, item):\n        \n            \n        tweet = str(self.tweets[item])\n         \n                \n    \n        encoding = self.tokenizer.encode_plus(\n                    tweet,\n                    add_special_tokens=True,\n                    max_len = self.max_len,\n                    return_token_type_ids =False,\n                    pad_to_max_length = True,\n                    return_attention_mask=True,\n                    return_tensors='pt',          \n                )\n        \n        return {\n                'tweets':tweet,\n                'input_ids':encoding['input_ids'].flatten(),\n                'attention_mask':encoding['attention_mask'].flatten(),\n                \n        }\n        \n        \n    \n","7ecf19c2":"def create_Testdata_loader(df, tokenizer, max_len, batch_size):\n    \n    ds = DisasterTestDataset(\n         tweets=df.text.to_numpy(),\n         tokenizer=tokenizer,\n         max_len=max_len\n      )\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=4\n          )\nBATCH_SIZE = 16\ntest_data_loader = create_Testdata_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)\n","99940390":"def get_predictions(model, data_loader):\n    \n    model = model.eval()\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"tweets\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            \n            outputs = model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask\n                      )\n            \n            _, preds = torch.max(outputs, dim=1)\n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            \n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n   \n    return review_texts, predictions, prediction_probs\n","0a0f00d2":"y_review_texts, y_pred, y_pred_probs = get_predictions(\n  model,\n  test_data_loader\n)","544fdbcb":"print(y_pred_probs[:10])","0c8eb442":"print(y_pred[:10])","2819a115":"#print(classification_report(y_test, y_pred, target_names=class_names))","115cca1d":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['target'] = y_pred\nsubmission.to_csv('submission12.csv', index=False)","9514b6f6":"PYTORCH DATASET","ca39e754":"Create Dataloader\n","d6a2217c":"100 would be great maxlen.","8333b7ac":"DATA PREProcessing Requires\n* Add special tokens to separate sentences and do classification [CLS] and [SEP]\n* Pass sequences of constant length (introduce padding)\n* Create array of 0s (pad token) and 1s (real token) called attention mask","75bc779b":"Function for Evaluation","69b5e729":"Inference","19c617b1":"Training","ba8a1c20":"Helper function for training ","f04cf246":"\nModel building\n"}}