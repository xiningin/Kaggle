{"cell_type":{"bda523e2":"code","43a02ff5":"code","5b53cb06":"code","2d136a48":"code","18aabe0f":"code","2a308b6b":"code","6935ba51":"code","33e65cc0":"code","38dd8c24":"code","c30cc7ea":"code","2b2da71d":"code","d9df02aa":"code","5c76993c":"code","917e5870":"code","3b249164":"code","dbe21d05":"code","562587c4":"code","59bcb700":"code","115a7c7c":"code","1dc9e365":"code","1475b7fb":"code","8ea3e67d":"code","076cb841":"code","d9e0b9c2":"code","0ff06ebc":"markdown","d2ff9153":"markdown","76f5ba78":"markdown","560f62b7":"markdown","57cdad94":"markdown","b94bc272":"markdown","d1850cb0":"markdown","c131f7f0":"markdown","175426b8":"markdown","131dfe22":"markdown","eb250c31":"markdown","887a5e59":"markdown","b0335192":"markdown","2419a688":"markdown","defd6826":"markdown","3937dec6":"markdown","49ab3715":"markdown","dccb1126":"markdown","56c303ca":"markdown","1acd802a":"markdown","4b0c5284":"markdown","5fbf6dc5":"markdown","a8716fa9":"markdown"},"source":{"bda523e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==1: \n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","43a02ff5":"!pip install scispacy","5b53cb06":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz","2d136a48":"# import commands to\n# convert text to a matrix of\n# tokens\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# import sklearn LDA function\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# import scispacy, a repo of commands\n# to deal with scientific documents\nimport scispacy\n\n# import spaCy, a repo of commands\n# to deal with natural language processing \n# (NLP)\nimport spacy\n\n# Of the spaCy library, \n# the en_core_sci_sm library contains\n# a full spaCy pipeline for biomedical data \nimport en_core_sci_sm\n\n# import command to measure\n# the Jensen-Shannon distance (metric)\nfrom scipy.spatial.distance import jensenshannon\n\n# import joblib, a repo of commands\n# to run python functions as pipeline\n# jobs\nimport joblib\n\n# import pyLDAvis, a repo of commands\n# to create interactic topic model visualizations\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()","18aabe0f":"biorxiv_clean = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv\")\nclean_comm_use = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv\")\nclean_noncomm_use = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv\")\nclean_pmc = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv\")\n\nall_data = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True)\n\nall_data.head()","2a308b6b":"print(\"Number of Rows in Table: %i\" % len(all_data))\nprint(\"Number of Titles: %i \" % all_data['title'].count())\nprint(\"Number of Abstracts: %i \" % all_data['abstract'].count())\nprint(\"Number of Texts: %i \" % all_data['text'].count())","6935ba51":"# replace empty text with empty strings\nall_text = all_data.text.str.replace('\\n\\n', '')","33e65cc0":"all_text[58][:500]","38dd8c24":"# initalize the spaCy pipeline\n# for biomedical data\nnlp = en_core_sci_sm.load()","c30cc7ea":"# ignore some words that are irrelevant for the content\nstop_words = text.ENGLISH_STOP_WORDS.union({'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', '10'})\n\ntf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n                                stop_words = stop_words,\n                                lowercase = True\n                               )\n\ntf = tf_vectorizer.fit_transform(all_text)\n\ntf.shape","2b2da71d":"lda_csv_file=\"..\/input\/exploring-corona-competition\/lda_10topics.csv\"\nntopics=10\nif os.path.exists(lda_csv_file):\n    lda_tf = joblib.load(lda_csv_file)\nelse:\n    lda_tf = LatentDirichletAllocation(n_components=ntopics,\n                                       random_state=0)\n    lda_tf.fit(tf)\njoblib.dump(lda_tf, 'lda.csv')","d9df02aa":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","5c76993c":"tfidf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda_tf, tfidf_feature_names, 20)","917e5870":"viz = pyLDAvis.sklearn.prepare(lda_tf, tf, tf_vectorizer)","3b249164":"pyLDAvis.display(viz)","dbe21d05":"pyLDAvis.save_html(viz, 'lda.html')","562587c4":"topic_dist = pd.DataFrame(lda_tf.transform(tf))\n\ntopic_dist.head()\n","59bcb700":"def get_k_nearest_docs(doc_dist, k=5, use_jensenshannon=True):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen\u2013Shannon divergence\/ Euclidean distance in topic space). \n    '''\n    \n    if use_jensenshannon:\n            distances = topic_dist.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    else:\n        diff_df = topic_dist.sub(doc_dist)\n        distances = np.sqrt(np.square(diff_df).sum(axis=1)) # euclidean distance (faster)\n        \n    return distances[distances != 0].nsmallest(n=k).index","115a7c7c":"def recommendation(paper_id, k=5):\n    '''\n    Returns the title of the k papers that are closest (topic-wise) to the paper given by paper_id.\n    '''\n    \n    print(all_data.title[all_data.paper_id == paper_id].values[0])\n    print('\\nRELATED DOCUMENTS: \\n')\n    recommended = get_k_nearest_docs(topic_dist[all_data.paper_id == paper_id].iloc[0], k)\n    for i in recommended:\n        print('- ', all_data.title[i] )","1dc9e365":"recommendation('11f96d05db6854ef95312aa3a4736724ce1f02d6', k=5)","1475b7fb":"recommendation('1d2947d5c0addcbbcfa71d87ea074a8b6ff2a973', k=5)","8ea3e67d":"def relevant_articles(tasks, k=3):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_tf = tf_vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda_tf.transform(tasks_tf))\n\n    for index, bullet in enumerate(tasks):\n        print('\\n=============================================')\n        print('\\nBULLET: ' + bullet + '\\n')\n\n        print('\\nRELATED DOCUMENTS: \\n')\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k)\n        for i in recommended:\n            print('- ', all_data.title[i] )","076cb841":"npi_task_raw=[\"Guidance on ways to scale up non-pharmaceutical interventions in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\", \\\n\"Rapid design and execution of experiments to examine and compare non-pharmaceutical interventions currently being implemented. Department of Homeland Security Centers for Excellence could potentially be leveraged to conduct these experiments.\", \\\n\"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\", \\\n\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\", \\\n\"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\", \\\n\"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with non-pharmaceutical interventions.\", \\\n\"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\", \\\n\"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]","d9e0b9c2":"relevant_articles(npi_task_raw, 5)","0ff06ebc":"Let's transform our texts into a token matrix.","d2ff9153":"Let's copy and paste the task details from the kaggle website and see what we get.","76f5ba78":"** Use NLP and training data to list what we know about non-pharmaceutical interventions **","560f62b7":"# Papers on Topic: What do we know about non-pharmaceutical interventions?\nWe can now also map a task or bullet point into the topic space and find related articles that might help to solve the question at hand.","57cdad94":"So the all_data table contains text for each paper. Before I dive deeper into the data, I want review the goal of this notebook:","b94bc272":"We want to know which words are the most prominent in each topic. These top words will help us get a general idea of the different topics.","d1850cb0":"# Install\/Load Packages","c131f7f0":"# Task\nWhat do we know about non-pharmaceutical interventions?","175426b8":"# Discovered Topics","131dfe22":"My code below is inspired by his notebook.","eb250c31":"Each article is a mixture of topics","887a5e59":"Next, we perform LDA and allocate 20 topics. Note that this takes awile so if we've already run this before we can just use the csv file made in the previous run.","b0335192":"# Future Directions\n* Optimize LDA\n* Analyze relative documents to provide answers to the tasks","2419a688":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*","defd6826":"Now that all our libraries are loaded we need data. We explore the full text in the files using the output generated from the following notebook:\nhttps:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv","3937dec6":"To start, we would want to filter for specific papers that discuss this topic.","49ab3715":"# Latent Dirichlet Allocation (LDA)\n[LDA](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) is an example of a [topic model](https:\/\/en.wikipedia.org\/wiki\/Topic_model). In other words,the general purpose of LDA is to classify documents into topics. For example, if we have a good topic model, we expect that documents with words such as \"bone\", \"puppy\", \"bark\", \"dogs\" would be classified as documents with \"dog\" topics, and documents with words such as \"meow\",\"litter\",\"feline\",\"cat\" would be classified as \"cat\" topics.","dccb1126":"Cool!","56c303ca":"The first block of code is (almost) directly from kaggle","1acd802a":"Shoutout to @danielwolffram who already  did some topic modeling: https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles","4b0c5284":"# Data Preparation\nFirst we build an object containing all the texts","5fbf6dc5":"# Introduction","a8716fa9":"# Get \"Nearest\" Papers (in Topic Space)"}}