{"cell_type":{"bd4532f0":"code","e1115e9a":"code","0888e084":"code","07dff4bb":"code","ede1743d":"code","e4677b1f":"code","c76a3bd3":"code","700bdde4":"code","f6d58c1c":"code","090a97a8":"code","27e746cb":"code","47ebe9d9":"code","0c9dd2cf":"markdown","2c4103b3":"markdown","a2c094e4":"markdown","4a79ac91":"markdown","5ff955e4":"markdown","16b750da":"markdown","b1543cec":"markdown","842a9b5f":"markdown","672fdb07":"markdown","62684c1a":"markdown","f04b8508":"markdown"},"source":{"bd4532f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1115e9a":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport nltk\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport nltk\nfrom nltk.tokenize import word_tokenize","0888e084":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nall_df = pd.concat([train, test], sort = False).reset_index(drop=True)","07dff4bb":"train['excerpt_len'] = train['excerpt'].apply(lambda x : len(x))\ntrain['excerpt_wordCnt'] = train['excerpt'].apply(lambda x : len(x.split(' ')))\ntrain['excerpt_sentCnt'] = train['excerpt'].apply(lambda x : len(x.split('.')))\ntrain[\"mean_WordPerSent\"] = train[\"excerpt_wordCnt\"]\/train[\"excerpt_sentCnt\"]\ntrain[\"mean_LenPerWord\"] = train[\"excerpt_len\"]\/train[\"excerpt_wordCnt\"]","ede1743d":"X_train = train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\n\nX_train[\"id\"] = X_train[\"id\"].astype(\"category\")\nX_train.head()","e4677b1f":"kf = KFold(n_splits = 3)\nmodels = []\nrmses =[]\nlgbm_params ={\"objective\":\"regression\", \"random_seed\":1234}\n\nfor train_index, val_index in kf.split(X_train):\n    XX_train = X_train.iloc[train_index]\n    XX_valid = X_train.iloc[val_index]\n    YY_train = Y_train.iloc[train_index]\n    YY_valid = Y_train.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(XX_train, YY_train)\n    lgbm_eval = lgbm.Dataset(XX_valid, YY_valid)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 100,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(YY_valid, y_pred))\n    print (tmp_rmse)\n    \n    models.append(model_lgbm)\n    rmses.append(tmp_rmse)","c76a3bd3":"countvec = CountVectorizer(stop_words = \"english\")\ncv = countvec.fit_transform(all_df[\"excerpt\"])\ndfcv = cv.toarray()\ndfcv_sum = pd.DataFrame(np.sum(dfcv,axis=1))\ndfcv_sum.columns = [\"excerpt_cv\"]\n\nall_df = pd.concat([train, test], sort = False).reset_index(drop=True)\nall_df['excerpt_len'] = all_df['excerpt'].apply(lambda x : len(x))\nall_df['excerpt_wordCnt'] = all_df['excerpt'].apply(lambda x : len(x.split(' ')))\nall_df['excerpt_sentCnt'] = all_df['excerpt'].apply(lambda x : len(x.split('.')))\nall_df[\"mean_WordPerSent\"] = all_df[\"excerpt_wordCnt\"]\/all_df[\"excerpt_sentCnt\"]\nall_df[\"mean_LenPerWord\"] = all_df[\"excerpt_len\"]\/all_df[\"excerpt_wordCnt\"]\n\nall_df = pd.concat([all_df, dfcv_sum], axis=1)\nX_train = all_df[~all_df[\"target\"].isnull()]\nX_train = X_train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\nX_train.head()","700bdde4":"tfvec = TfidfVectorizer(stop_words=\"english\")\ntfv = tfvec.fit_transform(all_df[\"excerpt\"])\ndftfv = tfv.toarray()\n\ndftfv_sum = pd.DataFrame(np.sum(dftfv,axis=1))\ndftfv_sum.columns = [\"excerpt_tfv\"]\n\nall_df = pd.concat([all_df, dfcv_sum, dftfv_sum], axis=1)\nX_train = all_df[~all_df[\"target\"].isnull()]\nX_train = X_train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\nX_train.head()","f6d58c1c":"all_df['tokenized_excerpt'] = [word_tokenize(p.lower()) for p in all_df[\"excerpt\"]]\nall_df['word_count_tk'] = all_df['tokenized_excerpt'].apply(lambda x: len(x))\nall_df[\"avg_LenPerWord\"] = all_df[\"excerpt_len\"]\/all_df[\"excerpt_wordCnt\"]\nall_df['sent_excerpt'] = [nltk.tokenize.sent_tokenize(x) for x in all_df[\"excerpt\"]]\nall_df['excerpt_sentCnt'] = all_df['sent_excerpt'].apply(lambda x: len(x))\nall_df[\"avg_WordPerSent\"] = all_df[\"excerpt_wordCnt\"]\/all_df[\"excerpt_sentCnt\"]","090a97a8":"import scipy\n\ncountvec = CountVectorizer()\ncv = countvec.fit_transform(all_df[\"excerpt\"])\n\ntfvec = TfidfVectorizer(stop_words=\"english\")\ntfv = tfvec.fit_transform(all_df[\"excerpt\"])\n\nX = scipy.sparse.hstack((cv, tfv)).tocsr()\n\nX_train = X[:len(train)]\nX_test = X[len(train):]\nY_train = train[\"target\"]\n\nX_train = X_train.toarray()\nX_test = X_test.toarray()\n\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)","27e746cb":"models = []\nrmses =[]\nlgbm_params ={\n    \"objective\":\"regression\",\n    \"random_seed\":1234,\n    \"learning_rate\":0.05,\n    \"n_estimators\":1000,\n    'num_leaves': 64,\n    'max_bin': 82,\n    'bagging_fraction': 0.7315391015500504,\n    'bagging_freq': 3,\n    'feature_fraction': 0.41032549973286436,\n    'min_data_in_leaf': 13,\n    'min_sum_hessian_in_leaf': 4\n}\n\nfor train_index, val_index in kf.split(X_train):\n    XX_train = X_train.iloc[train_index]\n    XX_valid = X_train.iloc[val_index]\n    YY_train = Y_train.iloc[train_index]\n    YY_valid = Y_train.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(XX_train, YY_train)\n    lgbm_eval = lgbm.Dataset(XX_valid, YY_valid)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 100,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(YY_valid, y_pred))\n    print (tmp_rmse)\n    \n    models.append(model_lgbm)\n    rmses.append(tmp_rmse)","47ebe9d9":"preds = []\n\nfor model in models:\n    pred = model.predict(X_test)\n    preds.append(pred)\n    \npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis =0)\n\nsubmission = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmission[\"target\"] = preds_mean\nsubmission.to_csv(\"submission.csv\", index=False)","0c9dd2cf":"When think about \"what is readability?\", what I came up with at first was \"less sentences, short sentences, less words\". Based on this easy idea, tried to use number of words\/sentences which were created by easy method, as well as average lengths...\n\n\n\u300c\u8aad\u307f\u3084\u3059\u3055\u3068\u306f\u4f55\u304b\uff1f\u300d\u3092\u8003\u3048\u305f\u3068\u304d\u3001\u307e\u305a\u601d\u3044\u3064\u304f\u306e\u306f\u300c\u6587\u7ae0\u304c\u5c11\u306a\u3044\u3001\u5404\u6587\u304c\u77ed\u3044\u3001\u5358\u8a9e\u304c\u5c11\u306a\u3044\u300d\u3068\u3044\u3046\u3053\u3068\u304b\u306a\uff1f\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u6587\u5b57\u6570\u30fb\u5358\u8a9e\u6570\u30fb\u6587\u7ae0\u6570\u300d\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u51fa\u3057\u3066\u307f\u308b\u3002\u5358\u8a9e\u6570\u30fb\u6587\u7ae0\u6570\u306f\u7c21\u6613\u7684\u306b\u30b9\u30da\u30fc\u30b9\u30fb\u30d4\u30ea\u30aa\u30c9\u3067\u533a\u5207\u3063\u305f\u6570\u3068\u3059\u308b\u3002\u305d\u308c\u3089\u3092\u5272\u308a\u7b97\u3057\u3066\u6587\u7ae0\u5f53\u305f\u308a\u306e\u5e73\u5747\u5358\u8a9e\u6570\u30fb\u5358\u8a9e\u5f53\u305f\u308a\u306e\u6587\u5b57\u6570\u3082\u51fa\u3057\u3066\u307f\u308b\u3002","2c4103b3":"Try to consider other Features. What came up is the idea \"Readability = written by easy words\", but not sure how to capture 'easy words'... I could say \"the easier and easier, the more and more frequently appears\". Based on this idea, try to use Countvectorizer. \n\n\n\u4ed6\u306e\u7279\u5fb4\u91cf\u3092\u8003\u3048\u3066\u307f\u308b\u3002\u8aad\u307f\u3084\u3059\u3055\uff1d\u300c\u7c21\u5358\u306a\u5358\u8a9e\u3067\u66f8\u304b\u308c\u3066\u3044\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u304b\u306a\uff1f\u3068\u3082\u601d\u3046\u306e\u3060\u304c\u3001\u3069\u3046\u3084\u3063\u3066\u300c\u7c21\u5358\u306a\u300d\u3092\u628a\u63e1\u3059\u308c\u3070\u826f\u3044\u304b\u5206\u304b\u3089\u306a\u3044\u30fb\u30fb\u30fb\u300c\u7c21\u5358\u306a\u5358\u8a9e\u307b\u3069\u4f55\u5ea6\u3082\u51fa\u3066\u304f\u308b\u300d\u3068\u8a00\u3048\u306a\u304f\u3082\u306a\u3044\u306e\u3067\u3001\u5358\u7d14\u306bCountvectorizer\u3067\u51fa\u73fe\u983b\u5ea6\u3092\u52a0\u3048\u3066\u307f\u308b\u3002\u65e2\u5b58\u306e\u7279\u5fb4\u91cf\u3068\u5408\u308f\u305b\u3066\u5b66\u7fd2\u3055\u305b\u308b\u305f\u3081\u3001\u5408\u7b97\u3057\u3066Pandas\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u8ffd\u52a0\u3059\u308b\u3002","a2c094e4":"From a quick look, it seems 'not so bad', and try to submit using these features as first try. I'll try to use LightGBM which I'm familiar with, after cross-validation.\n\n\n\u307e\u305a\u307e\u305a\u306e\u3088\u3046\u306b\u3082\u898b\u3048\u308b\u306e\u3067\u3001\u3068\u308a\u3042\u3048\u305a\u3053\u308c\u3089\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u3066\u6700\u521d\u306eSubmit\u3092\u3057\u3066\u307f\u308b\u3002\u4f7f\u3044\u6163\u308c\u305fLightGBM\u3067\u3001\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u3057\u305f\u5f8c\u3001\u4e88\u6e2c\u3057\u3066\u307f\u308b\u3002","4a79ac91":"Execute LightGBM again after setting the best parameters found by Optuna.\n\n\nOptuna\u3067\u5f97\u3089\u308c\u305f\u30d9\u30b9\u30c8\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30bb\u30c3\u30c8\u3057\u3001LightGBM\u3092\u518d\u5ea6\u5b9f\u884c\u3002","5ff955e4":"Try to predict again and submit after re-counting words or sentences using Tokenizer. The score shows 0.860. Again, a little bit improved. I'd like to expect more improvement. Reconsider whether my first approach to count words or sentences.. Was it really correct? As for the countvectorizer or tfidfvectorizer as well, would it be better to use them as an array, rather than sum up? \n\n\nTokenizer\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u6570\u30fb\u6587\u6570\u30fb\u5e73\u5747\u5024\u7b49\u3092\u6570\u3048\u306a\u304a\u3057\u3001\u518d\u5ea6\u4e88\u6e2c\u3092\u3084\u308a\u76f4\u3057Submit\u3057\u3066\u307f\u308b\u3002\u7d50\u679c\u306f0.860\u3002\u307e\u305f\u5fae\u5999\u306b\u5c11\u3057\u6539\u5584\u3002\u5f90\u3005\u306b\u306f\u6539\u5584\u3057\u3066\u3044\u308b\u3082\u306e\u306e\u3001\u3082\u3046\u5c11\u3057\u5927\u5e45\u306b\u6539\u5584\u3057\u305f\u3044\u3002\u6700\u521d\u306b\u601d\u3044\u3064\u3044\u305f\u5358\u8a9e\u6570\u3068\u304b\u6587\u7ae0\u6570\u3068\u304b\u306e\u7279\u5fb4\u91cf\u3063\u3066\u3001\u672c\u5f53\u306b\u6709\u52b9\u3060\u3063\u305f\u306e\u3060\u308d\u3046\u304b\uff1fCountvectorizer\u3068\u304bTfidfvectorizer\u3068\u304b\u3082\u914d\u5217\u3092\u5408\u7b97\u3057\u3066\u5024\u3068\u3057\u3066\u7528\u3044\u3066\u3044\u308b\u304c\u305d\u306e\u307e\u307e\u914d\u5217\u3068\u3057\u3066\u4f7f\u3063\u305f\u307b\u3046\u304c\u826f\u304b\u3063\u305f\u308a\u3057\u306a\u3044\u304b\uff1f","16b750da":"Predict again using the newly-added Tfidif sum. The score shows 0.885. Slightly improved again, but still alomost the same...By chance, it may have been bad that I used the easy method to separate using a space or a period in order to count the number of words or sentences. try to separate using the tokenizer.\n\n\n\u65b0\u305f\u306b\u52a0\u3048\u305fTfidf\u5408\u7b97\u5024\u3082\u4f7f\u3063\u3066\u518d\u5ea6\u4e88\u6e2c\u3002\u7d50\u679c\u306f0.885\u3002\u3061\u3087\u3063\u3068\u826f\u304f\u306a\u3063\u305f\u304b\u306a\u3041\u3002\u3067\u3082\u3084\u306f\u308a\u4f59\u308a\u5909\u308f\u3089\u306a\u3044\u3002\u5358\u8a9e\u3084\u5206\u306e\u6570\u3092\u6570\u3048\u308b\u306e\u306b\u30b9\u30da\u30fc\u30b9\u3067\u533a\u5207\u3063\u305f\u308a\u30d4\u30ea\u30aa\u30c9\u3067\u533a\u5207\u3063\u305f\u308a\u3068\u7c21\u6613\u7684\u306a\u65b9\u6cd5\u3092\u7528\u3044\u305f\u306e\u304c\u826f\u304f\u306a\u304b\u3063\u305f\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002tokenizer\u3067\u533a\u5207\u3063\u3066\u307f\u308b\u3002","b1543cec":"(Prediction portion omitted) First submission. The score was 0.909...\"Bad!\" \n\uff08\u4e88\u6e2c\u90e8\u5206\u306f\u7701\u7565\uff09\u307e\u305a\u306f\u6700\u521d\u306eSubmit\u3002\u30b9\u30b3\u30a2\u306e\u7d50\u679c\u306f\"0.909\"\u30fb\u30fb\u30fb\u300c\u4f4e\u3063\uff01\u300d","842a9b5f":"The result is 0.779. This is what a begginer like me can do after trial and error.\n\n\u7d50\u679c\u306f0.779\u3002\u521d\u5fc3\u8005\u306e\u8a66\u884c\u932f\u8aa4\u3068\u3057\u3066\u306f\u3053\u3093\u306a\u3082\u306e\u3067\u3057\u3087\u3046\u304b\u3002","672fdb07":"**Beginner's gradual improvement from 0.909 to 0.779 (English \/ \u65e5\u672c\u8a9e)**\n\nThis is the first time for me to participate the Competition after joining Kaggle 2 months ago. I intend to accumulate my experiences by joinnig anything. Below is the record of gradual score improvement which a beginner like me made after trial and error.\n\nKaggle\u3092\u59cb\u3081\u30662\u304b\u6708\u3001\u59cb\u3081\u3066Competition\u306b\u53c2\u52a0\u3057\u3066\u307f\u308b\u3002Titanic\u3082\u6c7a\u3057\u3066\u4e0a\u4f4d\u3067\u306f\u306a\u3044\u304c\u3001\u3068\u306b\u304b\u304f\u8272\u3005\u53c2\u52a0\u3057\u3066\u7d4c\u9a13\u5024\u3092\u7a4d\u3093\u3067\u3044\u3053\u3046\u3068\u601d\u3046\u3002\u521d\u5fc3\u8005\u304c\u56db\u82e6\u516b\u82e6\u3057\u3066\u3001\u30d1\u30d6\u30ea\u30c3\u30af\u30b9\u30b3\u30a2\u30920.909\u21920.900\u21920.885\u21920.860\u21920.798\u21920.779\u3078\u3068\u3001\u5f90\u3005\u306b\u6539\u5584\u3057\u3066\u3044\u3063\u305f\u8a18\u9332\u3067\u3059\u3002","62684c1a":"Try to submit after adding the sum of Countetorizer as a feature. The score becomes 0.99. Slightly improved, but alomost the same...Based on the idea that DIFFICULT sentence includes difficult words which rarely appear, try to add Tfidf.\n\n\nCountvectorizer\u306e\u5408\u7b97\u5024\u3092\u7279\u5fb4\u91cf\u306b\u52a0\u3048\u3066\u518d\u5ea6Submit\u3057\u3066\u307f\u308b\u3002\u30b9\u30b3\u30a2\u306e\u7d50\u679c\u306f\u300c0.900\u300d\u3002\u5fae\u5999\u306b\u6539\u5584\u3057\u305f\u304c\u6b86\u3069\u5909\u308f\u3089\u306a\u3044\u3002\u8aad\u307f\u306b\u304f\u3044\u6587\u7ae0\u306b\u306f\u3042\u307e\u308a\u4f7f\u308f\u308c\u306a\u3044\u30e0\u30ba\u30ab\u30b7\u30a4\u5358\u8a9e\u304c\u51fa\u3066\u304f\u308b\u3001\u3068\u3044\u3046\u3053\u3068\u3082\u8a00\u3048\u308b\u304b\u306a\u3068\u3044\u3046\u8003\u3048\u3067\u3001Tfidf\u3082\u7279\u5fb4\u91cf\u306b\u52a0\u3048\u3066\u307f\u308b\u3002","f04b8508":"Try to predict and submit again, using Countectorizer and Tfidfvectorizer as an array and not using number of words \/ sentences. The score shows 0.798. Improvement is the biggest. As expected, it'd be better not to stick to the number of words \/ sentences. Aiming a bit more improvement, try to optimize the parameters using Optuna.\n\n\n\u5358\u8a9e\u6570\u3084\u6587\u7ae0\u6570\u306f\u4f7f\u308f\u305a\u306b\u3001Countvectorizer\u3068Tfidfvectorizer\u3092\u305d\u306e\u307e\u307e\u914d\u5217\u3068\u3057\u3066\u7528\u3044\u3001\u518d\u5ea6\u4e88\u6e2c\u30fbsubmit\u3057\u3066\u307f\u308b\u3002\u7d50\u679c\u306f0.798\u3002\u3053\u308c\u307e\u3067\u3088\u308a\u6539\u5584\u5e45\u304c\u5927\u304d\u3044\u3002\u3084\u3063\u3071\u308a\u5909\u306b\u5358\u8a9e\u6570\u3084\u6587\u7ae0\u6570\u3092\u7528\u3044\u306a\u3044\u307b\u3046\u304c\u826f\u3055\u305d\u3046\u3002\u3082\u3046\u5c11\u3057\u6539\u5584\u3057\u306a\u3044\u304b\u306a\u3041\u3068\u3044\u3046\u3053\u3068\u3067Optuna\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u6700\u9069\u5316\u3057\u3066\u307f\u308b\u3002"}}