{"cell_type":{"e67f3180":"code","a73c356e":"code","fe3807ac":"code","f9d72cd8":"code","13549ba3":"code","b4ce188d":"code","c10a6ed9":"code","342ba15d":"code","5e97484d":"code","21b1ae8d":"code","c6450ed9":"code","d73bd74c":"code","480819c3":"code","b561ae36":"code","ec3f7179":"code","1e27649c":"code","649fd487":"code","01463a69":"code","41d85665":"code","cb526480":"code","70d8e183":"code","a0a655af":"code","7ccb5a89":"code","c7572ae8":"code","1676bf94":"code","49672f26":"code","ff1bc512":"code","5f8e739c":"code","448229e7":"code","4a9eeb09":"code","6df21cb9":"code","6309a0c5":"code","e6c3d170":"code","e3b20ae6":"code","87db2a53":"code","b420f105":"code","e0a55d3b":"code","28a4ac41":"code","09b9cad6":"markdown","4e1b0687":"markdown","5eb6ca83":"markdown","5e27e394":"markdown","a73f379d":"markdown","e5b2e428":"markdown","730f0147":"markdown","4a85a6d9":"markdown","6351dc64":"markdown","1d209163":"markdown","50fb40ff":"markdown","d429044f":"markdown","60f7c4d7":"markdown","7e1990bc":"markdown","5d4fd77c":"markdown","30ce0fa7":"markdown","3ab34834":"markdown","bb431e2d":"markdown","441f67ee":"markdown","32d07547":"markdown","5d70e75f":"markdown","f1ad2673":"markdown","3d49f249":"markdown","f9c1512f":"markdown","2f8502eb":"markdown","c13edf00":"markdown","830a9d92":"markdown","9ee99757":"markdown","8d5828b5":"markdown","698c2fd6":"markdown","a2eee009":"markdown","ea8cc360":"markdown"},"source":{"e67f3180":"#Importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nimport matplotlib.pyplot as plt#visualization\nfrom PIL import  Image\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns #visualization\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport io\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization","a73c356e":"telcom = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n#first few rows\ntelcom.head()","fe3807ac":"print(\"Rows: \\n\", telcom.shape[0])\nprint(\"Columns: \\n\", telcom.shape[1])\nprint(\"Features: \\n\", telcom.columns.tolist())\nprint(\"Nulled: \\n\", telcom.isnull().sum().values.sum())\nprint(\"Unique values: \\n\", telcom.nunique())","f9d72cd8":"telcom.info()","13549ba3":"#Data Manipulation\n\n#Replacing spaces with null values in total charges column\ntelcom['TotalCharges'] = telcom[\"TotalCharges\"].replace(\" \",np.nan)\n\n#Dropping null values from total charges column which contain .15% missing data \ntelcom = telcom[telcom[\"TotalCharges\"].notnull()]\ntelcom = telcom.reset_index()[telcom.columns]\n\n\n#convert to float type\ntelcom[\"TotalCharges\"] = telcom[\"TotalCharges\"].astype(float)\n\n#replace 'No internet service' to No for the following columns\nreplace_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n               'TechSupport','StreamingTV', 'StreamingMovies']\n\nfor i in replace_cols :\n    telcom[i] = telcom [i].replace({'No internet service' : 'No'})\n\n#replace values\ntelcom[\"SeniorCitizen\"] = telcom[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n\n\n#Tenure to categorical column\ndef tenure_lab(telcom) :\n    \n    if telcom[\"tenure\"] <= 12 :\n        return \"Tenure_0-12\"\n    elif (telcom[\"tenure\"] > 12) & (telcom[\"tenure\"] <= 24 ):\n        return \"Tenure_12-24\"\n    elif (telcom[\"tenure\"] > 24) & (telcom[\"tenure\"] <= 48) :\n        return \"Tenure_24-48\"\n    elif (telcom[\"tenure\"] > 48) & (telcom[\"tenure\"] <= 60) :\n        return \"Tenure_48-60\"\n    elif telcom[\"tenure\"] > 60 :\n        return \"Tenure_gt_60\"\ntelcom[\"tenure_group\"] = telcom.apply(lambda telcom:tenure_lab(telcom),\n                                      axis = 1)\n\n#Separating churn and non churn customers\nchurn     = telcom[telcom[\"Churn\"] == \"Yes\"]\nnot_churn = telcom[telcom[\"Churn\"] == \"No\"]\n\n#Separating catagorical and numerical columns\nId_col     = ['customerID']\ntarget_col = [\"Churn\"]\ncat_cols   = telcom.nunique()[telcom.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\nnum_cols   = [x for x in telcom.columns if x not in cat_cols + target_col + Id_col]","b4ce188d":"telcom.head()","c10a6ed9":"#labels\nlab = telcom[\"Churn\"].value_counts().keys().tolist()\n\n#values\nval = telcom[\"Churn\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer attrition in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","342ba15d":"#function  for pie plot for customer attrition types\ndef plot_pie(column) :\n    \n    trace1 = go.Pie(values  = churn[column].value_counts().values.tolist(),\n                    labels  = churn[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"Churn Customers\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    hole    = .6\n                   )\n    trace2 = go.Pie(values  = not_churn[column].value_counts().values.tolist(),\n                    labels  = not_churn[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    domain  = dict(x = [.52,1]),\n                    hole    = .6,\n                    name    = \"Non churn customers\" \n                   )\n\n\n    layout = go.Layout(dict(title = column + \" distribution in customer attrition \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            annotations = [dict(text = \"churn customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .15, y = .5),\n                                           dict(text = \"Non churn customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .88,y = .5\n                                               )\n                                          ]\n                           )\n                      )\n    data = [trace1,trace2]\n    fig  = go.Figure(data = data,layout = layout)\n    py.iplot(fig)\n\n\n#function  for histogram for customer attrition types\ndef histogram(column) :\n    trace1 = go.Histogram(x  = churn[column],\n                          histnorm= \"percent\",\n                          name = \"Churn Customers\",\n                          marker = dict(line = dict(width = .5,\n                                                    color = \"black\"\n                                                    )\n                                        ),\n                         opacity = .9 \n                         ) \n    \n    trace2 = go.Histogram(x  = not_churn[column],\n                          histnorm = \"percent\",\n                          name = \"Non churn customers\",\n                          marker = dict(line = dict(width = .5,\n                                              color = \"black\"\n                                             )\n                                 ),\n                          opacity = .9\n                         )\n    \n    data = [trace1,trace2]\n    layout = go.Layout(dict(title =column + \" distribution in customer attrition \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = column,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                           )\n                      )\n    fig  = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n#function  for scatter plot matrix  for numerical columns in data\ndef scatter_matrix(df)  :\n    \n    df  = df.sort_values(by = \"Churn\" ,ascending = True)\n    classes = df[\"Churn\"].unique().tolist()\n    classes\n    \n    class_code  = {classes[k] : k for k in range(2)}\n    class_code\n\n    color_vals = [class_code[cl] for cl in df[\"Churn\"]]\n    color_vals\n\n    pl_colorscale = \"Portland\"\n\n    pl_colorscale\n\n    text = [df.loc[k,\"Churn\"] for k in range(len(df))]\n    text\n\n    trace = go.Splom(dimensions = [dict(label  = \"tenure\",\n                                       values = df[\"tenure\"]),\n                                  dict(label  = 'MonthlyCharges',\n                                       values = df['MonthlyCharges']),\n                                  dict(label  = 'TotalCharges',\n                                       values = df['TotalCharges'])],\n                     text = text,\n                     marker = dict(color = color_vals,\n                                   colorscale = pl_colorscale,\n                                   size = 3,\n                                   showscale = False,\n                                   line = dict(width = .1,\n                                               color='rgb(230,230,230)'\n                                              )\n                                  )\n                    )\n    axis = dict(showline  = True,\n                zeroline  = False,\n                gridcolor = \"#fff\",\n                ticklen   = 4\n               )\n    \n    layout = go.Layout(dict(title  = \n                            \"Scatter plot matrix for Numerical columns for customer attrition\",\n                            autosize = False,\n                            height = 800,\n                            width  = 800,\n                            dragmode = \"select\",\n                            hovermode = \"closest\",\n                            plot_bgcolor  = 'rgba(240,240,240, 0.95)',\n                            xaxis1 = dict(axis),\n                            yaxis1 = dict(axis),\n                            xaxis2 = dict(axis),\n                            yaxis2 = dict(axis),\n                            xaxis3 = dict(axis),\n                            yaxis3 = dict(axis),\n                           )\n                      )\n    data   = [trace]\n    fig = go.Figure(data = data,layout = layout )\n    py.iplot(fig)\n\n#for all categorical columns plot pie\nfor i in cat_cols :\n    plot_pie(i)\n\n#for all categorical columns plot histogram    \nfor i in num_cols :\n    histogram(i)\n\n#scatter plot matrix\nscatter_matrix(telcom)","5e97484d":"#cusomer attrition in tenure groups\ntg_ch  =  churn[\"tenure_group\"].value_counts().reset_index()\ntg_ch.columns  = [\"tenure_group\",\"count\"]\ntg_nch =  not_churn[\"tenure_group\"].value_counts().reset_index()\ntg_nch.columns = [\"tenure_group\",\"count\"]\n\n#bar - churn\ntrace1 = go.Bar(x = tg_ch[\"tenure_group\"]  , y = tg_ch[\"count\"],\n                name = \"Churn Customers\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                opacity = .9)\n\n#bar - not churn\ntrace2 = go.Bar(x = tg_nch[\"tenure_group\"] , y = tg_nch[\"count\"],\n                name = \"Non Churn Customers\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Customer attrition in tenure groups\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"tenure group\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1,trace2]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","21b1ae8d":"telcom[['MonthlyCharges', 'TotalCharges','tenure',\"tenure_group\"]]\n\n#scatter plot monthly charges & total charges by tenure group\n\ndef plot_tenure_scatter(tenure_group,color) :\n    tracer = go.Scatter(x = telcom[telcom[\"tenure_group\"] == tenure_group][\"MonthlyCharges\"],\n                        y = telcom[telcom[\"tenure_group\"] == tenure_group][\"TotalCharges\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = tenure_group,\n                        opacity = .9\n                       )\n    return tracer\n\n#scatter plot monthly charges & total charges by churn group\ndef plot_churncharges_scatter(churn,color) :\n    tracer = go.Scatter(x = telcom[telcom[\"Churn\"] == churn][\"MonthlyCharges\"],\n                        y = telcom[telcom[\"Churn\"] == churn][\"TotalCharges\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = \"Churn - \" + churn,\n                        opacity = .9\n                       )\n    return tracer\n\ntrace1 = plot_tenure_scatter(\"Tenure_0-12\",\"#FF3300\")\ntrace2 = plot_tenure_scatter(\"Tenure_12-24\",\"#6666FF\")\ntrace3 = plot_tenure_scatter(\"Tenure_24-48\",\"#99FF00\")\ntrace4 = plot_tenure_scatter(\"Tenure_48-60\",\"#996600\")\ntrace5 = plot_tenure_scatter(\"Tenure_gt_60\",\"grey\")\ntrace6 = plot_churncharges_scatter(\"Yes\",\"red\")\ntrace7 = plot_churncharges_scatter(\"No\",\"blue\")\n\ndata1   = [trace1,trace2,trace3,trace4,trace5] \ndata2   = [trace7,trace6]\n\n#layout\ndef layout_title(title) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Monthly charges\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Total Charges\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            height = 600\n                           )\n                      )\n    return layout\n\nlayout1  = layout_title(\"Monthly Charges & Total Charges by Tenure group\")\nlayout2  = layout_title(\"Monthly Charges & Total Charges by Churn group\")\nfig1 = go.Figure(data = data1,layout = layout1)\nfig2 = go.Figure(data = data2,layout = layout2)\npy.iplot(fig1)\npy.iplot(fig2)","c6450ed9":"avg_tgc = telcom.groupby([\"tenure_group\",\"Churn\"])[[\"MonthlyCharges\",\n                                                    \"TotalCharges\"]].mean().reset_index()\n\n#function for tracing \ndef mean_charges(column,aggregate) :\n    tracer = go.Bar(x = avg_tgc[avg_tgc[\"Churn\"] == aggregate][\"tenure_group\"],\n                    y = avg_tgc[avg_tgc[\"Churn\"] == aggregate][column],\n                    name = aggregate,marker = dict(line = dict(width = 1)),\n                    text = \"Churn\"\n                   )\n    return tracer\n\n#function for layout\ndef layout_plot(title,xaxis_lab,yaxis_lab) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = xaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = yaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                           )\n                      )\n    return layout\n    \n\n#plot1 - mean monthly charges by tenure groups\ntrace1  = mean_charges(\"MonthlyCharges\",\"Yes\")\ntrace2  = mean_charges(\"MonthlyCharges\",\"No\")\nlayout1 = layout_plot(\"Average Monthly Charges by Tenure groups\",\n                      \"Tenure group\",\"Monthly Charges\")\ndata1   = [trace1,trace2]\nfig1    = go.Figure(data=data1,layout=layout1)\n\n#plot2 - mean total charges by tenure groups\ntrace3  = mean_charges(\"TotalCharges\",\"Yes\")\ntrace4  = mean_charges(\"TotalCharges\",\"No\")\nlayout2 = layout_plot(\"Average Total Charges by Tenure groups\",\n                      \"Tenure group\",\"Total Charges\")\ndata2   = [trace3,trace4]\nfig2    = go.Figure(data=data2,layout=layout2)\n\npy.iplot(fig1)\npy.iplot(fig2)","d73bd74c":"##copy data\ntel_df = telcom.copy()\n#Drop tenure column\ntelcom = telcom.drop(columns = \"tenure_group\",axis = 1)\n\ntrace1 = go.Scatter3d(x = churn[\"MonthlyCharges\"],\n                      y = churn[\"TotalCharges\"],\n                      z = churn[\"tenure\"],\n                      mode = \"markers\",\n                      name = \"Churn customers\",\n                      text = \"Id : \" + churn[\"customerID\"],\n                      marker = dict(size = 1,color = \"red\")\n                     )\ntrace2 = go.Scatter3d(x = not_churn[\"MonthlyCharges\"],\n                      y = not_churn[\"TotalCharges\"],\n                      z = not_churn[\"tenure\"],\n                      name = \"Non churn customers\",\n                      text = \"Id : \" + not_churn[\"customerID\"],\n                      mode = \"markers\",\n                      marker = dict(size = 1,color= \"green\")\n                     )\n\n\n\nlayout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n                                                   center=dict(x=0, y=0, z=0),\n                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n                                     xaxis  = dict(title = \"monthly charges\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'),\n                                     yaxis  = dict(title = \"total charges\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  ),\n                                     zaxis  = dict(title = \"tenure\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  )\n                                    ),\n                        height = 700,\n                       )\n                  )\n                  \n\ndata = [trace1,trace2]\nfig  = go.Figure(data = data,layout = layout)\npy.iplot(fig)","480819c3":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#customer id col\nId_col     = ['customerID']\n#Target columns\ntarget_col = [\"Churn\"]\n#categorical columns\ncat_cols   = telcom.nunique()[telcom.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in telcom.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = telcom.nunique()[telcom.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    telcom[i] = le.fit_transform(telcom[i])\n    \n#Duplicating columns for multi value columns\ntelcom = pd.get_dummies(data = telcom,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(telcom[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_telcom_og = telcom.copy()\ntelcom = telcom.drop(columns = num_cols,axis = 1)\ntelcom = telcom.merge(scaled,left_index=True,right_index=True,how = \"left\")","b561ae36":"summary = (df_telcom_og[[i for i in df_telcom_og.columns if i not in Id_col]].\n           describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['#119DFF']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [200,60,100,100,60,60,80,80,80])\nlayout = go.Layout(dict(title = \"Variable Summary\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)","ec3f7179":"#correlation\ncorrelation = telcom.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   colorscale = \"Viridis\",\n                   colorbar   = dict(title = \"Pearson Correlation coefficient\",\n                                     titleside = \"right\"\n                                    ) ,\n                  )\n\nlayout = go.Layout(dict(title = \"Correlation Matrix for variables\",\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                      ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9))\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","1e27649c":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\n\nX = telcom[[i for i in telcom.columns if i not in Id_col + target_col]]\nY = telcom[target_col + Id_col]\n\nprincipal_components = pca.fit_transform(X)\npca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\npca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\npca_data[\"Churn\"] = pca_data[\"Churn\"].replace({1:\"Churn\",0:\"Not Churn\"})\n\ndef pca_scatter(target,color) :\n    tracer = go.Scatter(x = pca_data[pca_data[\"Churn\"] == target][\"PC1\"] ,\n                        y = pca_data[pca_data[\"Churn\"] == target][\"PC2\"],\n                        name = target,mode = \"markers\",\n                        marker = dict(color = color,\n                                      line = dict(width = .5),\n                                      symbol =  \"diamond-open\"),\n                        text = (\"Customer Id : \" + \n                                pca_data[pca_data[\"Churn\"] == target]['customerID'])\n                       )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Visualising data with principal components\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"principal component 1\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"principal component 2\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        height = 600\n                       )\n                  )\ntrace1 = pca_scatter(\"Churn\",'red')\ntrace2 = pca_scatter(\"Not Churn\",'royalblue')\ndata = [trace2,trace1]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","649fd487":"#separating binary columns\nbi_cs = telcom.nunique()[telcom.nunique() == 2].keys()\ndat_rad = telcom[bi_cs]\n\n#plotting radar chart for churn and non churn customers(binary variables)\ndef plot_radar(df,aggregate,title) :\n    data_frame = df[df[\"Churn\"] == aggregate] \n    data_frame_x = data_frame[bi_cs].sum().reset_index()\n    data_frame_x.columns  = [\"feature\",\"yes\"]\n    data_frame_x[\"no\"]    = data_frame.shape[0]  - data_frame_x[\"yes\"]\n    data_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n    \n    #count of 1's(yes)\n    trace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill  = \"toself\",name = \"count of 1's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            )\n    #count of 0's(No)\n    trace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill  = \"toself\",name = \"count of 0's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            ) \n    layout = go.Layout(dict(polar = dict(radialaxis = dict(visible = True,\n                                                           side = \"counterclockwise\",\n                                                           showline = True,\n                                                           linewidth = 2,\n                                                           tickwidth = 2,\n                                                           gridcolor = \"white\",\n                                                           gridwidth = 2),\n                                         angularaxis = dict(tickfont = dict(size = 10),\n                                                            layer = \"below traces\"\n                                                           ),\n                                         bgcolor  = \"rgb(243,243,243)\",\n                                        ),\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            title = title,height = 700))\n    \n    data = [trace2,trace1]\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig)\n\n#plot\nplot_radar(dat_rad,1,\"Churn -  Customers\")\nplot_radar(dat_rad,0,\"Non Churn - Customers\")","01463a69":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score,recall_score\nfrom yellowbrick.classifier import DiscriminationThreshold\n#splitting train and test data \ntrain,test = train_test_split(telcom,test_size = .25 ,random_state = 111)\n    \n##seperating dependent and independent variables\ncols    = [i for i in telcom.columns if i not in Id_col + target_col]\ntrain_X = train[cols]\ntrain_Y = train[target_col]\ntest_X  = test[cols]\ntest_Y  = test[target_col]\n\n#Function attributes\n#dataframe     - processed dataframe\n#Algorithm     - Algorithm used \n#training_x    - predictor variables dataframe(training)\n#testing_x     - predictor variables dataframe(testing)\n#training_y    - target variable(training)\n#training_y    - target variable(testing)\n#cf - [\"coefficients\",\"features\"](cooefficients for logistic \n                                 #regression,features for tree based models)\n\n#threshold_plot - if True returns threshold plot for model\n    \ndef telecom_churn_prediction(algorithm,training_x,testing_x,\n                             training_y,testing_y,cols,cf,threshold_plot) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    #coeffs\n    if   cf == \"coefficients\" :\n        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n    elif cf == \"features\" :\n        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n        \n    column_df     = pd.DataFrame(cols)\n    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n    \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not churn\",\"Churn\"],\n                        y = [\"Not churn\",\"Churn\"],\n                        showscale  = False,colorscale = \"Picnic\",\n                        name = \"matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot coeffs\n    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Picnic\",\n                                  line = dict(width = .6,color = \"black\")))\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                            subplot_titles=('Confusion Matrix',\n                                            'Receiver operating characteristic',\n                                            'Feature Importances'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,1,2)\n    fig.append_trace(trace4,2,1)\n    \n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 900,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n                                        tickangle = 90))\n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n        \nlogit  = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit,train_X,test_X,train_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","41d85665":"from imblearn.over_sampling import SMOTE\n\ncols    = [i for i in telcom.columns if i not in Id_col+target_col]\n\nsmote_X = telcom[cols]\nsmote_Y = telcom[target_col]\n\n#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n                                                                         test_size = .25 ,\n                                                                         random_state = 111)\n\n#oversampling minority class using smote\nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns=target_col)\n###\n\n\n\nlogit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","cb526480":"from sklearn.feature_selection import RFE\n\nlogit = LogisticRegression()\n\nrfe = RFE(logit,10)\nrfe = rfe.fit(os_smote_X,os_smote_Y.values.ravel())\n\nrfe.support_\nrfe.ranking_\n\n#identified columns Recursive Feature Elimination\nidc_rfe = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : [i for i in telcom.columns if i not in Id_col + target_col],\n                       \"ranking\" : rfe.ranking_,\n                      })\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n\n\n#separating train and test data\ntrain_rf_X = os_smote_X[cols]\ntrain_rf_Y = os_smote_Y\ntest_rf_X  = test[cols]\ntest_rf_Y  = test[target_col]\n\nlogit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n#applying model\ntelecom_churn_prediction(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\ntab_rk = ff.create_table(idc_rfe)\npy.iplot(tab_rk)","70d8e183":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\n#select columns\ncols = [i for i in telcom.columns if i not in Id_col + target_col ]\n\n#dataframe with non negative values\ndf_x = df_telcom_og[cols]\ndf_y = df_telcom_og[target_col]\n\n#fit model with k= 3\nselect = SelectKBest(score_func = chi2,k = 3)\nfit    = select.fit(df_x,df_y)\n\n#Summerize scores\nprint (\"scores\")\nprint (fit.scores_)\nprint (\"P - Values\")\nprint (fit.pvalues_)\n\n#create dataframe\nscore = pd.DataFrame({\"features\":cols,\"scores\":fit.scores_,\"p_values\":fit.pvalues_ })\nscore = score.sort_values(by = \"scores\" ,ascending =False)\n\n\n#createing new label for categorical and numerical columns\nscore[\"feature_type\"] = np.where(score[\"features\"].isin(num_cols),\"Numerical\",\"Categorical\")\n\n#plot\ntrace  = go.Scatter(x = score[score[\"feature_type\"] == \"Categorical\"][\"features\"],\n                    y = score[score[\"feature_type\"] == \"Categorical\"][\"scores\"],\n                    name = \"Categorial\",mode = \"lines+markers\",\n                    marker = dict(color = \"red\",\n                                  line = dict(width =1))\n                   )\n\ntrace1 = go.Bar(x = score[score[\"feature_type\"] == \"Numerical\"][\"features\"],\n                y = score[score[\"feature_type\"] == \"Numerical\"][\"scores\"],name = \"Numerical\",\n                marker = dict(color = \"royalblue\",\n                              line = dict(width =1)),\n                xaxis = \"x2\",yaxis = \"y2\"\n               )\nlayout = go.Layout(dict(title = \"Scores for Categorical & Numerical features\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     tickfont = dict(size =10),\n                                     domain=[0, 0.7],\n                                     tickangle = 90,zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"scores\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(b=200),\n                        xaxis2=dict(domain=[0.8, 1],tickangle = 90,\n                                    gridcolor = 'rgb(255, 255, 255)'),\n                        yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                        )\n                  )\n\ndata=[trace,trace1]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","a0a655af":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG,display\n\n#top 3 categorical features\nfeatures_cat  = score[score[\"feature_type\"] == \"Categorical\"][\"features\"][:3].tolist()\n\n#top 3 numerical features\nfeatures_num  = score[score[\"feature_type\"] == \"Numerical\"][\"features\"][:3].tolist()\n\n\n#Function attributes\n#columns        - selected columns\n#maximum_depth  - depth of tree\n#criterion_type - [\"gini\" or \"entropy\"]\n#split_type     - [\"best\" or \"random\"]\n#Model Performance - True (gives model output)\n\ndef plot_decision_tree(columns,maximum_depth,criterion_type,\n                       split_type,model_performance = None) :\n    \n    #separating dependent and in dependent variables\n    dtc_x = df_x[columns]\n    dtc_y = df_y[target_col]\n    \n    #model\n    dt_classifier = DecisionTreeClassifier(max_depth = maximum_depth,\n                                           splitter  = split_type,\n                                           criterion = criterion_type,\n                                          )\n    dt_classifier.fit(dtc_x,dtc_y)\n    \n    #plot decision tree\n    graph = Source(tree.export_graphviz(dt_classifier,out_file=None,\n                                        rounded=True,proportion = False,\n                                        feature_names = columns, \n                                        precision  = 2,\n                                        class_names=[\"Not churn\",\"Churn\"],\n                                        filled = True                         \n                                       )\n                  )\n    \n    #model performance\n    if model_performance == True :\n        telecom_churn_prediction(dt_classifier,\n                                 dtc_x,test_X[columns],\n                                 dtc_y,test_Y,\n                                 columns,\"features\",threshold_plot = True)\n    display(graph)\n    \nplot_decision_tree(features_num,3,\"gini\",\"best\")","7ccb5a89":"#Using top three categorical features\n\nplot_decision_tree(features_cat,3,\"entropy\",\"best\",\n                   model_performance = True ,)","c7572ae8":"#using contract,tenure and paperless billing variables\ncolumns = ['tenure','Contract_Month-to-month', 'PaperlessBilling',\n           'Contract_One year', 'Contract_Two year']\n\nplot_decision_tree(columns,3,\"gini\",\"best\",model_performance= True)","1676bf94":"def telecom_churn_prediction_alg(algorithm,training_x,testing_x,\n                                 training_y,testing_y,threshold_plot = True) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy Score   : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n     \n    #plot roc curve\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    trace2 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot confusion matrix\n    trace3 = go.Heatmap(z = conf_matrix ,x = [\"Not churn\",\"Churn\"],\n                        y = [\"Not churn\",\"Churn\"],\n                        showscale  = False,colorscale = \"Blues\",name = \"matrix\",\n                        xaxis = \"x2\",yaxis = \"y2\"\n                       )\n    \n    layout = go.Layout(dict(title=\"Model performance\" ,\n                            autosize = False,height = 500,width = 800,\n                            showlegend = False,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(title = \"false positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         domain=[0, 0.6],\n                                         ticklen=5,gridwidth=2),\n                            yaxis = dict(title = \"true positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         zerolinewidth=1,\n                                         ticklen=5,gridwidth=2),\n                            margin = dict(b=200),\n                            xaxis2=dict(domain=[0.7, 1],tickangle = 90,\n                                        gridcolor = 'rgb(255, 255, 255)'),\n                            yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                           )\n                  )\n    data = [trace1,trace2,trace3]\n    fig = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n\n    \nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\ntelecom_churn_prediction_alg(knn,os_smote_X,test_X,\n                             os_smote_Y,test_Y,threshold_plot = True)","49672f26":"from sklearn.ensemble import RandomForestClassifier\n\n#function attributes\n#columns  - column used\n#nf_estimators   - The number of trees in the forest.\n#estimated_tree  - tree number to be displayed\n#maximum_depth   - depth of the tree\n#criterion_type  - split criterion type [\"gini\" or \"entropy\"]\n#Model performance - prints performance of model\n\ndef plot_tree_randomforest(columns,nf_estimators,\n                           estimated_tree,maximum_depth,\n                           criterion_type,model_performance = None) :\n    \n    dataframe = df_telcom_og[columns + target_col].copy()\n    \n    #train and test datasets\n    rf_x     = dataframe[[i for i in columns if i not in target_col]]\n    rf_y     = dataframe[target_col]\n    \n    #random forest classifier\n    rfc   = RandomForestClassifier(n_estimators = nf_estimators,\n                                   max_depth = maximum_depth,\n                                   criterion = criterion_type,\n                                  )\n    rfc.fit(rf_x,rf_y)\n    \n    estimated_tree = rfc.estimators_[estimated_tree]\n    \n    graph = Source(tree.export_graphviz(estimated_tree,out_file=None,\n                                        rounded=True,proportion = False,\n                            feature_names = columns, \n                            precision  = 2,\n                            class_names=[\"Not churn\",\"Churn\"],\n                            filled = True))\n    display(graph)\n    \n    #model performance\n    if model_performance == True :\n        telecom_churn_prediction(rfc,\n                                 rf_x,test_X[columns],\n                                 rf_y,test_Y,\n                                 columns,\"features\",threshold_plot = True)\n        \n\ncols1 = [ i for i in train_X.columns if i not in target_col + Id_col] \nplot_tree_randomforest(cols1,100,99,3,\"entropy\",True)","ff1bc512":"#making 10 trees with random forest.\nn = np.arange(0,10).tolist()\ncols1 = [ i for i in train_X.columns if i not in target_col + Id_col] \nfor i in n :\n    plot_tree_randomforest(cols1,10,i,3,\"entropy\",model_performance=False)","5f8e739c":"#making 10 trees with random forest for columns \n#selected from recursive feature elimination\n\nn = np.arange(0,10).tolist()\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist() \nfor i in n :\n    plot_tree_randomforest(cols,10,i,3,\"gini\",model_performance=False)","448229e7":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB(priors=None)\n\ntelecom_churn_prediction_alg(gnb,os_smote_X,test_X,os_smote_Y,test_Y)","4a9eeb09":"from sklearn.svm import SVC\n\n#Support vector classifier\n#using linear hyper plane\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\n\ncols = [i for i in telcom.columns if i not in Id_col + target_col]\ntelecom_churn_prediction(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = False)","6df21cb9":"#tuning parameters\n#Support vector classifier\n#using non-linear hyper plane(\"rbf\")\n\nsvc_rbf  = SVC(C=1.0, kernel='rbf', \n               degree= 3, gamma=1.0, \n               coef0=0.0, shrinking=True,\n               probability=True,tol=0.001,\n               cache_size=200, class_weight=None,\n               verbose=False,max_iter= -1,\n               random_state=None)\n\ntelecom_churn_prediction_alg(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,threshold_plot = False)","6309a0c5":"from lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\ncols = [i for i in telcom.columns if i not in Id_col + target_col]\ntelecom_churn_prediction(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"features\",threshold_plot = True)","e6c3d170":"from xgboost import XGBClassifier\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n\n\ntelecom_churn_prediction(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"features\",threshold_plot = True)","e3b20ae6":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\n\n#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(logit,train_X,test_X,train_Y,test_Y,\n                      \"Logistic Regression(Baseline_model)\")\nmodel2 = model_report(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Logistic Regression(SMOTE)\")\nmodel3 = model_report(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                      \"Logistic Regression(RFE)\")\ndecision_tree = DecisionTreeClassifier(max_depth = 9,\n                                       random_state = 123,\n                                       splitter  = \"best\",\n                                       criterion = \"gini\",\n                                      )\nmodel4 = model_report(decision_tree,train_X,test_X,train_Y,test_Y,\n                      \"Decision Tree\")\nmodel5 = model_report(knn,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"KNN Classifier\")\nrfc = RandomForestClassifier(n_estimators = 1000,\n                             random_state = 123,\n                             max_depth = 9,\n                             criterion = \"gini\")\nmodel6 = model_report(rfc,train_X,test_X,train_Y,test_Y,\n                      \"Random Forest Classifier\")\nmodel7 = model_report(gnb,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Naive Bayes\")\nmodel8 = model_report(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier Linear\")\nmodel9 = model_report(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier RBF\")\nmodel10 = model_report(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"LGBM Classifier\")\nmodel11 = model_report(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"XGBoost Classifier\")\n\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","87db2a53":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","b420f105":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,3,j+1)\n    predictions = i.predict(test_X)\n    conf_matrix = confusion_matrix(predictions,test_Y)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = \"Set1\")\n    plt.title(k,color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","e0a55d3b":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nplt.style.use(\"dark_background\")\nfig = plt.figure(figsize=(12,16))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    fpr,tpr,thresholds = roc_curve(test_Y,probabilities[:,1])\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(test_Y,predictions),3)))\n    plt.plot([0,1],[0,1],linestyle = \"dashed\",\n             color = \"orangered\",linewidth = 1.5)\n    plt.fill_between(fpr,tpr,alpha = .4)\n    plt.fill_between([0,1],[0,1],color = \"k\")\n    plt.legend(loc = \"lower right\",\n               prop = {\"size\" : 12})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xticks(np.arange(0,1,.3))\n    plt.yticks(np.arange(0,1,.3))","28a4ac41":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n\nlst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,17))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    \n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    recall,precision,thresholds = precision_recall_curve(test_Y,probabilities[:,1])\n    plt.plot(recall,precision,linewidth = 1.5,\n             label = (\"avg_pcn : \" + \n                      str(np.around(average_precision_score(test_Y,predictions),3))))\n    plt.plot([0,1],[0,0],linestyle = \"dashed\")\n    plt.fill_between(recall,precision,alpha = .2)\n    plt.legend(loc = \"lower left\",\n               prop = {\"size\" : 10})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xlabel(\"recall\",fontsize =7)\n    plt.ylabel(\"precision\",fontsize =7)\n    plt.xlim([0.25,1])\n    plt.yticks(np.arange(0,1,.3))","09b9cad6":"# 5.12. LightGBMClassifier","4e1b0687":"# Please don't forget to upvote this notebook","5eb6ca83":"# 5.10. Support Vector Machine\n\n\n\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space .where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes","5e27e394":"# Visualising data with principal components","a73f379d":"3.3. Customer attrition in tenure groups","e5b2e428":"# 5.9. Gaussian Naive Bayes.","730f0147":"# 5.8. A random forest classifier.\n# \n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement .\nBelow are the trees produced by random forest model with 10 estimated trees with maximum depth of three for each tree. Each tree produced is slightly different from other.","4a85a6d9":"3.2. Varibles distribution in customer attrition","6351dc64":"# Data Manipulation","1d209163":"3. Exploratory Data Analysis\n3.1. Customer attrition in data","50fb40ff":"3.5. Average Charges by tenure groups","d429044f":"# 6.2. Compare model metrics","60f7c4d7":"3.6. Monthly charges,total charges and tenure in customer attrition","7e1990bc":"Thanks Pavan Raj","5d4fd77c":"# 6.5. Precision recall curves","30ce0fa7":"# 5. Model Building\n\n**5.1. Baseline Model**","3ab34834":"# 6.3. Confusion matrices for models","bb431e2d":"# 5.11. Tuning parameters for support vector machine","441f67ee":"# 3.10. Binary variables distribution in customer attrition(Radar Chart)","32d07547":"3.7. Variable Summary","5d70e75f":"3.4. Monthly Charges and Total Charges by Tenure and Churn groups\u00b6","f1ad2673":"# 5.13. XGBoost Classifier","3d49f249":"# 6. Model Performances\n\n**6.1. model performance metrics**","f9c1512f":"3.8. Correlation Matrix","2f8502eb":"# 5.2. Synthetic Minority Oversampling TEchnique (SMOTE)\n\n\n* Randomly pick a point from the minority class.\n* Compute the k-nearest neighbors (for some pre-specified k) for this point.\n* Add k new points somewhere between the chosen point and each of its neighbors","c13edf00":"# 5.5. Decision Tree Visualization\n\nUsing top three numerical features","830a9d92":"4. Data preprocessing","9ee99757":"# 5.3. Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","8d5828b5":"# 5.7. Vizualising a decision tree from random forest classifier","698c2fd6":"# 6.4. ROC - Curves for models","a2eee009":"# 5.4. Univariate Selection\n\n\nFeature Extraction with Univariate Statistical Tests (Chi-squared for classification)\nuses the chi squared (chi^2) statistical test for non-negative features to select the best features","ea8cc360":"# 5.6. KNN Classifier\n\nApplying knn algorithm to smote oversampled data."}}