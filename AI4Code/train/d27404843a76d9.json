{"cell_type":{"71ab9ded":"code","3ec132a3":"code","26485c7b":"code","5fc3838e":"code","3ac76f3d":"code","8b217f95":"code","803c58bb":"code","4e1771fa":"code","7536dc34":"code","8ee9d630":"code","1e298b96":"code","dd9bc928":"code","3d76901b":"code","9672fa51":"code","4d73047e":"markdown","f17a0171":"markdown"},"source":{"71ab9ded":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n#plt.style.use('fivethirtyeight')\nimport xgboost as xgb\nimport sklearn","3ec132a3":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","26485c7b":"train_median = train.median()\ntrain = train.fillna(train_median)","5fc3838e":"train = train.loc[(train.weight > 0) & (train.date > 85), :]","3ac76f3d":"train['action'] = np.where(train['resp'] < 0, 0, 1)\n\ncols = [col for col in list(train.columns) if 'feature' in col]\n\nx = train.loc[:, cols]\ny = train['action']","8b217f95":"del train","803c58bb":"scaler = StandardScaler()\nscaler.fit(x)\nx = scaler.transform(x)\n\npca = PCA()\ncomp = pca.fit(x)\n\n# We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();","4e1771fa":"# Using the first 50 principal components, we apply the PCA mapping on both the training and test set\npca = PCA(n_components=50).fit(x)\nx = pca.transform(x)","7536dc34":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","8ee9d630":"def objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n    }\n    \n    trn_data = lgb.Dataset(x_train, label=y_train)\n    val_data = lgb.Dataset(x_test, label=y_test)\n    \n    bst = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 10) \n    preds = bst.predict(x_test)\n    pred_labels = np.rint(preds)\n    \n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    return accuracy","1e298b96":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=25, timeout=1200)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"{}: {}\".format(key, value))","dd9bc928":"best_params = trial.params\nbest_params['objective'] = 'binary'\nbest_params['metric'] = 'auc'\n\ntrn_data = lgb.Dataset(x_train, label=y_train)\nval_data = lgb.Dataset(x_test, label=y_test)\n\nlg_model = lgb.train(best_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 10)","3d76901b":"def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","9672fa51":"import janestreet\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfrom tqdm.notebook import tqdm\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = np.where(\n        lg_model.predict(\n            pca.transform(\n                scaler.transform(\n                    fillna_npwhere(\n                        test_df[cols].values,\n                        train_median[cols].values\n                    )\n                )\n            ),\n            num_iteration = lg_model.best_iteration\n        ) >=0.5,\n        1,\n        0\n    ).astype(int)\n    \n    env.predict(sample_prediction_df)","4d73047e":"* This competition is a classifier problem\n* This notebook is intended to give anyone a chance to copy and build off this approach\n* Takes a very simple approach of:\n    * Take only important features\n    * Tune model parameters\n    * Train LGBM\n    * Submit","f17a0171":"* PCA for identifying important features and throwing away unimportant ones\n* Credit to: https:\/\/www.kaggle.com\/wongguoxuan\/eda-pca-xgboost-classifier-for-beginners"}}