{"cell_type":{"2f5640be":"code","31478467":"code","f13e77bd":"code","e8b92087":"code","1ddada30":"code","55c05aee":"code","dde8d906":"code","5f8d7a7a":"code","fa24c37b":"code","32a74e04":"code","1db32820":"code","34d152d1":"code","a48550bb":"code","644c5650":"code","c1e54adf":"code","cb24bf3c":"code","75a49f6a":"code","d512b867":"code","694567ef":"code","2be713ea":"code","19a31492":"code","4cb06672":"code","9e1d0015":"code","695797a5":"code","cbce5486":"code","f44f9e60":"code","b3911b9d":"code","0fc03275":"code","7c5e867b":"code","8e16975d":"code","bcbaa84c":"markdown","84f7bc37":"markdown","a648c0ee":"markdown","a6bff194":"markdown","19891583":"markdown","90e0b8a0":"markdown","a640a311":"markdown","6b4a10f3":"markdown","fed85764":"markdown","c5ebce18":"markdown","7f359e65":"markdown","e36c2101":"markdown","129d0fb2":"markdown","876d9d35":"markdown","cd39fa81":"markdown","130c686e":"markdown","a09cd5ed":"markdown","58523616":"markdown","038f2289":"markdown","9485d4a5":"markdown","947880af":"markdown","4899e788":"markdown","ffff4d16":"markdown","91f63a96":"markdown","2b1eac47":"markdown","b53b4938":"markdown","44b109c6":"markdown","02ccee54":"markdown","5fca238e":"markdown","0f3f4579":"markdown","bdc5ac2f":"markdown","76370f41":"markdown","3d2ffc23":"markdown","c4720097":"markdown","01595464":"markdown","7e22df04":"markdown","43b792d5":"markdown","a1c2f154":"markdown","9ea164bd":"markdown","415dd266":"markdown","bbf44b7b":"markdown","bd4ec4bc":"markdown","3771564c":"markdown"},"source":{"2f5640be":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve, auc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set()\n\n%matplotlib inline","31478467":"dataset = pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndataset.head()","f13e77bd":"dataset.describe().T","e8b92087":"print(pd.isnull(dataset).sum())","1ddada30":"dataset['Gender'].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,figsize=(6,6))\nplt.title('Gender percentages', fontsize = 20)\nplt.tight_layout()\nplt.show()","55c05aee":"sns.countplot(dataset['Gender'], palette = 'Set2')\nplt.title ('Gender vs. quantity', fontsize = 20)\nplt.show()","dde8d906":"sns.distplot(dataset['Age'], bins = 5, color = 'orange', label = 'KDE')\nplt.legend()\nplt.gcf().set_size_inches(12, 5)","5f8d7a7a":"plt.figure(figsize = (22,10))\nsns.countplot(x = 'Age',data = dataset , hue='Gender', palette = 'Set2')\nplt.legend(loc='upper center')\nplt.show()","fa24c37b":"tag1 = 'Male'\ntag2 = 'Female'\nMale = dataset[dataset[\"Gender\"] == tag1][['Age','EstimatedSalary']]\nFemale = dataset[dataset[\"Gender\"] == tag2][['Age','EstimatedSalary']]","32a74e04":"f, (ax1, ax2) = plt.subplots(1, 2, sharey = True)\n\nax1.scatter(Male.Age, Male.EstimatedSalary, c = 'green', s = 15, alpha = 0.7)\nax1.set_title('Male age vs. Estimated Salary', c = 'green')\nax2.scatter(Female.Age, Female.EstimatedSalary, c='red', s = 15, alpha = 0.7)\nax2.set_title('Female age vs. Estimated Salary', c ='red')\nplt.gcf().set_size_inches(15, 7)\n\nplt.ylabel('Estimated Salary', fontsize = 20)\n\nplt.show()","1db32820":"sns.catplot(x=\"Age\", col = 'Purchased', data=dataset, kind = 'count', palette='pastel')\nplt.gcf().set_size_inches(20, 10)\nplt.show()","34d152d1":"sns.catplot(x=\"Gender\", col = 'Purchased', data=dataset, kind = 'count', palette='pastel')\nplt.show()","a48550bb":"dataset2 = dataset.copy()\ndataset2 = dataset2.drop(['User ID'], axis = 1)","644c5650":"X = dataset2.iloc[:, 0:3]\ny = dataset2.iloc[:, -1]","c1e54adf":"X = pd.get_dummies(X)\nX = X[['Gender_Male','Gender_Female','Age','EstimatedSalary']]\nX = X.drop(['Gender_Male'], axis = 1)","cb24bf3c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","75a49f6a":"# sc_X = MinMaxScaler()\n# X_train = sc_X.fit_transform(X_train)\n# X_test = sc_X.transform(X_test)","d512b867":"classifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","694567ef":"y_pred = classifier.predict(X_test)","2be713ea":"cm = confusion_matrix(y_test, y_pred)\n\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\n\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot = labels, fmt = '', cmap = 'Blues', cbar = False)\nplt.gcf().set_size_inches(5, 5)\nplt.title('Confusion Matrix Logistic Regression', fontsize = 20)\nplt.show()","19a31492":"accuracy_LR = accuracy_score(y_test,y_pred) *100\nprint('The accuracy of the logistic regression is: ' +str(accuracy_LR) + ' %.')","4cb06672":"parameters_LR = classifier.coef_\nparameters_LR","9e1d0015":"def clf_model(model):\n    clf = model\n    clf.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, clf.predict(X_test).round())\n    return clf, accuracy","695797a5":"model_performance = pd.DataFrame(columns = [\"Model\", \"Accuracy\"])\n\nmodels_to_evaluate = [RandomForestClassifier(n_estimators=1000), KNeighborsClassifier(n_neighbors = 7, metric = \"minkowski\", p = 2),\n                      SVC(kernel = 'rbf'), GaussianNB(), GradientBoostingRegressor(n_estimators=300, learning_rate=0.01), \n                     AdaBoostClassifier(n_estimators=300, learning_rate=0.01), XGBClassifier(n_estimators=300, learning_rate=0.01)]\n\nfor model in models_to_evaluate:\n    clf, accuracy = clf_model(model)\n    model_performance = model_performance.append({\"Model\": model, \"Accuracy\": accuracy}, ignore_index=True)\n\nmodel_performance","cbce5486":"XGclassifier = XGBClassifier(n_estimators=300, learning_rate=0.01)\nXGclassifier.fit(X_train, y_train)\ny_pred_xg = XGclassifier.predict(X_test)","f44f9e60":"KNN = KNeighborsClassifier(n_neighbors = 7, metric = \"minkowski\", p = 2)\nKNN.fit(X_train, y_train)\ny_pred_knn = KNN.predict(X_test)","b3911b9d":"SVC_clf = SVC(kernel = 'rbf')\nSVC_clf.fit(X_train, y_train)\ny_pred_SVC = SVC_clf.predict(X_test)","0fc03275":"# length of the test data \ntotal = len(y_test) \n  \n# Counting '1' labels in test data \none_count = np.sum(y_test) \n  \n# counting '0' lables in test data  \nzero_count = total - one_count \n\nplt.figure(figsize = (10, 6)) \n  \n# x-axis ranges from 0 to total people on y_test  \n# y-axis ranges from 0 to the total positive outcomes. \n\n# K-NN plot\n\nK = [y for _, y in sorted(zip(y_pred_knn, y_test), reverse = True)] \n\nx = np.arange(0, total + 1) # Shape of Y_test\ny = np.append([0], np.cumsum(K)) # Y values\n\nplt.plot(x, y, c = 'green', label = 'K-NN', linewidth = 2)\n\n# SVC Plot\n\nS = [y for _, y in sorted(zip(y_pred_SVC, y_test), reverse = True)] \n\nx2 = np.arange(0, total + 1) # Shape of Y_test\ny2 = np.append([0], np.cumsum(S)) # Y values\n\nplt.plot(x2, y2, c = 'orange', label = 'SVC', linewidth = 2)\n\n\n# XGClassifier plot \n\nXG = [y for _, y in sorted(zip(y_pred_xg, y_test), reverse = True)] \n\nx3 = np.arange(0, total + 1) # Shape of Y_test\ny3 = np.append([0], np.cumsum(XG)) # Y values\n\nplt.plot(x3, y3, c = 'red', label = 'XGClassifier', linewidth = 2)\n\n\n# Random Model plot\n  \nplt.plot([0, total], [0, one_count], c = 'blue',  \n         linestyle = '--', label = 'Random Model') \n\n# Perfect model plot\n\nplt.plot([0, one_count, total], [0, one_count, one_count], \n         c = 'grey', linewidth = 2, label = 'Perfect Model') \n\nplt.title('Cumulative Accuracy Profile of different models', fontsize = 20)\nplt.xlabel('Total y_test observations', fontsize = 15)\nplt.ylabel('N\u00b0 class 1 scores', fontsize = 15)\nplt.legend() \nplt.show()","7c5e867b":"# Area under Random Model\na = auc([0, total], [0, one_count])\n\n# Area between Perfect and Random Model\naP = auc([0, one_count, total], [0, one_count, one_count]) - a\n\n# Area K-NN\n\naKNN = auc(x, y) - a\nprint(\"Accuracy Rate for K-NN: {}\".format(aKNN \/ aP))\n\n# Area SVC\n\naSVC = auc(x2, y2) - a\nprint(\"Accuracy Rate for Support Vector Classifier: {}\".format(aSVC \/ aP))\n\n# Area XGClassifier\n\naXG = auc(x3, y3) - a\nprint(\"Accuracy Rate for XGClassifier: {}\".format(aXG \/ aP))","8e16975d":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_knn)\n\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_SVC)\n\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_xg)\n\nroc_auc = auc(fpr, tpr)\nroc_auc2 = auc(fpr2, tpr2)\nroc_auc3 = auc(fpr3, tpr3)\n\nplt.figure(figsize = (10, 6)) \n\nplt.plot(fpr, tpr, c = 'green', linewidth = 2, label = 'K-NN AUC:' + ' {0:.2f}'.format(roc_auc))\nplt.plot(fpr2, tpr2, c = 'orange', linewidth = 2, label = 'Support Vector Classifier AUC:' + ' {0:.2f}'.format(roc_auc2))\nplt.plot(fpr3, tpr3, c = 'red', linewidth = 2, label = 'XGClassifier AUC:' + ' {0:.2f}'.format(roc_auc3))\nplt.plot([0,1], [0,1], c = 'blue', linestyle = '--')\n\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('ROC', fontsize = 20)\nplt.legend(loc = 'lower right', fontsize = 13)\nplt.show()","bcbaa84c":"*There are no missing values*","84f7bc37":"## Social network product purchase","a648c0ee":"**XGClassifier**","a6bff194":"Thanks for reaching the end! Upvote if you liked it!","19891583":"**Conclusion:** There is no correlation between the estimated salary of a male or female with their age.","90e0b8a0":"**As we didn't scale the data, the results and accuracy are pretty low, let's see what happens with other models**","a640a311":"Accuracy of the regression","6b4a10f3":"Splitting train - test","fed85764":"**Statistics for purchased column**","c5ebce18":"**Conclusion:**\n- Age is normally distributed around 37 years, and most of the people studied are from 35 to 45 years.","7f359e65":"**Conclusions:**\n- The product seem to be intended for people within the range of 50 - 60 years, and it also seems to be intended both for male and Female gender.\n- As we saw on the Age vs Salary plots, it seems that the women analyzed have higher income than men, so this might be a reason for the slight difference between gender in purchased or not terms.","e36c2101":"**K-NN**","129d0fb2":"*Age*","876d9d35":"*Importing Modules*","cd39fa81":"**The objective of this analysis is to predict whether a person will buy a product displayed on a social network ad or not, given their age, gender and salary and to compare the accuracy of different classification algorithms.**","130c686e":"- There are 400 people analyzed.\n- The age goes from 18 to 60 years.\n- The salary goes from 15000 USD to 150000 USD per year.","a09cd5ed":"Predictions","58523616":"**Plotting CAP**","038f2289":"**Conclusion:**\n- Having calculated the accuracy score, ROC and CAP, we can certainly say that the XGClassifier ensembled model is the best one yet at classifying this dataset.\n- Logistic regression might have been improved scaling data, but as more robust models where used, there was no imperative need.","9485d4a5":"Splitting features","947880af":"Hi I'm new to ML so I'd be glad to receive your feedback!","4899e788":"**Area analysis (AUC)**","ffff4d16":"**Loading and getting to know the dataset**","91f63a96":"Receiver Operating Characteristic curve is another way to evaluate the accuracy of classification models and also to compare between them.\n\nIt plots the True Positive Rate in the Y-axis and False Positive Rate in the X-axis. It is a way to summarize information that could be obtained from many confusion matrices.\n\nAs the AUC gets bigger, the model is better at classifying.","2b1eac47":"**CAP curve for model evaluation**","b53b4938":"Parameters of the regression","44b109c6":"**Conclusion:** \n- Random Forest classifier, K-NN, SVC and Naive Bayes all reach a better accuracy than logistic regression even without scaling the data.\n- Scaling the data with MinMaxScaler (commented above) made all the models reach an accuracy of 92.5%\n- Ensambled models reached a better accuracy > 93%.","02ccee54":"Scaling data","5fca238e":"*Gender*","0f3f4579":"**Let's see how gender, salary and age are distributed**","bdc5ac2f":"There seems to be equal analyzed people per gender per age, seeing only more men of 35 years.","76370f41":"**Logistic regression modeling**","3d2ffc23":"**Conclusion:** \n- As expected, XGclassifier with an accuracy of 0.95 has almost the same area under the curve as the ideal model.\n- SVC and K-NN have the same Accuracy rate from the AUC although they don't have the same accuracy score. ","c4720097":"**Are there any missing values?**","01595464":"**Accuracy with confusion matrix**","7e22df04":"**Comparing classifier algorithms: Random Forest Classifier, K-NN, Naive Bayes, and Ensambled models**","43b792d5":"**The set has:**\n\n   - Id column\n   - Gender: Categorical column (Male\/Female)\n   - Numerical features: Age and Estimated Salary","a1c2f154":"**SVC**","9ea164bd":"First let\u00b4s drop the ID column as it doesn't give any information.","415dd266":"**Plotting ROC**","bbf44b7b":"**Feature Engineering**","bd4ec4bc":"**Let's see how age is distributed per gender**","3771564c":"Cumulative Cccuracy Profile curve is a tool that will help evaluate the performance and accuracy of the classification model. I'm going to compare three models (SVC, K-NN and XGBoost) which all have different accuracy score. To do this, I'll compare how CAP curves for these algorithms relate to a random model and to an ideal model.\n\nAs the CAP curve gets more and more similar to the ideal model, the accuracy of the algorithm improves. We might expect that area under the plots increase as the accuracy improves. (Ideal model > XGClassifier > K-NN > SVC > Random model)"}}