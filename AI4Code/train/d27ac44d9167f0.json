{"cell_type":{"289ceb28":"code","a6e32e7a":"code","793ae8fc":"code","5267efea":"code","ecddb841":"code","2209ebc9":"code","a50e4ddd":"code","e5d207ba":"code","26993672":"code","dd6e7d31":"code","cb9722f8":"code","03562d3f":"code","8d7b02ea":"code","4a8b70a1":"code","8f981543":"code","9c509fb7":"code","cc9db4e0":"code","0d85493a":"code","ca492ae6":"code","80f1b4bc":"code","e72556a8":"code","5b524d75":"code","7109192b":"code","f43a821f":"code","ccc04f65":"code","7e5c6f46":"code","c3c0855d":"code","755f76b0":"code","e6f14e55":"code","6e102e89":"code","179ae6b8":"code","68d395c2":"code","5cd55242":"code","3804c6fc":"code","7e5166e2":"code","399ecccb":"code","3c61d785":"code","516ba19f":"code","bacb0013":"code","8277d422":"code","51bcda91":"code","3348b997":"code","90191399":"code","d6bbba7b":"code","4f774bf1":"code","01e24e64":"code","dbb1164d":"code","20fb1767":"code","3a7f1264":"code","00003bbb":"code","38ed5176":"code","ad784650":"code","198257c4":"code","99eaa5f5":"code","5d15baad":"code","79f1fab2":"code","9898cbe4":"code","4487e34e":"code","4e0cab0a":"code","a74a50a9":"code","42767f66":"code","3ec33087":"code","693f1c92":"code","7c5d3bb7":"code","c77f1b15":"code","d534c158":"code","6d711282":"code","fde5c614":"code","2d6e1275":"code","8d9207e7":"code","be5b5617":"code","c082e164":"code","b03125e7":"code","054a4b0a":"code","715871f6":"code","203c49d1":"code","dcfaee39":"code","48a4069a":"code","5e63c252":"code","eb8ed209":"code","addcc4eb":"code","f6c2ac75":"code","87c290f5":"code","0f159abb":"code","51201979":"code","a93d017b":"code","26892ae7":"code","3db64370":"code","aa6507d0":"code","8bfb2b01":"code","95b0826f":"code","3a0fb643":"code","d687262f":"code","5a7ac3ba":"code","d9cbc156":"code","4210471e":"code","c8b58a9a":"code","f5313c65":"code","e0ac3a18":"code","d93c028d":"code","1043aa08":"code","bfb09963":"code","8dac680a":"code","085ee07a":"code","046e5bc4":"code","1934a1de":"code","c99788d0":"code","a341954d":"code","f8842764":"code","0e14aa99":"code","cd74121a":"code","64e364bf":"markdown","707a7b07":"markdown","c8b4f809":"markdown","3c3e26b1":"markdown","711d9b7a":"markdown","2c4910cb":"markdown","ac58c404":"markdown","a8d40aaa":"markdown","57b28f89":"markdown"},"source":{"289ceb28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6e32e7a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport scipy.stats as stats\nimport pandas_profiling   #need to install using anaconda prompt (pip install pandas_profiling)\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 7.5\nplt.rcParams['axes.grid'] = True\n\nfrom matplotlib.backends.backend_pdf import PdfPages","793ae8fc":"from sklearn.cluster import KMeans\n\n# center and scale the data\nfrom sklearn.preprocessing import StandardScaler","5267efea":"from sklearn.decomposition import PCA","ecddb841":"# reading data into dataframe\nCC_GENERAL= pd.read_csv(\"\/kaggle\/input\/CC_GENERAL.csv\")\nCC_GENERAL","2209ebc9":"CC_GENERAL.head()","a50e4ddd":"CC_GENERAL.tail()","e5d207ba":"CC_GENERAL.info()","26993672":"CC_GENERAL.dtypes","dd6e7d31":"CC_GENERAL.describe().T","cb9722f8":"CC_GENERAL.columns","03562d3f":"CC_GENERAL.count()","8d7b02ea":"CC_GENERAL.dtypes.value_counts()","4a8b70a1":"CC_GENERAL[\"Monthly_avg_purchase\"] = CC_GENERAL.PURCHASES\/CC_GENERAL.TENURE\nCC_GENERAL[\"Monthly_avg_purchase\"]\nCC_GENERAL","8f981543":"CC_GENERAL[\"Monthly_CASH_ADVANCE\"] = CC_GENERAL.CASH_ADVANCE\/CC_GENERAL.TENURE\nCC_GENERAL[\"Monthly_CASH_ADVANCE\"]\nCC_GENERAL","9c509fb7":"CC_GENERAL[\"Purchases_by_type\"] = CC_GENERAL.ONEOFF_PURCHASES+CC_GENERAL.INSTALLMENTS_PURCHASES\nCC_GENERAL[\"Purchases_by_type\"]\nCC_GENERAL","cc9db4e0":"CC_GENERAL[\"Avg_amt_per_purchase_cash_advance_trx\"] = CC_GENERAL.CASH_ADVANCE_TRX + CC_GENERAL.PURCHASES_TRX\nCC_GENERAL[\"Avg_amt_per_purchase_cash_advance_trx\"]\nCC_GENERAL","0d85493a":"CC_GENERAL[\"Limit_usage\"] = CC_GENERAL.BALANCE\/CC_GENERAL.CREDIT_LIMIT\nCC_GENERAL[\"Limit_usage\"]\nCC_GENERAL","ca492ae6":"CC_GENERAL[\"Payments_to_minimum_payments_ratio\"] = CC_GENERAL.PAYMENTS\/CC_GENERAL.MINIMUM_PAYMENTS\nCC_GENERAL[\"Payments_to_minimum_payments_ratio\"]\nCC_GENERAL","80f1b4bc":"conditions = [\n    (CC_GENERAL['ONEOFF_PURCHASES'] == 0) & (CC_GENERAL['INSTALLMENTS_PURCHASES'] == 0),\n    (CC_GENERAL['ONEOFF_PURCHASES'] > 0) & (CC_GENERAL['INSTALLMENTS_PURCHASES'] == 0),\n    (CC_GENERAL['ONEOFF_PURCHASES'] == 0) & (CC_GENERAL['INSTALLMENTS_PURCHASES'] > 0),\n     (CC_GENERAL['ONEOFF_PURCHASES'] > 0) & (CC_GENERAL['INSTALLMENTS_PURCHASES'] > 0)]\nchoices = ['None', 'One_of', 'Installment_Purchases','Both']\nCC_GENERAL['Purchases_type'] = np.select(conditions, choices)\nCC_GENERAL","e72556a8":"for CUST_ID in CC_GENERAL.columns:\n    if(CC_GENERAL[CUST_ID].dtype == 'object'):\n        CC_GENERAL[CUST_ID]= CC_GENERAL[CUST_ID].astype('category')\n        CC_GENERAL[CUST_ID] = CC_GENERAL[CUST_ID].cat.codes","5b524d75":"CC_GENERAL","7109192b":"CC_GENERAL.describe().T","f43a821f":"CC_GENERAL.dtypes","ccc04f65":"#Detailed profiling using pandas profiling\n\npandas_profiling.ProfileReport(CC_GENERAL)","7e5c6f46":"numeric_var_names=[key for key in dict(CC_GENERAL.dtypes) if dict(CC_GENERAL.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(CC_GENERAL.dtypes) if dict(CC_GENERAL.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","c3c0855d":"data_num = CC_GENERAL[numeric_var_names]\ndata_num","755f76b0":"data_cat = CC_GENERAL[cat_var_names]\ndata_cat","e6f14e55":"# Creating Data audit Report\n# Use a general function that returns multiple values\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary=data_num.apply(lambda x: var_summary(x)).T","6e102e89":"num_summary","179ae6b8":"data_num.info()","68d395c2":"import numpy as np\nfor col in data_num.columns:\n    percentiles = data_num[col].quantile([0.01,0.99]).values\n    data_num[col] = np.clip(data_num[col], percentiles[0], percentiles[1])","5cd55242":"data_num","3804c6fc":"#Handling missings - Method2\ndef Missing_imputation(x):\n    x = x.fillna(x.mean())\n    return x\n\ndata_num=data_num.apply(lambda x: Missing_imputation(x))\ndata_num","7e5166e2":"data_num.corr()","399ecccb":"# visualize correlation matrix in Seaborn using a heatmap\nsns.heatmap(data_num.corr())","3c61d785":"#Handling missings - Method2\ndef Cat_Missing_imputation(x):\n    x = x.fillna(x.mode())\n    return x\n\ndata_cat=data_cat.apply(lambda x: Cat_Missing_imputation(x))\ndata_cat","516ba19f":"# An utility function to create dummy variable\ndef create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname, drop_first=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df\n\nfor c_feature in data_cat.columns:\n    data_cat[c_feature] = data_cat[c_feature].astype('category')\n    data_cat = create_dummies(data_cat , c_feature )","bacb0013":"data_cat.head()","8277d422":"#car_sales=pd.concat(car_sales_num, car_sales_cat)\ndata_new = pd.concat([data_num, data_cat], axis=1)\n\ndata_new.head()","51bcda91":"sc=StandardScaler()\n\ndata_new_scaled=sc.fit_transform(data_num)\ndata_new_scaled","3348b997":"pd.DataFrame(data_new_scaled).head()","90191399":"pd.DataFrame(data_new_scaled).describe()","d6bbba7b":"pc = PCA(n_components=23)","4f774bf1":"pc.fit(data_new_scaled)","01e24e64":"pc.explained_variance_","dbb1164d":"#The amount of variance that each PC explains\nvar= pc.explained_variance_ratio_","20fb1767":"var","3a7f1264":"#Cumulative Variance explains\nvar1=np.cumsum(np.round(pc.explained_variance_ratio_, decimals=4)*100)","00003bbb":"var1","38ed5176":"pc_final=PCA(n_components=7).fit(data_new_scaled)","ad784650":"pc_final.explained_variance_","198257c4":"reduced_cr=pc_final.fit_transform(data_new_scaled)  # the out put is Factors (F1, F2, ...F9)","99eaa5f5":"dimensions = pd.DataFrame(reduced_cr)","5d15baad":"dimensions.columns = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\",\"C7\"]","79f1fab2":"dimensions.head()","9898cbe4":"#pc_final.components_\n\n#print pd.DataFrame(pc_final.components_,columns=telco_num.columns).T\n\nLoadings =  pd.DataFrame((pc_final.components_.T * np.sqrt(pc_final.explained_variance_)).T,columns=data_num.columns).T\nLoadings","4487e34e":"Loadings.to_csv(\"Loadings.csv\")","4e0cab0a":"list_var = ['PURCHASES','CASH_ADVANCE','Payments_to_minimum_payments_ratio','Monthly_avg_purchase',\n'PURCHASES_INSTALLMENTS_FREQUENCY','TENURE','Purchases_by_type','BALANCE','Limit_usage']\nlist_var","a74a50a9":"data_new_scaled1=pd.DataFrame(data_new_scaled, columns=data_num.columns)\ndata_new_scaled1.head(5)\n\ndata_new_scaled2=data_new_scaled1[list_var]\ndata_new_scaled2.head(5)","42767f66":"km_3=KMeans(n_clusters=3,random_state=123)\nkm_3","3ec33087":"km_3.fit(data_new_scaled2)\n#km_4.labels_","693f1c92":"km_3.labels_","7c5d3bb7":"km_3.cluster_centers_","c77f1b15":"pd.Series(km_3.labels_).value_counts()","d534c158":"pd.Series(km_3.labels_).value_counts()\/sum(pd.Series(km_3.labels_).value_counts())","6d711282":"km_4=KMeans(n_clusters=4,random_state=123).fit(data_new_scaled2)\n#km_5.labels_a\n\nkm_5=KMeans(n_clusters=5,random_state=123).fit(data_new_scaled2)\n#km_5.labels_\n\nkm_6=KMeans(n_clusters=6,random_state=123).fit(data_new_scaled2)\n#km_6.labels_\n\nkm_7=KMeans(n_clusters=7,random_state=123).fit(data_new_scaled2)\n#km_7.labels_\n\nkm_8=KMeans(n_clusters=8,random_state=123).fit(data_new_scaled2)\n#km_5.labels_","fde5c614":"# Conactenating labels found through Kmeans with data \n#cluster_df_4=pd.concat([telco_num,pd.Series(km_4.labels_,name='Cluster_4')],axis=1)\n\n# save the cluster labels and sort by cluster\ndata_num['cluster_3'] = km_3.labels_\ndata_num['cluster_4'] = km_4.labels_\ndata_num['cluster_5'] = km_5.labels_\ndata_num['cluster_6'] = km_6.labels_\ndata_num['cluster_7'] = km_7.labels_\ndata_num['cluster_8'] = km_8.labels_","2d6e1275":"data_num.head(5)","8d9207e7":"pd.Series(km_3.labels_).value_counts()\/sum(pd.Series(km_3.labels_).value_counts())","be5b5617":"pd.Series(km_4.labels_).value_counts()\/sum(pd.Series(km_4.labels_).value_counts())","c082e164":"pd.Series(km_5.labels_).value_counts()\/sum(pd.Series(km_5.labels_).value_counts())","b03125e7":"pd.Series(km_6.labels_).value_counts()\/sum(pd.Series(km_6.labels_).value_counts())","054a4b0a":"pd.Series(km_7.labels_).value_counts()\/sum(pd.Series(km_7.labels_).value_counts())","715871f6":"pd.Series(km_8.labels_).value_counts()\/sum(pd.Series(km_8.labels_).value_counts())","203c49d1":"# calculate SC for K=3\nfrom sklearn import metrics\nmetrics.silhouette_score(data_new_scaled2, km_8.labels_)","dcfaee39":"# calculate SC for K=3 through K=12\nk_range = range(3, 9)\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=123)\n    km.fit(data_new_scaled2)\n    scores.append(metrics.silhouette_score(data_new_scaled2, km.labels_))","48a4069a":"scores","5e63c252":"# plot the results\nplt.plot(k_range, scores)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Coefficient')\nplt.grid(True)","eb8ed209":"data_num.cluster_3.value_counts()\/sum(data_num.cluster_3.value_counts())","addcc4eb":"data_num.cluster_4.value_counts()\/sum(data_num.cluster_4.value_counts())","f6c2ac75":"data_num.cluster_5.value_counts()\/sum(data_num.cluster_5.value_counts())","87c290f5":"data_num.cluster_6.value_counts()\/sum(data_num.cluster_6.value_counts())","0f159abb":"data_num.cluster_7.value_counts()\/sum(data_num.cluster_7.value_counts())","51201979":"data_num.cluster_8.value_counts()\/sum(data_num.cluster_8.value_counts())","a93d017b":"cluster_range = range( 1, 20 )\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans( num_clusters )\n    clusters.fit( data_new_scaled2 )\n    cluster_errors.append( clusters.inertia_ )","26892ae7":"clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n\nclusters_df[0:10]","3db64370":"# allow plots to appear in the notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","aa6507d0":"# DBSCAN with eps=1 and min_samples=3\nfrom sklearn.cluster import DBSCAN\ndb = DBSCAN(eps=2.05, min_samples=10)\ndb.fit(data_new_scaled2)","8bfb2b01":"pd.Series(db.labels_).value_counts()","95b0826f":"# review the cluster labels\ndb.labels_","3a0fb643":"# save the cluster labels and sort by cluster\ndata_num['DB_cluster'] = db.labels_","d687262f":"# review the cluster centers\nDBSCAN_clustering = data_num.groupby('DB_cluster').mean()\nDBSCAN_clustering","5a7ac3ba":"DBSCAN_clustering.T","d9cbc156":"data_num.head()","4210471e":"data_num.cluster_3.value_counts()\/1000","c8b58a9a":"data_num.cluster_3.value_counts()*100\/sum(data_num.cluster_3.value_counts())","f5313c65":"pd.Series.sort_index(data_num.cluster_5.value_counts())","e0ac3a18":"data_num.cluster_3.size","d93c028d":"size=pd.concat([pd.Series(data_num.cluster_3.size), pd.Series.sort_index(data_num.cluster_3.value_counts()), pd.Series.sort_index(data_num.cluster_4.value_counts()),\n           pd.Series.sort_index(data_num.cluster_5.value_counts()), pd.Series.sort_index(data_num.cluster_6.value_counts()),\n           pd.Series.sort_index(data_num.cluster_7.value_counts()), pd.Series.sort_index(data_num.cluster_8.value_counts())])","1043aa08":"size","bfb09963":"Seg_size=pd.DataFrame(size, columns=['Seg_size'])\nSeg_Pct = pd.DataFrame(size\/data_num.cluster_3.size, columns=['Seg_Pct'])\nSeg_size.T","8dac680a":"Seg_Pct.T","085ee07a":"data_num.head()","046e5bc4":"# Mean value gives a good indication of the distribution of data. So we are finding mean value for each variable for each cluster\nProfling_output = pd.concat([data_num.apply(lambda x: x.mean()).T, data_num.groupby('cluster_3').apply(lambda x: x.mean()).T, data_num.groupby('cluster_4').apply(lambda x: x.mean()).T,\n          data_num.groupby('cluster_5').apply(lambda x: x.mean()).T, data_num.groupby('cluster_6').apply(lambda x: x.mean()).T,\n          data_num.groupby('cluster_7').apply(lambda x: x.mean()).T, data_num.groupby('cluster_8').apply(lambda x: x.mean()).T], axis=1)\n","1934a1de":"Profling_output","c99788d0":"Profling_output_final=pd.concat([Seg_size.T, Seg_Pct.T, Profling_output], axis=0)","a341954d":"Profling_output_final","f8842764":"#Profling_output_final.columns = ['Seg_' + str(i) for i in Profling_output_final.columns]\nProfling_output_final.columns = ['Overall', 'KM3_1', 'KM3_2', 'KM3_3',\n                                'KM4_1', 'KM4_2', 'KM4_3', 'KM4_4',\n                                'KM5_1', 'KM5_2', 'KM5_3', 'KM5_4', 'KM5_5',\n                                'KM6_1', 'KM6_2', 'KM6_3', 'KM6_4', 'KM6_5','KM6_6',\n                                'KM7_1', 'KM7_2', 'KM7_3', 'KM7_4', 'KM7_5','KM7_6','KM7_7',\n                                'KM8_1', 'KM8_2', 'KM8_3', 'KM8_4', 'KM8_5','KM8_6','KM8_7','KM8_8',]","0e14aa99":"Profling_output_final","cd74121a":"Profling_output_final.to_csv('Profiling_output.csv')","64e364bf":"**Choosing number clusters using Silhouette Coefficient**","707a7b07":"**Number of components have choosen as 06 based on cumulative variance is explaining >75 % and individual component explaining >0.8 variance**","c8b4f809":"**Finding Optimal number of clusters (optional)**\n\n**Elbow Analysis**","3c3e26b1":"**Clustering**","711d9b7a":"Outlier treatment","2c4910cb":"Question-1\n\n\uf0fc Advanced data preparation: Build an \u2018enriched\u2019 customer profile by deriving \u201cintelligent\u201d KPIs such as: \n\uf0b7 Monthly average purchase and cash advance amount \n\uf0b7 Purchases by type (one-off, installments) \n\uf0b7 Average amount per purchase and cash advance transaction, \n\uf0b7 Limit usage (balance to credit limit ratio), \n\uf0b7 Payments to minimum payments ratio etc.","ac58c404":"**DBSCAN clustering**\n\n**Density-based spatial clustering of applications with noise (DBSCAN)**","a8d40aaa":"**Profiling**","57b28f89":"DATA AVAILABLE:\n\n\uf0d8 CC GENERAL.csv\n\nBUSINESS CONTEXT:\n\nThis project requires trainees to develop a customer segmentation to define marketing strategy. The sample dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables. Expectations:\n \nEXPECTATIONS:\n\nAdvanced data preparation: Build an \u2018enriched\u2019 customer profile by deriving \u201cintelligent\u201d KPIs such as:\n\n* Monthly average purchase and cash advance amount\n* Purchases by type (one-off, installments)\n* Average amount per purchase and cash advance transaction,\n* Limit usage (balance to credit limit ratio),\n* Payments to minimum payments ratio etc.\n* Advanced reporting: Use the derived KPIs to gain insight on the customer profiles.\n* Identification of the relationships\/ affinities between services.\n* Clustering: Apply a data reduction technique factor analysis for variable reduction technique and a clustering algorithm to reveal the behavioural segments of credit card holders\n* Identify cluster characterisitics of the cluster using detailed profiling.\n* Provide the strategic insights and implementation of strategies for given set of cluster characteristics\n\nDATA DICTIONARY:\n\n* CUST_ID: Credit card holder ID\n* BALANCE: Monthly average balance (based on daily balance averages)\n* BALANCE_FREQUENCY: Ratio of last 12 months with balance\n* PURCHASES: Total purchase amount spent during last 12 months\n* ONEOFF_PURCHASES: Total amount of one-off purchases\n* INSTALLMENTS_PURCHASES: Total amount of installment purchases\n* CASH_ADVANCE: Total cash-advance amount\n* PURCHASES_ FREQUENCY: Frequency of purchases (Percent of months with at least one purchase)\n* ONEOFF_PURCHASES_FREQUENCY: Frequency of one-off-purchases PURCHASES_INSTALLMENTS_FREQUENCY: Frequency of installment purchases\n* CASH_ADVANCE_ FREQUENCY: Cash-Advance frequency\n* AVERAGE_PURCHASE_TRX: Average amount per purchase transaction\n* CASH_ADVANCE_TRX: Average amount per cash-advance transaction\n* PURCHASES_TRX: Average amount per purchase transaction\n* CREDIT_LIMIT: Credit limit\n* PAYMENTS: Total payments (due amount paid by the customer to decrease their statement balance) in the period\n* MINIMUM_PAYMENTS: Total minimum payments due in the period.\n* PRC_FULL_PAYMEN: Percentage of months with full payment of the due statement balance\n* TENURE: Number of months as a customer"}}