{"cell_type":{"6b7c7a37":"code","3891ba87":"code","9b012d68":"code","555ca23b":"code","1a3a80e6":"code","e1ec7cc9":"code","f8273bfe":"code","afdd6d33":"code","49e376d5":"code","1b840f15":"code","bd0cfd5b":"code","b36f788c":"code","d3168f78":"code","434be310":"code","c742b937":"code","eb398e54":"code","a47c484d":"code","fe1cb89a":"code","4d479391":"code","d9fe13b4":"code","df621aac":"code","029700e8":"markdown","0d13e731":"markdown","3ee9df8f":"markdown","b586a813":"markdown","d7c41c09":"markdown","72147954":"markdown","8eff27d8":"markdown","936b3c42":"markdown"},"source":{"6b7c7a37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3891ba87":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\npd.options.display.max_columns = 1000\ndata = pd.read_csv('\/kaggle\/input\/ameshousingdataset\/AmesHousing.tsv', delimiter='\\t')","9b012d68":"data","555ca23b":"# create initial function to transform features\ndef transform_features(df):\n    return df\n\n# create initial function to select features\ndef select_features(df):\n    return df[['Gr Liv Area','SalePrice']]\n\n# create function to train and test the model\ndef train_and_test(df):\n    train = df[:1460]\n    test = df[1460:]\n    \n    #select only the numerical columns\n    numeric_train = train.select_dtypes(include=['integer','float'])\n    numeric_test = test.select_dtypes(include=['integer','float'])\n    \n    numeric_train = train.select_dtypes(include=['integer', 'float'])\n    numeric_test = test.select_dtypes(include=['integer', 'float'])\n    \n    # drop target column from training\n    features = numeric_train.columns.drop('SalePrice')\n    features = numeric_train.columns.drop(\"SalePrice\")\n\n    \n    # train and test the model\n    lr = LinearRegression()\n    lr.fit(train[features], train[\"SalePrice\"])\n\n    pred = lr.predict(test[features])\n    mse = mean_squared_error(test['SalePrice'], pred)\n    rmse = mse**0.5\n    return rmse\n\n# test the functions and check the final output\ntransform_df = transform_features(data)\nfiltered_df = select_features(transform_df)\nrmse = train_and_test(filtered_df)\n\nrmse","1a3a80e6":"# drop column with 5% or more missing values\nmissing = data.isnull().sum()\nmissing_col = (missing[missing > len(data)\/20]).index\ndata = data.drop(missing_col, axis=1)","e1ec7cc9":"# drop text columns with 1 or more missing values\ntext_col = data.select_dtypes(include=['object']).columns\nmissing_text = data[text_col].isnull().sum()\nms_text_col = missing_text[missing_text >= 1].index\ndata = data.drop(ms_text_col, axis=1)","f8273bfe":"#fill missing value in numerical column with the most frequent value\nnum_col = data.select_dtypes(include=['integer','float']).columns\nnum_miss = data[num_col].isnull().sum()\nnum_miss_cols = num_miss[num_miss > 0].index\ndata[num_miss_cols] = data[num_miss_cols].fillna(data[num_miss_cols].mode().iloc[0])\ndata.isnull().sum().sort_values()","afdd6d33":"# create new feature to indicate the amount of years until the house is sold\ndata['years_sold'] = data['Yr Sold'] - data['Year Built']\n\n# check for incorrect value\ndata['years_sold'][data['years_sold'] < 0] ","49e376d5":"# create new feature to indicate the amount of years until it's being renovated from sale time\ndata['years_since_remod'] = data['Yr Sold'] - data['Year Remod\/Add']\n\n# check for incorrect value\ndata['years_since_remod'][data['years_since_remod'] < 0]","1b840f15":"# drop the incorrect values from previous step\ndata = data.drop([2180, 1702, 2180, 2181], axis=0)\n\n# remove the original column (not needed anymore)\ndata = data.drop(['Yr Sold', 'Year Remod\/Add', 'Year Built'], axis=1)","bd0cfd5b":"# drop the columns that are not useful for the model\ndata = data.drop(['PID', 'Order'], axis=1)\n\n# drop the columns that leak information about the sale\ndata = data.drop(['Mo Sold', 'Sale Type', 'Sale Condition'], axis=1)","b36f788c":"def transform_features(df):\n    # drop column with 5% or more missing values\n    missing = df.isnull().sum()\n    missing_col = (missing[missing > len(df)\/20]).index\n    df = df.drop(missing_col, axis=1)\n    \n    # drop text columns with 1 or more missing values\n    text_col = df.select_dtypes(include=['object']).columns\n    missing_text = df[text_col].isnull().sum()\n    ms_text_col = missing_text[missing_text >= 1].index\n    df = df.drop(ms_text_col, axis=1)\n    \n    #fill missing value in numerical column with the most frequent value\n    num_col = df.select_dtypes(include=['integer','float']).columns\n    num_miss = df[num_col].isnull().sum()\n    num_miss_cols = num_miss[num_miss > 0].index\n    df[num_miss_cols] = df[num_miss_cols].fillna(df[num_miss_cols].mode().iloc[0])\n    \n    # create new features\n    df['years_sold'] = df['Yr Sold'] - df['Year Built']\n    df['years_since_remod'] = df['Yr Sold'] - df['Year Remod\/Add']\n    \n    df = df.drop([2180, 1702, 2180, 2181], axis=0)\n\n    # drop not needed & leaking columns\n    df = df.drop(['Yr Sold', 'Year Remod\/Add', 'Year Built', 'PID', 'Order', 'Mo Sold',\n                  'Sale Type', 'Sale Condition'], axis=1)\n\n    return df\n\n# test the function\ndf = pd.read_csv('\/kaggle\/input\/ameshousingdataset\/AmesHousing.tsv', delimiter='\\t')\ntransformed = transform_features(df)\nselected_features = select_features(transformed)\ntest = train_and_test(selected_features)\ntest","d3168f78":"# check the numerical columns\nnum_df = transformed.select_dtypes(include = ['float', 'integer'])\nnum_df","434be310":"# build the correlation with target column\ncorr = num_df.corr()['SalePrice'].abs().sort_values()\ncorr","c742b937":"# filter the columns with the correlation > 0.4\ncorr = corr[corr > 0.4] \ncorr","eb398e54":"# drop the low correlating columns\ntransformed = transformed.drop(corr[corr < 0.4].index, axis=1)","a47c484d":"# Create a list of column names from documentation that should be categorical\nnominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n\n# list the columns that need to be transformed\ntrans_col = []\nfor i in nominal_features:\n    if i in transformed.columns:\n        trans_col.append(i)\n\n# find out the unique values in each column\nunique_counts = transformed[trans_col].apply(lambda x: len(x.value_counts())).sort_values()\n\n# set the threshold for the amount of unique values. Here we will use column with <10 unique values.\nnonunique_counts = unique_counts[unique_counts > 10]\n\n# drop the columns with unique values >10\ntransformed = transformed.drop(nonunique_counts.index, axis=1) ","fe1cb89a":"# select the remaining text columns and convert it to categorical data\ntext_cols = transformed.select_dtypes(include=['object'])\n\nfor i in text_cols:\n    transformed[i] = transformed[i].astype('category')\n\n# create dummy columns and drop the original columns\ntransformed = pd.concat([transformed,\n                         pd.get_dummies(transformed.select_dtypes(include=['category']))\n                        ], axis =1).drop(text_cols, axis=1)","4d479391":"def select_features(df, corrval=0.4, threshval=10):\n    # check the numerical columns\n    num_df = df.select_dtypes(include = ['float', 'int'])\n    \n    # build the correlation with target column\n    corr = num_df.corr()['SalePrice'].abs().sort_values()\n    \n    # drop the low correlating columns\n    df = df.drop(corr[corr < corrval].index, axis=1)\n    \n    # List the categorical columns\n    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n    \n    # list the columns that need to be transformed\n    trans_col = []\n    for i in nominal_features:\n        if i in df.columns:\n            trans_col.append(i)\n            \n    # find out the unique values in each column\n    unique_counts = df[trans_col].apply(lambda x: len(x.value_counts())).sort_values()\n\n    # set the threshold for the amount of unique values\n    nonunique_counts = unique_counts[unique_counts > threshval]\n\n    # drop the columns with unique values exceeding threshold value\n    df = df.drop(nonunique_counts.index, axis=1) \n    \n    # select the remaining text columns and convert it to categorical data\n    text_cols = df.select_dtypes(include=['object'])\n\n    for i in text_cols:\n        df[i] = df[i].astype('category')\n\n    # create dummy columns and drop the original columns\n    df = pd.concat([df,\n                         pd.get_dummies(df.select_dtypes(include=['category']))\n                        ], axis =1).drop(text_cols, axis=1)\n\n    return df\n\n# test the function\ndf = pd.read_csv('\/kaggle\/input\/ameshousingdataset\/AmesHousing.tsv', delimiter='\\t')\ntransformed = transform_features(df)\nselected_features = select_features(transformed)\ntest = train_and_test(selected_features)\ntest","d9fe13b4":"from sklearn.model_selection import cross_val_score, KFold","df621aac":"# The function accepts k parameter, k=0 (default) for holdout validation, k=1 for cross validation,\n# and k fold validation\ndef train_and_test(df, k=0):\n    num_df = df.select_dtypes(include=['float', 'int'])\n    features = df.columns.drop('SalePrice')\n    lr = LinearRegression()\n\n    if k==0:\n        train = df[:1460]\n        test = df[1460:]\n    \n        # train and test the model\n        lr.fit(train[features], train[\"SalePrice\"])\n        pred = lr.predict(test[features])\n        mse = mean_squared_error(test['SalePrice'], pred)\n        rmse = np.sqrt(mse)\n        \n    elif k==1:\n        # randomize order of rows\n        np.random.seed(1)\n        shuffled_index = np.random.permutation(df.index)\n        df = df.reindex(shuffled_index)\n        \n        train = df[:1460]\n        test = df[1460:]\n    \n        # train and test the model\n        lr.fit(train[features], train[\"SalePrice\"])\n        pred1 = lr.predict(test[features])\n        mse1 = mean_squared_error(test['SalePrice'], pred1)\n        rmse1 = np.sqrt(mse1)\n        \n        lr.fit(test[features], test[\"SalePrice\"])\n        pred2 = lr.predict(train[features])\n        mse2 = mean_squared_error(train['SalePrice'], pred2)\n        rmse2 = np.sqrt(mse2)\n        \n        rmse = (rmse1 + rmse2) \/ 2\n        \n    else:\n        kf = KFold(n_splits=k, shuffle=True, random_state=1)\n        mses = cross_val_score(estimator=lr, X=df[features], y=df['SalePrice'], scoring='neg_mean_squared_error', cv=kf)\n        rmse = np.mean(abs(mses)**0.5)\n\n    return rmse\n\ndf = pd.read_csv('\/kaggle\/input\/ameshousingdataset\/AmesHousing.tsv', delimiter='\\t')\ntransformed = transform_features(df)\nselected_features = select_features(transformed)\ntest = train_and_test(selected_features, k=5)\ntest","029700e8":"Now we will incorporate the steps that we have done previously into the transform_features function","0d13e731":"# Feature Selection\n\n","3ee9df8f":"We will create new features that is useful for the model by performing operation on the columns:\n1. 'Yr Sold'\n1. 'Year Built'\n1. 'Year Remod\/Add'","b586a813":"We will also drop columns that:\n1. Not useful for the machine learning model\n1. Leak data about the sale: the sale price is what we're trying to predict, and these informations give out clue to better predict the price, which is unknown in real practice.","d7c41c09":"Currently train_and_test function only perform holdout validation (splitting into 2 subsets, train and test). Now we will update the function to perform cross validation as well.\n","72147954":"# Feature Engineering\nWe will remove features with many missing values, diving deeper into potential categorical features, and transforming text and numerical columns. The transform_features() will be updated so that any column from the data frame exceeding the threshold of missing values is dropped. We will also remove any columns that leak information about the sale (e.g. like the year the sale happened). In general, the goal of this function is to:\n\n1. remove features that we don't want to use in the model, just based on the number of missing values or data leakage\n1. transform features into the proper format (numerical to categorical, scaling numerical, filling in missing values, etc)\n1. create new features by combining other features\n\nThis is how we're going to handle the missing values:\n1. All columns: drop any with 5% or more missing values\n1. Text column: drop any with 1 or more missing values\n1. Numerical column: fill the most common value in that column","8eff27d8":"Update the logic for the select_features() function. This function should take in the new, modified train and test data frames that were returned from transform_features()","936b3c42":"In this project, we will practice model building, including the techniques of cleaning, transforming, and selecting features, and eventually exploring ways to improve the models built.\n\nThe data used is the housing data for the city of Ames, Iowa, United States from 2006 to 2010.\n\nThe functions pipeline to test on different models is the following:\n1. Train set\n1. Transform features\n1. Select Features\n1. Train and test\n1. Result: RMSE values\n\n"}}