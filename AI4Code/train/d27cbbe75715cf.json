{"cell_type":{"773f0bb8":"code","8ad4aadb":"code","350ca775":"code","19ce517f":"code","938ec1b2":"code","35939a0c":"code","8b0d3637":"code","77654690":"code","58049efa":"code","80f8e4a3":"code","d812e2bb":"code","f50d58aa":"code","41bb4f25":"code","869a9650":"code","a52cf2c9":"code","99db9850":"code","7208a6f8":"code","ed3fec5a":"code","fa5a48f0":"markdown","699b6a7b":"markdown","95d6d8c7":"markdown","e55a5cc8":"markdown","2bc947b0":"markdown"},"source":{"773f0bb8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV, KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom keras.callbacks import EarlyStopping\nfrom keras.metrics import RootMeanSquaredError\nfrom sklearn.metrics import mean_squared_error as MSE","8ad4aadb":"trainData = pd.read_csv(\"..\/input\/assignment3\/train.csv\")\nevalData = pd.read_csv(\"..\/input\/assignment3\/eval.csv\")\n\nprint(trainData.dtypes)","350ca775":"# Checking for missing values\nhasMissing = []\nfor colName in trainData.columns:\n    totalMissing = trainData[colName].isnull().sum()\n    if (totalMissing > 0):\n        hasMissing.append((colName, totalMissing))\nprint(hasMissing)\n# There are no missing values","19ce517f":"correlations = trainData.corr()\nsns.heatmap(correlations, square = True, cmap=sns.diverging_palette(100, 0, n=200))\n# There are area of high correlation and areas of little correlation\n# Some have no correlation with anything","938ec1b2":"# Splitting trainData into training and testing data\n# Removing id and pubchem_id because they have no useful correlations\n# Removing eat as it is the label of the data and should not be used for training\nfeatures = trainData.loc[:, ~trainData.columns.isin([\"id\", \"pubchem_id\", \"Eat\"])]\nX = pd.get_dummies(trainData[features.columns])\ny = trainData[\"Eat\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, random_state = 8, test_size = 0.1)","35939a0c":"params = {'epochs': [10], 'learning_rate': [0.1, 0.01, 0.001], \"activation\": [\"relu\"]}\nmonitor = \"loss\"\nearlyStopper = EarlyStopping(patience = 7, monitor = monitor)\n\ndef createModel1(learning_rate = 0.001, activation = \"relu\"):\n    opt = Adam(learning_rate = learning_rate)\n    \n    model = Sequential()\n    model.add(Dense(100, input_shape = (1275,), activation = activation))\n    model.add(Dense(50, activation = activation))\n    model.add(Dense(1))\n    \n    model.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [RootMeanSquaredError()])\n    return model\n\ndef createModel2(learning_rate = 0.001, activation = \"relu\"):\n    opt = Adam(learning_rate = learning_rate)\n    \n    model = Sequential()\n    model.add(Dense(50, input_shape = (1275,), activation = activation))\n    model.add(Dense(100, activation = activation))\n    model.add(Dense(100, activation = activation))\n    model.add(Dense(1))\n    \n    model.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [RootMeanSquaredError()])\n    return model\n\ndef createModel3(learning_rate = 0.001, activation = \"relu\"):\n    opt = Adam(learning_rate = learning_rate)\n    \n    model = Sequential()\n    model.add(Dense(500, input_shape = (1275,), activation = activation))\n    model.add(Dense(100, activation = activation))\n    model.add(Dense(100, activation = activation))\n    model.add(Dense(500, activation = activation))\n    model.add(Dense(1))\n    \n    model.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [RootMeanSquaredError()])\n    return model","8b0d3637":"# model 1: [100, 50, 1]\nmodel1 = KerasRegressor(build_fn = createModel1)\nmodel1Search = RandomizedSearchCV(model1, param_distributions = params, cv = KFold(3))\nmodel1CrossVal = cross_val_score(model1, X_train, y_train, cv = 3)\nmodel1Search.fit(X_train, y_train)","77654690":"model1Params = model1Search.best_params_\nprint(model1Params)\nprint(pd.DataFrame(model1CrossVal).describe())\nmodel1Best = createModel1(model1Params[\"learning_rate\"], model1Params[\"activation\"])\nmodel1Training = model1Best.fit(X_train, y_train, epochs = 100)\nplt.plot(model1Training.history[monitor])","58049efa":"# model 2: [50, 50, 100, 1]\nmodel2 = KerasRegressor(build_fn = createModel2)\nmodel2Search = RandomizedSearchCV(model2, param_distributions = params, cv = KFold(3))\nmodel2CrossVal = cross_val_score(model2, X_train, y_train, cv = 3)\nmodel2Search.fit(X_train, y_train)","80f8e4a3":"model2Params = model2Search.best_params_\nprint(model2Params)\nprint(pd.DataFrame(model2CrossVal).describe())\nmodel2Best = createModel2(model2Params[\"learning_rate\"], model2Params[\"activation\"])\nmodel2Training = model2Best.fit(X_train, y_train, epochs = 100)\nplt.plot(model2Training.history[monitor])","d812e2bb":"# model 3: [100, 50, 50, 100, 1]\nmodel3 = KerasRegressor(build_fn = createModel3)\nmodel3Search = RandomizedSearchCV(model3, param_distributions = params, cv = KFold(3))\nmodel3CrossVal = cross_val_score(model3, X_train, y_train, cv = 3)\nmodel3Search.fit(X_train, y_train)","f50d58aa":"model3Params = model3Search.best_params_\nprint(model3Params)\nprint(pd.DataFrame(model3CrossVal).describe())\nmodel3Best = createModel3(model3Params[\"learning_rate\"], activation = \"relu\") # , model3Params[\"activation\"]\nmodel3Training = model3Best.fit(X_train, y_train, epochs = 100) #, callbacks = [earlyStopper]\nplt.plot(model3Training.history[monitor])","41bb4f25":"fig, axs = plt.subplots(1, 3)\n\naxs[0].plot(model1Training.history[monitor])\naxs[0].set_title(\"Model 1 \" + monitor)\naxs[1].plot(model2Training.history[monitor])\naxs[1].set_title(\"Model 2 \" + monitor)\naxs[2].plot(model3Training.history[monitor])\naxs[2].set_title(\"Model 3 \" + monitor)","869a9650":"model1Pred = model1Best.predict(X_test)\nmodel1PredFixed = model1Pred.reshape(len(model1Pred),)\nmodel2Pred = model2Best.predict(X_test)\nmodel2PredFixed = model2Pred.reshape(len(model2Pred),)\nmodel3Pred = model3Best.predict(X_test)\nmodel3PredFixed = model3Pred.reshape(len(model3Pred),)\n\n#plt.scatter(model1PredFixed, np.sqrt((model1PredFixed - y_test) ** 2))\n\nfig, axs = plt.subplots(1, 3)\n\n\n\naxs[0].scatter(model1PredFixed, np.sqrt((model1PredFixed - y_test) ** 2), c = np.sqrt((model1PredFixed - y_test) ** 2))\naxs[0].set_title(\"Model 1 test data RMSE\")\naxs[0].set_ylim(0, 2)\naxs[1].scatter(model2PredFixed, np.sqrt((model2PredFixed - y_test) ** 2), c = np.sqrt((model2PredFixed - y_test) ** 2))\naxs[1].set_title(\"Model 2 test data RMSE\")\naxs[1].set_ylim(0, 2)\naxs[2].scatter(model3PredFixed, np.sqrt((model3PredFixed - y_test) ** 2), c = np.sqrt((model3PredFixed - y_test) ** 2))\naxs[2].set_title(\"Model 3 test data RMSE\")\naxs[2].set_ylim(0, 2)\nfig.tight_layout()\nplt.subplots_adjust(wspace = 1)\nplt.show()\n# The closer to 0, the lower the RMSE of the test scores\n# Each model seems to have smilar RMSE scores for the test values","a52cf2c9":"print(\"Model 1 test score RMSE summary:\\n\", pd.DataFrame(np.sqrt((model1PredFixed - y_test) ** 2)).describe())\nprint(\"Model 2 test score RMSE summary:\\n\", pd.DataFrame(np.sqrt((model2PredFixed - y_test) ** 2)).describe())\nprint(\"Model 3 test score RMSE summary:\\n\", pd.DataFrame(np.sqrt((model3PredFixed - y_test) ** 2)).describe())","99db9850":"mod1 = model1Best.evaluate(X_test, y_test)\nmod2 = model2Best.evaluate(X_test, y_test)\nmod3 = model3Best.evaluate(X_test, y_test)\nmods = [(model1Best, mod1[1]), (model2Best, mod2[1]), (model3Best, mod3[1])]\nlowestRMSE = np.inf\nbestModel = None    \nfor model in mods:\n    if (model[1] < np.inf):\n        lowestRMSE = model[1]\n        bestModel = model[0]\nprint(lowestRMSE)\nprint(bestModel.summary())\n\n\n# Model 3 is (typically) the best scoring one, as it usually has the lowest loss and lowest RMSE, and on multiple runs,\n# it consistently wins","7208a6f8":"testFeatures = evalData.loc[:, ~evalData.columns.isin(['id', \"pubchem_id\"])]\nfixedTestData = pd.get_dummies(evalData[testFeatures.columns])","ed3fec5a":"pred = bestModel.predict(fixedTestData)\nfixedPred = pred.reshape(len(pred),)\noutput = pd.DataFrame({'id': evalData[\"id\"], \"Eat\": fixedPred})\noutput.to_csv('submission.csv', index = False)\nprint(\"Your submission was successfully saved!\")\nprint(output.to_string())","fa5a48f0":"### 1. Importing modules and loading data","699b6a7b":"# Assignment 3\n## Dalton Kajander","95d6d8c7":"#### Functions","e55a5cc8":"### 3. Creating models","2bc947b0":"### 2. Exploratory data analysis, cleaning, and feature engineering"}}