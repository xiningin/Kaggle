{"cell_type":{"25f903d9":"code","039c2b2a":"code","16ce335a":"code","073ea211":"code","ceb5d80e":"code","77588478":"code","cb0f9e7d":"code","d57fb714":"code","04f107b3":"code","d25a4730":"code","aa2f04c4":"code","0b939f0d":"code","7bf1f639":"code","858654f9":"code","3fd16ba6":"code","0bbf143e":"code","972d581f":"code","d9987e48":"code","920a2680":"code","34a591e3":"code","90a07c84":"code","91af59c1":"code","7a75d5c3":"code","a36af3a6":"code","5c90110a":"code","90b01062":"code","285baf1e":"code","f617055f":"markdown","92236975":"markdown","21411169":"markdown","ae28d73d":"markdown","e57f8014":"markdown","7bdd62dd":"markdown","05542709":"markdown"},"source":{"25f903d9":"import numpy as np\nimport pandas as pd\nimport random\nimport copy\nimport time\n\nimport plotly\nfrom plotly import tools\nfrom plotly.graph_objs import *\nfrom plotly.offline import iplot\n\nimport torch\nfrom torch import optim\nfrom torch import nn\n\nfrom collections import namedtuple","039c2b2a":"df = pd.read_csv('..\/input\/binance-top-cryptocurrencies\/BTC.csv')\ndf.head()","16ce335a":"# convert date format\ndf['date'] = pd.to_datetime(df['date'])\n\n# set date as index\ndf = df.set_index('date')","073ea211":"df.head()","ceb5d80e":"df.sort_values(by=['date'], inplace=True, ascending=True)","77588478":"# 0.7 split limit\nsplit_limit = int(len(df) * 0.7)\n\ndf_train = df.iloc[:split_limit]\ndf_test = df.iloc[split_limit:]","cb0f9e7d":"def plot_train_test(train, test, date_split):\n    data = [Candlestick(x=train.index, open=train['open'], high=train['high'], low=train['low'], close=train['close'], name='train'),\n            Candlestick(x=test.index, open=test['open'], high=test['high'], low=test['low'], close=test['close'], name='test')]\n    \n    layout = { 'shapes': [{'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}],\n               'annotations': [{'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n                               {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}]}\n    \n    figure = Figure(data=data, layout=layout)\n    iplot(figure)","d57fb714":"plot_train_test(df_train, df_test, df_train.iloc[-1].name)","04f107b3":"class Environment:\n    def __init__(self, data, time=90):\n        self.data = data\n        self.time = time\n        self.actions = {\"stay\": 0, \"buy\": 1, \"sell\": 2}\n        self.reset()\n        \n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.profits = 0\n        self.positions = []\n        self.position_value = 0\n        self.history = [0 for _ in range(self.time)]\n        return torch.tensor([self.position_value]  + self.history).type(torch.float32)\n    \n    def next(self):\n        self.t += 1\n        self.position_value = 0\n        for p in self.positions:\n            self.position_value += (self.data.iloc[self.t, :]['close'] - p)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['close'] - self.data.iloc[(self.t-1), :]['close'])\n    \n    def step(self, act):\n        reward = 0\n        \n        if act == self.actions[\"buy\"]:\n            self.positions.append(self.data.iloc[self.t, :]['close'])\n        elif act == self.actions[\"sell\"]:\n            if len(self.positions) == 0:\n                reward -= 1\n            else:\n                profits = 0\n                for p in self.positions:\n                    profits += (self.data.iloc[self.t, :]['close'] - p)\n                    reward += profits\n                    self.profits += profits\n                    self.positions = []\n                    \n        self.next()\n        reward = 1 if reward > 0 else -1\n        \n        obs = torch.tensor([self.position_value] + self.history).type(torch.float32)\n        \n        return obs, reward, self.done","d25a4730":"env = Environment(df_train)","aa2f04c4":"Transition = namedtuple('Transition',\n                        ('pobs', 'pact', 'reward', 'obs', 'done'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        \"\"\"Saves a transition.\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","0b939f0d":"class QNetwork(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super(QNetwork, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.l2 = nn.Linear(hidden_size, hidden_size * 2)\n        self.l3 = nn.Linear(hidden_size * 2, hidden_size)\n        self.l4 = nn.Linear(hidden_size, hidden_size \/\/ 2)\n        self.l5 = nn.Linear(hidden_size \/\/ 2, output_size)\n        self.r = nn.ReLU()\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = self.r(self.l1(x))\n        x = self.r(self.l2(x))\n        x = self.r(self.l3(x))\n        x = self.r(self.l4(x))\n        x = self.l5(x)\n        return x","7bf1f639":"Q = QNetwork(input_size=env.time + 1, hidden_size=256, output_size=3)\nQ_ast = copy.deepcopy(Q)\noptimizer = optim.Adam(Q.parameters(), lr=0.005)\nloss_fn = nn.MSELoss()","858654f9":"epoch_num = 50\nstep_max = len(env.data)-1\nmemory_size = 200\nbatch_size = 32\nepsilon = 1.0\nepsilon_decrease = 0.01\nepsilon_min = 0.1\nstart_reduce_epsilon = 10\ntrain_freq = 10\nupdate_q_freq = 10\ngamma = 0.97\nshow_log_freq = 10\n\nmemory = ReplayMemory(memory_size)\ntotal_step = 0\ntotal_rewards = []\ntotal_losses = []\n\nstart = time.time()\nfor epoch in range(epoch_num):\n    pobs = env.reset()\n    step = 0\n    done = False\n    total_reward = 0\n    total_loss = 0\n    \n    while not done and step < step_max:\n        \n        # select act\n        pact = torch.randint(0, 3, (1,))\n        if np.random.rand() > epsilon:\n            pact = Q.forward(pobs.reshape(1, -1))\n            pact = torch.argmax(pact.data)\n            \n        # act\n        obs, reward, done = env.step(pact)\n        \n        # add memory\n        memory.push(pobs, pact, reward, obs, done)\n        \n        if len(memory) == memory_size:\n            if total_step % train_freq == 0:\n                batch = memory.sample(batch_size)\n                \n                b_pobs = torch.zeros(batch_size, 91)\n                b_pact = torch.zeros(batch_size, 1, dtype=torch.long)\n                b_reward = torch.zeros(batch_size, 1)\n                b_obs = torch.zeros(batch_size, 91)\n                b_done = torch.zeros(batch_size, 1, dtype=torch.bool)\n\n                for i, b in enumerate(batch):\n                    b_pobs[i] = b.pobs\n                    b_pact[i] = b.pact\n                    b_reward[i] = b.reward\n                    b_obs[i] = b.obs\n                    b_done[i] = b.done\n                    \n                q = Q(b_pobs)\n                maxq = torch.max(Q_ast(b_obs).data, axis=1)\n                target = copy.deepcopy(q.data)\n                \n                for j in range(batch_size):\n                    target[j, b_pact[j]] = b_reward[j] + gamma * maxq.values[j] * (not b_done[j])\n                \n                optimizer.zero_grad()\n                \n                loss = loss_fn(q, target)\n                total_loss += loss.data\n                loss.backward()\n                optimizer.step()\n                \n                \n            if total_step % update_q_freq == 0:\n                Q_ast = copy.deepcopy(Q)\n        \n        # epsilon\n        if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n            epsilon -= epsilon_decrease\n        \n        # next step\n        total_reward += reward\n        pobs = obs\n        step += 1\n        total_step += 1\n        \n    total_rewards.append(total_reward)\n    total_losses.append(total_loss)\n\n    if (epoch+1) % show_log_freq == 0:\n        log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n        log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n        elapsed_time = time.time()-start\n        print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n        start = time.time()","3fd16ba6":"def plot_loss_reward(total_losses, total_rewards):\n\n    figure = plotly.subplots.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n    figure['layout']['xaxis1'].update(title='epoch')\n    figure['layout']['xaxis2'].update(title='epoch')\n    figure['layout'].update(height=400, width=900, showlegend=False)\n    iplot(figure)","0bbf143e":"plot_loss_reward(total_losses, total_rewards)","972d581f":"def plot_train_test_by_q(train_env, test_env, Q, algorithm_name, date_split):\n    \n    # train\n    pobs = train_env.reset()\n    train_acts = []\n    train_rewards = []\n\n    for _ in range(len(train_env.data)-1):\n        \n        pact = Q.forward(pobs.reshape(1, -1))\n        pact = torch.argmax(pact.data)\n        train_acts.append(pact)\n            \n        obs, reward, done = train_env.step(pact)\n        train_rewards.append(reward)\n\n        pobs = obs\n        \n    train_profits = train_env.profits\n    \n    # test\n    pobs = test_env.reset()\n    test_acts = []\n    test_rewards = []\n\n    for _ in range(len(test_env.data)-1):\n    \n        pact = Q.forward(pobs.reshape(1, -1))\n        pact = torch.argmax(pact.data)\n        test_acts.append(pact)\n            \n        obs, reward, done = test_env.step(pact)\n        test_rewards.append(reward)\n\n        pobs = obs\n        \n    test_profits = test_env.profits\n    \n    # plot\n    train_copy = train_env.data.copy()\n    test_copy = test_env.data.copy()\n    train_copy['act'] = train_acts + [np.nan]\n    train_copy['reward'] = train_rewards + [np.nan]\n    test_copy['act'] = test_acts + [np.nan]\n    test_copy['reward'] = test_rewards + [np.nan]\n    train0 = train_copy[train_copy['act'] == 0]\n    train1 = train_copy[train_copy['act'] == 1]\n    train2 = train_copy[train_copy['act'] == 2]\n    test0 = test_copy[test_copy['act'] == 0]\n    test1 = test_copy[test_copy['act'] == 1]\n    test2 = test_copy[test_copy['act'] == 2]\n    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n\n    data = [\n        Candlestick(x=train0.index, open=train0['open'], high=train0['high'], low=train0['low'], close=train0['close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=train1.index, open=train1['open'], high=train1['high'], low=train1['low'], close=train1['close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=train2.index, open=train2['open'], high=train2['high'], low=train2['low'], close=train2['close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n        Candlestick(x=test0.index, open=test0['open'], high=test0['high'], low=test0['low'], close=test0['close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=test1.index, open=test1['open'], high=test1['high'], low=test1['low'], close=test1['close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=test2.index, open=test2['open'], high=test2['high'], low=test2['low'], close=test2['close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n    ]\n    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n        algorithm_name,\n        int(sum(train_rewards)),\n        int(train_profits),\n        int(sum(test_rewards)),\n        int(test_profits)\n    )\n    layout = {\n        'title': title,\n        'showlegend': False,\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)","d9987e48":"plot_train_test_by_q(Environment(df_train), Environment(df_test), Q, 'DQN', df_train.iloc[-1].name)","920a2680":"Q = QNetwork(input_size=env.time + 1, hidden_size=100, output_size=3)\nQ_ast = copy.deepcopy(Q)\noptimizer = optim.Adam(Q.parameters(), lr=0.005)\nloss_fn = nn.MSELoss()","34a591e3":"epoch_num = 100\nstep_max = len(env.data)-1\nmemory_size = 200\nbatch_size = 32\nepsilon = 1.0\nepsilon_decrease = 0.01\nepsilon_min = 0.1\nstart_reduce_epsilon = 200\ntrain_freq = 10\nupdate_q_freq = 20\ngamma = 0.97\nshow_log_freq = 10\n\nmemory = ReplayMemory(memory_size)\ntotal_step = 0\ntotal_rewards = []\ntotal_losses = []\n\nstart = time.time()\nfor epoch in range(epoch_num):\n    pobs = env.reset()\n    step = 0\n    done = False\n    total_reward = 0\n    total_loss = 0\n    \n    while not done and step < step_max:\n        \n        # select act\n        pact = torch.randint(0, 3, (1,))\n        if np.random.rand() > epsilon:\n            pact = Q.forward(pobs.reshape(1, -1))\n            pact = torch.argmax(pact.data)\n            \n        # act\n        obs, reward, done = env.step(pact)\n        \n        # add memory\n        memory.push(pobs, pact, reward, obs, done)\n        \n        if len(memory) == memory_size:\n            if total_step % train_freq == 0:\n                batch = memory.sample(batch_size)\n                \n                b_pobs = torch.zeros(batch_size, 91)\n                b_pact = torch.zeros(batch_size, 1, dtype=torch.long)\n                b_reward = torch.zeros(batch_size, 1)\n                b_obs = torch.zeros(batch_size, 91)\n                b_done = torch.zeros(batch_size, 1, dtype=torch.bool)\n\n                for i, b in enumerate(batch):\n                    b_pobs[i] = b.pobs\n                    b_pact[i] = b.pact\n                    b_reward[i] = b.reward\n                    b_obs[i] = b.obs\n                    b_done[i] = b.done\n                    \n                q = Q(b_pobs)\n                indices = torch.argmax(q.data, axis=1)\n                maxqs = Q_ast(b_obs).data\n                \n                target = copy.deepcopy(q.data)\n                \n                for j in range(batch_size):\n                    target[j, b_pact[j]] = b_reward[j] + gamma * maxqs[j, indices[j]] * (not b_done[j])\n                    \n                optimizer.zero_grad()\n                \n                loss = loss_fn(q, target)\n                total_loss += loss.data\n                loss.backward()\n                optimizer.step()\n                \n                \n            if total_step % update_q_freq == 0:\n                Q_ast = copy.deepcopy(Q)\n        \n        # epsilon\n        if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n            epsilon -= epsilon_decrease\n        \n        # next step\n        total_reward += reward\n        pobs = obs\n        step += 1\n        total_step += 1\n        \n    total_rewards.append(total_reward)\n    total_losses.append(total_loss)\n\n    if (epoch+1) % show_log_freq == 0:\n        log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n        log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n        elapsed_time = time.time()-start\n        print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n        start = time.time()","90a07c84":"plot_loss_reward(total_losses, total_rewards)","91af59c1":"plot_train_test_by_q(Environment(df_train), Environment(df_test), Q, 'DDQN', df_train.iloc[-1].name)","7a75d5c3":"class QNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(QNetwork, self).__init__()\n\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc_value = nn.Linear(hidden_size, hidden_size \/\/ 2)\n        self.fc_adv = nn.Linear(hidden_size, hidden_size \/\/ 2)\n        self.relu = nn.ReLU()\n\n        self.value = nn.Linear(hidden_size \/\/ 2, 1)\n        self.adv = nn.Linear(hidden_size \/\/ 2, output_size)\n\n    def forward(self, state):\n        y = self.relu(self.fc1(state))\n        value = self.relu(self.fc_value(y))\n        adv = self.relu(self.fc_adv(y))\n\n        value = self.value(value)\n        adv = self.adv(adv)\n\n        advAverage = torch.mean(adv, dim=1, keepdim=True)\n        Q = value + adv - advAverage\n\n        return Q","a36af3a6":"Q = QNetwork(input_size=env.time + 1, hidden_size=64, output_size=3)\nQ_ast = copy.deepcopy(Q)\noptimizer = optim.Adam(Q.parameters(), lr=0.001)\nloss_fn = nn.MSELoss()","5c90110a":"epoch_num = 100\nstep_max = len(env.data)-1\nmemory_size = 200\nbatch_size = 8\nepsilon = 1.0\nepsilon_decrease = 0.01\nepsilon_min = 0.1\nstart_reduce_epsilon = 10\ntrain_freq = 10\nupdate_q_freq = 20\ngamma = 0.97\nshow_log_freq = 10\n\nmemory = ReplayMemory(memory_size)\ntotal_step = 0\ntotal_rewards = []\ntotal_losses = []\n\nstart = time.time()\nfor epoch in range(epoch_num):\n    pobs = env.reset()\n    step = 0\n    done = False\n    total_reward = 0\n    total_loss = 0\n    \n    while not done and step < step_max:\n        \n        # select act\n        pact = torch.randint(0, 3, (1,))\n        if np.random.rand() > epsilon:\n            pact = Q.forward(pobs.reshape(1, -1))\n            pact = torch.argmax(pact.data)\n            \n        # act\n        obs, reward, done = env.step(pact)\n        \n        # add memory\n        memory.push(pobs, pact, reward, obs, done)\n        \n        if len(memory) == memory_size:\n            if total_step % train_freq == 0:\n                batch = memory.sample(batch_size)\n                \n                b_pobs = torch.zeros(batch_size, 91)\n                b_pact = torch.zeros(batch_size, 1, dtype=torch.long)\n                b_reward = torch.zeros(batch_size, 1)\n                b_obs = torch.zeros(batch_size, 91)\n                b_done = torch.zeros(batch_size, 1, dtype=torch.bool)\n\n                for i, b in enumerate(batch):\n                    b_pobs[i] = b.pobs\n                    b_pact[i] = b.pact\n                    b_reward[i] = b.reward\n                    b_obs[i] = b.obs\n                    b_done[i] = b.done\n                    \n                q = Q(b_pobs)\n                indices = torch.argmax(q.data, axis=1)\n                maxqs = Q_ast(b_obs).data\n                \n                target = copy.deepcopy(q.data)\n                \n                for j in range(batch_size):\n                    target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n                    \n                optimizer.zero_grad()\n                \n                loss = loss_fn(q, target)\n                total_loss += loss.data\n                loss.backward()\n                optimizer.step()\n                \n                \n            if total_step % update_q_freq == 0:\n                Q_ast = copy.deepcopy(Q)\n        \n        # epsilon\n        if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n            epsilon -= epsilon_decrease\n        \n        # next step\n        total_reward += reward\n        pobs = obs\n        step += 1\n        total_step += 1\n        \n    total_rewards.append(total_reward)\n    total_losses.append(total_loss)\n\n    if (epoch+1) % show_log_freq == 0:\n        log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n        log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n        elapsed_time = time.time()-start\n        print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n        start = time.time()","90b01062":"plot_loss_reward(total_losses, total_rewards)","285baf1e":"plot_train_test_by_q(Environment(df_train), Environment(df_test), Q, 'Dueling DDQN', df_train.iloc[-1].name)","f617055f":"# Deep Q-Networks (DQN)","92236975":"# Dueling DDQN","21411169":"# Double Deep Q-Networks (DDQN)","ae28d73d":"<pre>\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2588\u2580\u2580\u2580\u2580\u2580\u2588\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2588\u2580\u2591\u2591\u2584\u2591\u2584\u2591\u2591\u2591\u2591\u2580\u2588\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2580\u2588\u2580\u2580\u2580\u2580\u2584\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2584\u2584\u2584\u2584\u2580\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2588\u2584\u2591\u2580\u2580\u2588\u2580\u2588\u2580\u2591\u2591\u2584\u2588\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2580\u2588\u2584\u2584\u2584\u2584\u2584\u2588\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n<\/pre>\n\n<h1>\nValue-Based Reinforcement Learning <br>\n<\/h1>\nBy Alin Cijov","e57f8014":"# Prepare Data","7bdd62dd":"# Environment","05542709":"# Replay Memory"}}