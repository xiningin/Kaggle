{"cell_type":{"393854b0":"code","d9e3168d":"code","b00f3e30":"code","ea6bb893":"code","64374b37":"code","34401285":"code","cfff7f5e":"code","1508fcf9":"code","98669677":"code","5273701c":"code","7a62f9f1":"code","616ea06c":"code","472c7cf1":"code","b5afcd76":"code","9d1ccd5f":"code","9e67925c":"code","203a07ab":"code","48c9ad36":"code","d7637a57":"code","6ce08883":"code","9d413962":"code","af961723":"code","d9e97820":"code","b77386ae":"code","40f3383f":"code","2e95e000":"code","99247bf9":"code","476d2ab4":"code","9be32928":"code","21307511":"code","a7a24bf5":"code","7c0f5e64":"code","d3849213":"code","93ff41dc":"code","da201535":"code","82449163":"code","be74d9be":"code","232b007e":"code","1f88df5c":"code","1a888244":"code","370d28b5":"code","b760703c":"code","a62fdb62":"code","c97939c1":"code","c9e55396":"code","45b202a2":"code","cd2f5a51":"code","6e0354a6":"code","a8c0a96e":"code","b3f24b75":"code","235489fc":"code","59d7ce66":"markdown","f3320aca":"markdown","c6550748":"markdown","7407ab40":"markdown","d8a0fb6d":"markdown","af2f1dc5":"markdown","30952c9d":"markdown","c0d52493":"markdown","ed5f73f5":"markdown","75a52dbf":"markdown","c6303b19":"markdown","603927fe":"markdown","4cb9edbe":"markdown","a9a0826e":"markdown","59f5591d":"markdown","38b0b640":"markdown","7fff5145":"markdown","602a5c77":"markdown","ec98c0b6":"markdown","0985b786":"markdown","f5e4d5c1":"markdown","989fae8e":"markdown"},"source":{"393854b0":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors","d9e3168d":"# Seaborn advanced settings\n\nsns.set(style='ticks',          # 'ticks', 'darkgrid'\n        palette='colorblind',   # 'colorblind', 'pastel', 'muted', 'bright'\n        #palette=sns.color_palette('Accent'),   # 'Set1', 'Set2', 'Dark2', 'Accent'\n        rc = {\n           'figure.autolayout': True,\n           'figure.figsize': (14, 8),\n           'legend.frameon': True,\n           'patch.linewidth': 2.0,\n           'lines.markersize': 6,\n           'lines.linewidth': 2.0,\n           'font.size': 20,\n           'legend.fontsize': 20,\n           'axes.labelsize': 16,\n           'axes.titlesize': 22,\n           'axes.grid': True,\n           'grid.color': '0.9',\n           'grid.linestyle': '-',\n           'grid.linewidth': 1.0,\n           'xtick.labelsize': 20,\n           'ytick.labelsize': 20,\n           'xtick.major.size': 8,\n           'ytick.major.size': 8,\n           'xtick.major.pad': 10.0,\n           'ytick.major.pad': 10.0,\n           }\n       )\n\nplt.rcParams['image.cmap'] = 'viridis'","b00f3e30":"books = pd.read_csv(\"..\/input\/bookcrossing-dataset\/Books.csv\", sep=\";\")\nusers = pd.read_csv(\"..\/input\/bookcrossing-dataset\/Users.csv\", sep=\";\")\nratings = pd.read_csv(\"..\/input\/bookcrossing-dataset\/Ratings.csv\", sep=\";\")","ea6bb893":"# Additional data cleaning\n# There was probably no check for valid ISBN...\nratings[\"ISBN\"] = ratings[\"ISBN\"].apply(lambda x: x.strip().strip(\"\\'\").strip(\"\\\\\").strip('\\\"').strip(\"\\#\").strip(\"(\"))","64374b37":"books.head()","34401285":"users.head()","cfff7f5e":"ratings.head()","1508fcf9":"# Group by and create basic additional users\nuser_groupby = ratings.groupby(\"User-ID\")\nbook_groupby = ratings.groupby(\"ISBN\")\naverage_user_rating = user_groupby[\"Rating\"].mean()\nnumber_of_ratings_by_user = user_groupby[\"Rating\"].count()\naverage_book_rating = book_groupby[\"Rating\"].mean()\nnumber_of_book_ratings = book_groupby[\"Rating\"].count()\n\naverage_user_rating.name = \"avg_rating\"\nnumber_of_ratings_by_user.name = \"N_ratings\"\naverage_book_rating.name = \"avg_rating\"\nnumber_of_book_ratings.name = \"N_ratings\"","98669677":"# Merge with original dataframes\nusers = users.join(number_of_ratings_by_user, on=\"User-ID\")\nusers = users.join(average_user_rating, on=\"User-ID\")\nbooks = books.join(number_of_book_ratings, on=\"ISBN\")\nbooks = books.join(average_book_rating, on=\"ISBN\")\n\nusers[\"N_ratings\"] = users[\"N_ratings\"].fillna(0)\nbooks[\"N_ratings\"] = books[\"N_ratings\"].fillna(0)\n\nusers[\"N_ratings\"] = users[\"N_ratings\"].astype(\"int64\")\nbooks[\"N_ratings\"] = books[\"N_ratings\"].astype(\"int64\")","5273701c":"print(f\"Out of {users.shape[0]} users only {users['N_ratings'].gt(0).sum(axis=0)} rated at least 1 book.\")\nprint(f\"Only {users['N_ratings'].gt(1).sum(axis=0)} rated at least 2 books.\")\nprint(f\"Only {users['N_ratings'].gt(9).sum(axis=0)} rated at least 10 books.\")\nprint(f\"Most active user rated {users['N_ratings'].max()} books.\")\nprint()\nprint(f\"Out of {books.shape[0]} books only {books['N_ratings'].gt(0).sum(axis=0)} are rated at least once.\")\nprint(f\"Only {books['N_ratings'].gt(1).sum(axis=0)} have at least 2 ratings.\")\nprint(f\"Only {books['N_ratings'].gt(9).sum(axis=0)} have at least 10 ratings.\")\nprint(f\"Most rated book was rated {books['N_ratings'].max()} times.\")","7a62f9f1":"users[users[\"N_ratings\"].gt(0)].describe()","616ea06c":"# Get the most rated book in the dataset\nbooks[books[\"N_ratings\"] == books[\"N_ratings\"].max()]","472c7cf1":"# Get top 20 best rated books in our dataset\nbooks.loc[books[\"N_ratings\"] > 20].sort_values(by=\"avg_rating\", ascending=False).head(20)","b5afcd76":"# Get all Harry Potter books and editions written by Rowling\nbooks[books[\"Title\"].str.contains(\"Harry Potter\") & books[\"Author\"].str.contains(\"Rowling\")]","9d1ccd5f":"ratings[\"Rating\"] = ratings[\"Rating\"].astype(\"int8\")","9e67925c":"pd_matrix = pd.merge(books.loc[books[\"N_ratings\"] > 20, \"ISBN\"], ratings, how=\"left\", left_on=\"ISBN\", right_on=\"ISBN\").drop_duplicates()","203a07ab":"pd_matrix","48c9ad36":"# Reshape so that ISBN is row index, User-ID is column index and values are ratings\npd_matrix = pd_matrix.pivot(index='ISBN', columns='User-ID', values='Rating').fillna(0).astype(\"int8\")","d7637a57":"pd_matrix","6ce08883":"# Change to sparse matrix if we didn't have enough memory\nmatrix = csr_matrix(pd_matrix.values)","9d413962":"# Create a model\nN_predicted_neighbours = 11\nKNN = NearestNeighbors(metric='cosine', n_neighbors=N_predicted_neighbours, n_jobs=-1)","af961723":"# Fit the model\nKNN.fit(matrix)","d9e97820":"# Predict\ndistances, indices = KNN.kneighbors(matrix)\n\n# Note that we do not have to split the data to train, valid and test, as we only need to compute distances between current data","b77386ae":"print(\"Index of first Harry Potter book is:\", np.where(pd_matrix.index==\"059035342X\")[0][0])","40f3383f":"selected_index = 4865","2e95e000":"# Just check it once again\nbooks.loc[books[\"ISBN\"] == pd_matrix.index[selected_index], \"Title\"].values[0]","99247bf9":"# Predictions\n\nprint(f\"Because you liked {books.loc[books['ISBN'] == pd_matrix.index[indices[4865][0]], 'Title'].values[0]} you may like:\")\nprint()\nfor i in range(1, N_predicted_neighbours):\n    print(f\"{books.loc[books['ISBN'] == pd_matrix.index[indices[4865][i]], 'Title'].values[0]} with distance {distances[4865][i]:.3f}.\")","476d2ab4":"def recommend_similar_book(isbn, indices, ratings_matrix, books_table, N_recommendations=1, distances=None):\n    \"\"\"\n    Recommends a book title.\n    \n    Parameters\n    ----------\n    ISBN: str\n        ISBN of a book a user liked\n    indices: np.array\n        indices of ratings_matrix as predicted by KNN\n    ratings_matrix: pd.DataFrame\n        user-book-rating matrix with ratings as values\n    N_recommendations: int (default 1)\n        How many books to recommend?\n    distances: np.array\n        How distant are books from each other by KNN?\n    \"\"\"\n    # TODO: This should be rather split in separate variables, this reads terribly\n    print(f\"Because you liked {books_table.loc[books_table['ISBN'] == ratings_matrix.index[indices[isbn][0]], 'Title'].values[0]} you may like:\")\n    print()\n    for i in range(1, 1+N_recommendations):\n        if distances:\n            print(f\"{books_table.loc[books_table['ISBN'] == ratings_matrix.index[indices[isbn][i]], 'Title'].values[0]} with distance {distances[isbn][i]:.3f}.\")\n        else:\n            print(f\"{books_table.loc[books_table['ISBN'] == ratings_matrix.index[indices[isbn][i]], 'Title'].values[0]}.\")","9be32928":"recommend_similar_book(4865, indices, pd_matrix, books)","21307511":"harry_potter_isbns = books.loc[books[\"Title\"].str.contains(\"Harry Potter\") & books[\"Author\"].str.contains(\"Rowling\"), \"ISBN\"].values\nharry_potter_ratings = ratings.loc[ratings[\"ISBN\"].isin(harry_potter_isbns)]","a7a24bf5":"# Group by and create new features\nuser_groupby = harry_potter_ratings.groupby(\"User-ID\")\naverage_user_rating = user_groupby[\"Rating\"].mean()\nnumber_of_ratings_by_user = user_groupby[\"Rating\"].count()\n\naverage_user_rating.name = \"HP_avg_rating\"\nnumber_of_ratings_by_user.name = \"HP_N_ratings\"","7c0f5e64":"# Merge with the main dataframe\nusers = users.join(number_of_ratings_by_user, on=\"User-ID\")\nusers = users.join(average_user_rating, on=\"User-ID\")\n\nusers[\"N_ratings\"] = users[\"N_ratings\"].fillna(0)\n\nusers[\"N_ratings\"] = users[\"N_ratings\"].astype(\"int64\")","d3849213":"# Get some statistics for those users who have read at least one book from the HP series\nusers.loc[users[\"HP_N_ratings\"].gt(0)].describe()","93ff41dc":"plt.figure()\n# sns.distplot(users.loc[users[\"HP_N_ratings\"].gt(0)][\"HP_avg_rating\"], bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\nbins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) - 0.5\nn, bins, patches = plt.hist(users.loc[users[\"HP_N_ratings\"].gt(0), \"HP_avg_rating\"], bins=bins)\ncm = plt.cm.get_cmap('RdYlGn')\nbin_centers = 0.5 * (bins[:-1] + bins[1:])\n# scale values to interval [0,1]\ncol = bin_centers - min(bin_centers)\ncol \/= max(col)\nfor c, p in zip(col, patches):\n    plt.setp(p, 'facecolor', cm(c))\nplt.title(\"Ratings of Harry Potter books\")\nplt.xlabel(\"Number of stars\")\nplt.ylabel(\"Number of ratings\")\nplt.show()","da201535":"# Get users who read seven books of the series\nusers.loc[users[\"HP_N_ratings\"]==7]\n\n# Note that this selection does not have to mean, they have read all parts of the series, just that they rated seven editions.\n# Nevertheless, as first testing data, it is probably OK.","82449163":"# Transform User-ID to the index in our user-book-rating matrix\nselected_matrix_indices = [pd_matrix.columns.get_loc(user_ID) for user_ID in users.loc[users[\"HP_N_ratings\"]==7].sort_values(by=\"HP_avg_rating\", ascending=False)[\"User-ID\"].values]","be74d9be":"KNN3 = NearestNeighbors(metric='cosine', n_neighbors=8, n_jobs=-1)","232b007e":"KNN3.fit(matrix.T[np.ix_(selected_matrix_indices)])","1f88df5c":"distances3, indices3 = KNN3.kneighbors(matrix.T[np.ix_(selected_matrix_indices)])","1a888244":"ind = np.argsort(indices3, axis=1)","370d28b5":"sorted_distances = np.take_along_axis(distances3, ind, axis=1)","b760703c":"plt.figure()\nax = sns.heatmap(sorted_distances, linewidth=0.5, cmap=\"viridis\")\nplt.title(\"Distances between users; darker = closer\")\nplt.show()","a62fdb62":"# Create a model\nKNN2 = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)","c97939c1":"# Fit\nKNN2.fit(matrix.T)","c9e55396":"# Predict\ndistances2, indices2 = KNN2.kneighbors(matrix.T)","45b202a2":"# Transform User-ID to the index in our user-book-rating matrix\npd_matrix.columns.get_loc(175003)","cd2f5a51":"# Get most similar users\nindices2[34620]","6e0354a6":"# Transform back and get User-ID of nearest neighbor\npd_matrix.columns[50133]","a8c0a96e":"users.loc[users[\"User-ID\"] == 252829]","b3f24b75":"def recommend_favourite_book_of_similar_user(userID, indices, ratings_matrix, users_table, books_table, ratings_table, N_recommendations=1, distances=None):\n    \"\"\"\n    Recommends a book title based on favourite books of ten most similar users.\n    \n    The order of books is following:\n    Take the most similar user, sort his books by rating,\n    exclude everything the current predicted user already read.\n    Output books one by one.\n    If there is only a few books from the most similar user and\n    we run out of books, take next similar user and output\n    his favorite books in a similar fashion.\n    \n    Parameters\n    ----------\n    userID: int\n        ID of a user we want a recommendation for\n    indices: np.array\n        indices of ratings_matrix as predicted by KNN\n    ratings_matrix: pd.DataFrame\n        user-book-rating matrix with ratings as values\n    users_table: pd.DataFrame\n        Information about users\n    books_table: pd.DataFrame\n        Information about books\n    ratings_table: pd.DataFrame\n        Information about ratings\n    N_recommendations: int (default 1)\n        How many books to recommend?\n    distances: np.array\n        How distant are books from each other by KNN?\n    \"\"\"\n    selected_index = ratings_matrix.columns.get_loc(userID)\n    already_read_book_isbns = list(ratings_table.loc[ratings_table[\"User-ID\"] == userID, \"ISBN\"].values)\n    not_read_books = ratings_table.loc[~ratings_table[\"ISBN\"].isin(already_read_book_isbns)]\n    books_to_recommend = list()\n    for i in range(1,10):\n        similar_user_index = indices[selected_index][i]\n        similar_user_ID = ratings_matrix.columns[similar_user_index]\n        possible_to_recommend = not_read_books.loc[not_read_books[\"User-ID\"] == similar_user_ID]\n        possible_to_recommend = possible_to_recommend.sort_values(by=\"Rating\", ascending=False)\n        for a, row in possible_to_recommend.iterrows():\n            books_to_recommend.append(books_table.loc[books[\"ISBN\"] == row[\"ISBN\"], \"Title\"].values[0])\n            if len(books_to_recommend) > N_recommendations-1:\n                break\n        if len(books_to_recommend) > N_recommendations-1:\n            break\n    print(f\"Based on users who like similar books as you, you may like:\")\n    print()\n    for book_name in books_to_recommend:\n        print(book_name)","235489fc":"recommend_favourite_book_of_similar_user(175003, indices2, pd_matrix, users, books, ratings, N_recommendations=3, distances=distances2)","59d7ce66":"# KNN model - recommend similar book\n\nFind the most similar book (using user-recommendation vectors) and recommend it.","f3320aca":"Lets first prepare data for testing a model. For every user add how many Harry Potter books did they read and what is their average rating. Using KNN the distance between users who liked the series should be low, whereas the distance between users who did like it and those who did not like it should be higher.","c6550748":"# Possible models\n\nWith additional features listed above the most transparent model would be lightGBM or a different regression forest as it can output the importance of features. I would guess that a neural network where user and book features are encoded separately would give better results. I would find interesting using a RNN or different NN with memory as the reader's taste changes and it does not make sense to base predictions on books read twenty years ago.","7407ab40":"There are no relevant data about books or users, I will have to use something basic as nearest neighbours, detect the most similar users and recommend books which they read but predicted user did not.","d8a0fb6d":"Awesome! I was recommended other books in the series when I read the first book and liked it. \n\nHowever, there is the problem I noticed earlier, that I am recommended the same book multiple times just because the model recommends editions not books.\n\nLets wrap it up and create a recommender function.","af2f1dc5":"[Free by Paul Vincent](https:\/\/www.goodreads.com\/book\/show\/8580970-free), the book with the highest average rating is almost unknown on Goodreads (with only 24 ratings and average rating 3.29 out of 5). Shel Silverstein's books are poetry for children.\n\nThe proof that the dataset is correlated with real world data is that Harry Potter books, Hobbit, Narnia, Discworld and King's and Gaiman's books place in top twenty books. This is what I would expect if there were much more data as everybody at least heard those names.","30952c9d":"Hmmm, there are people who read more than 7 different editions, so my earlier guess that people probably read max 7 editions (1 per book in series) is not true.\n\nGiven that the average rating is 5 out of 10, there are probably two groups of people, those who really like it and those who do not like it. Lets visualize it!","c0d52493":"# EDA","ed5f73f5":"# Data cleaning\n\n- Data in ISO-8859-1\n    - Detected with chardet (has to be run over bigger sample, after 10000 lines outputs ASCII)\n    - Transformed to UTF-8 using codecs\n- Semicolons used as delimiters but original file contains amp; instead of &\n- Manually fixed few lines with different length\n- Users dataset contains addresses in format town;state;country, but data are inconsistent, users probably wrote it instead from selecting from a list. It is possible to clean it, but it would take some time so only age is in the cleaned dataset.","75a52dbf":"Awesome, those are exactly people we need. There is one hater, who rated every book, but gave very low rating to each one (I wonder if he had actually read the series or he may also did not understand the rating system), there is only one person who read everything and gave 10 starts to each book and finally there are people who liked it and some who did not.","c6303b19":"This obviously works as well as the last user (who did not like Harry Potter) is the furthest from all other users.\n\nThese distances were computed only for eight users, they will be different in reality.","603927fe":"# What would be ideal\n\n## Additional data possible to mine\nGoodreads has an [API](https:\/\/www.goodreads.com\/api) which is possible to use to get more features. Number of ratings and average rating should definitely be more informative feature than from the current dataset. Goodreads also specifies what genre a book is and also allows users to hashtag any book with any hashtag. These hashtag could also serve as additional info about books if they are properly grouped.\n\nGoodreads features:\n- Number of ratings\n- Average rating\n- Number of stars (it is different if a book always get 3 stars from everybody or if it gets fives and ones)\n- How many times was the page with the book visited?\n- Number of pages\n- Average reading time\n- Genre\n- Is part of series? How many books does the series have? What is the number of the book in series?\n- Hashtag groups\n- How many books did the author write?\n- Is the author popular? (number of followers)\n- Number of editions\n- Number of translations \n\n## Additional ideal data\nAsk user what kind of books do they like; ideally have a page with sliders for different kind of books (sci-fi, romance...). Perhaps ask them what kind of book would they want to read next. They could also check a type of recommendations. \n\nIdeal additional features:\n- How much does the user like sci-fi\/romance...","4cb9edbe":"## Harry Potter check","a9a0826e":"# KNN model - recommend similar user's favourite book\n\nFind the most similar user and recommend his top rated books that the current user have not read yet.","59f5591d":"# Types of recommendations\n\n- Recommend similar book to a book a user rated high\n- Recommend a book rated highly by a similar user (or a group of similar users)\n- Recommend a book of a user's highly rated author (if someone likes Pratchett, they will probably like almost all of his books)\n- Recommend the topmost highly rated book from all books that user did not read\n- Recommend a random book (I guess that some people just do not like to decide and this can be used to collect data about book almost nobody reads)","38b0b640":"Our Harry Potter fan's most similar user is probably another Harry Potter fan, so the output is fine.","7fff5145":"With average rating 1 out of 10 (where 0 is minimum), there is probably a reason why we have never heard of Wild Animus by Rich Chapero.","602a5c77":"Well... I didn't expect that there will be so many so negative ratings, but lets continue and find out a few persons who read the whole series and didn't like it (if there are any, else take someone who read at least two books and did not like it) and people who liked it. Then we will have some people to test the next model on.","ec98c0b6":"However, there is a problem. We do not have ratings for books but for editions of books. That means that if we now make a model for recommending books, it will recommend editions instead of books. Without another database linking Edition with ISBN, we can not see, what is really the most favourite book, how many times it was rated etc. (and these seem as important features to me).\n\nMaybe, recommending editions instead of books will not be that bad, as people probably did not read all the editions of Harry Potter and the Sorcerer's Stone. I would guess they read only a single edition of every book in the series, so there will be some distance between users. Plus, if somebody read the spanish edition, it is possible that the model will recommend spanish edition as well. We will see.\n\nNext, just check how many Harry Potter books there are:","0985b786":"The most popular book series ever (info from Goodreads) is Harry Potter. The first book in the series is [the most rated book](https:\/\/www.goodreads.com\/list\/show\/35080.One_Million_Ratings_) (with 6.1 million ratings) and the last book is almost the book with [the best average rating](https:\/\/www.goodreads.com\/list\/show\/10198.Books_With_a_Goodreads_Average_Rating_of_4_5_or_higher_and_With_At_Least_100_Ratings) (4.62 out of 5; you can find better up to 4.75, but not with two millions of ratings).\n\nLets see what are the most popular books and how does Harry Potter stand out.","f5e4d5c1":"Ninety editions of heptalogy. There is also a lot different inputs in Author column like: \"J. K. Rowling\", \"Joanne K. Rowling\", \"J.K.Rowling\", \"Rowling J K\" and we do not have a database connecting all different names of a single author.","989fae8e":"Great, the most similar user to our Harry Potter lover is another user who rated more editions than there is HP books with 10 stars. This shows that the model is working, but the problem with edition still persists. If we recommend a favourite book of the most similar user, specifically for this one, it will probably be another HP edition."}}