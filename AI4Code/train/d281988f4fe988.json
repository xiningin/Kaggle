{"cell_type":{"121ad750":"code","f583391c":"code","456a3354":"code","493383ec":"code","0725c7bd":"code","2c6d8ac5":"code","2917bec4":"code","74a99e31":"code","149ebb52":"code","01283526":"code","34227b12":"code","1c99c68b":"code","8ea22da0":"code","5abcdfe1":"code","ac2973b1":"code","ba05acb1":"code","cdfdbabc":"code","3fb42283":"code","c6b35224":"code","4d50f197":"code","1b8b8715":"code","23009f26":"code","795c133d":"code","2ea1094a":"code","6edba60c":"code","7c521ccb":"code","bb93a428":"code","f157ba1c":"code","37f2eec9":"code","e6b66f68":"code","86c2e4c1":"code","bb594730":"code","ea9520b7":"code","1f9ef1a7":"code","244ea572":"code","9ad05f24":"code","5456453d":"code","36862800":"code","e205fef9":"code","a2d05418":"code","1eddcb6c":"code","b954aaf2":"code","68be4021":"code","b51178f6":"code","305e12dc":"code","69cc6007":"code","4adfce6b":"code","c361ea82":"code","fe59cfc0":"code","b9d62d23":"code","c469ca22":"code","57b66590":"code","2a203947":"code","f9b0dc37":"markdown","b9ed2e8c":"markdown","db82cdce":"markdown","a875ae5c":"markdown","23ca5cc3":"markdown","bf66a8a5":"markdown","5701c203":"markdown","85631d59":"markdown","1cfc0b83":"markdown","05549aa9":"markdown","1854750f":"markdown","21325de5":"markdown","3e6754b6":"markdown","f35c1f06":"markdown","eeac0721":"markdown","6e7c462b":"markdown","8eb770d9":"markdown","2e8ecf77":"markdown","582ddfeb":"markdown","150bae60":"markdown","a57ab055":"markdown","1f0abd95":"markdown","8946ff69":"markdown","b1ad887e":"markdown","ec4d84f0":"markdown","54e1a2f7":"markdown","0da3f04d":"markdown","8a096628":"markdown","73d9f85e":"markdown","ff078f06":"markdown","80a86193":"markdown","3c194674":"markdown","c72d8461":"markdown","4c8cb3e6":"markdown","311c0b7e":"markdown","5b5da4bc":"markdown","6945ed95":"markdown","36b4c83a":"markdown","86d020c3":"markdown","d58abdf3":"markdown","7a8b9f05":"markdown","dbb9f5c9":"markdown","678c5691":"markdown","abf83038":"markdown","454e02e2":"markdown","eab65d28":"markdown","feb0dbf0":"markdown","de7175e4":"markdown","80e9384f":"markdown","288ef03c":"markdown","37c3a290":"markdown","84076177":"markdown","0679eeca":"markdown","1e8772e6":"markdown","98a9a1ec":"markdown","03774ab4":"markdown","96ac11dd":"markdown","247f13c5":"markdown","b8d10387":"markdown","5dded0cd":"markdown","6eaa963b":"markdown","2da7d5b5":"markdown","2cd3f9a0":"markdown","7e3a095c":"markdown","7c937d95":"markdown","5f4deff3":"markdown","c0e0bc63":"markdown","3de770dc":"markdown","0ee3f47d":"markdown","832542f3":"markdown","9233a18f":"markdown","5a080954":"markdown","d1aa8d92":"markdown","580b28f4":"markdown","0a32fa4e":"markdown","fdc0835a":"markdown","c740a102":"markdown","c37f10d8":"markdown","ba81172a":"markdown"},"source":{"121ad750":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To ignore all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","f583391c":"# Load training data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# Load test data\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","456a3354":"# First look at the train data\ntrain_data.sample(5)","493383ec":"train_data.set_index('PassengerId', inplace=True)\ntest_data.set_index('PassengerId', inplace=True)","0725c7bd":"# Check the information of datasets\nprint('*' * 20,'Training data', '*' * 20)\nprint(train_data.info())\nprint('*' * 20,'Test data', '*' * 20)\nprint(test_data.info())","2c6d8ac5":"# Descriptive measures of data\ntrain_data.describe(include='all')","2917bec4":"print(train_data.Cabin.describe())\n\n\nprint('*' * 20, 'Unique values in cabin', '*' * 20)\nprint(train_data['Cabin'].unique())","74a99e31":"train_data.Cabin.isnull().sum()\/len(train_data) * 100","149ebb52":"# Checking the skewness\ntrain_data.skew()","01283526":"# Pclass\nprint('*' * 20, \"Passenger's count by Socio - economic status\", '*' * 20)\nprint(train_data['Pclass'].value_counts())\n\n# Sex\nprint('*' * 20, \"Passenger's count by Gender\", '*' * 20)\nprint(train_data['Sex'].value_counts())\n\n# Embarked\nprint('*' * 20, \"Passenger's count by Port of Embarkation\", '*' * 20)\nprint(train_data['Embarked'].value_counts())","34227b12":"# pclass and sex\nprint('*' * 20, \"Frequency count of passengers by Passenger's class and gender\", '*' * 20)\ntable1=pd.crosstab(train_data['Pclass'],train_data['Sex'],margins=True,margins_name='Sum')\nprint(table1)\n\n# Pclass and Embarked\nprint('*' * 20, \"Frequency count of passengers by Passenger's class and their port of Embarkation\", '*' * 20)\ntable2=pd.crosstab(train_data['Pclass'],train_data['Embarked'],margins=True,margins_name='Sum')\nprint(table2)\n\n# Gender and Embarked\nprint('*' * 20, \"Frequency count of passengers by Passenger's port of Embarkation and gender\", '*' * 20)\ntable3=pd.crosstab(train_data['Sex'],train_data['Embarked'],margins=True,margins_name='Sum')\nprint(table3)","1c99c68b":"# Age\nplt.figure(figsize=(10,6))\nplot = plt.hist(train_data.Age, bins = 8, histtype='bar')   # plot a histogram\nplt.ylabel('Count of values in each bin')\nplt.xlabel('Ranges')\nfor i in range(8):\n    plt.text(plot[1][i],plot[0][i],str(plot[0][i]))   # display the count of values falling in each range","8ea22da0":"# check the skewness of Age\ntrain_data.Age.skew()","5abcdfe1":"# Fare\nplt.figure(figsize=(10,6))\nplot = plt.hist(train_data.Fare, bins = 10, histtype='bar')\nplt.ylabel('Count of values in each bin')\nplt.xlabel('Ranges')\nfor i in range(10):\n    plt.text(plot[1][i],plot[0][i],str(plot[0][i]))   # display the count of values falling in each range","ac2973b1":"# check skewness of Fare varible\ntrain_data.Fare.skew()","ba05acb1":"# Boxplot: to observe outliers\nplt.figure(figsize=(10,6))\nplt.boxplot(train_data.Fare)\nplt.ylabel('Fare')","cdfdbabc":"# SibSp\nsns.countplot(train_data.SibSp)   # counts the frequency of each values","3fb42283":"# Parch\nsns.countplot(train_data.Parch) # counts the frequency of each values","c6b35224":"# Survived\nsns.countplot(train_data.Survived)","4d50f197":"# Sex\nsns.countplot(train_data.Sex)","1b8b8715":"# Pclass\nsns.countplot(train_data.Pclass)","23009f26":"# Boxplot: shows the distribution of quantitative data in a way \n# that facilitates comparisons between variables or across levels of a categorical variable\n\nplt.figure(figsize=(10,6))\nbox_plot = sns.boxplot(x = train_data.Survived, y = train_data.Age, data = train_data)\n\n\n# Adding text in the boxplot like median value, first quartile value and third quartile value\nmedians = train_data.groupby(['Survived'])['Age'].median()\nfirst_quartile = train_data.groupby(['Survived'])['Age'].quantile(0.25)\nthird_quartile = train_data.groupby(['Survived'])['Age'].quantile(0.75)\n\n# Vertical distance from lines to display the particular value\nvertical_offset_median = train_data['Age'].median() * 0.05 \nvertical_offset_fquartile = train_data['Age'].quantile(0.25) * 0.05 \nvertical_offset_tquartile = train_data['Age'].quantile(0.75) * 0.05\nfor xtick in box_plot.get_xticks():\n    \n    # Display text at median (Second quartile)\n    box_plot.text(xtick,medians[xtick] + vertical_offset_median,medians[xtick], \n            horizontalalignment='center',size='medium',color='w',weight='semibold')\n    \n    # Display text at first quartile\n    box_plot.text(xtick,first_quartile[xtick] + vertical_offset_fquartile,first_quartile[xtick], \n            horizontalalignment='center',size='medium',color='w',weight='semibold')\n    \n    # Display text at third quartile\n    box_plot.text(xtick,third_quartile[xtick] + vertical_offset_tquartile,third_quartile[xtick], \n            horizontalalignment='left',size='medium',color='b',weight='semibold')","795c133d":"# Boxplot: shows the distribution of quantitative data in a way \n# that facilitates comparisons between variables or across levels of a categorical variable\n\nplt.figure(figsize=(10,6))\nbox_plot = sns.boxplot(x = train_data.Survived, y = train_data.Fare, data = train_data)\n\n\n# Adding text in the boxplot like median value, first quartile value and third quartile value\nmedians = train_data.groupby(['Survived'])['Fare'].median()\nfirst_quartile = train_data.groupby(['Survived'])['Fare'].quantile(0.25)\nthird_quartile = train_data.groupby(['Survived'])['Fare'].quantile(0.75)\n\n# Vertical distance from lines to display the particular value\nvertical_offset_median = train_data['Fare'].median() * 0.05 \nvertical_offset_fquartile = train_data['Fare'].quantile(0.25) * 0.05 \nvertical_offset_tquartile = train_data['Fare'].quantile(0.75) * 0.05\nfor xtick in box_plot.get_xticks():\n    \n    # Display text at median (Second quartile)\n    box_plot.text(xtick,medians[xtick] + vertical_offset_median,medians[xtick], \n            horizontalalignment='center',size='medium',color='w',weight='semibold')\n    \n    # Display text at third quartile\n    box_plot.text(xtick,third_quartile[xtick] + vertical_offset_tquartile,third_quartile[xtick], \n            horizontalalignment='left',size='medium',color='b',weight='semibold')","2ea1094a":"# Boxplot: shows the distribution of quantitative data in a way \n# that facilitates comparisons between variables or across levels of a categorical variable\n\nplt.figure(figsize=(10,6))\nbox_plot = sns.boxplot(x = train_data.Sex, y = train_data.Age, data = train_data)\n\n\n# Adding text in the boxplot like median value, first quartile value and third quartile value\nmedians = train_data.groupby(['Sex'])['Age'].median()\nfirst_quartile = train_data.groupby(['Sex'])['Age'].quantile(0.25)\nthird_quartile = train_data.groupby(['Sex'])['Age'].quantile(0.75)\n\n# Vertical distance from lines to display the particular value\nvertical_offset_median = train_data['Age'].median() * 0.05 \nvertical_offset_fquartile = train_data['Age'].quantile(0.25) * 0.05 \nvertical_offset_tquartile = train_data['Age'].quantile(0.75) * 0.1\nfor xtick in box_plot.get_xticks():\n    \n    # Display text at median (Second quartile)\n    box_plot.text(xtick,medians[xtick] + vertical_offset_median,medians[xtick], \n            horizontalalignment='center',size='medium',color='w',weight='semibold')\n    \n    # Display text at first quartile\n    box_plot.text(xtick,first_quartile[xtick] + vertical_offset_fquartile,first_quartile[xtick], \n            horizontalalignment='center',size='medium',color='b',weight='semibold')\n    \n    # Display text at third quartile\n    box_plot.text(xtick,third_quartile[xtick] + vertical_offset_tquartile,third_quartile[xtick], \n            horizontalalignment='left',size='medium',color='b',weight='semibold')","6edba60c":"plt.figure(figsize=(10,6))\ncount_plot = sns.countplot(x = train_data.Sex, hue = train_data.Survived)","7c521ccb":"plt.figure(figsize=(10,6))\ncount_plot = sns.countplot(x = train_data.Pclass, hue = train_data.Survived)","bb93a428":"plt.figure(figsize=(10,6))\ncount_plot = sns.countplot(x = train_data.Embarked, hue = train_data.Survived)","f157ba1c":"plt.figure(figsize=(10,6))\ncount_plot = sns.countplot(x = train_data.SibSp, hue = train_data.Survived)","37f2eec9":"plt.figure(figsize=(10,6))\ncount_plot = sns.countplot(x = train_data.Parch, hue = train_data.Survived)\nplt.legend(bbox_to_anchor=(1, 1))","e6b66f68":"plt.figure(figsize=(12,8))\nsns.heatmap(train_data.corr(), annot=True) # Returns correlation among features which have numerical observations","86c2e4c1":"# Function to calculate missing values\ndef calc_missing_values(df):\n    \n    \"\"\"\n    This function will take dataframe as an input and return the missing value information for each features as a dataframe.\n    \"\"\"\n    missing_count = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = round(missing_count \/ len(df) * 100, 2)\n    missing_info = pd.concat([missing_count, missing_percent], axis = 1, keys=['Missing Value Count','Percent of missing values'])\n    return missing_info\n\n\nprint('*' * 20, 'Missing values information of Training data', '*' * 20)\nprint(calc_missing_values(train_data))\n\nprint()\nprint('*' * 20, 'Missing values information of Test data', '*' * 20)\nprint(calc_missing_values(test_data))","bb594730":"print('Mean age of passengers: ', train_data.Age.mean())\nprint('Median age of passengers: ', train_data.Age.median())","ea9520b7":"train_data.Age.fillna(29.6, inplace=True)\ntest_data.Age.fillna(29.6, inplace=True)","1f9ef1a7":"train_data.Embarked.fillna(train_data.Embarked.mode().values[0], inplace=True)","244ea572":"test_data.Fare.fillna(test_data.Fare.median(), inplace=True)","9ad05f24":"print(calc_missing_values(train_data))\nprint(calc_missing_values(test_data))","5456453d":"# Fare column have some outliers as observed during visualization \nplt.figure(figsize=(10,6))\nbox_plot = sns.boxplot(x = train_data.Survived, y = train_data.Fare, data = train_data)","36862800":"train_data[train_data.Fare > 300]","e205fef9":"columns_to_drop = ['Cabin','Name','Ticket']\ntrain_data.drop(columns=columns_to_drop, axis=1, inplace=True)\ntest_data.drop(columns=columns_to_drop, axis=1, inplace=True)","a2d05418":"# Training Data\ntrain_data = pd.get_dummies(train_data, columns=['Sex','Embarked'], drop_first=True)\n\n# Test Data\ntest_data = pd.get_dummies(test_data, columns=['Sex','Embarked'], drop_first=True)","1eddcb6c":"def age_bucketizer(r):\n    if r <= 12:\n        return 0\n    elif r <= 18:\n        return 1\n    elif r <= 59:\n        return 2\n    else:\n        return 3\n    \n# Apply the above function on age column for both train data and test data\ntrain_data['age_class'] = train_data.Age.apply(age_bucketizer)\ntest_data['age_class'] = test_data.Age.apply(age_bucketizer)","b954aaf2":"first_quartile_fare = train_data.Fare.quantile(0.25)\nsecond_quartile_fare = train_data.Fare.quantile(0.5)\nthird_quartile_fare = train_data.Fare.quantile(0.75)\ndef fare_bucketizer(r):\n    if r <= first_quartile_fare:\n        return 0\n    elif r <= second_quartile_fare:\n        return 1\n    elif r <= third_quartile_fare:\n        return 2\n    else:\n        return 3\n    \n# Apply above function to fare column of train data and test data\ntrain_data['fare_class'] = train_data.Fare.apply(fare_bucketizer)\ntest_data['fare_class'] = test_data.Fare.apply(fare_bucketizer)","68be4021":"# drop Age and Fare columns \ntrain_data.drop(columns=['Age', 'Fare'], axis = 1, inplace = True)\ntest_data.drop(columns=['Age', 'Fare'], axis = 1, inplace = True)","b51178f6":"feature_data = train_data.drop(columns=['Survived'])\ntarget = train_data.Survived\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(feature_data, target, test_size = 0.3, random_state=42)","305e12dc":"models = []  # To store all the models\naccuracy = []  # To store the accuracy of respective models","69cc6007":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\n\n# fit the model\nlog_model.fit(X_train, Y_train)\n\n# predict the data\nlog_pred = log_model.predict(X_test)\n\n# store the model in models\nmodels.append('Logistic Regression')","4adfce6b":"from sklearn.metrics import confusion_matrix, accuracy_score\n\nprint(confusion_matrix(Y_test, log_pred))\nprint(round(accuracy_score(Y_test, log_pred),2))\n\n# store the accuracy score\naccuracy.append(round(accuracy_score(Y_test, log_pred),2))","c361ea82":"from sklearn.neighbors import KNeighborsClassifier\n\n# checking different values of k\nfor k in range(1, 15):\n    knn = KNeighborsClassifier(k)\n    knn.fit(X_train, Y_train)\n    print(k)\n    print(knn.score(X_train, Y_train))\n    print(knn.score(X_test, Y_test))","fe59cfc0":"# Fitting model for k = 7\nknn = KNeighborsClassifier(7)\n\n# store the model\nmodels.append('KNN')\n\n# fit the model\nknn.fit(X_train, Y_train)\n\n# predict for X_test\nknn_pred = knn.predict(X_test)\n\nprint(confusion_matrix(Y_test, knn_pred))\nprint(round(accuracy_score(Y_test, knn_pred),2))\n\n# store the accuracy score\naccuracy.append(round(accuracy_score(Y_test, knn_pred),2))","b9d62d23":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n\n# store the model in models\nmodels.append('Decision Tree')\n\n# fit the data\ndtree.fit(X_train, Y_train)\n\n# predict the data\ndtree_pred = dtree.predict(X_test)\n\n\nprint(confusion_matrix(Y_test, dtree_pred))\nprint(round(accuracy_score(Y_test, dtree_pred),2))\n\n# store the accuracy score\naccuracy.append(round(accuracy_score(Y_test, dtree_pred),2))","c469ca22":"model_compare = pd.DataFrame({'models': models, 'accuracy': accuracy})\nmodel_compare","57b66590":"test_data.index","2a203947":"# prediction for test data\ntest_prediction = knn.predict(test_data)\n\n\n# creating dataframe for the predicted value with their passenger id\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data.index,\n        \"Survived\": test_prediction\n    })\n\n# type conversion\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\n# Creating a csv file with test prediction\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","f9b0dc37":"##### Split the training data","b9ed2e8c":"#### Descrete variables","db82cdce":"## 3.3 Data Visualization and Feature Relation\n**Data => Information, Visualization => Pictorial \/ Graphical Representation**\nThe pictorial or graphical representation of data is termed as data visualization.\n\nWhy Data Visualization?\n* For understanding the data clearly\n* To find relationship among different features of data\n* To do comparative analysis\n* The information gained through visual is far beyond any other way.","a875ae5c":"##### Fare\n* As per our visualizations, this column has some outliers. So, we will use median to fill the missing values here.","23ca5cc3":"# 3. Exploratory Data Analysis (EDA)\n**Why EDA:**\n* To analyse and understand the patterns of each of the variables.\n* To check missing values, outliers, etc.\n* Observe the relation between different variables.","bf66a8a5":"# 6 Feature Engineering\n","5701c203":"## Credits\n1. [Dr. Nisha Arora](https:\/\/www.linkedin.com\/in\/drnishaarora\/) for the Statistics.\n2. [Awantik Das](https:\/\/www.linkedin.com\/in\/awantik\/) from [EdYoda](https:\/\/www.edyoda.com\/program\/data-scientist-program) for Machine Learning and Basic Data Wrangling\n3. [Kunal Kirange](https:\/\/www.linkedin.com\/in\/kunal-kirange-295b9465\/) from [EdYoda](https:\/\/www.edyoda.com\/program\/data-scientist-program) for being a wonderful Python Programming tutor.\n4. Some other E - learning platforms like [DPhi](https:\/\/dphi.tech\/step-by-step-process-to-solve-a-data-science-challenge-problem\/), NPTEL, Coursera, GeeksforGeeks, etc.","85631d59":"## 4.2 Outliers Detection and Treating Outliers\n* Outliers are extreme values which occurs on both sides (minimum and maximum)\n* An outlier can be natural or non - natural (artificial)\n* An outlier can be univariate or multivariate.\n* Outliers are subjective; one analyst can take a point as outlier while other analyst may not take the same point as outlier. Some analyst consider outliers beyond 2.7 standard deviation on both positive and negative sides of means while other may consider outliers as 3 standard deviation on both positive and negative sides of mean.\n* In a normal distribution 0.4% are outliers (>2.7 standard deviation) and one in a million is extreme outlier (>4.72 standard deviation).\n* Outliers affects all the machine learning algorithm.\n\n#### Identification of Outliers\n1. Visualizations\n    * Boxplot\n    * Histograms \/ Density plots\n    * Scatter plots\n2. The Mathematical way of finding outliers: Using statistical formula\n    * Lower limit = Q1 - 1.5 * IQR, Upper limit = Q3 + 1.5 * IQR. Observations < Lower limit and observations > Upper limit are considered as outliers. This is the most commonly used value of lower limit and upper limit, analyst also use 2 instead of 1.5.\n    \n#### Understand why there is an outlier?\n* May be data entry error\n* There may be actual extreme point\n    \n#### Handling Outliers\n1. Deleting Outliers: Delete if the dataset is large.\n2. Transforming Values: Use transformation like log(x)\n3. Imputation: Using mean, median or mode.\n4. Flooring or Capping: Setting lower limit (Flooring) or upper limit (Capping). Replacing the values greater than 'x' with x and lower than 'x' with x.","1cfc0b83":"### 3.1 Descriptive Analysis\n* **Statistics is concerned with:** Processing and Analyzing data. Collecting, presenting and transforming data to assist decision makers.\n* **Descriptive measures:** The **statistical** methods used to extract and measure three features, **central tendency, dispersion and skewness** in the data. Also known as **summary measures**. ","05549aa9":"##### The best value of k seems to be 7 as the train score and test score here are higher than other values of k as well as the difference between test score and train score is minimum. ","1854750f":"##### Well, this is my first Kaggle Notebook. If you find anything incorrect, help me to correct that or if you have any suggestions that would improve this notebook then you are welcomed and will be appreciated.","21325de5":"#### Distribution of Continuous Variables","3e6754b6":"### 3.2 Categorical Data Analysis\nCategorical columns \/ variables are:\n* **pclass:** A proxy for socio-economic status (SES); 1st = Upper, 2nd = Middle, 3rd = Lower\n* **Sex:** Gender of the passenger; Male or Female\n* **Embarked:** Port of embarkation; C = Cherbourg, Q = Queenstown, S = Southampton","f35c1f06":"* Passengers with 0, 2, 3, 4, 5, 8 siblings \/ spouses aboard the titanic died more than survived.\n* Passengers with 1 siblings \/ spouses aboard the titanic survived more than died.","eeac0721":"#### Bucketizing Continuous Feature Age.\n* Divide age feature into 4 parts:\n    - age <= 12: Child (0)\n    - age <= 18: Teenage (1)\n    - age <= 59: Adult (2)\n    - age >= 60: Old (3)","6e7c462b":"#### Age and Survived","8eb770d9":"* In each classes of passengers travelling in titanic, male counts were more than female.\n* In each classes of passengers travelling in titanic, passengers from Southampton were maximum in count.\n* From each port of Embarkation, Male passengers were more than female passengers.","2e8ecf77":"* Survived is our target variable. We have to predict whether the passenger survived or not for the test data.\n* Convert PassengerId to index column.","582ddfeb":"### 2.1 Load the Datasets","150bae60":"#### Remove Redundant Features","a57ab055":"* Maximum passengers travelling in titanic were from 3rd class of socio-economic status\n* Maximum passengers travelling in titanic were Male\n* Maximum passengers travelling in titanic were embarked from Southampton (S)","1f0abd95":"* Survived, Pclass and Age are nearly symmetrical in shape.\n* SibSp, Parch and Fare variables are positively skewed (asymmetrical in shape).","8946ff69":"#### Correlation between different features","b1ad887e":"# Step By Step Process to Solve a Data Science Problem\n1. Problem Identification\n2. Get the relavent data\n        2.1 Load the dataset\n3. Exploratory data analysis\n        3.1 Descriptive Analysis\n        3.2 Categorical Data Analysis\n        3.3 Data Visualization and Feature Relation\n4. Data Preprocessing\n        4.1 Data Cleaning and Treating Missing data\n        4.2 Outliers Detection and Treating Outliers\n5. Identify Evaluation Parameters\n        5.1 Accuracy\n6. Feature Engineering\n7. Pre - modeling Work\n8. Model Building\n        8.1 Logistic Regression\n        8.2 K Nearest Neighbours\n        8.3 Decision Tree\n9. Perform Model Comparison","ec4d84f0":"* There are no missing values in both trainig and test data.","54e1a2f7":"# 7 Pre - modelling Work","0da3f04d":"**Training data:**\n    * There are total 891 observations with 11 columns\/variables\/features.\n    * There are both numerical and categorical data.\n    * Age, Cabin and Embarked variables have 714, 204 and 889 non null values respectively. So there are some missing values in these calumns.\n**Test data:**\n    * There are total 418 observations with 10 columns\/variables.features.\n    * The missing variable is Survived as it is the target column that we want to predict.\n    * There are both numerical and categorical data.\n    * Age and Cabin variables have 332 and 91 non null values respectively. So there are missing values in these columns.","8a096628":"##### Age","73d9f85e":"#### Confusion Matrix and Accuracy","ff078f06":"#### Missing Values Interpretation\n* More than 77% of observations are missing in **Cabin** feature of both training data and test data.\n* Approx. 20% of the observations are missing in the **Age** feature of both training and test data.\n* 2 observations are missing in the **Embarked** feature only in training data. No missing value for Embarked feature in test data.\n* 1 observation is missing in the **Fare** feature only in test data.\n\n<hr>\n\n#### Treating Missing values\n1. We will drop **Cabin** column from both test and training dataset.\n2. For **Age** column we will use **Imputation** based on some observations.\n3. **Embarked** is a categorical column so we will use **Mode - Imputation** to fill missing values \n4. As per our data visualizations, there were some outliers in **Fare** column, so we will fill missing value here with the median value.","80a86193":"#### Survived and Parch","3c194674":"#### Skewness\n* Describes how data are distributed\n* It is measures of shape: Symmetrical or Asymmetrical","c72d8461":"* Maximum male passengers had died and most of the female passengers survived. Female passengers were given priority for survival.\n* Gender seems to be an important feature impacting the survival of passengers.","4c8cb3e6":"* Dropping Cabin, Name and Ticket from both training data and test data. Cabin has lot of missing values.\n* Name and Ticket number does not have much impact on the survival of the passengers","311c0b7e":"#### Pclass and Survived","5b5da4bc":"* As per our visualization, there are no outliers in the age variable. So, we will fill the missing values with mean age.","6945ed95":"#### Converting Categorical Data to Numerical\n* There are two categorical features:\n    - Sex: Male or Female\n    - Embarked: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","36b4c83a":"* Cabin has missing values more than 77%. We can think of removing this feature too.","86d020c3":"# 2. Get the relavent data\nThe data is provided here in this challenge. One can read the data description in the data section of this competition page. Link to the data overview: [Titanic Data Description](https:\/\/www.kaggle.com\/c\/titanic\/data) ","d58abdf3":"##### A look at missing values","7a8b9f05":"* Median Fare for those who did not survived was 10.5 and those who survived was 26.5. We can say that the survival of the passenger was dependent on the Fare.\n* There seems to be some outliers greater than 300.","dbb9f5c9":"#### Categorical Variables","678c5691":"* Fare is highly skewed\n* Distribution is asymmetrical\n* There are outliers in Fare variable\n* Will treat outliers later","abf83038":"# 1. Problem Identification\nOne can identify what the problem is about in the description of this competition. Here is the overview:\n\n*The sinking of the Titanic is one of the most infamous shipwrecks in history.*\n\n*On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.*\n\n*While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.*\n\n**In this challenge, we are asked to build a predictive model that answers the question: \"What sort of people were more likely to survive?\" using the passenger data.**\n\nLink to the problem description: [Titanic Challenge Overview](https:\/\/www.kaggle.com\/c\/titanic\/overview)","454e02e2":"#### Two way Frequency Table\nA way to display frequencies or relative frequencies of two categorical variable.","eab65d28":"* Age variable is moderately skewed (-0.5 < skewness < 0.5 are moderately skewed) \n* The distribution of the Age variable is nearly symmetrical.","feb0dbf0":"* Cabin feature has 147 unique observations. Let's have a look at it.","de7175e4":"#### One way Frequency Table\nA way to display frequencies or relative frequencies of one categorical column.","80e9384f":"* Descrete varibles can aslo be considered as categorical variables.\n* Maximum passengers travelling in titanic had no Sibling or Spouse aboard the titanic\n* Maximum passengers travelling in titanic had no parent \/ children aboard the titanic\n* 0 = Not Survived, 1 = Survived, Maximum passengers had died.","288ef03c":"#### Fare and Survived","37c3a290":"#### Interpreting Data Desciption\n* **count** is the total number of non null observations in each column.\n* **mean** is the mean value of all non null ovservations in each column.\n* **std** is the standard deviation (i.e. measure of variation).\n* **min, 25%, 50%, 75% and max** are obtained after sorting each column in ascending order. \n    * **min:** the smallest value\n    * **25%:** the first quartile value\n    * **50%:** the second quartile value or the median\n    * **75%:** the third quartile value\n    * **max:** the largest value\n    \n<hr>\n\n* Name and Ticket features have 891 and 681 unique values. It seems to be difficult to convert these two feature into some useful information. So better we will drop it latter.\n* We can also think of a passenger cannot survive on the basis of his name or ticket number. But if there are any impact of these two variables on the passengers survival, that would be very less and we could easily drop these features.","84076177":"## 8.1 Logistic Regression\n* This model is used to fit the line that best fits the data such that the line separates two classes.\n* This model is used only when the target variable is categorical in Nature. For example, in this case to predict if the passenger survived or not.\n* The line equation is given as: ![image.png](attachment:image.png)\n* The output class is classified using logit\/sigmoid function.\n* Mathematically, sigmoid function is: ![sigmoid.png](attachment:sigmoid.png)\n* Graphically sigmoid function is:![sigmoid%20graph.png](attachment:sigmoid%20graph.png)\n* f(x) value ranges between 0 and 1.\n* Sigmoid function crosses 0.5 at the origin\n* This model calculates: ![CodeCogsEqn%20%2818%29.gif](attachment:CodeCogsEqn%20%2818%29.gif)\n* If Y < 0.5, class 0 is the output else class 1 is the output\n\n##### Important points for Logistic Regression\n* It is a supervised learning algorithm\n* Widely used for classification problems\n* Deosn't require linear relationship between dependent and independent variables\n* Works well for large sample sizes\n* If the value of target variable is ordinal, then it is called as ordinal logistic regression\n* If dependent variable is multi - class then it is known as Multinomial Logistic Regression\n\n##### Logistic Regression Tutorials:\n1. [https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc](https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc)\n2. [https:\/\/www.geeksforgeeks.org\/understanding-logistic-regression\/](https:\/\/www.geeksforgeeks.org\/understanding-logistic-regression\/)","0679eeca":"* Cabin has only 204 non null observations.\n* Out of 204, 147 unique observations are there.\n* Let's see the missing value percentage for this feature.","1e8772e6":"#### Sex and Survived","98a9a1ec":"* Maximum passengers were embarked from Sounthampton.\n* Number of passengers died and survived are more from Southampton than other two port of embarkation.\n* In case of Southampton and Queenstown, more passengers died than survived.\n* Survived passengers count is more than died in case of Cherbourg.","03774ab4":"* Most of the passengers died were from 3rd (lower) class of socio-economic status.\n* Upper class (1st class) passengers survival freqency was more than othe two classes.\n* Pclass seems to be an important feature impacting the survival of passengers.","96ac11dd":"### Thanks for reading this Notebook. If you found this notebook helpful, please upvote this notebook to motivate me to write such more notebooks.","247f13c5":"### Import Libraries for EDA\n1. [NumPy](https:\/\/numpy.org\/): \n    * It stands for Numerical Python. \n    * It is the fundamental package for scientific computing. \n    * It is used for working with arrays. \n    * It has functions for working in domain of linear algebra and matrices.\n2. [Pandas](https:\/\/pandas.pydata.org\/about\/):\n    * Data analysis \/ manipulation tool in Python.\n    * It is used for doing practical, real world data analysis in Python.\n    * It provides high performance, easy - to - use data structures and data analysis tools.\n3. [Matplotlib](https:\/\/matplotlib.org\/):\n    * For creating interactive visualizations.\n    * Built on NumPy arrays.\n4. [Seaborn](https:\/\/seaborn.pydata.org\/):\n    * Data visualization library based on matplotlib.\n    * For informative statistical graphics.","b8d10387":"## 8.3 Decision Tree\n* Decision Tree is a graph that uses a branching method to illustrate every possible outcome of a decision.\n* In decision tree, each internal node tests an attribute, each branch corresponds to attribute value, and each leaf node assigns a decision.\n\n##### Working of the algorithm\n* It starts with the root node.\n* It iterates through the unused attributes and calculates Entropy(H) and Information Gain(IG) of this attribute.\n* It then selects the attribute with highest Information Gain.\n* Then again the set is split by the selected attribute in order to produce subset of data.\n* The algorithm continues to recur on each subset considering unused attributes.\n\n##### Attribute Selection Measures\n* Selecting the root node randomly among all the nodes may give us a bad results with low accuracy.\n* That is why deciding the attribute to be placed at the root node or at different levels of the tree as internal node is a complex task. \n* Some criteria\/measures used for attribute selection are: \n    1. Entropy and Information Gain, \n    2. Gini Index and Gini Gain\n    \n###### Entropy and Information Gain\n**Entropy**\n* Entropy is the measure of the randomness in the information being produced.\n* Entropy can also be roughly thought as how much variance the data has.\n* Higher Entropy makes hard to come to a decision.\n* Mathematically, entropy is: ![image.png](attachment:image.png)\n**Information Gain**\n* It is a decrease in Entropy.\n* It is the difference between entropy before split and average entropy after split\n\n###### Gini Index and Gini Gain\n**Gini Index**\n* A cost function used to evaluate splits in the data set.\n* Calculated by subtracting the sum of the squared of probabilities of each class from 1.\n![gini%20index.PNG](attachment:gini%20index.PNG)\n**Gini Gain**\n* It is again the decrease in gini index.\n* It is the difference between gini index before split and average gini index afer split.","5dded0cd":"* Pclass, SibSp and Age are negatively correlated with survival of passengers\n* Fare and Parch are positively correlated with the survival of passengers\n* Pclass and Fare have relativley high negative correlation","6eaa963b":"#### Survived and Embarked","2da7d5b5":"#### Bucketizing Continuous feature Fare","2cd3f9a0":"## 8.2 K Nearest Neighbours (KNN)\n* KNN is a non parametric method used for classification problems.\n* It is one of the simplest yet very powerful classification algorithm.\n* Here K is nothing but the number of nearest neighbours voting for the the test data.\n* It uses the least distance measure(like Euclidean distance, Manhattan distance, etc) to find the nearest neighbour.\n* It is also said as lazy learning algorithm because there is no learning phase of the model and all the computations are hold off until the classification.\n* Here the function or the test data is predicted locally and so it is also said as an instance based learning algorithm.\n\n#### Working of K Nearest Neighbours\nLet's have a simple case to understand this algorithm. We have some data points of two classes as shown in the figure below. Let two classes be class yellow and class purple. There is a test data point black star.\n![fig1.jpg](attachment:fig1.jpg)\nHere we are trying to find the class of the star. Let's take k = 3. Now constructing a boundary including black star and those 3 data points which are nearest to it:\n![fig2.jpg](attachment:fig2.jpg)\nOut of 3 closest point two belongs to class yellow and confidently we can say that black star also belongs to class yellow as majority vote from the nearest neighbours went for class yellow.\n\n##### Important points about KNN\n* This algorithm is simple and easy to implement. \n* This algorithm can be used when there is non linear decision boundary between classes. \n* It is good to use when there is large amount of data.\n* It is a supervised learning algorithm","7e3a095c":"# Submission","7c937d95":"# 8 Model Building","5f4deff3":"#### Age and Sex","c0e0bc63":"* K Nearest Neighbours is the model with highest accuracy.","3de770dc":"#### Data Types\nLet's first understand different data types. \n**Some facts and statistics that are collected together for reference or analysis are data.**\n\n**Types of Data:**\n* **Qualitative \/ Categorical:** Data is descriptive in nature; describes an attribute that can be observed but not measured\n    * **Nominal:** Unordered (Finite number of possible values)\n            Sex, Embarked are of this type\n    * **Ordinal:** Ordered (Finite number of possible values)\n            Pclass is of this type, it has values 1st class, 2nd class and 3rd class\n    * **Text:** Text in nature, \n            Name is text data in this data set.\n* **Quantitative \/ Numerical:** Data is numeric in nature; describes measure of an attribute\n    * **Descrete:** Finite number of possible values (only integers)(can also be treated as categorical)\n            sibSp and parch are of this type\n    * **Continuous:** Infinite number of possible values (decimals or integers, usually decimals)\n            Age and Fare are of this type","0ee3f47d":"#### Survived and SibSp","832542f3":"* There are three records with id 259, 680 and 738 who have paid very high than other normal people. \n* So we can do two things:\n    - First is to drop these three observations from the data set.\n    - Second is to bucketize the **Fare** feature into 4 buckets. \n* We will bucketize the **Fare** into 4 buckets in the Feature Engineering section.","9233a18f":"# 5. Identify Evaluation Parameters\n* The evaluation metric used in this challenge is accuracy.\n#### Accuracy\n* Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition.\n* That is, the accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined.\n* The formula for quantifying binary accuracy is:\n    - Accuracy = (TP + TN)\/(TP + TN + FP + FN)\n    - where: TP = True positive; FP = False positive; TN = True negative; FN = False negative\n* [https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision#In_binary_classification](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision#In_binary_classification)","5a080954":"![image.png](attachment:image.png)","d1aa8d92":"# The \"Unsinkable\" RMS(Royal Mail Steamer) Titanic Sank","580b28f4":"##### Embarked\n* This is a categorical column.\n* We will fill missing values here with mode (i.e. maximum occuring value).","0a32fa4e":"## 4.1 Missing Values and its Treatment\n* Generally datasets always have some missing values\n* May be done during data collection, or due to some data validation rule.\n\n<hr>\n### Treatment of missing values\n* Filling the missing values with right technique can change our results drastically.\n* Also, there is no fixed rule of filling the missing values.\n* No method is perfect for filling the missing values. We need to use our common sense, our logic, or may need to see what works for that particular data set. \n\n**Ways of filling missing values:**\n1. **Default value:** One can fill the missing value by default value on the basis of one's 1) understanding of variable, 2) context \/ data insight or 3) common sense \/ logic. For example, suppose if we had some missing values in SibSp (i.e. # of siblings\/spouses aboard the titanic), we could think of that the passenger had 0 sibling\/spouses and that is why he might have left that particular field blank.\n\n2. **Deleting:** Suppose in our dataset we have too many missing values in\n    * **Column,** we can drop the column\n    * **Row,** drop the row. Usually we do this for a large enough dataset.\n    \n3. **Mean\/Median\/Mode - Imputation:** We fill missing values by mean or median or mode(i.e. maximum occuring value). Generally we use mean but if there are some outliers, we fill missing values with median. Mode is used to fill missing values for categorical column.","fdc0835a":"# 4. Data Preprocessing\n* Data Preprocessing is a step in Machine Learning to get the data transformed or encoded in such a way that the machine can easily parse it.","c740a102":"# 9. Model Comparison","c37f10d8":"* Median age for both survived passengers and not survived passengers are equal.","ba81172a":"* Passengers with 0 parent \/ children aboard the titanic died more than survived\n* Passengers with 1 parent \/ children aboard the titanic survived more than died"}}