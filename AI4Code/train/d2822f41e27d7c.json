{"cell_type":{"0aa02173":"code","5bbd277d":"markdown","ed144702":"markdown","612aaf25":"markdown","c5e4bf7d":"markdown","b4a3b3d8":"markdown"},"source":{"0aa02173":"# Basic packages\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n# Scikit-learn package\nfrom sklearn.preprocessing import StandardScaler\n\n# Import data\ndataset = pd.read_csv('https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/datasets\/Iris.csv').drop(columns=['Id'])\n\n# See a tabular sample\ndataset.head()\n\n# Get feature distribution\ndataset.describe()\n\n# Generate X and y\nX = dataset.drop(columns=['Species'])\ny = dataset['Species']\n\n# Standardization\nX = StandardScaler().fit_transform(X)\nX = pd.DataFrame(X)\nX = X.rename(columns={0:'SepalLengthCm',1:'SepalWidthCm',2:'PetalLengthCm',3:'PetalWidthCm'})\n\n# Singular Value Decoposition\nu,s,v = np.linalg.svd(X)\n\n# Estimate singular values\nsingular_values = s*s\/(X.shape[0]-1)\n\n# Top k=2 singular values and corresponding eigenvectors\nk = 2\nprint(f\"Top {k} eigen values:\")\nprint(singular_values[:k])\n\nprint(f\"Top {k} eigen vectors:\")\nprint(v.T[:,0])\nprint(v.T[:,1])\n\nprint('Projection matrix is: ')\nprint(v.T[:,:2])\n\n# Data in reduced dimension\nX_reduced = np.matmul(np.array(X),v.T[:,:2])\npd.DataFrame(X_reduced)\n\n# Visualize the samples in reduced space\ndataset_new = pd.concat([pd.DataFrame(X_reduced),pd.DataFrame(dataset['Species'])], axis=1)\ndataset_new = dataset_new.rename(columns={0:\"feature_1\",1:\"feature_2\"})\nax = sns.scatterplot(x=\"feature_1\", y=\"feature_2\", hue=\"Species\", data=dataset_new)\nplt.show()\n\nprint('Plot of principal components estimated using scikit-learn')\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_reduced_scikit = pca.fit_transform(X)\ndataset_new_scikit = pd.concat([pd.DataFrame(X_reduced_scikit),pd.DataFrame(dataset['Species'])], axis=1)\ndataset_new_scikit = dataset_new.rename(columns={0:\"feature_1\",1:\"feature_2\"})\nax = sns.scatterplot(x=\"feature_1\", y=\"feature_2\", hue=\"Species\", data=dataset_new_scikit)\nplt.show()","5bbd277d":"# Data\n\nWe will use a practical hands-on approach to understand the algorithms. Let's get familiar with the simple yet popular [iris dataset](https:\/\/www.kaggle.com\/arshid\/iris-flower-dataset) that we are going to use in our illustrative examples.\n\nThe dataset description says,\n> The Iris dataset was used in R.A. Fisher's classic 1936 paper, \"[The Use of Multiple Measurements in Taxonomic Problems](http:\/\/rcs.chemometrics.ru\/Tutorials\/classification\/Fisher.pdf)\", and can also be found on the [UCI Machine Learning Repository](http:\/\/archive.ics.uci.edu\/ml\/). It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. This dataset became a typical test case for many statistical classification techniques in machine learning such as support vector machines.\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris.png)\n\nThe data set consists of 50 samples from each of three species of Iris: **Iris Setosa, Iris virginica, and Iris versicolor.**. Therefore, each sample will have a ```label``` from one of these three species.\n\nFour ```features``` were measured from each sample: the length and the width of the sepals and petals, in centimeters. These features are: **Sepal Length, Sepal Width, Petal Length, and Petal Width.**\n\nHere is a peek at the data in tabular format:\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-table.png)\n\nStatistical distribution of the features are:\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-dist.png)\n\nNow, we have a general idea of the dataset, and we are ready to use it in our future discussions.","ed144702":"# Principal Component Analysis (PCA)","612aaf25":"Now, consider a classification task: given a vector of 4 feature values (in real numbers) ```x=[SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm]``` we have to predict what is the label ```y```, i.e. what is the species of flower it represents.  \n\nNow, the question that arises is do we need all these 4 features to predict the species of flower, or we can do a \"good\" classification with a reduced number of features, say 2? That's where dimensionality reduction algorithm comes in handy. With a good dimensionality reduction algorithm we can reduce the number of features needed to perform a good classification. It has several benefits, such as:\n\n1. Dropping less important features that do not significantly contribute to the prediction,\n2. With less number of features we can perform predictions faster,\n3. It allows to transform the existing features to new mutually independent features which have better predictive power\n\nOf course, it comes with some disadvantages, such as, less interpretability of the transformed features, and loss of details from the data due to feature ellimination, but in practice in most of the cases, benefits from PCA trumps these disadvantages, and it is (or its variants) are widely used to solve real-world problems. \n\nIt's time to look at a brief fomulation of PCA, and it will follow an example of PCA with our Iris dataset.\n\n\n## Theory\nPCA belongs to the ```unsupervised machine learning algorithms```. In unsupervised algorithms, we try to understand the data without looking at the labels, instead we analyze the distribution of data and recognize patterns, such as finding clusters and orthogonal features. \n\nAs we mentioned before, the goal of PCA is to reduce the number of features by projecting them into a reduced space constructed my mutually orthogonal features (also known as \"principal components\") with a compact represention of the distribution of data.\n\n\nWe can break down the compuation of PCA into several steps:\n\n### **Generation of feature matrix:** \n\nSince PCA is an unsupervised algorithm, we do not use the labels, but only the features. Let's consider $X$ is the feature matrix of size $n$ x $m$, where $n$ is the total number of samples and $m$ is the number of features. In our Iris Dataset example $n=150$ and $m=4$.\n\nTherefore, $X=$ \n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-x.png)\n\n### **[Data standarization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html):** \n\nTo remove the sensitivity of the variance of the feature values, we standardize features by removing the mean and scaling to unit variance as $X_{standadized}=(X-X_{m})\/X_{v}$, where, $X_{standadized}$ is the standardized feature matrix, $X_m$ is the feature wise mean and $X_v$ is feature wise standard deviation.\n\nTherefore, $X_{standadized}=$\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-standard.png)\n\n\n### **[Singular value decomposition](https:\/\/en.wikipedia.org\/wiki\/Singular_value_decomposition) (SVD):** \n\nNow, we need to find the eignevalues and eigenvectors of the dataset. There are different ways of doing it, such as (1) eignevalue decomposition of the covariance matrix of the data, (2) singular value decomposition, etc. All of them essentially give the same result, but considering the computational efficiency of SVD, and also, to remain aligned with our original goal to relate PCA with other related concepts, we will show the application of SVD here. \n\nIf we now perform singular value decomposition of $\\mathbf X$, we obtain a decomposition \n\n$$\\mathbf X = \\mathbf U \\mathbf S \\mathbf V^\\top,$$ \n\nwhere $\\mathbf U$ is a unitary matrix and $\\mathbf S$ is the diagonal matrix of singular values $s_i$. \n\nFrom here one can easily see that $$\\mathbf C = \\mathbf V \\mathbf S \\mathbf U^\\top \\mathbf U \\mathbf S \\mathbf V^\\top \/(n-1) = \\mathbf V \\frac{\\mathbf S^2}{n-1}\\mathbf V^\\top,$$ meaning that right singular vectors $\\mathbf V$ are principal directions and that singular values are related to the eigenvalues of covariance matrix via $\\lambda_i = s_i^2\/(n-1)$. \n\nNow, if we perform singular value decomposition of $X$, we will get:\n\n$V$ is a 4x4 matrix, and represent the eigenvectors:\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-svd-u.png)\n\n$S$ is a 4x150 matrix, and 4 diagonal components are represented by (say, $S_{diag}$):\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-svd-s.png)\n\n$U$ is a 150x150 matrix:\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-svd-v.png)\n\nThe eigenvalues are estimated as: ```[2.93035378, 0.92740362, 0.14834223, 0.02074601]```\n\n\n### **Drop eigenvectors to reduce dimensionality:** \n\nWe sort the eigenvectors by decreasing eigenvalues (or singular values) and choose $k$ eigenvectors with the largest eigenvalues to form a $m$ \u00d7 $k$ dimensional matrix $P$. This matrix $P$ can be called a \"projection matrix\" and it can now be used to sample new points with 4 features into the reduced space with only $k$ dimensions.\n\nIf we use $k=2$; our singular values are: \n\n```[2.93035378, 0.92740362]```\n\nand corresponding eigen vectors are\n\n```[-0.522372, 0.263355, -0.581254, -0.565611]``` and ```[-0.372318,-0.925556,-0.021095,-0.065416]```.\n\nTherefore, $P=$\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-projection.png)\n\n### **Project features to reduced space**\n\nNow, using $P$ we can project our original 4 feature data, $X$ to a reduced 2 dimentional space as $X_{reduced}$:\n\n$$X_{reduced} = X \\times P$$\n\nand $X_{reduced}$=\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris_reduced.png)\n\n\n## Visualize new features\n\nNow, let's visualize the samples in reduced 2 dimensional space and see if PCA has been able to separate different iris species.\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-plot.png)\n\nAs we can see, even after reducing the number of features from 4 to 2, the new features can separate the iris species in the form of separate clusters.\n\n\n## Our analysis can be done just in a few lines using scikit-learn package\n\nFortunately, in future we don't have to do all the above analysis, as ```PCA``` function is available in ```scikit-learn``` python package. After data standardization step we can simply call the ```PCA``` function as follows to get $X_{reduced}$:\n\n```\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X_standardized)\n```\n\nAfter plotting the results we can see that we get the same results as before:\n\n![](https:\/\/raw.githubusercontent.com\/debanga\/depurr\/master\/images\/iris-reduced-scikit.png)\n\n\n# Code","c5e4bf7d":"# <center> A Brief overview of Principal Component Analysis (PCA)\n![](https:\/\/c1.wallpaperflare.com\/preview\/270\/321\/283\/block-chain-data-records-concept-system-communication.jpg)\n\nIn this article, we discuss PCA and its related algorithm Singular Value Decomposition (SVD) with example codes. Looking at PCE and SVD under one single lens of dimensionality reduction will provide us with a global understanding, and therefore, it will make it easier to remember these concepts.\n\nPlease note that there are different variants of PCA (e.g. probabilistic PCA) that can outperform vanilla PCA, that we will discuss in this article, in many occasions. But, my goal is not to explore PCA exhaustively, rather keep the discussion beginner friendly. I can discuss other advance variants in future articles.","b4a3b3d8":"## References\n[1] http:\/\/suruchifialoke.com\/2016-10-13-machine-learning-tutorial-iris-classification\/\n\n[2] https:\/\/towardsdatascience.com\/the-mathematics-behind-principal-component-analysis-fff2d7f4b643\n\n[3] https:\/\/www.kaggle.com\/vipulgandhi\/pca-beginner-s-guide-to-dimensionality-reduction\n\n[4] https:\/\/stats.stackexchange.com\/posts\/134283\n"}}