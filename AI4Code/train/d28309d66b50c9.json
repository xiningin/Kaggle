{"cell_type":{"afaed562":"code","eee26ac6":"code","f3762a81":"code","b1f40124":"code","45d6e565":"code","e941faa3":"code","f73342c4":"code","077623a4":"code","f346ec38":"code","00c497b4":"code","9f2dd073":"code","d74851bd":"code","7ea75baf":"markdown","fdd7658a":"markdown","aac6c979":"markdown","be251632":"markdown","0cd8e6c5":"markdown","d95b988d":"markdown","ff883e30":"markdown"},"source":{"afaed562":"import tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom keras_preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np","eee26ac6":"seed = 42\nimg_size = 100\nnum_classes = 131\nbatch_size = 32\nroot_path = '\/kaggle\/input\/fruits\/fruits-360\/'\n\ntrain_path = root_path + 'Training'\ntest_path = root_path + 'Test'","f3762a81":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1,5,figsize = (20, 20))\n    axes = axes.flatten()\n    \n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n        \n    plt.tight_layout()\n    plt.show()","b1f40124":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=(img_size,img_size),\n    batch_size = 128, # default 32\n    class_mode='categorical' # default 'categorical'\n)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_generator = test_datagen.flow_from_directory(\n    test_path,\n    target_size=(img_size,img_size),\n    batch_size = 128, # default 32\n    class_mode='categorical' # default 'categorical'\n)","45d6e565":"classes = train_generator.class_indices\nprint('img shape: ', train_generator.image_shape)\nprint('num classes: ', train_generator.num_classes)\ntrain_generator.labels[0]","e941faa3":"images_to_show = [train_generator[i][0][0] for i in range(5)]\nplotImages(images_to_show)","f73342c4":"model = keras.Sequential([\n    keras.Input(shape=(img_size, img_size, 3)),\n    # ===============================\n    layers.Conv2D(16, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(2),\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(2),\n    # ===============================\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(num_classes,  activation='softmax')\n])\n\nmodel.summary()","077623a4":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(\n  train_generator,\n  steps_per_epoch=20,  \n  validation_steps=20,\n  validation_data=test_generator,\n  epochs=60,\n  verbose=1\n)","f346ec38":"def plot_metric(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_' + metric])\n    plt.title('Model '+ metric)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(metric)\n    plt.legend([\"train \" + metric, 'val '+ metric])\n    plt.show()\n\nplot_metric(history,\"loss\")\nplot_metric(history,\"accuracy\")","00c497b4":"score = model.evaluate_generator(test_generator,verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","9f2dd073":"test_ds = keras.preprocessing.image_dataset_from_directory(\n    directory = test_path,\n    labels = \"inferred\",\n    image_size=(img_size,img_size),\n    batch_size = batch_size,\n    seed = seed\n)","d74851bd":"def plot_img(image, y, y_prima, i):\n    y_label = find_label_by_index(classes, y)\n    y_prima_label = find_label_by_index(classes, y_prima)\n    ax = plt.subplot(3, 4, i + 1)\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(y_label + '-' + str(y) + ' Y' + '\\n' + y_prima_label + '-' + str(y_prima) + ' Y_H')\n    plt.axis(\"off\")\n    \ndef find_label_by_index(classes, index):\n    for label, value in classes.items():\n        if value == index:\n            return label\n\nplt.figure(figsize=(10, 10))\nfor images, labels in test_ds.take(1):\n    predictions = model.predict(images\/255)\n    for i in range(12):\n        img = images[i].numpy()       \n        y = labels[i].numpy()\n        y_prima = np.argmax(predictions[i])\n        plot_img(img, y, y_prima, i)","7ea75baf":"## Show some examples from the generator","fdd7658a":"### Resources\n- [Image data Preprocessing](https:\/\/keras.io\/api\/preprocessing\/image\/)\n- [How to extract data\/labels back from TensorFlow dataset](https:\/\/stackoverflow.com\/questions\/56226621\/how-to-extract-data-labels-back-from-tensorflow-dataset)\n- [logits and labels must have the same first dimension](https:\/\/stackoverflow.com\/questions\/49161174\/tensorflow-logits-and-labels-must-have-the-same-first-dimension)\n\n#### Guides\n- Keras [Image classification from scratch](https:\/\/keras.io\/examples\/vision\/image_classification_from_scratch\/)\n- TF [Image classification](https:\/\/www.tensorflow.org\/tutorials\/images\/classification#standardize_the_data)\n\n\n","aac6c979":"### Informacion de kaggle del dataset:\n\n- Total number of images: 90483.\n- Training set size: 67692 images (one fruit or vegetable per image).\n- Test set size: 22688 images (one fruit or vegetable per image).\n- Multi-fruits set size: 103 images (more than one fruit (or fruit class) per image)\n- Number of classes: 131 (fruits and vegetables).\n- Image size: 100x100 pixels.\n\nhttps:\/\/www.kaggle.com\/moltean\/fruits","be251632":"## Predict some fruits using the test dataset","0cd8e6c5":"## Define data generators","d95b988d":"## Build Architecture","ff883e30":"- Y   -> The right label\n- Y_H -> The predicted label"}}