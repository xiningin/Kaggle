{"cell_type":{"b49b3f20":"code","e74853ae":"code","56eba0fc":"code","08cf82cb":"code","ffdc5ce5":"code","7944ba4f":"code","a4c0ff0e":"code","84aeda68":"code","2887616b":"code","f4e08138":"code","eda82372":"code","53889cf9":"code","ee3396ec":"code","454a65ea":"code","aa26ab32":"code","3476a0cb":"code","422409bf":"code","d42ec8ba":"code","f41b4d8d":"markdown","edd13834":"markdown","05c33a87":"markdown","bb1f7952":"markdown","8affeb5f":"markdown"},"source":{"b49b3f20":"!pip install ..\/input\/efficientnet\/efficientnet-1.0.0-py3-none-any.whl","e74853ae":"import pandas as pd\nimport tensorflow as tf\nimport cv2\nimport glob\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport os\nimport efficientnet.keras as efn\nfrom keras.layers import *\nfrom keras import Model\nimport matplotlib.pyplot as plt","56eba0fc":"detection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.compat.v1.GraphDef()\n    with tf.io.gfile.GFile('..\/input\/mobilenet-face\/frozen_inference_graph_face.pb', 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')","08cf82cb":"cm = detection_graph.as_default()\ncm.__enter__()","ffdc5ce5":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess=tf.compat.v1.Session(graph=detection_graph, config=config)\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\nboxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')\nscores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')","7944ba4f":"def get_img(images):\n    global boxes,scores,num_detections\n    im_heights,im_widths=[],[]\n    imgs=[]\n    for image in images:\n        (im_height,im_width)=image.shape[:-1]\n        imgs.append(image)\n        im_heights.append(im_height)\n        im_widths.append(im_widths)\n    imgs=np.array(imgs)\n    (boxes, scores_) = sess.run(\n        [boxes_tensor, scores_tensor],\n        feed_dict={image_tensor: imgs})\n    finals=[]\n    for x in range(boxes.shape[0]):\n        scores=scores_[x]\n        max_=np.where(scores==scores.max())[0][0]\n        box=boxes[x][max_]\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                      ymin * im_height, ymax * im_height)\n        left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n        image=imgs[x]\n        finals.append(cv2.cvtColor(cv2.resize(image[max([0,top-40]):bottom+80,max([0,left-40]):right+80],(240,240)),cv2.COLOR_BGR2RGB))\n    return finals\ndef detect_video(video):\n    frame_count=10\n    capture = cv2.VideoCapture(video)\n    v_len = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_idxs = np.linspace(0,v_len,frame_count, endpoint=False, dtype=np.int)\n    imgs=[]\n    i=0\n    for frame_idx in range(int(v_len)):\n        ret = capture.grab()\n        if not ret: \n            print(\"Error grabbing frame %d from movie %s\" % (frame_idx, video))\n        if frame_idx >= frame_idxs[i]:\n            if frame_idx-frame_idxs[i]>20:\n                return None\n            ret, frame = capture.retrieve()\n            if not ret or frame is None:\n                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, video))\n            else:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                imgs.append(frame)\n            i += 1\n            if i >= len(frame_idxs):\n                break\n    imgs=get_img(imgs)\n    if len(imgs)<10:\n        return None\n    return np.hstack(imgs)\n","a4c0ff0e":"os.mkdir('.\/videos\/')\nfor x in tqdm(glob.glob('..\/input\/deepfake-detection-challenge\/test_videos\/*.mp4')):\n    try:\n        filename=x.replace('..\/input\/deepfake-detection-challenge\/test_videos\/','').replace('.mp4','.jpg')\n        a=detect_video(x)\n        if a is None:\n            continue\n        cv2.imwrite('.\/videos\/'+filename,a)\n    except Exception as err:\n        print(err)","84aeda68":"cm.__exit__(None,Exception,'exit')\nsess.close()","2887616b":"bottleneck = efn.EfficientNetB1(weights=None,include_top=False,pooling='avg')\ninp=Input((10,240,240,3))\nx=TimeDistributed(bottleneck)(inp)\nx = LSTM(128)(x)\nx = Dense(64, activation='elu')(x)\nx = Dense(1,activation='sigmoid')(x)","f4e08138":"model=Model(inp,x)\nmodel.load_weights('..\/input\/dfdc-model3\/model.h5')\n\ndef get_birghtness(img):\n    return img\/img.max()\n# %% [code]\ndef process_img(img,flip=False):\n    imgs=[]\n    for x in range(10):\n        if flip:\n            imgs.append(get_birghtness(cv2.flip(img[:,x*240:(x+1)*240,:],1)))\n        else:\n            imgs.append(get_birghtness(img[:,x*240:(x+1)*240,:]))\n    return np.array(imgs)","eda82372":"sample_submission = pd.read_csv(\"..\/input\/deepfake-detection-challenge\/sample_submission.csv\")\ntest_files=glob.glob('.\/videos\/*.jpg')\nsubmission=pd.DataFrame()\nsubmission['filename']=os.listdir(('..\/input\/deepfake-detection-challenge\/test_videos\/'))\nsubmission['label']=0.5\nfilenames=[]\nbatch=[]\nbatch1=[]\npreds=[]","53889cf9":"for x in test_files:\n    img=process_img(cv2.cvtColor(cv2.imread(x),cv2.COLOR_BGR2RGB))\n    if img is None:\n        continue\n    batch.append(img)\n    batch1.append(process_img(cv2.cvtColor(cv2.imread(x),cv2.COLOR_BGR2RGB),True))\n    filenames.append(x.replace('.\/videos\/','').replace('.jpg','.mp4'))\n    if len(batch)==16:\n        preds+=(((0.5*model.predict(np.array(batch))))+((0.5*model.predict(np.array(batch1))))).tolist()\n        batch=[]\n        batch1=[]\nif len(batch)!=0:\n    preds+=(((0.5*model.predict(np.array(batch))))+((0.5*model.predict(np.array(batch1))))).tolist()","ee3396ec":"new_preds=[]\nfor x in preds:\n    new_preds.append(x[0])\nprint(sum(new_preds)\/len(new_preds))","454a65ea":"for x,y in zip(new_preds,filenames):\n    submission.loc[submission['filename']==y,'label']=min([max([0.1,x]),0.9])","aa26ab32":"plt.hist(submission['label'])","3476a0cb":"submission.head()","422409bf":"np.array(submission['label']).mean()","d42ec8ba":"submission.to_csv('submission.csv', index=False)\n!rm -r videos","f41b4d8d":"# Initialize Face Extractor","edd13834":"Thanks for reading. Please upvote if you found it helpful.","05c33a87":"# Initialize Model","bb1f7952":"Here is just an example of how to use mobilenet face extractor for inference, and also lrcn. \n\n\nI didn't make the weights public, because people(including me) don't like high scoring infernece kernel. \n\n\n**BUT I'm also taking request to make the weights public. I'm OK with making the weights public.**","8affeb5f":"# Import Libraries"}}