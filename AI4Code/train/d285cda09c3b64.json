{"cell_type":{"252d027d":"code","4408da71":"code","f02ca836":"code","ce8c9b0e":"code","fff0bd5e":"code","6e53d93b":"code","d3365c45":"code","0ea518d5":"code","bc738d84":"code","1279e5c5":"code","a11ccfa8":"code","5314cde9":"code","c77b659b":"code","218c88e9":"code","1182582f":"code","b1a39723":"code","3147c92e":"code","ec775372":"code","d9df92f0":"code","20572c40":"code","c7988af8":"code","39e0af3f":"code","d9fc72cb":"code","c3f456b9":"code","56b125bd":"code","111dcf96":"code","2458b7cc":"code","f68f1b56":"code","217c02ba":"code","34f22930":"code","03ca4618":"code","bfb56195":"code","0150f62c":"code","02eab1e4":"code","2cde7e79":"code","754a0bde":"code","231ca5a3":"code","862f2634":"code","94485847":"code","d858a76b":"code","09d86a6e":"code","a7e72a29":"code","12feda5c":"code","d268cc76":"code","b9986d06":"code","ade68d2a":"code","ba250614":"code","fc3c4810":"code","3c238a7d":"code","721b2e1f":"code","6efd610f":"code","acec3620":"code","6b72fdfc":"code","22fb7f26":"code","c855f0ea":"code","12c40860":"code","724c90a7":"code","54c7c6cb":"code","2f43b73c":"code","39e9676b":"code","05b3e00d":"code","0a15d49c":"markdown","fb1fb32f":"markdown","82458a02":"markdown","152e2d23":"markdown","38deea4f":"markdown","cdb63e29":"markdown","29fb11a0":"markdown","d993eaf8":"markdown","2e5af8ec":"markdown","23510dd2":"markdown","b20555bf":"markdown","595fa29a":"markdown","99d12949":"markdown","6a9758a5":"markdown","21916d5a":"markdown"},"source":{"252d027d":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\")","4408da71":"train = pd.read_csv('..\/input\/titanic-classification\/train.csv')\ntest =  pd.read_csv('..\/input\/titanic-classification\/test.csv')","f02ca836":"df_train = train.copy()\ndf_train.shape","ce8c9b0e":"df_train.info()","fff0bd5e":"col_drop = [\"PassengerId\", \"Name\", \"Ticket\"]\ndf_train = df_train.drop(col_drop, axis = 1)\ndf_train.head()","6e53d93b":"df_train.isnull().sum()","d3365c45":"df_train[df_train[\"Age\"].isnull()].index","0ea518d5":"df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train.groupby([\"Sex\"])[\"Age\"].transform('mean'))\ndf_train[\"Embarked\"].value_counts()","bc738d84":"df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')\ndf_train[\"Cabin\"] = df_train[\"Cabin\"].fillna(0)\ndf_train[\"Cabin_new\"] = np.where(df_train[\"Cabin\"] == 0, 0,1)\ndf_train.head()","1279e5c5":"df_train = df_train.drop(\"Cabin\", axis = 1)\ndf_train.head()","a11ccfa8":"df_train_new = pd.get_dummies(df_train, columns = [\"Sex\", \"Embarked\"], drop_first = True)\ndf_train_new.head()","5314cde9":"X = df_train_new.iloc[:,1:].values\ny = df_train_new.iloc[:,0].values","c77b659b":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)","218c88e9":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","1182582f":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","b1a39723":"ann.fit(X_train, y_train, batch_size = 32, epochs = 100)","3147c92e":"y_pred_train = ann.predict(X_train)\ny_train_new = y_train.reshape((-1))\ny_pred_train_new = y_pred_train.reshape((-1))\npred_results = pd.DataFrame({'Actuals' : y_train_new, 'Pred_Prob' : y_pred_train_new})\npred_results.head()","ec775372":"num = [float(x)\/10 for x in range(10)]\nfor i in num:\n    pred_results[i] = pred_results.Pred_Prob.map(lambda x:1 if x > i else 0)\npred_results.head(10)","d9df92f0":"# Creating the probability dataframe\n\nfrom sklearn import metrics\n\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(pred_results.Actuals, pred_results[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","20572c40":"# Plotting the accuracy-sensitivity-specificity curve\ncutoff_df.plot.line(x='prob',y=['accuracy','sensi','speci'])\nplt.show()","c7988af8":"pred_results[\"Predicted\"] = pred_results.Pred_Prob.map(lambda x:1 if x > 0.3 else 0)\npred_results.head(10)","39e0af3f":"train_accuracy = metrics.accuracy_score(pred_results.Actuals, pred_results.Predicted)\nprint(\"Train Accuarcy : \", train_accuracy)","d9fc72cb":"# Confusion Matrix\nconfusion_train=metrics.confusion_matrix(pred_results.Actuals,pred_results.Predicted)\nconfusion_train","c3f456b9":"print(\"Train Sensitivity : \",round((confusion_train[1,1]\/float(confusion_train[1,1]+confusion_train[1,0])),2))\nprint(\"Train Specificity : \",round((confusion_train[0,0]\/float(confusion_train[0,0]+confusion_train[0,1])),2))","56b125bd":"y_test_pred = ann.predict(X_test)\ny_test_pred = (y_test_pred > 0.3)","111dcf96":"confusion = confusion_matrix(y_test, y_test_pred)\nprint(confusion)\nprint(\"Test Accuracy : \", accuracy_score(y_test, y_test_pred))","2458b7cc":"TP = confusion[1,1]\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\nTest_Sensitivity = TP\/float(TP+FN)\nTest_Specificity = TN\/float(TN+FP)\nprint(\"Test Sensitivity : \",round(Test_Sensitivity,4))\nprint(\"Test Specificity : \",round(Test_Specificity,4))","f68f1b56":"df_test = test.copy()\ndf_test.head()","217c02ba":"df_test = df_test.drop(col_drop, axis = 1)\ndf_test.isnull().sum()","34f22930":"df_test[\"Age\"] = df_test[\"Age\"].fillna(df_test.groupby([\"Sex\"])[\"Age\"].transform('mean'))\ndf_test.isnull().sum()","03ca4618":"df_test[\"Fare\"] = df_test[\"Fare\"].fillna(df_test[\"Fare\"].mean())\ndf_test[\"Cabin\"] = df_test[\"Cabin\"].fillna(0)\ndf_test[\"Cabin_new\"] = np.where(df_test[\"Cabin\"] == 0, 0,1)\ndf_test = df_test.drop(\"Cabin\", axis = 1)\ndf_test.head()","bfb56195":"df_test_new = pd.get_dummies(df_test, columns = [\"Sex\", \"Embarked\"], drop_first = True)\ndf_test_new.head()","0150f62c":"X_test_ori = df_test_new.iloc[:,:].values\nX_test_model = sc.transform(X_test_ori)","02eab1e4":"y_pred_ori = ann.predict(X_test_model)\ny_pred_ori = (y_pred_ori > 0.5)","2cde7e79":"ann_pred = pd.DataFrame()\nann_pred[\"PassengerId\"] = test[\"PassengerId\"]\nann_pred[\"Survived_bool\"] = y_pred_ori\nann_pred[\"Survived\"] = np.where(ann_pred[\"Survived_bool\"] == False, 0, 1)\nann_pred.head()","754a0bde":"ann_pred[\"Survived\"].value_counts()","231ca5a3":"ann_pred = ann_pred.drop(\"Survived_bool\", axis = 1)\nann_pred.head()","862f2634":"df_train_new.head()","94485847":"X = df_train_new.iloc[:,1:].values\ny = df_train_new.iloc[:,0].values","d858a76b":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)","09d86a6e":"print(\"Before : \", Counter(y_train))\n\nsmt = SMOTE()\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\nprint(\"After : \", Counter(y_train_sm))","a7e72a29":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train_sm)\nX_test = sc.transform(X_test)","12feda5c":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=10, activation='relu'))\nann.add(tf.keras.layers.Dense(units=15, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","d268cc76":"ann.fit(X_train, y_train_sm, batch_size = 32, epochs = 200)","b9986d06":"y_pred_train = ann.predict(X_train)\ny_train_new = y_train_sm.reshape((-1))\ny_pred_train_new = y_pred_train.reshape((-1))\npred_results = pd.DataFrame({'Actuals' : y_train_new, 'Pred_Prob' : y_pred_train_new})\npred_results.head()","ade68d2a":"num = [float(x)\/10 for x in range(10)]\nfor i in num:\n    pred_results[i] = pred_results.Pred_Prob.map(lambda x:1 if x > i else 0)\npred_results.head(10)","ba250614":"cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(pred_results.Actuals, pred_results[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","fc3c4810":"# Plotting the accuracy-sensitivity-specificity curve\ncutoff_df.plot.line(x='prob',y=['accuracy','sensi','speci'])\nplt.show()","3c238a7d":"pred_results[\"Predicted\"] = pred_results.Pred_Prob.map(lambda x:1 if x > 0.5 else 0)\npred_results.head(10)","721b2e1f":"train_accuracy = metrics.accuracy_score(pred_results.Actuals, pred_results.Predicted)\nprint(\"Train Accuarcy : \", train_accuracy)","6efd610f":"# Confusion Matrix\nconfusion_train=metrics.confusion_matrix(pred_results.Actuals,pred_results.Predicted)\nconfusion_train","acec3620":"print(\"Train Sensitivity : \",round((confusion_train[1,1]\/float(confusion_train[1,1]+confusion_train[1,0])),2))\nprint(\"Train Specificity : \",round((confusion_train[0,0]\/float(confusion_train[0,0]+confusion_train[0,1])),2))","6b72fdfc":"y_test_pred = ann.predict(X_test)\ny_test_pred = (y_test_pred > 0.5)","22fb7f26":"confusion = confusion_matrix(y_test, y_test_pred)\nprint(confusion)\nprint(\"Test Accuracy : \", accuracy_score(y_test, y_test_pred))","c855f0ea":"TP = confusion[1,1]\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\nTest_Sensitivity = TP\/float(TP+FN)\nTest_Specificity = TN\/float(TN+FP)\nprint(\"Test Sensitivity : \",round(Test_Sensitivity,4))\nprint(\"Test Specificity : \",round(Test_Specificity,4))","12c40860":"df_test_new.head()","724c90a7":"X_test_ori = df_test_new.iloc[:,:].values\nX_test_model = sc.transform(X_test_ori)","54c7c6cb":"y_pred_ori = ann.predict(X_test_model)\ny_pred_ori = (y_pred_ori > 0.5)","2f43b73c":"ann_pred = pd.DataFrame()\nann_pred[\"PassengerId\"] = test[\"PassengerId\"]\nann_pred[\"Survived_bool\"] = y_pred_ori\nann_pred[\"Survived\"] = np.where(ann_pred[\"Survived_bool\"] == False, 0, 1)\nann_pred.head()","39e9676b":"ann_pred[\"Survived\"].value_counts()","05b3e00d":"ann_pred = ann_pred.drop(\"Survived_bool\", axis = 1)\nann_pred.head()","0a15d49c":"# ANN USING SMOTE","fb1fb32f":"# Data Cleaning","82458a02":"# Model building using Tensorflow","152e2d23":"# Fitting the model on test set","38deea4f":"# Prediction Results","cdb63e29":"# Prediction Results","29fb11a0":"# Exploring the Dataset","d993eaf8":"# Model Building using Tensorflow","2e5af8ec":"# Evaluation Metrics","23510dd2":"# Splitting the dataset into train and test sets","b20555bf":"# Importing the Libraries","595fa29a":"# Splitting the dataset into train and test sets","99d12949":"# Evaluation Metrics","6a9758a5":"# Treating the Imbalance in the dataset","21916d5a":"# Fitting the model on Test Set"}}