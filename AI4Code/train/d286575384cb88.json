{"cell_type":{"b4f64dda":"code","002ebfd7":"code","b925a4cb":"code","40785373":"code","cc980834":"code","935ddd22":"code","a86bc19c":"code","591723e3":"code","6fe381ee":"code","862763a8":"code","5fc702a4":"code","a8d8e007":"code","5b5c4f2d":"code","214a188f":"code","bdf5f14e":"code","7f1263e8":"code","062e3040":"code","36e23e26":"code","bdc46d00":"code","b02eb0de":"code","dc2f0644":"code","37680d1c":"code","a9b10a64":"code","8bc6360c":"code","4d2e83a8":"code","0bf57069":"code","e9859c50":"code","09599e9b":"code","84660d33":"code","42e14492":"code","a0a3617a":"code","fd838316":"code","5bfd5349":"code","dde7b821":"code","1e147a99":"code","50ac7718":"code","b992b827":"code","af9d3017":"code","1d5e43cd":"code","ae8bd02b":"code","1a674622":"code","b06ab74e":"code","bf6ab156":"code","8bd7e1cf":"code","3c1c491e":"code","97bfc417":"markdown","6aa4a4f0":"markdown","8880a6d1":"markdown","117d69f5":"markdown","d7438b3e":"markdown","0157dd86":"markdown","65aa2998":"markdown"},"source":{"b4f64dda":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, gc, random\nimport keras\npd.set_option('max_columns', None)\nfrom scipy import stats\nimport scipy\nimport tensorflow as tf\nplt.style.use('fivethirtyeight')\nfrom tqdm.notebook import tqdm\nfrom sklearn import preprocessing, model_selection\nfrom colorama import Fore, Back, Style\nfrom IPython.display import clear_output\n%matplotlib inline","002ebfd7":"try:\n    import holidays\nexcept:\n    !pip install holidays\n    \nclear_output()","b925a4cb":"#funtion to clean plot\n\n#github: https:\/\/gist.github.com\/Ankitkalauni\/093ddbd4435246ec1826598db4a67e9d\n\ndef setup_plot(w=1,h=1,rows=1,cols=1,X_label='\\n',y_label='\\n',dpi=600, color = None):\n    '''\n    w: width of the figsize parameter of matplotlib.pyplot\n    h: height of the figsize parameter of matplotlib.pyplot\n    rows: numbers of rows in gridspec\n    cols: numbers of cols in gridspec\n    X_label: Label of the x-axis\n    y_label: Label of the y-axis\n    dpi: to set dpi for the figure.dpi\n    color: if None the colors of bar will different. if not None(pass any value) all bars colors will be same.\n    \n    Return axes (only single axes for now, still working on the multiple axes)\n    \n    design by @ankitkalauni\n    '''\n    #setting plot theme\n    plt.rcParams['figure.dpi'] = dpi\n\n    fig = plt.figure(figsize=(w,h), facecolor='#f6f5f5')\n    gs = fig.add_gridspec(rows, cols)\n    gs.update(wspace=1.5, hspace=0.05)\n    \n    PAL = ['#336b87', '#b4b4b4','#dd4124','#009473',\n           '#F1C40F','#DFFF00','#40E0D0','#6495ED',\n           '#FF0000','#00FF00','#0000FF','#FF00FF','#800080']\n    \n    if color:\n        random = np.random.randint(low = 0,high = len(PAL))\n        PAL = [PAL[random] for _ in range(0,120)]\n\n    background_color = \"#f6f5f5\"\n    sns.set_palette(PAL)\n    \n    #making single ax\n    ax_dict = {}\n    for row in range(rows):\n        for col in range(cols):\n            ax_dict[\"ax%s%s\" %(row,col)] = fig.add_subplot(gs[row, col])\n\n    locals().update(ax_dict)\n\n    #setting theme for every ax in local()\n    for row in range(rows):\n        for col in range(cols):\n                    \n            for s in [\"right\", \"top\"]:\n                locals()['ax' + str(row) + str(col)].spines[s].set_visible(False)\n                \n            locals()['ax' + str(row) + str(col)].set_facecolor(background_color)\n\n            locals()['ax' + str(row) + str(col)].set_facecolor(background_color)\n            locals()['ax' + str(row) + str(col)].set_xlabel(X_label,fontsize=3, weight='bold')\n            locals()['ax' + str(row) + str(col)].set_ylabel(y_label,fontsize=3, weight='bold')\n\n            locals()['ax' + str(row) + str(col)].tick_params(labelsize=3, width=0.5, length=1.5)\n            locals()['ax' + str(row) + str(col)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n            locals()['ax' + str(row) + str(col)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n\n    return locals()['ax' + str(row) + str(col)]","40785373":"working_dir = '..\/input\/tabular-playground-series-jan-2022\/'\n\ntrain = pd.read_csv(f'{working_dir}train.csv')\ntest = pd.read_csv(f'{working_dir}test.csv')\n\nsample_submission = pd.read_csv(f'{working_dir}sample_submission.csv')","cc980834":"#knobs\nto_plot = False\nto_train = True\ncyclic = False","935ddd22":"def view_data(df):\n    print(\"DataFrame has shape of: \",df.shape)\n    print('='*60)\n    print('Dataframe Info')\n    display(df.info())\n    print('='*60)\n    print('Dataframe Describe')\n    display(df.describe().T)\n    print('='*60)\n    print('Number of Null Values:', df.isnull().sum().sum())\n    print('='*60)\n    print(f'''\n    number of columns having Int dtypes: {train.select_dtypes('int').shape[-1]}\\n\n    number of columns having Float dtypes: {train.select_dtypes('float').shape[-1]}\\n\n    number of columns having Object dtypes: {train.select_dtypes('object').shape[-1]}\\n\n    ''')\n    \n    print('='*60)\n    display(df.head())","a86bc19c":"if to_plot:\n    display(view_data(train))\n    print(Back.GREEN)\n    print('-'*130)\n    print('= END ='*20)\n    print('-'*130)\n    print(Style.RESET_ALL)\n    view_data(test)","591723e3":"to_drop = ['row_id']\n\ndef get_date(df):\n    df[['year','month', 'day']] = df['date'].str.split('-', expand=True).astype('int32').values #spliting column date\n    \n    df.drop(to_drop, axis=1, inplace=True) #droping useless columns\n    \n    return df\n\ntrain = get_date(train)\ntest = get_date(test)","6fe381ee":"if to_plot:\n    sns.kdeplot(data = train,x='num_sold', hue='country', ax = setup_plot(w=6, h=3, X_label = 'num_sold'), fill=True, alpha=0.6, zorder=5)\n    display(train.groupby(by='country')['num_sold'].sum())\n    plt.title('No. of Sold by Countries', fontsize=8)\n    sns.barplot(data = train,y='num_sold',x='country', ax = setup_plot(w=6, h=3, X_label = 'country'), fill=True, alpha=0.6, zorder=5)","862763a8":"if to_plot:\n    sns.kdeplot(data = train,x='num_sold', hue='product', ax = setup_plot(w=6, h=3, X_label = 'num_sold'), fill=True, alpha=0.6, zorder=5);\n    plt.title('No. of Sold by Products', fontsize=8)\n    sns.barplot(data = train,y='num_sold',x='country', hue='product', ax = setup_plot(w=6, h=3, X_label = 'country'), fill=True, alpha=0.6, zorder=5)\n    plt.legend(loc='best', fontsize='small');","5fc702a4":"if to_plot:\n    sns.kdeplot(data = train,x='num_sold', hue='store', ax = setup_plot(w=6, h=3, X_label = 'num_sold'), fill=True, alpha=0.6, zorder=5)\n    plt.title('No. of Sold by Stores', fontsize=8)\n    sns.barplot(data = train,y='num_sold',x='country', hue='store', ax = setup_plot(w=6, h=3, X_label = 'country'), fill=True, alpha=0.6, zorder=5)\n    plt.legend(loc='best', fontsize='small');","a8d8e007":"if to_plot:    \n    sns.barplot(data = train,y='num_sold',x='product', hue='store', ax = setup_plot(w=6, h=3, X_label = 'product'), fill=True, alpha=0.6, zorder=5)\n    plt.legend(loc='best', fontsize='small');\n    plt.title('Product-wise No. of Sold by Stores', fontsize=8)","5b5c4f2d":"if to_plot:\n    sns.lineplot(data = train,x = 'date', y = 'num_sold', hue = 'country', ax = setup_plot(w=8, h=3,X_label='Date', y_label = 'num_sold'), zorder = 5)\n    plt.legend(loc='best', fontsize='small'); #long time to run\n    plt.tick_params(bottom = False,labelbottom = False)","214a188f":"_train = train.loc[train['product'] == 'Kaggle Sticker'].copy()\n\nif to_plot:\n    sns.histplot(data = _train,x='num_sold', hue='store', ax = setup_plot(w=6, h=3, X_label = 'num_sold'), alpha=0.6, zorder=5)\n    display(_train.groupby(by = 'store')['num_sold'].sum())\n    plt.title('No. of Sold of Kaggle Sticker by Stores', fontsize=8)\n    \ndel _train\ngc.collect()","bdf5f14e":"if not to_train:\n    bruh","7f1263e8":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DateProcessor(BaseEstimator, TransformerMixin):\n    def __init__(self, date_format='%Y-%m-%d', hours_secs=False):\n        self.format = date_format\n        self.columns = None\n        self.time_transformations = [\n            ('day_sin', lambda x: np.sin(2*np.pi*x.dt.day\/31)),\n            ('day_cos', lambda x: np.cos(2*np.pi*x.dt.day\/31)),\n            ('dayofweek_sin', \n                lambda x: np.sin(2*np.pi*x.dt.dayofweek\/6)),\n            ('dayofweek_cos', \n                lambda x: np.cos(2*np.pi*x.dt.dayofweek\/6)),\n            ('month_sin', \n                lambda x: np.sin(2*np.pi*x.dt.month\/12)),\n            ('month_cos', \n                lambda x: np.cos(2*np.pi*x.dt.month\/12)),\n            ('year', \n                lambda x: (x.dt.year - x.dt.year.min()\n                          ) \/ (x.dt.year.max() - x.dt.year.min()))\n        ]\n        if hours_secs:\n            self.time_transformations = [\n                ('hour_sin', \n                lambda x: np.sin(2*np.pi*x.dt.hour\/23)),\n                ('hour_cos', \n                lambda x: np.cos(2*np.pi*x.dt.hour\/23)),\n                ('minute_sin', \n                lambda x: np.sin(2*np.pi*x.dt.minute\/59)),\n                ('minute_cos', \n                lambda x: np.cos(2*np.pi*x.dt.minute\/59))\n            ] + self.time_transformations\n\n    def fit(self, X, y=None, **fit_params):\n        self.columns = self.transform(X.iloc[0:1,:]).columns\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        transformed = list()\n        for col in X.columns:\n            time_column = pd.to_datetime(X[col],\n                              format=self.format)\n            for label, func in self.time_transformations:\n                transformed.append(func(time_column))\n                transformed[-1].name += '_' + label\n        transformed = pd.concat(transformed, axis=1)\n        return transformed\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)","062e3040":"if cyclic:\n    test['num_sold'] = -1\n\n    all_data = pd.concat([train, test], axis=0)\n\n    temp = DateProcessor().fit_transform(all_data[['date']])\n\n    all_data = pd.concat([all_data, temp], axis=1)\n\n    train = all_data.loc[all_data['num_sold'] != -1, :]\n    test = all_data.loc[all_data['num_sold'] == -1, :]\n    \n    test = test.drop(columns = ['num_sold'])","36e23e26":"train.shape, test.shape","bdc46d00":"#thanks to cv13j0 for teaching me this preprocessing for time series data\n\ndef calender(df):\n    '''preprocess the date column and getting more information from\n    that column'''\n    \n    df['date'] = pd.to_datetime(df['date']) #converting to datetime dtype\n#     df.index = df['date']\n    df['quater'] = df['date'].dt.quarter\n    df['day_of_week'] = df['date'].dt.day_of_week\n    df['day_of_year'] = df['date'].dt.day_of_year\n    df['days_in_month'] = df['date'].dt.days_in_month\n    df['is_leap_year'] = df['date'].dt.is_leap_year.astype('int32')\n    df['week'] = df['date'].dt.week\n    df['is_weekend'] = 0 \n    df.loc[(df['day_of_week'] == 6) | (df['day_of_week'] == 5), ['is_weekend']] = 1\n    \n    return df","b02eb0de":"train = calender(train)\ntest = calender(test)\n\nclear_output()","dc2f0644":"if to_plot:\n    display(train)\n    display(test)","37680d1c":"def chutii(df):\n    df = df.reset_index(drop=True)\n    countries = df['country'].unique().tolist()\n    df['is_holiday'] = 0\n    for count in range(0, len(countries)):\n        print('Imputing Holidays for {} Country'.format(countries[count]))\n        temp = df.loc[train['country'] == countries[count]]\n        chutii = holidays.CountryHoliday(countries[count])\n        temp['is_holiday'] = temp['date'].apply(lambda x: np.where(x in chutii, 1, 0))  \n        df.loc[df['country'] == countries[count], 'is_holiday'] = temp['is_holiday']\n            \n    return df","a9b10a64":"train = chutii(train)\ntest = chutii(test)\n\nclear_output()","8bc6360c":"temp1 = np.log(train['num_sold'])\ntemp2 = stats.boxcox(train['num_sold'])[0]\n\nbro = sns.kdeplot(temp1, color = 'red')\nsns.kdeplot(temp2, color = 'blue', ax = bro)\n\nplt.show();","4d2e83a8":"from scipy import stats\ny, lambda_ = stats.boxcox(train['num_sold'].astype('float32'))\ny = pd.Series(y)\n# train.index = train['date']\n# test.index = test['date']\n\nX = train.drop(columns = ['num_sold'])\n","0bf57069":"cat_cols = ['country', 'store', 'product', 'year', 'month', 'day', 'quater', 'day_of_week', 'day_of_year', 'days_in_month', 'week']\nto_keep = ['is_leap_year','is_weekend','is_holiday']","e9859c50":"X.shape, test.shape","09599e9b":"all_data = pd.concat([X, test],axis=0).reset_index(drop=True)\n\nfor col in tqdm(cat_cols):\n    temp = pd.get_dummies(all_data[col], prefix='_'+col, drop_first=True)\n    all_data = pd.concat([all_data, temp], axis=1)\n    \nall_data = all_data.drop(columns = cat_cols)","84660d33":"X = all_data.loc[0:X.shape[0]-1,:]\ntest = all_data.loc[X.shape[0]:,:]","42e14492":"X.index = X['date']\ntest.index = test['date']\n\nX = X.drop(columns = ['date'])\ntest = test.drop(columns = ['date'])","a0a3617a":"X.shape, test.shape","fd838316":"from keras.layers import Dense, BatchNormalization, Input, Concatenate, Multiply, Activation, Dropout, Add\nfrom keras import Model\n\ndef get_model(SEED):\n    tf.keras.backend.clear_session()\n    np.random.seed(SEED)\n    random.seed(SEED)\n    tf.random.set_seed(SEED)\n    \n    input_ = Input(shape = (X.shape[-1],))\n        \n    x = Dense(1024, kernel_initializer='he_normal')(input_)\n    x = Activation('swish')(x)\n    x = Dropout(0.45)(x)\n    \n#     x1 = Dense(150, kernel_initializer='he_normal')(x)\n#     x1 = BatchNormalization()(x1)\n#     x1 = Activation('swish')(x1)\n#     x1 = Dropout(0.55)(x1)\n    \n#     x2 = Dense(71, kernel_initializer='he_normal')(x1)\n#     x2 = BatchNormalization()(x2)\n#     x2 = Activation('swish')(x2)\n#     x2 = Dropout(0.45)(x2)\n    \n#     x3 = Dense(150, kernel_initializer='he_normal')(x2)\n#     x3 = BatchNormalization()(x3)\n#     x3 = Activation('swish')(x3)\n#     x3 = Multiply()([x1, x3])\n#     x3 = Dropout(0.55)(x3)\n\n    \n#     x3 = Dense(71, kernel_initializer='he_normal')(x3)\n#     x3 = BatchNormalization()(x3)\n#     x3 = Multiply()([x2, x3])\n#     x3 = Activation('swish')(x3)\n#     x3 = Dropout(0.45)(x3)\n    \n    x1 = Dense(228, kernel_initializer='he_normal')(input_)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n    x1 = Dropout(0.15)(x1)\n    \n    x2 = Dense(124, kernel_initializer='he_normal')(x1)\n    x2 = BatchNormalization()(x2)\n    x2 = Activation('relu')(x2)\n    x2 = Dropout(0.15)(x2)\n    \n    x3 = Dense(64, kernel_initializer='he_normal')(x2)\n    x3 = BatchNormalization()(x3)\n    x3 = Activation('relu')(x3)\n#     x3 = Dropout(0.15)(x3)\n    \n    \n#     x3 = Dense(21, kernel_initializer='he_normal')(x2)\n# #     x3 = Concatenate()([x3,x2])\n#     x3 = BatchNormalization()(x3)\n#     x3 = Activation('relu')(x3)\n#     x3 = Dropout(0.25)(x3)\n    \n    output_ = Dense(1)(x3)\n    \n    model = Model(inputs=[input_], outputs = [output_])\n    \n    return model","5bfd5349":"# tf.keras.utils.plot_model(get_model(42), show_dtype=True, show_layer_names=True)","dde7b821":"class SMAPE(keras.losses.Loss):\n    def __init__(self,**kwargs):\n        super().__init__(**kwargs)        \n\n    def call(self, y_true, y_pred):\n        num = tf.math.abs(tf.math.subtract(y_true, y_pred))\n        denom = tf.math.add(tf.math.abs(y_true), tf.math.abs(y_pred))\n        denom = tf.math.divide(denom,200.0)\n\n        val = tf.math.divide(num,denom)\n        val = tf.where(denom == 0.0, 0.0, val)\n        \n        return tf.reduce_mean(val)\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config}","1e147a99":"def smape(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","50ac7718":"import optuna\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n\n# create trial function\n# OPTUNA_OPTIMIZATION = True\n\ndef objective(trial, X=X, y=y):\n    cv = TimeSeriesSplit(n_splits=4)\n\n    cv_scores = []\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n        \n        scaler = preprocessing.StandardScaler()\n\n        X_train = pd.DataFrame(columns=X_train.columns, data=scaler.fit_transform(X_train))\n        X_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))\n        \n        params = {\n            'iterations':trial.suggest_int(\"iterations\", 10000, 20000),\n            'objective': trial.suggest_categorical('objective', ['MAE']),\n            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n            'learning_rate' : trial.suggest_uniform('learning_rate',0.001,0.01),\n            'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n            'random_strength': trial.suggest_uniform('random_strength',10,50),\n            'depth': trial.suggest_int('depth',1,15),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n            'verbose': False,\n#             'task_type' : 'GPU',\n#             'devices' : '0'\n        }\n\n        params['grow_policy'] = 'Depthwise'\n        params['iterations'] = 10000\n\n        if params['bootstrap_type'] == 'Bayesian':\n            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n        elif params['bootstrap_type'] == 'Bernoulli':\n            params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n\n        model = CatBoostRegressor(**params, eval_metric = 'MAE')\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_test,y_test)],\n            early_stopping_rounds=500,\n            use_best_model=True\n        )\n\n        # validation prediction\n        y_hat = model.predict(X_test)\n        rmse = smape(y_test, y_hat)\n        cv_scores.append(rmse)\n    \n    del X_train, y_train, X_test, y_test, y_hat\n    gc.collect()\n    return np.mean(cv_scores)","b992b827":"study = optuna.create_study(direction=\"minimize\", study_name=\"Catboost Classifier\")\nstudy.optimize(objective, n_trials=100,timeout=7*60*60)","af9d3017":"study.best_trial.params","1d5e43cd":"break","ae8bd02b":"# %%time\n\n# #X, y, X_test\n# from sklearn.model_selection import StratifiedKFold, KFold, train_test_split, TimeSeriesSplit\n# from sklearn.metrics import roc_curve, auc,roc_auc_score\n# import lightgbm as lgb\n# from xgboost import XGBClassifier\n# from catboost import CatBoostClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.neighbors import KNeighborsClassifier\n# from lightgbm import LGBMClassifier\n# from imblearn.over_sampling import SVMSMOTE as smote\n# import keras\n# from sklearn.ensemble import RandomForestRegressor\n# from tqdm import tqdm\n# from sklearn import preprocessing\n# from sklearn import linear_model\n# # create dictionaries to store predictions\n# oof_pred_tmp = dict()\n# test_pred_tmp = dict()\n# scores_tmp = dict()\n\n# name = 'dnn'\n\n# oof_pred_tmp[name] = list()\n# oof_pred_tmp['y_valid'] = list()\n# test_pred_tmp[name] = list()\n# scores_tmp[name] = list()\n\n\n# # create cv\n# kf = TimeSeriesSplit(n_splits=4)\n\n# for fold, (idx_train, idx_valid) in enumerate(kf.split(X)):\n# #     model = get_model(42)\n    \n# #     callback = tf.keras.callbacks.EarlyStopping(\n# #         monitor='val_loss', patience=20, verbose=0,\n# #         mode='min',restore_best_weights=True)\n# #     reduce_lr = keras.callbacks.ReduceLROnPlateau(\n# #         monitor='val_loss', \n# #         factor=0.2,\n# #         patience=5,\n# #         mode='min'\n# #         )\n    \n# #     model.compile(\n# #         optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n# #         loss=smape,\n# #     )\n    \n#     # create train, validation sets\n#     X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n#     X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n#     ### standardize data\n#     print('scaling')\n#     scaler = preprocessing.StandardScaler()\n\n#     X_train = pd.DataFrame(columns=X_train.columns, data=scaler.fit_transform(X_train))\n#     test_scaled = pd.DataFrame(columns=test.columns, data=scaler.transform(test))\n#     X_valid = pd.DataFrame(columns=X_valid.columns, data=scaler.transform(X_valid))\n        \n# #     print('fitting')\n# #     history = model.fit(X_train,y_train,validation_data=(X_valid, y_valid),\n# #         callbacks=[callback,reduce_lr], epochs=1000,batch_size=2024,\n# #         shuffle=True, verbose=1,validation_batch_size=len(y_valid)\n# #     )\n# #     print('done fitting')\n\n# #     lol = pd.DataFrame(history.history['loss']).plot(label='loss')\n# #     pd.DataFrame(history.history['val_loss']).plot(ax=lol, label='val_loss')\n    \n# #     plt.legend(['loss','val_loss'])\n\n# #     plt.show()\n#     # validation prediction\n#     print('fitting')\n#     model = lgb.LGBMRegressor(metrics='mae',verbose=0)\n    \n#     model.fit(X_train,y_train, eval_metric='mae', eval_set=(X_valid, y_valid), early_stopping_rounds=500)\n    \n#     pred_valid = model.predict(X_valid)    \n    \n#     score = smape(y_valid, pred_valid)\n\n#     scores_tmp[name].append(score)\n#     oof_pred_tmp[name].extend(pred_valid)\n\n#     print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n#     print('--'*20)\n#     # test prediction\n#     y_hat = model.predict(test_scaled)\n\n#     test_pred_tmp[name].append(y_hat)\n    \n    \n#     print('Prediction')\n    \n#     # store y_validation for later use\n#     oof_pred_tmp['y_valid'].extend(pd.DataFrame(y_valid,columns=['y_valid']).values.reshape(-1,1))\n    \n#     gc.collect()\n#     # print overall validation scores\n# # for name, model in models:\n# print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n# print('::'*20)","1a674622":"%%time\n\n#X, y, X_test\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split, TimeSeriesSplit\nfrom sklearn.metrics import roc_curve, auc,roc_auc_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SVMSMOTE as smote\nimport keras\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\n# create dictionaries to store predictions\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nname = 'dnn'\n\noof_pred_tmp[name] = list()\noof_pred_tmp['y_valid'] = list()\ntest_pred_tmp[name] = list()\nscores_tmp[name] = list()\n\n\n# create cv\nkf = TimeSeriesSplit(n_splits=4)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X)):\n    model = get_model(42)\n    \n    callback = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=20, verbose=0,\n        mode='min',restore_best_weights=True)\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.2,\n        patience=5,\n        mode='min'\n        )\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n        loss=SMAPE(),\n    )\n    \n    # create train, validation sets\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    ### standardize data\n    print('scaling')\n    scaler = preprocessing.StandardScaler()\n\n    X_train = pd.DataFrame(columns=X_train.columns, data=scaler.fit_transform(X_train))\n    test_scaled = pd.DataFrame(columns=test.columns, data=scaler.transform(test))\n    X_valid = pd.DataFrame(columns=X_valid.columns, data=scaler.transform(X_valid))\n        \n    print('fitting')\n    history = model.fit(X_train,y_train,validation_data=(X_valid, y_valid),\n        callbacks=[callback,reduce_lr], epochs=1000,batch_size=2024,\n        shuffle=True, verbose=1,validation_batch_size=len(y_valid)\n    )\n    print('done fitting')\n\n    lol = pd.DataFrame(history.history['loss']).plot(label='loss')\n    pd.DataFrame(history.history['val_loss']).plot(ax=lol, label='val_loss')\n    \n    plt.legend(['loss','val_loss'])\n\n    plt.show()\n    # validation prediction\n    pred_valid = model.predict(X_valid)    \n    \n    score = SMAPE()(y_valid, pred_valid)\n\n    scores_tmp[name].append(score)\n    oof_pred_tmp[name].extend(pred_valid)\n\n    print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n    print('--'*20)\n    # test prediction\n    y_hat = model.predict(test_scaled)\n\n    test_pred_tmp[name].append(y_hat)\n    \n    \n    print('Prediction')\n    \n    # store y_validation for later use\n    oof_pred_tmp['y_valid'].extend(pd.DataFrame(y_valid,columns=['y_valid']).values.reshape(-1,1))\n    \n    gc.collect()\n    # print overall validation scores\n# for name, model in models:\nprint(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\nprint('::'*20)","b06ab74e":"# create df with base predictions on test_data\nbase_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n    for name in test_pred_tmp.keys()}\n)\n\n# save csv checkpoint\nbase_test_predictions.to_csv('.\/base_test_predictions.csv', index=False)\n\n# create simple average blend \nbase_test_predictions['simple_avg'] = base_test_predictions.mean(axis=1)\n\n# create submission file with simple blend average\nsimple_blend_submission = sample_submission.copy()\nsimple_blend_submission['num_sold'] = np.exp(base_test_predictions['simple_avg'])\nsimple_blend_submission.to_csv('simple_blend_submission.csv', index=False)\n\n\nfor indx, name in enumerate(oof_pred_tmp.keys()):\n    temp = pd.Series(oof_pred_tmp[name], name=name)\n\n    if indx == 0:\n        df = temp\n    else:\n        df = pd.concat([df,temp], axis=1)\n\ndf.to_csv('oof_predictionseries.csv', index=False)","bf6ab156":"import scipy\nsample = sample_submission.copy()\nsample['num_sold'] = np.floor(scipy.special.inv_boxcox(base_test_predictions['simple_avg'].values, lambda_))\n\nsample.to_csv('simple_sub_floor.csv',index=False)","8bd7e1cf":"temp = sample['num_sold']\ntemp.index = test.index\ntemp2 = scipy.special.inv_boxcox(y, lambda_)\ntemp2.index = X.index\n\ntemp = pd.concat([temp2, temp], axis=0)","3c1c491e":"# temp.loc[temp.index.year < 2019]\n\n# fig = plt.figure(figsize=(18,6))\ntemp.loc[temp.index.year < 2019].plot(figsize=(10,3),linewidth=1, color='red')\ntemp.loc[temp.index.year >= 2019].plot(figsize=(10,3),linewidth=1, color='green')","97bfc417":"\n<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n1. Version Control\n<\/div>\n\n\n\n<br>\n\n<mark>v1<\/mark> -> FAILED\n\n<mark>v2<\/mark> -> Getting Started with my first time series dataset - Data Viz\n\n<mark>v3<\/mark> -> Swish Robustscaler 2 layer DNN, no FE - oof DNN: 64.15303039550781 public: 16.7\n\n<mark>v4<\/mark> -> RELU Standardscaler 3 layer DNN, no FE - oof DNN: 65.89269256591797 public: 15.9\n\n<mark>v5<\/mark> -> Fixed Columns and make spare matrix - 10. public lb \n\n<mark>v6<\/mark> -> Cyclic time measure - 11. public lb","6aa4a4f0":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n2. Findings\n    \n<\/div>\n\n\n<br>\n\n\n\n<mark>Period<\/mark>: 2015 - 2018\n\n<mark>Stores<\/mark>: KaggleRama, KaggleMart\n\n<mark>Countries<\/mark>: Finland, Sweden, Norway \n\n<mark>Products<\/mark>: Kaggle Mug, Kaggle Hat, Kaggle Sticker\n\n___\n\nMost sold is Kaggle Sticker by KaggleRama in every country given. i am lazy","8880a6d1":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n3.EDA\n<\/div>\n<br>","117d69f5":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n4. Preprocessing\n    \n<\/div>","d7438b3e":"# OPTUNA\n\n___\n","0157dd86":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n5. Model Training\n    \n<\/div>","65aa2998":"<h1 style=\"color:#FF4533;font-size:50px;\"><strong>TPS <strong style=\"color:#337CFF\"> JAN 2022<\/strong><\/strong><\/h1>\n\n\n![](https:\/\/res.cloudinary.com\/teepublic\/image\/private\/s--LYq6T5Y0--\/t_Resized%20Artwork\/c_crop,x_10,y_10\/c_fit,w_470\/c_crop,g_north_west,h_626,w_470,x_0,y_0\/g_north_west,u_upload:v1462829017:production:blanks:qe3008lhp5hquxmwp4a0,x_-395,y_-325\/b_rgb:eeeeee\/c_limit,f_auto,h_630,q_90,w_630\/v1521830347\/production\/designs\/2522554_0.jpg)"}}