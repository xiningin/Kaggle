{"cell_type":{"c3e39d2f":"code","34b4b41b":"code","435c3c77":"code","be151d7b":"code","76d42863":"code","bf99d5b3":"code","46348d84":"code","52a6cfa1":"code","fa0a590f":"code","df013409":"code","e4e15f63":"code","a3c1e4c4":"code","f42c36e1":"code","d9918f76":"markdown"},"source":{"c3e39d2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34b4b41b":"# ==================\n# Library\n# ==================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\nfrom sklearn.decomposition import TruncatedSVD\ntqdm.pandas()","435c3c77":"# ==================\n# Constant\n# ==================\nCITY_PATH = '\/kaggle\/input\/ml-study-meetup-osaka\/OsakaWinter_city.csv'","be151d7b":"city = pd.read_csv(CITY_PATH)","76d42863":"city.head()","bf99d5b3":"class BertSequenceVectorizer:\n    def __init__(self):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = 'bert-base-uncased'\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = 128\n\n\n    def vectorize(self, sentence : str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n\n        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n        else:\n            return seq_out[0][0].detach().numpy()","46348d84":"BSV = BertSequenceVectorizer() # \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u307e\u3059\ncity['wiki_description'] = city['wiki_description'].fillna(\"NaN\") # null \u306f\u4ee3\u308f\u308a\u306e\u3082\u306e\u3067\u57cb\u3081\u307e\u3059\ncity['wiki_description_feature'] = city['wiki_description'].progress_apply(lambda x: BSV.vectorize(x))","52a6cfa1":"bert_array = np.zeros([len(city),768])\nfor n,i in enumerate(city['wiki_description_feature']):\n    bert_array[n,:] = i","fa0a590f":"svd = TruncatedSVD(n_components=50)\nX = svd.fit_transform(bert_array)\ndf = pd.DataFrame(X, columns=[f\"wiki_description_bert_svd_{i}\" for i in range(50)])","df013409":"Prefecture\tMunicipality","e4e15f63":"df[\"Prefecture\"] = city[\"Prefecture\"]\ndf[\"Municipality\"] = city[\"Municipality\"]","a3c1e4c4":"df.head()","f42c36e1":"df.to_csv(\"city_wiki_description_bert.csv\",index=False)","d9918f76":"### BERT\u3092\u4f7f\u3063\u3066\u3001text\u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3057\u307e\u3059\nhttps:\/\/huggingface.co\/transformers\/model_doc\/bert.html <br>\n\n\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30b3\u30f3\u30da\u3067\u306f\u30c6\u30ad\u30b9\u30c8\u51e6\u7406\u306b\u3088\u304fBERT\u306f\u4f7f\u308f\u308c\u307e\u3059 <br>\n\u4e0b\u8a18URL\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059<br>\nhttps:\/\/www.guruguru.science\/competitions\/16\/discussions\/fb792c87-6bad-445d-aa34-b4118fc378c1\/\n\nSettings\u3067\u4e0b\u8a18\u8a2d\u5b9a\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\n- Accelerator\u3092GPU\n- Internet\u3092ON"}}