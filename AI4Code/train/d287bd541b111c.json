{"cell_type":{"02fac140":"code","2f15d32b":"code","0b74ab7d":"code","db73f8db":"code","740e0a78":"code","603cb64b":"code","b6f98309":"code","664ae3b9":"code","a482210d":"code","1bac0e06":"code","22e8550c":"code","e0308314":"code","03ce788b":"code","58cf7b62":"code","2a3a4acd":"code","0531fc1a":"code","31c86708":"code","0b7e5f45":"code","bc6db829":"code","73fddb5b":"code","17a4b64f":"code","e77e2261":"code","51237056":"markdown","418773c2":"markdown","c0c18c8e":"markdown","62f66942":"markdown","f3381924":"markdown","5b302ffd":"markdown","143f5081":"markdown","3eb8d68e":"markdown","143610f3":"markdown","2085e5c4":"markdown","6ee69b1c":"markdown","5b3b3638":"markdown","6dc2232f":"markdown","e6fae256":"markdown"},"source":{"02fac140":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nimport time\nimport pandas as pd\nfrom IPython import display","2f15d32b":"def sample_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","0b74ab7d":"item_size = 10\nbatch_size = item_size ** 2\nn_epochs = 10\nimage_width = 32\nlatent_dimension = 2\ninput_shape = [image_width, image_width, 1]","db73f8db":"def preprocess_image(item):\n    image = item[\"image\"]\n    image = tf.cast(image, \"float\")  \/ 255.0\n    image =tf.image.resize(image, (image_width, image_width))\n    return image","740e0a78":"train = tfds.load(\"mnist\", split='train', as_supervised=False).map(preprocess_image).shuffle(1024).batch(batch_size, drop_remainder=True).prefetch(1).repeat(n_epochs)\ntest = tfds.load(\"mnist\", split='test', as_supervised=False).map(preprocess_image).batch(batch_size, drop_remainder=True).prefetch(1)","603cb64b":"for images in train.take(1):\n    sample_images(images, item_size, item_size)","b6f98309":"tf.keras.backend.clear_session()","664ae3b9":"class Sampling(tf.keras.layers.Layer):\n  def call(self, inputs):\n    mu, sigma = inputs\n    batch = tf.shape(mu)[0]\n    dim = tf.shape(mu)[1]\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n    return mu + tf.exp(0.5 * sigma) * epsilon","a482210d":"def get_encoder(latent_dimension, input_shape):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n    batch_norm = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_norm)\n    x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    mu = tf.keras.layers.Dense(latent_dimension, name='mu')(x)\n    sigma = tf.keras.layers.Dense(latent_dimension, name ='sigma')(x)\n    z = Sampling()((mu, sigma))\n    encoder = tf.keras.Model(inputs, outputs=(mu, sigma, z))\n    return encoder, batch_norm.shape","1bac0e06":"encoder, conv_shape = get_encoder(latent_dimension, input_shape)\ntf.keras.utils.plot_model(encoder, show_shapes=True)","22e8550c":"def get_decoder(latent_dimension, conv_shape):\n    inputs = tf.keras.layers.Input(shape=(latent_dimension,))\n    decoder_input_units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n    x = tf.keras.layers.Dense(decoder_input_units, activation = 'relu', name=\"decode_dense1\")(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    outputs = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n    decoder = tf.keras.Model(inputs, outputs)\n    return decoder","e0308314":"decoder = get_decoder(latent_dimension, conv_shape)\ntf.keras.utils.plot_model(decoder, show_shapes=True)","03ce788b":"def kl_divergence(encoder, decoder, mu, sigma):\n  kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))\n  return tf.reduce_mean(kl_loss)","58cf7b62":"def get_vae(encoder, decoder, input_shape):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n    mu, sigma, z = encoder(inputs)\n    reconstructed = decoder(z)\n    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n    loss = kl_divergence(inputs, z, mu, sigma)\n    model.add_loss(loss)\n    return model","2a3a4acd":"vae = get_vae(encoder, decoder, input_shape)\nvae.summary()","0531fc1a":"optimizer = tf.keras.optimizers.Adam()\nloss_metric = tf.keras.metrics.Mean()\nbce_loss = tf.keras.losses.BinaryCrossentropy()","31c86708":"steps_per_epoch = 60000 \/\/ batch_size\nfor step, images in enumerate(train):\n    with tf.GradientTape() as tape:\n        reconstructed = vae(images)\n        flattened_inputs = tf.reshape(images, shape=(-1))\n        flatten_outputs = tf.reshape(reconstructed, shape=(-1))\n        loss = bce_loss(flattened_inputs, flatten_outputs) * (image_width ** 2)\n        loss += sum(vae.losses)\n    grads = tape.gradient(loss, vae.trainable_weights)\n    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n    loss_metric(loss)\n    if step % steps_per_epoch == 0 and step > 0:\n        display.clear_output(wait=False)    \n        images = decoder.predict(tf.random.normal(shape=[item_size ** 2, latent_dimension]))\n        sample_images(images, item_size, item_size)","0b7e5f45":"def plot_label_clusters(encoder, data, labels):\n    z_mean, _, _ = encoder.predict(data)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.show()\ntest_labels = tfds.load(\"mnist\", split='test', as_supervised=True).map(lambda image, label: label).batch(batch_size, drop_remainder=True).prefetch(1)\ntest_labels = np.array([label for label in test_labels]).reshape(-1)\nplot_label_clusters(encoder, test, test_labels)","bc6db829":"mse = tf.keras.metrics.MeanSquaredError()\nmae = tf.keras.metrics.MeanAbsoluteError()\ntest_metrics = {\"mse\": [], \"mae\": []}\nfor images in test:\n    gen_images = vae.predict(images)\n    test_metrics[\"mse\"].append(mse(images, gen_images).numpy())\n    test_metrics[\"mae\"].append(mae(images, gen_images).numpy())\nprint(\"MSE: \",np.mean(test_metrics[\"mse\"]))\nprint(\"MAE: \",np.mean(test_metrics[\"mae\"]))","73fddb5b":"encoder.save(\"encoder.h5\")","17a4b64f":"decoder.save(\"decoder.h5\")","e77e2261":"vae.save(\"vae.h5\")","51237056":"## Save the Model","418773c2":"## Latent Space Visualization","c0c18c8e":"## Model Development","62f66942":"## Import Datasets","f3381924":"# Digits Generation with VAE\n## Import Packages","5b302ffd":"### Build the Encoder","143f5081":"### Build the VAE Model","3eb8d68e":"## Utilities","143610f3":"### Build the Decoder","2085e5c4":"### Sampling Class","6ee69b1c":"## Evaluation","5b3b3638":"## Train the Model","6dc2232f":"### KL Divergence","e6fae256":"Let's what the images looks like."}}