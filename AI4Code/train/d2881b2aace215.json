{"cell_type":{"c9cc74dc":"code","bf0f7a5a":"code","d787dea7":"code","c03e7cbb":"code","b4ca41e5":"code","f4c3c4a4":"code","4ce94790":"code","40b0ea11":"code","9b04b9e8":"code","3eb22fcc":"code","c6d9c762":"code","030530bd":"code","fb627fa2":"code","b76e2f01":"code","fb070813":"markdown","e786df41":"markdown","23867804":"markdown","fbe9b293":"markdown","ea9a7685":"markdown","72fbfe19":"markdown","96f0d974":"markdown","c7b78c0b":"markdown","b3a42837":"markdown","09096202":"markdown","266e4a97":"markdown","7513b5eb":"markdown"},"source":{"c9cc74dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf0f7a5a":"data_gender=pd.read_csv('..\/input\/voicegender\/voice.csv')\ndata_gender.info()","d787dea7":"data_gender.head()","c03e7cbb":"data_gender['label']=[1 if each == \"female\" else 0 for each in data_gender[\"label\"]]","b4ca41e5":"#Dependent Values:\ny=data_gender.label.values\n#Independent Values:\nx_data=data_gender.drop([\"label\"],axis=1)\n# x_data Normalization ---->>> (x-min(x))\/(max(x)-min(x))\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","f4c3c4a4":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\n\"\"\"print(\"x_train:\",x_train.shape)\nprint(\"x_test:\",x_test.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"y_test:\",y_test.shape)\"\"\"","4ce94790":"def initalize_weight_and_bias(dimension):\n    w=np.full((dimension,1),0.01) \n    b=0.0\n    return w,b","40b0ea11":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","9b04b9e8":"def forward_back_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients={\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    \n    return cost,gradients","3eb22fcc":"def update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n\n    for i in range(number_of_iteration):\n        cost,gradients=forward_back_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        w=w-learning_rate*gradients[\"derivative_weight\"]\n        b=b-learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0: #her 10 ad\u0131mda costu g\u00f6rmek istiyoruz\n            cost_list2.append(cost)\n            index.append(i)\n            #print(\"Cost after iteration %i : %f\" %(i,cost))\n            \n    parameters={\"weight\": w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","c6d9c762":"def predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            y_prediction[0,i]=0\n        else: \n            y_prediction[0,i]=1\n        \n    return y_prediction","030530bd":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension=x_train.shape[0]\n    w,b=initalize_weight_and_bias(dimension)\n    parameters,gradients,cost_list=update(w, b, x_train, y_train, learning_rate, num_iterations)\n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"Test Accuracy: {0} %\".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))","fb627fa2":"#Let's Try the \"logistic_regression\" Function we created\nlogistic_regression(x_train,y_train,x_test,y_test,learning_rate=2,num_iterations=1000)","b76e2f01":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Test Accuracy: {} %\".format(lr.score(x_test.T,y_test.T)*100))","fb070813":"**Dependent Values and Independt Values:**","e786df41":"**Sigmoid Function**\n\n\n\nA sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:\n![sigmoid%20formula.PNG](attachment:sigmoid%20formula.PNG)\n\n\n**Graph of the Sigmoid Function:**\n![sigmoid%20graph.PNG](attachment:sigmoid%20graph.PNG)","23867804":"**PREDICT FUNCTION**","fbe9b293":"# First, Let's Apply Logistic Regression with Our Own Algorithm","ea9a7685":"**Let's change the label features of the data set to 0 for male and 1 for female.**","72fbfe19":"**Train Test Split**","96f0d974":"**Forward and Back Propagation**\n\n\n![PROPAGATION.png](attachment:PROPAGATION.png)","c7b78c0b":"**LOGISTIC REGRESSION FUNCTION**","b3a42837":"**Updating (learning) Parameters**\n\n![gd.PNG](attachment:gd.PNG)","09096202":"# Let's Apply Logistic Regression Using Sklearn Library","266e4a97":"# INTRODUCTION\n* **Logistic Regression:**\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value).\n\n(Wikipedia)","7513b5eb":"**Initialize**"}}