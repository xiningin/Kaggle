{"cell_type":{"5b60af37":"code","07d1f7dd":"code","f3346514":"code","c23de10e":"code","850158bf":"markdown","4a366580":"markdown","93ff689e":"markdown","d4beac75":"markdown","7063748f":"markdown","f0da5831":"markdown"},"source":{"5b60af37":"import numpy as np\nfrom sklearn import metrics ","07d1f7dd":"n = 8 # number of 'training examples'\n\n# create some dummy data \ny_pred = np.zeros(n*10).reshape((n, 10))\ny_true = np.zeros(n*10).reshape((n, 10))\ny_pred[:4] = [1,0,0,0,0,0,0,0,0,0] # (play with it to see the effects!)\ny_true[:] = [1,1,0,0,0,0,0,0,0,0] # (play with it to see the effects!)","f3346514":"print('Micro F1:',metrics.f1_score(y_true, y_pred, average='micro'))\nprint('Macro F1:',metrics.f1_score(y_true, y_pred, average='macro')) ","c23de10e":"# Let's recreate the functions and have a closer look:\n\ndef f1_micro(y_true, y_preds, thresh=0.5, eps=1e-20):\n    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n    truepos = preds_bin * y_true\n    \n    p = truepos.sum() \/ (preds_bin.sum() + eps) # take sums and calculate precision on scalars\n    r = truepos.sum() \/ (y_true.sum() + eps) # take sums and calculate recall on scalars\n    \n    f1 = 2*p*r \/ (p+r+eps) # we calculate f1 on scalars\n    return f1\n\ndef f1_macro(y_true, y_preds, thresh=0.5, eps=1e-20):\n    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n    truepos = preds_bin * y_true\n\n    p = truepos.sum(axis=0) \/ (preds_bin.sum(axis=0) + eps) # sum along axis=0 (classes)\n                                                            # and calculate precision array\n    r = truepos.sum(axis=0) \/ (y_true.sum(axis=0) + eps)    # sum along axis=0 (classes) \n                                                            #  and calculate recall array\n\n    f1 = 2*p*r \/ (p+r+eps) # we calculate f1 on arrays\n    return np.mean(f1) # we take the average of the individual f1 scores at the very end!\n\nprint('Micro F1 (sklearn):',metrics.f1_score(y_true, y_pred, average='micro'))\nprint('Micro F1 (own)    :',f1_micro(y_true, y_pred))\nprint('Macro F1 (sklearn):',metrics.f1_score(y_true, y_pred, average='macro')) \nprint('Macro F1 (own)    :',f1_macro(y_true, y_pred))","850158bf":"## So what's the difference?","4a366580":"Obviously, those functions can be combined into one , use `axis=None` to generate micro, calculate the mean always. They were separated for educational purposes only.\n\nOf course, this doesn't help you get a better score, but it should help you iterate faster, when your score actually reflects the leaderboard better. ;-)\nGood Luck!","93ff689e":"The difference is mainly in when\/where the averages are taken, and that makes a huge difference:\n- Micro-F1 basically adds up all the metrics (true positives, ...) accross classes and calculates f1 on the averages. Most accuracy scores do the same.\n- Macro-F1 first aggregates the classes in themselves (columns) before calculating the scores for each, then calculates the average\n","d4beac75":"## Show me some code!","7063748f":"## Why is my leaderboard score so low, my accuracy \/ F1 is so much better ?","f0da5831":"This seems to be a (beginner?) question here in the kernels\/discussion and I had the same issue.\nTurns out it is quite simple:\n\n - your model sucks and you use the wrong metric! ;-) (applies to me too) \n - The evaluation metric for this competition (and therefore the Leaderboard) is **Macro**-F1\n - Some (most?) libraries default to **Micro**-F1 and simple accuracy scores behave similarily\n - Micro-F1 gives you much better score in this competition than Macro-F1, which is why your \"local\" score is better than the leaderboard\n\nUPDATE: If you are sure you are already using the correct metric and still have low LB scores, do check out this discussion:\nhttps:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification\/discussion\/69366\nIt basically points out the fact, that the **order of the image-ids in the submission file has to be identical with the sample_submission.csv**. So reorder your results accordingly!"}}