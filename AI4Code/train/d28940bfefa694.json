{"cell_type":{"a986bbf3":"code","2e8e7a73":"code","a21fec5f":"code","2ca7d4f9":"code","289ed71c":"code","21eb5dbb":"code","9e0fa783":"code","e30116cd":"code","54b108ab":"code","698dd998":"code","e8fb4f34":"code","1e70bcda":"code","faba9992":"code","0d371c81":"code","fe14b0bf":"code","1cbc4d3b":"code","b6e2a37c":"code","fc78fe5b":"code","b9886677":"code","adb15186":"code","34324bcf":"code","2997a7f2":"code","d2de79f6":"code","bc4d72f4":"code","e0d37eac":"code","a9899874":"markdown","0a52c565":"markdown","bf3fc05b":"markdown","b3805acc":"markdown","269554d2":"markdown","d649cbba":"markdown","e8b13e73":"markdown","3ecb1c94":"markdown","b192f05c":"markdown","7055f2a0":"markdown","75fc3d8f":"markdown","6a72df15":"markdown","7c03a4e0":"markdown","60a2fc0a":"markdown","0a299b0c":"markdown","6a3a90cf":"markdown","672ec15c":"markdown","f33cea88":"markdown","b2c28cb1":"markdown","8d0b61e7":"markdown","7fed000b":"markdown","df83c5b7":"markdown"},"source":{"a986bbf3":"import os  \nimport time\nimport numpy as np    #work with numpy arrays\nimport pandas as pd    #working with dataframes\nimport seaborn as sns  #rendering beautiful plots\nimport matplotlib.pyplot as plt #python's basic plotting library\nfrom sklearn.preprocessing import LabelEncoder    #encoding categories to numerical values\n#to view plots inline here's a magic function\n%matplotlib inline  \nimport warnings     #to remove any warnings that causes headache\nwarnings.filterwarnings('ignore')","2e8e7a73":"train=pd.read_csv('..\/input\/X_train.csv')\ny=pd.read_csv('..\/input\/y_train.csv')\ntest=pd.read_csv('..\/input\/X_test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nprint(\"\\nX_train shape: {}, X_test shape: {}\".format(train.shape, test.shape))\nprint(\"y_train shape: {}, submission shape: {}\".format(y.shape, sub.shape))","a21fec5f":"def display_all(df):\n    with pd.option_context(\"display.max_rows\",4,\"display.max_columns\",10):\n        display(df)\ndisplay_all(train)","2ca7d4f9":"list(pd.unique(y['surface']))","289ed71c":"plt.figure(figsize=(10,6))\nplt.title(\"Training labels\")\nax = sns.countplot(y='surface', data=y)","21eb5dbb":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef feature_extraction(actual):\n    new = pd.DataFrame()\n    actual['total_angular_velocity'] = actual['angular_velocity_X'] + actual['angular_velocity_Y'] + actual['angular_velocity_Z']\n    actual['total_linear_acceleration'] = actual['linear_acceleration_X'] + actual['linear_acceleration_Y'] + actual['linear_acceleration_Z']\n    \n    actual['acc_vs_vel'] = actual['total_linear_acceleration'] \/ actual['total_angular_velocity']\n    \n    x, y, z, w = actual['orientation_X'].tolist(), actual['orientation_Y'].tolist(), actual['orientation_Z'].tolist(), actual['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in actual.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        new[col + '_mean'] = actual.groupby(['series_id'])[col].mean()\n        new[col + '_min'] = actual.groupby(['series_id'])[col].min()\n        new[col + '_max'] = actual.groupby(['series_id'])[col].max()\n        new[col + '_std'] = actual.groupby(['series_id'])[col].std()\n        new[col + '_max_to_min'] = new[col + '_max'] \/ new[col + '_min']\n        \n        # Change. 1st order.\n        new[col + '_mean_abs_change'] = actual.groupby('series_id')[col].apply(f2)\n        \n        # Change of Change. 2nd order.\n        new[col + '_mean_change_of_abs_change'] = actual.groupby('series_id')[col].apply(f1)\n        \n        new[col + '_abs_max'] = actual.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        new[col + '_abs_min'] = actual.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n\n    return new","9e0fa783":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        \n        return mis_val_table_ren_columns\n     ","e30116cd":"train_df = feature_extraction(train)\ntest_df = feature_extraction(test)\n# train_df.to_csv('New_train2.csv')\n# test_df.to_csv('New_test2.csv')","54b108ab":"missing_values_table(train_df) ","698dd998":"train_df['acc_vs_vel_std']=train_df['acc_vs_vel_std'].fillna(0)\ntrain_df['acc_vs_vel_mean_change_of_abs_change']=train_df['acc_vs_vel_mean_change_of_abs_change'].fillna(0)","e8fb4f34":"missing_values_table(train_df) ","1e70bcda":"train_df.fillna(0, inplace = True)\ntest_df.fillna(0, inplace = True)\ntrain_df.replace(-np.inf, 0, inplace = True)\ntrain_df.replace(np.inf, 0, inplace = True)\ntest_df.replace(-np.inf, 0, inplace = True)\ntest_df.replace(np.inf, 0, inplace = True)","faba9992":"train_df=train_df.astype('float32')\ntest_df=test_df.astype('float32')\ntrain_df.info()","0d371c81":"le = LabelEncoder()\ntarget = le.fit_transform(y['surface'])    #label Encoding is required to convert names to num","fe14b0bf":"target","1cbc4d3b":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(random_state=42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","b6e2a37c":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","fc78fe5b":"# rf = RandomForestClassifier()\n\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n\n# rf_random.fit(train_df, target)","b9886677":"# best_random = rf_random.best_estimator_\n# best_random.get_params()\n# pred=best_random.predict(test_df)","adb15186":"# sub['surface'] = le.inverse_transform(pred)\n# sub.to_csv('sample2.csv', index=False)","34324bcf":"# from sklearn.model_selection import GridSearchCV\n# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [False],\n#     'max_depth': [40, 50, 60, 80],\n#     'max_features': ['auto','sqrt'],\n#     'min_samples_leaf': [1, 2, 3],\n#     'min_samples_split': [4, 3, 2],\n#     'n_estimators': [500, 900, 1000, 1200]\n# }\n# # Create a based model\n# rf = RandomForestClassifier()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 3)\n# # Fit the grid search to the data\n# grid_search.fit(train_df, target)","2997a7f2":"# best_model=grid_search.best_estimator_\n# x=grid_search.best_params_\n# best_model.set_params()\n# best_model.get_params()","d2de79f6":"# best_model.fit(train_df,target)","bc4d72f4":"# pred2=best_model.predict(test_df)\n# pred2","e0d37eac":"# sub['surface'] = le.inverse_transform(pred2)\n# sub.to_csv('sample-4.csv', index=False)\n# sub.head(3)","a9899874":"# This is a classification problem with nine possible classes (floor surfaces):\n-  \n['fine_concrete',\n 'concrete',\n 'soft_tiles',\n 'tiled',\n 'soft_pvc',\n 'hard_tiles_large_space',\n 'carpet',\n 'hard_tiles',\n 'wood']","0a52c565":"**The following hyperparameters were obtained from more exhaustive GridSearchCV**\n-  max_depth: 80\n-  max_features: 'auto'\n-  n_estimators: 900\n-  min_samples_split : 3\n-  min_samples_leaf : 1\n- bootstrap: False","bf3fc05b":"**Now we will extract the best estimator and will make our test data run on this model.**","b3805acc":"**It can be seen that there are two columns having 2-2 missing values each**\n-  They must be removed","269554d2":"-  Reading csv files provided.\n    -  X_train.csv \n    - X_test.csv\n    -  sample_submission.csv\n    -  y_train.csv","d649cbba":"**Label Encoding**\n- Encoding categories into numerical values.","e8b13e73":"# Create the parameter grid based on the results of random search \n-  Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:\n","3ecb1c94":"On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n\n**Run Below Code to run RandomizedSearchCV**","b192f05c":"# Random Forest HyperParameter tuning \n-  Gathering more data and feature engineering usually has the greatest payoff in terms of time invested versus improved performance, but when we have exhausted all data sources, it\u2019s time to move on to model hyperparameter tuning.\n\n### Provided above are some of the hyperparameters we use in random forest.\n- That is quite an overwhelming list! How do we know where to start? A good place is the documentation on the random forest in Scikit-Learn. This tells us the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features). We could go read the research papers on the random forest and try to theorize the best hyperparameters, but a more efficient use of our time is just to try out a wide range of values and see what works! We will try adjusting the following set of hyperparameters:\n    -  n_estimators = number of trees in the foreset\n    - max_features = max number of features considered for splitting a node\n    - max_depth = max number of levels in each decision tree\n    - min_samples_split = min number of data points placed in a node before the node is split\n    - min_samples_leaf = min number of data points allowed in a leaf node\n    - bootstrap = method for sampling data points (with or without replacement)\n    \n## Random Hyperparamter Grid    \n- Usually, we only have a vague idea of the best hyperparameters and thus the best approach to narrow our search is to evaluate a wide range of values for each hyperparameter. Using Scikit-Learn\u2019s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values.\n-  To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:","7055f2a0":"**Writing a function that will present the missing values in a table well presented**","75fc3d8f":"# Trying RandomForestClassifier from sklearn.ensemble\n","6a72df15":"**Submitting sample2.csv scored 0.69  on the leaderboard**","7c03a4e0":"**Checking Missing values for train_df**\n-  It may have been there because of the use of tan function in the code which turns 0 to infinity. Just to be sure this check has been made.","60a2fc0a":"# Uncomment and run below cells to train your own model.","0a299b0c":"# Feature Engineering\n-  Creating new features from the one's already there.","6a3a90cf":"**The Above submission scored 0.70 on the leaderboard.**\n","672ec15c":"**Retraining our best estimator on the training data and finding the predictions**","f33cea88":"# 2nd attempt","b2c28cb1":"**display_all() function**\n-  It will allow to print specified no. of rows and columns mentioned in the function.","8d0b61e7":"**The Best parameter that i got from above RandomSearch are :**\n - 'bootstrap': False\n - 'max_depth': 50\n - 'max_features': 'auto'\n - 'min_samples_leaf': 1\n - 'min_samples_split': 2\n - 'n_estimators': 1000","7fed000b":"-  Importing Libraries","df83c5b7":"**Plotting Count Plot**\n-  It can be observed that there's a certain imbalance in the categories, that can be handled using Upsampling ( But will not be done in this notebook )"}}