{"cell_type":{"25f9c1dd":"code","fcaee1b1":"code","ea4292da":"code","8e13531a":"code","b0dc09b0":"code","6c8ae57d":"code","d44dc380":"code","cf5be6db":"code","a47b441c":"code","da04a51e":"code","b591cba7":"code","8c7e086c":"code","028c66c4":"code","29453fe4":"code","2a57fa83":"code","0bfc56af":"markdown","7680bd00":"markdown","2827ab03":"markdown","f8448d43":"markdown","64430eaf":"markdown","da57a732":"markdown","ee8caa04":"markdown","c8c244fc":"markdown"},"source":{"25f9c1dd":"# #install HyperGBM\n!pip3 install -U hypergbm\n!pip3 install -U scikit-learn==0.23.2","fcaee1b1":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ea4292da":"##load data\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col=0)\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col=0)","8e13531a":"#create new features\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\n\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntrain['abs_sum'] = train[features].abs().sum(axis=1)\ntrain['sem'] = train[features].sem(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['avg'] = train[features].mean(axis=1)\ntrain['max'] = train[features].max(axis=1)\ntrain['min'] = train[features].min(axis=1)\n\ntest['n_missing'] = test[features].isna().sum(axis=1)\ntest['abs_sum'] = test[features].abs().sum(axis=1)\ntest['sem'] = test[features].sem(axis=1)\ntest['std'] = test[features].std(axis=1)\ntest['avg'] = test[features].mean(axis=1)\ntest['max'] = test[features].max(axis=1)\ntest['min'] = test[features].min(axis=1)","b0dc09b0":"## custom search space\nROOT = '\/kaggle\/input\/d\/wumingyang\/resource\/'\nimport sys\nsys.path.append(ROOT)\nfrom MySearchSpace import MyGeneralSearchSpaceGenerator","6c8ae57d":"from hypergbm import make_experiment\nfrom hypernets.core.trial import TrialHistory\nfrom hypernets.searchers import PlaybackSearcher\n\nhistory_file = f\"{ROOT}history_stepxgb.txt\" ## get from my datasets.\ntarget = 'claim'\nreward = 'auc' \n\nsearch_space_ = MyGeneralSearchSpaceGenerator(class_balancing=None)\nhistory = TrialHistory.load_history(search_space_, history_file)\n## for search algorithm, hypergbm support evolution,mcts, random, but here, i just need reload history with PlaybackSearcher.\nplayback = PlaybackSearcher(history, top_n=9, optimize_direction='max')","d44dc380":"##run make_experiment\nexp = make_experiment(\n                      train.copy(),\n                      target=target,\n                      reward_metric=reward,\n                      cv = True,\n                      num_folds= 5, ## 5 folds for Cross-validate\n                      max_trials=4,\n                      ensembel_size = 4, ## ensembel models with GreedyEnsembel\n                      early_stopping_time_limit=3600*3, ##early stop for 3h\n                      log_level = 'info',  ##some info will output while trainning\n                      searcher = playback,\n                      random_state=0)\nestimator = exp.run()","cf5be6db":"preds = estimator.predict_proba(test)[:,1]","a47b441c":"submit_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmit_df['claim'] = preds\nsubmit_df.to_csv(\"submission.csv\", index=False)","da04a51e":"submit_df.head(3)","b591cba7":"# ## get new_train, new_test\n# from hypergbm import HyperGBMEstimator\n\n# new_train = np.zeros((train.shape[0],10)) ## 9 models's predict result + claim\n# new_test = np.zeros((test.shape[0],9))\n\n# for i,trial in enumerate(exp.hyper_model_.history.trials):  ## get oof result from exp\n#     new_train[:,i] = trial.memo['oof'][:,1]  ##oof result saved in trial.memo for train data\n    \n#     model = HyperGBMEstimator.load(trial.model_file)  ##get trained model from model_file.\n#     new_test[:,i] = model.predict_proba(test)[:,1]  \n    \n# new_train[:,-1] = train[target].values\n\n","8c7e086c":"# # transform numpy to Dataframe\n# _columns = [f'feature{i}' for i in range(9)]\n# new_train = pd.DataFrame(new_train,columns=_columns+[target])\n# new_test = pd.DataFrame(new_test,columns=_columns)\n# new_train.head(3)","028c66c4":"# import catboost\n# from sklearn.linear_model import LogisticRegression\n\n# meta_model1 = LogisticRegression(max_iter=1000,multi_class='multinomial',solver='lbfgs')\n# meta_model2 = catboost.CatBoostClassifier(depth=1,verbose=False)\n","29453fe4":"# iterators = StratifiedKFold(n_splits=5, shuffle=True,random_state=0)\n# y = new_train.pop(target)\n# X = new_train\n# preds = np.zeros((test.shape[0],2)) ## for 2 models\n# for n_fold, (train_idx, valid_idx) in enumerate(iterators.split(X, y)):\n#     x_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n#     x_val_fold, y_val_fold = X.iloc[valid_idx], y.iloc[valid_idx]\n#     for i,meta_model in enumerate([meta_model1,meta_model2]):\n#         meta_model.fit(x_train_fold,y_train_fold)\n#         pred = meta_model.predict_proba(x_val_fold)[:,1]\n#         preds[valid_idx,i] = pred\n    \n# ## just average the result\n# preds = np.mean(preds,axis=1)\n\n# # submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv', index_col='id')\n# # submission['claim'] = preds\n# # submission.to_csv('submission_simpleStacking.csv')\n\n# ## PLB: 0.81825","2a57fa83":"\n# from itertools import combinations\n# def generate_new_features(df):\n#         combs = list(combinations(df.columns, 2))\n#         for i, j in combs:\n#             column_name = '%s-%s' % (i, j)\n#             df[column_name] = df[i] - df[j]\n#             # column_name = '%s+%s' % (i, j)\n#             # df[column_name] = df[i] + df[j]\n#         return df\n\n# new_train = generate_new_features(X)\n# new_train[target] = y\n# new_test = generate_new_features(new_test)","0bfc56af":"About new features, i referred  https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm, thanks here.","7680bd00":"##### About GreedyEnsemble:\n\n    \"\"\"\n    References\n    ----------\n        Caruana, Rich, et al. \"Ensemble selection from libraries of models.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n    \"\"\"","2827ab03":"##### Now, i have a new_train data which has 9 features,before move to the next step,add some new features.","f8448d43":"For this experiment, I used HyperGBM, one kind of autoML tool.\nSimple introduction, HyperGBM is a library that supports full-pipeline AutoML, which completely covers the end-to-end stages of data cleaning, preprocessing, feature generation and selection, model selection and hyperparameter optimization.It is a real-AutoML tool for tabular data. You can visit [HyperGBM](https:\/\/github.com\/DataCanvasIO\/HyperGBM) for more details.","64430eaf":"##### For first step, there are some things i did:\n1. custom search space.\n2. search best models.\n3. choice xgb models Top4, lgb models Top3, cat models Top2 (total 9 models) for next step.\n\n*(run lightgbm model for once--cv5, i cost 150mins,and only 2 cpu is running, i donot know why.\nso for this code, i only run xgboost ,it's so fast by using kaggle gpu)*","da57a732":"##### I searched the base models locally, so just need reload history file.","ee8caa04":"# introduce\nIn this experiment, i try to get a nice score through some basic models\n\nI'm not good at FE, so,i only add some new features, and for Imputer and Scaler, i just let them be done by autoML tool, and I think the different preprocessing methods are good for Ensemble\/stacking.\n\nIn short,my method is like:\n1. get original data\n2. send to Hypergbm\n3. get TopN best models\n4. run GreedyEnsemble\n\n\nAbout  PLB:\n\n\n    a. best xgb model, **PLB:0.81793**\n    \n    b. best lgb model, **PLB:0.81767**\n    \n    c. best cat model, **PLB:0.81744**\n    \n    e. GreedyEnsemble, **PLB:0.81825**\n    \nThere may also be performance improvements in basic models. Since we know trainning cost lot's of time in this competition.\n\nHope it can be helpful for u.\n","c8c244fc":"##### Try to do simple stacking.\n1. get new_train,new_test from model's predict result.\n2. train new_data"}}