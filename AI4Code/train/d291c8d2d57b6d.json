{"cell_type":{"096dc66f":"code","03d0b02f":"code","6bf653c7":"code","7435d98e":"code","6c960a8c":"code","79a745b3":"code","1bb9165e":"code","2a882d80":"code","21499748":"code","003c871a":"code","08d72220":"code","078a889c":"code","9e98ce2c":"code","b6916708":"code","dc7a4c61":"code","f471e45c":"code","cf8a07f3":"code","e94ef8c9":"code","a643b5af":"code","7374a306":"code","d53c12d0":"code","2e8f89fd":"code","6f252472":"code","8156cd4d":"code","79ca3ffc":"code","5b72d2da":"code","5507eedf":"code","dd9cad87":"code","cc5749cb":"code","d53f7601":"code","f204a0d6":"code","ae83344d":"code","07b06554":"code","af0a0558":"code","996b3791":"code","9c7ca2e6":"code","1468b45d":"code","704f50c1":"code","f6d5bbf4":"code","a6352fea":"code","b9406e91":"code","d79bb6b9":"code","ec4b6b8a":"code","a3369cae":"code","0d557f5f":"code","0cd7ff4b":"code","fd98d6e6":"code","60068ff1":"code","f909e843":"code","a10f380f":"code","650f07fc":"code","d0bfa5fd":"code","6d7342b7":"code","a4833dc5":"code","26daab74":"code","38a5a5bb":"code","825ad47d":"code","e89f5929":"code","ec433a33":"code","64863e77":"code","8a84f487":"code","64634525":"code","1f9d815c":"code","d580216c":"code","6bbd7123":"code","e13b9a56":"code","c315eeb8":"code","99f6255f":"code","80936944":"code","58e35f93":"code","d2dc6322":"code","e8a501fc":"code","157cdc94":"code","0c1fb8c0":"code","bfcb1384":"code","77cbb584":"code","1f861223":"code","fed73441":"code","4cb4cbf7":"code","63d6b1ae":"code","d744c922":"code","01f1cfd3":"code","7387f6d0":"code","a9a0d35c":"code","222400ba":"code","d6843c7a":"code","8d04ed06":"code","bcea956d":"code","217a47b4":"code","77903a6a":"code","3a72ad8d":"code","b6739577":"code","a01f0e27":"code","ac05342f":"code","f198ff2e":"code","1b16245d":"code","df3b549a":"code","d2a940bd":"code","f56ccc15":"code","b9eb5f91":"code","d17fa095":"code","dd6e01f1":"code","3608d487":"code","cffd93bb":"code","a0a30810":"code","0c4bc6b9":"code","0995cf29":"code","000b0241":"code","8c615e1f":"code","edec0e88":"code","ccf09aaf":"code","2a32c38d":"markdown","e3f031c1":"markdown","5ebd174e":"markdown","4c7b8d4e":"markdown","08495c2a":"markdown","b2961278":"markdown","71ca51fe":"markdown","a1343afd":"markdown","7cd0df70":"markdown","b6de1209":"markdown","91e95892":"markdown","faed37c7":"markdown","e4efafff":"markdown","6ec6c7c0":"markdown","ba2b069d":"markdown","96c885ff":"markdown","ab9c61f2":"markdown","6083cedc":"markdown","fb13f2a1":"markdown","b88f0606":"markdown","f813c33d":"markdown","aea82aaa":"markdown","1bd55c93":"markdown","792068c0":"markdown","672e6cd8":"markdown","6032b778":"markdown","b53fcbe0":"markdown","78744aa1":"markdown","499ec0e9":"markdown","7530bbaf":"markdown","af62d284":"markdown","f9090aa1":"markdown","5d39ca73":"markdown","aabab569":"markdown","5cca33eb":"markdown","fa4d9ba2":"markdown","4f262cb1":"markdown","0626ae07":"markdown","7c0ea6d1":"markdown","97239a92":"markdown","e305d145":"markdown","ce81d158":"markdown","b33af5e6":"markdown","addda12a":"markdown"},"source":{"096dc66f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","03d0b02f":"import numpy as np   \nimport pandas as pd    \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport scipy.stats as stats\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,train_test_split,cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom scipy.stats import randint\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#Models\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","6bf653c7":"# Loading the data\n# df = pd.read_csv('train.csv')\ndf = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","7435d98e":"test1 = test.copy()","6c960a8c":"# Checking top 5 rows of data\ndf.head()","79a745b3":"# Checking Shape\ndf.shape","1bb9165e":"# Checking Information\ndf.info()","2a882d80":"# Converting Region_Code & Policy_Sales_Channel to integer\ndf.Region_Code = df.Region_Code.astype('int64')\ndf.Policy_Sales_Channel = df.Policy_Sales_Channel.astype('int64')","21499748":"test.Region_Code = test.Region_Code.astype('int64')\ntest.Policy_Sales_Channel = test.Policy_Sales_Channel.astype('int64')","003c871a":"# Checking null values\ndf.isnull().sum()","08d72220":"# Checking description of numerical columns\ndf.describe(include = 'number')","078a889c":"# Checking value counts for numerical columns\nfor col in df.columns:\n    if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n        print(col,\":\",df[col].nunique())","9e98ce2c":"# Checking description of object columns\ndf.describe(include = 'object')","b6916708":"# Checking for duplicates\ndups = df.duplicated()\ndups.sum()","dc7a4c61":"# Dropping the duplicates\ndf.drop_duplicates(inplace=True)\ndf.shape","f471e45c":"# Target variable count\ngraph = sns.countplot(df.Response)\nfor p in graph.patches:\n    graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),ha='center', va='bottom',color= 'black')","cf8a07f3":"df.Response.value_counts().plot(kind='pie', autopct='%1.0f%%');","e94ef8c9":"# Counts of Categorical variables\nfig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    graph = sns.countplot(df[x[i]])\n    \n    for p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","a643b5af":"# Analyzing continuous variables\nfig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(14, 14)\na = sns.distplot(df['Age'] , ax=axes[0][0])\na.set_title(\"Age\",fontsize=15)\na = sns.boxplot(df['Age'] , orient = \"v\" , ax=axes[0][1])\na.set_title(\"Age\",fontsize=15)\n\na = sns.distplot(df['Annual_Premium'] , ax=axes[1][0])\na.set_title(\"Annual_Premium\",fontsize=15)\na = sns.boxplot(df['Annual_Premium'] , orient = \"v\" , ax=axes[1][1])\na.set_title(\"Annual_Premium\",fontsize=15)\nplt.show()\n\nplt.show()","7374a306":"# Top 10 regions with highest number of insurers\nlabels= df['Region_Code'].value_counts()[:10].keys()\nvalues= df['Region_Code'].value_counts()[:10]\n\nplt.figure(figsize = (15, 5))\ngraph = sns.barplot(x = labels, y = values)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","d53c12d0":"# Top 10 policy channels covering highest number of insurers\nlabels= df['Policy_Sales_Channel'].value_counts()[:10].keys()\nvalues= df['Policy_Sales_Channel'].value_counts()[:10]\n\nplt.figure(figsize = (15, 5))\ngraph = sns.barplot(x = labels, y = values)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","2e8f89fd":"sns.distplot(df.Vintage)","6f252472":"# Pearson Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),annot=True,mask=np.triu(df.corr(),+1))","8156cd4d":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    graph = sns.countplot(df[x[i]],hue = df['Response'])\n    \n    for p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","79ca3ffc":"pd.crosstab(df['Vehicle_Damage'],df['Previously_Insured'])","5b72d2da":"pd.crosstab(df['Previously_Insured'],df['Response'])","5507eedf":"pd.crosstab(df['Vehicle_Age'],df['Previously_Insured'])","dd9cad87":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    sns.pointplot(df[x[i]],df['Response'])","cc5749cb":"fig, axes = plt.subplots(nrows=3,ncols=3,  figsize=(20,20))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Age','Annual_Premium','Vintage']\nfor i in range(0,len(x)):\n    sns.barplot(df['Response'],df[x[i]],ax=axes[i][0])\n    sns.violinplot(df['Response'],df[x[i]],ax=axes[i][1])\n    sns.boxplot(df['Response'],df[x[i]],ax=axes[i][2])","d53f7601":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Vehicle_Age','Vehicle_Damage','Driving_License','Previously_Insured']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    sns.barplot(df[x[i]],df['Annual_Premium'],hue = df['Response'])","f204a0d6":"sns.barplot('Vehicle_Age','Age',data=df)","ae83344d":"sns.barplot('Vehicle_Damage','Age',data=df)","07b06554":"sns.pointplot('Vehicle_Damage','Driving_License',data=df)","af0a0558":"df_feature_importance = df.copy()","996b3791":"df_feature_importance.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)\ndf_feature_importance = pd.get_dummies(df_feature_importance, columns=['Gender','Vehicle_Damage'],drop_first=True)","9c7ca2e6":"def feature_importance(model):\n    x=pd.DataFrame(model.feature_importances_*100,index=X_train.columns).sort_values(by=0,ascending=False)\n    plt.figure(figsize=(12,7))\n    sns.barplot(x[0],x.index,palette='rainbow')\n    plt.ylabel('Feature Name')\n    plt.xlabel('Feature Importance in %')\n    plt.title('Feature Importance Plot')\n    plt.show()","1468b45d":"# Copy all the predictor variables into X dataframe\nX = df_feature_importance.drop('Response', axis=1)\n\n# Copy target into the y dataframe\ny = df_feature_importance['Response']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)","704f50c1":"import lightgbm as lgb\nlgb_classfr = lgb.LGBMClassifier(objective='binary', \n         boosting_type = 'gbdt', \n         n_estimators = 10000)\nlgb_classfr.fit(X_train, y_train, early_stopping_rounds=200, verbose = 200, eval_set = [(X_test, y_test)])","f6d5bbf4":"feature_importance(lgb_classfr)","a6352fea":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","b9406e91":"feature_importance(rf)","d79bb6b9":"df_outlier = df.copy()","ec4b6b8a":"Q1 = df_outlier['Annual_Premium'].quantile(0.25)\nQ3 = df_outlier['Annual_Premium'].quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(3 * IQR)\nupper_range= Q3+(3 * IQR)\nprint('Number of Outliers:')\n((df_outlier['Annual_Premium'] < (lower_range)) | (df_outlier['Annual_Premium'] > (upper_range))).sum()","a3369cae":"((((df_outlier['Annual_Premium'] < (lower_range)) | (df_outlier['Annual_Premium'] > (upper_range))).sum())\/df_outlier.shape[0])*100","0d557f5f":"print('Upper Range: {}\\nLower Range:{}'.format(upper_range,lower_range))","0cd7ff4b":"df_outlier['Annual_Premium']=np.where(df_outlier['Annual_Premium']>upper_range,upper_range,df_outlier['Annual_Premium'])\ndf_outlier['Annual_Premium']=np.where(df_outlier['Annual_Premium']<lower_range,lower_range,df_outlier['Annual_Premium'])","fd98d6e6":"sns.boxplot(df_outlier['Annual_Premium'])","60068ff1":"sns.distplot(df_outlier['Annual_Premium'])","f909e843":"Q1 = test['Annual_Premium'].quantile(0.25)\nQ3 = test['Annual_Premium'].quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(3 * IQR)\nupper_range= Q3+(3 * IQR)\nprint('Number of Outliers:')\n((test['Annual_Premium'] < (lower_range)) | (test['Annual_Premium'] > (upper_range))).sum()","a10f380f":"test['Annual_Premium']=np.where(test['Annual_Premium']>upper_range,upper_range,test['Annual_Premium'])\ntest['Annual_Premium']=np.where(test['Annual_Premium']<lower_range,lower_range,test['Annual_Premium'])","650f07fc":"# Age and annual premium highly skewed. Not normally distributed. Both right skewed with non negative values","d0bfa5fd":"# function to plot a histogram and a Q-Q plot\n# side by side, for a certain variable\ndef diagnostic_plots(X,var):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    plt.title(var)\n    X.hist()\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(X, dist=\"norm\", plot=plt)\n\n    plt.show()","6d7342b7":"def box_plots(X,Y,var1,var2):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    plt.title(var1)\n    sns.boxplot(X)\n\n    plt.subplot(1, 2, 2)\n    plt.title(var2)\n    sns.boxplot(Y)\n\n    plt.show()","a4833dc5":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i],i)\n    print('Skewness for',i,\":\",df_outlier[i].skew())","26daab74":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(np.log(df_outlier[i]+1),i)\n    print('Skewness for',i,\":\",np.log(df_outlier[i]+1).skew())","38a5a5bb":"box_plots(np.log(df_outlier['Age']+1),np.log(df_outlier['Annual_Premium']+1),'Age','Annual_Premium')","825ad47d":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i]**(1\/2),i)\n    print('Skewness for',i,\":\",(df_outlier[i]**(1\/2)).skew())","e89f5929":"box_plots(df_outlier['Age']**(1\/2),df_outlier['Annual_Premium']**(1\/2),'Age','Annual_Premium')","ec433a33":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i]**(1\/3),i)\n    print('Skewness for',i,\":\",(df_outlier[i]**(1\/3)).skew())","64863e77":"box_plots(df_outlier['Age']**(1\/3),df_outlier['Annual_Premium']**(1\/3),'Age','Annual_Premium')","8a84f487":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(1\/(df_outlier[i]+1),i)\n    print('Skewness for',i,\":\",(1\/(df_outlier[i]+1)).skew())","64634525":"box_plots(1\/(df_outlier['Age']+1),1\/(df_outlier['Annual_Premium']+1),'Age','Annual_Premium')","1f9d815c":"df_outlier['Age_boxcox'], param = stats.boxcox(df_outlier.Age) \ndf_outlier['Annual_Premium_boxcox'], param = stats.boxcox(df_outlier.Annual_Premium) ","d580216c":"test['Age_boxcox'], param = stats.boxcox(test.Age) \ntest['Annual_Premium_boxcox'], param = stats.boxcox(test.Annual_Premium) ","6bbd7123":"for i in ['Age_boxcox','Annual_Premium_boxcox']:\n    diagnostic_plots(df_outlier[i],i)\n    print('Skewness for',i,\":\",df_outlier[i].skew())","e13b9a56":"box_plots(df_outlier['Age_boxcox'],df_outlier['Annual_Premium_boxcox'],'Age','Annual_Premium')","c315eeb8":"df_outlier.drop(['id','Age','Annual_Premium','Driving_License'],axis=1,inplace=True)","99f6255f":"test.drop(['id','Age','Annual_Premium','Driving_License'],axis=1,inplace=True)","80936944":"df_outlier.head()","58e35f93":"df1 = df_outlier.copy()","d2dc6322":"df1.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)","e8a501fc":"test.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)","157cdc94":"# dummy variable encoding\ndf1 = pd.get_dummies(df1, columns=['Gender','Vehicle_Damage'],drop_first=True)","0c1fb8c0":"test = pd.get_dummies(test, columns=['Gender','Vehicle_Damage'],drop_first=True)","bfcb1384":"df1.head() ","77cbb584":"# Pearson Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(df1.corr(),annot=True,mask=np.triu(df1.corr(),+1))","1f861223":"# Checking level of multicollinearity with VIF of all variables\nX = df1.drop('Response', axis=1)\nvif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] \ni=0\nfor column in X.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","fed73441":"df1.drop('Age_boxcox',axis=1,inplace=True)","4cb4cbf7":"test.drop('Age_boxcox',axis=1,inplace=True)","63d6b1ae":"X = df1.drop('Response', axis=1)\nvif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] \ni=0\nfor column in X.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","d744c922":"print(\"Unique values in Policy_Sales_Channel: {}\\nUnique values in Region_Code: {}\".format(df1.Policy_Sales_Channel.nunique(),df1.Region_Code.nunique()))","01f1cfd3":"## Mean encoding for Policy_Sales_Channel and Region_Code\ndf_mean_encode = df1.copy()","7387f6d0":"encod_type_Region_Code = df_mean_encode.groupby('Region_Code')['Response'].mean()\nencod_type_Policy_Sales_Channel = df_mean_encode.groupby('Policy_Sales_Channel')['Response'].mean()","a9a0d35c":"df_mean_encode.loc[:, 'Region_Code'] = df_mean_encode['Region_Code'].map(encod_type_Region_Code)\ndf_mean_encode.loc[:, 'Policy_Sales_Channel'] = df_mean_encode['Policy_Sales_Channel'].map(encod_type_Policy_Sales_Channel)","222400ba":"test.loc[:, 'Region_Code'] = test['Region_Code'].map(encod_type_Region_Code)\ntest.loc[:, 'Policy_Sales_Channel'] = test['Policy_Sales_Channel'].map(encod_type_Policy_Sales_Channel)","d6843c7a":"df_mean_encode.head() ","8d04ed06":"## Frequency encoding for Policy_Sales_Channel and Region_Code\ndf_frequency_encode = df1.copy()","bcea956d":"df_frequency_map_region = df_frequency_encode.Region_Code.value_counts().to_dict()\ndf_frequency_map_Policy_Sales_Channel = df_frequency_encode.Policy_Sales_Channel.value_counts().to_dict()","217a47b4":"df_frequency_encode['Region_Code'] = df_frequency_encode['Region_Code'].map(df_frequency_map_region)\ndf_frequency_encode['Policy_Sales_Channel'] = df_frequency_encode['Policy_Sales_Channel'].map(df_frequency_map_Policy_Sales_Channel)","77903a6a":"df_frequency_encode.head()","3a72ad8d":"## KDD encoding for Policy_Sales_Channel and Region_Code\ndf_KDD_encode = df1.copy()","b6739577":"# function to create the dummy variables for the most frequent labels\n# we can vary the number of most frequent labels that we encode\ndef one_hot_encoding_top_x(df, variable, x):\n    \n    top_x_labels = [y for y in df[variable].value_counts().sort_values(ascending=False).head(x).index]\n    \n    for label in top_x_labels:\n        df[variable+'_'+str(label)] = np.where(df[variable]==label, 1, 0)","a01f0e27":"one_hot_encoding_top_x(df_KDD_encode, 'Region_Code', 10)\none_hot_encoding_top_x(df_KDD_encode, 'Policy_Sales_Channel', 10)","ac05342f":"df_KDD_encode.drop(['Region_Code','Policy_Sales_Channel'],axis=1,inplace=True)","f198ff2e":"df_KDD_encode.head()","1b16245d":"mm = MinMaxScaler()\ndef split_data(dataframe):\n    # Copy all the predictor variables into X dataframe\n    X = dataframe.drop('Response', axis=1)\n    \n    # Copy target into the y dataframe\n    y = dataframe['Response']\n    \n    # Split X and y into training and test set in 70:30 ratio\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)\n    \n    # Transform data using MinMaxScaler\n    X_trains = mm.fit_transform(X_train)\n    X_tests = mm.transform(X_test)\n    return X_trains,X_tests,y_train,y_test","df3b549a":"X_trains,X_tests,y_train,y_test = split_data(df_mean_encode);","d2a940bd":"test = mm.transform(test)","f56ccc15":"results_mean_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})\nmodels = [GaussianNB(), DecisionTreeClassifier(random_state=1), RandomForestClassifier(random_state=1), LogisticRegression(random_state = 1), LinearDiscriminantAnalysis(), xgb.XGBClassifier(random_state=1), LGBMClassifier(random_state=1)]","b9eb5f91":"for model in models:\n    model.fit(X_trains, y_train)\n    results_mean_encode = results_mean_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                      'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                      'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                      'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                      'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                      'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                      'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                      'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                      'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                      'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                     }, ignore_index=True)\n    \nresults_mean_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_mean_encode","d17fa095":"X_trains,X_tests,y_train,y_test = split_data(df_frequency_encode);","dd6e01f1":"results_frequency_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})","3608d487":"for model in models:\n    model.fit(X_trains, y_train)\n    results_frequency_encode = results_frequency_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                                'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                                'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                                'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                                'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                                'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                                'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                                'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                                'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                                'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                               }, ignore_index=True)\n    \nresults_frequency_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_frequency_encode","cffd93bb":"X_trains,X_tests,y_train,y_test = split_data(df_KDD_encode);","a0a30810":"results_KDD_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})","0c4bc6b9":"for model in models:\n    model.fit(X_trains, y_train)\n    results_KDD_encode = results_KDD_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                    'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                    'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                    'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                    'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                    'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                    'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                    'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                    'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                    'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                    }, ignore_index=True)\n    \nresults_KDD_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_KDD_encode","0995cf29":"X_trains,X_tests,y_train,y_test = split_data(df_mean_encode);","000b0241":"test = mm.transform(test)","8c615e1f":"XGB = xgb.XGBClassifier(random_state=1)\nXGB.fit(X_trains,y_train)","edec0e88":"result =  XGB.predict(test)","ccf09aaf":"submit = pd.DataFrame({'id': test1.id, 'Response': result})\nsubmit.to_csv('Submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2a32c38d":"Mean age of customers with vehicle age < 1 year is around early 20s. It is around late 40s for vehicle age between 1-2 years and mid 50s for vehicle age > 2 years","e3f031c1":"* A strong positive correlation exists between Vehicle age and age\n* A strong negative correlation exists between Vehicle damage and previously insured","5ebd174e":"# Building Models","4c7b8d4e":"## Encoding","08495c2a":"The F1-score, recall and precision values are coming very poor for the models which is not desirable as per the business problems hence I would be applying sampling technique and tuning the hyperparameters in an attempt to increase performance of models.","b2961278":"* df_mean_encode\n* df_frequency_encode\n* df_KDD_encode","71ca51fe":"## Data Transformation","a1343afd":"We can see that driving license has been provided with importance next to 0 hence we will go ahead and drop the variable in upcoming steps","7cd0df70":"**Models tested:**\n* Naive Bayes (as benchmark)\n* Decision Tree\n* Random Forest\n* Logistic Regression\n* LDA\n* XGBoost\n* LightGBM\n\nFirst building the models without any hyperparameters to check impact of encoding results on different models.","b6de1209":"The variables are highly right skewed and premium column has a lot of outliers too.","91e95892":"# EDA","faed37c7":"### Frequency encoding","e4efafff":"* Males have a higher chance to be interested in vehicle insurance than females\n* Customer having driving license have a higher chance to be interested in vehicle insurance\n* Customer who don't already have vehicle insurance are more likely to be interested in vehicle insurance\n* Customer having vehicle age >2 years are highly likely to be interested in vehicle insurance followed by 1-2 years of age\n* Customer who got his\/her vehicle damaged in the past have a higher chance to be interested in vehicle insurance","6ec6c7c0":"* Region_Code and Policy_Sales_Channel are nominal in nature\n* Vintage is the count (number of days) so ordinal in nature\n* Driving_License, Previously_Insured and Response have binary categories\n* Age and Annual_Premium are continuous variables","ba2b069d":"# Data Preprocessing","96c885ff":"The best results considering performance of all models seems to be from mean encoding method hence we will proceed with that for model building. In that best AUC scores were for XGBoost and it will be considered for submission.","ab9c61f2":"# Importing important Libraries","6083cedc":"### Sqaure Root Tranformation","fb13f2a1":"## In this notebook\n* Exploratory Data Analysis\n* Data Preprocessing\n    1. Feature Selection\n    2. Outlier Treatment\n    3. Data Transformation\n    4. Encoding \n* Checking Performance of models without performing hyperparameter tuning\n\nAny suggestions and comments are welcome :)","b88f0606":"### KDD encoding","f813c33d":"## Feature Importances","aea82aaa":"### BoxCox Transformation","1bd55c93":"### Mean encoding","792068c0":"* Gender seems to have similar reponse wrt vehicle insurance\n* There are very few customers who didn't have driving license on them\n* Hardly any customer has opted for vehicle insurance if they were already insured\n* Very few customers have their vehicle age > 2years\n* Customers whose vehicle was not previously damaged didn't seem to be interested in vehicle insurance","672e6cd8":"The mean age of cutomers who got his\/her vehicle damaged in the past is higher and around mid 40s as compared to who didn't had their vehicles damaged with mean age around early 30s","6032b778":"## Frequency Encoding","b53fcbe0":"### Reciprocal transformation","78744aa1":"* Customer who are interested in vehicle insurance have a higher age bracket\n* Annual premium for both groups are similar with people interested having slightly higher mean annual premium\n* There is no significance of number of days of insurer associated with the company,  their distribution is same regardless of the response","499ec0e9":"Dataset is highly imbalanced.","7530bbaf":"There are 381109 insured customers and 11 variables containing their information.","af62d284":"## KDD Encoding","f9090aa1":"One hot encoding only the top 10 categories of Policy_Sales_Channel and Region_Code","5d39ca73":"### Logarithmic Transformation","aabab569":"### Cube Root Tranformation","5cca33eb":"# Splitting & Scaling Data","fa4d9ba2":"## Univariate Analysis","4f262cb1":"There is not much correlation between variables except slight negative correlation between Age and Policy Sales channel.","0626ae07":"The value is pretty high for age variable, hence we will drop it off and again check the level of multicolliearity of other variables.","7c0ea6d1":"Now the values are around 5 which is acceptable hence we will proceed with the remaining variables","97239a92":"## Mean Encoding","e305d145":"## Outlier Treatment","ce81d158":"The people who didn't got their vehicle damaged in the past had a slightly higher proportion of having a driving license.","b33af5e6":"* No difference in annual premium charged wrt gender\n* Vehicles having age >2 years have higher annual premium'\n* **Customers who are interested in vehicle insurance have a lower annual premium**\n* People not having driving license have a slightly higher annual premium\n* **Customers who are previously insured and interested in vehicle insurance have a lower annual premium**","addda12a":"## Bivariate Analysis"}}