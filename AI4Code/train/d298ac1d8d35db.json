{"cell_type":{"ca430f65":"code","09e3a39d":"code","510e5ddc":"code","e4197c51":"code","f324cf2c":"code","3e7fa15b":"code","d6e3821c":"code","aed4f616":"code","51ef56cd":"code","79497b77":"code","bf54e728":"code","591e94b0":"code","bf53af8e":"code","e5f77cdf":"code","1f354ed0":"code","d59b388d":"code","47fee5c8":"code","56d7f6f3":"code","8224e744":"code","db68f288":"code","dbaa0bbd":"code","8dd0ca41":"code","facb544c":"code","06c4562e":"code","5bde15c4":"code","4f15495d":"code","e69df3e7":"markdown","9cf89f32":"markdown","569a34c9":"markdown","ed611cef":"markdown"},"source":{"ca430f65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09e3a39d":"pip install tensorflow_text","510e5ddc":"import tensorflow_hub as hub\nimport tensorflow_text as text\nimport tensorflow as tf","e4197c51":"data = pd.read_csv('..\/input\/spam-mails-dataset\/spam_ham_dataset.csv')","f324cf2c":"data.drop({'Unnamed: 0'},axis=1,inplace=True)","3e7fa15b":"data['label'].value_counts()","d6e3821c":"data.sample(5)","aed4f616":"from sklearn.model_selection  import train_test_split\nX_train, X_test,y_train,y_test = train_test_split(data['text'],data['label_num'],stratify =data['label_num'] )","51ef56cd":"X_train.head(2)","79497b77":"bert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nbert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4\")","bf54e728":"def get_sentence_embeding(sentences):\n    preprocessed_text = bert_preprocess(sentences) \n    return bert_encoder(preprocessed_text)['pooled_output']\n    \n\nget_sentence_embeding([\"500$ Discount.. hurry up\",\n                      \"hey Lokesh, how are you?\"])","591e94b0":"e = get_sentence_embeding([\"banana\",\"grapes\",\"Elon Musk\"])","bf53af8e":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([e[0]],[e[1]])","e5f77cdf":"# Bert Layer\ntext_input = tf.keras.layers.Input(shape = (),dtype = tf.string, name = \"text\")\npreprocessed_text = bert_preprocessor(text_input)\nbert_encoder= bert_encoder(preprocessed_text)\n\n# Neural Network layer\ntf.keras.layers.Dropout(0.1,name = 'dropout')(outputs['pooled_output'])","1f354ed0":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","d59b388d":"model.summary()\n","47fee5c8":"len(X_train)","56d7f6f3":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall')\n]\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=METRICS)","8224e744":"model.fit(X_train, y_train, epochs=2)","db68f288":"model.evaluate(X_test, y_test)","dbaa0bbd":"y_predicted = model.predict(X_test)\ny_predicted = y_predicted.flatten()","8dd0ca41":"import numpy as np\n\ny_predicted = np.where(y_predicted > 0.5, 1, 0)\ny_predicted","facb544c":"from sklearn.metrics import confusion_matrix, classification_report\n\ncm = confusion_matrix(y_test, y_predicted)\ncm ","06c4562e":"from matplotlib import pyplot as plt\nimport seaborn as sn\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","5bde15c4":"print(classification_report(y_test, y_predicted))\n","4f15495d":"reviews = [\n    'Enter a chance to win $5000, hurry up, offer valid until march 31, 2021',\n    'You are awarded a SiPix Digital Camera! call 09061221061 from landline. Delivery within 28days. T Cs Box177. M221BP. 2yr warranty. 150ppm. 16 . p p\u00c2\u00a33.99',\n    'it to 80488. Your 500 free text messages are valid until 31 December 2005.',\n    'Hey Sam, Are you coming for a cricket game tomorrow',\n    \"Why don't you wait 'til at least wednesday to see if you get your .\"\n]\noutput = model.predict(reviews)\nnp.where(output > 0.5, 1, 0)","e69df3e7":"# Encoding for Some Words","9cf89f32":"## Sentence Encoding for some sentences\n\n- This encoding are written by the pre trained BERT model ","569a34c9":"# Creating a Input Layer","ed611cef":"- here we find that Banana and Grapes are closely related to each other"}}