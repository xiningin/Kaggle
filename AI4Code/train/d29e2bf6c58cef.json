{"cell_type":{"05a1a970":"code","ca14e7fc":"code","c98f40c3":"code","3f4cb986":"code","871c521a":"code","e5bf4983":"code","26b1d5a3":"code","f8fa4b80":"code","d10113eb":"code","12f46975":"code","d19ed753":"code","72ec7c6d":"code","e4571903":"code","ee813c0b":"code","5b459735":"code","6ba05227":"code","1de212c3":"code","17fd75cc":"code","b9304f4b":"code","f55a5a10":"markdown","e35bfbb3":"markdown","ac7ef2cb":"markdown","8eb1fcaf":"markdown","7b5d81b2":"markdown","3c460667":"markdown","f0d50866":"markdown"},"source":{"05a1a970":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","ca14e7fc":"!export XLA_USE_BF16=1","c98f40c3":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport utils\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom joblib import Parallel, delayed\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3f4cb986":"def reduce_fn(vals):\n    return sum(vals) \/ len(vals)","871c521a":"class config:\n    LEARNING_RATE = 3e-5\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 50\n    VALID_BATCH_SIZE = 32\n    EPOCHS = 3\n    TRAINING_FILE = \"..\/input\/tweet-train-folds-v2\/train_8folds.csv\"\n    ROBERTA_PATH = \"..\/input\/roberta-base\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n        merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True\n    )","e5bf4983":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","26b1d5a3":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","f8fa4b80":"class TweetModel_old(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","d10113eb":"#MX1","12f46975":"(768-5+1)\/\/2","d19ed753":"(382-3+1)\/\/2","72ec7c6d":"(190-5+1)\/\/2","e4571903":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.3)\n        \n        \n        self.channels = config.MAX_LEN\n        self.conv1 = nn.Conv1d(self.channels, self.channels\/\/2, 5, 2, 0)## 768->382\n        self.bn1 = nn.BatchNorm1d(self.channels\/\/2)\n        \n        self.relu_1 = nn.LeakyReLU()\n        self.conv2 = nn.Conv1d(self.channels\/\/2, self.channels, 3, 2, 0) ## 382->190\n        self.bn2 = nn.BatchNorm1d(self.channels)\n        \n        self.relu_2 = nn.LeakyReLU()\n        \n        self.drop_out2 = nn.Dropout(0.3)\n        '''\n        self.conv3 = nn.Conv1d(self.channels\/\/4, self.channels\/\/2, 7, 2, 0) ## 190->92\n        self.relu_3 = nn.LeakyReLU()\n        self.conv4 = nn.Conv1d(self.channels\/\/2, self.channels, 5, 2, 0) ## 92->46\n        self.relu_4 = nn.LeakyReLU()\n        \n        self.drop_out3 = nn.Dropout(0.2)\n        '''\n        '''\n        self.channels = config.MAX_LEN\n        self.conv1 = nn.Conv1d(self.channels, self.channels\/\/2, 5, 1, 2)\n        self.relu_1 = nn.LeakyReLU()\n        self.conv2 = nn.Conv1d(self.channels\/\/2, self.channels\/\/4, 5, 1, 2)\n        self.relu_2 = nn.LeakyReLU()\n        \n        self.drop_out2 = nn.Dropout(0.3)\n        \n        self.conv3 = nn.Conv1d(self.channels\/\/4, self.channels\/\/2, 3, 1, 1)\n        self.relu_3 = nn.LeakyReLU()\n        self.conv4 = nn.Conv1d(self.channels\/\/2, self.channels, 5, 1, 2)\n        self.relu_4 = nn.LeakyReLU()\n        \n        self.drop_out3 = nn.Dropout(0.2)\n        '''\n        #self.l0 = nn.Linear(768 * 2, 2)\n        #self.l0 = nn.Linear(191 * 2, 2)\n        self.l0 = nn.Linear(190, 2)\n        \n        #self.l0 = nn.Linear(46 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        nn.init.normal_(self.l0.bias, 0)\n        \n        '''\n        self.conv1 = nn.Conv1d(self.channels, 64, 5, 1, 2)\n        self.relu_1 = nn.LeakyReLU()\n        self.conv2 = nn.Conv1d(64, 16, 5, 2, 0)\n        self.relu_2 = nn.LeakyReLU()\n        \n        self.drop_out2 = nn.Dropout(0.3)\n        \n        self.conv3 = nn.Conv1d(16, 4, 3, 2, 0)\n        self.relu_3 = nn.LeakyReLU()\n        self.conv4 = nn.Conv1d(4, 1, 5, 2, 0)\n        self.relu_4 = nn.LeakyReLU()\n        \n        self.drop_out3 = nn.Dropout(0.2)\n        \n        #self.l0 = nn.Linear(768 * 2, 2)\n        ##self.l0 = nn.Linear(93 * 2, 2)\n        \n        self.l0 = nn.Linear(189, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    '''\n    def forward(self, ids, mask, token_type_ids):\n        \n        #_, _, hs = self.roberta(input_ids, attention_mask)\n        \n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        ### 192 channels, hidden size length = 768\n        #out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = torch.stack([out[-1], out[-2], out[-3]])\n        out = torch.mean(out, 0)\n        \n        \n        out = self.drop_out(out)\n        \n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.relu_1(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu_2(out)\n        \n        out = self.drop_out2(out)\n        '''\n        out = self.conv3(out)\n        out = self.relu_3(out)\n        \n        out = self.conv4(out)\n        out = self.relu_4(out)\n        \n        out = self.drop_out3(out)\n        '''\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","ee813c0b":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","5b459735":"def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n    model.train()\n    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n        scheduler.step()\n        tk0.set_postfix(loss=loss.item())","6ba05227":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Validating\")\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].cpu().numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=loss.item())\n\n    return jaccards.avg","1de212c3":"dfx = pd.read_csv(config.TRAINING_FILE)","17fd75cc":"def run(fold):\n    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n    model_config.output_hidden_states = True\n    MX = TweetModel(conf=model_config)\n    \n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n\n    device = xm.xla_device(fold + 1)\n    model = MX.to(device)\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=1\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\n        \"bias\",\n        \"LayerNorm.bias\",\n        \"LayerNorm.weight\"\n    ]\n    optimizer_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(\n                    nd in n for nd in no_decay\n                )\n            ], \n         'weight_decay': 0.001\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(\n                    nd in n for nd in no_decay\n                )\n            ], \n            'weight_decay': 0.0\n        },\n    ]\n    num_train_steps = int(\n        len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS\n    )\n    optimizer = AdamW(\n        optimizer_parameters, \n        lr=config.LEARNING_RATE\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    best_jac = 0\n    es = utils.EarlyStopping(patience=2, mode=\"max\")\n    num_batches = int(len(df_train) \/ config.TRAIN_BATCH_SIZE)\n    \n    for epoch in range(config.EPOCHS):\n        train_fn(\n            train_data_loader, \n            model, \n            optimizer, \n            device,\n            num_batches,\n            scheduler\n        )\n\n        jac = eval_fn(\n            valid_data_loader, \n            model, \n            device\n        )\n        print(f'Epoch={epoch}, Fold={fold}, Jaccard={jac}')\n        if jac > best_jac:\n            xm.save(model.state_dict(), f\"model_{fold}.bin\")\n            best_jac = jac","b9304f4b":"Parallel(n_jobs=8, backend=\"threading\")(delayed(run)(i) for i in range(8))","f55a5a10":"# Training","e35bfbb3":"# Loss Function","ac7ef2cb":"# Evaluation Functions","8eb1fcaf":"# Data loader","7b5d81b2":"# Data Processing","3c460667":"# The Model","f0d50866":"# Training Function"}}