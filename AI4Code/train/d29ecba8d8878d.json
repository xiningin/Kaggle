{"cell_type":{"af6c15ec":"code","f717271e":"code","5a53e3d9":"code","dc2b910f":"code","52e45448":"code","2ea42b91":"code","4dcf9412":"code","998a1552":"code","67193dd6":"code","d11c4fc2":"code","5cc65ff7":"code","c92e23f1":"code","a5496a74":"code","9095ad69":"code","86b529fe":"code","4b357ce4":"code","98f501cc":"code","9c51416a":"code","f0caf6de":"code","f81ebf1d":"code","73b3b8ca":"code","58e9b348":"code","7b425bbe":"code","343be70c":"code","2b1a721b":"code","9147965b":"code","ef26e892":"code","a5e7ed73":"code","5a40248f":"code","2d002909":"code","6a3fa007":"code","2532589b":"code","164ddbc5":"code","51c5b3f2":"code","7d010d70":"code","d0702524":"code","40073d05":"code","c97feecf":"code","eb44bb4d":"code","7ff4ec00":"code","afed9d73":"code","784b9aa4":"code","beee9ff8":"code","b00bbc88":"code","2a74d38f":"markdown","cad831da":"markdown","79875606":"markdown","fabfb2ed":"markdown","5438b3fd":"markdown","898764c5":"markdown","a8f2c415":"markdown","57be7a4f":"markdown","9c0d9a25":"markdown","8a78ce08":"markdown","9b992c90":"markdown","39823aa9":"markdown","01bf608f":"markdown","acdf2950":"markdown"},"source":{"af6c15ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f717271e":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport collections","5a53e3d9":"df = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines = True)\ndf.head()","dc2b910f":"# Finding the shape of the data\ndf.shape","52e45448":"# GEtting more info about the data\ndf.info()","2ea42b91":"df.isnull().sum()","4dcf9412":"# Describing the data\ndf.describe()","998a1552":"# Counting the number of unique category and then priting them\nprint(df['category'].nunique())\ndf['category'].unique()","67193dd6":"# Printing the number of words in each category\nprint(df.groupby('category').size())","d11c4fc2":"# Trying to know about the year and month from the Data\nimport datetime\ndf['year'] = pd.DatetimeIndex(df['date']).year\ndf.head()","5cc65ff7":"# Trying to add the month details\ndf['month'] = pd.DatetimeIndex(df['date']).month\ndf.head()","c92e23f1":"# Lets find out the unique values of months and year in the dataset\nprint('These data are of {} years'.format(df['year'].nunique()))\nprint(\"So below is the name of the year\")\nprint(df['year'].unique())\n\nprint('These data are of {} months'.format(df['month'].nunique()))\nprint(\"So below is the name of the months\")\nprint(df['month'].unique())","a5496a74":"from sklearn.preprocessing import LabelEncoder\n\ndef category_merge(x):\n    \n    if x == 'THE WORLDPOST':\n        return 'WORLDPOST'\n    elif x == 'TASTE':\n        return 'FOOD & DRINK'\n    elif x == 'STYLE':\n        return 'STYLE & BEAUTY'\n    elif x == 'PARENTING':\n        return 'PARENTS'\n    elif x == 'COLLEGE':\n        return 'EDUCATION'\n    elif x == 'ARTS' or x == 'CULTURE & ARTS':\n        return 'ARTS & CULTURE'\n    \n    else:\n        return x\n    \ndf['category'] = df['category'].apply(category_merge)\nle = LabelEncoder()\ndata_labels = le.fit_transform(df['category'])\nlist(le.classes_)","9095ad69":"# Counting the number of unique author and then printing the name\nprint(df['authors'].nunique())\ndf['authors'].unique()","86b529fe":"# Removing the space between the authors name and writing them in simplified way\ndf['authors'] = df['authors'].apply(lambda x: x.split(',')[0])\ndf['authors'] = df['authors'].str.replace(' ', '', regex=False)\ndf['authors'].unique()","4b357ce4":"# Counting the contributions of different authors \nprint(df.groupby('authors').size())","98f501cc":"# Plotting to see the yearwise writing and publishing of the news\n\nlabels = df['year'].value_counts().index\nvalues = df['year'].value_counts().values\n\ncolors = df['year']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                               marker = dict(colors = colors))])\nfig.show()","9c51416a":"# Plotting to see the monthwise pattern of writing and publishing of the news\n\nlabels = df['month'].value_counts().index\nvalues = df['month'].value_counts().values\n\ncolors = df['month']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                               marker = dict(colors = colors))])\nfig.show()","f0caf6de":"labels = df['category'].value_counts().index\nvalues = df['category'].value_counts().values\n\ncolors = df['category']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                               marker = dict(colors = colors), pull=[0, 0, 0.2, 0]\n       )])\nfig.show()","f81ebf1d":"# Visaulizing the same plot but withou using the Ploty\n\nplt.figure(figsize=(20,20))\nsizes = df['category'].value_counts().values\nlabels = df['category'].value_counts().index\nplt.pie(sizes, labels=labels, autopct='%.1f%%',\n        shadow=True, pctdistance=0.85, labeldistance=1.05, startangle=20, \n        explode = [0 if i > 0 else 0.2 for i in range(len(sizes))])\nplt.axis('equal')\nplt.show()","73b3b8ca":"sns.barplot(y=df['category'].value_counts()[:5].index, x=df['category'].value_counts()[:5].values, orient='h')","58e9b348":"data_labels","7b425bbe":"df['target'] = data_labels\ndf.head()","343be70c":"df['target'].unique()","2b1a721b":"# Defining function to clean the text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","9147965b":"# Applying clean text function on short_description to clean the text\n\ndf['short_description'] = df['short_description'].apply(lambda x: clean_text(x))","ef26e892":"df.head()","a5e7ed73":"# PLotting the Wordcloud\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                        # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(\" \".join(df['short_description']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('WordCloud of short_description', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","5a40248f":"print()\ntext = \"I love you, don't you\"\n\n# instantiate tokenizer class\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \", text)\nprint(\"Tokenization by whitespace: \", tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer: \", tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation: \", tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression: \", tokenizer4.tokenize(text))","2d002909":"# instantiate tokenizer class\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# Tokenizing the trainig set\ndf['short_description'] = df['short_description'].apply(lambda x: tokenizer.tokenize(x))","6a3fa007":"print()\nprint('Tokenized string:')\ndf['short_description'].head()","2532589b":"nltk.download('stopwords')","164ddbc5":"# Definig a function to remove the stopwords\n\ndef remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","51c5b3f2":"# Removing the stopwords \n\ndf['short_description'] = df['short_description'].apply(lambda x: remove_stopwords(x))","7d010d70":"df.head()","d0702524":"nltk.download('wordnet')","40073d05":"# Stemming and Lemmatization examples\n\ntext = \"How is the Josh\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer \nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer = nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","c97feecf":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    \n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf['short_description'] = df['short_description'].apply(lambda x : combine_text(x))\ndf.head()","eb44bb4d":"# text preprocessing function\ndef text_preprocessing(text):\n   \n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [word for word in tokenized_text if word not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","7ff4ec00":"# CountVectorizer can do all the above task of preprocessing, tokenization, and stop words removal\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df['short_description'])\n    \n# Keeping only non-zero elements to preserve spaces\nprint(train_vectors[0].todense())","afed9d73":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df['short_description'])","784b9aa4":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"MultinimialNB\": MultinomialNB()\n}","beee9ff8":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(df['target'].values.reshape(-1,1))\nscaler.transform(df['target'].values.reshape(-1,1))","b00bbc88":"# using headlines and short_description as input X\n\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\n\ndf['text'] = df.headline + \" \" + df.short_description\n\n# tokenizing\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nX = tokenizer.texts_to_sequences(df.text)\ndf['words'] = X\n\n# delete some empty and short data\n\ndf['word_length'] = df.words.apply(lambda i: len(i))\ndf = df[df.word_length >= 5]\n\ndf.head()","2a74d38f":"### Transforming of Tokens to Vector","cad831da":"### Tokenizing","79875606":"Normalizing the Tokens","fabfb2ed":"## Analysing the Data","5438b3fd":"From the above pie-chart it has been clear that most of maximum news coverage priority has been given to Politics followed by Wellness, Entertainement, Parenting, Style & Beauty\n\nIronically Education, Tech, Environment and Science has been given the least voice","898764c5":"### Cleaning the  Text","a8f2c415":"From the above we can see that in 2013 the max number of news headlines were published followed by 2014 and 2016","57be7a4f":"## Importing the Necessary Libraries","9c0d9a25":"## Visualization of the Data","8a78ce08":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n* Lemmatization","9b992c90":"As we can see that there are many category name which are interlinked so its better to combine them\n\n* 'ARTS' || 'ARTS & CULTURE' || 'CULTURE & ARTS'\n* 'STYLE & BEAUTY' || 'STYLE'\n* 'EDUCATION' || 'COLLEGE'\n* 'WORLDPOST' || 'THE WORLDPOST'\n* 'PARENTS' || 'PARENTING'\n* 'TASTE' || 'FOOD & DRINK'","39823aa9":"The next step is to remove stop words. Stop words are words that don't add significant meaning to the text.","01bf608f":"### Building the Final Classification Model","acdf2950":"Compare to the other months, March (3), April (4) and May (5) seems to be more busy months in terms of publishing the news headlines, so more headlines means this months have more news coverage too"}}