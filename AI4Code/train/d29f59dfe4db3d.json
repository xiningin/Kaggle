{"cell_type":{"8038783c":"code","870bdacb":"code","dba95fee":"code","ad923e39":"code","fabea18c":"code","02ff321e":"code","a442574d":"code","819ae891":"code","a312c109":"code","707c213d":"code","7ec389d2":"code","81854dd2":"code","3bee8a75":"code","23518dbf":"code","0916c386":"code","b6eefe4b":"code","603ad465":"code","1097c341":"code","83c91fbb":"code","d9bd146d":"markdown","bf4e23cb":"markdown","876efc43":"markdown","a48ceae1":"markdown","2c4cbc98":"markdown","2ff715a2":"markdown","25417535":"markdown","6c27ff68":"markdown","a70de800":"markdown","8e00a995":"markdown","756e577b":"markdown"},"source":{"8038783c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, accuracy_score, plot_confusion_matrix\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","870bdacb":"data = pd.read_csv(\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")","dba95fee":"# Trim last 2 columns out as they ar not related\ndata = data[data.columns.tolist()[:-2]]\ndata['Attrition_Flag'].unique()","ad923e39":"# Convert \"Attrition Flag\" to 1 (Attried Customer) and 0 (Existing Customer)\ndata['Is_Churn'] = data['Attrition_Flag'].apply(lambda x: 1 if(x == 'Attrited Customer') else 0)\ndata.head()","fabea18c":"# Data profiling using Pandas Data Profiling package\ndata.profile_report()","02ff321e":"data.columns","a442574d":"# Numerical Data\ndata['Total_Relationship_Count'] = data['Total_Relationship_Count'] \/ data['Total_Relationship_Count'].max()\ndata['Months_Inactive_12_mon'] = data['Months_Inactive_12_mon'] \/ 12\ndata['Contacts_Count_12_mon'] = data['Contacts_Count_12_mon'] \/ 12\ndata['Revolving_Balance'] = data['Total_Revolving_Bal'] \/ data['Credit_Limit']\ndata['Total_Trans_Ct'] = data['Total_Trans_Ct'] \/ data['Total_Trans_Ct'].max()\ndata['Total_Trans_Amt'] = data['Total_Trans_Amt'] \/ data['Total_Trans_Amt'].max()\n","819ae891":"numerical_columns = ['Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Revolving_Balance', 'Total_Trans_Ct', 'Total_Trans_Amt', 'Avg_Utilization_Ratio', 'Total_Ct_Chng_Q4_Q1']\ncategorical_columns = ['Income_Category', 'Education_Level', 'Gender']\nlabel_column = ['Is_Churn']","a312c109":"selected_data = data[numerical_columns + categorical_columns + label_column]","707c213d":"# Categorical Data\n# Convert these data to multipler columns\nselected_data = pd.concat([selected_data, pd.get_dummies(selected_data['Income_Category'], prefix='Income'), pd.get_dummies(selected_data['Education_Level'], prefix='Education'), pd.get_dummies(selected_data['Gender'], prefix='Gender')], axis=1)\nselected_data = selected_data.drop(categorical_columns, axis=1)","7ec389d2":"# Features used in model training\nselected_data.columns","81854dd2":"X_train, X_test, y_train, y_test = train_test_split(selected_data.drop(label_column, axis=1), selected_data[label_column], test_size=0.2, random_state=123)","3bee8a75":"parameters = {\n    \"learning_rate\": [0.01, 0.1, 0.5],\n    \"max_depth\":[3,5,10],\n    \"subsample\":[0.5, 0.8, 0.9, 1.0],\n    \"n_estimators\":[10, 50, 100]\n    }","23518dbf":"# parameters = {\n# #     \"loss\":[\"deviance\"],\n#     \"learning_rate\": [0.01, 0.1, 0.5],\n# #     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n# #     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n#     \"max_depth\":[3,5,10],\n# #     \"max_features\":[\"log2\",\"sqrt\"],\n#     \"subsample\":[0.5, 0.8, 0.9, 1.0],\n#     \"n_estimators\":[10, 50, 100]\n#     }","0916c386":"clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, n_jobs=-1, scoring='f1', verbose=2.5)","b6eefe4b":"clf.fit(X_train, y_train)","603ad465":"# Get model with best hyperparameters\nbest_model = clf.best_estimator_\nprint(\"F1-score: \", clf.score(X_test, y_test))\nprint(\"Accuracy: \", accuracy_score(best_model.predict(X_test), y_test))\nprint(\"Best Model Param: \", clf.best_params_)","1097c341":"# Find out what features the model depended on\n\nfeature_importances = dict(zip(X_train.columns.tolist(), best_model.feature_importances_))\nfeature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: -item[1])}\n\nplt.figure(figsize=(20, 15))G\n# plt.rcParams.update({'font.size': 25})\nplt.bar(range(len(feature_importances)), list(feature_importances.values()), align='center')\nplt.xticks(range(len(feature_importances)), list(feature_importances.keys()), rotation='vertical')\nplt.ylabel(\"Feature Importance\")","83c91fbb":"# Plot confusion matrix to find how the model performs\nplot_confusion_matrix(best_model, X_test, y_test)","d9bd146d":"## Feature Engineering and Preprocessing\n\nWe are going to select only some features for the prediction\nThese features have to be normalized to before fitting the model\n- Total relationship count (number of products held by customers)\n- Months inactive within last 12 months \/ 12 (divided by number of months)\n- Contact counts within last 12 months \/ 12 (divided by number of months)\n- Total revolving balance \/ credit limit (to nomalize as porpotion of balance out of total credit limit)\n- Total transaction count (nomalized by max value)\n- Total transaction amount (nomalized by max value)\n- Total changes in transaction count from Q4 to Q1\n- Utilization ratio\n- Income category (have to be preprocessed as it is categorical data)\n- Edcuation level (have to be preprocessed as it is categorical data)\n- Gender (have to be preprocessed as it is categorical data)","bf4e23cb":"4. From Cramer's V for correlation of categorical data, we found high correlation between gender and income category\n![image.png](attachment:image.png)","876efc43":"## Correlation Metrix from Pandas Profiling\n1. When we take a look at Peason's Correlation of Is_Churn flag (Attrition_Flag), there are some features that show relatively high postitive and negative correlation with Is_Churn flag.\n![image.png](attachment:image.png)\n\n### **Positive Correlation**\n\n-> No. of contacts in the last 12 months - This is pretty surprising but also possible that higher number of contacts means worse user experience\n\n-> No. of inactive months - Higher inactive months can indicates churning behavior\n\n### **Negative Correlation**\n\n-> Total transaction counts - Higher number of transactions = Higher engagement\n\n-> Total transaction counts changes from Q4 to Q1 - Positive\/Incresing number of transactions = Less likely to attrited\n\n-> Total revolving balance - Higher balance = Less likely to attrited\n\n-> Total transaction amount and amount changes from Q4 to Q1 - However this has weaker correlation comparing to number of transaction\n\n-> Average utilization ratio - which align with total revolving balance","a48ceae1":"## Model Training using Gradient Boosted Classifier\nUsing grid search for finding optimal hyperparameters for GBClassifer","2c4cbc98":"2. Customer Age and Months on Book (number of mounths customer open the book with the bank)\nThis one is stright forward\n![image.png](attachment:image.png)","2ff715a2":"## Credit Card Churning Classification \n### For IBM Advanced Data Science Capstone - Pat Hornchaiya","25417535":"## Data Profiling","6c27ff68":"## Train and Test Data Preparation","a70de800":"## Data Loading and Preprocessing","8e00a995":"## Future Improvment\n* Trim down the model to consider only important features\n* Try more model variation to reduce false positive and false negative","756e577b":"3. Credit limit has positive correlation with Avg open to buy\n> Credit Limit = Total Revolving Balance + Avg Open to Buy (the amount left for buying things)\n\n![image.png](attachment:image.png)"}}