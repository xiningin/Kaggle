{"cell_type":{"10d8bbc7":"code","b6ba5947":"code","3984a999":"code","666d512d":"code","907d2f44":"code","eeab3943":"code","e88fbdd7":"code","da204f6e":"code","b790c0bd":"code","b9825198":"code","5dcc69b0":"code","3c7c6650":"code","08d191d0":"code","06b455cd":"code","f39bf4fc":"code","0c4f41df":"code","4156ee1c":"code","dea13b7d":"code","7ca48bf9":"code","89cd7b4d":"code","bac2998f":"code","46ce82ff":"code","8ba83d8c":"code","d0222d5b":"code","f9f3d0d2":"code","da72e137":"code","007dc610":"code","9ebf4448":"code","5e386e89":"code","d3400ce6":"code","ea1217ba":"code","b1beb128":"code","cb492735":"code","0b5ed632":"code","4812315f":"code","bd6e0a94":"code","3c414317":"code","cd2e4fed":"code","d0da2faa":"code","aee00d5e":"code","f9d699ba":"code","b1efb461":"code","8182a61a":"code","2dd16ece":"code","ae9e66a5":"code","2206ec0a":"code","bbcf06b4":"code","2a620a58":"code","3ab56662":"code","f389b316":"code","7f823a21":"code","d033d47f":"code","d62273a6":"code","9792a442":"code","25b8d317":"code","c77f6747":"code","834d705a":"code","aa2fe40f":"code","3ea4e78f":"code","3f011066":"code","95decae3":"code","08d5633b":"code","7d614208":"code","4ef5c52a":"code","94e575cd":"code","53552a00":"code","49de90a5":"code","63a87286":"code","ccb23a9f":"code","465f12a8":"code","71a99a18":"code","f272eb58":"code","712f4874":"code","9f5a2f06":"code","d1e6fa52":"markdown","60c35132":"markdown","81740a2b":"markdown","b39eba25":"markdown","2af70ab5":"markdown","c2883599":"markdown","6374528c":"markdown","0931add9":"markdown","14cefb1c":"markdown","f3cb7ba9":"markdown","c66e373e":"markdown","3b09f8a8":"markdown","d6e27899":"markdown","e23b5f43":"markdown","1a8f732a":"markdown"},"source":{"10d8bbc7":"# import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom ipywidgets import interact\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb","b6ba5947":"# read data\ndf = pd.read_csv('..\/input\/boston-house-prices\/housing.csv',delim_whitespace = True,header = None)\ndf.head()","3984a999":"# set column names\ndf.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\ndf.head()","666d512d":"# shape of dataset\ndf.shape","907d2f44":"#Statistical info\ndf.describe()","eeab3943":"# datatype info\ndf.info()","e88fbdd7":"#check for null values\ndf.isnull().sum()","da204f6e":"# See rows with missing values\ndf[df.isnull().any(axis=1)]","b790c0bd":"# Columns distributions \nhist = df.hist(bins=40,figsize=(20,15))","b9825198":"# Feature scaling\nscaler = StandardScaler()\nsc_cols = ['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\ndf[sc_cols] = scaler.fit_transform(df[sc_cols])","5dcc69b0":"# Columns distributions after scaling\nhist = df.hist(bins=40,figsize=(20,15))","3c7c6650":"# create box plots\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.boxplot(y=col, data=df, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","08d191d0":"# Finding out the correlation between the features\ncorr = df.corr()\ncorr","06b455cd":"# Plotting the heatmap of correlation between features\nfig, ax = plt.subplots(figsize=(15 ,10))\nsns.heatmap(corr,  annot=True, cmap='RdYlGn')","f39bf4fc":"# Plotting correlations  in interactive way\ndef f(corr):\n    if corr == 'MEDV and LSTAT':\n        plt.scatter(df['MEDV'],df['LSTAT'],marker= '*', c= 'red')\n        plt.xlabel('LSTAT')\n        plt.ylabel('MEDV')\n        plt.title('Correlation between MEDV and LSTAT')\n        plt.show\n    else:\n        plt.scatter(df['MEDV'],df['RM'])\n        plt.xlabel('RM')\n        plt.ylabel('MEDV')\n        plt.title('Correlation between MEDV and RM')\n        plt.show     \ninteract(f, corr = ['MEDV and RM','MEDV and LSTAT'])","0c4f41df":"# Create dependent and independent features \nX = df.drop(columns = ['MEDV','RAD'], axis = 'column')\ny = df['MEDV']","4156ee1c":"X.columns","dea13b7d":"# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","7ca48bf9":"LR = LinearRegression(normalize=True)\nLR.fit(X_train,y_train)","89cd7b4d":"LR_cv_score = cross_val_score(LR, X_train, y_train, cv = 5) \nLR_cv_score","bac2998f":"LR.coef_","46ce82ff":"LR_predictions = LR.predict(X_test)","8ba83d8c":"LR_RMSE = np.sqrt(mean_squared_error(y_test, LR_predictions))","d0222d5b":"Models_eval = np.array([])\nModels_eval = np.append(Models_eval,[('Linear Regression',LR_RMSE,np.mean(LR_cv_score))])\nModels_eval","f9f3d0d2":"# plotting Linear Regression coefficents\ncoef = pd.Series(LR.coef_, X.columns).sort_values()\ncoef.plot(kind='bar', title='Linear Regression Coefficents')","da72e137":"x_ax = range(len(y_test))\nplt.scatter(x_ax, y_test, label=\"original\")\nplt.scatter(x_ax, LR_predictions, label=\"predicted\")\nplt.title(\"Original and predicted data\")\nplt.legend()\nplt.show()","007dc610":"LR_eval = pd.DataFrame({'Model': 'Linear Regression','RMSE':[LR_RMSE],'CV Score':[np.mean(LR_cv_score)]})\nprint('Model evaluation')\nLR_eval","9ebf4448":"DT = DecisionTreeRegressor(max_depth = 2)\nDT.fit(X_train,y_train)","5e386e89":"DT_params = {'max_depth': range(1,11)}\nDT_grid_search = GridSearchCV(estimator = DT, param_grid = DT_params, cv=10,return_train_score = True )\nDT_grid_search.fit(X_train, y_train)","d3400ce6":"DT_cv_results = pd.DataFrame(DT_grid_search.cv_results_)\nDT_cv_results","ea1217ba":"DT_cv_results[['params','mean_train_score','mean_test_score']]","b1beb128":"plt.plot(DT_grid_search.cv_results_['mean_test_score'],)\nplt.plot(DT_grid_search.cv_results_['mean_train_score'])\nplt.legend(['test score', 'train score'], loc='upper left')\nplt.xlabel('depth')\nplt.ylabel('Accuray')","cb492735":"DT = DT_grid_search.best_estimator_\nDT","0b5ed632":"DT_cv_score = DT_grid_search.best_score_\nDT_cv_score","4812315f":"DT.fit(X_train,y_train)","bd6e0a94":"DT_predictions = DT.predict(X_test)","3c414317":"from sklearn import tree\n\nfig = plt.figure(figsize=(40,40))\n_ = tree.plot_tree(DT, feature_names= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX',\n                                   'PTRATIO', 'B', 'LSTAT'],  \n                   class_names=['MEDV'],\n                   filled=True, node_ids = True)","cd2e4fed":"DT_RMSE = np.sqrt(mean_squared_error(y_test, DT_predictions))","d0da2faa":"Models_eval = np.append(Models_eval,[('Decision Tree',DT_RMSE,np.mean(DT_cv_score))])\nModels_eval","aee00d5e":"coef = pd.Series(DT.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","f9d699ba":"x_ax = range(len(y_test))\nplt.scatter(x_ax, y_test, label=\"original\")\nplt.scatter(x_ax, DT_predictions, label=\"predicted\")\nplt.title(\"Original and predicted data\")\nplt.legend(loc='best')\nplt.show()","b1efb461":"DT_eval = pd.DataFrame({'Model': 'Decision Tree','RMSE':[DT_RMSE],'CV Score':[np.mean(DT_cv_score)]})\nprint('Model evaluation')\nDT_eval","8182a61a":"RF = RandomForestRegressor(n_estimators = 30) ","2dd16ece":"RF_params = {'max_depth': range(2, 10), 'min_samples_split': [2, 4, 6, 8, 10],'n_estimators': range(1, 50) }\nRF_grid_search = GridSearchCV(estimator = RF, param_grid = RF_params, cv=10,return_train_score = True )\nRF_grid_search.fit(X_train, y_train)","ae9e66a5":"RF_cv_results = pd.DataFrame(RF_grid_search.cv_results_)\nRF_cv_results.columns","2206ec0a":"RF_cv_results[['params','param_n_estimators','mean_train_score','mean_test_score']]","bbcf06b4":"plt.plot(RF_grid_search.cv_results_['mean_test_score'],)\nplt.plot(RF_grid_search.cv_results_['mean_train_score'])\nplt.legend(['test score', 'train score'], loc='upper left')\nplt.xlabel('depth')\nplt.ylabel('Accuray')","2a620a58":"RF = RF_grid_search.best_estimator_\nRF","3ab56662":"RF_cv_score = RF_grid_search.best_score_\nRF_cv_score","f389b316":"RF.fit(X_train,y_train)","7f823a21":"RF_predictions = RF.predict(X_test)","d033d47f":"RF_RMSE = np.sqrt(mean_squared_error(y_test, RF_predictions))","d62273a6":"Models_eval = np.append(Models_eval,[('Random Forest',RF_RMSE,np.mean(RF_cv_score))])\nModels_eval","9792a442":"RF_eval = pd.DataFrame({'Model': 'Decision Tree','RMSE':[RF_RMSE],'CV Score':[np.mean(RF_cv_score)]})\nprint('Model evaluation')\nRF_eval","25b8d317":"coef = pd.Series(RF.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","c77f6747":"x_ax = range(len(y_test))\nplt.scatter(x_ax, y_test, label=\"original\")\nplt.scatter(x_ax, RF_predictions, label=\"predicted\")\nplt.title(\"Original and predicted data\")\nplt.legend()\nplt.show()","834d705a":"RF_eval = pd.DataFrame({'Model': 'Random Forest','RMSE':[RF_RMSE],'CV Score':[np.mean(RF_cv_score)]})\nRF_eval","aa2fe40f":"XGB = xgb.XGBRegressor()","3ea4e78f":"XGB_params = {'n_estimators':range(1, 50),'learning_rate':[0.1,0.07],'gamma':[0,0.03,0.1,0.3],'max_depth':[3,5]}","3f011066":"XGB_grid_search = GridSearchCV(estimator = XGB, param_grid = XGB_params, cv=10, return_train_score = True)\nXGB_grid_search.fit(X_train, y_train)","95decae3":"XGB_cv_results = pd.DataFrame(RF_grid_search.cv_results_)\nXGB_cv_results.columns","08d5633b":"plt.plot(XGB_grid_search.cv_results_['mean_test_score'])\nplt.plot(XGB_grid_search.cv_results_['mean_train_score'])\nplt.legend(['test score', 'train score'], loc='upper left')\nplt.xlabel('depth')\nplt.ylabel('Accuray')","7d614208":"XGB = XGB_grid_search.best_estimator_\nXGB","4ef5c52a":"XGB_cv_score = XGB_grid_search.best_score_\nXGB_cv_score","94e575cd":"XGB.fit(X_train,y_train)","53552a00":"XGB_predictions = XGB.predict(X_test)","49de90a5":"XGB_RMSE = np.sqrt(mean_squared_error(y_test, XGB_predictions))","63a87286":"Models_eval = np.append(Models_eval,[('XGB Regression',XGB_RMSE,np.mean(XGB_cv_score))])\nModels_eval","ccb23a9f":"coef = pd.Series(XGB.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","465f12a8":"x_ax = range(len(y_test))\nplt.scatter(x_ax, y_test, label=\"original\")\nplt.scatter(x_ax, XGB_predictions, label=\"predicted\")\nplt.title(\"Original and predicted data\")\nplt.legend()\nplt.show()","71a99a18":"XGB_eval = pd.DataFrame({'Model': 'XGB Regression','RMSE':[XGB_RMSE],'CV Score':[np.mean(XGB_cv_score)]})\nprint('Model evaluation')\nXGB_eval","f272eb58":"Models_eval = Models_eval.reshape(4,3)\nmodels_eval = pd.DataFrame(Models_eval,columns = ['Model','RMSE','CV Score'])\nmodels_eval['RMSE'] = pd.to_numeric(models_eval['RMSE'])\nmodels_eval['CV Score'] = pd.to_numeric(models_eval['CV Score'])\nmodels_eval","712f4874":"f, axe = plt.subplots(1,1, figsize=(10,3))\n\nmodels_eval.sort_values(by=['CV Score'], ascending=False, inplace=True)\n\nsns.barplot(x='CV Score', y='Model', data = models_eval, ax = axe)\naxe.set_xlabel('Cross-Validaton Score', size=16)\naxe.set_ylabel('Model', size=16)\naxe.set_xlim(0,1.0)\nplt.show()","9f5a2f06":"models_eval.sort_values(by=['RMSE'], ascending=False, inplace=True)\n\nf, axe = plt.subplots(1,1, figsize=(10,3))\nsns.barplot(x='Model', y='RMSE', data=models_eval, ax = axe)\naxe.set_xlabel('Model', size=16)\naxe.set_ylabel('RMSE', size=16)\n\nplt.show()","d1e6fa52":"#### We can see that \"RM\" is highly correlated with  target variable. \"LSTAT\" is highly negatively correlated with output. There are also highly correlated features, which are \"RAD\" and \"TAX\". I'll drop one of them.","60c35132":"The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. To train our machine learning model with boston housing dataset, we will try several regression algorithms.","81740a2b":"### Split data ","b39eba25":"# Boston house price prediction","2af70ab5":"### __Try different regression models__","c2883599":"### Dataset information","6374528c":"# Evaluation and comparision of all the models","0931add9":"### XGB regression","14cefb1c":"Boston House Prices Dataset has 506 rows with 14 attributes or features for homes from various suburbs in Boston.\n\n```\nBoston Housing Dataset Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n```","f3cb7ba9":"\n### Feedback is greatly appreciated!","c66e373e":"### Decision Tree Regressor","3b09f8a8":"### Linear Regression","d6e27899":"### Please  <b><font color=\"green\">UPVOTE <\/font><\/b> if you found this kernel useful !\ud83d\udc4d","e23b5f43":"So In this notebook, I have built four regression models using the Boston Housing Dataset. These are linear regression, decision tree regression, random forest regression and XGB regression. Afterward I have visualized calculated the performance measure of the models. Out of which XGB regression is the best suit for this dataset. ","1a8f732a":"### Random Forest Regression "}}