{"cell_type":{"4209a1b0":"code","e33af3e7":"code","6c13e9b7":"code","e7a57781":"code","70556f4d":"code","715fdb08":"code","acde443e":"code","ce17e873":"code","842a46cf":"code","99d7f7f7":"code","7677c7ed":"code","ad98e10e":"code","f69820ba":"code","0abb013c":"code","7ebe2a67":"code","b54bb819":"code","cfc8d74a":"code","b4778108":"code","b8b41d80":"code","ec1a26d1":"code","13c50529":"code","b47eba21":"code","933d8982":"code","1c594705":"code","dfaed0c0":"code","8420ef38":"code","a8ccaab7":"code","7b5fc929":"code","fbfffa75":"code","29db1266":"code","3009b13c":"code","adf06ff7":"code","1688c314":"code","d9618de4":"code","50ee216d":"code","e10800d6":"code","1e0e0903":"code","bd32589d":"code","8ae4808f":"code","0cdaa830":"code","214313ea":"code","5d074bdc":"code","9f92ffc3":"code","99efe94c":"code","d2a0c670":"code","4f410bd9":"code","e3e24b4c":"code","ba68ca04":"code","e4044432":"code","04a8569d":"code","4680945e":"code","737e884d":"code","5203b126":"code","d872933f":"code","def17bc4":"code","c28b5879":"code","0c9ac9f2":"code","58e032ca":"code","ec39a94c":"code","03183561":"code","9c921ac9":"code","70d69eee":"code","297ed52b":"code","a209c1bf":"code","0163b362":"code","46aa6b77":"code","6ecc9f19":"code","136b91fc":"code","9dbaec2e":"code","5729bf27":"code","198a3dc2":"code","f571bf8c":"code","b52c960f":"code","af207702":"code","3ca6b2d1":"code","97b70fe6":"code","146430b5":"code","f055002f":"code","0d647385":"code","01fbbfac":"code","b54a10f3":"code","fa442045":"code","01260916":"code","e474c2f3":"code","e4935e34":"code","b4deeed6":"code","f4da3b32":"code","de63bc95":"code","236bbeff":"code","301ecd6b":"code","498b04c4":"code","054aafb8":"code","7c7b8a75":"code","04c052d9":"code","3e115206":"markdown","6e8a93fa":"markdown","961d76db":"markdown","d3200594":"markdown","a371f0f6":"markdown","442c978a":"markdown","84199d82":"markdown","404e0f5e":"markdown","ac6340b6":"markdown","2f51efc2":"markdown","ac9b3ac1":"markdown","cc8204fa":"markdown","5839b356":"markdown","3574afa0":"markdown","34d8a25e":"markdown","e3477dbb":"markdown","ca9c90fb":"markdown","ef869dd9":"markdown","eff4d838":"markdown","aebaa642":"markdown","239fa88e":"markdown","9f2a5db8":"markdown","dc19b3ea":"markdown","30a1650f":"markdown","fcf791f5":"markdown","d3ade02b":"markdown","8567c47f":"markdown","d5418886":"markdown","a3097806":"markdown","a0fb64be":"markdown","3aa89b30":"markdown","1fa4844a":"markdown"},"source":{"4209a1b0":"# Import all the tools we need\n\n# Regular EDA ( exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Models from scikit-learn\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.ensemble import RandomForestClassifier\n\n# Model evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","e33af3e7":"# Load the data (training and validation sets)\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\", low_memory=False)","6c13e9b7":"df.head()","e7a57781":"df.info()","70556f4d":"df.isna().sum()","715fdb08":"df.columns","acde443e":"fig, ax = plt.subplots()\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000]);","ce17e873":"# plot histogram to see distribution of sales price\ndf.SalePrice.plot.hist();","842a46cf":"df.saledate.dtype","99d7f7f7":"# Import the data again but this time parse dates\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n                 low_memory=False,\n                 parse_dates= [\"saledate\"])","7677c7ed":"df.saledate.dtype","ad98e10e":"df.saledate[:1000]","f69820ba":"fig, ax = plt.subplots(figsize=(8,5))\nax.scatter(df.saledate[:1000], df.SalePrice[:1000]);","0abb013c":"df.saledate.head(20)","7ebe2a67":"# Sort dataframe by date\ndf.sort_values(by = [\"saledate\"], inplace=True, ascending=True)\ndf.saledate.head(20)","b54bb819":"df.head()","cfc8d74a":"# make a copy\ndf_tmp = df.copy()","b4778108":"df_tmp.head()","b8b41d80":"df_tmp.saledate.head()","ec1a26d1":"df[:5].saledate","13c50529":"df[:5].saledate.dt.year  #dt - accessing datetime index","b47eba21":"df_tmp[\"saleYear\"]  =df_tmp.saledate.dt.year\ndf_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\ndf_tmp[\"saleDay\"] = df_tmp.saledate.dt.day\ndf_tmp[\"saleDayOfWeek\"] = df_tmp.saledate.dt.dayofweek\ndf_tmp[\"saleDayOfYear\"] = df_tmp.saledate.dt.dayofyear","933d8982":"df_tmp.head().T","1c594705":"# Now we've enriched our dataframe with date time features, we can remove 'saledate' column\ndf_tmp.drop(\"saledate\", axis=1, inplace=True)","dfaed0c0":"# Checking the values of different columns\ndf_tmp.state.value_counts()","8420ef38":"df_tmp.isna().sum()","a8ccaab7":"df_tmp.info()","7b5fc929":"df_tmp.UsageBand.dtype == \"object\"","fbfffa75":"pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])","29db1266":"# Find the columns which contain strings\n\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","3009b13c":"# Find the columns which contain strings\nfor label in df_tmp.keys():\n    if df_tmp[label].dtype == \"object\":\n        print(label)","adf06ff7":"df_tmp.info()","1688c314":"# this will turn all of the string values into category values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype(\"category\").cat.as_ordered()\n        ","d9618de4":"df_tmp.info()","50ee216d":"df_tmp.state  # ordered categories","e10800d6":"type(df_tmp.state)","1e0e0903":"df_tmp.state.cat.categories","bd32589d":"df_tmp.state.cat.codes","8ae4808f":"d = dict(enumerate(df_tmp.state.cat.categories)) # Returns the state codes with their respective categories\nprint (d)","0cdaa830":"print(dict(enumerate(df_tmp.Hydraulics.cat.categories)))","214313ea":"# Check missing values\ndf_tmp.isnull().sum()\/len(df_tmp)","5d074bdc":"df_tmp.isnull().sum()","9f92ffc3":"# Finding numerical columns\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","99efe94c":"df_tmp.ModelID","d2a0c670":"# Check for which numeric columns have null values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","4f410bd9":"# Fill numeric rows with median\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Add a binary column which tells us if the data was missing or not\n            df_tmp[label+\"_is_missing\"]  = pd.isnull(content)\n            # Fill missing numeric values with median\n            df_tmp[label] = content.fillna(content.median())  # median is more robust than mean (susceptible to outliers)","e3e24b4c":"# Check if there's any null numeric values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","ba68ca04":"# Check to see how many examples were missing\ndf_tmp.auctioneerID_is_missing.value_counts()","e4044432":"# Check for column which aren't numeric\n\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","04a8569d":"pd.Categorical(df_tmp['state']).codes","4680945e":"pd.Categorical(df_tmp.UsageBand).codes","737e884d":"df_tmp.UsageBand.cat.codes","5203b126":"# turn categorical variables into numbers and fill missing\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # add binary column to indicate whether sample had missing value\n        df_tmp[label +\"_is_missing\"] = pd.isnull(content)\n        # Turn categories into numbers and add +1\n        df_tmp[label] = pd.Categorical(content).codes+1 # +1 because pandas has assigned code -1 to missing values. We want it to be 0.        ","d872933f":"df_tmp.isna().sum()","def17bc4":"df_tmp.info()","c28b5879":"len(df_tmp)","0c9ac9f2":"%%time\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model\nmodel = RandomForestRegressor(random_state=42)\n\n# Fit the model\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","58e032ca":"model.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","ec39a94c":"df_tmp.saleYear.value_counts()","03183561":"# Split data into train and validation sets\ndf_valid = df_tmp[df_tmp.saleYear == 2012]\ndf_train = df_tmp[df_tmp.saleYear != 2012]\n\nlen(df_valid), len(df_train)","9c921ac9":"# Split data into x and y\nx_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train[\"SalePrice\"]\nx_valid, y_valid = df_valid.drop(\"SalePrice\", axis=1), df_valid[\"SalePrice\"]\n\nx_train.shape, y_train.shape, x_valid.shape, y_valid.shape","70d69eee":"y_train","297ed52b":"# Create evaluation function (kaggle competition uses RMSLE)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    \"\"\"\n    Calculates root mean squared log error between predictions\n    and true labels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create function to evaluate model on few different levels\ndef show_scores(model):\n    train_preds = model.predict(x_train)\n    valid_preds = model.predict(x_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Validatin MAE\": mean_absolute_error(y_valid, valid_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Validation RMSLE\": rmsle(y_valid, valid_preds),\n              \"Training R^2\": r2_score(y_train, train_preds),\n              \"Validation R^2\": r2_score(y_valid, valid_preds)}\n    return scores","a209c1bf":"# This take far too long...for experimenting\n\n# %%timeit\n\n# model = RandomForestRegressor()\n# model.fit(x_train, y_train)","0163b362":"# Change max_samples value\nmodel = RandomForestRegressor(random_state=42,\n                              max_samples=10000)\nmodel","46aa6b77":"%%time\n# Cutting down on the max number of samples each estimator can see improves training time.\nmodel.fit(x_train, y_train)","6ecc9f19":"show_scores(model)","136b91fc":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# different Random Forest regressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10,100,10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2,20,2),\n           \"min_samples_leaf\": np.arange(1,20,2),\n           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n\n# Insatntiate RandomizedSearchCV model\nrs_model = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n                              param_distributions=rf_grid,\n                              n_iter=2,\n                              verbose=True)\n\n# Fit the randomizedSearchCV model\nrs_model.fit(x_train, y_train) ","9dbaec2e":"# find the best hyperparameters\nrs_model.best_params_","5729bf27":"# evaluate the model\nshow_scores(rs_model)","198a3dc2":"%%time\n\n# Most ideal hyperparameters\nideal_model = RandomForestRegressor(n_estimators=40,\n                                    min_samples_split=14,\n                                    min_samples_leaf=1,\n                                    max_features=0.5, \n                                    max_samples=None,\n                                    random_state=42) \n\n# fit the ideal model\nideal_model.fit(x_train, y_train)","f571bf8c":"# evaluate the ideal model\nshow_scores(ideal_model)","b52c960f":"# Import the test data\ndf_test = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/Test.csv\",\n                      low_memory=False,\n                      parse_dates= [\"saledate\"])\ndf_test.head()","af207702":"df_test.shape","3ca6b2d1":"df_test.isna().sum()","97b70fe6":"df_test.info()","146430b5":"df_test.columns","f055002f":"x_train.columns","0d647385":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    # Add datetime parameters to `saledate` column\n    df[\"saleYear\"]  =df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n\n    # Now we've enriched our dataframe with date time features, we can remove 'saledate' column\n    df.drop(\"saledate\", axis=1, inplace=True) \n    \n    for label, content in df.items():\n        # Fill numeric rows with median\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                #a binary column which tells us if the data was missing or not\n                df[label+\"_is_missing\"]  = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n\n        # turn categorical variables into numbers and fill missing\n        if not pd.api.types.is_numeric_dtype(content):\n            # add binary column to indicate whether sample had missing value\n            df[label +\"_is_missing\"] = pd.isnull(content)\n            # Turn categories into numbers and add +1\n            df[label] = pd.Categorical(content).codes+1\n\n    return df  ","01fbbfac":"# Process the test data\ndf_test = preprocess_data(df_test)\ndf_test.head()","b54a10f3":"df_test.isna().sum()","fa442045":"df_test.info()","01260916":"x_train.shape[1]","e474c2f3":"# make predictions on test data\n# test_preds = ideal_model.predict(df_test)","e4935e34":"# we can find how the columns differ using sets\nset(x_train.columns) - set(df_test.columns)","b4deeed6":"# manually adjust df_test to have 'auctioneerID_is_missing' column\ndf_test[\"auctioneerID_is_missing\"] = False\ndf_test.head()","f4da3b32":"# make predictions on test data\ntest_preds = ideal_model.predict(df_test)\ntest_preds","de63bc95":"len(test_preds)","236bbeff":"# format predictions\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalePrice\"] = test_preds\ndf_preds","301ecd6b":"# Export prediction data to csv\ndf_preds.to_csv(\"test-predictions.csv\", index=True)","498b04c4":"# find feature importance of best model\nideal_model.feature_importances_","054aafb8":"x_train.columns","7c7b8a75":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importance\"][:n])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature Importance\")\n    ax.invert_yaxis()","04c052d9":"plot_features(x_train.columns, ideal_model.feature_importances_)","3e115206":"### Filling and turning categorical data into numbers","6e8a93fa":"Finally, our test dataframe has same features as training dataframe and we can make predictions!","961d76db":"### Train a model with best hyperparameters\n\n**Note:** These were found after 100 iterations of `RandomizedSearchCV`","d3200594":"By default pandas assigns `code` = -1 for rows with missing values (for any column).","a371f0f6":"**Question:** Why doesn't the above metric hold water? ( why isn't the metric reliable) - Because model is evaluated on the same data that it has been trained on.","442c978a":"Our dataset contains non-numeric data as well as many columns have missing data. Thus, we need to convert non-numeric data to numeric and handle missing values before building a model.","84199d82":"### Testing our model on a subset (to tune the hyperparameters)","404e0f5e":"## 5. Modelling\n\nLet's start to do some model-driven EDA.","ac6340b6":"## Fill missing values\n\n### Fill numerical missing values","2f51efc2":"### Parsing Dates\n\nWhen we work with time series data, we want to enrich the time & date component as much as possible.\n\nWe can do that by telling pandas which of our columns has dates in it using the `parse_dates` paramter.","ac9b3ac1":"Now that all of data is numeric as well as our dataframe has no missing values, we should be able to build a machine learning model.","cc8204fa":"Now all of our data can be accessed as numbers (thanks to pandas `categories`!).\n\nBut we still have a bunch of missing data..","5839b356":"### Convert strings to categories - Label Encoding\n\nOne way we can turn all of our data into numbers is by converting them into pandas categories.\n\nPandas categories: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/categorical.html\n\nDifferent datatypes compatible with pandas: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.api.types.is_categorical_dtype.html\n\nHelpful resource: https:\/\/towardsdatascience.com\/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd","3574afa0":"Format predictions into the same format kaggle is asking for:\n* Have a header: \"SalesID,SalePrice\"\n* Contain two columns\n\n  **SalesID**: SalesID for the validation set in sorted order\n  \n  **SalePrice**: Your predicted price of the sale","34d8a25e":"# Predicting the sale price of Bulldozers using machine learning\n\nThis notebook goes through an example machine learning project with the goal of predicting sale price of bulldozers.\n\n## 1. Problem Definition\n\n> How well can we predict the future sale price of a bulldozer, given it's characteristics and previous examples of how much similar bulldozers have been sold for?\n\n\n## 2. Data\n\nThe data is downloaded from Kaggle's \"Bluebook for Bulldozers\" competition: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data\n\nThere are 3 main datasets:\n\n* **Train.csv** is the training set, which contains data through the end of 2011.\n* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n* **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\n## 3.Evaluation\n\nThe evaluation metric for this competition is the **RMSLE (root mean squared log error)** between the actual and predicted auction prices.\n\nThe goal for most regression evaluation metrics is to **minimize the error**. \n\n## 4. Features\n\nKaggles provides a data dictionary detailing all of the features of the dataset: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data?select=Data+Dictionary.xlsx\n","e3477dbb":"### Feature Importance\n\nFeature Importance seeks to figure out which different attributes of data are most important when it comes to predicting the **target variable** (salePrice). ","ca9c90fb":"### Preparing the tools\n\nWe're going to use:\n* pandas for data analysis.\n* NumPy for numerical operations.\n* Matplotlib\/seaborn for plotting or data visualization.\n* Scikit-Learn for machine learning modelling and evaluation.","ef869dd9":"dtype of `object` is changed to `category` type","eff4d838":"### Add datetime parameters to `saledate` column","aebaa642":"### Preprocessing the data (getting the test data in same format as train dataset)","239fa88e":"### Building an evaluation function","9f2a5db8":"## Make predictions on test data","dc19b3ea":"There is a significant decrease in Validation `RMSLE` in ideal mdel (trained on all data with best hyperparameters) from the previous randomizedSearchCVA model trained on ~10000 samples.","30a1650f":"### Hyperparameter tuning with RandomizedSearchCV","fcf791f5":"### Sort Dataframe by saledate\n\nWhen working with time series data, it's good idea to sort it by date.","d3ade02b":"### Make a copy of the original dataframe\n\nWe make a copy of the original dataframe so when we manipulate the copy, we've still got our original data.","8567c47f":"Splitting data into train\/validation sets can be done on the basis of sale year 2012. Everything sold in 2012 will be part of validation set and everything sold before 2012 will be part of training set.","d5418886":"Test data has missing values as well as non-numeric columns.\nAlso, test data has only 52 columns whereas x_train (data on which model is trained) has 102 columns.\n\nTherefore, our ideal model can't predict on the test data directly as it is not in the same format as the data model has been trained on.","a3097806":"`.cat` is used to access the category.\n\n`cat.as_ordered()` - this means each columns that gets turned into a category will have an assumed order, and corresponding min(), max(), etc..\n\nIf categorical data is ordered (s.cat.ordered == True), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, .min()\/.max() will raise a TypeError.","a0fb64be":"This means that test data did not have any missing values for `auctioneerID` column and hence `auctioneerID_is_missing` column was not created.","3aa89b30":"There is a mismatch between number of columns in training dataset and test dataset.","1fa4844a":"`cat.categories` - returns the categories of this categorical. The assigned value has to be a list-like object. \n\nSince the categories are ordered, under the hood, pandas has assigned numerical values\/codes (order) to the items in each category. The codes can be accessed using `cat.codes`"}}