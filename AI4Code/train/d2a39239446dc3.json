{"cell_type":{"da19fb75":"code","54dc7ecd":"code","101a7c14":"code","4797f5ac":"code","294d0714":"code","25cc3f13":"code","1b6e2dba":"code","2da554a2":"code","fc2a760a":"code","fa6a5cdf":"code","bc2087ef":"code","391727c7":"code","29c7f465":"code","6fe4e880":"code","02289df2":"code","692ab505":"code","671e5aa3":"code","63476be1":"code","33c2ca60":"code","ae74654d":"markdown","61b19ab1":"markdown","b47745d3":"markdown","762b66a3":"markdown","2ef79cb8":"markdown","9221b8eb":"markdown","ecdf8fce":"markdown","26c3e53b":"markdown","f7b012a3":"markdown","b946dc2d":"markdown","76bf82ba":"markdown","18c5742f":"markdown","091a3bae":"markdown","d380ba9b":"markdown","e9fd76ef":"markdown","2eda702d":"markdown","7939c316":"markdown","777ecaff":"markdown","bc2db218":"markdown","dfe2b1d2":"markdown","876fea30":"markdown","3be6572f":"markdown","b6a98f70":"markdown","5911f186":"markdown","cedc1078":"markdown","5ebc38da":"markdown"},"source":{"da19fb75":"import json\nimport zipfile\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 2)\n\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as imbPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import (confusion_matrix, plot_confusion_matrix,\n                             plot_precision_recall_curve)\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tqdm.keras import TqdmCallback\ntf.random.set_seed(42)\nprint(tf.__version__)","54dc7ecd":"seed = 1\naccuracy = 'average_precision' # equals to area under recall precision curve\nlr_max_iterations = 10000 # increased max_iter to allow lbfgs-solver converging\nresults = []","101a7c14":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\nassert(df.shape[0] == 284807) # make sure the data is loaded as expected\nassert(df.shape[1] == 31)","4797f5ac":"X_orig = df.drop('Class', axis=1)\ny_orig = df.Class\n\nX_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.2,\n                                                    random_state=seed, stratify=y_orig)\ntest_data = [X_test, y_test]\n\nprint(f\"Training data class counts:\\n{y_train.value_counts()}\")\nprint('')\nprint(f\"Test data class counts:\\n{y_test.value_counts()}\")\nprint('')\nassert(y_test.shape[0]\/y_orig.shape[0] > 0.19)","294d0714":"scaler = StandardScaler() \nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ntest_data_scaled = [X_test_scaled, y_test]","25cc3f13":"def evaluate_model(test_data, estimator, name='Not specified'):\n    \n    X_test = test_data[0]\n    y_test = test_data[1]\n    \n    num_fraud_cases_in_test = len(y_test[y_test==1])\n    num_normal_cases_in_test = len(y_test[y_test==0])\n    \n    predictions = estimator.predict(X_test)\n    cm = confusion_matrix(y_test, predictions)\n    \n    # Plot normalized confusion matrix and precision recall curve\n    fig, axes = plt.subplots(1,2, figsize=(14,6))\n    \n    plot_confusion_matrix(estimator, X_test, y_test, normalize='true',\n                          display_labels=['Normal', 'Fraud'], cmap='Greens', values_format=\".4f\", ax=axes[0])\n    axes[0].set_title('Confusion Matrix (normalized)')\n    \n    prc = plot_precision_recall_curve(estimator, X_test, y_test, name=name, ax=axes[1])\n    axes[1].set_title('Precision Recall Curve')\n    plt.tight_layout()\n    \n    # Print summary    \n    print(f\"Classified \\t{cm[1,1]} out of {num_fraud_cases_in_test} \\tfraud cases correctly\")\n    print(f\"Misclassified \\t{cm[0,1]} out of {num_normal_cases_in_test} normal cases\")\n    print(f\"Average preicision score is {prc.average_precision:.4f}\")\n    \n    return [name, cm[1,1], cm[1,1]\/num_fraud_cases_in_test*100, cm[0,1], prc.average_precision]","1b6e2dba":"simple_LR = LogisticRegression(max_iter=lr_max_iterations, random_state=seed) \nsimple_LR.fit(X_train_scaled, y_train)\nres = evaluate_model(test_data_scaled, simple_LR, name='Logistic Regression Baseline')\nresults.append(res)","2da554a2":"X_train_0 = X_train[y_train == 0]\nX_train_AE = X_train_0.sample(frac=0.5, random_state=seed)\nX_train_AE_scaled = StandardScaler().fit_transform(X_train_AE)\n\nX_train_est = X_train.drop(X_train_AE.index)\ny_train_est = y_train.drop(X_train_AE.index)\n\nlatent_scaler = StandardScaler()\nX_train_est_scaled = latent_scaler.fit_transform(X_train_est)\nX_test_est_scaled = latent_scaler.transform(X_test)","fc2a760a":"def create_autoencoder(input_dim=30, latent_dim=50):\n    \"\"\" Creates an Autoencoder Model where input_dim is the number of features.\n    The encoding part uses L1-regularization as sparsity constraint \"\"\"\n    \n    input_layer = Input(shape=(input_dim,), name='Input')\n    encoded = Dense(100, activation='relu', activity_regularizer=regularizers.l1(10e-5), name='Encoding')(input_layer)\n    latent =  Dense(latent_dim, activation='relu', name='Latent')(encoded)\n    decoded = Dense(100, activation='relu', name='Decoding')(latent)\n    output_layer = Dense(input_dim, activation='linear', name='Output')(decoded)\n    \n    autoencoder = Model(input_layer, output_layer)\n    return autoencoder","fa6a5cdf":"autoencoder = create_autoencoder()\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\nautoencoder.summary()\n    \nhistory = autoencoder.fit(X_train_AE_scaled, X_train_AE_scaled,\n                          batch_size=64, epochs=500, verbose=0, validation_split=0.15,\n                          callbacks=[TqdmCallback(), EarlyStopping(patience=3)])","bc2087ef":"plt.figure(figsize=(12,8))\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.ylabel('MSE')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.title(\"Autoencoder Training History\")\nplt.show()","391727c7":"def create_encoder(autoencoder):\n    encoder = Sequential([autoencoder.layers[0],\n                          autoencoder.layers[1],\n                          autoencoder.layers[2]])\n    return encoder","29c7f465":"encoder = create_encoder(autoencoder)\nencoder.summary()","6fe4e880":"X_train_latent = encoder.predict(X_train_est_scaled)\nX_test_latent = encoder.predict(X_test_est_scaled)\ntest_data_latent = [X_test_latent, y_test]","02289df2":"log_reg_cv = LogisticRegressionCV(Cs=5, scoring=accuracy, max_iter=lr_max_iterations, n_jobs=-1, random_state=seed)\nlog_reg_cv.fit(X_train_latent, y_train_est)\nres = evaluate_model(test_data_latent, log_reg_cv, name='Autoencoder')\nresults.append(res)","692ab505":"oversampler = SMOTE(sampling_strategy='minority', n_jobs=-1, random_state=seed)\nlog_reg_os  = LogisticRegressionCV(Cs=5, scoring=accuracy, max_iter=lr_max_iterations, n_jobs=-1, random_state=seed)\n\npipeline = imbPipeline([\n    ('sampler', oversampler),\n    ('transformer', StandardScaler()),\n    ('classification', log_reg_os)])\n\npipeline.fit(X_train, y_train)\n\nres = evaluate_model(test_data, pipeline, name='Oversampling')\nresults.append(res)","671e5aa3":"undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=seed)\nlog_reg_us   = LogisticRegressionCV(Cs=5, scoring=accuracy, max_iter=lr_max_iterations, n_jobs=-1, random_state=seed)\n\npipeline = imbPipeline([\n        ('sampler', undersampler),\n        ('transformer', StandardScaler()),\n        ('estimator', log_reg_us)])\n\npipeline.fit(X_train, y_train)\n\nres = evaluate_model(test_data, pipeline, 'Undersampling')\nresults.append(res)","63476be1":"combined_sampler = SMOTETomek(n_jobs=-1, random_state=seed)\nlog_reg_comb   = LogisticRegressionCV(Cs=5, scoring=accuracy, max_iter=lr_max_iterations, n_jobs=-1, random_state=seed)\n\npipeline = imbPipeline([\n        ('sampler', combined_sampler),\n        ('transformer', StandardScaler()),\n        ('estimator', log_reg_comb)])\n\npipeline.fit(X_train, y_train)\n\nres = evaluate_model(test_data, pipeline, 'Combined Sampling')\nresults.append(res)","33c2ca60":"pd.DataFrame(results,\n             columns=[\"Model\", \"True Negatives (TN)\", \"TN in %\", \"False Positives (FP)\", \"AUPRC\"]\n            ).set_index(\"Model\")","ae74654d":"# Logistic Regression Baseline\nLet us create a simple Logistic Regression model to see how things go. We will not compare multiple models but rather focus on dealing with the imbalance in this dataset. To estimate the models performance we use [average precision score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) because it equals the Area Under the Precision-Recall Curve. The function *evaluate_model* will provide the results and plot a confusion matrix as well as a precision recall curve.","61b19ab1":"### Encoder Model and Latent Representation\nThe encoder part of the Autoencoder consists of all the layers from the input layer up to the latent layer, which in this case are the first three layers. By predicting the compressed representation of the data one can easily extract the latent data for classification.","b47745d3":"# Data Preparation","762b66a3":"Combined sampling yields almost the same results as oversampling. It performs slightly better, classifying one additional normal case correctly. The change in average precision is minor with now 0.8047.","2ef79cb8":"### Approach and Evaluation Metric\nA Logistic Regression will be performed after applying different techniques for dealing with imbalanced data to the data set. A couple of options have been considered for dealing with the imbalance problem and the following five are implemented now:\n\n1. **Logistic Regression:** this will show the performance on the imbalanced data and serves as a baseline for further comparison.\n2. **Autoencoder:** used to extract a latent representation of the training data. Compression of the data might improve the inherent information.\n3. **Over-sample the data:** over-sampling will yield balanced classes but with a lot of data that makes training computationally expensive.\n4. **Under-sample the data:** under-sampling will yield balanced classes but with the cost of loss of information.\n5. **Combined Sampling:** use SMOTE and Tomek links to combine both over- and under-sampling techniques.\n\nAs proposed by the provider of this dataset, all models will be evaluated using the **Area Under the Precision-Recall Curve (AUPRC)** score, because confusion matrix accuracy is not meaningful for unbalanced classification. Sklearn provides this with score in the *average_precision_score* function, also accessible via 'average_precision'.","9221b8eb":"### Import Data and Split into Train and Test Set\nFor splitting we use sklearns *train_test_split* with the stratify option in order to keep the ratio of normal and fraudulent transactions in the test and training data equal.","ecdf8fce":"## Introduction\n\n- **Context:** In order to enhance security, credit card companies would like to detect fraudulent transactions. Machine Learning can help with this.\n- **Limitations:** It is relatively simple to increase the number of correctly classified fraudulent transactions at the cost of misclassifying normal transactions. Weighing the economic costs of misclassifying normal and fraudulent transactions is a business decision and therefore beyond the scope of this analysis.\n- **The Dataset:** The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.","26c3e53b":"Please feel free to leave a comment about this kernel or your ideas for dealing with the imbalance in this dataset. I am sure that the score of 0.83 AURPC can be improved quite a bit! :)","f7b012a3":"### Classification\nNow that the latent representation has been extracted the data can be classified, again using a Linear Regression model. This time the model will be optimized using 5-fold cross-validation.","b946dc2d":"# Conclusion\nTo compare the results we can convert the list of results to a DataFrame:","76bf82ba":"### Oversampling","18c5742f":"Classifying on the latent representations leads to an improvement of ~10% points for the fraudulent transactions. A few more normal transactions have been misclassified but it is almost negligible. The average precision is now 0.8335, so only a little bit better than the baseline.","091a3bae":"# Autoencoder","d380ba9b":"# Sampling Methods\nBy sampling we understand the draw of an example - in this case a credit card transaction - from the distribution of known examples. In case of SMOTE (oversampling) this is more or less what happens. Most under-sampling methods on the other hand remove examples of the majority class, reducing it to the number of examples in the minority class. These methods can also be combined.\n\nOne important thing when using sampling methods combined with cross-validation is to avoid data leakage. In order to do this, it is essential to sample during cross-validation and not before. Luckily the imbalanced-learn package provides us with a pipeline that takes care of exactly that. The picture below shows the framework that is applied here.\n\n![A framework for using sampling methods with cross-validation](attachment:sampling-framework.PNG)\n\nAll the sampling methods are from the [imbalanced-learn](https:\/\/imbalanced-learn.org\/stable\/index.html) package.","e9fd76ef":"## Prerequisites","2eda702d":"### Standardizing (and avoiding Data Leakage)\nThe dataset contains two features ('Amount' and 'Time') that are on a totally different scale than the rest of the features. who are the result of a Principal Component Analysis). In theory it isn't required to standardize data for Logistic Regression (e.g. [here](https:\/\/stats.stackexchange.com\/questions\/48360\/is-standardization-needed-before-fitting-logistic-regression)). Nonetheless, tests have shown better performance of Logistic Regression when the data has been standardized in advance and because of that the data here will be scaled.\n\nHowever, we did not scale the data before the split. In order to avoid information from the training data [leaking](https:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/) into the test-set, this is done afterwards. Data Scientists far too often neglect the effects of data leakage, scaling their whole dataset before the model training. This may improve the models performance in an unwanted manner and result in worse accuracy when dealing with data in production. \n\nHere is a simple \"recipe\" for standardization:\n - Create the [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) object\n - Fit scaler on the training data and transform the data\n - Transform the test data without fitting the scaler again","7939c316":"Applying the model to the under-sampled data detects one more fraud case than with oversampling, the score now being at almost 93%. But the increase in misclassified normal transactions is huge with around 7% leading to a decrease in average precision to merely 0.6490.","777ecaff":"### Import Packages and Set Global Variables","bc2db218":"The baseline model does not perform too bad. It already identifies 70% of frauds and classifies 99,98% of normal transactions correctly. The area under the precision-recall-curve or average precision is 0.8136.","dfe2b1d2":"### Undersampling","876fea30":"With oversampling the model identifies almost 92% of the fraudulent transactions correctly. On the downside almost 1% of the normal cases are misclassified. The average precision dropped 0.8046, the worst so far.","3be6572f":"### Autoencoder Model\nThe Autoencoder consists of 5 layers: an input layer and the encoding part, the latent representation itself and the decoder part as well as an output layer. It is up to discussion what fraction of the data should be used for Autoencoder training and respectively kept for the Linear Regression. Furthermore, no optimization has been performed on this model. As we will see later, it has a nice and converging training curve and does the job.","b6a98f70":"### Combinded Sampling","5911f186":"It is noticeable that the classifier seems to identify fraudulent transactions way easier when sampling methods have been applied (69-77 TN vs. 90-91 TN) but on the other hand it gets a lot more normal transactions wrong (10-15 FP vs 531-1389 FP).\n\nLooking at the Area Under the Precision-Recall Curve, all sampling models are worse than the baseline (0.65 and 0.80 < 0.81) whereas the Autoencoder clearly has the best score with 0.83. All in all, the Autoencoder seems to be the most balanced approach to tackle this problem and can certainly be optimized.","cedc1078":"# Data Preparation for Imbalanced Data: Credit Card Fraud Detection\n\n**TL;DR** This notebook evaluates the performance of a simple Logistic Regression on the imbalanced Credit Card Fraud data from kaggle. The focus lies on the data preparation: **Autoencoder, Oversampling (SMOTE), Random Under-sampling and a Combined Sampling method** are used to transform the data before applying the model. Because it is a common mistake to apply data transformations in a way that leads to data leakage, this topic is also covered. In general, classification on sampled datasets achieves higher accuracy in classifying fraudulent transactions while it gets more normal transactions wrong, whereas the Logistic Regression on the latent data from the Autoencoder seems to be a more balanced approach, achieving the highest average precision.","5ebc38da":"### Prepare Data for Autoencoder\nFor training the Autoencoder only needs samples of the majority class because it is supposed to learn a compressed representation of those. Therefore the training data set is separated by class and a fraction of the normal samples are used for the Autoencoder training. To improve convergence they are standardized as well.\nThe samples showed to the Autoencoder should not be used to train the Logistic Regression model. Therefore, a new training set for the estimator is created."}}