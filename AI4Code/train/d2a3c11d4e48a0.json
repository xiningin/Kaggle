{"cell_type":{"e9803791":"code","5446f1af":"code","1e95d712":"code","f4986c34":"code","7d44e711":"code","7fd76287":"code","0b86afad":"code","2ceaa2ae":"code","6712b2e7":"code","25fe94de":"code","3f8c2f5e":"code","ae2c8d27":"code","41691e35":"code","6780cddd":"code","8a32b6b4":"code","9438fd51":"code","82c8dc91":"code","e7594d2e":"code","6d5bd67b":"code","fffaf030":"code","1fc73890":"code","761cea48":"code","b40105c5":"code","32874a63":"markdown","b5e8be60":"markdown","c0aa0f57":"markdown","17104d0c":"markdown","c1b22102":"markdown","308a2173":"markdown","b9a22017":"markdown","7b408021":"markdown","ab4b6d2a":"markdown","07637adb":"markdown","e0013e0f":"markdown","b802d231":"markdown"},"source":{"e9803791":"!git clone https:\/\/github.com\/huggingface\/transformers.git\n!pip3 install --upgrade .\/transformers","5446f1af":"! pip install emoji","1e95d712":"import os\nimport random\nimport time\nimport datetime\nimport torch\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom torch.nn import functional as F\nfrom transformers import (get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer,\n                            AutoModelForSequenceClassification)\nfrom torch.utils.data import (TensorDataset,DataLoader,\n                             RandomSampler, SequentialSampler, Dataset)","f4986c34":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","7d44e711":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","7fd76287":"train_df","0b86afad":"def bert_encode(df, tokenizer):\n    input_ids = []\n    attention_masks = []\n    for sent in df[[\"text\"]].values:\n        sent = sent.item()\n        encoded_dict = tokenizer.encode_plus(\n                            sent,                      \n                            add_special_tokens = True, \n                            max_length = 128,           \n                            pad_to_max_length = True,\n                            truncation = True,\n                            return_attention_mask = True,   \n                            return_tensors = 'pt',    \n                    )\n           \n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    inputs = {\n    'input_word_ids': input_ids,\n    'input_mask': attention_masks}\n\n    return inputs","2ceaa2ae":"def prepare_dataloaders(train_df,test_df,batch_size=8):\n    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"vinai\/bertweet-base\", use_fast=False, normalization=True)\n    \n    tweet_train = bert_encode(train_df, tokenizer)\n    tweet_train_labels = train_df.target.astype(int)\n    \n    tweet_test = bert_encode(test_df, tokenizer)\n\n    input_ids, attention_masks = tweet_train.values()\n    labels = torch.tensor(tweet_train_labels.values)\n    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n\n    \n    input_ids, attention_masks = tweet_test.values()\n    test_dataset = TensorDataset(input_ids, attention_masks)\n\n    \n    train_dataloader = DataLoader(\n                train_dataset,\n                sampler = RandomSampler(train_dataset), \n                batch_size = batch_size \n            )\n\n\n    test_dataloader = DataLoader(\n                test_dataset, \n                sampler = SequentialSampler(test_dataset), \n                batch_size = batch_size\n            )\n    return train_dataloader, test_dataloader","6712b2e7":"train_dataloader,test_dataloader = prepare_dataloaders(train_df, test_df)\n","25fe94de":"def test_encode(sentence):\n    tokenizer = AutoTokenizer.from_pretrained(\"vinai\/bertweet-base\", use_fast=False, normalization=True)\n\n    encoded_dict = tokenizer.encode_plus(\n                        sentence,                      \n                        add_special_tokens = True, \n                        max_length = 128,           \n                        pad_to_max_length = True,\n                        truncation = True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',    \n                )\n           \n    return encoded_dict['input_ids']","3f8c2f5e":"def test_decode(tokens):\n    tokenizer = AutoTokenizer.from_pretrained(\"vinai\/bertweet-base\", use_fast=False, normalization=True)\n    return tokenizer.convert_ids_to_tokens(tokens)","ae2c8d27":"train_df.text[0]","41691e35":"text_test = train_df.text[0]\ntext_preprocessed = test_encode(text_test)\n\n\nprint(f'Shape      : {text_preprocessed.shape}')\nprint(f'Word Ids   : {text_preprocessed[0, :128]}')\nprint(test_decode(text_preprocessed[0, :128]))","6780cddd":"def prepare_model(model_class=\"vinai\/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n\n\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_class,\n        num_labels = num_classes,  \n        output_attentions = False, \n        output_hidden_states = False,\n    )\n\n    optimizer = AdamW(model.parameters(),\n                    lr = 5e-5,\n                    eps = 1e-8\n                    )\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                num_warmup_steps = 0, \n                                                num_training_steps = total_steps)\n\n    if model_to_load is not None:\n        try:\n            model.roberta.load_state_dict(torch.load(model_to_load))\n            print(\"LOADED MODEL\")\n        except:\n            pass\n    return model, optimizer, scheduler","8a32b6b4":"epochs = 5\ntotal_steps = len(train_dataloader) * epochs\n\nmodel, optimizer, scheduler = prepare_model(\"vinai\/bertweet-base\" ,num_classes=2, model_to_load=None, total_steps = total_steps)","9438fd51":"def train(model,optimizer,scheduler,train_dataloader,epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n\n        print(\"\")\n        print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        \n        t0 = time.time()\n        total_train_loss = 0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            if step % 40 == 0 and not step == 0:\n                elapsed = format_time(time.time() - t0)\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()        \n            outputs = model(b_input_ids, \n                                token_type_ids=None, \n                                attention_mask=b_input_mask, \n                                labels=b_labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            total_train_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n        avg_train_loss = total_train_loss \/ len(train_dataloader)            \n        training_time = format_time(time.time() - t0)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","82c8dc91":"train(model,optimizer,scheduler,train_dataloader, epochs)","e7594d2e":"def predict(model,test_dataloader):\n    model.eval()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    preds = []\n\n    for batch in test_dataloader:\n        \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            outputs = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        for logit in logits:\n            preds.append(logit)\n\n    return preds","6d5bd67b":"result = predict(model,test_dataloader)","fffaf030":"from scipy.special import softmax\n\npred_labels = np.argmax(result, axis = 1)\npred_scores = softmax(result, axis=1)[:, 1]","1fc73890":"pred_labels","761cea48":"output = pd.DataFrame({'id':test_df.id,'target':pred_labels})\noutput","b40105c5":"output.to_csv('submission.csv',index=False)","32874a63":"We feed the text input to the BERTweet tokenizer. \n\nFor each input,we construct the following:\n* **input ids:** a sequence of integers identifying each input token to its index number in the PureBERT tokenizer vocabulary\n* **attention mask:** a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens","b5e8be60":"## Training","c0aa0f57":"Install `emoji`","17104d0c":"We first install the latest version of the `transformers` library (provides access to pre-trained language models like BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet etc.) ","c1b22102":"We define some helper functions to handle formatting:","308a2173":"We construct the PyTorch DataLoader for the training and test dataset. Note that we do not perform any pre-processing at all (the BERTweet tokenizer takes care of handling URLS, emojis etc.)!","b9a22017":"Below we study the tokenization process (i.e. mapping the input text from the tweets to input_ids from tokenizer) ","7b408021":"This is the training loop - during training, we print out the loss.","ab4b6d2a":"The `predict` method allows you to perform inference on the test set.","07637adb":"## Experiments","e0013e0f":"Define the model using AdamW optimizer and an initial learning rate of 5e-5 and set epsilon to 1e-8.\n\nPassing a model to *model_to_load* allows you to load a pre-trained BERTweet model - the default BERTweet model is `vinai\/bertweet-bas`).","b802d231":"## Inference"}}