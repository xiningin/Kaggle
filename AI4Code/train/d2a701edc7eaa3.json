{"cell_type":{"7f711325":"code","1b4b71d3":"code","b7cf5210":"code","d017a08a":"code","e6d6dbcf":"code","dcc04ea0":"code","8f8ed01e":"code","65853b75":"code","862b2dbb":"code","9df1c825":"code","e49dba65":"code","b8a8414e":"code","12f8562e":"code","81619cde":"code","79644179":"code","c3b9761d":"code","c1b975d9":"code","f1e751f5":"code","74c29706":"code","2ab4cf05":"code","47cd2a21":"code","9cc6d6e7":"code","67d43fce":"code","52b07b61":"code","f1672c12":"code","065d4aff":"code","3852c6bc":"code","88a0e0de":"code","c135f736":"code","4bd1696b":"code","ef92ee28":"markdown","7a75ba2d":"markdown","7f87a021":"markdown","0eabb684":"markdown","bfa9c189":"markdown"},"source":{"7f711325":"import os\nimport cv2\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\n\n# For Visualization\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For Image Augmentation\nimport albumentations as A\nfrom PIL import Image,ImageDraw\nfrom ast import literal_eval\n\n# For parallel processing\nfrom joblib import Parallel,delayed\n\n# For Object Detection\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator","1b4b71d3":"def load_model(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model","b7cf5210":"os.chdir('\/kaggle\/input\/global-wheat-detection')\nos.listdir()","d017a08a":"test=os.listdir('test')","e6d6dbcf":"train=pd.read_csv('train.csv')","dcc04ea0":"train.head()","8f8ed01e":"train['x_min']=train['bbox'].apply(lambda x:float(re.findall(r'[0-9.]+',x.split(',')[0])[0]))\ntrain['y_min']=train['bbox'].apply(lambda x:float(re.findall(r'[0-9.]+',x.split(',')[1])[0]))\ntrain['box_width']=train['bbox'].apply(lambda x:float(re.findall(r'[0-9.]+',x.split(',')[2])[0]))\ntrain['box_height']=train['bbox'].apply(lambda x:float(re.findall(r'[0-9.]+',x.split(',')[3])[0]))","65853b75":"train.drop('bbox',axis=1,inplace=True)","862b2dbb":"train['box_area']=train['box_width']*train['box_height']\ntrain['x_max']=train['x_min']+train['box_width']\ntrain['y_max']=train['y_min']+train['box_height']","9df1c825":"print(f'No of Unique Images are {train.image_id.nunique()}')","e49dba65":"def show_images(df,rows,cols,title,linecolor):\n    \"\"\" Function to show images with detected objects\"\"\"\n    fig, axs = plt.subplots(rows, cols, figsize=(10,10))\n    \n    for row in range(rows):\n        for col in range(cols):\n            idx = np.random.randint(len(df), size=1)[0]\n            images=df.iloc[idx].image_id\n\n            path=os.path.join('train',images+'.jpg')\n            image=Image.open(path)\n\n            objects=train[train['image_id']==images][['x_min','x_max','y_min','y_max']].values\n            # Drawing on the Image\n            draw=ImageDraw.Draw(image)\n\n            for box in objects:\n                draw.rectangle([box[0],box[2],box[1],box[3]],width=10,outline=linecolor)\n            plt.figure(figsize=(10,10))\n            axs[row,col].imshow(image)\n            axs[row,col].axis('off')\n    plt.suptitle(title)","b8a8414e":"# No of Bounding Boxes per Image\ntrain['count']=train.apply(lambda row: 1 if np.isfinite(row.width) else 0, axis=1)\ntrain_images_spikes= train.groupby('image_id')['count'].sum().reset_index()","12f8562e":"# Visualizing Spikes\ncounts,bins=np.histogram(train_images_spikes['count'],bins=range(0,117,4))\nlabels=[]\nfor i in range(0,len(bins)-1):\n    labels.append(f'({bins[i]}-{bins[i+1]}]')\npx.bar(x=labels,y=counts,labels={'x':'Range of Wheats per Images','y':'count'},title=\"Number of wheat spikes per image\")","81619cde":"# Visualizing Images with Higher spikes\nshow_images(train_images_spikes[train_images_spikes['count']>50],3,3,'Images with Higher Spikes','red')","79644179":"sns.distplot(train['box_area'],kde=False,bins=100)","c3b9761d":"show_images(train[train['box_area']>500000],3,3,'Images with large bounding box','red')","c1b975d9":"image_transforms=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],\n                            [0.229,0.224,0.225])\n])","f1e751f5":"# Spliting the dataset\ntrain_set,val_set=train_test_split(train,test_size=0.15,random_state=1)","74c29706":"class DatasetGenerator(Dataset):\n    \n    def __init__(self,dataset,transforms=None,test=False):\n        super().__init__()\n        self.dataset=dataset # Train dataset which contains boxes\n        self.transforms=transforms # Image Transformation\n        self.test=test # Test set\n    \n    def __getitem__(self,index):\n        image_id = self.dataset.iloc[index]['image_id']\n        img_path=os.path.join(f'train\/{image_id}.jpg')\n        img=Image.open(img_path)\n        \n        # Convert everything into a torch tensor\n        boxes=torch.tensor(self.dataset[self.dataset['image_id']==image_id]\n                           [['x_min','y_min','x_max','y_max']].values,dtype=torch.float32)\n        \n        # Classes\n        labels=torch.ones((boxes.shape[0]),dtype=torch.int64)\n        iscrowd=torch.zeros((boxes.shape[0],), dtype=torch.int64)        \n        target={}\n        \n        target['boxes']=boxes\n        target['labels']=labels\n        target['image_id']=torch.tensor([index])\n        target['area']=torch.tensor(self.dataset[self.dataset['image_id']==image_id]['box_area'].values,\n                                    dtype=torch.float32)\n        target['iscrowd']=iscrowd\n        \n        if self.transforms is not None:\n            img=self.transforms(img)\n        return img,target\n    \n    def __len__(self):\n        return self.dataset['image_id'].nunique()","2ab4cf05":"train_dataset = DatasetGenerator(\n    dataset=train_set,\n    transforms=image_transforms,\n    test=False,\n)\n\nvalidation_dataset = DatasetGenerator(\n    dataset=val_set,\n    transforms=image_transforms,\n    test=True,\n)","47cd2a21":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader=DataLoader(train_dataset,batch_size=4,shuffle=False,num_workers=4,collate_fn=collate_fn)\ntest_loader=DataLoader(validation_dataset,batch_size=4,shuffle=False,num_workers=4,collate_fn=collate_fn)","9cc6d6e7":"DEVICE=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nNUM_EPOCHS=1\nLR=0.005","67d43fce":"# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (Wheat spikes) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","52b07b61":"model=model.to(DEVICE)","f1672c12":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","065d4aff":"train_loss=0\nval_loss=0\nepochs=list(range(NUM_EPOCHS))\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=LR,\n                            momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","3852c6bc":"loss_hist = Averager()\nitr = 1\n\nfor epoch in range(NUM_EPOCHS):\n    loss_hist.reset()\n    \n    for images, targets in train_loader:\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        del images,targets\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") \n","88a0e0de":"os.chdir('\/kaggle\/working')","c135f736":"checkpoint={'model':model,\n           'state_dict':model.state_dict()}\n\n\ntorch.save(checkpoint, 'fasterrcnn_resnet50_fpn.pth')","4bd1696b":"\"\"\"model=load_model('fasterrcnn_resnet50_fpn.pth')\n\nmodel=model.to(DEVICE)\n\nbox=[]\nwith torch.no_grad():\n    for img_name in test:\n        img_path=os.path.join(f'test\/{img_name}')\n        img=Image.open(img_path)\n\n        img=image_transforms(img)\n        img=img.unsqueeze(0)\n\n        img=img.to(DEVICE)\n\n        output=model(img)\n        output=output[0]['boxes'].cpu().detach().flatten().tolist()\n        boxes=''\n        for i in output:\n            boxes=boxes+str(i)+' '\n        box.append(boxes)\n        del img,boxes,output,img_path\n\nprediction=pd.DataFrame({'image_id':test,'PredictionString':box})\n\nos.chdir('\/kaggle\/working')\n\n#prediction.to_csv('submission.csv',index=False)\"\"\"","ef92ee28":"# Loading Pretrained FasterRCNN Model","7a75ba2d":"* https:\/\/www.kaggle.com\/aleksandradeis\/globalwheatdetection-eda\n\n* https:\/\/www.kaggle.com\/shonenkov\/training-efficientdet","7f87a021":"# Train","0eabb684":"# Visualizing Images","bfa9c189":" So we need to delete boxes with large areas"}}