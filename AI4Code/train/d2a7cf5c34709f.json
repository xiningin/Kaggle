{"cell_type":{"6d3cfc73":"code","8a277f9f":"code","72d605c0":"markdown","3628c1b0":"markdown","d218c255":"markdown"},"source":{"6d3cfc73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8a277f9f":"#init libraries\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap  # package used to calculate Shap values\nfrom IPython.display import display\n\n\nclass patient_data():\n    \n    features = []\n    query = '' # future proofing idea for finer details of patient data analysis (not implemented yet)\n    groupby = []\n    my_model = RandomForestClassifier()\n    \n    val_X = pd.DataFrame()\n    val_y = pd.DataFrame()\n    data_for_prediction = pd.DataFrame()\n    data = pd.DataFrame()\n    train_X = pd.DataFrame()\n    train_y = pd.DataFrame()\n    \n    @classmethod\n    def _initdata(self, train_data):\n        self.train_data = train_data\n        self.data = pd.read_csv(self.train_data)\n       # data.columns\n        \n        y = self.data.readmitted\n        base_features = [c for c in self.data.columns if c != \"readmitted\"]\n        X = self.data[base_features]\n        self.train_X, self.val_X, self.train_y, self.val_y = train_test_split(X, y, random_state=1)\n        self.my_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(self.train_X, self.train_y)\n        self.data_for_prediction = self.val_X.iloc[0,:]  # use 1 row of data here. Could use multiple rows if desired\n        \n    def __init__(self, train_data):\n        self.train_data = train_data\n        self._initdata( self.train_data)\n\nclass patient_risk():\n    \n    dp = patient_data('..\/input\/hospital-readmissions\/train.csv')\n  \n    def _initdata(self):\n        print('PR init')    \n    \n    def __init__(self):\n        self._initdata()\n            \n    @classmethod        \n    def permutation_importance(cls):        \n        perm = PermutationImportance( cls.dp.my_model, random_state=1).fit( cls.dp.val_X, cls.dp.val_y)\n        display(eli5.show_weights(perm, feature_names = cls.dp.val_X.columns.tolist()) )\n\n    @classmethod     \n    def partial_dependence(cls):\n    #**Calculate and show partial dependence plot:**\n    # Create the data that we will plot\n        pdp_goals = pdp.pdp_isolate(model=cls.dp.my_model, dataset = cls.dp.val_X, model_features=cls.dp.val_X.columns.tolist(), feature= cls.dp.features)\n    # plot it\n        pdp.pdp_plot(pdp_goals, cls.dp.features)\n        plt.show()\n    \n    @classmethod        \n    def shap_view(cls):\n       # Create object that can calculate shap values\n        explainer = shap.TreeExplainer(cls.dp.my_model)\n        shap_values = explainer.shap_values( cls.dp.data_for_prediction)\n        shap.initjs()\n        display(shap.force_plot(explainer.expected_value[0], shap_values[0], cls.dp.data_for_prediction))\n        \n    @classmethod        \n    def show_visuals(cls):\n        cls.permutation_importance()\n        cls.partial_dependence()\n        cls.shap_view()\n    \n    @classmethod\n    def show_risk_analysis(cls):\n       # data_analysis = pd.concat([cls.dp.train_X, cls.dp.train_y], axis=1) \n        #data[['time_in_hospital', 'readmitted']]  # use 1 row of data here. Could use multiple rows if desired or choose individual rows in question\n        #data_for_prediction.groupby(['time_in_hospital']).head() #.mean()\n        cls.dp.data.groupby(cls.dp.groupby ).mean().readmitted.plot()\n        plt.show()      \n        \n        \ndef patient_risk_factors():\n    \n      \n    try:\n        pr = patient_risk() \n        pr.dp.features=['number_inpatient', 'number_diagnoses','num_procedures']\n        pr.show_visuals()\n        \n        pr.dp.groupby = ['time_in_hospital']\n        pr.show_risk_analysis()\n        \n    except Exception as e:\n        return print( \"Error: \", e )\n   \n   \n#usage:\npatient_risk_factors()","72d605c0":"Acknowledgement and thanks to Dan Becker for the data explain-ability course\nhttps:\/\/www.kaggle.com\/kernels\/fork\/1637226","3628c1b0":"The following classes and methods simplifies, encapsulates and organizes functionality that should remain hidden from medical doctors, who have little patience (no pun) for things like:\n* exception handling \n* displaying output\n* debugging\n* data stuff\n","d218c255":"## Step 5\nNow the doctors are convinced you have the right data, and the model overview looked reasonable.  It's time to turn this into a finished product they can use. Specifically, the hospital wants you to create a function `patient_risk_factors` that does the following\n- Takes a single row with patient data (of the same format you as your raw data)\n- Creates a visualization showing what features of that patient increased their risk of readmission, what features decreased it, and how much those features mattered.\n\nIt's not important to show every feature with every miniscule impact on the readmission risk.  It's fine to focus on only the most important features for that patient."}}