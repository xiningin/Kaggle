{"cell_type":{"ccffc9a2":"code","904461b1":"code","3e010447":"code","63174fc4":"code","8cc7befb":"code","d707a533":"code","ba49d4d7":"code","8e20a850":"code","ebf35800":"code","78a5a68a":"code","05a1c6d5":"code","2c852e43":"code","ee798411":"code","677ca8e8":"code","29cc0cb2":"code","a3c3a29e":"code","a483812e":"code","04fb34c1":"code","831a764b":"code","6ddf9fa8":"code","3be6de6b":"code","d1648523":"code","a4be150c":"code","22541b31":"code","8737ebd0":"code","032e6265":"code","5b0543dc":"code","178e7a9b":"code","aac367a1":"code","768dc120":"code","43d36098":"code","24cbbbdf":"code","d784f6d2":"code","2dcc4276":"code","4e8c9746":"markdown","a5201ef2":"markdown","2885feec":"markdown","f68f95a6":"markdown","302c697c":"markdown","d36b247c":"markdown","82c27f43":"markdown","faaa4785":"markdown","a328ddbe":"markdown","cfa5b8c6":"markdown","d700ddc4":"markdown","4e6748e1":"markdown"},"source":{"ccffc9a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","904461b1":"dataset = pd.read_csv(\"..\/input\/itjen-data-science-competition\/train.csv\")\ndataset.head()","3e010447":"dataset.info()","63174fc4":"dataset.describe()","8cc7befb":"#FillAge = dataset['Age'].mean()\n#let's drop rows with NA Age value\nAgeNotNull = dataset[['Age','Pclass','Sex']].dropna()\n\nClass1FemaleAgeMedian = np.median(((AgeNotNull.query('Pclass==1 and Sex==\"female\"')))['Age'])\nClass2FemaleAgeMedian = np.median(((AgeNotNull.query('Pclass==2 and Sex==\"female\"')))['Age'])\nClass3FemaleAgeMedian = np.median(((AgeNotNull.query('Pclass==3 and Sex==\"female\"')))['Age'])\nClass1MaleAgeMedian = np.median(((AgeNotNull.query('Pclass==1 and Sex==\"male\"')))['Age'])\nClass2MaleAgeMedian = np.median(((AgeNotNull.query('Pclass==2 and Sex==\"male\"')))['Age'])\nClass3MaleAgeMedian = np.median(((AgeNotNull.query('Pclass==3 and Sex==\"male\"')))['Age'])\n\n## Get the subset data and fillna with median\ndataset1 = dataset.query('Pclass == 1 and Sex == \"female\"')\ndataset1['Age'] = dataset1['Age'].fillna(Class1FemaleAgeMedian)\n\ndataset2 = dataset.query('Pclass == 2 and Sex == \"female\"')\ndataset2['Age'] = dataset2['Age'].fillna(Class2FemaleAgeMedian)\n\ndataset3 = dataset.query('Pclass == 3 and Sex == \"female\"')\ndataset3['Age'] = dataset3['Age'].fillna(Class3FemaleAgeMedian)\n\ndataset4 = dataset.query('Pclass == 1 and Sex == \"male\"')\ndataset4['Age'] = dataset4['Age'].fillna(Class1MaleAgeMedian)\n\ndataset5 = dataset.query('Pclass == 2 and Sex == \"male\"')\ndataset5['Age'] = dataset5['Age'].fillna(Class2MaleAgeMedian)\n\ndataset6 = dataset.query('Pclass == 3 and Sex == \"male\"')\ndataset6['Age'] = dataset6['Age'].fillna(Class3MaleAgeMedian)\n\n## Merge all subsetted datasets and sort by PassengerID\ndataset = pd.concat([dataset1,dataset2,dataset3,dataset4,dataset5,dataset6])\n\ndataset = dataset.sort_values('PassengerId')\n\n#dataset['Age'].fillna(FillAge,inplace=True)\ndataset.info()","d707a533":"bins = [0,12,18, 30, 40, 50, 60, 70, 120]\nlabels = ['0-12','12-18','18-29', '30-39', '40-49', '50-59', '60-69', '70+']\ndataset['agegroup'] = pd.cut(dataset.Age, bins, labels = labels,include_lowest = True)\ndataset.head()","ba49d4d7":"MostEmbarked = dataset['Embarked'].value_counts().idxmax()\ndataset['Embarked'].fillna(MostEmbarked,inplace=True)\ndataset.info()","8e20a850":"MeanFare = dataset['Fare'].mean()\nMeanFare","ebf35800":"dataset['LogFare']=dataset['Fare']\ndataset['LogFare']=dataset.LogFare.mask(dataset.LogFare == 0,MeanFare)\ndataset['LogFare'] = np.log(dataset.LogFare)\ndataset.info()","78a5a68a":"df = dataset[['Sex','agegroup','Parch','LogFare','SibSp','Embarked','Survived']]\nimport category_encoders as ce\nohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\ndf_OHE = ohe.fit_transform(df)\ndf_OHE.head()","05a1c6d5":"df_OHE.info()","2c852e43":"X_train = df_OHE.drop(['Survived'],axis=1)\ny_train=df_OHE[['Survived']]","ee798411":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n#create new a knn model\nknn = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparams_knn = {'n_neighbors':np.arange(1,25),\n          'leaf_size':[1,2,3,5],\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n          'n_jobs':[-1]}\n#use gridsearch to test all values for n_neighbors\nknn_gs = GridSearchCV(knn, params_knn, cv=5)\n#fit model to training data\nknn_gs.fit(X_train, y_train)","677ca8e8":"#save best model\nknn_best = knn_gs.best_estimator_\n#check best n_neigbors value\nprint(knn_gs.best_params_)","29cc0cb2":"from sklearn.ensemble import RandomForestClassifier\n#create a new random forest classifier\nrf = RandomForestClassifier()\n#create a dictionary of all values we want to test for n_estimators\nparams_rf = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n#use gridsearch to test all values for n_estimators\nrf_gs = GridSearchCV(rf, params_rf, cv=5)\n#fit model to training data\nrf_gs.fit(X_train, y_train)","a3c3a29e":"#save best model\nrf_best = rf_gs.best_estimator_\n#check best n_estimators value\nprint(rf_gs.best_params_)","a483812e":"from sklearn.linear_model import LogisticRegression\n#create a new logistic regression model\nlog_reg = LogisticRegression()\n#fit the model to the training data\nlog_reg.fit(X_train, y_train)","04fb34c1":"from sklearn import svm\n#making the instance\nsvm_model=svm.SVC()\n#Hyper Parameters Set\nparams_svm = {'C': [6,7,8,9,10,11,12], \n          'kernel': ['linear','rbf']}\n#Making models with hyper parameters sets\nsvm_gs = GridSearchCV(svm_model, param_grid=params_svm, n_jobs=-1,cv=5)\n#Learning\nsvm_gs.fit(X_train,y_train)","831a764b":"#save best model\nsvm_best = svm_gs.best_estimator_\n#check best n_estimators value\nprint(svm_gs.best_params_)","6ddf9fa8":"from sklearn.tree import DecisionTreeClassifier\n#making the instance\ndtree= DecisionTreeClassifier(random_state=1234)\n#Hyper Parameters Set\nparams_dtree = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n#Making models with hyper parameters sets\ndtree_gs = GridSearchCV(dtree, param_grid=params_dtree, n_jobs=-1,cv=5)\n#Learning\ndtree_gs.fit(X_train,y_train)","3be6de6b":"#save best model\ndtree_best = dtree_gs.best_estimator_\n#check best n_estimators value\nprint(dtree_gs.best_params_)","d1648523":"test_dataset = pd.read_csv(\"..\/input\/itjen-data-science-competition\/test.csv\")\ntest_dataset.head()","a4be150c":"#test_dataset['Age'].fillna(FillAge,inplace=True)\n\n## Get the subset data and fillna with median\ntest_dataset1 = test_dataset.query('Pclass == 1 and Sex == \"female\"')\ntest_dataset1['Age'] = test_dataset1['Age'].fillna(Class1FemaleAgeMedian)\n\ntest_dataset2 = test_dataset.query('Pclass == 2 and Sex == \"female\"')\ntest_dataset2['Age'] = test_dataset2['Age'].fillna(Class2FemaleAgeMedian)\n\ntest_dataset3 = test_dataset.query('Pclass == 3 and Sex == \"female\"')\ntest_dataset3['Age'] = test_dataset3['Age'].fillna(Class3FemaleAgeMedian)\n\ntest_dataset4 = test_dataset.query('Pclass == 1 and Sex == \"male\"')\ntest_dataset4['Age'] = test_dataset4['Age'].fillna(Class1MaleAgeMedian)\n\ntest_dataset5 = test_dataset.query('Pclass == 2 and Sex == \"male\"')\ntest_dataset5['Age'] = test_dataset5['Age'].fillna(Class2MaleAgeMedian)\n\ntest_dataset6 = test_dataset.query('Pclass == 3 and Sex == \"male\"')\ntest_dataset6['Age'] = test_dataset6['Age'].fillna(Class3MaleAgeMedian)\n\n## Merge all subsetted datasets and sort by PassengerID\ntest_dataset = pd.concat([test_dataset1,test_dataset2,test_dataset3,test_dataset4,test_dataset5,test_dataset6])\n\ntest_dataset = test_dataset.sort_values('PassengerId')\n\ntest_dataset.info()","22541b31":"test_dataset['agegroup'] = pd.cut(test_dataset.Age, bins, labels = labels,include_lowest = True)\ntest_dataset.head()","8737ebd0":"test_dataset['Embarked'].fillna(MostEmbarked,inplace=True)\ntest_dataset.info()","032e6265":"test_dataset['Fare'].fillna(MeanFare,inplace=True)\ntest_dataset.info()","5b0543dc":"test_dataset['LogFare']=test_dataset['Fare']\ntest_dataset['LogFare']=test_dataset.LogFare.mask(test_dataset.LogFare == 0,MeanFare)\ntest_dataset['LogFare'] = np.log(test_dataset.LogFare)\ntest_dataset.info()","178e7a9b":"X_test = test_dataset[['Sex','agegroup','Parch','LogFare','SibSp','Embarked']]\nX_test_OHE = ohe.fit_transform(X_test)\nX_test_OHE.head()","aac367a1":"from sklearn.ensemble import VotingClassifier\n#create a dictionary of our models\nestimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg),('svm',svm_best),('dtree',dtree_best)]\n#create our voting classifier, inputting our models\nensemble = VotingClassifier(estimators, voting='hard')","768dc120":"#fit model to training data\nensemble.fit(X_train, y_train)","43d36098":"prediction=pd.DataFrame(ensemble.predict(X_test_OHE))\nprediction","24cbbbdf":"from sklearn.metrics import accuracy_score\nactual_data = pd.read_csv(\"..\/input\/testwithsurvived\/test-with-survived.csv\", sep=\";\")\ntrue_data = pd.DataFrame(actual_data[['Survived']])\nscore = accuracy_score(true_data, prediction)\nscore","d784f6d2":"finalresult = pd.concat([test_dataset['PassengerId'],prediction],axis=1)\nfinalresult.columns = ['PassengerId','Survived']\nfinalresult.head()","2dcc4276":"finalresult.to_csv(\"final-result-ensemble-tuned-5-algorithms.csv\", sep=',', encoding='utf-8',index=False)","4e8c9746":"save the best Random Forest model to \u2018rf_best\u2019 using the \u2018best_estimator_\u2019 function and check what the best value was for \u2018n_neighbors\u2019.","a5201ef2":"Random Forest\n\nThe next model we will build is a random forest. A random forest is considered an ensemble model in itself, since it is a collection of decision trees combined to make a more accurate model.\n\nWe will create a new random forest classifier and set the hyperparameters we want to tune. \u2018n_estimators\u2019 is the number of trees in our random forest. Then we can run our grid search to find the optimal number of trees.\n\n","2885feec":"Building the models\n\nNext, we have to build our models. Each model we build has a set of hyper parameters that we can tune. Tuning parameters is when you go through a process to find the optimal parameters for your model to improve accuracy. We will use grid search to find the optimal hyperparamters for each model.\n\nGrid search works by training our model multiple times on a range of parameters that we specify. That way, we can test our model with each hyperparameter value and figure out the optimal values to get the best accuracy results.","f68f95a6":"Now we should do the preprocessing for the test dataset","302c697c":"Voting Classifier\n\nNow that we\u2019ve built our three individual models, it\u2019s time we built our voting classifier.\n\nPlace our three models in an array called \u2018estimators\u2019. Next, we will create our voting classifier. It takes two inputs. The first is our estimator array of our three models. We will set the voting parameter to hard, which tells our classifier to make predicitons by majority vote.","d36b247c":"Try implementing dTree algorithm as well","82c27f43":"Try implementing SVM for ensemble","faaa4785":"Logistic Regression\n\nOur last model is logistic regression. Even though it has \u2018regression\u2019 in its name, logistic regression is a classification method. This one is more simple since we won\u2019t tune any hyperparameters. We just need to create and train the model.","a328ddbe":"fit our ensemble model to our training data and score it on our testing data.","cfa5b8c6":"save the best k-NN model to \u2018knn_best\u2019 using the \u2018best_estimator_\u2019 function and check what the best value was for \u2018n_neighbors\u2019.","d700ddc4":"k-Nearest Neighbors (k-NN)\nThe first model we will build is k-Nearest Neighbors (k-NN). k-NN models work by taking a data point and looking at the \u2018k\u2019 closest labeled data points. The data point is then assigned the label of the majority of the \u2018k\u2019 closest points.\n\nFor example, if k = 5, and 3 of points are \u2018green\u2019 and 2 are \u2018red\u2019, then the data point in question would be labeled \u2018green\u2019, since \u2018green\u2019 is the majority","4e6748e1":"First, we will create a new k-NN classifier. Next, we need to create a dictionary to store all the values we will test for \u2018n_neighbors\u2019, which is the hyperparameter we need to tune. We will test 24 different values for \u2018n_neighbors\u2019. Then we will create our grid search, inputing our k-NN classifier, our set of hyperparamters and our cross validation value.\n\nCross-validation is when the dataset is randomly split up into \u2018k\u2019 groups. One of the groups is used as the test set and the rest are used as the training set. The model is trained on the training set and scored on the test set. Then the process is repeated until each unique group as been used as the test set.\n\nIn our case, we are using 5-fold cross validation. The dataset is split into 5 groups, and the model is trained and tested 5 separate times so each group would get a chance to be the test set. This is how we will score our model running with each hyperparamter value to see which value for \u2018n_neighbors\u2019 gives us the best score.\n\nThen we will use the \u2018fit\u2019 function to run our grid search."}}