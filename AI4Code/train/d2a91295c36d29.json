{"cell_type":{"239e788e":"code","e8bf5aea":"code","c241759e":"code","57ada7f3":"code","e2dd4691":"code","e14f1745":"code","91b6271c":"code","716ba1a8":"code","1b6820f4":"code","dd461338":"code","27dbbe47":"code","77b455ab":"code","1b552703":"code","1dce09bd":"code","df865082":"code","be5ba202":"code","a62a3497":"code","2b7de0e5":"code","f9ab6c57":"code","bcfb01c7":"code","215c0525":"code","0b8ae288":"code","5c4c60cd":"code","47813130":"code","6ba08dd4":"code","c9f6f271":"code","c4835ffa":"code","3d59deb9":"code","9a790864":"code","ddfd6aef":"code","17b31019":"code","451d18ff":"code","7bdaf697":"code","7e9ded71":"markdown","405f728f":"markdown","83a677e5":"markdown","32e20f6d":"markdown","04b15bd6":"markdown","9f821a9e":"markdown","2f174a28":"markdown","7a198282":"markdown","ce31a256":"markdown","4eb70529":"markdown","b59de626":"markdown","148588e2":"markdown","3a775902":"markdown","bdb3b716":"markdown","158bbdbd":"markdown","764a0ee7":"markdown","29f99b64":"markdown","96230d70":"markdown","413abbe9":"markdown","2a8d619e":"markdown","f7faea46":"markdown","10d3075a":"markdown"},"source":{"239e788e":"!pip install chart-studio","e8bf5aea":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n#tf.enable_eager_execution()\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport unicodedata\nimport re\nimport numpy as np\nimport time\nimport string\n\nimport chart_studio.plotly\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\n#%plotly.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c241759e":"file_path = '..\/input\/englishportuguese-translation\/por.txt' # please set the path according to your system","57ada7f3":"lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\nlines[5000:5010]","e2dd4691":"print(\"total number of records: \",len(lines))","e14f1745":"exclude = set(string.punctuation) # Set of all special characters\nremove_digits = str.maketrans('', '', string.digits) # Set of all digits","91b6271c":"def preprocess_eng_sentence(sent):\n    '''Function to preprocess English sentence'''\n    sent = sent.lower() # lower casing\n    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n    sent = ''.join(ch for ch in sent if ch not in exclude)\n    sent = sent.translate(remove_digits) # remove the digits\n    sent = sent.strip()\n    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n    return sent","716ba1a8":"def preprocess_port_sentence(sent):\n    '''Function to preprocess Portuguese sentence'''\n    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n    sent = ''.join(ch for ch in sent if ch not in exclude)\n    #sent = re.sub(\"[\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]\", \"\", sent) # remove the digits\n    sent = sent.strip()\n    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n    return sent","1b6820f4":"# Generate pairs of cleaned English and Portuguese sentences\nsent_pairs = []\nfor line in lines:\n    sent_pair = []\n    eng = line.rstrip().split('\\t')[0]\n    port = line.rstrip().split('\\t')[1]\n    eng = preprocess_eng_sentence(eng)\n    sent_pair.append(eng)\n    port = preprocess_port_sentence(port)\n    sent_pair.append(port)\n    sent_pairs.append(sent_pair)\nsent_pairs[5000:5010]","dd461338":"# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n# (e.g., 5 -> \"dad\") for each language,\nclass LanguageIndex():\n    def __init__(self, lang):\n        self.lang = lang\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab = set()\n\n        self.create_index()\n\n    def create_index(self):\n        for phrase in self.lang:\n            self.vocab.update(phrase.split(' '))\n\n        self.vocab = sorted(self.vocab)\n\n        self.word2idx['<pad>'] = 0\n        for index, word in enumerate(self.vocab):\n            self.word2idx[word] = index + 1\n\n        for word, index in self.word2idx.items():\n            self.idx2word[index] = word","27dbbe47":"def max_length(tensor):\n    return max(len(t) for t in tensor)","77b455ab":"def load_dataset(pairs, num_examples):\n    # pairs => already created cleaned input, output pairs\n\n    # index language using the class defined above    \n    inp_lang = LanguageIndex(en for en, ma in pairs)\n    targ_lang = LanguageIndex(ma for en, ma in pairs)\n    \n    # Vectorize the input and target languages\n    \n    # English sentences\n    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n    \n    # Marathi sentences\n    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n    \n    # Calculate max_length of input and output tensor\n    # Here, we'll set those to the longest sentence in the dataset\n    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n    \n    # Padding the input and output tensor to the maximum length\n    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n                                                                 maxlen=max_length_inp,\n                                                                 padding='post')\n    \n    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n                                                                  maxlen=max_length_tar, \n                                                                  padding='post')\n    \n    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar","1b552703":"input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))","1dce09bd":"# Creating training and validation sets using an 80-20 split\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)\n\n# Show length\nlen(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)","df865082":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nN_BATCH = BUFFER_SIZE\/\/BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word2idx)\nvocab_tar_size = len(targ_lang.word2idx)\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","be5ba202":"def gru(units):\n\n    return tf.keras.layers.GRU(units, \n                                   return_sequences=True, \n                                   return_state=True, \n                                   recurrent_activation='sigmoid', \n                                   recurrent_initializer='glorot_uniform')\n","a62a3497":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.enc_units)\n        \n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)        \n        return output, state\n    \n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","2b7de0e5":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.dec_units)\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        \n        # used for attention\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\n        self.V = tf.keras.layers.Dense(1)\n        \n    def call(self, x, hidden, enc_output):\n\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        \n        # score shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n        \n        # attention_weights shape == (batch_size, max_length, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n        \n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        \n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        \n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n        \n        # output shape == (batch_size * 1, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        \n        # output shape == (batch_size * 1, vocab)\n        x = self.fc(output)\n        \n        return x, state, attention_weights\n        \n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.dec_units))","f9ab6c57":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)","bcfb01c7":"optimizer = tf.optimizers.Adam()\n\ndef loss_function(real, pred):\n    mask = 1 - np.equal(real, 0)\n    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n    return tf.reduce_mean(loss_)","215c0525":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","0b8ae288":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    \n    hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    \n    for (batch, (inp, targ)) in enumerate(dataset):\n        loss = 0\n        \n        with tf.GradientTape() as tape:\n            enc_output, enc_hidden = encoder(inp, hidden)\n            \n            dec_hidden = enc_hidden\n            \n            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n            \n            # Teacher forcing - feeding the target as the next input\n            for t in range(1, targ.shape[1]):\n                # passing enc_output to the decoder\n                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n                \n                loss += loss_function(targ[:, t], predictions)\n                \n                # using teacher forcing\n                dec_input = tf.expand_dims(targ[:, t], 1)\n        \n        batch_loss = (loss \/ int(targ.shape[1]))\n        \n        total_loss += batch_loss\n        \n        variables = encoder.variables + decoder.variables\n        \n        gradients = tape.gradient(loss, variables)\n        \n        optimizer.apply_gradients(zip(gradients, variables))\n        \n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                         batch,\n                                                         batch_loss.numpy()))\n    # saving (checkpoint) the model every epoch\n    checkpoint.save(file_prefix = checkpoint_prefix)\n    \n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                        total_loss \/ N_BATCH))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","5c4c60cd":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","47813130":"def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n    \n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = ''\n    for i in inputs[0]:\n        if i == 0:\n            break\n        sentence = sentence + inp_lang.idx2word[i] + ' '\n    sentence = sentence[:-1]\n    \n    inputs = tf.convert_to_tensor(inputs)\n    \n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n        \n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += targ_lang.idx2word[predicted_id] + ' '\n\n        if targ_lang.idx2word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n        \n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot\n","6ba08dd4":"def predict_random_val_sentence():\n    actual_sent = ''\n    k = np.random.randint(len(input_tensor_val))\n    random_input = input_tensor_val[k]\n    random_output = target_tensor_val[k]\n    random_input = np.expand_dims(random_input,0)\n    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n    print('Input: {}'.format(sentence[8:-6]))\n    print('Predicted translation: {}'.format(result[:-6]))\n    for i in random_output:\n        if i == 0:\n            break\n        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n    actual_sent = actual_sent[8:-7]\n    print('Actual translation: {}'.format(actual_sent))\n    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n    sentence, result = sentence.split(' '), result.split(' ')\n    sentence = sentence[1:-1]\n    result = result[:-2]\n\n    # use plotly to generate the heat map\n    trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='greens')\n    data=[trace]\n    iplot(data)\n","c9f6f271":"predict_random_val_sentence()","c4835ffa":"predict_random_val_sentence()","3d59deb9":"predict_random_val_sentence()","9a790864":"predict_random_val_sentence()","ddfd6aef":"predict_random_val_sentence()","17b31019":"predict_random_val_sentence()","451d18ff":"predict_random_val_sentence()","7bdaf697":"predict_random_val_sentence()","7e9ded71":"**The Dataset :** We need a dataset that contains English sentences and their Portuguese translations which can be freely downloaded from this [link](http:\/\/www.manythings.org\/anki\/). Download the file fra-eng.zip and extract it. On each line, the text file contains an English sentence and its French translation, separated by a tab.","405f728f":"### Create GRU units","83a677e5":"### Generate pairs of cleaned English and Portuguese sentences with start and end tokens added.","32e20f6d":"### Tokenization and Padding","04b15bd6":"### Training the Model","9f821a9e":"We'll be using GRUs instead of LSTMs as we only have to create one state and implementation would be easier.","2f174a28":"Create encoder and decoder objects from their respective classes.","7a198282":"### As in case of any NLP task, after reading the input file, we perform the basic cleaning and preprocessing as follows:","ce31a256":"The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the GRU.","4eb70529":"### Function to preprocess Portuguese sentence","b59de626":"![Figure3_attention_1-624x352.png](attachment:Figure3_attention_1-624x352.png)","148588e2":"### Define the optimizer and the loss function.","3a775902":"### The next step is to define the encoder and decoder network.","bdb3b716":"# Neural Machine Translation with Attention mechanism","158bbdbd":"### Function to predict (translate) a randomly selected test point\n","764a0ee7":"The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with a token appended at the beginning.","29f99b64":"### Inference setup and testing:","96230d70":"### What is Attention?\n\nAttention is an interface between the encoder and decoder that provides the decoder with information from every encoder hidden state. With this setting, the model is able to selectively focus on useful parts of the input sequence and hence, learn the alignment between them. This helps the model to cope effectively with long input sentences .","413abbe9":"### Restoring the latest checkpoint","2a8d619e":"### Create a class to map every word to an index and vice-versa for any given vocabulary.","f7faea46":"### Function to preprocess English sentence","10d3075a":"### Creating training and validation sets using an 80-20 split"}}