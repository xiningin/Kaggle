{"cell_type":{"495999f8":"code","9dc5e1ff":"code","4a6c2563":"code","9d6a72a6":"code","983238a3":"code","4ca379c8":"code","68cdd40c":"code","67c49ba6":"code","fd562d03":"code","1342ab72":"code","906b9bb6":"code","425679b4":"code","9136d58c":"code","b7b3fa75":"code","e68d532a":"code","ba801cdb":"code","3c444049":"markdown","c174dd8d":"markdown","58004f21":"markdown","c50d4bfe":"markdown","d7d5a174":"markdown","d59ec746":"markdown","9a17fee8":"markdown","9e0d3694":"markdown","8e4d2cb4":"markdown","306b9c39":"markdown","2428ae83":"markdown","aa2fcd28":"markdown","025be234":"markdown","24316d6d":"markdown","da3595cb":"markdown","9ff7760f":"markdown","44bd7bfb":"markdown","369b7f0e":"markdown","68739f30":"markdown","ad21d3c5":"markdown","11916a41":"markdown","cd69853d":"markdown","14398388":"markdown","be0d1eeb":"markdown","b1b170c8":"markdown","b3dbb12d":"markdown","90e77551":"markdown","9d002526":"markdown","03c9df04":"markdown","bd20a540":"markdown","1aca8bfc":"markdown"},"source":{"495999f8":"# Importing all the necessary libraries (Hidden Input)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'axes.facecolor':'#c5c6c7'})\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import r2_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9dc5e1ff":"# Importing the dataset and looking at the top 5 rows (Hidden Input)\n\ndf = pd.read_csv(\"..\/input\/rental-bike-sharing\/day.csv\")\ndf.head().style.background_gradient(cmap='bone')","4a6c2563":"# Checking the number of rows and columns\n\ndf.shape","9d6a72a6":"# Dropping the necessary columns (Hidden Input)\n\ncols_to_drop = ['dteday','instant','casual','registered']\n\ndf.drop(columns=cols_to_drop, axis=1, inplace=True)\n\n# Performing the necessary mappings\n\ndf['season'] = df['season'].map({1:\"Winter\",2:\"Spring\",3:\"Summer\",4:\"Fall\"})\ndf['weekday'] = df['weekday'].map({0:\"Saturday\",1:\"Sunday\",2:\"Monday\",3:\"Tuesday\",4:\"Wednesday\",5:\"Thursday\",6:\"Friday\"})\ndf['mnth'] = df['mnth'].map({1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"})\ndf['weathersit'] = df['weathersit'].map({1:\"Clear\",2:\"Cloudy\",3:\"LightRain\",4:\"Snow_Thunderstorm\"})","983238a3":"# Checking the dataframe for any null values (Hidden Input)\n\ndf.info()","4ca379c8":"# Performing EDA on the Categorical columns (Hidden Input)\n\nnum_cols = list(df.select_dtypes([\"int64\",\"float64\"]))\ncat_cols = list(df.select_dtypes(\"object\"))\n\n# Columns required for Categrical visualizations\n\ncat_col_vis = ['season','mnth','weekday','weathersit']\n\nfig, ax = plt.subplots(ncols=1, nrows=4, figsize=(16, 28))\n                \ni = 0 #counter\n\nfor cols in cat_col_vis:\n    \n    sns.barplot(x=df[cols], y=df['cnt'], fill=True, alpha=1, ci=None, ax=ax[i], palette=('#17252a', '#2b7a78','#3aafa9',\n                                                                                                     '#def2f1','#feffff'),\n                                                                                            edgecolor=\"#c5c6c7\")\n                \n    ax[i].set_xlabel(' ')\n    ax[i].set_xlabel(' ')\n    ax[i].set_ylabel(' ')\n    ax[i].xaxis.set_tick_params(labelsize=14)\n    ax[i].tick_params(left=False, labelleft=False)\n    ax[i].set_ylabel(cols, fontsize=16)    \n    ax[i].grid(False)\n    ax[i].bar_label(ax[i].containers[0], size=\"12\")\n    i=i+1\n    \n      \nplt.show()","68cdd40c":"# Visualizing the Numerical Columns (Hidden Input) and treating outliers\n\nplt.figure(figsize=[16,12])\nplt.subplot(2,2,1)\nplt.scatter( x=df['temp'], y=df['cnt'], c=\"#2b7a78\")\nplt.xlabel(\"Temperature\")\nplt.grid(False)\nplt.subplot(2,2,2)\nplt.scatter( x=df['atemp'], y=df['cnt'], c='#1f2833')\nplt.xlabel(\"Feeling Temperature\")\nplt.grid(False)\nplt.subplot(2,2,3)\nplt.scatter( x=df['hum'], y=df['cnt'], c='#17252a')\nplt.xlabel(\"Humidity\")\nplt.grid(False)\nplt.subplot(2,2,4)\nplt.scatter( x=df['windspeed'], y=df['cnt'], c='#feffff')\nplt.xlabel(\"Wind Speed\")\nplt.grid(False)\nplt.show()\n","67c49ba6":"# Creating a heatmap (Hidden Input)\n\nheat = df.corr()\nplt.figure(figsize=[16,8])\nplt.title(\"Correlation between all the Numerical Features\", size=25, pad=20, color='#0b0c10')\nsns.heatmap(heat, cmap=['#0b0c10', '#1f2833','#c5c6c7','#45a29e','#66fcf1'], annot=True)\nplt.xticks(size=15)\nplt.yticks(size=15, rotation=360)\nplt.show()","fd562d03":"# Creating dummy variables for the categorical features(One Hot Encoding)\n\nfor col in cat_cols:\n    dummy_cols = pd.get_dummies(df[col], drop_first=True, prefix=col)\n    df = pd.concat([df,dummy_cols],axis=1)\n    df.drop(columns=col, inplace=True)","1342ab72":"# Splitting the data into X and y\n\ny = df.pop('cnt')\n\nX = df\n\n# Performing the train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=69)","906b9bb6":"# Normalizing our data...\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","425679b4":"# Applying PCA on our data\n\npca = PCA(random_state=69) # Initially not deciding the number of component, we will check that using scree plot\n\npca.fit(X_train) # Fitting the model","9136d58c":"# Visualizing the scree plot (Hidden Input)\n\nvar_cumulative = np.cumsum(pca.explained_variance_ratio_)\nplt.figure(figsize=[16,8])\nplt.plot(var_cumulative, color=\"#0b0c10\")\nplt.axvline(17, color=\"red\", linestyle='--', linewidth='3')\nplt.axhline(0.95, color=\"blue\", linestyle='--', linewidth='3')\nplt.title(\"Scree Plot\", size=25, pad=20, color=\"#0b0c10\")\nplt.ylabel(\"Cumulative Variance Explained\\n\", size=18)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.xlim(0,26)\nplt.grid(False)\nplt.show()","b7b3fa75":"# Using IncrementalPCA \n\npca_final = IncrementalPCA(n_components=17) # Specifying the n_components to 17 as per Scree Plot\n\ndf_train_pca = pca_final.fit_transform(X_train) # Always fit_transform() on train data\n\ndf_test_pca = pca_final.transform(X_test)# Always transform() on test data","e68d532a":"# From the above LazyPredict output, it is totally upto us to choose the model for our business decisions\n\n# In this case, I want to experiment with ExtraTreesRegressor, because, why not :)\n\netr = ExtraTreesRegressor()\n\netr.fit(df_train_pca, y_train)","ba801cdb":"# Checking our Model Performance (Always on test data) \u26a0Never do it on train data!:\ny_pred_test = etr.predict(df_test_pca)\n\nprint(\"R2 score on test data: \", r2_score(y_true=y_test, y_pred=y_pred_test)) # r2_score on test data","3c444049":"<h1> After PCA: <\/h1>\n\n![image.png](attachment:3e743aae-6ef6-4931-8453-64606d850d33.png)","c174dd8d":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"1\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Initial Data Cleaning <\/h1>\n    <\/a>\n<\/div>","58004f21":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black;\">\ud83c\udfaf When we first look at the data, we can see that the feature <strong>\"dteday\"<\/strong> is not useful for our analysis as we have variable like <strong>\"mnth\"<\/strong>, <strong>\"weekday\"<\/strong> and <strong>\"yr\"<\/strong> to understand about the month, day of the week and year respectively! Hence, we will get rid of this column.<\/p>\n \n<p style=\"font-size:120%;color:black;\">\ud83c\udfaf Another thing that we can notice here is that, the variable <strong>\"instant\"<\/strong> is useless for our analysis as we already have indexed, thanks to Pandas.<\/p>\n \n<p style=\"font-size:120%;color:black;\">\ud83c\udfaf Looking closely at the dataframe, we notice that <strong>\"season\"<\/strong>, <strong>\"weekday\"<\/strong>, <strong>\"mnth\"<\/strong> and <strong>\"weathersit\"<\/strong> are all categorical columns. Hence, we need to change the datatype of these features for better interpretability.<\/p>\n \n<p style=\"font-size:120%;color:black;\">\ud83c\udfaf Since, we have the value of cnt, the features <strong>\"casual\"<\/strong> and <strong>\"registered\"<\/strong> are also not required. <\/p>\n<\/div>","c50d4bfe":"<div class=\"alert alert-success\">\n    <h1 style=\"color:black\">\ud83d\udcc4About the Data:<\/h1><br>\n        <p style=\"color:black;font-size:120%;\"> Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. <\/p>\n            <p style=\"color:black;font-size:120%;\">With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.<\/p>\n    \n<h1 style=\"color:black\">\ud83d\udcc4Problem Statement:<\/h1>   \n    \n<p style=\"color:black;font-size:120%;\">Predicting the price of houses in Ames, Iowa.<\/p>    \n    \n<h1 style=\"color:black\">\ud83d\udcc4Understanding our features(Metadata):<\/h1>\n<li style=\"font-size:120%;color:black\"> <strong>instant:<\/strong> This feature is the index for each row. <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>dteday:<\/strong> Date on which the other values were recorded. <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>season:<\/strong> Season (1:winter, 2:spring, 3:summer, 4:fall). <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>yr:<\/strong> Year (0: 2011, 1:2012) <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>mnth:<\/strong> month ( 1 to 12) <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>holiday:<\/strong> whether day is holiday or not <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>weekday:<\/strong> day of the week <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>workingday:<\/strong>  if day is neither weekend nor holiday is 1, otherwise is 0. <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>weathersit:<\/strong> Weather conditions <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>temp:<\/strong> The true temperature on that day <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>atemp:<\/strong> The feeling temperature on that day <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>hum:<\/strong> Humidity on that day <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>windspeed:<\/strong> The Windspeed on that day <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>casual:<\/strong> Count of casual users <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>registered:<\/strong> Count of registered users <\/li>\n<li style=\"font-size:120%;color:black\"> <strong>cnt:<\/strong> Total users <\/li>\n    \n<\/div>","d7d5a174":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\u2b50 Will the Customer Churn? (Classification) <a href=\"https:\/\/www.kaggle.com\/vivek468\/will-the-customer-churn\">here<\/a>.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Heart Attack Prediction (Classification) <a href=\"https:\/\/www.kaggle.com\/vivek468\/heartattackprediction-decisiontree\">here<\/a>.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Boom Bikes (Similar to this) (Regression) <a href=\"https:\/\/www.kaggle.com\/vivek468\/boombikes-lr-r2score83\">older notebook<\/a>.<\/p>    \n<\/div>","d59ec746":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"2\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Exploratory Data Analysis <\/h1>\n    <\/a>\n<\/div>","9a17fee8":"<div class=\"alert alert-info\">\n    <a id=\"9\">\n    <h1 style=\"text-align:center;font-weight:20px;color:black;\"> My Other Notebooks <\/h1>\n    <\/a>\n<\/div>","9e0d3694":"<div class=\"alert alert-success\">\n<h2 style=\"color:Black;\">Inference:<\/h2>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Here, from the heatmap, we can see that there is a positive high correlation between year and count of riders. This suggests that the number of riders have increased from last year, and that more and more people are using it.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Also, actual temperature and the feeling temperature has very high correlation within themselves, so it is better to drop one to remove any sort of multicollinearity.<\/p>\n<\/div>","8e4d2cb4":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"3\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Visualizing Categorical Variables <\/h3>\n    <\/a>\n<\/div>","306b9c39":"<div class=\"alert alert-success\">\n<h2 style=\"color:Black;\">Inference:<\/h2>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf There are comparatively more number of riders during summer than in winter. Maybe the company can provide discounts or offers during this time to encourage riders to use their bikes.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Weather conditions do play a crucial role in deciding the number of people that are going to use the bikes. During bad weather, less people are going to use it. Clear weather on the other hand attracts more people to use these bikes. <\/p>\n<\/div>","2428ae83":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf <strong>Principal component analysis (PCA)<\/strong> is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf In the example below, the original data are plotted in 3D, but you can project the data into 2D through a transformation no different than finding a camera angle: rotate the axes to find the best angle. To see the \"official\" PCA transformation. The PCA transformation ensures that the horizontal axis PC1 has the most variation, the vertical axis PC2 the second-most, and a third axis PC3 the least. Obviously, PC3 is the one we drop. <\/p>\n    \n<p style=\"font-size:120%;color:black\">\u2b50 To understand better follow <a href=\"https:\/\/setosa.io\/ev\/principal-component-analysis\/\"> this <\/a> link<\/p>\n<\/div>","aa2fcd28":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"4\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Visualizing Numerical Variables <\/h3>\n    <\/a>\n<\/div>","025be234":"<p style=\"font-size:120%;color:black;\"><strong>If you learnt something new from my notebook or liked anything about it, please drop your valuable comments. Upvote if you like\/fork. It will motivate to make more notebooks and share with you all.\ud83d\ude01\ud83d\udc4d<\/strong><\/p>","24316d6d":"<div class=\"alert alert-info\">\n    <a id=\"6\">\n    <h1 style=\"text-align:center;font-weight:20px;color:black;\"> Dimentionality Reduction using PCA <\/h1>\n    <\/a>\n<\/div>","da3595cb":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Instead of eliminating features and reducing it's Dimensionality using PCA, another approach would have been to use <strong>Recursive Feature Elimination<\/strong> a.k.a RFE.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Read more about RFE <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html\">here<\/a>.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Another way to eliminate features from your data is using <strong>Variance Inflation Factor<\/strong> a.k.a VIF.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Read more about VIF <a href=\"https:\/\/www.statisticshowto.com\/variance-inflation-factor\/\">here<\/a>.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83d\udc8e Now, one might ask, why should we remove features? Won't that make our model weak? The answer is: A simple model is anyday better than a complex model! The reason we eliminate features using PCA, RFE or VIF is also to reduce <strong>multi-collinearity<\/strong> in the data. A complex model will mostly overfit. Hence it is always better to check which features are significat for our model. Incase we wish to understand the statistis of each feature in-depth, I would recommend using StatsModels instead of Scikit Learn.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Check out the prediction using RFE and VIF in my <a href=\"https:\/\/www.kaggle.com\/vivek468\/boombikes-lr-r2score83\">older notebook<\/a>.<\/p>\n<\/div>","9ff7760f":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf From the <strong>Scree Plot<\/strong> we are able to see that almost 95% of the variance is being explained by <strong>17 variables<\/strong>. Therefore, we are going to use IncrementalPCA from Scikit Learn now to transform only 17 variables.<\/p>\n<\/div>","44bd7bfb":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Table of Contents <\/h1>\n<\/div>","369b7f0e":"<img src=\"http:\/\/gif-finder.com\/wp-content\/uploads\/2016\/01\/Funny-Bike-Fail.gif\" style=\"display: block; margin-left: auto; margin-right: auto; width: 85%;\"><\/img>","68739f30":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"5\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Preparing the Data before Modelling <\/h1>\n    <\/a>\n<\/div>","ad21d3c5":"<div class=\"alert alert-info\">\n    <a id=\"7\">\n    <h1 style=\"text-align:center;font-weight:20px;color:black;\"> Using LazyPredict to check perfomance of different models <\/h1>\n    <\/a>\n<\/div>","11916a41":"<div class=\"alert alert-success\">\n<h2 style=\"color:Black;\">Inference:<\/h2>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Here, we can see that as the temperature increase, the count of riders also increases. A higher temperature would probably suggest a better weather condition, and hence more riders.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf As the windspeed increases, the number of riders also decreases. This is possibly due to bad weather conditions.<\/p>\n<\/div>","cd69853d":"<div class=\"alert alert-info\">\n    <a id=\"10\">\n    <h1 style=\"text-align:center;font-weight:20px;color:black;\"> Credits <\/h1>\n    <\/a>\n<\/div>","14398388":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf <strong>Lazy Predict<\/strong> is a module in Python, that allows users to simultaneously build a lot of models for comparision. Below picture is an example for the same dataset.<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf I have performed this operation of Lazy Predict on my local computer as this module tried to downgrade and uninstall some required packages from the Kaggle Kernal.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 Please refer <a href=\"https:\/\/pypi.org\/project\/lazypredict\/\">this<\/a> link for more information on this package.<\/p>\n<\/div>","be0d1eeb":"<div class=\"alert alert-success\">\n<li style=\"font-size:150%;color:black\"><a href=\"#1\"> Initial Data Cleaning <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#2\"> Exploratory Data Analysis(EDA) <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#3\"> Visualizing Categorical Variables <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#4\"> Visualizing Numerical Variables <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#5\"> Preparing the data before Modelling <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#6\"> Dimentionality Reduction using PCA <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#7\"> Using LazyPredict to check performance of different models <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#8\"> Building the Model <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#9\"> My Other Notebooks <\/a><\/li>\n   \n<li style=\"font-size:150%;color:black\"><a href=\"#10\"> Credits <\/a><\/li>\n    \n    \n  <h2 style=\"color:Black;\">Legend:<\/h2>\n    <p style=\"color:Black;font-size:120%\">\ud83c\udfaf: Important Steps or points<\/p>\n    <p style=\"color:Black;font-size:120%\">\ud83d\udc8e: Important tips<\/p>\n    <p style=\"color:Black;font-size:120%\">\u2b50: Reference Links<\/p>\n    <p style=\"color:Black;font-size:120%\">\ud83d\udcc4: Article, thread or informations<\/p>\n<\/div>\n\n    \n","b1b170c8":"<div class=\"alert alert-info\">\n    <a id=\"8\">\n    <h1 style=\"text-align:center;font-weight:20px;color:black;\"> Building Our Model<\/h1>\n    <\/a>\n<\/div>","b3dbb12d":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf No Null Values detected in any column<\/p>\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Now that our Initial Cleaning is done, we will head into the next step, Exploratory Data Analysis... <\/p>\n<\/div>","90e77551":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\u2b50 <a href=\"https:\/\/www.kaggle.com\/imakash3011\">Akash Patel<\/a> for this awesome dataset.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 <a href=\"https:\/\/www.kaggle.com\/mpwolke\">Marilia Prata<\/a> for motivation.<\/p>\n<p style=\"font-size:120%;color:black\">\u2b50 <a href=\"https:\/\/www.kaggle.com\/najeebahmadbhuiyan\">Najeeb Ahmed Bhuiyan<\/a> and his notebooks to learn EDA from.<\/p>","9d002526":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Will we be able to predict the count of Riders in the city today?<br><\/br> Let's find out... <\/h1>\n<\/div>","03c9df04":"<h1> Before PCA: <\/h1>\n\n![PCA.PNG](attachment:73b7de8d-d2fd-43a7-882f-991e1a544cc9.PNG)","bd20a540":"![Lazy Predict best models](attachment:90fb3e5e-5086-4792-97c9-c02707052768.png)","1aca8bfc":" <p style=\"text-align:center; font-size:150%\"><i><strong> \u201cA bicycle ride around the world begins with a single pedal stroke\u201d <\/i>\u2013 Scott Stoll <\/strong><\/p>"}}