{"cell_type":{"67847035":"code","be577cae":"code","8419bee7":"code","d7eb9ace":"code","be1235cf":"code","bda0bb70":"code","b9a98f22":"code","e6fc05ff":"code","6064ff89":"code","b82e6f85":"code","77e220d9":"code","4692881d":"code","055d4e56":"code","71fd29c6":"code","e517eaae":"code","b35e29c8":"code","392fe0d5":"code","f74d5ff1":"code","a0d1fb48":"code","21ded21d":"code","c7dac24d":"code","da27a0ef":"code","2b670f7c":"code","cdc94d3d":"code","24596c7b":"code","550f8457":"code","3cad7fec":"code","ea530982":"code","7f261a46":"code","89a31d81":"code","8df4b2f0":"code","3d66efa2":"code","b66bb6c7":"code","a3baf29e":"code","7f9ac512":"code","50034b1b":"code","a3f736f1":"code","712c49de":"code","e7a5727e":"code","d92156ed":"code","fa0d4b4e":"code","698c50ae":"code","3a9f590c":"code","dab511dc":"code","4b60a0f1":"code","f53f7473":"code","84296efb":"code","9333cb87":"code","7d8cac0f":"code","dbf81e0e":"code","163d046a":"code","8ecc656d":"code","802f9f4b":"code","5e4f821c":"code","af6eec66":"code","f9a50456":"code","c986e137":"code","993ad0e5":"code","e1d2ceba":"code","68591581":"code","7b94330f":"code","f9e1f33b":"code","f341d10f":"code","a1f9297f":"code","f665bb53":"code","1f79556b":"code","94ec7767":"code","e840f1a4":"code","1704fdac":"code","58275901":"code","3d1dbc5e":"code","0dc6ad89":"code","084f0783":"code","bb2b52e7":"code","a6706743":"code","b849efb1":"code","689d3af8":"code","a13d1ecf":"code","2e2e0d6b":"code","2f20f89f":"code","0d4848f1":"code","dc4c108a":"code","7b47f66f":"code","cb77a11d":"code","d2a6601d":"markdown","576c1c83":"markdown","2dd166b2":"markdown","c3e753af":"markdown","dc0a5c8f":"markdown","769537b0":"markdown","dd8359dc":"markdown","22e9e846":"markdown","d3fe797b":"markdown"},"source":{"67847035":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be577cae":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Disable Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# NLP functionalities and libraries\nimport re\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Classification evaluation\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, \n                             recall_score, f1_score)\n\n# Word Cloud\n#from wordcloud import WordCloud, STOPWORDS","8419bee7":"# Importing the dataset\ndataset = pd.read_csv('\/kaggle\/input\/bbc-text\/bbc_text.csv')","d7eb9ace":"dataset.head()","be1235cf":"dataset['category'].value_counts()","bda0bb70":"dataset.shape","b9a98f22":"dataset['text'][0]","e6fc05ff":"re.sub('[^a-zA-Z]',' ', \"Wow... love this place!\")","6064ff89":"news = re.sub('[^a-zA-Z]',' ', dataset['text'][0])\nnews","b82e6f85":"news = news.lower()\nnews","77e220d9":"news = news.split()\nnews","4692881d":"len(news)","055d4e56":"stopword_list=stopwords.words('english')\nstopword_list","71fd29c6":"ss = SnowballStemmer(language='english')\nnews = [ss.stem(word) for word in news if not word in set(stopwords.words('english'))]\nlen(news)","e517eaae":"news","b35e29c8":"news = ' '.join(news)\nnews","392fe0d5":"def clean_text(text):\n    news = re.sub('[^a-zA-Z]', ' ',text)\n    news = news.lower()\n    news = news.split()\n    ss = SnowballStemmer(language='english')\n    news = [ss.stem(word) for word in news if not word in set(stopwords.words('english'))]\n    return ' '.join(news)","f74d5ff1":"dataset[\"clean_text\"] = dataset[\"text\"].map(lambda x: clean_text(x))\ndataset.head()","a0d1fb48":"corpus = dataset[\"clean_text\"].tolist()","21ded21d":"corpus[:5]","c7dac24d":"# Creating the Bag of Words model with Count Vectors\ncv = CountVectorizer(max_features=5000)","da27a0ef":"#vocab = cv.vocabulary_\nX = cv.fit_transform(corpus).toarray()\ny = dataset['category'].values","2b670f7c":"vocab_cv = cv.vocabulary_","cdc94d3d":"vocab_cv","24596c7b":"len(vocab_cv)","550f8457":"X.shape","3cad7fec":"X[0,:]","ea530982":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\ny","7f261a46":"le.classes_","89a31d81":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","8df4b2f0":"# Logistic Reegression\nfrom sklearn.linear_model import LogisticRegression\nlr_cv = LogisticRegression()\nlr_cv.fit(X_train, y_train)","3d66efa2":"from sklearn.naive_bayes import MultinomialNB\nmnb_cv = MultinomialNB()\nmnb_cv.fit(X_train, y_train)","b66bb6c7":"# Predicting the Test set results\ny_pred_lr = lr_cv.predict(X_test)\ny_pred_mnb = mnb_cv.predict(X_test)","a3baf29e":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\ncm = confusion_matrix(y_test, y_pred_lr)\ncm","7f9ac512":"ac = accuracy_score(y_test, y_pred_lr)\nac","50034b1b":"print(classification_report(y_test, y_pred_lr))","a3f736f1":"cm = confusion_matrix(y_test, y_pred_mnb)\ncm","712c49de":"ac = accuracy_score(y_test, y_pred_mnb)\nac","e7a5727e":"print(classification_report(y_test, y_pred_mnb))","d92156ed":"# Creating the Bag of Words model Tf-Idf\ntfidf = TfidfVectorizer(max_features=5000)\nX = tfidf.fit_transform(corpus).toarray()\nvocab_tf = tfidf.vocabulary_","fa0d4b4e":"len(vocab_tf)","698c50ae":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","3a9f590c":"# Logistic Reegression\nlr_tf = LogisticRegression()\nlr_tf.fit(X_train, y_train)","dab511dc":"mnb_tf = MultinomialNB()\nmnb_tf.fit(X_train, y_train)","4b60a0f1":"# Predicting the Test set results\ny_pred_lr = lr_tf.predict(X_test)\ny_pred_mnb = mnb_tf.predict(X_test)","f53f7473":"cm = confusion_matrix(y_test, y_pred_lr)\ncm","84296efb":"ac = accuracy_score(y_test, y_pred_lr)\nac","9333cb87":"print(classification_report(y_test, y_pred_lr))","7d8cac0f":"cm = confusion_matrix(y_test, y_pred_mnb)\ncm","dbf81e0e":"ac = accuracy_score(y_test, y_pred_mnb)\nac","163d046a":"print(classification_report(y_test, y_pred_mnb))","8ecc656d":"import gensim\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\n# Others\nimport re\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\nfrom sklearn.manifold import TSNE\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom bs4 import BeautifulSoup as soup\nfrom nltk.stem.snowball import SnowballStemmer","802f9f4b":"# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, load_model, Model\nfrom keras.utils import plot_model\nfrom keras.layers import Flatten, Dropout, Activation, Input, Dense, concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant","5e4f821c":"dummy_y = pd.get_dummies(dataset['category']).values\ndummy_y[:10]","af6eec66":"dummy_y.shape","f9a50456":"def clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) > 3]\n    \n    text = \" \".join(text)\n    \n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","c986e137":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset['clean_text'].values, \n                                                    dummy_y, \n                                                    test_size = 0.20, \n                                                    random_state = 0)","993ad0e5":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)","e1d2ceba":"tokenizer.word_index","68591581":"trainsequences = tokenizer.texts_to_sequences(X_train)\nprint(trainsequences)","7b94330f":"X_train.shape","f9e1f33b":"len(trainsequences)","f341d10f":"avg_len = [len(seq) for seq in trainsequences]\nmean_len = np.mean(avg_len)\nmean_len","a1f9297f":"MAXLEN = 220","f665bb53":"trainseqs = pad_sequences(trainsequences, maxlen=MAXLEN, padding='post')\nprint(trainseqs)","1f79556b":"trainseqs.shape","94ec7767":"testsequences = tokenizer.texts_to_sequences(X_test)\ntestseqs = pad_sequences(testsequences, maxlen=MAXLEN, padding='post')","e840f1a4":"print(testseqs)","1704fdac":"testseqs.shape","58275901":"y_test.shape","3d1dbc5e":"EMBEDDING_SIZE = 8","0dc6ad89":"VOCAB_SIZE = len(tokenizer.word_index) + 1\nVOCAB_SIZE","084f0783":"OP_UNITS = dataset['category'].nunique()","bb2b52e7":"# define the model\nmodel = Sequential()\nembedding_layer = Embedding(VOCAB_SIZE,\n                            EMBEDDING_SIZE,\n                            input_length=MAXLEN)\nmodel.add(embedding_layer)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(OP_UNITS, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","a6706743":"print(model.summary())","b849efb1":"# fit the model\nhistory = model.fit(trainseqs,\n                    y_train,\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(testseqs,y_test),\n                    verbose=1).history","689d3af8":"res_df = pd.DataFrame(history)\nres_df.head()","a13d1ecf":"# Plot training vs validation Loss\nplt.plot(res_df['loss'],label=\"Training\")\nplt.plot(res_df['val_loss'],label=\"Validation\")\nplt.legend(loc='best')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training vs Validation Loss')","2e2e0d6b":"# Plot training vs validation Accuracy\nplt.plot(res_df['accuracy'],label=\"Training\")\nplt.plot(res_df['val_accuracy'],label=\"Validation\")\nplt.legend(loc='best')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training vs Validation Accuracy')","2f20f89f":"model.layers","0d4848f1":"model.layers[0].get_weights()[0].shape","dc4c108a":"model.layers[0].get_weights()[0][0]","7b47f66f":"# Extract weights from the Embedding Layers\nembeddings = model.layers[0].get_weights()[0]\n\n# `embeddings` has a shape of (num_vocab, embedding_dim) \n\n# `word_to_index` is a mapping (i.e. dict) from words to \n# their index\nwords_embeddings = {w:embeddings[idx - 1] for w, idx in tokenizer.word_index.items()}","cb77a11d":"words_embeddings['play']","d2a6601d":"## Bag of Words - Approach","576c1c83":"## Document Classification and Embeddings","2dd166b2":"#### Keras text processing","c3e753af":"#### Save the dictonary ==>> \"word_embeddings\" as .txt file","dc0a5c8f":"# NLP","769537b0":"#### Standard Keras Embedding","dd8359dc":"#### Bag of Words Model - CountVectorizer","22e9e846":"**Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.**","d3fe797b":"#### Bag of Words Model - TfidfVectorizer"}}