{"cell_type":{"741f03af":"code","c5207861":"code","fad60212":"code","9163c635":"code","df4a7e10":"code","175fc0dd":"code","4366c536":"code","eb40d848":"code","619497b1":"code","986735dd":"code","18f86a87":"code","50bd6594":"code","7df67791":"code","ccfbea3d":"code","1d620663":"code","866edf25":"code","1140eb8e":"code","abf3b3b9":"code","439613ae":"code","c025bf69":"code","a029a2d3":"markdown","e7bb5d27":"markdown","f883a1af":"markdown","bb1f62dc":"markdown"},"source":{"741f03af":"!apt-get install python-opengl -y\n\n!apt install xvfb -y\n\n!pip install pyvirtualdisplay\n\n!pip install piglet","c5207861":"import gym\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.distributions import Categorical\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fad60212":"num_episodes = 500 #Mudar para 10000\nverbose = True\nprint_every = 100\ntarget_avg_reward_100ep = 10\nrewards = []\nrunning_rewards = []\nrestore_model = True\nsuccesses = 0\n\ngamma=0.95\nepsilon=.2","9163c635":"from pyvirtualdisplay import Display\nDisplay().start()\n\nimport gym\nfrom IPython import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nenv = gym.make('MountainCar-v0')\nenv.reset()\n# img = plt.imshow(env.render('rgb_array'))","df4a7e10":"class Policy(nn.Module):\n    \"\"\"\n    Define a politica para tomar uma a\u00e7\u00e3o a partir de um estado\n    \"\"\"\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.num_actions = env.action_space.n\n        self.state_dim = env.observation_space.shape[0]\n        self.fc1 = torch.nn.Linear(self.state_dim,64,'relu')\n        self.fc2 = torch.nn.Linear(64,self.num_actions,'linear')\n        \n    def forward(self,x):\n        x = F.relu(self.fc1(x))\n        y = self.fc2(x)\n        return y","175fc0dd":"# Usado como base: https:\/\/github.com\/orrivlin\/MountainCar_DQN_RND\nclass Policy2(nn.Module):\n    \"\"\"\n    Define a politica para tomar uma a\u00e7\u00e3o a partir de um estado\n    \"\"\"\n    def __init__(self):\n        super(Policy2, self).__init__()\n        self.num_actions = env.action_space.n\n        self.state_dim = env.observation_space.shape[0]    \n        self.fc1 = torch.nn.Linear(self.state_dim,64,'linear')\n        self.hidden1 = nn.Dropout(0.1)\n        self.fc2 = torch.nn.Linear(64,64,'linear')\n        self.hidden2 = nn.Dropout(0.08)\n        self.fc3 = torch.nn.Linear(64,32,'linear')\n        self.hidden3 = nn.Dropout(0.05)\n        self.fc4 = torch.nn.Linear(32,16,'linear')\n        self.fc5 = torch.nn.Linear(16,self.num_actions,'linear')\n        \n    def forward(self,x):\n        \n        x = self.hidden1(self.fc1(x))\n        x = self.hidden2(F.relu(self.fc2(x)))\n        x = self.hidden3(F.relu(self.fc3(x)))        \n        x = F.relu(self.fc4(x))\n        y = F.relu(self.fc5(x))\n        return y","4366c536":"def get_policy_values(state,policy):\n    \"\"\"\n    Calcula o valor de pol\u00edtica (a\u00e7\u00e3o) a partir do estado\n    \"\"\"\n    state = Variable(torch.from_numpy(state)).type(torch.FloatTensor).unsqueeze(0)\n    policy_values = policy(state)\n    return policy_values","eb40d848":"# Utliziado como base: https:\/\/medium.com\/@ts1829\/solving-mountain-car-with-q-learning-b77bf71b1de2\ndef generate_episode(policy,loss_fn, optimizer, successes, epsilon, gamma, t_max=1000):\n    \"\"\"\n    Gera um episodio e salva estados, a\u00e7\u00f5es, recompensas e log prob para atualizar pol\u00edtica\n    Entrada: passos m\u00e1ximos no epis\u00f3dio\n    \"\"\"\n    states, actions, rewards, log_probs = [], [], [], []\n    state = env.reset()\n    \n    for t in range(t_max):\n        Q = get_policy_values(state,policy)\n        \n        if np.random.rand() <= epsilon:\n            action = random.randrange(policy.num_actions)\n        else:\n            _, action = torch.max(Q, -1)\n            action = action.item()\n            \n            \n       # Step forward and receive next state and reward\n        state_1, reward, done, _ = env.step(action)\n        \n        # Adjust reward based on car position\n        reward = state_1[0] + 0.5\n        \n        # Adjust reward for task completion\n        if state_1[0] >= 0.5:\n            reward += 1\n            print(\"Solved \",done)\n        \n        # Find max Q for t+1 state\n        Q1 = get_policy_values(state_1,policy)\n        maxQ1, _ = torch.max(Q1, -1)\n        \n        # Create target Q value for training the policy\n        Q_target = Q.clone()\n        Q_target = Variable(Q_target)\n        Q_target[0][action] = reward + torch.mul(maxQ1.detach(), gamma)\n        \n        # Calculate loss\n        loss = loss_fn(Q, Q_target)\n        \n        # Update policy\n        policy.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        rewards.append(reward)\n\n        if done:\n            if state_1[0] >= 0.5:\n                epsilon *= .9\n                successes += 1\n            break\n        else:\n            state = state_1\n    return policy, loss_fn, rewards, epsilon, successes","619497b1":"def play_episodes(policy):\n    state = env.reset()\n    img = plt.imshow(env.render('rgb_array')) # only call this once\n\n    done = False\n    while not done:\n        img.set_data(env.render('rgb_array')) # just update the data\n        display.display(plt.gcf())\n        display.clear_output(wait=True)\n        \n        Q = get_policy_values(state,policy)\n        _, action = torch.max(Q, -1)\n        state,reward,done,_ = env.step(action.item())","986735dd":"def plot_rewards(rewards, running_rewards):\n    \"\"\"\n    Mostra recompensa m\u00e9dia (\u00faltimos 100) no decorrer da execu\u00e7\u00e3o\n    \"\"\"\n    plt.style.use('seaborn-darkgrid')\n    fig = plt.figure(figsize=(12,7))\n    ax1 = fig.add_subplot(2, 1, 1)\n    ax2 = fig.add_subplot(2, 1, 2)\n    plt.subplots_adjust(hspace=.5)\n    \n    ax1.set_title('Episodic rewards')\n    ax1.plot(rewards, label='Episodic rewards')\n    ax1.set_xlabel(\"Episodes\")\n    ax1.set_ylabel(\"Rewards\")\n    \n    ax2.set_title('Running rewards')\n    ax2.plot(running_rewards, label='Running rewards')\n    ax2.set_xlabel(\"Episodes\")\n    ax2.set_ylabel(\"Average rewards\")\n    \n    plt.show(fig)","18f86a87":"def test(policy, episode, t_max=1000):\n    if episode % 100 == 0 and episode > 10:\n        total_reward = []\n        for i in range(10):\n            state = env.reset()\n            for j in range(t_max):\n                Q = get_policy_values(state,policy)\n                _, action = torch.max(Q, -1)\n                state,reward,done,_ = env.step(action.item())\n                  \n                # Adjust reward based on car position\n                reward = state[0] + 0.5\n\n                # Adjust reward for task completion\n                if state[0] >= 0.5:\n                    reward += 1\n                    print(\"Solved \",done)\n                total_reward.append(reward)\n                if done:\n                    break\n        ave_reward = np.mean(total_reward)\n        print(\"Test: Episode: {}. Running reward: {}\".format(episode, ave_reward))\n        return ave_reward > .3\n    return False","50bd6594":"def train(policy,loss_fn, optimizer, num_episodes = 1000):\n    rewards = []\n    running_rewards = []\n    successes = 0\n    epsilon = .1\n    gamma=0.9\n    for i in range(num_episodes):\n        \n        state = env.reset()\n        \n        policy,loss_fn,reward,epsilon,successes = generate_episode(policy,loss_fn,optimizer, successes, epsilon, gamma) \n        rewards.append(sum(reward))   \n        running_reward = np.mean(reward)\n        running_rewards.append(running_reward)\n        \n        if verbose:\n            if not i % print_every:\n                print(\"Episode: {}. Running reward: {}. Epsilon: {}\".format(i+1, running_reward, epsilon))\n\n        if test(policy, i):\n            print(\"Ran {} episodes. Solved after {} episodes.\".format(i+1, i-100+1))\n            break\n            \n        if i == num_episodes-1:\n            print(\"Couldn't solve after {} episodes\".format(num_episodes))\n            \n    print('successful episodes: {:d} - {:.4f}%'.format(successes, successes\/num_episodes*100))\n    return rewards, running_rewards","7df67791":"verbose = True\nprint_every = 100\nrunning_reward = None\nrewards = []\nrunning_rewards = []\nrestore_model = True","ccfbea3d":"#print(\"shallow learning\")\n#state = env.reset()\n#policy = Policy()\n#loss_fn = nn.MSELoss()\n#optimizer = optim.Adam(policy.parameters(), lr=0.001)\n#rewards,running_rewards = train(policy, loss_fn, optimizer, num_episodes=1000)","1d620663":"#plot_rewards(rewards, running_rewards)","866edf25":"# play_episodes(policy)","1140eb8e":"verbose = True\nprint_every = 100\nrunning_reward = None\nrewards = []\nrunning_rewards = []\nrestore_model = True","abf3b3b9":"#print(\"shallow learning 2\")\n#state = env.reset()\n#policy2 = Policy2()\n#loss_fn = nn.MSELoss()\n#optimizer2 = optim.Adam(policy2.parameters(), lr=0.001)\n#rewards,running_rewards = train(policy2, loss_fn, optimizer2,num_episodes=4000)","439613ae":"#plot_rewards(rewards, running_rewards)","c025bf69":"# play_episodes(policyDQN)","a029a2d3":"#### Treinamento da shallow learning","e7bb5d27":"Fabricio Torquato - 153124","f883a1af":"#### Treinamento da shallow learning 2","bb1f62dc":"Utilizar a estrat\u00e9gia DQN para resolver o problema MountainCar da biblioteca OpenAI-Gym (https:\/\/gym.openai.com\/envs\/MountainCar-v0\/ (Links para um site externo.))\n\nA solu\u00e7\u00e3o deve ser baseada no c\u00f3digo visto em aula e a entrega \u00e9 em grupo, m\u00e1ximo de 4 integrantes (1 pessoa apenas deve enviar).\n\nA entrega deve ter o c\u00f3digo \/ link da implementa\u00e7\u00e3o e um relat\u00f3rio analisando a evolu\u00e7\u00e3o do treinamento da rede de pol\u00edtica considerando diferentes arquiteturas."}}