{"cell_type":{"d3e1c705":"code","4cca55e3":"code","7512bbad":"code","39b47d84":"code","c4160cf6":"code","50cb8f84":"code","c3d2fbbb":"code","76c45063":"code","285e1383":"code","cb10dd92":"code","6c02e637":"code","0723d731":"code","cd759333":"code","19fa96d5":"code","aafb1279":"code","8506f624":"code","93067399":"code","12e1c838":"code","404694cb":"code","32278747":"markdown","67cbcae1":"markdown","8ce0e929":"markdown","6a49070c":"markdown","5aeaca58":"markdown","3bb90480":"markdown","474d3bd4":"markdown","a122c76c":"markdown","5c00cb5b":"markdown","fddeaadf":"markdown","b4d4db40":"markdown","6526ba1b":"markdown"},"source":{"d3e1c705":"#!pip install albumentations\nimport albumentations\n\n\nimport torch\nfrom albumentations import ( Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, IAAAdditiveGaussianNoise, Transpose, ToGray )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport matplotlib.pyplot as plt\n\nseed = 42\n\nimport pandas as pd\nimport os\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm","4cca55e3":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1\/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","7512bbad":"train_path = '..\/input\/ranzcr-clip-catheter-line-classification\/train'\ntrain_files = os.listdir(train_path)\n\ntrain_df = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train.csv')\n\ntrain = train_df.reset_index(drop=True) # reset index on both dataframes\n\nprint(train.shape)\n\nnum_channel = 3\n\nimg_size = 255","39b47d84":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\t#image = image*(1\/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","c4160cf6":"\ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 16 , num_workers = 3 , shuffle = True)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 3:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(4,4, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            ","50cb8f84":"mean_list = [[ 0.4914168 , 0.4914168 , 0.4914168] , [0.485,0.456,0.406] , [0.9,0.9,0.9] , 0.496 ]\nstd_list = [[0.407278, 0.407278 , 0.407278] , [0.229,0.224,0.225] , [0.9,0.9,0.9] , 0.407278]\n\nmean = mean_list[0]\nstd=  std_list[0]\n\nprint(mean , std)","c3d2fbbb":"train_augs = albumentations.Compose([albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     albumentations.Normalize(mean= mean ,std= std ,),\n                                    ])\n\nShow_Xrays(train_augs)","76c45063":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),                            \n                                    ])\nShow_Xrays(train_augs)","285e1383":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        #albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","cb10dd92":"train_augs = albumentations.Compose([   albumentations.Normalize(mean= mean ,std= std ,) , \n                                         albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        #albumentations.CLAHE(clip_limit=(1,10), p= 1)\n                                        \n                                    ])\nShow_Xrays(train_augs)","6c02e637":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                #albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.3), lightness=(0.5, 0.7), p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","0723d731":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t#albumentations.ShiftScaleRotate(shift_limit_x=(-0.0125, 0.0125),shift_limit_y=(-0.0125, 0.0125) ,rotate_limit=(-15, 15) , p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                ########albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t#####albumentations.ElasticTransform(alpha=3),\n                #albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                #albumentations.MotionBlur(p=1),\n                #albumentations.MedianBlur(p=1),\n                #albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                ###albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","cd759333":"augment_list = [albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.ShiftScaleRotate(p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\talbumentations.ElasticTransform(alpha=3),\n                albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=10, p= 0.5),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","19fa96d5":"augment_list = [  albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t  ########albumentations.HorizontalFlip(p=1),\n\t\t\t\t  albumentations.ShiftScaleRotate(p=1),\n\t\t\t\t   albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\t   albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\t   albumentations.CLAHE(clip_limit=(1,4), p=1),\n                   albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t   #albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n\t\t\t\t   albumentations.ElasticTransform(alpha=3),\n                 albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                 #albumentations.GaussianBlur(p=1),\n                 albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                #albumentations.imgaug.transforms.IAASuperpixels(p_replace=0.1, n_segments=100, p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                ######albumentations.augmentations.domain_adaptation.HistogramMatching (reference_images , blend_ratio=(0.5, 1.0), p=0.5),\n                #albumentations.augmentations.transforms.ToSepia(p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.imgaug.transforms.IAAAdditiveGaussianNoise(loc=0, scale=(2.5500000000000003, 12.75), per_channel=False, p=1),\n                \n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.augmentations.transforms.Solarize(threshold=128, p=1),\n                #albumentations.augmentations.transforms.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, p=1),\n                #albumentations.augmentations.transforms.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, p=1),\n\t\t\t\t#albumentations.Cutout(max_h_size=int(img_size * 0.1), max_w_size=int(img_size * 0.1), num_holes=5, p=1),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","aafb1279":"def Show_save_Xrays(augmentation , name):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1 ,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            fig.savefig( str(name) + '.png', dpi=200)","8506f624":"Save_to_folder = False\n\n\nif Save_to_folder == True:\n    for i in range(len(augment_list)):\n        train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size) , augment_list[i]])\n        Show_save_Xrays(train_augs , augment_list[i])","93067399":"!zip -r -q 'Albumentation_1*1.zip'  .\/","12e1c838":"!rm .\/*.zip","404694cb":"!rm .\/*.png","32278747":"# Baseline","67cbcae1":"### Findings- \nIf we are training from scratch, we need to find a new mean,std.\nIf using pretrained weights, then most likely stick to the values on pretraining dataset.\n\nNotebook on how to find your own mean and std deviation ? https:\/\/www.kaggle.com\/amritpal333\/using-custom-mean-std-ranzcr-comp","8ce0e929":"https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RandomRain","6a49070c":"# Normalizing data first","5aeaca58":"\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\t\n    def __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\tif self.num_channels == 1:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ), 0)\n        \n\t\telse:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = image #*(1\/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","3bb90480":"# Trying differnt combinations of image augmentation\n\n(I hope to find one that magically works!)","474d3bd4":"# defining your own dataset\n\nRather than looking into images from folder at random, i want to look at the images as they are sent to the model.\nthis removes any scope of error.","a122c76c":"# Saving all varints of augmentation to folder\n\nto download later on","5c00cb5b":"make a smaller version of list , the ones you are interested in.","fddeaadf":"###  Findings- CLAHE is awesome\nCLAHE works like magic to improve the visualisation of the catheters.\nYou can look into my detailed analysis on how CLAHE effects xrays in this competition in my notebook -\n\n### [link](https:\/\/www.kaggle.com\/amritpal333\/clahe-augmentation-ranzcr-comp)","b4d4db40":"# Understanding Image Augmentation\n\nYou can look into my other notebooks to get more detailed augmentation insights.\nlist of my other notebooks - \n\n1. Understanding Image Augmentation - [link](https:\/\/www.kaggle.com\/amritpal333\/understanding-image-augmentation-ranzcr-comp)\n2. CLAHE augmentation  - [link](https:\/\/www.kaggle.com\/amritpal333\/clahe-augmentation-ranzcr-comp)\n3. Findings your own mean,standerd deviation for the dataset - [link](https:\/\/www.kaggle.com\/amritpal333\/using-custom-mean-std-ranzcr-comp)\n\n\n## Upvote the notebooks if you find them insightful.","6526ba1b":"### findings - Normalizing images before using other augmentation works only in simple cases, and raises an error in others.\n\ne.g. using CLAHE on normalized data\n>     raise TypeError(\"clahe supports only uint8 inputs\")\n>     TypeError: clahe supports only uint8 inputs\n\nSo, use Normalize as the last in the list of your augmentations."}}