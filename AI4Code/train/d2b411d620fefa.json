{"cell_type":{"ee4dea03":"code","639caf62":"code","b7be47c8":"code","fd3d2f5b":"code","e90c5748":"code","f91d5876":"code","d4301f13":"code","ee2a3ca4":"code","906cfb47":"code","9f9ee61a":"code","a108d079":"code","6f2fcd5d":"code","58dd393c":"code","bade2014":"code","f4f794e0":"code","080d4748":"code","9f29afa5":"code","bff9e1ec":"code","897e43f1":"code","2f5bb73d":"code","cedeec49":"code","785532d0":"code","126c641a":"code","cd493609":"code","fd557fe8":"code","fd3f8413":"code","508294ab":"code","e501098d":"code","48ea93d6":"code","a583fab0":"code","8cbdb87a":"code","1e817b8a":"code","38dee5f4":"code","cac4c524":"code","6a7d7896":"code","e6cbc399":"code","d809726a":"code","d3782d1f":"code","b4cd957d":"code","6a3037c7":"code","3b966ea0":"code","64193296":"code","0648249a":"code","f6752004":"code","7e9580f2":"code","3a3b15b9":"code","a74dda87":"code","7aea5e49":"code","77eca01a":"code","a28eb366":"code","c98e324a":"code","e8605151":"code","13cfaefb":"code","1c9d0289":"code","cb54a4ab":"code","806edc24":"code","83b163c4":"code","8d12e2f1":"code","68612fde":"markdown","86034926":"markdown","7e884d4c":"markdown","f9b66308":"markdown","be2f652c":"markdown","1c5da325":"markdown","7a72762c":"markdown","9a2177e8":"markdown","baffb8d9":"markdown","d70ac327":"markdown","03fad41c":"markdown","bed3beec":"markdown","81183ca1":"markdown","2cf28a60":"markdown","2af2bea5":"markdown","3fa62268":"markdown","65198d7a":"markdown","ebaf9020":"markdown"},"source":{"ee4dea03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, model_selection, metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","639caf62":"data = pd.read_csv(\"..\/input\/KAG_energydata_complete.csv\")","b7be47c8":"data.head()","fd3d2f5b":"data.info()","e90c5748":"data.describe()","f91d5876":"print('The number of rows in dataset is - ' , data.shape[0])\nprint('The number of columns in dataset is - ' , data.shape[1])","d4301f13":"#Number of null values in all columns\ndata.isnull().sum().sort_values(ascending = True)","ee2a3ca4":"from sklearn.model_selection import train_test_split\n\n# 75% of the data is usedfor the training of the models and the rest is used for testing\ntrain, test = train_test_split(data,test_size=0.25,random_state=40)","906cfb47":"train.describe()","9f9ee61a":"# Divide the columns based on type for clear column management \n\ncol_temp = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\"]\n\ncol_hum = [\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_5\",\"RH_6\",\"RH_7\",\"RH_8\",\"RH_9\"]\n\ncol_weather = [\"T_out\", \"Tdewpoint\",\"RH_out\",\"Press_mm_hg\",\n                \"Windspeed\",\"Visibility\"] \ncol_light = [\"lights\"]\n\ncol_randoms = [\"rv1\", \"rv2\"]\n\ncol_target = [\"Appliances\"]","a108d079":"# Seperate dependent and independent variables \nfeature_vars = train[col_temp + col_hum + col_weather + col_light + col_randoms ]\ntarget_vars = train[col_target]","6f2fcd5d":"feature_vars.describe()","58dd393c":"# Check the distribution of values in lights column\nfeature_vars.lights.value_counts()","bade2014":"target_vars.describe()","f4f794e0":"# Due to lot of zero enteries this column is of not much use and will be ignored in rest of the model\n_ = feature_vars.drop(['lights'], axis=1 , inplace= True) ;","080d4748":"feature_vars.head(2)","9f29afa5":"# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= data.date  ,  mode = \"lines\", y = data.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption measurement' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","bff9e1ec":"# Adding column to mark weekdays (0) and weekends(1) for time series evaluation , \n# decided not to use it for model evaluation as it has least impact\n\ndata['WEEKDAY'] = ((pd.to_datetime(data['date']).dt.dayofweek)\/\/ 5 == 1).astype(float)\n# There are 5472 weekend recordings \ndata['WEEKDAY'].value_counts()","897e43f1":"# Find rows with weekday \ntemp_weekday =  data[data['WEEKDAY'] == 0]\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= temp_weekday.date  ,  mode = \"lines\", y = temp_weekday.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption measurement on weekdays' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","2f5bb73d":"# Find rows with weekday \n\ntemp_weekend =  data[data['WEEKDAY'] == 1]\n\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= temp_weekend.date  ,  mode = \"lines\", y = temp_weekend.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption measurement on weekend' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","cedeec49":"# Histogram of all the features to understand the distribution\nfeature_vars.hist(bins = 20 , figsize= (12,16)) ;","785532d0":"# focussed displots for RH_6 , RH_out , Visibility , Windspeed due to irregular distribution\nf, ax = plt.subplots(2,2,figsize=(12,8))\nvis1 = sns.distplot(feature_vars[\"RH_6\"],bins=10, ax= ax[0][0])\nvis2 = sns.distplot(feature_vars[\"RH_out\"],bins=10, ax=ax[0][1])\nvis3 = sns.distplot(feature_vars[\"Visibility\"],bins=10, ax=ax[1][0])\nvis4 = sns.distplot(feature_vars[\"Windspeed\"],bins=10, ax=ax[1][1])","126c641a":"# Distribution of values in Applainces column\nf = plt.figure(figsize=(12,5))\nplt.xlabel('Appliance consumption in Wh')\nplt.ylabel('Frequency')\nsns.distplot(target_vars , bins=10 ) ;","cd493609":"#Appliance column range with consumption less than 200 Wh\nprint('Percentage of the appliance consumption is less than 200 Wh')\nprint(((target_vars[target_vars <= 200].count()) \/ (len(target_vars)))*100 )","fd557fe8":"# Use the weather , temperature , applainces and random column to see the correlation\ntrain_corr = train[col_temp + col_hum + col_weather +col_target+col_randoms]\ncorr = train_corr.corr()\n# Mask the repeated values\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n  \nf, ax = plt.subplots(figsize=(16, 14))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, annot=True, fmt=\".2f\" , mask=mask,)\n    #Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n    #Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n    #show plot\nplt.show()","fd3f8413":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# Function to get top correlations \n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_corr, 40))","508294ab":"#Split training dataset into independent and dependent varibales\ntrain_X = train[feature_vars.columns]\ntrain_y = train[target_vars.columns]","e501098d":"#Split testing dataset into independent and dependent varibales\ntest_X = test[feature_vars.columns]\ntest_y = test[target_vars.columns]","48ea93d6":"# Due to conlusion made above below columns are removed\ntrain_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\"],axis=1 , inplace=True)","a583fab0":"# Due to conlusion made above below columns are removed\ntest_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\"], axis=1, inplace=True)","8cbdb87a":"train_X.columns","1e817b8a":"test_X.columns","38dee5f4":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\n# Create test and training set by including Appliances column\n\ntrain = train[list(train_X.columns.values) + col_target ]\n\ntest = test[list(test_X.columns.values) + col_target ]\n\n# Create dummy test and training set to hold scaled values\n\nsc_train = pd.DataFrame(columns=train.columns , index=train.index)\n\nsc_train[sc_train.columns] = sc.fit_transform(train)\n\nsc_test= pd.DataFrame(columns=test.columns , index=test.index)\n\nsc_test[sc_test.columns] = sc.fit_transform(test)\n","cac4c524":"sc_train.head()","6a7d7896":"sc_test.head()","e6cbc399":"# Remove Appliances column from traininig set\n\ntrain_X =  sc_train.drop(['Appliances'] , axis=1)\ntrain_y = sc_train['Appliances']\n\ntest_X =  sc_test.drop(['Appliances'] , axis=1)\ntest_y = sc_test['Appliances']","d809726a":"train_X.head()","d3782d1f":"train_y.head()","b4cd957d":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\n","6a3037c7":"models = [\n           ['Lasso: ', Lasso()],\n           ['Ridge: ', Ridge()],\n           ['KNeighborsRegressor: ',  neighbors.KNeighborsRegressor()],\n           ['SVR:' , SVR(kernel='rbf')],\n           ['RandomForest ',RandomForestRegressor()],\n           ['ExtraTreeRegressor :',ExtraTreesRegressor()],\n           ['GradientBoostingClassifier: ', GradientBoostingRegressor()] ,\n           ['XGBRegressor: ', xgb.XGBRegressor()] ,\n           ['MLPRegressor: ', MLPRegressor(  activation='relu', solver='adam',learning_rate='adaptive',max_iter=1000,learning_rate_init=0.01,alpha=0.01)]\n         ]\n","3b966ea0":"# Run all the proposed models and update the information in a list model_data\nimport time\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\nmodel_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model.random_state = 78\n    curr_model_data[\"Name\"] = name\n    start = time.time()\n    curr_model.fit(train_X,train_y)\n    end = time.time()\n    curr_model_data[\"Train_Time\"] = end - start\n    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(train_y,curr_model.predict(train_X))\n    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(test_y,curr_model.predict(test_X))\n    curr_model_data[\"Test_RMSE_Score\"] = sqrt(mean_squared_error(test_y,curr_model.predict(test_X)))\n    model_data.append(curr_model_data)","64193296":"model_data","0648249a":"# Convert list to dataframe\ndf = pd.DataFrame(model_data)","f6752004":"df","7e9580f2":"df.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;","3a3b15b9":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{\n              'max_depth': [80, 150, 200,250],\n              'n_estimators' : [100,150,200,250],\n              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n            }]\nreg = ExtraTreesRegressor(random_state=40)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\ngrid_search.fit(train_X, train_y)","a74dda87":"# Tuned parameter set\ngrid_search.best_params_","7aea5e49":"# Best possible parameters for ExtraTreesRegressor\ngrid_search.best_estimator_","77eca01a":"# R2 score on training set with tuned parameters\n\ngrid_search.best_estimator_.score(train_X,train_y)","a28eb366":"# R2 score on test set with tuned parameters\ngrid_search.best_estimator_.score(test_X,test_y)","c98e324a":"# RMSE score on test set with tuned parameters\n\nnp.sqrt(mean_squared_error(test_y, grid_search.best_estimator_.predict(test_X)))","e8605151":"# Get sorted list of features in order of importance\nfeature_indices = np.argsort(grid_search.best_estimator_.feature_importances_)","13cfaefb":"importances = grid_search.best_estimator_.feature_importances_\nindices = np.argsort(importances)[::-1]\nnames = [train_X.columns[i] for i in indices]\n# Create plot\nplt.figure(figsize=(10,6))\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(train_X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(train_X.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()","1c9d0289":"# Get top 5 most important feature \nnames[0:5]","cb54a4ab":"# Get 5 least important feature \nnames[-5:]","806edc24":"# Reduce test & training set to 5 feature set\ntrain_important_feature = train_X[names[0:5]]\ntest_important_feature = test_X[names[0:5]]","83b163c4":"# Clone the Gridsearch model with his parameter and fit on reduced dataset\n\nfrom sklearn.base import clone\ncloned_model = clone(grid_search.best_estimator_)\ncloned_model.fit(train_important_feature , train_y)","8d12e2f1":"# Reduced dataset scores \n\nprint('Training set R2 Score - ', metrics.r2_score(train_y,cloned_model.predict(train_important_feature)))\nprint('Testing set R2 Score - ', metrics.r2_score(test_y,cloned_model.predict(test_important_feature)))\nprint('Testing set RMSE Score - ', np.sqrt(mean_squared_error(test_y, cloned_model.predict(test_important_feature))))\n","68612fde":"# Data Pre Processing","86034926":"### Observations \n\n1. Based on parameter tunning step we can see that \n\n    a. 5 most important features are - 'RH_out', 'RH_8', 'RH_1', 'T3', 'RH_3'\n    \n    b. 5 least important features are - 'T7','Tdewpoint','Windspeed','T1','T5'\n    \n\n2. As can be observed with R2 Score , compared to Tuned model 0.63 the R2 score has come down to 0.47 which is decrease of 16% .\n\n\n3. The reduction in R2 score is high and we should not use reduced feature set for this data set","7e884d4c":"### Observations \n\n1. Temperature - All the columns follow normal distribution except T9\n2. Humidity - All columns follow normal distribution except RH_6 and RH_out , primarly because these sensors are outside the house \n3. Appliance - This column is postively skewed , most the values are around mean 100 Wh . There are outliers in this column \n4. Visibilty - This column is negatively skewed\n5. Windspeed - This column is postively skewed\n","f9b66308":"### As shown above , there are no null values in the dataset ","be2f652c":"### Observations\n\nBased on parameter tunning step we can see that \n\n1. Best possible parameter combination are - 'max_depth': 80, 'max_features': 'sqrt', 'n_estimators': 200\n\n    \n2. Training set  R2 score of 1.0 may be signal of overfitting on training set \n\n\n3. Test set R2 score is 0.63 improvement over 0.57 achieved using untuned model\n\n\n4. Test set RMSE score is 0.60 improvement over 0.65 achieved using untuned model \n\n\n","1c5da325":"# Model Implementation\n\nWe will be looking at following Algorithms \n\n**Improved Linear regression models**\n\n1.Ridge regression \n\n2.Lasso regression \n\n**Support Vector Machine**\n\n3.Support vector regression \n\n**Nearest neighbour Regressor**\n\n4.KNeighborsRegressor\n\n**Ensmble models**\n\n5.Random Forest Regressor\n\n6.Gradient Boosting Regressor\n\n7.ExtraTrees Regressor\n\n**Neural Network**\n\n8.Multi Layer Preceptron Regressor\n\n","7a72762c":"### Obervations\n\n1. Best results over test set are given by Extra Tree Regressor with R2 score of 0.57\n2. Least RMSE score is also by Extra Tree Regressor 0.65\n2. Lasso regularization over Linear regression was worst performing model\n","9a2177e8":"### Given this is not a timeseries problem and we will focus on predicting the appliance consumption  , we can ignore Date column","baffb8d9":"### Observations based on correlation plot\n\n1. Temperature - All the temperature variables from T1-T9 and T_out have positive correlation with the target Appliances . For the indoortemperatures, the correlations are high as expected, since the ventilation is driven by the HRV unit and minimizes air tempera-ture differences between rooms. Four columns have a high degree of correlation with T9 - T3,T5,T7,T8 also T6 & T_Out has high correlation (both temperatures from outside) . Hence T6 & T9 can be removed from training set as information provided by them can be provided by other fields.\n\n2. Weather attributes - Visibility, Tdewpoint, Press_mm_hg  have low correlation values\n\n3. Humidity - There are no significantly high  correlation cases (> 0.9) for humidity sensors.\n\n4. Random variables have no role to play\n","d70ac327":"\n## Appliance Energy Prediction\n","03fad41c":"# Reading the data","bed3beec":"### Correlation Plots","81183ca1":"# Parameter Tuning","2cf28a60":"# Data Exploration ","2af2bea5":"### Observations \n\n1. Temperature columns - Temperature inside the house varies between 14.89 Deg & 29.85 Deg , temperatire outside (T6) varies between  -6.06 Deg to 28.29 Deg . The reason for this variation is sensors are kept outside the house\n\n2. Humidiy columns - Humidity inside house varies is between 20.60% to 63.36% with exception of RH_5 (Bathroom) and RH_6 (Outside house) which varies between 29.82% to 96.32% and 1% to 99.9% respectively.\n\n3. Appliances - 75% of Appliance consumption is less than 100 Wh . With the maximum consumption of 1080 Wh , there will be outliers in this column and there are small number of cases where consumption is very high\n\n4. Lights column - Intially I believed lights column will be able to give useful information . With 11438 0 (zero) enteries in 14801 rows , this column will not add any value to the model . I believed light consumption along with humidity level in a room will give idea about human presence in the room and hence its impact on Appliance consumption. Hence for now , I will dropping this column ","3fa62268":"### Feature Importance ","65198d7a":"# Conclusion\n\n1. The best Algorithm to use for this dataset Extra Trees Regressor\n\n2. The untuned model was able to explain 57% of variance on test set .\n\n3. The tuned model was able to explain 63% of varaince on tese set which is improvement of 10%\n\n4. The final model had 22 features \n\n5. Feature reduction was not able to add to better R2 score \n\n","ebaf9020":"# Data Visualization"}}