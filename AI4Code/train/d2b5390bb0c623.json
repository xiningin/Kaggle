{"cell_type":{"4c422134":"code","689e831a":"code","8b0a2c5d":"code","f417c036":"code","6a964adb":"code","39db534a":"code","be470a29":"code","7aab5e18":"code","df3f5122":"code","d260b629":"code","9de8ca86":"code","a9a56d44":"code","21250cd6":"code","06970cbe":"code","64e5f8ef":"code","482ddd2f":"code","11c554d0":"code","d93d782f":"code","2fac5598":"code","06a6c788":"code","0f56bef5":"code","9ed46624":"code","205f23c2":"code","e2e948de":"code","8c9c4861":"code","8634c3ce":"code","ca87988f":"code","26fed0f7":"code","13ca290e":"code","24c4d8eb":"code","486dc931":"code","bbaeb5bb":"code","41aa4614":"code","489cbf40":"code","fc599d4c":"code","9d824c56":"code","05ca6480":"code","b1055539":"code","4b6276ad":"code","7ea90d1e":"code","6d4bf85c":"code","fc0d7333":"code","9000a471":"code","dc8762c9":"code","0fa62488":"code","19a5b161":"code","f3847b60":"code","0f44a941":"code","b486177e":"code","aa52ae5d":"code","175de28a":"code","6ec561be":"code","f88895b4":"code","7405e46b":"code","5c0e31be":"code","dc534dd2":"code","0de78ec4":"code","7c8eeaa4":"code","ec8e23fe":"code","4f46c076":"code","64e19925":"code","f4472a21":"code","87711681":"code","7f3aca70":"code","251e3c55":"code","439140de":"code","fa81dd49":"code","81c6d2e4":"markdown","a89057d4":"markdown","9d52689a":"markdown","0d340b58":"markdown","19ecccb0":"markdown","7f448c71":"markdown","7cbdb7de":"markdown","e24263b4":"markdown","7a209003":"markdown","c8672517":"markdown","6599f6d4":"markdown","eb362e75":"markdown"},"source":{"4c422134":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n\nfrom IPython.display import display\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')","689e831a":"!ls \"..\/input\/\"","8b0a2c5d":"# Reading train&test data\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","f417c036":"# Drop the id's from both dataset.\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","6a964adb":"# Evaluation metric is root mean squared log error. So we are taking the log of dependent variable.\n\ntrain.SalePrice = np.log(train.SalePrice)\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","39db534a":"# Concatenate train and test features in order to preprocess both of them equally.\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","be470a29":"# Print numeric and categoric columns.\n\nnumeric_columns = features.select_dtypes(exclude=['object']).columns\ncategoric_columns = features.select_dtypes(include=['object']).columns\nprint(numeric_columns)\nprint(categoric_columns)\n\nassert len(numeric_columns) + len(categoric_columns) == features.shape[1], \"Some columns are missing?\"","7aab5e18":"def nulls(df):\n    null = (features.isnull().sum() \/ len(features)).sort_values(ascending=False)\n    null = null[null > 0]    \n    return null\n\ndef plot_nulls(null_count):\n    f, ax = plt.subplots(figsize=(15, 6))\n    plt.xticks(rotation='90')\n    sns.barplot(x=null_count.index, y=null_count)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15);","df3f5122":"# Plot the missing features. \n\nnull_count = nulls(features)\nplot_nulls(null_count)\nprint(f\"{len(null_count)} null features\")","d260b629":"print(features.PoolQC.dtype)\nprint(\"Unique vals: \", features.PoolQC.unique())\nprint(features.PoolQC.value_counts())","9de8ca86":"# So most of our observations is already null, we fill those with string \"None\"\n\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")","a9a56d44":"# Garage Columns(Features)\n\ngarage_cols = []\nfor col in features.columns:\n    if col.startswith(\"Garage\"):\n        print(f\"{col} : {null_count[col]}\")\n        garage_cols.append(col)","21250cd6":"features.GarageCars.value_counts()","06970cbe":"features.GarageQual.value_counts()","64e5f8ef":"features.GarageCond.value_counts()","482ddd2f":"features.GarageType.value_counts()","11c554d0":"# For int dtypes we fill the NaN values with 0s and for the object types,\n# we fill with the 'None's.\n\nfor col in garage_cols:\n    print(f\"{col} : {null_count[col]} : {features[col].dtype}\")\n    \nfor col in [\"GarageYrBlt\", \"GarageCars\", \"GarageArea\"]:\n    features[col].fillna(0, inplace=True)\n    \nfor col in [\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]:\n    features[col].fillna(\"None\", inplace=True)","d93d782f":"# We filled some null features with just 0's or 'None's\n\nnull_count = nulls(features)\nplot_nulls(null_count)\nprint(f\"{len(null_count)} null features\")","2fac5598":"# Only 1 observation has null value, so we fill that observation with most\n# frequent element which is \"SBrkr\"\n\nprint(features.Electrical.value_counts().sum(), \" - \", len(features))\nprint(null_count.Electrical)\nprint(features.Electrical.value_counts())\n\nfeatures.Electrical.fillna(\"SBrkr\", inplace=True)","06a6c788":"# Only 1 observation has null value, so we fill that observation with most\n# frequent element which is \"WD\"\n\nprint(features.SaleType.value_counts().sum(), \" - \", len(features))\nprint(null_count.SaleType)\nprint(features.SaleType.value_counts())\n\nfeatures.SaleType.fillna(\"WD\", inplace=True)","0f56bef5":"# Only 1 observation has null value, so we fill that observation with most\n# frequent element which is \"VinylSd\"\n\nprint(features.Exterior1st.value_counts().sum(), \" - \", len(features))\nprint(null_count.Exterior1st)\nprint(features.Exterior1st.value_counts())\n\nfeatures.Exterior1st.fillna(\"VinylSd\", inplace=True)","9ed46624":"# Only 1 observation has null value, so we fill that observation with most\n# frequent element which is \"VinylSd\"\n\nprint(features.Exterior2nd.value_counts().sum(), \" - \", len(features))\nprint(null_count.Exterior2nd)\nprint(features.Exterior2nd.value_counts())\n\nfeatures.Exterior2nd.fillna(\"VinylSd\", inplace=True)","205f23c2":"# Only 1 observation has null value, so we fill that observation with most\n# frequent element which is \"TA\"\n\nprint(features.KitchenQual.value_counts().sum(), \" - \", len(features))\nprint(null_count.KitchenQual)\nprint(features.KitchenQual.value_counts())\n\nfeatures.KitchenQual.fillna(\"TA\", inplace=True)","e2e948de":"# Only 2 observation has null value, so we fill that observation with most\n# frequent element which is \"Typ\"\n\nprint(features.Functional.value_counts().sum(), \" - \", len(features))\nprint(null_count.Functional)\nprint(features.Functional.value_counts())\n\nfeatures.Functional.fillna(\"Typ\", inplace=True)","8c9c4861":"null_count = nulls(features)\nplot_nulls(null_count)\nprint(f\"{len(null_count)} null features\")","8634c3ce":"null_count[null_count < 0.1]","ca87988f":"# Those are categoric columns. So we fill them with the \"None\". \n\ncat_columns = [\"BsmtCond\", \"BsmtExposure\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\"]\nfor col in cat_columns:\n    features[col].fillna(\"None\", inplace=True)","26fed0f7":"null_count = nulls(features)\nplot_nulls(null_count)\nprint(f\"{len(null_count)} null features\")","13ca290e":"features[features.MSZoning.isnull()]","24c4d8eb":"# groupby MSSubClass and find the most frequent MSZoning for that particular MSSubClass. \n# Then fill the MSZoning with that element. \n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","486dc931":"# For the rest of the object\/categoric columns we fill the null values with \"None\".\n\nfor col in categoric_columns:\n    features[col].fillna(\"None\", inplace=True)\n\n# For the numeric columns, we fill with 0s.\n\nfor col in numeric_columns:\n    features[col].fillna(0, inplace=True)","bbaeb5bb":"pd.get_dummies(features.MSZoning)[:10]","41aa4614":"print(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)","489cbf40":"X = final_features.iloc[:len(y), :]\ntest_X = final_features.iloc[len(X):, :]\n\nprint('X: ', X.shape)\nprint('y: ', y.shape)\nprint('test_X: ', test_X.shape)","fc599d4c":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X, y)","9d824c56":"# This is from fastai library.\n# https:\/\/github.com\/fastai\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","05ca6480":"fi = rf_feat_importance(m, X); fi[:10]","b1055539":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","4b6276ad":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);","7ea90d1e":"# Total size of the house in square feet. \nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\n# This is the most basic type of feature engineering. \n# There are also BsmtUnfSF, LowQualFinSF features as well to \n# add additional information. \nfeatures['TotalSQFootage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\n# Total number of bathrooms.\nfeatures['TotalBathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\n# Total size of the porch in square feet.\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])","6d4bf85c":"print(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)\n\nX = final_features.iloc[:len(y), :]\ntest_X = final_features.iloc[len(X):, :]\n\nprint('X: ', X.shape)\nprint('y: ', y.shape)\nprint('test_X: ', test_X.shape)","fc0d7333":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X, y)","9000a471":"fi = rf_feat_importance(m, X); fi[:10]","dc8762c9":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","0fa62488":"plot_fi(fi[:30]);","19a5b161":"# After filling nulls with bunch of 0's and 'None's there are no more nulls.\n\nnull_count = nulls(features)\nnull_count","f3847b60":"# Current columns\n\nfeatures.columns","0f44a941":"# We are going to drop some features that we used above to create new ones.\n\nfeatures.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1',\n               'BsmtFinSF2', '1stFlrSF', 'FullBath', 'GarageArea', \n               'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'OpenPorchSF',\n               '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF', 'PoolArea',\n               'TotalBsmtSF', 'Fireplaces'], inplace=True, axis=1)","b486177e":"# This is from fastai's machine learning course. \n# http:\/\/course18.fast.ai\/ml\n\nfrom scipy.cluster import hierarchy as hc\n\ncorr = np.round(scipy.stats.spearmanr(features).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16, 20))\ndendrogram = hc.dendrogram(z, labels=features.columns, orientation='left', leaf_font_size=16)\nplt.show()","aa52ae5d":"print(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)","175de28a":"X = final_features.iloc[:len(y), :]\ntest_X = final_features.iloc[len(X):, :]\n\nprint('X: ', X.shape)\nprint('y: ', y.shape)\nprint('test_X: ', test_X.shape)","6ec561be":"def rmse(preds, labels):\n    return np.sqrt(mean_squared_error(preds, labels))\n\ndef train_error(estimator, X_train, y_train):\n    preds = estimator.predict(X_train)\n    print(\"RMSE: \", rmse(preds, y_train))","f88895b4":"elastic_net = ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10],\n                                    l1_ratio=[.01, .1, .5, .9, .99],\n                                    max_iter=5000).fit(X, y)\ntrain_error(elastic_net, X, y)","7405e46b":"en_model = elastic_net.fit(X, y)","5c0e31be":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\n# Hyperparameter options\n\ngbr_hyperparams = {'n_estimators': [1000, 3000],\n                   'learning_rate': [0.05],\n                   'max_depth': [3],\n                   'max_features': ['sqrt', 0.5],\n                   'min_samples_leaf': [10, 15],\n                   'min_samples_split': [5, 10],\n                   'loss': ['huber']}\n\nmodel_gbr = GradientBoostingRegressor()\n\ngs = GridSearchCV(model_gbr, gbr_hyperparams)\ngs.fit(X, y)\n# print(gs.best_params_)","dc534dd2":"gs.best_params_","0de78ec4":"best_gbr = gs.best_estimator_\nbest_gbr","7c8eeaa4":"# https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html#lightgbm.LGBMRegressor\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n# Hyperparameter options\n\nlgb_hyperparams = {'num_leaves': [5, 10], \n               'learning_rate': [0.05],\n               'n_estimators': [500, 1000],\n               'bagging_fraction': [0.8],\n               'bagging_freq': [3, 5],\n               'feature_fraction': [0.6, 0.8],\n               'feature_fraction_seed': [9],\n               'bagging_seed': [9],\n               'min_data_in_leaf': [5, 10]\n              }\n\nmodel_lgb = LGBMRegressor(objective='regression', n_jobs=-1)\n\ngs = GridSearchCV(model_lgb, lgb_hyperparams)\ngs.fit(X, y)","ec8e23fe":"best_lgb = gs.best_estimator_\nbest_lgb","4f46c076":"# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n# Hyperparameter options\n\nxgb_hyperparams = {'colsample_bytree': [0.4, 0.5],\n                   'gamma': [0.04],\n                   'learning_rate': [0.05],\n                   'max_depth': [3, 5],\n                   'n_estimators': [1000, 2000],\n                   'reg_alpha': [0.4, 0.6],\n                   'reg_lambda': [0.6, 0.8],\n                   'subsample': [0.5, 0.8]\n                   }\n\n\nmodel_xgb = XGBRegressor(random_state=7)\n\ngs = GridSearchCV(model_xgb, xgb_hyperparams)\ngs.fit(X, y)","64e19925":"best_xgb = gs.best_estimator_\nbest_xgb","f4472a21":"best_xgb = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.05, loss='huber', max_depth=3,\n             max_features='sqrt', max_leaf_nodes=None,\n             min_impurity_decrease=0.0, min_impurity_split=None,\n             min_samples_leaf=10, min_samples_split=5,\n             min_weight_fraction_leaf=0.0, n_estimators=3000,\n             presort='auto', random_state=None, subsample=1.0, verbose=0,\n             warm_start=False).fit(X, y)\n             \n             \nbest_lgb = LGBMRegressor(bagging_fraction=0.8, bagging_freq=3, bagging_seed=9,\n       boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n       feature_fraction=0.6, feature_fraction_seed=9,\n       importance_type='split', learning_rate=0.05, max_depth=-1,\n       min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=5,\n       min_split_gain=0.0, n_estimators=1000, n_jobs=-1, num_leaves=5,\n       objective='regression', random_state=None, reg_alpha=0.0,\n       reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0).fit(X, y)\n       \n       \nbest_gbr = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.4, gamma=0.04, importance_type='gain',\n       learning_rate=0.05, max_delta_step=0, max_depth=3,\n       min_child_weight=1, missing=None, n_estimators=2000, n_jobs=1,\n       nthread=None, objective='reg:linear', random_state=7, reg_alpha=0.6,\n       reg_lambda=0.6, scale_pos_weight=1, seed=None, silent=True,\n       subsample=0.8).fit(X, y)","87711681":"train_error(best_gbr, X, y)","7f3aca70":"train_error(best_lgb, X, y)","251e3c55":"train_error(best_xgb, X, y)","439140de":"# Stacking models\n# Here we exponentiate the predictions to transform original version.\n\npreds = (np.exp(best_xgb.predict(test_X)) + \n                np.exp(en_model.predict(test_X)) + \n                np.exp(best_lgb.predict(test_X)) + \n                np.exp(best_gbr.predict(test_X))) \/ 4","fa81dd49":"# 0.11865\n\nsubmission = pd.DataFrame({'Id': test_ID, 'SalePrice': preds})\nsubmission.to_csv('submission.csv', index =False)","81c6d2e4":"# STACKING MODELS","a89057d4":"There are some columns which consists kind of similar information about the house. For example, TotalBsmtSF is the size of basement in square feet, 1stFlrSF is the size of first floor and 2ndFlrSF is the size of third floor in square feet. We can create some additional features by combining those attributes like what is the total size of the house or what is the total size of porch? We can also create random forests to check the importance of that features that we just created. Before creating the new features, let's build some forests and see what is the most important features.","9d52689a":"We will train 4 different models. Then we will take the average prediction of trained models.\n\n- [ElasticNet](https:\/\/scikit-learn.org\/0.15\/modules\/generated\/sklearn.linear_model.ElasticNetCV.html)\n- [GradientBoostingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html)\n- [LGBMRegressor](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html)\n- [XGBRegressor](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html)\n\n\nFor the hyperparameters we apply grid search.\n\n- [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)","0d340b58":"Let's do the exact steps as before. Some code duplication here. This time we will have 4 more features. ","19ecccb0":"Okey there are some plenty of changes here. The most important feature is still the OverallQual for our random forest. And the second most important feature became TotalSF. That does make sense because if we would consider to buy a house, total size of the house would be really important, isn't it? Lastly, TotalSQFootage became the 3rd most important feature.\n","7f448c71":"# REFERENCES\n\n- [house-prices-lasso-xgboost-and-a-detailed-eda](https:\/\/www.kaggle.com\/erikbruin\/house-prices-lasso-xgboost-and-a-detailed-eda)\n- [regularized-linear-models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)\n- [simple-house-price-prediction-stacking](https:\/\/www.kaggle.com\/vhrique\/simple-house-price-prediction-stacking)","7cbdb7de":"Now here the most important features are OverallQual, GrLivArea and YearBuilt. Let us add the features that we mentioned above.","e24263b4":"Before creating forests we need to convert categorical features to numerical features. In order to do that, we are applying **one-hot encoding** to categoric features.\n\n#### One-hot Encoding\nWe basically create *n* more features to represent categorical variable. n is the total category number. In below, you can find some little demonstration on how to do that. For example in the first row, MSZoning is RL, so we put 1 for RL and 0 for others. This process is done for every row in our dataset.","7a209003":"There is some duplicate code. I just wanted to show everything clearly. And also we could just use mode() function to find most frequent element. Again just for clearity.","c8672517":"**I found those models by running the code above which basically does grid search for hyperparameters.**","6599f6d4":"Finally, we transform categoric features into numerics using one-hot encoding again. ","eb362e75":"Before the final touch, let us draw the dendogram of features. We can see some similar features here, but when I tried to remove them, performance decreased. So we won't make any further changes. But it's a good technique to be aware of.\n"}}