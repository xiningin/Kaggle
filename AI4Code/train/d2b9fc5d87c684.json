{"cell_type":{"5ba371df":"code","0f278dd1":"code","b31f8581":"code","7896d2c9":"code","47884af2":"code","a14ca5f3":"code","2c3cda10":"code","ee4d7dd2":"code","2591b58b":"code","c931d539":"code","236c0c2f":"code","8ea74df5":"code","740952bb":"code","87bc5bbe":"code","01387d8b":"code","063c0410":"code","5069b019":"code","ff08231c":"code","8091326d":"code","099eea9a":"code","93a4dc4f":"code","59de5b21":"code","c5e50c90":"code","cf4271a9":"code","a22eca18":"code","b19384f8":"code","767705f3":"code","21aa697e":"code","4bbbcfda":"markdown","01ae47ed":"markdown","40b679e8":"markdown","cdc780ba":"markdown"},"source":{"5ba371df":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.offline as py\nimport plotly.figure_factory as ff\n\n\n# Data Pre-processing Libraries\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder,PowerTransformer,OrdinalEncoder\nfrom sklearn.model_selection import train_test_split,cross_validate, cross_val_score,GridSearchCV\nfrom sklearn.pipeline import Pipeline,make_pipeline\n\nfrom sklearn.compose import make_column_transformer,make_column_selector\nfrom sklearn.feature_selection import SelectKBest,f_classif,mutual_info_classif,chi2,SelectFromModel\n\n# Modelling Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC,LinearSVC,SVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score,mean_squared_error,r2_score,confusion_matrix\nfrom sklearn.metrics import classification_report, plot_confusion_matrix,roc_auc_score,f1_score,recall_score\n\nimport optuna\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option(\"max_columns\",100)\npd.set_option(\"max_rows\",900)\npd.set_option(\"max_colwidth\",200)","0f278dd1":"data = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\ndf=data.copy()","b31f8581":"df.head()","7896d2c9":"df.describe()","47884af2":"df.skew()","a14ca5f3":"def missing(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","2c3cda10":"df.duplicated().sum()","ee4d7dd2":"numerical= df.drop(['HeartDisease'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns","2591b58b":"df[\"HeartDisease\"].value_counts()","c931d539":"matrix = np.triu(df.corr())\nfig,ax = plt.subplots(figsize=(12,6),dpi=100)\nsns.heatmap(df.corr(),annot=True,vmax=1,vmin=-1,center=0,ax=ax,mask=matrix,fmt=\".2f\");","236c0c2f":"plt.figure(figsize=(15,5))\nsns.countplot(x ='Age', data = df)\nplt.title('Age Distribution')\nplt.ylabel('Age')\nplt.show();","8ea74df5":"plt.figure(figsize=(15,5))\nplt.subplot(221)\nsns.histplot(x='Age', data=df, kde =True)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\n\nplt.subplot(222)\nsns.histplot(x ='Cholesterol', data=df, color='red', kde = True)\nplt.title('Cholesterol Distribution')\nplt.xlabel('Cholesterol')\nplt.ylabel('Count')\n\nplt.figure(figsize=(15,5))\nplt.subplot(223)\nsns.histplot(x='MaxHR', data=df, kde =True)\nplt.title('MaxHR Distribution')\nplt.xlabel('MaxHR')\nplt.ylabel('Count')\n\nplt.subplot(224)\nsns.histplot(x ='RestingBP', data=df, color='red', kde = True)\nplt.title('RestingBP Distribution')\nplt.xlabel('RestingBP')\nplt.ylabel('Count');","740952bb":"plt.figure(figsize=(8,8))\n\nexplode = [0,0.1]\nplt.pie(df['Sex'].value_counts(), explode=explode,autopct='%1.1f%%', shadow=True,startangle=140)\nplt.legend(labels=['Male','Female'])\nplt.title('Male and Female Distribution')\nplt.axis('off');","87bc5bbe":"fig,ax = plt.subplots(figsize=(6,6),dpi =100)\nsns.countplot(x='Sex', data=df,ax=ax)\nplt.title('Sex Distribution')\nplt.xlabel('Sex')\nplt.ylabel('Count')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() +0.4, p.get_height() + 10));","01387d8b":"px.histogram(df, x=df.ExerciseAngina, color=\"HeartDisease\",facet_col=\"Sex\")","063c0410":"px.histogram(df, x=\"RestingECG\", color=\"HeartDisease\",facet_col=\"Sex\")","5069b019":"px.histogram(df, x=\"Sex\", color=\"HeartDisease\",width=800)","ff08231c":"px.box(df,x=df.MaxHR,facet_col=\"HeartDisease\",animation_frame=df.Age)","8091326d":"plt.figure(figsize=(8,8))\n\nexplode = [0,0.1]\nplt.pie(df['HeartDisease'].value_counts(), explode=explode,autopct='%1.1f%%', shadow=True,startangle=140)\nplt.legend(labels=['1','0'])\nplt.title('HeartDisease Distribution')\nplt.axis('off');","099eea9a":"ohe = OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\nscaled = StandardScaler()\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nct =make_column_transformer((ohe,categorical),\n                            (scaled,numerical),remainder='passthrough')\n\npipe = make_pipeline(ct,LogisticRegression(random_state=42,class_weight=\"balanced\"))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nlog_f1 = f1_score(y_test, y_pred)\nlog_recall = recall_score(y_test, y_pred)\nlog_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"LOG_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\npipe = make_pipeline(ct,KNeighborsClassifier())\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nknn_f1 = f1_score(y_test, y_pred)\nknn_recall = recall_score(y_test, y_pred)\nknn_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"KNN_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\npipe = make_pipeline(ct,SVC(random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nsvc_f1 = f1_score(y_test, y_pred)\nsvc_recall = recall_score(y_test, y_pred)\nsvc_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"SVM_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\npipe = make_pipeline(ct,DecisionTreeClassifier(random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\ndt_f1 = f1_score(y_test, y_pred)\ndt_recall = recall_score(y_test, y_pred)\ndt_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"DT_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\npipe = make_pipeline(ct,RandomForestClassifier(random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\n\nrf_f1 = f1_score(y_test, y_pred)\nrf_recall = recall_score(y_test, y_pred)\nrf_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"RF_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\npipe = make_pipeline(ct,AdaBoostClassifier(n_estimators=50, random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\n\nada_f1 = f1_score(y_test, y_pred)\nada_recall = recall_score(y_test, y_pred)\nada_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"ADABOOST_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\n\npipe = make_pipeline(ct,GradientBoostingClassifier(random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\n\ngb_f1 = f1_score(y_test, y_pred)\ngb_recall = recall_score(y_test, y_pred)\ngb_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"GRAD\u0130ENT BOST\u0130NG_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\npipe = make_pipeline(ct,XGBClassifier(random_state=42))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\n\nxgb_f1 = f1_score(y_test, y_pred)\nxgb_recall = recall_score(y_test, y_pred)\nxgb_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"XGB_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\npipe = make_pipeline(ct,CatBoostClassifier(random_state=42,verbose=0))\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\ncat_f1 = f1_score(y_test, y_pred)\ncat_recall = recall_score(y_test, y_pred)\ncat_auc = roc_auc_score(y_test, y_pred)\nprint(\"-------------------------\")\nprint(\"CATBOOST_MODEL\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\n\ncompare = pd.DataFrame({\"Model\": [\"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n                                 \"GradientBoost\", \"XGBoo\",\"CatBoost\"],\n                        \"F1\": [log_f1, knn_f1, svc_f1, dt_f1, rf_f1, ada_f1, gb_f1, xgb_f1,cat_f1 ],\n                        \"Recall\": [log_recall, knn_recall, svc_recall, dt_recall, rf_recall, ada_recall, gb_recall, xgb_recall,cat_recall],\n                        \"ROC_AUC\": [log_auc, knn_auc, svc_auc, dt_auc, rf_auc, ada_auc, gb_auc, xgb_auc,cat_auc]})\n\ndef labels(ax):\n    for p in ax.patches:\n        width = p.get_width()                        # get bar length\n        ax.text(width,                               # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2,      # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width),             # set variable to display, 2 decimals\n                ha = 'left',                         # horizontal alignment\n                va = 'center')                       # vertical alignment\n    \nplt.figure(figsize=(14,10))\nplt.subplot(311)\ncompare = compare.sort_values(by=\"F1\", ascending=False)\nax=sns.barplot(x=\"F1\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(312)\ncompare = compare.sort_values(by=\"Recall\", ascending=False)\nax=sns.barplot(x=\"Recall\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(313)\ncompare = compare.sort_values(by=\"ROC_AUC\", ascending=False)\nax=sns.barplot(x=\"ROC_AUC\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()","93a4dc4f":"def objective(trial):\n    \n    X= df.drop('HeartDisease', axis=1)\n    y= df['HeartDisease']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    params = {\"C\":trial.suggest_int(\"C\",1,5),\n             \"degree\":trial.suggest_int(\"degree\",2,6),\n             \"kernel\": trial.suggest_categorical(\"kernel\",(\"rbf\",\"linear\")),\n             \"coef0\":trial.suggest_float(\"coef0\",0,1)}\n    ct =  make_column_transformer((StandardScaler(),make_column_selector(dtype_exclude=object)),\n         (OneHotEncoder(sparse=False,handle_unknown=\"ignore\"),make_column_selector(dtype_include=object)),remainder=\"passthrough\")\n    \n    model =SVC(random_state=42)\n    pipe = make_pipeline(ct,model)\n    pipe.fit(X_train,y_train)\n    preds = pipe.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50, timeout=600)\n\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","59de5b21":"svc_model = SVC(C= 2,degree= 2,kernel = \"rbf\",coef0=  0.4371031882388341,random_state=42,verbose =0 )\n\nct = make_column_transformer((ohe,categorical),\n                            (scaled,numerical),remainder='passthrough')\n\npipe = make_pipeline(ct,svc_model)\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nprint(\"-------Test Scores-------\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"-------Train Scores-------\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))","c5e50c90":"ohe =  OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\nmodel = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\nscaled = StandardScaler()","cf4271a9":"X= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nct =make_column_transformer((ohe,categorical),\n                            (scaled,numerical),remainder='passthrough')\n\npipe = make_pipeline(ct,model)\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nprint(\"-------Test Scores-------\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"-------Train Scores-------\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))","a22eca18":"def objective(trial):\n    X= df.drop('HeartDisease', axis=1)\n    y= df['HeartDisease']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    ct =make_column_transformer((ohe,categorical),\n                                (scaled,numerical),remainder='passthrough')\n\n    \n    param = {\"n_estimators\":trial.suggest_int(\"n_estimators\",100,500),\n        \"max_depth\": trial.suggest_float(\"max_depth\",1,10),\n        \"max_samples\": trial.suggest_float(\"max_samples\",0.01,1)}\n\n    rf_model = RandomForestClassifier(random_state=42,**param,n_jobs=-1,verbose=False)\n    pipe = make_pipeline(ct,rf_model)\n    pipe.fit(X_train,y_train)\n\n    preds = pipe.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50, timeout=600)\n\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","b19384f8":"rf_model = RandomForestClassifier(n_estimators = 105,max_depth = 8.415478369250112,\n                                  max_samples = 0.8643345901350294,random_state=42)\n\nct = make_column_transformer((ohe,categorical),\n                            (scaled,numerical),remainder='passthrough')\n\npipe = make_pipeline(ct,rf_model)\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\ny_pred_train = pipe.predict(X_train)\n\nprint(\"-------Test Scores-------\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"-------Train Scores-------\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))","767705f3":"def objective(trial):\n    X= df.drop('HeartDisease', axis=1)\n    y= df['HeartDisease']\n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12)}\n    \n    cat_model = CatBoostClassifier(**param)\n    cat_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n    preds = cat_model.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=75)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","21aa697e":"X= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=42,\n                          objective= 'Logloss',\n                          colsample_bylevel= 0.08369028629134112,\n                          depth= 9)\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\n\nprint(\"-------Test Scores-------\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"-------Train Scores-------\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))","4bbbcfda":"## **Compare models with default values.**","01ae47ed":"## **CatBoostClassifier Model**","40b679e8":"## **SVC Model**","cdc780ba":"\n## **RandomForest Model**"}}