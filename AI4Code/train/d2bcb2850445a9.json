{"cell_type":{"3246f4d6":"code","a4e72ba8":"code","8b8fb585":"code","6309466f":"code","ab1d0e22":"code","4928066f":"code","5a96b0a1":"code","13d447ed":"code","2a643d96":"code","5d10dcbf":"code","da0d0c63":"code","fd2afb6b":"code","594c4567":"code","9b6af43b":"code","f3fda1d4":"code","7b2f9138":"code","6289cee8":"code","26dd485a":"code","ab6b5478":"code","d446b132":"code","010aae60":"code","a2d8cbb6":"code","b577c3f7":"code","aa9d44d9":"code","b46978ea":"code","75072985":"code","78049c3c":"code","3d02c3b6":"code","627008cd":"code","9f30bbd3":"code","150dae75":"code","62da389b":"code","a0f2b484":"code","151d4e47":"code","7350b44c":"code","cd704057":"code","5b301b68":"code","ef88b9f9":"code","a6269706":"code","caf03391":"code","54cea403":"code","bf233d10":"code","4fc21129":"code","4824d0e1":"code","1b0481d9":"code","623212ce":"code","e1e80e8d":"code","3948ef81":"code","2f31f786":"code","0844d04e":"code","04f8b417":"markdown","d0a2e565":"markdown","8602f87e":"markdown","40c4f1aa":"markdown","c4dd5ea4":"markdown","d1f3e0f9":"markdown","d30dc534":"markdown","bde48d0c":"markdown","9a112d9e":"markdown","4df67cf8":"markdown","8560a8b4":"markdown","eda6866e":"markdown","72854fa3":"markdown","51c69148":"markdown","a8d42ed1":"markdown","57db1863":"markdown","d2d54714":"markdown","c62261a9":"markdown","d5ae1521":"markdown","0eef7104":"markdown","1897c819":"markdown","05059c6d":"markdown","8a14147a":"markdown","1bd42903":"markdown","d42a6a02":"markdown","448617e2":"markdown"},"source":{"3246f4d6":"import pandas as pd\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport plotly.express as px\nfrom plotly.colors import n_colors\nimport numpy as np\nimport seaborn as sns\nimport pandas_profiling\n%matplotlib inline\nfrom matplotlib import rc\nimport scipy.stats\nfrom matplotlib.gridspec import GridSpec\nfrom wordcloud import WordCloud, STOPWORDS \nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom textblob import TextBlob\nfrom nltk.sentiment import SentimentAnalyzer\nfrom wordcloud import WordCloud\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport plotly.graph_objects as go","a4e72ba8":"#reading the csv file\nnewyear = pd.read_csv(\"..\/input\/new-year-resolution-maven-analytics\/New_years_resolutions.csv\")","8b8fb585":"newyear.head(10)","6309466f":"newyear.shape","ab1d0e22":"newyear.info()","4928066f":"labels = 'Career', 'Education\/Training','Family\/Friends\/Relationships', 'Finance','Health & Fitness','Humor','Personal Growth','Philanthropic','Recreation & Leisure', 'Time Management\/Organization'\nsizes = [123,87,327,167,825,887,1678,83,461,85]\nexplode = (0, 0, 0, 0,0,0,0.1,0,0,0)","5a96b0a1":"#pie chart to plot tweet categories\nplt.figure(1, figsize=(40,20))\nthe_grid = GridSpec(2, 2)\ncmap = plt.get_cmap('Spectral')\ncolors = [cmap(i) for i in np.linspace(0, 1, 8)]\nplt.subplot(the_grid[0, 1], aspect=0, title='Category of Tweets on NewYear Resolutions')\ntype_show_ids = plt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, colors=colors)\nplt.show()","13d447ed":"newyear.insert(0, 'id', range(1, 1 + len(newyear)))","2a643d96":"#calculating gender\nnewyear.user_gender.value_counts()","5d10dcbf":"tweetcategory_gender = newyear.groupby(by=['tweet_category','user_gender'],sort=False)['id'].agg([('count','count')]).reset_index().sort_values(by='count',ascending=False)\nlist = ['Career', 'Education\/Training','Family\/Friends\/Relationships', 'Finance','Health & Fitness','Humor','Personal Growth','Philanthropic','Recreation & Leisure', 'Time Management\/Organization']\ncategory_gender = tweetcategory_gender[tweetcategory_gender.tweet_category.str.contains('|'.join(list))]\npivot_df = category_gender.pivot(index='tweet_category', columns='user_gender', values='count')\ncolors = [\"#8B0A50\", \"#EE1289\",\"#1E90FF\"]\n\npivot_df.loc[:,['male','female']].plot.barh(stacked=True, color=colors, figsize=(10,7))","da0d0c63":"fig = plt.figure(figsize = (10, 5)) \n  \n# creating the bar plot \nplt.barh(newyear['tweet_category'], newyear['retweet_count'], color ='maroon') \n  \nplt.xlabel(\"Sum of Retweet\") \nplt.ylabel(\"Categories\") \nplt.title(\"Sum of Retweets By Categories\") \nplt.show() ","fd2afb6b":"sns.countplot(x='tweet_region',data=newyear,palette='viridis')\nplt.title('Number of tweets by different region')","594c4567":"region_state = newyear.groupby(['tweet_region','tweet_state'],sort=False)['id'].agg([('count','count')]).reset_index().sort_values(by=['tweet_region','count'],ascending=[True,False])\nregion_state['us'] = 'US' # in order to have a single root node\nfig = px.treemap(region_state, path=['us', 'tweet_region', 'tweet_state'], values='count', color='tweet_state',color_continuous_scale='RdBu')\nfig.show()","9b6af43b":"newyear.tweet_topics = newyear.tweet_topics.str.replace(' ', '')\nplt.subplots(figsize=(20,15))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=1920,\n                          height=1080\n                         ).generate(\" \".join(newyear.tweet_topics))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('tweet_topics.png')\nplt.show()","f3fda1d4":"newyear['date_time'] = pd.to_datetime(newyear['tweet_created'])","7b2f9138":"newyear = newyear.drop(columns = ['tweet_created'])","6289cee8":"newyear['hour'] = newyear['date_time'].dt.hour\nnewyear['date_hour']  = newyear['date_time'].dt.strftime('%Y\/%m\/%d %H')\nnewyear['Date_hour'] = pd.to_datetime(newyear['date_hour'])","26dd485a":"newyear = newyear.drop(columns = ['date_hour'])","ab6b5478":"daily_hour_count = newyear.groupby('Date_hour').agg('count')\ndaily_hour_df = pd.DataFrame(daily_hour_count)","d446b132":"newyear['date'] = newyear['date_time'].dt.date","010aae60":"newyear['Date'] = pd.to_datetime(newyear['date'])","a2d8cbb6":"newyear = newyear.drop(columns = ['date'])","b577c3f7":"daily_count = newyear.groupby('Date').agg('count')\ndaily_df = pd.DataFrame(daily_count)","aa9d44d9":"# Start and end of the date range to extract\nstart, end = '2014-12-21','2015-01-02'\n# Plot daily and weekly resampled time series together\nfig, ax = plt.subplots(figsize=(18,5))\nax.plot(daily_hour_df.loc[start:end, 'tweet_text'],\nmarker='.', linestyle='-', linewidth=0.5, label='Hourly')\nax.plot(daily_df.loc[start:end, 'tweet_text'],\nmarker='o', markersize=8, linestyle='-', label='Daily')\nax.set_ylabel('Number of Tweets')\nax.legend()\nax.set_facecolor(\"silver\")","b46978ea":"cleaned_words = newyear['tweet_text'].str.replace(\"[^a-zA-Z]\", \" \")","75072985":"cleaned_words","78049c3c":"removed_short_words = cleaned_words.apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","3d02c3b6":"removed_short_words","627008cd":"lowercase_words = removed_short_words.str.lower()","9f30bbd3":"lowercase_words","150dae75":"tokenized_tweet = lowercase_words.apply(lambda x: x.split())\ntokenized_tweet","62da389b":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","a0f2b484":"stop = set(stopwords.words('english'))\nstop.add(\"years\")\nstop.add(\"newyearsresolution\")\nstop.add(\"resolution\")\nstop.add(\"http\")\nstop.add(\"year\")\nfiltered_words = tokenized_tweet.apply(lambda x: [item for item in x if item not in stop])\nprint(filtered_words)","151d4e47":"wordcloud = WordCloud(stopwords=stop).generate(filtered_words.to_string())\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","7350b44c":"import nltk\nnltk.download('all')","cd704057":"stemmer = PorterStemmer()\nstemmed_tweet = filtered_words.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\nstemmed_tweet","5b301b68":"nltk.download('wordnet')","ef88b9f9":"df = filtered_words.to_frame()","a6269706":"df['tweet'] = df['tweet_text'].astype(str)","caf03391":"lemmatizer = WordNetLemmatizer()\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\ndf['lemmatize_tweet'] = df.tweet.apply(lemmatize_text)\ndf['lemmatize_tweet'] = df['lemmatize_tweet'].astype('str') ","54cea403":"df[['polarity', 'subjectivity']] = df['lemmatize_tweet'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))","bf233d10":"def senti(x):\n    return TextBlob(x).sentiment  \n \ndf['senti_score'] = df['lemmatize_tweet'].apply(senti)\n \ndf.senti_score.head()","4fc21129":"def getAnalysis(score):\n    if score < 0:\n          return 'Negative'\n    elif score == 0:\n          return 'Neutral'\n    else:\n          return 'Positive'\ndf['analysis'] = df['polarity'].apply(getAnalysis)\n# Show the dataframe\ndf","4824d0e1":"# Plotting \nplt.figure(figsize=(8,6)) \nfor i in range(0, df.shape[0]):\n      plt.scatter(df[\"polarity\"][i], df[\"subjectivity\"][i], color='Blue')   \nplt.title('Sentiment Analysis') \nplt.xlabel('Polarity') \nplt.ylabel('Subjectivity') \nplt.show()","1b0481d9":"# Plotting and visualizing the counts\nplt.title('Sentiment Analysis')\nplt.xlabel('Sentiment')\nplt.ylabel('Counts')\ndf['analysis'].value_counts().plot(kind = 'bar')\nplt.show()","623212ce":"df['gender'] = newyear['user_gender']\nfc = sns.factorplot(x=\"gender\", hue=\"analysis\", \n                    data=df, kind=\"count\", \n                    palette={\"Negative\": \"#FE2020\", \n                             \"Positive\": \"#BADD07\", \n                             \"Neutral\": \"#68BFF5\"})","e1e80e8d":"df['state'] = newyear['tweet_state']","3948ef81":"df.analysis[df.analysis == 'Positive'] = 1\ndf.analysis[df.analysis == 'Negative'] = -1\ndf.analysis[df.analysis == 'Neutral'] = 0","2f31f786":"df.analysis.value_counts()","0844d04e":"fig = go.Figure(data=go.Choropleth(\n    locations=df['state'], # Spatial coordinates\n    z = df['analysis'].astype(float), # Data to be color-coded\n\n    locationmode = 'USA-states', \n    colorscale = \"Reds\",\n    text=df['state'],\n    marker_line_color='white', # line markers between states\n    colorbar_title = \"Sentiments\"\n    \n))\n\nfig.update_layout(\n    geo_scope='usa', \n)\n\nfig.show()","04f8b417":"#### I have used matplotlib package and provided the values of sizes, labels, and colors.The category Personal Growth is the most popular followed by Humor, Health & Fitness, Recreation & Leisure, Family\/Friends\/Relationships, Finance and so on.","d0a2e565":"#### The further step is removing stop words. Stop words are the words that add little meaning to body of text. Those words could add noise in the text as well as take longer processing time, so that it would be appropriate to remove those words. NLTK library has list of stop words stored in different languages. We have used stop words for in English. The code for that is following:","8602f87e":"#### As we can observe, 29,31 and 1st dates have gotten highest amount of tweets. Hourly pattern follows almost same path as daily pattern.\n\n#### Now we are going to perform some operations on the column tweet_text as part of Natural Language Processing.\u00a0\n#### As the part of first step I am going to remove special characters with space. The code for that is following:","40c4f1aa":"#### The most popular words in the tweets are 'time', 'stop', 'make', 'start', and 'make', 'going'.\n#### The upcoming step is Stemming. We are reducing the size of the text by converting its words to their root words. This is basically in other words is a process of linguistic normalization.","c4dd5ea4":"#### Following is the dataset given for this analysis and visualization.","d1f3e0f9":"#### The next step is to remove short words with less than 3 length","d30dc534":"#### For the purpose of visualization and analysis let's understand scatter plot of polarity and subjectivity.","bde48d0c":"#### Above is the bar graph of sentiment analysis.\n#### Following is the sentiment analysis of tweets genderwise.","9a112d9e":"# Visualizations and NLP on New Year Resolution dataset","4df67cf8":"#### Out of 4723 tweets, 2367 tweets are tweeted by females and 2356 tweets are tweeted by males.\n#### In order to show the popularity of categories by gender I have used stacked bar chart.","8560a8b4":"#### I have also added some unnecessary words as stop words.\n#### Now after removing stop words I have created word cloud to see most popular words in the tweets.\n#### The code for that is following:","eda6866e":"#### Highest number of retweets are in the categories of Finance, Family\/Friends\/Relationships, and Personal Growth.\n#### Let's find out popularity of tweets region wise.","72854fa3":"#### South Region has popular states of Texas and Florida, West has California, Washington, Arizona, Northeast has New York, Pennsylvania, and Midwest has Illinois, and Ohio.\n\n#### The next visualization is about popularity of tweet topics.","51c69148":"#### Females are more interested in Personal Growth, Health & Fitness while males have tweeted on Humor and Career than females.\n#### Let's further more explore the popularity of tweet categories in terms of retweeting. \n#### To show the sum of retweets by categories I have used simple bar chart using matplotlib library. The code is and output is following:","a8d42ed1":"#### Let's further explore the popularity of categories according to gender.","57db1863":"#### The next step is tokenization. I have used word tokenization where we have broken down the tweets into an array of words or small units. These small units are called as tokens.","d2d54714":"#### Most popular tweet topics are 'Bemorepositive', 'Improvemyattitude', 'HumoraboutPersonalGrowthandInterestsResolutions', and 'HumoraboutnotResolutionsingeneral'.\n\n#### Since we have given the time frame of tweets in the column tweet_created, we will do time series analysis and observe the number of tweets. To work on this column I have changed the datatype to datetime and stored in date_time column. I have also extracted date and hour values from date_time column and stored it into Date_hour.\n#### The code for this is following:","c62261a9":"#### In order to do the daily and hourly analysis I have created two dataframes daily_df and daily_hour_df using group by functions. For daily anaylsis I have extracted date from date_time and stored it into Date column.","d5ae1521":"#### Another step is to convert all letters to lower case for convenience.","0eef7104":"#### It seems south region highest number of tweets followed by West, Northeast and Midwest.\n#### For more granular analysis of let's see popularity state wise as well.","1897c819":"#### Females have indulged more in positive tweets than males.\n#### To further explore sentiment analysis state wise and see which states have which majority of sentiments.","05059c6d":"#### Next step is Lemmatization. It is the process of reducing a word to its base word. Root word may or may not have meaning when taken alone while base word has meaning even when taken alone.","8a14147a":"#### Visualizations are essential to convey insights easily to human brain in form of graphs and charts or any other visual format. Being existed in visual world, it is reliable to communicate data with images for easy interpretation.\n#### Since this dataset includes column defining tweets I decided to apply NLP on that as well.\n#### Let's start by importing necessary libraries in Jupyter Notebook.","1bd42903":"#### The given dataframe has 4723 rows and 10 columns. To explore the popularity of the tweet categories, pie chart would make it easy to visualize the proportions. Following is the python code for that.","d42a6a02":"#### I have applied lemmatization to filtered_words which is the text after removing stop words. I have defined function first then apply that on tweet column of dataframe df.\n#### Both Stemming and Lemmatization are text normalization techniques as well as essential for further processing. For stemming I have used Porter Stemmer for English Language in nltk package.\n#### Now we will perform sentiment analysis on lemmatized text. Sentiment analysis is the way of defining attitude or emotion of writer. It can be positive, negative or neutral. The sentiment function of TextBlob package returns the score including subjectivity and polarity of the text. Polarity is used to identify sentiment orientation in the given language. Polarity gives float values which lies in the range of -1 to 1 where -1 is negative and +1 meaning positive sentence. Subjectivity checks subjective expressions(opinions that describe people's feelings towards a specific subject or topic) on the text. Subjectivity also gives float values whose range lies between 0 to 1.","448617e2":"#### We can see that most of the tweets have polarity between 0.0 to 0.50. That means majority tweets are either neutral or positive. Also tweets are increasing with the value of the subjectivity."}}