{"cell_type":{"e3c2ce5a":"code","8d189e3f":"code","cbb8a898":"code","9b8c500e":"code","da74d2e0":"code","632a93ea":"code","1872dd73":"code","e49cdebf":"code","cd223fad":"code","78ecf0e9":"code","8691fa48":"code","c0d22e81":"code","b8553068":"code","8dae13e1":"code","864d8acb":"code","e2ab94f4":"code","6868b545":"code","46ff8b03":"code","293eac1d":"code","e7adc647":"code","2dec4675":"code","173fb241":"code","1316e778":"code","83400091":"code","dd0fb593":"code","68391d33":"code","de3d72d4":"code","0de2aee3":"code","78071cc2":"code","f8d50bb5":"code","7ca53826":"code","4735fcd9":"code","4d2472b9":"code","8773f342":"code","8450cadf":"code","a374b730":"code","d5382261":"code","7aedfbe7":"code","15e3dc19":"code","d9f53c5f":"code","48f855d9":"code","931ba072":"code","9c4bc93a":"code","fddd270a":"code","7ab618f9":"code","f8798521":"code","47d56c65":"code","0d29363b":"code","c92c2bf2":"markdown","83790543":"markdown","0b358e43":"markdown","2d4b8545":"markdown","57300f40":"markdown","11cce76b":"markdown","7e71dc34":"markdown","803f6df6":"markdown","9c728d60":"markdown","e9d57a67":"markdown","b8c26973":"markdown","0dd00f16":"markdown","7d4f9d39":"markdown","b8921652":"markdown"},"source":{"e3c2ce5a":"from os.path import join as pjoin\nfrom typing import Tuple, List, Dict, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","8d189e3f":"PATH_TO_DATA = '..\/input\/lab12-classification-problem'","cbb8a898":"data_train = pd.read_csv(pjoin(PATH_TO_DATA, 'train.csv'))\ndata_train.shape","9b8c500e":"data_train.head()","da74d2e0":"data_test = pd.read_csv(pjoin(PATH_TO_DATA, 'test.csv'))\ndata_test.shape","632a93ea":"data_test.head()","1872dd73":"words = data_train['Word']\nlabels = data_train['Label']","e49cdebf":"labels.value_counts()","cd223fad":"def explore_words_length(data):\n    word_len = data['Word'].str.len()\n    print(f'Max length: {word_len.max()}')\n    print(f'Mean length: {word_len.mean()}')\n    print(f'Median length: {word_len.median()}')    ","78ecf0e9":"explore_words_length(data_train)","8691fa48":"explore_words_length(data_test)","c0d22e81":"def label_distplots(values, labels, kde=True, hist=True):\n    sns.distplot(values[labels == 1], kde=kde, hist=hist, label='Label=1', norm_hist=True)\n    sns.distplot(values[labels == 0], kde=kde, hist=hist, label='Label=0', norm_hist=True)\n    plt.legend();","b8553068":"words_len = words.str.len()\nlabel_distplots(words_len, labels, hist=False)","8dae13e1":"is_first_letter_capital = words.str.slice(0, 1).str.isupper()\npd.crosstab(is_first_letter_capital, labels).plot(kind='bar')","864d8acb":"is_all_letters_capital = words.str.isupper()\npd.crosstab(is_all_letters_capital, labels, margins=True, normalize=True)","e2ab94f4":"from collections import Counter\nconcatenated_words = ''.join(words.str.lower().values)\ncounter = Counter(concatenated_words)\ncounter","6868b545":"is_symbol_in_word = words.str.contains('\\W')\npd.crosstab(is_symbol_in_word, labels)","46ff8b03":"vowels = '\u0430\u0435\u0451\u0438\u043e\u0443\u044b\u044d\u044e\u044f'\nconsonants = '\u0431\u0432\u0433\u0434\u0436\u0437\u0439\u043a\u043b\u043c\u043d\u043f\u0440\u0441\u0442\u0444\u0445\u0447\u0446\u0448\u0449\u044a\u044c'","293eac1d":"n_vowels = words.str.lower().str.count(f'[{vowels}]')\nlabel_distplots(n_vowels, labels, kde=False)","e7adc647":"n_consonants = words.str.lower().str.count(f'[{consonants}]')\nlabel_distplots(n_vowels, labels, kde=False)","2dec4675":"plt.rcParams['figure.figsize'] = 6, 4","173fb241":"def plot_labels_ratio_bar(values, labels):\n    plt.rcParams['figure.figsize'] = 12, 5\n    crosstab  = pd.crosstab(values, labels)\n    crosstab['ratio'] = crosstab[1] \/ np.maximum(crosstab[0].values, 1)\n    crosstab.query('ratio > 0', inplace=True)\n    ratios = crosstab.sort_values('ratio', ascending=False)['ratio']\n    ratios.plot(kind='bar')\n    plt.ylabel('Labels ratio')\n    plt.rcParams['figure.figsize'] = 6, 4","1316e778":"last_letter = words.str.slice(-1, None).str.lower()\nlast_letter.name = 'last_letter'\nplot_labels_ratio_bar(last_letter, labels)","83400091":"first_letter = words.str.slice(0, 1).str.lower()\nfirst_letter.name = 'first_letter'\nplot_labels_ratio_bar(first_letter, labels)","dd0fb593":"from scipy  import sparse\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier","68391d33":"def extract_features(words):\n    data = pd.DataFrame()\n    data['word_len'] = words.str.len()\n    \n    is_first_letter_capital = words.str.slice(0, 1).str.isupper()\n    data['is_first_letter_capital'] = is_first_letter_capital\n    \n    is_symbol_in_word = words.str.contains('\\W')\n    data['is_symbol_in_word'] = is_symbol_in_word\n\n    last_letter = words.str.slice(-1, None).str.lower()\n    data['last_letter'] = last_letter\n\n    letter_before_last = words.str.slice(-2, -1).str.lower()\n    data['letter_before_last'] = letter_before_last\n\n    second_letter_before_last = words.str.slice(-3, -2).str.lower()\n    data['second_letter_before_last'] = second_letter_before_last\n    \n    third_letter_before_last = words.str.slice(-4, -3).str.lower()\n    data['third_letter_before_last'] = third_letter_before_last\n\n    first_letter = words.str.slice(0, 1).str.lower()\n    data['first_letter'] = first_letter\n\n    n_vowels = words.str.lower().str.count(f'[{vowels}]')\n    data['n_vowels'] = n_vowels\n\n    n_consonants = words.str.lower().str.count(f'[{consonants}]')\n    data['n_consonants'] = n_consonants\n    \n    return data","de3d72d4":"def get_cat_features(X: pd.DataFrame) -> List[str]:\n    return X.columns[X.dtypes == 'O'].tolist()\n\n\ndef label_encode(\n    X: pd.DataFrame, \n    encoders: Optional[Dict[str, LabelEncoder]] = None,\n) -> Tuple[pd.DataFrame, Dict[str, LabelEncoder]]:\n\n    X = X.copy()\n    encoders = encoders or {}\n    for col in get_cat_features(X):\n        if col not in encoders:\n            encoder = LabelEncoder().fit(X[col])\n            encoders[col] = encoder\n        else:\n            encoder = encoders[col]\n        X[col] = encoder.transform(X[col])\n    return X, encoders\n\n\ndef one_hot_encode(\n    X: pd.DataFrame, \n    encoders: Optional[Dict[str, OneHotEncoder]] = None,\n) -> Tuple[sparse.csr_matrix, Dict[str, OneHotEncoder]]:\n    cat_features = get_cat_features(X)\n    feature_matrices = []\n    encoders = encoders or {}\n    for col in X.columns:\n        if col in cat_features:\n            if col not in encoders:\n                encoder = OneHotEncoder().fit(X[[col]])\n                encoders[col] = encoder\n            else:\n                encoder = encoders[col]\n            feature_matrix = encoder.transform(X[[col]])\n        else:\n            feature_matrix = sparse.csr_matrix((\n                X[col].values, \n                (\n                    np.arange(X.shape[0], dtype=int), \n                    np.zeros(X.shape[0], dtype=int),\n                ),\n            ))\n        feature_matrices.append(feature_matrix)\n    features = sparse.hstack(feature_matrices, format='csr')\n    return features, encoders  ","0de2aee3":"def calc_metrics(\n    y_true: Union[np.ndarray, pd.Series], \n    pred_proba: Union[np.ndarray, pd.Series], \n    threshold: float = 0.5,\n) -> Dict[str, float]:\n    res = {}\n    pred = np.zeros_like(pred_proba)\n    pred[pred_proba > threshold] = 1\n    res['accuracy'] = accuracy_score(y_true, pred)\n    res['auc'] = roc_auc_score(y_true, pred_proba)\n    res['f1'] = f1_score(y_true, pred)\n    res['precision'] = precision_score(y_true, pred)\n    res['recall'] = recall_score(y_true, pred)\n    return res","78071cc2":"X_train = extract_features(data_train['Word'])\ny_train = data_train['Label']","f8d50bb5":"X_train_ohe, one_hot_encoders = one_hot_encode(X_train)","7ca53826":"X_train_le, label_encoders = label_encode(X_train)","4735fcd9":"lr = LogisticRegression(solver='liblinear', penalty='l2', random_state=1)\ngscv_lr = GridSearchCV(\n    estimator=lr,\n    param_grid={'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10]},\n    scoring='roc_auc',\n    n_jobs=1,\n    cv=3,\n    refit=False,\n    return_train_score=True,\n    verbose=True,\n)\n\ngscv_lr.fit(X_train_ohe, y_train);","4d2472b9":"gscv_lr.best_params_, gscv_lr.best_score_","8773f342":"dt = DecisionTreeClassifier(max_leaf_nodes=100, random_state=1)\ngscv_dt = GridSearchCV(\n    estimator=dt,\n    param_grid={'max_depth': [6, 8, 10, 12, 14], 'min_samples_leaf': [100, 200, 300]},\n    scoring='roc_auc',\n    n_jobs=1,\n    cv=3,\n    refit=False,\n    return_train_score=True,\n    verbose=True,\n)\n\ngscv_dt.fit(X_train_le, y_train);","8450cadf":"gscv_dt.best_params_, gscv_dt.best_score_","a374b730":"gb = GradientBoostingClassifier(random_state=1)\ngscv_gb = GridSearchCV(\n    estimator=gb,\n    param_grid={\n        'n_estimators': [50, 100, 200], \n        'max_depth': [3, 6, 9],\n        'min_samples_leaf': [100, 200, 300],\n    },\n    scoring='roc_auc',\n    n_jobs=-2,\n    cv=3,\n    refit=False,\n    return_train_score=True,\n    verbose=True,\n)\n\ngscv_gb.fit(X_train_le, y_train);","d5382261":"gscv_gb.best_params_, gscv_gb.best_score_","7aedfbe7":"cv_results = pd.DataFrame(gscv_gb.cv_results_)\ncv_results[['params', 'mean_fit_time', 'mean_train_score', 'mean_test_score']] \\\n    .sort_values('mean_test_score', ascending=False)","15e3dc19":"cb = CatBoostClassifier(\n    cat_features=get_cat_features(X_train),\n    eval_metric='AUC',\n    random_seed=1,\n    nan_mode='Forbidden',\n    task_type='CPU',\n    verbose=False,\n)\n\n\ngscv_cb = GridSearchCV(\n    estimator=cb,\n    param_grid={\n        'n_estimators': [50, 100, 150], \n        'max_depth': [5, 6, 7],\n    },\n    scoring='roc_auc',\n    n_jobs=2,\n    cv=3,\n    refit=False,\n    return_train_score=True,\n    verbose=True,\n)\n\ngscv_cb.fit(X_train, y_train);","d9f53c5f":"gscv_cb.best_params_, gscv_cb.best_score_","48f855d9":"cv_results = pd.DataFrame(gscv_cb.cv_results_)\ncv_results[['params', 'mean_fit_time', 'mean_train_score', 'mean_test_score']] \\\n    .sort_values('mean_test_score', ascending=False)","931ba072":"classifier = CatBoostClassifier(\n    cat_features=get_cat_features(X_train),\n    eval_metric='AUC',\n    random_seed=1,\n    nan_mode='Forbidden',\n    task_type='CPU',\n    verbose=True,\n    n_estimators=150,\n    max_depth=6,\n)\nclassifier.fit(X_train, y_train);","9c4bc93a":"X_test = extract_features(data_test['Word'])","fddd270a":"pred_train = classifier.predict_proba(X_train)[:, 1]\ncalc_metrics(y_train, pred_train)","7ab618f9":"pd.DataFrame({\n    'column': X_train.columns,\n    'importance': classifier.feature_importances_,\n}).sort_values('importance', ascending=False)","f8798521":"pred_test = classifier.predict_proba(X_test)[:, 1]\nres = pd.DataFrame({'Id': data_test.index, 'Prediction': pred_test})\nres.to_csv('res_150est_d6.csv', index=False)","47d56c65":"pr, rec, thr = precision_recall_curve(y_train, pred_train)\nf1 = 2 * (pr * rec) \/ (pr + rec)\nplt.plot(thr, f1[:-1])\nplt.xlabel('threshold')\nplt.ylabel('F1')\nplt.grid()","0d29363b":"best_thr = thr[f1.argmax() - 1]\nbest_thr, f1.max()","c92c2bf2":"## Load data","83790543":"### Some letters in word","0b358e43":"### Capital letters","2d4b8545":"### Words length","57300f40":"### Tree model","11cce76b":"### Linear model","7e71dc34":"## Explore data","803f6df6":"**<font color=\"#00aa00\">Tip: the real metric in competition is ROC AUC<\/font>**","9c728d60":"### Gradient boosting model","e9d57a67":"### Improved gradient boosting model","b8c26973":"### What are words made of?","0dd00f16":"## Build model","7d4f9d39":"## Build final model","b8921652":"X_train_train, X_train_test, y_train_train, y_train_test = \\\n    train_test_split(X_train, y_train, test_size=0.3, random_state=1)"}}