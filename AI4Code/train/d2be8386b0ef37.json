{"cell_type":{"b4e12cf0":"code","ec49cb69":"code","aef328f1":"code","1855104b":"code","2231245d":"code","fd3d1782":"code","48d4fdad":"code","6dd49d89":"code","8140c02e":"code","018b333b":"code","effb3467":"code","17d3e8a4":"code","f76acd0a":"code","756d20a4":"code","5be38cc4":"markdown"},"source":{"b4e12cf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport joblib\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec49cb69":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv\", index_col=[\"id\"])\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\", index_col=[\"id\"])\n\nX = train_df.iloc[:, :-1].to_numpy()\ny = train_df.iloc[:, -1].to_numpy()\nX_test = test_df.to_numpy()","aef328f1":"corr=train_df.corr()[\"target\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","1855104b":"from sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostRegressor,  HistGradientBoostingRegressor, StackingRegressor, RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import SGDRegressor\n\nimport xgboost as xgb\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport optuna \nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\n# optuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndef train(model):\n    X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size=0.1, random_state=156)\n    y_train = y_train.reshape(-1, 1)\n    y_test  = y_test.reshape(-1, 1)\n        \n    model = model.fit(X_train, y_train, early_stopping_rounds=100, verbose=False, eval_set=[(X_test, y_test)])\n    score = mean_squared_error(model.predict(X_train), y_train, squared=False)\n    print(score)\n    return model","2231245d":"def objectiveXGB(trial: Trial, X, y, test):\n    param = {\n        \"n_estimators\" : trial.suggest_int('n_estimators', 500, 4000),\n        'max_depth':trial.suggest_int('max_depth', 8, 16),\n        'min_child_weight':trial.suggest_int('min_child_weight', 1, 300),\n        'gamma':trial.suggest_int('gamma', 1, 3),\n        'learning_rate': 0.01,\n        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.5, 1, 0.1),\n        'nthread' : -1,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0] ),\n        'random_state': 42\n    }\n    X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size=0.1)\n    \n    y_train = y_train.reshape(-1, 1)\n    y_test  = y_test.reshape(-1, 1)\n\n    model = xgb.XGBRegressor(**param)\n    xgb_model = model.fit(X_train, y_train, verbose=False, eval_set=[(X_test, y_test)])\n    score = mean_squared_error(xgb_model.predict(X_test), y_test, squared=False)\n\n    return score","fd3d1782":"study = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy.optimize(lambda trial : objectiveXGB(trial, X,  y, X_test), n_trials=50)\nprint('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n\nbest_param = study.best_trial.params\nxgbReg = train(xgb.XGBRegressor(**best_param, tree_method='gpu_hist', random_state=42, predictor='gpu_predictor', learning_rate=0.01, nthread=-1))\n\n#params =  {'n_estimators': 3520, 'max_depth': 11, 'min_child_weight': 231, 'gamma': 2, 'colsample_bytree': 0.7, 'lambda': 0.014950936465569798, 'alpha': 0.28520156840812494, 'subsample': 0.6}\n#xgbReg = train(xgb.XGBRegressor(**params, tree_method='gpu_hist', random_state=42, predictor='gpu_predictor', learning_rate=0.01, nthread=-1))\n\n# 0.6744648190960726","48d4fdad":"optuna.visualization.plot_optimization_history(study)","6dd49d89":"optuna.visualization.plot_param_importances(study)","8140c02e":"def objectiveLGBM(trial: Trial, X, y, test):\n    param = {\n        'objective': 'regression',\n        'metric': 'root_mean_squared_error',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n        'learning_rate': 0.01,\n        'n_estimators': trial.suggest_int('n_estimators', 700, 3000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'device':\"gpu\",\n        'gpu_use_dp':True\n    }\n    X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size=0.1)\n        \n    lgbm_regr = LGBMRegressor(**param)\n    lgbm_regr = lgbm_regr.fit(X_train, y_train, verbose=False)\n    \n    score = mean_squared_error(lgbm_regr.predict(X_test), y_test, squared=False)\n    return score","018b333b":"study = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy.optimize(lambda trial : objectiveLGBM(trial, X, y, X_test), n_trials=20)\nprint('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n\nbest_param2 = study.best_trial.params\nlgbm = LGBMRegressor(**best_param2, device=\"gpu\",gpu_use_dp=True, objective='regression', metric='root_mean_squared_error',  learning_rate= 0.01, boosting_type='gbdt')\n\n# Best trial: score 0.6934602592622415,\n# params {'lambda_l1': 4.168316306871167e-05, 'lambda_l2': 1.1288557039193647e-05, 'num_leaves': 98, 'n_estimators': 2280, 'feature_fraction': 0.7977209715952681, 'bagging_fraction': 0.4353577523638581, 'bagging_freq': 4, 'min_child_samples': 69}","effb3467":"from vecstack import stacking\n\nfinal_model = xgb.XGBRegressor(n_estimators= 2000, max_depth= 16,tree_method='gpu_hist', predictor='gpu_predictor')\nsgd = SGDRegressor(max_iter=1000)\nhgb = HistGradientBoostingRegressor( max_depth=3, min_samples_leaf=1)\ncat = CatBoostRegressor(task_type=\"GPU\", verbose=False)\n\nestimators = [\n    lgbm, cat, sgd, hgb, xgbReg\n]\n\nS_train, S_test = stacking(estimators,  X, y, X_test, regression=True, metric=mean_squared_error, n_folds=5, \n                           shuffle=False, random_state=0, verbose=2)\nfinal_model.fit(S_train, y)\n\nprint(mean_squared_error(final_model.predict(S_train), y, squared=False))","17d3e8a4":"submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\", index_col=[\"id\"])\ny_hat = final_model.predict(S_test)        \n\nsubmission[\"target\"] = y_hat \nsubmission[[\"target\"]].to_csv(\"\/kaggle\/working\/submission_stacking.csv\")\njoblib.dump(final_model, '\/kaggle\/working\/skacking.pkl')","f76acd0a":"submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\", index_col=[\"id\"])\nlgbm = LGBMRegressor(**best_param2, device=\"gpu\",gpu_use_dp=True, objective='regression', learning_rate= 0.01, metric='root_mean_squared_error', boosting_type='gbdt')\nlgbm = lgbm.fit(X, y, verbose=False)\ny_hat = lgbm.predict(submission.to_numpy())        \nprint(mean_squared_error(lgbm.predict(X), y, squared=False))\nsubmission[\"target\"] = y_hat \nsubmission[[\"target\"]].to_csv(\"\/kaggle\/working\/submission_lgbm.csv\")\njoblib.dump(lgbm, '\/kaggle\/working\/lgbm.pkl')","756d20a4":"submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\", index_col=[\"id\"])\nparams =  {'n_estimators': 3520, 'max_depth': 11, 'min_child_weight': 231, 'gamma': 2, 'colsample_bytree': 0.7, 'lambda': 0.014950936465569798, 'alpha': 0.28520156840812494, 'subsample': 0.6}\nxgbReg = train(xgb.XGBRegressor(**params, tree_method='gpu_hist', random_state=42, predictor='gpu_predictor', learning_rate=0.01, nthread=-1))\n\ny_hat = xgbReg.predict(submission.to_numpy())        \nprint(mean_squared_error(xgbReg.predict(X), y, squared=False))\n\nsubmission[\"target\"] = y_hat \nsubmission[[\"target\"]].to_csv(\"\/kaggle\/working\/submission_xgb.csv\")\njoblib.dump(xgbReg, '\/kaggle\/working\/xgb_reg.pkl')","5be38cc4":"Integer parameter : A uniform distribution on integers.  \n```n_estimators = trial.suggest_int('n_estimators',100,500)```\n\nCategorical parameter : A categorical distribution.  \n```criterion = trial.suggest_categorical('criterion' ,['gini', 'entropy'])```\n\nUniform parameter : A uniform distribution in linear domain.  \n```subsample = trial.suggest_uniform('subsample' ,0.2,0.8)```\n\nDiscrete-uniform parameter : A discretized uniform distribution in linear domain.  \n```max_features = trial.suggest_discrete_uniform('max_features', 0.05,1,0.05)```\n\nLoguniform parameter : A uniform distribution in log domain.  **\n```learning_rate = trial.sugget_loguniform('learning_rate' : 1e-6, 1e-3)```"}}