{"cell_type":{"12428e2f":"code","1875a961":"code","74d4ca7f":"code","6371079b":"code","b31da911":"code","501ff58a":"code","ae22b1e0":"code","792fcc5c":"code","85e2dc17":"code","ff9120e4":"code","2174997a":"code","4aeff369":"code","92927fe2":"code","ad5fe550":"code","14f80784":"code","e547a325":"code","ca172fff":"code","7305bab1":"code","e6a656f2":"code","c78c928b":"code","592faf4e":"code","282134d6":"code","65a26fd4":"code","295097d8":"code","8191295e":"code","56bb8206":"code","2917db51":"code","48034a51":"markdown","fce5976f":"markdown","f91face6":"markdown","a7c93dcc":"markdown","25454190":"markdown","f8ea76af":"markdown","5730ae46":"markdown","918a77e6":"markdown","75e49dae":"markdown","d54f0e40":"markdown","4bb6fa2f":"markdown","1a87faf4":"markdown","22102a4c":"markdown","d6ef5474":"markdown","c52defb3":"markdown","1cd54bd9":"markdown","d34328db":"markdown"},"source":{"12428e2f":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport tqdm\nfrom multiprocessing import  Pool\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom math import sqrt\ntrain_on_gpu = False\n\n# Visualisation libs\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor","1875a961":"print('In input directory:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","74d4ca7f":"train = pd.read_csv('\/kaggle\/input\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/sample_submission.csv')\n\ntrain.shape, test.shape, sample_submission.shape","6371079b":"def score(y_actual, y_predicted):\n    return sqrt(mean_squared_log_error(y_actual, y_predicted))\n    \ndef fillNaNInfinity(df):\n    df.replace([np.inf, -np.inf], np.nan)\n    df.fillna(0, inplace=True)\n    return df\n\ndef fillInfinity(df):\n    df.replace([np.inf, -np.inf], np.nan)\n    return df","b31da911":"train.corr().style.background_gradient(cmap='coolwarm')","501ff58a":"# From https:\/\/www.kaggle.com\/miguelangelnieto\/pca-and-regression#Simple-Neural-Network, loved it\nnans = pd.isnull(train).sum()\nnans[ nans > 0 ]","ae22b1e0":"columns_to_remove = nans[ nans > 500 ].reset_index()['index'].tolist()\ncolumns_to_remove","792fcc5c":"train.dtypes.value_counts()","85e2dc17":"data = pd.concat([\n    train.loc[:, train.columns != 'SalePrice'], test\n])\n\ntarget = np.log(train['SalePrice'] + 1)\n\ndata = fillInfinity(data)\n\ndata.shape, target.shape","ff9120e4":"data.drop(labels=columns_to_remove, axis=1, inplace=True)\ndata.shape","2174997a":"data = pd.get_dummies(data)\ndata.shape","4aeff369":"imp = Imputer(missing_values='NaN', strategy='most_frequent', copy=False)\n\nimp.fit_transform(data)\ndata.shape","92927fe2":"scaler = StandardScaler()\n\ndata = scaler.fit_transform(data)\n\ndata.shape","ad5fe550":"data","14f80784":"data[np.isnan(data)] = 0","e547a325":"# pca = PCA(n_components=250)\n# pca.fit(data)\n# data = pca.transform(data)\n# data.shape","ca172fff":"# print('Total variance', pca.explained_variance_ratio_.sum())\n# pca.explained_variance_ratio_\n","7305bab1":"X = data[:1460]\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nn_estimators = [5, 10, 20, 25, 50, 100, 200]\ntrain_pred_mse = []\ntest_pred_mse = []\n\nfor n_estimator in n_estimators:\n    model = RandomForestRegressor(\n        n_estimators = n_estimator,\n        min_samples_leaf = 1,\n        max_depth = 8\n    )\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    train_pred_mse.append(score(y_train, y_train_pred))\n    test_pred_mse.append(score(y_test, y_test_pred))\n        \nfig = plt.figure(figsize=[12,6])\n\nline1, = plt.plot(n_estimators, train_pred_mse, 'b', label=\"Train MSE\")\nline2, = plt.plot(n_estimators, test_pred_mse, 'r', label=\"Test MSE\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('MSE score')\nplt.xlabel('n_estimators')\nplt.show()","e6a656f2":"max_depths = [1, 2, 4, 6, 8, 12, 16, 20, 24, 32]\n\ntrain_pred_mse = []\ntest_pred_mse = []\n\nfor max_depth in max_depths:\n    model = RandomForestRegressor(\n        n_estimators = 25, # because showing best performace in above plot\n        min_samples_leaf = 1,\n        max_depth = max_depth\n    )\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    train_pred_mse.append(score(y_train, y_train_pred))\n    test_pred_mse.append(score(y_test, y_test_pred))\n\nfig = plt.figure(figsize=[12,6])\n\nline1, = plt.plot(max_depths, train_pred_mse, 'b', label=\"Train MSE\")\nline2, = plt.plot(max_depths, test_pred_mse, 'r', label=\"Test MSE\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('MSE score')\nplt.xlabel('max_depth')\nplt.show()","c78c928b":"# based on above plot, picking the best params (note, don't just check the best dip, also look for overfitting)\nclf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42)","592faf4e":"X = data[:1460]\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","282134d6":"clf.fit(X_train, y_train)","65a26fd4":"score(y_train, clf.predict(X_train)), score(y_test, clf.predict(X_test))","295097d8":"xgbr = XGBRegressor(max_depth=5, n_estimators=400)\nX = data[:1460]\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n\nxgbr.fit(X_train, y_train)","8191295e":"score(np.exp(y_train) - 1, np.exp(xgbr.predict(X_train)) - 1), score(np.exp(y_test) - 1, np.exp(xgbr.predict(X_test)) - 1)","56bb8206":"test = data[1460:]\nsample_submission['SalePrice'] = xgbr.predict(test)\nsample_submission['SalePrice'] = np.exp(sample_submission['SalePrice']) - 1\nsample_submission.head()","2917db51":"sample_submission.to_csv('submission.csv', index=False)","48034a51":"# Feature engineering","fce5976f":"# Load the data","f91face6":"# Trying XGBoost","a7c93dcc":"# Helper functions","25454190":"We have almost half of the features of categorial type. Let's use OneHotEncoding to convert them to multiple boolean columns","f8ea76af":"Check the number of null values in each column","5730ae46":"# Import libraries","918a77e6":"See columns which have more than 500 cells null","75e49dae":"We saw we've many null values in dataset, so we will use imputer to replace null values with most frequent values in that feature.","d54f0e40":"# EDA","4bb6fa2f":"# Random Forest","1a87faf4":"Check score after taxing exponential of saleprice, because we took the log in start","22102a4c":"Now our `data` is numpy array, so please note, any pandas functions won't work on it","d6ef5474":"## Random forest - finding best params","c52defb3":"This kernel is for playing around with House Prices data.\nThe competition uses RMSLE, so I only use this metric to validate my model. R2Score is not usefull for this data.\n\nI'll try to use Random forest only to get the best result (note: previously in this kernel, I also used Linear Regression)","1cd54bd9":"Before running random forest, we will do few things. We will run `StandardScaler` to scale the data and `PCA` to reduce the number of features. Our `data` variable has already imputed train and test data","d34328db":"I did PCA and my results were not that good. After disabling them, result got little better, so not doing PCA for this data."}}