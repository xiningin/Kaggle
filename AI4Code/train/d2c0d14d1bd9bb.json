{"cell_type":{"cbe93a00":"code","b1bd6fc1":"code","a449dcf1":"code","3af75f6b":"code","ed0b79d5":"code","45acc98e":"code","fa2e6475":"code","17244283":"code","d19373b1":"code","6b7eba87":"code","b8c2d491":"code","f8390ad1":"code","6c31a34f":"code","6a4b151e":"code","4c66542d":"code","dbc26148":"code","4d4a5ae2":"code","df812f3a":"code","81a1a76a":"code","44979ed7":"code","bdc65c90":"code","bf2d81e0":"code","72e1d19b":"code","ad8201cf":"code","d2d10209":"code","baf8dd98":"code","a64fa0cc":"code","5b1a1d23":"code","488fe726":"code","3892b1a9":"code","bef33070":"code","af14e187":"code","97921e50":"code","79a9bc46":"code","e2055b80":"code","e2e8d0f4":"code","b3dce581":"code","69b00f2d":"code","52f059b5":"code","a504b6da":"code","92e31683":"code","24a78883":"code","0fa76d50":"code","7993edcb":"markdown","246b3228":"markdown","2f229574":"markdown","e4841513":"markdown","838b6f03":"markdown","8dbf6f90":"markdown","32a73d05":"markdown","59003c90":"markdown","fed5fa05":"markdown"},"source":{"cbe93a00":"import cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport random\nimport pickle\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport gc\nimport xgboost as xgb\nimport pickle\nfrom collections import Counter","b1bd6fc1":"REMAKE_TRAINING = False","a449dcf1":"PATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()","3af75f6b":"SCALE = 100\ndev_df[\"target\"] *= SCALE\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)","ed0b79d5":"order_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\nlen(order_book_training), len(order_book_test)","45acc98e":"trades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\nlen(trades_training), len(trades_test)","fa2e6475":"%cd \/kaggle\/input\/rapids-kaggle-utils\/","17244283":"import cu_utils.transform as cutran\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\", method='cudf')\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\ndef extract_raw_book_features(df, null_val=-9999):\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[f\"wap{n}\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n        df[f\"log_return{n}\"] = 100 * log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = 100 * df[f\"log_return{n}\"] ** 2\n    \n    n2 = 0.99\n    df['bid_size1_vibration'] = (df['bid_size1'] * (df['seconds_in_bucket'] \/ 300).exp())\/10000000\n    df['full_wap'] = (df['wap1'] * (df['bid_size1'] + df['ask_size1']) + df['wap2'] * n2 * (df['bid_size2'] + df['ask_size2']))\/ (df['bid_size1'] + df['ask_size1'] + n2 * df['bid_size2'] + n2 * df['ask_size2'])\n    df['log_full_return'] = 100 * log_diff(df, in_col = \"full_wap\", null_val=null_val)\n    df['realized_full_vol'] = 100 * df['log_full_return'] ** 2\n    df['resonance'] = df['realized_full_vol'] * df['seconds_in_bucket']\n    df['exp_resonance'] = 1000 * df['realized_full_vol'] * (df['seconds_in_bucket'] \/ 600).exp()\n    df[\"skewness\"] = 10 * df[f\"log_full_return\"] ** 3\n    df['realized_full_abs'] = abs(df['log_full_return'])\n    df['wap_balance'] = 10000 * abs(df['wap1'] - df['wap2'])\n    #df['wap_balance_return'] = 10 * log_diff(df, in_col=\"wap_balance\", null_val=null_val)\n    df['wap_balance_rel'] = 10000 * abs(df['wap1'] - df['wap2']) \/ (df['wap1'] + df['wap2'])\n    df['price_spread'] = 10000 * (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    # Include price_spread TWO? No. It's already included there indirectly\n    df['bid_spread'] = 10000 * (df['bid_price1'] - df['bid_price2'])\n    df['bid_spread_rel'] = df['bid_spread'] \/ df['bid_price1']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['ask_spread_rel'] = 1000 * df['ask_spread'] \/ df['ask_price2']\n    df['total_volume'] = ((df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])) \/ 10000\n    df['volume_imbalance'] = 0.001 * abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])) \/ df['total_volume']\n    df['real_action_imbalance'] = 0.001 * (abs(df['ask_size1'] + df['bid_size1']) - abs(df['ask_size2'] + df['bid_size2'])) \/ df['total_volume']\n    df['quad_volume_vibration'] = df['total_volume'] * (df['seconds_in_bucket'] \/ 600) ** 2\n    df['exp_volume_vibration'] = df['total_volume'] * ((df['seconds_in_bucket'] - 300) \/ 300).exp()\n    df['volume_vibration'] = df['total_volume'] * df['seconds_in_bucket'] \/ 10000\n    df['volume_imbalance_vibration'] = df['volume_imbalance'] * (df['seconds_in_bucket'] \/ 600).exp()\n    df[\"c\"] = 1\n    df = df[df[\"keep_row\"]]\n    df['bid_size1'] \/= 1000000\n    return df\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df['per_order'] = df['size'] \/ (df['order_count'] + 1)\n    df['per_order_vibration'] = df['per_order'] * (df['seconds_in_bucket'] \/ 1200).exp()\n    df['realized_price'] = log_diff(df, in_col=f\"price\", null_val=null_val)\n    df[\"realized_vol_trade\"] = df['realized_price']**2\n    df[\"realized_abs_trade\"] = abs(df['realized_price'])\n    df[\"exp_vibration\"] = (df['seconds_in_bucket'] \/ 600).exp() * df['size']\n    df[\"vibration\"] = df['seconds_in_bucket'] * df['size']\n    df['order_count_vibration'] = (df['seconds_in_bucket'] \/ 400).exp() * df['order_count']\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    return(agg_df)\n\ndef extract_book_stats(df):\n    default_stats = [\"mean\", \"std\", \"var\"]\n    feature_dict = {\n        'bid_size1': [\"mean\", \"std\"],\n        'bid_size1_vibration': [\"mean\"],\n        'bid_price1': [\"mean\", \"std\", \"var\"],\n        'log_return1': [\"mean\", \"std\"],\n        'log_return2': [\"mean\", \"std\"],\n        'realized_vol1': [\"mean\", \"std\", \"sum\", \"var\"],\n        'realized_vol2': [\"mean\", \"std\", \"sum\"],\n        'wap1': [\"mean\", \"std\"],\n        'wap2': [\"mean\", \"std\"],\n        'full_wap': [\"sum\", \"mean\", \"std\", \"max\", \"min\"],\n        'log_full_return': [\"mean\", \"std\"],\n        'realized_full_vol': [\"mean\", \"std\"],\n        'skewness': [\"mean\"],\n        'realized_full_abs': [\"mean\"],\n        'wap_balance': [\"mean\", \"std\"],\n        #'wap_balance_return': default_stats,\n        'wap_balance_rel': default_stats,\n        'price_spread': default_stats,\n        'bid_spread': default_stats,\n        'ask_spread': default_stats,\n        'bid_spread_rel': default_stats,\n        'ask_spread_rel': default_stats,\n        'total_volume': default_stats,\n        'volume_imbalance': default_stats,\n        'real_action_imbalance': default_stats,\n        'volume_vibration': [\"mean\", \"var\", \"std\"],\n        'volume_imbalance_vibration': default_stats,\n        'c': [\"sum\"],\n        # Include sum of kurtosis? - I should because vol of \n        # Include a measure of amplitude (look at the rmspe definition below for an example)\n    }\n    all_df = agg(df, feature_dict)\n    sum_df = extract_book_sum_stats(df)\n    all_df = all_df.merge(sum_df, on=\"time_id\", how=\"left\")\n    return(all_df)\n\ndef extract_book_sum_stats(df):\n    # This one extracts the sums that cause trouble, because of missing values (I fill with zeros)\n    feature_dict = {\n        'bid_size1': [\"sum\"],\n        'bid_size1_vibration': [\"sum\"],\n        'bid_price1': [\"sum\"],\n        'log_return1': [\"sum\"],\n        'log_return2': [\"sum\"],\n        'log_full_return': [\"sum\"],\n        'realized_full_vol': [\"sum\"],\n        'wap1': [\"sum\"],\n        'wap2': [\"sum\"],\n        'skewness': [\"sum\"],\n        'realized_full_abs': [\"sum\"],\n        'wap_balance': [\"sum\"],\n        'wap_balance_rel': [\"sum\"],\n        'price_spread': [\"sum\"],\n        'bid_spread': [\"sum\"],\n        'ask_spread': [\"sum\"],\n        'bid_spread_rel': [\"sum\"],\n        'ask_spread_rel': [\"sum\"],\n        'total_volume': [\"sum\"],\n        'volume_imbalance': [\"sum\"],\n        'real_action_imbalance': [\"sum\"],\n        'volume_vibration': [\"sum\"],\n        'volume_imbalance_vibration': [\"sum\"],\n        'exp_volume_vibration': [\"sum\"],\n        'quad_volume_vibration': [\"sum\"],\n        'exp_resonance': [\"sum\"],\n        'resonance': [\"sum\"]\n        # Include sum of kurtosis? - I should because vol of \n        # Include a measure of amplitude (look at the rmspe definition below for an example)\n    }\n    dfzero = df[[\"time_id\"] + list(feature_dict.keys())]\n    dfzero.fillna(0.0)\n    return agg(dfzero, feature_dict)\n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"sum\", \"std\"],\n        'realized_abs_trade': [\"sum\", \"var\"],\n        'seconds_in_bucket':[\"count\", \"sum\"], #Include count unique? Review!\n        'size': [\"mean\", \"sum\"],\n        'order_count': [\"mean\"],\n        'vibration': [\"mean\"],\n        'order_count_vibration': [\"mean\"],\n        'exp_vibration': [\"mean\"],\n        'price': [\"mean\", \"std\"],\n        'per_order':[\"mean\", \"std\"],\n        'per_order_vibration': [\"mean\"]\n    }\n    return agg(df, feature_dict)\n\ndef time_constraint_fe(df, stats_df, last_sec, fe_function, cols):\n    sub_df = df[df[\"seconds_in_bucket\"] >= (600 - last_sec)].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{last_sec}'))\n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    book_cols = book_stats.columns\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    trade_cols = trade_stats.columns\n    \n    for last_sec in [150, 300, 450]:\n        book_stats = time_constraint_fe(book_df, book_stats, last_sec, extract_book_stats, book_cols) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, last_sec, extract_trade_stats, trade_cols)\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\ndef last_touches(a):\n    # Create the oscillations\n    plus_signs = [n for n in a.columns if 'max' in n]\n    minus_signs = [n.replace('max', 'min') for n in plus_signs]\n    new_sign = [n.replace('max', 'oscillation') for n in plus_signs]\n    for i, j, new_name in zip(plus_signs, minus_signs, new_sign):\n        a[new_name] = (a[i] - a[j]) \/ a[j]\n        a.drop([i, j], axis = 1, inplace = True)\n    \ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    smat = cudf.concat(stock_dfs)\n    #smat = last_touches(smat)\n    return(smat)","d19373b1":"past_test_volatility = process_data(order_book_test, trades_test, stock_ids)\nif REMAKE_TRAINING:\n    past_volatility = process_data(order_book_training, trades_training, stock_ids)\n    past_volatility.shape, past_test_volatility.shape\n    pickle.dump(past_volatility.to_pandas(), open(\"\/kaggle\/working\/pastVolatility.pickle\", \"wb\"))\nelse:\n    past_volatility = cudf.from_pandas(pickle.load(open(\"\/kaggle\/input\/optiver-trainingmatrix\/pastVolatility.pickle\", \"rb\")))","6b7eba87":"def stock_time_fe(df):\n    cols = ['realized_vol1_sum', 'realized_vol2_sum', 'realized_vol_trade_sum',\n            'realized_vol1_sum_150', 'realized_vol2_sum_150', 'realized_vol_trade_sum_150',\n            'realized_vol1_sum_300', 'realized_vol2_sum_300', 'realized_vol_trade_sum_300',\n            'realized_vol1_sum_450', 'realized_vol2_sum_450', 'realized_vol_trade_sum_450',\n            'order_count_mean', 'real_action_imbalance_mean', \n            'volume_imbalance_mean', 'skewness_mean', 'volume_imbalance_mean_150',\n            'full_wap_mean', 'real_action_imbalance_mean_300',\n            'real_action_imbalance_mean_450', 'volume_imbalance_mean_450',\n            'full_wap_mean_300', 'volume_vibration_mean', 'vibration_mean',\n            'volume_imbalance_vibration_sum',\n            'volume_imbalance_vibration_sum_450', 'order_count_vibration_mean', 'exp_resonance_sum_450',\n            'exp_resonance_sum', 'resonance_sum', 'exp_vibration_mean', 'quad_volume_vibration_sum',\n            'per_order_vibration_mean', 'bid_size1_std'\n           ] \n    #Include others here?\n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\"]:\n            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\nprint(past_volatility.shape)\npast_volatility[\"is_test\"] = False\npast_test_volatility[\"is_test\"] = True\nall_df = past_volatility.append(past_test_volatility).reset_index(drop=True)\nall_df = stock_time_fe(all_df)\nall_df.dropna(axis=1, how='all', inplace = True) # Drop columns with all positions NAN\n\n#Convert stock_id to categorical using one-hot encoding (Choose only a few that work since they're not that useful.\n# Candidates are on version 123)\n# codes = all_df['stock_id'].unique()\n# all_df = all_df.one_hot_encoding('stock_id', 'dummy_id', codes)\n# di = [f for f in all_df.columns if 'dummy_id' in f]\n# all_df[di] = all_df[di].astype('int8') # Reduce memory size\npickle.dump(all_df.columns, open(\"\/kaggle\/working\/stayingcolumns.pickle\", \"wb\"))\npast_volatility = all_df[~all_df[\"is_test\"]]\npast_test_volatility = all_df[all_df[\"is_test\"]]\ndel all_df\ngc.collect()\npast_volatility.shape","b8c2d491":"fs = set(pickle.load(open(\"\/kaggle\/input\/optiver-trainingmatrix\/featureschampion.pickle\", \"rb\"))[1])\npickle.dump(fs, open(\"\/kaggle\/working\/fslist.pickle\", \"wb\"))\nfspv = set([col for col in list(past_volatility.columns) if col not in {\"time_id\", \"stock_id\", \"target\", \"is_test\"}])\npickle.dump(fspv, open(\"\/kaggle\/working\/fspvlist.pickle\", \"wb\"))\nprint('tools that I erased', fs - fspv)\nprint('tools that I created that are new', len(fspv - fs))\nprint(len(fs), len(fspv))","f8390ad1":"# features_PCA will be deleted\nfeatures_PCA = list(fspv - fs)\nlen(features_PCA)","6c31a34f":"print('are there columns with some missing values?', past_volatility.isna().any().any())\nprint('are there columns with all missing values?', past_volatility.isna().all().any())","6a4b151e":"features_PCA","4c66542d":"# mydicts = Counter()\n# for i in range(5):\n#     thismodel = pickle.load(open(\"\/kaggle\/input\/optiver-trainingmatrix\/modelv1-fold-\" + str(i) + \".pickle\", \"rb\"))\n#     mydicts += Counter(thismodel[3])\n# # The features below are the features with less information. These will build the PCA\n# features_PCA = list(set([item[0] for item in mydicts.most_common()[-350:]]).intersection(set(past_volatility.columns)))\n# print(len(features_PCA))\n# features_PCA[:5]","dbc26148":"# Change data type\n# di = [[f, 'float32'] for f in past_volatility.columns if past_volatility[f].dtype == 'float64']\n# past_volatility = past_volatility.astype(dict(di))\n# print([f for f in past_volatility.columns if past_volatility[f].dtype == 'float64'])\n\n# di = [[f, 'float32'] for f in past_test_volatility.columns if past_test_volatility[f].dtype == 'float64']\n# past_test_volatility = past_test_volatility.astype(dict(di))\n# print([f for f in past_test_volatility.columns if past_test_volatility[f].dtype == 'float64'])\n\n# di = [f for f in past_test_volatility.columns if past_test_volatility[f].dtype == 'float64']\n# for f in di:\n#     print('statistics for', f)\n#     print(past_volatility[f].min(), past_volatility[f].mean(), past_volatility[f].max())\n# print('list the features with memory float64')","4d4a5ae2":"# Keep a record of the features for the PCA analysis for later, in case I want to review in a cheap CPU\n# pickle.dump(past_volatility[features_PCA].to_pandas(), open(\"\/kaggle\/working\/pastVolatility4PCA.pickle\", \"wb\"))","df812f3a":"#Let's move to the pandas world to run PCA\npv_pandas = past_volatility.to_pandas()\nptv_pandas = past_test_volatility.to_pandas()","81a1a76a":"column_means_dict = dict(pv_pandas[features_PCA].median())\npickle.dump(column_means_dict, open(\"\/kaggle\/working\/column_means_dict.pickle\", \"wb\"))\n#print(len(column_means_dict), column_means_dict)","44979ed7":"print('are there columns with some missing values?', pv_pandas[features_PCA].isna().any().any())\nprint('are there columns with all missing values?', pv_pandas[features_PCA].isna().all().any())","bdc65c90":"pv_pandas[features_PCA] = pv_pandas[features_PCA].fillna(column_means_dict)\nptv_pandas[features_PCA] = ptv_pandas[features_PCA].fillna(column_means_dict)","bf2d81e0":"print('are there columns with some missing values?', pv_pandas[features_PCA].isna().any().any())\nprint('are there columns with all missing values?', pv_pandas[features_PCA].isna().all().any())","72e1d19b":"NCOMP = 2\npca = PCA(n_components = NCOMP)\npca.fit(pv_pandas[features_PCA])\nprint(pca.explained_variance_ratio_)\ncolumns_pca = ['pc_' + str(i) for i in range(NCOMP)]\npc = pd.DataFrame(pca.transform(pv_pandas[features_PCA]), columns = columns_pca)\nptc = pd.DataFrame(pca.transform(ptv_pandas[features_PCA]), columns = columns_pca)\n\nNFPCA = 125\nfpca = FastICA(n_components = NFPCA)\nfpca.fit(pv_pandas[features_PCA])\ncolumns_fastica = ['fpca_' + str(i) for i in range(NFPCA)]\nfpc = pd.DataFrame(fpca.transform(pv_pandas[features_PCA]), columns = columns_fastica)\nfptc = pd.DataFrame(fpca.transform(ptv_pandas[features_PCA]), columns = columns_fastica)\n\npickle.dump(pca, open(\"\/kaggle\/working\/pca.pickle\", \"wb\"))\npickle.dump(fpca, open(\"\/kaggle\/working\/fpca.pickle\", \"wb\"))","ad8201cf":"fptc.head()","d2d10209":"# from __future__ import print_function  # for Python2\n# import sys\n\n# local_vars = list(locals().items())\n# for var, obj in local_vars:\n#     print(var, sys.getsizeof(obj))","baf8dd98":"pv_pandas.drop(features_PCA, axis = 1, inplace = True)\nptv_pandas.drop(features_PCA, axis = 1, inplace = True)\ndel column_means_dict\ndel past_volatility\ndel past_test_volatility\ngc.collect()","a64fa0cc":"pv_pandas = pd.concat([pv_pandas.reset_index(), pc.reset_index(), fpc.reset_index()], axis = 1).drop('index', axis = 1)\nptv_pandas = pd.concat([ptv_pandas.reset_index(), ptc.reset_index(), fptc.reset_index()], axis = 1).drop('index', axis = 1)","5b1a1d23":"pv_pandas.head()","488fe726":"ptv_pandas.head()","3892b1a9":"# di = [[f, 'float32'] for f in pv_pandas.columns if pv_pandas[f].dtype == 'float64']\n# pv_pandas = pv_pandas.astype(dict(di))\n# print([f for f in pv_pandas.columns if pv_pandas[f].dtype == 'float64'])\n\n# di = [[f, 'float32'] for f in ptv_pandas.columns if ptv_pandas[f].dtype == 'float64']\n# ptv_pandas = ptv_pandas.astype(dict(di))\n# print([f for f in ptv_pandas.columns if ptv_pandas[f].dtype == 'float64'])","bef33070":"past_volatility = cudf.DataFrame(pv_pandas)\npast_test_volatility = cudf.DataFrame(ptv_pandas)","af14e187":"del pv_pandas\ndel ptv_pandas\ndel pc\ndel ptc\ndel fpc\ndel fptc\ngc.collect()","97921e50":"dev_df = dev_df.merge(past_volatility, on=[\"stock_id\", \"time_id\"], how=\"left\")\nfeatures = [col for col in list(dev_df.columns)\n            if col not in {\"stock_id\", \"target\", \"is_test\"}]\nlen(features)","79a9bc46":"pd_df_full = dev_df.to_pandas()\n#limit the fine-tuning to only a seventh of the data\n#num1 = random.randint(0, 4)\n#pd_df = pd_df_full[pd_df_full[\"time_id\"].values % 5 == num1]\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n","e2055b80":"# Long Model\n# def objective(trial):\n#     num1 = 3 #random.randint(0, 4)\n#     #limit the fine-tuning to only a seventh of the data\n#     train_x, test_x, train_y, test_y = train_test_split(pd_df_full[features], pd_df_full['target'], test_size=0.22, random_state=42)\n#     # To select which parameters to optimize, please look at the XGBoost documentation:\n#     # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n#     param = {\n#         'booster': trial.suggest_categorical('booster', ['gbtree']),\n#         'tree_method':'gpu_hist',  # Use GPU acceleration\n#         'lambda': trial.suggest_loguniform(\n#             'lambda', 0.0001, 0.15\n#         ),\n#         'alpha': trial.suggest_loguniform(\n#             'alpha', 5e-1, 10.0\n#         ),\n#         'colsample_bytree': trial.suggest_float(\n#             'colsample_bytree', 0.6, 0.85 #Default is 1.0\n#         ),\n#         'colsample_bylevel': trial.suggest_float(\n#             'colsample_bylevel', 0.6, 0.8 #Default is 1.0\n#         ),\n#         'colsample_bynode': trial.suggest_float(\n#             'colsample_bynode', 0.65, 0.99 #Default is 1.0\n#         ),\n#         'subsample': trial.suggest_float(\"subsample\", 0.6,  0.95),\n#         'learning_rate': trial.suggest_float(\n#             'learning_rate', 0.002, 0.07\n#         ),\n#         'n_estimators': trial.suggest_categorical(\t\n#             \"n_estimators\", [3000, 3500]\n#         ),\n#         'max_depth': trial.suggest_int(\n#             'max_depth', 22, 27\n#         ),\n#         'random_state': 42,\n#         'min_child_weight': trial.suggest_int(\n#             'min_child_weight', 55, 160\n#         ),\n#     }\n#     model = XGBRegressor(**param)\n    \n#     model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds = 50, verbose=False)\n    \n#     preds = model.predict(test_x)\n#     #rmse = mean_squared_error(test_y, preds, squared=False)\n#     rmspeval = rmspe(test_y, preds)\n#     return rmspeval\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials = 90)\n\n# print(\"Number of finished trials: \", len(study.trials))\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))\n\n# fig = optuna.visualization.plot_param_importances(study)\n# fig.show()\n\n# # Assign best model\n# best_params = study.best_params\n# best_params['tree_method'] = 'gpu_hist'\n# best_params['random_state'] = 42\n\n# best_params = {\n#          \"booster\": 'gbtree',\n#          \"objective\": 'reg:squarederror',\n#          \"lambda\": 0.003692070094677976,\n#          \"alpha\": 0.843665639617332,\n#          \"colsample_bytree\": 0.6890567485273894,\n#          \"colsample_bylevel\": 0.7522102593718745,\n#          \"colsample_bynode\": 0.6643596703827361,\n#          \"subsample\": 0.8033471108457755,\n#          \"learning_rate\": 0.027930261435649854,\n#          \"max_depth\": 26,\n#          \"min_child_weight\": 127,\n#          #\"reg_alpha\": 10.0,\n#          \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n#          'disable_default_eval_metric': 1\n#     }","e2e8d0f4":"# Short Model\n# def objective(trial):\n#     num1 = 3 #random.randint(0, 4)\n#     #limit the fine-tuning to only a seventh of the data\n#     pd_df = pd_df_full\n#     train_x, test_x, train_y, test_y = train_test_split(pd_df[features], pd_df['target'], test_size=0.22, random_state=42)\n#     # To select which parameters to optimize, please look at the XGBoost documentation:\n#     # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n#     param = {\n#         'booster': trial.suggest_categorical('booster', ['gbtree']),\n#         'tree_method':'gpu_hist',  # Use GPU acceleration\n#         'lambda': trial.suggest_loguniform(\n#             'lambda', 0.0001, 0.5\n#         ),\n#         'alpha': trial.suggest_loguniform(\n#             'alpha', 5e-1, 20.0\n#         ),\n#         'reg_alpha': trial.suggest_float(\n#             'reg_alpha', 5e-1, 20.0\n#         ),\n#         'colsample_bytree': trial.suggest_float(\n#             'colsample_bytree', 0.7, 0.9 #Default is 1.0\n#         ),\n#         'colsample_bylevel': trial.suggest_float(\n#             'colsample_bylevel', 0.75, 0.99 #Default is 1.0\n#         ),\n#         'colsample_bynode': trial.suggest_float(\n#             'colsample_bynode', 0.9, 1.0 #Default is 1.0\n#         ),\n#         'subsample': trial.suggest_float(\"subsample\", 0.6,  0.95),\n#         'learning_rate': trial.suggest_float(\n#             'learning_rate', 0.002, 0.06\n#         ),\n#         'n_estimators': trial.suggest_categorical(\t\n#             \"n_estimators\", [3500, 5000]\n#         ),\n#         'max_depth': trial.suggest_int(\n#             'max_depth', 6, 10\n#         ),\n#         'random_state': 42,\n#         'min_child_weight': trial.suggest_int(\n#             'min_child_weight', 80, 200\n#         ),\n#     }\n#     model = XGBRegressor(**param)\n    \n#     model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds = 50, verbose=False)\n    \n#     preds = model.predict(test_x)\n#     #rmse = mean_squared_error(test_y, preds, squared=False)\n#     rmspeval = rmspe(test_y, preds)\n#     return rmspeval\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials = 50)\n\n# print(\"Number of finished trials: \", len(study.trials))\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))\n\n# fig = optuna.visualization.plot_param_importances(study)\n# fig.show()\n\n# # Assign best model\n# best_params = study.best_params\n# best_params['tree_method'] = 'gpu_hist'\n# best_params['random_state'] = 43\n\n# best_params = {\n#          \"booster\": 'gbtree',\n#          \"objective\": 'reg:squarederror',\n#          \"lambda\": 0.3900133957228146,\n#          \"alpha\": 1.6143973425818008,\n#          \"colsample_bytree\": 0.8240236558740445,\n#          \"colsample_bylevel\": 0.7055628112659318,\n#          \"colsample_bynode\": 0.9858168527166616,\n#          \"subsample\": 0.8164845919960058,\n#          \"learning_rate\": 0.023404748889126782,\n#          \"max_depth\": 16,\n#          \"min_child_weight\": 187,    \n#          \"reg_alpha\": 5.0,\n#          \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n#          'disable_default_eval_metric': 1\n#     }\n\n# best_params = {\n#          \"booster\": 'gbtree',\n#          \"objective\": 'reg:squarederror',\n#          \"max_depth\": 7,\n#          \"min_child_weight\": 156,    \n#          \"lambda\": 0.006654377522518237,\n#          \"alpha\": 3.398238828590107,\n#          \"colsample_bytree\": 0.6,\n#          \"subsample\": 0.9134709113526331,\n#          \"learning_rate\": 0.010751601995412153,\n#          \"reg_alpha\": 10.0,\n#          \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n#          'disable_default_eval_metric': 1\n#     }\n# # This is 22092\nbest_params = {\n         \"booster\": 'gbtree',\n         \"objective\": 'reg:squarederror',\n         \"lambda\": 0.000120494603280191,\n         \"alpha\": 4.655248899194473,\n         \"reg_alpha\": 5.75750136407511,\n         \"colsample_bytree\": 0.7033853760069456,\n         \"colsample_bylevel\": 0.9241863146227871,\n         \"colsample_bynode\": 0.9902499881033184,\n         \"subsample\": 0.926724493495993,\n         \"learning_rate\": 0.059258258818345,\n         \"max_depth\": 10,\n         \"min_child_weight\": 97,    \n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1\n    }\n\nbest_params = {\n         \"booster\": 'dart',\n         \"rate_drop\": 0.10,\n         \"skip_drop\": 0.5,\n         \"objective\": 'reg:squarederror',\n         \"max_depth\": 9,\n         \"min_child_weight\": 147,    \n         \"lambda\": 0.006654377522518237,\n         \"alpha\": 3.398238828590107,\n         \"colsample_bytree\": 0.7033853760069456,\n         \"colsample_bylevel\": 0.9241863146227871,\n         \"colsample_bynode\": 0.9902499881033184,\n         \"subsample\": 0.9134709113526331,\n         \"learning_rate\": 0.012751601995412153,\n         \"reg_alpha\": 10.0,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1\n    }","b3dce581":"# Change data type and save some memory\n# di = [[f, 'float32'] for f in pd_df_full.columns if pd_df_full[f].dtype == 'float64']\n# pd_df_full = pd_df_full.astype(dict(di))\n# print([f for f in pd_df_full.columns if pd_df_full[f].dtype == 'float64'])\n# gc.collect()","69b00f2d":"import xgboost as xgb\n\ndef get_xgb_imp(xgb, feat_names):\n    from numpy import array\n    imp_vals = xgb.get_booster().get_fscore()\n    imp_dict = {feat_names[i]:float(imp_vals.get('f'+str(i),0.)) for i in range(len(feat_names))}\n    total = array(imp_dict.values()).sum()\n    return {k:v\/total for k,v in imp_dict.items()}\n\ndef rmspe(y_true, y_pred):\n    return (cp.sqrt(cp.mean(cp.square((y_true - y_pred) \/ y_true))))\n\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(cp.array(y), cp.array(pred))\n\n\nNUM_FOLDS = 5\n\ntarget = \"target\"\n\noof_preds = cp.zeros(dev_df.shape[0])\ntest_preds = cp.zeros(past_test_volatility.shape[0])\nkfold = GroupKFold(n_splits = NUM_FOLDS)\n\nfor fold, (train_ind, val_ind) in enumerate(kfold.split(pd_df_full[features], pd_df_full[target].values, pd_df_full[\"time_id\"].values)):\n    print(\"Fold\", fold)    \n    train_df, val_df = dev_df.iloc[train_ind], dev_df.iloc[val_ind]\n    \n    d_train = xgb.DMatrix(train_df[features], train_df[target], weight=1\/cp.square(train_df[target]))\n    d_val = xgb.DMatrix(val_df[features], val_df[target], weight=1\/cp.square(val_df[target]))\n    \n    model = xgb.train(best_params, d_train, evals = [(d_train, \"train\"), (d_val, \"val\")], \n                      num_boost_round = 3500, \n                      verbose_eval = 50, feval = rmspe_xgb,\n                      early_stopping_rounds = 200)\n    importances = {k: v for k, v in sorted(model.get_fscore().items(), key=lambda item: item[1], reverse = True)}\n    print('Importances for this iteration:', importances)\n    \n    pickle.dump({\"model\": model, \"features\": features, \"best_params\": best_params, \"importances\":importances, \n                 \"features_PCA\": features_PCA, \"pca\": pca, \"fpca\": fpca}, \n                open(\"\/kaggle\/working\/modelv2-fold-\" + str(fold) + \".pickle\", \"wb\"))\n    \n    oof_preds[val_ind] = model.predict(d_val)\n    test_preds += cp.array(model.predict(xgb.DMatrix(past_test_volatility[features].astype(\"float\")))\/NUM_FOLDS)","52f059b5":"dev_df[\"pred\"] = oof_preds\nprint(f'The RMSPE score of XGB is {rmspe(dev_df[\"target\"], dev_df[\"pred\"])}')","a504b6da":"past_test_volatility[\"row_id\"] = past_test_volatility[\"stock_id\"].astype(str) + \"-\" + past_test_volatility[\"time_id\"].astype(str) \npast_test_volatility[\"target\"] = test_preds.clip(0.0, 100.0)\/SCALE","92e31683":"%cd \/kaggle\/working","24a78883":"sub_df = load_data(\"test\", path=PATH).merge(past_test_volatility[[\"row_id\", \"target\"]], on=\"row_id\", how=\"left\")\nsub_df['target'] = sub_df['target'].fillna(0.00373)\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])","0fa76d50":"cudf.read_csv(\"submission.csv\")","7993edcb":"## Using rapids-kaggle-utils for missing cuDF aggregation functions","246b3228":"Remove the columns from pandas. Make sure that they get replaced by the PCA and fastICA columns","2f229574":"optuna selection","e4841513":"## Train XGBoost model on GPU","838b6f03":"Optuna regression setup","8dbf6f90":"Fill NAs","32a73d05":"Cocatenate the results","59003c90":"#Just drop the features that have NANs from any analysis","fed5fa05":"Create the PCA columns"}}