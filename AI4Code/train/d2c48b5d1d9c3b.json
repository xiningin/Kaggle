{"cell_type":{"4da127d2":"code","54d0f6a1":"code","a6105a30":"code","7f0a3190":"code","d8c50ddb":"code","d2969228":"code","d7cc9f87":"code","0f5dcd86":"code","150b6d3e":"code","e54b287a":"code","c2ef5d23":"code","e6a05793":"code","019719be":"code","3c7ac337":"code","ce559416":"code","2369477c":"code","89278e9e":"code","168235f2":"code","f7fa2b09":"code","7f5b885b":"code","4e677dd1":"code","623e2e48":"code","a82c5dd0":"code","03abffe3":"code","3b5d90d7":"code","03ebaa52":"code","2d5df61e":"code","69b86af7":"code","e13913ef":"code","dd6d9224":"code","52aeaf6b":"code","5caf6960":"code","d6f1e125":"code","71ed03cc":"code","0ec44e7e":"code","3ccd6176":"code","e0629393":"code","425d4fba":"markdown","703c6d09":"markdown","02939a4d":"markdown","58656c6b":"markdown","8b96f276":"markdown","194a1052":"markdown","4350dad8":"markdown","5939b27b":"markdown","8da59270":"markdown","861ddeab":"markdown","33b4f504":"markdown","7a484408":"markdown","cd970525":"markdown","58e7b0a0":"markdown"},"source":{"4da127d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport IPython\nimport IPython.display\nimport PIL\nimport psutil\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport librosa\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","54d0f6a1":"! ls ..\/input\/fat2019_prep_mels1","a6105a30":"! ls ..\/input\/fat2019-multipreprocessed-package","7f0a3190":"! ls ..\/input\/fat19-fastai-weights-of-mixup-mp","d8c50ddb":"# Fix result\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 999\nseed_everything(SEED)","d2969228":"def _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n            retrieved_cumulative_hits[class_rankings[pos_class_indices]] \/\n            (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class \/ float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) \/\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) \/ np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class","d7cc9f87":"TRAIN_MODE = False\nCONTINUOUS_TRAIN = True\nMIXMATCH_SSL = 0 # >= 0 (0 is mixup mode)","0f5dcd86":"DATA = Path('..\/input\/freesound-audio-tagging-2019')\nPREPROCESSED_N1K = Path('..\/input\/fat2019_prep_mels1')\nPREPROCESSED_MP = Path('..\/input\/fat2019-multipreprocessed-package')\nLAST_WEIGHTS = Path('..\/input\/fat19-fastai-weights-of-mixup-mp')\nWORK = Path('work')\nPath(WORK).mkdir(exist_ok=True, parents=True)\n\nCSV_TRAIN_CURATED = PREPROCESSED_MP\/'train_curated_valid.csv'\nCSV_TRAIN_NOISY = PREPROCESSED_MP\/'train_noisy_valid.csv'\nCSV_TRAIN_NOISY_BEST50S = PREPROCESSED_N1K\/'trn_noisy_best50s.csv'\nCSV_SUBMISSION = DATA\/'sample_submission.csv'\n\nSAVED_WEIGHT = LAST_WEIGHTS\/'last'\nDEPLOYED_MODEL = LAST_WEIGHTS\/'export.pkl'\n\ndef add_prefix_path(df, prefix):\n    fnames = df.fname.copy()\n    df.fname = prefix + os.sep\n    df.fname = df.fname.str.cat(fnames)\n    return df\ndef filter_df(df):\n    few_labels = list(map(lambda p: len(p[1]) <= 3, df.labels.str.split(',').iteritems()))\n    return df[few_labels]\n\ndf_curated = add_prefix_path(pd.read_csv(CSV_TRAIN_CURATED), 'train_curated')\ndf_noisy = add_prefix_path(pd.read_csv(CSV_TRAIN_NOISY), 'train_noisy')\n# df_noisy = filter_df(df_noisy) # filtering fewer labels than 3 or equal it\ndf_noisy50s = add_prefix_path(pd.read_csv(CSV_TRAIN_NOISY_BEST50S), 'train_noisy')\ndf_submission = pd.read_csv(CSV_SUBMISSION)\ndf_test = add_prefix_path(df_submission.copy(), 'test')\n\ndf_train = df_curated\n# df_train = pd.concat([df_curated, df_noisy], ignore_index=True)\n# df_train = pd.concat([df_curated, df_noisy50s], ignore_index=True, sort=True)\nprint('Training dataset size:', len(df_train))\ndf_train.head()","150b6d3e":"# post augmentations\nUSE_MASK_FREQ       = True\nMASK_FREQ_RANGE     = 8   #[mels]\nMASK_FREQ_MAX_COUNT = 3\n\nUSE_MASK_TIME       = True\nMASK_TIME_RANGE     = 8   #[frames]\nMASK_TIME_MAX_COUNT = 3\n\ndef freq_mask(x, num=1, mask_size=10, mask_value=None, inplace=False):\n    cloned = x.clone() if not inplace else x\n    num_bins = cloned.shape[1]\n    mask_value = cloned.mean() if mask_value is None else mask_value\n    for i in range(num):\n        f = random.randrange(0, mask_size + 1)\n        if f == 0 or f >= num_bins: continue\n\n        f_low = random.randrange(0, num_bins - f)\n        f_high = f_low + f\n        cloned[:, f_low:f_high] = mask_value\n    return cloned\n\ndef time_mask(x, num=1, mask_size=10, mask_value=None, inplace=False):\n    cloned = x.clone() if not inplace else x\n    num_frames = cloned.shape[2]\n    mask_value = cloned.mean() if mask_value is None else mask_value\n    for i in range(num):\n        t = random.randrange(0, mask_size + 1)\n        if t == 0 or t >= num_frames: continue\n\n        t_beg = random.randrange(0, num_frames - t)\n        t_end = t_beg + t\n        cloned[:, :, t_beg:t_end] = mask_value\n    return cloned","e54b287a":"# Data processing configuration\nclass AugmentationConfig:\n    padding_scale = 1.\n    whitenoise = True\n    whitenoise_level = 1e-3 # 0.~1.\n    pitchshift = True\n    pitchshift_steps = 2. # steps (12\/oct)\n\nclass PreproConfig:\n    sr = 44100\n    duration = 2. # secs\n    n_out = 128\n    n_mels = 128\n    n_fft = n_mels * 20\n    \n    hop_len = int(sr * duration \/\/ n_out)\n    sample_size = int(sr * duration)\n    padding_size = int(sample_size * AugmentationConfig.padding_scale)\n\n# preprocessor\ndef load_audio(file, eps=1e-6):\n    pcm, sr = librosa.load(file, sr=PreproConfig.sr)\n    if len(pcm) <= 0:\n        # raise 'No audio samples'\n        pcm = np.zeros(PreproConfig.sample_size) + 1e-8\n        sr = PreproConfig.sr\n\n    pcm_max = np.max(np.abs(pcm))\n    if pcm_max > eps:\n        pcm = pcm \/ pcm_max\n\n    min_size = PreproConfig.padding_size\n    if len(pcm) > min_size: pcm, _ = librosa.effects.trim(pcm)\n    pcm = pad_zeros(pcm, min_size)\n    return pcm, pcm_max\n\ndef pad_zeros(x, size):\n    if len(x) > size: # long enough\n        return x\n    else: # pad blank\n        padding = size - len(x)\n        offset = padding \/\/ 2\n        return np.pad(x, (offset, size - len(x) - offset), 'constant')\n\ndef conv_pcm_to_magphs(x):\n    fft = librosa.core.stft(x, n_fft=PreproConfig.n_fft, hop_length=PreproConfig.hop_len)\n    return np.abs(fft), np.angle(fft)\n\ndef conv_magphs_to_mels(magphs):\n    mel_basis = librosa.filters.mel(PreproConfig.sr, PreproConfig.n_fft, n_mels=PreproConfig.n_mels)\n    return np.dot(mel_basis, magphs)\n\ndef conv_mag_to_melspec(mag):\n    mels = conv_magphs_to_mels(mag ** 2.)\n    mels = librosa.power_to_db(mels)\n    m_max = mels.max()\n    m_min = mels.min()\n    return (mels - m_min) \/ (m_max - m_min)\n\ndef normalize_melspec(melspec, eps=1e-6, dtype=np.uint8):\n    mean = melspec.mean()\n    std = melspec.std()\n    ms_std = (melspec - mean) \/ (std + eps)\n    ms_min, ms_max = ms_std.min(), ms_std.max()\n    if (ms_max - ms_min) > eps:\n        ms_norm = 255. * (ms_std - ms_min) \/ (ms_max - ms_min)\n        return ms_norm.astype(dtype)\n    else:\n        return np.zeros_like(ms_std, dtype=dtype)\n\ndef preprocess(pcm, debug=False, dtype=np.uint8):\n    melspec, melphss = conv_pcm_to_magphs(pcm)\n    melspec = conv_mag_to_melspec(melspec)\n    melspec = normalize_melspec(melspec, dtype=dtype)\n    return melspec","c2ef5d23":"# multi-preprocessed data loader\nclass MPLoader:\n    cache = dict()\n    cache_vmem_percent = 45.0\n    \n    data_type = 'msp' # msp, mph, mfcc\n    \n    use_augmentation = True\n    max_of_augid = 2\n    \n    def reset():\n        MPLoader.cache = dict()\n    \n    def get(fname, augmentation=False, cache=False):\n        if fname in MPLoader.cache:\n            data = MPLoader.cache[fname]\n        else:\n            data = MPLoader.load(fname, cache)\n\n        if augmentation and MPLoader.use_augmentation:\n            return random.choice(data)\n        else:\n            return data[0]\n\n    def load(fname, cache=False):\n        data_raw = np.load((PREPROCESSED_MP\/fname).with_suffix('.wav.npz'))\n        data = list()\n\n        if MPLoader.use_augmentation:\n            for i in range(1, MPLoader.max_of_augid+1):\n                tid = f'{MPLoader.data_type}{i}'\n                if tid in data_raw:\n                    data.append(MPLoader._mp2tensor(data_raw[tid]))\n                else:\n                    break\n        else:\n            tid = f'{MPLoader.data_type}1'\n            data.append(MPLoader._mp2tensor(data_raw[tid]))\n\n        if cache: MPLoader.cache[fname] = data\n        return data\n\n    def load_and_process(fname, cache=False):\n        pcm, _ = load_audio(DATA\/fname)\n        mels = preprocess(pcm)\n        data = [MPLoader._mp2tensor(mels)]\n        if cache: MPLoader.cache[fname] = data\n        return data\n    \n    def _mp2tensor(mp):\n        mp = torch.FloatTensor(mp).div_(255)\n        return mp\n    \n    def full_load(df, use_preprocess=True):\n        vmem_limit = MPLoader.cache_vmem_percent\n        virt = psutil.virtual_memory()\n        if virt.percent >= vmem_limit:\n            vmem_limit = virt.percent + 0.1\n        \n        for i, row in tqdm_notebook(df.iterrows(), total=len(df)):\n            if use_preprocess:\n                MPLoader.load(row.fname, cache=True)\n            else:\n                MPLoader.load_and_process(row.fname, cache=True)\n            if i % 10 == 0:\n                virt = psutil.virtual_memory()\n                if virt.percent >= vmem_limit:\n                    mb = 1024 * 1024\n                    print(f'Stopped loading preprocessed data as cache, '\n                          f'cause memory usage reached {vmem_limit}%.')\n                    print('Memory usage:', virt.available>>20, '\/', virt.total>>20, 'MB')\n                    break\n                elif i % 1000 == 0:\n                    print(virt.percent, '%', virt.available>>20, 'MB')\n        print('Completed loading preprocessed data.')\n\nif TRAIN_MODE:\n    MPLoader.full_load(df_train)\n    if MIXMATCH_SSL > 0:\n        MPLoader.full_load(df_noisy)","e6a05793":"plt.figure(figsize=(8,4))\nplt.imshow(MPLoader.get(df_train.fname[0]).numpy(), origin='lower')\nplt.title(df_train.labels[0])\nplt.show()","019719be":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.data import *\nfrom fastai.callbacks import *\nimport random\n\nTIME_DIM = 128\n\ndef open_fat2019_image(fn, convert_mode, after_open)->Image:\n    # open\n    fname = '\/'.join(fn.split('\/')[-2:])\n    x = MPLoader.get(fname, augmentation=True)\n    # crop 2sec\n    base_dim, time_dim = x.shape\n    if time_dim < TIME_DIM:\n        x2 = torch.zeros((base_dim,TIME_DIM), dtype=x.dtype)\n        crop = random.randint(0, TIME_DIM - time_dim)\n        x2[:, crop:crop+time_dim] = x\n        x = x2\n    else:\n        crop = random.randint(0, time_dim - TIME_DIM)\n        x = x[:, crop:crop+TIME_DIM]\n    x = torch.stack((x,x,x), dim=0)\n    # masking\n    if USE_MASK_FREQ: freq_mask(\n        x,\n        num=random.randrange(MASK_FREQ_MAX_COUNT),\n        mask_size=MASK_FREQ_RANGE,\n        mask_value=0,\n        inplace=True,\n    )\n    if USE_MASK_TIME: time_mask(\n        x,\n        num=random.randrange(MASK_TIME_MAX_COUNT),\n        mask_size=MASK_TIME_RANGE,\n        mask_value=0,\n        inplace=True,\n    )\n    # standardize\n    return Image(x)\n\nvision.data.open_image = open_fat2019_image","3c7ac337":"BATCH_SIZE = 48 # used 3-times CUDA memory\n\ntfms = get_transforms(do_flip=True, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\n# src = (ImageList.from_csv(WORK, Path('..')\/CSV_TRAIN_CURATED, folder='train_curated')\nsrc = (ImageList.from_df(df_train, WORK, folder='')\n#        .split_by_rand_pct(0.1, seed=SEED) # for Validation\n       .split_none()\n       .label_from_df(label_delim=',')\n      )\ndata = (src.transform(tfms, size=128)\n        .databunch(bs=BATCH_SIZE).normalize(imagenet_stats)\n       )\n\nif MIXMATCH_SSL > 0:\n    noisy_src = (ImageList.from_df(df_noisy, WORK, folder='')\n                 .split_none()\n                 .label_from_df(label_delim=',')\n                )\n    noisy_data = (noisy_src.transform(tfms, size=128)\n                  .databunch(bs=BATCH_SIZE).normalize(imagenet_stats)\n                 )","ce559416":"if TRAIN_MODE: data.show_batch(3)","2369477c":"def lwlrap(y_pred,y_true):\n    score, weight = calculate_per_class_lwlrap(y_true.cpu().numpy(), y_pred.cpu().numpy())\n    lwlrap = (score * weight).sum()\n    return torch.from_numpy(np.array(lwlrap))","89278e9e":"# Customized MixMatch for multi-label\n# referenced:\n#  - https:\/\/github.com\/perrying\/realistic-ssl-evaluation-pytorch\n#  - https:\/\/github.com\/YU1ut\/MixMatch-pytorch\nclass MixMatchCallback(LearnerCallback):\n    def __init__(self, learn:Learner,\n                 unlabeled_dl:DeviceDataLoader,\n                 temperature:float=0.5, n_augment:int=2,\n                 alpha:float=0.75, lambda_u:float=100, rampup:int=16):\n        super().__init__(learn)\n        self.unlabeled_dl = unlabeled_dl\n        self.T = temperature\n        self.K = n_augment\n        self.beta_distirb = torch.distributions.beta.Beta(alpha, alpha)\n        self.lambda_u = lambda_u\n        self.rampup = rampup\n        self.n_iterations = len(learn.data.train_dl)\n\n    def on_train_begin(self, **kwargs):\n        self.unlabeled_iter = iter(self.unlabeled_dl)\n        self.learn.loss_func = MultiSemiLoss(self.learn.loss_func, lambda_u=self.lambda_u, rampup=self.rampup)\n\n    def on_train_end(self, **kwargs):\n        self.unlabeled_iter = None\n        self.learn.loss_func = self.learn.loss_func.get_old()\n    \n    def unlabeled_next_batch(self):\n        try:\n            return next(self.unlabeled_iter)\n        except:\n            self.unlabeled_iter = iter(self.unlabeled_dl)\n            return next(self.unlabeled_iter)\n\n    def sharpen(self, y):\n        y = y.pow(1 \/ self.T)\n        return y \/ y.sum(dim=1, keepdim=True)\n\n    def on_batch_begin(self, epoch, iteration, last_input, last_target, train, **kwargs):\n        if not train: return\n        with torch.no_grad():\n            bs = len(last_input)\n            \n            # K augmentation and make prediction labels\n            u_x_hat = [self.unlabeled_next_batch()[0] for _ in range(self.K)]\n#             u_x_hat = [u_x for _ in range(self.K)]\n            y_hat = sum([self.learn.model(u_x_hat[i]).sigmoid_() for i in range(self.K)]) \/ self.K\n            y_hat = self.sharpen(y_hat)\n            y_hat = y_hat.repeat(self.K, 1)\n            \n            # mixup\n            u_x_hat = torch.cat([last_input] + u_x_hat, dim=0)\n            y_hat = torch.cat((last_target, y_hat), dim=0)\n            index = torch.randperm(u_x_hat.shape[0])\n            \n            lam = self.beta_distirb.sample().item()\n            lam = max(lam, 1-lam)\n            \n            mixed_input = lam * u_x_hat + (1-lam) * u_x_hat[index]\n            mixed_input = self.interleave(torch.split(mixed_input, bs), bs)\n            mixed_target = lam * y_hat + (1-lam) * y_hat[index]\n            \n            loss_func = self.learn.loss_func\n            loss_func.labeled_bs = bs\n            loss_func.epoch = iteration \/ self.n_iterations\n        return {'last_input': mixed_input, 'last_target': mixed_target}\n\n    def on_loss_begin(self, last_input, last_output, iteration, **kwargs):\n        bs = len(last_input)\n        last_output = self.interleave(torch.split(last_output, bs), bs)\n        return {'last_output': last_output}\n\n    def interleave_offsets(self, batch, nu):\n        groups = [batch \/\/ (nu + 1)] * (nu + 1)\n        for x in range(batch - sum(groups)):\n            groups[-x - 1] += 1\n        offsets = [0]\n        for g in groups:\n            offsets.append(offsets[-1] + g)\n        return offsets\n\n    def interleave(self, xy, batch):\n        nu = len(xy) - 1\n        offsets = self.interleave_offsets(batch, nu)\n        xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n        for i in range(1, nu + 1):\n            xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n        return torch.cat([torch.cat(v, dim=0) for v in xy], dim=0)\n\nMM_LOG = pd.DataFrame(columns=['loss', 'loss_labeled', 'loss_unlabeled', 'weight'])\n\nclass MultiSemiLoss(nn.Module):\n    \"Adapt the loss function `crit` to go with mixmatch.\"\n    \n    def __init__(self, crit, lambda_u=100.0, rampup=16):\n        super().__init__()\n        self.crit = crit\n        self.labeled_bs = 0\n        self.lambda_u = lambda_u\n        self.rampup = rampup\n        self.epoch = 0\n        \n    def forward(self, output, target):\n        if self.labeled_bs == 0: return self.crit(output, target)\n        \n        output_x, target_x = output[:self.labeled_bs], target[:self.labeled_bs]\n        output_u, target_u = output[self.labeled_bs:], target[self.labeled_bs:]\n        \n        q = output_u.sigmoid()\n        \n        Lx = self.crit(output_x, target_x)\n        Lu = torch.mean((q - target_u) ** 2)\n        \n        w = self.lambda_u * (np.clip(self.epoch \/ self.rampup, 0., 1.) if self.rampup > 0 else 1.)\n        \n        self.labeled_bs = 0\n        d = Lx + w * Lu\n        MM_LOG.loc[self.epoch] = [float(d), float(Lx), float(Lu), w]\n        return d\n    \n    def get_old(self):\n        return self.crit\n\ndef _mixmatch(learn:Learner, unlabeled_data:ImageDataBunch,\n              temperature:float=0.5, n_augment:int=2,\n              alpha:float=0.75, lambda_u:float=100, rampup:int=16) -> Learner:\n    \"Add mixup https:\/\/arxiv.org\/abs\/1905.02249 to `learn`.\"\n    unlabeled_dl = unlabeled_data.dl(DatasetType.Train)\n    learn.callback_fns.append(partial(MixMatchCallback,\n                                      unlabeled_dl=unlabeled_dl,\n                                      temperature=temperature, n_augment=n_augment,\n                                      alpha=alpha, lambda_u=lambda_u, rampup=rampup))\n    return learn\nLearner.mixmatch = _mixmatch","168235f2":"class MixUpCallback(LearnerCallback):\n    \"Callback that creates the mixed-up input and target.\"\n    def __init__(self, learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True):\n        super().__init__(learn)\n        self.alpha,self.stack_x,self.stack_y = alpha,stack_x,stack_y\n    \n    def on_train_begin(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)\n        \n    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        \"Applies mixup to `last_input` and `last_target` if `train`.\"\n        if not train: return\n        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n        lambd = last_input.new(lambd)\n        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n        x1, y1 = last_input[shuffle], last_target[shuffle]\n        if self.stack_x:\n            new_input = [last_input, last_input[shuffle], lambd]\n        else: \n            new_input = (last_input * lambd.view(lambd.size(0),1,1,1) + x1 * (1-lambd).view(lambd.size(0),1,1,1))\n        if self.stack_y:\n            new_target = torch.cat([last_target[:,None].float(), y1[:,None].float(), lambd[:,None].float()], 1)\n        else:\n            if len(last_target.shape) == 2:\n                lambd = lambd.unsqueeze(1).float()\n            new_target = last_target.float() * lambd + y1.float() * (1-lambd)\n        return {'last_input': new_input, 'last_target': new_target}  \n    \n    def on_train_end(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()\n\nclass MixUpLoss(nn.Module):\n    \"Adapt the loss function `crit` to go with mixup.\"\n    \n    def __init__(self, crit, reduction='mean'):\n        super().__init__()\n        if hasattr(crit, 'reduction'): \n            self.crit = crit\n            self.old_red = crit.reduction\n            setattr(self.crit, 'reduction', 'none')\n        else: \n            self.crit = partial(crit, reduction='none')\n            self.old_crit = crit\n        self.reduction = reduction\n        \n    def forward(self, output, target):\n        if len(target.size()) == 2:\n            loss1, loss2 = self.crit(output,target[:,0].long()), self.crit(output,target[:,1].long())\n            d = (loss1 * target[:,2] + loss2 * (1-target[:,2])).mean()\n        else:  d = self.crit(output, target)\n        if self.reduction == 'mean': return d.mean()\n        elif self.reduction == 'sum': return d.sum()\n        return d\n    \n    def get_old(self):\n        if hasattr(self, 'old_crit'):  return self.old_crit\n        elif hasattr(self, 'old_red'): \n            setattr(self.crit, 'reduction', self.old_red)\n            return self.crit\n\ndef _mixup(learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True) -> Learner:\n    \"Add mixup https:\/\/arxiv.org\/abs\/1710.09412 to `learn`.\"\n    learn.callback_fns.append(partial(MixUpCallback, alpha=alpha, stack_x=stack_x, stack_y=stack_y))\n    return learn\nLearner.mixup = _mixup","f7fa2b09":"from fastai.core import *\nfrom fastai.basic_data import *\nfrom fastai.basic_train import *\nfrom fastai.torch_core import *\ndef _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n    \"Computes the outputs for several augmented inputs for TTA\"\n    dl = learn.dl(ds_type)\n    ds = dl.dataset\n    old = ds.tfms\n    aug_tfms = [o for o in learn.data.train_ds.tfms]\n    try:\n        pbar = master_bar(range(num_pred))\n        for i in pbar:\n            ds.tfms = aug_tfms\n            yield get_preds(learn.model, dl, pbar=pbar)[0]\n    finally: ds.tfms = old\n\nLearner.tta_only = _tta_only\n\ndef _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n    \"Applies TTA to predict on `ds_type` dataset.\"\n    preds,y = learn.get_preds(ds_type)\n    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n    avg_preds = torch.stack(all_preds).mean(0)\n    if beta is None: return preds,avg_preds,y\n    else:            \n        final_preds = preds*beta + avg_preds*(1-beta)\n        if with_loss:\n            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n            return final_preds, y, loss\n        return final_preds, y\n\nLearner.TTA = _TTA","7f5b885b":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n        \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n    \nclass Classifier(nn.Module):\n    def __init__(self, num_classes=1000): # <======== modificaition to comply fast.ai\n        super().__init__()\n        \n        self.conv = nn.Sequential(\n            ConvBlock(in_channels=3, out_channels=64),\n            ConvBlock(in_channels=64, out_channels=128),\n            ConvBlock(in_channels=128, out_channels=256),\n            ConvBlock(in_channels=256, out_channels=512),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((4, 1)) # <======== modificaition to comply fast.ai\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(512*4, 256),\n            nn.PReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        #x = torch.mean(x, dim=3)   # <======== modificaition to comply fast.ai\n        #x, _ = torch.max(x, dim=2) # <======== modificaition to comply fast.ai\n        x = self.avgpool(x)         # <======== modificaition to comply fast.ai\n        x = self.fc(x)\n        return x","4e677dd1":"labels = df_submission.columns[1:].tolist()\nlabel_size = len(labels)\n\ndef calc_P_R_AP(y_true, y_pred):\n    P = [None] * label_size\n    R = [None] * label_size\n    AP = np.zeros(label_size)\n    for i in range(label_size):\n        P[i], R[i], _ = precision_recall_curve(y_true[:,i], y_pred[:,i])\n        AP[i] = average_precision_score(y_true[:,i], y_pred[:,i])\n\n    df_ap = pd.DataFrame(data=AP, index=labels, columns=['AP'])\n    return P, R, AP, df_ap","623e2e48":"def normalize_predict(y):\n    min_pred = y.min(axis=1).reshape(-1,1)\n    max_pred = y.max(axis=1).reshape(-1,1)\n    return (y - min_pred) \/ (max_pred - min_pred)","a82c5dd0":"def borrowed_model(pretrained=False, **kwargs):\n    return Classifier(**kwargs)\n\nif TRAIN_MODE:\n    f_score = partial(fbeta, thresh=0.2)\n    learn = cnn_learner(\n        data,\n        borrowed_model, pretrained=False,\n        metrics=[lwlrap],\n        loss_func=nn.MultiLabelSoftMarginLoss()\n    )\n    if MIXMATCH_SSL > 0:\n        if CONTINUOUS_TRAIN:\n            learn.mixmatch(noisy_data, temperature=0.5, n_augment=MIXMATCH_SSL, alpha=0.4, lambda_u=50., rampup=0)\n        else:\n#             learn.mixmatch(noisy_data, temperature=0.5, n_augment=MIXMATCH_SSL, alpha=0.4, lambda_u=100., rampup=16)\n            learn.mixmatch(noisy_data, temperature=0.5, n_augment=MIXMATCH_SSL, alpha=0.4, lambda_u=10., rampup=1000)\n    else:\n        learn.mixup(alpha=0.4, stack_y=False)\n    if CONTINUOUS_TRAIN: learn.load(Path('..\/..')\/SAVED_WEIGHT)\n    learn.unfreeze()","03abffe3":"# Weights\nif TRAIN_MODE and CONTINUOUS_TRAIN:\n    df_ap = pd.read_csv(LAST_WEIGHTS\/'labels_ap.csv', index_col=0)\n#     y_pred, y_true = learn.TTA(ds_type=DatasetType.Valid, num_pred=25)\n#     y_pred = normalize_predict(y_pred.cpu().numpy())\n#     _, _, _, df_ap = calc_P_R_AP(y_true.numpy(), y_pred)\n#     fig = plt.figure(figsize=(8,11))\n#     ax = fig.add_subplot(1,1,1)\n#     df_ap.sort_values('AP').plot.barh(\n#         title='Average Precision per Labels',\n#         grid=True, legend=False, xlim=(0.,1.), ax=ax,\n#     )\n#     plt.tight_layout()\n#     plt.show()\n    \n    loss_weights = torch.FloatTensor((1\/df_ap.AP).values ** 4).cuda()\n    print(loss_weights)\n    learn.loss_func = nn.MultiLabelSoftMarginLoss(weight=loss_weights)\nelse:\n    loss_weights = None","3b5d90d7":"if TRAIN_MODE:\n    learn.lr_find()\n    learn.recorder.plot(suggestion=True)","03ebaa52":"if TRAIN_MODE:\n    gc.collect() # for GPU memory releasing when interrupted in training\n    callbacks = [\n        SaveModelCallback(learn, every='improvement', monitor='lwlrap', name='best'),\n    ]\n    if CONTINUOUS_TRAIN:\n        learn.fit_one_cycle(50, slice(1e-7,1e-4), callbacks=callbacks)\n    else:\n        learn.fit_one_cycle(300, 2e-2, callbacks=callbacks)","2d5df61e":"# if TRAIN_MODE:\n# #     gc.collect()\n#     MM_LOG.plot(logy=True)\n#     plt.show()","69b86af7":"USE_MASK_FREQ = USE_MASK_TIME = False\nMPLoader.use_augmentation = False\n\nif TRAIN_MODE:\n    y_pred, y_true = learn.get_preds(ds_type=DatasetType.Train)\n    y_pred = torch.from_numpy(normalize_predict(y_pred.cpu().numpy()))\n    print('Local LwLRAP score (Train dataset):', float(lwlrap(y_pred, y_true).float()))","e13913ef":"if TRAIN_MODE:\n    learn.save('last')\n    learn.export()","dd6d9224":"if TRAIN_MODE:\n    from sklearn.metrics import *\n    \n    y_true2 = y_true.numpy()\n    y_pred2 = y_pred.numpy()\n    \n    labels = df_submission.columns[1:].tolist()\n    label_size = len(labels)\n    \n    P = [None] * label_size\n    R = [None] * label_size\n    AP = np.zeros(label_size)\n    for i in range(label_size):\n        P[i], R[i], _ = precision_recall_curve(y_true2[:,i], y_pred2[:,i])\n        AP[i] = average_precision_score(y_true2[:,i], y_pred2[:,i])\n    df_ap = pd.DataFrame(data=AP, index=labels, columns=['AP'])\n    df_ap.to_csv('labels_ap.csv')\n    \n    P_micro, R_micro, _ = precision_recall_curve(y_true2.ravel(), y_pred2.ravel())\n    AP_micro = average_precision_score(y_true2, y_pred2, average='micro')\n\n    fig = plt.figure(figsize=(8,11))\n    ax = fig.add_subplot(1,1,1)\n    df_ap.sort_values('AP').plot.barh(\n        title='Average Precision per Labels (\u03bcAP=%.5f)' % AP_micro,\n        grid=True, legend=False, xlim=(0.,1.), ax=ax,\n    )\n    plt.tight_layout()\n    plt.show()","52aeaf6b":"if TRAIN_MODE:\n    from itertools import cycle\n    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\n    plt.figure(figsize=(12, 9))\n    f_scores = np.linspace(0.2, 0.9, num=7)\n\n    for f_score in f_scores:\n        x = np.linspace(0.01, 1)\n        y = f_score * x \/ (2 * x - f_score)\n        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\n    l, = plt.plot(R_micro, P_micro, color='gold', lw=2)\n\n    for i, color in zip(range(label_size), colors):\n        l, = plt.plot(R[i], P[i], color=color, lw=2)\n\n    fig = plt.gcf()\n    fig.subplots_adjust(bottom=0.25)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Extension of Precision-Recall curve to multi-class')\n\n    plt.show()","5caf6960":"MPLoader.reset()\nMPLoader.full_load(df_test, use_preprocess=False)","d6f1e125":"USE_MASK_FREQ = USE_MASK_TIME = False\n\ntest = ImageList.from_df(df_test, WORK, folder='')\nlearn = load_learner(WORK, test=test) if TRAIN_MODE else load_learner('.', DEPLOYED_MODEL, test=test)\npreds, _ = learn.TTA(ds_type=DatasetType.Test, num_pred=50)","71ed03cc":"df_submission[learn.data.classes] = preds\ndf_submission.to_csv('submission.csv', index=False)\ndf_submission.head()","0ec44e7e":"test_val = normalize_predict(df_submission[df_submission.columns[1:]].head(240).values).T\nplt.figure(figsize=(16,4))\nplt.title('Prediction map of Test dataset')\nplt.imshow(np.where(test_val > 1, test_val, test_val))\nplt.xlabel('Samples')\nplt.ylabel('Tags')\nplt.colorbar()\nplt.show()","3ccd6176":"from fastai.callbacks.hooks import *\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.data.train_ds[data_index]\n    y = _y.data\n    if not isinstance(y, (list, np.ndarray)): # single label -> one hot encoding\n        y = np.eye(learn.data.train_ds.c)[y]\n\n    m = learn.model.eval()\n    xb,_ = learn.data.one_item(x)\n    xb_im = Image(learn.data.denorm(xb)[0])\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a:\n            with hook_output(m[0], grad=True) as hook_g:\n                preds = m(xb)\n                preds[0,int(cat)].backward()\n        return hook_a,hook_g\n    def show_heatmap(img, hm, label):\n        img_ch, img_w, img_h = img.data.shape\n        \n        _,axs = plt.subplots(1, 2)\n        axs = axs.flat\n        axs[0].set_title(label, size=10)\n        img.show(ax=axs[0])\n        axs[1].set_title(f'CAM of {label}', size=10)\n        \n        # convert conv heatmap resolution to image\n        hm = (hm - hm.min()) \/ (hm.max() - hm.min()) * 255\n        hm = hm.numpy().astype(np.uint8)\n        hm = PIL.Image.fromarray(hm)\n        hm = hm.resize((img_w, img_h), PIL.Image.ANTIALIAS)\n        hm = np.uint8(hm) \/ 255\n        img.show(ax=axs[1])\n        axs[1].imshow(hm, alpha=0.6, cmap='magma');\n        plt.show()\n\n    for y_i in np.where(y > 0)[0]:\n        hook_a,hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n        grad_chan = grad.mean(1).mean(1)\n        mult = (acts*grad_chan[...,None,None]).mean(0)\n        show_heatmap(img=xb_im, hm=mult, label=str(learn.data.train_ds.y[data_index]))\n\nif TRAIN_MODE:\n    learn = cnn_learner(data, borrowed_model, pretrained=False, metrics=[lwlrap]).mixup(stack_y=False)\n    if TRAIN_MODE:\n        learn.load('last')\n    else:\n        learn.load(Path('..\/..')\/SAVED_WEIGHT)\n\n    for idx in range(10):\n        visualize_cnn_by_cam(learn, idx)","e0629393":"def plot_multi_sound_top_losses(interp, df, samples:int=3, figsize:Tuple[int, int]=(8, 8)):\n    losses, idxs = interp.top_losses()\n    l_dim = len(losses.size())\n    infolist, ordlosses_idxs, mismatches_idxs, mismatches, losses_mismatches, mismatchescontainer = [],[],[],[],[],[]\n    truthlabels = np.asarray(interp.y_true, dtype=int)\n    classes_ids = [k for k in enumerate(interp.data.classes)]\n    predclass = np.asarray(interp.pred_class)\n    for i,pred in enumerate(predclass):\n        where_truth = np.nonzero((truthlabels[i]>0))[0]\n        mismatch = np.all(pred!=where_truth)\n        if mismatch:\n            mismatches_idxs.append(i)\n            if l_dim > 1 : losses_mismatches.append((losses[i][pred], i))\n            else: losses_mismatches.append((losses[i], i))\n        if l_dim > 1: infotup = (i, pred, where_truth, losses[i][pred], np.round(interp.probs[i], decimals=3)[pred], mismatch)\n        else: infotup = (i, pred, where_truth, losses[i], np.round(interp.probs[i], decimals=3)[pred], mismatch)\n        infolist.append(infotup)\n    ds = interp.data.dl(interp.ds_type).dataset\n    mismatches = ds[mismatches_idxs]\n    ordlosses = sorted(losses_mismatches, key = lambda x: x[0], reverse=True)\n    for w in ordlosses: ordlosses_idxs.append(w[1])\n    mismatches_ordered_byloss = ds[ordlosses_idxs]\n    print(f'{str(len(mismatches))} misclassified samples over {str(len(interp.data.valid_ds))} samples in the validation set.')\n    for ima in range(len(mismatches_ordered_byloss)):\n        mismatchescontainer.append(mismatches_ordered_byloss[ima][0])\n    for sampleN in range(samples):\n        actualclasses = ''\n#         print(ordlosses[sampleN])\n#         print(ordlosses_idxs[sampleN])\n#         print(mismatches_ordered_byloss[sampleN][1])\n#         print(df.iloc[ordlosses_idxs[sampleN]])\n#         print(classes_ids[infolist[ordlosses_idxs[sampleN]][1]])\n        pred_samples = df[df['labels'] == classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]]\n        for clas in infolist[ordlosses_idxs[sampleN]][2]:\n            actualclasses = f'{actualclasses} -- {str(classes_ids[clas][1])}'\n        imag = mismatches_ordered_byloss[sampleN][0]\n        plt.figure(figsize=(12,6))\n        ax = plt.subplot(1,3,1)\n        imag = show_image(imag, ax=ax, figsize=figsize)\n        imag.set_title(f\"\"\"Predicted: {classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]} \\nActual: {actualclasses}\\nLoss: {infolist[ordlosses_idxs[sampleN]][3]}\\nProbability: {infolist[ordlosses_idxs[sampleN]][4]}\"\"\",\n                        loc='left')\n        if len(pred_samples) > 0:\n            ax = plt.subplot(1,3,2)\n            ax.axis('off')\n            ax.imshow(MPLoader.get(pred_samples.iloc[0].fname)[:,0:0+128], cmap='gray')\n            plt.title(f\"{classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]} #1\")\n            if len(pred_samples) > 1:\n                ax = plt.subplot(1,3,3)\n                ax.axis('off')\n                ax.imshow(MPLoader.get(pred_samples.iloc[1].fname)[:,0:0+128], cmap='gray')\n                plt.title(f\"{classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]} #2\")\n\n        plt.tight_layout()\n        plt.show()\n\nif TRAIN_MODE:\n    interp = learn.interpret(ds_type=DatasetType.Train)\n    plot_multi_sound_top_losses(interp, df_curated, 10, figsize=(4,4))","425d4fba":"## utils","703c6d09":"## Preprocessing for PCM data","02939a4d":"## Custom `open_image` for fast.ai library to load data from memory\n\n- Important note: Random cropping 1 sec, this is working like augmentation.","58656c6b":"MixMatch is **INCOMPLETE**! This is useless in FAT2019.","8b96f276":"## GradCAM","194a1052":"## Masking filters","4350dad8":"## Follow multi-label classification\n\n- Almost following fast.ai course: https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson3-planet.ipynb\n- But `pretrained=False`","5939b27b":"## Validate and Analyze model","8da59270":"# FAT2019 fast.ai implements with MixUp, MultiPreprocessed\n\nBased from https:\/\/www.kaggle.com\/vinayaks\/2d-cnn-high-score-fast-ai\n\nHistory:\n* V38: Modify version history\n* V37: Downgrade kaggle docker-container\n* V36: Rename title, and add some comments\n* V35: [PREDICTION] Final submission commit\n* V34: [LEARNING] Final training transferred V27 weights\n* V21: [LEARNING] Training with curated and noisy dataset","861ddeab":"## Test prediction and making submission file simple\n- Switch to test data.\n- Overwrite results to sample submission; simple way to prepare submission file.","33b4f504":"## Mode definisions","7a484408":"Loader for MultiPreprocessed dataset","cd970525":"## File\/folder definitions\n\n- `df_train` will handle training data.\n- `df_test` will handle test data.","58e7b0a0":"## Top Losses"}}