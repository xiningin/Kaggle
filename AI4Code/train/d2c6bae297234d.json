{"cell_type":{"7052dbe7":"code","ee01542a":"code","31895f67":"code","246b832b":"code","2a1c56d6":"code","07d4af61":"code","12f6519d":"code","4f65d202":"code","a8f09d47":"code","1a18914a":"code","cf4b78b3":"code","befbaade":"code","54e86532":"code","1d89c6fd":"code","87e9eee8":"code","ed4d94cd":"code","86c37884":"code","112da6a4":"code","c195129b":"code","b2c91761":"code","8c77cde7":"code","c882babe":"code","a8f0f552":"code","074eaee0":"code","5c62ca8e":"code","014ad6c9":"code","6c7ad2b3":"code","deb69a2c":"code","df5d79f4":"code","c2b4d8a8":"code","e928a39b":"code","386e1b26":"code","38738ab5":"markdown","9e9cd2a6":"markdown","d6ed5d07":"markdown","6ef44dd4":"markdown","72000f3b":"markdown","0c5233c3":"markdown"},"source":{"7052dbe7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee01542a":"train = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntrain.head()","31895f67":"test= pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')\ntest.head()","246b832b":"train.shape","2a1c56d6":"train.describe()","07d4af61":"train.isna().sum()","12f6519d":"train['label'].nunique()","4f65d202":"sns.countplot(train['label'])","a8f09d47":"train_images= train.drop('label', axis=1)\ntrain_label= train['label']\n\ntest_images= test.drop('label', axis=1)\ntest_label= test['label']","1a18914a":"train_images= train_images\/255\ntest_images= test_images\/255\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val= train_test_split(train_images, train_label, test_size=0.2 )","cf4b78b3":"X_train= np.array(X_train)\nX_train= X_train.reshape(len(X_train), 28,28,1)\nX_train.shape","befbaade":"X_val= np.array(X_val)\nX_val= X_val.reshape(len(X_val), 28,28,1)\nX_val.shape","54e86532":"test_images= np.array(test_images)\ntest_images= test_images.reshape(len(test_images), 28,28,1)\ntest_images.shape","1d89c6fd":"y_train= np.array(y_train)\ny_val= np.array(y_val)\ntest_label= np.array(test_label)","87e9eee8":"y_train.shape","ed4d94cd":"y_val.shape","86c37884":"test_label.shape","112da6a4":"plt.imshow(X_train[1], cmap='Greys')","c195129b":"y_train[1]","b2c91761":"import keras\nfrom keras.layers import Conv2D, Dense, BatchNormalization, MaxPool2D, Flatten\nfrom keras.models import Sequential","8c77cde7":"model= Sequential()\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()          ","c882babe":"from keras.callbacks import EarlyStopping\nearly_stopping= EarlyStopping(patience=2, monitor= 'val_loss')","a8f0f552":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory= model.fit(X_train, y_train, batch_size=164, epochs=10, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])","074eaee0":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])","5c62ca8e":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","014ad6c9":"model.evaluate(test_images, test_label, batch_size= 164)","6c7ad2b3":"pred= model.predict(test_images)","deb69a2c":"predictions= np.argmax(pred, axis=1)","df5d79f4":"predictions","c2b4d8a8":"plt.imshow(test_images[67], cmap='Greys')","e928a39b":"test_label[67]","386e1b26":"predictions[67]","38738ab5":"# Little preprocessing ","9e9cd2a6":"# Like and Upvote if you liked my notebook :)","d6ed5d07":"# Model","6ef44dd4":"**Looks like we have an equal distribution of data which is very good**","72000f3b":"# Load Data","0c5233c3":"**The accuracy and losses of validation set look good. Let's try using our test set**"}}