{"cell_type":{"b4e01c9e":"code","24529892":"code","e8a80583":"code","4d0772ce":"code","1229edd4":"code","9aff1118":"code","2c378a1d":"code","6c318cc0":"code","72333c18":"code","bb1a19c2":"code","0db488e7":"code","08d6c879":"code","bf0e576d":"code","3ce9460f":"code","a1980dcc":"code","07e663f1":"code","99f2b90a":"code","404267b1":"code","d54a55a3":"code","a4fe8a1b":"code","0407fd2c":"code","2c3b5506":"code","60e4e08a":"code","6bbbbcc6":"code","2325cba2":"code","46d11d11":"code","2f65c766":"code","c24bebb4":"code","ebc651f0":"code","72f069ca":"code","1ac36a6b":"code","3ac1cd82":"code","a833c8d4":"code","d695423c":"code","e7b7e113":"code","f6f51eb0":"code","8f151c84":"code","9cebb508":"code","bfca5d6f":"code","03d25b07":"code","46faf632":"code","7428314e":"code","6f10f344":"markdown","7d3802e3":"markdown","a68ea451":"markdown","7239e9ff":"markdown","93379f1c":"markdown","ff45ada3":"markdown","5a22956c":"markdown","1e202b6a":"markdown","db3d6c68":"markdown","048d91c3":"markdown","40e2796a":"markdown","e2552e04":"markdown","5e9ab497":"markdown","770940d5":"markdown","4a57cff9":"markdown","3e1115a7":"markdown","31272cf1":"markdown","764ba681":"markdown","76af3eb4":"markdown","0209916f":"markdown","253ccf7c":"markdown","f73862e9":"markdown","80c014ed":"markdown","752a35c8":"markdown","0607d530":"markdown","8041ff42":"markdown","01af06ae":"markdown","e6613c86":"markdown","ad6f20a3":"markdown","88d733d9":"markdown","43e5dc16":"markdown","c325696c":"markdown","a7f41ac2":"markdown","8c931d45":"markdown"},"source":{"b4e01c9e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport string\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\n\nimport warnings  \nwarnings.filterwarnings('ignore')","24529892":"# reading the dataset\ndataset = pd.read_csv('..\/input\/malbends\/malicious_benign.csv')\ndataset.head(5000)","e8a80583":"# understanding the dataset\ndataset.describe(include='all')","4d0772ce":"# Checking the null values\n\nprint(dataset.isnull().sum())","1229edd4":"#dropping not required records\ndataset.dropna(inplace=True)\n\n# dropping the not needed columns\ndataset.drop(['URL','CONTENT_LENGTH','CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE', ], axis =1, inplace=True)\n\nprint(dataset.isnull().sum())","9aff1118":"# Verifying correlation among features\n\ncorr = dataset.corr()\ncorr.style.background_gradient(cmap='coolwarm')","2c378a1d":"# removing highly correlated features\ndataset.drop(['TCP_CONVERSATION_EXCHANGE','URL_LENGTH','APP_BYTES','SOURCE_APP_PACKETS','REMOTE_APP_PACKETS','SOURCE_APP_BYTES','REMOTE_APP_BYTES'], axis = 1, inplace=True)\n\n# Re-verification of the rest of features\ncorr = dataset.corr()\ncorr.style.background_gradient(cmap='coolwarm')","6c318cc0":"dataset.columns","72333c18":"import seaborn as sns\nsns.distplot(dataset.loc[dataset['Type'] == 1]['NUMBER_SPECIAL_CHARACTERS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['NUMBER_SPECIAL_CHARACTERS'], bins = 50, color='blue')","bb1a19c2":"# for malicious websites\n\nprint(dataset.loc[dataset['Type'] == 1]['DIST_REMOTE_TCP_PORT'].value_counts())\nsns.distplot(dataset.loc[dataset['Type'] == 1]['DIST_REMOTE_TCP_PORT'], bins = 50, color='red')","0db488e7":"# for benign website\n\nprint(dataset.loc[dataset['Type'] == 0]['DIST_REMOTE_TCP_PORT'].value_counts())\nsns.distplot(dataset.loc[dataset['Type'] == 0]['DIST_REMOTE_TCP_PORT'], bins = 50, color='blue')","08d6c879":"sns.distplot(dataset.loc[dataset['Type'] == 1]['REMOTE_IPS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['REMOTE_IPS'], bins = 50, color='blue')","bf0e576d":"sns.boxplot(dataset['APP_PACKETS'])","3ce9460f":"# correcting the outliers\ndataset = dataset[((dataset.APP_PACKETS - dataset.APP_PACKETS.mean()) \/ dataset.APP_PACKETS.std()).abs() < 3]\n\n# reverifying the outliers, they are removed.\nsns.boxplot(dataset['APP_PACKETS'])","a1980dcc":"# histogram for APP_PACKETS\nsns.distplot(dataset.loc[dataset['Type'] == 1]['APP_PACKETS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['APP_PACKETS'], bins = 50, color='blue')","07e663f1":"\nprint(dataset['DNS_QUERY_TIMES'].value_counts())\n\nsns.distplot(dataset.loc[dataset['Type'] == 1]['DNS_QUERY_TIMES'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['DNS_QUERY_TIMES'], bins = 50, color='blue')","99f2b90a":"dataset.columns\ndataset.info()","404267b1":"# Creating custom transformer using spaCy\nclass transformer(TransformerMixin):\n    def transform(self,X,**transform_params):\n        \n        # Cleaning the text\n        return [clean(i) for i in X]\n    # Fitting the transformer\n    \n    def fit(self,X,y=None,**fit_params):\n        return self\n    \n    # Predicting the transformer\n    def get_params(self,deep=True):\n        return{}\n    \n# Basic clean function\ndef clean(i):\n    return i\n    \n    # Removing the spaces and converting all the text into lowercase\n#     return i.strip().lower()","d54a55a3":"# Creating our list of punctuations\npunc = string.punctuation\n\n# Creating our list of stopwords\nnlp = spacy.load('en_core_web_sm')\nstopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n# Loading parser\nparse = English()\n\n# Creating the tokenizer function\ndef tokenize(token):\n    # Creating the token object where all the tokenization functions are applied starting \n    # with parsing\n    token_obj = parse(token)\n    \n    # Lemmatizing each token and converting it into lowercase\n#     token_obj = [i.lemma_.lower().strip() if i.lemma_!='-PRON-' else i.lower_ for i in token_obj]\n    \n    # Removing stop words and punctuations\n    token_obj = [i for i in token_obj if i not in stopwords and i not in punc]\n    \n    # Returning the token\n    return token_obj","a4fe8a1b":"# Creating TF-IDF Vectorizer\ntfidf_vect= TfidfVectorizer(tokenizer=tokenize,use_idf=True, smooth_idf=True, sublinear_tf=False)","0407fd2c":"# Preparing data\n# Scale data then split\n\nfrom sklearn import preprocessing\n# Separate into train and test as well as features and predictor\nX = dataset.drop('Type',axis=1) #Predictors\ny = dataset['Type']\nX = preprocessing.scale(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11)","2c3b5506":"# Importing Random Forest Classifier and fitting the model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(class_weight='balanced')\n\n# Importing RandomizedSearchCV and assigning the parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'criterion':['entropy','gini'],'max_depth':range(1,15,2)}\n\n# Fitting the RandomizedSearchCV model\nrfc = RandomizedSearchCV(rf,params)","60e4e08a":"# Creating the pipeline\npipe_rf = Pipeline([('clean',transformer()),\n#                  ('vectorizer',tfidf_vect), # we have all the attributes int, so we dont need this\n                 ('model',rfc)])\n\n# Model generation\npipe_rf.fit(X_train,y_train)","6bbbbcc6":"# Importing the metrics\nfrom sklearn import metrics\n\n# Predicting with test data\npred = pipe_rf.predict(X_test)","2325cba2":"# Model accuracy,precision and recall\nprint('Accuracy',metrics.accuracy_score(y_test,pred))\nprint('Precision',metrics.precision_score(y_test,pred,average=None))\nprint('Recall',metrics.recall_score(y_test,pred,average=None))\nprint('F1-Score',metrics.f1_score(y_test,pred,average=None))","46d11d11":"# Confusion matrix\nconf_matrix_rf = metrics.confusion_matrix(y_test,pred)\nprint(conf_matrix_rf)","2f65c766":"# Plotting the confusion matrix\ncfm= conf_matrix_rf\nlbl1=[\"Predicted Negative\", \"Predicted Positive\"]\nlbl2=[\"Actual Negative\", \"Actual Poistive\"]\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nax.set_ylim([0,2])\nplt.show()","c24bebb4":"# Importing RandomizedSearchCV and assigning the parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'criterion':['entropy','gini'],'max_depth':range(1,15,2)}\n\n# Fitting the RandomizedSearchCV model\nrfc = RandomizedSearchCV(rf,params)","ebc651f0":"# Importing Support Vector Classifier algorithm\nfrom sklearn.svm import SVC\nsvc = SVC(class_weight='balanced')\n\nparams1 = {'kernel':['linear','rbf','poly','sigmoid'],'C': [0.01, 0.1, 1,10],'gamma': [0.01,0.1,1,10]}\n\n# Fitting the RandomizedSearchCV model\nsvcc = RandomizedSearchCV(svc,params1)","72f069ca":"# Creating the pipeline\npipe = Pipeline([('clean',transformer()),\n#                  ('vectorizer',tfidf_vect), # all the parameters are int, we do not need this.\n                 ('model',svcc)])\n\n# Model generation\npipe.fit(X_train,y_train)","1ac36a6b":"# Predicting with test data\npred = pipe.predict(X_test)\n\n# Model accuracy,precision and recall\nprint('Accuracy',metrics.accuracy_score(y_test,pred))\nprint('Precision',metrics.precision_score(y_test,pred,average=None))\nprint('Recall',metrics.recall_score(y_test,pred,average=None))\nprint('F1-Score',metrics.f1_score(y_test,pred,average=None))","3ac1cd82":"# Confusion matrix\nconf_matrix_rf = metrics.confusion_matrix(y_test,pred)\nprint(conf_matrix_rf)","a833c8d4":"# Plotting the confusion matrix\ncfm= conf_matrix_rf\nlbl1=[\"Predicted Negative\", \"Predicted Positive\"]\nlbl2=[\"Actual Negative\", \"Actual Poistive\"]\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nax.set_ylim([0,2])\nplt.show()","d695423c":"# Method for evaluating results\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\ndef calculateScores(y_test, predictions):\n    accuracy = 100*accuracy_score(y_test, predictions)\n    precision = 100*precision_score(y_test, predictions)\n    recall = 100*recall_score(y_test, predictions)\n    f1 = 100*f1_score(y_test, predictions)\n    print (' Accuracy  %.2f%%' % accuracy)\n    print (' Precision %.2f%%'% precision)\n    print (' Recall    %.2f%%'% recall)\n    print (' F1        %.2f%%'% f1)\n    print('Confusion Matrix')\n    print(confusion_matrix(y_test,predictions))\n    return {'Accuracy':accuracy, 'F1': f1}","e7b7e113":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression(solver='lbfgs')\nmodel = reg.fit(X_train, y_train)\npredictions = model.predict(X_test)\nscores = calculateScores(y_test, predictions)","f6f51eb0":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(random_state=1)\nmlp.fit(X_train, y_train)\npredictions = mlp.predict(X_test)\nscores = calculateScores(y_test, predictions)","8f151c84":"def predict( X_train, y_train, **kwargs):\n    mlp = MLPClassifier(**kwargs, random_state=1)\n    mlp.fit(X_train, y_train)\n    return mlp.predict(X_test)","9cebb508":"def calculateScoresNoOutput(y_test, predictions):\n    accuracy = 100*accuracy_score(y_test, predictions)\n    precision = 100*precision_score(y_test, predictions)\n    recall = 100*recall_score(y_test, predictions)\n    f1 = 100*f1_score(y_test, predictions)\n    return {'Accuracy':accuracy, 'F1': f1}","bfca5d6f":"# Let's try the different solvers\nsolvers = ['lbfgs', 'sgd', 'adam']\nresults = []\nfor solver in solvers:\n    result_dict = calculateScoresNoOutput(y_test, predict(X_train, y_train, solver=solver))\n    result_dict['Solver'] = solver\n    results.append(result_dict)\ndf = pd.DataFrame(results, columns = ['Solver','Accuracy', 'F1'])\ndf","03d25b07":"# Generalise attempting different values\ndef try_different_values(values, column_name, X_train, y_train, **kwargs):\n    results = []\n    for value in values:\n        kwargs[column_name] = value\n        result_dict = calculateScoresNoOutput(y_test, predict(X_train, y_train, **kwargs))\n        result_dict[column_name] = value\n        results.append(result_dict)\n    df = pd.DataFrame(results, columns = [column_name,'Accuracy', 'F1'])\n    return df","46faf632":"activations = ['identity', 'logistic', 'tanh', 'relu']\ntry_different_values(activations, 'activation', X_train, y_train, solver='lbfgs')","7428314e":"print(\"Notebook Completd!\")","6f10f344":"### DIST REMOTE TCP PORT","7d3802e3":"# Conclusion\n* The evaluation using NLP pipeline of RandomForestClassifier gives best result of 0.95 accuracy followed by ML Logistic Regression accuracy of 0.92.","a68ea451":"### APP PACKETS","7239e9ff":"# Malicious and benign websites\n\nThis dataset contains information about malicious and benign websites, based on application\nlayer and network characteristics. The data were obtained by using different verified sources of\nbenign and malicious URL's. The dataset attributes are as following:\n\n* URL: it is the anonymous identification of the URL analyzed in the study\n* URL_LENGTH: it is the number of characters in the URL\n* NUMBER_SPECIAL_CHARACTERS: it is number of special characters identified in the URL, such as, \u201c\/\u201d, \u201c%\u201d, \u201c#\u201d, \u201c&\u201d, \u201c. \u201c, \u201c=\u201d\n* CHARSET: it is a categorical value and its meaning is the character encoding standard (also called character set).\n* SERVER: it is a categorical value and its meaning is the operative system of the server got from the packet response.\n* CONTENT_LENGTH: it represents the content size of the HTTP header.\n* WHOIS_COUNTRY: it is a categorical variable, its values are the countries we got from the server response (specifically, our script used the API of Whois).\n* WHOIS_STATEPRO: it is a categorical variable, its values are the states we got from the server response (specifically, our script used the API of Whois).\n* WHOIS_REGDATE: Whois provides the server registration date, so, this variable has date values with format DD\/MM\/YYY HH:MM\n* WHOIS_UPDATED_DATE: Through the Whois we got the last update date from the server analyzed\n* TCP_CONVERSATION_EXCHANGE: This variable is the number of TCP packets exchanged between the server and our honeypot client\n* DIST_REMOTE_TCP_PORT: it is the number of the ports detected and different to TCP\n* REMOTE_IPS: this variable has the total number of IPs connected to the honeypot\n* APP_BYTES: this is the number of bytes transfered\n* SOURCE_APP_PACKETS: packets sent from the honeypot to the server\n* REMOTE_APP_PACKETS: packets received from the server\n* APP_PACKETS: this is the total number of IP packets generated during the communication between the honeypot and the server\n* DNS_QUERY_TIMES: this is the number of DNS packets generated during the communication between the honeypot and the server\n* TYPE: this is a categorical variable, its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites\n\nObjective: Classify website into **Malicious and Benign**\n","93379f1c":"Observations:\n* The URL column is a unique identifier. We may not need it for analysis, so we may drop it.","ff45ada3":"Observation:\n* After removal of outliers, it is better. Lets see it in histogram","5a22956c":"### REMOTE IPS","1e202b6a":"# ML Model Evaluation","db3d6c68":"Observation:\n\n* In NLP, RandomForestClassifier gives best result of 0.95 accuracy.","048d91c3":"# Exploratory Data Analysis","40e2796a":"Observation:\n\n* In ML, Logistic regression gives best result of 0.92 accuracy.","e2552e04":"Observation:\n\n* There are only 3 colums SERVER, CONTENT_LENGTH and DNS_QUERY_TIMES having null values. \n* We can easily drop these recoreds \/ place a dummy values having null values in SERVER and DNS_QUERY_TIMES columns. \n* The column CONTENT_LENGTH is important but having quite a numbers of and adding dummy data might distort the data somewhat. Given that there are plenty of other features, we may drop the column. There are many other colums which may be dropped as they are not important.\n* Looking at the data there are several categorical features (WHOIS_COUNTRY, SERVER etc.). For simplicity we'll ignore these features in our network.\n* We will then see correlation among rest of the features.","5e9ab497":"### LogisticRegression","770940d5":"Observations:\n* There are some features which are highly correlated which may be safely dropped e.g. TCP_CONVERSATION_EXCHANGE','URL_LENGTH','APP_BYTES','SOURCE_APP_PACKETS','REMOTE_APP_PACKETS','SOURCE_APP_BYTES','REMOTE_APP_BYTES' etc.\n* We can also safely remove URL_LENGTH column as it seems not useful","4a57cff9":"Observations:\n* It looks like malicious websites have more special characters. The red bars are the malicious websites, there are some definite odd spikes.","3e1115a7":"### DNS_QUERY_TIMES","31272cf1":"Observations:\n* There are some outliers which may be corrected.","764ba681":"Straight away that's a small improvement on the simple model, it is better at classifying the malicious websites, as seen in the confusion matrix and the F1 score.\n\nLet's have a play with the parameters to see if this can be improved.","76af3eb4":"# Visualization","0209916f":"Observation:\n* Let's see if we can improve this with a neural network. We'll use the standard SciKit learn class, starting off by using all the default values, before attempting to optimise it by adjusting its parameters.","253ccf7c":"# NLP Evaluation completed","f73862e9":"Looks like the lbfgs solver is the best, with highest accuray and F1 scores. We'll use that from now on. ","80c014ed":"### Creating Tokenizer","752a35c8":"Observations:\n* REMOTE_IPS means that 'this variable has the total number of IPs connected to the honeypot'. It seems like malicious websites have a slightly lower grouping of remote IPs connected than benign.","0607d530":"### NUMBER SPECIAL CHARACTERS","8041ff42":"### MLPClassifier","01af06ae":"### Creating NLP Pipeline for SupportVectorClassifier","e6613c86":"### Creating NLP Pipeline for RandomForestClassifier","ad6f20a3":"Observations:\n* It looks like malicious websites generally don't have a port associated. We may verify it in further analysis and may remove if not affects the modelling.","88d733d9":"Observation:\n* This chart does not infer anything special.","43e5dc16":"# NLP Model Evaluation","c325696c":"# Importing Libraries","a7f41ac2":"### Creating TF-IDF Vectorizer","8c931d45":"### Creating custom Transformer"}}