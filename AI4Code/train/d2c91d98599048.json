{"cell_type":{"d0f50ce8":"code","1af08047":"code","9c8c0c24":"code","76afcb94":"code","bbff30e4":"code","b10f0262":"code","17740574":"code","d6a2bbbf":"code","dd33db47":"code","f6b9e9cb":"code","b379b336":"code","51bfe1fa":"code","bb52b702":"code","49717cf4":"code","bd5b331a":"code","ad464ebe":"code","246485b5":"code","34b53299":"code","9ea05c85":"code","6bbdf043":"code","dc4224b5":"markdown","683423a7":"markdown","dbc6e6df":"markdown","3bcd71c6":"markdown","e231013a":"markdown","e7eb6d51":"markdown","f243e10f":"markdown","85735543":"markdown","71b16149":"markdown","036f67d2":"markdown","66408d9f":"markdown","999b41c8":"markdown","c3041f68":"markdown","ee915e32":"markdown","03dfe75c":"markdown","a9dfb3e7":"markdown","5d000afd":"markdown","5347736d":"markdown","79cde3b8":"markdown"},"source":{"d0f50ce8":"import tensorflow as tf\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1af08047":"zoo = pd.read_csv(\"..\/input\/zoo.csv\")\nzoo.head()","9c8c0c24":"print(\"This ZOO dataset is consised of\",len(zoo),\"rows.\")","76afcb94":"sns.countplot(zoo['class_type'],label=\"Count\")","bbff30e4":"corr = zoo.iloc[:,1:-1].corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 12},\n            cmap = colormap, linewidths=0.1, linecolor='white')\nplt.title('Correlation of ZOO Features', y=1.05, size=15) ","b10f0262":"x_data = zoo.iloc[:,:-1]\nx_data.head()","17740574":"y_data = zoo.iloc[:,-1:]\ny_data.head()","d6a2bbbf":"print(\"Feature Data :\", x_data.shape)\nprint(\"Label Data :\", y_data.shape)","dd33db47":"train_x, test_x, train_y, test_y = train_test_split(x_data, y_data, test_size=0.3, random_state=42, stratify=y_data)\nprint(\"Training Data has\",train_x.shape)\nprint(\"Testing Data has\",test_x.shape)","f6b9e9cb":"train_name = train_x['animal_name']\ntest_name = test_x['animal_name']\n\ntrain_x = train_x.iloc[:,1:]\ntest_x = test_x.iloc[:,1:]\n\nprint(\"Training Data has\",train_x.shape)\nprint(\"Testing Data has\",test_x.shape)","b379b336":"X = tf.placeholder(tf.float32, [None,16]) \nY = tf.placeholder(tf.int32, [None, 1])","51bfe1fa":"Y_one_hot = tf.one_hot(Y, 7)  # one hot encoding\nY_one_hot = tf.reshape(Y_one_hot, [-1, 7])","bb52b702":"W = tf.Variable(tf.random_normal([16, 7],seed=0), name='weight')\nb = tf.Variable(tf.random_normal([7],seed=0), name='bias')","49717cf4":"logits = tf.matmul(X, W) + b\n# hypothesis = tf.nn.softmax(logits)","bd5b331a":"hypothesis = tf.nn.softmax(logits)\n\ncost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y_one_hot)\ncost = tf.reduce_mean(cost_i)\n# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis)))","ad464ebe":"train  = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost)\n# train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost) ","246485b5":"prediction = tf.argmax(hypothesis, 1)\ncorrect_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","34b53299":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(5001):\n        sess.run(train, feed_dict={X: train_x, Y: train_y})\n        if step % 1000 == 0:\n            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n    print(\"Model Prediction =\", train_acc)\n    print(\"Test Prediction =\", test_acc)","9ea05c85":"sub = pd.DataFrame()\nsub['Name'] = test_name\nsub['Predict_Type'] = test_predict\nsub['Origin_Type'] = test_y\nsub['Correct'] = test_correct\nsub","6bbdf043":"sub[['Name','Predict_Type']].to_csv('submission.csv',index=False)","dc4224b5":"## 6-2) One-Hot Encoding variable\nAs I explained above, for one-hot encoding, we have to change **y_data form** in to **y -one hot encoding** form. \n\nYou can simply use **tensorflow one_hot** function.\n* y_data(type results) has 7 factors, so we will make place with 7 columns. **-1** means **no size limits** of rows.","683423a7":"---\n\n# 6. Make ANN-SLP Model\n## 6-1) Make \"Placeholder\" for dinamic variable allocation\nPlaceholder is one of the function in tensorflow.\nIt is a space to put and change values while the program is running.\n* for X, a place must have 16 columns, since zoo data has 16 features.\n* for Y, a place must have 1 columns, since the results has 1 outcome.\n* If you see the row \"None\", it means it has no size limits. (You can write -1 instead of \"None\")","dbc6e6df":"---\n\n# 10. Conclusion\nYou can make your own ANN model with modifying **learning_rate, step range**.\n\n\nPlanning : ANN-SLP with PCA, ANN-MLP\n\nWant to see my another kernels?\n\n\n* **Classification(R ver.) [ Breast Cancer or Not (with 15 ML)](https:\/\/www.kaggle.com\/mirichoi0218\/classification-breast-cancer-or-not-with-15-ml).**\n* **Linear Regression(R ver.) [ How much will the premium be?](https:\/\/www.kaggle.com\/mirichoi0218\/regression-how-much-will-the-premium-be)**\n\nUpvotes and Comments are fully Welcomed :-)\n\nThank you for watching!","3bcd71c6":"## 5-3) Drop animal_name column\n* Save the **animal_name** column for later combination(results).\n* Drop the **animal_name** column in train, test datasets, because it's unnecessary for model learning predictions.","e231013a":"## 6-3) Make Weight, Bias value with randomly\n* W(weight) : why **[16,7]**?  16 for 16 features, 7 for 7 types of Outcome(results).\n* b(bias) : why **[7]**?  outcome has 7 layers(types).","e7eb6d51":"## 6-4) Make Output Results\n * **Output = Weight * Input + Bias**\n * tf.matmul() : for array multiply\n * tf.nn.softmax_cross_entropy_with_logits(): for gradient_descent with softmax results(hypothesis).","f243e10f":"## 4-2) Summary the animal_type","85735543":"## 6-5) Cross Entropy\nBefore this, you have to know **How Linear Regression Works**\n* Linear Regression: Draw a random line to find the **mean square root error** and find the slope and intercept to minimize this value (reduce the error to the minimum)\n* Since Logits is also linear equation, you have to find minimum cost!\n\n![Imgur](https:\/\/machinelearningblogcom.files.wordpress.com\/2018\/01\/bildschirmfoto-2018-01-24-um-14-32-02.png?w=1400)\n\nFor example, logits(we get above) is **red line**, and the real dataset is **blue dot**. \n1. For finding cost, you have to substract all blue dot value with red line. \n2. Next, You add all distance you find and get average. \n3. For good prediction, this average distance of red line & blue dot must be minimum value. ","71b16149":"## 6-6) Gradient Descent Optimizer\n\n![Imgur](http:\/\/sebastianraschka.com\/images\/blog\/2015\/singlelayer_neural_networks_files\/perceptron_gradient_descent_1.png)\n\n* GradientDescentOptimizer: It makes the best result with the least error\n* There are lots of optimizer methods provided in tensorflow. (GradientDescent, Adam, RMSProp, etc.)\n* learning rate : It indicates the degree of descending size.\n\n![Imgur](https:\/\/pbs.twimg.com\/media\/DK26ibcXUAEOwel.jpg)\n","036f67d2":"## 4-3) Correlation Plot of 16 features","66408d9f":"## 6-7) Compare : original vs. prediction\n* tf.argmax() : since hypothesis is one-hot encoding(with 7 layers) you have to find max softmax results. \n(It returns the index value of the array with the largest probability)","999b41c8":"---\n\n# 9. Submission","c3041f68":"---\n\n# 8. Show Results","ee915e32":"---\n\n# 5. Prepare Data for machine learning\n## 5-1) Seperate Feature data(17) \/ Label data(1)\nEliminate **animal_name** columns for predictions.\nSeperate by **x_data, y_data**\n* x_data : columns(features to predict class_type) for training. (eliminate class_type)\n* y_data : columns for comparing with predictions results. (need original class_type)","03dfe75c":"## 5-2) Divide \"ZOO data\" into Train(70%) \/ Test data(30%)\nDivide the data into two(train\/test) to see the predictive power of the model.\n* test_size : proportion of test data (0.3 means split into 30% test data, and the rest(70%) is train data)\n* random_state : you can handle the fix dataset everytime, you can write any number you like :-)\n* stratify : split data randomly and y variable(7 class_type) proportionately","a9dfb3e7":"---\n\n# 7. Activate Model\n","5d000afd":"---\n\n# 4. Explore Dataset\n## 4-1) Import dataset","5347736d":"---\n\n# 2. What is ANN & SLP?\n### 2-1) ANN(Artificial Neural Network)\n* The Artificial Neural Network consists of an input layer, a hidden layer, and an output layer.\n\n![Imgur](https:\/\/elogeel.files.wordpress.com\/2010\/05\/050510_1627_multilayerp1.png)\n\n\n\n### 2-2) SLP(Single Layer Perceptron)\n\n* If ANN model has no hidden layer, it is called single layer perceptron. \n* In contrast, MLP(Multiple Layer Perceptron) model is ANN which has multiple hidden layers (more than 1) \n\n\n![Imgur](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/07\/SLP.png)\n\n\n\n### 2-3) Basic equation of ANN-SLP\n* **Output = Weight * Input + Bias**\n* For this equation, we already have output, input layers. But don't have weight value and bias value.\n* Weight : a value that can give different weights depending on features and output \n    => [len(features), len(output)]\n* bias : a value that can give different weights depending on features\n    => [len(output)]\n\n\n\n\n### 2-4) One-Hot Encoding\n\n* This **ZOO DATASET** result is consisted of 7 types.\n\n*(1: mammal, 2: bird, 3: reptile, 4: fish, 5: amphibian, 6: insect \/ arachnid, 7: invertebrate)*\n\n* If the prediction results are more than 2 factors, It is more comforable using one-hot encoding.\n* For example, in official test, we write our answer in OMR card, cuz is much more easier for computer to read answers.\n\n![Imgur](https:\/\/chrisalbon.com\/images\/machine_learning_flashcards\/One-Hot_Encoding_print.png)\n\n\n---\n\n# 3. Import Libraries","79cde3b8":"# 1. Intro\nHi, Kagglers!\nI'm back with Python version.\n\nFor ANN Beginners like me, I hope this kernel helps you to understand Neural Network Algorithms, especially SLP(Single-Layer Perceptron).\nAt the end of this kernel, I hope you can make your own ANN model!\n\n\n* **ANN Model with Binary classification(python ver.)[ Neural Network Model for Classification ](https:\/\/www.kaggle.com\/mirichoi0218\/ann-making-model-for-binary-classification).**\n* **Binary Classification(R ver.) [ Breast Cancer or Not (with 15 ML) ](https:\/\/www.kaggle.com\/mirichoi0218\/classification-breast-cancer-or-not-with-15-ml).**\n"}}