{"cell_type":{"85f48ee4":"code","e18763df":"code","2a0df06d":"code","1b0ab883":"code","c8948edb":"code","b2cd3f41":"code","4ea1ebae":"code","6493accc":"code","d5954688":"code","4bb7e131":"code","410eda4f":"code","05a353b9":"code","72900dc6":"code","55bc44f2":"code","a0f13e5b":"code","9f6da6a5":"code","dd38915b":"code","64fcf198":"code","11a4e9ec":"code","bde5783d":"code","07ab4df1":"code","f77422c1":"code","57a62eb8":"markdown","ccaae4d3":"markdown","b8f9ff85":"markdown"},"source":{"85f48ee4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom torch import nn\nimport torch\nfrom sklearn.preprocessing import RobustScaler\n#from torch import Dataset\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom math import sqrt\nimport os\n\n# This code is based on https:\/\/medium.com\/pytorch\/implementing-an-autoencoder-in-pytorch-19baa22647d1\nclass AE(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.encoder_hidden_layer = nn.Linear(\n            in_features=kwargs[\"input_shape\"], out_features=128\n        )\n        self.encoder_output_layer = nn.Linear(\n            in_features=128, out_features=128\n        )\n        self.decoder_hidden_layer = nn.Linear(\n            in_features=128, out_features=128\n        )\n        self.decoder_output_layer = nn.Linear(\n            in_features=128, out_features=kwargs[\"input_shape\"]\n        )\n\n    def forward(self, features):\n        activation = self.encoder_hidden_layer(features)\n        activation = torch.relu(activation)\n        code = self.encoder_output_layer(activation)\n        code = torch.relu(code)\n        activation = self.decoder_hidden_layer(code)\n        activation = torch.relu(activation)\n        activation = self.decoder_output_layer(activation)\n        reconstructed = torch.relu(activation)\n        return reconstructed\n    \n    def encoder_forward(self, features):\n        activation = self.encoder_hidden_layer(features)\n        activation = torch.relu(activation)\n        code = self.encoder_output_layer(activation)\n        code = torch.relu(code)\n        return code\n    \ndef train_loop(model, batch_size, df, relevant_columns, epochs=10):\n    for epoch in range(epochs):\n        loss = 0\n        i = 0\n        criterion = torch.nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n        while i<len(df)\/batch_size:\n            relevant_rows = df.iloc[i:i+batch_size][relevant_columns].values\n            the_tensor = torch.tensor(relevant_rows).float()\n\n            # reset the gradients back to zero\n            # PyTorch accumulates gradients on subsequent backward passes\n            optimizer.zero_grad()\n\n            # compute reconstructions\n            outputs = model(the_tensor)\n\n            # compute training reconstruction loss\n            train_loss = criterion(outputs, the_tensor)\n            \n            # compute accumulated gradients\n            train_loss.backward()\n\n            # perform parameter update based on current gradients\n            optimizer.step()\n\n            # add the mini-batch training loss to epoch loss\n            loss += train_loss.item()\n            i+=batch_size\n            print(loss)\n        # compute the epoch training loss\n        print(loss)\n        loss = loss \/ len(df) \n    \n    # display the epoch training loss\n    print(\"epoch : {}\/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))","e18763df":"relevant_cols = ['TotalPop', 'Men', 'Women', 'Hispanic',\n       'White', 'Black', 'Native', 'Asian', 'Pacific',\n       'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n       'SelfEmployed', 'FamilyWork', 'Unemployment']","2a0df06d":"train_df = pd.read_csv(\"..\/input\/us-census-demographic-data\/acs2015_census_tract_data.csv\")\ntest_df = pd.read_csv(\"..\/input\/us-census-demographic-data\/acs2017_county_data.csv\")","1b0ab883":"model = AE(**{\"input_shape\":33})","c8948edb":"# Train the model. Since this is an autoencoder it doesn't matter if we use test set for training.\n# We will evaluate the utility of the autoencoder based on embeddings produced. Clustering analysis will be below.\nr_train = RobustScaler()\ndf_train = pd.DataFrame(r_train.fit_transform(train_df[relevant_cols]))\ndf_train = df_train.dropna()\ndf_train.columns = relevant_cols\nr_test = RobustScaler()\ndf_test = pd.DataFrame(r_test.fit_transform(test_df[relevant_cols]))\ndf_test.columns = relevant_cols\ntrain_loop(model, 10, df_train, relevant_columns=['TotalPop', 'Men', 'Women', 'Hispanic', \n                                                    'White', 'Black', 'Native', 'Asian', 'Pacific',\n                                                    'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n                                                    'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n                                                    'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n                                                    'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n                                                    'SelfEmployed', 'FamilyWork', 'Unemployment'])\n\n","b2cd3f41":"train_loop(model, 10, df_test, relevant_columns=['TotalPop', 'Men', 'Women', 'Hispanic', \n                                                    'White', 'Black', 'Native', 'Asian', 'Pacific',\n                                                    'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n                                                    'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n                                                    'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n                                                    'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n                                                    'SelfEmployed', 'FamilyWork', 'Unemployment'])\nmodel.eval()","4ea1ebae":"def preprocess_prod(df, index, relevant_cols, batch_size=1):\n    d = torch.tensor(df.iloc[index:index+batch_size][relevant_cols].values)\n    return d","6493accc":"relevant_cols = ['TotalPop', 'Men', 'Women', 'Hispanic',\n       'White', 'Black', 'Native', 'Asian', 'Pacific',\n       'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n       'SelfEmployed', 'FamilyWork', 'Unemployment']","d5954688":"test_df[\"embeddings\"] = list(df_test[relevant_cols].values)","4bb7e131":"test_df[\"embeddings\"] = test_df[\"embeddings\"].map(lambda x: model.encoder_forward(torch.tensor(x).unsqueeze(0).float()).detach().numpy())","410eda4f":"def get_county_from_df(county, state, df):\n    return df.query(\"County=='\" + county + \"' and State=='\" +state+\"'\")","05a353b9":"get_county_from_df(\"Queens County\", \"New York\", test_df)\n","72900dc6":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity(get_county_from_df(\"Aroostook County\", \"Maine\", test_df).iloc[0][\"embeddings\"], get_county_from_df(\"Broward County\", \"Florida\", test_df).iloc[0][\"embeddings\"])\n","55bc44f2":"!pip install umap-learn\nimport umap\nreducer = umap.UMAP()","a0f13e5b":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10, Category20c\nfrom bokeh.palettes import magma\nimport pandas as pd\noutput_notebook()","9f6da6a5":"val_arr = [ ]\nname_list = [ ] ","dd38915b":"for embed in test_df[[\"embeddings\", \"County\", \"State\"]].values[0:300]: \n    val_arr.append(embed[0])\n    name_list.append(embed[1] +\"_\" +embed[2])\nres = np.vstack(val_arr)\n#res = np.ndarray(res)","64fcf198":"def make_plot(red, title_list, number=200, color = True, color_mapping_cat=None, color_cats = None, bg_color=\"white\"):   \n    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n    if color_mapping_cat:\n        digits_df['colors'] = color_mapping_cat\n    digits_df['digit'] = title_list\n    datasource = ColumnDataSource(digits_df)\n    plot_figure = figure(\n    title='UMAP projection Counties',\n    plot_width=890,\n    plot_height=600,\n    tools=('pan, wheel_zoom, reset'),\n    background_fill_color = bg_color\n    )\n    plot_figure.legend.location = \"top_left\",\n    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n    <div>\n    <div>\n        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'\/>\n    <\/div>\n    <div>\n        <span style='font-size: 10px; color: #224499'><\/span>\n        <span style='font-size: 10px'>@digit<\/span>\n    <\/div>\n    <\/div>\n    \"\"\"))\n    if color:   \n        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    elif color_mapping_cat:\n        color_mapping = CategoricalColorMapper(factors=color_cats, palette=magma(len(color_cats)+2)[2:])\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='colors', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=8,\n            legend_field='colors'\n        )\n        show(plot_figure)\n    else:\n        \n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit'),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\nred = reducer.fit_transform(res)   \nmake_plot(red, name_list, number=199)","11a4e9ec":"new_york_ny = get_county_from_df(\"New York County\", \"New York\", test_df).iloc[0][\"embeddings\"]","bde5783d":"def get_most_similar_counties(target_embed, test_df, target_name):\n    test_df = test_df.dropna()\n    test_df[target_name] = test_df['embeddings'].map(lambda x: cosine_similarity(x, target_embed))\n    return test_df.sort_values(by=target_name)\nget_most_similar_counties(new_york_ny, test_df, \"new_yor_sim\")","07ab4df1":"kansas_chase = get_county_from_df(\"Chase County\", \"Kansas\", test_df).iloc[0][\"embeddings\"]\nget_most_similar_counties(kansas_chase, test_df, \"kansas\")","f77422c1":"denver_county = get_county_from_df(\"Denver County\", \"Colorado\", test_df).iloc[0][\"embeddings\"]\nget_most_similar_counties(denver_county, test_df, \"d_colorado\")","57a62eb8":"### Sample cosine similarity between Broward County in Florida and Aroostook County in Maine","ccaae4d3":" # Auto-encoding County\nThe goal of this task is to train an autoencoder to encode a county in a specific ","b8f9ff85":"## Visualizing Encoder Embedding\nIn this section we will now visualize reuslts.\n"}}