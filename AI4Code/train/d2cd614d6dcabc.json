{"cell_type":{"1c9c0f14":"code","ea9861f6":"code","e7f562ab":"code","cafd1a5a":"code","f6d65848":"code","970e9978":"code","eff49608":"code","e6ef781f":"code","b56eab2a":"code","6c8e06f1":"code","73dd9659":"code","ef9f086e":"code","f9da826b":"code","19f7d194":"code","147712f1":"code","b9b381c9":"code","75679223":"code","9ccf65af":"code","1d09eedb":"code","c05b0735":"code","f1fb5bdc":"code","928ef1e4":"code","6e41d95e":"code","d259e88a":"code","c78f40fd":"code","7703d58c":"code","f321afdf":"code","a0395fa2":"code","eeb3d2b5":"code","3826b74c":"code","e58ece87":"code","9b97e93a":"code","63d79caf":"code","f9090a2c":"code","b045f92f":"code","43384e63":"code","84e8bbe0":"code","9ce3901c":"code","ca6a0d86":"code","6b817e83":"code","a92aa62f":"code","cb301122":"code","5b04f758":"code","0cb2fcb6":"code","b389b46b":"code","9723cdc9":"code","7e21327f":"code","9910d8bb":"code","e541c004":"code","c04ef49d":"code","f6af257c":"code","7488b6ab":"code","221c629a":"code","db3f4916":"code","1469de3a":"code","5e31bb23":"code","a098909c":"code","784a7c53":"code","4bdebb98":"code","020430d3":"code","67d159ff":"code","6cb26486":"code","c10cdb44":"code","d8cb0f57":"code","e6b6c1a1":"code","07ba7fef":"code","f6bb4dba":"code","71a8ac85":"code","73967b4a":"code","6f07b538":"code","4c2a087e":"code","79da37ee":"code","2c2a3c2f":"code","f0662516":"code","a059c3cf":"code","c38dc280":"code","0f8b02b4":"code","3ff29b52":"code","97404b2b":"code","90afe2fa":"code","f2160274":"code","19799a8c":"code","0dcc1dfb":"code","b228233d":"code","359a3762":"code","765b1791":"code","c41d0c92":"code","6dea9983":"code","86786f65":"code","8a1d98ce":"code","d5f4d020":"code","535839da":"code","818e0c10":"code","f1fad65b":"code","0a42dcc3":"markdown","87c4a9a8":"markdown","5f3a4fca":"markdown","140d499c":"markdown","08b26b93":"markdown","46904908":"markdown","6e9b3ca4":"markdown","015fe9ff":"markdown","b40b9906":"markdown","ea597c87":"markdown","d422ab65":"markdown"},"source":{"1c9c0f14":"#Importing require libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Setting Format\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_columns = None\npd.options.display.max_rows = None\nnp.random.seed(100)","ea9861f6":"# Importing our data\n\nday = pd.read_csv(\"..\/input\/bike-sharing-dataset\/day.csv\")\nhour = pd.read_csv(\"..\/input\/bike-sharing-dataset\/hour.csv\")","e7f562ab":"day.head()","cafd1a5a":"hour.head()","f6d65848":"hour.info()","970e9978":"#Description of our data\n\nwith open('..\/input\/bike-sharing-dataset\/Readme.txt', 'r') as txt:\n    print(txt.read())","eff49608":"# As per given information these data was transformed\n# So lets do transfrom them back to their real format to get better understanding of data\n\nday['temp'] = day['temp']*41\nhour['temp'] = hour['temp']*41\n\nday['atemp'] = day['atemp']*50\nhour['atemp'] = hour['atemp']*50\n\nday['hum'] = day['hum']*100\nhour['hum'] = hour['hum']*100\n\nday['windspeed'] = day['windspeed']*67\nhour['windspeed'] = hour['windspeed']*67","e6ef781f":"day.head()","b56eab2a":"hour.info() #Checking data type","6c8e06f1":"day.isna().sum() # is their any Null vales?","73dd9659":"hour.isna().sum()# is their any Null vales?","ef9f086e":"hour.info()","f9da826b":"day.describe().T #Lets look at Mean, median and Standard Deviation","19f7d194":"# These columns should be Category not int\n\ncol = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n       'workingday', 'weathersit']","147712f1":"def change_dtype(data, col):\n    for i in col:\n        if i in data.columns.to_list():\n            data[i] = data[i].astype('category')","b9b381c9":"for i in col:\n    print(\"Name of {} col\".format(i)) #Name of Col\n    print(\"No. of NUnique\", hour[i].nunique()) #Total Nunique Values\n    print(\"Unique Values\", hour[i].unique())# All unique vales\n    print('*'*30) # to make differnce i each col\n    print()\n    print()","75679223":"change_dtype(day, col) #Changing Col\nchange_dtype(hour, col) #Changing Col","9ccf65af":"#How they look after transformation\n\nfor i in col:\n    print(\"Name of {} col\".format(i)) #Name of Col\n    print(\"No. of NUnique\", hour[i].nunique()) #Total Nunique Values\n    print(\"Unique Values\", hour[i].unique())# All unique vales\n    print('*'*30) # to make differnce i each col\n    print()\n    print()","1d09eedb":"def drop_instant(data):\n    data.drop(['instant'], axis=1, inplace=True)\n    \ndrop_instant(day)\ndrop_instant(hour)","c05b0735":"day.describe().T","f1fb5bdc":"hour.describe().T","928ef1e4":"for i in day.select_dtypes(include='int'):\n    sns.distplot(day[i]) #Lets check how data is distributed\n    plt.show() ","6e41d95e":"for i in hour.select_dtypes(include='int'):\n    sns.distplot(hour[i]) #Lets check how data is distributed\n    plt.show()","d259e88a":"for i in day.select_dtypes(include='int'):\n    sns.boxplot(day[i]) #Is their any outlier\n    plt.show()","c78f40fd":"for i in hour.select_dtypes(include='int'):\n    sns.boxplot(hour[i]) #Is their any outlier\n    plt.show()","7703d58c":"for i in day.select_dtypes(include='float'):\n    sns.distplot(day[i])\n    plt.show()","f321afdf":"for i in hour.select_dtypes(include='float'):\n    sns.distplot(hour[i])\n    plt.show()","a0395fa2":"for i in day.select_dtypes(include='float'):\n    sns.boxplot(day[i])\n    plt.show()","eeb3d2b5":"for i in hour.select_dtypes(include='float'):\n    sns.boxplot(hour[i])\n    plt.show()","3826b74c":"sns.heatmap(day.corr()) #How are data is related to each other","e58ece87":"day.corr()['cnt'] #Co-relation with Tagret Variable","9b97e93a":"hour.corr()['cnt'] #Co-relation with Tagret Variable","63d79caf":"day.head()","f9090a2c":"def get_df_name(df):\n    '''\n    This Function returns the name of a dateset\n    '''\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name\n\n\ndef plot_stack_bar_chart(data, col, name):\n    plt.figure(figsize=(12,8)) #Size of a PLot\n    p1 = plt.bar(data[col].unique(),  # the x locations for the groups\n                data.groupby([col])['casual'].sum()) # Count of casual per season\n\n    p2 = plt.bar(data[col].unique(),  # the x locations for the groups\n                data.groupby([col])['registered'].sum(), # Count of Registered per season\n                 bottom = data.groupby([col])['casual'].sum()) # Count of casual per season\n\n    plt.ylabel('Count')\n    plt.title(\"Count by Casual and Registered for each {} in {} Data\".format(col, get_df_name(data)))\n    plt.xticks(data[col].unique(), name) # Name of unique values in columns\n    plt.legend((p1[0], p2[0]), ('Casual', 'Registered')) #setting legends as per target\n    plt.show()","b045f92f":"plot_stack_bar_chart(day, 'season', ('1:springer', '2:summer', '3:fall', '4:winter'))","43384e63":"plot_stack_bar_chart(day, 'yr', ('2011', '2012'))","84e8bbe0":"plot_stack_bar_chart(day, 'mnth', [str(i) for i in day['mnth'].unique()])","9ce3901c":"plot_stack_bar_chart(day, 'holiday', ('Yes', 'No'))","ca6a0d86":"plot_stack_bar_chart(day, 'weekday', [str(i) for i in day['weekday'].unique()])","6b817e83":"plot_stack_bar_chart(day, 'workingday', ('Yes', 'No'))","a92aa62f":"plot_stack_bar_chart(day, 'weathersit', ('Clear', 'Mist', 'Light Snow', 'Rain'))","cb301122":"plot_stack_bar_chart(hour, 'season', ('1:springer', '2:summer', '3:fall', '4:winter'))","5b04f758":"plot_stack_bar_chart(hour, 'yr', ('2011', '2012'))","0cb2fcb6":"plot_stack_bar_chart(hour, 'mnth', [str(i) for i in hour['mnth'].unique()])","b389b46b":"plot_stack_bar_chart(hour, 'hr', [str(i) for i in hour['hr'].unique()])","9723cdc9":"plot_stack_bar_chart(hour, 'holiday', ('Yes', 'No'))","7e21327f":"plot_stack_bar_chart(hour, 'weekday', [str(i) for i in hour['weekday'].unique()])","9910d8bb":"plot_stack_bar_chart(hour, 'workingday', ('Yes', 'No'))","e541c004":"plot_stack_bar_chart(hour, 'weathersit', ('Clear', 'Mist', 'Light Snow', 'Rain'))","c04ef49d":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['holiday'], y = hour['cnt'],hue = hour['season'])\nplt.title('Holiday wise distribution of counts')\nplt.show()","f6af257c":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['workingday'], y = hour['cnt'],hue = hour['season'])\nplt.title('Working Day wise distribution of counts')\nplt.show()","7488b6ab":"plt.figure(figsize=(18,10))\nsns.barplot(x = hour['mnth'], y = hour['cnt'], hue = hour['season'])\nplt.title('Month wise distribution of counts')\nplt.show()","221c629a":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['weathersit'], y = hour['cnt'],hue = hour['season'])\nplt.title('Weather Situation wise distribution of counts')\nplt.show()","db3f4916":"sns.boxplot(hour['hum'])","1469de3a":"sns.boxplot(hour['windspeed'])","5e31bb23":"hour.describe(include='all').T","a098909c":"def treat_outlier_iqr(data, col):\n    \n    #Finding 25 and 75 Quantile\n    q25, q75 = np.percentile(data[col], 25), np.percentile(data[col], 75)\n    # Inter Quantile Range\n    iqr = q75-q25\n    #Minimum and Maximum Range\n    min_r, max_r = q25-(iqr*1.5), q75+(iqr*1.5)\n    #Replacing Outliers with Mean\n    data.loc[data.loc[:, col] < min_r, col] = data[col].mean()\n    data.loc[data.loc[:, col] > max_r, col] = data[col].mean()\n    \n    return sns.boxplot(data[col])","784a7c53":"treat_outlier_iqr(hour, 'hum') # Treating Outliers in Hum Column","4bdebb98":"treat_outlier_iqr(hour, 'windspeed') # Treating Outliers in Hum Column","020430d3":"hour.head()","67d159ff":"plt.figure(figsize=(12,8))\nsns.heatmap(hour.corr())","6cb26486":"df = hour.copy()","c10cdb44":"hour.info()","d8cb0f57":"hour.describe(include='all').T","e6b6c1a1":"y = hour['cnt']\nx = hour.drop(['cnt', 'dteday'], axis=1) # removing cnt and dteday\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n#Adding Constant to data\nX = add_constant(x)\n\n# Checking ViF for Multi-Collinearity\n\npd.Series([variance_inflation_factor(X.values, i) \n           for i in range(X.shape[1])], \n              index=X.columns)","07ba7fef":"def feature_eng(data, col):\n    data['temp_and_atemp'] = (data['temp'] + data['atemp'])\/2 #Average the column to remove multicollinearity\n    data['dteday'] = data['dteday'].astype('datetime64') # Converting column to datetime64\n    data['day'] = data['dteday'].astype('datetime64').dt.day # Extrating day from date\n    data['day'] = data['day'].astype('category') #Converting day to category\n    data.drop(['casual', col, 'dteday', 'temp', 'atemp'], axis=1, inplace=True) #Droping all the irrelevant column\n\n#Transforming Hour Data\nfeature_eng(hour, 'registered')","f6bb4dba":"#Checking to see if their is still any multicollinearity\n\ny = hour['cnt']\nx = hour.drop(['cnt'], axis=1)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX = add_constant(x)\n\npd.Series([variance_inflation_factor(X.values, i) \n           for i in range(X.shape[1])], \n              index=X.columns)","71a8ac85":"hour.info()","73967b4a":"x.head()","6f07b538":"x.describe(include='all').T","4c2a087e":"hour.info()","79da37ee":"# Getting all category columns to list\ncat_col = hour.select_dtypes(include='category').columns.to_list()\n\n#Printing all unique values in Category Columnns\nfor i in cat_col:\n    print(\"Name of {} col\".format(i))\n    print(\"No. of NUnique\", hour[i].nunique())\n    print(\"Unique Values\", hour[i].unique())\n    print('*'*30)","2c2a3c2f":"def Categorical_transformation(data):\n    '''\n    Transforming all Categorical Columns to int\n    '''\n    cat = data.select_dtypes(include='category').columns.to_list()\n    for i in cat:\n        data[i] = data[i].astype('int64')\n    return \"Successful\"\n\nCategorical_transformation(hour)","f0662516":"hour.info()","a059c3cf":"y = hour['cnt']\nx = hour.drop(['cnt'], axis=1)","c38dc280":"#for Spliting Data and Hyperparameter Tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Importing Machine Learning Model\nfrom catboost import CatBoostRegressor\nfrom sklearn import ensemble\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRFRegressor, XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\n\n#statistical Tools\nfrom sklearn import metrics\n\n#To tranform data\nfrom sklearn import preprocessing","0f8b02b4":"# Spliting data into Training and Testing\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=10)","3ff29b52":"accuracy = {}\nrmse = {}\nexplained_variance = {}\nmax_error = {}\nMAE = {}\n\ndef train_model(model, model_name):\n    print(model_name) # Printing model name\n    model.fit(x_train,y_train) # fitting the defined model\n    pred = model.predict(x_test) # predicting our data\n\n    acc = metrics.r2_score(y_test, pred)*100 #Checking R2_Score\n    accuracy[model_name] = acc # Saving R2_Score to dict.\n    print('R2_Score',acc)\n\n    met = np.sqrt(metrics.mean_squared_error(y_test, pred)) #Calculating RMSE\n    print('RMSE : ', met) \n    rmse[model_name] = met #Saving RMSE\n\n    var = (metrics.explained_variance_score(y_test, pred)) #Calculating explained_variance_score\n    print('Explained_Variance : ', var)\n    explained_variance[model_name] = var #Saving explained_variance_score\n\n    error = (metrics.max_error(y_test, pred)) #Calculating Max_Error\n    print('Max_Error : ', error)\n    max_error[model_name] = error #Saving Max_Error\n    \n    err = metrics.mean_absolute_error(y_test, pred) #Calculating mean_absolute_error\n    print(\"Mean Absolute Error\", err)\n    MAE[model_name] = err #Saving mean_absolute_error","97404b2b":"xgb = XGBRegressor(n_jobs = 4, n_estimators = x.shape[0], max_depth = 5)\n\n#Training Model\ntrain_model(xgb, \"Xtreme Gradient\")","90afe2fa":"#Training Model\ngbr = ensemble.GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000, \n                                         max_depth=5, min_samples_split=8) # Gradient Boosting Model\n\ntrain_model(gbr, \"Gradient Boost\")","f2160274":"#Training Model\ncat = CatBoostRegressor(verbose=0, n_estimators = x_train.shape[0]) #Cat Booting Regression model\n\ntrain_model(cat, \"Cat Boost\")","19799a8c":"lgbr = LGBMRegressor(n_estimators = x_train.shape[0], learning_rate=0.01, max_depth=12, \n                     objective='tweedie', num_leaves=15, n_jobs = 4) #Light Gradient Boosting Model\n\n#Training Model\ntrain_model(lgbr, 'Light Gradient Boost')","0dcc1dfb":"#Training Model\nlr = LinearRegression(normalize = True, n_jobs=4) #Linear Regression Model\n\ntrain_model(lr, \"Linear Regression\")","b228233d":"#Training Model\nrfc = ensemble.RandomForestRegressor(n_estimators=1000, bootstrap=True, min_samples_leaf=100, \n                                     n_jobs=-1, min_samples_split=8, max_depth=6) #Random Forest Bagging Model\n\ntrain_model(rfc, \"Random Forest\")","359a3762":"#Training Model\nada = ensemble.AdaBoostRegressor(n_estimators=1000, learning_rate=0.01) #Adaptive Boosting Bodel\n\ntrain_model(ada, \"Ada Boost\")","765b1791":"#Training Model\ndtr = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100) # Decision tree model\n\ntrain_model(dtr, \"Decision Tree\")","c41d0c92":"#Training Model\nmlp = MLPRegressor(hidden_layer_sizes=(200,2), learning_rate='adaptive', max_iter=400) #Multi-Layer Percepton Regression model\n\ntrain_model(mlp, \"Multi-layer Perceptron\")","6dea9983":"#Training Model\nknn = KNeighborsRegressor(n_neighbors=10, n_jobs=4, leaf_size=50) # K Nearest Neighbors Regressor model\n\ntrain_model(knn, \"K Nearest Neighbors\")","86786f65":"# Training model using Deep Learning Keras Library\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense\n\nmodel = keras.Sequential([\n    layers.Dense(256, activation = tf.keras.layers.ELU(), input_shape=[x_train.shape[1]]), #Input Layer\n    layers.Dense(256, activation=tf.keras.layers.ELU()), #Hidden Layer\n    layers.Dense(16, activation = 'relu'), #Hidden Layer\n    layers.Dense(4, activation = 'relu'), #Hidden Layer\n    layers.Dense(1) #Output Layer\n   ])\n\n# Compile the network :\nmodel.compile(loss = tf.keras.losses.MeanSquaredError(), \n                 optimizer = 'adam', metrics = tf.keras.metrics.RootMeanSquaredError())\nmodel.summary()","8a1d98ce":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nz = scaler.fit_transform(x)","d5f4d020":"#Creating check point to retreive best weights\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]\n\n# Fitting the model\nhistory = model.fit(x, y, epochs=200, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","535839da":"#Creating Dataframe to check history\nhistory_df = pd.DataFrame(history.history)\n\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Loss\")\nhistory_df.loc[:, ['root_mean_squared_error', 'val_root_mean_squared_error']].plot(title=\"Root Mean Square Error\")","818e0c10":"# Load wights file of the best model :\nwights_file = '.\/Weights-021--5126.26074.hdf5' # choose the best checkpoint \n\nmodel.load_weights(wights_file) # load it\nmodel.compile(loss = tf.keras.losses.MeanSquaredError(), \n                 optimizer = 'adam', metrics = tf.keras.metrics.RootMeanSquaredError())","f1fad65b":"train_model(model, \"NN Model\")","0a42dcc3":"# Let's Visualize and understand Hour Data","87c4a9a8":"# Let's Visualize and understand **Day** Data","5f3a4fca":"From above we can determine that\n\ntemp and atemp are highly Correlated\n\ncasual, Registered and Cnt are highly correlated as well","140d499c":"## Lets do some Visualization","08b26b93":"mean and median in \"**hour**\" data is approxminately nearby except in case of \"**Casual, registered and cnt**\"   ","46904908":"mean and median in \"**Day**\" data is approxminately nearby except in case of \"**Casual**\"   ","6e9b3ca4":"# Finding and Replacing outliers","015fe9ff":"# So Temp and atemp is showing Multi-Collinarity","b40b9906":"Casual, Registered and cnt have outlier and need to be fix\n\nPoint to be noted that cnt is sum total of casual and Registered.","ea597c87":"## Checking Multi - Collinearity ","d422ab65":"# Light Gradient Boosting Model is giving the Best Result Lowest Mean Squared Error"}}