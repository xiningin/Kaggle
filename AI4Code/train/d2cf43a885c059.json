{"cell_type":{"232f32df":"code","6fe4127c":"code","866c8a4c":"code","f9751c0a":"code","2869182c":"code","17574f1a":"code","43b0a469":"code","e3becb44":"code","a64522bc":"code","77474992":"code","659e8cf7":"code","1879ff40":"code","d27cc360":"code","e7b5e5e6":"code","141c94b5":"code","e673f692":"code","d0b1553c":"code","4a44b91b":"code","a936c856":"code","c8678ece":"code","c1ed69c3":"code","ee37aa21":"code","d96ec5fa":"code","fa48439d":"code","9efeabae":"code","3181e535":"code","3088d526":"code","a24b21e3":"code","1e0d127d":"code","b5d220d1":"code","29288ff2":"code","ac8e76ee":"code","6ce75156":"code","7cd6bdce":"code","03583b66":"markdown","8932ed52":"markdown","ec0ab65d":"markdown","ab0f01f6":"markdown","2a23e73f":"markdown","85bd5d1f":"markdown","045b4b14":"markdown","428c10ac":"markdown","bf80ec8f":"markdown","ae651211":"markdown","4ac3f090":"markdown"},"source":{"232f32df":"import pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nsns.set()\nimport pylab as plot","6fe4127c":"#Load the Data\ntrain = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\n","866c8a4c":"train.head()","f9751c0a":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","2869182c":"train.info()","17574f1a":"train.describe().T","43b0a469":"target_count = train['target'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['target(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('target(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","e3becb44":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_count = train['target'].value_counts().sort_index()\n\nax.bar(target_count.index, target_count, color=['#1520E6' if i%2==0 else '#93D1FF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(1,2):\n    ax.annotate(f'{target_count[i]\/len(train)*100:.3}', xy=(i, target_count[i]+1000),\n                   va='center', ha='center',\n               )\n#Annotate the point xy with text text.\n\n#In the simplest form, the text is placed at xy.\n\nax.set_title('target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","a64522bc":"train.drop([\"ID_code\"] , axis = 1 , inplace = True)\n","77474992":"y=train['target']\nX=train.drop(labels=['target'], axis=1)","659e8cf7":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X)","1879ff40":"var_thres.get_support()","d27cc360":"### Finding non constant features\nsum(var_thres.get_support())","e7b5e5e6":"# Lets Find non-constant features \nlen(X.columns[var_thres.get_support()])","141c94b5":"constant_columns = [column for column in X.columns\n                    if column not in X.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","e673f692":"from sklearn.datasets import load_boston\nX.corr()","d0b1553c":"import seaborn as sns\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","4a44b91b":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","a936c856":"corr_features = correlation(X, 0.7)\nlen(set(corr_features))","c8678ece":"test.head()","c1ed69c3":"print(f'Number of rows: {test.shape[0]};  Number of columns: {test.shape[1]}; No of missing values: {sum(test.isna().sum())}')","ee37aa21":"test.describe().T","d96ec5fa":"test.drop([\"ID_code\"] , axis = 1 , inplace = True)","fa48439d":"x_test=test","9efeabae":"submission.head()","3181e535":"submission.drop([\"ID_code\"] , axis = 1 , inplace = True)","3088d526":"y_test=submission.target","a24b21e3":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n#Cross Validation (K-fold)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","1e0d127d":"# calculate manually\ndef my_function(y,y_preds):\n  \n  d = y - y_preds\n  mse_f = np.mean(d**2)\n  mae_f = np.mean(abs(d))\n  rmse_f = np.sqrt(mse_f)\n\n\n  print(\"Results by manual calculation:\")\n  print(\"MAE:\",mae_f)\n  print(\"MSE:\", mse_f)\n  print(\"RMSE:\", rmse_f)\n","b5d220d1":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n","29288ff2":"import lightgbm\nfrom sklearn.metrics import roc_auc_score\n#Step2: Create a simple Light GBM Model and evaluate performance\n#LightGBM has function Dataset to read the data. This is required for using LightGBM\ntrain_data = lightgbm.Dataset(X_train, label=y_train)\nvalid_data = lightgbm.Dataset(X_valid, label=y_valid)\n\nparameters = {'objective': 'binary',\n              'metric': 'auc',\n              'is_unbalance': 'true',\n              'boosting': 'gbdt',\n              'num_leaves': 63,\n              'feature_fraction': 0.5,\n              'bagging_fraction': 0.5,\n              'bagging_freq': 20,\n              'learning_rate': 0.01,\n              'verbose': -1\n             }\n\nmodel_lgbm = lightgbm.train(parameters,\n                            train_data,\n                            valid_sets=valid_data,\n                            num_boost_round=5000,\n                            early_stopping_rounds=50)\ny_train_pred = model_lgbm.predict(X_train)\ny_valid_pred = model_lgbm.predict(X_valid)\n\nprint(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))\n","ac8e76ee":"import xgboost\nmodel_xgboost = xgboost.XGBClassifier(learning_rate=0.1,\n                                      max_depth=5,\n                                      n_estimators=5000,\n                                      subsample=0.5,\n                                      colsample_bytree=0.5,\n                                      eval_metric='auc',\n                                      verbosity=1)\n\neval_set = [(X_valid, y_valid)]\n\nmodel_xgboost.fit(X_train,\n                  y_train,\n                  early_stopping_rounds=10,\n                  eval_set=eval_set,\n                  verbose=True)","6ce75156":"# castboost\n#importing library and building model\nfrom catboost import CatBoostRegressor\nmodel=CatBoostRegressor(iterations=50, depth=5, learning_rate=0.1, loss_function='RMSE')\nmodel.fit(X_train, y_train,eval_set=(X_valid, y_valid), verbose=True,plot=True)","7cd6bdce":"print('lightgbm model')\ny_preds_lgbm = model_lgbm.predict(x_test )\nmy_function(y_test,y_preds_lgbm)\nprint('*********************************')\nprint(\" xgboost model\")\ny_preds_lgbm = model_xgboost.predict(x_test )\nmy_function(y_test,y_preds_lgbm)\n\nprint('*********************************')\nprint(\" castboost   model\")\ny_preds_catboost = model.predict(x_test )\nmy_function(y_test,y_preds_catboost)","03583b66":"2. Feature Selection- With Correlation\u00b6\nIn this step we will be removing the features which are highly correlated","8932ed52":"corr_features","ec0ab65d":"Infos","ab0f01f6":"Below is the first 5 rows of test dataset:","2a23e73f":"Summarie and statistics\u00b6","85bd5d1f":"The dimension and number of missing values in the train dataset is as below:","045b4b14":"Summarie and statistics\u00b6\n","428c10ac":"Below is the first 5 rows of test dataset:","bf80ec8f":"Load and check data","ae651211":"The dimension and number of missing values in the train dataset is as below:\n","4ac3f090":"Feature Selection- Dropping constant features\nIn this step we will be removing the features which have constant features which are actually not important for solving the problem statement"}}