{"cell_type":{"07e18621":"code","c6fd8f19":"code","b6ba203e":"code","2c49ee5f":"code","715ae5c9":"code","cd3480d0":"code","7f22f824":"code","b39c2dc3":"code","7b78af28":"code","cccffbe0":"code","c35fed43":"code","e1d50a38":"code","29822a94":"code","93e50551":"code","5bceddea":"code","41cce213":"code","6c2e8ebf":"code","40be209d":"code","2e21b521":"code","c9ce565f":"code","e3c784c2":"code","d12769b8":"code","4bd841f6":"code","22761f24":"code","cbfed718":"markdown","5e045c70":"markdown","24a7146a":"markdown"},"source":{"07e18621":"import os\nimport csv\nimport json\nimport string\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *","c6fd8f19":"## Load the news aggregator titles\n\ntitles = []\n\nwith open(\"..\/input\/news-aggregator-dataset\/uci-news-aggregator.csv\") as file:\n    reader = csv.reader(file)\n    for i, line in enumerate(reader):\n        if i!=0:\n            titles.append(line[1])\n            \n    file.close()","b6ba203e":"titles[-5:]","2c49ee5f":"len(titles)","715ae5c9":"df = pd.read_json(\"..\/input\/news-category-dataset\/News_Category_Dataset_v2.json\", lines=True)\ndf.head()","cd3480d0":"## get the news headlines\n\nheadlines = df.headline.to_list()\nheadlines[-5:]","7f22f824":"full_titles = titles + headlines\n\nlen(full_titles)","b39c2dc3":"chars = sorted(list(string.printable))\n\n\nEOS = \"<EOS>\" # end of sequence\nUNK = \"<UNK>\" # unknown\nPAD = \"<PAD>\" # padding\n\nchars.append(UNK)\nchars.append(EOS) # last token in sequence\n\nchars.insert(0, PAD) # padding should get index 0","7b78af28":"char2idx = {letter:i for i, letter in enumerate(chars)}\nidx2char = np.array(chars)","cccffbe0":"def char_id(c):\n    \"\"\"\n    takes a character and returns its index\n    Otherwise returns unknown index\n    \"\"\"\n    if c in chars:\n        return char2idx[c]\n    return char2idx[UNK]","c35fed43":"data = []\nmax_len = 75\n\nfor title in full_titles:\n    line = [char_id(c) for c in title]\n    \n    if len(line)>=max_len:\n        line = line[:max_len-1]\n        line.append(char2idx[EOS])\n    else:\n        line.append(char2idx[EOS])\n        remain = max_len - len(line)\n        if remain>0:\n            line = line + [char2idx[PAD]] * remain\n            \n    data.append(line)\n    \nprint(\"***Data File Loaded***\")","e1d50a38":"data = np.array(data)","29822a94":"data[:3]","93e50551":"data.shape","5bceddea":"# prepare the feature and target\n\ndata_in = data[:, :-1]\ndata_out = data[:, 1:]","41cce213":"batch_size = 256\nbuffer_size = 10000\nvocab_size = len(chars)\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((data_in, data_out))\n\ntrain_ds = train_ds.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(1)","6c2e8ebf":"model = Sequential([\n    Embedding(vocab_size, 256, mask_zero=True, batch_input_shape=[batch_size, None]),\n    GRU(1024, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n    Dropout(0.1),\n    Dense(vocab_size)\n])\n\nmodel.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n\nmodel.summary()","40be209d":"%%time\n\ncheckpoints = keras.callbacks.ModelCheckpoint(\".\/logs2\/cp_{epoch:02d}.ckpt\",monitor='loss',\n                                             save_weights_only=True)\n\nprint(\"-------Training Start------\")\n\nhistory = model.fit(train_ds, epochs=3, callbacks=[checkpoints])\n\nprint(\"------Training Complete-------\")","2e21b521":"gen_batch_size = 1\n\ngen_model = Sequential([\n    Embedding(vocab_size, 256, mask_zero=False,                # mask = False\n              batch_input_shape=[gen_batch_size, None]),\n    GRU(1024, return_sequences=True, \n        stateful=True, \n        recurrent_initializer='glorot_uniform'),\n    Dense(vocab_size)\n])\n\n","c9ce565f":"checkpoints_dir = \".\/logs2\"\n\ngen_model.build(input_shape=tf.TensorShape([1, None]))\n\ngen_model.load_weights(tf.train.latest_checkpoint(checkpoints_dir))","e3c784c2":"def generate_text(model, start_string, temp=0.7, num_generate=75):\n    \n    \n    # convering our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    \n    # store results\n    text_generated = []\n    \n    # \n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove batch dimension\n        predictions = tf.squeeze(predictions, axis=0)\n        \n        predictions = predictions \/ temp\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n        \n        # pass the predicted word as the next input to the model\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n        \n    return (start_string +''.join(text_generated))","d12769b8":"print(generate_text(gen_model, start_string=u\"Google\"))","4bd841f6":"print(generate_text(gen_model, start_string=u\"Google\", temp=0.3))","22761f24":"print(generate_text(gen_model, start_string=u\"The meeting is t\", temp=0.7))","cbfed718":"## Data Loading","5e045c70":"## Data Preprocessing","24a7146a":"## Generating Text"}}