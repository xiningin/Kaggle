{"cell_type":{"2dcdb2ae":"code","99211d55":"code","8303133a":"code","4354830b":"code","140c7ebc":"code","fd2ef091":"code","9aff8eaa":"code","51fc948c":"code","de93e568":"code","65a99d14":"code","dae1e597":"code","3250d26d":"code","fa03e3e4":"code","390eaba8":"code","032597ae":"code","12577c13":"code","1173e22b":"code","a1412cc3":"code","a4277b4b":"code","18803626":"markdown","8f1587bb":"markdown","8e9ad75b":"markdown","1b05a310":"markdown","565b04b7":"markdown"},"source":{"2dcdb2ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","99211d55":"###raw mae\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold,TimeSeriesSplit,KFold,GroupKFold\nfrom sklearn.metrics import roc_auc_score,mean_squared_error,mean_absolute_error\nimport tqdm\nimport sqlite3\nimport datetime\nfrom scipy.stats import pearsonr\nimport gc\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom kaggle.competitions import nflrush","8303133a":"train_df = pd.read_csv('..\/input\/nfl-big-data-bowl-2020\/train.csv',low_memory=False)","4354830b":"#from https:\/\/www.kaggle.com\/c\/nfl-big-data-bowl-2020\/discussion\/112681#latest-649087\nTurf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n        'SISGrass':'Artificial', 'Twenty-Four\/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\n# from https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\nmap_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\nfor abb in train_df['PossessionTeam'].unique():\n    map_abbr[abb] = abb\n    \ntrain_df[\"HomeTeamAbbr\"] = train_df[\"HomeTeamAbbr\"].map(map_abbr)\ntrain_df[\"VisitorTeamAbbr\"] = train_df[\"VisitorTeamAbbr\"].map(map_abbr)\ntrain_df[\"Possession\"] = train_df[\"PossessionTeam\"].map(map_abbr)\n    \ndef uid_aggregation(comb, main_columns, uids, aggregations):\n    X = pd.DataFrame()\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = col+'_'+main_column+'_'+agg_type\n                temp_df = comb[[col, main_column]]\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                X[new_col_name] = comb[col].map(temp_df)\n                del temp_df\n                gc.collect()\n    return X\n\ndef preprocess(df, labelEncoders=None):\n    X = df.copy()\n    X = X.select_dtypes(include=['number', 'object'])\n    def gameclock2min(x):\n        clock = x.split(\":\")\n        return 60 * int(clock[0]) + int(clock[1])\n    def height2inch(x):\n        height = x.split(\"-\")\n        return 12 * int(height[0]) + int(height[1])\n    def birthday2day(x):\n        days = x.split(\"\/\")\n        return 30 * int(days[0]) + int(days[1]) + 365 * int(days[2])\n    def timesnap2day(x):\n        days = x.split(\"-\")\n        return 365 * int(days[0]) + 30 * int(days[1]) + int(days[2][:2])\n    def transform_time_quarter(str1):\n        return int(str1[:2])*60 + int(str1[3:5])\n    def transform_time_all(str1,quarter):\n        if quarter<=4:\n            return 15*60 - (int(str1[:2])*60 + int(str1[3:5])) + (quarter-1)*15*60\n        if quarter ==5:\n            return 10*60 - (int(str1[:2])*60 + int(str1[3:5])) + (quarter-1)*15*60\n    def utc2sec(x):\n        return int(x.split(\"-\")[2].split(\":\")[2].split(\".\")[0])\n    def group_stadium_types(stadium):\n        outdoor       = [\n            'Outdoor', 'Outdoors', 'Cloudy', 'Heinz Field',\n            'Outdor', 'Ourdoor', 'Outside', 'Outddors',\n            'Outdoor Retr Roof-Open', 'Oudoor', 'Bowl'\n        ]\n        indoor_closed = [\n            'Indoors', 'Indoor', 'Indoor, Roof Closed', 'Indoor, Roof Closed',\n            'Retractable Roof', 'Retr. Roof-Closed', 'Retr. Roof - Closed', 'Retr. Roof Closed',\n        ]\n        indoor_open   = ['Indoor, Open Roof', 'Open', 'Retr. Roof-Open', 'Retr. Roof - Open']\n        dome_closed   = ['Dome', 'Domed, closed', 'Closed Dome', 'Domed', 'Dome, closed']\n        dome_open     = ['Domed, Open', 'Domed, open']\n\n        if stadium in outdoor:\n            return 'outdoor'\n        elif stadium in indoor_closed:\n            return 'indoor closed'\n        elif stadium in indoor_open:\n            return 'indoor open'\n        elif stadium in dome_closed:\n            return 'dome closed'\n        elif stadium in dome_open:\n            return 'dome open'\n        else:\n            return 'unknown'\n\n    def group_game_weather(weather):\n        rain = [\n            'Rainy', 'Rain Chance 40%', 'Showers',\n            'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.',\n            'Scattered Showers', 'Cloudy, Rain', 'Rain shower', 'Light Rain', 'Rain'\n        ]\n        overcast = [\n            'Cloudy, light snow accumulating 1-3\"', 'Party Cloudy', 'Cloudy, chance of rain',\n            'Coudy', 'Cloudy, 50% change of rain', 'Rain likely, temps in low 40s.',\n            'Cloudy and cold', 'Cloudy, fog started developing in 2nd quarter',\n            'Partly Clouidy', '30% Chance of Rain', 'Mostly Coudy', 'Cloudy and Cool',\n            'cloudy', 'Partly cloudy', 'Overcast', 'Hazy', 'Mostly cloudy', 'Mostly Cloudy',\n            'Partly Cloudy', 'Cloudy'\n        ]\n        clear = [\n            'Partly clear', 'Sunny and clear', 'Sun & clouds', 'Clear and Sunny',\n            'Sunny and cold', 'Sunny Skies', 'Clear and Cool', 'Clear and sunny',\n            'Sunny, highs to upper 80s', 'Mostly Sunny Skies', 'Cold',\n            'Clear and warm', 'Sunny and warm', 'Clear and cold', 'Mostly sunny',\n            'T: 51; H: 55; W: NW 10 mph', 'Clear Skies', 'Clear skies', 'Partly sunny',\n            'Fair', 'Partly Sunny', 'Mostly Sunny', 'Clear', 'Sunny'\n        ]\n        snow  = ['Heavy lake effect snow', 'Snow']\n        none  = ['N\/A Indoor', 'Indoors', 'Indoor', 'N\/A (Indoors)', 'Controlled Climate']\n\n        if weather in rain:\n            return 'rain'\n        elif weather in overcast:\n            return 'overcast'\n        elif weather in clear:\n            return 'clear'\n        elif weather in snow:\n            return 'snow'\n        elif weather in none:\n            return 'none'\n        else:\n            return 'none'\n\n    def clean_wind_speed(windspeed):\n        \"\"\"\n        This is not a very robust function,\n        but it should do the job for this dataset.\n        \"\"\"\n        ws = str(windspeed)\n        # if it's already a number just return an int value\n        if ws.isdigit():\n            return int(ws)\n        # if it's a range, take their mean\n        if '-' in ws:\n            return (int(ws.split('-')[0]) + int(ws.split('-')[1]))\/2\n        # if there's a space between the number and mph\n        if ws.split(' ')[0].isdigit():\n            return int(ws.split(' ')[0])\n        # if it looks like '10MPH' or '12mph' just take the first part\n        if 'mph' in ws.lower():\n            return int(ws.lower().split('mph')[0])\n        else:\n            return 0\n        \n    X[\"Dir\"] = np.mod(90 - df[\"Dir\"].values, 360)\n#     X['Team'] = df['Team'].map({\"home\": 0, \"away\": 1})\n    X['Turf'] = df['Turf'].map(Turf)\n#     X['Turf'] = X['Turf'].map({\"Natural\": 0,\"Artificial\": 1})\n    X['HomePossesion'] = 1 * (df['PossessionTeam'] == df['HomeTeamAbbr'])\n#     X[\"OffenseFormation\"] = df['OffenseFormation'].apply(clean_offenceformation)\n#     X['OffenseFormation'] = X['OffenseFormation'].fillna(7)\n    X['PassDuration'] = df['TimeHandoff'].apply(utc2sec) - df['TimeSnap'].apply(utc2sec)\n    # from https:\/\/www.kaggle.com\/zero92\/best-lbgm-new-features\n    X['Month'] = df['TimeHandoff'].apply(lambda x : int(x[5:7]))\n    X['Year'] = df['TimeHandoff'].apply(lambda x : int(x[0:4]))\n    X['Morning'] = df['TimeHandoff'].apply(lambda x : 1 if (int(x[11:13]) >=0 and int(x[11:13]) <12) else 0)\n    X['Afternoon'] = df['TimeHandoff'].apply(lambda x : 1 if (int(x[11:13]) <18 and int(x[11:13]) >=12) else 0)\n    X['Evening'] = df['TimeHandoff'].apply(lambda x : 1 if (int(x[11:13]) >= 18 and int(x[11:13]) < 24) else 0)\n    X['MatchDay'] = df['TimeSnap'].apply(timesnap2day)\n    X['PlayerBirthDate'] = df['PlayerBirthDate'].apply(birthday2day)\n    X['PlayerAge'] = X['MatchDay'] - X['PlayerBirthDate']\n    X['PlayerWeight'] = df['PlayerWeight']\n    X['PlayerHeight'] = df['PlayerHeight'].apply(height2inch)\n    X['BMI'] = X['PlayerWeight'] \/ X['PlayerHeight']\n    X['time_quarter'] = df[\"GameClock\"].map(lambda x:transform_time_quarter(x)).values\n    X['time_end'] = df.apply(lambda x:transform_time_all(x.loc['GameClock'],x.loc['Quarter']),axis=1).values\n    X['GameClock'] = df['GameClock'].apply(gameclock2min)\n    X['StadiumType'] = df['StadiumType'].apply(group_stadium_types)\n    X['GameWeather'] = df['GameWeather'].apply(group_game_weather)\n    X['WindSpeed'] = df['WindSpeed'].apply(clean_wind_speed)\n#     X['WindDirection'] = df['WindDirection'].apply(clean_wind_direction)\n#     X['WindDirection'] = 2 * np.pi * (90 - X['WindDirection'].values) \/ 360\n    X['Humidity'] = df['Humidity'].fillna(df['Humidity'].median())\n    X['Temperature'] = df['Temperature'].fillna(df['Temperature'].median())\n    X['DefendersInTheBox'] = df['DefendersInTheBox'].fillna(df['DefendersInTheBox'].median())\n        \n    # from https:\/\/www.kaggle.com\/ryches\/model-free-benchmark\n    X['Field_eq_Possession'] = 1 * (df['FieldPosition'] == df['PossessionTeam'])    \n    X['is_rusher'] = 1 * (df['NflId'] == df['NflIdRusher'])\n    X['seconds_need_to_first_down'] = (df['Distance']*0.9144) \/ (df['Dis'].values + 0.01)\n    X['seconds_need_to_YardsLine'] = (df['YardLine']*0.9144) \/ (df['Dis'].values + 0.01)\n    X['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] \/ df['Distance']\n    \n    # based on https:\/\/www.kaggle.com\/sryo188558\/cox-proportional-hazard-model\n    playdir = df['PlayDirection'].map({'right': 1, 'left': -1}).values\n    X[\"Start\"] = X[\"YardLine\"]\n    X.loc[(X[\"Field_eq_Possession\"] == 1) & (playdir == 1), \"Start\"] = X.loc[(X[\"Field_eq_Possession\"] == 1) & (playdir == 1), \n                                                                                       \"YardLine\"] + 10\n    X.loc[(X[\"Field_eq_Possession\"] == 1) & (playdir == -1), \"Start\"] = 120 - X.loc[(X[\"Field_eq_Possession\"] == 1) & (playdir == -1), \n                                                                                       \"YardLine\"] - 10\n    X.loc[(X[\"Field_eq_Possession\"] == 0) & (playdir == 1), \"Start\"] = 120 - X.loc[(X[\"Field_eq_Possession\"] == 0) & (playdir == 1), \n                                                                                       \"YardLine\"] - 10\n    X.loc[(X[\"Field_eq_Possession\"] == 0) & (playdir == -1), \"Start\"] = X.loc[(X[\"Field_eq_Possession\"] == 0) & (playdir == -1), \n                                                                                       \"YardLine\"] + 10\n    X['Orientation'] = 2 * np.pi * (90 - X['Orientation']) \/ 360\n    X['locX'] = (X['X'].values - X['Start'].values) * playdir\n    X['locY'] = X['Y'].values - 53.3 \/ 2\n    X['velX'] = X['S'].values * np.cos(X['Orientation'].values) * playdir\n    X['velY'] = X['S'].values * np.sin(X['Orientation'].values)\n    X['accX'] = X['A'].values * np.cos(X['Orientation'].values) * playdir\n    X['accY'] = X['A'].values * np.sin(X['Orientation'].values)\n    \n    i_cols = ['VisitorScoreBeforePlay','HomeScoreBeforePlay','YardLine']\n    uids = ['DisplayName']\n    aggregations = ['mean','std']\n    X_agg = uid_aggregation(df, i_cols, uids, aggregations)\n    X = pd.concat([X, X_agg], axis=1)\n\n    return X","140c7ebc":"train_single = train_df.copy()\ntrain_single = train_single.loc[train_single.NflId == train_single.NflIdRusher, :]\ntrain_single['own_field'] = 1 * (train_single['FieldPosition'].values == train_single['PossessionTeam'].values)\ndist_to_end_train = train_single.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)","fd2ef091":"play = preprocess(train_df)\nprint(play.shape)\nplay.head()","9aff8eaa":"rm_cols = ['index','GameId','PlayId']\nfeatures = [c for c in train_df.columns.values if c not in rm_cols]\nprint(\"There are {} features\".format(len(features)))\nprint(features)","51fc948c":"play = play[features]\nprint(play.shape)\nplay.head()","de93e568":"play['is_run'] = play.NflId == play.NflIdRusher\nplay = play[play.is_run==True]","65a99d14":"y_train = play[\"Yards\"]\nX_train = play.drop(['Yards'],axis=1)\n\ny_train = y_train.reset_index(drop=True, inplace=False)\nX_train = X_train.reset_index(drop=True, inplace=False)","dae1e597":"categorical_features_indices = np.where(X_train.dtypes != np.float)[0]\nfeatures = X_train.columns.values\nfor c in categorical_features_indices:\n    X_train[features[c]] = X_train[features[c]].fillna(\"nan\")","3250d26d":"model = CatBoostRegressor(iterations=2000, \n                          learning_rate = 0.01,\n                          use_best_model = True,\n                          eval_metric = 'RMSE',\n                          loss_function = 'RMSE',\n                          cat_features = categorical_features_indices,\n                          boosting_type = 'Ordered', # or Plain\n                          verbose = 0)                                     ","fa03e3e4":"# train, test split\ntrainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n\n# model training\nmodel.fit(trainX, trainY, eval_set=[(trainX, trainY)], early_stopping_rounds=100)\n\n# feature importance\nimportance = model.get_feature_importance()\nranking = np.argsort(-importance)\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.barplot(x=importance[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()","390eaba8":"# categorical_features_indices = np.where(X_train.dtypes != np.float)[0]\nnumeric_features_indices = np.where(X_train.dtypes == np.float)[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 20))\nax = ax.flatten()\nsns.barplot(x=importance[categorical_features_indices], \n            y=X_train.columns.values[categorical_features_indices], \n            orient='h', ax=ax[0])\nax[0].set_xlabel(\"feature importance\")\nax[0].set_xlim([0, 50])\nax[0].set_title(\"categorical features\")\nsns.barplot(x=importance[numeric_features_indices], \n            y=X_train.columns.values[numeric_features_indices], \n            orient='h', ax=ax[1])\nax[1].set_xlabel(\"feature importance\")\nax[1].set_xlim([0, 50])\nax[1].set_title(\"numeric features\")\nplt.tight_layout()","032597ae":"def get_cdf_df(yards_array):\n    pdf, edges = np.histogram(yards_array, bins=199,\n                 range=(-99,100), density=True)\n    cdf = pdf.cumsum().clip(0, 1)\n    cdf_df = pd.DataFrame(data=cdf.reshape(-1, 1).T, \n                            columns=['Yards'+str(i) for i in range(-99,100)])\n    return cdf_df\ncdf = get_cdf_df(y_train).values.reshape(-1,)\n\ndef get_score(y_pred,cdf,w,dist_to_end):\n    y_pred = int(y_pred)\n    if y_pred ==w:\n        y_pred_array = cdf.copy()\n    elif y_pred - w >0:\n        y_pred_array = np.zeros(199)\n        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n    elif w - y_pred >0:\n        y_pred_array = np.ones(199)\n        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n    y_pred_array[-1]=1\n    y_pred_array[(dist_to_end+99):]=1\n    return y_pred_array    \n\ndef get_score_pingyi1(y_pred,y_true,cdf,w,dist_to_end):\n    y_pred = int(y_pred)\n    if y_pred ==w:\n        y_pred_array = cdf.copy()\n    elif y_pred - w >0:\n        y_pred_array = np.zeros(199)\n        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n    elif w - y_pred >0:\n        y_pred_array = np.ones(199)\n        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n    y_pred_array[-1]=1\n    y_pred_array[(dist_to_end+99):]=1\n    y_true_array = np.zeros(199)\n    y_true_array[(y_true+99):]=1\n    return np.mean((y_pred_array - y_true_array)**2)\n\n\ndef CRPS_pingyi1(y_preds,y_trues,w,cdf,dist_to_ends):\n    if len(y_preds) != len(y_trues):\n        print('length does not match')\n        return None\n    n = len(y_preds)\n    tmp = []\n    for a,b,c in zip(y_preds, y_trues,dist_to_ends):\n        tmp.append(get_score_pingyi1(a,b,cdf,w,c))\n    return np.mean(tmp)","12577c13":"n_splits = 2\nkf=KFold(n_splits = n_splits)\nresu1 = 0\nimpor1 = 0\nresu2_cprs = 0\nresu3_mae=0\n##y_pred = 0\nstack_train = np.zeros([X_train.shape[0],])\nmodels = []\nfor train_index, test_index in kf.split(X_train, y_train):\n    # split\n    X_train2= X_train.iloc[train_index,:]\n    y_train2= y_train.iloc[train_index]\n    X_test2= X_train.iloc[test_index,:]\n    y_test2= y_train.iloc[test_index]\n    \n    # catboost\n    model = CatBoostRegressor(iterations=2000, \n                          learning_rate = 0.01,\n                          use_best_model = True,\n                          eval_metric = 'RMSE',\n                          loss_function = 'RMSE',\n                          cat_features = categorical_features_indices,\n                          boosting_type = 'Ordered', # or Plain\n                          verbose = 0)   \n    model.fit(X_train2,y_train2,eval_set = [(X_train2,y_train2),(X_test2,y_test2)], early_stopping_rounds=100)\n    \n    # cv\n    models.append(model)\n    temp_predict = model.predict(X_test2)\n    stack_train[test_index] = temp_predict\n    ##y_pred += clf.predict(X_test)\/5\n    mse = mean_squared_error(y_test2, temp_predict)\n    crps = CRPS_pingyi1(temp_predict,y_test2,4,cdf,dist_to_end_train.iloc[test_index])\n    mae = mean_absolute_error(y_test2, temp_predict)\n    print(crps)\n    \n    resu1 += mse\/n_splits\n    resu2_cprs += crps\/n_splits\n    resu3_mae += mae\/n_splits \n    impor1 += model.feature_importances_\/n_splits\n    gc.collect()\nprint('mean mse:',resu1)\nprint('oof mse:',mean_squared_error(y_train,stack_train))\nprint('mean mae:',resu3_mae)\nprint('oof mae:',mean_absolute_error(y_train,stack_train))\nprint('mean cprs:',resu2_cprs)\nprint('oof cprs:',CRPS_pingyi1(stack_train,y_train,4,cdf,dist_to_end_train))","1173e22b":"env = nflrush.make_env()","a1412cc3":"for (test_df, sample_prediction_df) in env.iter_test():\n    \n    test_df[\"HomeTeamAbbr\"] = test_df[\"HomeTeamAbbr\"].map(map_abbr)\n    test_df[\"VisitorTeamAbbr\"] = test_df[\"VisitorTeamAbbr\"].map(map_abbr)\n    test_df[\"Possession\"] = test_df[\"PossessionTeam\"].map(map_abbr)\n    test_single = test_df.copy()\n    test_single['own_field'] = 1 * (test_single['FieldPosition'] == test_single['PossessionTeam'])\n    dist_to_end_test = test_single.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)\n    X_test = preprocess(test_df)\n    X_test['is_run'] = X_test.NflId == X_test.NflIdRusher\n    X_test = X_test[features]    \n    for c in categorical_features_indices:\n        X_test[features[c]] = X_test[features[c]].fillna(\"nan\")\n    pred_value = 0\n    for model in models:\n        pred_value += model.predict(X_test)[0]\/n_splits\n    pred_data = list(get_score(pred_value,cdf,4,dist_to_end_test.values[0]))\n    pred_data = np.array(pred_data).reshape(1,199)\n    pred_target = pd.DataFrame(index = sample_prediction_df.index, \\\n                               columns = sample_prediction_df.columns, \\\n                               #data = np.array(pred_data))\n                               data = pred_data)\n    #print(pred_target)\n    env.predict(pred_target)\nenv.write_submission_file()\n    ","a4277b4b":"# kf=KFold(n_splits = 5)\n# resu1 = 0\n# impor1 = 0\n# ##y_pred = 0\n# stack_train = np.zeros([X_train.shape[0],])\n# for train_index, test_index in kf.split(X_train, y_train):\n#     X_train2= X_train.iloc[train_index,:]\n#     y_train2= y_train.iloc[train_index]\n#     X_test2= X_train.iloc[test_index,:]\n#     y_test2= y_train.iloc[test_index]\n#     clf = lgb.LGBMRegressor(n_estimators=10000, random_state=47,subsample=0.7,\n#                              colsample_bytree=0.7,learning_rate=0.03,importance_type = 'gain',\n#                      max_depth = -1, num_leaves = 256,min_child_samples=20,min_split_gain = 0.001,\n#                        bagging_freq=1,reg_alpha = 0,reg_lambda = 0,n_jobs = -1)\n#     clf.fit(X_train2,y_train2,eval_set = [(X_train2,y_train2),(X_test2,y_test2)],early_stopping_rounds=100,verbose=50)\n#     temp_predict = clf.predict(X_test2)\n#     stack_train[test_index] = temp_predict\n#     ##y_pred += clf.predict(X_test)\/5\n#     mse = mean_squared_error(y_test2, temp_predict)\n#     print(mse)\n#     resu1 += mse\/5\n#     impor1 += clf.feature_importances_\/5\n#     gc.collect()","18803626":"The following submission code is based on https:\/\/www.kaggle.com\/newbielch\/lgbm-regression-view.","8f1587bb":"## CatBoost","8e9ad75b":"\"Season\" and \"Yardline\" are relatively importanct, but overall it seems that categorical variables in this dataset are not very important for the prediction...","1b05a310":"\n### There are so many useless categorical features!\n\nThere are many categorical variables in this dataset, and I heard that lots of people do not use them because these variables do not seem to contribute to the prediction very much. I wonder if this is due to the way we treat these categorical variables ... there are actually a number of ways to deal with categorical columns such as label encoding, one-hot encoding, target encoding, and so on. Also we might want to seek for important interactions between these categorical variables.\n\nIt is exhaustive to try all kinds of encoding for categorical data, yet what we can try is to use CatBoost to see what this model can do. CatBoost automatically performs target encoding for categorical data and tries to estimate interactions between them. So it may be worthwhile looking into what CatBoost has to say about our categorical variables. ","565b04b7":"If we split the plot into one with categorical and the other with numeric..."}}