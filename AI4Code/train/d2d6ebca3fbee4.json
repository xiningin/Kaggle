{"cell_type":{"4a2d1593":"code","ee17bc13":"code","2ac1fc86":"code","cdca9f18":"code","23ca1c4b":"code","7e2b54cf":"code","cf77dabf":"code","0f03598d":"code","bd2c1e04":"code","ef0dea0f":"code","e947abcf":"code","c4f9c04a":"code","c8e38723":"code","939c8637":"markdown","0a4daa65":"markdown","566ac442":"markdown","6d7f1ed4":"markdown","b757a6ab":"markdown","30abed21":"markdown","a67c914a":"markdown","53827e16":"markdown","1eafa7cf":"markdown","9c64ddde":"markdown","5fe0ee4c":"markdown","0f0324f3":"markdown","2a126cd5":"markdown","d63ccf81":"markdown","8406a83d":"markdown","ef7a7c67":"markdown"},"source":{"4a2d1593":"# IMPORT LIBRARIES\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport csv\nimport os\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\nfrom itertools import zip_longest\nimport tensorflow as tf\nimport sklearn.metrics as m\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.mobilenet import MobileNet","ee17bc13":"\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nY_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels=[\"label\"], axis=1)\n# Normalize the data\nX_train = X_train \/ 255.0\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1, 28, 28, 1)\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_true=Y_train\nY_train = to_categorical(Y_train, num_classes=10)","2ac1fc86":"random_seed = 2\nimg_train, X_test, label_train, Y_test= train_test_split(X_train , Y_train, test_size=0.3, random_state=random_seed)\nx_train, x_val, y_train, y_val= train_test_split(img_train , label_train, test_size=0.1, random_state=random_seed)\n\nY_train_splits= np.split(label_train,3)\nX_train_splits = np.split(img_train, 3)\nprint(\"Total Rows(Images) in Test Data\")\nprint(X_test.shape)\nprint(\"Total Rows(Images) in Train Data\")\nprint(img_train.shape)\nfor i in range(3):\n\n    Y_train_split=Y_train_splits[i]\n    pickle_out=open(\"Y_train_split_\"+str(i+1)+\".pickle\",\"wb\")\n    pickle.dump(Y_train_split,pickle_out)\n    pickle_out.close()\n\n    X_train_split = X_train_splits[i]\n    print(\"Total Rows(Images) in Bag \"+str(i+1))\n    print(X_train_splits[i].shape)\n    pickle_out2 = open(\"X_train_split_\" + str(i + 1) + \".pickle\", \"wb\")\n    pickle.dump(X_train_split, pickle_out2)\n    pickle_out2.close()","cdca9f18":"pickle_in = open(\"Y_train_split_1.pickle\", \"rb\")\nY_train_split = pickle.load(pickle_in)\npickle_in2 = open(\"X_train_split_1.pickle\", \"rb\")\nX_train_split = pickle.load(pickle_in2)\n\n# Set the random seed\nrandom_seed = 2\n# Split the train and the validation 50% set for the fitting\nX_train_B1, X_val_B1, Y_train_B1, Y_val_B1= train_test_split(X_train_split, Y_train_split, test_size=0.1, random_state=random_seed)\n","23ca1c4b":"pickle_in = open(\"Y_train_split_2.pickle\", \"rb\")\nY_train_split = pickle.load(pickle_in)\npickle_in2 = open(\"X_train_split_2.pickle\", \"rb\")\nX_train_split = pickle.load(pickle_in2)\n\n# Set the random seed\nrandom_seed = 2\n# Split the train and the validation 50% set for the fitting\nX_train_B2, X_val_B2, Y_train_B2, Y_val_B2= train_test_split(X_train_split, Y_train_split, test_size=0.1, random_state=random_seed)","7e2b54cf":"pickle_in = open(\"Y_train_split_3.pickle\", \"rb\")\nY_train_split = pickle.load(pickle_in)\npickle_in2 = open(\"X_train_split_3.pickle\", \"rb\")\nX_train_split = pickle.load(pickle_in2)\n\n# Set the random seed\nrandom_seed = 2\n# Split the train and the validation 50% set for the fitting\nX_train_B3, X_val_B3, Y_train_B3, Y_val_B3= train_test_split(X_train_split, Y_train_split, test_size=0.1, random_state=random_seed)","cf77dabf":"def model_defination():\n\n    # Set the CNN model\n    # my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\n    model= Sequential()\n\n    model.add(Conv2D(filters=16, kernel_size=(7, 7), strides=(2,2), padding='Same',\n                     activation='relu', kernel_initializer='he_normal' ,input_shape=(28, 28, 1)))\n\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n                     activation='relu'))\n    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.10))\n    model.add(Flatten())\n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation=\"softmax\"))\n\n    # Define the optimizer\n    optimizer = RMSprop(lr=0.0001)\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    return model","0f03598d":"model=model_defination()\nprint(\"Starting Training... \")\nprint(\"Training custom CNN on complete train data... \")\nprint(\"Total Rows(Images) in Train Data\")\nprint(x_train.shape)\nprint(\"Total Labels in Train Data\")\nprint(y_train.shape)\nhistory = model.fit(x_train, y_train, \n                    epochs=6, \n                    batch_size=128\n                    ,validation_data=(x_val, y_val))\nprint(\"Evaluating model on Testing Data\")\nresults = model.evaluate(X_test, Y_test, batch_size=128)\n\nprint(\"test loss, test acc:\", results)","bd2c1e04":"model=model_defination()\nprint(\"Starting Training... \")\nprint(\"Training custom CNN on Bag 1... \")\nprint(\"Total Rows(Images) in Bag 1\")\nprint(X_train_B1.shape)\nprint(\"Total Labels in Bag 1\")\nprint(Y_train_B1.shape)\nhistory = model.fit(X_train_B1, Y_train_B1, \n                    epochs=5, \n                    batch_size=128\n                    ,validation_data=(X_val_B1, Y_val_B1))\nmodel.save('custom_B1')\nfor i in range(len(X_train_B1)-1):\n    model_predict=model.predict( np.array( [X_train_B1[i],] ))\n    pred_classes = np.argmax(model_predict)\n    true=np.argmax(Y_train_B1[i])\n    if true != pred_classes:\n        X_train_B2=np.append(arr=X_train_B2, values=[X_train_B1[i]], axis=0)\n        Y_train_B2=np.append(arr=Y_train_B2, values=[Y_train_B1[i]], axis=0)\n\nprint(X_train_B2.shape)\nprint(Y_train_B2.shape)\n\npickle_out=open(\"X_train_B2.pickle\",\"wb\")\npickle.dump(X_train_B2,pickle_out)\npickle_out.close()\n\npickle_out2 = open(\"Y_train_B2.pickle\", \"wb\")\npickle.dump(Y_train_B2, pickle_out2)\npickle_out2.close()","ef0dea0f":"pickle_in = open(\"X_train_B2.pickle\", \"rb\")\nX_train_B2 = pickle.load(pickle_in)\npickle_in2 = open(\"Y_train_B2.pickle\", \"rb\")\nY_train_B2 = pickle.load(pickle_in2)","e947abcf":"model=tf.keras.models.load_model('custom_B1')\nprint(\"Starting Training... \")\nprint(\"Training custom CNN on Bag 2... \")\nprint(\"Total Rows(Images) in Bag 2\")\nprint(X_train_B2.shape)\nprint(\"Total Labels in Bag 2\")\nprint(Y_train_B2.shape)\nhistory = model.fit(X_train_B2, Y_train_B2, \n                    epochs=5, \n                    batch_size=128\n                    ,validation_data=(X_val_B2, Y_val_B2))\nmodel.save('custom_B2')\nfor i in range(len(X_train_B2)-1):\n    model_predict=model.predict( np.array( [X_train_B2[i],] ))\n    pred_classes = np.argmax(model_predict)\n    #print(pred_classes)\n    true=np.argmax(Y_train_B2[i])\n    if true != pred_classes:\n        X_train_B3=np.append(arr=X_train_B3, values=[X_train_B2[i]], axis=0)\n        Y_train_B3=np.append(arr=Y_train_B3, values=[Y_train_B2[i]], axis=0)\n\nprint(X_train_B3.shape)\nprint(Y_train_B3.shape)\n\npickle_out=open(\"X_train_B3.pickle\",\"wb\")\npickle.dump(X_train_B3,pickle_out)\npickle_out.close()\n\npickle_out2 = open(\"Y_train_B3.pickle\", \"wb\")\npickle.dump(Y_train_B3, pickle_out2)\npickle_out2.close()","c4f9c04a":"pickle_in = open(\"X_train_B3.pickle\", \"rb\")\nX_train_B3 = pickle.load(pickle_in)\npickle_in2 = open(\"Y_train_B3.pickle\", \"rb\")\nY_train_B3 = pickle.load(pickle_in2)","c8e38723":"model=tf.keras.models.load_model('custom_B2')\nprint(\"Starting Training... \")\nprint(\"Training custom CNN on Bag 3... \")\nprint(\"Total Rows(Images) in Bag 3\")\nprint(X_train_B3.shape)\nprint(\"Total Labels in Bag 2\")\nprint(Y_train_B3.shape)\nhistory = model.fit(X_train_B3, Y_train_B3, \n                    epochs=5, \n                    batch_size=128\n                    ,validation_data=(X_val_B3, Y_val_B3))\nprint(\"Evaluating model on Testing Data\")\nresults = model.evaluate(X_test, Y_test, batch_size=128)\nprint(\"test loss, test acc:\", results)","939c8637":"# Train custom Model\n<font size=\"4\"> To comapare the our approach we first train our Neural Network on Complete **train** data in a Normal fashion. After training the model we **evaluate** on **testing** data <\/font>","0a4daa65":"# Load Model\n<font size=\"4\"> As we want to convert weak learners to strong ones. We need the knowledge of previous models, so we will use transfer learning approach. Similar to previous step we will we include these instances (Images) in Bag 3 <\/font>","566ac442":"# Prepare data\n<font size=\"4\"> \n1. Load data from csv file\n2. Get train lables to list(Y_train) \n3. Get train images to list(X_train)\n4. Normalize data to get number between 0 and 1\n5. Reshape image of **784** vector to **28 x 28** vector\n6. Encode lable to one hot vector. <\/font>","6d7f1ed4":"# Our approach\n\n<font size=\"4\">Now to apply our approach,\n1. First we need to **train** our neural network on **Bag 1** (training data)\n2. In next step we will **evaluate** our model on **Bag 1** (training data)\n3. Find out the instances which are **wrongly** predicted\n4. To increase the weight of these instances (Images) we include these instances (Images) in **Bag 2**\n5. By using this approach the model will quickly shift its **decision boundary** to instances (Images) which are misclassified to correct the errors of the previous model <\/font>","b757a6ab":"# Read Bags 3 as B3 (as explained above)","30abed21":"# Read Bags 1 as B1\n![](https:\/\/media.istockphoto.com\/vectors\/an-empty-burlap-sack-isolated-on-white-background-vector-illustration-vector-id1023330618?b=1&k=6&m=1023330618&s=612x612&w=0&h=Yuhxa8i9tcDBVlEfDW1dHxurBwWNOw8nKP9ZB0VQmG8=)\n<font size=\"4\">\n1. Read bag 1 using pickle.\n2. Split each bag in 10% validation an 90% of training data. \n3. **X_train_B1, X_val_B1, Y_train_B1, Y_val_B1**<\/font>\n","a67c914a":"<font size=\"5\">Question- We train 3 models on 5 epochs does it mean 5 x 3=15 epoch of the orignal model?<\/font>","53827e16":"<font size=\"5\">Question- May this not lead to overfitting of the model?<\/font>","1eafa7cf":"# **Digit Recognizer: Using ensemble Learning Boosting** \n![](https:\/\/miro.medium.com\/max\/1000\/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png)","9c64ddde":"# Final Model\n<font size=\"4\"> To train a final model we will load model train on Bag 2 and apply transfer learning on Bag 3 which include instances wrongly classified by model 2. At last we will Test model on Testing data , You can clearly see it increase the accuracy even it is train on only 5 epochs where as original model was train on 6 epochs <\/font>","5fe0ee4c":"# Read Bag 3","0f0324f3":"# Import Libaries","2a126cd5":"# Read Bag 2","d63ccf81":"# Make Bags\n<font size=\"4\"> \n1. Split labels and images to **30%** test and **70%** training data\n2. Split lables and images to **3** equal bags\n3. Save lables and images **Bags** in form of Pickle. <\/font>\n\n![](https:\/\/www.smartfile.com\/blog\/wp-content\/uploads\/2015\/11\/python-pickle-800x200.png)","8406a83d":"# Read Bags 2 as B2 (as explained above)","ef7a7c67":"# Define a coustom Neural Network \n![](https:\/\/wp.wwu.edu\/machinelearning\/files\/2017\/02\/dnn-28vmlum.png)\n"}}