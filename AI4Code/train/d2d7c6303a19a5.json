{"cell_type":{"f44e07e4":"code","0f7ee9aa":"code","ca28aaf7":"code","62864d8d":"code","57a959d6":"code","6bc992cc":"code","43f7461a":"code","d1b4fc72":"code","5fd6524b":"code","e87d66bd":"code","a89d14f6":"code","e7068eb9":"code","a2715134":"code","c08cf43c":"code","5c6dc35f":"code","932c2e11":"code","8ee40768":"code","52c57672":"code","a7515fa3":"code","34f32f7e":"code","c08b155a":"code","3d16533c":"code","b064d8ab":"code","0508bbeb":"code","b31b7fb7":"code","67d0f581":"code","c0228ecb":"code","d2c2737a":"code","96213e37":"code","ee848418":"markdown","3084df63":"markdown","4cfbaff8":"markdown","5e4f6d9c":"markdown","74c47516":"markdown","bbf35815":"markdown","7d3d31a3":"markdown","ad524681":"markdown","4c2d8652":"markdown","6fd006d3":"markdown","1283920b":"markdown","9f818993":"markdown","6f835930":"markdown","f62b253e":"markdown","4d2e668f":"markdown","70fc5390":"markdown","16888464":"markdown","214ef445":"markdown","9c41e91b":"markdown","43c2ed3e":"markdown","7eab6ece":"markdown","08b2c5c9":"markdown","95d51f40":"markdown"},"source":{"f44e07e4":"#Importing the pandas for data processing and numpy for numerical computing\nimport numpy as np\nimport pandas as pd","0f7ee9aa":"# Importing the Boston Housing dataset from the sklearn\nfrom sklearn.datasets import load_boston\nboston = load_boston()","ca28aaf7":"#Converting the data into pandas dataframe\ndata = pd.DataFrame(boston.data)","62864d8d":"#First look at the data\ndata.head()","57a959d6":"#Adding the feature names to the dataframe\ndata.columns = boston.feature_names","6bc992cc":"#Adding the target variable to the dataset\ndata['PRICE'] = boston.target ","43f7461a":"#Looking at the data with names and target variable\ndata.head()","d1b4fc72":"#Shape of the data\nprint(data.shape)","5fd6524b":"#Checking the null values in the dataset\ndata.isnull().sum()","e87d66bd":"#Checking the statistics of the data\ndata.describe()","a89d14f6":"data.info()","e7068eb9":"#checking the distribution of the target variable\nimport seaborn as sns\nsns.distplot(data.PRICE)","a2715134":"#Distribution using box plot\nsns.boxplot(data.PRICE)","c08cf43c":"#checking Correlation of the data \ncorrelation = data.corr()\ncorrelation.loc['PRICE']","5c6dc35f":"# plotting the heatmap\nimport matplotlib.pyplot as plt\nfig,axes = plt.subplots(figsize=(15,12))\nsns.heatmap(correlation,square = True,annot = True)","932c2e11":"# Checking the scatter plot with the most correlated features\nplt.figure(figsize = (20,5))\nfeatures = ['LSTAT','RM','PTRATIO']\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = data[col]\n    y = data.PRICE\n    plt.scatter(x, y, marker='o')\n    plt.title(\"Variation in House prices\")\n    plt.xlabel(col)\n    plt.ylabel('\"House prices in $1000\"')","8ee40768":"#X = data[['LSTAT','RM','PTRATIO']]\nX = data.iloc[:,:-1]\ny= data.PRICE","52c57672":"# Splitting the data into train and test for building the model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 4)","a7515fa3":"#Linear Regression \nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()","34f32f7e":"#Fitting the model\nregressor.fit(X_train,y_train)","c08b155a":"#Prediction on the test dataset\ny_pred = regressor.predict(X_test)","3d16533c":"# Predicting RMSE the Test set results\nfrom sklearn.metrics import mean_squared_error\nrmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(rmse)","b064d8ab":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(r2)","0508bbeb":"#Scaling the dataset\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b31b7fb7":"#Creating the neural network model\nimport keras\nfrom keras.layers import Dense, Activation,Dropout\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\nmodel.add(Dense(128,activation  = 'relu',input_dim =13))\nmodel.add(Dense(64,activation  = 'relu'))\nmodel.add(Dense(32,activation  = 'relu'))\nmodel.add(Dense(16,activation  = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer = 'adam',loss = 'mean_squared_error')","67d0f581":"model.fit(X_train, y_train, epochs = 100)","c0228ecb":"y_pred = model.predict(X_test)","d2c2737a":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(r2)","96213e37":"# Predicting RMSE the Test set results\nfrom sklearn.metrics import mean_squared_error\nrmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(rmse)","ee848418":"By looking at the correlation plot LSAT is negatively correlated with -0.75 and RM is positively correlated to the price and PTRATIO is correlated negatively with -0.51","3084df63":"<a id = 'build'><\/a>\n### Building the Model ","4cfbaff8":"In this notebook,we will develop, evaluate and compare the performance and predictive power of a simple linear regression model and neural network on boston housing price data\n\nHere the target is to determine the price of the property based on the features","5e4f6d9c":"* <strong>If you liked the worked, an upvote will be nice!<\/strong>\n* <strong>Any suggestions and feedback are always welcome<\/strong>\n* <strong>If you have any doubt, let's dicuss in the comments<\/strong>","74c47516":"### First look at the dataset","bbf35815":"This is sometimes very useful, for example if you look at the CRIM the max is 88.97 and 75% of the value is below 3.677083 and mean is 3.613524 so it means the max values is actually an outlier or there are outliers present in the column","7d3d31a3":"* <strong>If you liked the worked, an upvote will be nice!<\/strong>\n* <strong>Any suggestions and feedback are always welcome<\/strong>\n* <strong>If you have any doubt, let's dicuss in the comments<\/strong>","ad524681":"<a id = 'eval'><\/a>\n### Evaluation of the model","4c2d8652":"No null values in the dataset, no missing value treatement needed","6fd006d3":"<a id  = 'NN'><\/a>\n## Neural Networks","1283920b":"<a id = 'split'><\/a>\n### Splitting the dependent feature and independent feature ","9f818993":"<a id = 'library'><\/a>\n# Importing libraries and the dataset\nUnlike the conventional way, I import the library when it is needed. It will actually help you to understand where the application of the class and it's function is used","6f835930":"<a id = 'intro'><\/a>\n# Introduction","f62b253e":"<a id = 'valid'><\/a>\n### Splitting the data for Model Validation ","4d2e668f":"<a id = 'data'><\/a>\n# About the Dataset","70fc5390":"* We are using Keras for developing the neural network.\n* Models in Keras are defined as a sequence of layers\n* We create a Sequential model and add layers one at a time with activation function\n* Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.The activation we are using is relu\n* As this is a regression problem, the output layer has no activation function\n* Elements of neural network has input layer, hidden layer and output layer\n* input layer:- This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer.\n* Hidden layer:-  Nodes of this layer are not exposed to the outer world, they are the part of the abstraction provided by any neural network. Hidden layer performs all sort of computation on the features entered through the input layer and transfer the result to the output layer.\n* Output layer:- This layer bring up the information learned by the network to the outer world.\n* Model Compilation:- The compilation is the final step in creating a model. Once the compilation is done, we can move on to training phase.\n* Optimizer : - The optimizer we are using is adam. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n* Loss - mean square error","16888464":"*  [Introduction](#intro)\n* [About the dataset](#data)\n* [Importing the library](#library)\n* [First look at the dataset](#datalook)\n* [Visualisation](#visual)\n* [Correlation](#corr)\n* [Splitting the data](#split)\n* [Model Validation](#valid)\n* [Building Linear Regression model](#build)\n   * [Model Evaluation](#evaluate) \n* [Neural Network](#NN)\n   * [Model Evaluation](#eval)\n* [Conclusion](#conclude)","214ef445":"<a id = 'corr'><\/a>\n### Checking the correlation of the independent feature with the dependent feature\n\nCorrelation is a statistical technique that can show whether and how strongly pairs of variables are related.An intelligent correlation analysis can lead to a greater understanding of your data","9c41e91b":"The dataset used in this project comes from the UCI Machine Learning Repository. This data was collected in 1978 and each of the 506 entries represents aggregate information about 14 features of homes from various suburbs located in Boston.\n\nThe features can be summarized as follows:\n* CRIM: This is the per capita crime rate by town\n* ZN: This is the proportion of residential land zoned for lots larger than 25,000 sq.ft.\n* INDUS: This is the proportion of non-retail business acres per town.\n* CHAS: This is the Charles River dummy variable (this is equal to 1 if tract bounds river; 0 otherwise)\n* NOX: This is the nitric oxides concentration (parts per 10 million)\n* RM: This is the average number of rooms per dwelling\n* AGE: This is the proportion of owner-occupied units built prior to 1940\n* DIS: This is the weighted distances to five Boston employment centers\n* RAD: This is the index of accessibility to radial highways\n* TAX: This is the full-value property-tax rate per 1000 bucks\n* PTRATIO: This is the pupil-teacher ratio by town\n* B: This is calculated as 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of people of African American descent by town\n* LSTAT: This is the percentage lower status of the population\n* MEDV: This is the median value of owner-occupied homes in 1000s","43c2ed3e":"<a id = 'evaluate'><\/a>\n### Model Evaluation","7eab6ece":"The distribution seems normal, has not be the data normal we would have perform log transformation or took to square root of the data to make the data normal. Normal distribution is need for the machine learning for better predictiblity of the model","08b2c5c9":"<a id = 'visual'><\/a>\n# Visualisation","95d51f40":"<a id = 'conclude'><\/a>\n## Conclusion\n\nUsing a simple neural network, we were able to improve the model significantly. I encourage you to try alterating the hyperparameters of the model and see if you can get better model"}}