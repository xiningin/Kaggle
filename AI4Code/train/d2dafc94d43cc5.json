{"cell_type":{"3410ba36":"code","d0916881":"code","535ec75b":"code","0fe697b6":"code","6306bd88":"code","1b9ae60b":"code","fc990d4f":"code","b129558c":"code","71caabc0":"code","51ca473b":"code","160046c4":"code","61411cea":"code","6dd4fa8b":"code","df92f974":"code","6868d8c4":"code","7a2ca2ef":"code","afa00d07":"code","dab361a3":"code","cb02d3ba":"code","fd75dc11":"code","8d11b9a5":"code","0aa77fff":"code","cbeb8ac0":"code","574de83f":"code","9d3bb8bd":"code","a70e3776":"code","5aa9249a":"code","97fb6bb3":"code","5380611f":"code","90436f1e":"code","f67e1f8d":"code","42021d3a":"code","48529540":"code","322efc5f":"code","ba35954d":"code","1b85815c":"code","2007e088":"code","e4fe80d3":"code","0cc75703":"code","61aeb81f":"code","6eb71d91":"markdown","ee5eb787":"markdown","629555a8":"markdown","db435e92":"markdown","2204ff2c":"markdown","904a91d5":"markdown","dee7799b":"markdown","2ef58efb":"markdown","c4b0bb99":"markdown","116ddb3c":"markdown","c4069062":"markdown","b0c5e8b2":"markdown","1fd6479a":"markdown"},"source":{"3410ba36":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport os\nimport random\nfrom tqdm import tqdm\nimport cv2 as cv\nimport PIL\nfrom PIL import Image\n!pip install plotly\nimport plotly.express as px\nfrom IPython import display\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","d0916881":"base_dir = '\/kaggle\/input\/cryptopunks\/'\nos.listdir(base_dir)","535ec75b":"!pwd","0fe697b6":"data_dir = '..\/input\/cryptopunks\/txn_history-2021-10-07.jsonl'\nimage_dir = \"..\/input\/cryptopunks\/imgs\/imgs\"\nimage_root = \"..\/input\/cryptopunks\/imgs\"","6306bd88":"df = pd.read_json(base_dir + 'txn_history-2021-10-07.jsonl', lines=True)\ndf.head()","1b9ae60b":"#look at 100 samples\n\n#define number of rows and cols\nno_plots = 10*10\n\n#define path\nimages = glob.glob(\"..\/input\/cryptopunks\/imgs\/imgs\/*.png\")\n\nplt.rcParams['figure.figsize'] = (30, 30)\nplt.subplots_adjust(wspace=0, hspace=0)\n\nprint(\"Sample 100 CryptoPunks\")\nfor idx,image in enumerate(images[:no_plots]):\n    sample_img = cv.imread(image)\n    plt.subplot(10, 10, idx+1)\n    plt.axis('off')\n    plt.imshow(cv.cvtColor(sample_img,cv.COLOR_BGR2RGB)) #covert color space\nplt.show()","fc990d4f":"punks = [img for img in glob.glob(\"..\/input\/cryptopunks\/imgs\/imgs\/*.png\")]\n\nfor punk in punks[0:3]:\n    img = plt.imread(punk)\n    plt.imshow(img)\n    plt.show()","b129558c":"#check data summary\ndf.info()","71caabc0":"#create new dataframe with only useful columns\ndf = df[[\"txn_type\", \"date\", \"eth\", \"punk_id\", \"type\", \"accessories\"]]\ndf.head()","51ca473b":"#value counts\ndf['txn_type'].unique()","160046c4":"df['accessories'].explode().unique()","61411cea":"len(df['accessories'].explode().unique())","6dd4fa8b":"#exploded 'type' list to rows of the subset columns\ndf = df.explode(\"type\")\ndf.head()","df92f974":"#visualize\nfig = px.bar(df[df.txn_type == 'Sold'].groupby(\"type\").agg({\"eth\": \"max\"}).sort_values(by=\"eth\").reset_index('type'),\n             x=\"type\", \n             y=\"eth\", \n             color=\"type\", \n             title=\"CryptoPunk Types vs. Price\")\nfig.show()","6868d8c4":"#visualize human\nhuman = df[(df.txn_type == 'Sold') & ((df.type == \"Female\") | (df.type == \"Male\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\n\n#visualize alien\nalien = df[(df.txn_type == 'Sold') & ((df.type == \"Alien\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\n\n#visualize zombie\nzombie = df[(df.txn_type == 'Sold') & ((df.type == \"Zombie\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\n\n#visualize ape\nape = df[(df.txn_type == 'Sold') & ((df.type == \"Ape\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\n\nplt.figure(figsize=(20,10))\nplt.plot(human['date'], \n         human['eth']['median'], \n         label=\"Human Median Eth\")\n\nplt.plot(alien['date'], \n         alien['eth']['median'], \n         label=\"Alien Median Eth\")\n\nplt.plot(zombie['date'], \n         zombie['eth']['median'], \n         label=\"Zombie Median Eth\")\n\nplt.plot(ape['date'], \n         ape['eth']['median'], \n         label=\"Ape Median Eth\")\n\nplt.legend()\nplt.xticks(rotation=60)\nplt.title(\"Median Eth Price for Punks Sold Over Time by Type\")\nplt.show()","7a2ca2ef":"#get number of attributes\ndf['num_attributes'] = df.accessories.apply(lambda x: len(x))","afa00d07":"#compare number of attributes vs. price\nfig = px.bar(df[(df.txn_type == \"Sold\") & ((df.type == \"Female\") | (df.type == \"Male\"))].groupby(\"num_attributes\").agg({\"eth\": \"mean\"}).reset_index(\"num_attributes\"),\n             x=\"num_attributes\", \n             y=\"eth\", \n             color=\"eth\", \n             title=\"CryptoPunk Price per Number of Attributes of Human Punks\")\nfig.show()","dab361a3":"#visualize number of attributes vs price for alien\nfig = px.bar(df[(df.txn_type == \"Sold\") & ((df.type == \"Alien\"))].groupby(\"num_attributes\").agg({\"eth\": \"mean\"}).reset_index(\"num_attributes\"),\n             x=\"num_attributes\", \n             y=\"eth\", \n             color=\"eth\", \n             title=\"CryptoPunk Price per Number of Attributes of Alien Punks\")\nfig.show()","cb02d3ba":"#visualize number of attributes vs price for zombie\nfig = px.bar(df[(df.txn_type == \"Sold\") & ((df.type == \"Zombie\"))].groupby(\"num_attributes\").agg({\"eth\": \"mean\"}).reset_index(\"num_attributes\"),\n             x=\"num_attributes\", \n             y=\"eth\", \n             color=\"eth\", \n             title=\"CryptoPunk Price per Number of Attributes of Zombie Punks\")\nfig.show()","fd75dc11":"#visualize number of attributes vs price for ape\nfig = px.bar(df[(df.txn_type == \"Sold\") & ((df.type == \"Ape\"))].groupby(\"num_attributes\").agg({\"eth\": \"mean\"}).reset_index(\"num_attributes\"),\n             x=\"num_attributes\", \n             y=\"eth\", \n             color=\"eth\", \n             title=\"CryptoPunk Price per Number of Attributes of Ape Punks\")\nfig.show()","8d11b9a5":"#display tensor image\ndef tensor_imshow(img, dnorm=True):\n    img = img.to('cpu')\n    npimg = img.detach().numpy()\n    if dnorm:\n        npimg = npimg*0.5+0.5\n    plt.figure(figsize=(3, 3))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","0aa77fff":"def get_dataloader(batch_size,           #batch size during training\n                   image_size,           #spatial size of training images\n                   data_dir=image_dir,   #root directory for dataset\n                   num_workers=3):       #number of sub-processes\n    \n    stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \n    \n    #create transformer to transform images\n    transform = transforms.Compose([transforms.Resize((image_size, image_size)),  #resize\n                                    transforms.ToTensor(),                        #convert to tensor\n                                    transforms.Normalize(*stats)])                #normalize to be between -1 and 1\n    \n    #create the dataset\n    dataset = datasets.ImageFolder(root=data_dir,\n                                   transform=transform)\n    \n    #create the dataloader\n    data_loader = torch.utils.data.DataLoader(dataset,\n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=num_workers,\n                                              pin_memory=True)\n    \n    return data_loader","cbeb8ac0":"#test dataloader\nbatch_size, image_size = 5, 24\n\ntrain_loader = get_dataloader(batch_size,\n                              image_size,\n                              image_root)\n\ndataiter = iter(train_loader) #dataloader is an iterator\n\nimg,_ = next(dataiter)\nsample_img = img[-1]\n\n#display tensor image\ntensor_imshow(sample_img)","574de83f":"class Generator(nn.Module):       #signals neural network\n    def __init__(self, \n                 z_dim=100,      #noise vector\n                 im_chan=3,      #color chanel, 3 for red green blue\n                 hidden_dim=64): #spatial size of feature map (conv)\n        \n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        self.im_chan = im_chan\n        self.hidden_dim = hidden_dim\n        \n        self.generator_cnn = nn.Sequential(self.make_gen_block(z_dim, hidden_dim*8, stride=1, padding=0),   \n                                           #(64*8) x 4 x 4\n                                           self.make_gen_block(hidden_dim*8, hidden_dim*4),                           \n                                           #(64*4) x 8 x 8\n                                           self.make_gen_block(hidden_dim*4, hidden_dim*2),                           \n                                           #(64*2) x 16 x 16\n                                           self.make_gen_block(hidden_dim*2, hidden_dim),                             \n                                           #(64) x 32 x 32\n                                           self.make_gen_block(hidden_dim, im_chan, final_layer=True))\n    \n    def make_gen_block(self, \n                       im_chan,     #image dimension\n                       op_chan,     #output dimension\n                       kernel_size=4, \n                       stride=2, \n                       padding=1, \n                       final_layer=False): \n        \n        layers = []\n        #de-convolutional layer\n        layers.append(nn.ConvTranspose2d(im_chan,     \n                                         op_chan, \n                                         kernel_size, \n                                         stride, \n                                         padding, \n                                         bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2))\n        else:\n            layers.append(nn.Tanh())\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self,noise):\n        x = noise.view(-1,self.z_dim,1,1)\n        return self.generator_cnn(x)\n\n    def get_noise(n_samples, \n                  z_dim, \n                  device='cpu'):\n        return torch.randn(n_samples, \n                           z_dim, \n                           device=device)","9d3bb8bd":"#test Generator\nnoise = Generator.get_noise(n_samples=5,\n                            z_dim=100)\n\ng = Generator(z_dim=100,\n              im_chan=3,\n              hidden_dim=64)","a70e3776":"print(g)","5aa9249a":"class Discriminator(nn.Module):\n    def __init__(self, \n                 im_chan=3,       #image channels, 3 for red green blue\n                 conv_dim=64,     #spatial dimension of feature map\n                 image_size=64):  #spatial size of training images\n        \n        super(Discriminator, self).__init__()\n        self.image_size = image_size\n        self.conv_dim = conv_dim\n        \n        self.disc_cnn = nn.Sequential(self.make_disc_block(im_chan, conv_dim),\n                                      self.make_disc_block(conv_dim, conv_dim*2),\n                                      self.make_disc_block(conv_dim*2, conv_dim*4),\n                                      self.make_disc_block(conv_dim*4, conv_dim*8),\n                                      #no need a sigmoid here since it is included in the loss function\n                                      self.make_disc_block(conv_dim*8, 1, padding=0, final_layer=True)) \n        \n        \n    def make_disc_block(self,\n                        im_chan,\n                        op_chan,\n                        kernel_size=4,\n                        stride=2,\n                        padding=1,\n                        final_layer=False):\n        layers = []\n        layers.append(nn.Conv2d(im_chan,\n                                op_chan,\n                                kernel_size,\n                                stride,\n                                padding,\n                                bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n        \n        return nn.Sequential(*layers)\n    \n    #given an image tensor, returns a 1-dimension tensor representing fake\/real\n    def forward(self,image):\n        pred = self.disc_cnn(image)\n        pred = pred.view(image.size(0),-1)\n        return pred\n    \n    def _get_final_feature_dimention(self):\n        final_width_height = (self.image_size \/\/  2**len(self.disc_cnn))**2\n        final_depth = self.conv_dim * 2**(len(self.disc_cnn)-1)\n        return final_depth*final_width_height","97fb6bb3":"#test Discriminator\nd = Discriminator(im_chan=3,\n                  conv_dim=64,\n                  image_size=64)","5380611f":"print(d)","90436f1e":"#custom weights initialization to randomly initialize all weights\n#mean=0, stdev=0.2\ndef weights_init_normal(m):\n    \n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02) \n        \n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)","f67e1f8d":"def real_loss(D_out,device='cpu'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    batch_size = D_out.size(0)\n    \n    #labels will be used when calculating the losses\n    #real labels = 1 and lable smoothing => 0.9\n    labels = torch.ones(batch_size, device=device)*0.9 \n    \n    loss = criterion(D_out.squeeze(), labels)\n    return loss","42021d3a":"def fake_loss(D_out, device='cpu'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    batch_size = D_out.size(0)\n    \n    #labels will be used when calculating the losses\n    #fake labels = 0\n    labels = torch.zeros(batch_size,\n                         device=device) \n    \n    loss = criterion(D_out.squeeze(), labels)\n    return loss","48529540":"def print_tensor_images(images_tensor):\n    \n    '''\n    Function for visualizing images: Given a tensor of images, prints the images.\n    '''\n        \n    plt.rcParams['figure.figsize'] = (15, 15)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    \n    images_tensor = images_tensor.to('cpu')\n    npimgs = images_tensor.detach().numpy()\n    \n    no_plots = len(images_tensor)\n\n    for idx,image in enumerate(npimgs):\n        plt.subplot(1, 8, idx+1)\n        plt.axis('off')\n        #dnorm\n        image = image * 0.5 + 0.5\n        plt.imshow(np.transpose(image, (1, 2, 0)))\n        \n    plt.show()","322efc5f":"def train(D, G, \n          n_epochs,\n          dataloader,\n          d_optimizer,\n          g_optimizer,\n          z_dim,\n          print_every=100,\n          device='cpu'):\n    \n    #to keep track of the generator\u2019s learning progression, \n    #we will generate a fixed batch of latent vectors that are drawn from a Gaussian distribution   \n    sample_size=8\n    fixed_z = Generator.get_noise(n_samples=sample_size,\n                                  z_dim=z_dim,\n                                  device=device)\n    \n    \n    \n    if os.path.exists(\"..\/input\/checkpoint\/ckpt_best_0.pth\"):\n#         path_checkpoint = log_dir  # \u65ad\u70b9\u8def\u5f84\n        checkpoint = torch.load(\"..\/input\/checkpoint\/ckpt_best_0.pth\")  # \u52a0\u8f7d\u65ad\u70b9\n#         d.load_state_dict(checkpoint['d'])  # \u52a0\u8f7d\u6a21\u578b\u53ef\u5b66\u4e60\u53c2\u53c2\u6570\\\n        g.load_state_dict(checkpoint['g'])  # \u52a0\u8f7d\u6a21\u578b\u53ef\u5b66\u4e60\u53c2\u53c2\u6570\\\n#         d_optimizer.load_state_dict(checkpoint['d_optimizer'])  # \u52a0\u8f7d\u4f18\u5316\u5668\u53c2\u6570\\\n        g_optimizer.load_state_dict(checkpoint['g_optimizer'])  # \u52a0\u8f7d\u4f18\u5316\u5668\u53c2\u6570\\\n        start_epoch = checkpoint['epoch']  # \u8bbe\u7f6e\u5f00\u59cb\u7684epoch \n    else:\n        start_epoch = 0\n        print('\u65e0\u4fdd\u5b58\u4e86\u7684\u6a21\u578b\uff0c\u5c06\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff01')\n\n    for epoch in range(start_epoch,n_epochs+1):\n        #use dataloader to fetch batches\n        for batch_i,(real_images,_) in enumerate(dataloader):\n            batch_size = real_images.size(0)\n            real_images = real_images.to(device)\n            \n            #Part 1: Train the Discriminator ========================================================\n            #goal: to maximize the probability of correctly classifying a given input as real or fake\n            \n            #zero out the gradients before backpropagation\n            d_optimizer.zero_grad()\n            \n            ##classify all-real batch\n            d_real_op = D(real_images) #average output (across the batch) of the discriminator\n            d_real_loss = real_loss(d_real_op,\n                                    device=device)\n            \n            #train with all-fake batch\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            fake_images = G(noise)\n            \n            #classify all-fake batch\n            d_fake_op = D(fake_images) #average output (across the batch) of the generator\n            d_fake_loss = fake_loss(d_fake_op,\n                                    device=device)\n            \n            #total loss\n            d_loss = d_real_loss + d_fake_loss\n            \n            #update gradients\n            d_loss.backward()\n            #update optimizer\n            d_optimizer.step()\n            \n            #Part 2: Train the Generator ==============================================================\n            #zero out the gradients before backpropagation\n            g_optimizer.zero_grad()\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            \n            #use discriminator to classify generator's output\n            g_out = G(noise)\n            d_out = D(g_out)\n            \n            g_loss = real_loss(d_out, \n                               device=device) \n            #update gradients\n            g_loss.backward()\n            #update optimizer\n            g_optimizer.step()\n        \n        print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch, \n                                                                               n_epochs, \n                                                                               d_loss.item(),  #keep track of loss\n                                                                               g_loss.item())) #keep track of loss\n        if (epoch % print_every == 0):\n            G.eval()\n            sample_image = G(fixed_z)\n            print_tensor_images(sample_image)\n            G.train()\n    # \u4fdd\u5b58\u6a21\u578b\n    checkpoint = {\n#             \"d\": d.state_dict(),\n            \"g\": g.state_dict(),\n            \"g_optimizer\":g_optimizer.state_dict(),\n#             \"d_optimizer\":d_optimizer.state_dict(),\n            'epoch': epoch\n        }\n    if not os.path.isdir(\".\/checkpoint\"):\n        os.mkdir(\".\/checkpoint\")\n    torch.save(checkpoint, '.\/checkpoint\/ckpt_best_0.pth')","ba35954d":"#hyperparameters\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device is \", device)\n\n#incorrect hyperparameter settings lead to mode collapse\n#we will follow Goodfellow\u2019s paper\nz_dim = 100       #noise\nbeta_1 = 0.5      #as specified in the original DCGAN paper\nbeta_2 = 0.999 \nlr = 0.0002       #as specified in the original DCGAN paper\nn_epochs = 1200\nbatch_size = 128\nimage_size = 64","1b85815c":"#initialize generator\n# generator = Generator(z_dim, \ng = Generator(z_dim,\n              im_chan=3, \n              hidden_dim=64).to(device)\n\n#initialize discriminator\n# discriminator = Discriminator(im_chan=3, \nd = Discriminator(im_chan=3, \n                  conv_dim=64, \n                  image_size=image_size).to(device)\n\n#setup Adam optimizers for generator\ng_optimizer = optim.Adam(g.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup Adam optimizers for discriminator\nd_optimizer = optim.Adam(d.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup dataloader\ndataloader = get_dataloader(batch_size, \n                            image_size, \n                            image_root)","2007e088":"#start training\n%time\nn_epochs = 1200\ntrain(d,\n      g,\n      n_epochs,\n      dataloader,\n      d_optimizer,\n      g_optimizer,\n      z_dim,\n      print_every=100,\n      device=device)","e4fe80d3":"plt.figure(figsize = (15, 8))\nplt.plot(noise)\nplt.title(\"Noise\")\nplt.show()","0cc75703":"# def save_model(generator,file_name):\n#     generator = generator.to('cuda')\n#     torch.save(generator.state_dict(),\"cryptopunks_generator.pth\")\n\n# save_model(generator,\"kaggle\")","61aeb81f":"#sample generation\ng.to(device)\ng.eval()       #eval mode\nsample_size=8\n\nfor i in range(8):    \n    \n    #generate latent vectors\n    fixed_z = Generator.get_noise(n_samples=sample_size, \n                                  z_dim=z_dim, \n                                  device=device)    \n    \n    #generate samples\n    sample_image = g(fixed_z)\n    \n    #display samples\n    print_tensor_images(sample_image)","6eb71d91":"# Training Procedure\n\n**Part 1\u200a-\u200aTrain the discriminator**\n\n* Generate fixed_z to get a fixed batch of latent vectors which we will use to keep track of the generator learning progression.\n* Train the discriminator with all-real batches of images: forward pass, calculate d_real_loss with real_loss() and calculate the gradients in a backward pass gradients in a backward pass with backward().\n* Use the generator to generate all-fake batches of images and use that to train the discriminator: forward pass, calculate d_fake_loss with fake_loss() and and calculate the gradients in a backward pass with backward()\n* Total d_loss with be the sum of d_real_loss and d_fake_loss.\n* Update d_optimizer with step().\n\n**Part 2\u200a-\u200aTrain the generator**\n\n* Use discriminator to classify generator's output g_out.\n* Computes generator's loss using real_loss().\n* Computes generator's gradients in a backward pass with backward().\n* Updates generator's optimizer with step().\n\nFor every mini-batch of data, we train the discriminator for one iteration, and then the generator for one iteration.","ee5eb787":"# Create New CryptoPunks","629555a8":"# Define Training Parameters","db435e92":"An [NFT](https:\/\/en.wikipedia.org\/wiki\/Non-fungible_token) is a Non Fungible Token, unique tokens that can be traded and exchanged on the [Ethereum blockchain](https:\/\/en.wikipedia.org\/wiki\/Ethereum).\n\n[CryptoPunks](https:\/\/www.larvalabs.com\/cryptopunks) is a collection of 10,000 unique collectible characters, each is a 24x24 pixel, 8-bit-style unique avatar, with proof of ownership stored on the Ethereum blockchain. Their creation began as an experiment, conducted by software developers Matt Hall and John Watkinson in 2017. The Cryptopunks were the inspiration for the [ERC-721](https:\/\/eips.ethereum.org\/EIPS\/eip-721) standard that powers most digital art and collectibles today.\n\nCryptoPunks is the very first NFT series to catch popular interest and one of the most actively traded today. As of October 2021, the market capitalization of all 10,000 CryptoPunks is estimated at around 1,600,00 ETH, or over 5B USD.\n\nIn this project, we will use DCGAN (Deep Convolutional Generative Adversarial Network) and train it on the CryptoPunks dataset to generate new punks. GAN belongs to a group of [unsupervised learning](https:\/\/www.ibm.com\/cloud\/learn\/unsupervised-learning#:~:text=Unsupervised%20learning,%20also%20known%20as,the%20need%20for%20human%20intervention) technique called [Generative Models](https:\/\/en.wikipedia.org\/wiki\/Generative_model). Generative Models are a type of machine learning models that are used to describe phenomena in data and enables computers to understand the real world. Generative Models and GANs have been ones of the most successful developments in the recent years in Computer Vision.","2204ff2c":"# Build Discriminator\n\nThe discriminator's training data comes from two sources:\n\n* real data: The discriminator uses these instances as positive examples during training. This is classified as 0.\n* fake data created by the generator: The discriminator uses these instances as negative examples during training. This is classified as 1.\n\nDuring discriminator training:\n\n* The discriminator takes in one input image and outputs a binary prediction to classify when the image is real or fake.\n* The discriminator_loss penalizes itself if it misclassifies the image. The total loss is the sum of the losses for real and fake images, discriminator_loss = real_loss + fake_loss.\n* The discriminator updates its weights through back-propagation from the discriminator loss through the discriminator network.","904a91d5":"# Introduction\n\n## Thanks for starter notebook [ZACH NUSSBAUM](https:\/\/www.kaggle.com\/zachnussbaum\/cryptopunks-exploratory-data-analysis)\n\n## Here is my [blog](https:\/\/baotramduong.medium.com\/generate-nft-cryptopunks-with-deep-convolutional-generative-adversarial-network-dcgan-db35f0a1adb4)","dee7799b":"## Weight Initialization\n\nAll model weights will be randomly initialized from a normal distribution with mean=0, stdev=0.2 according to Goodfellow (2014).\n\n* weights_init_normal: takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization.","2ef58efb":"# DataLoader & Preprocessing","c4b0bb99":"## Loss Functions\n\nNow we create loss functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves.","116ddb3c":"## Train Time!","c4069062":"# Build Generator\n\nDuring generator training:\n\n* The generator takes in a random input and output a single 64x64 colored image.\n* The image is upsampled: doubling in size and quadrupling in the area of activations each time it passes through the ConvTranspose2d layer.\n\nAs the result, the generator learns how to map the low dimensional image to the high dimensional image more and more effectively to fool the discriminator.","b0c5e8b2":"# DC-GAN Intuition\n\n* generator generates data for discriminator training using latent embedding (usually random noise)\n* discriminator distinguishes the generator's generated data from real data and penalizes the generator for fake data.\n* The two networks are set out to compete with each other i.e. \"adversarial\". Overtime, the generator gets better at generating images that are super close to real images and discriminator gets better at differentiating them.\n\nThe generator network takes random Gaussian noise and maps it into input images such that the discriminator cannot tell which images came from the dataset and which images came from the generator. The discriminator will be trained to learn to tell the difference between images comes from the dataset and images comes from the generator.\n\nGANs typically work with image data and use Convolutional Neural Networks, (CNNs). A CNN architecture is composed of convolutional layer with ReLU, pooling layer, and lastly fully connected Dense layers.","1fd6479a":"# Exploratory Data Analysis"}}