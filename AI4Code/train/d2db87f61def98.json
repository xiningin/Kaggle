{"cell_type":{"51092aeb":"code","40c5062b":"code","952b3867":"code","4cecd1cc":"code","3a2ac62d":"code","205ac61e":"code","74e0e06f":"code","ce138840":"code","068c5d37":"code","5e276ec4":"code","8033aaa7":"code","6af5fad4":"code","dd502fec":"code","e2e993f1":"code","45138ca9":"code","22fe86fa":"code","4563e86a":"code","4093de74":"code","f7ab0771":"code","bb47d16b":"code","e7513e4d":"code","e1ff6ef3":"code","72622352":"code","8d4ff63d":"code","5b2a1fdf":"code","df9fe83e":"code","a7d6c162":"code","878bc47a":"code","388a1669":"code","1af58a17":"code","12510047":"code","b5b9e2e9":"code","52384357":"code","3c1ef4fa":"code","edb39c7f":"code","139e410f":"code","714686fc":"code","b1530a9a":"code","a350a3f8":"code","b6beafa8":"code","2798baeb":"markdown","a14d12c8":"markdown","bd0dd108":"markdown","626b29cf":"markdown","499dd8b7":"markdown","493ca187":"markdown","0f81495a":"markdown","80e952fa":"markdown","16df9a37":"markdown","de271bdd":"markdown","26436a12":"markdown","efd8515d":"markdown","44b5f0be":"markdown","3b62fcc9":"markdown","9065a44b":"markdown","4b384c2b":"markdown","5edfde76":"markdown","44eb29cb":"markdown","fdbb3436":"markdown","edec4c39":"markdown","96c30371":"markdown","de07c726":"markdown","d38dfeab":"markdown","bef68062":"markdown","9bc4e045":"markdown","ac3cad9a":"markdown","c5ec585a":"markdown","ba381608":"markdown","2b88459e":"markdown","b3a02819":"markdown","a5ca5335":"markdown","d5fb68a5":"markdown","74373143":"markdown","4c5c5e1a":"markdown","44ee68cd":"markdown","9079248c":"markdown","f413b0d3":"markdown","2228e97a":"markdown","6e983c4a":"markdown","ec51f82d":"markdown","a735ce24":"markdown","9b0925fe":"markdown","ec969c9b":"markdown","b4e46e87":"markdown","525f85ee":"markdown","2c63950f":"markdown","4d9ee998":"markdown","f755586b":"markdown","e0e0fc37":"markdown","ec400e84":"markdown","1fab7ae6":"markdown","032cad3c":"markdown","4c37a952":"markdown","a124e77c":"markdown"},"source":{"51092aeb":"#Import the modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport missingno as msno\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom time import time\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n#Import the dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npath = os.path.join(dirname, filename)\ndf = pd.read_csv(path)","40c5062b":"#Look into the size of the dataset\nprint('The dataset contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\n\n#Describe the type of columns\nprint('\\nThese are the columns and the type of data they contain:')\nprint(df.dtypes)\n\n#To visualize the percentage I create a pie chart\nfreq = Counter(list(df.dtypes))\nfreq = pd.DataFrame.from_dict(freq, orient='index')\n\nfig, ax = plt.subplots(figsize = (8, 8))\n_, _, freq_pie = plt.pie(freq[0], labels = ['int64', 'float64'], colors = ['gold', 'navy'],\n                         explode = [0, .05], shadow = True, autopct='%1.1f%%')\nplt.setp(freq_pie, **{'color' : 'white', 'weight':'bold', 'fontsize' : 20})\nplt.title('Composition of Data Types', fontsize = 15)\nplt.show()","952b3867":"df.describe()","4cecd1cc":"#Separate the continuous columns first\nis_numeric = []\nfor col in df.columns:\n    is_numeric.append(df[col].dtypes == 'float64')\ndf_numeric = df.columns[is_numeric]\n\n#Create the histograms\nplt.style.use('seaborn')\nfig, axes = plt.subplots(2, 2, figsize = (15, 15))\naxes[1][1].remove()\n\ni = 0\nfor ax_group in axes:\n    for axis in ax_group:\n        if i == 3:\n            continue\n        else:\n            sns.histplot(df[df_numeric[i]], bins = 100, ax = axis, color = 'gold', edgecolor = 'navy')\n            axis.set_xlabel('')\n            axis.grid(axis = 'x')\n            axis.set_title(df_numeric[i], fontsize = 15)\n        i = i+1\nplt.suptitle('Distribution of Continuous Data', fontsize = 20, y = .92)\nplt.show()","3a2ac62d":"#Take the discrete columns, which are all the other ones\ndiscrete = [not x for x in is_numeric]\ndf_discrete = df.columns[discrete]\n\n#Create the histograms\nfig, axes = plt.subplots(3, 3, figsize = (15,15))\naxes[2][1].remove()\naxes[2][2].remove()\n\ni = 0\nj = 0\nfor col in df_discrete:\n    unique_values = df[col].unique()\n    unique_values.sort()\n    count_unique = []\n    \n    for values in unique_values:\n        count_unique.append((df[col] == values).sum())\n    \n    sns.barplot(x = unique_values, y = count_unique, ax = axes[j][i], color = 'gold', edgecolor = 'navy')\n    axes[j][i].set_title(col)\n    \n    if i == 2:\n        j = j + 1\n        i = 0\n    else:\n        i = i + 1\nplt.suptitle('Distribution of Discrete Data', fontsize = 20, y = .92)\nplt.show()","205ac61e":"df.loc[df['num_rooms'] <= 0, 'num_rooms'] = round(df['num_rooms'].mean())\ndf.loc[df['num_people'] <= 0, 'num_people'] = round(df['num_people'].mean())\n\ndf.loc[df['ave_monthly_income'] < 0, 'ave_monthly_income'] = df['ave_monthly_income'].mean()","74e0e06f":"#Check the duplicated rows\nprint('The dataset contains {} duplicated rows.'.format(df.duplicated().sum()))\n\n#Represent the null values in the columns\nmsno.matrix(df)\nplt.show()","ce138840":"#Create a correlation heatplot\nplt.figure(figsize = (10, 8))\nsns.heatmap(df.corr(), annot = True)\nplt.show()","068c5d37":"#Create a list of the columns that will need to be transformed\nscale_columns = ['num_rooms', 'num_people', 'housearea', 'ave_monthly_income', 'num_children']\ndf[scale_columns].head()","5e276ec4":"#Create the scaler\nscaler = StandardScaler()\n\n#Scale the columns previously selected and replace them in the dataset\ndf[scale_columns] = scaler.fit_transform(df[scale_columns])\ndf.head()","8033aaa7":"#Create the selector\nvt = VarianceThreshold()\n\n#Apply the selector on the features\nfeatures = df.iloc[:, :9]\nvt = vt.fit_transform(features)\n\n#Compare the shape of the features and the selector to see if any feature was removed\nprint('The dataset contains {} features.'.format(features.shape[1]))\nprint('The selector contains {} features.'.format(vt.shape[1]))","6af5fad4":"#Select all the columns except the last one as features\nx = df.iloc[:, :9]\n\n#Select amount_paid as the target variable\ny = df['amount_paid']\n\n#The train set will have 80% of the data and the other 20% will be for testing\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 9)","dd502fec":"scores = pd.DataFrame(columns = ['Random Search', 'Grid Search'])","e2e993f1":"#Ordinary least squares Linear Regression.\n#Create the estimator\nmodel_LR = LinearRegression()\n\n#Train the model\nmodel_LR.fit(x_train,y_train)\n\n#Create predictions based on the test set\ny_pred = model_LR.predict(x_test)\n\n#Evaluate the predictions with the coefficient of determination (R^2)\nresults_linear = r2_score(y_test, y_pred)\n\n#Store the scores\nscores = scores.append({'Random Search' : results_linear, 'Grid Search' : results_linear},\n                       ignore_index = True)","45138ca9":"#Linear least squares with l2 regularization.\n#Create the estimator\nmodel_R = Ridge(random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_R = GridSearchCV(model_R,\n                      {'alpha' : [0, .25, .5, .75, 1]},\n                      cv = 10)\n\n#Train the models\ngrid_R.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_R = pd.DataFrame(grid_R.cv_results_).sort_values('mean_test_score',\n                                                         ascending = False)\n\n#Keep only the parameter tested and its score\nresults_R = results_R[['param_alpha', 'mean_test_score']]\n\n#Store the scores\nscores = scores.append({'Random Search' : results_R.iloc[0, -1], 'Grid Search' : results_R.iloc[0, -1]},\n                       ignore_index = True)\n\n#Show the results\nresults_R","22fe86fa":"#Linear Model trained with L1 prior as regularizer (aka the Lasso)\n#Create the estimator\nmodel_La = Lasso(random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_La = GridSearchCV(model_La,\n                       {'alpha' : [.1, .25, .5, .75, 1]},\n                       cv = 10)\n\n#Train the models\ngrid_La.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_La = pd.DataFrame(grid_La.cv_results_).sort_values('mean_test_score',\n                                                           ascending = False)\n\n#Keep only the parameter tested and its score\nresults_La = results_La[['param_alpha', 'mean_test_score']]\n\n#Store the scores\nscores = scores.append({'Random Search' : results_La.iloc[0, -1], 'Grid Search' : results_La.iloc[0, -1]},\n                       ignore_index = True)\n\n#Show the results\nresults_La","4563e86a":"#Create the estimator\nmodel_DTR = DecisionTreeRegressor(random_state = 9)\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_DTR = RandomizedSearchCV(model_DTR,\n                                {'max_depth' : np.arange(10, 210, 10).tolist(),\n                                 'min_weight_fraction_leaf' : [.1, .2, .3, .4, .5],\n                                 'max_features' : ['auto', 'sqrt', 'log2', None],          \n                                 'max_leaf_nodes' : np.arange(10, 110, 10).tolist()},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n                                \n#Train the models\nrandom_DTR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raDTR = pd.DataFrame(random_DTR.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and its scores\nresults_raDTR = results_raDTR[['param_max_depth',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_features',\n                               'param_max_leaf_nodes',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raDTR.loc[(results_raDTR['mean_test_score'] == results_raDTR['mean_test_score'].max())]","4093de74":"#Create the estimator. This will have max_features set to None,\n#since None and auto both are equal to n_features\nmodel_DTR = DecisionTreeRegressor(random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_DTR = GridSearchCV(model_DTR,\n                        {'max_depth' : np.arange(8, 20, 2).tolist(),\n                         'min_weight_fraction_leaf' : [.05, .1, .15, .2],\n                         'max_leaf_nodes' : np.arange(10, 110, 10).tolist()},\n                        cv = 10)\n\n#Train the models\ngrid_DTR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grDTR = pd.DataFrame(grid_DTR.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_grDTR = results_grDTR[['param_max_depth',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_leaf_nodes',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score and lowest max_depth\nresults_grDTR.loc[(results_grDTR['mean_test_score'] == results_grDTR['mean_test_score'].max())\n                & (results_grDTR['param_max_depth'] == results_grDTR['param_max_depth'].min())]","f7ab0771":"#Store the scores\nscores = scores.append({'Random Search' : results_raDTR.iloc[0, -1], 'Grid Search' : results_grDTR.iloc[0, -1]},\n                       ignore_index = True)","bb47d16b":"#Create the estimator\nmodel_RFR = RandomForestRegressor(random_state = 9)\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_RFR = RandomizedSearchCV(model_RFR,\n                                {'n_estimators': np.arange(100, 1100, 100).tolist(),\n                                 'max_depth' : np.arange(50, 160, 10).tolist(),\n                                 'min_weight_fraction_leaf' : [.1, .2, .3, .4, .5],\n                                 'max_features' : ['auto', 'sqrt', 'log2', None],\n                                 'max_leaf_nodes' : np.arange(10, 60, 10).tolist(),\n                                 'bootstrap' : [True, False]},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n\n#Train the models\nrandom_RFR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raRFR = pd.DataFrame(random_RFR.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_raRFR = results_raRFR[['param_n_estimators',\n                               'param_max_depth',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_features',\n                               'param_max_leaf_nodes',\n                               'param_bootstrap',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raRFR.loc[(results_raRFR['mean_test_score'] == results_raRFR['mean_test_score'].max())]","e7513e4d":"#Create the estimator\nmodel_RFR = RandomForestRegressor(random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_RFR = GridSearchCV(model_RFR,\n                        {'n_estimators': np.arange(180, 230, 10).tolist(),\n                         'max_depth' : np.arange(80, 130, 10).tolist(),\n                         'min_weight_fraction_leaf' : [.05, .1, .15],\n                         'max_leaf_nodes' : np.arange(20, 45, 5).tolist()},\n                        cv = 10)\n\n#Train the models\ngrid_RFR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grRFR = pd.DataFrame(grid_RFR.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_grRFR = results_grRFR[['param_n_estimators',\n                               'param_max_depth',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_leaf_nodes',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_grRFR.loc[(results_grRFR['mean_test_score'] == results_grRFR['mean_test_score'].max())]","e1ff6ef3":"#Store the scores\nscores = scores.append({'Random Search' : results_raRFR.iloc[0, -1], 'Grid Search' : results_grRFR.iloc[0, -1]},\n                       ignore_index = True)","72622352":"#Create the estimator\nmodel_SVR = SVR()\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_SVR = RandomizedSearchCV(model_SVR,\n                                {'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n                                 'degree' : np.arange(1, 7, 1).tolist(),\n                                 'gamma' : ['scale', 'auto'],\n                                 'C' : np.arange(2, 22, 2).tolist(),\n                                 'epsilon' : [.1, .25, .5, .75]},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n               \n#Train the models\nrandom_SVR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raSVR = pd.DataFrame(random_SVR.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_raSVR = results_raSVR[['param_kernel',\n                               'param_degree',\n                               'param_gamma',\n                               'param_C',\n                               'param_epsilon',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raSVR.loc[(results_raSVR['mean_test_score'] == results_raSVR['mean_test_score'].max())]","8d4ff63d":"#Create the estimator with kernel as lineal and degree and gamma as default\nmodel_SVR = SVR(kernel = 'linear')\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_SVR = GridSearchCV(model_SVR,\n                        {'C' : np.arange(18, 23, 1).tolist(),\n                         'epsilon' : [.3, .4, .5, .6]},\n                        cv = 10)\n\n#Train the models\ngrid_SVR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grSVR = pd.DataFrame(grid_SVR.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_grSVR = results_grSVR[['param_C',\n                               'param_epsilon',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_grSVR.loc[(results_grSVR['mean_test_score'] == results_grSVR['mean_test_score'].max())]","5b2a1fdf":"#Store the scores\nscores = scores.append({'Random Search' : results_raSVR.iloc[0, -1], 'Grid Search' : results_grSVR.iloc[0, -1]},\n                       ignore_index = True)","df9fe83e":"#Create the estimator\nmodel_KNN = KNeighborsRegressor()\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_KNN = RandomizedSearchCV(model_KNN,\n                                {'n_neighbors' : np.arange(5, 55, 5).tolist(),\n                                 'weights' : ['uniform', 'distance'],\n                                 'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                                 'leaf_size' : np.arange(5, 55, 5).tolist(),\n                                 'p' : [1, 2]},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n               \n#Train the models\nrandom_KNN.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raKNN = pd.DataFrame(random_KNN.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_raKNN = results_raKNN[['param_n_neighbors',\n                               'param_weights',\n                               'param_algorithm',\n                               'param_leaf_size',\n                               'param_p',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raKNN.loc[(results_raKNN['mean_test_score'] == results_raKNN['mean_test_score'].max())]","a7d6c162":"#Create the estimator with the values define from the random search\nmodel_KNN = KNeighborsRegressor(weights = 'distance', p = 1)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_KNN = GridSearchCV(model_KNN,\n                        {'n_neighbors' : np.arange(8, 13, 1).tolist(),\n                         'algorithm' : ['ball_tree', 'brute'],\n                         'leaf_size' : np.arange(10, 34, 2).tolist()},\n                        cv = 10)\n\n#Train the models\ngrid_KNN.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grKNN = pd.DataFrame(grid_KNN.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n#Keep only the parameters tested and their scores\nresults_grKNN = results_grKNN[['param_n_neighbors',\n                               'param_algorithm',\n                               'param_leaf_size',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_grKNN.loc[(results_grKNN['mean_test_score'] == results_grKNN['mean_test_score'].max())]","878bc47a":"#Store the scores\nscores = scores.append({'Random Search' : results_raKNN.iloc[0, -1], 'Grid Search' : results_grKNN.iloc[0, -1]},\n                       ignore_index = True)","388a1669":"#Create the estimator\nmodel_GBR = GradientBoostingRegressor(random_state = 9)\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_GBR = RandomizedSearchCV(model_GBR,\n                                {'loss' : ['ls', 'lad', 'huber', 'quantile'],\n                                 'learning_rate' : [.1, .25, .5, .75],\n                                 'n_estimators' : np.arange(10, 90, 10).tolist(),\n                                 'min_weight_fraction_leaf' : [.1, .2, .3, .4, .5],\n                                 'max_depth' : [5, 10, 20, 30],\n                                 'max_features' : ['auto', 'sqrt', 'log2', None]},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n                                \n#Train the models\nrandom_GBR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raGBR = pd.DataFrame(random_GBR.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_raGBR = results_raGBR[['param_loss',\n                               'param_learning_rate',\n                               'param_n_estimators',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_depth',\n                               'param_max_features',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raGBR.loc[(results_raGBR['mean_test_score'] == results_raGBR['mean_test_score'].max())]","1af58a17":"#Create the estimator with the values define from the random search\nmodel_GBR = GradientBoostingRegressor(loss = 'huber', random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_GBR = GridSearchCV(model_GBR,\n                        {'learning_rate' : [.2, .25, .30, .35],\n                         'n_estimators' : np.arange(40, 65, 5).tolist(),\n                         'min_weight_fraction_leaf' : [.05, .1, .15, .2],\n                         'max_depth' : [25, 30, 35]},\n                        cv = 10)\n                                \n#Train the models\ngrid_GBR.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grGBR = pd.DataFrame(grid_GBR.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_grGBR = results_grGBR[['param_learning_rate',\n                               'param_n_estimators',\n                               'param_min_weight_fraction_leaf',\n                               'param_max_depth',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_grGBR.loc[(results_grGBR['mean_test_score'] == results_grGBR['mean_test_score'].max())]","12510047":"#Store the scores\nscores = scores.append({'Random Search' : results_raGBR.iloc[0, -1], 'Grid Search' : results_grGBR.iloc[0, -1]},\n                       ignore_index = True)","b5b9e2e9":"#Create the estimator\nmodel_XGB = XGBRegressor(random_state = 9)\n\n#Create the random search inputing the estimator, parameters,\n#number of iterations and cross validation value\nrandom_XGB = RandomizedSearchCV(model_XGB,\n                                {'eta' : [.1, .2, .3, .4, .5],\n                                 'max_depth' : np.arange(10, 60, 10).tolist(),\n                                 'min_child_weight' : np.arange(5, 35, 5).tolist(),\n                                 'subsample' : [.1, .2, .3, .4, .5],\n                                 'colsample_bytree' : [.1, .2, .3, .4, .5]},\n                                n_iter = 100,\n                                cv = 5,\n                                random_state = 9)\n                                \n#Train the models\nrandom_XGB.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_raXGB = pd.DataFrame(random_XGB.cv_results_).sort_values('mean_test_score',\n                                                                 ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_raXGB = results_raXGB[['param_eta',\n                               'param_max_depth',\n                               'param_min_child_weight',\n                               'param_subsample',\n                               'param_colsample_bytree',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_raXGB.loc[(results_raXGB['mean_test_score'] == results_raXGB['mean_test_score'].max())]","52384357":"#Create the estimator\nmodel_XGB = XGBRegressor(random_state = 9)\n\n#Create the grid search inputing the estimator, parameters and cross validation value\ngrid_XGB = GridSearchCV(model_XGB,\n                        {'eta' : [.05, .1, .15, .2],\n                         'max_depth' : np.arange(6, 16, 2).tolist(),\n                         'min_child_weight' : np.arange(16, 26, 2).tolist(),\n                         'subsample' : [.4, .5, .6],\n                         'colsample_bytree' : [.4, .5, .6]},\n                        cv = 10)\n                                \n#Train the models\ngrid_XGB.fit(x, y)\n\n#Create a Dataframe with the results of the search\nresults_grXGB = pd.DataFrame(grid_XGB.cv_results_).sort_values('mean_test_score',\n                                                               ascending = False)\n\n#Keep only the parameters tested and their scores\nresults_grXGB = results_grXGB[['param_eta',\n                               'param_max_depth',\n                               'param_min_child_weight',\n                               'param_subsample',\n                               'param_colsample_bytree',\n                               'mean_test_score']]\n\n#Show the parameters with the highest score\nresults_grXGB.loc[(results_grXGB['mean_test_score'] == results_grXGB['mean_test_score'].max())]","3c1ef4fa":"#Store the scores\nscores = scores.append({'Random Search' : results_raXGB.iloc[0, -1], 'Grid Search' : results_grXGB.iloc[0, -1]},\n                       ignore_index = True)","edb39c7f":"#Multiply the scores by 100 to make it easier to read\nscores = scores * 100\n\n#Add the name of the model to every pair of scores and sort it in descending order\nmodel_labels = ['Linear','Ridge', 'Lasso', 'Decision\\nTree', 'Random\\nForest', 'Support\\nVector', 'K-Nearest\\nNeighbors', 'Gradient\\nBoosting', 'XG\\nBoosting']\nscores.index = model_labels\nscores = scores.sort_values('Grid Search', ascending = False)\n\n#Create the bar plot\nw = .4\nbar1 = np.arange(len(scores))\nbar2 = [i+w for i in bar1]\n\nplt.style.use('seaborn')\nplt.figure(figsize = (10, 8))\nbar_random=plt.bar(bar1, scores['Random Search'], w, label = 'Random', color = 'navy')\nbar_grid=plt.bar(bar2, scores['Grid Search'], w, label = 'Grid', color = 'gold')\n\n#Format the plot\nplt.xticks(bar1 + (w\/2), scores.index, fontsize = 12)\nplt.legend()\nplt.title('Model Scores', fontsize = 15)\n\n#Add labels on top of the bars to make the comparison easier\nfor index, value in enumerate(scores['Grid Search']):\n    plt.text(index + (w\/2), value + .5, str(round(value, 2)), fontsize = 12)","139e410f":"#Create the estimators with the best parameters\nRidge_final = Ridge(alpha = 1, random_state = 10)\nLasso_final = Lasso(alpha = .5, random_state = 10)\nLinear_final = LinearRegression()\n\n#Train the models using the train data\nRidge_final.fit(x_train, y_train)\nLasso_final.fit(x_train, y_train)\nLinear_final.fit(x_train, y_train)\n\n#Create prediction values using the test data\nRidge_predict = Ridge_final.predict(x_test)\nLasso_predict = Lasso_final.predict(x_test)\nLinear_predict = Linear_final.predict(x_test)\n\n#Evaluate the predictions with R^2\nRidge_score = r2_score(y_test, Ridge_predict)\nLasso_score = r2_score(y_test, Lasso_predict)\nLinear_score = r2_score(y_test, Linear_predict)\n\n#Group the scores and the model names\nbar_x = ['Ridge', 'Lasso', 'Linear']\nbar_y = [Ridge_score * 100, Lasso_score * 100, Linear_score * 100]\n\n#Create a Dataframe and sort the values\nbar_df = pd.DataFrame(bar_y, bar_x, columns = ['Score'])\nbar_df = bar_df.sort_values('Score', ascending = False)\n\n#Create the plot\nsns.set(font_scale = 1.2)\nplt.figure(figsize = (10, 6))\npalette = ['gold', 'navy', 'forestgreen']\nfinal_bar = sns.barplot(x = bar_df.index, y = bar_df['Score'], palette = palette)\n\n#Add labels on top of the bars to make the comparison easier\nfor p in final_bar.patches:\n    final_bar.annotate(format(p.get_height(), '.2f'), \n                       (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                       ha = 'center', va = 'center',\n                       xytext = (0, 7), size = 12,\n                       textcoords = 'offset points')\n\nplt.title('Scores for the Best Models', fontsize = 15)\nplt.show()","714686fc":"#Access the feature coefficients from the trained model and get the absolute value\nimportance = Lasso_final.coef_\nimportance = np.abs(importance)\n\n#Create a Dataframe of each feature and its importance\nfeature_importance = pd.DataFrame(list(zip(x_train.columns, importance)),\n                                 columns = ['Feature', 'Importance'])\n\n#Sort Dataframe by Importance in descending order\nfeature_importance = feature_importance.sort_values('Importance',\n                                                    ascending = False).reset_index(drop = True)\n\n#Create a bar plot\nplt.figure(figsize = (15, 10))\nsns.barplot(x = feature_importance['Feature'], y = feature_importance['Importance'], color = 'gold', edgecolor = 'navy')\nplt.xticks(rotation = 45)\nplt.xlabel('')\nplt.title('Feature Importance', fontsize = 20)\nplt.show()","b1530a9a":"#Take amount_paid into a list depending on the value of is_urban\nurban = df.loc[df['is_urban'] == 1, 'amount_paid']\nnot_urban = df.loc[df['is_urban'] == 0, 'amount_paid']\n\n#Join the lists\nurban_df = pd.concat([urban, not_urban]).reset_index(drop = True)\n\n#Create a Dataframe\nurban_df = pd.DataFrame(urban_df)\n\n#Add a column to label the row as urban or not urban\nurban_df['Category'] = np.where(urban_df.index >= len(urban), 'Not Urban', 'Urban')\n\n#Create the plot\nplt.figure(figsize = (10, 10))\npalette = ['gold', 'navy']\nsns.stripplot(x = 'Category', y = 'amount_paid', data = urban_df, palette = palette)\nplt.xlabel('')\nplt.title('Importance of Urban', fontsize = 20);","a350a3f8":"#Join predicted y and true y and create a Dataframe\ny_values = list(zip(y_test, Lasso_predict))\ny_values = pd.DataFrame(y_values, columns = ['Target', 'Prediction'])","b6beafa8":"#Take 10 random samples from the Dataframe\nsample = y_values.sample(10).reset_index(drop = True)\n\n#Create the plot\nplt.figure(figsize = (15, 10))\n\n#This will be for the true values\nsns.barplot(x = sample.index, y = sample['Target'], color = 'gold', edgecolor = 'navy', alpha = .8)\n\n#This will be just a horizontal line to represent the predicted value\nsns.scatterplot(x = sample.index, y = sample['Prediction'], color = 'navy',\n                marker = '_', s = 3000)\nplt.xlabel('Sample')\nplt.title('Target vs Predicted Values', fontsize = 20)\nplt.show()","2798baeb":"## Feature Importance","a14d12c8":"It is not logical for a house to have -1 rooms or people, so I will change those values to 0. Also, some values in ave_monthly_income are negative, these will be changed to the mean of the column.","bd0dd108":"### Support Vector Regression","626b29cf":"Having the dataset loaded, the first stage is to explore how it is composed to understand what I am working with.","499dd8b7":"Because all the best parameters are the same, for this model I will select the smallest max_depth. The score improved by less than 0.01.","493ca187":"### K-Nearest Neighbors","0f81495a":"Now that the columns are scaled, the range is -3 to 3, but the proportion between the numbers is the same.\n\nThere is no need to scale the other columns because they are binary.","80e952fa":"### Random Forest","16df9a37":"### Gradient Boosting Regressor","de271bdd":"After selecting the best model, I will take a look into how the features influence the target variable. I will do this by looking at the coefficients of each feature. The importance of a feature is the absolute value of its coefficient.","26436a12":"This notebook was made as part of the course end project for a masters program in Big Data and Business Analytics.\n\nThe dataset contains information about houses and its residents and the amount paid for electricity in a month, which will be the target variable of the machine learning model.","efd8515d":"I will select loss and max_features but will make a grid search with the other values.","44b5f0be":"### Lasso","3b62fcc9":"Now, the same with the discrete columns.","9065a44b":"Now that I have tested the models, I will compare their scores to select the best ones. I will do so by visualizing the scores with a bar plot.","4b384c2b":"This time there is only one set of parameters with the highest score, the grid search will be around those values and bootstrap and max_features will be set to default values.","5edfde76":"### Linear Regression","44eb29cb":"## Comparing Values","fdbb3436":"The best 3 models here are Lasso, Ridge and Support Vector Regression. I will retrain these models by using the parameters that yielded the best results.","edec4c39":"Next, I will see if the dataset contains any duplicated rows or null values. The second one will be achieved by text and by visual representation showing white rectangles for null values.","96c30371":"## Dataset Description","de07c726":"There are many parameters that produced the best score, but from here I will take the least complex model.","d38dfeab":"This search defined that p should be 1 and weights should be distance. I will do a grid search to try to get better n_neighbors and leaf_size and define the best algorithm.","bef68062":"The first 3 models do not have many parameters, so I will only use GridSearchCV on them.","9bc4e045":"## Conclusions and next steps","ac3cad9a":"No features were removed, meaning none of them have a low variance.","c5ec585a":"If this model is selected as the final, the parameters that would define it are max_depth=80 and max_leaf_nodes=20. Here, the score improved by 0.09.","ba381608":"The first step is to import the modules and the dataset that will be used.","2b88459e":"Once again the best model for our data is Lasso.","b3a02819":"## Imports","a5ca5335":"## Data Transformation","d5fb68a5":"Other than picking the smallest leaf_size, the decision here is in the algorithm, but I will use brute as it is more accurate because it considers all the points. The score increased by around 0.01.","74373143":"### Ridge","4c5c5e1a":"### Decision Tree Regressor","44ee68cd":"After selecting Lasso as the best model, I want to show how the accuracy looks in individual cases. For that, I will take random samples of true values and predicted values and plot them.","9079248c":"The Lasso model has proven to be a reliable model to predict the price in an electric bill based on the characteristics of a house and its inhabitants, it has an accuracy of 87.31%.\n\nThis model can be implemented by electric companies to give their users a good estimate of what they can expect to pay at the end of the month.\nOn the other side, electric companies can benefit from this by translating amount_paid into kilowatts consumed. By knowing how much energy their clients will use, companies can calculate in advance how much power they need to produce.\n\nMy recommendations on ways of improving the model are:\n1. Add more cases, this will make the model more accurate by learning new combinations of the different features.\n2. Add new features for month and weather. These two things are cyclical, which can help the model predict. The second most important feature was is_ac, the hotter the weather is, the more time the AC unit will be on, therefore a more expensive electric bill.","f413b0d3":"The third stage is to create a model that will accurately understand and imitate the relationship between the features and the target variable.\n\nBecause I am looking at how a number gets affected by different variables, this is a problem of regression. And because I will input the target variable into the model, this is supervised machine learning.\n\nI will train 9 models and because each model has different parameters I will hypertune them to find the most appropriate ones for the model. This will be done by first using RandomizedSearchCV to pick random values inside a big range, training the model. Then, I will take the parameters that output the best score and use GridSearchCV to train the model on parameters on a smaller range around them.\n\nI will create an empty Dataframe where I will store the best scores to compare them at the end.","2228e97a":"The second stage is to prepare the data to make it easier for the model to understand it and process it.","6e983c4a":"Finally, show the correlation between each column.","ec51f82d":"Once again the result is one set of parameters and the score increased by less than 0.01.","a735ce24":"Before training the models I will check if any of the features have a low variance, meaning they do not change much and so they do not affect the model.","9b0925fe":"max_depth and min_weight_fraction_leaf have the same values, the important parameter here is the smallest leaf_nodes. By making a more thorough search the score increased by 0.11.","ec969c9b":"### Extreme Gradient Boosting","b4e46e87":"Starting from this model, I will start testing more parameters. I will now use a random and a grid search.","525f85ee":"is_urban has the greatest influence on the model, its importance is significantly greater than the importance of the other features.\n\nTo have a better understanding of how amount_paid changes depending on is_urban, I will plot on the two values of is_urban.","2c63950f":"## Testing Models","4d9ee998":"## Selecting Model","f755586b":"There are no options in this case and the score is lower.","e0e0fc37":"There was only one model with the highest score, but I will still make a grid search around these values.","ec400e84":"# Predicting household electricity bills","1fab7ae6":"Now that the data is ready to be input into the model, I will assign all the features to x and the target variable (amount_paid) as y.\n\nThen, I will split x and y into training and testing sets that will be used in the final model.","032cad3c":"The previous cell can be executed many times to plot different samples.","4c37a952":"Some of the columns contain high values, like ave_monthly_income, which goes to over 50,000, on the other hand, columns like num_rooms have 5 as the highest value. This will cause the model to give more weight to ave_monthly_income in the predictions.","a124e77c":"The kernel for the best scores is linear, and since degree is only used when the kernel is poly, it will be set to default. Other than degree, the parameters are uniform, but I will do a grid search to try to improve the score."}}