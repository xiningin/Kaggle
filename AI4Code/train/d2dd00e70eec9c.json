{"cell_type":{"c1a2112f":"code","5cfbe749":"code","d7e79725":"code","2831a6c4":"code","ffec5a71":"code","ec19b758":"code","bc1e5234":"code","595b0c65":"code","cae48f65":"code","7f84cd95":"code","45f18955":"code","d03824c1":"code","1e774b34":"code","d171cda4":"code","12acd9f9":"code","36a71ee5":"code","14f875e1":"code","89ea254b":"code","70d0d398":"code","fbe82e5e":"markdown","99fe68c4":"markdown","9405b3e7":"markdown","b053c17c":"markdown","2268e141":"markdown"},"source":{"c1a2112f":"import os\nimport copy\nimport pickle\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import logging\nlogging.set_verbosity_error() #turn off bert warning\nlogging.set_verbosity_warning() #turn off bert warning","5cfbe749":"def set_seed(seed):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \ndef generate_fold_num_for_dataset(data, num_fold):\n    skf = StratifiedKFold(n_splits=num_fold, shuffle=True)\n    for fold, ( _, val_) in enumerate(skf.split(X=data, y=data.worker)):\n        data.loc[val_ , \"kfold\"] = int(fold)\n    data[\"kfold\"] = data[\"kfold\"].astype(int)\n    return data\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        self.use_tfidf = use_tfidf\n        if use_tfidf:\n            self.more_toxic_tfidf_idx = df['more_toxic_tfidf_idx'].values\n            self.less_toxic_tfidf_idx = df['less_toxic_tfidf_idx'].values\n            self.tfidf_matrix = tfidf_matrix\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        if self.use_tfidf:\n            more_toxic_tfidf_idx = self.more_toxic_tfidf_idx[index]\n            less_toxic_tfidf_idx = self.less_toxic_tfidf_idx[index]\n            more_toxic_tfidf = self.tfidf_matrix[more_toxic_tfidf_idx]\n            less_toxic_tfidf = self.tfidf_matrix[less_toxic_tfidf_idx]\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }\n        else:\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }","d7e79725":"class NN(nn.Module):\n    def __init__(self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                \n                nn.Linear(768+tfidf_len, 1)\n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n                \nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(self, ids, mask, tfidf_vec=None, use_tfidf=False):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","2831a6c4":"def train_step_combine(\n    model, criterion, optimizer, \n    train_loader, progress_bar, device, epoch, use_tfidf = False\n):\n    y_preds = []\n    epoch_loss = 0\n    model.train()\n    for i, data in enumerate(train_loader):\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        \n        if use_tfidf:\n            more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n            less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n            more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n            less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n        else:\n            more_out = model(more_toxic_ids, more_toxic_mask)\n            less_out = model(less_toxic_ids, less_toxic_mask)\n            \n        loss = criterion(more_out, less_out, targets)\n\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        for j in range(len(data['more_toxic_ids'])):\n            y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n        if progress_bar is not None:\n            progress_bar.update(1)  \n            \n        print('[ Epoch {}: {}\/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n    df_score = pd.DataFrame(y_preds,columns=['less','more'])\n    train_accuracy = validate_accuracy(df_score)         \n    \n    return df_score, train_accuracy, epoch_loss \/ len(train_loader) # return loss\n\n\ndef validate_all_combine(\n    model, criterion, \n    valid_loader, device, use_tfidf=False\n):\n    epoch_loss = 0\n    y_preds = []\n    \n    model.eval()\n    with torch.no_grad():\n        for data in valid_loader:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n            \n            if use_tfidf:\n                more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n                less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n                more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n                less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n            else:\n                more_out = model(more_toxic_ids, more_toxic_mask)\n                less_out = model(less_toxic_ids, less_toxic_mask)\n            \n            loss = criterion(more_out, less_out, targets)\n\n            epoch_loss += loss.item()\n            for i in range(len(data['more_toxic_ids'])):\n                y_preds.append([less_out[i].item(), more_out[i].item()])\n        df_score = pd.DataFrame(y_preds,columns=['less','more'])\n        accuracy = validate_accuracy(df_score)\n    return df_score, accuracy, (epoch_loss \/ len(valid_loader))\n\ndef validate_accuracy(df_score):\n    return len(df_score[df_score['less'] < df_score['more']]) \/ len(df_score)\n\ndef return_wrong_text(df_score, df_valid):\n    df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n    return df_score_text[df_score_text['less'] > df_score_text['more']]\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)","ffec5a71":"def plot_loss_plot(train_losses, valid_losses):\n    plt.plot(train_losses,label=\"Training\")\n    plt.plot(valid_losses,label=\"Validation\")\n    plt.title(\"Loss plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n    \ndef plot_acc_plot(train_accs, valid_accs):\n    plt.plot(train_accs,label=\"Training\")\n    plt.plot(valid_accs,label=\"Validation\")\n    plt.title(\"Accuracy plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","ec19b758":"def remove_duplicates(df, used_col):\n    \"\"\"Combine `less_toxic` text and `more_toxic` text,\n    then remove duplicate pair of comments while keeping the last pair\n    \"\"\"\n    df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n    df = df.drop_duplicates(subset=used_col, keep=\"last\")\n    return df\n\ndef create_corpus(df_train):\n    all_corpus = df_train[\"more_toxic\"].to_list()\n    all_corpus += df_train[\"less_toxic\"].to_list()\n    #remove duplicates\n    all_corpus = list(\n        set(all_corpus)\n    )\n    return all_corpus\n\ndef create_mapping_dict(corpus):\n    idx = np.arange(len(corpus))\n    sentence2idx = dict(\n        zip(corpus, idx)\n    )\n    return sentence2idx\n\ndef tokenize_by_bert_tokenizer(corpus, tokenizer):\n    corpus_tokenized = [\n        tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef identity_tokenizer(text):\n    return text\n\ndef corpus2tfidf(corpus_tokenized):\n    tfidf = TfidfVectorizer(preprocessor=' '.join, tokenizer=identity_tokenizer)    \n    tfidf_matrix_sparse = tfidf.fit_transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf, tfidf_matrix\n    \ndef construct_tfidf_matrix(df_train, tokenizer):\n    corpus = create_corpus(df_train)\n    sentence2idx = create_mapping_dict(corpus)\n    corpus_tokenized = tokenize_by_bert_tokenizer(corpus, tokenizer)\n    tfidf_obj, tfidf_matrix = corpus2tfidf(corpus_tokenized)\n    return sentence2idx, tfidf_obj, tfidf_matrix","bc1e5234":"set_seed(5080)\ndata_train = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndata_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndata_train2 = pd.read_csv(\"..\/input\/jigsaw-training-with-classification\/classification_data.csv\")\n\n#load model and tokenizer\n# PRETRAINED_MODEL_NAME = \"GroNLP\/hateBERT\"\n# bert_tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nPRETRAINED_MODEL_NAME = \"roberta-base\"\nbert_tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)","595b0c65":"data_train3 = data_train2.drop(columns = ['Unnamed: 0','less_score','more_score'])\ndata_train3['worker'] = [-1] * len(data_train3)\ndata_train = data_train.append(data_train3)","cae48f65":"data_train","7f84cd95":"non_toxic = [' article ',' will ',' page ',' people ',' one ',' edit ',' wikipedia ',' talk ',' user ',' comment ',' know ',' really ',' think ',' Please ',' make ' ,' someone ',' thing ',' many' ,' name ',' said ',' time ',' need ',' look ',' editor ',' china ',' world ',' see ',' source ',' well ',' Fan ', ' fact ', ' Thank ',' wiki']","45f18955":"len(non_toxic)","d03824c1":"# columns = ['less_toxic','more_toxic']\n\n# for col in columns:\n#     result = []\n#     for sentense in data_train[col].to_list():\n#         temp = sentense\n#         for word in non_toxic:\n#             temp = temp.replace(word,' ')\n#         result.append(temp)\n#     data_train[col] = result","1e774b34":"# columns = ['text']\n\n# for col in columns:\n#     result = []\n#     for sentense in data_test[col].to_list():\n#         temp = sentense\n#         for word in non_toxic:\n#             temp = temp.replace(word,' ')\n#         result.append(temp)\n#     data_test[col] = result","d171cda4":"#text preprocessing\ndata_train_removed = remove_duplicates(data_train, \"combine\") #(30108, 4) -> (15410, 4)\n\n#construct tfidf\u4e00\u5b9a\u8981\u7528removed!\nsentence2idx, tfidf_obj, tfidf_matrix = construct_tfidf_matrix(data_train_removed, bert_tokenizer)\ndata_train[\"less_toxic_tfidf_idx\"] = data_train[\"less_toxic\"].apply(lambda x: sentence2idx[x])\ndata_train[\"more_toxic_tfidf_idx\"] = data_train[\"more_toxic\"].apply(lambda x: sentence2idx[x])","12acd9f9":"with open('.\/tfidf_roberta_obj.pickle', 'wb') as f:\n    pickle.dump(tfidf_obj, f)","36a71ee5":"print(tfidf_obj)\nprint(tfidf_matrix.shape)","14f875e1":"#paramters settings\ntrain_valid_ratio = 0.25\nmax_token_length = 128 #\u4ee3\u8868\u6700\u591a\u653e\u5165BERT\u7684token\u9577\u5ea6\ntrain_batch_size = 32\nvalid_batch_size = 64\nuse_tfidf = False\n\ndf_train, df_valid = train_test_split(\n    data_train, test_size = train_valid_ratio\n)\n\n\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\nelse:\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=train_batch_size, shuffle=True,\n    num_workers=2)\nvalid_loader = DataLoader(\n    valid_dataset, batch_size=valid_batch_size, shuffle=False,\n    num_workers=2)","89ea254b":"LR = 1e-4\nWD = 1e-6\nbert_drop_out = 0.3\n# margin_list = [0.5]\n\n# LR_list = [1e-5, 1e-4]\n# WD_list = [0, 1e-6]\n# bert_dropout_list = [0.2, 0.3]\nmax_patience = 3\n\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0113\"\nmodel_name = \"roberta\"\n\n\n# for LR in LR_list:\n#     for WD in WD_list:\n#         for bert_drop_out in bert_dropout_list:\n\nprint(f\"LR = {LR}, WD = {WD}, bert drop out = {bert_drop_out}\")\n\n####################################################MODEL SETTINGS########################################\nbert = RobertaModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\nif use_tfidf:\n    dnn = NN(bert_drop_out, HID_DIM, \n             tfidf_len, use_tfidf=True\n            ).to(device)\n\nelse:\n    dnn = NN(\n        bert_drop_out, HID_DIM\n        ).to(device)\n\ndnn.apply(init_weights)\nmodel = JigsawModel(bert, dnn)\ntrainable_params = list(model.parameters())\nnum_trainable_params = sum(p.numel() for p in trainable_params)\n\ncriterion = nn.MarginRankingLoss(margin=MARGIN)\noptimizer = AdamW(\n    trainable_params\n    ,lr=LR,weight_decay=WD)\n\nprint(f\"Total trainable parameters {num_trainable_params}\")\n\nnum_training_steps = EPOCH * len(train_loader)\n###########################################################################################################\n\n####################################################Records################################################\ntrain_accs = []\nvalid_accs = []\ntrain_losses = []\nvalid_losses = []\nbest_valid_loss = np.inf\nbest_valid_acc = 0\nbest_epoch = 0\nbest_model = None\nno_update = 0 #number of non-updated epochs\nMODEL_DIR = f\".\/{DATE}_{model_name}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.pth\"\n###########################################################################################################\n\n# start training\n#progress_bar = None\nprogress_bar = tqdm(range(num_training_steps))\nfor epoch in range(1, EPOCH+1):\n    _, train_acc, train_loss = train_step_combine(model, criterion, optimizer, train_loader, progress_bar, device, epoch, use_tfidf)\n    df_score, valid_acc, valid_loss = validate_all_combine(model, criterion, valid_loader, device, use_tfidf)\n    train_accs.append(train_acc)\n    valid_accs.append(valid_acc)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    print(f\"Epoch {epoch}, Loss(Train\/Valid) = {round(train_loss, 4)}\/{round(valid_loss, 4)}, Accuracy(Train\/Valid) = {round(train_acc*100, 3)}%\/{round(valid_acc*100, 3)}%\")\n\n    if valid_acc > best_valid_acc:\n        no_update = 0\n        print(f\"Saving model...\")\n        best_epoch = epoch\n        best_valid_acc = valid_acc\n        best_valid_loss = valid_loss\n        best_model = model\n        torch.save(\n            {\"BERT\": best_model.bert.state_dict(),\"NN\": best_model.fc.state_dict()}\n            ,MODEL_DIR\n        )\n    #early stopping\n    else:\n        no_update += 1 \n\n    if no_update == max_patience:\n        break\n\nprint(f\"Best epoch: {best_epoch}, valid loss: {round(best_valid_loss, 4)}, valid acc: {round(best_valid_acc*100, 4)}%\")\n\n#compute acc for all dataset\nall_dataset = JigsawDataset(\n    data_train, tokenizer=bert_tokenizer, \n    max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n)\n\nall_loader = DataLoader(\n    all_dataset, batch_size=valid_batch_size, \n    shuffle=True, num_workers=2\n)\n\ndf_score, accuracy, valid_loss = validate_all_combine(model, criterion, all_loader, device, use_tfidf)\ndf_score_text = return_wrong_text(df_score, df_valid)\ncsv_out_path = f\".\/wrong_text_{DATE}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.csv\"\ndf_score_text.to_csv(csv_out_path)\nprint(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n#Evaluation\nplot_loss_plot(train_losses, valid_losses)\nplot_acc_plot(train_accs, valid_accs)","70d0d398":"# all_dataset = JigsawDataset(\n#     data_train, tokenizer=bert_tokenizer, \n#     max_length=max_token_length)\n\n# all_loader = DataLoader(\n#     train_dataset, batch_size=batch_size, shuffle=True,\n#     num_workers=2)\n\n# df_score, accuracy, valid_loss = validate_all(bert, dnn, criterion, all_loader, device)\n# print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n# df_score_text = return_wrong_text(df_score, df_valid)\n# df_score_text.to_csv(\".\/wrong_text_1229_2.csv\")\n# print(df_score_text)\n\n# plot_loss_plot(train_losses, valid_losses)\n# plot_acc_plot(train_accs, valid_accs)","fbe82e5e":"#### best settings (until 1\/12 10:30)\n- PRETRAINED_MODEL_NAME = \"roberta-base\"\n- train_valid_ratio = 0.25\n- max_token_length = 128\n- batch_size = 32\n- LR = 1e-4\n- WD = 1e-6\n- bert dropout rate = 0.3\n- EPOCH = 10\n- NO-TFIDF","99fe68c4":"# Remove Useless Words by Jack","9405b3e7":"# Dataset and Dataloader","b053c17c":"# Modeling","2268e141":"# TFIDF preprocessing"}}