{"cell_type":{"01285fb3":"code","10dc15ad":"code","0eb3e9f1":"code","5441323c":"code","da84f9e7":"code","a0e03b27":"code","7c16e95b":"code","5ace6bb9":"code","d7de6b1c":"code","f18d4104":"code","89338128":"code","0d50410c":"code","974d78c9":"code","7ce931d3":"code","644545ca":"code","c27f1754":"code","b344794a":"code","ba6322a8":"code","a93b0467":"code","9652f113":"code","5a7e9a2a":"code","1d606a97":"code","c570ba4b":"code","18ddc7e5":"code","7a62b758":"code","a23ce4d8":"code","ff3cd91b":"code","c118a86f":"code","346af85e":"code","0f81bce6":"code","a4192fb2":"code","f2dec14e":"code","e859bc7a":"markdown","f4866be7":"markdown","e39b6f48":"markdown","ed25ebbf":"markdown","8af7ef8a":"markdown","9636a92c":"markdown","2d173de5":"markdown","999ae135":"markdown","5fc1cfa8":"markdown","f84499b4":"markdown","e003f0bc":"markdown","364aa7d8":"markdown","f2c1078a":"markdown","34697ac3":"markdown","35c59532":"markdown","db3e9787":"markdown","295f5af2":"markdown","ed0bf54a":"markdown","4fd64465":"markdown","a9a4af01":"markdown","bfd0e089":"markdown","4536e237":"markdown","575b95fc":"markdown","86e773d8":"markdown","744fb223":"markdown","61b1d0f5":"markdown","60215e11":"markdown","648f5934":"markdown","0fbb759d":"markdown","d34493dc":"markdown","746265b0":"markdown","35267cc9":"markdown"},"source":{"01285fb3":"# with typing installed, the pip install of esm was breaking\n!pip uninstall -y typing\n!pip install git+https:\/\/github.com\/facebookresearch\/esm.git\n!pip install trimap\n!pip install heatmapz","10dc15ad":"import pandas as pd\nimport torch\nimport esm\nimport requests\nimport re\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\nimport trimap\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LassoCV, HuberRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score, make_scorer\nimport scipy\nimport numpy as np\nfrom IPython.display import Image\nfrom tqdm.auto import tqdm\nimport joblib\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom heatmap import corrplot\n%matplotlib inline","0eb3e9f1":"model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\nbatch_converter = alphabet.get_batch_converter()\nif torch.cuda.is_available():\n    model = model.cuda()","5441323c":"activities = pd.read_excel('..\/input\/enzyme-substrate-data\/Supplementary_Table_S2.xlsx', engine='openpyxl')\nactivities.columns = activities.iloc[0].values\nactivities = activities.iloc[1:-1]\n# to enable stratification\nactivities['Average enzyme activity Category'] = pd.qcut(activities['Average enzyme activity\u2628'], 10)\nncbi_accession_2_enzyme_activity = activities[['NCBI Accession', 'Average enzyme activity\u2628']].set_index('NCBI Accession')","da84f9e7":"sns.distplot(activities['Average enzyme activity\u2628'])\nplt.xlabel('Average enzyme activity', fontsize=12)\nplt.title('Histogram of available enzyme activities', fontsize=16)","a0e03b27":"header_plasmid_seq_list = list(esm.data.read_fasta('..\/input\/enzyme-substrate-data\/Supplementary_Material_S1.fasta'))","7c16e95b":"def get_protein_seq(plasmid_seq):\n    data = {\n        'dna_sequence': plasmid_seq,\n        'output_format': 'fasta'\n    }\n\n    response = requests.post('https:\/\/web.expasy.org\/cgi-bin\/translate\/dna2aa.cgi', data=data)\n\n    reading_frames = re.split(r'>.*Frame [0-3]\\n', response.text)[1:]\n    len_before_stop_codon = [seq.find('-') for seq in reading_frames]\n    argmax_len_before_stop = max(range(6), key=lambda i: len_before_stop_codon[i])\n    selected_protein_seq = reading_frames[argmax_len_before_stop]\n    return selected_protein_seq","5ace6bb9":"random.seed(42)\nplasmid_seqs_to_check = random.sample(header_plasmid_seq_list, k=3)","d7de6b1c":"i = 0\nf'Header: {plasmid_seqs_to_check[i][0]}, selected translation: {get_protein_seq(plasmid_seqs_to_check[i][1])}'","f18d4104":"Image('..\/input\/helpingillustrations\/translation_blast_check_1.jpg', width=500)","89338128":"input_seq_list = []\nenzyme_activity_list = []\nfor header, plasmid_seq in tqdm(header_plasmid_seq_list):\n    enzyme_seq = get_protein_seq(plasmid_seq)\n    input_seq_list.append((_, enzyme_seq))\n    # getting enzyme activity\n    ncbi_accession = header.split('.')[0].replace('>', '').replace(' ', '_')\n    enzyme_activity = ncbi_accession_2_enzyme_activity.loc[ncbi_accession].values[0]\n    enzyme_activity_list.append(enzyme_activity)\n    \n_, _, batch_tokens = batch_converter(input_seq_list)\nif torch.cuda.is_available():\n    batch_tokens = batch_tokens.to(device='cuda', non_blocking=True)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[33])\nenzyme_encodings_np = results[\"representations\"][33].mean(axis=1).cpu().numpy()\nenzyme_activities_np = np.array(enzyme_activity_list)","0d50410c":"train_size = 0.75\n(enzyme_encodings_train, enzyme_encodings_test, \n ys_train, ys_test, \n indices_train, indices_test) = train_test_split(enzyme_encodings_np, \n                                       enzyme_activities_np,\n                                       np.arange(len(enzyme_encodings_np)),\n                                        train_size=train_size, \n                                        random_state=42, \n                                        stratify=activities['Average enzyme activity Category'])","974d78c9":"scaler = StandardScaler()\nenzyme_encodings_train_scaled = scaler.fit_transform(enzyme_encodings_train)\nenzyme_encodings_test_scaled = scaler.transform(enzyme_encodings_test)","7ce931d3":"def get_enzyme_name(enzyme_header):\n    match = re.search(r'>.*[\\.0-9]+\\s(.*)\\s\\[.*', enzyme_header)\n    return match.group(1)\nenzyme_names = [get_enzyme_name(header) for header, _, in header_plasmid_seq_list]\nenzyme_names_train = [enzyme_names[i] for i in indices_train]\n\n\ndef jitter(coordinates_seq):\n    stdev = .05 * (max(coordinates_seq) - min(coordinates_seq))\n    jittered_seq = coordinates_seq + np.random.rand(len(coordinates_seq)) * stdev\n    return jittered_seq\n\n\nenzyme_name_2_color = dict()\ndef visualize_dim_reduction_with_enzyme_names(enzyme_encodings_reduced_np, jitter=False, title=''):\n    # to have consistent enzymes' coloring\n    global enzyme_name_2_color\n    plt.figure(figsize=(5, 5))\n    cycled_colors = plt.get_cmap(\"tab10\")(np.linspace(0, 1, 8))\n    legend_entries = set()\n    for point_i, enzyme_name in enumerate(enzyme_names_train):\n        if enzyme_name in enzyme_name_2_color:\n            color_this = enzyme_name_2_color[enzyme_name]\n        else:\n            color_this = cycled_colors[len(enzyme_name_2_color)]\n            enzyme_name_2_color[enzyme_name] = color_this\n        if enzyme_name not in legend_entries:\n            label = enzyme_name\n            legend_entries.add(enzyme_name)\n        else:\n            label = None\n        plt.scatter(enzyme_encodings_reduced_np[point_i, 0], \n                    enzyme_encodings_reduced_np[point_i, 1], \n                    marker='.', s=100, color=color_this, label=label)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.title(title, fontsize=15)\n    \n\nreducers_list = [trimap.TRIMAP(n_dims=2, verbose=False, n_iters=5000), \n                 umap.UMAP(n_components=2), \n                 TSNE(n_components=2), \n                 PCA(n_components=2)]\n\n# storing encoding for each method, for further reuse\nreducer_name_2_reduced_coords = dict()\nfor reducer in reducers_list:\n    _enzyme_encodings_reduced = reducer.fit_transform(np.concatenate((enzyme_encodings_train_scaled, \n                                                                      enzyme_encodings_test_scaled)))\n    method_name = str(reducer.__class__).split('.')[-1][:-2]\n    reducer_name_2_reduced_coords[method_name] = _enzyme_encodings_reduced\n    \nfor reducer in reducers_list:\n    method_name = str(reducer.__class__).split('.')[-1][:-2]\n    _enzyme_encodings_train_reduced = reducer_name_2_reduced_coords[method_name][:len(indices_train)]\n    visualize_dim_reduction_with_enzyme_names(_enzyme_encodings_train_reduced, title=method_name)","644545ca":"for reducer in reducers_list:\n    method_name = str(reducer.__class__).split('.')[-1][:-2]\n    _enzyme_encodings_train_reduced = reducer_name_2_reduced_coords[method_name][:len(indices_train)]\n    \n    fig_dims = (6, 4.5)\n    fig, ax = plt.subplots(figsize=fig_dims)\n    sc = ax.scatter(jitter(_enzyme_encodings_train_reduced[:,0]), \n                    jitter(_enzyme_encodings_train_reduced[:,1]), \n                    c=ys_train, marker='.', s=100)\n    plt.colorbar(sc, label='Average Enzyme Activity')\n    plt.title(method_name, fontsize=15)","c27f1754":"dim_UB = 10\ndim_LB = 5\n\n_helping_vis_array = np.zeros((dim_UB, dim_UB - dim_LB + 1))\nmax_mean_abs_top_corr = -float('inf')\nbest_num_reduced_features = None\n\n\ndef get_enzyme_encodings_reduced(num_reduced_features, \n                                 enzyme_encodings_train_scaled=enzyme_encodings_train_scaled, \n                                 enzyme_encodings_test_scaled=enzyme_encodings_test_scaled):\n    reducer = trimap.TRIMAP(n_dims=num_reduced_features, verbose=False, n_iters=5000)\n    enzyme_encodings_reduced = reducer.fit_transform(np.concatenate((enzyme_encodings_train_scaled, \n                                                                     enzyme_encodings_test_scaled)))\n    pca = PCA(num_reduced_features)\n    enzyme_encodings_reduced = pca.fit_transform(StandardScaler().fit_transform(enzyme_encodings_reduced))\n    return enzyme_encodings_reduced\n\n\nfor num_reduced_features in tqdm(range(dim_LB, dim_UB + 1)):\n    enzyme_encodings_reduced = get_enzyme_encodings_reduced(num_reduced_features)\n    enzyme_encodings_train_reduced = enzyme_encodings_reduced[:len(indices_train)]\n    train_encodings_with_activations = np.hstack((enzyme_encodings_train_reduced,\n                                                  ys_train.reshape(-1, 1)))\n    \n    correlations_with_activity = pd.DataFrame(train_encodings_with_activations).corr().iloc[:-1, -1]\n    _helping_vis_array[:num_reduced_features, num_reduced_features - dim_LB] = correlations_with_activity\n    \n    mean_abs_top_corr = np.mean(sorted(np.abs(correlations_with_activity))[-3:])\n    if mean_abs_top_corr > max_mean_abs_top_corr:\n        max_mean_abs_top_corr = mean_abs_top_corr\n        best_num_reduced_features = num_reduced_features   ","b344794a":"f'The estimated best number of dimensions: {best_num_reduced_features}'","ba6322a8":"fig = go.Figure(data=go.Heatmap(z=_helping_vis_array,\n                                zmin=-1, \n                                zmax=1,\n                                hovertemplate='%{x}<br>' + '%{y}<br>Corr: %{z}<extra><\/extra>',\n                                y=[f'New Feature: {i}' for i in range(dim_UB)],\n                                x=[f'{i} dims.' for i in range(dim_LB, dim_UB + 1)],\n                                colorscale=px.colors.diverging.RdBu_r,\n                                colorbar = dict(title='Correlation with Activation'))\n               )\nfig.update_xaxes(tickangle=45)\nfig.update_layout(title='Feature Correlations with Enzyme Activity')\nfig.show()","a93b0467":"enzyme_encodings_reduced = get_enzyme_encodings_reduced(best_num_reduced_features)\nenzyme_encodings_train_reduced = enzyme_encodings_reduced[:len(indices_train)]\nplt.figure(figsize=(8, 8))\ncorrplot(pd.DataFrame(enzyme_encodings_train_reduced).corr(), size_scale=300)\nplt.title('Multicollinearity check', fontsize=16, loc='right')","9652f113":"mse = make_scorer(r2_score, greater_is_better=True, needs_proba=False)\nskf = KFold(n_splits=2, shuffle=True, random_state=42)","5a7e9a2a":"lgb_model = lgb.LGBMRegressor(boosting_type='gbdt',\n                              objective='rmse',\n                              n_jobs=-1,\n                              verbose=0)\n\ndimensions = [Real(0.01, 0.8, 'log-uniform', name='learning_rate'),\n              Integer(2, 5, name='num_leaves'),\n              Integer(1, 3, name='max_depth'),\n              Integer(1, 3, name='min_child_samples'),\n              Real(0.2, 1.0, 'uniform', name='subsample'),\n              Real(0.1, 1.0, 'uniform', name='colsample_bytree'),\n              Real(1e-9, 20, 'log-uniform', name='reg_lambda'),\n              Real(1e-9, 1.0, 'log-uniform', name='reg_alpha'),\n              Integer(1, 50, name='n_estimators')]\n\n# The objective function to be minimized\ndef make_objective(model, X, y, space, cv, scoring):\n    # This decorator converts your objective function with named arguments into one that\n    # accepts a list as argument, while doing the conversion automatically.\n    @use_named_args(space) \n    def objective(**params):\n        model.set_params(**params)\n        return np.mean(cross_val_score(model, \n                                        X, y, \n                                        cv=cv, \n                                        n_jobs=-1,\n                                        scoring=scoring))\n    return objective\n\nobjective = make_objective(lgb_model,\n                           enzyme_encodings_train_reduced, ys_train,\n                           space=dimensions,\n                           cv=skf,\n                           scoring=mse)","1d606a97":"gp_round = gp_minimize(func=objective,\n                       dimensions=dimensions,\n                       acq_func='gp_hedge',\n                       n_calls=100, \n                       random_state=42)","c570ba4b":"best_params = {dimensions[i].name: param_value \n               for i, param_value in enumerate(gp_round.x)}\nprint(f'The best params for LGB are: {best_params}')\nlgb_model.set_params(**best_params)\nlgb_model.fit(enzyme_encodings_train_reduced, ys_train)","18ddc7e5":"lgb.plot_importance(lgb_model, max_num_features=10)","7a62b758":"def plot_out_of_sample_predictions(model, enzyme_encodings_test_reduced):\n    preds = model.predict(enzyme_encodings_test_reduced)\n    print(f'Correlation: {scipy.stats.spearmanr(ys_test, preds)}')\n    print(f'R^2: {r2_score(ys_test, preds)}')\n    print(f'MAE: {np.sqrt(mean_absolute_error(ys_test,preds))}')\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(ys_test, preds, s=100)\n    plt.scatter(np.arange(min(ys_test), max(ys_test), 0.05), \n                np.arange(min(ys_test), max(ys_test), 0.05), \n                c='b', s=5, label='ideal')\n\n    plt.xlabel('True Enzyme Activity', fontsize=12)\n    plt.ylabel('Predicted Enzyme Activity', fontsize=12)\n    plt.legend()\n    plt.title('Visual Check of out-of-sample predictions', fontsize=16)\n\n    \nplot_out_of_sample_predictions(lgb_model, enzyme_encodings_reduced[len(indices_train):])","a23ce4d8":"selected_lightgbm_features = sorted(range(best_num_reduced_features), \n                                    key=lambda i: -lgb_model.feature_importances_[i])[:4]","ff3cd91b":"lasso_model = LassoCV()\nlasso_model.fit(enzyme_encodings_train_reduced, ys_train)\nplot_out_of_sample_predictions(lasso_model, enzyme_encodings_reduced[len(indices_train):])","c118a86f":"selected_lasso_features = list(np.where(lasso_model.coef_ != 0)[0])","346af85e":"sorted_corr_feature_idx = sorted(zip(np.abs(_helping_vis_array[:, -1]), range(best_num_reduced_features)), \n                                 key=lambda x: -x[0])\n\nselected_corr_features = [feature_idx for _, feature_idx in sorted_corr_feature_idx[:3]]\n\nrobust_regressor = HuberRegressor()\nrobust_regressor.fit(enzyme_encodings_train_reduced[:, selected_corr_features], \n                     ys_train)  \nenzyme_encodings_test_reduced = enzyme_encodings_reduced[len(indices_train):][:, selected_corr_features]\nplot_out_of_sample_predictions(robust_regressor, enzyme_encodings_test_reduced)","0f81bce6":"robust_regressor = HuberRegressor()\nrobust_regressor.fit(enzyme_encodings_train_reduced[:, selected_lightgbm_features], \n                     ys_train)  \nenzyme_encodings_test_reduced = enzyme_encodings_reduced[len(indices_train):][:, selected_lightgbm_features]\nplot_out_of_sample_predictions(robust_regressor, enzyme_encodings_test_reduced)","a4192fb2":"robust_regressor = HuberRegressor()\nrobust_regressor.fit(enzyme_encodings_train_reduced[:, selected_lasso_features], \n                     ys_train)  \nenzyme_encodings_test_reduced = enzyme_encodings_reduced[len(indices_train):][:, selected_lasso_features]\nplot_out_of_sample_predictions(robust_regressor, enzyme_encodings_test_reduced)","f2dec14e":"robust_regressor = HuberRegressor()\nrobust_regressor.fit(enzyme_encodings_train_reduced, \n                     ys_train)  \nenzyme_encodings_test_reduced = enzyme_encodings_reduced[len(indices_train):]\nplot_out_of_sample_predictions(robust_regressor, enzyme_encodings_test_reduced)","e859bc7a":"Let's visually compare the results of the different dimensionality reductions. We'll check out our target variable values in the reduced 2D feature space, as it was done in the [example](https:\/\/github.com\/facebookresearch\/esm\/blob\/master\/examples\/variant_prediction.ipynb) provided by the transformer authors. Also, we'll use regexp to parse enzyme names from the fasta header and we'll check out these names on top of the reduced feature spaces as well.\n\nNote, that we'll always visualize train data only.\nTriMap and t-SNE require to reduce the dimensionality of all the data at once. Then the pipeline for new samples prediction will be:\n\u00a0 \u00a0* concatenate the new (unknown) samples with the training dataset,\n\u00a0 \u00a0* perform dim. reduction,\n\u00a0 \u00a0* retrain the model on the training data,\n   * predict the new unknown samples.\n\u00a0 \u00a0\nNot ideal, but we need the best in the class dim. reduction given the size of our training data. We'll stick to the outlined pipeline for all the dim. reduction techniques.","f4866be7":"## Feature selection based on Lasso Regression\nWe'll use a default grid search for tuning the regularization parameter.","e39b6f48":"### Features selected based on correlation","ed25ebbf":"### Enzyme names in new feature spaces","8af7ef8a":"# Conclusion","9636a92c":"### Plasmid sequences","2d173de5":"# Huber Regression\n\nTo check the usefulness of LightGBM-based feature selection, let's first create a model using all TriMap features, and features selected based on individual feature correlation.","999ae135":"### Features selected with Lasso Regression","5fc1cfa8":"It's interesting to observe how some groups of samples obviously stay nearby in all dim. reductions. TriMap seems to live up to its reputation on our data.\u00a0\u00a0\n\nNext, let's check the activity in the different feature spaces.\n\n**Update after the [suggestion by one of the Transformer authors, Joshua Meier](https:\/\/github.com\/facebookresearch\/esm\/issues\/17#issuecomment-747854466)**: `esm1b_t33_650M_UR50S` Transformer encodes enzymes differently. One interesting observation would be emerged compact groups in other than TriMap techniques (previously the compact groups were as obvious in the TriMap projections only).\n\n### Activities in new feature spaces","f84499b4":"## Dimensionality reduction\n\n### Visual inspection\n[Before applying PCA it's important to scale the data](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html). So we'll stick to the standardized data with zero mean and unit variance.","e003f0bc":"We can see that with the new Transformer model ESM-1b robust regression performs well directly on TriMap features.","364aa7d8":"Routines for the Bayesian hyperparameters tuning are reused from [this workshop](https:\/\/colab.research.google.com\/github\/lmassaron\/kaggledays-2019-gbdt\/blob\/master\/skopt_workshop_part2.ipynb).","f2c1078a":"**Update after the [suggestion by one of the Transformer authors, Joshua Meier](https:\/\/github.com\/facebookresearch\/esm\/issues\/17#issuecomment-747854466) to switch to `esm1b_t33_650M_UR50S`**: In several projections, we can see local feature space regions in which proteins have similar activity values. Let's proceed with TriMap, but now it seems that it might be worth combining features from different dimensionality reduction techniques.","34697ac3":"### Enzyme activites","35c59532":"The out-of-sample correlation is impressive! Sure, the result is far from perfect. \n\nHowever, for such a tiny dataset I find the result astonishing: the self-supervised protein embeddings enabled us to extract some signal about protein activity from 54 training samples only! \n\nExciting times!\n_______________\nFor the sake of completeness, I must mention that \n\u00a0 \u00a0* the result seems to be dependent on the dimensionality reduction quality. During my brief experimentation, I limited TriMap's number of iterations and it resulted in poor outcomes;\n\u00a0 \u00a0* one must be cautious when choosing hyperparameter bounds for LightGBM, in order to limit the model's opportunities to overfit. \n___________________\n\n***Declaimers***: \n   1. Training, hyperparameters tuning, feature selection is performed using training data. **However**, experimentation with several Transformer models, experimentation with bounds for LightGBM parameters, and creation of several final models in fact turn our test set into a validation set, as we inevitably overfit to it, picking the best final models based on it. This might become a possible reason for worse model performance once it is used in practice. For rigorous assessment of the final model performance, the test dataset must be used just once.\n   2. Enzymes in the training data were bacterial thiolases. Due to the extremely small size of the training dataset one should not expect the models to generalize well on bacterial thiolases with properties not covered by the tiny dataset. For this task on top of such a tiny dataset domain-specific features would provide better model robustness, compared to our 'blind' dimensionality reduction\/feature selection on top of the high-dim. self-learned encodings. Please, note that the purpose of the presented experiments was to verify the quality of the Transformer embeddings, not to create a generally applicable average bacterial thiolases activity predictor.","db3e9787":"# Libraries","295f5af2":"## Feature selection based on LightGBM","ed0bf54a":"# Loading data\nLet's load the discussed supplementary data from the paper by Serina L Robinson et al.","4fd64465":"### Initial TriMap features","a9a4af01":"## Constructing the final dataset\nLet's gather raw enzyme encodings with corresponding activities.","bfd0e089":"Let's check the individual feature correlation with the target.","4536e237":"It checks out. Analogously, the top result from BLAST matched the header for the remaining 2 sampled cases.","575b95fc":"The selected reduction has a new feature with one of the highest correlations with the target. \n\nThere are several features that are similarly correlated to the target. Intuitively, it might seem like a possible multicollinearity symptom. But, please, note that we've applied PCA on top of the raw TriMap features. And PCA resolves multicollinearity explicitly.\u00a0","86e773d8":"### Pre-lowering the dimensionality\nThe TriMap visualization above suggests that linear transformation on top of the TriMap result might be helpful: there's an obvious diagonal line in the TriMap feature space, so it seems reasonable to further project data on this line using PCA, as the diagonal obviously would correspond to the first principal component.\n\nWhy are we talking about dim. reduction again? Previously, we reduced dimensionality into 2D for visual inspection. 2D might require too aggressive compression of the original information from 1280D, making it hard for our models to focus on the enzyme activity. For this reason, we'd like to start with slightly less aggressive compression. \n\nLet's experiment with dimensionalities starting from 5D and higher. But, again, we cannot go into too high dimensions, as we have too few training samples. Let's set 10D as our upper bound on the pre-lowered dimensionality. \n\nWe'll iterate over dimensionalities between 5 and 10 and for each dim. reduction, we'll compute the correlation between enzyme activities and values of each new feature. We'll heuristically chose the dimensionality reduction corresponding to the highest mean absolute value of correlation, averaged over the top 3 features, i.e. for a given dim. reduction we're talking about 3 features which are correlated with activation the most, in abs value.","744fb223":"### Features selected with LightGBM","61b1d0f5":"## Train-test split","60215e11":"**Version 8 Update**: *luckily, [this notebook was briefly reviewed](https:\/\/github.com\/facebookresearch\/esm\/issues\/17#issuecomment-747854466) by one of the FAIR Transformer authors, Joshua Meier. Joshua suggested to use a different pre-trained net:*\n> @SamusRam, this is very cool work! Thanks for sharing it with us. Feel free to try our newest model ESM-1b (esm1b_t33_650M_UR50S) which should give you even stronger results. Excited to see what you build with this next!\n\n*After migration to the newer model, I've also briefly played with outputs of dimensionality reduction algorithms on top of the new protein embeddings.*\n\n# Intro\n\nI came across a paper [\"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\"](https:\/\/www.biorxiv.org\/content\/10.1101\/622803v3.full.pdf) by A. Rives, J. Meier, T. Sercu, S. Goyal et al.\u00a0\n\nIn the paper, the authors created a deep language model for proteins. A Transformer neural net was trained on 250 million protein sequences in a self-supervised manner, i.e. the net was trained to predict masked amino acid tokens based on the remaining context of the protein sequence, without providing any additional domain-specific labels.\u00a0\n\nThe resulting encodings of individual amino acid tokens were shown to reflect the acid's biochemical properties.\n\nFurthermore, after processing the whole sequence for a particular protein, the authors applied global average pooling to the final hidden layer of the decoder. The obtained vector representation of the protein was shown to encode biological variations of proteins.\u00a0\n\n![](https:\/\/media.giphy.com\/media\/vqX9dLCrh6ZLG\/giphy.gif)\n\nThis seems to be huge. [Proteins are behind crucial functions in living organisms](https:\/\/www.youtube.com\/watch?v=oefAI2x2CQM&t=46s). And now we have a numerical encoding of protein characteristics.\n\n\nThe purpose of this notebook is to gain hands-on experience with such a powerful tool and to check its generalization promise first-hand.\n\n# Idea how to challenge the FAIR Transformer\n\nIn order to check what the learned protein embeddings are capable of, let's use data shared with a recent paper [\"Machine learning-based prediction of activity and substrate\u00a0specificity for OleA enzymes in the thiolase superfamily\"](https:\/\/academic.oup.com\/synbio\/article\/5\/1\/ysaa004\/5847618) by S. L. Robinson et al.\n\nEnzymes are proteins that enable or accelerate biochemical reactions. \n\nFor each enzyme, one can find its average activity in the [paper supplementary data](https:\/\/academic.oup.com\/synbio\/article\/5\/1\/ysaa004\/5847618#supplementary-data). In the supplementary materials, we can also find plasmid sequences responsible for enzyme production. The provided plasmid sequences dictated which proteins were generated in the conducted experiments, therefore this data enable us to translate plasmid sequences into\u00a0amino acid sequences. And on top of the\u00a0amino acid sequences, the FAIR transformer can be used to obtain vector representations, which arguably contain enzyme characteristics. Enzyme activity seems to be one particular characteristic.\n\nIs it possible to predict the enzyme activity from the self-learned protein encodings? Can we do it using only a few training samples?\n\nThe thing is, there are just 73 underlying enzymes. And the transformer embedding dimensionality is considerable.\n\nIt seems to be a serious test for the quality of the self-learned protein encodings.\u00a0\n\nTo better understand how challenging this task is, let's compare it with the problem the paper addresses. The paper aims to predict how a particular enzyme would act on a selected molecule (such molecules are called substrates). The dataset used in the paper consists of more than a thousand samples. Moreover, for each training sample, the authors carefully engineered physicochemical and structural features. To sum up,\n\n|| Our ML Task | The Paper ML task |\n| --- | --- | --- | \n| **Features** | self-learned from protein sequences | physicochemical and structural domain knowledge |\n| **Number of Training Samples** | 54 | 821 |\n| **Number of Features** | 1280 | 335 |\n\nFor us to be able to handle such a\u00a0curse of dimensionality,\u00a0\u00a0 \u00a0\n  * the FAIR transformer embeddings need to be awesome,\u00a0 \u00a0\n  * we need to reduce dimensionality efficiently.\n  \n*A note on obtaining protein sequences: the path of least resistance would be to look up the protein sequences in open databases, based on the NCBI Accession provided together with the activities. However, our proposed plan would better correspond to reality, when the FAIR transformer might be used on the translated sequence rather than on a DB entry. Moreover, it will motivate us to check out the transcription\/translation and to fully leverage the provided paper data.*\u00a0 \u00a0\n\n## Plan\u00a0\n\n   1. First, we'll gather all the data and translate plasmid sequences. Then we'll split our data into training and test in proportion 75%\/25%, which corresponds to\u00a054 training samples and 19 test samples. We'll use a random stratified split. For stratification, we'll binarize activation values into 10\u00a0equal-sized intervals (in terms of sample count).\u00a0 \u00a0\n   2. Next, we'll reduce the dimensionality of the protein embeddings. We'll visually check if the unsupervised dimensionality reduction leads to features that are informative enough for the prediction of enzymes' activity. In the paper by\u00a0A. Rives, J. Meier, T. Sercu, S. Goyal et al. it has been shown that PCA preserves domain information encoded into the embeddings. [TriMap](https:\/\/arxiv.org\/abs\/1910.00204), which is a relatively novel technique for non-linear dimensionality reduction, is initialized with PCA. Thus, it seems logical to try it out as well. Moreover, TriMap was reported to outperform the commonly used PCA, t-SNE, and UMAP. To see it first hand we'll create t-SNE and UMAP visualizations as well. The method showing the most promising results in 2D will be used to map the data into a lower-dimensional space.\n   3. Even after reducing the number of features from 1280 down to dozen(s), we still might have too many features for our tiny training set. So, as our next step, we'll create a boosting ensemble of decision trees to predict enzyme activity. The ensemble model performs feature selection. Therefore, we can use the trained model to select features that are the most relevant for our predictive task. The ensembles' hyperparameters will be tuned using Bayesian optimization. For this, we're going to reuse some pieces from [the great workshop by Luca Massaron and Pietro Marinelli](https:\/\/github.com\/lmassaron\/kaggledays-2019-gbdt). We'll use the coefficient of determination as our metric. Based on feature importance averaged over LightGBM, we'll select a few top features.\u00a0 \u00a0\n   4. Please note, that the gradient boosting tree-ensembles, in general, have too high variance for such a tiny dataset. We'll have to limit it brutally to avoid overfitting. For these reasons, as a counterpart to LightGBM, we'll also create a model with higher bias which also internally performs feature selection: \u00a0linear regression with L1 regularization, known as Lasso Regression.\n\u00a0 \u00a05. Next, we'll get down to training one more model. We'll again use linear regression considering the small dataset size. Now, however, we'll use Huber loss to experiment with models robust to outliers. We'll create the robust regression for:\n       *  all TriMap features;\n       *  features selected based on correlation in univariate fashion,\n       *  features selected using Gradient Tree Boosting (LightGBM),\n       *  features selected using Lasso Regression.\n       \n*Declaimer*: Training, hyperparameters tuning, feature selection is performed using training data. **However**, experimentation with several Transformer models, experimentation with bounds for LightGBM parameters, and creation of several final models in fact turn our test set into a validation set, as we inevitably overfit to it, picking the best final models based on it. This might become a possible reason for worse model performance once it is used in practice. For rigorous assessment of the final model performance, the test dataset must be used just once.","648f5934":"# Loading the pretrained transformer net","0fbb759d":"BLAST result:","d34493dc":"# Translating plasmid sequences\nA basic overview of the protein generation from generic code can be found in [this youtube video](https:\/\/www.youtube.com\/watch?v=bKIpDtJdK8Q). There are [6 possible ways how genetic code can be read](https:\/\/en.wikipedia.org\/wiki\/Reading_frame#Genetic_code) in the transcription process. We're going to use [Expasy\u00a0tool](https:\/\/web.expasy.org\/translate\/) to obtain all 6 possible reading frames. To get the most likely translation, we'll use the rule of thumb from [this tutorial](https:\/\/ase.tufts.edu\/biology\/bioinformatics\/exercise3.asp): \"What we typically look for in identifying the proper translation is the frame that gives the longest amino acid sequence before a stop codon is encountered.\"\u00a0\nTo verify the obtained protein sequences, we'll use [BLAST](https:\/\/blast.ncbi.nlm.nih.gov\/Blast.cgi) tool to find a protein from the database with the best alignment score with the amino acid sequence we obtained.\u00a0","746265b0":"### Checking the selected protein sequences\nLet's use the [BLAST tool](https:\/\/blast.ncbi.nlm.nih.gov\/Blast.cgi) to manually verify 3 obtained protein sequences (chosen at random out of all sequences).","35267cc9":"Installing and importing the required packages, including [ESM package](https:\/\/github.com\/facebookresearch\/esm) that contains PyTorch implementation of and pre-trained weights for the transformer protein language models in \"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\" by A. Rives, J. Meier, T. Sercu, S. Goyal et al."}}