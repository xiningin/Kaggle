{"cell_type":{"6a1eb7a4":"code","1cbed462":"code","20e7c827":"code","25882e4f":"code","36eda3bd":"code","48b96132":"code","bc467520":"code","088648d0":"code","adc7b857":"code","51725275":"code","eabfff83":"code","e99c9261":"code","30d42323":"code","0f171a4e":"code","607d6148":"code","5396cac5":"code","a18e8a2c":"code","98e9fd63":"code","e76902b5":"code","4dce1fd0":"code","37169b65":"code","e016ff3f":"code","f60de132":"code","01887910":"code","1ce24f4b":"code","6b951738":"code","631f355b":"code","a15e451c":"code","94ae0281":"code","75f27b1a":"code","8187d37a":"code","17b25b91":"code","ca962e1f":"code","2968a79b":"code","fedf3dd5":"code","6791d618":"code","cf121c16":"code","a1280032":"code","d20b4b55":"code","4ba12b58":"code","e1758bf4":"code","0feff586":"code","2a0f0391":"code","b2f7d7ae":"code","6da0e5f9":"code","8e329647":"code","fc006424":"code","cf36816b":"code","cc84c98e":"code","5b9214ec":"code","cfdae2f3":"code","481c4391":"code","9a7d0751":"code","9e1565b9":"code","e9b0b210":"code","8d547ec6":"code","4aa750ec":"code","b3d08eea":"code","8f6da66c":"code","6059f4f3":"code","c2023da4":"markdown","bf251955":"markdown","86c2d79d":"markdown","673e16f8":"markdown","a2c6f2e6":"markdown","43cebf7c":"markdown","1389a319":"markdown","73982181":"markdown","cd2dfa8b":"markdown","e3bd8396":"markdown","06981651":"markdown","e87e9431":"markdown","b5e72646":"markdown","df706a5c":"markdown","d655bd98":"markdown","8d944ece":"markdown","1490937d":"markdown","c36a4218":"markdown"},"source":{"6a1eb7a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nimport string\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom textblob import TextBlob\nfrom nltk import ngrams\nfrom tqdm import tqdm\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth = 170\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, Callback\nimport tensorflow_hub as hub","1cbed462":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n# glove_embeddings = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')","20e7c827":"print(train.shape, test.shape)","25882e4f":"train.target.value_counts()","36eda3bd":"plt.figure(figsize=(10, 5))\nplt.pie(train.target.value_counts(),\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nplt.legend(['Fake Diaster Tweet', 'Real Diaster Tweet'])\nplt.title(\"Real and Fake Tweet Distribution\", fontsize=16)\nplt.show()","48b96132":"train = train.reindex(np.random.permutation(train.index))","bc467520":"train.head()","088648d0":"print(train.info(), \"\\n\", test.info())","adc7b857":"train_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain_keyword_null_count = train[train.keyword.isnull() == True].shape[0]\ntest_keyword_null_count = test[test.keyword.isnull() == True].shape[0]\n\ntrain_location_null_count = train[train.location.isnull() == True].shape[0]\ntest_location_null_count = test[test.location.isnull() == True].shape[0]\n\nprint(\"Training data having {}% Keyword as Null\".format((train_keyword_null_count*100)\/train_len))\nprint(\"Test data having {}% Keyword as Null\\n\".format((test_keyword_null_count*100)\/test_len))\n\nprint(\"Training data having {}% Loaction as Null\".format(train_location_null_count*100\/train_len))\nprint(\"Test data having {}% Location as Null\\n\".format(test_location_null_count*100\/test_len))","51725275":"plt.figure(figsize=(10, 5))\nplt.pie(train.keyword.isnull().value_counts(),\n        autopct = '%1.2f%%',\n        labels = ['Not Null', \"Null\"],\n        shadow = True,\n        explode=(0.05, 0),\n        startangle=60)\nplt.title('Keyword Distribution', fontsize=16)\nplt.legend()\nplt.show()","eabfff83":"train.location.isnull().value_counts()","e99c9261":"plt.figure(figsize=(10, 5))\nplt.pie(train.location.isnull().value_counts(),\n        autopct = '%1.2f%%',\n        labels = ['Not Null', \"Null\"],\n        shadow = True,\n        explode=(0.05, 0),\n        startangle=80)\nplt.title('Location Distribution')\nplt.legend()\nplt.show()","30d42323":"train[~train.location.isnull()].location.head(10)","0f171a4e":"train.text[:10]","607d6148":"# Let's drop location column from train and test data.\ntrain.drop(\"location\", axis = 1, inplace = True)\ntest.drop(\"location\", axis = 1, inplace = True)","5396cac5":"# merge keyword with the text. You can keep it seperate also.\ntrain.keyword.fillna(\"\", inplace = True)\ntest.keyword.fillna(\"\", inplace = True)\n\ntrain.text = train.text + \" \" + train.keyword\ntest.text = test.text + \" \" + test.keyword","a18e8a2c":"train.text[0:2]","98e9fd63":"# dropping keyword column\ntrain.drop(\"keyword\", axis = 1, inplace = True)\ntest.drop(\"keyword\", axis = 1, inplace = True)","e76902b5":"train_filter0 = train.target == 0\ntrain_filter1 = train.target == 1","4dce1fd0":"# Calculating length of texts\ntrain[\"length\"] = train.text.map(len)\ntest[\"length\"] = test.text.map(len)","37169b65":"#lets check the histogram of length in train and test data\nplt.figure(figsize=(12, 8))\nsns.distplot(train.length, label = \"Training Data\", color='red')\nsns.distplot(test.length, label = \"Testing Data\", color='yellow')\nplt.title(\"Tweet Length Distribution(Train vs Test)\", fontsize=16)\nplt.legend(fontsize=12)\nplt.show()","e016ff3f":"#lets check the histogram of length in training data for both targets\nplt.figure(figsize=(12, 8))\nsns.distplot(train[train_filter0].length, label = \"Fake Diaster\", color='red')\nsns.distplot(train[train_filter1].length, label = \"Real Diaster\", color='yellow')\nplt.title(\"Tweet Length Distribution(Training Data)\", fontsize=16)\nplt.legend(fontsize=12)\nplt.show()","f60de132":"train[\"word_cnt\"] = train.text.apply(lambda x : len(x.split(\" \")))\ntest[\"word_cnt\"] = test.text.apply(lambda x : len(x.split(\" \")))\n\ntrain[\"a_count\"] = train.text.apply(lambda x : len([char for char in str(x) if char == \"@\"]))\ntest[\"a_count\"] = test.text.apply(lambda x : len([char for char in str(x) if char == \"@\"]))\n\ntrain[\"hash_count\"] = train.text.apply(lambda x : len([char for char in str(x) if char == \"#\"]))\ntest[\"hash_count\"] = test.text.apply(lambda x : len([char for char in str(x) if char == \"#\"]))","01887910":"plt.figure(figsize=(12, 8))\nsns.distplot(train[train_filter0].word_cnt, label = \"Fake Diaster\", color='red')\nsns.distplot(train[train_filter1].word_cnt, label = \"Real Diaster\", color='yellow')\nplt.title(\"Tweet Word Count Distribution(Training Data)\", fontsize=16)\nplt.legend(fontsize=12)\nplt.show()","1ce24f4b":"plt.figure(figsize=(12, 8))\nsns.distplot(train[train_filter0].a_count, label = \"Fake Diaster\", color='red')\nsns.distplot(train[train_filter1].a_count, label = \"Real Diaster\", color='green')\nplt.title(\"Tweet @-tagging Count Distribution(Training Data)\", fontsize=16)\nplt.legend(fontsize=12)\nplt.show()","6b951738":"plt.figure(figsize=(12, 8))\nsns.distplot(train[train_filter0].hash_count, label = \"Fake Diaster\", color='red')\nsns.distplot(train[train_filter1].hash_count, label = \"Real Diaster\", color='green')\nplt.title(\"Tweet #-tag Count Distribution(Training Data)\", fontsize=16)\nplt.legend(fontsize=12)\nplt.show()","631f355b":"def dict_formation(data):\n    '''\n    Module creates dictonary of words\n    \n    Input - Text\n    \n    Returns - Word Dictonary\n    '''\n    dict_word = {}\n    #length = data.shape[0]\n    for sent in data.text.tolist():\n        words = sent.split(\" \")\n        for word in words:\n            word = word.lower()\n            try:\n                dict_word[word] = dict_word[word]+1\n            except:\n                dict_word[word] = 1\n    return dict_word","a15e451c":"train_target0_words_dict = dict_formation(train[train_filter0])\ntrain_target1_words_dict = dict_formation(train[train_filter1])\n\ntest_words_dict = dict_formation(test)\ntrain_word_dict = dict_formation(train)","94ae0281":"target0_words = train_target0_words_dict.keys()\ntarget1_words = train_target1_words_dict.keys()\n\ntrain_words = list(target0_words) + list(target1_words)\n\ncorrelation_of_words_target01 = len(set(target0_words) & set(target1_words))*100\/(len(target0_words) + len(target1_words))\ncorrelation_of_words_train_test = len(set(train_words) & set(test_words_dict.keys()))*100\/(len(train_words) + len(test_words_dict.keys()))\n\nprint(\"Train data target labels having common words {:.2f}% \".format(correlation_of_words_target01))\nprint(\"Train and Test data having common words {:.2f}% \".format(correlation_of_words_train_test))","75f27b1a":"def get_ngram_dataframe(n, data, label):\n    train_ngram = ngrams(data.text.str.cat(sep=' ').split( ), n=n)\n    train_ngram = Counter(train_ngram)\n    train_ngram = dict(train_ngram)\n    train_ngram = dict(sorted(train_ngram.items(), key=lambda x: x[1], reverse=True))\n\n    train_ngram_df = pd.DataFrame()\n    train_ngram_df[label] = train_ngram.keys()\n    \n    train_ngram_df['Count'] = train_ngram.values()\n    \n    return train_ngram_df","8187d37a":"unigram_0 = get_ngram_dataframe(1, train[train_filter0], 'Unigram')\nunigram_1 = get_ngram_dataframe(1, train[train_filter1], 'Unigram')\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 12), constrained_layout=True)\nfig.suptitle('Most common 30 Unigram', fontsize=16)\nsns.barplot(y='Unigram', x='Count', data=unigram_0.head(30), color='red', ax=ax[0], label='Fake Diaster Tweet')\nsns.barplot(y='Unigram', x='Count', data=unigram_1.head(30), color='green', ax=ax[1], label='Real Diaster Tweet')\nax[0].legend(fontsize=12)\nax[1].legend(fontsize=12)\nplt.show()","17b25b91":"bigram_0 = get_ngram_dataframe(2, train[train_filter0], 'Bigram')\nbigram_1 = get_ngram_dataframe(2, train[train_filter1], 'Bigram')\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 12), constrained_layout=True)\nfig.suptitle('Most common 30 Bigrams', fontsize=16)\nsns.barplot(y='Bigram', x='Count', data=bigram_0.head(30), color='red', ax=ax[0], label='Fake Diaster Tweet')\nsns.barplot(y='Bigram', x='Count', data=bigram_1.head(30), color='green', ax=ax[1], label='Real Diaster Tweet')\nax[0].legend(fontsize=12)\nax[1].legend(fontsize=12)\nplt.show()","ca962e1f":"trigram_0 = get_ngram_dataframe(3, train[train_filter0], 'Trigram')\ntrigram_1 = get_ngram_dataframe(3, train[train_filter1], 'Trigram')\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 12), constrained_layout=True)\nfig.suptitle('Most common 30 Trigram', fontsize=16)\nsns.barplot(y='Trigram', x='Count', data=trigram_0.head(30), color='red', ax=ax[0], label='Fake Diaster Tweet')\nsns.barplot(y='Trigram', x='Count', data=trigram_1.head(30), color='green', ax=ax[1], label='Real Diaster Tweet')\nax[0].legend(fontsize=12)\nax[1].legend(fontsize=12)\nplt.show()","2968a79b":"def load_embed(file):\n    '''\n    Module create the Glove embedding from the Glove text file.\n    \n    Input - Embedding file.\n    \n    Returns - Embedding Dictonary\n    '''\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","fedf3dd5":"%%time\nglove = '..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\nprint(\"Extracting GloVe embedding\")\nembed_glove = load_embed(glove)","6791d618":"def build_vocab(texts):\n    '''\n    Creates vocabulary\n    \n    Input - Text\n    \n    Returns - vocab Dictonary\n    '''\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word.lower()] += 1\n            except KeyError:\n                vocab[word.lower()] = 1\n    return vocab","cf121c16":"import operator\ndef check_coverage(vocab, embeddings_index):\n    '''\n    To check coverage of the data vocabulary and embedding index\n    \n    Input:\n        vocab - Data Vocabulary\n        embeddings_index - Already trained embedding index\n    \n    Returns - Out of vocabulary dictionary and prints the coverge.\n    '''\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    \n    return unknown_words\n","a1280032":"# Lets check the embedding coverage before cleaning of data.\nvocab_train = build_vocab(train['text'])\nprint(\"Glove : Train\")\noov_glove_train = check_coverage(vocab_train, embed_glove)\n\nvocab_test = build_vocab(test['text'])\nprint(\"Glove : Test\")\noov_glove_test = check_coverage(vocab_test, embed_glove)","d20b4b55":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                       \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                       \"you're\": \"you are\", \"you've\": \"you have\" }","4ba12b58":"%%time\ntrain.text = train.text.apply(lambda x: x.lower())\ntest.text = test.text.apply(lambda x: x.lower())","e1758bf4":"%%time\ntrain.text = train.text.apply(lambda x : \" \".join([contraction_mapping[word].lower() if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\ntest.text = test.text.apply(lambda x : \" \".join([contraction_mapping[word].lower() if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))","0feff586":"# Let's check coverage after contraction replacement. Yeyy, coverage percentage has increased.\n\nvocab_train = build_vocab(train['text'])\nprint(\"Glove : Train\")\noov_glove_train = check_coverage(vocab_train, embed_glove)\n\nvocab_test = build_vocab(test['text'])\nprint(\"Glove : Test\")\noov_glove_test = check_coverage(vocab_test, embed_glove)","2a0f0391":"def split_textnum(text):\n    '''\n    To seperate numbers from the words.\n    \n    Input - Word\n    \n    Returns - Number seperated list of items.\n    '''\n    match = re.match(r\"([a-z]+)([0-9]+)\", text, re.I)\n    if match:\n        items = \" \".join(list(match.groups()))\n    else:\n        match = re.match(r\"([0-9]+)([a-z]+)\", text, re.I)\n        if match:\n            items = \" \".join(list(match.groups()))\n        else:\n            return text\n    return (items)","b2f7d7ae":"# Let's remove special charecters and unwanted datas. This is totally manual task, kind of real pain of NLP data cleaning.\ndef clean_text(text): \n            \n    # Special characters\n    text = re.sub(r\"%20\", \" \", text)\n    #text = text.replace(r\".\", \" \")\n    text = text.replace(r\"@\", \" \")\n    text = text.replace(r\"#\", \" \")\n    #text = text.replace(r\":\", \" \")\n    text = text.replace(r\"'\", \" \")\n    text = text.replace(r\"\\x89\u00fb_\", \" \")\n    text = text.replace(r\"??????\", \" \")\n    text = text.replace(r\"\\x89\u00fb\u00f2\", \" \")\n    text = text.replace(r\"16yr\", \"16 year\")\n    text = text.replace(r\"re\\x89\u00fb_\", \" \")\n    \n    text = text.replace(r\"mh370\", \" \")\n    text = text.replace(r\"prebreak\", \"pre break\")\n    text = re.sub(r\"\\x89\u00fb\", \" \", text)\n    text = re.sub(r\"re\\x89\u00fb\", \"re \", text)\n    text = text.replace(r\"nowplaying\", \"now playing\")\n    text = re.sub(r\"\\x89\u00fb\u00aa\", \"'\", text)\n    text = re.sub(r\"\\x89\u00fb\", \" \", text)\n    text = re.sub(r\"\\x89\u00fb\u00f2\", \" \", text)\n    \n    \n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    \n    text = re.sub(r'http\\S+', ' ', text)\n    text = re.sub(r\"\u00aas\", \" \", text)\n    text = re.sub(r\"\u00aa\", \" \", text)\n    text = re.sub(r\"\\x9d\", \" \", text)\n    text = re.sub(r\"\u00f2\", \" \", text)\n    text = re.sub(r\"\u00aat\", \" \", text)\n    text = re.sub(r\"\u00f3\", \" \", text)\n    text = text.replace(r\"11yearold\", \"11 year old\")\n    text = re.sub(r\"typhoondevastated\", \"typhoon devastated\", text)\n    text = re.sub(r\"bestnaijamade\", \"best nijamade\", text)\n    text = re.sub(r\"gbbo\", \"The Great British Bake Off\", text)\n    text = re.sub(r\"\u00ef\", \"\", text)\n    text = re.sub(r\"\u00efwhen\", \"when\", text)\n    text = re.sub(r\"selfimage\", \"self image\", text)\n    text = re.sub(r\"20150805\", \"2015 08 05\", text)\n    text = re.sub(r\"20150806\", \"2015 08 06\", text)\n    text = re.sub(r\"subreddits\", \"website for weird public sentiment\", text)\n    text = re.sub(r\"disea\", \"chinese famous electronic company\", text)\n    text = re.sub(r\"lmao\", \"funny\", text)\n    text = text.replace(r\"companyse\", \"company\")\n    \n    text = text.replace(r\"worldnews\", \"world news\")\n    text = text.replace(r\"animalrescue\", \"animal rescue\")\n    text = text.replace(r\"freakiest\", \"freak\")\n    \n    text = text.replace(r\"irandeal\", \"iran deal\")\n    text = text.replace(r\"directioners\", \"mentor\")\n    text = text.replace(r\"justinbieber\", \"justin bieber\")\n    text = text.replace(r\"okwx\", \"okay\")\n    text = text.replace(r\"trapmusic\", \"trap music\")\n    text = text.replace(r\"djicemoon\", \"music ice moon\")\n    text = text.replace(r\"icemoon\", \"ice moon\")\n    text = text.replace(r\"mtvhottest\", \"tv hottest\")\n    text = text.replace(r\"r\u00ec\u00a9union\", \"reunion\")\n    text = text.replace(r\"abcnews\", \"abc news\")\n    text = text.replace(r\"tubestrike\", \"tube strike\")\n    text = text.replace(r\"prophetmuhammad\", \"prophet muhammad muslim dharma\")\n    text = text.replace(r\"chicagoarea\", \"chicago area\")\n    text = text.replace(r\"yearold\", \"year old\")\n    text = text.replace(r\"meatloving\", \"meat love\")\n    text = text.replace(r\"standuser\", \"standard user\")\n    text = text.replace(r\"pantherattack\", \"panther attack\")\n    text = text.replace(r\"youngheroesid\", \"young hearos id\")\n    text = text.replace(r\"idk\", \"i do not know\")\n    text = text.replace(r\"usagov\", \"united state of america government\")\n    text = text.replace(r\"injuryi\", \"injury\")\n    text = text.replace(r\"summerfate\", \"summer fate\")\n    text = text.replace(r\"kerricktrial\", \"kerrick trial\")\n    text = text.replace(r\"viralspell\", \"viral spell\")\n    text = text.replace(r\"collisionno\", \"collision\")\n    text = text.replace(r\"socialnews\", \"social news\")\n    text = text.replace(r\"nasahurricane\", \"nasa hurricane\")\n    text = text.replace(r\"strategicpatience\", \"strategic patience\")\n    text = text.replace(r\"explosionproof\", \"explosion proof\")\n    text = text.replace(r\"selfies\", \"photo\")\n    text = text.replace(r\"selfie\", \"photo\")\n    text = text.replace(r\"worstsummerjob\", \"worst summer job\")\n    text = text.replace(r\"realdonaldtrump\", \"real america president\")\n    text = text.replace(r\"omfg\", \"oh my god\")\n    text = text.replace(r\"jap\u00ecn\", \"japan\")\n    text = text.replace(r\"breakingnews\", \"breaking news\")\n    \n    text = \" \".join([split_textnum(word) for word in text.split(\" \")])\n    \n    text = \"\".join([c if c not in string.punctuation else \"\" for c in text])\n    text = ''.join(c for c in text if not c.isdigit())\n    text = text.replace(r\"\u00f7\", \"\")\n    \n    text = re.sub(' +', ' ', text)\n    # text = text.encode('utf-8')\n    return text","6da0e5f9":"%%time\ntrain.text = train.text.apply(lambda x : clean_text(x))\ntest.text = test.text.apply(lambda x : clean_text(x))\ntrain.text = train.text.apply(lambda x : \" \".join([contraction_mapping[word].lower() if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\ntest.text = test.text.apply(lambda x : \" \".join([contraction_mapping[word].lower() if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))","8e329647":"# Let's check the coverage, Yeyy, It's improved again. We are moving in the right dirrection.\n\nvocab_train = build_vocab(train['text'])\nprint(\"Glove : Train\")\noov_glove_train = check_coverage(vocab_train, embed_glove)\n\nvocab_test = build_vocab(test['text'])\nprint(\"Glove : Test\")\noov_glove_test = check_coverage(vocab_test, embed_glove)","fc006424":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ntrain.text = train.text.apply(lambda x : \"\".join([lemmatizer.lemmatize(word) for word in x]))\ntest.text = test.text.apply(lambda x : \"\".join([lemmatizer.lemmatize(word) for word in x]))","cf36816b":"vocab_train = build_vocab(train['text'])\nprint(\"Glove : Train\")\noov_glove_train = check_coverage(vocab_train, embed_glove)\n\nvocab_test = build_vocab(test['text'])\nprint(\"Glove : Test\")\noov_glove_test = check_coverage(vocab_test, embed_glove)","cc84c98e":"import gc\ndel oov_glove_test\ndel embed_glove\ngc.collect()","5b9214ec":"from sklearn.metrics import f1_score, recall_score, precision_score\n\nclass ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.Y_train = train_data\n        self.X_val, self.Y_val = validation_data\n        \n        self.train_precision_score = []\n        self.train_recall_score = []\n        self.train_f1_score = []\n        \n        self.val_precision_score = []\n        self.val_recall_score = []\n        self.val_f1_score = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        \n        train_prediction = np.round(self.model.predict(self.X_train, verbose=0))\n        \n        train_precision = precision_score(self.Y_train, train_prediction, average='macro')\n        train_recall = recall_score(self.Y_train, train_prediction, average='macro')\n        train_f1 = f1_score(self.Y_train, train_prediction, average='macro')\n        \n        self.train_precision_score.append(train_precision)\n        self.train_recall_score.append(train_recall)\n        self.train_f1_score.append(train_f1)\n        \n        val_prediction = np.round(self.model.predict(self.X_val, verbose=0))\n        \n        val_precision = precision_score(self.Y_val, val_prediction, average='macro')\n        val_recall = recall_score(self.Y_val, val_prediction, average='macro')\n        val_f1 = f1_score(self.Y_val, val_prediction, average='macro')\n        \n        self.val_precision_score.append(val_precision)\n        self.val_recall_score.append(val_recall)\n        self.val_f1_score.append(val_f1)\n        \n        print('\\n Epoch - {} - Training Precision - {:.6} - Training Recall - {:.6} - Training F1-Score - {:.6}'.format(\n        epoch+1, train_precision, train_recall, train_f1))\n        \n        print('\\n Epoch - {} - Validation Precision - {:.6} - Validation Recall - {:.6} - Validation F1-Score - {:.6}'.format(\n        epoch+1, val_precision, val_recall, val_f1))\n        ","cfdae2f3":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n\n\nimport tokenization\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","481c4391":"class BertTraining:\n    \n    def __init__(self, bert_layer, fold_k=2, dropout=0.2, max_seq_len=160, lr=0.0001, epochs=15, batch_size=32):\n        \n        self.fold_k = fold_k\n        self.bert_layer = bert_layer\n        self.max_seq_len = max_seq_len\n        self.lr = lr\n        self.dropout = dropout\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.models = []\n        self.scores = {}\n        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n    def bert_encode(self, texts):\n        \n        all_token_ids = []\n        all_masks = []\n        all_segments = []\n        \n        for text in texts:\n            \n            text = self.tokenizer.tokenize(text)\n            text = text[: self.max_seq_len-2]\n            input_seqence = ['[CLS]'] + text + ['[SEP]']\n            padding_length = self.max_seq_len - len(input_seqence)\n            \n            tokens = self.tokenizer.convert_tokens_to_ids(input_seqence)\n            tokens = tokens + [0]*padding_length\n            pad_masks = [1]*len(input_seqence) + [0]*padding_length\n            segment_ids = [0]*self.max_seq_len\n            \n            all_token_ids.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n        \n        return np.array(all_token_ids), np.array(all_masks), np.array(all_segments)\n    \n    def bert_model(self):\n        \n        input_token_id = Input(shape=(self.max_seq_len, ), dtype=tf.int32, name='input_token_id')\n        input_mask = Input(shape=(self.max_seq_len, ), dtype=tf.int32, name='input_mask')\n        input_segment = Input(shape=(self.max_seq_len, ), dtype=tf.int32, name='input_segment')\n        \n        _, sequence_output = self.bert_layer([input_token_id, input_mask, input_segment])\n        clf_output = sequence_output[:, 0, :]\n        \n        if self.dropout == 0:\n            output = Dense(1, activation='sigmoid')(clf_output)\n        else:\n            dropout = Dropout(self.dropout)(clf_output)\n            output = Dense(1, activation='sigmoid')(dropout)\n        \n        model = Model(inputs=[input_token_id, input_mask, input_segment], outputs=output)\n        \n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n        \n        return model\n    \n    def train_model(self, X):\n        skf = StratifiedKFold(n_splits=self.fold_k, random_state=SEED, shuffle=True)\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X.text, X.text)):\n            \n            print(\"\\n Fold {}\\n\".format(fold))\n            \n            X_Train_Encoded = self.bert_encode(X.loc[train_idx, 'text'].str.lower())\n            Y_Train = X.loc[train_idx, 'target']\n            \n            X_Val_Encoded = self.bert_encode(X.loc[val_idx, 'text'].str.lower())\n            Y_Val = X.loc[val_idx, 'target']\n            \n            metrics = ClassificationReport(train_data=(X_Train_Encoded, Y_Train), validation_data=(X_Val_Encoded, Y_Val))\n            \n            model = self.bert_model()\n            model.fit(X_Train_Encoded, Y_Train, epochs=self.epochs, batch_size=self.batch_size, callbacks=[metrics],\n                     validation_data=(X_Val_Encoded, Y_Val), verbose=1)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train' : {\n                    'precision' : metrics.train_precision_score,\n                    'recall' : metrics.train_recall_score,\n                    'f1_score': metrics.train_f1_score\n                },\n                \n                'validation' : {\n                    'precision' : metrics.val_precision_score,\n                    'recall' : metrics.val_recall_score,\n                    'f1_score': metrics.val_f1_score\n                }\n            }\n            \n    \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=self.fold_k, ncols=2, figsize=(20, self.fold_k * 6), dpi=100)\n    \n        for i in range(self.fold_k):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1_score'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n\n    def predict(self, X_test):\n        X_test_encode = self.bert_encode(X_test.text.str.lower())\n        Y_pred = np.zeros((X_test_encode[0].shape[0], 1))\n        \n        for model in self.models:\n            Y_pred += model.predict(X_test_encode)\/len(self.models)\n        \n        return Y_pred","9a7d0751":"SEED = 42\nclf = BertTraining(bert_layer, fold_k=3, dropout=0.5, max_seq_len=140, lr=0.0001, epochs=20, batch_size=64)\n\nclf.train_model(train)","9e1565b9":"clf.plot_learning_curve()","e9b0b210":"prediction = clf.predict(test)\nprediction","8d547ec6":"prediction = np.where(prediction < 0.5, 0, 1)\nprediction","4aa750ec":"# test.head(2)","b3d08eea":"result = pd.DataFrame()\nresult[\"id\"] = test['id']\nresult[\"target\"] = np.squeeze(prediction)\nresult.head()","8f6da66c":"result.target.value_counts()","6059f4f3":"result.to_csv('submission.csv', index=False)","c2023da4":"### 2. Exploratory Data Analysis\n* > Keyword - \n> In traing data ~0.80% are missing and ~0.79% in test data\n* > Location - \n> ~33% locations are missing in both traing and test data.","bf251955":"> By looking at the above graph, it seems like fake tweets are having more number of hashtags and most of the tweets have 0 hashtags. Distribution looks almost same for both real or fake tweets.","86c2d79d":"> #### There are two ways two handle this seperate text based columns.\n* Either you can concatenate all text columns or\n* Use it seperately. For this you will have to create seperate layers for each column and finally before dense sigmoid layer you can concatenate all layers.\n\n> In this project will opt the first option.","673e16f8":"> Length distribution of both training and testing set is almost same. Partially we an say that both training and testing data is from the same distribution","a2c6f2e6":"> #### By looking at the location, it seems like data is maually entered. Will drop this column later. Keyword looks like good and seems standard data because in most of the cases spelling is correct and format is also correct.\nLet's check few tweets and understand. It seems like data is coming from standard source like news agency or any other standard sources.","43cebf7c":"> In above graph we can see real tweets have more number of @ tagging than fake ones. Althogh dsitribution looks same for both target labels.","1389a319":"Import all packages which are required in our code and we are using Tensorflow 2.0.","73982181":"# Real or Not? NLP with Disaster Tweets","cd2dfa8b":"> Let's mix the real and fake diaster tweeet so that in a batch both type of tweet will come.","e3bd8396":"> Word count distribution looks approximately same for both target labels. ","06981651":"> We have 7613 records in traing dataset and 3263 records in testing data set. No. of records are less for deep neural network but let's see what can we do to achieve good result.","e87e9431":"### **1. Load the Data**\n> We are loading all the files of this competition. Glove embedding was not part of the project kernel so I have loaded it from external source.","b5e72646":"> Let's lemmatize the words. For more information https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html follow this link.","df706a5c":"> By looking at the above graph and its distribution of lengths, seems like both target labels having approximately same tweet lengths. And most of the tweets having length range 75-150. Let's do some more EDA, will extract some more metadata.","d655bd98":"### 3. Cleaning of Data.\nWe have collected some contraction words and it's mapping. So we will replace the contraction words in the data. \nFor more information, there is one python package also which can be used for this task. Anyways I am using my own.","8d944ece":"> ~13% dictionary is common between training data real and fake labels and ~14% dictionary is common between training and test dataset.","1490937d":"### Introduction\nThis Competition is all about the dataset of twitter specifically tweets. In this era when everybody has internet and digital device like mobiles, tablets and laptops; many people are using it for good cause but at the same time some people misuse these facilities and try to create panic in bad situation. I really don't know the intension of these people why they are against the human and humanity. But at the same time we also have the same weapon I mean the technology, internet etc. to make them stop. So let's stop them by building our model.\n\nBasically we have tweets of diaster time and we just want to classify whether a particular tweet is real or fake. Simple...Isn't it?\n\nAs the project name itself suggests that, it's a binary classification problem. The objective of the competition is to predict about the tweet, whether it's a real disaster tweet or fake. Real tweet label is encoded with 1 and fake with 0.\n\nWe will explore the project in sequential order. Below is the list. Cheers!\n\n1. Load the data.\n2. Exploratory Data Analysis (EDA).\n3. Cleansing of data.\n4. Pre trained Embedding.\n5. Tokenization and vectorization.\n5. Metrices definition.\n6. Model Layes Structure.\n7. validation.\n\n### References\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert","c36a4218":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQVpRaBupmaGPkMbBcuvWeNlW5JszXWACPu7w&usqp=CAU)"}}