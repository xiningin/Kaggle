{"cell_type":{"57a5c5c9":"code","822a1e0c":"code","137177e4":"code","7c034d43":"code","5c900fbb":"code","efe37bdf":"code","8765139f":"code","fde30030":"code","c5a5b989":"code","812bd288":"code","cfe47a90":"code","a81d8585":"code","adcc3bf0":"code","dbcba2fa":"code","a16db694":"code","62310ec9":"code","29a70f4d":"code","652317d2":"code","40346ce2":"markdown","c33abfb3":"markdown","24d3ce1d":"markdown","436d0f4f":"markdown","7debed1a":"markdown","25ec6a9f":"markdown","3639b10c":"markdown","6f13f1a1":"markdown","1f1302b2":"markdown","0bc76177":"markdown","e34b002a":"markdown","20ee519f":"markdown"},"source":{"57a5c5c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport multiprocessing\n\ntry:\n    cpus = multiprocessing.cpu_count()\nexcept NotImplementedError:\n    cpus = 2   # arbitrary default\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","822a1e0c":"dataset = pd.read_csv(\"..\/input\/pubg-finish-placement-prediction\/train_V2.csv\")\ntest_set = pd.read_csv(\"..\/input\/pubg-finish-placement-prediction\/test_V2.csv\")\ndataset.head()","137177e4":"match_amount = len(dataset.matchId.unique())\nprint('Number of matches in the dataset: %d' % match_amount)\ngroup = dataset.groupby(\"groupId\")['matchId'].nunique()\ngroup[group > 1]","7c034d43":"def get_group_values(dataframe,groupby, values):\n    \n    grouped_set = dataframe.groupby(groupby)\n        \n    for value in values:\n        \n        dataframe = dataframe.join(grouped_set[value].rank(ascending=False, pct=True),on=groupby,rsuffix='_percentile')\n        \n        description = grouped_set[value].describe()\n        description.columns = [value+'_count', value+'_mean', value+'_std', value+'_min', value+'_25%', value+'_50%', value+'_75%', value+'_max']\n        dataframe = pd.concat([dataframe,description], axis=1)\n        \n    return dataframe\n    \n%timeit get_group_values(dataset.head(1000), 'matchId', dataset.columns)","5c900fbb":"def get_group_value(args):\n    \n    value, grouped_dataframe = args\n    \n    percentile = grouped_dataframe[value].rank(ascending=False, pct=True)\n    description = grouped_dataframe[value].describe()\n    variable_description = pd.concat([percentile,description], axis=1)\n    variable_description.columns = [value+'_percentile',value+'_count', value+'_mean', value+'_std', value+'_min', value+'_25%', value+'_50%', value+'_75%', value+'_max']\n    return variable_description\n\ndef get_group_values(dataframe,groupby, values):\n    \n    grouped_set = dataframe.groupby(groupby)\n        \n    pool = multiprocessing.Pool(processes=cpus)\n    work = [[value, grouped_set] for value in values]\n    return pd.concat([dataframe]+pool.map(get_group_value, work), axis=1)\n\n%timeit get_group_values(dataset.head(1000), 'matchId', dataset.columns)","efe37bdf":"group_size = dataset.groupby([\"matchId\",\"groupId\"])['Id'].nunique()\ngroup_size.plot(kind='hist', logy = True, title='Group size distribution', bins=100)","8765139f":"matches = group_size.reset_index().matchId.unique()\nnot_solo_matches = group_size[group_size != 1].reset_index().matchId.unique()\nsolo_matches = np.setdiff1d(matches, not_solo_matches)\nsolo_matches_amount = len(solo_matches)\n#not_solo_matches = pd.Series(not_solo_matches)\nprint('Number of matches in the dataset: %d' % match_amount)\nprint('Number of solo matches in the dataset: %d' % solo_matches_amount)\nprint('Percentage of solo matches in the dataset: %f%%' % ((solo_matches_amount\/match_amount)*100))","fde30030":"not_duo_matches = group_size[group_size != 2].reset_index().matchId.unique()\nduo_matches = np.setdiff1d(matches, not_duo_matches)\nduo_matches_amount = len(duo_matches)\n#not_duo_matches = pd.Series(not_duo_matches)\nprint('Number of matches in the dataset: %d' % match_amount)\nprint('Number of duo matches in the dataset: %d' % duo_matches_amount)\nprint('Percentage of duo matches in the dataset: %f%%' % ((duo_matches_amount\/match_amount)*100))","c5a5b989":"not_squad_matches = group_size[group_size >= 5].reset_index().matchId.unique()\nsquad_matches = np.setdiff1d(np.setdiff1d(np.setdiff1d(matches, not_squad_matches), duo_matches),solo_matches)\nsquad_matches_amount = len(squad_matches)\nprint('Number of matches in the dataset: %d' % match_amount)\nprint('Number of squad matches in the dataset: %d' % squad_matches_amount)\nprint('Percentage of squad matches in the dataset: %f%%' % ((squad_matches_amount\/match_amount)*100))","812bd288":"custom_matches = np.setdiff1d(np.setdiff1d(np.setdiff1d(matches, squad_matches), duo_matches),solo_matches)\n#not_squad_matches = np.setdiff1d(groupsize.reset_index().matchID.unique(), custom_matches)\ncustom_matches_amount = len(custom_matches)\nprint('Number of matches in the dataset: %d' % match_amount)\nprint('Number of custom matches in the dataset: %d' % custom_matches_amount)\nprint('Percentage of custom matches in the dataset: %f%%' % ((custom_matches_amount\/match_amount)*100))","cfe47a90":"labels = 'Duo', 'Squad', 'Solo' ,'Custom'\nsizes = [duo_matches_amount, squad_matches_amount, solo_matches_amount, custom_matches_amount]\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nexplode = (0.05, 0.05, 0.05, 0.05)\nplt.pie(sizes, labels=labels, explode=explode, colors=colors)\nplt.axis('equal')\nplt.show()","a81d8585":"solo = set(solo_matches)\nduo = set(duo_matches)\nsquad = set(squad_matches)\ncustom = set(custom_matches)\n\ndef one_hot_game_mode(Id, solo, duo, squad, custom):\n    return int(Id in solo), int(Id in duo), int(Id in squad), int(Id in custom)\n\ndataset['solo'] = 0\ndataset['duo'] = 0\ndataset['squad'] = 0\ndataset['custom'] = 0\ndataset[['solo','duo','squad','custom']] = pd.DataFrame(elem for elem in dataset.matchId.apply(one_hot_game_mode,args=(solo,duo,squad,custom)))\n\ndataset.head()","adcc3bf0":"dataset.damageDealt.plot(kind='hist', logy=True)","dbcba2fa":"norm_list = [\"winPoints\",\"killPoints\",\"damageDealt\",\"rideDistance\",\"walkDistance\", \"swimDistance\", \"longestKill\"]\ndataset[norm_list] =(dataset[norm_list] - dataset[norm_list].mean())\/(dataset[norm_list].max() - dataset[norm_list].min())\ntest_set[norm_list] =(test_set[norm_list] - dataset[norm_list].mean())\/(dataset[norm_list].max() - dataset[norm_list].min())\n#train.killPoints =(train.winPoints-train.winPoints.mean())\/(train.winPoints.max() - train.winPoints.min())\ntrain.winPoints.plot(kind='hist', logy=True)\ntrain.walkDistance.plot(kind='hist', logy=True)","a16db694":"correlation_matrix = dataset.corr()\ncorrelation_matrix.winPlacePerc.sort_values(ascending=False)","62310ec9":"aug_dataset = dataset = pd.read_csv(\"..\/input\/pubgplayerstats\/PUBG_Player_Statistics.csv\")\nprint(str(aug_dataset.columns.values))","29a70f4d":"xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)\n\ndataset_labels = dataset.pop('winPlacePerc')\ndataset = dataset.drop(['Id','groupId','matchId'], axis=1)\ntest_set = test_set.drop(['Id','groupId','matchId'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(dataset, dataset_labels ,test_size=0.2)\n#test = pd.read_csv(\"..\/input\/test.csv\")\n\nxgb.fit(X_train,y_train, eval_metric='rmse', verbose=True)\n\npredictions = xgb.predict(X_test)\nprint(explained_variance_score(predictions,y_test))\nprint(mean_squared_error(predictions,y_test))\n","652317d2":"pred = xgb.predict(test_set)\nprint(pred)\ntest = pd.read_csv(\"..\/input\/sample_submission.csv\")\ntest['winPlacePerc'] = pd.Series(pred)\ntest.to_csv(\"..\/submission.csv\")","40346ce2":"**XGBoost baseline**\n\nXGboosting is a resilient tree-bosting machine learning algorithm known for being robust in these type of datasets without needing of a lot of pre-processing.","c33abfb3":"**Correlation Matrix**\n\nThe main correlators with success include:\n\n1. Walking distance\n2. Number of boosts\n3. Number of weapons acquired\n\nThese all correlate with playtime as someone who plays for longer will end up walking more, getting more boosts (which increase life points over the cap) and acquiring more weapons.\n\nInterestingly winPoints doesn't  correlate strongly with actually winning ","24d3ce1d":"**Match Awareness**\n\n>*\"If you know the enemy and know yourself, you need not fear the result of a hundred battles. If you know yourself but not the enemy, for every victory gained you will also suffer a defeat. If you know neither the enemy nor yourself, you will succumb in every battle.\u201d*  \u2015 Sun Tzu, The Art of War\n\nAs relevant advice as ever. Pretentious quotes aside, my hypothesis is that other's players performance in a match is as important as how well you perform.\n\nIt is not about being good, it is about being better than others.\n\nTo this end I propose a series of context variables:\n\n\n    For individual variable in match:\n         group describe of a feature\n       \n    ","436d0f4f":"\n\nNow we know which matches belong to which game modes we will split the model in three and evaluate its performance.\n\n","7debed1a":"There are some big squads in this ","25ec6a9f":"**Feature Scaling**\n\nThe scale of some of these values are very different and might lead to problems down the line with some machine learning algorithms so we perform min-max normalisation.\n\nThis won't affect the performance of tree estimators like XGBoost or Random Forest but it is known to increase performance of gradient based machine learning methods. ","3639b10c":"**Sepxarating game modes**\n\n> *The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group.*\n\nFirst things first, we need to fix the game mode problem. In this dataset group sizes vary from 1 to just under 100. \n\nThese game modes are fundamentally different specially those nearing 100 players as it likely is a 'zombie mode' match.","6f13f1a1":"**Data Augmentation**\n\nAnother PUBG statistics dataset is available on Kaggle, let's see if we can obtain any useful insights.\n\nThe data contained in the dataset is comprised of 150 diferent data points pertaining to the top-players at the time of collection. \n\nThis might prove be an issue down the line as, in my experience, the average player isn't very good. So the data might be fundamentally different depending on the sample collected. \n\n\nAnother possible concern is that the data contained was collected over a year ago. This, however, should prove a non-issue as anyone familiar with the game can testify all of the development efforts have gone into sketchy monetization and not gameplay, so the meta-game should still be pretty similar.","1f1302b2":"\n**Group Experience**\n\n> *If the same group of players plays in different matches, they will have a different groupId each time.*\n\nPossibly interesting concept: Number of matches played within a group and aggregations based of group history.\n\nIf groupId was kept consistent across matches we can essentially track the group's history and experience playing together.\n\nSadly, it seems like groupIds are only used for a match so cross-match tracking is not possible as is.\n\nIf the player IDs it might be beneficial to try to generate our own group Id's that actually mean something.","0bc76177":"As we lack the convinient WinPlacePerc variable in our augmentation dataset what we will do is compare everything but that.\n\nThe plan is that for every player we would be able to assign the closest player in the augmented dataset and hence have an increased number of data points. This may or may not be useful.\n\nFirst we need a way to convert our augmented data to look like our normal data.\n\nExample:\n\nDepending on our group size (Solo, Duo, Squad) we will look at diferent parts of the dataframe in order to make an apples to apples comparison. ","e34b002a":"**Game Mode as a categorical value**\n\nWe will now add the game mode to the entry to allow the model to learn the difference among game types. \nTo this end we will encode the mode as a one-hot vector so it plays nicely with XGboost and ANNs. ","20ee519f":"Diferent game modes by squad size (Sadly, no longer necessary on V2 data):\n\nSolo - Only 1 player groups allowed\nDuo - Only two player groups allowed\nSquad - Between 2 and 4 player groups\nCustom - Anything goes. Several different game modes like zombies or 20vs.\n"}}