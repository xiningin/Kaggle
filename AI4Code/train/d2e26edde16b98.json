{"cell_type":{"1de5e9ac":"code","e659b398":"code","d692174e":"code","e8451739":"code","2f30cc72":"code","f9fd5cb7":"code","525e7222":"code","220eac1a":"code","589c1892":"code","9fb22deb":"code","71fc2c41":"code","95994849":"code","05922096":"code","8b4592db":"code","6e849fa7":"code","48d70507":"code","dcdee268":"code","2650cde9":"code","97c188aa":"code","fc267414":"code","c171a00c":"markdown","12c1637d":"markdown","a7614512":"markdown","1c2fa1cd":"markdown","b56373cd":"markdown","087bde4a":"markdown","d79575ab":"markdown","4e514805":"markdown","6ebfa6cf":"markdown","c9a60809":"markdown","1c61d3c8":"markdown","3410f4be":"markdown","9b70aeaf":"markdown","879271cc":"markdown","0a6ad41b":"markdown","8a9520be":"markdown","407fc909":"markdown","aa6d9ac5":"markdown","cb6913e1":"markdown"},"source":{"1de5e9ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e659b398":"import tensorflow as tf\nimport keras\nfrom matplotlib import pyplot as plt\n\nINPUT_DIR = \"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\"\nGLOVE_DIR = \"..\/input\/glove-global-vectors-for-word-representation\"\n\nprint(os.listdir(INPUT_DIR))\nprint(os.listdir(GLOVE_DIR))","d692174e":"train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ntrain.head()","e8451739":"with open(os.path.join(INPUT_DIR, 'sample_submission.csv')) as sample_submission:\n    for x in range(5):\n        print(next(sample_submission), end='')","2f30cc72":"with open(os.path.join(INPUT_DIR, 'test.csv')) as sample_submission:\n    for x in range(10):\n        print(next(sample_submission), end='')","f9fd5cb7":"with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as glove_file:\n    for x in range(5):\n        print(next(glove_file))","525e7222":"embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\nprint('Embeddings_index is a map of the words to a', len(embeddings_index['the']), 'dimentional vector.')","220eac1a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Generating the Text corpus in the form of a Numpy list\ncorpus = train['comment_text'].tolist()\nprint(\"Some sample comments we train the Tokenizer on:\\n\", corpus[:3])\n\n# Fitting the tokenizer on the corpus, \ntokenizer = Tokenizer(num_words=1000000)\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# Padding to convert Jagged array into uniform length 2-D time series data\ndata = pad_sequences(sequences)","589c1892":"labels = train['target'].as_matrix()\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\n# Split into Train and Validation Sets\nVALIDATION_SPLIT = 0.25 # Percentage of sample going to the Validation set\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","9fb22deb":"EMBEDDING_DIM = 100\nMAX_SEQUENCE_LENGTH = len(data[0])\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","71fc2c41":"embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n                                            EMBEDDING_DIM,\n                                            weights=[embedding_matrix],\n                                            input_length=MAX_SEQUENCE_LENGTH,\n                                            trainable=False)","95994849":"def build_model():\n    words = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n    x = embedding_layer(words)\n    x = tf.keras.layers.SpatialDropout1D(0.2)(x)\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128, return_sequences=True))(x)\n\n    hidden = tf.keras.layers.concatenate([\n        tf.keras.layers.GlobalMaxPooling1D()(x),\n        tf.keras.layers.GlobalAveragePooling1D()(x),\n    ])\n    hidden = tf.keras.layers.add([hidden, tf.keras.layers.Dense(512, activation='relu')(hidden)])\n    hidden = tf.keras.layers.add([hidden, tf.keras.layers.Dense(512, activation='relu')(hidden)])\n    result = tf.keras.layers.Dense(1, activation='sigmoid')(hidden)\n    \n    model = tf.keras.models.Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","05922096":"model = build_model()\nhistory = model.fit(x = x_train, y = y_train, validation_data=(x_val, y_val), epochs = 2)","8b4592db":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png')","6e849fa7":"print(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('The Loss Function')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","48d70507":"test = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\ntest.head()","dcdee268":"questions = test['comment_text'].tolist()\nq_data = pad_sequences(tokenizer.texts_to_sequences(questions), maxlen=MAX_SEQUENCE_LENGTH)\nprint(q_data.shape)","2650cde9":"result = model.predict(q_data)\nids = test['id'].tolist()","97c188aa":"assert len(result) == len(ids)\nwith open('submission.csv', 'w') as file:\n    file.write('id,prediction\\n')\n    for item in range(len(ids)):\n        file.write(str(ids[item]) + ',' + str(result[item][0]) + '\\n')","fc267414":"with open('submission.csv') as sample_submission:\n    for x in range(5):\n        print(next(sample_submission), end='')","c171a00c":"## Preprocessing Text with Embeddings","12c1637d":"Let's plot a few graphs, see how our model did, and where we can do better.","a7614512":"## Analyzing the Model","1c2fa1cd":"Lets wire up a model. This architecture is derived from this Kernel: https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\n\nTODO: Understand LSTMs and replace with my own architecture.","b56373cd":"### Generating Embedding Matrix and Feeding to Embedding Layer","087bde4a":"We load the data from the CSV file and print the first few lines to see what the data is like. The **Comment_text** column will be what we start working on first, preprocess it into a form we can use.","d79575ab":"Now for the Heavy operation, let's fit the model to the DataSet.","4e514805":"Here we print a sample output and the test set input. The thing to note is that all we have to submit is the expected value of **TARGET** given the **COMMENT TEXT**. Our Classifier can be trained simply on this data and everything else is to aid rejecting false positives.","6ebfa6cf":"### Loading the GloVe Embeddings onto Keras","c9a60809":"### Loading the Training Set","1c61d3c8":"Here we start by reading the GloVe text file. The format here is simple, it's the **token followed by it's 100-D representation, space-separated, in each line**. The token include both words and puncutations, and 's, etc. Next we shall extract data out of this.","3410f4be":"### Some comments from the Problem Statement\n\nHere are the different types of Toxicity labels to help us fine tune our predictions.\n* severe_toxicity\n* obscene\n* threat\n* insult\n* identity_attack\n* sexual_explicit\n\nThere are many more classes storing the severity of attack \/ count of certain targetted entities. Here are the once that we will be tested on, that have 500 examples or more in the provided Training Set.\n* male\n* female\n* homosexual_gay_or_lesbian\n* christian\n* jewish\n* muslim\n* black\n* white\n* psychiatric_or_mental_illness","9b70aeaf":"We have the **Data Tensor**, which is a 317-Dimentional representation of every sentence, padded in the front by 0s till it fits the Max-length of 317.\nNow we use the Embedding matrix to freeze the weights in the Embeddings layer. `keras.Embedding` takes in the *Data Tensor* and outputs a *2-D Vectors array representation of the sentence*.","879271cc":"## Outputting the Result","0a6ad41b":"## Understanding the Input and Output","8a9520be":"### Reading through the input and tokenizing the Comments","407fc909":"### Understanding what the Output should look like","aa6d9ac5":"## The Neural Network Architecture","cb6913e1":"# Simple LSTM with GloVe Embeddings (using only Targets)"}}