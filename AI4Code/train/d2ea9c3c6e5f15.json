{"cell_type":{"0aa02f77":"code","6d2ca0ef":"code","f0163286":"code","cba153b1":"code","00ec5c64":"code","789ca07a":"code","0238830f":"code","3b699a56":"code","2477ff61":"code","19bbe239":"code","5a6a07de":"code","2fbb015c":"code","b05cd3f0":"code","86bcfb3d":"code","fbb124c0":"code","f71531bd":"code","f52e2d89":"code","3a7e047d":"code","6df18ea3":"code","80ed3e6b":"code","aae6b632":"code","e8ff189a":"markdown"},"source":{"0aa02f77":"import numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","6d2ca0ef":"file_path = '\/kaggle\/input\/tabular-playground-series-jun-2021'\ntrain = pd.read_csv(os.path.join(file_path,'train.csv'))\ntest = pd.read_csv(os.path.join(file_path,'test.csv'))","f0163286":"display(train.head())\ndisplay(test.head())","cba153b1":"display(train.info())\ndisplay(test.info())","00ec5c64":"train['target'].value_counts()","789ca07a":"target_replacement = {\n    'Class_1':0,\n    'Class_2':1,\n    'Class_3':2,\n    'Class_4':3,\n    'Class_5':4,\n    'Class_6':5,\n    'Class_7':6,\n    'Class_8':7,\n    'Class_9':8,\n}\n\ntrain['target'] = train['target'].replace(target_replacement)","0238830f":"train['target'].value_counts()","3b699a56":"submission = pd.read_csv(os.path.join(file_path,'sample_submission.csv'),index_col='id')","2477ff61":"submission.head()","19bbe239":"from sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\nkf = StratifiedKFold(n_splits=10)\ntrain[\"kfold\"] = -1\ntrain = train.sample(frac=1).reset_index(drop=True)\ny = train.target.values\nfor f, (t_, v_) in enumerate(kf.split(X=train, y=y)):\n    train.loc[v_, 'kfold'] = f\n\n","5a6a07de":"def predict_lgb(iteration):\n    evaluation_lgb = []\n    test_lgb = np.zeros((len(test),9))\n    params = {\n            'bagging_freq': 1, \n            'verbosity': -1, \n            'seed': 42, \n            'num_threads': -1, \n            'feature_pre_filter': True, \n            'objective': 'multiclass',\n            'n_estimators':2000,\n            'metric': 'multi_logloss', \n            'boosting': 'gbdt', \n            'bagging_fraction': 0.6000000000000001, \n            'feature_fraction': 0.5, \n            'lambda_l1': 10, \n            'lambda_l2': 0.1, \n            'learning_rate': 0.060119000245064017, \n            'max_depth': 8, \n            'min_child_samples': 100, \n            'num_leaves': 127\n        }\n    for fold in range(iteration):\n        i = fold + 1\n        df_train = train[train.kfold != fold].reset_index(drop=True)\n        df_valid = train[train.kfold == fold].reset_index(drop=True)\n        y_train = df_train.target.values\n        y_valid = df_valid.target.values\n        x_train = df_train.drop(['id','target','kfold'],axis = 1)\n        x_valid = df_valid.drop(['id','target','kfold'],axis = 1)\n        clf = LGBMClassifier(**params)\n        clf.fit(x_train,y_train,eval_metric='multi_logloss',eval_set=[(x_valid,y_valid)],\n                early_stopping_rounds = 200,verbose = -1)\n        pred_probs = clf.predict_proba(x_valid)\n        logloss = log_loss(y_valid,pred_probs)\n        evaluation_lgb.append(logloss)\n        print(f'The logloss value of iteration {fold} is {logloss}')\n        test_final = test.drop('id',axis = 1)\n        test_lgb += clf.predict_proba(test_final)\/i\n    return test_lgb,evaluation_lgb\n    ","2fbb015c":"fold = 10\ntest_lgb,evaluation_lgb = predict_lgb(fold)","b05cd3f0":"print(evaluation_lgb)","86bcfb3d":"from catboost import CatBoostClassifier\ndef predict_cb(iteration):\n    evaluation_cb = []\n    test_cb = np.zeros((len(test),9))\n    for fold in range(iteration):\n        i = fold + 1\n        df_train = train[train.kfold != fold].reset_index(drop=True)\n        df_valid = train[train.kfold == fold].reset_index(drop=True)\n        y_train = df_train.target.values\n        y_valid = df_valid.target.values\n        x_train = df_train.drop(['id','target','kfold'],axis = 1)\n        x_valid = df_valid.drop(['id','target','kfold'],axis = 1)\n        clf = CatBoostClassifier(n_estimators=2000,\n                        colsample_bylevel=0.06,\n                        max_leaves=31,\n                        subsample=0.67,\n                        verbose=0,\n                        bootstrap_type='Bernoulli',\n                        thread_count=6,\n                        random_state=42)\n        clf.fit(x_train,y_train)\n        pred_probs = clf.predict_proba(x_valid)\n        logloss = log_loss(y_valid,pred_probs)\n        evaluation_cb.append(logloss)\n        print(f'The logloss value of iteration {fold} is {logloss}')\n        test_final = test.drop('id',axis = 1)\n        test_cb += clf.predict_proba(test_final)\/i\n    return test_cb,evaluation_cb","fbb124c0":"fold = 10\ntest_cb,evaluation_cb = predict_cb(fold)","f71531bd":"print(evaluation_lgb)\nprint(evaluation_cb)","f52e2d89":"evaluation_lgb = np.array(evaluation_lgb)\nevaluation_cb = np.array(evaluation_cb)","3a7e047d":"print(evaluation_lgb.mean())\nprint(evaluation_cb.mean())","6df18ea3":"test_preds = (test_lgb + test_cb)\/2","80ed3e6b":"submission[\"Class_1\"] = test_preds[:,0]\nsubmission[\"Class_2\"] = test_preds[:,1]\nsubmission[\"Class_3\"] = test_preds[:,2]\nsubmission[\"Class_4\"] = test_preds[:,3]\nsubmission[\"Class_5\"] = test_preds[:,4]\nsubmission[\"Class_6\"] = test_preds[:,5]\nsubmission[\"Class_7\"] = test_preds[:,6]\nsubmission[\"Class_8\"] = test_preds[:,7]\nsubmission[\"Class_9\"] = test_preds[:,8]","aae6b632":"submission.to_csv('blend_cb_lgb_3')","e8ff189a":"Let's create the folds first then we will do some feature selection and then baseline prediction. "}}