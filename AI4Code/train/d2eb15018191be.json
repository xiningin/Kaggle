{"cell_type":{"c49c02a9":"code","793bfae3":"code","dff4ade4":"code","cf290f89":"code","c6ed9095":"code","bf535153":"code","c68fea1d":"code","d8018fd4":"code","9426c7c2":"code","5775f80b":"code","81115050":"code","1042299c":"markdown","c6bfc5fd":"markdown","372e1fb7":"markdown","a90ba002":"markdown","90855fc2":"markdown","35766f4f":"markdown","e0de349c":"markdown","f5ae245e":"markdown","042f1a68":"markdown","537bb196":"markdown","e5dd88fa":"markdown","c041e496":"markdown"},"source":{"c49c02a9":"import time\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier","793bfae3":"vehicle = np.loadtxt(\n    '\/kaggle\/input\/vehicle\/vehicle.data', delimiter=',')\nX = vehicle[:, :-1]\ny = vehicle[:, -1]","dff4ade4":"for i in range(len(np.unique(y)-1)):\n    y[:][y[:] == i+1] = i","cf290f89":"def _dataframe(X, y):\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    X = X.astype(\"int64\")\n    y = y.astype(\"int64\")\n\n    feature = []\n    for i in range(len(X.columns)):\n        feature.append(str(i))\n\n    col_rename = {i: j for i, j in zip(X.columns, feature)}\n    X = X.rename(columns=col_rename, inplace=False)\n\n    return X, y, feature","c6ed9095":"def _make_input_fn(X, y, n_epochs=None, shuffle=True):\n    def input_fn():\n        NUM_EXAMPLES = len(y)\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n            dataset = dataset.shuffle(NUM_EXAMPLES)\n        # For training, cycle thru dataset as many times as need (n_epochs=None).\n        dataset = dataset.repeat(n_epochs)\n        # In memory training doesn't use batching.\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn","bf535153":"def _accuracy(evaluate):\n    item = list(evaluate.items())\n    array = np.array(item)\n    return (array[0, 1]).astype(np.float64)","c68fea1d":"def BostedTree(X, y, step):\n    X, y, feature = _dataframe(X, y)\n\n# Training and evaluation input functions.\n    train_input_fn = _make_input_fn(X, y)\n    eval_input_fn = _make_input_fn(X, y,\n                                   shuffle=False,\n                                   n_epochs=1)\n\n# feature selection\n    num_columns = feature\n    feature_columns = []\n    n_classes = len(np.unique(y))\n\n    for feature_name in num_columns:\n        feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                                                dtype=tf.float32))\n    est = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                              n_batches_per_layer=1,\n                                              n_classes=n_classes,\n                                              n_trees=100,\n                                              max_depth=5,\n                                              learning_rate=0.1)\n    fit = est.train(train_input_fn, max_steps=step)\n    score = _accuracy(est.evaluate\n                      (eval_input_fn, steps=1))\n\n    return fit, score","d8018fd4":"K = 2\nN = 50\n\n\nerr_skl = np.zeros((K, N))\nerr_tfbt = np.zeros((K, N))\nfit_ti_skl = np.zeros((K, N))\nfit_ti_tfbt = np.zeros((K, N))\n\n\nkfold = StratifiedKFold(n_splits=K, shuffle=True)\n\nfor k, (train_index, test_index) in enumerate(kfold.split(X, y)):\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    for i in range(1, N + 1):\n        skl = GradientBoostingClassifier(max_depth=10,\n                                         subsample=0.75,\n                                         max_features=None,\n                                         learning_rate=0.25,\n                                         random_state=2,\n                                         criterion=\"mse\",\n                                         n_estimators=i)\n        s0 = time.time()\n        skl.fit(x_train, y_train)\n        s1 = time.time()\n        fit_ti_skl[k, i - 1] = s1-s0\n        err_skl[k, i - 1] = skl.score(x_test, y_test)\n\n        t0 = time.time()\n        fit, score = BostedTree(x_train, y_train, i)\n        t1 = time.time()\n        fit_ti_tfbt[k, i-1] = t1-t0\n        err_tfbt[k, i - 1] = score","9426c7c2":"mean_err_skl = np.mean(err_skl, axis=0)\nmean_err_tfbt = np.mean(err_tfbt, axis=0)\n\nfit_ti_skl = np.mean(fit_ti_skl, axis=0)\nfit_ti_tfbt = np.mean(fit_ti_tfbt, axis=0)","5775f80b":"n = range(1, N + 1)\nfig, ax1 = plt.subplots()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"training time average Sklearn\", color=\"tab:blue\")\nlns1 = ax1.plot(n, fit_ti_skl, '-',  label='Sklearn', color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"training time average - TF.BostedTree\", color=\"tab:green\")\nlns2 = ax2.plot(n, fit_ti_tfbt, '-', label='TF.BostedTree',  color=\"tab:green\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n\nlns = lns1 + lns2\nlabs = [l.get_label() for l in lns]\nax2.legend(lns, labs)\nplt.title(\"Average training time\")\nplt.show()","81115050":"n = range(1, N + 1)\nfig, ax1 = plt.subplots()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Sklearn\", color=\"tab:blue\")\nlns1 = ax1.plot(n, mean_err_skl, '-',  label='Sklearn', color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"TF.BostedTree\", color=\"tab:green\")\nlns2 = ax2.plot(n, mean_err_tfbt, '-', label='TF.BostedTree',  color=\"tab:green\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n\nlns = lns1 + lns2\nlabs = [l.get_label() for l in lns]\nax2.legend(lns, labs)\nplt.title(\"Average accuracy\")\nplt.show()","1042299c":"## 2.3 Convert to Pandas DataFrame\nDefine a method to convert the dataset into the pandas DataFrame. \nThe Tensorflow model here considers the data frame format.","c6bfc5fd":"## 2.2 Replace the labels\nChanging the labels of the target to fit in the TensorFlow model","372e1fb7":"## 4.2 Plot the accuracy trend","a90ba002":"As we saw in the plots, the model from the Sklearn library has better performance in the terms of learning speed and accuracy.\nI showed that Sklearn has a higher speed and TensorFlow is much slower during the training process.\n\nMoreover, the TensorFlow boosted tree by itself does not the ability to use in the Cross-validation loop. By considering the above modification, one could consider pipeline and GridsearchCV as well.\nfor computational reasons, I did not apply the pipeline in this study.\n\nIf you want to use the prepared model as a library, you can find the [repository](https:\/\/github.com\/samanemami\/TFBoostedTree) which I created.\n","90855fc2":"# A comparison between two well-known gradients boosting classifier\n### **Seyedsaman Emami**\n#### 18\/April\/2021\n\n* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Load data\n    * 2.2 Replace the labels\n    * 2.2 Convert to Pandas DataFrame\n* **3. Model preparation**\n     * 3.1 Customize the TF method \n     * 3.2 Training the models\n* **4. Evaluation**\n    * 4.1 training time\n    * 4.2 Accuracy\n* **5. Conclusion**","35766f4f":"# 1. Introduction\nThis is comparison between two well-known gradients boosting classifier from two major libraries (Namely Sklearn and TensorFlow)\n\nI used the [vehicle](https:\/\/www.kaggle.com\/samanemami\/vehicle) dataset for this matter.\nThe TensorFlow model has not been updated for almost three years. \nTherefore, I added some additional methods and illustrated the training time and trend of the accuracy by adding a tree to each boosting iteration between these two models.\n\n**For computational reasons, I set the number of steps (epochs) and the number of trees to 2, and 50. If you want to achieve better accuracy, set them to 10, and 100, and respectively**","e0de349c":"# Conclusion","f5ae245e":"# 4. Evaluation\n## 4.1 Plot the training time","042f1a68":"# 3. Model preparation","537bb196":"## 3.1 Customize the TF method for the sake of scalability\nI defined some methods to make scalability possible between the two models.","e5dd88fa":"## 3.2 Training the models in the same cross-validation\nAt each iteration, we add a new tree to the boosting machine.","c041e496":"# 2. Data preparation\n## 2.1 Load data"}}