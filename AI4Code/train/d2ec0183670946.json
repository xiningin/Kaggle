{"cell_type":{"0ca03d99":"code","a3c3b7ad":"code","2d23d6b3":"code","76ecb210":"code","aca6e9e4":"code","e1194535":"code","eb6744b5":"code","7e6d03e8":"code","f15835bd":"code","2505aad8":"code","70364bd7":"code","fccbaa76":"code","efd8c43a":"code","f79a370c":"code","7c5a2c77":"code","f2023abd":"code","91790c84":"code","004d179d":"code","761c41f6":"code","39b4c236":"code","b6d12dba":"code","e815c308":"code","79b5ced7":"code","dea80d82":"code","63f9f4c7":"code","d53f3004":"code","69022369":"code","aad10a52":"code","95e46a6f":"code","35a8a8fe":"markdown","e38a7704":"markdown","a4c9f12e":"markdown","ab97cc67":"markdown","562793f1":"markdown","b9630b04":"markdown","f9e1bcab":"markdown","65fd2811":"markdown","c46717b9":"markdown","589a5e6c":"markdown","b1618d66":"markdown","989a82c8":"markdown","e8cd2aa3":"markdown","63216a98":"markdown","2e0a5bf6":"markdown","2e3c4c1f":"markdown","5344d54e":"markdown","3efa20da":"markdown","7edf3349":"markdown"},"source":{"0ca03d99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm\n\nfrom scipy.stats import skew\n\nsns.set(style='white', context='notebook', palette='deep')\n\n\n\n# Any results you write to the current directory are saved as output.","a3c3b7ad":"dataset  = pd.read_csv(\"..\/input\/DC_Properties.csv\")\n\ndataset.head()","2d23d6b3":"#dataset.isnull().sum()\n\ndummy_dataset = dataset\n\ndummy_dataset['Price_Flag'] = np.where(dummy_dataset.PRICE > 0 , 1,0)\n\nunknown_dataset = dummy_dataset[dummy_dataset.Price_Flag != 1]\n\nunknown_dataset.shape","76ecb210":"from sklearn.model_selection import train_test_split\n\ndataset = dummy_dataset[dummy_dataset.Price_Flag != 0]\n","aca6e9e4":"dataset = dataset.drop(['Price_Flag','X','Y','CMPLX_NUM','FULLADDRESS','LONGITUDE','CITY','STATE','NATIONALGRID','CENSUS_BLOCK','SALEDATE','QUADRANT'],axis=1)\n\ndataset.GBA = dataset.GBA.fillna(dataset.GBA.mean())\n\ndataset.AYB = dataset.AYB.fillna(dataset.AYB.median())\n\ndataset.STORIES = dataset.STORIES.fillna(dataset.STORIES.median())\n\ndataset.KITCHENS = dataset.KITCHENS.fillna(dataset.KITCHENS.median())\n\ndataset.NUM_UNITS = dataset.NUM_UNITS.fillna(dataset.NUM_UNITS.median())\n\ndataset.YR_RMDL = dataset.YR_RMDL.fillna(dataset.YR_RMDL.median())\n\ndataset.LIVING_GBA = dataset.LIVING_GBA.fillna(dataset.LIVING_GBA.mean())\n\ndataset.STYLE = dataset.STYLE.fillna(dataset.STYLE.mode()[0])\n\ndataset.STRUCT = dataset.STRUCT.fillna(dataset.STRUCT.mode()[0])\n\ndataset.GRADE = dataset.GRADE.fillna(dataset.GRADE.mode()[0])\n\ndataset.CNDTN = dataset.CNDTN.fillna(dataset.CNDTN.mode()[0])\n\ndataset.EXTWALL = dataset.EXTWALL.fillna(dataset.EXTWALL.mode()[0])\n\ndataset.ROOF = dataset.ROOF.fillna(dataset.ROOF.mode()[0])\n\ndataset.INTWALL = dataset.INTWALL.fillna(dataset.INTWALL.mode()[0])\n\ndataset.ASSESSMENT_SUBNBHD  = dataset.ASSESSMENT_SUBNBHD.fillna(dataset.ASSESSMENT_SUBNBHD.mode()[0])\n\ndataset.isnull().sum()\n\n","e1194535":"train_dataset = dataset\n\ntrain_dataset = train_dataset.drop('Unnamed: 0',axis =1)\n\ncat = len(train_dataset.select_dtypes(include=['object']).columns)\nnum = len(train_dataset.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","eb6744b5":"corrmat = train_dataset.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","7e6d03e8":"k = 11 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'PRICE')['PRICE'].index\ncm = np.corrcoef(train_dataset[cols].values.T)\nsns.set(font_scale=1.00)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","f15835bd":"most_corr = pd.DataFrame(cols)\nmost_corr.columns = ['Most Correlated Features']\nmost_corr","2505aad8":"# Gross Building Area vs  Price\n\nsns.jointplot(x=train_dataset['GBA'], y=train_dataset['PRICE'], kind='reg')","70364bd7":"# AYB vs  Price\n\nvar = 'AYB'\ndata = pd.concat([train_dataset['PRICE'], train_dataset[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"PRICE\", data=data)\nfig.axis(ymin=0, ymax=2000000);\nplt.xticks(rotation=90);","fccbaa76":"# Kitchens vs  Price\n\nvar = 'KITCHENS'\ndata = pd.concat([train_dataset['PRICE'], train_dataset[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"PRICE\", data=data)\nfig.axis(ymin=0, ymax=2000000);\nplt.xticks(rotation=90);","efd8c43a":"# Stories vs  Price\n\nvar = 'STORIES'\ndata = pd.concat([train_dataset['PRICE'], train_dataset[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"PRICE\", data=data)\nfig.axis(ymin=0, ymax=5000000);\nplt.xticks(rotation=90);","f79a370c":"from sklearn.preprocessing import LabelEncoder\ncols = train_dataset.select_dtypes(include=['object']).columns\n# Process columns and apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train_dataset[c].values)) \n    train_dataset[c] = lbl.transform(list(train_dataset[c].values))\n\ntrain_dataset.head()","7c5a2c77":"# We use the numpy fuction log which  applies log to all elements of the column\ntrain_dataset[\"PRICE\"] = np.log(train_dataset[\"PRICE\"])\n\n#Check the new distribution \nsns.distplot(train_dataset['PRICE'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_dataset['PRICE'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train_dataset['PRICE'], plot=plt)\nplt.show()\n\ny_train = train_dataset.PRICE.values\n\nprint(\"Skewness: %f\" % train_dataset['PRICE'].skew())\nprint(\"Kurtosis: %f\" % train_dataset['PRICE'].kurt())","f2023abd":"train_dataset = (train_dataset - train_dataset.mean()) \/ (train_dataset.max() - train_dataset.min())","91790c84":"train_dataset = pd.get_dummies(train_dataset)\nprint(train_dataset.shape)","004d179d":"train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.2)\n\ntrain_dataset.shape","761c41f6":"import statsmodels.api as sm\n\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.metrics import mean_squared_error\n\ntrain_dataset_Y = train_dataset.PRICE.values\n\ntrain_dataset_X = train_dataset.drop('PRICE',axis =1)\n\ntrain_dataset_X.shape\n\ntrain_dataset_X = sm.add_constant(train_dataset_X)\n\nPricing_model = sm.OLS(train_dataset_Y,train_dataset_X)\n\nresult = Pricing_model.fit()\n\nprint(result.summary())\n\nprint(\"RMSE: \",np.sqrt(mean_squared_error(result.fittedvalues,train_dataset_Y)))","39b4c236":"test_dataset_Y = test_dataset.PRICE.values\n\ntest_dataset = test_dataset.drop('PRICE',axis=1)\n\npredictions = result.predict(sm.add_constant(test_dataset))\n\nRMSE = np.sqrt(mean_squared_error(predictions,test_dataset_Y))\n\nprint('The RMSE of the predicted values is ',RMSE)\n\n","b6d12dba":"plt.scatter(test_dataset_Y, predictions)\n\nplt.legend()\nplt.title('OLS predicted values')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.show()","e815c308":"from sklearn.linear_model import Ridge\n\nfrom sklearn.linear_model import RidgeCV\n\nfrom sklearn.metrics import r2_score\n\n## training the model\n\ntrain_dataset_X = train_dataset_X.drop('const',axis=1)\n\nregr_cv = RidgeCV(alphas=[0.1,1,2,3,4,5,6,7,0.5,0.8])\n\nmodel_cv = regr_cv.fit(train_dataset_X,train_dataset_Y)\n\nmodel_cv.alpha_\n","79b5ced7":"ridgeReg = Ridge(alpha=5, normalize=True)\n\nridgeReg.fit(train_dataset_X,train_dataset_Y)\n\npred = ridgeReg.predict(test_dataset)\n\n# calculating mse\n\nmse = np.sqrt(mean_squared_error(pred , test_dataset_Y))\n\nprint(\"The Root mean square error of Ridge Regression is \", mse)\n\nprint(\"The R2 value of Ridge Regression is \",r2_score(test_dataset_Y,pred))","dea80d82":"from sklearn.linear_model import Lasso\n\nlassoReg = Lasso(alpha=20, normalize=True)\n\nlassoReg.fit(train_dataset_X,train_dataset_Y)\n\npred = lassoReg.predict(test_dataset)\n\n# calculating mse\n\nrmse = np.sqrt(mean_squared_error(pred, test_dataset_Y))\n\nprint(\"The Root mean square error of Lasso Regression is \", rmse)\n\nprint(\"The R2 value of Lasso Regression is \",r2_score(test_dataset_Y,pred))\n\n#lassoReg.score(test_dataset,test_dataset_Y)\n\n","63f9f4c7":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(train_dataset_X,train_dataset_Y)\n \n# Predicting a new result\ny_pred = regressor.predict(test_dataset)\n\nrmse = np.sqrt(mean_squared_error(y_pred, test_dataset_Y))\n\nprint(\"The Root mean square error of Decision Tree Regression is \", rmse)\n\nprint(\"The R2 value of Decision Tree Regression is \",r2_score(test_dataset_Y,y_pred))\n\n\n","d53f3004":"from sklearn import neighbors\n\n\nknn = neighbors.KNeighborsRegressor(5)\n\npred_test = knn.fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\nRMSE = np.sqrt(mean_squared_error(test_dataset_Y, pred_test))\n\nprint(\"The Root Mean Squared Error of KNN Regression is \",RMSE)\n\nprint(\"The R2 value of KNN Regression is \",r2_score(test_dataset_Y,pred_test))","69022369":"# from sklearn.svm import SVR\n\n# svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n\n# svr_lin = SVR(kernel='linear', C=1e3)\n\n# svr_poly = SVR(kernel='poly', C=1e3, degree=3)\n\n# y_rbf = svr_rbf.fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\n# y_lin = svr_lin.fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\n# y_poly = svr_poly.fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\n# RMSE_1 = np.sqrt(mean_squared_error(test_dataset_Y, y_rbf))\n\n# print(\"The Root Mean Squared Error of SVM (Radial Basis Function) Regression is \",RMSE_1)\n\n# print(\"The R2 value of SVM (Radial Basis Function) Regression is \",r2_score(test_dataset_Y,y_rbf))\n\n# RMSE_2 = np.sqrt(mean_squared_error(test_dataset_Y, y_lin))\n\n# print(\"The Root Mean Squared Error of SVM(Linear) Regression is \",RMSE_2)\n\n# print(\"The R2 value of SVM(Linear) Regression is \",r2_score(test_dataset_Y,y_lin))\n\n# RMSE_3 = np.sqrt(mean_squared_error(test_dataset_Y, y_poly))\n\n# print(\"The Root Mean Squared Error of SVM(Polynomial) Regression is \",RMSE_3)\n\n# print(\"The R2 value of SVM(Polynomial) Regression is \",r2_score(test_dataset_Y,y_poly))","aad10a52":"from sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 25)\n# Train the model on training data\npred = rf.fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\nRMSE_1 = np.sqrt(mean_squared_error(test_dataset_Y, pred))\n\nprint(\"The Root Mean Squared Error of Random Forest Regression is \",RMSE_1)\n\nprint(\"The R2 value of Random Forest Regression is \",r2_score(test_dataset_Y,pred))","95e46a6f":"from sklearn.ensemble import GradientBoostingRegressor\n\npred = GradientBoostingRegressor(n_estimators=100, learning_rate=0.3,max_depth=1, random_state=0, loss='ls').fit(train_dataset_X,train_dataset_Y).predict(test_dataset)\n\nRMSE_1 = np.sqrt(mean_squared_error(test_dataset_Y, pred))\n\nprint(\"The Root Mean Squared Error of Gradient Boosting Regression is \",RMSE_1)\n\nprint(\"The R2 value of Gradient Boosting Regression is \",r2_score(test_dataset_Y,pred))","35a8a8fe":"**Finding the Skewness of the Data**","e38a7704":"**Decision Tree Regression**","a4c9f12e":"**Splitting the dataset to Test and Train after Cleaning and Data Handling**","ab97cc67":"**Random Forest Regression**","562793f1":"**Ridge Regression**","b9630b04":"**The above are the list of Top  Variables (Independent Factors) with postive correlation to the house price that contribute to the Price of the house**\n\nGBA -  Gross building area in square feet\n\nAYB - The earliest time the main portion of the building was built\n\nEYB - The year an improvement was built more recent than actual year built\n\nLatitude - The location of the propert\n\nKitchens - Number of kitchens\n\nStories - Number of stories in primary dwelling\n\nYr_Model - Year structure was remodeled\n\n","f9e1bcab":"**\nLasso Regression**","65fd2811":"**Weighted KNN for Regression**","c46717b9":"**Support Vector Machine Regression**","589a5e6c":"**Running OLS on the Train Dataset**","b1618d66":"**It makes sense that people would pay for the more living area. **","989a82c8":"1. **With 39 features, we cant possibly tell which feature is most related to house prices. so we find the correlation of the Price of the house with every other variables**","e8cd2aa3":"**Finding the top 10 Independent factors that contribute to the Price of the house**","63216a98":"**Data Cleaning and Handling Outliers**","2e0a5bf6":"**a.\tSplit the data into 3 parts \u2013 training_data, test_data and unknown_data**","2e3c4c1f":"**Importing the dataset**\n","5344d54e":"**Gradient Boosting Trees**","3efa20da":"**Labelling the Categorical Data**","7edf3349":"**We cant come to any probable conclusion between age of the house and the Price of the house with this boxplot**"}}