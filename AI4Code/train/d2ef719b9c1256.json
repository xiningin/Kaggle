{"cell_type":{"3e953b5e":"code","3f0daabc":"code","013d41d5":"code","4016a2c6":"code","eb97da4b":"code","461e4613":"code","c3f630b6":"code","3daae691":"code","fa7760fe":"code","4e8107de":"code","8131156a":"code","f437a66f":"code","91df97c4":"code","41b09bdd":"code","be33f114":"code","9882752e":"code","ad29d916":"code","aa823a3d":"code","c4e5e6e3":"code","906547c6":"code","31811792":"code","2a5677b4":"code","2dd70c99":"code","a6243369":"code","d5656f5b":"code","be99c828":"markdown","ac475e9e":"markdown","497ec74b":"markdown","efdd3f30":"markdown","e994def3":"markdown","2ded65b9":"markdown","793de7a4":"markdown"},"source":{"3e953b5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # Linear algebra\nimport pandas as pd # Data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pf # Generates profile reports from a pandas DataFrame\nimport seaborn as sns # data visualization library based on matplotlib\nfrom sklearn import preprocessing # provides several common utility functions and transformer classes to change raw feature vectors into a representation more suitable for the downstream estimators\nfrom sklearn.model_selection import train_test_split # split arrays or matrices into random train and test subsets\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression (aka logit, MaxEnt) classifier\nfrom sklearn.feature_selection import RFE # Feature ranking with recursive feature elimination.\nfrom sklearn import metrics #  includes score functions, performance metrics and pairwise metrics and distance computations\nfrom sklearn.metrics import classification_report, confusion_matrix # Build a text report showing the main classification metrics,Compute confusion matrix to evaluate the accuracy of a classification. \nfrom sklearn.metrics import recall_score,precision_score,f1_score,roc_auc_score,accuracy_score\nimport matplotlib.pyplot as plt # is a state-based interface to matplotlib. It provides a MATLAB-like way of plotting\nimport math\n%matplotlib inline \n# a magic function which sets the backend of matplotlib to the 'inline' backend\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f0daabc":"# Adjust pandas display and formatting settings\n\n# Remove scientific notations and display numbers with 2 decimal points instead\npd.options.display.float_format = '{:,.3f}'.format        \n\n# Increase cell width\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:98% !important; }<\/style>\"))\n\n# Update default style and size of charts\nplt.style.use('ggplot')\n\n# Increase max number of rows and columns to display in pandas tables\npd.set_option('display.max_columns', 500)           \npd.set_option('display.max_rows', 500) ","013d41d5":"# load the data \ndfPersonalLoanData = pd.read_csv(\"..\/input\/bank-personal-loan-modelling\/Bank_Personal_Loan_Modelling.csv\")\ndfPersonalLoanData.head(10)\n","4016a2c6":"# Checking number of raws and columns\ndfPersonalLoanData.shape","eb97da4b":"# Check field types\ndfPersonalLoanData.dtypes","461e4613":"dfPersonalLoanData.describe().T","c3f630b6":"# Check for null values\nprint(dfPersonalLoanData.isnull().sum())\n\n# check for na\nprint(\"\\n\")\nprint(\"*****check for na***** \\n \\n\",  dfPersonalLoanData.isna().sum())","3daae691":"# Number of unique in column(s)\ndfPersonalLoanData.nunique()","fa7760fe":"# Number of people with zero mortgage\nprint (\"Number of people with zero mortgage:\", (dfPersonalLoanData['Mortgage']==0).sum())\nprint(\"Number of people with zero credit card spending per month:\", (dfPersonalLoanData['CCAvg']==0).sum())\n","4e8107de":"# value counts for all categorical fields\nfor col in ['Education', 'Personal Loan', 'Securities Account', 'CD Account', 'Online', 'CreditCard']:\n    print('\\nColumn:', col)         # \"\\n\" indicates new line\n    print(dfPersonalLoanData[col].value_counts())","8131156a":"sns.pairplot(dfPersonalLoanData[['Mortgage', 'Income', 'CCAvg', 'Personal Loan']], hue = 'Personal Loan', diag_kind = 'kde');","f437a66f":"corr = dfPersonalLoanData.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","91df97c4":"dfPersonalLoanData.groupby(dfPersonalLoanData['Personal Loan']).mean()\n\n","41b09bdd":"dfPersonalLoanData.head()","be33f114":"# Let's drop Experience which have some negative values, ID and Zip Code\ndfPersonalLoanData.drop(columns ='ID',inplace=True)\ndfPersonalLoanData.drop(columns='Education', inplace=True)\ndfPersonalLoanData.drop(columns='Family', inplace=True)\ndfPersonalLoanData.drop(columns ='Experience',inplace= True)\ndfPersonalLoanData.drop(columns ='ZIP Code',inplace= True)","9882752e":"dfPersonalLoanData['Personal Loan'].value_counts(normalize=True) ","ad29d916":"# Storing predictors and target in X and y variables\n\n\nX = dfPersonalLoanData.drop('Personal Loan', axis=1)\ny= dfPersonalLoanData['Personal Loan'] # target variable\n\n# Split the data into train and test\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Creating logistic regression model\n\nmodel = LogisticRegression(solver = 'liblinear')\nmodel.fit(X_train, y_train)\n\ny_predicted = model.predict(X_test)\n\n# Evaluate mode performance \nprint(classification_report(y_test, y_predicted))","aa823a3d":"# draw confusion metrix\ndef draw_cm(actual,predicted):\n    cm = confusion_matrix(actual,predicted)\n    sns.heatmap(cm,annot=True, fmt='.2f', xticklabels=[0,1], yticklabels=[0,1])\n    plt.ylabel('observed')\n    plt.xlabel('Predicted')\n    plt.show()\n\n","c4e5e6e3":"draw_cm(y_test, y_predicted)","906547c6":"print('Accuracy on train set: {:.2f}'.format(model.score(X_train, y_train)))\nprint('Accuracy on test set: {:.2f}'.format(model.score(X_test, y_test)))\nprint('Recall score: {:.2f}'.format(recall_score(y_test,y_predicted)))\nprint('ROC AUC score: {:.2f}'.format(roc_auc_score(y_test,y_predicted)))\nprint('Precision score: {:.2f}'.format(precision_score(y_test,y_predicted)))","31811792":" #!pip install yellowbrick","2a5677b4":"from yellowbrick.classifier import ClassificationReport, ROCAUC\nviz = ClassificationReport(LogisticRegression(solver = 'liblinear',random_state=42))\nviz.fit(X_train,y_train)\nviz.score(X_test,y_test)\nviz.show()","2dd70c99":"# Improve the model using GridSearchCV\n# model paramters \nmodel.get_params()\n","a6243369":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{'solver': ['newton-cg','lbfgs','liblinear','sag','saga'], 'C': [0.001,0.01,0.1,0.25,0.5,0.75,1],\n              'class_weight':['balanced'], 'penalty':['l2']}]\ngrid_search = GridSearchCV(LogisticRegression(),param_grid,cv=5, verbose=0)\ngrid_search.fit(X_train,y_train)\ngrid_search.best_estimator_","d5656f5b":"#new model\nmodel_new = LogisticRegression(C=0.25, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=42, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_new.fit(X_train, y_train)\n\npredicted = model_new.predict(X_test)\n\n# Evaluate mode performance \nprint(classification_report(y_test, predicted))\ndraw_cm(y_test, predicted)\nprint('Accuracy on train set: {:.2f}'.format(model_new.score(X_train, y_train)))\nprint('Accuracy on test set: {:.2f}'.format(model_new.score(X_test, y_test)))\nprint('Recall score: {:.2f}'.format(recall_score(y_test,predicted)))\nprint('ROC AUC score: {:.2f}'.format(roc_auc_score(y_test,predicted)))\nprint('Precision score: {:.2f}'.format(precision_score(y_test,predicted)))\n\nviz = ClassificationReport(model_new)\nviz.fit(X_train,y_train)\nviz.score(X_test,y_test)\nviz.show()","be99c828":"### Data Description: ###\nThe file Bank_Personal_Loan_Modelling.csv contains data on 5000 customers. The data include\ncustomer demographic information (age, income, etc.), the customer's\nrelationship with the bank (mortgage, securities account, etc.), and the\ncustomer response to the last personal loan campaign (Personal Loan).\nAmong these 5000 customers, only 480 (= 9.6%) accepted the personal\nloan that was offered to them in the earlier campaign.\n\n### Context: ###\nThis case is about a bank (Thera Bank) whose management wants to\nexplore ways of converting its liability customers to personal loan\ncustomers (while retaining them as depositors). A campaign that the\nbank ran last year for liability customers showed a healthy conversion\nrate of over 9% success. This has encouraged the retail marketing\ndepartment to devise campaigns with better target marketing to\nincrease the success ratio with a minimal budget.\n\n### Attribute Information: ###\n - ID : Customer ID\n - Age : Customer's age in completed years\n - Experience : #years of professional experience\n - Income : Annual income of the customer (\\\\$000)\n - ZIP Code : Home Address ZIP code.\n - Family : Family size of the customer\n - CCAvg : Avg. spending on credit cards per month (\\\\$000)\n - Education : Education Level. \n     - 1: Undergrad \n     - 2: Graduate \n     - 3: Advanced\/Professional\n - Mortgage : Value of house mortgage if any. (\\\\$000)\n - Personal Loan : Did this customer accept the personal loan offered in\nthe last campaign?\n - Securities Account : Does the customer have a securities account with\nthe bank?\n - CD Account : Does the customer have a certificate of deposit\n(CD) account with the bank?\n - Online : Does the customer use internet banking facilities?\n - Credit card : Does the customer use a credit card issued byUniversalBank?","ac475e9e":"90% customers don't have credit card, also this means highly baised data","497ec74b":"# Our Model","efdd3f30":"* True Positive (observed=1,predicted=1):\n     Model predicted that 144 customers shall take Personal loan and they customer took it\n* False Positive (observed=0,predicted=1):\n    Model Predicted 184 Personal loan will take and the customer did not take it but bank didn't loose any money\n* True Negative (observed=0,predicted=0):\n    Model Predicted 1159 Personal loan will not take and the customer did not take it\n* False Negative (observed=1,predicted=0):\n    Model Predicted 13 Personal loan will not take and the customer took it - This is where model should have done better\n    \n* Since the bank wants more people to accept personal loan i.e. less number of False Negative, so that bank doesn't lose real customers who want to take loan. Hence the focus should be on increasing Recall. And improved model did extactly same false negative were down to 13 from 93\n\n\n","e994def3":"### Some observations from data\n* Average Age is 45 years\n* Experience have negative values\n* 90% of the customer doesn\u2019t have a certificate of deposit (CD) account with the bank.\n* Around 71% of the customer doesn\u2019t use a credit card issued by UniversalBank.\n* Around 60% of customers use internet banking facilities.\n* Around 90% of the customer didn\u2019t accept the personal loan offered in the last campaign.\n* Around 90% of the customer don\u2019t have a securities account with the bank.","2ded65b9":"That means target variabe 'Personal Loan ' has strong correlation with `CCAvg` and `Income`","793de7a4":"## Performance metrics\n* Precision: Fraction of actuals per label that were correctly classified by the model\n* Recall: Fraction of predictions that were correctly classified by the model\n* F1-score: Weighted harmonic mean of the precision and recall. F1-score: 2 * (precision * recall) \/ (precision + recall)\n* Support: Number of occurrences of each class in y_test\n* Accuracy: Fraction of all observations that were correctly classified by the model\n* Macro avg: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n* Micro\/weighted avg: Calculate metrics globally by counting the total true positives, false negatives and false positives\n* AUC: Probability that a classifier will rank a random positive sample higher than than a random negative sample."}}