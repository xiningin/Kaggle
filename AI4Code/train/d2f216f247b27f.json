{"cell_type":{"a1d6dd87":"code","ee0cc986":"code","f9046402":"code","56743099":"code","72985931":"code","90999e0e":"code","0dc79308":"code","b74d5c75":"code","0d0a2cfe":"code","44aaef69":"code","f0042f58":"code","9073a51d":"code","65ebceb5":"code","85112a5c":"code","f97c9324":"code","d5a63919":"code","cff19c58":"code","a781a145":"code","184e4f0e":"code","8747ee3b":"code","41f9a246":"code","4b7f7012":"code","07a0ace0":"code","ed300737":"code","5b425a14":"code","8d4aa28c":"code","76b33d26":"code","00b106dd":"code","e6065810":"code","11bc132f":"code","8b09925a":"code","f7ba4cd0":"code","2530938f":"code","940370d1":"code","3740cabd":"code","9f00d2cf":"code","7ed04a9f":"code","5360d8d5":"code","3fa80717":"code","98c5912f":"code","762142a8":"code","5317f414":"code","b0817a47":"code","119132d8":"code","7d0114a8":"code","9c08876c":"code","fee3b305":"code","1aee5798":"code","9a583588":"code","035ebe78":"code","fc2256bd":"code","216873b8":"code","84ecf5bd":"code","b43398e5":"code","cea93cb8":"code","52e3d139":"code","cffef572":"code","06dac999":"code","0d22f2b0":"code","68347d09":"code","6554956b":"code","56a1e76e":"code","10593e1d":"markdown","9b8a6fdc":"markdown","c56ac5a1":"markdown","a21ec730":"markdown","db177cb3":"markdown","10b481fb":"markdown","7f0ea0e1":"markdown"},"source":{"a1d6dd87":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ee0cc986":"\ndata = pd.read_csv(\"..\/input\/Admission_Predict.csv\")","f9046402":"data.head(3)","56743099":"## remove un necessary columns - as Serial No doenst have any impact of results\ndata.drop(['Serial No.'],inplace=True,axis=1)","72985931":"data.head(2)","90999e0e":"## check the correlation between data types","0dc79308":"data.corr()","b74d5c75":"## here the Target is Chance of Admit \n## so now he have to understand the percentage of correlation between available features to target\n## lets visualize the relation using heat map\nsns.heatmap(data.corr(),annot=True)","0d0a2cfe":"## the CGPA stand first followed by GRE Score and TOEFL score\n## lets understand each data in detail\n## 1.CGPA\ndata['CGPA'].hist()","44aaef69":"## the distribution is symetric here for CGPA\n## 2.GRE Score\nx=sns.barplot(y='Chance of Admit ',x='GRE Score',data=data)\nplt.gcf().set_size_inches(15,10)","f0042f58":"## from above we can observe in most of cases with the raise in GRE score chance of admit is increasing\n## same way lets check for TOEFL\n","9073a51d":"sns.kdeplot(data['TOEFL Score'],data['Chance of Admit '],shade=True)","65ebceb5":"data.columns","85112a5c":"## you dont have any set of rules in choosing the plot type for data visualization\n## the plot that best describes the relation and infromation best suits for visualization\n## you can try with any no of plots you like\n## lets also see the regression plot to validate the features\nsns.regplot(x='GRE Score',y='Chance of Admit ',data=data)","f97c9324":"## the above plot clearly says the relationship,as the data points are almost close to the regression line in most of the cases\n","d5a63919":"## box plot is the best plot to find out the outliers\nsns.boxplot(y='CGPA',data=data,showmeans=True,meanline=True)\nplt.gcf().set_size_inches(5,5)","cff19c58":"## the above plot says only one outlier avaliable and inner quartile range gives 50% of data -most of the cgpa in inner quartile range varies in b\/w 8.2 to 9.1\n","a781a145":"## Now bivariate analysis \n## here we are going to comare two features","184e4f0e":"sns.boxplot(x='University Rating',y='GRE Score',data=data)","8747ee3b":"## it shows many box plots \n## now lets understand the above figure\n## here we can see outliers in second,fourth and fifth box plots\n## second box says - student with score in 330 - 340 range is expecting university of rating 2 - but why :) ?\n## in 4 and 5,students with range of 290 - 310 are expecting top rated universities ","41f9a246":"## can try the same for CGPA\nsns.boxplot(y='CGPA',x='University Rating',data=data)","4b7f7012":"## now its time to compare multiple feature values\n## note the word - lmplot, you many come across this many times","07a0ace0":"data.columns","ed300737":"sns.lmplot(x='Chance of Admit ',y='GRE Score',col='University Rating',data=data)","5b425a14":"## its (facetgrid - means a combination of (axes - means a single plot))","8d4aa28c":"## okay now ,we understood our data using the heat map ,correlation and other data visualization\n## let me draw a heat map for reference\nsns.heatmap(data.corr(),annot=True)","76b33d26":"## from above grap i can see all the features having good correlation- all features with more than 65%\n## so we can try with different combination of features \n## fisrtly im going to select GRE,TOEFL,CGPA as features which are top 3 and Chance of Admit is our Target\n\nfeatures=data[['GRE Score','TOEFL Score','CGPA']]\ntarget=data[['Chance of Admit ']]","00b106dd":"features.head()","e6065810":"target.head()","11bc132f":"## lets import ML libraries\n## sklearn library holds all the algorithms that is used to predict the values\n## though we are just reusing the in built libraries,it is very important to learn algorithms and the logics behind it\nfrom sklearn.model_selection import train_test_split","8b09925a":"X_train,X_val,y_train,y_val=train_test_split(features,target,random_state=1,test_size=0.2)","f7ba4cd0":"## what did i do-\n## I've imported a method called train_test_split,as name says its going to split the data into train and test part\n## lets see the data one by one","2530938f":"X_train.head()","940370d1":"X_val.head()","3740cabd":"y_train.head()","9f00d2cf":"y_val.head()","7ed04a9f":"## now with train data we are going to make our model learn our data \n## like i mentioned before ,many algorithms are available in sklearn library to build our model\n## lets use linear regression mmodel\nfrom sklearn.linear_model import LinearRegression","5360d8d5":"## here model is studing our train data\nstudent_model=LinearRegression()\nstudent_model.fit(X_train,y_train)","3fa80717":"## to check the score of our current train data\nstudent_model.score(X_train,y_train)","98c5912f":"## its 80% which is pretty good to build the model","762142a8":"## here we have predited the score by giving the X_val(sample output - that is chances of getting admission as input)\ny_pred=student_model.predict(X_val)","5317f414":"## now we are going to compare how close the above predicted value is with the y_val\n## for this we have set of inbuilt functions that can be imported\nfrom sklearn.metrics import r2_score","b0817a47":"accuracy_result_LR=round(r2_score(y_pred,y_val)*100,2)","119132d8":"## its 69 which is a good model,but we can try for better accuracy if possible\naccuracy_result_LR","7d0114a8":"from sklearn.tree import DecisionTreeRegressor","9c08876c":"DTR_model=DecisionTreeRegressor()","fee3b305":"DTR_model.fit(X_train,y_train)","1aee5798":"y_pred=DTR_model.predict(X_val)\naccuracy_result_DTR=round(r2_score(y_pred,y_val)*100,2)\naccuracy_result_DTR","9a583588":"## the decision tree model is giving very low prediction results, so lets try with new feature combination ","035ebe78":"## here ive included all features\n## for this data we havent encountered any feature with less correlation, but you ll going to deal with lot of data that gives very bad correlation\nfeatures2=data.drop(['Chance of Admit '],axis=1)\ntarget2=data['Chance of Admit ']","fc2256bd":"X_train2,X_val2,y_train2,y_val2=train_test_split(features2,target2,random_state=1,test_size=0.2)","216873b8":"lin_model=LinearRegression()","84ecf5bd":"lin_model.fit(X_train2,y_train2)","b43398e5":"y_pred=lin_model.predict(X_val2)\naccuracy_per=round(r2_score(y_pred,y_val2)*100,2)\naccuracy_per","cea93cb8":"lin_model.score(X_train2,y_train2)","52e3d139":"## this is a good considerable accuracy score\nprint(f'the accuracy percentage for our model is {accuracy_per}')","cffef572":"pd.to_pickle(lin_model,'chance_prediction.pickle')","06dac999":"## why do we pickle?\n## when we deal with large data frames ,its going to take long time to build and train a model\n## so on pickling a model ,we dont need to build and train a model ","0d22f2b0":"## this is the way to extrat the pickle file and reuse it\n## my_model=pd.read_pickle('chance_prediction.pickle')","68347d09":"##we have built our model sucessfully - when any user inputs are to be given , uncomment the below code and pass the input\n## GRE=int(input('enter GRE score ,360 is max score - '))\n## TOEFL=int(input('enter TOEFL score ,120 is max score -'))\n## required_university_rating= int(input('enter university rating in 1-5 range -'))\n## SOP=float(input('enter sop score in 1-5 range-'))\n## LOR=float(input('enter LOR score in 1-5 range-'))\n## CGPA=float(input('enter CGPA in 1-10 range-'))\n## Research=int(input('enter Research score 0-if no and 1-if yes-'))","6554956b":"## inputs=[GRE,TOEFL,required_university_rating,SOP,LOR,CGPA,Research]","56a1e76e":"## this will be the result predition\n## result=my_model.predict([inputs])\n## print(f'so,the probability of you getting desired University is {round(result[0]*100,2)}')","10593e1d":"### step 1-\n#### import libraries","9b8a6fdc":"### consume the created model to check user inputs","c56ac5a1":"here comes Data Preparation\n","a21ec730":"### its time to look into the outliers \n### outliers plays very important role in Data Analysis ","db177cb3":"## Building ML Model ","10b481fb":"## The probability estimator for student getting desired university","7f0ea0e1":"### pickle the model"}}