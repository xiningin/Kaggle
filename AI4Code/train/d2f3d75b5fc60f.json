{"cell_type":{"ab8dd4f8":"code","1ba24764":"code","eaf19bc0":"code","1498ca8f":"code","3f44c36f":"code","98a8c7b3":"code","e78c6d80":"code","6b84c3bb":"code","4f86c658":"code","5f05b523":"code","b36e198a":"code","9cf9c6e9":"code","723123b1":"code","9f998b4f":"code","527dd4b8":"code","11e0cc31":"code","9bba02fc":"code","dcfa499d":"code","8bdd0218":"code","8438deb2":"code","f0c34e5e":"code","c5cc73dd":"code","7b17ee40":"code","5476641c":"code","6b8afbbe":"code","426e5551":"code","3c3abf50":"code","23ee78a3":"code","7c595d6e":"code","c4a4b475":"code","9325e70f":"code","55663d2d":"code","37353b50":"code","0532a4fe":"code","246a7d87":"code","655b36ab":"code","168e11fe":"code","7dd599b1":"code","5eb61731":"code","2f280276":"code","a886ec3f":"code","179bec60":"code","fc94e39f":"code","8691e9f9":"code","c81d88b9":"code","923e45c0":"code","e3135976":"code","c5f6ee58":"code","5a0ccf99":"code","7d460883":"code","032f8e9f":"code","d599df33":"code","8f1f124b":"code","cd653a98":"code","c26144eb":"markdown","308456e4":"markdown","0032f496":"markdown","06aedbd7":"markdown","ef9fad51":"markdown","4b9db7c8":"markdown"},"source":{"ab8dd4f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1ba24764":"# An\u00e1lise explorat\u00f3ria dos dados\n# Estat\u00edstica do neg\u00f3cio\n# correla\u00e7\u00e3o\n# distribui\u00e7\u00e3o\n# interpreta\u00e7\u00e3o desses dados\n# tratamento converter, dividir, criar colunas, etc\n# grid search\n# hiperparametros","eaf19bc0":"df = pd.read_csv('..\/input\/hmeq-data\/hmeq.csv')\n\ndf.info() ","1498ca8f":"df.shape","3f44c36f":"df.head()","98a8c7b3":"df.sample(30)","e78c6d80":"df.describe().T","6b84c3bb":"print (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","4f86c658":"df['REASON'].astype('category').cat.categories","5f05b523":"df['JOB'].astype('category').cat.categories","b36e198a":"df['REASON'].astype('category').cat.codes\n\n# -1 \u00e9 nulo","9cf9c6e9":"df['JOB'].astype('category').cat.codes\n\n# -1 \u00e9 nulo","723123b1":"# manter apenas rows com pelo menos 8 campos n\u00e3o nulos \ndf.dropna(thresh=8, inplace=True)","9f998b4f":"# preencher no campo REASON DebtCon como default para nulo\ndf.REASON.fillna('DebtCon', inplace=True)\n\n# preencher no campo JOB Other como default para nulo\ndf.JOB.fillna('Other', inplace=True)\n","527dd4b8":"df.JOB.fillna('Other', inplace=True)\n","11e0cc31":"\nprint (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","9bba02fc":"# Convertendo as colunas categ\u00f3rias em colunas num\u00e9ricas\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        df[col] = df[col].astype('category').cat.codes","dcfa499d":"\nprint (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nTotal Missing: \\n\", df.isna().sum())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","8bdd0218":"df[\"DEROG\"].value_counts()\n","8438deb2":"df[\"DEROG\"].value_counts()\n","f0c34e5e":"df[\"DELINQ\"].value_counts()","c5cc73dd":"df[\"MORTDUE\"].value_counts()\n","7b17ee40":"df[\"VALUE\"].value_counts()","5476641c":"df[\"YOJ\"].value_counts()","6b8afbbe":"df[\"CLAGE\"].value_counts()","426e5551":"df[\"NINQ\"].value_counts()","3c3abf50":"df[\"CLNO\"].value_counts()","23ee78a3":"df[\"DEROG\"].fillna(value=0,inplace=True)\ndf[\"DELINQ\"].fillna(value=0,inplace=True)","7c595d6e":"print (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nTotal Missing: \\n\", df.isna().sum())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","c4a4b475":"# criar uma base com dados limpos (removendo todos os missing)\ndf2 = df.dropna()\n\n# criar uma base com dados utilizando ffill e bfill\ndf3 = df.fillna(method='ffill')\ndf3 = df3.fillna(method='bfill')\n","9325e70f":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(18,18))\n\n\ncorrMatrix = df[df['DEBTINC'].notnull()].corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","55663d2d":"import seaborn as sns\nfrom scipy.stats import norm\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 30, 15\n\ndic = {\"LOAN\":df[\"LOAN\"],\"BAD\":df[\"BAD\"],\"MORTDUE\":df[\"MORTDUE\"],\"VALUE\":df[\"VALUE\"],\"YOJ\":df[\"YOJ\"]}\nrcParams['figure.figsize'] = 5, 5\n\ndf_pair = pd.DataFrame(dic)\nsns.pairplot(df_pair,vars=['LOAN', 'MORTDUE',\"VALUE\",\"YOJ\"],hue=\"BAD\")","37353b50":"sns.countplot(x=df2['BAD'], data=df2).set_title(\"Distribui\u00e7\u00e3o para df2\")","0532a4fe":"ax = sns.countplot(x=df3['BAD'], data=df3).set_title(\"Distribui\u00e7\u00e3o para df3\")","246a7d87":"df2['BAD'].value_counts()","655b36ab":"df3['BAD'].value_counts()","168e11fe":"fcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=df[col], data=df)\n    plt.show()","7dd599b1":"\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nfeats  = [c for c in df.columns if c not in ['BAD']]\n\n\n# df2\ndf2_x_treino, df2_x_valid, df2_y_treino, df2_y_valid = train_test_split(df2[feats],\n                                                                              df2['BAD'], \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\n# df3\ndf3_x_treino, df3_x_valid, df3_y_treino, df3_y_valid = train_test_split(df3[feats],\n                                                                              df3['BAD'], \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\nsmote = SMOTE()\n\n# resample df2\ndf2_rx, df2_ry = smote.fit_sample(df2[feats], df2['BAD'])\n\ndf2_rx_treino, df2_rx_valid, df2_ry_treino, df2_ry_valid = train_test_split(df2_rx,\n                                                                              df2_ry, \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\n# resample df3\ndf3_rx, df3_ry = smote.fit_sample(df3[feats], df3['BAD'])\n\ndf3_rx_treino, df3_rx_valid, df3_ry_treino, df3_ry_valid = train_test_split(df3_rx,\n                                                                              df3_ry, \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n","5eb61731":"df2_rx.shape, df2_ry.shape, df2_rx_treino.shape, df2_rx_valid.shape, df2_ry_treino.shape, df2_ry_valid.shape","2f280276":"df3_rx.shape, df3_ry.shape, df3_rx_treino.shape, df3_rx_valid.shape, df3_ry_treino.shape, df3_ry_valid.shape","a886ec3f":"df2_x.shape, df2_y.shape, df2_x_treino.shape, df2_x_valid.shape, df2_y_treino.shape, df2_y_valid.shape","179bec60":"df3_x.shape, df3_y.shape, df3_x_treino.shape, df3_x_valid.shape, df3_y_treino.shape, df3_y_valid.shape","fc94e39f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom xgboost import XGBClassifier\n\n\nmodels = []\nmodels.append(('RandomForest', RandomForestClassifier(random_state=123)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=123)))\nmodels.append(('XGB', XGBClassifier(random_state=123)))\n\n\nX_train = df2_x_treino\nX_valid =df2_x_valid\ny_train = df2_y_treino\ny_valid = df2_y_valid\n\nscore = []\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base dropada - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    score.append(f_dict)\n    \n\n","8691e9f9":"\nX_train = df3_x_treino\nX_valid =df3_x_valid\ny_train = df3_y_treino\ny_valid = df3_y_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base fill  - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    \n    score.append(f_dict)\n    ","c81d88b9":"\nX_train = df2_rx_treino\nX_valid =df2_rx_valid\ny_train = df2_ry_treino\ny_valid = df2_ry_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base dropada resample - ' + name,\n        'accuracy': accuracy_valid,\n    }\n    score.append(f_dict)\n    \n\n","923e45c0":"\nX_train = df3_rx_treino\nX_valid =df3_rx_valid\ny_train = df3_ry_treino\ny_valid = df3_ry_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base fill resample - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    \n    score.append(f_dict)\n    \nscore = pd.DataFrame(score, columns = ['model', 'accuracy'])","e3135976":"print(score)","c5f6ee58":"# From https:\/\/www.kaggle.com\/ajay1735\/my-credit-scoring-model\n# fun\u00e7\u00e3o alterada e utilizada de maneira simplificada para exemplo abaixo\nimport itertools\ndef plot_confusion_matrix(cm,title='Matrix de confus\u00e3o',classes=[0,1], cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Valor real')\n    plt.xlabel('Valor predito')","5a0ccf99":"X_train = df2_rx_treino\nX_valid =df2_rx_valid\ny_train = df2_ry_treino\ny_valid = df2_ry_valid\n\nparam_grid = {}\nmy_model = GridSearchCV(RandomForestClassifier(random_state=123),param_grid)\nmy_model.fit(X_train, y_train)\npredictions_t = my_model.predict(X_train) \npredictions_v = my_model.predict(X_valid)\naccuracy_train = accuracy_score(y_train, predictions_t) \naccuracy_valid = accuracy_score(y_valid, predictions_v)","7d460883":"# informa\u00e7\u00f5es do melhor modelo, random forest com resample na base com nulos dropados\nprint(my_model.best_estimator_)\npd.Series(my_model.best_estimator_.feature_importances_, index=feats).sort_values().plot.barh()","032f8e9f":"cm = confusion_matrix(y_valid, predictions_v)\nnp.set_printoptions(precision=2)\nprint('Matrix de confus\u00e3o')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm, 'base dropada resample - RandomForest')","d599df33":"X_train = df2_x_treino\nX_valid =df2_x_valid\ny_train = df2_y_treino\ny_valid = df2_y_valid\n\nparam_grid = {}\nmy_model = GridSearchCV(RandomForestClassifier(random_state=123),param_grid)\nmy_model.fit(X_train, y_train)\npredictions_t = my_model.predict(X_train) \npredictions_v = my_model.predict(X_valid)\naccuracy_train = accuracy_score(y_train, predictions_t) \naccuracy_valid = accuracy_score(y_valid, predictions_v)","8f1f124b":"cm = confusion_matrix(y_valid, predictions_v)\nnp.set_printoptions(precision=2)\nprint('Matrix de confus\u00e3o')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm, 'base dropada - RandomForest')","cd653a98":"# informa\u00e7\u00f5es do melhor modelo, random forest com resample na base com nulos dropados\nprint(my_model.best_estimator_)\npd.Series(my_model.best_estimator_.feature_importances_, index=feats).sort_values().plot.barh()","c26144eb":"# Informa\u00e7\u00f5es dos melhores modelos\n\nPode-se ver a matrix de confus\u00e3o para base resample e base sem resample (melhor caso) com valores muito bons, tendo em vista sua acuracia, era de se esperar isso\n\nA base sem resample \u00e9 poss\u00edvel perceber a importancia do DEBTINC bem grande se comparada a importancia no outro modelo.","308456e4":"# Analise dos resultados\n\nTodos nossos modelos foram submetidos ao GridSearchCV para encontrar os melhores parametros (hiper parameters)\n\nO melhor modelo para a base com dados dropados (df2) os dados foi a mesma random forest e XGB com resample SMOTE tendo uma acuracia de 98.1%\n\nJ\u00e1 para a base preenchida com ffill e bfill aconteceu o mesmo comportamento aciuma com o algoritmo random forest e XGB utilizando resample SMOTE tendo acuracia de 95.6%\n\nO melhor modelo para sem resample foi a com base com dados nulos dropados utilizando XGB com acuracia de 96%\n","0032f496":"\nExiste uma correla\u00e7\u00e3o entre montdue e value que \u00e9 percebida nos 2 gr\u00e1ficos abaixo\n\npode-se perceber que a base df3 tem uma quantidade bem maior de valores BAD = 1, espera-se que isso n\u00e3o cause problemas nos modelos, caso aconte\u00e7a vai ser percebido nos resultados finais","06aedbd7":"\n\n# Trabalho final Data Mining e Machine Learning II\n\nNosso objetivo \u00e9 criar uma modelo para que consiga se prever quando uma pessoa vai ou n\u00e3o pagar seu emprestimo\n\nTemos uma base de dados com 5960 rows e 13 colunas. A coluna com o resultado \u00e9 a 'BAD' que com valor 0 significa que pagou e com valor 1 significa que n\u00e3o pagou.\n\nTemos uma propor\u00e7\u00e3o inicial de quase 20% para BAD. Para resolver isso vamos tentar utilizar resample nos dados.\n\nTemos 5271 missing values, vamos utilizar diversos m\u00e9todos abaixo, preencher com valores default, como Other e DebtCon para os campos JOB e REASON que s\u00e3o os valores com maior quantidade e depois transformar essas 2 colunas em n\u00famericas.\nDEROG e DELINQ tem como maior quantidade dos valores 0.0, foi feito default para os 2 campos como 0.\n\nDepois destes tratamento temos ainda a maioria de missing values no campo DEBTINC que acredito que seja ter um valor bem importante para a previs\u00e3o.\n\n","ef9fad51":"# Tratamento final dos campos\n\nforam criadas 2 bases df2 e df3.\n\nna df2 foram dropadas qualquer row com dados nulos\nna df3 foram preenchidos utilizando ffill e bfill que \u00e9 forward propagation e backward propagation, pois acredito que a m\u00e9dia n\u00e3o seria um valor muito bom pela diferen\u00e7a dos valores","4b9db7c8":"# Trabalho Data Mining e Machine Learning II\n\nCampos do arquivo CSV\n\n**BAD** \n1 = client defaulted on loan 0 = loan repaid\n\n**LOAN** \nAmount of the loan request\n\n**MORTDUE** \nAmount due on existing mortgage\n\n**VALUE** \nValue of current property\n\n**REASON** \nDebtCon = debt consolidation HomeImp = home improvement\n\n**JOB** \nSix occupational categories\n\n**YOJ** \nYears at present job\n\n**DEROG** \nNumber of major derogatory reports\n\n**DELINQ** \nNumber of delinquent credit lines\n\n**CLAGE** \nAge of oldest trade line in months\n\n**NINQ** \nNumber of recent credit lines\n\n**CLNO** \nNumber of credit lines\n\n**DEBTINC** \nDebt-to-income ratio\n\n\n"}}