{"cell_type":{"d50148a4":"code","a116e9ad":"code","7d06cd2b":"code","9af13668":"code","5e3bd327":"code","472648b5":"code","e19627bd":"code","f7e5fb59":"code","51dcd01a":"code","2bdb9d49":"code","168d7d57":"code","8374729a":"code","cf120f9a":"code","34c9f39d":"code","f9b5337e":"code","677272c7":"code","36f61218":"code","e861e9a1":"code","0297dc80":"code","ed6306fd":"code","9e422d1e":"code","3f86a5ed":"code","1cbeb2ee":"code","e950b884":"code","3d747f66":"code","1b3ea44c":"code","b592bee3":"code","8a581c53":"code","370d4992":"code","4011dffe":"code","585dd61e":"code","4a5e9d79":"code","a7a83e12":"code","63617f7d":"code","e0682a0a":"code","cfe51c13":"code","132ca8df":"code","323698f1":"code","621a4943":"code","8edcf4b1":"code","7834781a":"code","a841848b":"markdown","8f89057c":"markdown","ff470678":"markdown","f3e3eda9":"markdown","b806ab4d":"markdown","b237eba8":"markdown","8aef539e":"markdown","2e853c53":"markdown","86895828":"markdown","d5a4936f":"markdown","8450a5d0":"markdown","b71e4871":"markdown","cf8eb08c":"markdown","0b5fcb12":"markdown","15552555":"markdown","fcf2fb1a":"markdown","48559828":"markdown","39106e4e":"markdown","32c37f5d":"markdown","978d5367":"markdown","2858bbe6":"markdown","425319c1":"markdown","944c28cd":"markdown","77583bc0":"markdown","b912f690":"markdown","b4410948":"markdown","afcd237a":"markdown","ad43e3ec":"markdown","e55de1ce":"markdown"},"source":{"d50148a4":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom patsy import dmatrices, dmatrix","a116e9ad":"Items = pd.read_csv(\"..\/input\/04a-sale-project-data-cleansing-and-feature-eng\/Items_ff_final6.csv\")\nItems.shape","7d06cd2b":"Items.columns","9af13668":"Items.drop(columns=['creation_time'], inplace=True)\nItems.drop(columns=['description'], inplace=True)\nItems['location'] = Items['location'].astype('category')\nItems['condition'] = Items['condition'].astype('category')\nItems['star_rating'] = Items['star_rating'].astype('category')\nItems['revised_star_rating'] = Items['revised_star_rating'].astype('category')\nItems['sales_cyber_monday'] = Items['sales_cyber_monday'].astype('category')\nItems['sales_black_friday'] = Items['sales_black_friday'].astype('category')\nItems['sales_colombus_day'] = Items['sales_colombus_day'].astype('category')\nItems['sales_labor_day'] = Items['sales_labor_day'].astype('category')\nItems['sales_presidents_day'] = Items['sales_presidents_day'].astype('category')\nItems['sales_4th_july'] = Items['sales_4th_july'].astype('category')\nItems['sales_newyear'] = Items['sales_newyear'].astype('category')\nItems['sales_xms'] = Items['sales_xms'].astype('category')\nItems['success'] = Items['success'].astype('category')\nItems['brand_size'] = Items['brand_size'].astype('category')\nItems['popular_brand'] = Items['popular_brand'].astype('category')\nItems['category_size_rank_q'] = Items['category_size_rank_q'].astype('category')\nItems['review_count_log'] = Items['review_count_log'].astype('category')\nItems['price_diff_competitor_log'] = Items['price_diff_competitor_log'].astype('category')\nItems['profit_pct_sqrt'] = Items['profit_pct_sqrt'].astype('category')\nItems['reviews_cnt_category'] = Items['reviews_cnt_category'].astype('category')\nItems.columns","5e3bd327":"#Items_numeric = Items.select_dtypes(include=np.number)\nnumlst = Items.select_dtypes(include=np.number).columns.tolist()\nnumlst=numlst[0:9]\nnumlst","472648b5":"from sklearn.preprocessing import PolynomialFeatures\n# retrieve just the numeric input values\nnumeric_data = Items[numlst]\n# perform a polynomial features transform of the dataset\ntrans = PolynomialFeatures(degree=3, include_bias=False)\nnumeric_data = trans.fit_transform(numeric_data)\n# convert the array back to a dataframe\nItems_numeric = pd.DataFrame(numeric_data, columns=[trans.get_feature_names()]) \n# summarize\nprint(Items_numeric.shape)\nprint(Items.shape)\nItems1 = pd.concat([Items, Items_numeric.iloc[: ,9:-1]], axis=1)\nprint(Items1.shape)","e19627bd":"cols = list(Items1.columns[0:33])\ncols = cols + (trans.get_feature_names(Items1.columns)[9:-1])\nItems1.columns = cols\nItems1","f7e5fb59":"text_df =  pd.read_csv(\"..\/input\/text-analysis-final\/text_analysis_2\")\nprint(Items1.shape)\ntext_df.shape","51dcd01a":"text_df.drop(['Unnamed: 0', 'text_df'], axis = 1, inplace = True)\nItems1 = pd.concat([Items1, text_df], axis=1)","2bdb9d49":"Items1.shape","168d7d57":"import pickle\npickle.dump(Items1, open( \".\/Items1\", \"wb\" ))","8374729a":"  \n\"\"\"\nanalysis                                                        ####\nAuthor: Tomas Karpati M.D.                                      ####\nCreation date: 2019-01-02                                       ####\nLast Modified: 2020-02-28                                       ####\n\"\"\"\n\n__author__ = \"Tomas Karpati <karpati@it4biotech.com>\"\n__version__ = \"0.1.3\"\n\n\"\"\"\nTable1\nUsage:\ndata: a pandas dataframe\nx: character list with the name of the variables\ny: the name of the strata variable (optional)\nrn: character list with the text we want to be used to desxribe the variable names\nmiss: include missing statistics: [0=none, 1=only for categorical variables, 2=for all variables]\ncatmiss: On categorical variables, adds a new category (Missing) to the available\ncategories. Default is FALSE. For activation change this to TRUE. If TRUE, the \"miss\"\nparameter will not be used for category variables.\nformatted: As default, the table output is formatted as text with values that\ninclude parenthesis and percentages, e.g. 153 (26.5\\%). If you are interested that the\ntable return each numeric value as aseparate cell, set this variable to FALSE.\ncategorize: If there are categorical variables that are defined as numeric we can\nforce the function to take them as categorical (factor) by changing this to TRUE. Default\nis FALSE.\nfactorVars: If categorize is set to TRUE, a list of variables to be considered as\ncategoricals may be given. In this case, maxcat will not be used and only the variables\nin the list will be converted to factors.\nmaxcat: If we force categorize to be TRUE, maxcat will be used to define the\nmaximum number of unique values permited for a variable to be considered as categorical.\nDefault is 10.\ndecimals: Determinate the number of decimal places of numerical data. Default is 1\nmessages: This switch will show the iterations of the function through the variables. In case you want to suppress those message set it to FALSE.\nexcel: export the table to excel [0=no, 1=yes]\nexcel_file: the name of the excel file we want to save the table (optional)\n\"\"\"\n\nimport time\nimport sys\nimport os\nimport numpy as np\nfrom scipy import stats\n#from statsmodels.stats import multitest\nfrom statsmodels.formula.api import ols\n#import statsmodels.stats.api as sms\nfrom statsmodels.stats.anova import anova_lm\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\n#import matplotlib\n### preventing matplotlib to open a graph window when saving...\n#matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n### preventing matplotlib to open a graph window when saving...\nplt.ioff()\nimport seaborn as sns\n## dissable the \"SettingWithCopyWarning\" warning\npd.options.mode.chained_assignment = None  # default='warn'\n#import warnings\n\nif(pd.__version__ < '0.21.0'):\n    pd.set_option('use_inf_as_null',True)\nelse:\n    pd.set_option('use_inf_as_na',True)\n\n    \ndef Table1(data=None, x='',y='', rn='', miss=True, catmiss=True, formatted=True, categorize=True, factorVars='', maxcat=6, decimals=1, messages=True, excel=False, excel_file=''):\n        data.head()\n        print(\"Begining analysis...\")\n        table1 = _getTable1(data,x,  y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file)\n    #    #self.table1 = self._getTable1(x, data, y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file)\n        return(table1)\n\ndef _g1(var):\n        res = {'mean':np.nanmean(var), 'sd':np.nanstd(var)}\n        return(res)\n\ndef _g2(var):\n        res = {'median':np.nanmedian(var), 'irq_25':np.nanpercentile(var,25), 'irq_75':np.nanpercentile(var,75)}\n        return(res)\n\ndef _getUniqueCount(data):\n        import pandas as pd\n        bb = data.columns.tolist()\n        cc = {}\n        for v in bb:\n            cc[v] = len(data.groupby(v).count())\n        return(pd.Series(cc))\n\n\ndef _to_categorical(x):\n        x = x.astype('category')\n        return(x)\n\ndef _setFactors(data, factorVars, unq, catmiss, maxcat):\n        aa =data.dtypes\n        if(len(factorVars) > 0):\n            for v in factorVars:\n                #print(\"Variable %s is a %s\" % (v,aa[v].name))\n                if(aa[v].name!='category'):\n                    data.loc[:,v] = _to_categorical(data[v])\n                if(catmiss==True):\n                    if(data[v].isnull().sum()>0):\n                        #print(\"Adding missing category to %s\" % v)\n                        data[v] = data[v].cat.add_categories(['Missing'])\n                        data.loc[data[v].isnull(),v] = \"Missing\"\n\n        elif(len(factorVars)==0):\n            #factorVars = self._getUniqueCount(data)\n            factorVars = unq\n            factorVars = factorVars[factorVars <= maxcat]\n            for v in factorVars.index:\n                if(aa[v].name!=\"category\"):\n                    data.loc[:,v] = _to_categorical(data[v])\n                if(catmiss==True):\n                    if(data[v].isnull().sum()>0):\n                        data[v].cat.add_categories(['Missing'])\n                        data.loc[data[v].isnull(),v] = \"Missing\"\n        return(data)\n\n\ndef _getSimpleTable(data, x, rn, miss, catmiss, formatted, categorize, factorVars, unq, maxcat, decimals, messages):\n        msg=[]\n        if (len(rn)==0):\n            rn = x\n        ln = len(x)\n        ### init the progress bar\n        sys.stdout.write(\"[%s]\" % (\"\" * ln))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (ln+1)) # return to start of line, after '['\n        toolbar_width = 80\n        ### define the column names\n        tableaaaa = [[0,\"Individuals\",\"n\",1, len(data)]]\n        tablebbbb = [[0,\"Individuals\",\"n\",1, len(data),'','']]\n        q = 0\n        n = 0\n        ii = 0\n        tm = 0\n        for v in x:\n          time.sleep(0.1) # do real work here\n          # update the progress bar\n          sys.stdout.write(\"*\")\n          sys.stdout.flush()\n          ### check if the variable name exists in the dataset\n          if({v}.issubset(data.columns)):\n            ### check  if there are no values on the variable\n            #if(data[v].nunique()==0):\n            if(unq[v]==0):\n                msg.append(\"The variable %s has no data... avoided\" % v)\n            ### define if the actual variable has to be treated as numeric or factor\n            aa = data.dtypes\n            #if(categorize==True and data[v].nunique() <= maxcat):\n            if(categorize==True and (unq[v] <= maxcat)):\n                data.loc[:,v] = _to_categorical(data[v])\n            ### If date\/time, don't show\n            if(aa[v].name == 'datetime64[ns]'):\n                msg.append(\"The variable %s is a date. Dates are not allowed in Table1... avoided\" % v)\n            ### if it is defined as object (not assigned numerical or categorical), ignore\n            elif(aa[v].name == 'object'):\n                msg.append(\"The variable %s is not well defined. This data type is not allowed in Table1... avoided\" % v)\n            ### if it is numeric, show\n            elif(aa[v].name == 'float64' or aa[v].name == 'int64'):\n                ## report mean and standard deviation\n                t_n = _g1(data[v])\n                tp = \"%s (%s)\" % ('{:8,.2f}'.format(round(t_n['mean'],decimals)), '{:8,.2f}'.format(round(t_n['sd'],decimals)))\n                tbl1 = [0, rn[q],\"Mean (SD)\",1, tp]\n                tbl2 = [0, rn[q],\"Mean (SD)\",1, round(t_n['mean'],5),round(t_n['sd'],5),'']\n                tableaaaa.append(tbl1)\n                tablebbbb.append(tbl2)\n                ## report median and Interquartile ranges (25%,75%)\n                t_n = _g2(data[[v]])\n                tp = \"%s (%s-%s)\" % ('{:8,.2f}'.format(round(t_n['median'],decimals)), '{:8,.2f}'.format(round(t_n['irq_25'],decimals)), '{:8,.2f}'.format(round(t_n['irq_75'],decimals)))\n                tbl1 = [0, rn[q],\"Median (IQR)\",2, tp]\n                tbl2 = [0, rn[q],\"Median (IQR)\",2, round(t_n['median'],5),round(t_n['irq_25'],5),round(t_n['irq_75'],5)]\n                tableaaaa.append(tbl1)\n                tablebbbb.append(tbl2)\n                ## report number and percent of missing\n                if (miss >= 1):\n                    if (data[v].isnull().sum()>0):\n                        t_n  = len(data)\n                        t_m = data[v].isnull().sum()\n                        tp = \"%s (%s%%)\" % ('{:8,.2f}'.format(t_m),'{:8,.2f}'.format(round((t_m\/t_n)*100,decimals)))\n                        tbl1 = [0,rn[q],\"Missing (%)\",3, tp]\n                        tbl2 = [0,rn[q],\"Missing (%)\",3, t_m, (t_m\/t_n)*100, ]\n                    else:\n                        tbl1 = [1,rn[q],\"Missing (%)\",3, \" -- \"]\n                        tbl2 = [1,rn[q],\"Missing (%)\",3,'' ,'' ,'' ]\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n            elif(aa[v].name == \"category\"):\n                #if(data[v].nunique()>8):\n                if(len(data.groupby(v).count())>8):\n                    tmpcat = pd.Series.value_counts(data[v],dropna=(not catmiss))\n                    n = len(tmpcat)\n                    if(n > 8):\n                        v1 = tmpcat[0:6].values\n                        v2 = np.append(v1,tmpcat[7:n].sum())\n                        a1 = tmpcat.index[0:6].values.tolist()\n                        a2 = a1.extend(['Other'])\n                        t_n = pd.Series(v2,a1)\n                else:\n                    t_n = pd.Series.value_counts(data[v],dropna=(not catmiss))\n                ttotal = len(data)\n                #nm = data[v].unique()\n                nm = t_n.index.values\n                for f in range(0,len(nm)):\n                    del1 = 0\n                    if(len(nm)==2 and (nm[f]==\"No\" or nm[f]==\"no\" or nm[f]==0 or nm[f]==\"0\" or nm[f]==\"None\" or nm[f]==\"none\")):\n                        del1 = 1\n                    tp = t_n.iloc[f] \/ ttotal * 100\n                    pct = \"%s (%s%%)\" % ('{:8,.2f}'.format(round(t_n.iloc[f],decimals)), '{:8,.2f}'.format(round(tp,decimals)))\n                    tbl1 = [del1,rn[q],nm[f],f, pct]             ########### delete rows 0\/1 !!!!!!!!!\n                    tbl2 = [del1,rn[q],nm[f],f, t_n.iloc[f], tp, ]    ########### delete rows 0\/1 !!!!!!!!!\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n                if (miss >= 2 and catmiss==False ):\n                    if (data[v].isnull().sum()>0):\n                      t_n = len(data)\n                      t_m = data[v].isnull().sum()\n                      tp = \"%s (%s%%)\" % ('{:8,.2f}'.format(t_m), '{:8,.2f}'.format(round((t_m\/t_n)*100,decimals)))\n                      tbl1 = [0, rn[q], \"Missing (%)\", f, tp]\n                      tbl2 = [0, rn[q], \"Missing (%)\", f, t_m, (t_m\/t_n)*100, ]\n                    else:\n                      tbl1 = [1,rn[q],\"Missing (%)\",f, \" -- \"]\n                      tbl2 = [1,rn[q],\"Missing (%)\",f,'' ,'' , '']\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n            else:\n                msg.append(\"The variable %s doesn't exists in the dataset... avoiding\" % v)\n\n            q = q + 1\n            ii = ii + 1\n            tm = tm + 1\n        if(formatted==True):\n          ### terminate the progress bar\n          sys.stdout.write(\"\\n\")\n          if(messages==True):\n              print(msg)\n          return(tableaaaa)\n        else:\n          ### terminate the progress bar\n          sys.stdout.write(\"\\n\")\n          if(messages==True):\n              print(msg)\n          return(tablebbbb)\n\n\ndef _pvals(x, y, rn, data, unq, messages):\n        msg = []\n        ptab = [] #[[\"Variables\",\"pval\", \"n\"]]\n        if (y!=''):\n          if ({y}.issubset(data.columns)):\n            if (len(rn)==0 or len(rn)<2):\n                rn = x\n            q = 0\n            for v in x:\n              #print(v)\n              if ({v}.issubset(data.columns) and v != y):\n                #factorY = data[y].nunique()\n                factorY = unq[y]\n                aa = data.dtypes\n                if((aa[y].name == 'float64' or aa[y].name == 'int64' or aa[y].name == 'object') and factorY <= maxcat):\n                    data.loc[:,y] = to_categorical(data[y])\n                elif (aa[y].name == 'float64' or aa[y].name == 'int64'):\n                  msg.append(\"The variable %s is not a factor. Please convert to factor or change the 'categorize' flag to TRUE.\" % y)\n                  pval = []\n\n                #if ((aa[v].name == 'float64' or aa[v].name == 'int64') and (data[y].nunique() > 1)):\n                if ((aa[v].name == 'float64' or aa[v].name == 'int64' or aa[v].name == 'float32' or aa[v].name == 'int32') and (unq[y] > 1)):\n                  ### first check for homoscedasticity\n                    bb = pd.Series({x : y.tolist() for x,y in data[v].groupby(data[y])})\n                    if (stats.bartlett(*bb)[1] >= 0.05):\n                        formula = \"%s ~ %s\" % (v,y)\n                        model = ols(formula,data=data,missing='drop').fit()\n                        aov_table = anova_lm(model, typ=2)\n                        pval =round(aov_table['PR(>F)'][0],3)\n                    else:\n                        prevar = data.loc[-data[v].isnull()]\n                        bb = pd.Series({x : y.tolist() for x,y in prevar[v].groupby(prevar[y])})\n                        pval = stats.f_oneway(*bb)[1]\n                #elif (data[y].nunique()==1):\n                elif (unq[y]==1):\n                  pval = np.nan\n                elif (aa[v].name==\"datetime64[ns]\"):\n                    pval = np.nan\n                else:\n                    if(pd.crosstab(data[v],data[y]).min().min()>5):\n                        pval = stats.chi2_contingency(pd.crosstab(data[v],data[y]))[1]\n                    else:\n                        ct = pd.crosstab(data[v],data[y])\n                        if(ct.min().min()==0):\n                        # in cases where there are cells with zero, we use Fisher's exact test\n                            if(ct.shape == (2,2)):\n                                pval = stats.fisher_exact(ct)[1]\n                            else:\n                                pval = stats.chi2_contingency(pd.crosstab(data[v],data[y]))[1]\n                                msg.append(\"Unable to calcualte the Fisher exact test for variables %s and %s... The p-value may be incorrect\" % (v,y))\n                        else:\n                            pval = stats.mstats.kruskalwallis(pd.crosstab(data[v],data[y]))[1]\n                ptab.append([rn[q],round(pval,3),1])\n              q = q + 1\n        if(messages==True):\n            print(msg)\n        return(ptab)\n\n###############################################################################################\n####################### Begin analysis\n###############################################################################################\n\ndef _getTable1(data, x, y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file):\n        import time\n        init = time.time()\n\n        print(\"Factorizing... please wait\")\n        if (x=='' or len(x)==0):\n            x = data.columns.tolist()\n        if(y!='' and {y}.issubset(x)):\n            #print(x)\n            x.remove(y)\n            #if ({'Unnamed: 0'}.issubset(x)):\n            #    x.drop('Unnamed: 0')\n        unq = _getUniqueCount(data)\n        if ({'Unnamed: 0'}.issubset(unq)):\n            unq.drop('Unnamed: 0')\n        if (len(factorVars)==0):\n            factorVars = unq[unq <= maxcat].index\n        #print(data.dtypes)\n        data = _setFactors(data=data, factorVars=factorVars, unq=unq, catmiss=catmiss, maxcat=maxcat)\n        #print(data.dtypes)\n        ##### if y is null then make a simple table\n        #print(\"_getSimpleTable pass 1...\")\n        tabaaa1 = _getSimpleTable(x=x, rn=rn, data=data, miss=miss, catmiss=catmiss, unq=unq, formatted=formatted, categorize=categorize, factorVars=factorVars, maxcat=maxcat, decimals=decimals, messages=messages)\n        if(formatted==True):\n            tabaaa1 = pd.DataFrame(tabaaa1,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"Population\"])\n        else:\n            tabaaa1 = pd.DataFrame(tabaaa1,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"val1\",\"val2\",\"val3\"])\n        #print(tabaaa1)\n        ##### if y has two levels, then make a compound comparison\n        if (y!=''): #1\n            if ({y}.issubset(data.columns)):  #2\n                if (data[y].dtype == \"category\"): #3\n                    if (unq[y] > 8): #4\n                        if (messages==True): #5\n                            print(\"The dependent variable has more than 8 levels, table too large!\")\n                    elif(min(pd.Series.value_counts(data[y]))==0): #4\n                        print(\"The dependent variable has one or more levels with no items assigned!\")\n                    else: # 4\n                        data.loc[:,y] = _to_categorical(data[y])\n                #if (data[y].nunique() >= 2): #3\n                if (unq[y] > 6): #3\n                    print(\"You have selected a Y that has more than six different values...\")\n                elif (unq[y] >= 2 and unq[y]<=6): #3\n                    for lv in data[y].unique(): #4\n                        #print(\"Category %s\" % lv)\n                        dtsub = data.loc[data[y]==lv]\n                        #print(\"_getSimpleTable Y pass ...\")\n                        tab = _getSimpleTable(x=dtsub.columns,data=dtsub, rn=rn, miss=miss, unq=unq, catmiss=catmiss, formatted=formatted, categorize=categorize, factorVars=factorVars, maxcat=maxcat, decimals=decimals, messages=False)\n                        if(formatted==True): # 5\n                            tab1 = pd.DataFrame(tab,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",'Category_%s' % lv])\n                        else: #5\n                            tab1 = pd.DataFrame(tab,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"Cat_%s_val1\" % lv,\"Cat_%s_val2\" % lv,\"Cat_%s_val3\" % lv])\n                        tab1 = tab1.drop(['n'], axis=1) #5\n                        #print(tab)\n                        tabaaa1 = pd.merge(tabaaa1, tab1, on=['Del','Variables','Categories'],how='left')\n                    #print(tabaaa1)\n\n                    #print(\"_pvals pass ...\")\n                    ptab = _pvals(x=x,y=y, rn=rn, data=data, unq=unq, messages=messages)\n                    ptab = pd.DataFrame(ptab,columns=[\"Variables\",\"p_value\", \"n\"])\n                    #print(ptab)\n                    tabaaa1 = pd.merge(tabaaa1, ptab, on=['Variables','n'],how='left')\n                    tabaaa1 = tabaaa1.loc[tabaaa1['Population']!=\" -- \"]\n                    tabaaa1 = tabaaa1.drop('n',1)\n                    tabaaa1 = tabaaa1.drop('Del',1)\n        #### Save as excel file\n        if(excel==True):\n            if(excel_file == ''):\n                print(\"Please fill in the <excel_file> parameter with the file name including the path.\")\n            else:\n                writer = pd.ExcelWriter(excel_file)\n                tabaaa1.to_excel(writer,'Table1',index=False)\n                writer.save()\n                print(\"Excel file written to %s\" % excel_file)\n        print(\"------ Finished in % seconds -----\" % (time.time() - init))\n        return(tabaaa1) #1\n\n    \n############################################################################\n#####   TEST & TRAIN DATASET GENERATION                                 ####\n#####   Author: Tomas Karpati M.D.                                      ####\n#####   Creation date: 2016-08-17                                       ####\n############################################################################\n\n\"\"\"\ntrain_test\nGenerates a training and test dataset and checks if it is well balanced\ndata: original dataset\nprop: the proportion of the training dataset. The value is a fractional number between 0 and 1. The value default value is set to 0.7, indicating that the training dataset will contain 60% of the cases and the test dataset will contain the 40% of the cases.\nseed: the desired seed. Using a constant seed value allows to obtain the same individuals on each group when running many times (important feature needed for replicability)\ntableone: a logical value indicating if the Table1 function has to be generated for comparing the train and test division. Default is FALSE \n\"\"\"\n\n\ndef train_test(data=None,prop=0.7,seed=1,tableone=False):\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    \n    train, test = train_test_split(data, test_size=1-prop, random_state=seed)\n    dtr = train\n    dte = test\n    dtr['split'] = 'train'\n    dte['split'] = 'test'\n    dd = dtr.append(dte)\n    \n    tab1 = Table1(dd,y='split')\n    vn = tab1.loc[tab1['p_value']<0.05]\n    vn = vn['Variables'].tolist()\n    if(len(vn) == 0):\n        print(\" \")\n        print(\"You got a perfectly balanced training and test datasets\")\n        print(\" \")\n    else:\n        print(\"WARNING: The following variables are not balanced between the training and test datasets:\")\n        print(vn)\n        print(\"You can try to change the seed value until you get a balanced partition.\")\n        print(\"Alternatively, you can ommit this warning and exclude those variables from your model\")\n        print(\" \")\n    \n    if(tableone == True):\n        print(tab1)\n    return([train,test])","cf120f9a":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","34c9f39d":"def classificationMetrics(y, yhat):\n    prf1 = metrics.precision_recall_fscore_support(y,yhat)\n    res = {'Accuracy': metrics.accuracy_score(y,yhat),\n           'Precision':prf1[0][1],\n           'Recall': prf1[1][1],\n           'f1-score': prf1[2][1],\n           'Log-loss': metrics.log_loss(y,yhat),\n           'AUC': metrics.roc_auc_score(y,yhat)\n          }\n    return res\n\n","f9b5337e":"numlst","677272c7":"from scipy import stats\nprint('image_cnt  Normality P-value: ',stats.shapiro(Items['image_cnt']).pvalue)\nstats.mannwhitneyu(Items['image_cnt'],Items['success'])\n# Found differences which are significant","36f61218":"print('total_sales  Normality P-value: ',stats.shapiro(Items['total_sales']).pvalue)\nstats.mannwhitneyu(Items['total_sales'],Items['success'])\n# Found differences which are significant","e861e9a1":"print('ebay_price_log  Normality P-value: ',stats.shapiro(Items['ebay_price_log']).pvalue)\nstats.mannwhitneyu(Items['ebay_price_log'],Items['success'])\n# Found differences which are significant","0297dc80":"print('competitor_price_log  Normality P-value: ',stats.shapiro(Items['competitor_price_log']).pvalue)\nstats.mannwhitneyu(Items['competitor_price_log'],Items['success'])\n# Found differences which are significant","ed6306fd":"rows= Items1.columns\nrows = rows.drop(['success'])\nvarSel = pd.DataFrame(rows, columns = ['Variable'])\n\n#varSel = varSel.drop([0,15], axis=0)\n#varSel['Univariable'] = 0\n#varSel.loc[varSel['Variable'].isin(vn1), 'Univariable'] = 1\nvarSel","9e422d1e":"import sklearn.model_selection as model\ny = Items1['success']\nX = Items1.drop(['success'], axis=1)\nx_train,x_test,y_train,y_test = model.train_test_split(X,y,stratify=y, test_size=0.2,random_state=2021)\n\nx_train_category = x_train.drop(numlst, axis=1).iloc[:,0:23]\nprint(x_train_category.shape)","3f86a5ed":"# Feature Selection with Univariate Statistical Tests\nfrom pandas import read_csv\nfrom numpy import set_printoptions\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n# feature extraction\nfeatures = SelectKBest(score_func=f_classif, k=50).fit(X, y)\n# summarize scores\nset_printoptions(precision=3)\n#print(fit.scores_)\n#features.get_feature_names_out()\n\n#print selected features\nselected_features= X.columns[(features.get_support())]\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","1cbeb2ee":"varSel['ANOVA'] = features.get_support().astype('int64')\nvarSel","e950b884":"numlst","3d747f66":"from sklearn.feature_selection import chi2\n\nfeatures = SelectKBest(score_func=chi2, k='all')\nfeatures.fit(x_train_category, y_train)\nx_train_fs = features.transform(x_train_category)\n#x_test_fs = fs.transform(x_test)\n\nselected_features= x_train_category.columns[(features.get_support())]\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","1b3ea44c":"res = selected_features\nkeys = x_train_category.columns\n\nvarSel['CHI2'] = 0\nvarSel.loc[varSel['Variable'].isin(keys), 'CHI2'] = 1\n\nvarSel","b592bee3":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\nlassomod = Lasso(alpha=0.01).fit(X, y)\nmodel = SelectFromModel(lassomod, prefit=True)","8a581c53":"#print selected features\nselected_features= X.columns[(model.get_support())]\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","370d4992":"varSel['Lasso'] = model.get_support().astype('int64')\nvarSel","4011dffe":"from sklearn.ensemble import RandomForestClassifier\n\nrfmod = RandomForestClassifier(random_state=1).fit(X, y)\n","585dd61e":"model = SelectFromModel(rfmod, prefit=True)\nselected_features= X.columns[(model.get_support())]\n\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","4a5e9d79":"varSel['RandomForest'] = model.get_support().astype('int64')\nvarSel","a7a83e12":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbmod = GradientBoostingClassifier(random_state=1).fit(X, y)\n\n#gbmod = GradientBoostingClassifier(random_state=1).fit(X, np.ravel(y))\nmodel = SelectFromModel(gbmod, prefit=True)","63617f7d":"selected_features= X.columns[(model.get_support())]\n\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","e0682a0a":"varSel['GradientBoost'] = model.get_support().astype('int64')\nvarSel","cfe51c13":"varSel['Sum'] =  np.sum(varSel,axis=1)\nvarSel","132ca8df":"varSel.groupby('Sum')['Variable'].count()","323698f1":"varSel[varSel['Sum']>=3].iloc[:, 0]\nfinal_vars = varSel[varSel['Sum']>=3].iloc[:, 0]\nfinal_vars","621a4943":"temp = train_test(data=Items1,prop=0.7,seed=2,tableone=True)","8edcf4b1":"test = temp[1]\ntrain = train_test(data=temp[0],prop=0.8,seed=2,tableone=True)\ndev = train[1]\ntrain = train[0]","7834781a":"import pickle\npickle.dump( train, open( \".\/train\", \"wb\" ))\npickle.dump( dev, open( \".\/dev\", \"wb\" ))\npickle.dump( test, open( \".\/test\", \"wb\" ))","a841848b":"### Summarization and Selection of Variables ","8f89057c":"### Variable Selection using SVM classification","ff470678":"tab1[tab1['p_value']<0.05]","f3e3eda9":"#print selected features\nselected_features= X.columns[(features.get_support())]\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","b806ab4d":"# Partition","b237eba8":"**# Feature Extraction with RFE**","8aef539e":"### Variable Selection using Random Forest\n\n---","2e853c53":"# Feature Engineering","86895828":"## Multivariable Analysis","d5a4936f":"vn1 = tab1.loc[tab1['p_value']<0.05,'Variables'].unique()\nprint(len(vn1))\nvn1","8450a5d0":"### Variable Selection using Gradient Boosting classification","b71e4871":"##############################","cf8eb08c":"final_vars.reset_index()\nfinal_vars[11] = 'success'\nItems1 = Items[final_vars]","0b5fcb12":"from sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\n# choose number of features\n#select = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=50)\n\n# Automatic choice of number of features\nselect = RFECV(estimator=DecisionTreeClassifier())\n\nselect.fit(x_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nprint(mask)\n\nfeatures = select","15552555":"**Mutual Information Feature Selection**","fcf2fb1a":"# polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\ndata = np.array(Items['ebay_price_log']).reshape(-1, 1)\npoly = PolynomialFeatures(degree=4, include_bias=False)\nebay_price_poly=poly.fit_transform(data)\npoly.get_feature_names()","48559828":"##############################","39106e4e":"**CHI2 test for categorical variables**","32c37f5d":"varSel['SVM'] = model.get_support().astype('int64')\nvarSel","978d5367":"add char_count, avg_words, stopwords, numerics count, upper case letters and Sentiment Analysis","2858bbe6":"selected_feat= X.columns[(model.get_support())]\nlen(selected_feat)","425319c1":"# Feature Selection","944c28cd":"varSel['RFE'] = features.get_support().astype('int64')\nvarSel","77583bc0":"# ANOVA","b912f690":"### Variable Selection using LASSO (L1 penalization)","b4410948":"from sklearn.feature_selection import mutual_info_classif\n\n# SelectPercentile or SelectKBest\nfeatures = SelectKBest(score_func=mutual_info_classif, k='all')\nfeatures.fit(x_train_category, y_train)\nx_train_fs = features.transform(x_train_category)\n#x_test_fs = fs.transform(x_test)\n\nselected_features= x_train_category.columns[(features.get_support())]\nprint(\"Num selected_features: %d\" % len(selected_features))\nprint(selected_features)","afcd237a":"from sklearn.svm import SVC\n\nsvmmod = SVC(C=0.01).fit(X, y)\nmodel = SelectFromModel(svmmod, prefit=True)","ad43e3ec":"# Text analysis\n**item description** feature analysis\n\nThis code was intended to be part of the DATA ENRICHMENT notebook but was added late and presented here.","e55de1ce":"tab1 = Table1(data=Items1, y=\"success\")"}}