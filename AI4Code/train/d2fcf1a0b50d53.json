{"cell_type":{"ef35659a":"code","7ba4dfd0":"code","171aa406":"code","3cf0e1ca":"code","fb61b23d":"code","4c18e180":"code","210a8244":"code","443fe6fa":"code","fb7464ee":"code","c40c1107":"code","77eae435":"code","6c6218f6":"code","26e3dd4b":"code","eea025d7":"code","49c56088":"code","e44c4d8c":"code","88c86bfa":"code","6e3aa8b0":"code","1df40aa1":"code","c0d6a396":"code","03a55655":"code","f2e6d515":"code","eeff500b":"code","e2460878":"code","387005d4":"code","f8131ceb":"code","5d40a9c7":"code","9f0a4279":"code","6d61a274":"code","6b79b6b7":"code","3917bc97":"code","003cde70":"code","a14df56b":"code","a310980c":"code","50de6a83":"code","f6ec4085":"code","0c588177":"code","a83fb95e":"code","06047f2a":"code","3ac4c33f":"code","cbbe607a":"code","96da2a19":"code","1448c3f3":"code","ad4492a5":"code","551328ef":"code","07aee67a":"code","72c82cbd":"code","b6c85932":"code","56576257":"code","57609de8":"code","a0e37dbe":"code","8cf09c70":"code","18a3203a":"code","82076d74":"code","20e93128":"code","4d8cfb7a":"code","2e36034f":"code","aed0f687":"code","ced926e9":"code","9a01e98a":"code","f4847b7c":"code","31937107":"code","7eb4e7ba":"code","033b9e59":"code","defec304":"code","76fcf4f2":"code","a61b4b0f":"code","fae6a825":"code","fd1ade96":"code","d3926307":"code","6d9b9e86":"markdown","24f9bf8e":"markdown","4eb0bf4f":"markdown","f1b875c1":"markdown","9950d923":"markdown","d9b8320b":"markdown","d38ce92b":"markdown","f321ab14":"markdown","6b3e98df":"markdown","6506d5fc":"markdown","390617e9":"markdown","632d68cb":"markdown","adb2d87b":"markdown","98bad8ec":"markdown","9b0e79d1":"markdown","b2508879":"markdown","e48a5c25":"markdown","fbe4e7dd":"markdown","42754a0f":"markdown","c3710a2f":"markdown","60b2e505":"markdown","3b8aa42b":"markdown","cca09c41":"markdown","be015983":"markdown","a3ae112f":"markdown","c36a9064":"markdown","42658c43":"markdown","9a67cd53":"markdown"},"source":{"ef35659a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ba4dfd0":"# Importing all libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","171aa406":"# Loading the dataset\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing-data\/bank_marketing_part1_Data-1.csv')","3cf0e1ca":"df.head()","fb61b23d":"df.shape","4c18e180":"df.info()","210a8244":"df.isnull().sum()","443fe6fa":"df.duplicated().sum()","fb7464ee":"df.describe().T","c40c1107":"df1 = df","77eae435":"plt.figure(figsize = (20,15))\nfeature_list = df1.columns\nfor i in range(len(feature_list)):\n    plt.subplot(4,2,i+1)\n    sns.boxplot(x=df1[feature_list[i]], data=df1, orient='h', color='g')\n    plt.title('Boxplot of {}'.format(feature_list[i]))\n    plt.tight_layout()","6c6218f6":"from scipy.stats import norm","26e3dd4b":"plt.figure(figsize = (20,15))\nfeature_list = df1.columns\nfor i in range(len(feature_list)):\n    plt.subplot(4,2,i+1)\n    sns.distplot(x=df1[feature_list[i]], color='r', fit=norm)\n    plt.title('Distribution Plot of {}'.format(feature_list[i]))\n    plt.tight_layout()","eea025d7":"for columns in df1.columns:\n    print('Skewness of {} is'.format(columns), round(df1[columns].skew(),2))\n    print('Kurtosis of {} is'.format(columns), round(df1[columns].kurt(),2))","49c56088":"sns.pairplot(df1, corner=True);","e44c4d8c":"sns.lmplot(x='spending', y='credit_limit', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of spending w.r.t credit_limit');","88c86bfa":"sns.lmplot(x='spending', y='advance_payments', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of spending w.r.t advance_payments');","6e3aa8b0":"sns.lmplot(x='spending', y='current_balance', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of spending w.r.t current_balance');","1df40aa1":"sns.lmplot(x='credit_limit', y='advance_payments', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of credit_limit w.r.t advance_payments');","c0d6a396":"sns.lmplot(x='current_balance', y='advance_payments', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of current_balance w.r.t advance_payments');","03a55655":"sns.lmplot(x='current_balance', y='max_spent_in_single_shopping', data=df1, palette=\"Set1\", line_kws={'color': 'red'});\nplt.title('Relationship of spending w.r.t max_spent_in_single_shopping');","f2e6d515":"corr = df1.corr()\ncorr","eeff500b":"plt.figure(figsize=(12,12))\nsns.heatmap(corr, annot=True, cmap='Blues');\nplt.show();","e2460878":"# Let us see the variance of each variable\ndf1.var()","387005d4":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","f8131ceb":"scaled_df = sc.fit_transform(df1)","5d40a9c7":"scaled_df","9f0a4279":"scaled_df.shape","6d61a274":"# Transforming scaled data array back to pandas dataframe\nscaled_df = pd.DataFrame(scaled_df, index=df1.index, columns=df1.columns)","6b79b6b7":"scaled_df.head()","3917bc97":"from scipy.cluster.hierarchy import dendrogram, linkage","003cde70":"link1 = linkage(scaled_df, method='single', metric='euclidean')\ndend1 = dendrogram(link1)\nplt.title('Dendrogram - Single Euclidean')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean Distances')\nplt.show();","a14df56b":"link2 = linkage(scaled_df, method='single', metric='cityblock')\ndend2 = dendrogram(link2)\nplt.title('Dendrogram - Single Manhattan')\nplt.xlabel('Customers')\nplt.ylabel('Manhattan Distances')\nplt.show();","a310980c":"link3 = linkage(scaled_df, method='complete', metric='euclidean')\ndend3 = dendrogram(link3)\nplt.title('Dendrogram - Complete Euclidean')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean Distances')\nplt.show();","50de6a83":"link4 = linkage(scaled_df, method='complete', metric='cityblock')\ndend4 = dendrogram(link4)\nplt.title('Dendrogram - Complete Manhattan')\nplt.xlabel('Customers')\nplt.ylabel('Manhattan Distances')\nplt.show();","f6ec4085":"wardlink = linkage(scaled_df, method='ward')\ndend = dendrogram(wardlink)","0c588177":"dend = dendrogram(wardlink, truncate_mode='lastp', p=10)","a83fb95e":"from scipy.cluster.hierarchy import fcluster","06047f2a":"clusters = fcluster(wardlink, t=3, criterion='maxclust')\nclusters","3ac4c33f":"# Adding the cluster profiles to the original dataset\ndf1['clusters'] = clusters","cbbe607a":"df1.head()","96da2a19":"# Cluster frequency\ndf1.clusters.value_counts().sort_index()","1448c3f3":"aggdata = df1.iloc[:,0:8].groupby('clusters').mean()\naggdata['frequency'] = df1.clusters.value_counts().sort_index()\naggdata","ad4492a5":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,3,1)\nsns.scatterplot(y='advance_payments', x='spending', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and advance_payments')\n\nplt.subplot(2,3,2)\nsns.scatterplot(y='current_balance', x='spending', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and current_balance')\n\nplt.subplot(2,3,3)\nsns.scatterplot(y='credit_limit', x='spending', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and credit_limit');\n\nplt.subplot(2,3,4)\nsns.scatterplot(y='min_payment_amt', x='spending', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and min_payment_amt');\n\nplt.subplot(2,3,5)\nsns.scatterplot(y='max_spent_in_single_shopping', x='spending', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and max_spent_in_single_shopping');\n\nplt.subplot(2,3,6)\nsns.scatterplot(y='advance_payments', x='credit_limit', hue='clusters', data=df1, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of advance_payments and credit_limit');","551328ef":"from sklearn.cluster import KMeans","07aee67a":"# forming 3 clusters with K = 3\nk_means = KMeans(n_clusters=3)","72c82cbd":"k_means.fit(scaled_df)","b6c85932":"k_means.labels_","56576257":"k_means.inertia_","57609de8":"# Forming clusters with K = 1,3,4,5,6 and comparing the WSS\nk_means1 = KMeans(n_clusters = 1)\nk_means1.fit(scaled_df)\nk_means1.inertia_","a0e37dbe":"k_means2 = KMeans(n_clusters = 2)\nk_means2.fit(scaled_df)\nk_means2.inertia_","8cf09c70":"k_means3 = KMeans(n_clusters = 3)\nk_means3.fit(scaled_df)\nk_means3.inertia_","18a3203a":"k_means4 = KMeans(n_clusters = 4)\nk_means4.fit(scaled_df)\nk_means4.inertia_","82076d74":"k_means5 = KMeans(n_clusters = 5)\nk_means5.fit(scaled_df)\nk_means5.inertia_","20e93128":"k_means6 = KMeans(n_clusters = 6)\nk_means6.fit(scaled_df)\nk_means6.inertia_","4d8cfb7a":"wss = []","2e36034f":"for i in range(1,11):\n    KM = KMeans(n_clusters=i)\n    KM.fit(scaled_df)\n    wss.append(KM.inertia_)","aed0f687":"wss","ced926e9":"plt.figure(figsize=(8,8))\nplt.plot(range(1,11), wss, marker='*', color='r');\nplt.title('The Elbow Mehtod')\nplt.xlabel('No of K clusters')\nplt.ylabel('WSS Scores')\nplt.show();","9a01e98a":"from sklearn.metrics import silhouette_samples, silhouette_score","f4847b7c":"labels = k_means3.labels_\nsilhouette_score(scaled_df,labels)","31937107":"sil_width = []","7eb4e7ba":"for i in range(2,11):\n    KM = KMeans(n_clusters=i)\n    KM.fit(scaled_df)\n    labels = KM.labels_\n    sil = silhouette_score(scaled_df,labels)\n    print('i ', i, sil)\n    sil_width.append(sil)","033b9e59":"plt.figure(figsize=(8,8))\nplt.plot(range(2,11), sil_width, marker='*', color='b')\nplt.title('Silhouette Scores Mehtod')\nplt.xlabel('No of K clusters')\nplt.ylabel('Silhouette Scores')\nplt.show();","defec304":"scaled_df","76fcf4f2":"df_kmeans = scaled_df","a61b4b0f":"# Adding the cluster profiles to the original dataset\ndf_kmeans['k_clusters'] = k_means3.labels_\ndf_kmeans.head()","fae6a825":"# Cluster frequency\ndf_kmeans['k_clusters'].value_counts().sort_index()","fd1ade96":"aggdata_kmeans = df_kmeans.iloc[:,0:8].groupby('k_clusters').mean()\naggdata_kmeans['frequency'] = df_kmeans.k_clusters.value_counts().sort_index()\naggdata_kmeans","d3926307":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,3,1)\nsns.scatterplot(y='advance_payments', x='spending', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and advance_payments')\n\nplt.subplot(2,3,2)\nsns.scatterplot(y='current_balance', x='spending', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and current_balance')\n\nplt.subplot(2,3,3)\nsns.scatterplot(y='credit_limit', x='spending', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and credit_limit');\n\nplt.subplot(2,3,4)\nsns.scatterplot(y='min_payment_amt', x='spending', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and min_payment_amt');\n\nplt.subplot(2,3,5)\nsns.scatterplot(y='max_spent_in_single_shopping', x='spending', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of spending and max_spent_in_single_shopping');\n\nplt.subplot(2,3,6)\nsns.scatterplot(y='advance_payments', x='credit_limit', hue='k_clusters', data=df_kmeans, palette='viridis')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Cluster of advance_payments and credit_limit');","6d9b9e86":"#### Problem Statement: Clustering\nA leading bank wants to develop a customer segmentation to give promotional offers to its customers. They collected a sample that summarizes the activities of users during the past few months. In this problem statement we are given the task to identify the segments based on credit card usage.","24f9bf8e":"Scaling or Standardization is an important step in data pre-processing. Most of the machine learning models use scaled data unless the data in hand is naturally scaled. \n\nLet us see the variances between variables in the provided dataset.","4eb0bf4f":"----HAPPY LEARNING-----","f1b875c1":"#### Cluster profiles","9950d923":"#### 1.4 Apply K-Means clustering on scaled data and determine optimum clusters. Apply elbow curve and silhouette score. Interpret the inferences from the model. K-means clustering code application with different number of clusters. Calculation of WSS(inertia for each value of k) Elbow Method must be applied and visualized with different values of K. Silhouette Score must be calculated for the same values of K taken above. Customer Segmentation can be visualized using appropriate graphs.","d9b8320b":"#### Skewness and Kurtosis","d38ce92b":"#### 1.2 Let us see if scaling is necessary for clustering in this case.","f321ab14":"#### Cluster evaluation for 3 clusters: The Silhouette score","6b3e98df":"Data Dictionary for Market Segmentation\n1.\tspending: Amount spent by the customer per month (in 1000s)\n2.\tadvance_payments: Amount paid by the customer in advance by cash (in 100s)\n3.\tprobability_of_full_payment: Probability of payment done in full by the customer to the bank\n4.\tcurrent_balance: Balance amount left in the account to make purchases (in 1000s)\n5.\tcredit_limit: Limit of the amount in credit card (10000s)\n6.\tmin_payment_amt : minimum paid by the customer while making payments for purchases made monthly (in 100s)\n7.\tmax_spent_in_single_shopping: Maximum amount spent in one purchase (in 1000s)\n","6506d5fc":"#### K-means Clusters Scatterplot","390617e9":"#### WSS scores keep reducing as we increase the number of clusters","632d68cb":"#### Hierarchical Clusters Scatterplot","adb2d87b":"### Bivariate Analysis","98bad8ec":"Silhouette score is the best for 3 clusters hence we will go with 3 cluster profiling for this dataset","9b0e79d1":"#### Boxplots","b2508879":"#### Pairplots","e48a5c25":"### Business insights based on Cluster profiles: \n\nWhen we look at the final clusters merged with original dataset and take the average values for the variables, below are the recommendations for each cluster profile. \n\nCluster 1: Platinum customers\nCluster 3: Gold customers\nCluster 2: Silver customers\n\nCustomers under cluster 1 have a high spending, current balance, credit_limit and max_spent_in_single_shopping which clearly shows that they are premium high-net worth customers who make expensive purchases on their credit cards.\n\nCustomers under cluster 3 have a relatively lesser spending, current balance, credit_limit and max_spent_in_single_shopping which indicate that they are upper middle class customers. The bank can provide promotional offers to this segment such that they increase their spending and are potential customers who can move into premium segments.\n\nCustomers under cluster 2 have the least spending and credit_limits compared to other clusters. This signifies that they are customers who have recently bought credit cards or youths who have started working recently. Bank can provide customized offers to this segment to promote more spending on credit cards.\n","fbe4e7dd":"### Univariate Analysis","42754a0f":"From the above table below are the observations.\n\n1.\tSpending which is the target variable looks like it\u2019s normally distributed as we can see that mean and median are same.\n2.\tadvance_payments also seems to be normally distributed. This variable might be of use as it shows that customers are paying the amount in advance which is timely payment for the bank.\n3.\tThe average probability_of_full_payment is 87.10%. Hence we need to analyse further to see the rest of the customers who fall under 13% who have not done the payment in full. This variable is normally distributed.\n4.\tMinimum current_balance held by customer is 4899.00.\n5.\tcredit_limit of customers range between 26300.00 to 40330.00. The average credit_limit of customers is 32586.05.\n6.\tThe minimum of min_payment_amt paid is 76.51. The maximum of min_payment_amt paid is 845.60. This suggests the data is widely spread for this variable and might have outliers. Also looks like normally distributed.\n7.\tThe average of max_spent_in_single_shopping is 5408.07. The maximum of max_spent_in_single_shopping is 6550.00.\n","c3710a2f":"#### 1.3 Apply hierarchical clustering to scaled data. Identify the number of optimum clusters using Dendrogram. It can be obtained via Fclusters or Agglomerative Clustering.","60b2e505":"From the above pairplot and correlation heatmaps, we can see that there is positive linear relationship between advance_payments and spending, current_balance and spending, credit_limit and spending, current_balance and advance_payments, credit_limit and advance_payments, max_spent_in_single_shopping and current_balance. This suggests that there is Multicollinearity between the variables.  ","3b8aa42b":"#### Distribution Plots","cca09c41":"#### Correlation Heatmaps","be015983":"#### Lmplots","a3ae112f":"#### 1.1 Exploratory Data Analysis","c36a9064":"From the above table though there is not much variance between most of the variables, our target variable spending has a variance of 8.46 whereas other variables variance lie between 0 and 2. Hence scaling is necessary. \n\nWe will be using the Standard Scaler method for scaling our data. This method will calculate the z-score for each data point and then scale the data such that mean = 0 and variance\/standard deviation = 1. ","42658c43":"#### Cluster Profiles","9a67cd53":"#### Calculating WSS for other values of K - Elbow Method"}}