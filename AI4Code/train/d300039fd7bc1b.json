{"cell_type":{"87197234":"code","4065da1e":"code","94c50f77":"code","5e663ea9":"code","dec21c50":"code","d8ee0a31":"code","05515866":"code","d71913dd":"code","48814e50":"code","d76b14e5":"code","0fd9e561":"code","1eecf3e8":"code","04577a52":"code","6feabf6f":"code","d4833b9c":"code","fb67143b":"code","bc508f3f":"code","9f37ac05":"code","da4f019b":"code","c29b05d4":"code","5629e065":"code","f22b6722":"code","327a124d":"code","57f696d2":"code","18d308ef":"code","48e24b3b":"code","92de0e8d":"code","0338c8a8":"code","bb9053b2":"code","a2a7bbf0":"code","76f534bc":"code","973712dd":"code","725a6c0a":"code","3a7b8c18":"code","bcd251fa":"code","516770be":"code","5810a567":"code","902a082b":"code","e9a5999e":"code","c902e892":"code","fd3f1b8d":"code","e6355026":"code","7d865bd1":"code","7ba4a385":"code","c0b41862":"code","0a1942a7":"code","eb6fc05e":"code","72b65ebc":"code","25082d6e":"code","34b436d9":"code","67852ec5":"code","e7ea0492":"code","7c94d3ac":"code","bd077d59":"code","d04a38aa":"code","f33ace25":"code","fc722b1b":"code","019e3c80":"code","008f52f3":"code","d25cfa80":"code","4eaa9c7b":"code","219b2475":"code","4b629f28":"markdown","4dfba177":"markdown","68ed03ea":"markdown","aa64b706":"markdown","93dd7774":"markdown","b1b4f875":"markdown","6a682f21":"markdown","9ff5f4bb":"markdown","3cd4d13c":"markdown","eff281b2":"markdown","d1a16b64":"markdown","544de306":"markdown","2cfb955c":"markdown","b147db77":"markdown","2e26d087":"markdown","f4c031a3":"markdown","795778de":"markdown","e8b1a876":"markdown","33b0d903":"markdown","5aa3efa5":"markdown","7d683012":"markdown","5920c041":"markdown","58727b79":"markdown","a43d8cfc":"markdown","18cd4482":"markdown","7a710277":"markdown","c03b8557":"markdown","bc3a35cb":"markdown","dd12f217":"markdown","6102115c":"markdown","258dbaa2":"markdown","07cce8df":"markdown","1e8472f5":"markdown","f3d62f50":"markdown","f458af60":"markdown","01b60da9":"markdown","1eec4594":"markdown","e2185e26":"markdown","dde2448f":"markdown","526b44eb":"markdown","fa9fac27":"markdown","356199cd":"markdown","b08633eb":"markdown","ca8f9e34":"markdown","e5706098":"markdown","87c3cc78":"markdown","09f76ae7":"markdown","30c5442b":"markdown","ad1d48fa":"markdown","709b7960":"markdown","fda2fddf":"markdown","b6ba8922":"markdown","64e0b9da":"markdown","55a962fb":"markdown","ce2a59cc":"markdown","718f2f98":"markdown","2ee2dc92":"markdown","9470ae74":"markdown","d3734657":"markdown","a17864cc":"markdown","e1586b16":"markdown","8c34bb34":"markdown","7e1fa02b":"markdown","2de180ad":"markdown","19cdd3f0":"markdown","8e6d8594":"markdown","138e25f4":"markdown","2299ad5f":"markdown","f8cfa9ea":"markdown","afff84c8":"markdown","08da08dd":"markdown","d0b521e2":"markdown","8f646449":"markdown","6a6127fe":"markdown","ee594b1d":"markdown","7bab617f":"markdown","39a4bcc4":"markdown","720deb7e":"markdown","b4211320":"markdown","e3558d7f":"markdown"},"source":{"87197234":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc, roc_curve, classification_report, confusion_matrix, roc_auc_score","4065da1e":"# %matplotlib inline","94c50f77":"# plt.rcdefaults()\n# sns.set_style()","5e663ea9":"# plt.rc(\"figure\", figsize=[9, 5])\n# plt.style.use(\"seaborn\")\n# sns.set(rc={\"figure.figsize\": [9, 5]})","dec21c50":"data = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndata.drop(labels=[\"sl_no\"], axis=1, inplace=True)","d8ee0a31":"data.head()","05515866":"data.dtypes","d71913dd":"data.info()","48814e50":"data.isna().sum().sort_values(ascending=False)","d76b14e5":"data.loc[data[\"salary\"].isna(), :]    # You can also use this code :) -->  data[data[\"salary\"].isna()]","0fd9e561":"data.describe()","1eecf3e8":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"status\", data=data, ax=ax)\nplt.title(\"Target Class Distribution\")\nplt.xlabel(\"Class Label\")\nplt.show()","04577a52":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"gender\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Gender\")\nplt.title(\"Gender vs Placement\")\nplt.show()","6feabf6f":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"specialisation\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Specialisation\")\nplt.title(\"Specialisation vs Placement\")\nplt.show()","d4833b9c":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"workex\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Work Experience\")\nplt.title(\"Work Experience vs Placement\")\nplt.show()","fb67143b":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"degree_t\", hue=\"status\", data=data, ax=ax)\nplt.title(\"Degree Priority for Placement\")\nplt.xlabel(\"Degree\")\nplt.show()","bc508f3f":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"ssc_b\", hue=\"status\", data=data, ax=ax)\nplt.title(\"Board of Education vs Placement\")\nplt.xlabel(\"Board of Education\")\nplt.show()","9f37ac05":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"hsc_s\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"HSC Groups\")\nplt.title(\"HSC Groups vs Placement\")\nplt.show()","da4f019b":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.barplot(x=\"status\", y=\"etest_p\", data=data, ax=ax, ci=None)\nplt.title(\"Employability Test vs Placement\")\nplt.xlabel(\"Status\")\nplt.ylabel(\"Employability Test\")\nplt.show()","c29b05d4":"fig = plt.figure(figsize=[11, 6])\nax = fig.add_subplot()\n\nsns.scatterplot(x=\"ssc_p\", y=\"hsc_p\", hue=data[\"status\"].tolist(),\n                style=data[\"ssc_b\"].tolist(), size=data[\"hsc_s\"].tolist(), data=data, ci=None, ax=ax)\nplt.xlabel(\"Secondary School Percentage\")\nplt.ylabel(\"Higher Secondary School Percentage\")\nplt.show()","5629e065":"categorical_variables = data.select_dtypes(include=\"object\").columns.tolist()\n\ntreat_not_as_same = [\"degree_t\", \"hsc_s\"]\n\ntreat_as_same = [var for var in categorical_variables if not var in treat_not_as_same]","f22b6722":"for var in treat_as_same[:-1]:\n    dict_to_map = {j:i for i, j in enumerate(data[var].unique())}\n    data[var] = data[var].map(dict_to_map)\n\ndata[\"status\"] = data[\"status\"].map({\"Not Placed\": 0, \"Placed\": 1})","327a124d":"for var in treat_not_as_same:\n    data = pd.concat(objs=[data, pd.get_dummies(data=data[var])], axis=1)\n    data.drop(labels=var, axis=1, inplace=True)\n    \ndata.drop(labels=\"salary\", axis=1, inplace=True)","57f696d2":"fig = plt.figure(figsize=[11, 6])\nax = fig.add_subplot()\n\nsns.heatmap(data=data.corr(), cmap=\"RdYlGn\", annot=True, fmt=\".2f\", ax=ax)\nplt.title(\"Correlation Among all Variables\")\nplt.show()","18d308ef":"X = data.drop(labels=\"status\", axis=1).values\ny = data[\"status\"].values","48e24b3b":"models = [(\"Logistic Regression\", LogisticRegression(random_state=0, n_jobs=-1)),\n         (\"Linear SVM\", SVC(kernel=\"linear\", random_state=0)),\n         (\"RBF SVM\", SVC(kernel=\"rbf\", random_state=0)),\n         (\"Decision Tree\", DecisionTreeClassifier(random_state=0)),\n         (\"Random Forest\", RandomForestClassifier(n_jobs=-1, random_state=0)),\n         (\"Adaboost RF\", AdaBoostClassifier(base_estimator=RandomForestClassifier(n_jobs=-1, random_state=0), random_state=0, learning_rate=0.1)),\n         (\"Adaboost DT\", AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=0), learning_rate=0.1)),\n         (\"Gradient Boosting\", GradientBoostingClassifier(random_state=0))]","92de0e8d":"stratified = StratifiedKFold()\nmodel_details = {name: [] for name, _ in models}\n\nfor train_index, test_index in stratified.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for name, model in models:\n        if name in [\"Logistic Regression\", \"Linear SVM\", \"RBF SVM\"]:\n            std = StandardScaler()\n            X_train = std.fit_transform(X_train)\n            X_test = std.transform(X_test)\n\n        model.fit(X_train, y_train)\n        train_accuracy = model.score(X_train, y_train)\n        test_accuracy = model.score(X_test, y_test)\n        model_details[name].append((train_accuracy, test_accuracy))","0338c8a8":"summary_df = pd.DataFrame(index=[\"Train Score\", \"Test Score\"])\n\nfor model, accuracy in zip(model_details.keys(), model_details.values()):\n    train_accuracy = [train_accuracy for train_accuracy, _ in accuracy]\n    test_accuracy = [test_accuracy for _, test_accuracy in accuracy]\n    summary_df[model] = [np.mean(train_accuracy), np.mean(test_accuracy)]","bb9053b2":"summary_df.T","a2a7bbf0":"stratified_split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n\nfor train_index, test_index in stratified_split.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","76f534bc":"std = StandardScaler()\nscaled_train = std.fit_transform(X_train)\nscaled_test = std.transform(X_test)","973712dd":"logistic = LogisticRegression(random_state=0, n_jobs=-1)\nlogistic.fit(scaled_train, y_train)\nprint(\"Logistic Regression Test Score :\", logistic.score(scaled_test, y_test))","725a6c0a":"con_mat = pd.DataFrame(confusion_matrix(y_test, logistic.predict(scaled_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()","3a7b8c18":"fpr, tpr, thresshold = roc_curve(y_test, logistic.predict_proba(scaled_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","bcd251fa":"C = [100, 10, 1.0, 0.1, 0.01]\npenalty = ['l2', 'l1']\nparam = {\"C\": C, \"penalty\": penalty}\n\ngrid = GridSearchCV(estimator=LogisticRegression(random_state=0, n_jobs=-1), param_grid=param, n_jobs=-1)\ngrid.fit(scaled_train, y_train)\n\ngrid_model = grid.estimator.fit(scaled_train, y_train)\ntest_score = grid_model.score(scaled_test, y_test)\nprint(\"Tuned Logistic Regression Test Score :\", test_score)","516770be":"print(classification_report(y_test, grid_model.predict(scaled_test)))","5810a567":"svc = SVC(kernel=\"rbf\", random_state=0, probability=True)\nsvc.fit(scaled_train, y_train)\nprint(\"SVM Test Score :\", svc.score(scaled_test, y_test))","902a082b":"con_mat = pd.DataFrame(confusion_matrix(y_test, svc.predict(scaled_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"SVM Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","e9a5999e":"fpr, tpr, thresshold = roc_curve(y_test, svc.predict_proba(scaled_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - SVM\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","c902e892":"C = [100, 10, 1.0, 0.1, 0.01]\nkernel = [\"linear\", \"RBF\", \"poly\"]\nparam = {\"C\": C, \"kernel\":kernel}\n\ngrid = GridSearchCV(estimator=SVC(random_state=0), param_grid=param, n_jobs=-1)\ngrid.fit(scaled_train, y_train)\n\ngrid_model = grid.estimator.fit(scaled_train, y_train)\ntest_score = grid_model.score(scaled_test, y_test)\nprint(\"Tuned SVM Test Score :\", test_score)","fd3f1b8d":"print(classification_report(y_test, grid_model.predict(scaled_test)))","e6355026":"dec = DecisionTreeClassifier(random_state=0)\ndec.fit(X_train, y_train)\nprint(\"Decision Tree Classifier Test Score :\", dec.score(X_test, y_test))","7d865bd1":"con_mat = pd.DataFrame(confusion_matrix(y_test, dec.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","7ba4a385":"fpr, tpr, thresshold = roc_curve(y_test, dec.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","c0b41862":"depth = list(range(1, 11))\nmin_sample_split = np.arange(5, 30, 5)\nmin_leaf_sample = np.arange(3, 16, 3)\nfeatures = [\"auto\", \"sqrt\", \"log2\"]\nmax_leaf_nodes = [4, 6, 8, 10]\nparam = {\"max_depth\": depth, \"min_samples_split\": min_sample_split, \"min_samples_leaf\": min_leaf_sample,\n         \"max_features\": features, \"max_leaf_nodes\": max_leaf_nodes}\n\ngrid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=param, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\ngrid_model = grid.estimator.fit(X_train, y_train)\ntest_score = grid_model.score(X_test, y_test)\nprint(\"Tuned Decision Tree Test Score :\", test_score)","0a1942a7":"print(classification_report(y_test, grid_model.predict(X_test)))","eb6fc05e":"ran = RandomForestClassifier(random_state=0)\nran.fit(X_train, y_train)\nprint(\"Random Forest Classifier Test Score :\", ran.score(X_test, y_test))","72b65ebc":"con_mat = pd.DataFrame(confusion_matrix(y_test, ran.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","25082d6e":"fpr, tpr, thresshold = roc_curve(y_test, ran.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","34b436d9":"n_estimators = np.arange(100, 140, 10)\nmax_depth = np.arange(3, 10, 2)\nmax_features = [\"auto\", \"sqrt\"]\nmax_leaf_nodes = np.arange(3, 10, 2)\nparam = {\"n_estimators\": n_estimators, \"max_depth\": max_depth, \"max_features\": max_features, \"max_leaf_nodes\": max_leaf_nodes}\n\ngrid = GridSearchCV(estimator=RandomForestClassifier(random_state=0, n_jobs=-1), param_grid=param, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\ngrid_model = grid.estimator.fit(X_train, y_train)\ntest_score = grid_model.score(X_test, y_test)\nprint(\"Tuned Random Forest Test Score :\", test_score)","67852ec5":"print(\"AUC afetr parameter tuning : \", roc_auc_score(y_test, grid_model.predict_proba(X_test)[:, 1]))\n\n#confusion matrix\ncon_mat = pd.DataFrame(confusion_matrix(y_test, grid_model.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","e7ea0492":"print(classification_report(y_test, grid_model.predict(X_test)))","7c94d3ac":"ada = AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state=0, n_jobs=-1), learning_rate=0.1, random_state=0)\nada.fit(X_train, y_train)\nprint(\"AdaBoost Classifier Test Score :\", ada.score(X_test, y_test))","bd077d59":"con_mat = pd.DataFrame(confusion_matrix(y_test, ada.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"AdaBoost Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","d04a38aa":"fpr, tpr, thresshold = roc_curve(y_test, ada.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - AdaBoost\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","f33ace25":"print(classification_report(y_test, ada.predict(X_test)))","fc722b1b":"grad = GradientBoostingClassifier(learning_rate=0.1, random_state=0)\ngrad.fit(X_train, y_train)\nprint(\"GradientBoost Classifier Test Score :\", grad.score(X_test, y_test))","019e3c80":"con_mat = pd.DataFrame(confusion_matrix(y_test, grad.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"GardientBoost Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","008f52f3":"print(classification_report(y_test, grad.predict(X_test)))","d25cfa80":"estimators = [(\"LR\", LogisticRegression(random_state=0, n_jobs=-1)),\n             (\"SVC\", SVC(random_state=0)),\n             (\"RF\", RandomForestClassifier(random_state=0, n_jobs=-1)),\n             (\"Ada\", AdaBoostClassifier(RandomForestClassifier(random_state=0, n_jobs=-1), learning_rate=0.1, random_state=0)),\n             (\"Dec\", DecisionTreeClassifier(random_state=0))]\nvot = VotingClassifier(estimators, n_jobs=-1)\nvot.fit(X_train, y_train)\nprint(\"Votting Classifier Test Score :\", vot.score(X_test, y_test))","4eaa9c7b":"con_mat = pd.DataFrame(confusion_matrix(y_test, vot.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Votting Classifier Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","219b2475":"print(classification_report(y_test, vot.predict(X_test)))","4b629f28":"### Classification Report","4dfba177":"### Confusion Matrix","68ed03ea":"### Hyperparameter Tuning ","aa64b706":"### Categorical Data Encoding","93dd7774":"### ROC Curve","b1b4f875":"### Classification Report","6a682f21":"Note:\n1. You can also use train_test_split here to get train and test dataset.\n2. But why I'm using StratifiedShuffleSplit here ?\n3. The answer is that our dataset is somehow imbalanced not fully and this ensure that our train and test will be more representive of both classes.","9ff5f4bb":"### Do Employability Test Helps Getting Job?","3cd4d13c":"###### Logistic Regression has proved to be a good classifier. However we have 4 False Positive (classified students who are not eligible for placement as eligible) and 4 False Negative (classified students who are eligible for placement as not eligible). But don't worry we will try to reduce this Type 1 and Type 2 error :)","eff281b2":"### Does Board of Education Matters ?","d1a16b64":"### Do Work Experience Helps You Get Placed ?","544de306":"###### More or less but same as Logistic Regression performance :( Because Decision Trees are more likely to overfit to training data. So we need to tune it's parameter","2cfb955c":"###### It says 148 (69%) people have been placed and 67 (31%) people have not placed. It concludes that our dataset is imbalanced but not that much :(","b147db77":"### Confusion Matrix","2e26d087":"# Conclusion :)\n1. From the mdoels we have built, Random Forest is doing better than the other models.\n2. You can also use Votting Classifier to predict the same as than Random Forest :(\n3. You can also use SVM other than Ensemble models.\n4. It is possible to select important categorical features using Chi-Square test but I have not implemented here.","f4c031a3":"###### From the above model we can conclude that the Support Vector Machine is doing better than the other models with \"RBF\" kernel. Note we can't stop right here by getting good accuracy, we need to evaluate our model using other metrics too.","795778de":"###### Again I recommend Random Forest :)","e8b1a876":"# Feature Engineering","33b0d903":"### Classification Report","5aa3efa5":"## GradientBoosting","7d683012":"###### The answer is Yes. Because more number of male have been placed than female. But when it comes to not placed people, it's not that much difference.  If you think it is unfair, feel free to post your opinion in the comment section.","5920c041":"# Welcome To This Notebook\n1. I tried my best to explain every tiny details in this notebook in between as I can.\n2. I have compared all famous machine learning models in data modelling section to find the best model as I can :)\n3. Don't Forget to Upvote this Notebook, If you really liked it :) and that encourage me to upload more interesting notebooks like this in the future.","58727b79":"# Required Libraries","a43d8cfc":"### Hyperparameter Tuning","18cd4482":"# Will You Get a Job or Not ?","7a710277":"### Do Specialisation Matters ?","c03b8557":"## Random Forest","bc3a35cb":"###### Really it looks weird but you gotta accept that. If you are a student, mostly you might have heard from your professor or some other saying that \"work experience through internship or any peoject really helps you to get placed\". But here you can see that most of students who have been placed are not having any work experience :(","dd12f217":"## Votting Classifier","6102115c":"###### Again Random Forest is good","258dbaa2":"### Which Degree has More Placements ?","07cce8df":"# Descriptive Analysis - Numeric Data","1e8472f5":"### Optional Settings","f3d62f50":"### ROC Curve","f458af60":"## AdaBoost Classifier","01b60da9":"### Hyperparameter Tuning","1eec4594":"### Take a Sneak Peek :)","e2185e26":"###### Now it is clear that students who have scored more than 60% in both secondary and higher secondary and have chosen commerce as their group in higher secondary have been placed more than the other students :)","dde2448f":"###### I guess the recruiters are mostly choosing \"Commerce\" students because in the above \"Degree Priority\" Chart, the recruiters were mostly selected the students who have completed \"Comm&Mgmt\" degree. But don't worry, they have also recruited more \"Science\" group students which is a good sign for me :)","526b44eb":"###### Don't worry it's doing same as before :)","fa9fac27":"###### Always don't trust too much on Decision Trees but try to trust it using Random Forest and Boosting models.","356199cd":"### Class Imbalance Check !","b08633eb":"###### As you can see, among 14 attributes (columns) there exist one attribute with 67 (215 - 67 = 148) missing values :(","ca8f9e34":"### Hyperparameter Tuning","e5706098":"###### Note: In this problem we have to reduce False Negatives more than False Positives because we can't miss any student who is eligible for placement but our model predicted as not eligible :(","87c3cc78":"###### The \"Comm&Mgmt\" degree has more priority for recruiters to recruit. But they also need some \"Sci&Tech\" students than \"Others\" category students which is good sign for Science and Technology students :)","09f76ae7":"###### When comparing Random Forest with AdaBoost, I recommend to use Random Forest than AdaBoost :)","30c5442b":"Note:\n1. I'm doing Cross Validation before building the model because I want to know which algorithm is giving good result.\n2. So that I can give more effort to that model to get best result as I can :)","ad1d48fa":"### ROC Curve","709b7960":"### Feature Selection","fda2fddf":"###### That's the power of Random Forest :)  It proved to be a good classifier than SVM in our case. SVM has 7 False Positives but Random Forest has only 3 and False Negatives also good when compared overall :)","b6ba8922":"### Classification Report","64e0b9da":"## Decision Tree","55a962fb":"###### As you can see there are some negative and positive correlations among one hot encoded variables.","ce2a59cc":"Note:\n1. Here we are only concerned about predicting whether a student will get placed or not which is a classification problem in our case.\n2. That's why I haven't included salary attribute (column) in EDA.\n3. However If you are interested in predicting the salary of placed and not placed student you can take it as a homework :)","718f2f98":"### Confusion Matrix","2ee2dc92":"###### Yes, AUC for Random Forest is 95 :) That's great isn't it !","9470ae74":"### Classification Report","d3734657":"### Does Gender Impacts Placement ?","a17864cc":"# Let's Look at the Data","e1586b16":"###### Again you can't be confident if you have taken Employability Test to get placed :(","8c34bb34":"# Model Cross Validation","7e1fa02b":"### Do Higher Secondary Group matters ?","2de180ad":"###### Why I'm treating \"degree_t\" and \"hsc_s\" as not same ? The answer is, they are not like all categorical variable they need some order to arange them which is nothing but one degree is bigger or smaller than the other or it has higher value than the others. For example Sci&Tech > Comm&Mgmt > Others","19cdd3f0":"### ROC Curve","8e6d8594":"###### After doing hyperparameter tuning, the model is producing the same accuracy as we got before without tuning parameter.","138e25f4":"### Confusion Matrix","2299ad5f":"### Classification Report","f8cfa9ea":"### Optional","afff84c8":"### Confusion Matrix","08da08dd":"###### The data types of the attributes are in the right form ","d0b521e2":"### Confusion Matrix","8f646449":"### Do High Percentage Holders Got Placed More Than Low Percentage Holders ?","6a6127fe":"### Classification Report","ee594b1d":"###### Looks like both board of education is having more equal (same but not exactly) chances of getting placed :)","7bab617f":"## Support Vector Machine","39a4bcc4":"###### The chances of getting placed for students who have taken \"Mkt&Fin\" specialisation is more than the students with \"Mkt&HR\" specialisation. But don't worry 53 (36 %) out of 148 placed students are from \"Mkt&HR\" :)","720deb7e":"## Logistic Regression","b4211320":"###### Wow, our SVM model has reduced the False Positive count to 1 which is pretty amazing :)","e3558d7f":"# Exploratory Data Analysis"}}