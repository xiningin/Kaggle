{"cell_type":{"675c59f4":"code","58f8afc8":"code","4d0b4c10":"code","f2bbccc5":"code","5d9e4537":"code","b9a8bffb":"code","79d1a529":"code","dbfdec44":"code","03e6768c":"code","ae105770":"code","2530c6e9":"code","ee8c5435":"code","17fdda55":"code","f36f0910":"code","3dcba344":"code","97ac2b73":"code","987ec7fb":"code","11b317fc":"code","86374f24":"code","6c0050ca":"code","063e9c03":"code","316f80cf":"code","9bd35e09":"code","47a78bd1":"code","ce9dbd30":"code","70930960":"code","a4921113":"code","30cec201":"code","b44d15b3":"code","94cc6fc7":"code","c5bc992b":"code","9b08485d":"code","f8e2bbe0":"code","41187d8e":"code","9e2483a3":"code","fc8f6bfc":"code","e938dabd":"code","fc928d32":"code","2226830a":"code","bdd302f5":"code","9e0434a7":"code","59357107":"code","6983c2d3":"code","78c9e489":"code","db294f3a":"code","39848942":"code","b60355a9":"code","e9fdb97a":"code","bba30b57":"code","9a47e663":"code","54069376":"code","d93b06bd":"code","f93b17dc":"code","c26fe2ff":"code","e8653e48":"code","6a9f39ac":"code","51f2b667":"code","047f15c5":"code","07acafd1":"code","d25cd6a5":"markdown","892a20cf":"markdown","e5c7979c":"markdown","5272d37b":"markdown","edcc4c8d":"markdown","5affad4b":"markdown","7848e503":"markdown","f5a8d84d":"markdown","689c024d":"markdown","00e05e93":"markdown","e68ff6ff":"markdown","a824420d":"markdown","a2b8cbcd":"markdown","6e53472d":"markdown","0673c741":"markdown","4573a91b":"markdown","c3d85ca6":"markdown","71b03911":"markdown","30f9495c":"markdown","683516dc":"markdown","5efe1908":"markdown","5c6fcfb6":"markdown","5983e2e3":"markdown","fe5adfaa":"markdown","05325c1b":"markdown"},"source":{"675c59f4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport emoji","58f8afc8":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import make_scorer\nfrom sklearn import set_config; set_config(display='diagram')\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","4d0b4c10":"df = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\n\ndf.head()","f2bbccc5":"print(df.shape)","5d9e4537":"print(df.dtypes)","b9a8bffb":"# checking for duplicates\n\nduplicate_count = len(df)-len(df.drop_duplicates()) # Original data lenght minus data length without duplicates\nduplicate_count #0 duplicates, all right!","79d1a529":"#selecting numerical features, dropping y\nnumerical_features= df.drop(columns='HeartDisease').select_dtypes(include=['float64','int64']) \nnumerical_features","dbfdec44":"plt.figure(figsize=(20, 10))\nfor i, col in enumerate(numerical_features):\n    ax = plt.subplot(2, 3, i+1)\n    sns.histplot(data=numerical_features, x=col)\nplt.tight_layout()","03e6768c":"plt.figure(figsize=(20, 10))\nfor i, col in enumerate(numerical_features):\n    ax = plt.subplot(2, 3, i+1)\n    sns.boxplot(x=numerical_features[col])\nplt.tight_layout()","ae105770":"df[df['RestingBP']==0] #ERROR!","2530c6e9":"index = df[df['RestingBP']==0].index #getting row index\ndf.drop(index, inplace=True) #dropping row from df","ee8c5435":"print(df.shape)","17fdda55":"#create X and y\n\nX= df.drop(columns= 'HeartDisease')\ny= df['HeartDisease']","f36f0910":"#Split data into train and test data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","3dcba344":"print(X_train.shape, y_train.shape)","97ac2b73":"print(X_test.shape, y_test.shape)","987ec7fb":"#Pairplot\n\n#plotting pairwise relationships in dataset\n\nsns.pairplot(df, hue = 'HeartDisease', diag_kind = 'kde',\n             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n             height = 4)","11b317fc":"#Pearson's correlation:\n\ncorr_num = numerical_features.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr_num, cmap='coolwarm', annot=True, vmin=-1, vmax=1) #heatmap\n\n#highest correlation coefficient: -0.38, if >=0.95 we would have dropped one feature to avoid multicollinearity","86374f24":"import plotly.express as px\n\nobservation= numerical_features.iloc[0]\n\ndata_1 = pd.DataFrame(dict(\n    features = observation.index,\n    values = observation.values))\n\nfig = px.line(data_1, x='features', y='values', \\\n              title='Healthy heartbeat', markers=True)\n\nfig.update_traces(line_color='#00CC96')\n\nfig.show()","6c0050ca":"import plotly.express as px\n\nobservation= numerical_features.iloc[1]\n\ndata_2 = pd.DataFrame(dict(\n    features = observation.index,\n    values = observation.values))\n\nfig = px.line(data_2, x='features', y='values', \\\n              title='At risk heartbeat', markers=True)\n\nfig.update_traces(line_color='#EF553B')\n\nfig.show()","063e9c03":"import plotly.graph_objects as go\n\nhealthy_count = df.HeartDisease.value_counts()[0]\nat_risk_count = df.HeartDisease.value_counts()[1]\n\nlabels=['Healthy Heartbeats','At risk Heartbeats']\n\ncolors = ['#00CC96','#EF553B']\n\nfig = go.Figure([go.Bar(x=labels, y=[healthy_count, at_risk_count], marker_color= colors)])\nfig.show()","316f80cf":"df.isnull().sum().sort_values(ascending=False)\/len(df) #NaN percentage for each column \n\n#no missing values detected","9bd35e09":"cat_features= X.select_dtypes(include= 'object')\n\nfor index, col in enumerate (cat_features):\n    \n    print(col, cat_features.iloc[:, index].unique())","47a78bd1":"numeric_cols= list(numerical_features.columns)","ce9dbd30":"numeric_cols.remove('RestingBP') #removing binary column to exclude from Standardization","70930960":"print(numeric_cols)","a4921113":"preproc_categorical = make_pipeline(\n    OneHotEncoder(handle_unknown=\"ignore\", sparse=False) \n)\n\npreproc_numerical = make_pipeline(\n    StandardScaler()\n)\n\npreproc = make_column_transformer(\n    (preproc_numerical, numeric_cols), \n    (preproc_categorical, make_column_selector(dtype_include=[\"object\"])),\n    remainder='passthrough' #keep columns not selected (RestingBP)\n)\n\npreproc","30cec201":"model = LogisticRegression(max_iter=1000)\n\npipe_log = make_pipeline(preproc, model)\n\npipe_log","b44d15b3":"log_cv_results = cross_validate(pipe_log, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', log_cv_results['test_accuracy'].mean())\nprint('Recall: ', log_cv_results['test_recall'].mean())\nprint('Precision: ', log_cv_results['test_precision'].mean())\nprint('F1: ', log_cv_results['test_f1'].mean())\n","94cc6fc7":"#sorted(pipe_log.get_params().keys())","c5bc992b":"solvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\nparam_grid =  {'logisticregression__solver': solvers \\\n               , 'logisticregression__penalty': penalty, \\\n               'logisticregression__C': c_values }\n\nsearch = GridSearchCV(pipe_log, param_grid, \n                              cv=5, n_jobs=-1, scoring='accuracy')\n\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\nprint(search.best_score_)","9b08485d":"pipe_log_final = search.best_estimator_ #it gives us the entire pipeline with the best model\n\npipe_log_final","f8e2bbe0":"final_results = cross_validate(pipe_log_final, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', final_results['test_accuracy'].mean())\nprint('Recall: ', final_results['test_recall'].mean())\nprint('Precision: ', final_results['test_precision'].mean())\nprint('F1: ', final_results['test_f1'].mean())","41187d8e":"from sklearn.metrics import plot_confusion_matrix\n\n# Plot confusion matrix by passing trained model and test data\nplot_confusion_matrix(pipe_log_final, X_test, y_test)","9e2483a3":"model = KNeighborsClassifier(n_neighbors=10)\n\npipe_knn = make_pipeline(preproc, model)\n\npipe_knn","fc8f6bfc":"knn_cv_results = cross_validate(pipe_knn, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', knn_cv_results['test_accuracy'].mean())\nprint('Recall: ', knn_cv_results['test_recall'].mean())\nprint('Precision: ', knn_cv_results['test_precision'].mean())\nprint('F1: ', knn_cv_results['test_f1'].mean())","e938dabd":"from sklearn.svm import SVC","fc928d32":"model= SVC()\n\npipe_svc = make_pipeline(preproc, model)\n\npipe_svc","2226830a":"svc_cv_results = cross_validate(pipe_svc, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', svc_cv_results['test_accuracy'].mean())\nprint('Recall: ', svc_cv_results['test_recall'].mean())\nprint('Precision: ', svc_cv_results['test_precision'].mean())\nprint('F1: ', svc_cv_results['test_f1'].mean())","bdd302f5":"#sorted(pipe_svc.get_params().keys())","9e0434a7":"kernels= ['linear', 'poly', 'rbf', 'sigmoid']\nc_values= [0.1,0.2,0.5, 0.7, 1, 2, 5, 10]\n\nparam_grid =  {'svc__kernel': kernels, \\\n               'svc__C': c_values }\n\nsearch = GridSearchCV(pipe_svc, param_grid, \n                              cv=5, n_jobs=-1, scoring='accuracy')\n\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\nprint(search.best_score_)","59357107":"pipe_svc_final = search.best_estimator_ #it gives us the entire pipeline with the best model\n\npipe_svc_final","6983c2d3":"final_results = cross_validate(pipe_svc_final, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', final_results['test_accuracy'].mean())\nprint('Recall: ', final_results['test_recall'].mean())\nprint('Precision: ', final_results['test_precision'].mean())\nprint('F1: ', final_results['test_f1'].mean())","78c9e489":"# Plot confusion matrix by passing trained model and test data\nplot_confusion_matrix(pipe_svc_final, X_test, y_test)","db294f3a":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel= AdaBoostClassifier(n_estimators=100, random_state=0)\n\npipe_ada = make_pipeline(preproc, model)\n\npipe_ada","39848942":"ada_cv_results = cross_validate(pipe_ada, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', ada_cv_results['test_accuracy'].mean())\nprint('Recall: ', ada_cv_results['test_recall'].mean())\nprint('Precision: ', ada_cv_results['test_precision'].mean())\nprint('F1: ', ada_cv_results['test_f1'].mean())","b60355a9":"from sklearn.ensemble import GradientBoostingClassifier","e9fdb97a":"model= GradientBoostingClassifier()\n\npipe_gbc = make_pipeline(preproc, model)\n\npipe_gbc","bba30b57":"gbc_cv_results = cross_validate(pipe_gbc, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', gbc_cv_results['test_accuracy'].mean())\nprint('Recall: ', gbc_cv_results['test_recall'].mean())\nprint('Precision: ', gbc_cv_results['test_precision'].mean())\nprint('F1: ', gbc_cv_results['test_f1'].mean())","9a47e663":"#sorted(pipe_gbc.get_params().keys())","54069376":"param_grid =  {\"gradientboostingclassifier__n_estimators\":[5,50,250,500],\n               \"gradientboostingclassifier__max_depth\":[1,3,5,7,9],\n               \"gradientboostingclassifier__learning_rate\":[0.01,0.1,1,10,100]\n              }\n\nsearch = GridSearchCV(pipe_gbc, param_grid, \n                              cv=5, n_jobs=-1, scoring='accuracy')\n\nsearch.fit(X_train, y_train)\n\nprint(search.best_params_)\nprint(search.best_score_)","d93b06bd":"pipe_gbc_final = search.best_estimator_ #it gives us the entire pipeline with the best model\n\npipe_gbc_final","f93b17dc":"final_results = cross_validate(pipe_gbc_final, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', final_results['test_accuracy'].mean())\nprint('Recall: ', final_results['test_recall'].mean())\nprint('Precision: ', final_results['test_precision'].mean())\nprint('F1: ', final_results['test_f1'].mean())","c26fe2ff":"# Plot confusion matrix by passing trained model and test data\nplot_confusion_matrix(pipe_gbc_final, X_test, y_test)","e8653e48":"from sklearn.ensemble import StackingClassifier\n\nlog_ridge = LogisticRegression(C= 0.1, penalty= 'l2', solver= 'newton-cg')\nsvc= SVC(C=0.1, kernel= 'linear')\ngboost= GradientBoostingClassifier(learning_rate= 0.1, max_depth= 1, n_estimators= 250)\n\nmodel = StackingClassifier(\n    estimators=[(\"lg\", log_ridge), (\"svc\", svc), (\"gboost\", gboost)],\n    \n    final_estimator=LogisticRegression(),\n    cv=5,\n    n_jobs=-1\n)\n\npipe_stacking = make_pipeline(preproc, model)\npipe_stacking","6a9f39ac":"ensemble_cv_results = cross_validate(pipe_stacking, X_train, y_train, cv=10, \n                            scoring=['accuracy','recall','precision','f1'])\n\nprint('Accuracy: ', ensemble_cv_results['test_accuracy'].mean())\nprint('Recall: ', ensemble_cv_results['test_recall'].mean())\nprint('Precision: ', ensemble_cv_results['test_precision'].mean())\nprint('F1: ', ensemble_cv_results['test_f1'].mean())","51f2b667":"pipe_stacking.fit(X_train,y_train)\n\nplot_confusion_matrix(pipe_stacking, X_test, y_test)","047f15c5":"from sklearn.metrics import confusion_matrix\n\n# predict labels\ny_pred= pipe_log_final.predict(X_test)\n# compare with true labels\ncf_matrix = confusion_matrix(y_test, y_pred)\n# print confusion matrix\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g') #fmt -> integer\n","07acafd1":"sns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","d25cd6a5":"## SVC MODEL \ud83d\udd2e","892a20cf":"### Grid Search CV (Hyperparameters tuning)","e5c7979c":"# EDA","5272d37b":"# TEST ML MODELS \u2697\ufe0f","edcc4c8d":"## ADA BOOSTCLASSIFIER \ud83d\udd2e","5affad4b":"## GRADIENT BOOSTING CLASSIFIER \ud83d\udd2e","7848e503":"##  How many observations of at-risk heartbeats\/healthy heartbeats do we have? (Balanced Dataset)","f5a8d84d":"## Analyse Data Distribution:","689c024d":"## CREATE PIPELINE","00e05e93":"## KNN MODEL \ud83d\udd2e","e68ff6ff":"## Check for duplicates:","a824420d":"## 1) LOGISTIC REGRESSION \ud83d\udd2e","a2b8cbcd":"# Data Exploration","6e53472d":"## CREATE X AND Y \u2794 X_train, y_train, X_test, y_test","0673c741":"## Check for Missing values","4573a91b":"# TESTING BEST MODEL (Logistic Regression - L2 Ridge penalty) \ud83e\udd16","c3d85ca6":"## Analyse Categorical Features","71b03911":"## GRID SEARCH CV (Hyperparameters tuning)","30f9495c":"### Plot an observation of each target class to get a visual idea of what the numbers represent.","683516dc":"# Heart Failure Prediction Dataset \u2764\ufe0f\ud83d\udc89 \ud83e\ude78","5efe1908":"# PIPELINE \ud83d\udcdc","5c6fcfb6":"## GRID SEARCH CV (Hyperparameters tuning)","5983e2e3":"# STACKING \ud83d\udd2e\ud83d\udd2e\ud83d\udd2e","fe5adfaa":"# \ud83c\udfc1","05325c1b":"## Check for Outliers:"}}