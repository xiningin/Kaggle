{"cell_type":{"f2cc640e":"code","94dc993e":"code","c8d8846c":"code","cc7cd83c":"code","974f6e9c":"code","4e0735da":"code","7c97afd7":"code","cd79a4a4":"code","119c09e8":"code","23fc06c1":"code","cd419a17":"code","31d139f4":"code","8dcdfb55":"code","961b8758":"code","3f14644e":"code","e55772ba":"code","9a552688":"code","1f4b54d0":"code","48e6e4c3":"code","135cc207":"code","f01b8c05":"code","9d4621cc":"code","3dabfc1c":"code","683ecc60":"code","d130286a":"code","865ec890":"code","12b933d9":"code","42b6d8dc":"code","f79a1f50":"code","9db6f44d":"code","09e75d69":"code","f9b6b7d8":"code","04064b87":"code","662eac98":"code","1dcb7009":"code","97d15d45":"code","3b8e2f93":"code","eb8306e9":"code","cbeb7c98":"code","12ad75c2":"code","01f12020":"markdown","16fdac62":"markdown","72186319":"markdown","3cbd8024":"markdown","cb323813":"markdown","08c84dcc":"markdown","d66cb8e5":"markdown","e2cd3522":"markdown","11a7c949":"markdown","d83b3b26":"markdown","06d0534a":"markdown","e18a5a86":"markdown","cb3ad8a7":"markdown","1b224c52":"markdown","45efa773":"markdown","55ffb9a5":"markdown","2545544b":"markdown","8290f48c":"markdown","4b0aef1f":"markdown","9006331f":"markdown","8509ff7c":"markdown"},"source":{"f2cc640e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94dc993e":"app_df = pd.read_csv(\"\/kaggle\/input\/credit-card\/application_data.csv\")","c8d8846c":"print(app_df.shape)","cc7cd83c":"#missing data \nmissing_fractions = app_df.isnull().mean().sort_values(ascending=False)\nmissing_fractions.head(10)","974f6e9c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set(style='whitegrid')","4e0735da":"plt.figure(figsize=(6,3), dpi=90)\nmissing_fractions.plot.hist(bins=20)\nplt.title('Histogram of Feature Incompleteness')\nplt.xlabel('Fraction of data missing')\nplt.ylabel('Feature count')","7c97afd7":"app_df['TARGET'].value_counts(dropna=False)","cd79a4a4":"drop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\nprint(drop_list)\nprint(\"\\n\\n Drop Features: \", len(drop_list))","119c09e8":"app_df.drop(labels=drop_list, axis=1, inplace=True)","23fc06c1":"app_df.shape","cd419a17":"print(app_df.columns)","31d139f4":"#useless columns:\n[\"SK_ID_CURR\"]","8dcdfb55":"app_df.drop(labels=[\"SK_ID_CURR\"], axis=1, inplace=True)","961b8758":"# Behavior of the applicant before the loan application\nBehavior=['AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n          'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_WEEK', \n          'AMT_REQ_CREDIT_BUREAU_YEAR']","3f14644e":"#Different Behavior variables\nBehavior_2=['DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE', 'DAYS_REGISTRATION','WEEKDAY_APPR_PROCESS_START',\n       'HOUR_APPR_PROCESS_START']","e55772ba":"# indicator (dummy variable) whether the applicant provided ...\nFlag=['FLAG_MOBIL',\n       'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE',\n       'FLAG_EMAIL','FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',\n       'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n       'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n       'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']","9a552688":"# does not match variable (fraud?)\nnotmatch=['REG_REGION_NOT_LIVE_REGION',\n       'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n       'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',\n       'LIVE_CITY_NOT_WORK_CITY']","1f4b54d0":"#alarming columns ,past default record\nDefault_record=['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']","48e6e4c3":"#column that doesn't contain object input\nvalue_column=[]\nfor i in range(app_df.shape[1]):\n    if type(app_df.iloc[1,i])!=str:\n        value_column.append(i)","135cc207":"#app_df.loc[:,app_df.columns[value_column]].describe().transpose()","f01b8c05":"# We may drop Flag variables\napp_df_2=app_df.drop(labels=Flag, axis=1)","9d4621cc":"sns.set(style=\"whitegrid\", font_scale=1)\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\nsns.heatmap(app_df_2.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"GnBu\",linecolor='w',\n            annot=False, cbar_kws={\"shrink\": .7})","3dabfc1c":"for i in range(app_df.loc[:,Flag].shape[1]):\n    print(app_df.loc[:,Flag].iloc[:,i].value_counts(dropna=False))\n    print(\"\\n\\n\")","683ecc60":"sns.set(style=\"whitegrid\", font_scale=1)\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation Matrix within Flag variable',fontsize=25)\nsns.heatmap(app_df.loc[:,Flag].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"GnBu\",linecolor='w',\n            annot=False, cbar_kws={\"shrink\": .7})","d130286a":"from statsmodels.stats.outliers_influence import variance_inflation_factor","865ec890":"#column that doesn't contain object input\nvalue_column=[]\nfor i in range(app_df.shape[1]):\n    if type(app_df.iloc[1,i])!=str:\n        value_column.append(i)","12b933d9":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX=app_df.iloc[:,value_column].drop(labels=['TARGET'],axis=1)\nX=X.dropna()","42b6d8dc":"vif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f79a1f50":"c=app_df.corr().unstack()\nc.sort_values(ascending=False).drop_duplicates().head(10)","9db6f44d":"c.sort_values(ascending=False).drop_duplicates().tail(10)","09e75d69":"from sklearn.model_selection import train_test_split","f9b6b7d8":"temp=app_df.dropna()\nX = temp.iloc[:,value_column].drop(['TARGET'],axis = 1)\ntarget = temp['TARGET']\nX_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size= 0.3, random_state = 0,stratify=target)","04064b87":"# X = X.interpolate(method ='linear', limit_direction ='forward')\n# interpolate is for missing data ","662eac98":"from sklearn.feature_selection import SelectKBest,mutual_info_classif\nbestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(X,target,)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns) \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score'] \nprint(featureScores.nlargest(10,'Score'))  ","1dcb7009":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE","97d15d45":"print('before SMOTE:',Counter(Y_train))\nsm = SMOTE(sampling_strategy='minority')\nX_train2, Y_train2 = sm.fit_resample(X_train, Y_train)\nprint('After SMOTE:',Counter(Y_train2))","3b8e2f93":"print(Counter(target))\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10,4.5))\nfig.subplots_adjust(bottom=0.10, left=0.10, top = 0.900, right=1.00)\nfig.suptitle(' Target Class Before and After SMOTE', fontsize = 20)\nsns.set_palette(\"bright\")\nsns.countplot(Y_train, ax=ax1)\nax1.margins(0.1)\nax1.set_facecolor(\"#e1ddbf\")\nfor p in ax1.patches:\n        ax1.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nsns.set_palette(\"bright\")\nsns.countplot(Y_train2, ax=ax2)\nax2.margins(0.1)\nax2.set_facecolor(\"#e1ddbf\")\nfor p in ax2.patches:\n        ax2.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nsns.set_style('dark')","eb8306e9":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, target):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index,:], X.iloc[test_index,:]\n    original_ytrain, original_ytest = target.iloc[train_index], target.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n","cbeb7c98":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression(solver='liblinear')\n\nlog_reg_params = {\"penalty\": ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(solver='liblinear'), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\n\n","12ad75c2":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","01f12020":"# Note: You may use one-hot for object variable\njust search one-hot in kaggle\n# Note: cross-validation\n# Note: Some models need Normalizing \/ scaling the data, others don't","16fdac62":"# use .drop() to drop the columns you don't want","72186319":"# roc_auc_score!","3cbd8024":"# example of using SMOTE on logistic regression","cb323813":"# Pearson correlation matrix\nWe use the Pearson correlation coefficient to examine the strength and direction of the linear relationship between two continuous variables.\n\nThe correlation coefficient can range in value from \u22121 to +1. The larger the absolute value of the coefficient, the stronger the relationship between the variables. For the Pearson correlation, an absolute value of 1 indicates a perfect linear relationship. A correlation close to 0 indicates no linear relationship between the variables.\n\nThe sign of the coefficient indicates the direction of the relationship. If both variables tend to increase or decrease together, the coefficient is positive, and the line that represents the correlation slopes upward. If one variable tends to increase as the other decreases, the coefficient is negative, and the line that represents the correlation slopes downward.","08c84dcc":"# Data Imbalance-Over Sampling\/ Under Sampling","d66cb8e5":"# haven't addressed outliers","e2cd3522":"<div align='left'><font size=\"3\" color=\"#000000\">\nRandom oversampling just increases the size of the training data set through repetition of the original examples. It does not cause any increase in the variety of training examples.\n\nOversampling using SMOTE not only increases the size of the training data set, it also increases the variety.\n\nSMOTE creates new (artificial) training examples based on the original training examples. For instance, if it sees two examples (of the same class) near each other, it creates a third artificial one, bang in the middle of the original two.\n\nSMOTE, when done right, is preferable over plain old random oversampling. One, however, has to be careful about the newly created examples and must make sure that they are \u2018legal\u2019. For example, for an input for which legal values are a table and chair, there is no legal in-between value. This, off course, does not apply to, for instance, an input representing the weight of an item.\n    <\/font><\/div> ","11a7c949":"# 1.3 scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = X.drop(columns = ['TARGET'])\n\nsaved_cols = app_df.columns\n\nscaler = MinMaxScaler(feature_range=(0,1))\n\nX = scaler.fit_transform(X)\n\nX = pd.DataFrame(X, columns = saved_cols)\n\n\n","d83b3b26":"# 1.Drop features ","06d0534a":"# VIF Doesn't work for whole dataset(with object variables) after one-hot.\n# divde by 0 error","e18a5a86":"# Selecting 10 best features","cb3ad8a7":"# high correlation between AMT_CREDIT, AMT_ANNUITY, AMT_GOODS_PRICE\n\n# high correlation between Default_record variables","1b224c52":"<div align='left'><font size=\"3\" color=\"#000000\"> SMOTE is a technique that generates new observations by interpolating between observations in the original dataset.Implementing SMOTE on our imbalanced dataset helped us with the imbalance of our labels (more no fraud than fraud transactions).<\/font><\/div> ","45efa773":"<a id = \"Imbalance\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Data Imbalance-Over Sampling<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> One of the common issues found in datasets that are used for classification is imbalanced classes issue. Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed. It usually reflects an unequal distribution of classes within a dataset.<\/font><\/div>   \n> <img style=\"float: centre;\" src=\"https:\/\/miro.medium.com\/max\/364\/1*QoW_njAnS3D0QWve7NNB8w.png\" width=\"350px\"\/>\n<div align='left'><font size=\"3\" color=\"#000000\"> If there are two classes, then balanced data would mean 50% points for each of the class.In our case 50:1 ratio between the fraud and non-fraud classes which is highly imbalance. There are 3 major techniques are there to eliminate the imbalance problem. \n<\/font><\/div>  \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1.Random Over Sampling<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\"> 2.Random Under Sampling<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">3.SMOTE(Synthetic Minority Over-sampling Technique).<\/font><\/div>   \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Since under sampling may discard the useful information which could be important for building good classifiers, we choose random over sampling and SMOTE (Synthetic Minority Oversampling Technique). Random over sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. Implementing Random over sampling on this  dataset helps to the balance the labels (more no fraud than fraud transactions).<\/font><\/div> \n","55ffb9a5":"# 1.2 one-hot encoding\n# app_df=pd.get_dummies(app_df)","2545544b":"# 1.1 Drop features missing more than 30% percent data","8290f48c":"# Note : there is a threshold for VIF in performing linear models!","4b0aef1f":"# Columns of choice","9006331f":"# Limit the Feature Space\nThe full dataset has 122 features for each loan. We'll select features in two steps:\n\n1. Drop features with more than 30% of their data missing.\n2. Of the remaining features, choose only those that would be available to an investor before deciding to fund the loan.","8509ff7c":"# 2. Multicollinearity\nAlthough highly correlated features (multicollinearity) aren't a problem for the machine learning models based on decision trees (as used here), these features decrease importances of each other and can make feature analysis more difficult. Therefore, I calculate feature correlations and remove the features with very high correlation coefficients before applying machine learning."}}