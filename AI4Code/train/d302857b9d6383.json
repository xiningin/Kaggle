{"cell_type":{"bf3965bb":"code","4c88e35e":"code","840cb6f5":"code","5799528f":"code","7313b072":"code","de77817d":"code","e4a9427e":"code","12b78f10":"code","f169d3d3":"code","a778d4ea":"code","e38a8ef7":"code","66ad3bf0":"code","daaaa272":"code","32948356":"code","956e83d9":"code","48bfd61c":"code","133a9287":"code","0a481b1c":"code","089673c1":"code","cca9248e":"code","c7539e50":"code","c075ecbd":"code","618e80f1":"code","323ee372":"code","538e9bba":"code","cb8241d5":"code","ee7382ba":"code","77cbb2be":"code","b6eb471c":"code","c3c9d537":"code","fd1b27fb":"code","8fec50ef":"code","2de557b2":"code","00ec75f7":"code","f326cbed":"code","152c8a38":"code","9698deba":"code","d77f07f7":"code","b0ff2919":"code","2292f531":"code","352432ce":"code","df382811":"code","f2b478b5":"code","8cf9fa43":"code","c3c07db5":"code","62a87dc2":"markdown","594a781b":"markdown","c987a24f":"markdown","29cbec1d":"markdown","649a8660":"markdown","22507d92":"markdown","7028f234":"markdown","e3d27350":"markdown","d98ad086":"markdown","6cbb1fba":"markdown","f738703a":"markdown","228afda6":"markdown","d4b1fc2b":"markdown","0d2100d7":"markdown","906ce862":"markdown","702056a0":"markdown","20ee36c5":"markdown","49de649e":"markdown","6bd09cfc":"markdown","53c1a811":"markdown","75c86dca":"markdown","63419be6":"markdown","af2f308c":"markdown","183a01bf":"markdown","d24eb5ef":"markdown","2007f35c":"markdown","78e89cb0":"markdown","8025802b":"markdown","25239518":"markdown","d544fdf3":"markdown","0e53576a":"markdown","f90ec434":"markdown","551d3ecf":"markdown","95328c77":"markdown","db25811e":"markdown","3db8de64":"markdown","0ca9b0f3":"markdown","87c18e9d":"markdown","86faece5":"markdown","cfca8faf":"markdown","03905e0f":"markdown","beef0752":"markdown","09ea95b5":"markdown","94b446c3":"markdown","a3d3df3c":"markdown","27b6cf8a":"markdown","437e7b9f":"markdown","3a895d4b":"markdown"},"source":{"bf3965bb":"import os\nimport gc\nimport re\nimport random\nimport string\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly import graph_objs as go\n\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nimport spacy\nfrom spacy.util import minibatch\nfrom nltk.corpus import stopwords\nfrom spacy.util import compounding\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n_ = nltk.download(\"stopwords\")","4c88e35e":"def func(pct, allvalues):\n    absolute = int(pct \/100.*np.sum(allvalues))\n    return \"{:.1f}%\".format(pct)\n    pass\n\n\ndef remove_single_word_num(sent):\n    '''\n    Remove numbers and words of single length such as \"x + 23 y - abc\" will become \"+ - abc\"\n    '''\n    dummy_list = []\n  \n    for token in sent.split():\n        if (not token.isdigit()) and ((token.isalpha() and len(token)>1) or (not token.isalnum())):\n            dummy_list.append(token)   \n  \n    return ' '.join(dummy_list)\n\n\ndef insert_spaces(sentence):\n    '''\n    Add a space around special characters, number and digits. So \"2x+y -1\/3x\" becomes: \"2 x + y - 1 \/ 3 x\"\n    '''\n    dummy_list = []\n    splitted_sent = list(sentence)\n    \n    for i in range(len(splitted_sent)-1):\n        dummy_list.append(splitted_sent[i])\n        \n        if splitted_sent[i].isalpha(): # if it is an alphabet\n            if splitted_sent[i+1].isdigit() or (not splitted_sent[i+1].isalnum()):\n                dummy_list.append(' ')\n    \n        elif splitted_sent[i].isdigit(): # if it is a number\n            if splitted_sent[i+1].isalpha() or (not splitted_sent[i+1].isalnum()):\n                dummy_list.append(' ')\n        \n        elif (not splitted_sent[i].isalnum()) and (splitted_sent[i] not in [' ','\\\\']): # if it is a special char but not ' ' already\n            if splitted_sent[i+1].isalnum():\n                dummy_list.append(' ')\n        \n    dummy_list.append(splitted_sent[-1])\n  \n    return ''.join(dummy_list)\n\n\ndef preprocess(a):\n    # convert the characters into lower case\n    a = a.lower()\n\n    # remomve newline character\n    a = re.sub(\"\\\\n\", \" \", a)\n\n    # remove the pattern [ whatever here ]. Use { } or  ( ) in place of [ ] in regex\n    a = re.sub(r\"\\[(.*?)\\]\",' ',a)\n\n    # remove Questions beginners Q5. 5. question 5. \n    a = re.sub(r\"^[\\w]+(\\s|\\.)(\\s|\\d+(\\.*(\\d+|\\s)))\\s*\", \" \", a)\n\n    # remove MathPix markdown starting from \\( and ending at \\) while preserving data inside \\text { preserve this }\n    a = re.sub(r'\\s*\\\\+\\((.*?)\\\\+\\)', lambda x: \" \".join(re.findall(r'\\\\[a-z]{3,}\\s*{([^{}]*)}', x.group(1))), repr(a))\n\n    # remove options from questions i.e character bounded by () given there is no spacing inside ()\n    a = re.sub(r\"\\s*\\([^)\\s]*\\)\\s*\", \" \", a)\n\n    # remove any repeating special character (more than one times) except \\(){}[] and space.  So it'll remove .. ,, ___ +++ etc\n    a = re.sub(r\"([^a-zA-Z0-9\\\\ (){}\\]\\[])\\1{1,}\",' ',a)\n\n    # remove data inside {} -> at max 2 characters {q.}, {5.}\n    a = re.sub(r\"{.{0,2}}\", \" \", a)\n\n    # Insert spaces among spec chars, digits and nums  and then remove every single len alphabet and number\n    a = remove_single_word_num(insert_spaces(a))\n\n    # remove whatever comes after \\\\ double slashes except space \n    a = re.sub(r\"(\\\\[^ ]+)\",' ',a)\n\n    #remove every special characcter\n    a = re.sub(r'(\\W)|([_])',' ',a)\n\n    # remomve newline character\n    a = re.sub(\"\\\\n\", \" \", a)\n\n    # remove repeated space if there is any\n    a = re.sub(r\"\\s+\", \" \", a)\n  \n    return a\n\n\ndef remove_stopword(x):\n    stopwords_new = stopwords.words('english')\n    return [y for y in x if y not in stopwords_new]\n\n\ndef common_tokens(data, col, top_most=50, title=None, return_temp=False, is_top=True):\n     \n    top = Counter([item for sublist in data[col] for item in sublist])\n    if not is_top:\n        temp = pd.DataFrame(top.most_common()[:-top_most:-1])\n    else:\n        temp = pd.DataFrame(top.most_common(top_most))\n    temp.columns = ['Common_words','count']\n    display(temp.style.background_gradient(cmap='Blues'))\n    \n    def plot_barchart(title=None):\n        fig = px.bar(temp, x=\"count\", y=\"Common_words\", title=title, orientation='h', \n                 width=700, height=700, color='Common_words')\n        fig.show()\n        pass\n    \n    # plot_barchart(title)\n    if return_temp:\n        return temp\n    \n    del temp, top\n    _ = gc.collect()\n    \n    pass\n\ndef plot_wordcloud(text, mask=None, max_words=250, max_font_size=100, figure_size=(24.0,16.0), color = 'black',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    pass","840cb6f5":"df = pd.read_csv(\"..\/input\/iitjee-neet-aims-students-questions-data\/subjects-questions.csv\")\ndf = df[df[\"Subject\"] != \"English\"].reset_index(drop=True)\ndf","5799528f":"df.isna().sum().to_frame().rename(columns={0:\"NaN_Count\"}).style.background_gradient(cmap=\"Wistia\")","7313b072":"df.dropna(subset=[\"eng\"], inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf.isna().sum().to_frame().rename(columns={0:\"NaN_Count\"}).style.background_gradient(cmap=\"Wistia\")","de77817d":"target_Count = df['Subject'].value_counts().to_frame()\ntarget_Count.style.background_gradient(cmap=\"BrBG\")","e4a9427e":"target_Count.reset_index(inplace=True)\ntarget_Count.columns = [\"Subject\", \"pct\"]\ntarget_Count.loc[:, \"pct\"] \/= len(df)\n\n\nplt.figure(figsize=(6, 10))\nwegdes, texts, autotexts = plt.pie(target_Count['pct'],\n                                  autopct=lambda pct: func(pct, target_Count['pct']),\n                                  explode=(0.05, 0.05, 0.05, 0.05),\n                                  labels=target_Count[\"Subject\"],\n                                  shadow=True,\n                                  startangle=45,\n                                  wedgeprops={\"linewidth\":1, \"edgecolor\":\"black\"},\n                                  textprops= dict(color=\"black\"))\nplt.legend(wegdes, target_Count[\"Subject\"],\n          title=\"Subjects\",\n          loc=\"center\",\n          bbox_to_anchor=(1, 0, 0, 0))\nplt.setp(autotexts, size=14, weight=\"bold\")\nplt.title(\"Subject Percentage distribution\")\nplt.show()","12b78f10":"# free up space\ndel target_Count\n_ = gc.collect()","f169d3d3":"# let's start with the length of the description.\ndf[\"length_eng\"] = df['eng'].apply(lambda x: len(x.split()))\nsns.distplot(df[\"length_eng\"], color=\"red\", bins=25)\nplt.title(\"Length of the description\")\nplt.show()","a778d4ea":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\nsns.boxplot(x=\"Subject\", y=\"length_eng\", data=df, ax=ax1)\nsns.violinplot(x=\"Subject\", y=\"length_eng\", data=df, ax=ax2)\nplt.show()","e38a8ef7":"pct_25 = lambda x: np.percentile(x, 25)\npct_75 = lambda x: np.percentile(x, 75)\npct_75.__name__ = \"75%\"\npct_25.__name__ = \"25%\"","66ad3bf0":"df.pivot_table(\"length_eng\", \"Subject\", aggfunc=[\"count\", \"min\", pct_25, \"mean\", \"median\", pct_75, \"max\", \"std\", \"var\"]).style.background_gradient(cmap=\"plasma\")","daaaa272":"display(df)","32948356":"for _ in range(5):\n    print(df.loc[np.random.randint(0, len(df)), \"eng\"])","956e83d9":"for _ in range(25):\n    print(\"\\n\")\n    i = np.random.randint(0, len(df))\n    x, y = df.loc[i, \"eng\"], df.loc[np.random.randint(0, len(df)), \"Subject\"]\n    print(x)\n    print()\n    print(y, \"\\t\", i, \"\\n\")\n    print(preprocess(x))","48bfd61c":"df[\"processed_eng\"] = df[\"eng\"].apply(lambda x: preprocess(x))\ndisplay(df.head())","133a9287":"df[\"token_list\"] = df[\"eng\"].apply(lambda x: x.split())\ncommon_tokens(df, col=\"token_list\", top_most=2000)  # experiment with stopwords(removing\/not removing)","0a481b1c":"common_tokens(df, col=\"token_list\", top_most=2000, is_top=False)","089673c1":"df[\"token_list_processed\"] = df[\"processed_eng\"].apply(lambda x: x.split())\nmost_2000 = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True)","cca9248e":"least_2000 = common_tokens(df, col=\"token_list_processed\", top_most=2000, is_top=False, return_temp=True)","c7539e50":"df[\"token_list\"] = df[\"token_list\"].apply(lambda x: remove_stopword(x))\ndf[\"token_list_processed\"] = df[\"token_list_processed\"].apply(lambda x: remove_stopword(x))","c075ecbd":"common_tokens(df, col=\"token_list\", top_most=2000)","618e80f1":"common_tokens(df, col=\"token_list\", top_most=2000, is_top=False)","323ee372":"most_2000_sr = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True)","538e9bba":"least_2000_sr = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","cb8241d5":"Phy = df[df['Subject']=='Physics']\nMath = df[df['Subject']=='Maths']\nChem = df[df['Subject']=='Chemistry']\nBio = df[df['Subject']=='Biology']","ee7382ba":"Phy.name = \"Phy\"\nChem.name = \"Chem\"\nBio.name = \"Bio\"\nMath.name = \"Math\"","77cbb2be":"top_phy = common_tokens(Phy, col=\"token_list_processed\", top_most=2000, return_temp=True)","b6eb471c":"least_Phy = common_tokens(Phy, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","c3c9d537":"top_chem = common_tokens(Chem, col=\"token_list_processed\", top_most=2000, return_temp=True)","fd1b27fb":"least_Chem = common_tokens(Chem, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","8fec50ef":"top_bio = common_tokens(Bio, col=\"token_list_processed\", top_most=2000, return_temp=True)","2de557b2":"least_Bio = common_tokens(Bio, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","00ec75f7":"top_math = common_tokens(Math, col=\"token_list_processed\", top_most=2000, return_temp=True)","f326cbed":"least_math = common_tokens(Math, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","152c8a38":"# we have dataframes for each subject as well as on the whole set\ncommon_words = pd.concat([most_2000.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_common\"}),\n           most_2000_sr.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_common_sr\"}),\n           least_2000.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_common\"}),\n           least_2000_sr.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_common_sr\"}),\n           top_phy.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_phy\"}),\n           least_Phy.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_phy\"}),\n           top_chem.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_chem\"}),\n           least_Chem.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_chem\"}),\n           top_bio.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_bio\"}),\n           least_Bio.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_bio\"}),\n           top_math.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_math\"}),\n           least_math.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_math\"})], axis=1)\ndisplay(common_words.head())\ncommon_words.to_csv(\"common-words-analysis.csv\", index=False)","9698deba":"special_dict = Counter([item for sublist in df[\"eng\"] for item in sublist if not item.isalnum()])\nspecial_dict = pd.DataFrame(special_dict.most_common())\nspecial_dict_pp = Counter([item for sublist in df[\"processed_eng\"] for item in sublist if not item.isalnum()])\nspecial_dict_pp = pd.DataFrame(special_dict_pp.most_common())","d77f07f7":"special_dict.columns = [\"Special_tokens\", \"count\"]\nspecial_dict_pp.columns = [\"Special_tokens\", \"count\"]","b0ff2919":"for ddf in [Phy, Chem, Bio, Math]:\n    \n    special_ = Counter([item for sublist in ddf[\"eng\"] for item in sublist if not item.isalnum()])\n    special_ = pd.DataFrame(special_.most_common())\n    special_.columns = [\"Special_tokens\", ddf.name]\n\n    special_dict = pd.merge(special_dict, special_, how=\"left\", on=\"Special_tokens\")\n    special_dict[ddf.name] = special_dict[ddf.name].fillna(0).astype(\"int64\")\n    \nspecial_dict","2292f531":"display(special_dict.style.background_gradient(cmap=\"twilight_shifted\"))","352432ce":"special_dict.to_csv(\"special-characters-analysis.csv\", index=False)","df382811":"plot_wordcloud(Phy.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Physics\")","f2b478b5":"plot_wordcloud(Chem.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Chemistry\")","8cf9fa43":"plot_wordcloud(Math.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Maths\")","c3c07db5":"plot_wordcloud(Bio.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Biology\")","62a87dc2":"#### Most common words in Chemistry","594a781b":"#### Most common words with preprocessing","c987a24f":"<h1 style=\"color:red\">If you find this notebook interesting and well written, don't forget to upvote :)<\/h1>","29cbec1d":"Seems like there are too many stop words, but we will see, because assuming stopwords based on `English` literature might not work in our case.","649a8660":"Oukaie, we can say that the length data is left skewed. Before going more statistical, let's look for outliers.","22507d92":"# Comparison with After and Before Processing","7028f234":"#### Most common words in Biology","e3d27350":"# Helper functions","d98ad086":"As far as this much of data is concerned, this dataset is pretty imbalanced with very less samples in 2 classes i.e. `English` and `Bio`. and compratively more samples in another 3 classes, i.e `Physics`, `Chemistry` and `Math`. So, students face more difficulties in `Physics`, then `Chemistry` and then `Math` respectively, as they are seeking for help in these subjects. Let's have a pie chart for better visualization.","6cbb1fba":"Let's check out what will happen if we analyse inter-category questions.","f738703a":"#### Least common words without any preprocessing","228afda6":"### Without removing stopwords","d4b1fc2b":"#### Most common words with Preprocessing","0d2100d7":"#### Most common words without any preprocessing","906ce862":"We can see the unexpected tokens as we haven't applied preprocessing yet. Let's apply our preprocessing function and look for the most common words.","702056a0":"Let's see if there lies any outliers by plotting the box plot.","20ee36c5":"Our target is to classify whether the description belongs to one of the categories `Physics`, `Maths`, `Chemistry` or `Bio`. Let's visualise the distribution of our target.","49de649e":"#### Most common words in Maths","6bd09cfc":"What can be our takeaways from here?\n\n- After stop words removal, still we can notice some irrelevant data being appeared as most common words.\n- In most of the cases, these are (b), (a_number), Q., etc. These can be assumed as the option number of the questions. (The same has been discussed with Vedant, and the suggestions on removing such instances are implemented.)\n\n*NOTE: Theses updates have been made in this version. I have removed all such ocurrences those might hamper the model peformance and cause unnecessary similarity between target classes.*","53c1a811":"# Number of samples per Subject","75c86dca":"#### Most common words without any preprocessing","63419be6":"# Length of the Questions\/ Description","af2f308c":"As we can see, there are 3 nan values `eng` column which is supposed to be the question description itself. Hence let's remove those 3 samples and move ahead.","183a01bf":"#### Most Common Words in Physics","d24eb5ef":"#### Least common words in Chemistry","2007f35c":"There are outliers, too much outliers in our length column. Well, the takeaways from these plots would be:\n\n- `Physics` questions are lengthier and `Biology` questions are shorter.\n- `Biology` seems to have lesser number of outliers which indicates, most of the biology question's length fall near their median value, only a few outlies. Variance is less.\n- `Physics` seems to have highest variance with lowest median.\n- `Chemistry` though has the longest description, yet the not so large variance indicates it has quite a significant number of samples roaming arond the median value. And same goes for `Math`, with a lower *longest Description* value.","78e89cb0":"#### Least common words without any preprocessing","8025802b":"# Category wise Data","25239518":"Well, it seems pretty clearer now.","d544fdf3":"Let's start with reading the data.","0e53576a":"# Most common words across classes","f90ec434":"Let's prepare our preprocessing function and see the effects.   \n*NOTE*: The functions can be found under the Helper functions section :)","551d3ecf":"The pie chart shows $96.5\\%$ doubts are from `PCM` only, with toughest among them being `Physics`. Now, let's analyse our text data. i.e `eng` column for better understading of our data.","95328c77":"#### Least common words in Physics","db25811e":"# Time for Wordcloud","3db8de64":"# Analysing most common words in the dataset and per subject","0ca9b0f3":"EOF!","87c18e9d":"#### Least common words in Maths","86faece5":"As we can see, most of the questions contain options such as `(a)`, `(b)`, `(i)` etc. Ofcourse, we will find such options in every subject. As our aim to build a classifier to classify that a given question or description belongs to a particular Subject, keeping these tokens will only bring redudant similarity, so let's remove them and apply some basic preprocessing to move ahead with our exploration.","cfca8faf":"#### Least Common words with preprocessing","03905e0f":"*Note*: Above analysis are obtained provided no preprocessing is applied yet.","beef0752":"# Check for Null values","09ea95b5":"# Statistical Analysis of Length","94b446c3":"#### Least common words in Biology","a3d3df3c":"#### Least common words with Preprocessing","27b6cf8a":"Well, here comes anoter crucial obeservation I think we should drop before movind ahead. Let me show.","437e7b9f":"### With Stopwords removal","3a895d4b":"# Analysing Special characters"}}