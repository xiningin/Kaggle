{"cell_type":{"3da460da":"code","36e20957":"code","39c9c9c4":"code","f84ffab6":"code","30347e30":"code","e8caa6a2":"code","7371f144":"code","ecdae60b":"code","9a694955":"code","3644c4ab":"code","df235713":"code","5ad3e799":"code","6030e583":"code","f4981106":"code","94112e1c":"markdown","ce57cffa":"markdown","a2bab2fb":"markdown","9b9c38fb":"markdown","3192a0a7":"markdown","7449d79d":"markdown","c45202f9":"markdown","8cf2620e":"markdown","933377ef":"markdown","a86ae086":"markdown","91435e40":"markdown","17888f6d":"markdown","485db463":"markdown","fd6fde22":"markdown","83f293b5":"markdown"},"source":{"3da460da":"import pandas as pd\ntrain = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\ntrain","36e20957":"# import matplotlib.pyplot as plt\n# import matplotlib.image as mpimg\n\n# img_1st = mpimg.imread('..\/input\/global-wheat-detection\/train\/b6ab77fd7.jpg')\n# imgplot = plt.imshow(img_1st)","39c9c9c4":"import cv2\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nimg_1st = cv2.imread('..\/input\/global-wheat-detection\/train\/b6ab77fd7.jpg') \n\n# Readed on cv2 format is BGR. But matplotlib shows as RGB format.\nimg_1st = cv2.cvtColor(img_1st, cv2.COLOR_BGR2RGB)\nprint(img_1st.shape)\nplt.imshow(img_1st)","f84ffab6":"data_1st_image = train[train['image_id']==\"b6ab77fd7\"]\n#print(data_1st_image)","30347e30":"import re\nimport numpy as np\n\ndata_bboxes = data_1st_image['bbox']\n\nb_list = [0,0,0,0]\nfor b_str in data_bboxes:\n    b_value = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\",b_str))\n    b_list = np.vstack([b_list,b_value])\n\nb_list = b_list[1:]\n#print(b_list)\n#b_list.shape","e8caa6a2":"data_1st_image.loc[:,'x'] = b_list.T[0].astype(np.float)\ndata_1st_image.loc[:,'y'] = b_list.T[1].astype(np.float)\ndata_1st_image.loc[:,'w'] = b_list.T[2].astype(np.float)\ndata_1st_image.loc[:,'h'] = b_list.T[3].astype(np.float)\n#data_1st_image","7371f144":"boxes = data_1st_image[['x', 'y', 'w', 'h']].values\nboxes[:, 2] = boxes[:, 0] + boxes[:, 2]\nboxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n#print(boxes)","ecdae60b":"import cv2\nimport numpy as np\n\n# cv2 can't show with float values. \nboxes_ = boxes.astype(np.int32)\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes_:\n    cv2.rectangle(img_1st,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (255,0,0),3)\n\nplt.imshow(img_1st)","9a694955":"cols = ['image_id','boxes','labels']\ntarget = pd.DataFrame(index=[], columns=cols)\n\n# get image_id\nimage_ids = train['image_id'].unique()\nimage_ids = image_ids[0]  #'b6ab77fd7'\n\nlabels = np.ones(len(boxes))\n\nimg_1st = cv2.imread('..\/input\/global-wheat-detection\/train\/b6ab77fd7.jpg') \nimg_1st = cv2.cvtColor(img_1st, cv2.COLOR_BGR2RGB).astype(np.float32)\nimg_1st \/= 255.0\n\ntarget = {\n    'image': img_1st,\n    'bboxes': boxes,\n    'labels':labels\n}","3644c4ab":"import albumentations as Albu\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# from\u30104\u3011 code\ndef get_train_transform():\n    return Albu.Compose([\n        Albu.Flip(p=1.0),  # In\u30104\u3011original code, p=0.5 \n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ntransforms = get_train_transform()\nsample = transforms(**target)\nimg = sample['image']\n\nplt.figure(figsize=(8, 5))\nplt.imshow(np.transpose(img, (1, 2, 0)));","df235713":"# from @pestipeti 's notebook\u30104\u3011\ntrain_ = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    return r\n\ntrain_['x'] = -1\ntrain_['y'] = -1\ntrain_['w'] = -1\ntrain_['h'] = -1\n\n\ntrain_[['x', 'y', 'w', 'h']] = np.stack(train_['bbox'].apply(lambda x: expand_bbox(x)))\n\ntrain_.drop(columns=['bbox'], inplace=True)\ntrain_['x'] = train_['x'].astype(np.float)\ntrain_['y'] = train_['y'].astype(np.float)\ntrain_['w'] = train_['w'].astype(np.float)\ntrain_['h'] = train_['h'].astype(np.float)\n\n\nimage_ids = train_['image_id'].unique()\nprint(\"len(image_ids) = {}\".format(len(image_ids)))","5ad3e799":"from torch.utils.data import DataLoader, Dataset\n\nclass WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","6030e583":"# Albumentations\ndef get_train_transform():\n    return Albu.Compose([\n        Albu.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","f4981106":"import torch\nfrom torch.utils.data.sampler import SequentialSampler\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = WheatDataset(train_, '\/kaggle\/input\/global-wheat-detection\/train', get_train_transform())\n\n# images, targets, image_ids = next(iter(train_dataset))\n# images = list(image.to(device) for image in images)\n# print(images)\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=3,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nprint(len(train_data_loader))","94112e1c":"![Wheat_target.jpg](attachment:Wheat_target.jpg)","ce57cffa":"# Overview:\n1. Show a wheat image and some bboxes\n2. transform() operation\n3. DataLoader() operation","a2bab2fb":"show bboxes on the first image","9b9c38fb":"# 1. Show a wheat image and some bboxes","3192a0a7":"In this notebook, I present some basic functions of PyTorch for biginner like me. I have picked up these functions and checked their operations with elementary Python code.\n\n[PyTorch tutorial](https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html) and [ @pestipeti's notebook](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)(which is one of the most famous notebook in \"Global Wheat Detection\") provided useful reference points for my understanding. I have compared the tutorial with the notebook as shown below. Then, I present 2 fuctions operation, \"transform()\" and \"DataLoader()\".","7449d79d":"# 3. DataLoader() operation","c45202f9":"According to\u30101\u3011&\u30102\u3011, transform works in def __getitem__(). Esperially in \u30103\u3011, it needs arguments of \"img\", \"bboxes\" and \"labels\" . ","8cf2620e":"transform() code is here,","933377ef":"In DataLoader() function makes batch files. And after making the bach files of Wheat data, collate_fn() convert data to the torch.Tensor format. This function is defined external file;utils.py\u30105\u3011. Let's check the making batch file operation. Code is based on \u30104\u3011.\n\n\u30105\u3011[github.com pytorch\/vision\/blob\/master\/references\/detection\/utils.py](https:\/\/github.com\/pytorch\/vision\/blob\/master\/references\/detection\/utils.py)","a86ae086":"This bbox value is *COCO Bounding box* format.\nTo show bboxes with cv2.rectangle, we must convert the format (like a *Pascal VOC Bounding box* format). \n\n* \u3000COCO\u3000\uff1a\u3000(x-top left, y-top left, width, height)\n* \u3000Pascal VOC\u3000\uff1a\u3000(x-top left, y-top left,x-bottom right, y-bottom right)","91435e40":"# 2. transform() operation\n\n\"transform()\" function is an argument of preprocessing class in PyTorch\u30101\u3011. PyTorch tutorial use a typical transform() function; \"get_transform()\" from train.py\u30102\u3011. And Albumentations provide some image processing fucntions\u30103\u3011. So, we can combine these functions and make an original transform().\n\nLet's make an example of original transform() and confirm its operation. One of the most famous notebook in \"Global Wheat Detection\"\u30104\u3011is a good example. It has flip and tensor convertion function in transform().\n\n\n\u30101\u3011 [PyTorch Tutorials \"TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL\"](https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html)\n\n\u30102\u3011[github pytorch\/vision\/blob\/master\/references\/detection\/train.py](https:\/\/github.com\/pytorch\/vision\/blob\/master\/references\/detection\/train.py)\n\n\u30103\u3011[github albumentations\/albumentations\/augmentations\/transforms.py](https:\/\/github.com\/albumentations-team\/albumentations\/blob\/master\/albumentations\/augmentations\/transforms.py)\n\n\u30104\u3011[Pytorch Starter - FasterRCNN Train  @pestipeti](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)","17888f6d":"![Wheat_differences.jpg](attachment:Wheat_differences.jpg)","485db463":"len(image_ids) = 3373 \/ (3 batchs) \u21d2 1125","fd6fde22":"Then you can see a fliped image.\nThe function of \".Flip(p=1.0)\" has operated.","83f293b5":"show the first image; \"*image_id = b6ab77fd7*\""}}