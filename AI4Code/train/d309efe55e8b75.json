{"cell_type":{"1d14e734":"code","4d000e2a":"code","8fd1abe3":"code","375863a7":"code","5719e81b":"code","d0de0b3c":"code","d5261c40":"code","3542fa95":"code","4fde9676":"code","fa1cdfe2":"code","b41f54f7":"code","a6638c2b":"code","842169aa":"code","67c21c3e":"code","ea680b2c":"code","9dac2b45":"code","5728e030":"code","a5916b09":"code","acde1878":"code","f758c3cc":"code","b9b6b20c":"code","3f0aff7a":"code","e5fcb925":"code","b21e92d0":"code","f338fbac":"code","971dcf7c":"code","d7084949":"code","2cc760ef":"code","532295ff":"code","90593a67":"code","58d95757":"code","392d1043":"markdown","dcd5c222":"markdown","89ee3279":"markdown","19b84610":"markdown","c36c7bcd":"markdown","4697fbc2":"markdown","c72dc998":"markdown","32b41c87":"markdown","654fac4b":"markdown","1eb3f4af":"markdown","13c5c6ff":"markdown","1d185a97":"markdown","07590ade":"markdown","40aa72b6":"markdown","2a5bc851":"markdown","9546dc73":"markdown","7c60226d":"markdown","5b28bc59":"markdown","9c44b0cb":"markdown","74fe5784":"markdown","001fdbc4":"markdown","59a55e49":"markdown","8e6809cd":"markdown","1d6ba394":"markdown","88931ebd":"markdown","1dbbeb05":"markdown","b98f478d":"markdown","a9201697":"markdown","24d6bc75":"markdown"},"source":{"1d14e734":"import pandas as pd\nimport numpy as np","4d000e2a":"data = pd.read_csv(\"..\/input\/glass\/glass.csv\")\ndata.head()","8fd1abe3":"data.shape","375863a7":"data.isnull().sum()","5719e81b":"data.describe()","d0de0b3c":"names = ['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','glass_type']\ndata.columns = names\ndata.head()","d5261c40":"data.head(3)","3542fa95":"from scipy import stats\n\nz = abs(stats.zscore(data))\n\n#np.where(z > 3)\n\ndata = data[(z < 3).all(axis=1)]\n\n#data.shape","4fde9676":"features = ['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']\nlabel = ['glass_type']\n\nX = data[features]\n\ny = data[label]","fa1cdfe2":"X.shape","b41f54f7":"type(X)","a6638c2b":"x2 = X.values\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfor i in range(1,9):\n        sns.distplot(x2[i])\n        plt.xlabel(features[i])\n        plt.show()","842169aa":"x2 = pd.DataFrame(X)\n\nplt.figure(figsize=(8,8))\nsns.pairplot(data=x2)\nplt.show()","67c21c3e":"correlation= X.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(correlation,cbar=True,square=True,annot=True,fmt='.1f',annot_kws={'size': 15},\n            xticklabels=features,yticklabels=features,alpha=0.7,cmap= 'coolwarm')\nplt.show()","ea680b2c":"## Normalizing \/ Scaling the data  \n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n#scaler.fit(X)\n#X = scaler.transform(X)\n#X = pd.DataFrame(X)","9dac2b45":"X.head(2)","5728e030":"y.head(2)","a5916b09":"from sklearn import preprocessing\nX=preprocessing.scale(X)","acde1878":"x2 = X\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfor i in range(1,9):\n        sns.distplot(x2[i])\n        plt.xlabel(features[i])\n        plt.show()","f758c3cc":"from sklearn.model_selection import train_test_split","b9b6b20c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0,stratify=y)","3f0aff7a":"## Flattening the array\ny_train = y_train.values.ravel()\ny_test = y_test.values.ravel()","e5fcb925":"print('Shape of X_train = ' + str(X_train.shape))\nprint('Shape of X_test = ' + str(X_test.shape))\nprint('Shape of y_train = ' + str(y_train.shape))\nprint('Shape of y_test = ' + str(y_test.shape))","b21e92d0":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nScores = []\n\nfor i in range (2,11):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    score = knn.score(X_test,y_test)\n    Scores.append(score)\n\nprint(knn.score(X_train,y_train))\nprint(Scores)","f338fbac":"from sklearn.tree import DecisionTreeClassifier\n\nScores = []\n\nfor i in range(1):\n    tree = DecisionTreeClassifier(random_state=0)\n    tree.fit(X_train, y_train)\n    score = tree.score(X_test,y_test)\n    Scores.append(score)\n\nprint(tree.score(X_train,y_train))\nprint(Scores)","971dcf7c":"from sklearn.linear_model import LogisticRegression\n\nScores = []\n\nfor i in range(1):\n    logistic = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial',max_iter=100)\n    logistic.fit(X_train, y_train)\n    score = logistic.score(X_test,y_test)\n    Scores.append(score)\n    \nprint(logistic.score(X_train,y_train))\nprint(Scores)","d7084949":"from sklearn.svm import SVC\n\nScores = []\n\nfor i in range(1):\n    svc = SVC(gamma='auto')\n    svc.fit(X_train, y_train)\n    score = svc.score(X_test,y_test)\n    Scores.append(score)\n\nprint(svc.score(X_train,y_train))\nprint(Scores)","2cc760ef":"from sklearn.svm import LinearSVC\n\nScores = []\n\nfor i in range(1):\n    svc = LinearSVC(random_state=0)\n    svc.fit(X_train, y_train)\n    score = svc.score(X_test,y_test)\n    Scores.append(score)\n\nprint(svc.score(X_train,y_train))\nprint(Scores)","532295ff":"from sklearn.ensemble import RandomForestClassifier\n\nScores = []\nRange = [10,20,30,50,70,80,100,120]\n\nfor i in range(1):\n    forest = RandomForestClassifier(criterion='gini', n_estimators=10, min_samples_leaf=1, min_samples_split=4, random_state=1,n_jobs=-1)\n    #forest = RandomForestClassifier(n_estimators=i ,random_state=0)\n    forest.fit(X_train, y_train)\n    score = forest.score(X_test,y_test)\n    #Scores.append(score)\n\nprint(forest.score(X_train,y_train))\nprint(score)","90593a67":"from sklearn.neural_network import MLPClassifier\n\nScores = []\n\nfor i in range(1):\n    NN = MLPClassifier(random_state=0)\n    NN.fit(X_train, y_train)\n    score = NN.score(X_test,y_test)\n    Scores.append(score)\n\nprint(NN.score(X_train,y_train))\nprint(Scores)","58d95757":"from sklearn.ensemble import GradientBoostingClassifier\n\ngd = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n\ngd.fit(X_train, y_train)\nscore = gd.score(X_test,y_test)\n\nprint(gd.score(X_train,y_train))\nprint(score)","392d1043":"### 6.Random Forest","dcd5c222":"## Visualizing Data After Preprocessing","89ee3279":"## Importing Dataset","19b84610":"### Separating Features and Label","c36c7bcd":"# UCI Glass Detection Dataset\n\n## Comparison of Classification ALgorithms\n\n","4697fbc2":"## Above diagrams shows that our dataset is skewed either on positive side or negative side and that the data is not normalized.","c72dc998":"Our Diagram shows correlation between different features of the dataset.\nConclusion:\n- Rl and Ca have strong correlation between each other\n- Al and Ba have intermediate correlation between each other ","32b41c87":"### The Descriptive Statistics helps us observe that the above data across all attributes is not in the same range. Hence, normalization of Data is required.","654fac4b":"## Exploring Dataset\n\n1.Shape of dataset  \n2.Count of Null values  \n3.Uniques values  \n4.Statisitics of dataset","1eb3f4af":"#### Adding meaningful column\/attribute names","13c5c6ff":"## Above diagrams depict that after relevant data preprocessing, the skewness is reduced and data is more normalized.","1d185a97":"## Importing Libraries","07590ade":"## Checking Outliers With The Help Of Z-score ","40aa72b6":"#### Scalling the data  (1-0 range)","2a5bc851":"### Out of all the models tested above:\n\n1. Random forest gives the best result with:  \n\n    - training accuracy: 0.9793103448275862  \n    - test accuracy: 0.7755102040816326\n\n### But since it is overfitting,it suggests that the model might not perform well on unknown data. Hence, we shall choose the next best model that is:\n\n2. SVM (Non Linear Kernel)\n    \n    - training accuracy: 0.7586206896551724\n    - testing accuracy:  0.7551020408163265","9546dc73":"### 1.KNN","7c60226d":"# Summary","5b28bc59":"### 8.Gradient Decent Tree Boosting","9c44b0cb":"### 4.SVC Classifier (Non-Linear Kernal)","74fe5784":"### 2.Decision Tree","001fdbc4":"## Applying Different Machine learning Models","59a55e49":"### 5.SVC Classifier (Linear Kernel)","8e6809cd":"### 7.Neural Network","1d6ba394":"# Outline\n\n- Importing libraries\n- Importing Dataset\n- Exploring Dataset\n- Preparing Dataset\n- Removing Outliers\n- Visualization of dataset\n- Train\/Test Split\n- Applying Machine Learning Models\n- Summary\n\n## List Of Machine Learning Models That Have Been applied:\n\n- KNN\n- Logistic Regression\n- Decision Tree\n- SVM (Linear Kernal)\n- SVM (Non Linear Kernal)\n- Random Forest\n- Neural Network\n- Gradient Decent Tree Boosting","88931ebd":"## Train-Test Split","1dbbeb05":"## Data Visualization","b98f478d":"### 3.Logistic Regression","a9201697":"### Scaling The Features","24d6bc75":"## Preparing The Dataset"}}