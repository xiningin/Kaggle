{"cell_type":{"96385467":"code","a57977ee":"code","e8e714de":"code","b7164317":"code","060cb6e1":"code","2407018c":"code","3e1c908b":"code","eb9f1ad5":"code","5afee3f6":"code","6321e7e5":"code","be49b91b":"code","7edd404e":"code","428e9b20":"code","960ae2f2":"code","01e102cf":"code","a953368a":"code","f099dc07":"code","cbdc00cf":"code","09563f6b":"code","7255f4c5":"code","2849c0b2":"code","beb011a7":"code","7ce58dd1":"code","83154c24":"code","68cfa828":"code","31dfa673":"code","03803077":"code","42ce35a0":"code","7be1a79b":"code","a54e2e4b":"code","45967ab9":"code","f8644006":"code","cd2a7f5f":"code","5f1be7bd":"markdown","ebda78e4":"markdown","f6ec1aab":"markdown","967d1fd4":"markdown","6ed31a4d":"markdown"},"source":{"96385467":"import torch.nn as nn\nimport torchvision.models as models\nimport torch","a57977ee":"new_model = models.inception_v3(pretrained=True, aux_logits=False)\nnew_model.fc","e8e714de":"class encoderCNN(nn.Module):\n    def __init__(self, embed_size, should_train=False):\n        super(encoderCNN, self).__init__()\n        self.should_train = should_train\n        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n        self.dropout= nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        features = self.inception(x)\n        \n        for name, param in self.inception.named_parameters():\n            param.requires_grad = False\n        \n#         for name, param in self.inception.named_parameters():\n#             if \"fc.weight\" in name or \"fc.bias\" in name:\n#                 param.requires_grad = True\n#             else:\n#                 param.required_grad = self.should_train\n        \n        \n        return self.dropout(self.relu(features))\n        ","b7164317":"class decoderRNN(nn.Module):\n    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n        super(decoderRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, features, caption):\n        embeddings = self.dropout(self.embedding(caption))\n        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n        hiddens, _ = self.lstm(embeddings)\n        outputs = self.linear(hiddens)\n        return outputs","060cb6e1":"class CNN2RNN(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(CNN2RNN, self).__init__()\n        self.encoderCNN = encoderCNN(embed_size)\n        self.decoderRNN = decoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n    \n    def forward(self, images, caption):\n        x = self.encoderCNN(images)\n        x = self.decoderRNN(x, caption)\n        return x\n    \n    def captionImage(self, image, vocabulary, maxlength=50):\n        result_caption = []\n        \n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n            \n            for _ in range(maxlength):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                print(predicted.shape)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embedding(output).unsqueeze(0)\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n        return [vocabulary.itos[i] for i in result_caption]","2407018c":"# class EncoderCNN(nn.Module):\n#     def __init__(self, embed_size, train_CNN=False):\n#         super(EncoderCNN, self).__init__()\n#         self.train_CNN = train_CNN\n#         self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n#         self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n#         self.relu = nn.ReLU()\n#         self.times = []\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, images):\n#         features = self.inception(images)\n#         return self.dropout(self.relu(features))","3e1c908b":"# class DecoderRNN(nn.Module):\n#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n#         super(DecoderRNN, self).__init__()\n#         self.embed = nn.Embedding(vocab_size, embed_size)\n#         self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n#         self.linear = nn.Linear(hidden_size, vocab_size)\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, features, captions):\n#         embeddings = self.dropout(self.embed(captions))\n#         embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n#         hiddens, _ = self.lstm(embeddings)\n#         outputs = self.linear(hiddens)\n#         return outputs\n\n\n","eb9f1ad5":"# class CNNtoRNN(nn.Module):\n#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n#         super(CNNtoRNN, self).__init__()\n#         self.encoderCNN = EncoderCNN(embed_size)\n#         self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n\n#     def forward(self, images, captions):\n#         features = self.encoderCNN(images)\n#         outputs = self.decoderRNN(features, captions)\n#         return outputs\n\n#     def caption_image(self, image, vocabulary, max_length=50):\n#         result_caption = []\n\n#         with torch.no_grad():\n#             x = self.encoderCNN(image).unsqueeze(0)\n#             states = None\n\n#             for _ in range(max_length):\n#                 hiddens, states = se\/lf.decoderRNN.lstm(x, states)\n#                 output = self.decoderRNN.linear(hiddens.squeeze(0))\n#                 predicted = output.argmax(1)\n#                 result_caption.append(predicted.item())\n#                 x = self.decoderRNN.embed(predicted).unsqueeze(0)\n\n#                 if vocabulary.itos[predicted.item()] == \"<EOS>\":\n#                     break\n\n#         return [vocabulary.itos[idx] for idx in result_caption]","5afee3f6":"import os\nimport pandas\nimport spacy\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom PIL import Image\nfrom torchvision.transforms import transforms","6321e7e5":"spacy_eng = spacy.load(\"en\")","be49b91b":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        \n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        \n        self.freq_threshold = freq_threshold\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n    \n    def build_vocabulary(self,sentences):\n        idx = 4\n        frequency = {}\n        \n        for sentence in sentences:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequency:\n                    frequency[word] = 1\n                else:\n                    frequency[word] += 1\n                \n                if (frequency[word] > self.freq_threshold-1):\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    idx += 1\n    \n    def numericalize(self,sentence):\n        tokenized_text = self.tokenizer_eng(sentence)\n        \n        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tokenized_text ]\n                    \n        ","7edd404e":"annotation = pandas.read_csv(\"..\/input\/flickr8kimagescaptions\/flickr8k\/captions.txt\")\nannotation.head()","428e9b20":"annotation['caption'].tolist()[:2]","960ae2f2":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir=\"..\/input\/flickr8kimagescaptions\/flickr8k\/images\", caption_path=\"..\/input\/flickr8kimagescaptions\/flickr8k\/captions.txt\", freq_threshold=5, transform=None, data_length=10000):\n        self.freq_threshold = freq_threshold\n        self.transform = transform\n        self.root_dir = root_dir\n    \n        self.df = pandas.read_csv(caption_path)[:data_length]\n        \n        self.captions = self.df['caption']\n        self.images = self.df['image']\n        \n        self.vocab = Vocabulary(freq_threshold)\n        \n        print(len(self.captions.tolist()))\n        self.vocab.build_vocabulary(self.captions.tolist())\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        caption = self.captions[index]\n        image = self.images[index]\n        \n        img = Image.open(os.path.join(self.root_dir,image)).convert(\"RGB\")\n        \n        if (self.transform):\n            img = self.transform(img)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        \n        numericalized_caption += self.vocab.numericalize(caption)\n        \n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        return img, torch.tensor(numericalized_caption)","01e102cf":"class MyCollate:\n    def __init__(self, pad_value):\n        self.pad_value = pad_value\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        img = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_value)\n        \n        return img, targets","a953368a":"# transform = transforms.Compose(\n#         [transforms.Resize((224, 224)), transforms.ToTensor(),]\n#     )\n\ntransform = transforms.Compose(\n        [\n            transforms.Resize((356, 356)),\n            transforms.RandomCrop((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )","f099dc07":"def get_loader(root_dir=\"..\/input\/flickr8kimagescaptions\/flickr8k\/images\", caption_path=\"..\/input\/flickr8kimagescaptions\/flickr8k\/captions.txt\", transform=transform, batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, transform=transform)\n    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n    \n    loader = DataLoader(dataset=dataset, batch_size=32, num_workers=8, shuffle=True, pin_memory=True, collate_fn=MyCollate(pad_value))\n    \n    return loader, dataset","cbdc00cf":" loader, dataset = get_loader()","09563f6b":"import random\nimport math\n\nx, y = dataset[math.floor(random.random() * len(dataset))]\nx.shape, y.shape","7255f4c5":"import matplotlib.pyplot as plt\n\nplt.imshow(x.permute(1,2,0))\nprint(y)\n\n# print(dataset.vocab.itos[1])\n\nfor i in y:\n    print(dataset.vocab.itos[int(i)],end=\" \")","2849c0b2":"def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n    print(\"saving checkpoint!\")\n    torch.save(state, filename)","beb011a7":"def load_checkpoint(checkpoint, model, optimizer):\n    print(\"loading checkpoint!\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    model.load_state_optimizer(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","7ce58dd1":"from tqdm import tqdm\n# from torchvision.utils.tensorboard import SummaryWriter","83154c24":"torch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nload_model = False\nsave_model=False\ntrain_CNN = False","68cfa828":"# model = CNN2RNN(embed_size=embed_size, hidden_size=hidden_size,vocab_size=vocab_size, num_layers=num_layers).to(device=device)","31dfa673":"import torch.optim as optim \n\nstep = 0\nembed_size = 256\nhidden_size = 256\nnum_layers = 5\nnum_epochs = 5\nlearning_rate = 3e-4\nvocab_size = len(dataset.vocab)\n","03803077":"model = CNN2RNN(embed_size=embed_size, hidden_size=hidden_size,vocab_size=vocab_size, num_layers=num_layers).to(device=device)","42ce35a0":"model.decoderRNN","7be1a79b":"loss_criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","a54e2e4b":"# # Only finetune the CNN\n# for name, param in model.encoderCNN.inception.named_parameters():\n#     if \"fc.weight\" in name or \"fc.bias\" in name:\n#         param.requires_grad = True\n#     else:\n#         param.requires_grad = train_CNN","45967ab9":"if load_model:\n    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)","f8644006":"model.train()\n\nfor epoch in range(num_epochs):\n    if save_model:\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": model.state_dict(),\n            \"step\": step,\n        }\n        save_checkpoint(checkpoint)\n\n#     for idx, (imgs, captions) in tqdm(\n#         enumerate(loader), total=len(loader), leave=False\n#     ):\n    for idx, (imgs, captions) in enumerate(loader):\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        \n        score = model(imgs, captions[:-1])\n        \n#         print(score.shape, captions.shape)\n#         print(score.reshape(-1, score.shape[2]).shape, captions.reshape(-1).shape)\n#         print(\"why are we reshaping it here?\")\n        optimizer.zero_grad()\n        loss = loss_criterion(score.reshape(-1, score.shape[2]), captions.reshape(-1))\n        \n       \n        step += 1\n        \n        loss.backward()\n        optimizer.step()\n    print(f\"Loss for epoch {epoch}: {loss}\")","cd2a7f5f":"image_path = \"..\/input\/flickr8kimagescaptions\/flickr8k\/images\/1032460886_4a598ed535.jpg\"\n\nimg = Image.open(image_path)\n\nimg = transform(img)\n\nplt.imshow(img.permute(1,2,0))\n\nimage_input = img.to(device=device) # check here\n\nprint(model.captionImage(image=image_input, vocabulary=dataset.vocab))\n\n\n","5f1be7bd":"## Training the model","ebda78e4":"Lets load a random example.","f6ec1aab":"Adapted from this wonderful tutorial:\n\nhttps:\/\/www.youtube.com\/watch?v=y2BaTt1fxJU&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=21","967d1fd4":"## Getting the dataset","6ed31a4d":"### Hyperparameters"}}