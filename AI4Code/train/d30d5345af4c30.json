{"cell_type":{"c387ce39":"code","1fe34610":"code","55458c60":"code","3fed5c2c":"code","73ea7225":"code","ffcf3ad5":"code","b23b8bae":"code","93fd5370":"code","99f9e124":"code","af5c8609":"code","ae4f153f":"markdown","a986b845":"markdown","5676f89d":"markdown","040f946c":"markdown","9aa653d8":"markdown","7833aa9d":"markdown","8a329420":"markdown","945c41b1":"markdown","9d3c9849":"markdown","040a1d18":"markdown","006a5400":"markdown","86ba8f1d":"markdown","34fe0066":"markdown","86bf48fd":"markdown","a7bb7626":"markdown","6e11c03c":"markdown"},"source":{"c387ce39":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"\/kaggle\/input\/arketing-campaign\/\"","1fe34610":"data=pd.read_csv(data_folder+'marketing_campaign.csv',header=0,sep=';') \n#Spending variable creation\ndata['Age']=2014-data['Year_Birth']\n\ndata['Spending']=data['MntWines']+data['MntFruits']+data['MntMeatProducts']+data['MntFishProducts']+data['MntSweetProducts']+data['MntGoldProds']\n#Seniority variable creation\nlast_date = date(2014,10, 4)\ndata['Seniority']=pd.to_datetime(data['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')\ndata['Seniority'] = pd.to_numeric(data['Seniority'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')\/30\ndata=data.rename(columns={'NumWebPurchases': \"Web\",'NumCatalogPurchases':'Catalog','NumStorePurchases':'Store'})\ndata['Marital_Status']=data['Marital_Status'].replace({'Divorced':'Alone','Single':'Alone','Married':'In couple','Together':'In couple','Absurd':'Alone','Widow':'Alone','YOLO':'Alone'})\ndata['Education']=data['Education'].replace({'Basic':'Undergraduate','2n Cycle':'Undergraduate','Graduation':'Postgraduate','Master':'Postgraduate','PhD':'Postgraduate'})\n\ndata['Children']=data['Kidhome']+data['Teenhome']\ndata['Has_child'] = np.where(data.Children> 0, 'Has child', 'No child')\ndata['Children'].replace({3: \"3 children\",2:'2 children',1:'1 child',0:\"No child\"},inplace=True)\ndata=data.rename(columns={'MntWines': \"Wines\",'MntFruits':'Fruits','MntMeatProducts':'Meat','MntFishProducts':'Fish','MntSweetProducts':'Sweets','MntGoldProds':'Gold'})\n\n\ndata=data[['Age','Education','Marital_Status','Income','Spending','Seniority','Has_child','Children','Wines','Fruits','Meat','Fish','Sweets','Gold']]\ndata","55458c60":"#Remove rows with missing values\ndata=data.dropna(subset=['Income'])\n#Remove the only outlier in the dataset\ndata=data[data['Income']<600000]","3fed5c2c":"#Normalize data before clustering\nscaler=StandardScaler()\ndataset_temp=data[['Income','Seniority','Spending']]\nX_std=scaler.fit_transform(dataset_temp)\nX = normalize(X_std,norm='l2')\n\n#fit the algorithm\ngmm=GaussianMixture(n_components=4, covariance_type='spherical',max_iter=2000, random_state=5).fit(X)\n#predict clusters\nlabels = gmm.predict(X)\ndataset_temp['Cluster'] = labels\ndataset_temp=dataset_temp.replace({0:'Stars',1:'Need attention',2:'High potential',3:'Leaky bucket'})\ndata = data.merge(dataset_temp.Cluster, left_index=True, right_index=True)","73ea7225":"pd.options.display.float_format = \"{:.0f}\".format\nsummary=data[['Income','Spending','Seniority','Cluster']]\nsummary.set_index(\"Cluster\", inplace = True)\nsummary=summary.groupby('Cluster').describe().transpose()\nsummary","ffcf3ad5":"PLOT = go.Figure()\nfor C in list(data.Cluster.unique()):\n    \n\n    PLOT.add_trace(go.Scatter3d(x = data[data.Cluster == C]['Income'],\n                                y = data[data.Cluster == C]['Seniority'],\n                                z = data[data.Cluster == C]['Spending'],                        \n                                mode = 'markers',marker_size = 6, marker_line_width = 1,\n                                name = str(C)))\nPLOT.update_traces(hovertemplate='Income: %{x} <br>Seniority: %{y} <br>Spending: %{z}')\n\n    \nPLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n                   scene = dict(xaxis=dict(title = 'Income', titlefont_color = 'black'),\n                                yaxis=dict(title = 'Seniority', titlefont_color = 'black'),\n                                zaxis=dict(title = 'Spending', titlefont_color = 'black')),\n                   font = dict(family = \"Gilroy\", color  = 'black', size = 12))","b23b8bae":"#Create Age segment\ncut_labels_Age = ['Young', 'Adult', 'Mature', 'Senior']\ncut_bins = [0, 30, 45, 65, 120]\ndata['Age_group'] = pd.cut(data['Age'], bins=cut_bins, labels=cut_labels_Age)\n#Create Income segment\ncut_labels_Income = ['Low income', 'Low to medium income', 'Medium to high income', 'High income']\ndata['Income_group'] = pd.qcut(data['Income'], q=4, labels=cut_labels_Income)\n#Create Seniority segment\ncut_labels_Seniority = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\ndata['Seniority_group'] = pd.qcut(data['Seniority'], q=4, labels=cut_labels_Seniority)\n\ndata=data.drop(columns=['Age','Income','Seniority'])\ndata","93fd5370":"cut_labels = ['Low consumer', 'Frequent consumer', 'Biggest consumer']\ndata['Wines_segment'] = pd.qcut(data['Wines'][data['Wines']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Fruits_segment'] = pd.qcut(data['Fruits'][data['Fruits']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Meat_segment'] = pd.qcut(data['Meat'][data['Meat']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Fish_segment'] = pd.qcut(data['Fish'][data['Fish']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Sweets_segment'] = pd.qcut(data['Sweets'][data['Sweets']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Gold_segment'] = pd.qcut(data['Gold'][data['Gold']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n\ndata.replace(np.nan, \"Non consumer\",inplace=True)\n\ndata.drop(columns=['Spending','Wines','Fruits','Meat','Fish','Sweets','Gold'],inplace=True)\ndata = data.astype(object)\ndata","99f9e124":"#Display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', 999)\npd.options.display.float_format = \"{:.3f}\".format\n\nassociation=data.copy() \ndf = pd.get_dummies(association)\n\n#Apriori min support\nmin_support = 0.08\n\n#Max lenght of apriori n-grams\nmax_len = 10\n\nfrequent_items = apriori(df, use_colnames=True, min_support=min_support, max_len=max_len + 1)\nrules = association_rules(frequent_items, metric='lift', min_threshold=1)","af5c8609":"# We select the product and the segment we want to analyze\nproduct='Wines'\nsegment='Biggest consumer'\ntarget = '{\\'%s_segment_%s\\'}' %(product,segment)\n\nresults_personnal_care = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\n\nresults_personnal_care.head(5)","ae4f153f":"To define customer personnas, I will create several variables :\n\n>- Variable __*Age*__ in replacement of the variable *Year_birth*\n>- Variable __*Spending*__ as the sum of the amount spent on the 6 product categories\n>- Variable __*Seniority*__ as the number of months the customer is enrolled with the company\n>- Variable __*Marital_Status*__ to group the different marital status in only 2 comprehensive categories : In couple vs Alone\n>- Variable __*Education*__ as either Undergraduate or Postgraduate\n>- Variable __*Children*__ as the total numberr of children at home\n>- Variable __*Has_child*__ as a binary variable equal to Yes if the customers has 1 child or more\n\nWe will remove the unused variables for this analysis","a986b845":"### A. Feature Engineering <a class=\"anchor\" id=\"section_1_1\"><\/a>","5676f89d":"### Table of Contents\n\n* [Data Preprocessing](#section_1)\n    * [Feature Engineering](#section_1_1)   \n    * [Outliers and missing values treatment](#section_1_2)\n    * [Customer clustering](#section_1_3)\n    ___\n* [Apriori Algorithm](#section_2)\n    * [Association Rules generation](#section_2_1)\n    * [Customer peronnas validation](#section_2_2)\n    ---\nLink to my previous Notebook on same dataset : <br>\nExploratory Data Analysis: https:\/\/www.kaggle.com\/raphael2711\/data-prep-visual-eda-and-statistical-hypothesis <br>\nCustomer segmentation : https:\/\/www.kaggle.com\/raphael2711\/customer-segmentation-with-gmm-clustering","040f946c":"We bin data before running Apriori algorithm ","9aa653d8":"### C. Customer clustering <a class=\"anchor\" id=\"section_1_3\"><\/a>","7833aa9d":"We are ready to start running Apriori algorithm. We will look for the profile of Wines biggest consumer","8a329420":"We define customer segments for each product based on their spending :\n- **Non consumer :** Customers with 0 amount of spending\n- **Low consumer :** Customers below the 1st quartile\n- **Frequent consumer :** Customers between the 1st and 3rd quartile\n- **Biggest consumer :** Customers above the 3rd quartile","945c41b1":"# 2. Apriori algorithm <a class=\"anchor\" id=\"section_2\"><\/a>","9d3c9849":"### A. Association Rules generation <a class=\"anchor\" id=\"section_2_1\"><\/a>","040a1d18":"I explained in my previous notebook how I clustered the customers based on their Income, Spending level and Seniority in the company :\nhttps:\/\/www.kaggle.com\/raphael2711\/customer-segmentation-with-gmm-clustering\n\nWe identified 4 equally weighted clusters :\n- __Stars__ is composed of __old customers__ with __high income__ and __high spending amount__<br>\n- __Need attention__ is composed of __new customers__ with __below average income__ and __small spending amount__<br>\n- __High potential__ is composed of __new customers__ with __high income__ and __high spending amount__<br>\n- __Leaky bucket__ is composed of __old customers__ with __below average income__  and __small spending amount__<br>","006a5400":"# 1. Data Preprocessing <a class=\"anchor\" id=\"section_1\"><\/a>","86ba8f1d":"### B. Customer personnas validation <a class=\"anchor\" id=\"section_2_2\"><\/a>","34fe0066":"### B. Outliers and missing values treatment <a class=\"anchor\" id=\"section_1_2\"><\/a>","86bf48fd":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Customer Personna<\/center><\/h1>","a7bb7626":"I covered in my previous notebook the outliers and missing values treatment : <br>\nhttps:\/\/www.kaggle.com\/raphael2711\/data-prep-visual-eda-and-statistical-hypothesis\n\nThere are 24 missing values and 1 outlier for the *Income* variable. We simply remove these observations for this analysis","6e11c03c":"<span style=\"font-size: 200%;color:#6d071a;font-weight:bold\">Customer profile of Wines best customers<\/span> <br>\nOur algorithm generated several rules from which we can clearly identify a profile of wines best customers.<br> Rule id 9440 tells us it is :\n- A customer with an __average income of 69500 dollars__\n- With an __average total spending of 1252 dollars__\n- Enrolled with the company for __21 months__\n- Owning a __Postgraduate diploma__\n- Who is also a __big consumer of Meat products__"}}