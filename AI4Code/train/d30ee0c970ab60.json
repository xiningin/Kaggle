{"cell_type":{"57de2997":"code","744c6ea4":"code","3abc5e9c":"code","bc6ac045":"code","ed8b5f83":"code","9847b736":"code","888e76ff":"code","35f886c6":"code","facd6aa6":"code","0e0e42c0":"code","50f51784":"code","3756d900":"code","9d94e4e4":"code","ee4603c3":"code","35b30353":"code","abcb1157":"code","d0fe1300":"code","25012108":"code","f0d18b5a":"code","c6176f28":"code","a2829792":"code","4acc9b41":"code","6fb3036c":"code","d582239e":"code","d636b663":"code","e1b143b9":"code","68715e5f":"code","52fa61b3":"code","bcb062e6":"code","f2d675a3":"code","cb6d88ac":"code","50cd0fe6":"code","9ae3856b":"code","fb9e0049":"code","6f762164":"code","28ce74da":"code","d73a7eb3":"code","b615da51":"code","002d4225":"code","754684d6":"code","cb21a81e":"code","73093477":"code","85a1cae2":"code","9e06d5df":"code","2ddee109":"code","88df36e5":"code","ae4459ed":"code","5c937cbd":"code","2308c55a":"code","d540e49c":"code","c0917473":"code","4a4cbb85":"code","3616b239":"code","bb975069":"code","688701b8":"code","c7d3d70d":"code","9c4e23cc":"code","1279a7be":"code","4bbcf64a":"code","e8ea8350":"code","303a34ce":"code","c9499faf":"code","2b040f95":"code","2b6f7778":"code","05ec22f1":"code","2b3d7273":"code","17d13750":"code","6b1245e6":"code","6dbeda34":"code","be83ef71":"code","ac72318b":"code","5ad81bc6":"code","925b12a6":"code","c09ebc73":"code","a467e5a6":"code","afe98672":"markdown","252b4219":"markdown","66a8edb9":"markdown","fcf7cea1":"markdown","97bcf99d":"markdown","d303aa2c":"markdown","6e6efde8":"markdown","2da94e20":"markdown","060a0298":"markdown","7d02b0da":"markdown","0f83c613":"markdown","31c52745":"markdown","d42cd115":"markdown","0d10090b":"markdown","ac8d7609":"markdown","98f6c816":"markdown","07be18ca":"markdown","0aad0f07":"markdown","da1d7dc7":"markdown","3c0c4ddd":"markdown","a8ed8726":"markdown","8a3ebac2":"markdown","84f760cd":"markdown","4c97d54a":"markdown","3d022026":"markdown","e37a63e4":"markdown","98fc5c95":"markdown","883563a9":"markdown","83890db0":"markdown","4db0d0f3":"markdown","ee528612":"markdown","0996d587":"markdown","0ee4c3b0":"markdown","128f8047":"markdown","9ecf5e11":"markdown","c13f827b":"markdown","189e5375":"markdown","0b880c8d":"markdown","4a49b11f":"markdown","c25f5e6c":"markdown","498eb818":"markdown","e6d8ea0a":"markdown","236fb714":"markdown","22e4f4a4":"markdown","1ccd34eb":"markdown","23893877":"markdown"},"source":{"57de2997":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","744c6ea4":"!pip install dython","3abc5e9c":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom dython import nominal\nfrom collections import Counter\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import EasyEnsembleClassifier\nimport xgboost\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\n\n\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedStratifiedKFold","bc6ac045":"#Creating Train and Test Dataframe\ndf_train = pd.read_csv('\/kaggle\/input\/employee-attrition\/employee_attrition_train.csv')\ndf_test =  pd.read_csv('\/kaggle\/input\/employee-attrition\/employee_attrition_test.csv')","ed8b5f83":"print(df_train.shape)\nprint(df_test.shape)","9847b736":"df_train.head()","888e76ff":"df_train.info()","35f886c6":"#Basic Statistics of the columns\ndf_train.describe().T","facd6aa6":"#Checking Number of Unique Values in Each Feature\nprint('Total Number of Unique Values in Columns:')\nfor features in df_train:\n  if(features != 'Attrition'):\n    print(str(features)+ ': (Datatype: ' + str(df_train[features].dtype) +') : ' + str(len(df_train[features].unique())))","0e0e42c0":"drop_cols = ['EmployeeCount','EmployeeNumber','Over18','StandardHours']\ndf_train_col_list = list(set(df_train.columns) - set(drop_cols))","50f51784":"#Categorizing Discrete and Continuous Features w.r.t. column datatype (Discrete=>'Object' and Continuous=>'Integer and Float')\ndiscrete_features = [feature for feature in df_train_col_list if df_train[feature].dtype == 'O']\ncontinuous_features = list(set(df_train[df_train_col_list].columns) - set(discrete_features))","3756d900":"#Checking the unique value counts of some Continuous Features\ncont_col_list = ['Education','EnvironmentSatisfaction','JobInvolvement','JobLevel','JobSatisfaction','PerformanceRating','RelationshipSatisfaction','StockOptionLevel','TrainingTimesLastYear','WorkLifeBalance']\nfor col in cont_col_list:\n    print(df_train[col].value_counts())","9d94e4e4":"new_features = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance']\ndiscrete_features.extend(new_features)\ncontinuous_features = [e for e in continuous_features if e not in new_features]\n\nprint(discrete_features)\nprint(continuous_features)","ee4603c3":"print(\"List of Missing Values: \")\nprint(df_train[df_train_col_list].isnull().sum())","35b30353":"# Missing values in Rows\ndf_train[df_train_col_list].isnull().sum(axis=1).value_counts()","abcb1157":"missing_cols_list = ['Age', 'DailyRate', 'DistanceFromHome','MaritalStatus','BusinessTravel']\nsns.heatmap(df_train[missing_cols_list].isnull(), yticklabels=False, cbar=False, cmap='viridis')","d0fe1300":"msno.heatmap(df_train[missing_cols_list])","25012108":"msno.dendrogram(df_train[df_train_col_list])","f0d18b5a":"# Target Variable\nplt.figure(figsize = (8, 5))\nsns.countplot(x = 'Attrition', data = df_train)","c6176f28":"#Bivariate analysis of Categorical Columns\nfor feature in discrete_features:\n  if(feature != 'Attrition'):\n    plt.figure(figsize = (8, 4))\n    sns.countplot(x = feature, hue = 'Attrition', data = df_train, order = df_train[feature].value_counts().index)\n    plt.title(feature)\n    plt.xticks(rotation = 45)","a2829792":"# Univariate Analysis of Continuous Features\ncol_list = ['DistanceFromHome']\nfor col in col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,6))\n  df=df_train[df_train_col_list].copy()\n  sns.distplot(df[col].dropna(), ax = ax[0]);\n  sns.boxplot(col, data = df[col_list], ax = ax[1], orient = 'v')\n  #sns.swarmplot(col, data = df[col_list], ax = ax[2], orient = 'v')","4acc9b41":"# Univariate Analysis of Continuous Features\ncol_list = ['YearsAtCompany', 'TotalWorkingYears', 'MonthlyIncome']\nfor col in col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,6))\n  df=df_train[df_train_col_list].copy()\n  sns.distplot(df[col].dropna(), ax = ax[0]);\n  sns.boxplot(col, data = df[col_list], ax = ax[1], orient = 'v')\n  #sns.swarmplot(col, data = df[col_list].dropna(), ax = ax[2], orient = 'v')","6fb3036c":"# Univariate Analysis of Continuous Features\ncol_list = ['PercentSalaryHike', 'YearsWithCurrManager']\nfor col in col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,6))\n  df=df_train.copy()\n  sns.distplot(df[col].dropna(), ax = ax[0]);\n  sns.boxplot(col, data = df[col_list], ax = ax[1], orient = 'v')\n  #sns.swarmplot(col, data = df[col_list], ax = ax[2], orient = 'v')","d582239e":"# Univariate Analysis of Continuous Features\ncol_list = ['HourlyRate', 'MonthlyRate', 'DailyRate', 'Age']\nfor col in col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,6))\n  df=df_train[df_train_col_list].copy()\n  sns.distplot(df[col].dropna(), ax = ax[0]);\n  sns.boxplot(col, data = df[col_list], ax = ax[1], orient = 'v')\n  #sns.swarmplot(col, data = df[col_list], ax = ax[2], orient = 'v')","d636b663":"# Univariate Analysis of Continuous Features\ncol_list = ['NumCompaniesWorked', 'YearsSinceLastPromotion']\nfor col in col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,6))\n  df=df_train[df_train_col_list].copy()\n  sns.distplot(df[col].dropna(), ax = ax[0]);\n  sns.boxplot(col, data = df[col_list], ax = ax[1], orient = 'v')\n  #sns.swarmplot(col, data = df[col_list], ax = ax[2], orient = 'v')","e1b143b9":"#Encoding Target Variable i.e. Attrition to include the feature in the correlation matrix.\ndf=df_train[df_train_col_list].copy()\nle_inc = LabelEncoder()\nle_inc.fit(df['Attrition'])\ndf['Encoded_Attrition'] = le_inc.transform(df['Attrition'])\ndf.head()","68715e5f":"# Pearson Correlation Matrix\ncorr_continuous_features = continuous_features.copy()\ncorr_continuous_features.append('Encoded_Attrition')\nplt.figure(figsize = (15, 15))\ncorr_mat = df[corr_continuous_features].corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True, cmap='BuGn')","52fa61b3":"# Slightly Correlated Features\ns = corr_mat.unstack()\nso = s.sort_values(kind=\"quicksort\").drop_duplicates()\n#Slightly Correlated\nres1 = so[so>=0.1]\nres1 = res1[res1<0.3]\nprint(res1)","bcb062e6":"# o.4 to 0.6 is moderately correlated features\nres2 = so[so>=0.4]\nres2 = res2[res2<0.6]\nprint(res2)","f2d675a3":"# Above 0.6 is highly correlated features\nres3 = so[so>=0.6]\nprint(res3)","cb6d88ac":"s = corr_mat.unstack()\nso = s.sort_values(kind=\"quicksort\").drop_duplicates()\nmostly_corr_features = ['MonthlyIncome','YearsInCurrentRole','YearsAtCompany','TotalWorkingYears','YearsWithCurrManager','Age','YearsSinceLastPromotion']\n# Function to calculate correlation coefficient between two arrays\ndef corr(x, y, **kwargs):\n    \n    # Calculate the value\n    coef = np.corrcoef(x, y)[0][1]\n    # Make the label\n    label = r'$\\rho$ = ' + str(round(coef, 2))\n    \n    # Add the label to the plot\n    ax = plt.gca()\n    ax.annotate(label, xy = (0.2, 0.95), size = 20, xycoords = ax.transAxes)\n    \n# Create a pair grid instance\ngrid = sns.PairGrid(data= df_train,\n                    vars = mostly_corr_features, size = 4)\n\n# Map the plots to the locations\ngrid = grid.map_upper(plt.scatter, color = 'darkred')\ngrid = grid.map_upper(corr)\ngrid = grid.map_lower(sns.kdeplot, cmap = 'Reds')\ngrid = grid.map_diag(plt.hist, bins = 10, edgecolor =  'k', color = 'darkred');","50cd0fe6":"df_new = df.drop('Encoded_Attrition', axis=1)\nnominal.associations(df_new, nominal_columns=['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime'], theil_u=True, figsize=(20, 20))","9ae3856b":"ohe_cols = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\nle_cols = ['Gender', 'OverTime']\ndf_new['Encoded_Gender'] = le_inc.fit_transform(df_new['Gender'])\ndf_new['Encoded_OverTime'] = le_inc.fit_transform(df_new['OverTime'])","fb9e0049":"for col in ohe_cols:\n    df_new = pd.concat([df_new, pd.get_dummies(df_new[col], prefix=col)], axis=1)","6f762164":"cols_drop = ['Attrition','BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', 'Gender', 'OverTime']\nfor col in cols_drop:\n  del df_new[col]","28ce74da":"plt.figure(figsize = (50, 50))\ncorr_mat = df_new.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True, cmap='RdYlGn')","d73a7eb3":"s = corr_mat.unstack()\nso = s.sort_values(kind=\"quicksort\").drop_duplicates()","b615da51":"#Slightly Correlated\nres1 = so[so>=0.1]\nres1 = res1[res1<0.3]\nprint(res1)","002d4225":"#o.4 to 0.6 is moderately correlated\nres2 = so[so>=0.4]\nres2 = res2[res2<0.6]\nprint(res2)","754684d6":"# Above 0.6 is highly correlated\nres3 = so[so>=0.6]\nprint(res3)","cb21a81e":"df_train_pp = df_train.copy()\ndf_test_pp =  df_test.copy()","73093477":"#creating subset of data where there are no missing values in Age and TotalWorkingYears\ndf_age_totalworking = df_train_pp.dropna(axis=0, subset=['Age','TotalWorkingYears'])\ndf_age_totalworking = df_age_totalworking.loc[:,['Age','TotalWorkingYears']]","85a1cae2":"#Finding the entries with Age Missing\nmissing_age = df_train_pp['Age'].isnull()\n#extract the TotalWorkingYears observations with Age missing\ntotworking_missage = pd.DataFrame(df_train_pp['TotalWorkingYears'][missing_age])","9e06d5df":"X = pd.DataFrame(df_age_totalworking['TotalWorkingYears'])\ny = pd.DataFrame(df_age_totalworking['Age'])\n\n#Train and Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Train using linear regression model\n#fit a linear model\nlm = LinearRegression().fit(X_train, y_train)\n\n#using fitted model to find Age missing values\n#miss_age_pred = lm.predict(totworking_missage)\ndf_train_pp.loc[df_train_pp['Age'].isnull(),'Age'] = lm.predict(lm.predict(totworking_missage))","2ddee109":"cols_list = ['Age']\nfig = plt.figure(figsize=(10,5))\n#  subplot #1\nfig.add_subplot(121)\nplt.title('Age Distribution Before Imputation', fontsize=14)\nsns.distplot(df_train[cols_list]);\nplt.xticks(rotation = 45)\n\nfig.add_subplot(122)\nplt.title('Age Distribution After Imputation', fontsize=14)\nsns.distplot(df_train_pp[cols_list]);\nplt.xticks(rotation = 45)","88df36e5":"#MEAN IMPUTATION\ndf_train_pp['DailyRate'] = df_train_pp['DailyRate'].fillna(df_train_pp['DailyRate'].mean())","ae4459ed":"cols_list = ['DailyRate']\nfig = plt.figure(figsize=(10,5))\n#  subplot #1\nfig.add_subplot(121)\nplt.title('DailyRate Distribution Before Imputation', fontsize=14)\nsns.distplot(df_train[cols_list]);\nplt.xticks(rotation = 45)\n\nfig.add_subplot(122)\nplt.title('DailyRate Distribution After Imputation', fontsize=14)\nsns.distplot(df_train_pp[cols_list]);\nplt.xticks(rotation = 45)","5c937cbd":"#MEAN IMPUTATION\ndf_train_pp['DistanceFromHome'] = df_train_pp['DistanceFromHome'].fillna(df_train_pp['DistanceFromHome'].median())","2308c55a":"cols_list = ['DistanceFromHome']\nfig = plt.figure(figsize=(10,5))\n#  subplot #1\nfig.add_subplot(121)\nplt.title('DistanceFromHome Distribution Before Imputation', fontsize=14)\nsns.distplot(df_train[cols_list]);\nplt.xticks(rotation = 45)\n\nfig.add_subplot(122)\nplt.title('DistanceFromHome Distribution After Imputation', fontsize=14)\nsns.distplot(df_train_pp[cols_list]);\nplt.xticks(rotation = 45)","d540e49c":"# Impute the missing values.\ndf_train_pp['MaritalStatus'].fillna(df_train_pp.dropna().mode()['MaritalStatus'][0], inplace = True)\ndf_train_pp['BusinessTravel'].fillna(df_train_pp.dropna().mode()['BusinessTravel'][0], inplace = True)","c0917473":"#creating dataset with outlier for further processing\ndf_train_pp_wo = df_train_pp.copy()","4a4cbb85":"df_outlier_cols = df_train_pp[['YearsAtCompany', 'TotalWorkingYears', 'MonthlyIncome', 'YearsWithCurrManager', 'NumCompaniesWorked', 'YearsSinceLastPromotion']].copy()","3616b239":"#melting the dataframe to bring the data into single column for FacetGrid\ndf_outlier_cols_melt=df_outlier_cols.melt()\ndf_outlier_cols_melt.head()","bb975069":"g = sns.FacetGrid(df_outlier_cols_melt, col='variable',sharex=False, sharey=False, size=4,col_wrap=5)\ng.map(sns.distplot,'value')\nplt.show()","688701b8":"g = sns.FacetGrid(df_outlier_cols_melt, col='variable',sharex=False, sharey=False, size=4,col_wrap=5)\ng.map(sns.boxplot,'value')\nplt.show()","c7d3d70d":"#Calculate Upper bound and Lower bound for all the columns having missing values\noutlier_col_list = ['YearsAtCompany', 'TotalWorkingYears', 'MonthlyIncome', 'YearsWithCurrManager', 'NumCompaniesWorked', 'YearsSinceLastPromotion']\nlower_bound = {}\nupper_bound = {}\nfor col in outlier_col_list:\n  IQR=df_outlier_cols[col].quantile(0.75)-df_outlier_cols[col].quantile(0.25)\n  lower_bound[col]=df_outlier_cols[col].quantile(0.25)-(IQR*1.5)\n  upper_bound[col]=df_outlier_cols[col].quantile(0.75)+(IQR*1.5)\n  print(\"Column Name: \",col)\n  print(\"Lower Bound: \", lower_bound[col])\n  print(\"Upper Bound: \", upper_bound[col])\n  print(\"Number of Outliers in Lower Bound: {}\". format(df_outlier_cols[df_outlier_cols[col]<lower_bound[col]][col].shape[0]))\n  print(\"Number of Outliers in Upper Bound: {}\". format(df_outlier_cols[df_outlier_cols[col]>upper_bound[col]][col].shape[0]))\n#print(lower_bound), print(upper_bound)","9c4e23cc":"# Replacing the outlier values with Upper Bound Value\ndf_outlier_cols_modified = df_outlier_cols.copy()\nfor col in outlier_col_list:\n  df_outlier_cols_modified.loc[df_outlier_cols_modified[col]>upper_bound[col],col] = upper_bound[col]\n  df_train_pp.loc[df_train_pp[col]>upper_bound[col],col] = upper_bound[col]","1279a7be":"#After Replacement Box Plots\nfor col in outlier_col_list:\n  fig, ax = plt.subplots(1,2, figsize = (10,5))\n  sns.boxplot(col, data = df_outlier_cols[outlier_col_list], ax = ax[0], orient = 'v')\n  sns.boxplot(col, data = df_outlier_cols_modified[outlier_col_list], ax = ax[1], orient = 'v')","4bbcf64a":"#Dropping 'EmployeeCount','EmployeeNumber','Over18','StandardHours' columns from dataset with and without outliers\ndf_train_pp_wo = df_train_pp_wo.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'],axis=1)\ndf_train_pp = df_train_pp.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'],axis=1)","e8ea8350":"#Encoding Variable\n\ndf_train_pp['Attrition'] = df_train_pp['Attrition'].map({'Yes':1 ,'No':0})\ndf_train_pp_wo['Attrition'] = df_train_pp_wo['Attrition'].map({'Yes':1 ,'No':0})\n\nfrom sklearn.preprocessing import LabelEncoder\nle_inc = LabelEncoder()\nohe_cols = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\nle_cols = ['Gender', 'OverTime']\ndf_train_pp['Encoded_Gender'] = le_inc.fit_transform(df_train_pp['Gender'])\ndf_train_pp['Encoded_OverTime'] = le_inc.fit_transform(df_train_pp['OverTime'])\ndf_train_pp_wo['Encoded_Gender'] = le_inc.fit_transform(df_train_pp_wo['Gender'])\ndf_train_pp_wo['Encoded_OverTime'] = le_inc.fit_transform(df_train_pp_wo['OverTime'])","303a34ce":"for col in ohe_cols:\n    df_train_pp = pd.concat([df_train_pp, pd.get_dummies(df_train_pp[col], prefix=col)], axis=1)\n    df_train_pp_wo = pd.concat([df_train_pp_wo, pd.get_dummies(df_train_pp_wo[col], prefix=col)], axis=1)","c9499faf":"cols_drop = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', 'Gender', 'OverTime']\nfor col in cols_drop:\n  del df_train_pp[col]\n  del df_train_pp_wo[col]","2b040f95":"print(df_train_pp.shape)\nprint(df_train_pp_wo.shape)","2b6f7778":"target_data_counter = Counter(df_train_pp_wo['Attrition'])\nprint(target_data_counter)","05ec22f1":"df_train_pp.shape","2b3d7273":"# WITHOUT SCALING\ny = df_train_pp['Attrition']\nX = df_train_pp.drop(['Attrition'], axis=1)\n\n# WITHOUT SCALING & WITH OUTLIER\n#y_wo = df_train_pp_wo['Attrition']\n#X_wo = df_train_pp_wo.drop(['Attrition'], axis=1)\n\n# WITH SCALING\nscaler = StandardScaler()\nX_scalled = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index=X.index)\n\n# WITH SCALING & WITH OUTLIER\n#X_wo_scalled = pd.DataFrame(scaler.fit_transform(X_wo), columns = X_wo.columns, index=X_wo.index)","17d13750":"# RESAMPLE USING SMOTE\ndef apply_smote_fun(X_train,y_train):\n    smk = SMOTETomek(random_state=42)\n    X_train_res,y_train_res=smk.fit_sample(X_train,y_train)\n    X_train_res = pd.DataFrame(X_train_res, columns=X_train.columns)\n    return (X_train_res, y_train_res)\n\ndef apply_nearmiss_func(X_train,y_train):\n    nr = NearMiss()\n    X_train_near, y_train_near= nr.fit_sample(X_train,y_train) \n    X_train_near = pd.DataFrame(X_train_near, columns=X_train.columns)\n    return (X_train_near, y_train_near)\n\n# TRAIN TEST SPLIT\ndef train_test_split_fun(X,y,testsize,rand_state):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=testsize, random_state=rand_state)\n    return (X_train, X_val, y_train, y_val)\n\n# APPLY MODEL WITHOUT HYPERPARAMETER TUNING\ndef apply_model(X,y,model,testsize=0.25,rand_state=20,apply_smote=0,apply_undersample=0,random_search_cv=0,grid_search_cv=0,params=[],cv=5,n_jobs=-1):\n    \n    X_train, X_val, y_train, y_val = train_test_split_fun(X,y,testsize,rand_state)\n    \n    #if apply_smote is 1\n    if(apply_smote != 0):\n        print(\"\\nSMOTE OVER-SAMPLING APPLIED\\n\")\n        X_train, y_train = apply_smote_fun(X_train,y_train)\n    \n    #if apply_undersample is 1\n    if(apply_undersample != 0):\n        print(\"\\nNEARMISS UNDERSAMPLING APPLIED\\n\")\n        X_train, y_train = apply_nearmiss_func(X_train,y_train)\n    \n    if(random_search_cv!=0):\n        print(\"------RANDOM SEARCH CV------\")\n        y_pred_val,y_pred_train = apply_model_randcv(X_train,y_train,X_val,model,params,cv,n_jobs)\n    elif(grid_search_cv!=0):\n        print(\"------GRID SEARCH CV------\")\n        y_pred_val,y_pred_train = apply_model_gridcv(X_train,y_train,X_val,model,params,cv,n_jobs)\n    else:\n        print(\"------WITHOUT HYPERPARAMETER TUNING------\")\n        model.fit(X_train, y_train)\n        y_pred_train = model.predict(X_train)\n        y_pred_val = model.predict(X_val)\n\n    print('----Confusion Matrix----\\n')\n    confusion_matrix(y_val, y_pred_val)\n    cm = pd.crosstab(y_val, y_pred_val, rownames = ['Actual'], colnames =['Predicted'], margins = True)\n    print (cm)\n\n    print('\\n\\n----Classification Report----\\n')\n    print('Classification Report of training set:\\n',classification_report(y_train, y_pred_train,labels=[1,0]),'\\n')\n    print('Classification Report of test set:\\n',classification_report(y_val, y_pred_val,labels=[1,0]),'\\n')\n\n    print('\\n--------Cross Validation (10 Fold) Scores--------')\n    print('Train set CV scores: %0.5f'%np.mean(cross_val_score(model, X_train, y_train, cv=10)),'\\n')\n    print('Test set CV scores: %0.5f'%np.mean(cross_val_score(model, X_val, y_val, cv=10)))\n    print('\\n----------------END----------------\\n\\n')\n\n    #matrix = classification_report(y_test, y_pred,labels=[1,0])\n    #print('Classification report : \\n',matrix)\n\n\n# APPLY MODEL WITH RandomSearchCV\ndef apply_model_randcv(X_train,y_train,X_val,model,param_grid,cv,n_jobs):\n    \n    classifier_random = RandomizedSearchCV(model, param_distributions=param_grid, cv=cv, verbose=True, n_jobs=-1)\n    best_classifier_random = classifier_random.fit(X_train, y_train)\n    print(best_classifier_random.best_estimator_)\n    y_pred_val = best_classifier_random.best_estimator_.predict(X_val)\n    y_pred_train = best_classifier_random.best_estimator_.predict(X_train)\n    return(y_pred_val,y_pred_train)\n    \n\n# APPLY MODEL WITH GridSearchCV\ndef apply_model_gridcv(X_train,y_train,X_val,model,param_grid,cv,n_jobs):\n    \n    classifier_grid = GridSearchCV(model, param_grid= param_grid, cv = cv, verbose=True, n_jobs=-1)\n    best_classifier_grid = classifier_grid.fit(X_train, y_train)\n    print(best_classifier_grid.best_estimator_)\n    y_pred_val = best_classifier_grid.best_estimator_.predict(X_val)\n    y_pred_train = best_classifier_grid.best_estimator_.predict(X_train)\n    return(y_pred_val,y_pred_train)\n","6b1245e6":"classifier = LogisticRegression(max_iter=10000)\n\n#Without Resampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20)\n\n#Using Oversampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_smote=1)\n\n#Using Undersampling\n#apply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_undersample=1)","6dbeda34":"#Random Search CV\nparam_grid_cv = param_grid = [    \n    {'penalty' : ['l1','elasticnet','l2', 'none'],\n     'tol' : np.logspace(-8, 0, 10),\n     'C' : np.logspace(-4, 4, 10),\n     'fit_intercept' : ['True', 'False'],\n     'class_weight' : [None, 'balanced'],\n     'solver' : ['liblinear', 'saga'],\n     'max_iter' : np.logspace(1, 4, 4)\n    }\n]\nclassifier = LogisticRegression()\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,grid_search_cv=1,params=param_grid_cv)","be83ef71":"classifier = DecisionTreeClassifier()\n\n#Without Resampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20)\n\n#Using Oversampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_smote=1)\n\n#Using Undersampling\n#apply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_undersample=1)","ac72318b":"param_grid_cv = [    \n    {'splitter' : ['best', 'random'],\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_split' : [50,100,150,200],\n     'min_samples_leaf' : [50,100,150,200],\n     'max_features' : list(range(1,150)),\n    }\n]\nclassifier = DecisionTreeClassifier()\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,grid_search_cv=1,params=param_grid_cv)","5ad81bc6":"classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 0)\n\n#Without Resampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20)\n\n#Using Oversampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_smote=1)\n\n#Using Undersampling\n#apply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_undersample=1)","925b12a6":"param_grid_cv = {\n    'n_estimators' : (10,30,50,70,90,100),\n    'criterion' : ('gini', 'entropy'),\n    'max_depth' : (3,5,7,9,10),\n    'max_features' : ('auto', 'sqrt'),\n    'min_samples_split' : (2,4,6),\n#    'min_weight_fraction_leaf' : (0.0,0.1,0.2,0.3),\n#    'class_weight' = ('balanced', 'balanced_subsample')\n}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nclassifier = RandomForestClassifier()\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,grid_search_cv=1,params=param_grid_cv,cv=cv)","c09ebc73":"classifier = xgboost.XGBClassifier()\n\n#Without Resampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20)\n\n#Using Oversampling\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_smote=1)\n\n#Using Undersampling\n#apply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,apply_undersample=1)","a467e5a6":"param_grid_cv = [    \n    {\n    \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n    \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n    \"min_child_weight\" : [ 1, 3, 5, 7 ],\n    \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n    \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    }\n]\nclassifier = xgboost.XGBClassifier()\napply_model(X_scalled,y,classifier,testsize=0.25,rand_state=20,grid_search_cv=1,params=param_grid_cv)","afe98672":"## **RANDOM FOREST**","252b4219":"Distribution of **YearAtCompany, TotalWorkingYears, MonthlyIncome** are right skewed, but there are several outliers present in these column.","66a8edb9":"For all columns, lower ranges are negative which makes no sense for these columns so we can ignore negative values here.","fcf7cea1":"## **DECISION TREE CLASSIFICATION**","97bcf99d":"From the Correlation Matrix, we can observe that 9 pairs of features has correlation coefficient more than 0.5. i.e.,\n\n- YearsAtCompany       VS    MonthlyIncome    :       0.525311\n- YearsSinceLastPromotion VS YearsInCurrentRole   :   0.545809\n- YearsSinceLastPromotion VS YearsAtCompany      :    0.592912\n- YearsAtCompany      VS     TotalWorkingYears   :    0.624096\n- TotalWorkingYears    VS    Age                 :    0.676650\n- YearsInCurrentRole    VS   YearsWithCurrManager  :  0.705717\n- YearsAtCompany      VS     YearsWithCurrManager  :  0.759755\n- MonthlyIncome      VS      TotalWorkingYears   :    0.773499\n- YearsAtCompany     VS      YearsInCurrentRole  :    0.777925","d303aa2c":"Distribution of **NumCompaniesWorked, YearSinceLastPromotion** are right skewed and there are some outliers present in the Column Data.","6e6efde8":"NUMBER OF MISSING VALUES\n\n1.   Age:136\n2.   BusinessTravel:5\n3.   DailyRate:27\n4.   DistanceFromHome:95\n5.   MaritalStatus:5\n","2da94e20":"Distribution of **DistanceFromHome** is right skewed, no outliers present in boxplot.","060a0298":"## **Categorical Feature Analysis**","7d02b0da":"**DAILYRATE**\n\nAs DailyRate is not highly correlated with other features and there is no skewness in the distribution of dailyrate, we shall use mean imputation to impute missing values of Dailyrate","0f83c613":"## Analyzing The Basic Metrics","31c52745":"## **LOGISTIC REGRESSION**","d42cd115":"## **Correlation Analysis**\n\n**CONTINUOUS COLUMNS**\n\nAs Pearson Correlation is one of the best way to calculate correlation between Numeric Columns, we are using Pearson Correlation here to find the correlation between numeric columns.","0d10090b":"Each columns are right skewed. We can calculate Upper Bound and Lower Bound for each Columns.","ac8d7609":"In terms of Target Variable (Attrition), the **dataset is highly imbalanced**","98f6c816":"From the above Matrix we can find strong correlation (correlation coefficient > 0.5) between the following pairs:\n\n1. YearsWithCurrManager VS YearsAtCompany : 0.76\n2. YearsWithCurrManager VS YearsInCurrentRole : 0.71\n3. YearsSinceLastPromotion VS YearsAtCompany : 0.59\n4. YearsSinceLastPromotion VS YearsInCurrentRole : 0.55\n5. YearsInCurrentRole VS YearsAtCompany : 0.78\n6. YearsAtCompany VS TotalWorkingYears : 0.62\n7. TotalWorkingYears VS Age : 0.54\n8. TotalWorkingYears VS JobLevel : 0.79\n9. TotalWorkingYears VS JobRole : 0.67\n10. TotalWorkingYears VS MonthlyIncome : 0.77\n11. StockOptionLevel VS MaritalStatus : 0.67\n12. PerformanceRating VS PercentSalaryHike : 0.77\n13. MonthlyIncome VS JobLevel : 0.95\n14. MonthlyIncome VS JobRole : 0.91\n15. JobRole VS Department: 0.91\n16. JobRole VS JobLevel: 0.88\n\nFrom the above findings, it can be said that some correlations are very common as per the feature types while some correlations are very interesting in nature. Correlation between StockOptionLevel and MaritalStatus is 0.67 which is a interesting insight. Whereas correlation between YearsSinceLastPromotion and YearsInCurrentRole should be higher than 0.55 as current role is strongly depends on last promotion.","07be18ca":"Although Datatypes of some features viz. **['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance']** are Integer or Float, these features are representing some categorical values. So these features should be added Categorical Features List and at the same time need to be removed from Continuous features list.","0aad0f07":"## Missing Vaules Exploration","da1d7dc7":"From the above Countplots it is clear that in every case for every feature negative attrition is more that positive attrition.","3c0c4ddd":"Distribution of **PercentSalaryHike, YearsWithCurrentManager** are right skewed and there is a bi-modal tendency of the distribution. In **PercentSalaryHike** no outliers present in boxplot but in **YearsWithCurrentManager** there are some outliers present in boxplot.","a8ed8726":"**It is observed that, the total dataset contains 35 columns out of which 5 columns (i.e. Age, BusinessTravel, DailyRate, DistanceFromHome and MaritalStatus) have missing values. \"Attrition\" is the target variable (\"Yes\" \/ \"No\"). Various datatype of the columns are Integer, Float and Object.**","8a3ebac2":"**Number of Rows with 0 NaN: 775, Number of Rows with 1 NaN: 240, Number of Rows with 2 NaN: 14**","84f760cd":"**LIST OF CATEGORICAL FEATURES**: ['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n\n**LIST OF CONTINUOUS FEATURES**: ['TotalWorkingYears', 'DistanceFromHome', 'NumCompaniesWorked', 'YearsInCurrentRole', 'YearsWithCurrManager', 'YearsSinceLastPromotion', 'PercentSalaryHike', 'DailyRate', 'Education', 'JobLevel', 'YearsAtCompany', 'Age', 'MonthlyRate', 'TrainingTimesLastYear', 'JobSatisfaction', 'WorkLifeBalance', 'MonthlyIncome', 'HourlyRate', 'StockOptionLevel', 'RelationshipSatisfaction', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobInvolvement']","4c97d54a":"**CORRELATION MATRIX BETWEEN ALL FEATURES AFTER ENCODING CATEGORICAL FEATURES**","3d022026":"# **DATA PRE-PROCESSING**","e37a63e4":"From the Heatmap we can see that all the missing values are sparsely located and there is not any big chunk of missing values present. By visualizing the heatmap plot, it can be said that, columns having missing values are not correlated. Although we will check further.","98fc5c95":"LIST OF MISSING VALUES:\n- Continuous Variables:\n  - Age: 136\n  - DailyRate: 27\n  - DistanceFromHome: 95\n- Discrete Variables:\n  - BusinessTravel: 5\n  - MaritalStatus: 5\n\n**Continuous Variables.**\n\n**AGE**\n\nAs Age and TotalWorkingYears are correlated, so we can think about imputing Age column missing values using TotalWorkingYears column. Also from the KDE Plot we can see that there is a linear relationship with Age and TotalWorkingYears. Thus, we can use a simple linear model regressing Age on TotalWorkingYears to fill the missing values in Age.","883563a9":"**DistanceFromHome**\n\nDistanceFromHome variable is also not highly correlated with other columns but the distribution of the variable is right skewed so we can use median as imputation method.","83890db0":"By plotting those mostly correlated features in a pairgird plot, we can overall see the relationships more specifically using the scatterplot and KDEplot.","4db0d0f3":"## **MISSING VALUE IMPUTATION**","ee528612":"The missingno correlation heatmap measures nullity correlation i.e. how strongly the presence or absence of one variable affects the presence of another. Columns that are full or empty have no meaningful correlation, and so are silently removed from the visualization. From the above heatmap it is absolutely clear that the columns having missing values are not at all correlated.","0996d587":"### **Count Plot of Discrete Features**\n\n(Test Data values are not used)","0ee4c3b0":"From the dendogram it is also clear that the missing value columns has no relationship with other columns. \n\n(Ref: https:\/\/github.com\/ResidentMario\/missingno)\n\nSo from here we can conclude that the missing values are **MCAR (Missing Completely At Random)** in nature.  ","128f8047":"**Finding relationships between columns having missing values in dataset** ","9ecf5e11":"**HourlyRate, MonthlyRate, DailyRate, Age** looks like almost normally distributed, no outliers present in boxplot.","c13f827b":"## **OUTLIER TREATMENT**\n\nAs per the EDA, [YearAtCompany, TotalWorkingYears, MonthlyIncome,YearsWithCurrentManager,NumCompaniesWorked, YearSinceLastPromotion] columns contains missing values. ","189e5375":"## **XGBOOST**","0b880c8d":"**As we can see there are no correlation between columns having missing values, so we have to apply some other methodologies to get more insights to handle those missing values.**","4a49b11f":"# **MODELLING**","c25f5e6c":"The above Describe function shows quick descriptive statistical summary of the Numerical data. Here we can see that Age, DailyRate and DistanceFromHome has some missing values, which we will explore later.\n\nOther descriptions of the features are:\n\n* **HourlyRate, MonthlyRate, DailyRate, Age are not skewed**\n* **DistanceFromHome, YearsAtCompany, PercentSalaryHike, YearsInCurrentRole, TotalWorkingYears, MonthlyIncome, NumCompaniesWorked, YearsSinceLastPromotion, YearsWithCurrentManager are Right Skewed**\n* **Education, EnvironmentSatisfaction, JobInvolvement, JobLevel, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TrainingTimesLastYear, WorkLifeBalance are Numerical but Discrete Variable**\n* **Values of EmployeeCount, StandardHours are same for each rows.**\n\nAlthough these are some overall observations and we will explore data further to find more Insights.","498eb818":"To Calculate Correlation between all features, we have to deal with 3 types cases (in terms of variable datatypes): \n1. **Categorical Nominal Vs Categorical Nominal**\n2. **Continuous Vs Continuous**\n3. **Categorical Nominal Vs Continuous**\n\nNow we can deal these different cases following way:\n\n**Categorical Nominal Features**: One common option to handle this scenario is by first using one-hot encoding, and break each possible option of each categorical feature to 0-or-1 features. This will then allow the use of correlation, but it can easily become too complex to analyse if the number of categorical features are high. So, in that scenario, we need something that will look like correlation, but will work with categorical nominal values. Cramer's V statistic helps to find the association between two nominal categorical variables. It is based on nominal variation of Pearson\u2019s Chi-Square Test. Similarly to correlation, the output is in the range of [0,1], where 0 means no association and 1 is full association. \n\n**Continuous Features**: In this case Pearson Correlation is used.\n\n**Continuous & Categorical Ordinal Features**: For this cases, we can use the Correlation Ratio. The Correlation Ratio answers the following question: Given a continuous number, how well can we know to which category it belongs to? Here also the output is on the range of [0,1]. \n\nIn the matrix below we handled the variable pairs as per the datatypes and find correlation between them.","e6d8ea0a":"## **Continuous Feature Analysis**","236fb714":"**'EmployeeCount', 'Over18', 'StandardHours'** are carrying same value for all the observations whereas **'EmployeeNumber'** is carrying unique value for each observation. Thus these columns have no significance in the desired target variable. \n\n***A little Explanation of why I think these column should be dropped.***\n\nAny Machine Learning Model is mathematical equation i.e.\n\ny = f(x), where,\n\ny = Target\/Dependent Variable\nf(x) = Function of Independent Variable.\n\nSo basically, ML models quantifies and estimates about for what value of X, what will the probable output y.\n\nNow, Assuming a single whole column is constant. So, a relationship between y and f(x=constant) is also constant because for whatever value of y, that x will remain same. No mathematical relationship is possible except for the only option that y is also an constant. Which we can safely assume isn't the case, else why we shall build a model to get a constant value.\n\nHence, we can safely drop any constant column, which doesn't add any variation in data to the DataFrame to save computational time, as that column won't affect y in any sense. ","22e4f4a4":"**CORRELATION MATRIX BETWEEN ALL FEATURES USING CRAMER'S V AND THEIL'S U**","1ccd34eb":"**Discrete Variables.**\n\n**MaritalStatus & BusinessTravel**","23893877":"# EXPLORATORY DATA ANALYSIS"}}