{"cell_type":{"1cf26ccf":"code","ae2ec0e6":"code","7c6d2337":"code","a80eb631":"code","86e50dc6":"code","e43b61dc":"code","9daf8546":"code","b885eb27":"code","7a3dc571":"code","88ab4232":"code","7db0546a":"code","67f99c25":"code","52e5e6af":"code","ecff5d8a":"code","8c2928d7":"code","d1a93469":"code","864f321e":"code","dd940d1d":"code","0f333270":"code","7c4ad6cf":"code","0b3983e4":"code","ab93a290":"code","dedce156":"code","c7f44d95":"code","42550947":"code","00849b6e":"code","b1619635":"code","4757d656":"code","becea4b8":"code","83cdb805":"code","7bb8eb0e":"markdown","44715f0c":"markdown","d08bf18f":"markdown","f6e2c650":"markdown","d89eaf6f":"markdown","c41f01ba":"markdown","d9c5555c":"markdown","433fc5bb":"markdown","071f6024":"markdown","17edfcc9":"markdown","086fdfd2":"markdown","ab0b9cdf":"markdown","ef41f9b7":"markdown","1fa09e37":"markdown","c7681184":"markdown","cbf6fe3c":"markdown","e26cfec7":"markdown","7261ae64":"markdown","2616c174":"markdown","52e917a8":"markdown","b223b5ad":"markdown"},"source":{"1cf26ccf":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n%matplotlib inline","ae2ec0e6":"train_df=pd.read_csv('..\/input\/cleanedtitanic\/train_df_cleaned.csv')\ntrain_df.head()","7c6d2337":"train_df.drop('Unnamed: 0',axis=1, inplace=True)\ntrain_df.head()\n","a80eb631":"test_df=pd.read_csv('..\/input\/cleanedtitanic\/test_df_cleaned.csv')\ntest_df.head()","86e50dc6":"test_df.drop('Unnamed: 0',axis=1, inplace=True)\ntest_df.head()\n","e43b61dc":"train_df.info()\n","9daf8546":"train_df.isnull().sum()\n","b885eb27":"test_df.info()\n","7a3dc571":"test_df.isnull().sum()\n","88ab4232":"plt.figure(figsize = [17,6])\na = sns.distplot(train_df['Age'].dropna(),bins = range(0,81,1), rug = True, fit = norm,color='green')\nplt.show()","7db0546a":"sns.heatmap(train_df.corr(), annot=True,cmap='Greens')\n","67f99c25":"pairplot_figure = sns.pairplot(train_df,size=3,x_vars=['Age','Fare','SibSp','Parch'],y_vars=['Age','Fare','SibSp','Parch'], hue=\"Survived\",palette='nipy_spectral')\nplt.show()","52e5e6af":"y_test_df=pd.read_csv('..\/input\/cleanedtitanic\/gender_submission.csv')\ny_test=y_test_df['Survived']\ny_test","ecff5d8a":"y_train = train_df['Survived']\ntrain_df.drop('Survived', axis = 1,inplace=True)","8c2928d7":"from sklearn.svm import SVC","d1a93469":"model= SVC()","864f321e":"model.fit(train_df, y_train)","dd940d1d":"y_pred= model.predict(test_df)","0f333270":"from sklearn.metrics import classification_report, confusion_matrix","7c4ad6cf":"confusion_matrix(y_test, y_pred)","0b3983e4":"print(classification_report(y_test, y_pred))","ab93a290":"#help(SVC)","dedce156":"from sklearn.model_selection import GridSearchCV","c7f44d95":"svm = SVC()\nparam_grid = {'C':[0.01,0.1,1, 10, 100, 1000],'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(svm,param_grid, cv=5)","42550947":"grid.fit(train_df, y_train)","00849b6e":"grid.best_estimator_","b1619635":"grid.best_params_","4757d656":"y_pred_grid= grid.predict(test_df)","becea4b8":"confusion_matrix(y_test, y_pred_grid)","83cdb805":"print(classification_report(y_test, y_pred_grid))","7bb8eb0e":"## Pros of SVM\nSVN can be very efficient, because it uses only a subset of the training data, only the support vectors\nWorks very well on smaller data sets, on non-linear data sets and high dimensional spaces\nIs very effective in cases where number of dimensions is greater than the number of samples\nIt can have high accuracy, sometimes can perform even better than neural networks\nNot very sensitive to overfitting\n\n## Cons of SVM\nTraining time is high when we have large data sets\nWhen the data set has more noise (i.e. target classes are overlapping) SVM doesn\u2019t perform well","44715f0c":"#### Choosing the best hyperparameters through GridSearchCV","d08bf18f":"## Tuning Parameters\n\nchoosing the right kernel is crucial, because if the transformation is incorrect, then the model can have very poor results. As a rule of thumb, always check if you have linear data and in that case always use linear SVM (linear kernel). Linear SVM is a parametric model, but an RBF kernel SVM isn\u2019t, so the complexity of the latter grows with the size of the training set. Not only is more expensive to train an RBF kernel SVM, but you also have to keep the kernel matrix around, and the projection into this \u201cinfinite\u201d higher dimensional space where the data becomes linearly separable is more expensive as well during prediction. Furthermore, you have more hyperparameters to tune, so model selection is more expensive as well! And finally, it\u2019s much easier to overfit a complex model!","f6e2c650":"![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png)","d89eaf6f":"#### Evaluate model","c41f01ba":"## What is Support Vector Machine(SVM)?\n\nSupport Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.\nIt uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. Simply put, it does some extremely complex data transformations, then figures out how to seperate your data based on the labels or outputs you've defined.\n\nThe goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n\nSVM chooses the extreme points\/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane:","d9c5555c":"### Hyperparameters","433fc5bb":"## Why is it important to use the kernel trick?\n\nAs you can see in the above picture, if we find a way to map the data from 2-dimensional space to 3-dimensional space, we will be able to find a decision surface that clearly divides between different classes. My first thought of this data transformation process is to map all the data point to a higher dimension (in this case, 3 dimension), find the boundary, and make the classification.\nThat sounds alright. However, when there are more and more dimensions, computations within that space become more and more expensive. This is when the kernel trick comes in. It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.","071f6024":"#### Predict","17edfcc9":"#### Test And Train set","086fdfd2":"*  Now in this notebook we use **Support Vector Machine(SVM)** to build a model to predict same thing.","ab0b9cdf":"## Data preparation\n","ef41f9b7":"Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n","1fa09e37":"## Build the model","c7681184":"![](https:\/\/miro.medium.com\/max\/1400\/1*ZpkLQf2FNfzfH4HXeMw4MQ.png)","cbf6fe3c":"#### Train","e26cfec7":"## Read Datasets and Check out the Data","7261ae64":"## Data Visualisation","2616c174":"#### In my previous notebook I talked about data visualization\u060c data cleaning and...\n#### we used the cleaned data in the previous notebook here\n#### Also, using the regression model, we predicted with 0.93% accuracy that the people who survived the sinking of the Titanic,\n*  Go **[HERE](https:\/\/www.kaggle.com\/zahrajai\/titanic-logistic-regression)** to access the above","52e917a8":"## Import Libraries","b223b5ad":"**As I said before, I used the cleaned data from the previous notebook\nI also made this dataset public([cleanedtitanic](https:\/\/www.kaggle.com\/zahrajai\/cleanedtitanic)) and you can use it and share it**"}}