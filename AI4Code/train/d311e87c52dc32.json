{"cell_type":{"d9200aea":"code","fb5598b7":"code","c45c129e":"code","125033da":"code","58a850ab":"code","7c121d9a":"code","ac003a7e":"code","ad418915":"code","740b9b35":"code","06566724":"code","3e8eee9b":"code","006f4a7a":"code","1a6ce48b":"code","a1f1bbe0":"code","61c4b5cc":"code","b2d27ac6":"code","1dafc1a5":"code","d762e638":"code","6ed29298":"code","480ea09c":"code","6a7e6668":"code","7d06189a":"code","760bfa83":"code","d97de4f7":"code","70a3e834":"code","73082e90":"code","7e8a88e7":"code","f2b5adcc":"code","70ce525c":"code","6600e9bb":"code","2d8c8833":"code","1fcaa0d5":"code","6c274000":"code","7af99724":"code","35c84154":"code","e0cc805d":"code","b82f8411":"code","5663c19a":"code","f7f232b1":"code","8ae5de63":"code","2d2a3fae":"code","4f367853":"code","20df636f":"code","29dae2c0":"code","1db78664":"code","916cc607":"code","9a4d47db":"code","317aa30e":"code","9bbeb5be":"code","a4187be4":"code","c84dc7a8":"code","b9685c2f":"code","e61e2b39":"code","2d9fa8da":"code","48835a81":"code","d33b4edc":"code","910023a7":"code","7f1d197a":"code","8abf9456":"code","31ef170d":"code","a64e50b5":"code","b0932408":"code","5f0004a5":"code","8286b2d1":"code","367a0f80":"code","4ee9cb3d":"markdown","f9791b06":"markdown","ac69f31f":"markdown","1141c8a7":"markdown","36132b2c":"markdown","9d6a48eb":"markdown","9badf83f":"markdown","6f2122de":"markdown","4efa455f":"markdown","582d70a7":"markdown","eaca5e94":"markdown","d694bac6":"markdown","039f3366":"markdown","430f5631":"markdown","6769f9f3":"markdown","fb3c7124":"markdown","fce4bfba":"markdown"},"source":{"d9200aea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nrandom.seed(42)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb5598b7":"# import the rare dataset\nrare_ds = pd.read_csv(\"\/kaggle\/input\/rare-kubernetes-anomalies\/RARE.csv\", sep=\"|\", header=0)\nrare_ds.head()\noriginal_rare_ds = rare_ds.copy()","c45c129e":"print(rare_ds.shape)","125033da":"rare_ds.dtypes.unique()","58a850ab":"rare_ds.columns =[col.strip() for col in rare_ds.columns]\nprint(rare_ds[\"anomaly\"].unique())\nprint(rare_ds[\"instance\"].unique())","7c121d9a":"rare_ds.hist(column='anomaly')","ac003a7e":"rare_ds.drop(labels=['instance'], axis=1, inplace = True)\n","ad418915":"final_columns = []\nfor column in sorted(rare_ds.columns):\n    if not column.strip().startswith((\"kafka_topic\", \"kafka_log\", \"kube_configmap\", \"kube_namespace\", \"kube_service\", \"kube_secret\", \"kube_pod_status\", \"kube_pod_container_status\", \"scrape_samples\", \"kafka_server_brokertopic\")):\n        final_columns.append(column.strip())","740b9b35":"features_ds = rare_ds[final_columns].copy()\nfeatures_ds.shape","06566724":"features_ds.dtypes.unique()","3e8eee9b":"# for all columns that are objects, convert to float\nfor column in features_ds.columns:\n    if features_ds[column].dtype == \"object\":\n        features_ds[column] = features_ds[column].astype(float)\nfeatures_ds.dtypes.unique()","006f4a7a":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0.1)\nsel.fit(features_ds)","1a6ce48b":"# get a list of the constant features. Those can be removed\nprint(\n    len([\n        x for x in features_ds.columns\n        if x not in features_ds.columns[sel.get_support()]\n    ]))","a1f1bbe0":"to_strip = [x for x in features_ds.columns if x not in features_ds.columns[sel.get_support()]]\nfeatures_ds_strip = features_ds.drop(labels=to_strip, axis=1)\nfeatures_ds_strip.shape","61c4b5cc":"X = features_ds_strip\nY = rare_ds[\"anomaly\"].to_frame()\nprint(X.shape)\nprint(Y.shape)","b2d27ac6":"# select with mutual information\nfrom sklearn.feature_selection import SelectPercentile as SP\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nselector = SP(percentile=2) # select features with top 2% MI scores\nselector.fit(X,Y)\nX_4 = selector.transform(X)\nX_train_4,X_test_4,y_train,y_test = tts(\n    X_4,Y\n    ,random_state=0\n    ,stratify=Y\n)\nmodel_4 = DTC().fit(X_train_4,y_train)\nscore_4 = model_4.score(X_test_4,y_test)","1dafc1a5":"print(f\"score_4:{score_4}\")\nprint(X_4.shape)","d762e638":"# get columns, we have reduced to a total of 35\ncolumns = np.asarray(X.columns.values)\nsupport = np.asarray(selector.get_support())\ncolumns_with_support = columns[support]\nprint(columns_with_support)","6ed29298":"# with this reduced amount of features, now we can do a correlation map\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nx = rare_ds[columns_with_support]\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","480ea09c":"common_features = ['java_lang_memorypool_committed_usage_g1_eden_space_0', 'java_lang_operatingsystem_freephysicalmemorysize_0', 'java_lang_operatingsystem_systemloadaverage_0', 'node_entropy_available_bits_0', 'node_filefd_allocated_0',\n                   'node_load1_0', 'node_memory_Active_anon_bytes_0', 'node_memory_Active_file_bytes_0', 'node_memory_Cached_bytes_0', 'node_memory_Committed_AS_bytes_0', 'node_memory_Inactive_bytes_0', 'node_memory_KernelStack_bytes_0',\n                   'node_memory_MemAvailable_bytes_0', 'node_memory_MemFree_bytes_0', 'node_memory_PageTables_bytes_0', 'node_memory_SReclaimable_bytes_0', 'node_memory_SUnreclaim_bytes_0', 'node_memory_Shmem_bytes_0', 'node_nf_conntrack_entries_0',\n                   'node_nf_conntrack_entries_1', 'node_nf_conntrack_entries_2', 'node_sockstat_TCP_alloc_0', 'node_sockstat_sockets_used_0']","6a7e6668":"Once we have reduced the relevant features to a reasonable number, we can proceed to train the model, having in consideration that we have labeled data, in a time series format. To train it, there are several options - some of the common ones are neural networks such as LSTM. We will start by scaling the features:","7d06189a":"from sklearn.preprocessing import MinMaxScaler\nfinal_df = rare_ds[common_features]\nfinal_df.head()","760bfa83":"# add the time and anomaly\nfinal_df = pd.concat([rare_ds[['anomaly']], final_df], axis=1)\nfinal_df.head()","d97de4f7":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11, 4)})\noriginal_rare_ds.columns =[col.strip() for col in original_rare_ds.columns]\noriginal_df = original_rare_ds.set_index(\"time\")\noriginal_df['factor'] = pd.factorize(original_df[\"instance\"])[0]\noriginal_df['factor'].plot(linewidth=0.5);","70a3e834":"original_df[['factor', 'anomaly']].iloc[0:1000].plot(linewidth=0.5);","73082e90":"original_df[['factor', 'anomaly']].iloc[3000:3500].plot(linewidth=0.5);","7e8a88e7":"We can see that the tests have been executed in time windows over 300 iterations, and the anomaly is just there for over 25 iterations. One good window could be to get metrics from the previous 60 iterations - that will be the previous minute:","f2b5adcc":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","70ce525c":"from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# integer encode direction\nencoder = LabelEncoder()\nvalues=final_df.values\nvalues[:,1] = encoder.fit_transform(values[:,1])","6600e9bb":"# ensure all data is float\nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","2d8c8833":"# drop columns we don't want to predict\nreframed = series_to_supervised(scaled, 120, 120)\nreframed.drop(reframed.columns[list(range(len(reframed.columns)-23,len(reframed.columns)))], axis=1, inplace=True)\n\nreframed.head()","1fcaa0d5":"print(reframed.shape)","6c274000":"# split into train and test sets\nvalues = reframed.values\ntrain_time=int(values.shape[0]*0.8)\ntest_time=int(values.shape[0]*0.9)\n\ntrain = values[:train_time, :]\nvalid = values[train_time:test_time, :]\ntest = values[test_time:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\nvalid_X, valid_y = valid[:, :-1], valid[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\nvalid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape)","7af99724":"# design network\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.metrics import BinaryAccuracy, AUC\n\nmodel1 = Sequential()\nmodel1.add(LSTM(units=50, return_sequences=False, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel1.add(Dense(units=10, activation='relu'))\nmodel1.add(Dense(units=1, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[BinaryAccuracy(),\n    AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel1.summary()","35c84154":"# fit network\nhistory1 = model1.fit(train_X, train_y, epochs=50, batch_size=60, validation_data=(valid_X, valid_y), verbose=0, shuffle=False)","e0cc805d":"from matplotlib import pyplot\npyplot.plot(history1.history['loss'], label='train')\npyplot.plot(history1.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","b82f8411":"pyplot.plot(history1.history['prc'], label='train')\npyplot.plot(history1.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","5663c19a":"from sklearn.metrics import f1_score\n\nscores = model1.evaluate(test_X, test_y, verbose=0)\nprint(\"Accuracy: {}\".format(scores[1]))\nprint(\"AUC ROC: {}\".format(scores[2]))\nprint(\"AUC precision-recall: {}\".format(scores[3]))","f7f232b1":"# will be true if the probability > 0.5\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model1.predict(test_X)\npredict_class = ((y_pred > 0.5)+0).ravel()\nconfusion_matrix(test_y, predict_class)","8ae5de63":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","2d2a3fae":"history2 = model1.fit(train_X, train_y, epochs=50, batch_size=512, validation_data=(valid_X, valid_y), verbose=0, shuffle=False)","4f367853":"pyplot.plot(history2.history['loss'], label='train')\npyplot.plot(history2.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","20df636f":"pyplot.plot(history2.history['prc'], label='train')\npyplot.plot(history2.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","29dae2c0":"scores = model1.evaluate(test_X, test_y, verbose=0)\nprint(\"Accuracy: {}\".format(scores[1]))\nprint(\"AUC ROC: {}\".format(scores[2]))\nprint(\"AUC precision-recall: {}\".format(scores[3]))","1db78664":"y_pred = model1.predict(test_X)\npredict_class = ((y_pred > 0.5)+0).ravel()\nconfusion_matrix(test_y, predict_class)","916cc607":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","9a4d47db":"predict_class = ((y_pred > 0.2)+0).ravel()\nconfusion_matrix(test_y, predict_class)","317aa30e":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","9bbeb5be":"model3 = Sequential()\nmodel3.add(LSTM(units=256, return_sequences=False, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel3.add(Dense(units=32, activation='relu'))\nmodel3.add(Dense(units=1, activation='sigmoid'))\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=[BinaryAccuracy(),\n    AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel3.summary()\nhistory3 = model3.fit(train_X, train_y, epochs=50, batch_size=60, validation_data=(valid_X, valid_y), verbose=0, shuffle=False)","a4187be4":"pyplot.plot(history3.history['loss'], label='train')\npyplot.plot(history3.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","c84dc7a8":"pyplot.plot(history3.history['prc'], label='train')\npyplot.plot(history3.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","b9685c2f":"We can see good results, where validation loss is finally converging, with some regular intervals. Let's evaluate the model to decide which one is best:","e61e2b39":"scores = model3.evaluate(test_X, test_y, verbose=0)\nprint(\"Accuracy: {}\".format(scores[1]))\nprint(\"AUC ROC: {}\".format(scores[2]))\nprint(\"AUC precision-recall: {}\".format(scores[3]))","2d9fa8da":"y_pred = model3.predict(test_X)\npredict_class = ((y_pred > 0.5)+0).ravel()\nconfusion_matrix(test_y, predict_class)","48835a81":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","d33b4edc":"predict_class = ((y_pred > 0.2)+0).ravel()\nconfusion_matrix(test_y, predict_class)","910023a7":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","7f1d197a":"model4 = Sequential()\nmodel4.add(LSTM(units=256, return_sequences=False, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel4.add(Dropout(0.2))\nmodel4.add(Dense(units=1, activation='sigmoid'))\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=[BinaryAccuracy(),\n    AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel4.summary()\nhistory4 = model4.fit(train_X, train_y, epochs=50, batch_size=60, validation_data=(valid_X, valid_y), verbose=0, shuffle=False)","8abf9456":"pyplot.plot(history4.history['loss'], label='train')\npyplot.plot(history4.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","31ef170d":"pyplot.plot(history4.history['prc'], label='train')\npyplot.plot(history4.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","a64e50b5":"scores = model4.evaluate(test_X, test_y, verbose=0)\nprint(\"Accuracy: {}\".format(scores[1]))\nprint(\"AUC ROC: {}\".format(scores[2]))\nprint(\"AUC precision-recall: {}\".format(scores[3]))","b0932408":"y_pred = model4.predict(test_X)\npredict_class = ((y_pred > 0.5)+0).ravel()\nconfusion_matrix(test_y, predict_class)","5f0004a5":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","8286b2d1":"predict_class = ((y_pred > 0.2)+0).ravel()\nconfusion_matrix(test_y, predict_class)","367a0f80":"print(\"F1 score: {}\".format(f1_score(test_y, predict_class)))","4ee9cb3d":"We can see that with this final model we have better results, specially on predicting the negative class. We still have some problems predicting the positive class, but if we reduce the threshold, the numbers are better.","f9791b06":"We can see that the anomaly is just a binary field, that we can use to know if there is an anomaly or not. We can check the distribution of the field. ","ac69f31f":"We can observe that we have a time series dataset, which contains the timestamp as first column, and anomaly as second column. Thee is the isntance field, that contains a descriptive value of the cluster status on that moment.\nThen, there are a set of prometheus metrics, collected for the specific timestamp. The anomaly is the field that we are going to use for training, and the other entries are numerical entries, that are going to influence the model.\nWe have a total of 10.010 rows for training, and a total of 7063 features, that will need to be selected.\nLet's show the different values for the anomaly and instance field:","1141c8a7":"Using the Mutual Information method, for the 3% with bigger MI score, returned a total of 36 columns. With a more reasonable number of features, we can now use visualization to discard the highly correlated ones.","36132b2c":"We can see that the anomaly is just a binary field, that we can use to know if there is an anomaly or not. For our case, we can drop the instance field, as it is not relevant for training based on anomalies.","9d6a48eb":"We have a very good model, but we have not experimented with diferent parameters or type of networks. Let's first train the first model, changing the batch size.","9badf83f":"Once we have the data in the proper shape, and it is divided properly into train\/validation\/test, is time to start applying a model. We are going to use an LSTM model, that will be able to pick information from a sequence and will be able to learn patterns, specially if they are long sequences. ","6f2122de":" Now we need to proceed with feature selection. We have a total of 7060 metrics and the objective is to select the relevant ones for the project. In order to do it, we could proceed with manual removal first, using the Kubernetes domain knowledge, to discard the non meaningful ones:\n - any information about resource creation is not relevant, because it depends on the workloads that are existing on each cluster\n - kafka topics, logs... are a consequence of other operations in the cluster, they cannot be part of cluster failures reason\n - any information related about pod status, container status.. that have some fixed id needs to be discarded, as it is something that will change depending on the workloads and cluster","4efa455f":"And now predict the results","582d70a7":"After manual selection we will use automated methods. First step would be to remove the constant features. Any metric that is having a constant value, with no relevant variance over the dataset, is a metric that is not useful to discriminate a target. We can find those constant values using variance:","eaca5e94":"With that technique we have reduced the number of features to 1740. It is still a long number of features, so we need to apply more feature reduction techniques to select the most relevant ones. As we deal with a large amount of features, we will reduce them using Mutual Information technique:","d694bac6":"We have plotted the chart to see the train and validation loss. We can see that is rapidly decreasing and converging. We also can see that the precision-recall finally converges around 1.","039f3366":"Next thing will be to convert the timestamp to datetime, and then we need to prepare data to have regular intervals. We check the interval time, and we can see that we have information for over 15 hours, for one day. We can see that the data is already divided into regular intervals of one second.","430f5631":"The next step is to tranform this data into a time series problem. In orer to do that, first we need to decide the time window. This is where the data we dropped before - with Instance information - can be useful, because it shows the time where the failure was introduced, and the time it took to fail. Let's plot it: ","6769f9f3":"We can see that this model is the one that is having less validation loss, and also the numbers for Binary accuracy and Precision\/Recall are better, so we will use that. Let's do a prediction with that:","fb3c7124":"Now let's predict values with the testing data we separated from our dataset.","fce4bfba":"We can see results where it took longer to converge, with inaccurate results."}}