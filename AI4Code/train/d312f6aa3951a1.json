{"cell_type":{"c86859d0":"code","bea26a59":"code","0bcad1e8":"code","1e8e8baf":"code","7ba5ed4f":"code","5fe23459":"code","aff9c034":"code","e9743d65":"code","68fc1632":"code","d3c3c0e3":"code","761e30af":"code","7b2823af":"code","4041759c":"code","fd5c7c16":"code","20c3b2be":"code","54bfaebe":"code","f085ae31":"code","efcd2ecb":"code","aa468a6d":"markdown","5cbb420d":"markdown","661f1a40":"markdown","f93d66ee":"markdown","9553563d":"markdown","80590af8":"markdown","9406eb24":"markdown","ff1d657e":"markdown","e1ef8f7d":"markdown"},"source":{"c86859d0":"!pip install transformers","bea26a59":"import tensorflow as tf\nimport numpy as np\nfrom transformers import *\nimport tokenizers\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix","0bcad1e8":"print(tf.__version__)","1e8e8baf":"vocab_file = '..\/input\/tf-roberta\/vocab-roberta-base.json'\nmerge_file = '..\/input\/tf-roberta\/merges-roberta-base.txt'\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file, merge_file,lowercase = True)","7ba5ed4f":"MAX_LEN = 100\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv').fillna('')\ntrain.sample(25)","5fe23459":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nfor k in range(ct):\n    keyword = train.loc[k ,'keyword']\n    keyword = \" \".join(keyword.split())\n    text1 = train.loc[k ,'text']\n    text1 = \" \" + \" \".join(text1.split())\n    text1_enc = tokenizer.encode(text1)\n    keyword_enc = tokenizer.encode(keyword)\n    enc = [0] + text1_enc.ids + [2 ,2] + keyword_enc.ids + [2]\n    input_ids[k ,:len(enc)] = enc\n    attention_mask[k ,:len(enc)] = 1\n    if k <=3:\n        print('#################################################')\n        print('text : {}'.format(text1))\n        print('encoding : {}'.format(text1_enc.ids))\n        print('tokens : {}'.format(text1_enc.tokens))\n        print('keyword : {}'.format(keyword))\n        print('keyword_enc : {}'.format(keyword_enc.ids))\n        print('keyword_tokens : {}'.format(keyword_enc.tokens))","aff9c034":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv').fillna('')\nct_t = test.shape[0]\ninput_ids_t = np.ones((ct_t,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct_t,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct_t,MAX_LEN),dtype='int32')\nfor k in range(ct_t):\n    keyword = test.loc[k ,'keyword']\n    keyword = \" \".join(keyword.split())\n    text1 = test.loc[k ,'text']\n    text1 = \" \" + \" \".join(text1.split())\n    text1_enc = tokenizer.encode(text1)\n    keyword_enc = tokenizer.encode(keyword)\n    enc = [0] + text1_enc.ids + [2 ,2] + keyword_enc.ids + [2]\n    input_ids_t[k ,:len(enc)] = enc\n    attention_mask_t[k ,:len(enc)] = 1\n    if k <=3:\n        print('#################################################')\n        print('text : {}'.format(text1))\n        print('encoding : {}'.format(text1_enc.ids))\n        print('tokens : {}'.format(text1_enc.tokens))\n        print('keyword : {}'.format(keyword))\n        print('keyword_enc : {}'.format(keyword_enc.ids))\n        print('keyword_tokens : {}'.format(keyword_enc.tokens))","e9743d65":"outputs = []\nfor k in range(ct):\n    sent = train.loc[k ,'target']\n    #checking for any labels other than 0 or1\n    if sent != 0 and sent != 1:\n        print(sent ,k)\n    if k<5:\n        print('{} is {} type'.format(sent ,type(sent)))\n    outputs.append(sent)\noutputs = np.asarray(outputs)\noutputs = outputs.astype('float32')\noutputs = outputs.reshape(-1)\n#just to check whether everything is going fine \nprint(type(outputs) ,outputs.shape)","68fc1632":"zeros = 0\nones = 0\nfor i in range(ct):\n    if outputs[i] == 0:\n        zeros+=1\n    else:\n        ones+=1\nprint(zeros ,ones)\n","d3c3c0e3":"ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\natt = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\ntok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nconfig = RobertaConfig.from_pretrained('..\/input\/tf-roberta\/config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained('..\/input\/tf-roberta\/pretrained-roberta-base.h5',config=config)\nx = bert_model(ids,attention_mask=att,token_type_ids=tok)\ndrop1 = tf.keras.layers.Dropout(0.2)(x[0])\nlayer2 = tf.keras.layers.Conv1D(1 ,kernel_size = 1)(drop1)\nlayer3 = tf.keras.layers.Flatten()(layer2)\nlayer4 = tf.keras.layers.Activation('elu')(layer3)\noutput = tf.keras.layers.Dense(1 ,activation = 'sigmoid')(layer4)\nmodel = tf.keras.Model(inputs = [ids ,att ,tok] ,outputs = [output])\nmodel.summary()","761e30af":"def my_loss(gamma):\n    '''defining focal loss with parameter gamma'''\n    def focal_loss(y_true ,y_pred):\n        y_pred =tf.keras.backend.clip(y_pred ,1e-6 ,1-(1e-6))\n        log_yp = tf.keras.backend.log(y_pred)\n        log_yp_ = tf.keras.backend.log(1-y_pred)\n        loss = ((1-y_pred)**gamma)*y_true*log_yp + (y_pred**gamma)*(1-y_true)*log_yp_\n        return -tf.keras.backend.sum(loss)\n    return focal_loss","7b2823af":"adam = tf.keras.optimizers.Adam(lr = 0.000001)\nmodel.compile(optimizer = adam ,loss = my_loss(1.5) ,metrics = ['acc'])","4041759c":"history = model.fit([input_ids[800:] ,attention_mask[800:] ,token_type_ids[800:]] ,\n                    outputs[800:] ,\n                   epochs = 45 ,\n                   batch_size = 32 ,\n                   validation_data = ([input_ids[:200] ,attention_mask[:200] ,token_type_ids[:200]] ,outputs[:200]) ,\n                   verbose = 1)","fd5c7c16":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model acc')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","20c3b2be":"pred = model.predict([input_ids[:800] ,attention_mask[:800] ,token_type_ids[:800]] ,verbose = 1)\ny_true = outputs[:800]\nprint(y_true.shape)\ny_pred = np.zeros_like(y_true)\nfor i in range(pred.shape[0]):\n    if pred[i] >= 0.5:\n        y_pred[i] = 1","54bfaebe":"accuracy = accuracy_score(y_true, y_pred)\nprint('Accuracy: %f' % accuracy)\nprecision = precision_score(y_true ,y_pred)\nprint('Precision: %f' % precision)\nrecall = recall_score(y_true ,y_pred)\nprint('Recall: %f' % recall)\nf1 = f1_score(y_true ,y_pred)\nprint('F1 score: %f' % f1)\nauc = roc_auc_score(y_true ,y_pred)\nprint('ROC AUC: %f' % auc)\nmatrix = confusion_matrix(y_true ,y_pred)\nprint('condusion matrix:{}'.format(matrix))\nplt.imshow(matrix ,cmap = 'gray')","f085ae31":"test_pred = model.predict([input_ids_t ,attention_mask_t ,token_type_ids_t] ,verbose = 1)\ntest_pred = test_pred.reshape(-1)\nall = []\nfor i in range(test_pred.shape[0]):\n    if test_pred[i] >= 0.5:\n        all.append(1)\n    else:\n        all.append(0)","efcd2ecb":"test['target'] = all\ntest[['id' ,'target']].to_csv('sample_submission.csv' ,index = False)\ntest.sample(25)","aa468a6d":"## Kaggle submission","5cbb420d":"## Tokenizing:\nComing to the most crucial part of the notebook ,tokenizing our data.<br\/>\nWe need to use the same tokenizer with which the roBERTa model was pretrained.<br\/>\nOne of the most interesting thing with roBERTa is it does not require token type ids.But still as a common practice ,I am justing making that zero(there will not be any difference even if you don't provide token type ids check the paper for more details)","661f1a40":"## Building our model:","f93d66ee":"So in this notebook I am trying to finetune a pretrained roBERTa model for this task.<br\/>\nEven models like alBERT will also do fine.","9553563d":"## Training","80590af8":"## Calculating the number of samples of each class","9406eb24":"Setting a maximum length will help us to pad every sentence to equal length\n","ff1d657e":"# About transformers and BERT models:\nLanguage models like BERT ,roBERTa ,alBERT are state of the art models in NLP currently.These models are created by stacking the encoders of transformers.These are pretrained on various NLP tasks like next sentence prediction(NSP) ,Masked Language Model(MLM) etc.,. just to make them learn language representation.These are really hard and time taking to implement from scratch.However ,models like BERT and alBERT are readily available with tensorflow but still roBERTa and other models aren't much available as far as of  my knowledge.Here ,the hugging face transformers comes to rescue.Their repository have almost all pretrained [transformers](https:\/\/github.com\/huggingface\/transformers) and [tokenizers](https:\/\/github.com\/huggingface\/tokenizers)(which are really fast and impressive).<br\/>\n### Note: \nI highly recommend to take look at the official [roBERTa](https:\/\/arxiv.org\/abs\/1907.11692) paper.","e1ef8f7d":"## defining loss function:\nPeople usually tends to use binary cross entropy as this is a  binary classification problem ,that will also work provided we don't have any class imbalance in our data.<br\/>\nBut focal loss is designed to tackle the class imabalance in the data if any. For a more detailed explanation check out this [blog](https:\/\/www.dlology.com\/blog\/multi-class-classification-with-focal-loss-for-imbalanced-datasets\/)"}}