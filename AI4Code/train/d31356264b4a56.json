{"cell_type":{"6aca24a9":"code","897af4e3":"code","3f954c28":"code","763e80ae":"code","70462764":"code","1320f91d":"code","903c0bfe":"code","ebf46275":"code","f3f82756":"code","c722a8d5":"code","408a0e7e":"code","61fa7683":"code","51528976":"code","aa4057b1":"code","c0fd9ab6":"code","309569bd":"code","21c295bb":"code","25e863f5":"code","551f4936":"markdown","9a2a1f85":"markdown"},"source":{"6aca24a9":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n# import lightgbm as lgb\nimport optuna\nimport functools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,confusion_matrix,f1_score\nimport datetime","897af4e3":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID') # ,nrows=12345\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\n","3f954c28":"import gc\ngc.collect()","763e80ae":"def fraud_datetime(df):\n    \"\"\"\n    Credit for picking 31.12\n    \"\"\"\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n    # df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n#     df['day'] = df['TransactionDT'].dt.day\n    df.drop(['TransactionDT'],axis=1,inplace=True)\n    return df","70462764":"%%time\nX_train[\"null_counts\"] = X_train.isna().sum(axis=1)\nX_test[\"null_counts\"] = X_test.isna().sum(axis=1)\n\n\n# nunique appears unstable\n# currently trying wit just numeric cols, may use less mem\nX_train[\"nuniques\"] = X_train.select_dtypes(include=[np.number]).nunique(axis=1)\nX_test[\"nuniques\"] = X_test.select_dtypes(include=[np.number]).nunique(axis=1)","1320f91d":"## note: we also drop the TransactionDT here\nX_train = fraud_datetime(X_train)\nX_test = fraud_datetime(X_test)","903c0bfe":"## probably better to impute NANS, as the distrib changes between train and test\n\n# X_train = X_train.fillna(-999)\n# X_test = X_test.fillna(-999)","ebf46275":"# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","f3f82756":"gc.collect()\nX_train.head()","c722a8d5":"## random split: \n# (X_train,X_eval,y_train,y_eval) = train_test_split(X_train,y_train,test_size=0.15,random_state=0)\n\n## temporal split (assuming data remains sorted): split by last records, e.g. by recordid or TransactionDT\n\n## 80% of data\nTR_ROW_CNT = int(X_train.shape[0]*0.8)\nprint(TR_ROW_CNT)\n\nX_eval = X_train[TR_ROW_CNT:]\nprint(\"eval shape\",X_eval.shape)\n\nX_train = X_train[:TR_ROW_CNT]\nprint(\"new train shape\",X_train.shape)\n\ny_eval = y_train[TR_ROW_CNT:]\ny_train = y_train[:TR_ROW_CNT]","408a0e7e":"# ## fast AUC metric + calc, from : https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013#latest-556434\n# ### gives errors, doesn't work ? \n\n# # import numpy as np \n# from numba import jit\n\n# @jit\n# def fast_auc(y_true, y_prob):\n#     y_true = np.asarray(y_true)\n#     y_true = y_true[np.argsort(y_prob)]\n#     nfalse = 0\n#     auc = 0\n#     n = len(y_true)\n#     for i in range(n):\n#         y_i = y_true[i]\n#         nfalse += (1 - y_i)\n#         auc += y_i * nfalse\n#     auc \/= (nfalse * (n - nfalse))\n#     return auc\n\n# def eval_auc(preds, dtrain):\n#     labels = dtrain.get_label()\n#     return 'auc', fast_auc(labels, preds), True\n\n","61fa7683":"\nfrom sklearn.metrics import roc_auc_score\n\ndef opt(X_train, y_train, X_test, y_test, trial):\n    #param_list\n    n_estimators = trial.suggest_int('n_estimators', 400, 900) # may relate to instability of kernel when predicting? \n    max_depth = trial.suggest_int('max_depth', 4, 20)\n    min_child_weight = trial.suggest_int('min_child_weight', 1, 25)\n    #learning_rate = trial.suggest_discrete_uniform('learning_rate', 0.01, 0.1, 0.01)\n    scale_pos_weight = trial.suggest_int('scale_pos_weight', 1, 30)\n    subsample = trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.1)\n    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.6, 1.0, 0.1)\n\n    xgboost_tuna = xgb.XGBClassifier(n_jobs=1,\n        random_state=41, \n        tree_method='gpu_hist',\n        n_estimators = n_estimators,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        #learning_rate = learning_rate,\n        scale_pos_weight = scale_pos_weight,\n        subsample = subsample,\n        colsample_bytree = colsample_bytree\n    )\n    xgboost_tuna.fit(X_train, y_train)\n\n    tuna_pred_test_proba = xgboost_tuna.predict_proba(X_test)[:,1]\n    return (1.0 - (roc_auc_score(y_test, tuna_pred_test_proba)))\n\n#     tuna_pred_test = xgboost_tuna.predict(X_test)\n#     return (1.0 - (accuracy_score(y_test, tuna_pred_test))) # default ,accuracy","51528976":"study = optuna.create_study()\nstudy.optimize(functools.partial(opt, X_train, y_train, X_eval, y_eval), n_trials=80)","aa4057b1":"print(\"Best score found:\",1-study.best_value)\nprint(study.best_params)","c0fd9ab6":"clf = xgb.XGBClassifier(tree_method='gpu_hist',**study.best_params)\nX_train = pd.concat([X_train,X_eval])\ny_train = pd.concat([y_train,y_eval])\ndel X_eval,y_eval\n\nprint(X_train.shape)\nprint(y_train.shape)\nclf.fit(X_train,y_train)","309569bd":"gc.collect()","21c295bb":"## plot features importance\nimport matplotlib.pyplot as plt\n\nfi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0005].sort_values('importance',ascending=False).head(32).plot(kind='barh', figsize=(8, 24), title='Feature Importance')\nplt.show()","25e863f5":"## kernel tends to run out of memory when doing predictions ?\n\nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('submission.csv')","551f4936":"* XGBOost model hyperparam tuner\n* modified to use temporal split for CV. \n* Added 3 of the features from my features kernel, and datetime : \n    * https:\/\/www.kaggle.com\/danofer\/ieee-fraud-features-xgboost-0-934-lb\n    * https:\/\/www.kaggle.com\/danofer\/ieee-fraud-new-features-export-0-9359-lb\n    \n* Credit for the specific date (vs just 1.1.2018\" goes to : https:\/\/www.kaggle.com\/kevinbonnes\/transactiondt-starting-at-2017-12-01","9a2a1f85":"## Training\n\n* metric to  AUC"}}