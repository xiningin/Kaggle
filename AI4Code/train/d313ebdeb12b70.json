{"cell_type":{"78604fae":"code","70b8e278":"code","6f8676f4":"code","c452fc17":"code","c67e4bb8":"code","15b34f8e":"code","65b0162d":"code","e96319ec":"code","421a5696":"code","5413ca31":"code","5c6c0711":"code","93533a0e":"code","adf22741":"code","0fdee5af":"code","913d96c0":"markdown","a8082aec":"markdown","a372f717":"markdown","6f9a7a57":"markdown","9eed9deb":"markdown","e2b36174":"markdown"},"source":{"78604fae":"import gc\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.autonotebook import tqdm\nimport pickle\n\nfrom transformers import *\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\ndevice = torch.device('cuda')","70b8e278":"class config:\n    TOKEN_ID_DIR = \"\/kaggle\/input\/aio-make-token-ids\"\n    SEED = 416\n    TRAIN_BATCH_SIZE = 2\n    VALID_BATCH_SIZE = 2\n    OPTIONS = 4\n    EPOCHS = 1\n    LEARNING_RATE = 1e-6\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese\"","6f8676f4":"with open(f\"{config.TOKEN_ID_DIR}\/train.pkl\", \"rb\") as f:\n    train = pickle.load(f)\nwith open(f\"{config.TOKEN_ID_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{config.TOKEN_ID_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)","c452fc17":"class BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE)\n        self.fc = nn.Linear(768, 1)\n\n        \n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        logits = self.fc(h)\n        logits = logits.view(-1, n_choice)\n\n        return logits","c67e4bb8":"class JaqketDataset:\n    def __init__(self, data, train=True):\n        self.data = data\n        self.train = train\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.train:\n            return {'ids': torch.tensor(d[\"input_ids\"][:config.OPTIONS], dtype=torch.long),\n                    'mask': torch.tensor(d[\"attention_mask\"][:config.OPTIONS], dtype=torch.long),\n                    'token_type_ids': torch.tensor(d[\"token_type_ids\"][:config.OPTIONS], dtype=torch.long),\n                    'targets': torch.tensor(d[\"label\"], dtype=torch.long)}\n        else:\n            return {'ids': torch.tensor(d[\"input_ids\"], dtype=torch.long),\n                    'mask': torch.tensor(d[\"attention_mask\"], dtype=torch.long),\n                    'token_type_ids': torch.tensor(d[\"token_type_ids\"], dtype=torch.long),\n                    'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","15b34f8e":"train_dataset = JaqketDataset(train, train=True)\ntrain_data_loader = DataLoader(train_dataset,\n                               batch_size=config.TRAIN_BATCH_SIZE,\n                               shuffle=True,\n                               drop_last=True,\n                               num_workers=2)\n\ndev1_dataset = JaqketDataset(dev1, train=False)\ndev1_data_loader = DataLoader(dev1_dataset,\n                              batch_size=config.TRAIN_BATCH_SIZE,\n                              shuffle=False,\n                              drop_last=False,\n                              num_workers=2)\n\ndev2_dataset = JaqketDataset(dev2, train=False)\ndev2_data_loader = DataLoader(dev2_dataset,\n                              batch_size=config.TRAIN_BATCH_SIZE,\n                              shuffle=False,\n                              drop_last=False,\n                              num_workers=2)","65b0162d":"model = BertForAIO()\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)","e96319ec":"trn_losses = []\nfor epoch in range(config.EPOCHS):\n    # \u5b66\u7fd2\n    model.train()\n    for d in tqdm(train_data_loader):\n        ids = d[\"ids\"].to(device, dtype=torch.long)\n        mask = d[\"mask\"].to(device, dtype=torch.long)\n        token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n        targets = d[\"targets\"].to(device, dtype=torch.long)\n        \n        model.zero_grad()\n        y_pred = model(ids, mask, token_type_ids)\n        loss = nn.CrossEntropyLoss()(y_pred, targets)\n        loss.backward()\n        optimizer.step()\n        trn_losses.append(loss.item())\n        \n    # \u8a55\u4fa1\n    model.eval()\n    dev1_scores, dev2_scores = [], []\n    with torch.no_grad(): \n        for d in tqdm(dev1_data_loader):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = d[\"targets\"].to(device, dtype=torch.long).cpu().numpy()\n\n            y_pred = model(ids, mask, token_type_ids)\n            y_pred = y_pred.cpu().detach().numpy().argmax(1)\n            acc = accuracy_score(targets, y_pred)\n            dev1_scores.append(acc)\n        dev1_acc = sum(dev1_scores)\/len(dev1_scores)\n        \n        for d in tqdm(dev2_data_loader):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = d[\"targets\"].to(device, dtype=torch.long).cpu().numpy()\n\n            y_pred = model(ids, mask, token_type_ids)\n            y_pred = y_pred.cpu().detach().numpy().argmax(1)\n            acc = accuracy_score(targets, y_pred)\n            dev2_scores.append(acc)\n        dev2_acc = sum(dev2_scores)\/len(dev2_scores)\n\n    print(f\"{epoch} epoch: dev1={dev1_acc} \/ dev2={dev2_acc}\")\n    torch.save(model.state_dict(), f\"aio_bert_epoch_{epoch}.bin\")","421a5696":"print(f\"{epoch} epoch: dev1={dev1_acc} \/ dev2={dev2_acc}\")","5413ca31":"plt.plot(trn_losses)","5c6c0711":"del train, train_dataset, train_data_loader\ndel dev1, dev1_dataset, dev1_data_loader\ndel dev2, dev2_dataset, dev2_data_loader\ngc.collect()","93533a0e":"df_aio_leaderboard = pd.read_json(f\"{config.TOKEN_ID_DIR}\/aio_leaderboard.json\", orient='records', lines=True)       \n\nwith open(f\"{config.TOKEN_ID_DIR}\/test.pkl\", \"rb\") as f:\n    test = pickle.load(f)\n    \ntest_dataset = JaqketDataset(test, train=False)","adf22741":"predicts = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, d in enumerate(test_dataset):\n        ids = d[\"ids\"].to(device, dtype=torch.long).unsqueeze(0)\n        mask = d[\"mask\"].to(device, dtype=torch.long).unsqueeze(0)\n        token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long).unsqueeze(0)\n        \n        y_pred = model(ids, mask, token_type_ids)\n        y_pred = y_pred.cpu().detach().numpy().argmax(1)\n\n        p = {\"qid\": df_aio_leaderboard.loc[idx, \"qid\"],\n             \"answer_entity\": df_aio_leaderboard.loc[idx, \"answer_candidates\"][y_pred[0]]}\n        predicts.append(p)\n\npd.DataFrame(predicts).to_json(f'lb_predict.jsonl', orient='records', force_ascii=False, lines=True)","0fdee5af":"!head \"lb_predict.jsonl\"","913d96c0":"\u5b66\u7fd2loss\u306e\u78ba\u8a8d","a8082aec":"\u30ea\u30fc\u30c0\u30fc\u30dc\u30fc\u30c9\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c","a372f717":"\u5b66\u7fd2","6f9a7a57":"# \u30af\u30a4\u30baAI\u738b BERT\u5b66\u7fd2notebook","9eed9deb":"\u30e2\u30c7\u30eb\u3068Optimizer\u306e\u5b9a\u7fa9","e2b36174":"DataLoader\u306e\u5b9a\u7fa9"}}