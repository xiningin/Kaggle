{"cell_type":{"db41422b":"code","bd358fc0":"code","df220818":"code","99339531":"code","3095f6b7":"code","a0bb7f4d":"code","f54ad73d":"code","d2ae40e9":"code","2ba71914":"code","02da193a":"code","246b86a0":"code","9dbea477":"code","00fe3825":"code","70174d28":"code","ab2ed39d":"code","44c8f9fd":"code","59f3d54b":"code","05e210ad":"code","a8b6a2bf":"code","7c9a051e":"code","8ea2ec07":"code","57ccb0ef":"code","d3da2cd8":"code","ee6e60a6":"code","e4d80ce8":"code","2d55223d":"markdown","e952f664":"markdown","88434ec5":"markdown","8bf56626":"markdown","2768d30e":"markdown"},"source":{"db41422b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pydicom\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,PowerTransformer","bd358fc0":"main_dir = '..\/input\/osic-pulmonary-fibrosis-progression'\n\ntrain_files = tf.io.gfile.glob(main_dir+\"\/train\/*\/*\")\ntest_files = tf.io.gfile.glob(main_dir+\"\/test\/*\/*\")\n\nsample_sub = pd.read_csv(main_dir+'\/sample_submission.csv')\ntrain = pd.read_csv(main_dir + \"\/train.csv\")\ntest = pd.read_csv(main_dir + \"\/test.csv\")\n\nprint (\"Number of train patients: {}\\nNumber of test patients: {:4}\"\n       .format(train.Patient.nunique(), test.Patient.nunique()))\n\nprint (\"\\nTotal number of Train patient records: {}\\nTotal number of Test patient records: {:6}\"\n       .format(len(train_files), len(test_files)))\n\ntrain.shape, test.shape, sample_sub.shape","df220818":"def laplace_log_likelihood(y_true, y_pred, sigma=70):\n    # values smaller than 70 are clipped\n    sigma_clipped = tf.maximum(sigma, 70)\n\n    # errors greater than 1000 are clipped\n    delta_clipped = tf.minimum(tf.abs(y_true - y_pred), 1000)\n    \n    # type cast them suitably\n    delta_clipped = tf.cast(delta_clipped, dtype=tf.float32)\n    sigma_clipped = tf.cast(sigma_clipped, dtype=tf.float32)\n    \n    # score function\n    score = - tf.sqrt(2.0) * delta_clipped \/ sigma_clipped - tf.math.log(tf.sqrt(2.0) * sigma_clipped)\n    \n    return tf.reduce_mean(score)","99339531":"# This will be the perfect score when actual and predicted values are exactly same\nlaplace_log_likelihood(train['FVC'], train['FVC'], 70)","3095f6b7":"train.head()","a0bb7f4d":"test","f54ad73d":"# Using Weeks, Age, Sex and Smoking Status columns from train data\nX = train[['Weeks','Age','Sex','SmokingStatus']].copy()\ny = train['FVC'].copy()\n\n# save the stats for future use\nstats = X.describe().T\n\n# One hot encoding on Sex and SmokingStatus columns\nX = pd.get_dummies(X, columns =['Sex','SmokingStatus'],drop_first=True)\n\n#Scaling numeric features \n# scaling the numeric features\nfor col in ['Weeks', 'Age']:\n    X[col] = (X[col] - stats.loc[col, 'min']) \/ (stats.loc[col, 'max'] - stats.loc[col, 'min'])","d2ae40e9":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import make_scorer","2ba71914":"sigma = 250","02da193a":"# Creating a scorer function\nl1 = (make_scorer(\n    lambda X,y : laplace_log_likelihood(X,y,sigma=sigma).numpy(),\n    greater_is_better=False))\n\ncross_val_score(LinearRegression(),X,y,cv=3,scoring=l1)","246b86a0":"X = train.copy()\ny = train['FVC'].copy()\n\nX['base_week'] = X.groupby('Patient')['Weeks'].transform('min')\nX['base_FVC'] = X.groupby('Patient')['FVC'].transform('first')\n\n# save the stats for future use\nstats = X.describe().T\n\n# one hot encoding for categorcial features\nX = pd.get_dummies(data=X, columns=['Sex','SmokingStatus'], drop_first=True)\n\n# Scaling numeric columns\nnum_cols = ['Age','Weeks','base_week','base_FVC']\n\n# Min-max scaling\nfor col in num_cols:\n    X[col] = (X[col]-stats.loc[col,'min']) \/ (stats.loc[col,'max'] - stats.loc[col,'min'])\n    \n# printing the correlation of all features with FVC\nprint(X.corr()['FVC'].abs().sort_values(ascending=False)[1:])","9dbea477":"# removing unnecesary columns after transformations\nX.drop(['Patient','Percent','FVC'], axis=1, inplace=True)\nX.head()","00fe3825":"# Checking the score on transformed data now\ncross_val_score(LinearRegression(),X,y,cv=3,scoring=l1)","70174d28":"# fit on the train dataset\nlr = LinearRegression().fit(X, y)","ab2ed39d":"# Processing submission file\nsub = sample_sub.Patient_Week.str.extract(\"(ID\\w+)_(\\-?\\d+)\").rename({0: \"Patient\", 1: \"Weeks\"}, axis=1)\nsub['Weeks'] = sub['Weeks'].astype(int)\nsub = pd.merge(sub, test[['Patient', 'Sex', 'SmokingStatus']], on='Patient')\nsub.head()","44c8f9fd":"week_temp = train.groupby([\"Weeks\", 'Sex'])['FVC'].median()\nsex_temp = train.groupby(['Sex'])['FVC'].median()\n\nfor index, week, sex in sub.iloc[:, 1:3].itertuples():\n    if (week, sex) in week_temp:\n        # we assume we are more accurate here\n        sub.loc[index, 'FVC'] = week_temp[week, sex]\n        sub.loc[index, 'Confidence'] = sigma\n    else:\n        # we assume we are less accurate here, boost confidence\n        sub.loc[index, 'FVC'] = sex_temp[sex]\n        sub.loc[index, 'Confidence'] = sigma + 100\n        \nsub.sample(5)","59f3d54b":"# swelling confidence as progress in the weeks\nsub[\"Patient_Week\"] = sub.Patient + \"_\" + sub.Weeks.astype(str)\nsub.head()","05e210ad":"x = (sub.drop(['Confidence', 'Patient_Week'], 1)\n     .merge(test[['Patient', 'Weeks', 'FVC', 'Age']], on='Patient')\n     .rename({\"Weeks_y\": \"base_Week\", \"FVC_y\": \"Base_FVC\", \"Weeks_x\": \"Weeks\"}, axis=1)\n     .drop(['Patient', 'FVC_x'], axis=1))\n\n# one hot encoding, We set drop_first as \n# false to ensure the test is same as train\nx = pd.get_dummies(x, columns=['Sex', 'SmokingStatus'])\n\n# # scaling the numeric features\n#for col in ['Weeks', 'Age', 'base_Week', 'Base_FVC']:\n#    x[col] = (x[col] - stats.loc[col, 'min']) \/ (stats.loc[col, 'max'] - stats.loc[col, 'min'])\n    \nnum_cols = ['Weeks', 'Age', 'base_Week', 'Base_FVC']\nscaler = StandardScaler()\nscaler.fit(x[num_cols])\n\nx = pd.concat([x[['Sex_Male','SmokingStatus_Ex-smoker', 'SmokingStatus_Never smoked']].reset_index(drop=True),\n                pd.DataFrame(scaler.transform(x[num_cols]),columns=num_cols)],axis=1)\n    \n\nx = x[['Weeks', 'Age', 'base_Week', 'Base_FVC', 'Sex_Male',\n   'SmokingStatus_Ex-smoker', 'SmokingStatus_Never smoked']]\n\nx.head()","a8b6a2bf":"sub['FVC'] = lr.predict(x)\nsub.head()","7c9a051e":"# LR submission\n#sub[['Patient_Week', 'FVC', 'Confidence']].to_csv(\"submission.csv\", index=False)","8ea2ec07":"x = train.copy()\n\n# Create base_Week, Base_FVC and Base_Percent for train\ntemp = (x.groupby(\"Patient\")\n        .apply(lambda x: x.loc[int(\n            np.percentile(x['Weeks'].index, q=25)\n        ), [\"Weeks\", \"FVC\", \"Percent\"]]))\n\ntemp.rename(\n    {\"Weeks\": \"Base_Week\", \n     \"FVC\": \"Base_FVC\", \n     \"Percent\": \"Base_Percent\"}, \n    axis=1, inplace=True)\n\n# merge it with train data\nx = x.merge(temp, on='Patient')\nx['Where'] = 'train'\n\n# merge the test dataset as well to be able to handle 1hC\ntemp = sub[['Patient', 'Weeks']].merge(\n    test.rename({\"Weeks\": \"Base_Week\", \n                 \"FVC\": \"Base_FVC\", \n                 \"Percent\": \"Base_Percent\"}, axis=1), \n    on='Patient')\n\n# concatente to the train dataset\ntemp['Where'] = 'test'\nx = pd.concat([x, temp], axis=0)\n\n# create week offsets\nx['Week_Offset'] = x['Weeks'] - x['Base_Week']\n\n# oridinal encode categorical values\nx['Sex'] = x['Sex'].map({\"Male\": 1, \"Female\": 0})\nx['SmokingStatus'] = x['SmokingStatus'].map({\"Ex-smoker\": 0, \"Never smoked\": 1, \"Currently smokes\": 2})\n\n# one hot encoding\nx = pd.get_dummies(x, columns=['Sex', 'SmokingStatus'], drop_first=True)\n\n# binned FVC does better?\nx['Bin_base_FVC'] = pd.cut(x['Base_FVC'], bins=range(0, 7501, 500)).cat.codes \/ 15\n\n# lets scale the numeric columns (We scale it with max possibe values)\nnum_cols = ['Weeks', 'Week_Offset', 'Base_Week', 'Age', 'Base_FVC', 'Percent', 'Base_Percent']\nfor col in num_cols:\n    x[col] = (x[col] - x[col].min()) \/ (x[col].max() - x[col].min())\n\nto_drop = (\n    [\"FVC\", 'Percent']\n    \n    + [\n#         \"Base_FVC\", \n#         'Base_Week', \n#         'Weeks', \n#         'Bin_base_FVC', \n#         'Base_Percent'\n    ] + \n    \n    ['Patient']\n)\n\n# print out how well our features would do\nprint (x[x.Where == 'train'].corr()['FVC'].abs().sort_values(ascending=False).drop(to_drop[:-1]))\n\ny = x['FVC'].dropna()\nx = x.drop(to_drop, axis=1)\n\nx.head()","57ccb0ef":"from sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\"SFromModel__k\": range(10)}\n\ntemp = Pipeline(\n    [(\"SFromModel\", SelectKBest(score_func=f_regression)),\n    (\"Model\", LinearRegression())])\n\ngrid = GridSearchCV(temp, param_grid=grid_params, n_jobs=-1, cv=3, scoring=l1)\ngrid.fit(x[x.Where == 'train'].drop('Where', 1), y)\n\nprint (grid.best_params_, grid.best_score_)\nmodel = grid.best_estimator_","d3da2cd8":"best_score = (0, np.inf, np.inf)\nfor i in range(50, 1500, 50):\n    sigma=i\n    temp = cross_val_score(model, x[x.Where == 'train'].drop('Where', 1), y, cv=3, scoring=l1)\n    if best_score[1] > temp.mean():\n        best_score = i, temp.mean(), temp.std()\n        \nsigma = best_score[0]\nbest_score","ee6e60a6":"lr = LinearRegression().fit(x[x.Where == 'train'].drop('Where', 1), y)\nsub['FVC'] = lr.predict(x[x.Where == 'test'].drop('Where', 1))\nsub.head()","e4d80ce8":"# LR submission\nsub['Confidence'] = best_score[0]\nsub[['Patient_Week', 'FVC', 'Confidence']].to_csv(\"submission.csv\", index=False)","2d55223d":"## Function to calculate Metrics","e952f664":"### Making Prediction on Test Data","88434ec5":"# Loading the Data","8bf56626":"As we can see, the fold scores have improved significantly. Lets fit the Linear model on this data.","2768d30e":"# Importing Libraries"}}