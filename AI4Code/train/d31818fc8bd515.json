{"cell_type":{"885b6972":"code","96b1f511":"code","538fd6ba":"code","d042acea":"code","b7f2176d":"code","d7864b29":"code","a8dc0d18":"code","cee8bbd3":"code","57f352dd":"code","5d8b1ce1":"code","1cc8f651":"code","1be0b42d":"code","ff5ea1d9":"code","26b189f5":"code","ca6f24c8":"code","cd7383de":"code","4d184d30":"code","862a0bd3":"markdown"},"source":{"885b6972":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96b1f511":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend\n#import keras.backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers, activations\nimport os\nimport pathlib\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.models import load_model\n#import face_recognition\nimport random\nfrom tensorflow.keras.models import model_from_json\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\n\n#import itertools\n#import shutil\n%matplotlib inline","538fd6ba":"def image_preprocessing():\n    image_text_open = open('\/kaggle\/input\/dataset_B_FacialImages\/EyeCoordinatesInfo_OpenFace.txt','r')\n    image_text_closed = open('\/kaggle\/input\/dataset_B_FacialImages\/EyeCoordinatesInfo_ClosedFace.txt','r')\n    image_text_open_ = [x.split(' ')[0] for x in image_text_open]\n    image_text_closed_ = [x.split(' ')[0] for x in image_text_closed]\n\n    image_info_closed = [ os.path.join('\/kaggle\/input\/dataset_B_FacialImages\/ClosedFace',b) for b in image_text_closed_] \n    image_info_open =  [ os.path.join('\/kaggle\/input\/dataset_B_FacialImages\/OpenFace',b) for b in image_text_open_] \n    image_final = image_info_closed + image_info_open\n\n    y = [str(0) if x in image_info_closed else str(1) for x in image_final]\n\n    image_data_input = [ cv2.imread(x) for x in image_final]\n    image_data_input = [i\/255 for i in image_data_input]\n\n    X_train, X_test, y_train, y_test = train_test_split(image_data_input, y, test_size=0.33, random_state=0)\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    image_df = pd.DataFrame({'Paths': image_final,'target': y})\n    image_df = image_df.sample(frac=1).reset_index(drop=True)\n    train_image_df = image_df.iloc[:1623,:]\n    test_image_df = image_df.iloc[1623:,:]\n    \n    return (train_image_df,test_image_df)","d042acea":"### CONVOLUTIONAL RESNET BLOCK ###\n\ndef convolutional_block(x,numfilt,filtsz,strides=1,pad='same',act=True,name=None):\n    x = Conv2D(numfilt,filtsz,strides,padding=pad,data_format='channels_last',kernel_initializer = 'he_normal',use_bias=False,name=name+'conv2d')(x)\n    x = BatchNormalization(axis=3,scale=False,name=name+'conv2d'+'bn')(x)\n    if act:\n        x = Activation('relu',name=name+'conv2d'+'act')(x)\n    return x","b7f2176d":"def incresA(x,scale,name=None):\n    pad = 'same'\n    branch0 = convolutional_block(x,32,1,1,pad,True,name=name+'b0')\n    branch1 = convolutional_block(x,32,1,1,pad,True,name=name+'b1_1')\n    branch1 = convolutional_block(branch1,32,3,1,pad,True,name=name+'b1_2')\n    branch2 = convolutional_block(x,32,1,1,pad,True,name=name+'b2_1')\n    branch2 = convolutional_block(branch2,48,3,1,pad,True,name=name+'b2_2')\n    branch2 = convolutional_block(branch2,64,3,1,pad,True,name=name+'b2_3')\n    branches = [branch0,branch1,branch2]\n    mixed = Concatenate(axis=3, name=name + '_concat')(branches)\n    filt_exp_1x1 = convolutional_block(mixed,384,1,1,pad,False,name=name+'filt_exp_1x1')\n    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n                      output_shape=backend.int_shape(x)[1:],\n                      arguments={'scale': scale},\n                      name=name+'act_scaling')([x, filt_exp_1x1])\n    return final_lay\n\ndef incresB(x,scale,name=None):\n    pad = 'same'\n    branch0 = convolutional_block(x,192,1,1,pad,True,name=name+'b0')\n    branch1 = convolutional_block(x,128,1,1,pad,True,name=name+'b1_1')\n    branch1 = convolutional_block(branch1,160,[1,7],1,pad,True,name=name+'b1_2')\n    branch1 = convolutional_block(branch1,192,[7,1],1,pad,True,name=name+'b1_3')\n    branches = [branch0,branch1]\n    mixed = Concatenate(axis=3, name=name + '_mixed')(branches)\n    filt_exp_1x1 = convolutional_block(mixed,1152,1,1,pad,False,name=name+'filt_exp_1x1')\n    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n                      output_shape=backend.int_shape(x)[1:],\n                      arguments={'scale': scale},\n                      name=name+'act_scaling')([x, filt_exp_1x1])\n    return final_lay\n\ndef incresC(x,scale,name=None):\n    pad = 'same'\n    branch0 = convolutional_block(x,192,1,1,pad,True,name=name+'b0')\n    branch1 = convolutional_block(x,192,1,1,pad,True,name=name+'b1_1')\n    branch1 = convolutional_block(branch1,224,[1,3],1,pad,True,name=name+'b1_2')\n    branch1 = convolutional_block(branch1,256,[3,1],1,pad,True,name=name+'b1_3')\n    branches = [branch0,branch1]\n    mixed = Concatenate(axis=3, name=name + '_mixed')(branches)\n    filt_exp_1x1 = convolutional_block(mixed,2048,1,1,pad,False,name=name+'fin1x1')\n    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n                      output_shape=backend.int_shape(x)[1:],\n                      arguments={'scale': scale},\n                      name=name+'act_scaling')([x, filt_exp_1x1])\n    return final_lay\n\n","d7864b29":"#### ResNet NETWORK ## \ndef resNet(x):\n#Inception-ResNet-A modules\n    x = incresA(x,0.15,name='incresA_1')\n    x = incresA(x,0.15,name='incresA_2')\n    x = incresA(x,0.15,name='incresA_3')\n    x = incresA(x,0.15,name='incresA_4')\n\n#35 \u00d7 35 to 17 \u00d7 17 reduction module.\n    x_red_11 = MaxPooling2D(3,strides=2,padding='valid',name='red_maxpool_1')(x)\n\n    x_red_12 = convolutional_block(x,384,3,2,'valid',True,name='x_red1_c1')\n\n    x_red_13 = convolutional_block(x,256,1,1,'same',True,name='x_red1_c2_1')\n    x_red_13 = convolutional_block(x_red_13,256,3,1,'same',True,name='x_red1_c2_2')\n    x_red_13 = convolutional_block(x_red_13,384,3,2,'valid',True,name='x_red1_c2_3')\n\n    x = Concatenate(axis=3, name='red_concat_1')([x_red_11,x_red_12,x_red_13])\n\n    #Inception-ResNet-B modules\n    x = incresB(x,0.1,name='incresB_1')\n    x = incresB(x,0.1,name='incresB_2')\n    x = incresB(x,0.1,name='incresB_3')\n    x = incresB(x,0.1,name='incresB_4')\n    x = incresB(x,0.1,name='incresB_5')\n    x = incresB(x,0.1,name='incresB_6')\n    x = incresB(x,0.1,name='incresB_7')\n\n    #17 \u00d7 17 to 8 \u00d7 8 reduction module.\n    x_red_21 = MaxPooling2D(3,strides=2,padding='valid',name='red_maxpool_2')(x)\n\n    x_red_22 = convolutional_block(x,256,1,1,'same',True,name='x_red2_c11')\n    x_red_22 = convolutional_block(x_red_22,384,3,2,'valid',True,name='x_red2_c12')\n\n    x_red_23 = convolutional_block(x,256,1,1,'same',True,name='x_red2_c21')\n    x_red_23 = convolutional_block(x_red_23,256,3,2,'valid',True,name='x_red2_c22')\n\n    x_red_24 = convolutional_block(x,256,1,1,'same',True,name='x_red2_c31')\n    x_red_24 = convolutional_block(x_red_24,256,3,1,'same',True,name='x_red2_c32')\n    x_red_24 = convolutional_block(x_red_24,256,3,2,'valid',True,name='x_red2_c33')\n\n    x = Concatenate(axis=3, name='red_concat_2')([x_red_21,x_red_22,x_red_23,x_red_24])\n\n    #Inception-ResNet-C modules\n    x = incresC(x,0.2,name='incresC_1')\n    x = incresC(x,0.2,name='incresC_2')\n    x = incresC(x,0.2,name='incresC_3')\n\n    x = GlobalAveragePooling2D(data_format='channels_last')(x)\n#   x = Dropout(0.1)(x)\n    x = Activation('sigmoid')(x)\n    \n    \n    return x\n\n","a8dc0d18":"## STEM BLOCK #\ndef stem(img_input):\n\n    x = convolutional_block(img_input,32,3,2,'valid',True,name='conv1')\n    x = convolutional_block(x,32,3,1,'valid',True,name='conv2')\n    x = convolutional_block(x,64,3,1,'valid',True,name='conv3')\n\n    x_11 = MaxPooling2D(3,strides=1,padding='valid',name='stem_br_11'+'_maxpool_1')(x)\n    x_12 = convolutional_block(x,64,3,1,'valid',True,name='stem_br_12')\n\n    x = Concatenate(axis=3, name = 'stem_concat_1')([x_11,x_12])\n\n    x_21 = convolutional_block(x,64,1,1,'same',True,name='stem_br_211')\n    x_21 = convolutional_block(x_21,64,[1,7],1,'same',True,name='stem_br_212')\n    x_21 = convolutional_block(x_21,64,[7,1],1,'same',True,name='stem_br_213')\n    x_21 = convolutional_block(x_21,96,3,1,'valid',True,name='stem_br_214')\n\n    x_22 = convolutional_block(x,64,1,1,'same',True,name='stem_br_221')\n    x_22 = convolutional_block(x_22,96,3,1,'valid',True,name='stem_br_222')\n\n    x = Concatenate(axis=3, name = 'stem_concat_2')([x_21,x_22])\n\n    x_31 = convolutional_block(x,192,3,1,'valid',True,name='stem_br_31')\n    x_32 = MaxPooling2D(3,strides=1,padding='valid',name='stem_br_32'+'_maxpool_2')(x)\n    x = Concatenate(axis=3, name = 'stem_concat_3')([x_31,x_32])\n    \n    return x\n","cee8bbd3":"### Custom accuracy ###\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.ops import array_ops\n    \ndef new_sparse_categorical_accuracy(y_true, y_pred):\n    y_pred_rank = ops.convert_to_tensor(y_pred).get_shape().ndims\n    y_true_rank = ops.convert_to_tensor(y_true).get_shape().ndims\n    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n        y_true = array_ops.squeeze(y_true, [-1])\n    y_pred = math_ops.argmax(y_pred, axis=-1)\n    # If the predicted output and actual output types don't match, force cast them\n    # to match.\n    if K.dtype(y_pred) != K.dtype(y_true):\n        y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())","57f352dd":"def save_model(model):\n    \n    model_json = model.to_json()\n    with open(\"model.json\", \"w\") as json_file:\n        json_file.write(model_json)\n    # serialize weights to HDF5\n    model.save_weights(\"model.h5\")\n\ndef augment_data(train_df,test_df):\n    tr_datagen =  ImageDataGenerator(rescale=1.0\/255, \n                                 horizontal_flip=True,\n                                 vertical_flip=True)\n\n    ts_datagen = ImageDataGenerator(rescale=1.0\/255)\n\n    train_gen = tr_datagen.flow_from_dataframe(train_df,x_col = 'Paths', y_col = 'target',\n                                target_size=(100,100),\n                                batch_size=32,\n                                class_mode=\"binary\")\n\n    test_gen = ts_datagen.flow_from_dataframe(test_df,x_col = 'Paths', y_col = 'target',\n                               target_size=(100,100),\n                               batch_size=32,\n                               class_mode=\"binary\",\n                               shuffle=False)\n    return train_gen,test_gen\n\ndef train_model(train_gen,lr,loss,metric,epochs):\n    \n    ##COMPILE MODEL ##\n    img_input = Input(shape=(100,100,3))\n    x = stem(img_input)\n    output = resNet(x)\n    model=Model(img_input,output)\n    opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=True)\n    model.compile(optimizer = opt ,loss = loss,metrics = [metric])\n    \n    \n#     filepath = 'model.h5'\n#     checkpoint = ModelCheckpoint(filepath, monitor='val_acc', \n#                         verbose=1, save_best_only=True, mode='max')\n\n#     early = EarlyStopping(monitor=\"val_loss\", \n#                       mode=\"min\", \n#                       patience=5, restore_best_weights=True)\n#     callbacks_list = [checkpoint, early]\n    \n    \n    # ## Fitting the model\n    model.fit_generator(train_gen,epochs = epochs,verbose = 1)\n    \n    save_model(model)\n    \n    return model\n    \n    \n    \ndef predict(img, model):\n    \n    img = Image.fromarray(img, 'RGB').convert('L')\n    img = imresize(img, (IMG_SIZE,IMG_SIZE)).astype('float32')\n    img \/= 255\n    img = img.reshape(1,IMG_SIZE,IMG_SIZE,1)\n    prediction = model.predict(img)\n    if prediction < 0.1:\n        prediction = 'closed'\n    elif prediction > 0.9:\n        prediction = 'open'\n    else:\n        prediction = 'idk'\n    return prediction\n\n\n\n\n    \n    \n","5d8b1ce1":"train_image_df,val_image_df = image_preprocessing()\ntrain_gen,test_gen = augment_data(train_image_df,val_image_df)\nmodel_ = train_model(train_gen,0.01,'sparse_categorical_crossentropy','accuracy',40)","1cc8f651":"def load_model():\n    json_file = open('model.json', 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    loaded_model = model_from_json(loaded_model_json)\n# load weights into new model\n    loaded_model.load_weights(\"model.h5\")\n    loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return loaded_model","1be0b42d":"def evaluate(model,test_gen,steps):\n    print('Evaluate model')\n    loss, acc = model.evaluate_generator(test_gen,steps = steps,verbose = 1)\n    print(acc * 100)","ff5ea1d9":"evaluate(model_,test_gen,800)","26b189f5":"#Image encoding\ndef process_and_encode(images):\n    known_encodings = []\n    known_names = []\n    print(\"[LOG] Encoding dataset ...\")\n\n    for image_path in tqdm(images):\n        # Load image\n        image = cv2.imread(image_path)\n        # Convert it from BGR to RGB\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n     \n        # detect face in the image and get its location (square boxes coordinates)\n        boxes = face_recognition.face_locations(image, model='hog')\n\n        # Encode the face into a 128-d embeddings vector\n        encoding = face_recognition.face_encodings(image, boxes)\n\n        # the person's name is the name of the folder where the image comes from\n        name = image_path.split(' ')[0]\n\n        if len(encoding) > 0 : \n            known_encodings.append(encoding[0])\n            known_names.append(name)\n\n    return {\"encodings\": known_encodings, \"names\": known_names}\n","ca6f24c8":"def init():\n    face_cascPath = 'haarcascade_frontalface_alt.xml'\n    # face_cascPath = 'lbpcascade_frontalface.xml'\n\n    open_eye_cascPath = 'haarcascade_eye_tree_eyeglasses.xml'\n    left_eye_cascPath = 'haarcascade_lefteye_2splits.xml'\n    right_eye_cascPath ='haarcascade_righteye_2splits.xml'\n    dataset = 'faces'\n\n    face_detector = cv2.CascadeClassifier(face_cascPath)\n    open_eyes_detector = cv2.CascadeClassifier(open_eye_cascPath)\n    left_eye_detector = cv2.CascadeClassifier(left_eye_cascPath)\n    right_eye_detector = cv2.CascadeClassifier(right_eye_cascPath)\n\n    print(\"[LOG] Opening webcam ...\")\n    video_capture = VideoStream(src=0).start()\n\n    model = load_model()\n\n\n    print(\"[LOG] Collecting images ...\")\n    images = []\n    for direc, _, files in tqdm(os.walk(dataset)):\n        for file in files:\n            if file.endswith(\"jpg\"):\n                images.append(os.path.join(direc,file))\n    return (model,face_detector, open_eyes_detector, left_eye_detector,right_eye_detector, video_capture, images) ","cd7383de":"def isBlinking(history, maxFrames):\n    \"\"\" @history: A string containing the history of eyes status \n         where a '1' means that the eyes were closed and '0' open.\n        @maxFrames: The maximal number of successive frames where an eye is closed \"\"\"\n    for i in range(maxFrames):\n        pattern = '1' + '0'*(i+1) + '1'\n        if pattern in history:\n            return True\n    return False","4d184d30":"\ndef detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected):\n        frame = video_capture.read()\n        # resize the frame\n        frame = cv2.resize(frame, (0, 0), fx=0.6, fy=0.6)\n\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        # Detect faces\n        faces = face_detector.detectMultiScale(\n            gray,\n            scaleFactor=1.2,\n            minNeighbors=5,\n            minSize=(50, 50),\n            flags=cv2.CASCADE_SCALE_IMAGE\n        )\n\n        # for each detected face\n        for (x,y,w,h) in faces:\n            # Encode the face into a 128-d embeddings vector\n            encoding = face_recognition.face_encodings(rgb, [(y, x+w, y+h, x)])[0]\n\n            # Compare the vector with all known faces encodings\n            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n\n            # For now we don't know the person name\n            name = \"Unknown\"\n\n            # If there is at least one match:\n            if True in matches:\n                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n                counts = {}\n                for i in matchedIdxs:\n                    name = data[\"names\"][i]\n                    counts[name] = counts.get(name, 0) + 1\n\n                # The known encoding with the most number of matches corresponds to the detected face name\n                name = max(counts, key=counts.get)\n\n            face = frame[y:y+h,x:x+w]\n            gray_face = gray[y:y+h,x:x+w]\n\n            eyes = []\n            \n            # Eyes detection\n            # check first if eyes are open (with glasses taking into account)\n            open_eyes_glasses = open_eyes_detector.detectMultiScale(\n                gray_face,\n                scaleFactor=1.1,\n                minNeighbors=5,\n                minSize=(30, 30),\n                flags = cv2.CASCADE_SCALE_IMAGE\n            )\n            # if open_eyes_glasses detect eyes then they are open \n            if len(open_eyes_glasses) == 2:\n                eyes_detected[name]+='1'\n                for (ex,ey,ew,eh) in open_eyes_glasses:\n                    cv2.rectangle(face,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n            \n            # otherwise try detecting eyes using left and right_eye_detector\n            # which can detect open and closed eyes                \n            else:\n                # separate the face into left and right sides\n                left_face = frame[y:y+h, x+int(w\/2):x+w]\n                left_face_gray = gray[y:y+h, x+int(w\/2):x+w]\n\n                right_face = frame[y:y+h, x:x+int(w\/2)]\n                right_face_gray = gray[y:y+h, x:x+int(w\/2)]\n\n                # Detect the left eye\n                left_eye = left_eye_detector.detectMultiScale(\n                    left_face_gray,\n                    scaleFactor=1.1,\n                    minNeighbors=5,\n                    minSize=(30, 30),\n                    flags = cv2.CASCADE_SCALE_IMAGE\n                )\n\n                # Detect the right eye\n                right_eye = right_eye_detector.detectMultiScale(\n                    right_face_gray,\n                    scaleFactor=1.1,\n                    minNeighbors=5,\n                    minSize=(30, 30),\n                    flags = cv2.CASCADE_SCALE_IMAGE\n                )\n\n                eye_status = '1' # we suppose the eyes are open\n\n                # For each eye check wether the eye is closed.\n                # If one is closed we conclude the eyes are closed\n                for (ex,ey,ew,eh) in right_eye:\n                    color = (0,255,0)\n                    pred = predict(right_face[ey:ey+eh,ex:ex+ew],model)\n                    if pred == 'closed':\n                        eye_status='0'\n                        color = (0,0,255)\n                    cv2.rectangle(right_face,(ex,ey),(ex+ew,ey+eh),color,2)\n                for (ex,ey,ew,eh) in left_eye:\n                    color = (0,255,0)\n                    pred = predict(left_face[ey:ey+eh,ex:ex+ew],model)\n                    if pred == 'closed':\n                        eye_status='0'\n                        color = (0,0,255)\n                    cv2.rectangle(left_face,(ex,ey),(ex+ew,ey+eh),color,2)\n                eyes_detected[name] += eye_status\n\n            # Each time, we check if the person has blinked\n            # If yes, we display its name\n            if isBlinking(eyes_detected[name],3):\n                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n                # Display name\n                y = y - 15 if y - 15 > 15 else y + 15\n                cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n\n        return frame","862a0bd3":"# #https:\/\/towardsdatascience.com\/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3\n    \n #   https:\/\/github.com\/sumantrajoshi\/Face-recognition-using-deep-learning\/blob\/master\/BasicFaceRecognizerUsingWebcam.ipynb\n\n#https:\/\/medium.com\/@mannasiladittya\/building-inception-resnet-v2-in-keras-from-scratch-a3546c4d93f0\n\n#"}}