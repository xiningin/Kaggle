{"cell_type":{"70beeba3":"code","c1e7418f":"code","7b0688bc":"code","139fbde7":"code","7ba69b04":"code","33953383":"code","e08035d2":"code","6b755d39":"code","01b4e8e2":"code","33c53b4b":"code","3995d146":"code","6d21a62e":"code","d3abfe6f":"code","ddab369f":"code","68303907":"code","4b164fc0":"code","75e50629":"code","089820e2":"code","fe306634":"code","e7b6c39c":"code","b4d4ae5d":"code","dd4a80e7":"code","7f740846":"code","508f7fba":"code","aad96ad5":"code","41c326c3":"code","ff43d7ec":"code","2941becf":"code","759b2a79":"code","43c01a61":"code","0be76dea":"code","5e974af8":"code","47cb0786":"code","a67f6b24":"code","5f6c66fc":"code","c690e2b1":"code","afe163e9":"code","197373eb":"code","c2ad1417":"code","ebbc9976":"code","f6e02233":"code","c2e4ce17":"code","53ef92a3":"code","3dcda3de":"code","4b1027c9":"code","e72a6d49":"code","8e25695c":"code","00658bab":"code","96423708":"code","4f90c161":"code","b59c015b":"code","f4cfc3fc":"code","c1c05252":"code","ebfac135":"code","3a7b5dae":"code","7ef89eaf":"code","a3396d5e":"code","d323a3fb":"code","f3aeb708":"code","2d13f5f5":"code","369f12a6":"code","9b810c81":"code","3524ce33":"code","783e2024":"code","67e7b3bc":"code","9d7b95ef":"code","81bab191":"code","0bd4a776":"code","c180845c":"code","94b92cbe":"code","62dab0f0":"code","7d70dc3e":"code","33183a83":"code","c8a8480e":"code","cd60fe24":"code","ebd0a0d4":"code","6a15d51d":"code","2ac4a310":"code","36eb4b2c":"code","e27be470":"code","7d23ef16":"code","ab64bc6e":"code","485ff788":"code","c6315b1b":"code","b380845a":"code","711cf57e":"code","3dae83ee":"code","66515311":"code","9ea3a0d6":"code","3c9c6265":"code","ea5324c4":"code","da45fdfa":"code","555f1b2a":"code","c0e07225":"code","d37fc1a1":"code","108f859c":"markdown","4e051d5e":"markdown","f308ea52":"markdown","5afb6e87":"markdown","13d321aa":"markdown","2965dca5":"markdown","3f15cc51":"markdown","67affb8a":"markdown","d1c280f7":"markdown","f85b03be":"markdown","aa1d51bc":"markdown","15d3f743":"markdown","bb08435d":"markdown","d3a14a99":"markdown","2121756f":"markdown","67f51967":"markdown","f08297b1":"markdown","f2093b43":"markdown","9a47d473":"markdown","ebf0c3c3":"markdown","06433869":"markdown","77de1060":"markdown","9c85df1e":"markdown","eabc7bde":"markdown","0b55fa8b":"markdown","6d662bf4":"markdown","0a6ec383":"markdown","936debdc":"markdown","e08e8523":"markdown","20a7c728":"markdown","656629d1":"markdown","f35dfebf":"markdown","5f5019eb":"markdown","5b7ec42a":"markdown","9859054d":"markdown","2be7bf24":"markdown","2f1ffd68":"markdown","bd061193":"markdown","aba8ec17":"markdown","8c5fe54e":"markdown","db4ad707":"markdown","928e8456":"markdown","7515b6d0":"markdown","3dcdbb80":"markdown","b9a68539":"markdown","ee19f779":"markdown","ea36ee5b":"markdown","59cd42c6":"markdown","916c4e00":"markdown","f66eff74":"markdown","a2d6883f":"markdown","033b50b3":"markdown","082b926a":"markdown","8d89c111":"markdown","9cca7b91":"markdown","80f48725":"markdown","75f174a2":"markdown","b84763ae":"markdown","464422af":"markdown","d768f935":"markdown","551eb872":"markdown","c643c639":"markdown","9f4f757e":"markdown","2549e170":"markdown","06398a53":"markdown","4e8ef228":"markdown","08c2320c":"markdown"},"source":{"70beeba3":"# have to install any python packages that are missing but all necessary packages are installed\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","c1e7418f":"import os\nos.getcwd()\nos.chdir('C:\\\\Users\\\\sexto\\\\titaniccontest')\nos.getcwd()","7b0688bc":"# Make code as reproducible as possible which includes where data was downloaded from.\n# reading in data with the web link set to a variable is causing problems \n\n# Instead, manually downloaded titanic data from kaggle website into working directory\n# Data downloaded from: https:\/\/www.kaggle.com\/c\/titanic\/data\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')","139fbde7":"# training dataframe\n# size \nsizeTrain = train_df.size\nsizeTest = test_df.size\n  \n# shape \nshapeTrain = train_df.shape \nshapeTest = test_df.shape\n\n# printing size and shape \nprint(\"SizeTrain = {}\\nShapeTrain = {}\".\nformat(sizeTrain, shapeTrain))\nprint(\"SizeTest = {}\\nShapeTest = {}\".\nformat(sizeTest, shapeTest))","7ba69b04":"train_df.head()","33953383":"test_df.head()","e08035d2":"train_df.tail()","6b755d39":"train_df.describe()","01b4e8e2":"train_df.info()\nprint('_'*40)\ntest_df.info()","33c53b4b":"# Combine train and test dataset by adding Survived column to test dataset and with NaN\n# Combine them so can clean data and feature engineer\ncombined_df =  pd.concat(objs=[train_df, test_df], axis=0, sort=False).reset_index(drop=True)\ncombined_df","3995d146":"# Select all duplicate rows based on one column\n# List first and all instance of duplicate names\nduplicateRowsName1 = combined_df[combined_df.duplicated(['Name'], keep='last')]\nduplicateRowsName2 = combined_df[combined_df.duplicated(['Name'])]\nduplicateRowsName3 = pd.concat([duplicateRowsName1, duplicateRowsName2])\nsortName = duplicateRowsName3.sort_values(by=['Name'])\nsortName","6d21a62e":"# Are there any null values?\nall = len(train_df[\"Name\"])\nprint (\"Total variables for Name are:\", all)\nnull_Name = train_df[\"Name\"].isnull().sum()\nprint(\"Missing values for Name are:\", null_Name)","d3abfe6f":"a = min(train_df[\"Name\"]), max(train_df[\"Name\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Name.dtype\nprint('Data type is:', b)\n\nc= train_df.Name.nunique()\nprint('Number of unique values is:', c)\n\n# comment out since too many unique\n#d= train_df.Name.unique()\n#print('Unique values are:', d)\n# if too many unique then print first 5 in dataframe instead\ne= train_df.filter(like='Name').head(n=5)\nprint(e)","ddab369f":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train_df[\"Name\"]]\ntrain_df[\"Title\"] = pd.Series(dataset_title)\ntrain_df[\"Title\"].head()","68303907":"# Count how many of each title there are\nsns.set(rc={'figure.figsize':(16,5)})\ng = sns.countplot(x=\"Title\",data=train_df)\nfor p in g.patches:\n    g.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\ng = plt.setp(g.get_xticklabels(), rotation=45)","4b164fc0":"# Convert to categorical values Title \ntrain_df[\"Title\"] = train_df[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_df[\"Title\"] = train_df[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":2, \"Mlle\":2, \"Mrs\":2, \"Mr\":3, \"Rare\":4})\ntrain_df[\"Title\"] = train_df[\"Title\"].astype(int)","75e50629":"ax = sns.countplot(train_df[\"Title\"], \n                   order = train_df[\"Title\"].value_counts().index)\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nax = ax.set_xticklabels([\"Mr\",\"Miss\/Ms\", \"Mme\/Mlle\/Mrs\",\"Master\",\"Rare\"])","089820e2":"x = sns.catplot(x=\"Title\",y=\"Survived\",data=train_df,kind=\"bar\", height=4, aspect=3.4, order = train_df[\"Title\"].value_counts().index)\nx = x.set_xticklabels([\"Mr\",\"Miss\/Ms\", \"Mme\/Mlle\/Mrs\",\"Master\",\"Rare\"])\nx = x.set_ylabels(\"survival probability\")","fe306634":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train_df[\"Name\"]]\ntrain_df[\"Title2\"] = pd.Series(dataset_title)","e7b6c39c":"mr_df = train_df[train_df['Title2'].isin(['Don','Rev', 'Dr', 'Major', 'Sir', 'Col', 'Capt', 'Jonkheer'])]\nsortName2 = mr_df.sort_values(by=['Title2'])\nsortName2","b4d4ae5d":"mrs_df = train_df[train_df['Title2'].isin(['Lady', 'the Countess'])]\nsortName2 = mrs_df.sort_values(by=['Title2'])\nsortName2","dd4a80e7":"# Convert to categorical values Title \ntrain_df[\"Title2\"] = train_df[\"Title2\"].replace(['Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\ntrain_df[\"Title2\"] = train_df[\"Title2\"].replace(['Lady', 'the Countess','Countess'], 'Mrs')\ntrain_df[\"Title2\"] = train_df[\"Title2\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":2, \"Mlle\":2, \"Mrs\":2, \"Mr\":3})\ntrain_df[\"Title2\"] = train_df[\"Title2\"].astype(int)","7f740846":"ax = sns.countplot(train_df[\"Title2\"], \n                   order = train_df[\"Title2\"].value_counts().index)\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nax = ax.set_xticklabels([\"Mr\",\"Miss\/Ms\", \"Mme\/Mlle\/Mrs\",\"Master\"])","508f7fba":"g = sns.catplot(x=\"Title2\",y=\"Survived\",data=train_df,kind=\"bar\", order = train_df[\"Title2\"].value_counts().index, height=4, aspect=3.4)\ng = g.set_xticklabels([\"Mr\",\"Miss\/Ms\", \"Mme\/Mlle\/Mrs\",\"Master\"])\ng = g.set_ylabels(\"survival probability\")","aad96ad5":"# Are there any missing values for Survived and Sex columns?\n# Are there any null values?\nall = len(train_df[\"Sex\"])\nprint (\"Total variables for Sex are:\", all)\nnull_Survived = train_df[\"Survived\"].isnull().sum()\nprint(\"Missing values for Survived are:\", null_Survived)\nnull_Sex = train_df[\"Sex\"].isnull().sum()\nprint(\"Missing values for Sex are:\", null_Sex)","41c326c3":"a = min(train_df[\"Sex\"]), max(train_df[\"Sex\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Sex.dtype\nprint('Data type is:', b)\n\nc= train_df.Sex.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Sex.unique()\nprint('Unique values are:', d)","ff43d7ec":"print(\"Ensure there are an adequate number of males and females who survived & didn't survive.\")\n# PassengerId was used because it has no missing values \nsextest1 = train_df[['Sex', 'Survived', 'PassengerId']].groupby(['Sex', 'Survived'], as_index=False).count()\nprint(sextest1)\nprint('The lowest number is 81 which is adequate for comparison.')","2941becf":"print('Probability for Survival for males and females:')\nsextest = train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\nprint(sextest)\n\nsns.set(rc={'figure.figsize':(7,3)})\ng = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df, palette=\"bwr\").set_title(\"Survival Probabilty by Sex\")\nplt.ylabel(\"survival probabilty\")","759b2a79":"# Are there any null values?\nall = len(train_df[\"Pclass\"])\nprint (\"Total variables for Pclass are:\", all)\n\nnull_Pclass = train_df[\"Pclass\"].isnull().sum()\nprint(\"Missing values for Pclass are:\", null_Pclass)","43c01a61":"a = min(train_df[\"Pclass\"]), max(train_df[\"Pclass\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Pclass.dtype\nprint('Data type is:', b)\n\nc= train_df.Pclass.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Pclass.unique()\nprint('Unique values are:', d)","0be76dea":"print(\"Ensure there are an adequate number of values for Survived in each Pclass to allow for reliable comparison.\")\n# PassengerId was used because it has no missing values \nPclasstest2 = train_df[['Pclass', 'Survived', 'PassengerId']].groupby(['Pclass', 'Survived'], as_index=False).count()\nprint(Pclasstest2)\nprint('The lowest number is 80 which is adequate for comparison.')","5e974af8":"print('Probability for Survival in each class (with 1 being 1st class) is as follows:')\nPclasstest = train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\nprint(Pclasstest)\n\n# Explore Pclass vs Survived\ng = sns.catplot(x=\"Pclass\",y=\"Survived\",data=train_df,kind=\"bar\", height = 6 , \ncolor = \"green\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ng = g.fig.suptitle(\"Survival Probability by Pclass\")\n\n# Explore Pclass vs Survived by Sex\ng = sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n                   height=6, kind=\"bar\", palette=\"bwr\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ng = g.fig.suptitle(\"Survival Probability by Pclass & Sex\")","47cb0786":"# Are there any null values?\nall = len(train_df[\"Embarked\"])\nprint (\"Total variables for Embarked are:\", all)\nnull_Embarked = train_df[\"Embarked\"].isnull().sum()\nprint(\"Missing values for Embarked are:\", null_Embarked)","a67f6b24":"# string so cant display\n#a = min(train_df[\"Embarked\"]), max(train_df[\"Embarked\"])\n#print('Min and Max values are:', a)\n\nb = train_df.Embarked.dtype\nprint('Data type is:', b)\n\nc= train_df.Embarked.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Embarked.unique()\nprint('Unique values are:', d)","5f6c66fc":"train_df[['Embarked', 'PassengerId']].groupby(['Embarked'], as_index=False).count()","c690e2b1":"# Fill Embarked nan values with 'S' most frequent value since only 2 values or 0.2%\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")","afe163e9":"g = sns.catplot(x=\"Embarked\", y=\"Survived\",  data=train_df,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","197373eb":"train_df['Embarked'] = train_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain_df.head()","c2ad1417":"# Explore Pclass vs Embarked \ng = sns.catplot(\"Pclass\", col=\"Embarked\",  data=train_df,\n                   height=6, kind=\"count\", palette=\"dark\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","ebbc9976":"# Are there any null values?\nall = len(train_df[\"Cabin\"])\nprint (\"Total variables for Cabin are:\", all)\nnull_Embarked = train_df[\"Cabin\"].isnull().sum()\nprint(\"Missing values for Cabin are:\", null_Embarked)","f6e02233":"# Strings not supported\n#a = min(train_df[\"Cabin\"]), max(train_df[\"Cabin\"])\n#print('Min and Max values are:', a)\n\nb = train_df.Cabin.dtype\nprint('Data type is:', b)\n\nc= train_df.Cabin.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Cabin.unique()\nprint('Unique values are:', d)","c2e4ce17":"# Replace the Cabin number by the type of cabin 'X' if not (in case decide to use X later)\ntrain_df[\"Cabin2\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train_df['Cabin'] ])\n# Delete rows with value of X\ncab = train_df[train_df.Cabin2 != 'X']\n# chart cabin letter\nsns.set(rc={'figure.figsize':(16,5)})\ng = sns.countplot(cab[\"Cabin2\"],order=['A','B','C','D','E','F','G','T'])","53ef92a3":"g = sns.catplot(y=\"Survived\", x=\"Cabin2\", data=cab, kind=\"bar\", order=['A','B','C','D','E','F','G','T'], height=4, aspect=3.4)\ng = g.set_ylabels(\"Survival Probability\")\ng = g.set_ylabels(\"Survival Probability\")","3dcda3de":"# Are there any null values?\ntick = len(train_df[\"Ticket\"])\nprint (\"Total variables for Ticket are:\", tick)\nnull_Embarked = train_df[\"Ticket\"].isnull().sum()\nprint(\"Missing values for Ticket are:\", null_Embarked)","4b1027c9":"a = min(train_df[\"Ticket\"]), max(train_df[\"Ticket\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Ticket.dtype\nprint('Data type is:', b)\n\nc= train_df.Ticket.nunique()\nprint('Number of unique values is:', c)\n\n#d= train_df.Ticket.unique()\n#print('Unique values are:', d)\n# if too many unique then print first 5 in dataframe instead\ne= train_df.filter(like='Ticket').head(n=5)\nprint(e)","e72a6d49":"## See if common occurences by extracting the ticket prefix. When there is no prefix it returns X. \n# Replace the Cabin number by the type of cabin 'X' if not\ntrain_df[\"Ticket\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train_df['Ticket'] ])","8e25695c":"# Delete rows with value of X\ntrain_df = train_df[train_df.Ticket != 'X']\n# chart ticket letter\nsns.set(rc={'figure.figsize':(16,5)})\ng = sns.countplot(train_df[\"Ticket\"])","00658bab":"g = sns.catplot(x=\"Ticket\",y=\"Survived\",data=train_df,kind=\"bar\", order = train_df[\"Ticket\"].value_counts().index, height=4, aspect=3.4)\ng = g.set_ylabels(\"survival probability\")","96423708":"# Are there any null values?\nfare = len(train_df[\"Fare\"])\nprint (\"Total variables for Fare are:\", fare)\nfare2 = train_df[\"Fare\"].isnull().sum()\nprint(\"Missing values for Fare are:\", fare2)","4f90c161":"a = min(train_df[\"Fare\"]), max(train_df[\"Fare\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Fare.dtype\nprint('Data type is:', b)\n\nc= train_df.Fare.nunique()\nprint('Number of unique values is:', c)\n\n# comment out since too many unique\n#d= train_df.Fare.unique()\n#print('Unique values are:', d)\n# if too many unique then print first 5 in dataframe instead\ne= train_df.filter(like='Fare').head(n=5)\nprint(e)","b59c015b":"# Are there any null values?\npar = len(train_df[\"Parch\"])\nprint (\"Total variables for Parch are:\", par)\nnull_Embarked = train_df[\"Parch\"].isnull().sum()\nprint(\"Missing values for Parch are:\", null_Embarked)","f4cfc3fc":"a = min(train_df[\"Parch\"]), max(train_df[\"Parch\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Parch.dtype\nprint('Data type is:', b)\n\nc= train_df.Parch.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Parch.unique()\nprint('Unique values are:', d)","c1c05252":"# Are there any null values?\nsib = len(train_df[\"SibSp\"])\nprint (\"Total variables for SibSp are:\", sib)\nnull_sib = train_df[\"SibSp\"].isnull().sum()\nprint(\"Missing values for SibSp are:\", null_sib)","ebfac135":"a = min(train_df[\"SibSp\"]), max(train_df[\"SibSp\"])\nprint('Min and Max values are:', a)\n\nb = train_df.SibSp.dtype\nprint('Data type is:', b)\n\nc= train_df.SibSp.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.SibSp.unique()\nprint('Unique values are:', d)","3a7b5dae":"# Explore SibSp feature vs Survived\ng = sns.catplot(x=\"SibSp\",y=\"Survived\",data=train_df, kind=\"bar\", height = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","7ef89eaf":"# Create a family size descriptor from SibSp and Parch\ntrain_df[\"Fsize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1","a3396d5e":"g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = train_df)\ng = g.set_ylabels(\"Survival Probability\")","d323a3fb":"# Are there any null values?\na = len(train_df[\"Age\"])\nprint (\"Total variables for Age are:\", a)\nnull_Embarked = train_df[\"Age\"].isnull().sum()\nprint(\"Missing values for Age are:\", null_Embarked)","f3aeb708":"a = min(train_df[\"Age\"]), max(train_df[\"Age\"])\nprint('Min and Max values are:', a)\n\nb = train_df.Age.dtype\nprint('Data type is:', b)\n\nc= train_df.Age.nunique()\nprint('Number of unique values is:', c)\n\nd= train_df.Age.unique()\nprint('Unique values are:', d)","2d13f5f5":"null_Age = train_df[\"Age\"].isnull().sum()\nprint(\"Missing values for Age are:\", null_Age)","369f12a6":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","9b810c81":"# Looking at Age separted by Pclass, it is significant that under age 50 9n 3rd class had low survival rate. \ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","3524ce33":"train_df.head()","783e2024":"# In training dataset, remove rows that are :\n# 1. redundant \n# 2. have shown in the Exploratory section are missing too many values & not been able to be cleaned up\n# Keep: Sex, Pclass, Title2, Embarked, Parch, FSize\n# Remove: Title (similar to Title2), Name (too unique), Cabin (too many values missing), Ticket (too unique),\n#         Fare (too unique), SibSp (similar to Fsize), Age (too unique), PassengerId (too unique), Cabin2 (duplicate), Parch (too unique)\nX_train = train_df.drop([\"Title\", \"Name\", \"Cabin\", \"Ticket\", \"Fare\", \"SibSp\", \"Age\", \"Survived\", \"PassengerId\", \"Cabin2\", \"Parch\"], axis=1)\nY_train = train_df[\"Survived\"]\nX_train.shape, Y_train.shape","67e7b3bc":"# Correlation matrix between variables chosen (SibSp Parch Age and Fare values) and Survived to ensure not correlated\nfig, ax = plt.subplots(figsize=(7,7))\ng = sns.heatmap(train_df[[\"Sex\",\"Pclass\", \"Title2\",\"Embarked\",\"Fsize\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\", ax=ax)\n# fix for mpl bug that cuts off top\/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show() # ta-da!","9d7b95ef":"# Create categorical values for Pclass\nX_train[\"Pclass\"] = X_train[\"Pclass\"].astype(\"category\")\nX_train = pd.get_dummies(X_train, columns = [\"Pclass\"],prefix=\"Pc\")","81bab191":"# Create categorical values for Sex\nX_train[\"Sex\"] = X_train[\"Sex\"].astype(\"category\")\nX_train = pd.get_dummies(X_train, columns = [\"Sex\"],prefix=\"Sex\")","0bd4a776":"# Create categorical values for Embarked\nX_train[\"Embarked\"] = X_train[\"Embarked\"].astype(\"category\")\nX_train = pd.get_dummies(X_train, columns = [\"Embarked\"],prefix=\"Embarked\")","c180845c":"# Create categorical values for Title2\nX_train[\"Title2\"] = X_train[\"Title2\"].astype(\"category\")\nX_train = pd.get_dummies(X_train, columns = [\"Title2\"],prefix=\"Title\")","94b92cbe":"X_train.head(n=2)","62dab0f0":"# Prepare test dataset by performing necessary data manipulation on columns and adding columns to ensure matches X_train\n# Determine what needs to be done by examining the test_df\nX_test = test_df\nX_test.head(n=2)","7d70dc3e":"X_test = X_test.drop([\"PassengerId\", \"Age\", \"Ticket\", \"Fare\", \"Cabin\"], axis=1)","33183a83":"# Ensure no missing values for Pclass, Sex, Embarked, Name, SibSp, Parch\nnull_Pclass = X_test[\"Pclass\"].isnull().sum()\nprint(\"Missing values for Pclass are:\", null_Pclass)\n\nnull_Sex = X_test[\"Sex\"].isnull().sum()\nprint(\"Missing values for Sex are:\", null_Sex)\n\nnull_Embarked = X_test[\"Embarked\"].isnull().sum()\nprint(\"Missing values for Embarked are:\", null_Embarked)\n\nnull_Name = X_test[\"Name\"].isnull().sum()\nprint(\"Missing values for Title are:\", null_Name)\n\nnull_SibSp = X_test[\"SibSp\"].isnull().sum()\nprint(\"Missing values for SibSp are:\", null_SibSp)\n\nnull_Parch = X_test[\"Parch\"].isnull().sum()\nprint(\"Missing values for Parch are:\", null_Parch)","c8a8480e":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test_df[\"Name\"]]\nX_test[\"Title\"] = pd.Series(dataset_title)\nX_test[\"Title\"].head()","cd60fe24":"# Ensure no unique title in test_df that were not in train_df\nc= X_test.Title.nunique()\nprint('Number of unique values is:', c)\n\nd= X_test.Title.unique()\nprint('Unique values are:', d)","ebd0a0d4":"# Convert Titles to main categories\n# Add Dona to Mrs\n# From wiki: Dona may refer to: Feminine form for don (honorific) a Spanish, Portuguese\n# Convert to categorical values Title\nX_test[\"Title\"] = X_test[\"Title\"].replace(['Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\nX_test[\"Title\"] = X_test[\"Title\"].replace(['Lady', 'the Countess','Countess', 'Dona'], 'Mrs')\nX_test[\"Title\"] = X_test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":2, \"Mlle\":2, \"Mrs\":2, \"Mr\":3})\nX_test[\"Title\"] = X_test[\"Title\"].astype(int)","6a15d51d":"# Create a family size descriptor from SibSp and Parch\nX_test[\"Fsize\"] = X_test[\"SibSp\"] + X_test[\"Parch\"] + 1","2ac4a310":"X_test = X_test.drop([\"Name\", \"SibSp\", \"Parch\"], axis=1)","36eb4b2c":"# Create categorical values for Pclass\nX_test[\"Pclass\"] = X_test[\"Pclass\"].astype(\"category\")\nX_test = pd.get_dummies(X_test, columns = [\"Pclass\"],prefix=\"Pc\")","e27be470":"# Create categorical values for Sex\nX_test[\"Sex\"] = X_test[\"Sex\"].astype(\"category\")\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"],prefix=\"Sex\")","7d23ef16":"# Create categorical values for Embarked\nX_test[\"Embarked\"] = X_test[\"Embarked\"].astype(\"category\")\nX_test = pd.get_dummies(X_test, columns = [\"Embarked\"],prefix=\"Embarked\")","ab64bc6e":"# Create categorical values for Title\nX_test[\"Title\"] = X_test[\"Title\"].astype(\"category\")\nX_test = pd.get_dummies(X_test, columns = [\"Title\"],prefix=\"Title\")","485ff788":"X_test.head(n=2)","c6315b1b":"# Suppress future warnings from sci-kit learn package\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","b380845a":"# Random Forest\n# set random seed for reproducibility\nimport random\nrandom.seed(1234)\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_predRF = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","711cf57e":"# K-nearest neighbor\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_predKNN = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","3dae83ee":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_predLR = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","66515311":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_predSVC = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","9ea3a0d6":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_predGNB = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","3c9c6265":"# Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_predPer = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","ea5324c4":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_predLSVC = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","da45fdfa":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_predSGD = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","555f1b2a":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_predDT = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","c0e07225":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","d37fc1a1":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_predRF\n    })\nsubmission.to_csv('submission.csv', index=False)","108f859c":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference Wikipedia.","4e051d5e":"##### Survival probability by title with the Rare category being grouped into one of the 4 main categories.","f308ea52":"- Sex does not have any outliers.\n- Sex does not have any inappropriates values. The column is made up of either male or female.\n- Sex is an object data type which is appropriate.","5afb6e87":"Prepare X_train, Y_train, X_test datasets. The X_train & X_test datasets must be of equal column length. The Y_train dataset must be the survived column and must be of equal row length to the X_train.\n\nTo prepare the data, look at train_df and drop unnecessary rows.","13d321aa":"### 9.2 Sex Column <a class=\"anchor\" id=\"sex\"><\/a>\n- Typically, the protocol aboard ships was to save women and children first. Does this seem to hold true?","2965dca5":"Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.","3f15cc51":"#### References\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n- [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n- [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)","67affb8a":"The decision trees implemented in scitkit-learn uses only numerical features and these features are interpretered always as continous numeric variables. Code the categeroical feature into multiple binary features with 1 when the category matches and 0 otherwise. This is called one-hot-encoding.","d1c280f7":"# 6. Load Dataset <a class=\"anchor\" id=\"dataset\"><\/a>","f85b03be":"Upon examining the ages and sex for the titles grouped in the Mrs category, the sample (albeit small) supported the definitions found, I will keep those titles grouped into the Mrs category.","aa1d51bc":"### 9.1 Name Column <a class=\"anchor\" id=\"name\"><\/a>","15d3f743":"There is a significant number of missing values for the Age value or almost 20%.  It will not be used as a variable for the model. Title is a rough adequate representation.\n\nSo must find a way to fill it in with:\n- median or mean\n- imputation ie. predictive model to predict what value of missing value should be (can use entire dataset or just training dataset)\n- find a proxy or other value that mimics value in question\n","bb08435d":"# 4. Load Libraries <a class=\"anchor\" id=\"libraries\">","d3a14a99":"#### Create categorical for all category values","2121756f":"### Preliminary observations\n\n#### From size\n- train_df has the extra column \"Survived\"\n\n#### From info\n- INTEGERS: PassengerId, Survived, PClass, SibSp, Parch\n- OBJECT: Name, Sex, Ticket, Cabin, Embarked\n- FLOAT: Age, Fare\n\n#### From describe\n- min age is 0.42 which seems small but could indicate a baby\n- max age is 80 which seems about right\n- extremely high far of 512 which could indicate more than one person on the ticket\n- ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n\n2. Age is missing 891-714 = 177 variables.\n3. Cabin seems to have a lot of nan values.\n\n#### Variable Type\n- Look at the given description of the column names (OR search the web to determine what the variable means) to determine if the type of variable is appropriate or should be changed.\n- Type of variables are: integer, factor, object, float64\n    \n- Survived is either 0 or 1 so should set as category\n- Pclass is either 1 or 2 or 3 so should set as category\n- Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names\n- Ticket is a mix of numeric and alphanumeric data types\n- Cabin is alphanumeric    ","67f51967":"### 9.4 Embarked Column <a class=\"anchor\" id=\"embarked\"><\/a>","f08297b1":"### 9.6 Ticket Column <a class=\"anchor\" id=\"ticket\"><\/a>","f2093b43":"### 9.9 SibSp column<a class=\"anchor\" id=\"sibsp\"><\/a>","9a47d473":"From the heatmap, you can see that none of the features are correlated which is the desired result when determining variables.","ebf0c3c3":"# TITANIC KAGGLE COMPETITION\n\n#### S Sexton\n#### May 5, 2020\n\n## Table of Contents\n\n* [1. Project Goal](#project_goal)\n* [2. Background Information](#background_info)\n* [3. Acquire Data](#acquire)\n* [4. Load Libraries](#libraries)\n* [5. Set Working Directory](#directory)\n* [6. Load Dataset](#dataset)\n* [7. Cursory Explanation](#cursory)\n* [8. Determine if Duplicates](#duplicate)\n* [9. Explore Each Variable](#explore)\n    * [9.1 Name Column](#name)\n    * [9.2 Sex Column](#sex)\n    * [9.3 Pclass Column](#pclass)\n    * [9.4 Embarked Column](#embarked)\n    * [9.5 Cabin Column](#cabin)\n    * [9.6 Ticket Column](#ticket)\n    * [9.7 Fare Column](#fare)\n    * [9.8 Parch Column](#parch)\n    * [9.9 SibSp Column](#sibsp)\n    * [9.10 Age Column](#age)\n* [10. Prepare Dataframes for Modeling](#prepare)\n* [11. Modeling](#model)\n","06433869":"Passengers having a lot of siblings\/spouses have less chance to survive, especially those with 3 or 4 siblings\/ spouses. ","77de1060":"### 9.8 Parch Column <a class=\"anchor\" id=\"parch\"><\/a>","9c85df1e":"The family size seems to play an important role, survival probability is worst for large families.","eabc7bde":"# 7. Cursory Exploration <a class=\"anchor\" id=\"cursory\"><\/a>","0b55fa8b":"- Will be keeping Pclass, Sex, Embarked, Parch columns\n- Will create Title column from Name column\n- Will create Fsize column from SibSp column","6d662bf4":"Because of the low number of passenger that have a cabin, cabin is not a reliable measure to use in the model.\n\nThere is no distinct differences in the cabins so they will not be used.","0a6ec383":"#### Create Fsize column from SibSp column","936debdc":"# 11. Modeling <a class=\"anchor\" id=\"model\"><\/a>","e08e8523":"Upon researching what each title means, group the following.\n\nGroup the following into Mr - \n- Don - Don, and dom, is derived from the Latin Dominus: a master of a household who were male\n- Rev - Reverend is an honorific style most often placed before the names of Christian clergy and ministers who were male\n- Dr - Contracted \"Dr\" or \"Dr.\", it is used as a designation for a person who has obtained a doctorate who were typically male in the early 20th century\n- Major - An army officer of high rank, in particular (in the US Army, Air Force, and Marine Corps) an officer ranking above captain and below lieutenant colonel. They were typically male in the early 20th century.\n- Sir - A formal English honorific address for men.\n- Col - Colonel (abbreviated Col., Col or COL and alternatively spelled coronel) is a senior military officer rank below the general officer ranks. They were typically male in the early 20th century.\n- Capt - Captain is a military rank in armies, navies, coast guards, etc. They were typically male in the early 20th century.\n- Jonkheer - an honorific in the Low Countries denoting the lowest rank within the nobility. Unsure if male or female typically.\n\nGroup the following into Mrs -\n- Lady - a woman of superior social position, especially one of noble birth\n- Countess\/ the Countess - the wife or widow of a count or earl","20a7c728":"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the computational burden, achieving faster iterations in trade for a lower convergence rate. Reference Wikipedia","656629d1":"#### Submission","f35dfebf":"#### Drop unnecessary columns","5f5019eb":"### Prepare X_test","5b7ec42a":"# 2. Background Information <a class=\"anchor\" id=\"background_info\"><\/a>\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. This may be likely to the prevailing thoughts at the time to save women and children first.","9859054d":"# 5. Set Working Directory <a class=\"anchor\" id=\"directory\"><\/a>","2be7bf24":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference Wikipedia.","2f1ffd68":"### Prepare X_train & Y_train","bd061193":"Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Random Forest\n- KNN or k-Nearest Neighbors\n- Logistic Regression\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine","aba8ec17":"Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","8c5fe54e":"Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference Wikipedia.","db4ad707":"### 9.5 Cabin Column <a class=\"anchor\" id=\"cabin\"><\/a>","928e8456":"Upon examining the ages and sex for the titles grouped in the Mr category, there is only 1 anamoly and that is the female doctor. Thus, because the sample (albeit small) supported the definitions found, I will keep those titles grouped into the Mr category.","7515b6d0":"### 9.3 Pclass Column <a class=\"anchor\" id=\"pclass\"><\/a>\n- Pclass is ticket class and typically rich people will buy a higher class ticket than poor people\n- Pclass 1 the is highest & most expensive class with Pclass 3 being the lowest & cheapest class","3dcdbb80":"##### Observation\n- Significantly more females than males survived\n- Sex seems like an important variable for modeling","b9a68539":"Examine the size, contents, types of variables, missing variables, etc.","ee19f779":"- Pclass does not have any outliers.\n- Pclass does not have any inappropriates values. The column is made up of either 1,2,3 Pclass.\n- Pclass is an int64 data type which is appropriate.","ea36ee5b":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference Wikipedia.","59cd42c6":"I chose RandomForest to submit.","916c4e00":"# 9. Explore each variable <a class=\"anchor\" id=\"explore\"><\/a>\n\nOnly the training dataset will be explored. Any variables which show promise for modeling in the training dataset and have been modified will be then be modified in the test dataset. \n\n### Explore each column by:\n\n##### First, clean up the variable, look at:\n1. Are there missing values? Fill in with proxy, etc.\n2. Is there an adequate number of that variable to properly represent the sample?\n3. Are there outliers that should be dropped or addressed?\n4. Are there inappropriate values for that variable?\n5. Is the data type appropriate? convert if necessary\n6. Can the variable be feature engineered?\n7. Look for outliers.\n\n##### Second, visualize the variable with charts and tables.","f66eff74":"- There are 77% of Cabin values missing. Thus I will not fill it in with a proxy and will only analyze given variables.\n- The first letter of the cabin indicates the deck which indicates the probable location of the passenger in the Titanic.","a2d6883f":"- Our submission to the competition site Kaggle results in scoring 13,457 of 22,334 competition entries.\n- Any suggestions to improve our score are most welcome.","033b50b3":"##### Observation\n- 1st class survival rate is better than 2nd class and 2nd class is better than 3rd class\n- Females overall have a much better chance of surviving\n- 1st class males and females have a better chance of surviving than lower class\n- Pclass seems to be a good variable for modeling","082b926a":"### 9.10 Age column <a class=\"anchor\" id=\"age\"><\/a>","8d89c111":"# 10. Prepare Dataframes for Modeling <a class=\"anchor\" id=\"prepare\"><\/a>","9cca7b91":"# 3. Acquire Data <a class=\"anchor\" id=\"acquire\"><\/a>\n\n- This is not necessary since it is supplied by the kaggle competition.","80f48725":"Upon examining the duplicate names, it appears that both are actually unique people since they have different ages and ticket numbers. These duplicate names will NOT be removed.","75f174a2":"# 1. Project Goal <a class=\"anchor\" id=\"project_goal\"><\/a>\nKaggle has already predefined the project goal as:\n- Given a training set which lists passengers who survived or did not survive the Titanic disaster, create a model to determine who survived in the test dataset.","b84763ae":"# 8. Determine if Duplicates <a class=\"anchor\" id=\"duplicate\"><\/a>\n- Combine entire dataset to determine if duplicates in entire dataset (test & train) that need to be removed.\n- Choose a variable that is unique.\n- Best variable to use is Name","464422af":"##### Extract title from the Name column","d768f935":"##### Observations\n- Significantly more Miss\/Ms survived than Mr. More Master survived than Mr.\n- Title is possibly a good variable for modeling","551eb872":"Note: The Rare group consists of only 23 samples AND it consists of mixed age ranges and sex. Thus, it would be best to group the Rare values into 1 of the 4 main categories.","c643c639":"Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.","9f4f757e":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference Wikipedia.","2549e170":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference Wikipedia.","06398a53":"### 9.7 Fare Column <a class=\"anchor\" id=\"fare\"><\/a>","4e8ef228":"##### Observations\n- Passengers coming from Cherbourg (C) have more chance to survive.\n- Perhaps the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).","08c2320c":"#### Create Title column from Name column"}}