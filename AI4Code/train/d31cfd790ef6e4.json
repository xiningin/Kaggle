{"cell_type":{"2a049ed4":"code","31b05ad0":"code","fe01f5e2":"code","13081ced":"code","2c4ea849":"code","a0c7f264":"markdown","1367a563":"markdown","e3380265":"markdown","d3600870":"markdown","2de601da":"markdown","a57fc435":"markdown"},"source":{"2a049ed4":"import numpy as np \nimport pandas as pd \nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.constraints import maxnorm\nfrom keras.layers import Dense,Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport os\nprint(os.listdir(\"..\/input\"))","31b05ad0":"seed = 7\n\nnp.random.seed(seed)\n\ndata = pd.read_csv('..\/input\/sonar.all-data.csv',sep=',',header=None)\n\ndataset = data.values\n\ndata.head()\n\n# Now split the dataset into input(X) variable and Output(y) variable\n\nX = dataset[:,0:60].astype(float)\n\ny = dataset[:,60]\n\n# Now let encode the class values as integers\n\nencoder = LabelEncoder()\n\nencoder.fit(y)\n\ny_encoded = encoder.transform(y)","fe01f5e2":"def base_model():\n    # Create model\n    model = Sequential()\n    model.add(Dense(60,input_dim=60,kernel_initializer='normal',activation = 'relu'))\n    model.add(Dense(30,kernel_initializer='normal',activation='relu'))\n    model.add(Dense(1,kernel_initializer='normal',activation='sigmoid'))\n    # Compile Model\n    sgd = SGD(lr=0.01,momentum=0.8,decay=0.0,nesterov=False)\n    model.compile(loss='binary_crossentropy',optimizer=sgd,metrics = ['accuracy'])\n    return model\n# Now let us estimate the classification accuracy\nnp.random.seed(seed)\n# Create a set of estimators and include in a pipeline\nestimators = []\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('mlp',KerasClassifier(build_fn=base_model,epochs=1000,batch_size=16,verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\nclassification_accuracy = cross_val_score(pipeline,X,y_encoded,cv=kfold)\nprint(\"Base Model Accuracy (Standard Deviation): %.2f%% (%.2f%%)\"% (classification_accuracy.mean()*100, classification_accuracy.std()*100))","13081ced":"def base_model_visible_layer_dropout():\n    # Create model\n    model = Sequential()\n    model.add(Dropout(0.2, input_shape=(60,)))\n    model.add(Dense(60,kernel_initializer='normal',activation = 'relu',kernel_constraint=maxnorm(3)))\n    model.add(Dense(30,kernel_initializer='normal',activation='relu',kernel_constraint=maxnorm(3)))\n    model.add(Dense(1,kernel_initializer='normal',activation='sigmoid'))\n    # Compile Model\n    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n    model.compile(loss='binary_crossentropy',optimizer=sgd,metrics = ['accuracy'])\n    return model\n# Create a set of estimators and include in a pipeline\nestimators = []\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('mlp',KerasClassifier(build_fn=base_model_visible_layer_dropout,epochs=1000,batch_size=16,verbose=0)))\npipeline = Pipeline(estimators)\n\nkfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\nclassification_accuracy = cross_val_score(pipeline,X,y_encoded,cv=kfold)\nprint(\"Visible Layer Accuracy (Standard Deviation): %.2f%% (%.2f%%)\"% (classification_accuracy.mean()*100, classification_accuracy.std()*100))","2c4ea849":"# Dropout in the hidden layer with weight constraint\ndef base_model_hidden_layer_dropout():\n    # Create model\n    model = Sequential()\n    model.add(Dense(60,input_dim=60,kernel_initializer='normal',activation = 'relu',kernel_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(30,kernel_initializer='normal',activation='relu',kernel_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,kernel_initializer='normal',activation='sigmoid'))\n    # Compile Model\n    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n    model.compile(loss='binary_crossentropy',optimizer=sgd,metrics = ['accuracy'])\n    return model\n\n# Create a set of estimators and include in a pipeline\nestimators = []\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('mlp',KerasClassifier(build_fn=base_model_hidden_layer_dropout,epochs=1000,batch_size=16,verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\nclassification_accuracy = cross_val_score(pipeline,X,y_encoded,cv=kfold)\nprint(\"Hidden Layer Accuracy (Standard Deviation): %.2f%% (%.2f%%)\"% (classification_accuracy.mean()*100, classification_accuracy.std()*100))","a0c7f264":"**Let us fix random seed for reproducibility **","1367a563":"\nAs you can see above for the chosen network configuration that using dropout in the hidden layers did not lift performance. In fact, performance was worse than the baseline.\n\nIt is possible that additional training epochs are required or that further tuning is required to the learning rate.Let us see in the next section more on how we need to tune the dropout level to improve accuracy.\n\n### 4. How to tune the dropout level on your problem?\n\nBelow are some of the useful heuristics to consider when using dropout in practice.\n\n- Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n\n- Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n\n- Use dropout on incoming (visible) as well as hidden units. Application of dropout at each layer of the network has shown good results.\n\n- Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n\n- Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results.\n\n### Acknowledgements :\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting by Srivastava.\nhttp:\/\/jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf\n\n\n## If you like this kernel Greatly Appreciate to  UPVOTE .Thank you","e3380265":"**Let us create base model **","d3600870":"Running the above example provides a small drop in classification accuracy, at least on a single test run.\n\n### 3. How to use dropout on your hidden layers?\n\nDropout can be applied to hidden neurons in the body of your network model.\n\nIn the example below Dropout is applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout rate of 20% is used as is a weight constraint on those layers.","2de601da":"# Dropout Regularization in Deep Learning Models With Keras\n\nDropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network.\n\nA simple and powerful regularization technique for neural networks and deep learning models is dropout.\nThis notebook will uncover the dropout regularization technique and how to apply it to deep learning models in Python with Keras.\n\n#### Table of Contents\n- How the dropout regularization technique works.\n\n- How to use dropout on your input layers.\n\n- How to use dropout on your hidden layers.\n\n- How to tune the dropout level on your problem.\n\nSo let's get started\n\n### How the dropout regularization technique works.\n\n***Dropout*** is a technique where randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n\nAs a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliant on context for a neuron during training is referred to ***complex co-adaptations***.\n\nYou can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n\nThe effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n\nSo enough of theory lets jump into implementation of this regularisation concept in Keras \n\nDropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model.\n\nLet us explore different ways of using Dropout in Keras.\n\nThe dataset that I am using is ***Sonar dataset*** which is a binary classification problem where the objective is to correctly identify rocks and mock-mines from sonar chirp returns. It is a good test dataset for neural networks because all of the input values are numerical and have the same scale.\n\n- We will evaluate the developed models using scikit-learn with 10-fold cross validation, in order to better tease out differences in the results.\n- There are 60 input values and a single output value and the input values are standardized before being used in the network. \n- The baseline neural network model has two hidden layers, the first with 60 units and the second with 30. \n- Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum.\n\nNow lets jump into the code\n\n### Import Libraries\n","a57fc435":"The above base model generated an estimated classification accuracy of 86% and standard deviation of 8%.\n\n### 2. How to use dropout on your input layers.\n\nDropout can be applied to input neurons called the ***Visible Layer(Input Layer)***.\n\n- In the example below we add a new Dropout layer between the input (or visible layer) and the first hidden layer. \n- The dropout rate is set to 20%, meaning one in 5 inputs will be randomly excluded from each update cycle.\n- Additionally a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the kernel_constraint argument on the Dense class when constructing the layers.\n- The learning rate is lifted by one order of magnitude and the momentum was increase to 0.9. \n\nContinuing on from the baseline example above, the code below exercises the same network with input dropout.\n\n**Dropout in the input layer with weight constraint**"}}