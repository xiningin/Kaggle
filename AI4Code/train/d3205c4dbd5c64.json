{"cell_type":{"8f45c49c":"code","c1499df6":"code","a2c0cbc3":"code","4bcf4fb8":"code","8584bf23":"code","f05e30dc":"code","30fa7b0d":"code","9d2d3ada":"code","7f1ac46a":"code","960a0eca":"code","629bb696":"code","e731ccbe":"code","6d4ded9d":"code","07c6f61a":"code","4d104499":"code","4b042d77":"code","2b7c39ba":"code","59f26a85":"code","3a4a6bf5":"code","7d6bdb1e":"code","0e6e7d01":"code","4f3233cf":"code","1867bb79":"code","aa5ad30f":"code","2afaa9e2":"code","96181499":"code","aadeb314":"code","b100ac68":"code","fa392eb9":"code","0df40d5c":"code","68b08110":"code","04652ba6":"code","23a319e3":"code","5d5ec45c":"code","48df77d0":"code","d3c1bf4b":"code","a099c75c":"code","3c8f9b0e":"code","9236c4cf":"code","544f2be4":"code","5b3e9d24":"code","e19c416a":"code","f567a2ff":"code","78a6bf9f":"code","0da4ebfb":"code","a3775881":"code","d6bded72":"code","d1782e35":"markdown","e2e984ec":"markdown","39109192":"markdown","894f7576":"markdown","9abfbd00":"markdown","ffa970a9":"markdown","a9ff8af1":"markdown","f123f907":"markdown","c944d99c":"markdown","756844db":"markdown","257ad432":"markdown","e93108b0":"markdown","be114294":"markdown","1a0580c9":"markdown","4c97b1ce":"markdown","94f150c2":"markdown","cb7cae2b":"markdown","a8bacc0d":"markdown","a1c4eda8":"markdown","dd8d23a3":"markdown","38622ecc":"markdown","bba3265c":"markdown","bf88a70d":"markdown","2cd98b78":"markdown","23d409ac":"markdown","79c4b6e9":"markdown","ac6e1f67":"markdown","3896ec63":"markdown","3e1ab034":"markdown","b508f781":"markdown","85b79209":"markdown","ad1d0606":"markdown"},"source":{"8f45c49c":"import pandas as pd\nimport pyarrow.parquet as pq # Used to read the data\nimport os \nimport numpy as np\nfrom keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\nfrom keras.models import Model\nfrom tqdm import tqdm # Processing time measurement\nfrom sklearn.model_selection import train_test_split \nfrom keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\nfrom keras import optimizers # Allow us to access the Adam class to modify some parameters\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold,RepeatedStratifiedKFold # Used to use Kfold to train our model\nfrom keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\nimport tensorflow as tf","c1499df6":"# select how many folds will be created\nN_SPLITS = 5\n# it is just a constant with the measurements data size\nsample_size = 800000","a2c0cbc3":"# It is the official metric used in this competition\n# below is the declaration of a function used inside the keras model, calculation with K (keras backend \/ thensorflow)\ndef matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n    y_true = tf.convert_to_tensor(y_true, np.float32)\n    \n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator \/ (denominator + K.epsilon())","4bcf4fb8":"# https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","8584bf23":"# just load train data\ndf_train = pd.read_csv('..\/input\/metadata_train.csv')\n# set index, it makes the data access much faster\ndf_train = df_train.set_index(['id_measurement', 'phase'])\ndf_train.head()","f05e30dc":"# in other notebook I have extracted the min and max values from the train data, the measurements\nmax_num = 127\nmin_num = -128","30fa7b0d":"# This function standardize the data from (-128 to 127) to (-1 to 1)\n# Theoretically it helps in the NN Model training, but I didn't tested without it\ndef min_max_transformation(ts, min_data, max_data, range_needed=(-1,1)):\n    if min_data < 0:\n        ts_std = (ts + abs(min_data)) \/ (max_data + abs(min_data))\n    else:\n        ts_std = (ts - min_data) \/ (max_data - min_data)\n    if range_needed[0] < 0:    \n        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n    else:\n        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]","9d2d3ada":"def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n    # convert data into -1 to 1\n    ts_std = min_max_transformation(ts, min_data=min_num, max_data=max_num)\n    # bucket or chunk size, 5000 in this case (800000 \/ 160)\n    bucket_size = int(sample_size \/ n_dim)\n    # new_ts will be the container of the new data\n    new_ts = []\n    # this for iteract any chunk\/bucket until reach the whole sample_size (800000)\n    for i in range(0, sample_size, bucket_size):\n        # cut each bucket to ts_range\n        ts_range = ts_std[i:i + bucket_size]\n        # calculate each feature\n        mean = ts_range.mean()\n        std = ts_range.std() # standard deviation\n        std_top = mean + std # I have to test it more, but is is like a band\n        std_bot = mean - std\n        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))\n    return np.asarray(new_ts)","7f1ac46a":"# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n# if we would try to do in one time, could exceed the RAM Memmory\ndef prepare_data(start, end):\n\n    praq_train = pq.read_pandas('..\/input\/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    X = []\n    y = []\n   \n    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start\/3):int(end\/3)]):\n        X_signal = []\n        # for each phase of the signal\n        for phase in [0,1,2]:\n            # extract from df_train both signal_id and target to compose the new data sets\n            signal_id, target = df_train.loc[id_measurement].loc[phase]\n            # but just append the target one time, to not triplicate it\n            if phase == 0:\n                y.append(target)\n            # extract and transform data into sets of features\n            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n        # concatenate all the 3 phases in one matrix\n        X_signal = np.concatenate(X_signal, axis=1)\n        # add the data to X\n        X.append(X_signal)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X, y","960a0eca":"# this code is very simple, divide the total size of the df_train into two sets and process it\nX = []\ny = []\ndef load_all():\n    total_size = len(df_train)\n    for ini, end in [(0, int(total_size\/2)), (int(total_size\/2), total_size)]:\n        X_temp, y_temp = prepare_data(ini, end)\n        X.append(X_temp)\n        y.append(y_temp)\nload_all()\nX = np.concatenate(X)\ny = np.concatenate(y)","629bb696":"print(X.shape, y.shape)\n# save data into file, a numpy specific format\nnp.save(\"X.npy\",X)\nnp.save(\"y.npy\",y)","e731ccbe":"warnings.filterwarnings('ignore')\n\npan = pd.Panel(X)\ndf = pan.swapaxes(1, 2).to_frame()\ndf.index = df.index.droplevel('major')\ndf.index = df.index+1","6d4ded9d":"df.shape","07c6f61a":"df_y = pd.DataFrame(y.reshape(1,-1))","4d104499":"df_y.shape","4b042d77":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn')\nfrom scipy.stats import norm, skew\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the training set\")\nsns.distplot(df.mean(axis=0),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","2b7c39ba":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(df.mean(axis=1),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","59f26a85":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of Standard Deviation values per row in the training set\")\nsns.distplot(df.std(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","3a4a6bf5":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of Standard Deviation values per column in the training set\")\nsns.distplot(df.std(axis=1),color=\"blue\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","7d6bdb1e":"plt.figure(figsize=(16,6))\nplt.title(\"Skewness Distribution per row in the training set\")\nsns.distplot(df.skew(axis=0),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","0e6e7d01":"plt.figure(figsize=(16,6))\nplt.title(\"Skewness Distribution per column in the training set\")\nsns.distplot(df.skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","4f3233cf":"plt.figure(figsize=(16,6))\nplt.title(\"Kurtosis Distribution per row in the training set\")\nsns.distplot(df.kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","1867bb79":"plt.figure(figsize=(16,6))\nplt.title(\"Kurtosis Distribution per row in the training set\")\nsns.distplot(df.kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='train')\nplt.legend()\nplt.show()","aa5ad30f":"df.shape","2afaa9e2":"df_y.shape","96181499":"#Now let's use t-SNE to reduce dimensionality down to 2D so we can plot the dataset:\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42, verbose = 2)\nTSNE_X = tsne.fit_transform(df)","aadeb314":"import matplotlib as mpl\nmin, max = (-40, 30)\nstep = 10\n\n# Setting up a colormap that's a simple transtion\nmymap = mpl.colors.LinearSegmentedColormap.from_list('mycolors',['green','yellow','red'])\n\n# Using contourf to provide my colorbar info, then clearing the figure\nZ = [[0,0],[0,0]]\nlevels = range(min,max+step,step)\nCS3 = plt.contourf(Z, levels, cmap=mymap)\nplt.clf()","b100ac68":"plt.figure(figsize=(13,10))\ncm = plt.cm.get_cmap('RdYlGn_r')\ncolors=[cm(1*i) for i in [75,145,255]]\nxy = range(3)\ncolorlist=[colors[x] for x in xy]\nplt.scatter(TSNE_X[:, 0], TSNE_X[:, 1], c=colorlist, cmap=cm)\nplt.colorbar(CS3)\nplt.axis('off')\nplt.show()","fa392eb9":"from sklearn.manifold import Isomap\n\nisomap = Isomap(n_components=200)\ndf_reduced = isomap.fit_transform(df)","0df40d5c":"plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=colorlist, cmap=plt.cm.hot)\nplt.axis('off')\nplt.colorbar(CS3)\nplt.grid(True)","68b08110":"from sklearn.decomposition import PCA\n\nPCA_train_x = PCA(n_components=2, random_state=42).fit_transform(df)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=colorlist, cmap=cm)\nplt.axis('off')\nplt.colorbar(CS3)\nplt.show()","04652ba6":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n    X_reduced = pca.fit_transform(df)\n    if subplot == 132:\n        X_reduced_rbf = X_reduced\n    \n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=colorlist, cmap=plt.cm.hot)\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)","23a319e3":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\nlle_X = lle.fit_transform(df)","5d5ec45c":"plt.title(\"Unrolling PCA graph using LLE\", fontsize=14)\nplt.scatter(lle_X[:, 0], lle_X[:, 1], c= colorlist, cmap=plt.cm.hot)\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18)\nplt.colorbar(CS3)\nplt.grid(True)\nplt.show()","48df77d0":"# This is NN LSTM Model creation\ndef model_lstm(input_shape):\n    # The shape was explained above, must have this order\n    inp = Input(shape=(input_shape[1], input_shape[2],))\n    \n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    \n    x = Attention(input_shape[1])(x)\n    \n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n    \n    return model","d3c1bf4b":"# First, create a set of indexes of the 5 folds\nsplits = list(StratifiedKFold(n_splits=N_SPLITS,random_state=42).split(X, y))\npreds_val = []\ny_val = []\n# Then, iteract with each fold\n# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n    print(\"Beginning fold {}\".format(idx+1))\n    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n    model = model_lstm(train_X.shape)\n    #es = EarlyStopping(monitor='val_matthews_correlation', verbose=2, patience=50, mode='max')\n    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n    # Train, train, train\n    model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n    # loads the best weights saved by the checkpoint\n    model.load_weights('weights_{}.h5'.format(idx))\n    # Add the predictions of the validation to the list preds_val\n    preds_val.append(model.predict(val_X, batch_size=512))\n    # and the val true y\n    y_val.append(val_y)\n\n# concatenates all and prints the shape    \npreds_val = np.concatenate(preds_val)[...,0]\ny_val = np.concatenate(y_val)\npreds_val.shape, y_val.shape","a099c75c":"import tensorflow as tf\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)]):\n        score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n    return search_result","3c8f9b0e":"best_threshold = threshold_search(y_val, preds_val)['threshold']","9236c4cf":"best_threshold","544f2be4":"meta_test = pd.read_csv('..\/input\/metadata_test.csv')","5b3e9d24":"meta_test = meta_test.set_index(['signal_id'])\nmeta_test.head()","e19c416a":"first_sig = meta_test.index[0]\nn_parts = 10\nmax_line = len(meta_test)\npart_size = int(max_line \/ n_parts)\nlast_part = max_line % n_parts\nprint(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\n# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\nstart_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\nstart_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\nprint(start_end)\nX_test = []\n# now, very like we did above with the train data, we convert the test data part by part\n# transforming the 3 phases 800000 measurement in matrix (160,57)\nfor start, end in start_end:\n    subset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    for i in tqdm(subset_test.columns):\n        id_measurement, phase = meta_test.loc[int(i)]\n        subset_test_col = subset_test[i]\n        subset_trans = transform_ts(subset_test_col)\n        X_test.append([i, id_measurement, phase, subset_trans])","f567a2ff":"X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\nnp.save(\"X_test.npy\",X_test_input)\nX_test_input.shape","78a6bf9f":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nprint(len(submission))\nsubmission.head()","0da4ebfb":"preds_test = []\nfor i in range(N_SPLITS):\n    model.load_weights('weights_{}.h5'.format(i))\n    pred = model.predict(X_test_input, batch_size=300, verbose=1)\n    pred_3 = []\n    for pred_scalar in pred:\n        for i in range(3):\n            pred_3.append(pred_scalar)\n    preds_test.append(pred_3)\n","a3775881":"preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\npreds_test.shape","d6bded72":"submission['target'] = preds_test\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","d1782e35":"<pre><b> I used Panel to convert the X and Y into Data Frames<\/b><\/pre>","e2e984ec":"<pre><a id = 78><b>Distribution of Kurtosis values per columns in the training set<\/b><\/a><\/pre>","39109192":"<pre><b>The distribution is Platokurtic.<\/b><\/pre>","894f7576":"<pre><a id = 10><b>Applying LLE Transformation<\/b><\/a><\/pre>","9abfbd00":"<pre><a id = 12><b>Applying Stratified K Fold Technique<\/b><\/a><\/pre>","ffa970a9":"<pre><a id = 9><b>Applying PCA Transformation<\/b><\/a><\/pre>","a9ff8af1":"<img src = \"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Enet%20Centre\/logo_vyska.png\" width = 300 hieght = 300\/>\n\n<h6><center><font size=\"6\">End to End Machine Learning for VSB Power Line Fault Detection<\/center><\/font><\/h6>\n<br>\n<b>Faults in electric transmission lines can lead to a destructive phenomenon called partial discharge. If left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs. Each signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a 3-phase power scheme, and all three phases are measured simultaneously.\n<br><br>\nThe Challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VSB. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.<\/b>\n\n<pre><b>Inspired from Bruno Aquino's Kernel:\nhttps:\/\/www.kaggle.com\/braquino\/5-fold-lstm-attention-fully-commented-0-694<\/b><\/pre>\n\n<pre><b>\n<a id = \"#\">Content<\/a>\n- <a href = \"#1\"> Import Library<\/a>\n- <a href = \"#2\"> Define Matthews Correlation<\/a>\n- <a href = \"#3\"> Create Attention Layer<\/a>\n- <a href = \"#4\"> Import Data<\/a>\n- <a href = \"#5\"> Define Transformations<\/a>\n- <a href = \"#6\"> Prepare Training Data<\/a>\n- <a href = \"#7\"> Data Exploration<\/a>\n    - <a href = \"#71\">Distribution of mean values per rows<\/a>\n    - <a href = \"#72\">Distribution of mean values per columns<\/a>\n    - <a href = \"#73\">Distribution of S.D values per rows<\/a>\n    - <a href = \"#74\">Distribution of S.D values per columns<\/a>\n    - <a href = \"#75\">Distribution of Skewness values per rows<\/a>\n    - <a href = \"#76\">Distribution of Skewness values per columns<\/a>\n    - <a href = \"#77\">Distribution of Kurtosis values per rows<\/a>\n    - <a href = \"#78\">Distribution of Kurtosis values per columns<\/a>\n- <a href = \"#8\">Applying TSNE Transformation<\/a>\n- <a href = \"#9\">Applying PCA Transformation><\/a>\n- <a href = \"#19\">Applying ISO Transformation<\/a>\n- <a href = \"#10\">Applying LLE Transformation><\/a>\n- <a href = \"#11\">Building the RNN Model Architecture<\/a>\n- <a href =\"#12\">Applying Stratified K Fold Technique<\/a>\n- <a href =\"#13\">Identify the Best Threshold<\/a>\n- <a href = \"#14\">Create Submission File<\/a>\n<\/b><\/pre>","f123f907":"<pre><a id = 4><b>Import Data<\/b><\/a><\/pre>","c944d99c":"<pre><b>What we understand now, is the reason behind the BIModal behavior. Since the power line fault is detected in a very few cases, there is a presence of class imbalance in the dataset, which is valid for this kind of data.<\/pre><\/b>","756844db":"<pre><b>Lets check the Explorations for Mean, Standard Deviation, Skewness and Kurtosis on the Data generated<\/b><\/pre>\n","257ad432":"<pre><a id = 77><b>Distribution of Kurtosis values per rows in the training set<\/b><\/a><\/pre>","e93108b0":"<a id=2><pre><b>Define Matthews Correlation<\/b><\/pre><\/a>","be114294":"<pre><a id = 3><b>Create Attention Layer<\/b><\/a><\/pre>","1a0580c9":"<pre><a id = 13><b>Identify the Best Threshold<\/b><\/a><\/pre>","4c97b1ce":"<pre><a id = 75><b>Distribution of Skewness values per rows in the training set<\/b><\/a><\/pre>","94f150c2":"<pre><a id = 73><b>Distribution of S.D values per row in the training set<\/b><\/a><\/pre>","cb7cae2b":"<pre><a id = 14><b>Create Submission File<\/b><\/a><\/pre>","a8bacc0d":"<pre><a id = 8><b>Applying TSNE Transformation<\/b><\/a><\/pre>","a1c4eda8":"<pre><b>After seeing the PCA model, I am eager to use Local Linear Embedding, as I do see the curve is resembling similar to a swiss roll.<\/b><\/pre>","dd8d23a3":"<pre><a id = 72><b>Distribution of mean values per column in the training set<\/b><\/a><\/pre>","38622ecc":"<pre><b>Now Lets perform TSNE to see if the distribution is well seperated?<\/b><\/pre>","bba3265c":"<pre><b>Please note that since y's shape is different than X, i cant really use Y's for the color bar.  I am using a customized function for the color bar.<\/b><\/pre>","bf88a70d":"<pre><a id = 76><b>Distribution of Skewness values per column in the training set<\/b><\/a><\/pre>","2cd98b78":"<a id=1><pre><b>Import Library<\/b><\/pre><\/a>","23d409ac":"<pre><b>Now Lets perform PCA to see if the distribution is well seperated?<\/pre><\/b>","79c4b6e9":"<pre><a id = 6><b>Prepare Training Data<\/b><\/a><\/pre>","ac6e1f67":"<pre><a id = 7><b>Data Exploration<\/b><\/a><\/pre>","3896ec63":"<pre><a id = 11><b>Building the RNN Model Architecture<\/b><\/a><\/pre>\n","3e1ab034":"<pre><a id = 71><b>Distribution of mean values per row in the training set<\/b><\/a><\/pre>","b508f781":"<pre><a id = 74><b>Distribution of S.D values per column in the training set<\/b><\/a><\/pre>","85b79209":"<pre><a id = 19><b>Applying ISO Transformation<\/b><\/a><\/pre>","ad1d0606":"<pre><a id = 5><b>Define Transformations<\/b><\/a><\/pre>"}}