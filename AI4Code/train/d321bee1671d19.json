{"cell_type":{"59b26ebc":"code","8b8afe8e":"code","3a1f6e5c":"code","4301487d":"code","fdebfc5d":"code","c6a2f9a0":"code","5fc30c78":"code","3a6f89d6":"code","3d2a5196":"code","6309ce91":"code","9f98d458":"code","7d6342e7":"code","da5d1cd1":"code","3e3cbe1c":"code","eca1274f":"code","f7542dfa":"code","fcf71517":"code","ab545fac":"code","aa034075":"code","c616a8c1":"code","d517b2ab":"code","e4cdd44e":"code","4abeb292":"code","69bf4f74":"code","8232180b":"code","ee8d3289":"code","f6ca3003":"code","c5ad47dc":"code","4dfbfd64":"code","8e349355":"code","c34c02bf":"code","5b35868d":"code","a2cab554":"code","7f5505a1":"code","3d9caa2d":"code","b6e0274e":"code","0b81e04d":"code","a09660b2":"code","add5d2ec":"code","9fef348d":"code","a6580738":"code","fad3c662":"code","e45fc802":"code","da1eae1a":"code","1a371e9f":"code","6a23eb55":"code","a9180db6":"code","5a8eae37":"code","7e70edf3":"code","6a852784":"code","74ad016a":"code","a9f759e0":"code","f0663e04":"code","ccdd142b":"code","c93b5208":"code","661a1160":"code","19c1a2d7":"code","adcdb99d":"code","8c1dd31d":"code","c3f5e72b":"code","1b8e108f":"code","a1b69c13":"code","1be85f1b":"code","db1f7e6d":"code","e0370fe0":"code","3933adc9":"code","666516d5":"code","a8a354e5":"code","05710443":"code","e1284b2e":"code","b1681c46":"code","d63a10f5":"code","6fcf0579":"code","b5f91628":"code","b8caf80e":"code","0e6566d2":"code","f1379c93":"code","29bee2e4":"code","484917f0":"code","398aed4e":"code","3675b2cc":"code","8c38d039":"code","858eb7e4":"code","3951fbfc":"code","54560460":"code","504f48a9":"code","b80c36b1":"code","6e09ceba":"code","0ff2b157":"code","84eec8ec":"code","777be415":"code","d4983efc":"code","5da568c2":"code","9795ad38":"code","a6cd2295":"code","60c91a7c":"code","c3fa6373":"code","7defa5fb":"code","d4c5f41d":"code","937ccc3f":"code","acf116eb":"code","89bf0ae8":"code","1dc3bf14":"code","5876e3c8":"code","9a9ccf11":"code","4ccea5a7":"code","b7c95adc":"code","e1f81491":"code","4209e9bc":"code","4cecc7ba":"code","c95c62e7":"code","4e572fb6":"code","94e867be":"code","21b00cc7":"code","71da6fad":"code","63afe1d7":"code","d7c4e42d":"code","46024695":"code","6bbbf5bb":"markdown","617c92fa":"markdown","af7aaad6":"markdown","1e9b75c4":"markdown","15fc7fe9":"markdown","1365848c":"markdown","d2ee6995":"markdown","9e13cdd1":"markdown","eb75f899":"markdown","8d103bab":"markdown","9dc9c4d1":"markdown","ad99ac5a":"markdown","64cc4d5d":"markdown","646ac200":"markdown","e4bca50e":"markdown","e9073565":"markdown","713a69e5":"markdown","cbc005bd":"markdown","eeaad54b":"markdown","cadbec8d":"markdown","5c687c53":"markdown","e2cb1f16":"markdown","c1ce71f5":"markdown","280de43d":"markdown","d7e71ddf":"markdown","0b8b9d7a":"markdown","8260ce6c":"markdown","8ac36abb":"markdown","0b1645ec":"markdown","ab94dd27":"markdown","f8a82b7f":"markdown","7a69876c":"markdown","75554a17":"markdown","c38d6df0":"markdown","065575de":"markdown","fe7e0978":"markdown","09e4ccaa":"markdown","5001142a":"markdown","ba82b557":"markdown","b39c06d4":"markdown","8a2bf553":"markdown","54b51309":"markdown","dfab1b2d":"markdown","d6d4b7ea":"markdown","828d006a":"markdown","39727576":"markdown","ea2d6462":"markdown","20a542f5":"markdown","01832da3":"markdown","d29bbb2b":"markdown","5a186e8e":"markdown","d5f22881":"markdown","5c3dba95":"markdown","a1692661":"markdown","689ea5f5":"markdown","373f1968":"markdown","abbcb45d":"markdown","6b19ab9e":"markdown","f738e6e4":"markdown","f641f074":"markdown","d29127e2":"markdown","0deaf286":"markdown","939cf6f3":"markdown","96ede2de":"markdown","39dde373":"markdown","01f7900e":"markdown","f28da805":"markdown","5da09fbf":"markdown","66cf3320":"markdown","b0ff63a9":"markdown","6120bcdd":"markdown","bcc12677":"markdown","f15e7a98":"markdown","f38cc662":"markdown","a259557a":"markdown","41c1d7ea":"markdown","d53d4ba8":"markdown","824b2004":"markdown","7a9909a2":"markdown","c8b43f78":"markdown","1ae72e3b":"markdown","8d66ef86":"markdown","d7dd74a6":"markdown","5cd325e4":"markdown","22d87a2e":"markdown","2edc8dcb":"markdown","a426b8b8":"markdown","6986955b":"markdown","66fe5901":"markdown","2baed227":"markdown","8d1e519b":"markdown","322a03b6":"markdown","24055c8e":"markdown","c10fd9da":"markdown","aae31edf":"markdown","9523b46b":"markdown","23e4aed9":"markdown","536da321":"markdown","777b7621":"markdown","529f69d1":"markdown","2a57f374":"markdown","f770ccc1":"markdown","59aa2e55":"markdown"},"source":{"59b26ebc":"import json \nimport pandas as pd \nfrom pandas.io.json import json_normalize \ndf = pd.read_json('..\/input\/turo-rental-car-pricing-info\/database.json')\ndf.head()\n# dff=df[['rating','distanceLabel','renterTripsTaken','reviewCount','responseRate','newListing','freeDeliveryPromotion','instantBookDisplayed']]","8b8afe8e":"images_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['images']],ignore_index=True)\nimages_df.head()","3a1f6e5c":"owner_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['owner']],ignore_index=True)\nowner_df.head()","4301487d":"rate_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['rate']],ignore_index=True)\nrate_df.head()\n# rate_dff=rate_df[['averageDailyPrice','monthly']]","fdebfc5d":"distanceWithUnit_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['distanceWithUnit']],ignore_index=True)\ndistanceWithUnit_df.head()\n# distance_dff=distanceWithUnit_df['scalar']","c6a2f9a0":"location_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['location']],ignore_index=True)\nlocation_df.head()\n# location_dff=location_df[['city','longitude','latitude','state']]","5fc30c78":"vehicle_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['vehicle']],ignore_index=True)\nvehicle_df.head()\n# vehicle_dff=vehicle_df[['model','make','id','listingCreatedTime','year','type','automaticTransmission']]","3a6f89d6":"dff=df[['rating','renterTripsTaken','reviewCount','responseRate','newListing','freeDeliveryPromotion','instantBookDisplayed']]\nrate_dff=rate_df['averageDailyPrice']\ndistance_dff=distanceWithUnit_df['scalar']\nlocation_dff=location_df[['city','longitude','latitude','state']]\nvehicle_dff=vehicle_df[['model','make','id','listingCreatedTime','year','type','automaticTransmission']]","3d2a5196":"pdList = [dff, rate_dff,distance_dff,location_dff,vehicle_dff]  # List of your dataframes\nnew_df = pd.concat(pdList,axis=1)","6309ce91":"#new_df.head()\nnew_df.head()","9f98d458":"new_df.to_csv('car rental turo.csv')","7d6342e7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly_express as px\nimport plotly.graph_objects as go\nsns.set()\nfrom pandas.io.json import json_normalize ","da5d1cd1":"df1 = pd.read_csv('..\/input\/car-rental-turo\/car rental turo.csv')\ndf1.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndf1.head()","3e3cbe1c":"df1.info()","eca1274f":"print(df1.isnull().sum())","f7542dfa":"df1.describe()","fcf71517":"print(\"Number of Categories in: \")\nfor ColName in df1[['year','city','state','make','model','type']]:\n    print(\"{} = {}\".format(ColName,len(df1[ColName].unique())))","ab545fac":"#1-What is the most type of vehicle ?\ndf1[\"type\"].value_counts()","aa034075":"#2- What is the most model of vehicle ?\n(df1[\"model\"].value_counts()).head(5)","c616a8c1":"#3- What is the most make of vehicle ?\n(df1[\"make\"].value_counts()).head(5)","d517b2ab":"#4- Which city have most of vehicle ?\n(df1[\"city\"].value_counts()).head(5)","e4cdd44e":"l= ['type', 'model', 'make', 'city', 'state']\n\n\nfor i in l:\n    la=df1[i].value_counts().index\n    la=la[0:5]\n    f, ax = plt.subplots(figsize=(7, 4))\n    sns.countplot(x= i, data= df1, order= la )\n    plt.figure()","4abeb292":"#Which car have a max daily rental price?\n\nprint(df1[df1.averageDailyPrice == df1.averageDailyPrice.max()])","69bf4f74":"sns.lmplot(data= df1, x='renterTripsTaken', y='reviewCount')","8232180b":"# Scatterplot\nf, axes = plt.subplots(figsize = (15,10))\nplt.subplot(221)\nsns.regplot(data= df1, x='renterTripsTaken', y='averageDailyPrice')\nplt.subplot(222)\nsns.regplot(data= df1, x='scalar', y='averageDailyPrice')\nplt.subplot(223)\nsns.regplot(data= df1, x='year', y='averageDailyPrice')\nplt.subplot(224)\nsns.regplot(data= df1, x='listingCreatedTime', y='averageDailyPrice')\n","ee8d3289":"f, ax = plt.subplots(figsize=(18, 7))\nsns.histplot(data=df1, x=\"rating\", binwidth=.01)\nax.set_ylim(0,1000)\nax.set_xlim(2,5)\nplt.title('Rental Car Rating')\nplt.show()\nplt.savefig('Rental Car Rating.png', format='png')","f6ca3003":"f, ax = plt.subplots(figsize=(18, 7))\nsns.histplot(data=df1, x=\"year\")\nax.set_xlim(1990,2021)\nplt.title('vehicle year release')\nplt.show()\nplt.savefig('vehicle year.png', format='png')","c5ad47dc":"labels=df1['type'].value_counts().index\nvalues=df1['type'].value_counts().values\n\n#visualization\nplt.figure(figsize=(7,7))\nplt.pie(values ,labels = labels ,autopct='%1.1f%%')\nplt.title('Vehicle Type')\nplt.show()\nplt.savefig('Vehicle Type.png', format='png')","4dfbfd64":"labels=df1['make'].value_counts().index\nlabels=labels[0:25]\nf, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x='make', data=df1,\n              order = labels,\n              #hue='vehicle.year'\n              palette=\"BuGn_r\"\n           )\nplt.xticks(rotation= 45,fontsize=12 )\nax.set_ylabel('count', fontsize=15, color='b')\nax.set_xlabel('make of the vehicle', fontsize=15, color='b')\n#Save plot as svg file format\nplt.savefig('vehicle make.svg', format='svg', dpi=1200)","8e349355":"labels=df1['state'].value_counts().index\nlabels=labels[0:30]\nf, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x='state', data=df1,\n              order = labels,\n              #hue='vehicle.year'\n              palette=\"Set2\"\n           )\nplt.xticks(rotation= 45,fontsize=12 )\nax.set_ylabel('Car count', fontsize=15, color='r')\nax.set_xlabel('states', fontsize=14, color='r')\n#Save plot as svg file format\nplt.savefig('Car count per state', format='svg', dpi=1200)","c34c02bf":"import plotly_express as px\ndata_make_model = df1.groupby(['make', 'model']).size().reset_index()\ndata_make_model.rename(columns = {0:'model_count'}, inplace=True)\ndata_make_model['make_count'] = data_make_model['make'].apply(\n    lambda x : data_make_model[data_make_model['make'] == x]['model_count'].sum())\ndata_make_model.sort_values(by = 'make_count', ascending=False, inplace=True)\nfig =px.scatter(data_make_model[data_make_model['make_count'] >45],\n             x = 'make', y='model_count', color = 'model',width=1100, height=700,\n                title='Make and Model of Top Most Rented Cars')\nfig.show()","5b35868d":"from matplotlib import pyplot\nsns.set_context(\"notebook\")\ndim = (20.00, 10.00)\nfig, ax = pyplot.subplots(figsize=dim)\nsns.set(style=\"white\")\n# Draw a categorical scatterplot to show each observation\nsns.boxplot(y=\"averageDailyPrice\", x=\"model\",  \n              data=df1.groupby('model').filter(lambda x: len(x) >= 30),\n              order = df1.groupby('model').filter(lambda x: len(x) >= 30)\\\n                     .groupby(['model']).median()\\\n                    .sort_values('averageDailyPrice', ascending = False)\\\n                    .iloc[0:50]\n                     .index)\n\nax.xaxis.grid(True)\nax.yaxis.grid(False)\nsns.despine(left=True, bottom=True)\n\nplt.ylim(0, 1200)\nplt.xticks(rotation=75, fontsize=14)\nax.set_xlabel('vehicle model', fontsize=14)\nax.set_ylabel('daily rate (dollars per day)', fontsize=14)","a2cab554":"from shapely.geometry import Point # Shapely for converting latitude\/longtitude to geometry\nimport geopandas as gpd # To create GeodataFrame","7f5505a1":"# creating a geometry column \ngeometry = [Point(xy) for xy in zip(df1['longitude'], df1['latitude'])]\n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n# Creating a Geographic data frame \ngdf = gpd.GeoDataFrame(df1, crs=crs, geometry=geometry)","3d9caa2d":"gdf.head(3)","b6e0274e":"gdf.plot(column = 'averageDailyPrice',figsize=(20, 8), legend = True)\nplt.title('Daily price per locations')\nplt.xlabel('longitude')\nplt.ylabel('latitude');\n","0b81e04d":"# Folium map \nimport folium\nmap = folium.Map([34.11, -118.356052],zoom_start = 12)\n# grouping dataframe by category\ndf = df1.groupby(\"city\")\nmap","a09660b2":"# extracting  all the rows with Los Angeles and land rover car\nLR = df.get_group('Los Angeles')\nLR = LR[LR[\"make\"]=='Land Rover']\nLR1 = LR [['latitude','longitude','make','city']]\nprint(LR1.head(5))\n\n# Separating the cars locations and converting each attribute into list\nlat = list(LR1[\"latitude\"])\nlon = list(LR1[\"longitude\"])\nplace = list(LR1[\"make\"])\ncat = list(LR1[\"city\"])\n\n# visualize \/ locate cars--->Markers in red\nfor lt,ln,pl,cat in zip(lat,lon,place,cat):\n    folium.Marker(location = [lt,ln], tooltip = str(pl) +\",\"+str(cat), icon = folium.Icon(color = 'red')).add_to(map)\nmap ","add5d2ec":"from geopy.distance import GreatCircleDistance\n# latitude and longitude of airport\nSource = (33.9416,-118.4085)\n\n# Empty list to store the distance\ndistance = []\nfor lt,ln in zip(lat,lon):\n    dist = GreatCircleDistance(Source,(lt,ln))\n    distance.append(dist)\n\n# Draw lines between points\nfor dist,lt,ln in zip(distance,lat,lon):\n    if (dist > 0) and (dist <= 20):\n        folium.PolyLine([Source,(lt,ln),],color = \"green\", weight = 1.5).add_to(map)  \n    elif (dist > 20) and (dist <= 30):\n        folium.PolyLine([Source,(lt,ln)],color = \"orange\", weight = 1).add_to(map)\n    else :\n        folium.PolyLine([Source,(lt,ln)],color = \"red\", weight = .5).add_to(map)\nmap","9fef348d":"#display randomly five distance\ndistance[:5]","a6580738":"import folium\nfrom folium.plugins import HeatMap\ncenter = [35.486798, -100.096052]  #data.describe(mean)\nm = folium.Map([df1.latitude.mean(), df1.longitude.mean()], zoom_start=4,center=center)\nfor index, row in df1.iterrows():\n    folium.CircleMarker([row['latitude'], row['longitude']],\n                        radius=row['renterTripsTaken']\/10,\n                        fill_color=\"#3db7e4\", \n                       ).add_to(m)\n    \npoints = df1[['latitude', 'longitude']].values\nm.add_children(HeatMap(points, radius=15)) # plot heatmap\nm.save('map.html')\nm","fad3c662":"df_state = pd.DataFrame(df1['state'].value_counts()).reset_index()\ndf_state.rename(columns = {'index':'state', 'state':'count'}, inplace=True)\n\nfig = go.Figure(data=go.Choropleth(\n    locations=df_state['state'], # Spatial coordinates\n    z = df_state['count'].astype(float), # Data to be color-coded\n    locationmode = 'USA-states', # set of locations match entries in `locations`\n    colorscale = 'Reds',\n    colorbar_title = \"Number of Cars Rented\",\n))\n\nfig.update_layout(\n    title_text = 'Car Rentals by State',\n    geo_scope='usa', # limite map scope to USA\n)\n\nfig.show()","e45fc802":"import geopandas as gpd\nimport geoviews as gv\nimport geoviews.feature as gf\nfrom geoviews import opts\nfrom cartopy import crs \nimport geoviews.tile_sources as gts\ngv.extension('bokeh','matplotlib')","da1eae1a":"gv.Layout([ts.relabel(name) for name, ts in gts.tile_sources.items()]).opts(\n 'WMTS', xaxis=None, yaxis=None, width=225, height=225).cols(6)","1a371e9f":"gv.tile_sources.OSM.opts( width=800, height=600)","6a23eb55":"points=gv.Points(gdf, vdims=[('renterTripsTaken','trips'), ('city', 'city')]).options(\n    tools=['hover', 'zoom_in', 'zoom_out','wheel_zoom'], width=800, height=500 \n)","a9180db6":"(gv.tile_sources.CartoDark * points).opts(opts.Points(global_extent=False, width=800, height=600, size=8, color='red'))","5a8eae37":"df1['listingCreatedTime'] = pd.to_datetime(df1['listingCreatedTime'],unit='ms')\ndf1.head()","7e70edf3":"df1['listingyear'] = df1['listingCreatedTime'].dt.year\ndf1['month'] = df1['listingCreatedTime'].dt.month\ndf1['weekday'] = df1['listingCreatedTime'].dt.dayofweek","6a852784":"# what is the date range?\nprint('begin: {0}, end: {1}'.format(df1.listingCreatedTime.min(), df1.listingCreatedTime.max()))","74ad016a":"tab1=df1.groupby([\"make\",\"model\", \"year\",\"state\"]).agg({'averageDailyPrice': ['min', 'max']})\ntab1.sort_values(['model',\"year\"], ascending=[True,True], inplace=True)\ntab1.head(35)","a9f759e0":"pop=pd.read_csv('..\/input\/us-population-20002019\/state_populations_2000_to_2019.csv')\npop.head()","f0663e04":"pop = pop[['state_name','pop_2018']]\npop=pop.rename(columns={'state_name': 'state', 'pop_2018': 'population'})\npop.head()","ccdd142b":"df1['state'].unique()","c93b5208":"# before we change states name i will make a copy from our df1 to visualization purposes using go.Figure()\ndf=df1.copy()","661a1160":"df1.replace({'state' : {             'WA' : 'Washington', 'NM' : 'New Mexico', 'GA' : 'Georgia',\n                                    'FL' :  'Florida' , 'TX' : 'Texas' , 'NC' : 'North Carolina',\n                                    'SC' : 'South Carolina', 'CT' : 'Connecticut', 'MA' : 'Massachusetts',\n                                    'ME' : 'Maine', 'AL' : 'Alabama', 'MT' : 'Montana',\n                                    'TN' : 'Tennessee', 'KY' : 'Kentucky', 'ID' : 'Idaho' ,\n                                    'UT' : 'Utah', 'MD' : 'Maryland', 'DC' : 'Washington' ,\n                                    'IA' : 'Iowa', 'OH' : 'Ohio', 'CO' : 'Colorado' ,\n                                    'VA' : 'Virginia', 'MI' : 'Michigan', 'NJ' : 'New Jersey' ,\n                                    'IN' : 'Indiana', 'WI' : 'Wisconsin', 'KS' : 'Kansas' ,\n                                    'MO' : 'Missouri', 'NV' : 'Nevada', 'CA' : 'California' ,\n                                    'LA' : 'Louisiana', 'AR' : 'Arkansas', 'IL' : 'Illinois' ,\n                                    'MS' : 'Mississippi', 'NH' : 'New Hampshire', 'MN' : 'Minnesota' ,\n                                    'OK' : 'Oklahoma', 'NE' : 'Nebraska', 'OR' : 'Oregon' ,\n                                    'PA' : 'Pennsylvania', 'DE' : 'Delaware', 'AZ' : 'Arizona' ,\n                                    'WV' : 'West Virginia', 'RI' : 'Rhode Island', 'AK' : 'Alaska',\n                                    'HI' : 'Hawaii', 'VT' : 'Vermont','ND' : 'North Dakota', 'WY' : 'Wyoming',\n                                    'SD' : 'South Dakota'}},\n              inplace=True)","19c1a2d7":"cars = pd.merge(df1, pop, how=\"left\", on=\"state\")","adcdb99d":"cars.head(10)","8c1dd31d":"#Let's take a look to the realtion between pop and daily rate.\nsplot = sns.regplot(x=\"population\", y=\"averageDailyPrice\", \n                    data=cars,\n                    scatter_kws={'alpha':0.2},\n                    fit_reg=False)\nsplot.set(xscale=\"log\")","c3f5e72b":"df.corr()['averageDailyPrice'].sort_values(ascending=False)","1b8e108f":"f, ax = plt.subplots(figsize=(16, 16))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdBu\")\nplt.show()","a1b69c13":"cars.isnull().sum()","1be85f1b":"cars['responseRate'] = cars['responseRate'].fillna(0)","db1f7e6d":"cars['rating'].fillna((cars['rating'].mean()), inplace=True)","e0370fe0":"cars['Listing Difference']=cars['listingyear']-cars['year']\ncars.drop(\"listingCreatedTime\", axis=1, inplace=True)\ncars.head()","3933adc9":"labels = cars['type'].astype('category').cat.categories.tolist()\nreplace_map_comp  = {'type' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n\nprint(replace_map_comp )","666516d5":"cars.replace(replace_map_comp, inplace=True)\n","a8a354e5":"cat_coulmns=['newListing' , 'freeDeliveryPromotion' , 'instantBookDisplayed' , 'city' , 'state', 'model' , 'make' ,'type' ,'automaticTransmission']\ndef Encoding(df,coulmn_name):\n    labels = df[coulmn_name].astype('category').cat.categories.tolist()\n    replace_map_comp  = {coulmn_name : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n\n    #return replace_map_comp\n\n    df.replace(replace_map_comp, inplace=True)\n    return df.head()\n    \nfor column in cat_coulmns:\n    \n    Encoding(cars,column)","05710443":"cars.head(3)","e1284b2e":"cars['population'] = pd.to_numeric(cars['population'], errors='coerce')\ncars['population']=cars['population'].round().astype('Int64')","b1681c46":"cars['population'].dtype","d63a10f5":"cars.columns","6fcf0579":"cars1 = cars.drop([\"city\",\"state\",'reviewCount','id','instantBookDisplayed','geometry'], axis=1).copy() ","b5f91628":"# sns.boxplot(df['averageDailyPrice'])\n# plt.title(\"Box Plot before outlier removing\")\n# plt.show()\n# def drop_outliers(df, field_name):\n#     iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))\n#     df.drop(df[df[field_name] > (iqr + np.percentile(df[field_name], 75))].index, inplace=True)\n#     df.drop(df[df[field_name] < (np.percentile(df[field_name], 25) - iqr)].index, inplace=True)\n# drop_outliers(df, 'averageDailyPrice')\n# sns.boxplot(df['averageDailyPrice'])\n# plt.title(\"Box Plot after outlier removing\")\n# plt.show()","b8caf80e":"cars1.head(2)","0e6566d2":"# Separating target variable and its features\ny = cars1['averageDailyPrice']\nX = cars1.drop('averageDailyPrice',axis=1)","f1379c93":"from sklearn.preprocessing import StandardScaler\n\n# define standard scaler\nscaler = StandardScaler()\n# transform data\ncars_main1 = scaler.fit_transform(X)","29bee2e4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cars_main1, y, test_size=0.2, random_state=None)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","484917f0":"# import all what you need for machine learning\nimport sklearn\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","398aed4e":"#fit decision tree\ntree = DecisionTreeRegressor()\ntree.fit(X_train, y_train)\n#fit random forest\nforest = RandomForestRegressor(n_jobs=-1)\nforest.fit(X_train, y_train)\n#fit XGBoost\nxg_reg = xgb.XGBRegressor(objective ='reg:linear')\nxg_reg.fit(X_train, y_train)","3675b2cc":"models= [('XGBoost', xg_reg), ('random forest', forest), ('decision tree', tree)]\n\nfor i, model in models:    \n    predictions = model.predict(X_train)\n    MSE = mean_squared_error(y_train, predictions)\n    RMSE = np.sqrt(MSE)\n    msg = \"%s = %.2f\" % (i, round(RMSE, 2))\n    print('RMSE of', msg)","8c38d039":"for i, model in models:\n    # Make predictions on train data\n    predictions = model.predict(X_train)\n    # Performance metrics\n    errors = abs(predictions - y_train)\n    # Calculate mean absolute percentage error (MAPE)\n    mape = np.mean(100 * (errors \/ y_train))\n    # Calculate and display accuracy\n    accuracy = 100 - mape    \n    #print result\n    msg = \"%s= %.2f\"% (i, round(accuracy, 2))\n    print('Accuracy of', msg,'%')","858eb7e4":"models= [('XGBoost', xg_reg), ('forest', forest), ('dt', tree)]\nscoring = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n\n#for each model I want to test three different scoring metrics. Therefore, results[0] will be lin_reg x MSE, \n# results[1] lin_reg x MSE and so on until results [8], where we stored dt x r2\n\nresults= []\nmetric= []\nfor name, model in models:\n    for i in scoring:\n        scores = cross_validate(model, X_train, y_train, scoring=i, cv=10, return_train_score=True)\n        results.append(scores)","3951fbfc":"#this is an example of the stored results,R2 score for XGBoost\nresults[2]","54560460":"#THIS IS FOR XGBoost\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nXG_RMSE_mean = np.sqrt(-results[0]['test_score'].mean())\nXG_RMSE_std= results[0]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nXG_MAE_mean = -results[1]['test_score'].mean()\nXG_MAE_std= results[1]['test_score'].std()\nXG_r2_mean = results[2]['test_score'].mean()\nXG_r2_std = results[2]['test_score'].std()\n\n#THIS IS FOR RF\nRF_RMSE_mean = np.sqrt(-results[3]['test_score'].mean())\nRF_RMSE_std= results[3]['test_score'].std()\nRF_MAE_mean = -results[4]['test_score'].mean()\nRF_MAE_std= results[4]['test_score'].std()\nRF_r2_mean = results[5]['test_score'].mean()\nRF_r2_std = results[5]['test_score'].std()\n\n#THIS IS FOR DT\nDT_RMSE_mean = np.sqrt(-results[6]['test_score'].mean())\nDT_RMSE_std= results[6]['test_score'].std()\nDT_MAE_mean = -results[7]['test_score'].mean()\nDT_MAE_std= results[7]['test_score'].std()\nDT_r2_mean = results[8]['test_score'].mean()\nDT_r2_std = results[8]['test_score'].std()","504f48a9":"modelDF = pd.DataFrame({\n    'Model'       : ['XGBoost', 'Random Forest', 'Decision Trees'],\n    'RMSE_mean'    : [XG_RMSE_mean, RF_RMSE_mean, DT_RMSE_mean],\n    'RMSE_std'    : [XG_RMSE_std, RF_RMSE_std, DT_RMSE_std],\n    'MAE_mean'   : [XG_MAE_mean, RF_MAE_mean, DT_MAE_mean],\n    'MAE_std'   : [XG_MAE_std, RF_MAE_std, DT_MAE_std],\n    'r2_mean'      : [XG_r2_mean, RF_r2_mean, DT_r2_mean],\n    'r2_std'      : [XG_r2_std, RF_r2_std, DT_r2_std],\n    }, columns = ['Model', 'RMSE_mean', 'RMSE_std', 'MAE_mean', 'MAE_std', 'r2_mean', 'r2_std'])\n\nmodelDF.sort_values(by='r2_mean', ascending=False)","b80c36b1":"sns.factorplot(x= 'Model', y= 'RMSE_mean', data= modelDF, kind='bar', legend='True')","6e09ceba":"#Grid Search\nparam_grid = [\n{'n_estimators': [50,100,500], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\ngrid_search_forest = GridSearchCV(forest, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_forest.fit(X_train, y_train)","0ff2b157":"#now let's how the RMSE changes for each parameter configuration\ncvres = grid_search_forest.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","84eec8ec":"#find the best model of grid search\ngrid_search_forest.best_estimator_","777be415":"#fit random forest\ngrid_best = RandomForestRegressor(bootstrap=False, max_features=10, n_estimators=500,n_jobs=-1)\n#grid_best= grid_search_forest.best_estimator_.predict(x_train)\ngrid_best.fit(X_train, y_train)\n# Performance metrics (R2)\nfrom sklearn.metrics import r2_score\n# R2 score of train set\ny_pred_train = grid_best.predict(X_train)\nR2_train_model = r2_score(y_train,y_pred_train)\nprint(\"'The best model from the grid search has an train R2-score :\",round(R2_train_model,2))","d4983efc":"#RMSE\ngrid_mse = mean_squared_error(y_train, y_pred_train)\ngrid_rmse = np.sqrt(grid_mse)\nprint('The best model from the grid search has a RMSE of', round(grid_rmse, 2))","5da568c2":"from pprint import pprint\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\npprint(random_grid)","9795ad38":"# Use the random grid to search for best hyperparameters\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 50 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1, scoring='neg_mean_squared_error')\n# Fit the random search model\nrf_random.fit(X_train, y_train)","a6cd2295":"#now let's how the RMSE changes for each parameter configuration\ncvres2 = rf_random.cv_results_\nfor mean_score, params in zip(cvres2[\"mean_test_score\"], cvres2[\"params\"]):\n    print(np.sqrt(-mean_score), params)","60c91a7c":"# best random model \nrf_random.best_estimator_","c3fa6373":"# Performance metrics (R2)\nfrom sklearn.metrics import r2_score\n# R2 score of train set\ny_pred_train1 = rf_random.predict(X_train)\nR2_train_model1 = r2_score(y_train,y_pred_train1)\nprint(\"'The best model from the randomized search has an train R2-score :\",round(R2_train_model1,2))\n","7defa5fb":"#this is the RMSE\nfinal_mse = mean_squared_error(y_train, y_pred_train1)\nfinal_rmse = np.sqrt(final_mse)\nprint('The best model from the randomized search has a RMSE of', round(final_rmse, 2))","d4c5f41d":"# extract the numerical values of feature importance from the grid search\nimportances = rf_random.best_estimator_.feature_importances_\n\n#create a feature list from the original dataset (list of columns)\n# What are this numbers? Let's get back to the columns of the original dataset\nfeature_list = list(X.columns)\n\n#create a list of tuples\nfeature_importance= sorted(zip(importances, feature_list), reverse=True)\n\n#create two lists from the previous list of tuples\ndf = pd.DataFrame(feature_importance, columns=['importance', 'feature'])\nimportance= list(df['importance'])\nfeature= list(df['feature'])\n\n#see df\nprint(df)","937ccc3f":"# Set the style\nplt.style.use('bmh')\n# list of x locations for plotting\nx_values = list(range(len(feature_importance)))\n\n# Make a bar chart\nplt.figure(figsize=(15,10))\nplt.bar(x_values, importance, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","acf116eb":"final_model = rf_random.best_estimator_\n# Predicting test set results\nfinal_pred1 = final_model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_pred1)\nfinal_rmse1 = np.sqrt(final_mse)\nprint('The final RMSE on the test set is', round(final_rmse1, 2))","89bf0ae8":"# R2 score of test set\ny_pred_test1 = final_model.predict(X_test)\nR2_test_model1 = r2_score(y_test,y_pred_test1)\nprint(\"The best model achieves on the test set an R2-score of :\",round(R2_test_model1,2))","1dc3bf14":"# R2 mean of train set using Cross validation\ncross_val = cross_val_score(final_model ,X_train ,y_train ,cv=5)\ncv_mean = cross_val.mean()\n\nprint(\"Train CV scores :\",cross_val)\nprint(\"Train CV mean :\",round(cv_mean,2))","5876e3c8":"#Let's drop the irrelevant columns\ncars2 = cars.drop([\"city\",\"state\",'reviewCount','id',\n                   'population','automaticTransmission','newListing','geometry'], axis=1).copy() ","9a9ccf11":"cars2.head(2) ","4ccea5a7":"# Separating target variable and its features\ny2 = cars2['averageDailyPrice']\nX2 = cars2.drop('averageDailyPrice',axis=1)","b7c95adc":"from sklearn.preprocessing import StandardScaler\n\n# define standard scaler\nscaler = StandardScaler()\n# transform data\ncars_main2 = scaler.fit_transform(X2)","e1f81491":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(cars_main2, y2, test_size=0.2, random_state=None)\nprint(\"x train: \",X_train2.shape)\nprint(\"x test: \",X_test2.shape)\nprint(\"y train: \",y_train2.shape)\nprint(\"y test: \",y_test2.shape)","4209e9bc":"import xgboost as xgb\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.4, learning_rate = 0.1,\n                max_depth = 10, alpha = 10, n_estimators = 100)\nxg_reg.fit(X_train2,y_train2)\n\npreds = xg_reg.predict(X_test2)","4cecc7ba":"rmse = np.sqrt(mean_squared_error(y_test2, preds))\nprint(\"RMSE: %f\" % (rmse))","c95c62e7":"data_dmatrix = xgb.DMatrix(data=X2,label=y2)\nparams = {\"objective\":\"reg:linear\",'colsample_bytree': 0.4,'learning_rate': 0.3,\n                'max_depth': 10, 'alpha': 10, 'gamma':5}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=100,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","4e572fb6":"cv_results.head()","94e867be":"#Extract and print the final boosting round metric.\nprint((cv_results[\"test-rmse-mean\"]).tail(1))","21b00cc7":"from sklearn.metrics import r2_score\npreds_t = xg_reg.predict(X_train2)\nr2_score(y_train2,preds_t)\n","71da6fad":"r2_score(y_test2,preds)","63afe1d7":"xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)","d7c4e42d":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [11, 11]\nplt.show()","46024695":"# #Try RandomizedSearchCV\n# Xgbo=xgb.XGBRegressor()\n# # Rate at which correcting is being made\n# learning_rate = [0.001, 0.01, 0.1, 0.2]\n# # Number of trees in Gradient boosting\n# n_estimators=list(range(100,1000,100))\n# # Maximum number of levels in a tree\n# colsample_bytree=list(range(0.5,1,0.1))\n# # Minimum number of samples required to split an internal node\n# gamma=[0,1,5]\n# # subsample.\n# subsample=[0.8,0.85,0.9,0.95,1]\n# # Hyperparameters dict\n# param_grid = {\"learning_rate\":learning_rate,\n#               \"n_estimators\":n_estimators,\n#               \"colsample_bytree\":colsample_bytree,\n#               \"gamma\":gamma,\n#               \"subsamplf\":subsampl}\n\n# xg_reg = RandomizedSearchCV(estimator = Xgbo, param_distributions = param_grid)\n# xg_reg = XGBRegressor( \n#                       scale_pos_weight=1,\n#                       learning_rate=0.01,  \n#                       colsample_bytree = 0.4,\n#                       subsample = 0.8,\n#                       objective='reg:squarederror', \n#                       n_estimators=1000, \n#                       alpha = 0.9,\n#                       max_depth=6, \n#                       gamma=10)\n\n","6bbbf5bb":"Notice that the `rating` attribute has 15219 car are missing value.As well `responseRate` attribute has 8434 missing value .  We will need to take care of this later.\n\n","617c92fa":"**A future aim may be to cut the less relevant features.** : rating ,automaticTransmission ,freeDeliveryPromotion \n ,newListing","af7aaad6":"**Feature Importance**","1e9b75c4":"In conclusion, the metrics for our best models on the training set after our hyperparameter fine-tuning are the following:\n\n**1-Grid-search:**\n\n.RMSE: 0.0\n\n.accuracy (R2): 100%\n\n**2-Randomized Search:**\n\n.RMSE: 40.34\n\n.accuracy: 87 %\n\nThis of course does not mean that the Randomized Search is inherently inferior to gridsearch search. It's just that in this notebook I used the information provided by the former to refine the latter. Moreover, I tested more combinations with the randomized search. In any case, this time I will consider the model obtained by the randomized search as the best one. First I will plot the feature importance for this model. This will give me precious insight to the most important factor in predicting our dependent variable Price. Then I will apply the model to the test set.","15fc7fe9":"**Histogram of vehicle year release**","1365848c":"Now, let's find out about the performance metrics of our new model: R2 and RMSE.","d2ee6995":"California, Florida,and Texas are top three states.","9e13cdd1":"#### Data Scaling","eb75f899":"#### 4-Data Cleaning","8d103bab":"![car aps.jpg](attachment:09c1f9de-7c87-4564-8eb6-ea2a2af5ad8f.jpg)","9dc9c4d1":"## 2.b Geospatial Analysis","ad99ac5a":"The count, **mean**, **min**, and **max** rows are self-explanatory. Note that the null values are ignored (so, for example, count of rating is `21060`, not `36279`). The std row shows the standard deviation, which measures how dispersed the values are. The `25%, 50%, and 75%` rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations falls. For example, `25%` of the cars have a rate.daily lower than `39`, while `50%` are lower than `64` and `75%` are lower than `110`. These are often called the 25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).","64cc4d5d":"#We should run it first time only \n\n**Best Estimator:** \nRandomForestRegressor(max_depth=45, min_samples_split=10, n_estimators=1625,\n                      n_jobs=-1)","646ac200":"#### Images","e4bca50e":"# Part 1\n### Explore nested array or list which we would like to flatten","e9073565":"Use the replace() function on the DataFrame by passing the mapping dictionary as argument:","713a69e5":"#### rate","cbc005bd":"**After several trials, I found that deleting these columns greatly improves the performance of the model**","eeaad54b":"all the latitude\nlongitude coordinate pairs were converted into\nGeomtric (GIS) points using the shapely framework in\nPython using a traditional **EPSG:4326** projection space.","cadbec8d":"#### 9-Data Scaling","5c687c53":"**Discrete\/ Categorical Data:** discrete data is quantitative data that can be counted and has a finite number of possible values or data which may be divided into groups e.g. days in a week, number of months in a year, sex (Male\/Female\/Others), Grades (High\/Medium\/Low), etc.","e2cb1f16":"#### 8-Detect Outliers","c1ce71f5":"from a quick view of the database, renter trips taken it seems to be very similar to Number of review counts. Let's check it with a simple scatterplot: as you can see below the relationship is almost perfectly linear, which tells me that they are almost a perfect match. I do not think that the variable provides additional information that trips taken . \nWe will drop `reviewcount` column ","280de43d":"#### 1-Convert to datetime type","d7e71ddf":"... and then test them on train data, in terms of RMSE:","0b8b9d7a":"#### a. Grid Search\nIn this grid search I will try different combinations of RF hyperparameters.\n\n**Most important hyperparameters of Random Forest:**\n\n-`n_estimators` = n of trees\n\n-`max_features` = max number of features considered for splitting a node\n\n-`max_depth` = max number of levels in each decision tree\n\n-`min_samples_split` = min number of data points placed in a node before the node is split\n\n-`min_samples_leaf` = min number of data points allowed in a leaf node\n\n-`bootstrap` = method for sampling data points (with or without replacement)\n","8260ce6c":"**Histogram of car count per top 30 states locations**","8ac36abb":"#### 6- Encoding Categorical Data","0b1645ec":"Let's ask ourselves a question here, is the same car model the same price in all states, or is there a difference in prices ? ","ab94dd27":"Let us shed light on a one sample.As we see that the `BMW` Model `1 Series` car with the year of manufacture is 2012, it's rate varies witin 6 different states. As a data scientist, your duty here is to find the answer this question :\n\n**What is the reason for the difference in daily car rent in different states?**","f8a82b7f":"Using the **merge()** function, for each of the rows in the **df** table, the corresponding coordinates are added from the **pop** table. Both tables have the column state in common which is used as a key to combine the information.\n\nBy choosing the **left join**, only the locations available in the **df** (left) table, end up in the resulting table.","7a69876c":"# Part 4 : Machine learning ","75554a17":"**the best model of grid search**\n\nRandomForestRegressor(bootstrap=False, max_features=10, n_estimators=500,\n                      n_jobs=-1)","c38d6df0":"### Save dataframe into CSV file","065575de":"The correlation coefficient ranges from `\u20131` to`1`. When it is close to `1`, it means that there is a strong positive correlation. When the coefficient is close to `\u20131`, it means that there is a strong negative correlation; you can see a small negative correlation between the **freeDeliveryPromotion** and the **averageDailyPrice** value . Finally, coefficients close to zero mean that there is no linear correlation.","fe7e0978":"lets say we want to display only the car within **Los Angeles** borders, and to make the matter more complicated, we also want to display luxury **Land Rover** cars instead of the rest of the cars.<br>\nFinally, we want to calculate the distances between all Land Rover cars and between **Los Angeles Airport** because we notice that many tourists, upon disembarking from the airport, request this car, all of this is presented to us by the great Folium .","09e4ccaa":"#### Basemaps\nGeoviews offers some neat base maps. Let us see what is available.","5001142a":"**Make and Model count of Top 25 Most Rented Cars**","ba82b557":"**Data Collection\u00b6**\n\nI attempt to compare a spatially\ninspired Random Forest with XGBoost models,\n for predicting daily rental rates, using\ndata from **Turo**.\n\nThe first dataset utilized was obtained from the kaggle profile of `Christopher Lambert`, and consisted of a `330MB JSON file` with a heavily nested format. The inital goal for data processing was to convert the data to a flat file format to facilitate further processing. Upon initially reading in the datasets using the Python Package Pandas, it was found that the dataset consisted of `36000` datapoints, each of which corresponded to a single car rental event.<br> Our data represents `837` vehicl models in more than `2900` cities around US . Given the size of the dataset, and the heavily nested nature of the data source, it was soon found that simple iteration and Pandas apply functions were impractical to flatten the dataset.","b39c06d4":"### convert latitude\/longtitude columns to Geometry Column","8a2bf553":"**Plotting the data on a map with Folium**\nThe data can be presented on a map, showing where is the car parking , with points radius based on number of trips taken\n\n**Folium** is a Python library used for visualizing geospatial data. It is easy to use and yet a powerful library. Folium is a Python wrapper for Leaflet. js which is a leading open-source JavaScript library for plotting interactive maps","54b51309":"Now I want to compute for each combination the mean and std, so that to put into a dataframe all the metrics and easily compare them","dfab1b2d":"### Interactive Geospatial Data Visualization with Geoviews","d6d4b7ea":"**Read Database file**","828d006a":"#### Missing Value","39727576":"cv_results contains train and test RMSE metrics for each boosting round.","ea2d6462":"Upon researching, i found that the daily rental rate for cars is affected by the `population` for particular area. Wherefore we need another dataset that exposure the population distribution in the different states in US.\n\nWhen searching, I was able to find a dataset showing the population of all states from 2000 to 2019","20a542f5":"**Let's Create an Encoding Function**","01832da3":"Compute the rmse by invoking the `mean_sqaured_error` function from sklearn's metrics module.","d29bbb2b":"Let\u2019s look at the other fields. The describe() method shows a summary of the numerical attributes","5a186e8e":"#### vehicle","d5f22881":"#### 3-Looking for Correlations\n\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using the corr() method:","5c3dba95":"I will cross-validate the three models and compare them . \n","a1692661":"Yes, as we expected, there is a direct relationship between them.","689ea5f5":"These results are interesting: it seems that decision tree is by far the best model, as its error is the lowest. However, at this point we are only evaluating our models on the train set, so the risk of overfitting for decision trees is quite high. To understand better the performances of the models we can use an alternative strategy: **cross-validation**.","373f1968":"**I will highlight to clarify the meaning of some features that may not be clear, but not all**\n\n`Response Rate`: The Response Rate is the number of\ntimes on average a host replied to a rental request (in percent). In case no data was present, it was imputed with a 0 percent.<br> \n`Make, Model, Year`: Represents the Make of the Car,\nModel of the Car, and the year in which the car was\nreleased.<br> \n`Renter Trips Taken`: This represents the number of\ntimes the car has been rented from that particular\nrenter.<br>\n`scalar`: The distance driven during the rental.<br>\n`Instant Book Displayed`: Whether the car could be\nbooked without any request processing time to the\nrenter.<br>\n`averageDailyPrice`: The price for each particular car rental. This is\nour response variable.","abbcb45d":"#### 5-Features Selection : \n\n**Important rule :** If two features are highly correlated, both of their scores largely decrease, regardless of the quality of the features. therefore , we will drop `city` , `state` , `reviewcount`","6b19ab9e":"#### k-fold Cross Validation using XGBoost\nXGBoost supports k-fold cross validation via the `cv()` method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build.\n\nThis time you will create a hyper-parameter dictionary `params` which holds all the hyper-parameters and their values as key-value pairs but will exclude the `n_estimators` from the hyper-parameter dictionary because you will use `num_boost_rounds` instead.\n\nYou will use these parameters to build a `3-fold` cross validation model by invoking XGBoost's `cv()` method and store the results in a `cv_results` DataFrame.\n\nBut first we will convert the dataset into an optimized data structure called **Dmatrix** that XGBoost supports and gives it acclaimed performance and efficiency gains.","f738e6e4":"# Price Prediction for Car Rentals","f641f074":"#### Replace Values Method\nThis will be useful when the categories count is high and you don't want to type out each mapping. You will store the category names in a list called labels and then zip it to a seqeunce of numbers and iterate over it.\n\n**Columns** : [newListing , freeDeliveryPromotion , instantBookDisplayed , city , state, model , make ,type ,automaticTransmission]","d29127e2":"We need to convert the state codes to names, so that we can merge the two tables. It is possible to scrape the table from Wikipedia that contains the names of the states and the code for each state, then we automatically replace the code in our data with the name of the state, but here I chose the manual method for changing the names, I expect it is faster in our case now.","0deaf286":"Display all cars ...","939cf6f3":"**b. Randomized search**\n\nThe grid search approach is often too costly, as many combinations are tested. In these cases it is easier to use a randomized search, that evaluates a only an user defined number of random combinations for each hyperparameter at every iteration. This way we could also test more hyperparameters.","96ede2de":"#### location","39dde373":"The global car rental industry is expected to reach an estimated $120 billion by 2025 with a CAGR of 6.1% from 2020 to 2025.\n\nThe future of the global car rental industry looks promising with opportunities in business and leisure travel industry. The major drivers for this market are the growing global tourism industry, an increase in international air travelers, and rising income levels across the globe. ","01f7900e":"#### distanceWithUnit","f28da805":"# Part 2 : Exploratory Data Analysis (EDA)","5da09fbf":"Upon initially\nreading in the datasets using the Python Package\nPandas, it was found that the dataset consisted of\n**36279** datapoints, each of which corresponded to a\nsingle car rental event. ","66cf3320":"**Listing Difference :** This feature is an engineered\nfeature, which represents the difference between the\nthe year in which a car was listed on the website, and\nthe year in which the car was released. Intuitively,\nif this difference is high, it should ideally mean that\neither this car is a classic (high rates), or is a really\nold car at the end of its lifetime.","b0ff63a9":"Display Numver of cars per each state using `go.figure plotly`","6120bcdd":"Fit our three basic models...","bcc12677":"In Geoviews, this is as simple as using * to overlay any two (more) features. In our example, we overlay `CartoDark base map` and city `points`.","f15e7a98":"**Histogram of Rental Car Rating**","f38cc662":"By `pd.concat` we will merge all dataframes agian,by default `axis=0` along the rows and `axis=1` along the columns.","a259557a":"## 2.a Discover and Visualize the Data to Gain Insights","41c1d7ea":"We can use the Mean absolute percentage error (MAPE) to compute a measure of accuracy that is more immediate to understand.","d53d4ba8":"As of this date, there are 19 different base maps you can use in Geoviews, so let us try some of them.","824b2004":"convert **population** from Object type to a int type ","7a9909a2":"#### Owner","c8b43f78":"With only these 3 lines of code, Geometry column is created inside a Geopandas GeodataFrame.","1ae72e3b":"#### 7-Check data type","8d66ef86":"#### Visualize Feature Importance","d7dd74a6":"## 1- Fine-tune Random Forest","5cd325e4":"**Histogram of vehicle make of Top 25 Most Rented Cars** ","22d87a2e":"You can find out what categories exist and how many types belong to each category by using the value_counts() method.<br>\nwe need to answer many questions like :","2edc8dcb":"#### 2-Add new dataset","a426b8b8":"#### XGBoost's hyperparameters\n\nAt this point, we should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost . But the most common ones that you should know are: \n\n`learning_rate:` step size shrinkage used to prevent overfitting. Range is [0,1]\n <br>\n`max_depth:` determines how deeply each tree is allowed to grow during any boosting round.\n<br>\n`subsample: `percentage of samples used per tree. Low value can lead to underfitting.\n<br>\n`colsample_bytree:` percentage of features used per tree. High value can lead to overfitting.\n<br>\n`n_estimators:` number of trees you want to build.\n<br>\n`objective:` determines the loss function to be used like `reg:linear` for regression problems, `reg:logistic` for classification problems with only decision, `binary:logistic` for classification problems with probability.\n<br>\nXGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n<br>\n`gamma: `controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n<br>\n`alpha:` L1 regularization on leaf weights. A large value leads to more regularization.\n<br>\n`lambda:` L2 regularization on leaf weights and is smoother than L1 regularization.\n<br>\nIt's also worth mentioning that though you are using trees as your base learners, you can also use XGBoost's relatively less popular linear base learners and one other tree learner known as dart. All you have to do is set the `booster` parameter to either `gbtree `(default), `gblinear` or `dart`.","6986955b":"Since vehicle data were collected in 2018, population data will include only 2018 .","66fe5901":"**This is an interesting. Feature, such as rating that was irrelevant in the Random Forests model, here it becomes second in importance.**","2baed227":"the time of listing of the car was provided in\nthe form of an `EPOCH`, and required the usage of the\ndatetime package in Python in order to render and\nextract useful features from it.","8d1e519b":"**Future goals:**\n\nmore automation: write a couple of functions to speed up the pre-processing part;<br> Build a pipeline to automate the preprocessing transformations.\n<br>\nmore models: I would like to try a SVM and a NN, maybe a MLP.","322a03b6":"**Map Showen : City Parking locations VS number of trips taken**","24055c8e":"Fit the regressor to the training set and make predictions on the test set using the familiar `.fit()` and `.predict()` methods.","c10fd9da":"## 2-XGBoost Tuning Parameters ","aae31edf":"Note that the Los Angeles airport coordinates : 33.9416\u00b0 N, 118.4085\u00b0 W","9523b46b":"Now `decision trees` are not so good in comparison with the other two methods, both in terms of error and variance explained. let's proceed with fine tuning `random forest` then `XGBoost`.","23e4aed9":"# Part 3 \n## Prepare our Data for Machine Learning Algorithms","536da321":" There are 20 attributes : \n rating, renterTripsTaken\t, reviewCount\t, responseRate\t, newListing\t, freeDeliveryPromotion, \tinstantBookDisplayed, \taverageDailyPrice, \tscalar, \tcity, \tlongitude, \tlatitude\t, state, \tmodel, \tmake, \tid, \tlistingCreatedTime, \tyear, \ttype, \tautomaticTransmission.\n\nThe info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute\u2019s type and number of non-null values","777b7621":"The above image shows **OpenStreetMap(OSM)** base maps for the whole world. Although it is empty right now, it is an interactive map.\nNext, we overlay the base map and the locations points we have.","529f69d1":"**Let us rule out the features that are not useful either in the analysis process or as important features of the algorithms**","2a57f374":"### Evaluate best model on the test set","f770ccc1":"**It's boring steps, let's write in a few lines of code to show what we want to answer.**","59aa2e55":"Here I have relied on changing the hyperparameters manually in a trial and error manner , the most importance is max_depth and n_estimators"}}