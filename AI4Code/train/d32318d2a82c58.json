{"cell_type":{"98a153b4":"code","ac1ba689":"code","f632c35f":"code","c7f2342b":"code","7bf21e38":"code","13152651":"code","b7833ed3":"code","fd6be3d1":"code","7475732d":"code","f286571b":"markdown","03dedd48":"markdown","ff59e779":"markdown","0d9d0ade":"markdown","54ddbf40":"markdown","6f589f51":"markdown","206e5c38":"markdown","54783694":"markdown","0041fb95":"markdown","3c2014af":"markdown","084b68c8":"markdown","5a1ed52c":"markdown","c3a8795c":"markdown"},"source":{"98a153b4":"\nimport pandas as pd\nks = pd.read_csv('..\/input\/kickstarter-projects\/ks-projects-201801.csv',\n                 parse_dates=['deadline', 'launched'])\nks.head(6)","ac1ba689":"print('Unique values in `state` column:', list(ks.state.unique()))","f632c35f":"# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))","c7f2342b":"ks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)","7bf21e38":"from sklearn.preprocessing import LabelEncoder\n\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\n\n# Apply the label encoder to each column\nencoded = ks[cat_features].apply(encoder.fit_transform)","13152651":"# Since ks and encoded have the same index and I can easily join them\ndata = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\ndata.head()","b7833ed3":"valid_fraction = 0.1\nvalid_size = int(len(data) * valid_fraction)\n\ntrain = data[:-2 * valid_size]\nvalid = data[-2 * valid_size:-valid_size]\ntest = data[-valid_size:]","fd6be3d1":"import lightgbm as lgb\n\nfeature_cols = train.columns.drop('outcome')\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)","7475732d":"from sklearn import metrics\nypred = bst.predict(test[feature_cols])\nscore = metrics.roc_auc_score(test['outcome'], ypred)\n\nprint(f\"Test AUC score: {score}\")","f286571b":"### Make predictions & evaluate the model\n\nFinally, let's make predictions on the test set with the model and see how well it performs. An important thing to remember is that you can overfit to the validation data. This is why we need a test set that the model never sees until the final evaluation.","03dedd48":"### Prep categorical variables\n\nNow for the categorical variables -- `category`, `currency`, and `country` -- we'll need to convert them into integers so our model can use the data. For this we'll use scikit-learn's `LabelEncoder`. This assigns an integer to each value of the categorical feature.","ff59e779":"The `state` column shows the outcome of the project.","0d9d0ade":"### Load the data\n\nWe'll work with data from Kickstarter projects. The first few rows of the data looks like this:","54ddbf40":"We collect all of these features in a new dataframe that we can use to train a model.","6f589f51":"### Prepare the target column\n\nFirst we'll convert the `state` column into a target we can use in a model.  Data cleaning isn't the current focus, so we'll simplify this example by:\n\n- Dropping projects that are \"live\"\n- Counting \"successful\" states as `outcome = 1`\n- Combining every other state as `outcome = 0`","206e5c38":"### Create training, validation, and test splits\n\nWe need to create data sets for training, validation, and testing. We'll use a fairly simple approach and split the data using slices. We'll use 10% of the data as a validation set, 10% for testing, and the other 80% for training.","54783694":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https:\/\/www.kaggle.com\/learn\/feature-engineering\/discussion) to chat with other learners.*","0041fb95":"Using this data, how can we use features such as project category, currency, funding goal, and country to predict if a Kickstarter project will succeed? ","3c2014af":"### Train a model\n\nFor this course we'll be using a LightGBM model. This is a tree-based model that typically provides the best performance, even compared to XGBoost. It's also relatively fast to train. \n\nWe won't do hyperparameter optimization because that isn't the goal of this course. So, our models won't be the absolute best performance you can get. But you'll still see model performance improve as we do feature engineering.","084b68c8":"### Convert timestamps\n\nNext, we convert the `launched` feature into categorical features we can use in a model. Since we loaded the columns as timestamp data, we access date and time values through the `.dt` attribute on the timestamp column.\n\n**Note**: If you're not familiar with categorical features and label encoding, please check out **[this lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** from the Intermediate Machine Learning course.","5a1ed52c":"# Your Turn\nNow you'll **[build your own baseline model](https:\/\/www.kaggle.com\/kernels\/fork\/5407496)** which you can improve with feature engineering techniques as you go through the course.\n","c3a8795c":"# Introduction\n\nIn this course, you will learn a practical approach to feature engineering. You'll be able to apply what you learn to Kaggle competitions and other machine learning applications. "}}