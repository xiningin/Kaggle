{"cell_type":{"22412961":"code","beb6b0d6":"code","9b7cab37":"code","da7ff307":"code","929f7221":"code","85a358f9":"code","4bcaf2e9":"code","cab62e5d":"code","fe9f12a8":"code","94c6b66c":"markdown","444effce":"markdown","8e35db71":"markdown","9e927759":"markdown","5869c3ff":"markdown","11201efc":"markdown","8b0cdd80":"markdown","a3fac848":"markdown"},"source":{"22412961":"import numpy as np\nimport pandas as pd\n\nfrom transformers import AutoTokenizer\n\nfrom tqdm import tqdm\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt","beb6b0d6":"def load_sentences(filepath):\n\n    final = []\n    sentences = []\n\n    with open(filepath, 'r') as f:\n        \n        for line in f.readlines():\n            \n            if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n                if len(sentences) > 0:\n                    final.append(sentences)\n                    sentences = []\n            else:\n                l = line.split(' ')\n                sentences.append((l[0], l[3].strip('\\n')))\n    \n    return final","9b7cab37":"base_path = '..\/input\/conll003-englishversion\/'\n\ntrain_samples = load_sentences(base_path + 'train.txt')\ntest_samples = load_sentences(base_path + 'test.txt')\nvalid_samples = load_sentences(base_path + 'valid.txt')\n\nsamples = train_samples + test_samples\n\nschema = ['_'] + sorted({tag for sentence in samples \n                             for _, tag in sentence})","da7ff307":"from transformers import AutoConfig, TFAutoModelForTokenClassification\n\nMODEL_NAME = 'bert-base-cased' \n\nconfig = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))\nmodel = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n                                                          config=config)\nmodel.summary()","929f7221":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_sample(sample):\n    seq = [\n               (subtoken, tag)\n               for token, tag in sample\n               for subtoken in tokenizer(token)['input_ids'][1:-1]\n           ]\n    return [(3, 'O')] + seq + [(4, 'O')]\n\ndef preprocess(samples):\n    tag_index = {tag: i for i, tag in enumerate(schema)}\n    tokenized_samples = list(tqdm(map(tokenize_sample, samples)))\n    max_len = max(map(len, tokenized_samples))\n    X = np.zeros((len(samples), max_len), dtype=np.int32)\n    y = np.zeros((len(samples), max_len), dtype=np.int32)\n    for i, sentence in enumerate(tokenized_samples):\n        for j, (subtoken_id, tag) in enumerate(sentence):\n            X[i, j] = subtoken_id\n            y[i,j] = tag_index[tag]\n    return X, y\n\nX_train, y_train = preprocess(train_samples)\nX_test, y_test = preprocess(test_samples)\nX_valid, y_valid = preprocess(valid_samples)","85a358f9":"EPOCHS=5\nBATCH_SIZE=8\n\noptimizer = tf.keras.optimizers.Adam(lr=0.000001)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\nhistory = model.fit(tf.constant(X_train), tf.constant(y_train),\n                    validation_data=(X_test, y_test), \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE)","4bcaf2e9":"plt.figure(figsize=(14,8))\nplt.title('Losses')\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Valid Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","cab62e5d":"plt.figure(figsize=(14,8))\nplt.title('Accuracies')\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Valid Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","fe9f12a8":"[loss, accuracy] = model.evaluate(X_valid, y_valid)\nprint(\"Loss:%1.3f, Accuracy:%1.3f\" % (loss, accuracy))","94c6b66c":"<h1 id=\"results\" style=\"color:#black; background:#2fbbab; border:0.5px dotted #black;\"> \n    <center>Results\n        <a class=\"anchor-link\" href=\"#results\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","444effce":"<h1 id=\"training\" style=\"color:#black; background:#bababa; border:0.5px dotted #black;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","8e35db71":"<h1 id=\"model\" style=\"color:#black; background:#ef60b4; border:0.5px dotted #black;\"> \n    <center>Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","9e927759":"## Training results","5869c3ff":"<div width=\"100%\">\n    <img width=\"100%\" src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1232095\/2056195\/3da2fe161c2e35efefa75f990b545d32\/dataset-cover.png\" \/>\n<\/div>","11201efc":"<h1 id=\"tokenize\" style=\"color:#black; background:#fc9720; border:0.5px dotted #black;\"> \n    <center>Tokenize\n        <a class=\"anchor-link\" href=\"#tokenize\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","8b0cdd80":"## Validation results","a3fac848":"<h1 id=\"dataset\" style=\"color:#black; background:#a6e22d; border:0.5px dotted #black;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}