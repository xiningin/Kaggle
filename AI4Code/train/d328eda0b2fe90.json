{"cell_type":{"eae76f44":"code","a6d797dd":"code","451d3212":"code","8eb736a1":"code","99b03454":"code","9f4f6ac2":"code","72175158":"code","289935d2":"code","461d6bd2":"code","7ecbcd58":"code","3a11f5d5":"code","f5a04ce3":"code","30044915":"code","a7413603":"code","d62aa5e6":"code","dc7b7357":"code","3337a4a3":"code","23fa5467":"code","4d64a6d0":"code","ddc23561":"code","b7254054":"code","5183cb71":"code","5be0fd49":"code","ffd9dd3d":"code","4b086e8d":"code","4610fd12":"code","33a6ab62":"code","eda614d5":"code","b4140347":"code","0c932b5e":"code","a9d8fc04":"code","e34bf1cc":"code","fab1098b":"code","bf491a25":"markdown","6bd4c450":"markdown","264eff63":"markdown","83cb80f9":"markdown","8f8e7fd9":"markdown","7f8ca516":"markdown","aac3d1b4":"markdown","03b7ba8f":"markdown","8e73c7d4":"markdown","fc019b53":"markdown","2ac801bc":"markdown","6cab5d48":"markdown","5fc23537":"markdown","10572c2d":"markdown","d5abe432":"markdown","45485069":"markdown","dae91d33":"markdown","a32b44ba":"markdown","2c3e4da8":"markdown","344e75c3":"markdown","93c80a7f":"markdown","c6c32c1d":"markdown","7bd81d64":"markdown","de8efbf5":"markdown","29362343":"markdown","ad0f7aaa":"markdown","76259839":"markdown","268e9f48":"markdown"},"source":{"eae76f44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a6d797dd":"import time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","451d3212":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","8eb736a1":"data.shape","99b03454":"data.describe()","9f4f6ac2":"pd.DataFrame(data.isna().sum())","72175158":"data=data.drop(\"Unnamed: 32\", axis =1)\ndata=data.drop(\"id\", axis =1)","289935d2":"sns.countplot(data['diagnosis'],label=\"Count\")","461d6bd2":"data[\"diagnosis\"].replace(\"M\",0,inplace = True)\ndata[\"diagnosis\"].replace(\"B\",1,inplace = True)","7ecbcd58":"data.boxplot()","3a11f5d5":"data.boxplot(column=(['radius_mean','texture_mean','perimeter_mean']),vert=False)","f5a04ce3":"data.boxplot(column=(['compactness_mean','concavity_mean','concave points_mean']),vert=False)","30044915":"data.boxplot(column=(['area_mean']),vert=False)","a7413603":"q25, q75 = np.percentile(data['radius_mean'], 25), np.percentile(data['radius_mean'], 75)\niqr = q75 - q25\ncut_off = iqr * 1.5\nlower, upper = q25 - cut_off, q75 + cut_off\n\noutliers = [x for x in data['radius_mean'] if x < lower or x > upper]\nlen(outliers)","d62aa5e6":"data['radius_mean'] = np.where(data['radius_mean']< lower, np.NaN, data['radius_mean'])\ndata['radius_mean'] = np.where(data['radius_mean']> upper, np.NaN, data['radius_mean'])\ndata['radius_mean'].isna().sum()\ndata['radius_mean'].replace(to_replace =np.NaN, \n                 value =data['radius_mean'].median(),inplace=True)","dc7b7357":"# Nomally above statement is coded as below, but if we do so it will identify the medean of data which \n# includes the outliers, which will be wrong hence we need to replace the outliers with 0 or NaN then idenfy the median\n\n# outliers = [x for x in data['radius_mean'] if x < lower or x > upper]\n# data['radius_mean'].replace(to_replace =[x for x in data['radius_mean'] if x > lower and x < upper], \n#                 value =data['radius_mean'].median(),inplace=True)\n\n#Same need to be performed for all the columns","3337a4a3":"for column in ['texture_mean','perimeter_mean','area_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']:\n    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n    iqr = q75 - q25\n    cut_off = iqr * 1.5\n    lower, upper = q25 - cut_off, q75 + cut_off\n    data[column].replace(to_replace =[x for x in data[column] if x > lower and x < upper], \n                 value =data[column].median(),inplace=True)\nprint( 'Outliers are replaced with Median')","23fa5467":"data.head()","4d64a6d0":"corr = data.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           cmap= 'coolwarm')","ddc23561":"data_backup=data\ndata=data.drop([\"radius_se\"], axis =1)\ndata.head()\nprediction_var = list(['texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',  'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se' ,'radius_worst', 'texture_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst'])\n#prediction_var ","b7254054":"corr = data.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           cmap= 'coolwarm')","5183cb71":"from sklearn.model_selection import train_test_split\n\n#convert the datset into Test and Train data\nX_train, X_test, Y_train, Y_test = train_test_split(data.drop('diagnosis', axis=1), data['diagnosis'],\\\n                                                    test_size=0.2, random_state=156)","5be0fd49":"from sklearn.linear_model import LogisticRegression\n\nlgr = LogisticRegression(max_iter = 200)\nlgr.fit(X_train,Y_train)\nypred=lgr.predict(X_test)\nypred_Log=ypred\nprint('Accuracy score - ' ,lgr.score(X_test,Y_test))","ffd9dd3d":"from sklearn.ensemble import RandomForestClassifier\nRFmodel=RandomForestClassifier()\nresult=RFmodel.fit(X_train, Y_train)\nypred=result.predict(X_test)\nypred_RandomForest=ypred\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","4b086e8d":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier() \nresult=gb.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","4610fd12":"from xgboost import XGBClassifier\nXGB = XGBClassifier() \nresult=XGB.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","33a6ab62":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB() \nresult=NB.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","eda614d5":"lr_diagnosis=lgr.predict(data.drop('diagnosis',axis=1))\nrf_diagnosis=RFmodel.predict(data.drop('diagnosis',axis=1))\ngb_diagnosis=gb.predict(data.drop('diagnosis',axis=1))\nxgb_diagnosis=XGB.predict(data.drop('diagnosis',axis=1))\nnb_diagnosis=NB.predict(data.drop('diagnosis',axis=1))","b4140347":"data['lr_diagnosis']=lr_diagnosis\ndata['rf_diagnosis']=rf_diagnosis\ndata['gb_diagnosis']=gb_diagnosis\ndata['xgb_diagnosis']=xgb_diagnosis\ndata['nb_diagnosis']=nb_diagnosis","0c932b5e":"data.head()","a9d8fc04":"X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(data.drop(\"diagnosis\",axis=1),data[\"diagnosis\"],test_size=0.25,random_state=123)","e34bf1cc":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier() \nresult=gb.fit(X_Train2, Y_Train2)\nypred2=result.predict(X_Test2)\naccuracy_score(Y_Test2,ypred2)","fab1098b":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix( Y_Test2 ,gb.predict(X_Test2))\n\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 1,fmt =\".0f\",ax = ax)","bf491a25":"For the radius_mean column we have 14 outliers identified which need to be removed.","6bd4c450":"Now the data looks good !","264eff63":"Now let us run  Gradient Boosting with all updated features","83cb80f9":"**Ensemble**\n\nNow let us do an Ensemble to get the bet model\nAdding back all the predictions got from the diffrent models executed above.","8f8e7fd9":"change value of diognose M = 0 and B = 1","7f8ca516":"**Naive Bayes**","aac3d1b4":"Exploratory Data Analysis","03b7ba8f":"**Discover outliers with Box plot**\n\nOutlier it will plotted as point in boxplot but other population will be grouped together and display as boxes.","8e73c7d4":"**Gradient Boosting XGBoost**","fc019b53":"**Logistic Regression**","2ac801bc":"data[\"diagnosis\"].replace(\"M\",0,inplace = True)\ndata[\"diagnosis\"].replace(\"B\",1,inplace = True)","6cab5d48":"**Gradient Boosting**","5fc23537":"**Conclusion**\n\nWe have executed 5 models on the data and none of the models were giving above 90% accuracy. Later I have added all 5 predictions back to actual data to improve the final predictions as features. After adding new features again(which is called as Ensamble) I have executed the Gradient boosting method on the updated data and which gave the 97% of accuracy.","10572c2d":"**Working with Outliers: Correcting, Removing**\n\nOutliers and should be dropped or correct , as they cause issues when you model your data.\n\nWe are going to use Interquartile Range Method for removing the outliers in the data.\nThe IQR is calculated as the difference between the 75th and the 25th percentiles of the data . We can then calculate the cutoff for outliers as 1.5 times the IQR and subtract this cut-off from the 25th percentile and add it to the 75th percentile to give the actual limits on the data.\n","d5abe432":"frequency of cancer stages","45485069":"63% Benign cases compared to 37% Malignant cases, potentially indicating higher number of Benign","dae91d33":"As identified there outliers present in the data, now let us check the data for each column for outliers","a32b44ba":"AS per the above heatmap we see highly correlated values to be removed, hence either of the one should be removed. Now below are the variables which will use for prediction","2c3e4da8":"Now let us execute different models and obtain the results","344e75c3":"**Random Forest**","93c80a7f":"Now all the outliers are removed with median","c6c32c1d":"Let us see the correlation metrics once again after removing the columns","7bd81d64":"All the identified outliers are initially replaced with NaN which is null.\nonce the outliers are replaced with NaN , calculatd the Median of the column and replaced with outliers.","de8efbf5":"Handling the Missing Data","29362343":"in the box plot we see the outliers","ad0f7aaa":"Once the outliers are removed we can look at the correlation between the columns","76259839":"Split data into training and test sets","268e9f48":"The last column Unnamed: 32 has all the missing value it have to be removed.\nAlso the id column need to be remove since its just an ID which wont contribute anything for our prediction"}}