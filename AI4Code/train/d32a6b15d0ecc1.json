{"cell_type":{"2dc7886c":"code","e24b50fc":"code","d49b12c1":"code","a9a9e23f":"code","9740a37e":"code","2c36ed4c":"code","86bf7eb3":"code","d505840a":"code","15a3ea52":"code","2077f5ea":"code","0857a640":"code","7076443f":"code","3f9e0791":"code","54fa5d72":"code","d3e5e1ee":"code","7d114788":"code","465f306f":"markdown","b278b476":"markdown","2713d769":"markdown","45b1c888":"markdown","9ddb5347":"markdown","83dc5350":"markdown","7060dad5":"markdown","c84ae367":"markdown","c2ab6a15":"markdown"},"source":{"2dc7886c":"import os\nfrom os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport tensorflow as tf\n# Any results you write to the current directory are saved as output.","e24b50fc":"cache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n    \n!cp ..\/input\/keras-pretrained-models\/*notop* ~\/.keras\/models\/\n!cp ..\/input\/keras-pretrained-models\/imagenet_class_index.json ~\/.keras\/models\/\n!cp ..\/input\/keras-pretrained-models\/resnet50* ~\/.keras\/models\/\n\nprint(\"Available Pretrained Models:\\n\")\n!ls ~\/.keras\/models\n","d49b12c1":"# dimensions of our images.\nimg_width, img_height = 224, 224 # we set the img_width and img_height according to the pretrained models we are\n# going to use. The input size for ResNet-50 is 224 by 224 by 3.\n\ntrain_data_dir = '..\/input\/fruits\/fruits-360_dataset_2018_06_03\/fruits-360\/Training\/'\nvalidation_data_dir = '..\/input\/fruits\/fruits-360_dataset_2018_06_03\/fruits-360\/Validation\/'\nnb_train_samples = 31688\nnb_validation_samples = 10657\nbatch_size = 16","a9a9e23f":"train_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","9740a37e":"import pandas as pd\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","2c36ed4c":"training_data = pd.DataFrame(train_generator.classes, columns=['classes'])\ntesting_data = pd.DataFrame(validation_generator.classes, columns=['classes'])","86bf7eb3":"def create_stack_bar_data(col, df):\n    aggregated = df[col].value_counts().sort_index()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values","d505840a":"x1, y1 = create_stack_bar_data('classes', training_data)\nx1 = list(train_generator.class_indices.keys())\n\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1200, title='Class Distribution in Training Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","15a3ea52":"x1, y1 = create_stack_bar_data('classes', testing_data)\nx1 = list(validation_generator.class_indices.keys())\n\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1100, title='Class Distribution in Validation Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","2077f5ea":"#import inception with pre-trained weights. do not include fully #connected layers\ninception_base = applications.ResNet50(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(512, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(65, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)","0857a640":"#import inception with pre-trained weights. do not include fully #connected layers\ninception_base_vanilla = applications.ResNet50(weights=None, include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base_vanilla.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(512, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(65, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer_vanilla = Model(inputs=inception_base_vanilla.input, outputs=predictions)","7076443f":"inception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\ninception_transfer_vanilla.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])","3f9e0791":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","54fa5d72":"import tensorflow as tf\nwith tf.device(\"\/device:GPU:0\"):\n    history_pretrained = inception_transfer.fit_generator(\n    train_generator,\n    epochs=5, shuffle = True, verbose = 1, validation_data = validation_generator)","d3e5e1ee":"with tf.device(\"\/device:GPU:0\"):\n    history_vanilla = inception_transfer_vanilla.fit_generator(\n    train_generator,\n    epochs=5, shuffle = True, verbose = 1, validation_data = validation_generator)","7d114788":"import matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_pretrained.history['val_acc'])\nplt.plot(history_vanilla.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_pretrained.history['val_loss'])\nplt.plot(history_vanilla.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()","465f306f":"### Visualizing the Data","b278b476":"## 2. Transfer Learning using Kaggle Kernels\n\n### Using the Keras Pretrained Models dataset\nKaggle Kernels cannot use a network connection to download pretrained keras models. This [Dataset](https:\/\/www.kaggle.com\/moltean\/fruits) helps us to use our favorite pretrained models in the Kaggle Kernel environment.\n\nAll we have to do is to copy the pretrained models to the cache directory (~\/.keras\/models) where keras is looking for them.","2713d769":"> - As we can see, all the classes are extremely well-balanced in training as well as the validation.\n\n## 4. Building and Compiling the Model\n### Building the Models\n\nHere, we load the ResNet-50 with the ImageNet weights. We remove the top so that we can add our own layer according to the number of our classes. We then add our own layers to complete the model architecture.\n\n* ### Building Pretrained Model","45b1c888":"## 5. Training and Validating the Pretrained Model\n\nWe use the **fit_generator()** function because we are using object of the **ImageDataGenerator** class to fetch data.","9ddb5347":"*  ### Building the Vanilla Model","83dc5350":"## 3. Reading and Visualizing the Data\n### Reading the Data\n\nLike the rest of Keras, the image augmentation API is simple and powerful. We will use the **ImageDataGenerator** to fetch data and feed it to our network\n\nKeras provides the **ImageDataGenerator** class that defines the configuration for image data preparation and augmentation. Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training.\n\nThe data generator itself is in fact an iterator, returning batches of image samples from the directory when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the **flow_from_directory()** function.","7060dad5":"## 1. Transfer Learning\n\nIn transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.\n\nLisa Torrey and Jude Shavlik in their chapter on transfer learning describe three possible benefits to look for when using transfer learning:\n\n* Higher start. The initial skill (before refining the model) on the source model is higher than it otherwise would be.\n* Higher slope. The rate of improvement of skill during training of the source model is steeper than it otherwise would be.\n* Higher asymptote. The converged skill of the trained model is better than it otherwise would be.\n\n<center><img src=\"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/Three-ways-in-which-transfer-might-improve-learning.png\"><\/center>\n\n\nBasically, we take a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and \u201cfine-tune\u201d the model with our own dataset. The idea is that this pre-trained model will either provide the initialized weights leading to a faster convergence or it will act as a fixed feature extractor for the task of interest.\n\n\n\nThese two major transfer learning scenarios look as follows:\n\n* Finetuning the convnet: Instead of random initializaion, we initialize the network with a pretrained network, like the one that has been trained on a large dataset like imagenet 1000. Rest of the training looks as usual. In this scenario the entire network needs to be retrained on the dataset of our interest\n\n* ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n\nIn this notebook we will demonstrate the first scenario.\n","c84ae367":"### Compiling the Models\nWe set the loss function, the optimization algorithm to be used and metrics to be calculated at the end of each epoch.","c2ab6a15":"# Fruits-360 - Transfer Learning using Keras and ResNet-50\n\n* This notebook is a brief application of transfer learning on the Fruits-360 [Dataset](https:\/\/www.kaggle.com\/moltean\/fruits). \n* This data set consists of 42345 images of 64 fruits.\n* We compare the Transfer learning approach to the regular approach.\n* Accuracy of 98.44% is achieved within 2 epochs.\n\n\n### **Contents:**\n\n*  **1. Brief Explanation of Transfer Learning**\n*  **2. Transfer Learning using Kaggle Kernels**\n*  **3. Reading and Visualizing the Data**   \n*  **4. Building and Compiling the Models**    \n*  **5. Training and Validating the Pretrained Model** \n*  **6. Training and Validating Vanilla Model**\n*  **7. Comparison between Pretrained Models and Vanilla Model**"}}