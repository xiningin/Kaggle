{"cell_type":{"365ab983":"code","fdcc187d":"code","ab8c2f97":"code","9dad086d":"code","ca595a43":"code","b1fe89e5":"code","4b34abd7":"code","6e1b3d67":"code","df3b08c1":"code","2a35e11a":"code","9a6ba3e1":"code","a90eb354":"code","3f40da1e":"code","5c096214":"code","34493f1b":"code","a6fdb92c":"code","677dced1":"code","a3590976":"code","3b540bca":"code","f7f60354":"code","e0255d56":"code","23b5c88d":"code","1c5075c5":"code","d01daec7":"code","4fb1ec2c":"code","59b4759d":"code","63943030":"code","9f795457":"code","07aedebf":"code","19d4e796":"code","ab703fae":"code","1736f7b3":"code","5f7075af":"code","2089f7b0":"code","bc60f065":"markdown","3f6eabfd":"markdown","6e99966c":"markdown","46247e48":"markdown","1c4f1c12":"markdown","f8884abc":"markdown","ed8a2df5":"markdown","6a9afbeb":"markdown","3cfad568":"markdown","da94d72a":"markdown"},"source":{"365ab983":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdcc187d":"!python3 -m pip install --upgrade nni\n#!pip3 install autofeat\n# !pip3 install multimodal-transformers","ab8c2f97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nni\nfrom nni.algorithms.feature_engineering.gradient_selector import FeatureGradientSelector\nfrom sklearn.model_selection import train_test_split\n#from autofeat import AutoFeatRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score, mean_squared_error\nsns.set_theme(style=\"darkgrid\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","9dad086d":"df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\ndf_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\ndf.describe()","ca595a43":"df.head()","b1fe89e5":"def get_cat_num_cols(df : pd.DataFrame, verbose: bool = True) -> tuple:\n    \"\"\"function to get categorical, numerical, and ordinal columns from dataframe\n    \n    Parameters\n    ----------\n        df : pd.DataFrame\n            input dataframe\n            \n        verbose : bool\n            to show individual stats.\n    Returns\n    -------\n        tuple : (cat_cols, num_cols)\n    \"\"\"\n    \n    cat_cols = [col for col in df.columns if df[col].dtype == \"O\"]\n    num_cols = [col for col in df.columns if col not in cat_cols]\n    if verbose:\n        print(f\"categorical columns: {len(cat_cols)}, numerical_cols: {len(num_cols)}\")\n    return cat_cols, num_cols","4b34abd7":"## separate target column from features\ny = df.SalePrice\ndf.drop(['SalePrice'], axis=1, inplace=True)\ncat_cols, num_cols = get_cat_num_cols(df)","6e1b3d67":"## Drop columns with #(missing values) > 0.60 * #(total rows) and rows with all missing values\nmissing_cols_info = df.isnull().sum()\nbad_cols = list(filter(lambda col : missing_cols_info[col] > 0.50 * len(df), missing_cols_info.keys()))\nprint(bad_cols)\n\n## drop bad columns\ncleaned_df = df.drop(bad_cols, axis = 1)\ncat_cols, num_cols = get_cat_num_cols(cleaned_df)\n\ndf_test.drop(bad_cols, axis = 1, inplace = True)","df3b08c1":"## remove rows with all missing values\ncleaned_df.dropna(how = \"all\", inplace = True)\ndf_test.dropna(how = \"all\", inplace = True)\nprint(len(cleaned_df), len(df_test))","2a35e11a":"# function for comparing different approaches\ndef score_dataset(X_train : np.ndarray, X_valid : np.ndarray, y_train : np.ndarray, y_valid : np.ndarray):\n    \"\"\"function to test different feature engineering approaches.\n    \n    Parameters\n    ----------\n        X_train : np.ndarray\n            numpy array containing training features\n        X_valid : np.ndarray\n            numpy array containing validation features\n        y_train : np.ndarray\n            numpy array containing target training values\n        y_valid : np.ndarray\n            numpy array containing target validation values\n    \"\"\"\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    mse = mean_squared_error(y_valid, preds, squared = False)\n    r2 = model.score(X_valid, y_valid) \n    return mse, r2","9a6ba3e1":"## seperate numerical cols\nnum_df = cleaned_df[num_cols]\nprint(len(num_df.columns))","a90eb354":"print(f\"# missing values before: {num_df.isnull().sum().values.sum()}\")\nnum_df[num_cols] = num_df[num_cols].fillna(num_df[num_cols].mean())\nprint(f\"# missing values after: {num_df.isnull().sum().values.sum()}\")","3f40da1e":"## prepare training and validation data\nX_train, X_valid, y_train, y_valid = train_test_split(num_df, y, test_size = 0.2, random_state = 37)\nprint(X_train.shape, X_valid.shape)","5c096214":"## Standardizing numerical features\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_norm_train, X_norm_val = scaler.transform(X_train), scaler.transform(X_valid)\nprint(X_norm_train.shape, X_norm_val.shape)","34493f1b":"num_fgs = FeatureGradientSelector(n_epochs=1, n_features = 20) ## change n_features according to your choice\n# fit data\nnum_fgs.fit(X_norm_train, y_train)\n# get improtant features\n# will return the index with important feature here.\nfeature_idx = num_fgs.get_selected_features()\nprint(f\"Feature indices: {feature_idx}\")\n\nnum_X_final_train, num_X_final_val = pd.DataFrame(X_norm_train[:, feature_idx], columns = np.array(num_cols)[feature_idx]), pd.DataFrame(X_norm_val[:, feature_idx], columns = np.array(num_cols)[feature_idx])\nnum_X_final_train.index = X_train.index\nnum_X_final_val.index = X_valid.index\nprint(num_X_final_train.shape, num_X_final_val.shape)","a6fdb92c":"mse1, r21 = score_dataset(X_norm_train, X_norm_val, y_train, y_valid)\nmse2, r22 = score_dataset(num_X_final_train, num_X_final_val, y_train, y_valid)\n\nprint(f\"MSE \\nWith All Numerical Features: {mse1 :.2f} \\\n        With Top-20 Features: {mse2 :.2f}\")\n\nprint(f\"R2 \\nWith All Numerical Features: {r21 :.2f} \\\n        With Top-20 Features: {r22 :.2f}\")","677dced1":"## apply changes to test data\ndf_test[num_cols] = df_test[num_cols].fillna(df_test[num_cols].mean())\ndf_test_num = df_test[num_cols]\nfinal_test_num = scaler.transform(df_test_num)\nfinal_test_num = pd.DataFrame(final_test_num[:, feature_idx], columns = np.array(num_cols)[feature_idx])\nfinal_test_num.index = df_test_num.index\nfinal_test_num.shape","a3590976":"final_test_num","3b540bca":"cat_df = cleaned_df[cat_cols]\ncat_df.shape","f7f60354":"def cat_cols_eda(df : pd.DataFrame, cat_cols):\n    \"\"\"Function to perform analysis on categorical columns.\n    \n    Parameters\n    ----------\n        df : pd.DataFrame\n            input dataframe\n    \"\"\"\n    \n    for col in cat_cols:\n        print(f\"{col}: Unique Values\")\n        print(df[col].value_counts())\n        print(\"\\n\")","e0255d56":"cat_cols_eda(cat_df, cat_cols)","23b5c88d":"## Categorical Columns\nfor col in cat_cols:\n    missing_values = cat_df[col].isnull().sum()\n    if missing_values > 0:\n        print(f\"{col} : {missing_values}\")","1c5075c5":"## Categorical Columns\ncat_df = cat_df.fillna(cat_df.mode().iloc[0])\ncat_df.isnull().sum().values.sum()","d01daec7":"X_train, X_valid, y_train, y_valid = train_test_split(cat_df, y, test_size = 0.2, random_state = 37)\nprint(X_train.shape, X_valid.shape)","4fb1ec2c":"## find high cardinality categorical columns\nhigh_card_cols = [col for col in cat_cols if cat_df[col].nunique() > 10]\n\nlow_card_cols = list(set(cat_cols) - set(high_card_cols))\n\nprint(f\"High Cardinality Columns: {high_card_cols}\")\nprint(f\"Low Cardinality Columns: {low_card_cols}\")","59b4759d":"encoded_X_train, encoded_X_valid = X_train.copy(), X_valid.copy()\nprint(encoded_X_train.shape, encoded_X_valid.shape)","63943030":"## Apply Ordinal Encoder to high cardinality columns\nordinal_encoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\nencoded_X_train[high_card_cols] = ordinal_encoder.fit_transform(X_train[high_card_cols])\nencoded_X_valid[high_card_cols] = ordinal_encoder.transform(X_valid[high_card_cols])\nprint(encoded_X_train.shape, encoded_X_valid.shape)","9f795457":"encoded_X_train.drop(low_card_cols, axis = 1, inplace = True)\nencoded_X_valid.drop(low_card_cols, axis = 1, inplace = True)","07aedebf":"## Apply One Hot Encoding to low cardinality columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_card_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_card_cols]))\n\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\nprint(OH_cols_train.shape, OH_cols_valid.shape)","19d4e796":"## concat One Hot and Ordinal Encoded Dataframes\nfinal_cat_X_train = pd.concat([encoded_X_train, OH_cols_train], axis = 1)\nfinal_cat_X_valid = pd.concat([encoded_X_valid, OH_cols_valid], axis = 1)\nprint(final_cat_X_train.shape, final_cat_X_valid.shape)","ab703fae":"## apply transformations to test data\ndf_test = df_test.fillna(df_test.mode().iloc[0])\nencoded_df_test = df_test[cat_cols].copy()\nencoded_df_test[high_card_cols] = ordinal_encoder.transform(df_test[high_card_cols])\nencoded_df_test.drop(low_card_cols, inplace = True, axis = 1)\nOH_df_test_cat = pd.DataFrame(OH_encoder.transform(df_test[low_card_cols]))\nOH_df_test_cat.index = df_test.index\n\nfinal_cat_test = pd.concat([encoded_df_test, OH_df_test_cat], axis = 1)\nprint(final_cat_test.shape)","1736f7b3":"## combine training data\nfinal_X_train = pd.concat([num_X_final_train, final_cat_X_train], axis = 1)\n\n## combine validation data\nfinal_X_val = pd.concat([num_X_final_val, final_cat_X_valid], axis = 1)\n\n## combine test data\nfinal_test_data = pd.concat([final_test_num, final_cat_test], axis = 1)\n\nprint(final_X_train.shape, final_X_val.shape, final_test_data.shape)","5f7075af":"from sklearn.model_selection import cross_val_score\nmodel = RandomForestRegressor(n_estimators=500, random_state=0)\nmodel.fit(final_X_train, y_train)\npreds = model.predict(final_X_val)\nmse = mean_squared_error(y_valid, preds, squared = False)\nr2 = model.score(final_X_val, y_valid)\n\nmse, r2","2089f7b0":"test_preds = model.predict(final_test_data)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': df_test.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","bc60f065":"## Compare 2 Models\n* One with all numerical features\n* Second with top-20 selected features","3f6eabfd":"### Selecting Categorical Features","6e99966c":"### Combine final numeric and categorical datasets","46247e48":"## Train the final model","1c4f1c12":"#### Filling missing values in numerical columns with mean values","f8884abc":"### Select Best Numerical Features","ed8a2df5":"#### Handling missing values","6a9afbeb":"#### Filling missing values in categorical columns with most frequent","3cfad568":"### getting predictions for test data","da94d72a":"##### Use NNI to select top 20 features"}}