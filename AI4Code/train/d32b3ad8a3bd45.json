{"cell_type":{"8070bf0b":"code","ce473f5f":"code","dfa54545":"code","e3208351":"code","8dbbf184":"code","b17d8193":"code","cc97f283":"code","7512243f":"code","f19f3c34":"code","31924911":"code","b959712f":"code","053cfdb8":"code","60cd0aa2":"code","13b3def2":"code","445baf8c":"code","4babd361":"code","5657f34d":"code","858ffe06":"code","d8e66dcc":"code","e7728ec6":"code","bfdf3340":"code","36897760":"code","9d5a2b75":"code","7c5f4b74":"code","97d29960":"code","44f19ed4":"code","a79ce86d":"code","1a357e37":"code","85ed0109":"code","9ffde43d":"code","d4a9552f":"code","d174ffbe":"markdown","4cd44917":"markdown","c304cb9f":"markdown","0e49c66c":"markdown","3da3a724":"markdown","df53716a":"markdown","79aa1ca5":"markdown","37798ca3":"markdown","6a41e7af":"markdown","e26fcdbf":"markdown","a7ac7d6e":"markdown","bfd6fcf5":"markdown","eb0410ef":"markdown","20db49ff":"markdown","5c15ae87":"markdown","b92a8c3b":"markdown","abbbcbe9":"markdown","075a75b3":"markdown","e391367b":"markdown"},"source":{"8070bf0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy import stats\n\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Embedding,LSTM\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nimport re\nimport nltk \nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce473f5f":"data = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","dfa54545":"# we are browsing the data\ndata.head()","e3208351":"# the columns \ndata.columns","8dbbf184":"# we are looking at the size of our data.\ndata.shape","b17d8193":"data.info()","cc97f283":"# statistical summary of our data\ndata.describe()","7512243f":"sns.countplot(data[\"sentiment\"], palette = [\"green\",\"red\"])\nplt.show()\nprint(data.sentiment.value_counts())","f19f3c34":"data.sentiment = [ 1 if each == \"positive\" else 0 for each in data.sentiment]","31924911":"data.head()","b959712f":"WPT = nltk.WordPunctTokenizer()\nstop_word_list = nltk.corpus.stopwords.words('english')\n#stop_word_list","053cfdb8":"data['review'] = data['review'].apply(lambda x: re.sub('[,\\.!?:()\"]', '', x))\ndata['review'] = data['review'].apply(lambda x: re.sub('[^a-zA-Z\"]', ' ', x))\n\ndata['review'] = data['review'].apply(lambda x: x.lower())\n\ndata['review'] = data['review'].apply(lambda x: x.strip())\n\n\"\"\"\nI closed the stopword process because it took a long time.\nIf you want, you can try opening the codes in the comment line.\n\"\"\"\n#def token(values):\n   # words = nltk.tokenize.word_tokenize(values)\n    #filtered_words = [word for word in words if word not in set(stopwords.words(\"english\"))]\n    #not_stopword_doc = \" \".join(filtered_words)\n    #return not_stopword_doc\n#data['review'] = data['review'].apply(lambda x: token(x))","60cd0aa2":"sentiment = data['sentiment'].values\nsentiment","13b3def2":"data = data['review']\n","445baf8c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data,sentiment,test_size = 0.2, random_state = 42)","4babd361":"from tensorflow.python.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = 15000)\ntokenizer.fit_on_texts(data)\n#tokenizer.word_index","5657f34d":"x_train_tokens = tokenizer.texts_to_sequences(x_train)\nx_test_tokens = tokenizer.texts_to_sequences(x_test)","858ffe06":"#Then we take the word count of each of our sentences in our data and create a list.\nnum_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\nnum_tokens = np.array(num_tokens)","d8e66dcc":"#Here, when setting the number of tokens, a number is determined by taking into account the variability around the average.\nmax_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\nmax_tokens = int(max_tokens)\nmax_tokens","e7728ec6":"#It is checked what percentage of the data this determined number covers.\nnp.sum(num_tokens < max_tokens) \/ len(num_tokens)","bfdf3340":"#data is adjusted according to the number of tokens specified\nx_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\nx_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)","36897760":"x_train_pad.shape","9d5a2b75":"idx = tokenizer.word_index\ninverse_map = dict(zip(idx.values(), idx.keys()))\n\ndef return_to_sentence(tokens):\n    words = [inverse_map[token] for token in tokens if token!=0]\n    text = ' '.join(words)\n    return text","7c5f4b74":"#normal comment\nprint(return_to_sentence(x_train_pad[9]))","97d29960":"#token equivalent of comment\nprint(x_train_pad[9])","44f19ed4":"model = Sequential()\n\nembedding_size = 50\n\nmodel.add(Embedding(input_dim=15000,output_dim=embedding_size,input_length=max_tokens,name='embedding_layer'))\n\nmodel.add(LSTM(units=16, return_sequences=True))\nmodel.add(Dropout(0.1))\n\nmodel.add(LSTM(units=8, return_sequences=True))\nmodel.add(Dropout(0.1))\n\nmodel.add(LSTM(units=4))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(lr=1e-3)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])","a79ce86d":"model.summary()","1a357e37":"history = model.fit(x_train_pad, y_train, validation_split=0.3, epochs=5, batch_size=1000, shuffle=True, verbose = 1)","85ed0109":"result = model.evaluate(x_test_pad, y_test)","9ffde43d":"plt.figure()\nplt.plot(history.history[\"accuracy\"], label = \"Train\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Test\")\nplt.title(\"Accuracy\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","d4a9552f":"plt.figure()\nplt.plot(history.history[\"loss\"], label = \"Train\")\nplt.plot(history.history[\"val_loss\"], label = \"Test\")\nplt.title(\"Loss\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","d174ffbe":"<a id ='9' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Visualization (Acc - Loss)<\/h2>","4cd44917":"<a id ='6' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Bring the comments to the same size <\/h2>\n\n<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > comments of different lengths cannot train the RNN model. That's why we need to make the sentences the same size. <\/p> <\/li>\n<\/ul>","c304cb9f":"<a id ='5' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Creating a Dictionary <\/h2>\n\n<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > We create a dictionary of 15000 most used words in English for later use. <\/p> <\/li>\n<\/ul>","0e49c66c":"<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > We convert our sentiment property to int values. <\/p> <\/li>\n        <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > positive --> 1 <\/p> <\/li>\n        <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > negative --> 0 <\/p> <\/li>\n<\/ul>","3da3a724":"<a id ='8' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Result <\/h2>","df53716a":"![Pozitive_Negative.png](attachment:Pozitive_Negative.png)","79aa1ca5":"<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > we train our model.\n <\/p> <\/li>\n<\/ul>","37798ca3":"<a id ='7' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Create LSTM Model <\/h2>","6a41e7af":"\n<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" >We replace the words in our sentences with whichever index they are included in the dictionary we have created above. <\/p> <\/li>\n<\/ul>","e26fcdbf":"<a id ='2' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\">Ineffective Words <\/h2>\n\n<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > Let's first take a look at the ineffective English words. <\/p> <\/li>\n<\/ul>","a7ac7d6e":"<center><h1 style = \"background-image: linear-gradient(90deg, green, red);border:0;color:white\">Introduction<\/h1><\/center>\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">Our data consists of 50000 comments. There are 25000 negative and 25000 positive comments. Comments are of different lengths and are given in sentences.\n<\/p>\n\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">Using the imdb dataset, we will train our LSTM model with positive and negative comments about movies. First, we will make comments available for the LSTM model. After training our model, we will examine the accuracy and loss values on the chart.\n <\/p>\n    \n\n<h2 style = \"background-image: linear-gradient(90deg, green, red);border:0;color:white\">Content :<\/h2>\n\n<ul>\n    <li style = \"color:red;font-size:15px\"> <a href = \"#1\" style = \"color:black;font-weight:bold\"> Load and Check Data <\/a> <\/li>\n    <li style = \"color:green;font-size:15px\"> <a href = \"#2\" style = \"color:black;font-weight:bold\"> Ineffective Words <\/a> <\/li> \n    <li style = \"color:red;font-size:15px\"> <a href = \"#3\" style = \"color:black;font-weight:bold\"> Clearing data <\/a> <\/li> \n    <li style = \"color:green;font-size:15px\"> <a href = \"#4\" style = \"color:black;font-weight:bold\">  Train - Test Split  <\/a> <\/li> \n    <li style = \"color:red;font-size:15px\"> <a href = \"#5\" style = \"color:black;font-weight:bold\"> Creating a Dictionary <\/a> <\/li> \n    <li style = \"color:green;font-size:15px\"> <a href = \"#6\" style = \"color:black;font-weight:bold\"> Bring the comments to the same size <\/a> <\/li> \n    <li style = \"color:red;font-size:15px\"> <a href = \"#7\" style = \"color:black;font-weight:bold\"> Create RNN Model <\/a> <\/li> \n    <li style = \"color:green;font-size:15px\"> <a href = \"#8\" style = \"color:black;font-weight:bold\"> Result <\/a> <\/li>\n    <li style = \"color:red;font-size:15px\"> <a href = \"#9\" style = \"color:black;font-weight:bold\"> Visualization <\/a> <\/li>\n    \n    \n<\/ul>\n\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\">Import Libraries<\/h2>","bfd6fcf5":"<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > max_tokens : This value will allow us to reduce the distribution of sentences in our data and the sentences with opposite lengths, if any, to the average. <\/p> <\/li>\n<\/ul>","eb0410ef":"<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > We write a function to revert the sentences that we have made int.\n <\/p> <\/li>\n<\/ul>","20db49ff":"<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > we see that there is no null value in our data. <\/p> <\/li>\n<\/ul>","5c15ae87":"<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > As you can see, we have set it to shape 567. So max_tokens\n <\/p> <\/li>\n<\/ul>","b92a8c3b":"<a id ='4' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\"> Train - Test Split <\/h2>","abbbcbe9":"<ul>\n    <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > We see that the number of positive and negative comments is equal. <\/p> <\/li>\n<\/ul>","075a75b3":"<a id ='3' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\">Clearing data <\/h2>\n\n<ul>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > process of clearing punctuation marks in data. <\/p> <\/li>\n     <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > cleaning unnecessary marks in data. <\/p> <\/li>\n        <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > capitalization to lowercase. <\/p> <\/li>\n     <li  style = \"color:green\" > <p style = \"color:black;font-weight:bold\" > cleaning extra spaces. <\/p> <\/li>\n    <li  style = \"color:red\" > <p style = \"color:black;font-weight:bold\" > removal of stopwords in sentences. <\/p> <\/li>\n<\/ul>","e391367b":"<a id ='1' ><\/a>\n<h2 style = \"background-image: linear-gradient(90deg, green, red);color:white;border:0\">Load and Check Data <\/h2>"}}