{"cell_type":{"7f5ffeda":"code","b007d48e":"code","7a45aaee":"code","6de22fc4":"code","db71efd5":"code","29b3be2e":"code","1fd3320f":"code","d9649c3b":"code","8cc26033":"code","bd6038e2":"code","af1e15f8":"code","5e274165":"code","76f58fde":"code","4a36ab44":"code","6f3d9269":"code","227c1819":"code","a8802280":"code","f158dda3":"code","c911f162":"code","f400da25":"code","f2fe3664":"code","ebaf3bfd":"code","4e140917":"code","0a098a00":"code","298c2e75":"code","af7e8fa1":"code","98d92718":"code","7e1d6081":"code","bedb60e0":"code","9c90709c":"code","7f1b274a":"code","acf99497":"code","f9795161":"code","e399e4d5":"code","dd8b3b2b":"code","eb1ef1c0":"code","8d8a07b5":"code","cc0dcaa8":"code","ce65ab90":"code","1a25bd84":"code","82b52117":"code","05bf4f69":"code","ce7946f7":"code","4f3a5816":"code","77845c21":"code","1589ec46":"code","a6781688":"code","0a70b8c8":"code","3f84cd65":"code","6719ebb0":"code","8d2f2fa8":"code","d1b11fe2":"code","be93f132":"code","e2c25426":"code","6a8a3200":"code","ef2b3729":"code","2f0fbb66":"code","ef5ebfd4":"code","d3aa6207":"code","b36fb8b5":"code","df6b7b40":"code","de68d8a1":"code","51ab516a":"code","0c010220":"code","c9e78ad1":"code","4001316b":"code","f5ff103f":"code","023959f3":"code","18c29f6c":"code","c968502a":"code","8109bd3f":"code","b627df9f":"code","f902f775":"code","1bd9227c":"code","efd450e2":"code","3e8c3cfc":"code","1c588b8c":"code","bc386837":"code","2bceb0fe":"code","55b48948":"code","271482a7":"code","42edc75c":"code","db01bf61":"code","db6289f2":"code","2cc38605":"code","a73e7f41":"code","5dc1119a":"code","59bba438":"code","5e22b237":"code","0e6cf657":"code","696b33ca":"code","b0a5d276":"code","9026c9ea":"code","c9277b50":"code","5717e342":"code","829dbacb":"code","3b4889cb":"code","3231d0e9":"code","6b55cb2f":"code","2841ef2a":"code","2dfd7184":"code","1686ca70":"code","0d2323a2":"code","af93e22c":"code","9649f6e9":"markdown","04bd068b":"markdown","bece6526":"markdown","cbfb7f6e":"markdown","aa88fe4e":"markdown","4e1776b8":"markdown","9137b0d8":"markdown","d43eaf95":"markdown","b569d0a9":"markdown","fe2de6cd":"markdown","2d7b28d2":"markdown","277eb2e5":"markdown","194a81a2":"markdown","9a3b20f4":"markdown","c34b7535":"markdown","e3574c35":"markdown","a87be728":"markdown","d7fbeaef":"markdown","6be704d7":"markdown","b0bde484":"markdown","a637f091":"markdown","af6e9ded":"markdown","3f5506e7":"markdown","2f791a1f":"markdown","0c3d7c3e":"markdown","79eb8682":"markdown","abe13e15":"markdown","64aec6ff":"markdown","82a1ac73":"markdown","8e401702":"markdown","1ed025d3":"markdown","80ce4509":"markdown","e0892b52":"markdown","58f9926c":"markdown","55ebd8fe":"markdown","0dece01b":"markdown","bfdadc93":"markdown","bf8cb808":"markdown","f2b90fe6":"markdown","cc3657e5":"markdown","28a141cb":"markdown","17fe10c7":"markdown","4bb21a1e":"markdown","b5701ef8":"markdown","2fe92a6e":"markdown","5e4c4a9d":"markdown","85316fb7":"markdown","1c7c44b4":"markdown","04cde739":"markdown","807180e0":"markdown","26bd9aac":"markdown","fa78bac2":"markdown","6b91899f":"markdown"},"source":{"7f5ffeda":"#basics\nimport numpy as np\nimport pandas as pd\n\n\n#data setting preprocessing and such\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as sm\nimport ast\nimport datetime as dt\n\n\n# plotting and graphing\nimport matplotlib.pyplot as plt\n\n\n# modeling\nfrom sklearn.cluster import KMeans\n#from sklearn.cluster import DBSCAN\n# model validating\nfrom sklearn.metrics import silhouette_score\nimport fbprophet as pp","b007d48e":"# https:\/\/www.officeholidays.com\/countries\/brazil\/2017","7a45aaee":"basefolder = \"\/kaggle\/input\/brazilian-ecommerce\/\"\n# declaring\n# life the universe and everything\nrandom_state = 42\n# first contact day\nstart_date = dt.datetime(2063, 4, 5)\nend_date = dt.datetime(2063, 4, 5)\nmodel_now = dt.datetime(2063, 4, 5)\nprod_pred = pd.DataFrame()","6de22fc4":"global_err = pd.DataFrame()","db71efd5":"# future class cats, now global label variables\ncats = [] # should be pcats\nsellersCats = []\nddLE = defaultdict(LabelEncoder)","29b3be2e":"def fillDf(df):\n    df.update(df.select_dtypes(include=[np.number]).fillna(0))\n    df.update(df.select_dtypes(include='object').fillna('0'))\n    return df","1fd3320f":"def pivotSequence(df, ts_col, start_date, end_date, days, col_to_group, col_to_agg, action = 'count'): #timePeriod to slice by, by days\n    \"\"\"returns data aggregated by the function, pivoted, by the selected index\n        parameters:\n        df - the dataframe to process\n        ts_col - the timestamp column by which to slice the df\n        start_date - the start date\n        end_date - the end date\n        days - the number of days to slice by\n        colToGroup - the column to group by\n        col_to_agg - the column to aggregate\n        action - the group action, default 'count'\n    \"\"\"\n    #validate df as DataFrame\n    # validate start_date, end_date as date\n    #validate days as integer\n    #validate colToGroup,col_to_agg as string\n    try:\n        end_date = pd.to_datetime(end_date)\n        iterator = pd.to_datetime(start_date)\n        delta = dt.timedelta(days=days)\n    except:\n        return -1\n    \n    n = 0\n    grped = pd.DataFrame()\n    retpd = pd.DataFrame(columns = [col_to_group])\n    retpd.set_index(col_to_group,inplace = True)\n    \n    while iterator <= end_date:\n        n = n + 1\n        grped = df[(df[ts_col] >= iterator) & (df[ts_col] < iterator + delta)].groupby(col_to_group).aggregate({col_to_agg:action})\n\n        grped.columns = [n]\n        retpd = pd.merge(grped, retpd, how='outer', left_index=True, right_index=True)\n        \n        iterator += delta\n        iterator = pd.to_datetime(iterator)\n        \n        \n    retpd.reset_index(inplace=True)\n    retpd.fillna(0, inplace = True)\n\n    return retpd","d9649c3b":"# Generates order items per per X, where is an aggregate by object: sellers, customers or products\n# we need this as a function to be flexible for various training and testing data\n# the main feature engineering code...\ndef createOipx(oi, lefter, start_date,end_date, newCats=False,merge_on='product_id',strliaze='product_category_name',date_filter='order_purchase_timestamp', group_by=\"'price':'min','freight_value':'mean','product_category_name':'min','product_name_length':'min','product_photos_qty':'mean','product_weight_g':'mean','product_height_cm':'mean','product_width_cm':'min','product_length_cm':'min'\"):\n    \"\"\"\n    createOipx(lefter, start_date,end_date, newCats=False,merge_on='product_id',strliaze='product_category_name',date_filter='shipping_limit_date', group_by=\"'price':'min','freight_value':'mean','product_category_name':'min','product_name_length':'min','product_photos_qty':'mean','product_weight_g':'mean','product_height_cm':'mean','product_width_cm':'min','product_length_cm':'min'\"):\n    creates a dedicated Dataframe for the anlysis of Order Items Per X, where X is seller, product or customer\n    \n        good for setting training and testing for various periods, for various objects\n    \n    oi - our local version of order_items - basically the righter, (even thuogh it's the first parameters...)\n    lefter - the dataframe to which we want to join the order items (i.e seller, product or customer)\n    since\n    \n    \"\"\"\n    #local var declaration\n    newOipp = pd.DataFrame() # the 1st return var\n    sales_per_week = pd.DataFrame() # an intermediary variable\n    newGrpp = pd.DataFrame() # the 2nd return var\n    tmp = []\n    \n    newOipp = pd.merge(lefter,oi, how=\"left\", on=merge_on)\n    \n    newOipp[date_filter] = pd.to_datetime(newOipp[date_filter])\n    newOipp = newOipp[(newOipp[date_filter] >= start_date) & (newOipp[date_filter] < end_date)]\n    newOipp[strliaze] = newOipp[strliaze].astype(str)\n    \n    #carefulness is required - the initial dataset that will generate the labale encoding needs to be robust enough !\n    newOipp = catLoader(newOipp,newCats)\n    \n    delta = end_date - start_date\n    delta = delta.days # + 1 or  -1\n    \n    tmp = list(np.arange(1,np.ceil(delta \/ 7)))\n    sales_per_week = pivotSequence(newOipp,date_filter,start_date,end_date,7,merge_on,'order_id','count')\n    \n    newOipp = pd.merge(newOipp, sales_per_week, how='left',on=merge_on)\n    #tmp = list(np.arange(1,8))\n    tmp = [str(item) for item in tmp]\n    tmp = \":'mean',\".join(tmp)\n    #group_by += \",'\" + date_filter + \"': [np.min, np.max]'\"\n    paramsdict = \"{\" + group_by + \",\" + tmp + \":'mean'}\"\n    paramsdict = ast.literal_eval(paramsdict)\n    #1:'mean',2:'mean',3:'mean',4:'mean',5:'mean',6:'mean',7:'mean',8:'mean',9:'mean'}\n    newGrpp = newOipp.groupby(merge_on).aggregate(paramsdict)\n    \n\n    ####to calculate merge_on's lifetime, average quantity per date??\n    tmp = newOipp.groupby(merge_on).aggregate({ date_filter : [np.min, np.max]})\n    tmp.columns = ['min_sale_date','max_sale_date']\n    tmp['lifetime'] = (tmp['max_sale_date'] - tmp['min_sale_date']).dt.days\n    tmp['is_active'] = ((end_date - tmp['max_sale_date']).dt.days < 30)*1\n    tmp['max_sale_date'] = (model_now - tmp['max_sale_date']).dt.days\n    tmp['min_sale_date'] = (model_now - tmp['min_sale_date']).dt.days\n    tmp.rename(columns={'max_sale_date':'age_of_last_sale','min_sale_date':'age'}, inplace=True)\n    \n    newGrpp = pd.merge(newGrpp,tmp,left_index=True, right_index=True,how='left')\n    \n    tmp = newOipp[newOipp[date_filter] >= (end_date - dt.timedelta(30))].groupby(merge_on).aggregate({ 'price' : 'sum'})\n    tmp.columns = ['last_30_days_sales']\n    \n    tmp2_1 = newOipp[(newOipp[date_filter] < (end_date - dt.timedelta(30))) & (newOipp[date_filter] >= (end_date - dt.timedelta(60)))].copy()\n    tmp2 = tmp2_1.groupby(merge_on).aggregate({ 'price' : 'sum'})\n    tmp2.columns = ['before_last_30_days_sales']\n    \n    tmp = pd.merge(tmp,tmp2,left_index=True, right_index=True,how='inner')\n    tmp['trend'] = round((tmp['last_30_days_sales'] \/ tmp['before_last_30_days_sales']) - 1,2)\n    tmp.drop(tmp.columns.difference([merge_on,'trend']), axis=1, inplace=True)\n    newGrpp = pd.merge(newGrpp,tmp,left_index=True, right_index=True,how='left')\n        \n    newOipp = fillDf(newOipp)\n    newGrpp = fillDf(newGrpp)\n    \n    return newOipp, newGrpp","8cc26033":"def fillMissingDates(inputDf,date_filter,start_date,end_date,specs_fill={}):\n    \n    returnDf = pd.DataFrame()\n    returnDf = inputDf.copy()\n        # missing dates filling\n    delta = end_date - start_date\n    delta = delta.days # + 1 or  -1\n    \n    returnDf['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'] = pd.to_datetime(returnDf[date_filter]).dt.date\n    returnDf['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'] = pd.to_datetime(returnDf['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'])\n    \n    datelist = pd.DataFrame(pd.date_range(start_date, periods=delta))\n    datelist.columns = ['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it']\n    datelist['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'] = pd.to_datetime(datelist['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'])\n    \n    returnDf = pd.merge(datelist,returnDf,how='left',on='date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it')\n    \n    returnDf[date_filter].fillna(returnDf['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'],inplace=True)\n    returnDf.drop(['date_with_suffix_so_no_way_someone_will_input_this_column_name_and_f_ing_break_it'],axis=1, inplace=True)\n    for key, value in specs_fill.items():\n        returnDf[key].fillna(value,inplace=True)\n    \n    returnDf = fillDf(returnDf)\n    \n    return returnDf","bd6038e2":"## this and it's following both work on the same variables, which are global\n## bottom line, they better both be a part of a class together with the vars, maybe in a future version\ndef catEncoder(oipx0,runOver=False):\n    oipx = pd.DataFrame()\n    oipx = oipx0.copy()\n    #variables declaration\n    fit = pd.DataFrame()\n    global cats\n    global ddLE\n    local_cats = []\n    global_cats = []\n    \n    for col in oipx.columns:\n        if oipx[col].dtype == 'O':\n            local_cats.append(col)\n\n # or, better, a more specific error (or errors)\n    removables = ['product_id','order_id', 'seller_id','customer_id','customer_unique_id']\n    # global var #1 setting - cats\n    for removable in removables:\n        try:\n            local_cats.remove(removable)\n        except Exception:\n            pass\n    \n    \n    # runOver - allows us run over existing label encoder and run over existing ones\n    # I hate too many ifs... a program should flow\n    if (not(runOver)):\n        for global_cat in cats:\n            if (global_cat in local_cats):\n                global_cats.append(global_cat)\n                local_cats.remove(global_cat)\n    \n    fit[global_cats] = oipx[global_cats].apply(lambda x: ddLE[x.name].transform(x))\n    fit[local_cats] = oipx[local_cats].apply(lambda x: ddLE[x.name].fit_transform(x))\n    for cat in (local_cats + global_cats):\n        oipx[cat] = fit[cat]\n        \n    local_cats = local_cats + global_cats\n    cats = list(set(cats) | set(local_cats))\n    \n    return oipx\n\ndef catLoader(oipx0,newCats=False):\n    oipx = pd.DataFrame()\n    oipx = oipx0.copy()\n    #local variable declaration\n    fitreal = pd.DataFrame()\n    global ddLE\n    local_cats = []\n\n    for col in oipx.columns:\n        if oipx[col].dtype == 'O':\n            local_cats.append(col)\n\n    \n    #if the label encoder is already set - call the existing labels to set the Oipp cats\n    #if there are none, encode a new one.\n    if ((local_cats) or (newCats)):\n        return catEncoder(oipx)\n    else:\n        fitreal = oipx[local_cats].apply(lambda x: ddLE[x.name].transform(x))\n        for cat in local_cats:\n            oipx[cat] = fitreal[cat]\n\n        return oipx\n    ","af1e15f8":"def predict(train0,col_to_agg,action,group_date,test_start_date,test_end_date,interval_width=0.8,outliers=True,regressor=pd.DataFrame()): #,graphs=False\n    prediction = pd.DataFrame()\n    train = pd.DataFrame()\n    train = train0.copy() # It seems that paramters are passed by reference (face-palm)\n    delta = test_end_date - test_start_date\n    periods = delta.days + 10\n    \n    if (train.shape[0] == 0):\n        return prediction\n    \n    train[group_date] = pd.to_datetime(train[group_date]).dt.date.copy()\n    train = train.groupby(group_date).agg({col_to_agg:action})\n    train.reset_index(inplace=True)\n    train.columns = ['ds','y']\n    \n    if (not(outliers)):\n        train = unOutlierOnTop(train,'y')\n    \n    reg_cols = []\n    if (not(regressor.empty)): ##\n        train = pd.merge(train,regressor[(regressor['ds'].dt.date < test_start_date.date())],on='ds',how='left')\n        regressor.drop(['ds'],axis=1,inplace=True)\n        reg_cols = regressor.columns\n        tsModel = pp.Prophet(yearly_seasonality=True,weekly_seasonality=True,daily_seasonality=True,interval_width=interval_width,regressor=regressor) # ,holidays=brazil_holidays the holidays actually make the prediction worse...\n    else:\n        tsModel = pp.Prophet(yearly_seasonality=True,weekly_seasonality=True,daily_seasonality=True,interval_width=interval_width) # ,holidays=brazil_holidays the holidays actually make the prediction worse...\n    \n    \n    for col in reg_cols:\n        tsModel.add_regressor(col)\n        \n    tsModel.fit(train) # the actual training\n    future = tsModel.make_future_dataframe(periods=periods) # presetting the timeseries dataframe for testing\n    if (not(regressor.empty)):\n        future = pd.merge(future,regressor[(regressor['ds'].dt.date >= test_start_date.date() & regressor['ds'].dt.date <= test_end_date.date())],on='ds',how='left')\n    \n    forecast = tsModel.predict(future)\n    \n    graphs = False\n    if (graphs):\n        figp = tsModel.plot_components(forecast)\n    \n    #prediction = forecast[(forecast['ds'] >= test_start_date) & (forecast['ds'] < test_end_date)]\n    prediction = forecast[(forecast['ds'] < test_end_date)] # (forecast['ds'] >= test_start_date) &\n    prediction.set_index('ds',inplace=True)\n    #fig = tsModel.plot_components(forecast)\n    \n    return prediction","5e274165":"def fit_predict(train,filter_date='order_purchase_timestamp',col_to_agg='price',action='sum',outliers=True,regressor=pd.DataFrame()): #,graphs=False\n    # works on quite a few global vars, kind of says - class, but in future versions...\n    global periods\n    global start_date\n    global end_date\n    global test_start_date\n    global test_end_date\n    \n    x_pred = pd.DataFrame()\n    tmp = pd.DataFrame()\n    \n    train[filter_date] = pd.to_datetime(train[filter_date])\n    train[filter_date] = train[filter_date].dt.date\n    clusters = list(train['cluster'].unique())\n    \n    \n    x_pred = pd.DataFrame(pd.date_range(test_start_date, periods=periods))\n    x_pred.columns = ['ds']\n    x_pred.set_index('ds', inplace=True)\n\n    for i in clusters:\n        tmp = train[train['cluster'] == i].copy()\n\n        if (tmp.shape[0] > 2):\n            tmp = fillMissingDates(tmp,filter_date,start_date,end_date)\n        #try:\n        prd = predict(tmp,col_to_agg=col_to_agg,action=action,group_date=filter_date,test_start_date=test_start_date,test_end_date=test_end_date,outliers=outliers,regressor=regressor)['yhat'] #,graphs=False\n        x_pred[i] = prd\n       # except:\n       #     print('cluster....: ' + str(i))\n       #     print('banged!')\n\n    x_pred['total'] = x_pred.sum(axis=1)\n    return x_pred","76f58fde":"def easterEgg():\n    print('Im weighing it')\n    print('...')\n    import time\n    time.sleep(3)\n    print('50 gramms')","4a36ab44":"def edapx(grpx,object_name='x'):\n    \n     \n    num_bins = 50\n    fig, ax = plt.subplots(1,1, tight_layout = True)\n    ax.hist(grpx['age_of_last_sale'], num_bins)\n    fig.tight_layout()\n    plt.title('time from the last sale, per ' + object_name)\n    plt.show()\n\n\n    num_bins = 50\n    fig, ax = plt.subplots(1,1, tight_layout = True)\n    ax.hist(grpx['lifetime'], num_bins)\n    fig.tight_layout()\n    plt.title('Lifetime distribution for ' + object_name)\n    plt.show()\n\n    num_bins = 50\n    fig, ax = plt.subplots(1,1, tight_layout = True)\n    ax.hist(grpx['lifetime'][grpx['lifetime'] > 0], num_bins)\n    fig.tight_layout()\n    plt.title('Lifetime distribution for returning ' + object_name)\n    plt.show()\n    \n    num_bins = 50\n    fig, ax = plt.subplots(1,1, tight_layout = True)\n    ax.hist(grpc0['price'], num_bins)\n    fig.tight_layout()\n    plt.title('Average purchase distribution per customers')\n    plt.show()","6f3d9269":"# EDA per x\ndef edapx_clustered(grpx,object_name='x'):\n    \n\n    tmp = pd.DataFrame()\n    tmp['cluster'] = grpx['cluster'].copy()\n    lbls = list(tmp['cluster'].unique())\n    \n    # simple pie chart    \n    tmp = tmp.groupby('cluster')['cluster']\n    plt.pie(tmp.count(),labels = lbls)\n    plt.title(object_name + ' distribution by cluster')\n    plt.show()\n    \n    # sales count\n    tmp = grpx.groupby('cluster').agg({'price':'count'})\n    plt.figure()\n    plt.barh(lbls,tmp['price'])\n    plt.title(object_name + ' total sales count per cluster')\n    plt.show()\n    \n    # average sale amount\n    tmp = grpx.groupby('cluster').agg({'price':'mean'})\n    plt.figure()\n    plt.barh(lbls,tmp['price'])\n    plt.title(object_name + ' total sales mean per cluster')\n    plt.show()\n    \n    # average lifetime\n    tmp = grpx.groupby('cluster').agg({'lifetime':'mean'})\n    plt.figure()\n    plt.barh(lbls,tmp['lifetime'])\n    plt.title(object_name + ' lifetime per cluster')\n    plt.show()\n    \n    #add_split\n","227c1819":"def countUniqueX(df, ts_col,start_date, end_date, col_to_work):\n    # This function can be generelized more but it's only set as a function to save multiple calling of the code\n    #pivotSequence(df, ts_col, start_date, end_date, days, colToGroup, col_to_agg, action = 'count')\n    # local var declaration\n    unique_count = pd.DataFrame()\n    unique_count = pivotSequence(df, ts_col, start_date, end_date, 1, col_to_work, col_to_work, action = 'count')\n    unique_count.set_index(col_to_work,inplace=True)\n    unique_count[unique_count > 0] = 1\n    unique_count = unique_count.sum(axis=0)\n    unique_count = pd.DataFrame(unique_count)\n    unique_count.reset_index(inplace=True)\n    unique_count.columns = ['days_from_start','x_count']\n    unique_count['date'] = start_date\n    unique_count['date'] = unique_count['date'] + pd.TimedeltaIndex(unique_count['days_from_start'], unit='D')\n    unique_count['date'] = unique_count['date'] - dt.timedelta(days=1)\n    \n    return unique_count","a8802280":"def countUniqueXPerCluster(df,cluster_col,idx,ts_col):\n    result = pd.DataFrame()\n    for i in df[cluster_col].unique():\n        temp = df[df[cluster_col] == i][[idx,ts_col]]\n        res = countUniqueX(temp,ts_col,start_date, end_date,idx)\n        res[cluster_col] = i\n        result = pd.concat([result,res],ignore_index=True)\n    result['date']  = pd.to_datetime(result['date'])\n    return result","f158dda3":"def x_elbow(grpx,range0=np.arange(2,10)):\n    distortions = []\n    silhuettes = []\n\n#K = range(1,10)\n    for k in range0:\n        x_cluster = KMeans(n_clusters=k,init='k-means++', n_init=20, random_state=random_state,max_iter=400)\n        x_cluster.fit(grpx)\n        distortions.append(x_cluster.inertia_)\n        silhuettes.append(silhouette_score(grpx, x_cluster.labels_, metric='euclidean'))\n\n    #https:\/\/matplotlib.org\/2.2.5\/gallery\/api\/two_scales.html\n    fig, ax1 = plt.subplots()\n\n    color = 'tab:red'\n    ax1.set_xlabel('k')\n    ax1.set_ylabel('Distortion', color=color)\n    ax1.plot(range0, distortions, 'bx-')\n    ax1.tick_params(axis='y', labelcolor=color)\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\n    color = 'tab:blue'\n    ax2.set_ylabel('silhuette score', color=color)  # we already handled the x-label with ax1\n    ax2.plot(range0, silhuettes, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n    \n    return distortions, silhuettes","c911f162":"def unOutlierOnTop(df,value_col):\n    q1 = df[value_col].quantile(0.25)\n    q3 = df[value_col].quantile(0.75)\n    threshold = q3 + 1.5 * (q3 - q1)\n    df.loc[df[value_col] > threshold,value_col] = threshold\n    \n    return df","f400da25":"def train_test_metrices(test,pred, x_axis): #each parameter is a series\n        #ret = pd.DataFrame()\n        #ret['']\n        #yes, counld have been a dictionary....\n        ret = []\n        ret.append(round(sm.mean_absolute_error(test, pred),2))\n        ret.append(round(np.sqrt(sm.mean_squared_error(test, pred)),2))\n        ret.append(round(sm.median_absolute_error(test, pred),2))\n        ret.append(round(sm.explained_variance_score(test, pred),2))\n        ret.append(round(sm.r2_score(test, pred),2))\n        retdesc = [\"mean absolute error: \",\"root mean squared error: \",\"median absolute error: \",\"explained variance score: \",\"r2 score: \"]\n        \n        print('prediction vs test')\n        for i in np.arange(0,5):\n            print(retdesc[i], ret[i])\n        #print(\"mean absolute error: \", round(sm.mean_absolute_error(test, pred),2))\n        #print(\"root mean squared error: \", round(np.sqrt(sm.mean_squared_error(test, pred)),2))\n        #print(\"median absolute error: \", round(sm.median_absolute_error(test, pred),2))\n        #print(\"explained variance score: \", round(sm.explained_variance_score(test, pred),2))\n        #print(\"r2 score: \", round(sm.r2_score(test, pred),2))\n        \n        #plt.bar(courses, values, color ='maroon', width = 0.4)\n        plt.figure(figsize=(12, 6))\n        plt.plot(x_axis,test, label='test',color ='maroon')\n        plt.plot(x_axis,pred, label='prediction')\n        plt.show()\n        \n        return ret","f2fe3664":"def printGraphs(df,x_axis_col):\n    cols = list(df.columns)\n    cols.remove(x_axis_col)\n    \n    plt.figure(figsize=(12, 6))\n    \n    for col in cols:\n        plt.plot(df[x_axis_col],df[col], label=col)\n    plt.show()\n    ","ebaf3bfd":"def yearMonth(odf,date_grouper,action):\n    df = pd.DataFrame()\n    df = odf.copy()\n    df['yearMonth'] = df[date_grouper].dt.year*100 + df[date_grouper].dt.month\n    df.drop(date_grouper, axis=1,inplace=True)\n    cols = list(df.columns)\n    cols.remove('yearMonth')\n    agg_params = dict.fromkeys(cols,action)\n    df = df.groupby('yearMonth').agg(agg_params)\n\n    return df","4e140917":"customers = pd.read_csv(basefolder +'olist_customers_dataset.csv')\ngeo = pd.read_csv(basefolder +'olist_geolocation_dataset.csv')\norder_items = pd.read_csv(basefolder +'olist_order_items_dataset.csv')\norder_payments = pd.read_csv(basefolder +'olist_order_payments_dataset.csv')\norder_reviews = pd.read_csv(basefolder +'olist_order_reviews_dataset.csv')\norders = pd.read_csv(basefolder +'olist_orders_dataset.csv')\nproducts = pd.read_csv(basefolder +'olist_products_dataset.csv')\nsellers = pd.read_csv(basefolder +'olist_sellers_dataset.csv')\n\npcat = pd.read_csv(basefolder +'product_category_name_translation.csv')\n#holidays = pd.read_csv(basefolder +'brazil_holidays.csv')","0a098a00":"products.rename(columns = {'product_name_lenght': 'product_name_length'},inplace=True) # too annoying to read !","298c2e75":"pcat.head()","af7e8fa1":"print('the product categories from the products table: '  + str(products[['product_category_name']].nunique()))\nprint('the product categories from the products categories table: '  + str(pcat[['product_category_name']].nunique()))","98d92718":"missing_products = products[['product_category_name']]\nmissing_products = pd.DataFrame(missing_products['product_category_name'].unique())\nmissing_products.columns = ['product_category_name']\nmissing_products['product_category_name_english'] = missing_products['product_category_name'].copy()","7e1d6081":"pcat = pd.concat([pcat, missing_products])\npcat.drop_duplicates(inplace=True)","bedb60e0":"pcat0 = pcat\npcat.loc[pcat.shape[0]] = ['0','0']\ncatEncoder(pcat)","9c90709c":"products.isna().any()","7f1b274a":"products = fillDf(products)","acf99497":"sellers.head()","f9795161":"tmp = pd.DataFrame(sellers['seller_state'].unique())\ntmp.loc[tmp.shape[0]] = ['0']\ntmp.columns = ['seller_state']\ncatEncoder(tmp,runOver=True)\ntmp = pd.DataFrame(sellers['seller_city'].unique())\ntmp.loc[tmp.shape[0]] = ['0']\ntmp.columns = ['seller_city']","e399e4d5":"customers.head()","dd8b3b2b":"tmp = pd.DataFrame(customers['customer_state'].unique())\ntmp.loc[tmp.shape[0]] = ['0']\ntmp.columns = ['customer_state']\ncatEncoder(tmp,runOver=True)\ntmp = pd.DataFrame(customers['customer_city'].unique())\ntmp.loc[tmp.shape[0]] = ['0']\ntmp.columns = ['customer_city']","eb1ef1c0":"orders.head()","8d8a07b5":"orders0 = orders.copy() # a backup of the original dataset, though we do have it on file...\norders = pd.merge(orders, customers[['customer_id','customer_unique_id']],how='left',on='customer_id')","cc0dcaa8":"order_items.head()","ce65ab90":"order_items = pd.merge(order_items,orders[['order_id','customer_id','customer_unique_id','order_purchase_timestamp']],how='left',on='order_id')\norder_items = pd.merge(order_items,sellers[['seller_id','seller_state']],how='left',on='seller_id')\n#order_items.drop(['seller_zip_code_prefix','seller_city','shipping_limit_date','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date'],axis=1, inplace=True) #'order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date'\n#order_items.drop(['order_status'],axis=1,inplace=True)\norder_items['order_purchase_timestamp'] = pd.to_datetime(order_items['order_purchase_timestamp'])","1a25bd84":"geo.head()","82b52117":"oiCust = order_items #pd.merge(order_items,orders,how='inner',on='order_id')\noiCust.head()\noiCust = pd.merge(oiCust,customers,how='inner',on='customer_unique_id')\noiCust['order_purchase_timestamp'] = pd.to_datetime(oiCust['order_purchase_timestamp'])\noiCust['yearmonth'] = oiCust['order_purchase_timestamp'].dt.year*100 + oiCust['order_purchase_timestamp'].dt.month","05bf4f69":"aggSalesPerDay = oiCust\naggSalesPerDay.head()\naggSalesPerDay['order_purchase_timestamp'] = pd.to_datetime(aggSalesPerDay['order_purchase_timestamp'])\naggSalesPerDay['o_date'] = aggSalesPerDay['order_purchase_timestamp'].dt.date","ce7946f7":"aggSalesPerDay = aggSalesPerDay.groupby('o_date').agg({'order_item_id': 'count','price':'sum','customer_unique_id': lambda x: x.nunique(),'seller_id': lambda x: x.nunique(),})\naggSalesPerDay.columns = ['sales_count','sales_sum','customer_count','sellers_count']\naggSalesPerDay.reset_index(inplace=True)\naggSalesPerDay['MeanSale'] = round(aggSalesPerDay['sales_sum'] \/ aggSalesPerDay['sales_count'],2)","4f3a5816":"plt.plot(aggSalesPerDay['o_date'], aggSalesPerDay['sales_count'])\nplt.show()","77845c21":"plt.plot(aggSalesPerDay['o_date'], aggSalesPerDay['sales_sum'])\nplt.show()","1589ec46":"plt.plot(aggSalesPerDay['o_date'], aggSalesPerDay['MeanSale'])\nplt.show()","a6781688":"working_start_date = dt.datetime(2017, 1, 1)\nworking_end_date = dt.datetime(2018, 7, 1)","0a70b8c8":"order_items = order_items[((order_items['order_purchase_timestamp'].dt.date >= working_start_date.date()) & (order_items['order_purchase_timestamp'].dt.date < working_end_date.date()))]","3f84cd65":"unique_sellers = countUniqueX(order_items[['seller_id','order_purchase_timestamp','price']],'order_purchase_timestamp',working_start_date, working_end_date,'seller_id')\nunique_sellers.head(2)","6719ebb0":"plt.plot(unique_sellers['date'], unique_sellers['x_count'])\nplt.show()","8d2f2fa8":"unique_products = countUniqueX(order_items[['product_id','order_purchase_timestamp','price']],'order_purchase_timestamp',working_start_date, working_end_date,'product_id')\nplt.plot(unique_products['date'], unique_products['x_count'])\nplt.show()","d1b11fe2":"unique_customers = countUniqueX(order_items[['customer_unique_id','order_purchase_timestamp','price']],'order_purchase_timestamp',working_start_date, working_end_date,'customer_unique_id')\nplt.plot(unique_customers['date'], unique_customers['x_count'])\nplt.show()","be93f132":"# by this definition\n# https:\/\/en.wikipedia.org\/wiki\/Outlier","e2c25426":"unique_sellers = unOutlierOnTop(unique_sellers,'x_count')\nplt.plot(unique_sellers['date'], unique_sellers['x_count'])\nplt.show()","6a8a3200":"oiCust['dte'] = oiCust['order_purchase_timestamp'].dt.date\noiCust['dte'] = pd.to_datetime(oiCust['dte'])\ntest47 = oiCust.groupby('dte').agg({'customer_unique_id': lambda x: x.nunique(),'seller_id': lambda x: x.nunique(),'product_id': lambda x: x.nunique()})\ntest47.reset_index(inplace=True)\n#test47['yearmonth'] = test47['dte'].dt.year*100 + test47['dte'].dt.month\ntest47 = yearMonth(test47,'dte','mean')\ntest47.tail()\n#test47.set_index('yearmonth',inplace=True)\n","ef2b3729":"grped_oiCust = oiCust.groupby('yearmonth').agg({'product_id': 'count','price':'sum'})\ngrped_oiCust = pd.merge(grped_oiCust,test47,left_index=True,right_index=True,how='inner')\ngrped_oiCust.columns = ['SalesCount','SalesSum','CustomersCount','SellersCount','ProductsCount']\ngrped_oiCust['MeanSale'] = round(grped_oiCust['SalesSum'] \/ grped_oiCust['SalesCount'],2)\ngrped_oiCust['SalesPerSeller'] = round(grped_oiCust['SalesSum'] \/ grped_oiCust['SellersCount'],2)\ngrped_oiCust['SalesPerCustomer'] = round(grped_oiCust['SalesSum'] \/ grped_oiCust['CustomersCount'],2)\ngrped_oiCust['PurchasesPerCustomer'] = round(grped_oiCust['SalesCount'] \/ grped_oiCust['CustomersCount'],2)","2f0fbb66":"grped_oiCust.T.head(11)","ef5ebfd4":"grped_oiCust.head(11)","d3aa6207":"#specifying\nstart_date = dt.datetime(2017, 1, 1)\nend_date = dt.datetime(2017, 11, 1)\nperiods = (end_date - start_date).days + 1\nprint('train periods: ' + str(periods))\n\ntest_start_date = end_date #dt.datetime(2017, 11, 1)\ntest_end_date = dt.datetime(2018, 7, 1)\nmodel_now = end_date\n\nperiods = (test_end_date - test_start_date).days + 1\nprint('test periods: ' + str(periods))","b36fb8b5":"#oi = order_items[order_items['seller_state'] == state].copy()\nprod_pred = pd.DataFrame()\nsellers_pred = pd.DataFrame()\navg = pd.DataFrame()\noi = order_items.copy()\noi.drop(['seller_state'],axis=1,inplace=True)","df6b7b40":"group_vars = \"'price':'mean','price':'min','freight_value':'mean','product_category_name':'min','product_name_length':'min','product_photos_qty':'mean','product_weight_g':'mean','product_height_cm':'mean','product_width_cm':'min','product_length_cm':'min'\"\noipp0, grpp0 = createOipx(order_items,products,start_date,end_date, merge_on='product_id', group_by=group_vars) ###\n\n#customer_zip_code_prefix\tcustomer_city\tcustomer_state\ngroup_vars = \"'price':'count','price':'sum','price':'mean','freight_value':'mean','seller_state_x':'min'\"\noips0, grps0 = createOipx(order_items, sellers, start_date,end_date, merge_on='seller_id',strliaze = 'seller_state_x', group_by=group_vars)\n\ngroup_vars = \"'price':'count','price':'sum','price':'mean','freight_value':'mean','customer_state':'min','customer_city':'min'\" # the stares are label ecustomer_idncoded and will always be the same, so minimize will just select the proper label\noipc0, grpc0 = createOipx(order_items, customers, start_date,end_date, merge_on='customer_unique_id',strliaze = 'customer_state', group_by=group_vars)","de68d8a1":"grpc0.head()","51ab516a":"edapx(grps0,'seller')\nedapx(grpc0,'customer')\nedapx(grpp0,'product')","0c010220":"disc, silc = x_elbow(grpc0,range0=np.arange(2,10))","c9e78ad1":"diss, sils = x_elbow(grps0,range0=np.arange(2,10))","4001316b":"discp, silp = x_elbow(grpp0,range0=np.arange(2,10))","f5ff103f":"customer_cluster = KMeans(n_clusters=3,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grpc0)\ngrpc0['cluster'] = customer_cluster.predict(grpc0)\nsellers_cluster = KMeans(n_clusters=3,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grps0)\ngrps0['cluster'] = sellers_cluster.predict(grps0)\nproducts_cluster = KMeans(n_clusters=4,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grpp0)\ngrpp0['cluster'] = products_cluster.predict(grpp0)","023959f3":"edapx_clustered(grpc0,'customer')","18c29f6c":"edapx_clustered(grps0,'sellers')","c968502a":"edapx_clustered(grpp0,'product')","8109bd3f":"for i in grpp0['cluster'].unique():\n    print('more eda per product, cluster: ' + str(i))\n    edapx(grpp0[grpp0['cluster'] == i],'product')","b627df9f":"for i in grps0['cluster'].unique():\n    print('more eda per seller, cluster: ' + str(i))\n    edapx(grps0[grps0['cluster'] == i],'seller')","f902f775":"for i in grpc0['cluster'].unique():\n    print('more eda per customer, cluster: ' + str(i))\n    edapx(grpc0[grpc0['cluster'] == i],'customer')","1bd9227c":"sellers_cluster = KMeans(n_clusters=6,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grps0)\ngrps0['cluster'] = sellers_cluster.predict(grps0)\nedapx_clustered(grps0,'sellers')","efd450e2":"for i in grps0['cluster'].unique():\n    print('more eda per seller, cluster: ' + str(i))\n    edapx(grps0[grps0['cluster'] == i],'seller')","3e8c3cfc":"oips0 = pd.merge(oips0,grps0[['cluster']],left_on='seller_id',right_index=True,how='inner')\noipp0 = pd.merge(oipp0,grpp0[['cluster']],left_on='product_id',right_index=True,how='inner')\noipc0 = pd.merge(oipc0,grpc0[['cluster']],left_on='customer_unique_id',right_index=True,how='inner')","1c588b8c":"order_items = pd.merge(order_items,grpp0[['cluster']],left_on='product_id',right_index=True,how='left')\norder_items.rename(columns={'cluster':'pcluster'}, inplace=True)\norder_items = pd.merge(order_items,grps0[['cluster']],left_on='seller_id',right_index=True,how='left')\norder_items.rename(columns={'cluster':'scluster'}, inplace=True)\norder_items = pd.merge(order_items,grpc0[['cluster']],left_on='customer_unique_id',right_index=True,how='left')\norder_items.rename(columns={'cluster':'ccluster'}, inplace=True)","bc386837":"group_vars = \"'ccluster':'min','scluster':'min','price':'mean','price':'min','freight_value':'mean','product_category_name':'min','product_name_length':'min','product_photos_qty':'mean','product_weight_g':'mean','product_height_cm':'mean','product_width_cm':'min','product_length_cm':'min'\"\noipp0, grpp0 = createOipx(order_items,products,start_date,end_date, merge_on='product_id', group_by=group_vars) ###\n\n#customer_zip_code_prefix\tcustomer_city\tcustomer_state\ngroup_vars = \"'ccluster':'min','pcluster':'min','price':'count','price':'sum','price':'mean','freight_value':'mean','seller_state_x':'min'\"\noips0, grps0 = createOipx(order_items, sellers, start_date,end_date, merge_on='seller_id',strliaze = 'seller_state_x', group_by=group_vars)\n\ngroup_vars = \"'pcluster':'min','scluster':'min','price':'count','price':'sum','price':'mean','freight_value':'mean','customer_state':'min','customer_city':'min'\" # the stares are label ecustomer_idncoded and will always be the same, so minimize will just select the proper label\noipc0, grpc0 = createOipx(order_items, customers, start_date,end_date, merge_on='customer_unique_id',strliaze = 'customer_state', group_by=group_vars)","2bceb0fe":"customer_cluster = KMeans(n_clusters=3,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grpc0)\ngrpc0['cluster'] = customer_cluster.predict(grpc0)\nsellers_cluster = KMeans(n_clusters=3,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grps0)\ngrps0['cluster'] = sellers_cluster.predict(grps0)\nproducts_cluster = KMeans(n_clusters=4,init='k-means++', n_init=20, random_state=random_state,max_iter=400).fit(grpp0)\ngrpp0['cluster'] = products_cluster.predict(grpp0)","55b48948":"grpp0.reset_index(inplace=True)\ngrps0.reset_index(inplace=True)\ngrpc0.reset_index(inplace=True)","271482a7":"oips0 = pd.merge(oips0,grps0[['seller_id','cluster']],on='seller_id',how='inner')\noipp0 = pd.merge(oipp0,grpp0[['product_id','cluster']],on='product_id',how='inner')\noipc0 = pd.merge(oipc0,grpc0[['customer_unique_id','cluster']],on='customer_unique_id',how='inner')","42edc75c":"def TrainTestModel(start_date,end_date,test_start_date,test_end_date,oipxs={'oipc0':oipc0,'oips0':oips0,'oipp0':oipp0},oipxs1={'oipp0':{'grpx':grpp0,'merge_on':'product_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'},'oipc0':{'grpx':grpc0,'merge_on':'customer_unique_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'},'oips0':{'grpx':grps0,'merge_on':'seller_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'}},reg_to_send=pd.DataFrame(),date_filter='order_purchase_timestamp',col_to_agg='price',action='sum'):\n    # after all of the parameters in world, there's still a global var reference, meaning - this should have been a part of a class\n    sales_sum = order_items[[date_filter,col_to_agg]].copy()\n    sales_sum[date_filter] = pd.to_datetime(sales_sum[date_filter]).dt.date\n    sales_sum = sales_sum.groupby(date_filter).agg({col_to_agg:action})\n    sales_sum.reset_index(inplace=True)\n    sales_sum.columns = ['ds','y']\n    sales_sum['ds'] = pd.to_datetime(sales_sum['ds']).dt.date\n    #train\n    sales_sum_train = sales_sum[(sales_sum['ds'] >= start_date.date()) & (sales_sum['ds'] < end_date.date())].copy()\n    sales_sum_train['ds'] = pd.to_datetime(sales_sum['ds'])\n    #test\n    sales_sum_test = sales_sum[(sales_sum['ds'] >= test_start_date.date()) & (sales_sum['ds'] <= test_end_date.date())].copy()\n    sales_sum_test['ds'] = pd.to_datetime(sales_sum['ds'])\n\n\n    final_preds = pd.DataFrame()\n    final_preds['ds'] = sales_sum_test['ds']\n    final_preds['ds'] = pd.to_datetime(final_preds['ds'])\n\n    final_preds_per_cluster = final_preds.copy()\n\n    #sales sum\n    sales_sum_preds = dict\n\n    oipx_preds = pd.DataFrame()\n    regressor = pd.DataFrame()\n\n    # the regressor is built for the entire period, to help the prophet predict\n    regressor['ds'] = sales_sum['ds']\n    regressor['ds'] = pd.to_datetime(regressor['ds'])\n\n    #train = pd.DataFrame()\n    #we need this stage so that in the very first iteration, the df will be empty, and the predictor won't break\n    #once we have something to work with, right after the 1st loop iteration, we'll populate and improve it\n    #reg_to_send = pd.DataFrame()\n\n    tmp = pd.DataFrame()\n    print('test shape: ')\n    print(sales_sum_test.shape)\n\n    # repeat until 1 result remains\n\n    while len(oipxs) >= 1:\n\n        oipx_preds = pd.DataFrame()\n        oipx_preds['ds'] = sales_sum_test['ds']\n        oipx_preds['ds'] = pd.to_datetime(oipx_preds['ds'])\n\n        oipx_preds_per_cluster = pd.DataFrame()\n        oipx_preds_per_cluster = oipx_preds.copy()\n        oipx_preds_per_cluster.set_index('ds', inplace=True)\n\n        #run on all of the clusters, and aggregate\n        for oi_name, oipx in oipxs.items():\n            print('running on ' + oi_name + ' clusters')\n            #filtering the data to the train set to be fit\n            train = pd.merge(oipx[[oipxs1[oi_name]['merge_on'],oipxs1[oi_name]['filter_date']]],oipxs1[oi_name]['grpx'][[oipxs1[oi_name]['merge_on'],oipxs1[oi_name]['col_to_agg'],'cluster']],how='inner',on=oipxs1[oi_name]['merge_on'])\n            train.drop(oipxs1[oi_name]['merge_on'],inplace=True,axis=1)\n            train.columns = [oipxs1[oi_name]['filter_date'],oipxs1[oi_name]['col_to_agg'],'cluster']\n            # the actual prediction !!\n            tmp = fit_predict(train,filter_date=oipxs1[oi_name]['filter_date'],col_to_agg=oipxs1[oi_name]['col_to_agg'],action=oipxs1[oi_name]['action'],outliers=False, regressor=reg_to_send) #\n\n            oipx_preds = pd.merge(oipx_preds,tmp[['total']],left_on='ds', right_index=True,how='inner')\n            oipx_preds.rename(columns={'total':oi_name}, inplace=True)\n\n            # results saving in the proper places\n            tmp.drop(['total'],axis=1,inplace=True)\n            tmp = tmp.add_suffix('_'+ oi_name + '_per_cluster')\n            oipx_preds_per_cluster = pd.merge(oipx_preds_per_cluster,tmp,left_index=True, right_index=True,how='inner')\n\n            train_test_metrices(sales_sum_test['y'], oipx_preds[oi_name], sales_sum_test['ds'])\n            \n\n        printGraphs(pd.merge(oipx_preds,sales_sum_test,on='ds',how='inner'),'ds')\n        \n        # check each aggregation with test\n        loss = {}\n        preds_list = list(oipx_preds.columns) #.remove('ds')\n        preds_list.remove('ds')\n        for oipx_pred in preds_list:\n            loss[oipx_pred] = sm.r2_score(sales_sum_test['y'],oipx_preds[oipx_pred]) #\n\n        # find the best results based on loss function\n        max_value = max(loss.values())  # maximum value\n        max_keys = [k for k, v in loss.items() if v == max_value] # getting all keys containing the `maximum`\n        max_key = max_keys[0]\n\n\n        # use the best results as regressor for the rest\n        backwarder = oipx_preds[['ds',max_key]]\n        backwarder.columns = ['ds','y']\n        backwarder = pd.concat([sales_sum_train[['ds','y']],backwarder],ignore_index=True)\n        backwarder.columns = ['ds',max_key]\n        regressor = pd.merge(regressor,backwarder,on='ds',how='left')\n        #we need this stage so that in the very first iteration, the df will be empty, and the predictor won't break\n        #once we have something to work with, right after the 1st loop iteration, we'll populate and improve it\n        \n        #kept in comment until the regressor is fixed\n        #reg_to_send = regressor\n        \n        \n        incol = '_' + max_key + '_per_cluster'\n        final_cols = [col for col in oipx_preds_per_cluster.columns if incol in col]\n        final_preds_per_cluster = pd.merge(final_preds_per_cluster,oipx_preds_per_cluster[final_cols],left_on='ds', right_index=True, how='inner')\n        final_preds = pd.merge(final_preds,oipx_preds[['ds',max_key]],on='ds', how='inner')\n\n        # remove the best results function from the network\n        #oipxs0.remove(max_key)\n        del oipxs[max_key]\n        print(oipxs.keys())\n\n    # average it all\n    final_preds.set_index('ds', inplace=True)\n    final_preds['mean'] = final_preds.mean(axis=1)\n    \n    return final_preds, final_preds_per_cluster, reg_to_send","db01bf61":"oipxs = {\n    'oipc0':oipc0,\n    'oips0':oips0,\n    'oipp0':oipp0\n}\n\noipxs1 = {\n        'oipp0':{'grpx':grpp0,'merge_on':'product_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'},\n        'oipc0':{'grpx':grpc0,'merge_on':'customer_unique_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'},\n        'oips0':{'grpx':grps0,'merge_on':'seller_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'sum'}\n         }\n    \nsum_final_preds, sum_final_preds_per_cluster, sum_reg_to_send = TrainTestModel(start_date,end_date,test_start_date,test_end_date,oipxs=oipxs,oipxs1=oipxs1,date_filter='order_purchase_timestamp',col_to_agg='price',action='sum')","db6289f2":"oipxs = {\n    'oipc0':oipc0,\n    'oips0':oips0,\n    'oipp0':oipp0\n}\n\noipxs2 = {\n        'oipp0':{'grpx':grpp0,'merge_on':'product_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'count'},\n        'oipc0':{'grpx':grpc0,'merge_on':'customer_unique_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'count'},\n        'oips0':{'grpx':grps0,'merge_on':'seller_id','filter_date':'order_purchase_timestamp','col_to_agg':'price','action':'count'}\n         }\ncnt_final_preds, cnt_final_preds_per_cluster, cnt_reg_to_send = TrainTestModel(start_date,end_date,test_start_date,test_end_date,oipxs=oipxs,oipxs1=oipxs2,date_filter='order_purchase_timestamp',col_to_agg='price',action='count')","2cc38605":"cnt_final_preds.head()\ncnt_final_preds_per_cluster.head()\ncnt_reg_to_send.head()","a73e7f41":"unique_sellers_pred = pd.DataFrame()\nunique_products_pred = pd.DataFrame()\nunique_customers_pred = pd.DataFrame()\n\nuniques = {\n 'sellers': {\n 'unique_x_per_cluster':countUniqueXPerCluster(oips0,'cluster','seller_id','order_purchase_timestamp'),#playing around with annoymous DFs...\n 'unique_x': unique_sellers,\n 'pred':unique_sellers_pred,\n 'idx':'seller_id'\n },\n  'products': {\n 'unique_x_per_cluster':countUniqueXPerCluster(oipp0,'cluster','product_id','order_purchase_timestamp'),\n 'unique_x': unique_products,\n 'pred':unique_products_pred,\n 'idx':'product_id'\n },\n  'customers': {\n  'unique_x_per_cluster':countUniqueXPerCluster(oipc0,'cluster','customer_unique_id','order_purchase_timestamp'),\n  'unique_x': unique_customers,\n  'pred':unique_customers_pred,\n  'idx':'customer_unique_id'\n }\n}","5dc1119a":"for unique_type, unique_ref in uniques.items():\n    unique_ref['pred'] = fit_predict(unique_ref['unique_x_per_cluster'],filter_date='date',col_to_agg='x_count',action='sum',outliers=False,regressor=pd.DataFrame())\n    unique_ref['pred'].reset_index(inplace=True)\n    \n    pred = unique_ref['pred'][(unique_ref['pred']['ds'].dt.date >= test_start_date.date()) & (unique_ref['pred']['ds'].dt.date <= test_end_date.date())]\n    test = unique_ref['unique_x'][(unique_ref['unique_x']['date'].dt.date >= test_start_date.date()) & (unique_ref['unique_x']['date'].dt.date <= test_end_date.date())]\n    print(unique_type)\n    train_test_metrices(test['x_count'],pred['total'], test['date'])\n\nunique_customers_pred = uniques['customers']['pred']\nunique_products_pred = uniques['products']['pred']\nunique_sellers_pred = uniques['sellers']['pred']","59bba438":"sum_final_preds.reset_index(inplace=True)\ncnt_final_preds.reset_index(inplace=True)","5e22b237":"#nique_sellers_pred\n#nique_products_pred\n#nique_customers_pred","0e6cf657":"#cnt_final_preds.head()\n#cnt_final_preds_per_cluster.head()","696b33ca":"#sum_final_predssum_final_preds\n#sum_final_preds_per_cluster","b0a5d276":"sum_final_preds.head()","9026c9ea":"grped_oiCust_predicted = pd.DataFrame(index=grped_oiCust.index)","c9277b50":"YM = yearMonth(cnt_final_preds,'ds','sum')\ngrped_oiCust_predicted = pd.merge(grped_oiCust_predicted,YM[['mean']],left_index=True, right_index=True,how='left')\ngrped_oiCust_predicted.rename(columns={'mean':'SalesCount'}, inplace=True)\n\nYM = yearMonth(sum_final_preds,'ds','sum')\ngrped_oiCust_predicted = pd.merge(grped_oiCust_predicted,YM[['mean']],left_index=True, right_index=True,how='left')\ngrped_oiCust_predicted.rename(columns={'mean':'SalesSum'}, inplace=True)\n\nYM = yearMonth(unique_customers_pred,'ds','mean')\nYM['total']\ngrped_oiCust_predicted = pd.merge(grped_oiCust_predicted,YM[['total']],left_index=True, right_index=True,how='left')\ngrped_oiCust_predicted.rename(columns={'total':'CustomersCount'}, inplace=True)\n\nYM = yearMonth(unique_sellers_pred,'ds','mean')\ngrped_oiCust_predicted = pd.merge(grped_oiCust_predicted,YM[['total']],left_index=True, right_index=True,how='left')\ngrped_oiCust_predicted.rename(columns={'total':'SellersCount'}, inplace=True)\n\nYM = yearMonth(unique_products_pred,'ds','mean')\ngrped_oiCust_predicted = pd.merge(grped_oiCust_predicted,YM[['total']],left_index=True, right_index=True,how='left')\ngrped_oiCust_predicted.rename(columns={'total':'ProductsCount'}, inplace=True)\n","5717e342":"grped_oiCust_predicted['MeanSale'] = round(grped_oiCust_predicted['SalesSum'] \/ grped_oiCust_predicted['SalesCount'],2)\ngrped_oiCust_predicted['SalesPerSeller'] = round(grped_oiCust_predicted['SalesSum'] \/ grped_oiCust_predicted['SellersCount'],2)\ngrped_oiCust_predicted['SalesPerCustomer'] = round(grped_oiCust_predicted['SalesSum'] \/ grped_oiCust_predicted['CustomersCount'],2)\ngrped_oiCust_predicted['PurchasesPerCustomer'] = round(grped_oiCust_predicted['SalesCount'] \/ grped_oiCust_predicted['CustomersCount'],2)","829dbacb":"test_start_year_month = test_start_date.year * 100 + test_start_date.month #dt.datetime(2017, 1, 1)\ntest_end_year_month = test_end_date.year * 100 + test_end_date.month","3b4889cb":"grped_oiCust_predicted[((grped_oiCust_predicted.index >= int(test_start_year_month)) & (grped_oiCust_predicted.index < int(test_end_year_month)))]","3231d0e9":"grped_oiCust[((grped_oiCust.index >= int(test_start_year_month)) & (grped_oiCust.index < int(test_end_year_month)))]","6b55cb2f":"pred_grouped = round(grped_oiCust_predicted[((grped_oiCust_predicted.index >= int(test_start_year_month)) & (grped_oiCust_predicted.index < int(test_end_year_month)))],2)\ntest_grouped = round(grped_oiCust[((grped_oiCust.index >= int(test_start_year_month)) & (grped_oiCust.index < int(test_end_year_month)))],2)","2841ef2a":"pred_grouped.head(8)","2dfd7184":"test_grouped.head(8)","1686ca70":"cols = test_grouped.columns\nfinal_metrices = pd.DataFrame()\nfm_index = pd.DataFrame([\"mean absolute error\",\"root mean squared error\",\"median absolute error\",\"explained variance score\",\"r2 score\"])\nfm_index.columns = ['index']\nfor col in cols:\n    print('metrices for: ' + col)\n    res = pd.DataFrame(train_test_metrices(test_grouped[col],pred_grouped[col], np.arange(1,1+len(test_grouped['SalesCount']))))\n    res.columns = [col]\n    \n    final_metrices = pd.concat([final_metrices,res[col]],ignore_index=False,axis=1)\n\nfinal_metrices = pd.concat([final_metrices,fm_index],axis=1)\nfinal_metrices.set_index('index',inplace=True)","0d2323a2":"final_metrices.head()","af93e22c":"grpc0.head()","9649f6e9":"#### we need to notice that - if we're predcting trend over time, the calendar year trend is important. right now we have only 1.5 years, and the outlier even happens in the last few days of noevember, within our single calendar year\n#### meaning, if we assume that the training is in our past, we'll need to treat this - remove the outliers","04bd068b":"Business issue to resolves:\nrecognizing the trend of revenues and product sales and prepare the appropriate budget\n\nAs a marketplace, we expect various trends to appear in the data, based on products, customers and sellers","bece6526":"next","cbfb7f6e":"the actual details of each order, where the products and sellers connection lies... what we need for it more is the order purchase date, the foreign keys and maybe the state of sale","aa88fe4e":"## results","4e1776b8":"Products viewing and arranging","9137b0d8":"Looking at the sales counts and sum over time we can see: <br>\n<ul>\n    <li>junk at the graphs edges, meaning we'll need to slice it to work on it<\/li>\n    <li>a general trend inside of which a smaller pulse, probably a weekly trend, and a huge unrelated surge <\/li>\n<\/ul>\n<br>\nLet's specifiy a work date range:","d43eaf95":"## Data presentation","b569d0a9":"### unique X prediction","fe2de6cd":"### Data Loading","2d7b28d2":"Now that we've set our main tables, let's EDA some more","277eb2e5":"##### (from this point onwards, regarding any preset function to facilitate the process, we can simply say: by coincidence I have prepared in advance)","194a81a2":"not too many sellers features... only location data <br>\nin nay case, let's encode in advance","9a3b20f4":"unless we do a GIS analysis, which is not intended, then there's no extra value in this data","c34b7535":"The main predictive metrices have an upwards bias that derives from the outlier case, happening in test period. We have to big drops in 2018 that are also hard to predict, as intuitively predicted\nLack of more data prevented a more robust model","e3574c35":"let's slice the order_items data to work with","a87be728":"All over time trends behave in a very similar fashion\nthe outlier shows up every time","d7fbeaef":"Distortion and Silhuette - Sellers","6be704d7":"What I need to do is to un-function the prediction...\nTo predict for each cluster, while I loop on the clusters directly\nThen aggregate the results, then compare them to the test data, unclustere\nIn that way I can add a whole bunch of features for clustering and make it better\nI can also be more accurate, I want the total accurate, not my own intermediary cluster\n\nalso, I can make a few clustered predictions: by sellers, customers, products, all of them in a single cluster then average the totals predictions then compare it to the test\n    run a few clustering models to be added to the pile ?\nin my free time I need to learn the prophet model more deeply. anything that I can add ?\n  ","b0bde484":"This project  aims to predict online retails sales of a multi vendor store using machine  learnong techniques, in python language\n\nThe data used in this project is from an online multi  vendor store, located in Barzil, for the brazilian market, named Olist. Their data  of salea for the period of 01\/2017 \u2013 07\/2018 the fully published on kaggle\nhttps:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce\n","a637f091":"like the sellers, nothing too special but location data <br>\nBut's there a mine here... <span style=\"font-style:italic\">customer_unique_id<\/span> implies that the customer_id is a \"stupid index\" not referring to the customer, unlike the product_id or seller_id <br>\n<span style=\"font-style:italic\">(why stick a mine there ? a joke by their DBA ... ?)<\/span><br>\nso, whenever we stumple upon the customer_id, we'll need to join the customer_unique_id\n","af6e9ded":"Over time","3f5506e7":"a lot of interesting information, mostly delivery stages dates <br>\nthe foreign keys for customers and orders <br>\nand our first mine hit...","2f791a1f":"### Final quality control and summarizing the data","0c3d7c3e":"## Forecasting","79eb8682":"after examining the distribution and graphs of a 3 clusters split for the sellers, we can see that some features are better split by cluster and some less\nso we can afford to take 6 clusters","abe13e15":"#### unique sellers, customers and product","64aec6ff":"Distortion and Silhuette - Customers","82a1ac73":"all of the products features can be filled with zeros, either as strings or as integers. there's no meaning for average","8e401702":"# Online Sales prediction","1ed025d3":"Customers viewing and arranging","80ce4509":"## Quick view and arranging  the data","e0892b52":"1st of all, this columns name is very annoying! <span style=\"font-style: italic;\">product_description_lenght<\/span>","58f9926c":"Sellers viewing and arranging","55ebd8fe":"Other stuff","0dece01b":"NULL checking","bfdadc93":"### main goal KPIs","bf8cb808":"### Functions implementations","f2b90fe6":"Our anchor dataset is of order items over time, for each main object: product, customer or seller, we\u2019ll join with order items, then group by the object, trying to get any extra information that we can\n\nSo, we got:\n<ul>\n\t<li>The obvious: average sales, total sale count, total sales sum <\/li>\n\t<li>more: lifetime, age in the system <\/li>\n\t<li>trends: the sales amount per week, for the training period, is the object active (sold in the month), was the object selling more than or less than in the last month in comparison to the previous <\/li>\n <\/ul>","cc3657e5":"### Global vars and consts settings","28a141cb":"### Data Setting and feature engineering","17fe10c7":"done with that - let's encode the categories, to have the same categories for any encoding","4bb21a1e":"#### now, we're going to add the cluster data for each X, and cluster for each X with the other 2 objects' clusters !","b5701ef8":"### Sales trend on a linear graph","2fe92a6e":"Let's see how does the data look without outliers - let's slice each outlier at the outlier threshold","5e4c4a9d":"for customer we'll choose 3 clusters, for sellers 3 as well and for products we can take 4\nfor sellers, this ","85316fb7":"Distortion and Silhuette - Products","1c7c44b4":"Looks much more ordered and trendy\nImportant ! - if we're to predict any sort of future, and we have this in our past data, we'll need to see if it helps our prediction or not","04cde739":"quickly viewing how the data looks","807180e0":"Orders and Order items viewing and arranging","26bd9aac":"before we EDA by cluster, and we cluster - then let's find the ideal cluster<br>\nwe'll check the ideal by looking at the elbow graph... <span style=\"font-style: italic;\">adding a siluette graph, for a second opinion<\/span>","fa78bac2":"There's a slight difference, which is strange, and might imply that both categoroeis list don't fully overlap","6b91899f":"Setting the data to be monthly, we'll check it's scoring, and start building the final data frames"}}