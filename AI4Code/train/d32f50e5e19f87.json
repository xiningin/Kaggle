{"cell_type":{"fb3d8660":"code","8eccdbc8":"code","9217190a":"code","e7015c82":"code","2f7026ac":"code","8c88ff9a":"code","f8a22ad8":"code","3203d9f2":"code","2b232370":"code","0014a92b":"code","3acda6b5":"code","970a2cb9":"code","b3bd453f":"code","f1f8773f":"code","1467cddb":"code","ed11f240":"code","12f3383c":"code","50182d41":"code","0549bc84":"code","6180b81a":"code","262682cf":"code","99705133":"code","003026c3":"markdown","feaf2f4f":"markdown","46feaa2d":"markdown","61352a4d":"markdown","edc09b0a":"markdown","482e120d":"markdown","3babfb7c":"markdown","9231559f":"markdown","a5d0b964":"markdown","f9dd7ada":"markdown","636e8866":"markdown","9a61fabc":"markdown","1b4ca8ea":"markdown","9b7b85a8":"markdown","fe6b6a87":"markdown","ab3b3a00":"markdown","c112a877":"markdown"},"source":{"fb3d8660":"import pandas as pd\nimport numpy as np\n\nimport plotly.express as px \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","8eccdbc8":"# Read csv file into dataframe\ndf = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\n\n# Show dataframe\ndf.head()","9217190a":"# Understand data\ndf.info()","e7015c82":"# Check for missing data\ndf.isnull().sum()","2f7026ac":"sns.set()\n\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(df.corr(), linewidths=.5, ax=ax, cmap='Blues')","8c88ff9a":"sns.set_style('white') \nfig, ax = plt.subplots(3,2,figsize=(13,15))\nsns.countplot(df['anaemia'], palette='Pastel1', ax=ax[0][0])\nsns.countplot(df['diabetes'], palette='Set3', ax=ax[0][1])\nsns.countplot(df['high_blood_pressure'], palette='Set2', ax=ax[1][0])\nsns.countplot(df['sex'], palette='Set1', ax=ax[1][1])\nsns.countplot(df['smoking'], palette='Pastel2', ax=ax[2][0])\nsns.countplot(df['DEATH_EVENT'], palette='Accent', ax=ax[2][1])","f8a22ad8":"fig, ax = plt.subplots(6,1,figsize=(13,20))\nplt.suptitle('Bivariate Analysis (Hue=Sex)', fontsize=20)\nplt.tight_layout(4)\n\nsns.lineplot(data=df, x='age', y='creatinine_phosphokinase', hue='sex', lw=2, ax=ax[0])\nsns.lineplot(data=df, x='age', y='ejection_fraction', hue='sex', lw=2, ax=ax[1])\nsns.lineplot(data=df, x='age', y='platelets', hue='sex', lw=2, ax=ax[2])\nsns.lineplot(data=df, x='age', y='serum_creatinine', hue='sex', lw=2, ax=ax[3])\nsns.lineplot(data=df, x='age', y='serum_sodium', hue='sex', lw=2, ax=ax[4])\nsns.lineplot(data=df, x='age', y='time', hue='sex', lw=2, ax=ax[5])","3203d9f2":"fig, ax = plt.subplots(6,1,figsize=(13,20))\nplt.suptitle('Bivariate Analysis (Hue=Death)', fontsize=20)\nplt.tight_layout(4)\n\nsns.lineplot(data=df, x='age', y='creatinine_phosphokinase', hue='DEATH_EVENT', lw=2, ax=ax[0])\nsns.lineplot(data=df, x='age', y='ejection_fraction', hue='DEATH_EVENT', lw=2, ax=ax[1])\nsns.lineplot(data=df, x='age', y='platelets', hue='DEATH_EVENT', lw=2, ax=ax[2])\nsns.lineplot(data=df, x='age', y='serum_creatinine', hue='DEATH_EVENT', lw=2, ax=ax[3])\nsns.lineplot(data=df, x='age', y='serum_sodium', hue='DEATH_EVENT', lw=2, ax=ax[4])\nsns.lineplot(data=df, x='age', y='time', hue='DEATH_EVENT', lw=2, ax=ax[5])","2b232370":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE","0014a92b":"Y = df['DEATH_EVENT']\nX = df[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']]","3acda6b5":"# SMOTE: Synthetic Minority Over-sampling Technique\nX_smote,Y_smote = SMOTE().fit_sample(X,Y)","970a2cb9":"X_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)","b3bd453f":"print('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of Y_train:', Y_train.shape)\nprint('Shape of Y_test:', Y_test.shape)","f1f8773f":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report \nfrom sklearn.linear_model import LogisticRegression\nimport scikitplot as skplt\n\nlogis = LogisticRegression(random_state=0, solver='lbfgs')\nmodel = logis.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Logistic Regression',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","1467cddb":"from sklearn import svm\n\nsvm = svm.SVC(kernel='linear', C = 1)\nmodel = svm.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: SVM',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","ed11f240":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\nmodel = dt.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","12f3383c":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50)\nmodel = rf.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Random Forest',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","50182d41":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\nmodel = gb.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Gradient Boosting Classifier',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","0549bc84":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\nmodel = lgbm.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: LGBM',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","6180b81a":"# Show importance features\nplt.figure()\nlgb.plot_importance(model)\nplt.title(\"Feature Importances\")\nplt.show()","262682cf":"import xgboost\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nmodel = xgb.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: XgBoost',\n                                    normalize=True,\n                                    cmap='Blues')\n\nprint(classification_report(Y_test, Y_predict))","99705133":"plt.figure()\nxgboost.plot_importance(model)\nplt.title(\"Feature Importances\")\nplt.show()","003026c3":"### XgBoost","feaf2f4f":"## 3. Prediction Algorithms","46feaa2d":"### Decision Tree","61352a4d":"### Support Vector Machine (SVM)","edc09b0a":"### Gradient Boosting Classifier","482e120d":"# 7 Models to Predict Death Causes by Heart Failure","3babfb7c":"### Random Forest","9231559f":"### Logistic Regression","a5d0b964":"### Countplot for Binary Features","f9dd7ada":"Below is the accuracy of each model in descending order:\n* Ramdom Forest : 0.93\n* Gradient Boosting Classifier : 0.90\n* XgBoost : 0.89\n* Decision Tree : 0.88\n* Logistic Regression : 0.87\n* LGBM : 0.87\n* Support Vector Machine (SVM) : 0.76\n","636e8866":"## 4. Results","9a61fabc":"## 1. Data Preparation","1b4ca8ea":"### Bivariate Analysis for Continous Variables","9b7b85a8":"### LGBM","fe6b6a87":"## Index \n\n1. Data Preparation     \n2. Exploratory Data Analysis    \n   * Heatmap to Invertigate Correlation in Data\n   * Countplot for Binary Features\n   * Bivariate Analysis for Continous Variables    \n3. Prediction Algorithms    \n   * Logistic Regression\n   * Support Vector Machine (SVM)\n   * Decision Tree\n   * Random Forest\n   * Gradient Boosting Classifier\n   * LGBM\n   * XgBoost           \n4. Conclusion \n    ","ab3b3a00":"## 2 Exploratory Data Analysis","c112a877":"### Heatmap to Invertigate Correlation in Data"}}