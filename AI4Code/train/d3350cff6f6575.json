{"cell_type":{"0a6fd54f":"code","1a3847a5":"code","60b5994c":"code","59d1af63":"code","b6a4730a":"code","f959c7c1":"code","83ba5606":"code","78493dea":"code","b1505e71":"code","5067a2ed":"code","49df9d4c":"code","41a4951f":"code","3adab4d8":"code","b090aa3c":"code","18dc2f9f":"code","5be661b6":"code","83f017d6":"code","f92b02eb":"code","529e8f39":"code","101aba5f":"code","87fe3111":"code","f96982e0":"markdown","59c565a0":"markdown","58e5af3f":"markdown","266e91e9":"markdown","862c25a9":"markdown","f987ed2a":"markdown","2c36317d":"markdown","029a1466":"markdown","d6c23931":"markdown","b83dd6e9":"markdown"},"source":{"0a6fd54f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1a3847a5":"import numpy as np # linear algebra\nimport pandas as pd \nimport random\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom nltk.tokenize import TweetTokenizer,sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport os\nimport torch\nimport warnings \n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, add\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks,Sequential\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\n\nimport gensim \nfrom gensim.models import Word2Vec","60b5994c":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nsub = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","59d1af63":"identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nfor col in identity_columns + ['target']:\n    train[col] = np.where(train[col] >= 0.5, True, False)\n    ","b6a4730a":"#Split train in train and validate\ntrain_df, valid_df = train_test_split(train, test_size=0.33, stratify=train['target'])\ntest_df=test\n\n#train_df=train_df[:250000]\ntrain_df=train_df\ntrain_df.loc[:,'set_']=\"train\"\nvalid_df.loc[:,'set_']=\"valid\"\ntest_df.loc[:,'set_']=\"test\"\n\n\n#Set_indices=train_df.loc[:,'set_'][:250000]\nSet_indices=train_df.loc[:,'set_']\nSet_indices=Set_indices.append(valid_df.loc[:,'set_'])\nSet_indices=Set_indices.append(test_df.loc[:,'set_'])\n\n\n#y_train = train_df['target'][:250000]\ny_train = train_df['target']\ny_valid = valid_df['target']\n\n#Set_indices_labels=train_df.loc[:,'set_'][:250000]\nSet_indices_labels=train_df.loc[:,'set_']\nSet_indices_labels=Set_indices_labels.append(valid_df.loc[:,'set_'])","f959c7c1":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    from tensorflow import set_random_seed\n    set_random_seed(2)\n\nseed_everything()","83ba5606":"texts=train_df['comment_text']\ntexts=texts.append(valid_df['comment_text'])\ntexts=texts.append(test_df['comment_text'])\n\nprint(texts.shape)\n\nlabels=train_df['target']\nlabels=labels.append(valid_df['target'])\n\nprint(labels.shape)","78493dea":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nmaxlen = 150\nmax_words = 50000\nembedding_size=100\nlr = 1e-3\nlr_d = 0\n\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)","b1505e71":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n","5067a2ed":"x_train = data[Set_indices == \"train\"]\nx_val = data[Set_indices == \"valid\"]\nx_test = data[Set_indices == \"test\"]\n\n\ny_train = labels[Set_indices_labels == \"train\"]\ny_val = labels[Set_indices_labels == \"valid\"]\n\nprint('Shape of train tensor:', x_train.shape)\nprint('Shape of validate tensor:', x_val.shape)\nprint('Shape of test tensor:', x_val.shape)","49df9d4c":"glove_dir = '..\/input\/glove6b100dtxt'\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n\n\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","41a4951f":"embedding_dim = 100\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","3adab4d8":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, SimpleRNN, LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(embedding_dim))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.layers[0].set_weights([embedding_matrix])\n#model.layers[0].trainable = False\n\n#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n\n\n","b090aa3c":"history = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=12000,\n                    validation_data=(x_val, y_val))","18dc2f9f":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.clf()\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","5be661b6":"import pandas\nvect_range=list(range(100))\nrange_epoch = pandas.DataFrame(vect_range)\nval_acc2 = pandas.DataFrame(val_acc)\nbest_acc = pandas.concat([range_epoch, val_acc2], axis=1)\nbest_acc.columns = ['a','b']\nepoch=best_acc.loc[best_acc['b']==max(best_acc['b']),\"a\"]\n\nnew_epoch=int(epoch+1)\nprint(int(new_epoch))","83f017d6":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, SimpleRNN, LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(embedding_dim))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.layers[0].set_weights([embedding_matrix])\n#model.layers[0].trainable = False\n\n#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n\n\nmodel.fit(x_train, y_train,\n                    epochs=new_epoch,\n                    batch_size=12000,\n                    validation_data=(x_val, y_val))","f92b02eb":"# evaluate the model\n#loss, accuracy = model.evaluate(x_train, y_train, verbose=2)\n#print('Accuracy train: %f' % (accuracy*100))\n\n# evaluate the model\n#loss, accuracy = model.evaluate(x_val, y_val, verbose=2)\n#print('Accuracy validate: %f' % (accuracy*100))","529e8f39":"\npred_val = model.predict(x_val, batch_size = 12000, verbose = 0)\n","101aba5f":"MODEL_NAME = 'my_model'\nTOXICITY_COLUMN = 'target'\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nvalid_df[MODEL_NAME] = pred_val\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\nbias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\nprint(bias_metrics_df)\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \nlocal_valid=get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, MODEL_NAME))\nprint(local_valid)\nlocal_valid.tofile('local_valid.csv',sep=',',format='%10.5f')\n#accuracy.tofile('accuracyEmbedding.csv',sep=',',format='%10.5f')\n","87fe3111":"\npred = model.predict(x_test, batch_size = 12000, verbose = 1)\n\n           \nsub = pd.DataFrame({\"id\": test['id'].values})\nsub[\"prediction\"] = pred\n\nsub.to_csv('submission.csv', index=False)\n\n","f96982e0":"**Parsing the GloVe word-embeddings file**","59c565a0":"**Apply the model on the test**","58e5af3f":"**Process to prepare the data:**","266e91e9":"**Test my word embedding on my local test with Jigsaw metric**","862c25a9":"Look at the loss and the gain in accuracy for each epoch","f987ed2a":"**Building the word embedding matrix**","2c36317d":"**Tokenization:**","029a1466":"This tutorial is based on my previous one which was building a word embedding more a LSTM layer.","d6c23931":"**Run final model with the right number of iteration**","b83dd6e9":"**Building the model:**"}}