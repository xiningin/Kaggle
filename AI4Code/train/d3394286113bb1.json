{"cell_type":{"75510001":"code","76ff3310":"code","9a67b805":"code","bedbb581":"code","87770cdf":"code","c3e8ebbb":"code","0420a835":"code","748f8eee":"code","3b14ae80":"code","6701cff6":"code","5674b72d":"code","fcdab522":"code","66d11777":"code","df4fbc53":"code","d9548066":"code","2dcfd49e":"code","7e1ee79c":"code","5b246d4f":"code","ccdbb9e0":"code","c2cdc553":"code","10517938":"code","4962f2bd":"code","c7918751":"code","4e809990":"code","964f4aca":"code","e2ef7846":"code","acc857c4":"code","3778f1c0":"code","6c794b2c":"code","2202cd8b":"code","245d964d":"code","2e433e66":"code","b0c21b15":"code","1c0a49ce":"code","6b6e92e3":"code","19f6f97b":"code","b13d4875":"code","a0508992":"code","80e177db":"code","e613d86b":"code","1cdf19c7":"code","4ec0f66e":"code","001327d9":"code","e5007c96":"code","f0991f13":"code","1d0f6d18":"code","a05323a6":"code","440bc9d3":"code","612dc478":"code","f1968365":"code","5c30e519":"code","817d46cb":"code","30986ab0":"markdown","ef5d097b":"markdown","6c406afe":"markdown","85670768":"markdown","61d05510":"markdown","c16099b7":"markdown","918302ff":"markdown"},"source":{"75510001":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76ff3310":"df = pd.read_csv('\/kaggle\/input\/income\/train.csv')\ndf.shape","9a67b805":"df.head()","bedbb581":"for col in list(df.columns):\n    print(col + ' --> ' + str(df[col].nunique()) + ' , data type '  + str(df[col].dtype))","87770cdf":"cat_columns = [ col for col in list(df.columns) if df[col].dtype =='object']\ncat_columns","c3e8ebbb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,10))\nsns.countplot(data = df, x = cat_columns[0])\n","0420a835":"plt.figure(figsize=(20,10))\nsns.countplot(data = df, x = cat_columns[1])","748f8eee":"plt.figure(figsize=(15,10))\nsns.countplot(data = df, x = cat_columns[2])","3b14ae80":"plt.figure(figsize=(20,10))\nsns.countplot(data = df, x = cat_columns[3])","6701cff6":"plt.figure(figsize=(10,10))\nsns.countplot(data = df, x = cat_columns[4])","5674b72d":"plt.figure(figsize=(10,10))\nsns.countplot(data = df, x = cat_columns[5])","fcdab522":"plt.figure(figsize=(8,8))\nsns.countplot(data = df, x = cat_columns[6])","66d11777":"plt.figure(figsize=(15,10))\n# selecting top 10 countries \ntop_10 = list(df[cat_columns[7]].value_counts().head(10).index)\nsns.countplot(data = df.loc[df[cat_columns[7]].isin(top_10)], x = cat_columns[7])","df4fbc53":"# Lets drop all Country Except US\n\ndf = df[df['native-country']=='United-States']\n\n# drop 'native-country' column from cat_columns list \ndf= df.drop('native-country', axis = 1)\ncat_columns.remove('native-country')","d9548066":"#list of categorical \ncat_columns","2dcfd49e":"num_cols = [ col for col in list(df.columns) if df[col].dtype !='object'  and  col != 'income_>50K']\nnum_cols\n","7e1ee79c":"df_dummy = pd.get_dummies(df[cat_columns])\ndf_dummy.shape","5b246d4f":"print(list(df_dummy.columns))","ccdbb9e0":"#final_dataset\n\nfinal_df = pd.DataFrame()\nfinal_df = pd.concat([df_dummy , df[num_cols] , df['income_>50K'] ] , axis = 1)\nfinal_df.shape","c2cdc553":"list(final_df.columns)","10517938":"final_df.info()","4962f2bd":"final_df.isna().sum().sum()","c7918751":"from sklearn.model_selection import train_test_split \nX = final_df.drop('income_>50K', axis =1)\ny = final_df['income_>50K']\nX_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.3, random_state = 0) \nprint(X_train.shape)\nprint(X_test.shape)","4e809990":"# Training using Decision Tree Classifier \n\nfrom sklearn.tree import DecisionTreeClassifier  \nclassifier1 = DecisionTreeClassifier(criterion='gini')  \nclassifier1.fit(X_train, y_train) \n\n","964f4aca":"\ny_pred_1 = classifier1.predict(X_test)  \nprint(y_pred_1)","e2ef7846":"from sklearn.metrics import accuracy_score #importing accuracy_score function from sklearn.metrics package\nacc_1 = accuracy_score(y_test,y_pred_1)\nprint(\"Accuracy for Gini model {} %\".format(acc_1*100))\n\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test, y_pred_1))","acc857c4":"## [[8760 1306]\n##  [1141 1981]]\n\n\n# 8760 -->  people having less $50 k  - rightly predicted  # True Negative  ( target ==0 )\n# 1981 -->  people having more $50 k  - rightly predicted  # True Positive  ( target ==1 )\n\n# 1306 -->  people having less $50 k  - model say - they have more than $50K # False Positive \n# 1141 -->  people having more $50 k  - model say - they have less than $50K # False Negative\n\naccuracy  = (8760 + 1981)\/(8760+1306+1141+1981)\nprint(accuracy)\n\n# precision  = True Positive \/ ( True Positive + False Positive)\nprecision = 1981\/(1981 +1306)\nprint(precision)\n\n# If I predict 10 people have more than $50K salary - then 60% of that \n#i.e. 6 people will have more than $50K salary\n\n# Recall  = True Positive \/ (True Positive + False Negative)\nrecall = 1981\/(1981+1141)\nprint(recall)\n\n# That model will catch 63% of people who have salary more than $50K \n","3778f1c0":"from sklearn.neighbors import KNeighborsClassifier\n\n\nclassifier2 = KNeighborsClassifier(n_neighbors= 3)  \nclassifier2.fit(X_train, y_train) \n\ny_pred_2 = classifier2.predict(X_test)  \n\nacc_2 = accuracy_score(y_test,y_pred_2)\nprint(\"Accuracy for KNN model {} %\".format(acc_2*100))\nprint(confusion_matrix(y_test, y_pred_2))","6c794b2c":"from sklearn.linear_model import LogisticRegression\nclassifier3 = LogisticRegression(random_state=0)\nclassifier3.fit(X_train, y_train) \n\ny_pred_3 = classifier3.predict(X_test)  \n\nacc_3 = accuracy_score(y_test,y_pred_3)\nprint(\"Accuracy for LR model {} %\".format(acc_3*100))\nprint(confusion_matrix(y_test, y_pred_3))","2202cd8b":"from sklearn.ensemble import GradientBoostingClassifier  \n\nclassifier4 = GradientBoostingClassifier()  \nclassifier4.fit(X_train, y_train) \n\ny_pred_4 = classifier4.predict(X_test)  \n\nacc_4 = accuracy_score(y_test,y_pred_4)\nprint(\"Accuracy for Gradient Boost model {} %\".format(acc_4*100))\nprint(confusion_matrix(y_test, y_pred_4))","245d964d":"from sklearn.ensemble import RandomForestClassifier  \n\nclassifier5 = RandomForestClassifier()  \nclassifier5.fit(X_train, y_train) \n\ny_pred_5 = classifier5.predict(X_test)  \n\nacc_5 = accuracy_score(y_test,y_pred_5)\nprint(\"Accuracy for Random Forest model {} %\".format(acc_5*100))\nprint(confusion_matrix(y_test, y_pred_5))","2e433e66":"# Lets create a manual ensemble \nfrom sklearn.metrics import f1_score\n\nensemble_df = pd.DataFrame()\nensemble_df['Pred1'] = y_pred_1\nensemble_df['Pred2'] = y_pred_2\nensemble_df['Pred3'] = y_pred_3\nensemble_df['Pred4'] = y_pred_4\nensemble_df['Pred5'] = y_pred_5\nensemble_df['Sum'] = ensemble_df.sum(axis = 1)\nensemble_df['Final'] = ensemble_df['Sum'] > 2 \nensemble_df['Final'] = ensemble_df['Final'].astype(int)\n\nprint(ensemble_df.head())\n\nacc = accuracy_score(y_test,ensemble_df['Final'])\nprint(\"Accuracy for Emsemble model {} %\".format(acc*100))\nprint(confusion_matrix(y_test,ensemble_df['Final']))\nprint('f1 Score -->' ,f1_score(y_test,ensemble_df['Final']))","b0c21b15":"# Lets create a manual ensemble with weighted average\n\nensemble_df = pd.DataFrame()\nensemble_df['Pred1'] = y_pred_1\nensemble_df['Pred2'] = y_pred_2\nensemble_df['Pred3'] = y_pred_3\nensemble_df['Pred4'] = y_pred_4\nensemble_df['Pred5'] = y_pred_5\n# DT 10% , KNN 5%  LR 5%  GB 40% RF 40 % \n\nensemble_df['Sum'] = 0.1*ensemble_df['Pred1'] + 0.05*ensemble_df['Pred2'] + \\\n                     0.05*ensemble_df['Pred3'] + 0.4*ensemble_df['Pred4'] + \\\n                     0.4*ensemble_df['Pred5']\nensemble_df['Final'] = ensemble_df['Sum'] >= 0.4\nensemble_df['Final'] = ensemble_df['Final'].astype(int)\n\nprint(ensemble_df.head())\n\nacc = accuracy_score(y_test,ensemble_df['Final'])\nprint(\"Accuracy for Emsemble model {} %\".format(acc*100))\nprint(confusion_matrix(y_test,ensemble_df['Final']))\nprint('f1 Score -->' ,f1_score(y_test,ensemble_df['Final']))","1c0a49ce":"from sklearn.model_selection import GridSearchCV","6b6e92e3":"# Base model \ng1 = DecisionTreeClassifier()\n\nparam_grid = { \n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5,10,15,20,25]\n}\n\n","19f6f97b":"%%time \ngs1 = GridSearchCV(estimator=g1, param_grid=param_grid, cv= 5, verbose = 3)\ngs1.fit(X_train, y_train) ","b13d4875":"gs1.best_params_","a0508992":"best_model_1 = gs1.best_estimator_\n\ny1 = best_model_1.predict(X_test)  \n\nacc = accuracy_score(y_test,y1)\nprint(\"Accuracy for Grid Search DT  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y1))","80e177db":"# Base model \ng2 = RandomForestClassifier()\n\nparam_grid = { \n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [50,100,200],\n    'max_depth': [5,10,15],\n    'max_features': ['auto', 'sqrt', ]\n}\n\n","e613d86b":"%%time \ngs2 = GridSearchCV(estimator=g2, param_grid=param_grid, cv= 5, verbose = 3)\ngs2.fit(X_train, y_train) ","1cdf19c7":"gs2.best_params_","4ec0f66e":"best_model_2 = gs2.best_estimator_\n\ny2 = best_model_2.predict(X_test)  \n\nacc = accuracy_score(y_test,y2)\nprint(\"Accuracy for Grid Search RF  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y2))","001327d9":"# Base model \ng3 = GradientBoostingClassifier()\n\nparam_grid = { \n    'n_estimators': [50,100,200],\n    'max_depth': [5,10,15],\n}\n\n","e5007c96":"%%time \ngs3 = GridSearchCV(estimator=g3, param_grid=param_grid, cv= 5, verbose = 3)\ngs3.fit(X_train, y_train) ","f0991f13":"gs3.best_params_","1d0f6d18":"best_model_3 = gs3.best_estimator_\n\ny3 = best_model_3.predict(X_test)  \n\nacc = accuracy_score(y_test,y3)\nprint(\"Accuracy for Grid Search Gradient Boosting  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y3))","a05323a6":"# # Implementation of SVM \n\n# from sklearn.svm import SVC\n# svc_clf = SVC(C= 1.0 , kernel='poly')\n\n# svc_clf.fit(X_train, y_train)\n# svc_pred = svc_clf.predict(X_test)  \n\n# acc_svc = accuracy_score(y_test,svc_pred)\n# print(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\n# print(confusion_matrix(y_test, svc_pred))","440bc9d3":"# Scaling  \n\nfrom sklearn.preprocessing import StandardScaler\nsc =StandardScaler()\n\nsc.fit(X_train)\nXS_train = sc.transform(X_train)\nXS_test = sc.transform(X_test)\n\n","612dc478":"XS_train","f1968365":"# Implementation of SVM \n\nfrom sklearn.svm import SVC\nsvc_clf = SVC(C= 1.0 , kernel='poly')\n\nsvc_clf.fit(XS_train, y_train)\nsvc_pred = svc_clf.predict(XS_test)  \n\nacc_svc = accuracy_score(y_test,svc_pred)\nprint(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\nprint(confusion_matrix(y_test, svc_pred))","5c30e519":"from sklearn.svm import SVC\nsvc_clf = SVC(C= 1.0 , kernel='rbf')\n\nsvc_clf.fit(XS_train, y_train)\nsvc_pred = svc_clf.predict(XS_test)  \n\nacc_svc = accuracy_score(y_test,svc_pred)\nprint(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\nprint(confusion_matrix(y_test, svc_pred))","817d46cb":"# Data set Info  - check missing values \n# Visulaization  - \n# Transformation  - Dummies \n# Test - Train Split \n# Modeling\n\n# R2 and adj R2  ","30986ab0":"## Hyper Parameter Tuning RandomForest ","ef5d097b":"## Hyper Parameter Tuning of Gradient Boosting","6c406afe":"## Best Model ","85670768":"## Initiate Grid Search ","61d05510":"## Best Parameters","c16099b7":"## base model & parameter grid","918302ff":"## Hyper Parameter Tuning of Decision Tree"}}