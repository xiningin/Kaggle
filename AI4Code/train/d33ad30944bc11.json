{"cell_type":{"ca944f19":"code","6cc6e7a3":"code","07250597":"code","084e3ebb":"code","6d07c0c3":"code","e713636f":"code","f35a974f":"code","1bf82640":"code","d446cb01":"code","257565fd":"code","a2fd2772":"code","3b64c5f2":"code","93a76be0":"code","e86dbb46":"code","8c949034":"code","ba3d70ba":"code","86a3faa2":"code","31a1b922":"code","e499da33":"code","eb811665":"code","f42e2498":"code","f26f212f":"code","7ad5003c":"code","b08f3db9":"code","69621edc":"code","3996b92e":"code","bd64e871":"code","7038ade7":"code","b84f1e9f":"code","0e9af28b":"code","ee969809":"code","16ef1cb5":"code","bc57e83f":"code","fb96b1f9":"code","ad3f2a06":"code","e1d8aa9b":"code","cea77c48":"code","dc9399aa":"code","a6b4f712":"code","1f7f1624":"code","2c471394":"code","21843298":"code","e2de0e05":"code","901bd1a9":"code","5226e638":"code","599e4882":"code","c167266d":"code","9626191a":"code","3a0e0d5b":"code","9cd58921":"code","ea1235b7":"code","3effa9e9":"code","9feb536d":"code","97ed4a89":"code","0d6283f2":"code","1817f17e":"code","cdd29288":"code","5dc3935c":"code","07bdf7c9":"code","86f742b1":"code","7d099a68":"code","5d8dbc58":"code","b0886685":"code","4cce20b9":"code","129456f2":"code","417849da":"code","32627a54":"code","88a7d295":"code","21b3537e":"code","5e148866":"code","94493bc7":"code","3ad8b3bd":"code","27a1de55":"code","88dbae14":"code","7d22c2a2":"code","10364c4a":"code","a24b412e":"markdown","da62d0c5":"markdown","6e0baba3":"markdown","d16c351a":"markdown","16b6d485":"markdown","146d420c":"markdown","9b3c5116":"markdown","48f2a631":"markdown","57fb73f1":"markdown","834bdddf":"markdown","8dbb6671":"markdown","ad96ba46":"markdown","b8551d19":"markdown","b27fb57b":"markdown","a8170f35":"markdown","41d33b34":"markdown","71720336":"markdown","07330453":"markdown","3ab3c18a":"markdown","309a6b61":"markdown","80581090":"markdown","d1290b9d":"markdown","0bbba60c":"markdown","0ea5b76d":"markdown","9d674156":"markdown","b313d817":"markdown","295b6fc8":"markdown","e1d892f0":"markdown","2c3a28fe":"markdown","1bf0cf41":"markdown","f82d3a6d":"markdown","7275c8df":"markdown","e72a8e52":"markdown","1f362358":"markdown","d1318f58":"markdown","c1acfb17":"markdown","4759f411":"markdown","373a2669":"markdown","7e288118":"markdown","203a1c49":"markdown","884e74f3":"markdown","42f2f4d2":"markdown","d41672a5":"markdown","314f4136":"markdown","099e32f1":"markdown","b34bdbce":"markdown","ed9dfcaa":"markdown","7fcc2069":"markdown","60bc81ce":"markdown","2ab51f5c":"markdown","e36c05eb":"markdown","c82d8867":"markdown","293a5678":"markdown","b7365005":"markdown","6389b49b":"markdown"},"source":{"ca944f19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6cc6e7a3":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","07250597":"# If the value in the 'train_test' column is 1 then it is training data otherwise it is test data.\ntrain['train_test'] = 1 \ntest['train_test'] = 0\ntest['SalePrice'] = np.NaN\n\nall_data = pd.concat([train, test]) # This will be our complete dataset (test + train)","084e3ebb":"all_data.info()","6d07c0c3":"# Let's try to plot the null values for a better intution about which column has how many null values.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Let's look at training and test data seperately.\n\nfig, ax = plt.subplots(figsize=(8,6))\nax = sns.heatmap(all_data[all_data['train_test'] == 1].isnull(), cmap=\"YlGnBu\", cbar = False, yticklabels=False).set_title('Training Data')\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(all_data[all_data['train_test'] == 0].isnull(), cmap=\"YlGnBu\", cbar = False, yticklabels=False).set_title('Test Data')\n\nplt.show()","e713636f":"# Dropping the columns that we decided\nall_data.drop(columns = ['PoolQC', 'Alley', 'MiscFeature'], axis = 1 ,inplace = True)","f35a974f":"# FireplaceQu looks confusing, let's see the percentage of null values in that column.\n\n(all_data['FireplaceQu'].isnull().sum() \/ all_data['FireplaceQu'].size) * 100","1bf82640":"# Let's crosscheck the null value situation now\n\nfor i in range(len(train.isnull().sum())):\n    if(train.isnull().sum()[i] > 0):\n        print(train.columns[i] + \" \" + str(train.isnull().sum()[i]))","d446cb01":"plt.figure(figsize = (25, 15))\nmask = np.triu(np.ones_like(all_data.corr(), dtype=bool))\nsns.heatmap(all_data.corr().round(2), cmap='coolwarm', annot=True,cbar_kws={\"shrink\": .5})","257565fd":"# Fence has a lot of null values, let's see what we can do with this.\nprint(\"The unique values in Fence are: \" + str(all_data['Fence'].unique() )) # This gives us all the types of Fence that are there for the houses, We will try to make it like either yes or no. (A house has fence or not.)","a2fd2772":"# Insert 1 in Fence column if there is any kind of fence available, else put 0.\nall_data['Fence'] = np.where(all_data['Fence'].isnull(), 0,1)","3b64c5f2":"# Now we will take care of the GarageCond column\nprint(\"The unique values in GarageCond are: \" + str(all_data['GarageCond'].unique()))\n\n# Let's visualize the distribution of values in this column:\nplt.figure(figsize = (12, 6))\nsns.countplot(x = 'GarageCond', data = all_data, palette=\"Set3\")","93a76be0":"all_data['GarageCond'].fillna('TA', inplace = True)","e86dbb46":"print(\"These are the unique values in the LotFrontage column: \" + str(all_data['LotFrontage'].unique()))","8c949034":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['LotFrontage'], c = 'r')","ba3d70ba":"all_data['LotFrontage'] = np.log(all_data['LotFrontage']) # This line will convert the values in the column to their log.\n\nplt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['LotFrontage'], c = 'r') # This should give us a normally distributed kde plot.","86a3faa2":"# Since we have a normally distributed plot, it's ideal to fill the missing values with the mean.\n\nall_data['LotFrontage'].fillna(all_data['LotFrontage'].mean(), inplace = True)","31a1b922":"all_data['BsmtFullBath'].value_counts()","e499da33":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['BsmtFullBath'], c = 'b')","eb811665":"# It's ideal to fill the null values with the mode here:\n\nall_data['BsmtFullBath'].fillna(0.0, inplace = True)","f42e2498":"all_data['BsmtHalfBath'].fillna(0.0, inplace = True)","f26f212f":"# Let's us start by plotting the categorical values in this column:\nplt.figure(figsize = (12, 6))\nsns.countplot(x = 'FireplaceQu', data = all_data, palette=\"Set3\") \n\n# For the reference, the abbrevations in this column stands for:\n#        Ex\tExcellent - Exceptional Masonry Fireplace\n#        Gd\tGood - Masonry Fireplace in main level\n#        TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n#        Fa\tFair - Prefabricated Fireplace in basement\n#        Po\tPoor - Ben Franklin Stove\n#        NA\tNo Fireplace","7ad5003c":"all_data['FireplaceQu'].unique()\nall_data.drop(columns = 'FireplaceQu', inplace = True)","b08f3db9":"# Let's try to plot a kde for this column:\n\nplt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['GarageYrBlt'], c = 'm')","69621edc":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data[all_data['train_test'] == 1]['GarageYrBlt'], c = 'm')","3996b92e":"all_data['GarageYrBlt'].fillna(all_data[all_data['train_test'] == 1]['GarageYrBlt'].mean(), inplace = True)","bd64e871":"print('Number of null values in TotalBsmtSF: ' + str(all_data['TotalBsmtSF'].isnull().sum()))\n\nplt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['TotalBsmtSF'], c = 'b')","7038ade7":"all_data['TotalBsmtSF'].fillna(all_data['TotalBsmtSF'].median(), inplace = True)","b84f1e9f":"all_data['BsmtFinSF1'].isnull().sum() # shows that there is only 1 null value.","0e9af28b":"# Let's see the unique values in this column:\nall_data['BsmtFinSF1'].unique()","ee969809":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['BsmtFinSF1'], c = 'y')","16ef1cb5":"all_data['BsmtFinSF1'].fillna(all_data['BsmtFinSF1'].median(), inplace = True)","bc57e83f":"# Let's look at the distribution of this chart\nplt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['BsmtUnfSF'], c = 'y')","fb96b1f9":"all_data['BsmtUnfSF'] = np.log(all_data['BsmtUnfSF'])","ad3f2a06":"all_data['BsmtUnfSF'].fillna(all_data['BsmtUnfSF'].mean(), inplace = True)","e1d8aa9b":"all_data['BsmtFinSF2'].isnull().sum() # shows that there is only 1 null value.","cea77c48":"# I tried to plot a kde plot using seaborn library, It was giving me the following error: \"seaborn: Selected KDE bandwidth is 0. Cannot estimate density\"\n# I looked it up and got to know that it is a knows issue and is happening because seaborn is not able to calculate the bandwidth.\n# Instead of a KDE I plotted a histogram, which can give us a similar insight.\nplt.figure(figsize = (12, 6))\nall_data['BsmtFinSF2'].hist()","dc9399aa":"all_data['BsmtFinSF2'].value_counts() # We can see that the most of the cells have 0 as value, it is ideal to fill the missing value with 0.","a6b4f712":"all_data['BsmtFinSF2'].fillna(0, inplace = True)","1f7f1624":"all_data['GarageCars'].isnull().sum()","2c471394":"all_data['GarageCars'].unique()\n\n# Note that the GarageCars represents the number of cars the Garage can hold. This should not be a decimal value, so we can't really use the mean.\n# Let's fill it up with the median.","21843298":"all_data['GarageCars'].fillna(all_data['GarageCars'].median(), inplace = True)","e2de0e05":"all_data['GarageArea'].isnull().sum()","901bd1a9":"all_data['GarageArea'].unique() #Shows a lot of values, which might look ambiguous... Let's try to plot this.","5226e638":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['GarageArea'], c = 'b')","599e4882":"all_data['GarageArea'].fillna(all_data['GarageArea'].mean(), inplace = True)","c167266d":"all_data.select_dtypes(exclude = ['float64', 'int64']).columns","9626191a":"all_data['MasVnrArea'].isnull().sum()","3a0e0d5b":"all_data['MasVnrArea'].unique()","9cd58921":"plt.figure(figsize = (12, 6))\nsns.kdeplot(all_data['MasVnrArea'], c = 'g')","ea1235b7":"all_data['MasVnrArea'].fillna(0, inplace = True)","3effa9e9":"# Let's have a look at columns having null values at this point.\n\nfor i in range(len(all_data.isnull().sum())):\n    if(all_data.isnull().sum()[i] > 0):\n        print(all_data.columns[i] + \"\\t - \\t \" + str(all_data.isnull().sum()[i]))","9feb536d":"all_data.GarageCond.fillna('NA', inplace=True)\nall_data.GarageQual.fillna('NA', inplace=True)       \nall_data.GarageType.fillna('NA', inplace=True)          \nall_data.BsmtCond.fillna('NA', inplace=True)        \nall_data.BsmtQual.fillna('NA', inplace=True)        \nall_data.Functional.fillna(all_data.Functional.mode()[0], inplace=True)  #Replace with mode \nall_data.SaleType.fillna(all_data.SaleType.mode()[0], inplace=True) #Replace with mode             \nall_data.KitchenQual.fillna(all_data.KitchenQual.mode()[0], inplace=True) #Replace with mode        \nall_data.Electrical.fillna(all_data.Electrical.mode()[0], inplace=True) #Replace with mode \nall_data.MSZoning.fillna(all_data.MSZoning.mode()[0], inplace=True) #Replace with mode \n","97ed4a89":"train = all_data[all_data['train_test'] == 1]\ntest = all_data[all_data['train_test'] == 0]\n\n# We will need to remove the column 'train_test' from both train and test. And 'SalePrice' from test\n\ntrain.drop(columns = 'train_test', axis = 1, inplace = True)\ntest.drop(columns = ['train_test', 'SalePrice'], axis = 1, inplace = True)","0d6283f2":"print(\"Shape of train set: \" + str(train.shape))\nprint(\"Shape of test set: \" + str(test.shape))","1817f17e":"# By convention it is referred as 'y'\n\ny = train['SalePrice']","cdd29288":"# Check if it is normally distributed\n\nplt.figure(figsize = (12, 6))\nsns.kdeplot(y, c = 'c')","5dc3935c":"y = np.log(y)","07bdf7c9":"plt.figure(figsize = (12, 6))\nsns.kdeplot(y, c = 'c')","86f742b1":"train.columns # This will give you all the columns, copy all and manually remove the one's which you don't want.","7d099a68":"features = ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n       'RoofMatl', 'MasVnrArea',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n       'Functional', 'Fireplaces', 'GarageType', 'GarageYrBlt',\n       'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive',\n       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'Fence', 'MiscVal', 'MoSold', 'YrSold',\n       'SaleType', 'SaleCondition']\n","5d8dbc58":"train = train[features]\ntest = test[features]","b0886685":"train['train_test'] = 1\ntest['train_test'] = 0\n\nall_data = pd.concat([train, test])","4cce20b9":"# First, let us find out the categorical columns in our dataset:\n\ntrain.select_dtypes(exclude = ['float64', 'int64']).columns","129456f2":"# Copy the categorical columns so that we can pass them in the pd.get_dummies (which gives one hot encoded df)\n\ncat_columns = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageType',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nall_data = pd.get_dummies(all_data, columns = cat_columns)\nall_data.head()","417849da":"X = all_data[all_data['train_test'] == 1]\ntest_X = all_data[all_data['train_test'] == 0]","32627a54":"X.shape # This is what we will use to train our model.","88a7d295":"test_X.shape # This is what it will be tested on.","21b3537e":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)","5e148866":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nxgb.fit(train_X, train_y, verbose = False)","94493bc7":"prediction = xgb.predict(val_X)\nprediction = np.exp(prediction)","3ad8b3bd":"# We will need to find the mean absolute error of our predicted values. This will tell us how bad we did, more means bad and less means good.\n\nfrom sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(prediction, np.exp(val_y))\nprint(mae)","27a1de55":"xgb = XGBRegressor(n_estimators = 5000, learning_rate = 0.05)\nxgb.fit(train_X, train_y, early_stopping_rounds = 50, eval_set = [(val_X, val_y)], verbose = False)","88dbae14":"prediction = xgb.predict(val_X)\nprediction = np.exp(prediction)\nmae = mean_absolute_error(prediction, np.exp(val_y))\nprint(mae)","7d22c2a2":"final_prediction = xgb.predict(test_X)\nfinal_prediction = np.exp(final_prediction)\n\nprint(\"This is our final prediction: \" + str(final_prediction))","10364c4a":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': final_prediction})\noutput.to_csv('submission.csv', index=False)","a24b412e":"#### BsmtFullBath","da62d0c5":"##### Has only one missing value.","6e0baba3":"### Now we will take all categorical columns with null values one by one and see what can we do with them:","d16c351a":"### Selecting features for our model training.\n#### Now our hard work will pay off, we can use all the features that have no null values. Remember that we just filtered out the null values from the features that we thought can be important.","16b6d485":"##### Plot shows that it is nicely distributed, we can go for the mean.","146d420c":"##### To have consistency in the column count for both train and test sets, its better to concat them and then do the encoding and split them again.","9b3c5116":"#### Fence\n","48f2a631":"# Create a validation set","57fb73f1":"#### BsmtFinSF2","834bdddf":"##### Now we will keep the test set aside and will not touch it until we need to predict.","8dbb6671":"## Let's have a look at the target column","ad96ba46":"##### The density against the column names shows how many null values are present in them.\n##### We can see that there are a lot of null values in the following columns in both train and test data: 'PoolQC', 'Alley', 'MiscFeature'","b8551d19":"##### The plot is heavy tailed, we can not use the mean.","b27fb57b":"# Data Cleaning","a8170f35":"##### The distribution shows that taking the mean of log can be filled in for null values.","41d33b34":"##### Note that SalePrice is null because the test data is also added in this 'all_data', and all other columns that have null values will be dealt with.","71720336":"#### GarageArea","07330453":"#### GarageCond","3ab3c18a":"### Now we will take columns one by one, see how important they are and will deal with them.","309a6b61":"# Create Model","80581090":"##### The plot tells us that the highest value is too far away from the mean value. So it's not a good idea to fill the null value with the mean, instead we will fill it up with the median.","d1290b9d":"##### The plot is not nicely distributed, we will choose the median to fill the null value.","0bbba60c":"##### Has only one missing value.","0ea5b76d":"##### Much better, now we can go ahead with this target column.","9d674156":"\n### Let's have a look at the correlation, it will help us understand which column are important and which are not.","b313d817":"##### Looks like majority of rows have 'TA' as value in GarageCond column. It's ideal to fill the null cells with this value.","295b6fc8":"### First, we will concat both the datasets so that whatever transformation we make, we don't have to do it two times for each.\n### We will be adding an extra column for identifying which one is the test data and which is train data. In the end, right before feeding the data to our model we will split the dataset again into train and test.\n","e1d892f0":"##### We can see that there are some columns which have plenty of null values. It's ideal to remove such columns when the missing data is more than 50% of the total size.","2c3a28fe":"##### The values of TA and Gd are both pretty high, we can't fill the null values randomly. Let's try to find the correlation of this column and see if it's really important to keep this one.\n##### Filling the values randomly can drastically effect the results.\n##### We can make one assumption that the cells that have nan actually mean NA (No fireplace), since it is there is the column description but not present in the actual data. We will go with dropping this column as it's quite unpredictible and not clear.","1bf0cf41":"##### Let's do the same for BsmtHalfBath also:","f82d3a6d":"### This submission gave me top 10% (Score of 15362.06551), I tried to present it in a very simple manner and tried to be clear about what's going on.\n### Feel free to post a comment if you have any suggestion or query, I would love to have a chat.\n### Do hit the upvote if you liked this notebook. :)","7275c8df":"##### We can see that the plot above is more evenly distributed. We can take the mean of this data and fill the null values.","e72a8e52":"#### LotFrontage","1f362358":"# One Hot Encoding for the Categorical Values (For both train and test sets)","d1318f58":"##### The plot shows that the maximum amount of cells have 0, so we can fill the null values with 0.","c1acfb17":"#### FireplaceQu","4759f411":"#### BsmtUnfSF","373a2669":"#### BsmtFinSF1","7e288118":"#### GarageCars","203a1c49":"# A very clear demo (with EDA and steps) for under top 10% \ud83d\udd25\ud83d\udd25\ud83d\udd25\n\n### The purpose of this notebook is to understand the lifecycle of a ML project in a clear & easy way.\n### For each step, I will be writing down why it's done in the beginning and what's the conclusion we had after the execution.\n","884e74f3":"# Create CSV for submission","42f2f4d2":"#### MasVnrArea","d41672a5":"## Dealing with the Categorical Data:","314f4136":"##### Note that the distribution is not bell shaped, so it's a good idea to take log of this column.","099e32f1":"We can conclude at this point that some imporatant features are: (With their respective correlations)\n1. LotFrontAge 0.35\n1. OverallQual 0.79\n1. YearBuilt 0.52\n1. YearRemodAdd 0.51\n1. MasVnrArea 0.48\n1. BsmtFrinSF1 0.39\n1. TotalBsmtSF 0.61\n1. 1stFlrSF 0.61\n1. GRLivArea 0.71\n1. Fullbath 0.56\n1. TotRmsAbvGrd 0.53\n1. Fieplaces 0.47\n1. GarageYrBlt 0.49\n1. GarageCars 0.64\n1. GarageArea 0.62*\n\n#### Note that these are just the numerical columns, we are dealing with the numerical columns first (and some categorical if we can by using simple stats). Then later we will deal with the categorical columns.","b34bdbce":"##### Let's find out the Kernel density of this column to see the distribution of values.","ed9dfcaa":"#### I will fill the null values with the text \"NA\" and see how it goes, if the score is not good enough I will come back here and manually fill the null values for each feature.","7fcc2069":"### At this point, we have dealt with all the features that we thought are important by looking at the correlations table.","60bc81ce":"##### Looks like the target column is not nicely distributed, let's take a log and see.","2ab51f5c":"##### Let's try to make it a little better by tuning some hyperparameters. I gave those values randomly for simplicity's sake.","e36c05eb":"##### Less than 50% so it's okay, we will not drop this column.","c82d8867":"##### By analysing for a while, I realised that there's year 2207 in the test data which was causing heavy tailed distribution for our all_data.\n##### Let's try to plot the same kde just for the Train data:","293a5678":"## Retrieve the train and test datasets","b7365005":"#### TotalBsmtSF","6389b49b":"#### GarageYrBlt \n##### Positive correlation with SalePrice (+0.49)"}}