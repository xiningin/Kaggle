{"cell_type":{"2fa85b09":"code","713a9f4f":"code","b7f74b23":"code","37b4e6a4":"code","4556077a":"code","032fa5b7":"code","dc0dda5a":"code","a8175f21":"code","c9f00dbc":"code","f40b3ba8":"code","edc2f183":"code","d18fab4c":"code","71d2661d":"code","952b87b3":"code","aac2d8c2":"code","dbbfa083":"code","1551ce92":"code","a2df34bb":"markdown","4f21784d":"markdown","cdfa9dbb":"markdown","7274f78c":"markdown","770a467e":"markdown","d200f645":"markdown","6aa54270":"markdown","865f05cb":"markdown","64fd5f39":"markdown","284209b8":"markdown","1ddee725":"markdown","496e84db":"markdown"},"source":{"2fa85b09":"# Import Modules\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *","713a9f4f":"# Show versions of Tensorflow\nprint('TF version',tf.__version__)","b7f74b23":"# Set other Seeds\nSEED = 4262\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","37b4e6a4":"# Set strategy for tpu\nUSE_TPU = False\nif USE_TPU:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.OneDeviceStrategy(device = \"\/gpu:0\")","4556077a":"# Constants\nMAX_LEN = 128\nFOLDS = 5\nEPOCHS = 3\nVERBOSE = 1\nROBERTA_BASE_PATH = '..\/input\/tf-roberta\/'\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nprint('BatchSize: {}'.format(BATCH_SIZE))\n\nLR = 1e-5 * strategy.num_replicas_in_sync\nprint('LearningRate: {}'.format(LR))\n\n# Set the following to True after training to make a submission\nINFERENCE = True","032fa5b7":"# Read Train Data (I read in a multiple of batch size 256 when training on TPU)\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv', nrows = 27200).fillna('')\n#train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\n\n# Summary\ntrain.head()","dc0dda5a":"# Read Test Data\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\n# Summary\ntest.head()","a8175f21":"# Tokenizer\ntokenizer = RobertaTokenizer(vocab_file = ROBERTA_BASE_PATH + 'vocab-roberta-base.json',\n                             merges_file = ROBERTA_BASE_PATH + 'merges-roberta-base.txt',\n                             add_prefix_space = True,\n                             do_lower_case = False)","c9f00dbc":"# Pre Process Training Data\nct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN), dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN), dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN), dtype='int32')\n\nfor k in range(ct):\n    # Process Text\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    \n    # Skip rows where Text1 is empty (RobertaTokenizer crashes: https:\/\/github.com\/huggingface\/transformers\/issues\/3809)\n    if text1 != '':\n        # Encode Full input sample\n        input_encoded = tokenizer.encode_plus(text1, train.loc[k,'sentiment'], add_special_tokens = True, max_length = MAX_LEN, padding = True)\n        input_ids_sample = input_encoded[\"input_ids\"]\n        attention_mask_sample = input_encoded[\"attention_mask\"]\n\n        # Attention Mask\n        attention_mask[k,:len(attention_mask_sample)] = attention_mask_sample\n\n        # Input Ids\n        input_ids[k,:len(input_ids_sample)] = input_ids_sample\n\n        # Find overlap between Full Text and Selected Text\n        idx = text1.find(text2)\n        chars = np.zeros((len(text1)))\n        chars[idx:idx + len(text2)] = 1\n        k_ids = tokenizer.encode(text1, add_special_tokens = False) \n\n        # ID_OFFSETS\n        offsets = [] \n        idx = 0\n        for t in k_ids:\n            w = tokenizer.decode([t])\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n\n        # Get Start and End Tokens\n        toks = []\n        for i,(a,b) in enumerate(offsets):\n            sm = np.sum(chars[a:b])\n            if sm>0: \n                toks.append(i) \n        if len(toks) > 0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,toks[-1]+1] = 1","f40b3ba8":"# Pre Process Test Data\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(ct):\n    # Process Text\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    \n    # Skip rows where Text1 is empty (RobertaTokenizer crashes: https:\/\/github.com\/huggingface\/transformers\/issues\/3809)\n    if text1 != '':\n        # Encode Full input sample\n        input_encoded = tokenizer.encode_plus(text1, test.loc[k, 'sentiment'], add_special_tokens = True, max_length = MAX_LEN, padding = True)\n        input_ids_sample = input_encoded[\"input_ids\"]\n        attention_mask_sample = input_encoded[\"attention_mask\"]\n\n        # Attention Mask\n        attention_mask_t[k,:len(attention_mask_sample)] = attention_mask_sample\n\n        # Input Ids\n        input_ids_t[k,:len(input_ids_sample)] = input_ids_sample","edc2f183":"# Metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    \n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    \n    c = a.intersection(b)\n    \n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d18fab4c":"# Config\nconfig = RobertaConfig.from_pretrained(ROBERTA_BASE_PATH+'config-roberta-base.json')\nconfig.attention_probs_dropout_prob = 0.15\nprint(config)","71d2661d":"def build_model():\n    # Create Model\n    with strategy.scope():      \n        ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n        att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n        tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n        roberta_model = TFRobertaForQuestionAnswering.from_pretrained(ROBERTA_BASE_PATH + 'pretrained-roberta-base.h5', config = config)\n        x = roberta_model(ids, attention_mask = att, token_type_ids = tok)\n\n        x1 = tf.keras.layers.Activation('softmax')(x[0])\n        x2 = tf.keras.layers.Activation('softmax')(x[1])\n\n        model = tf.keras.models.Model(inputs = [ids, att, tok], outputs=[x1, x2])\n        optimizer = tf.keras.optimizers.Adam(learning_rate = LR)\n        loss = tf.keras.losses.CategoricalCrossentropy(from_logits = False, label_smoothing = 0.1)\n\n        model.compile(loss = loss, optimizer = optimizer)\n\n        return model","952b87b3":"# Placeholders\njac, jac1 = [], []\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    # Clear session and create Model\n    K.clear_session()\n    model = build_model()\n        \n    # Callbacks\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint('roberta-%i.h5'%(fold), monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                                          save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n    \n    if not INFERENCE:\n        # Train Model \n        model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n                  epochs = EPOCHS, \n                  batch_size = BATCH_SIZE, \n                  verbose = VERBOSE, \n                  callbacks = [model_checkpoint],\n                  validation_data = ([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], [start_tokens[idxV,], end_tokens[idxV,]]),\n                  shuffle = True)\n\n        print('Loading model...')\n        model.load_weights(f'roberta-{fold}.h5')        \n    else:\n        print('Loading model...')\n        model.load_weights(f'\/kaggle\/input\/tensorflow-roberta-qa-model\/roberta-{fold}.h5')\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose = VERBOSE)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose = VERBOSE)\n    preds_start += preds[0] \/ skf.n_splits\n    preds_end += preds[1] \/ skf.n_splits\n    \n    # Display Fold Jaccard\n    all = []\n    all1 = []\n\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        \n        # Encode Text\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1, add_special_tokens = False)\n        \n        if train.loc[k,'sentiment'] == 'neutral':\n            st = \" \".join(train.loc[k,'text'].split())\n            st1 = \" \".join(train.loc[k,'text'].split())\n        else:\n            # Check if start comes after end\n            if a > b:\n                st = text1\n                st1 = text1\n\n                # Sort according to max probabilities and get the indices            \n                start_sort = np.argsort(oof_start[k,])[::-1] \n                end_sort = np.argsort(oof_end[k,])[::-1]\n\n                a1 = start_sort[1]\n                b1 = end_sort[1]\n                a2 = start_sort[2]\n                b2 = end_sort[2]\n\n                # Try the next 2 positions..if one of them has the correct order\n                if a1 < b1:\n                    st1 = tokenizer.decode(enc[a1-1:b1])\n                elif a2 < b2:\n                    st1 = tokenizer.decode(enc[a2-1:b2])   \n            else:\n                st = tokenizer.decode(enc[a-1:b])\n                st1 = tokenizer.decode(enc[a-1:b])\n\n        # Store Results\n        all.append(jaccard(st, train.loc[k,'selected_text']))\n        all1.append(jaccard(st1, train.loc[k,'selected_text']))\n\n    jac.append(np.mean(all))\n    jac1.append(np.mean(all1))\n    \n    print('>>>> FOLD %i Jaccard ='%(fold+1), np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1), np.mean(all1))\n    \n    print()","aac2d8c2":"print(f'=== OVERALL 5 Fold CV Jaccard                  = {np.mean(jac)}')\nprint(f'=== OVERALL 5 Fold CV Jaccard - Post Processed = {np.mean(jac1)}')","dbbfa083":"# Generate final results for submission\nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    \n    # Encode Text\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1, add_special_tokens = False)\n    \n    if test.loc[k, 'sentiment'] == 'neutral':\n        st = \" \".join(test.loc[k,'text'].split())\n    else:\n        # Check if start comes after end\n        if a > b:  \n            st = text1\n\n            # Sort according to max probabilities and get the indices            \n            start_sort = np.argsort(preds_start[k,])[::-1] \n            end_sort = np.argsort(preds_end[k,])[::-1]\n\n            a1 = start_sort[1]\n            b1 = end_sort[1]\n            a2 = start_sort[2]\n            b2 = end_sort[2]\n\n            # Try the next 2 positions..if one of them has the correct order\n            if a1 < b1:\n                st = tokenizer.decode(enc[a1-1:b1])\n            elif a2 < b2:\n                st = tokenizer.decode(enc[a2-1:b2])  \n        else:\n            st = tokenizer.decode(enc[a-1:b])\n\n    all.append(st)","1551ce92":"# Store Submission and show some results\ntest['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","a2df34bb":"After training the model I apply some simple post processing. See for yourself if you want to use it. In the situation where the start position is after the end position I try to find if the 2nd or 3rd options provides a solution for that situation.\n\nWith the similarity between 'text' and 'selected_text' when the sentiment is neutral I just use the complete 'text'. Risky because we don't know the data for the private board..but worth a try ;-)","4f21784d":"For the tokenizer I'am using the default Huggingface Roberta Tokenizer and loading the vocab and merges file.","cdfa9dbb":"For processing of the training data I mostly use the proces as was shown in one of the earlier kernels from Abhishek Thakur. Main difference is I'am using the default Roberta Tokenizer. Also I just use the tokenizers functionality to generate the complete input sample.","7274f78c":"When building the model I use the recently added TFRobertaForQuestionAnswering model as a basis.","770a467e":"When setting the config I increase the dropout for the attention layers a bit.","d200f645":"# Test Data","6aa54270":"# Training Data","865f05cb":"## Models","64fd5f39":"So this is yet another RoBERTa tweet sentiment extraction notebook. However with some different things and approaches used so I hope you like it. And if so then please upvote it ;-)\n\nMy original work in this competition was inspired by the kernels of Chris Deotte. \n\nThis kernel is based somewhat on that work yet offers a couple of different\/new attempts:\n* It is using the TFRobertaForQuestionAnswering model that was recently released by Huggingface. And with custom head layers added.\n* It is using the default RobertaTokenizer\n* It is using some different preprocessing where I use more of the default tokenizer capabilities.\n* Label Smoothing.\n* Added some simple post-processing.\n\nUpdate: In the last version I've added support for running on TPU. Note that this only works for Training. Inference and submission should be done on GPU as TPU is not allowed in this competition.","284209b8":"# Kaggle Submission","1ddee725":"## Train Model","496e84db":"## Metric"}}