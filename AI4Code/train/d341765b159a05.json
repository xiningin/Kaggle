{"cell_type":{"a25f77a8":"code","673a8209":"code","aec422ad":"code","0d084c21":"code","4ab4d908":"code","4c5acec2":"code","231c89de":"code","3a70f08e":"code","018f8637":"code","ad068122":"code","f5589aa5":"code","72737917":"code","4e64ddbb":"code","cbbd2fcf":"code","86df8172":"code","502c0697":"code","8ceee292":"code","c84625af":"code","8683cbee":"code","c2243d1e":"code","74adf990":"code","e87d265d":"code","c25d811a":"code","0fb6590e":"code","7b9b722e":"code","50ba81f0":"code","e02e4a56":"code","740220b5":"code","09d4400b":"code","10195d95":"code","0f3d385e":"markdown","587fab24":"markdown","cea0cafd":"markdown","3246145d":"markdown","4c91fff0":"markdown","9b0c8777":"markdown","3f7ee0de":"markdown","b00f9309":"markdown","d7d44944":"markdown","3bb07419":"markdown","e19508b2":"markdown","04ca7602":"markdown","758574d2":"markdown","1bdacf3d":"markdown","e6f68c63":"markdown","cc9325c1":"markdown","19fc216f":"markdown","b2bc6eb6":"markdown","b156b26a":"markdown","53e2d2ff":"markdown","53b52cab":"markdown","d04fff93":"markdown","921d5b3b":"markdown","8e52e2ab":"markdown","fd6e465c":"markdown","c93d03ab":"markdown"},"source":{"a25f77a8":"# data processing, linear algebra\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt","673a8209":"# imprime os arquivos\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aec422ad":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Treino - linhas: %d colunas: %d' % df_train.shape)\nprint('Teste  - linhas: %d colunas: %d' % df_test.shape)","0d084c21":"df_train.sample(3)","4ab4d908":"df_train.info()","4c5acec2":"# natural language toolkit\nimport gensim\nimport nltk\n\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nstemmer = SnowballStemmer('english')\nnltk.download('wordnet')","231c89de":"# Lemmatization process\ndef lemmatize(text):\n    return WordNetLemmatizer().lemmatize(text, pos='v')\n\n\n# Stemming process\ndef stemming(text):\n    return stemmer.stem(text)\n\n\n# Remove special characteres\ndef remove_special_char(text):\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n\n\n# Tokenization process\ndef preprocess(text):\n    result = []\n    new_text = remove_special_char(text)\n    for token in gensim.utils.simple_preprocess(new_text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            LemmatizedToken = lemmatize(token)\n            result.append(stemming(LemmatizedToken))\n    return result","3a70f08e":"random_index = 956\nrandom_text = df_train.iloc[random_index]['text']\nprint(random_text)","018f8637":"remove_special_char(random_text)","ad068122":"print(preprocess(random_text))","f5589aa5":"df_train['text.process'] = df_train['text'].map(preprocess)\ndf_test['text.process']  = df_test['text'].map(preprocess)\ndisplay(df_train['text.process'])","72737917":"# dense matrix\nfrom gensim.matutils import corpus2dense","4e64ddbb":"dictionary = gensim.corpora.Dictionary(df_train['text.process'])","cbbd2fcf":"keep_terms = 3_000\ndictionary.filter_extremes(no_below=10, no_above=0.85, keep_n=keep_terms)","86df8172":"bow_corpus      = [dictionary.doc2bow(doc) for doc in df_train['text.process'].tolist()]\nbow_corpus_test = [dictionary.doc2bow(doc) for doc in df_test['text.process'].tolist()]","502c0697":"display(bow_corpus[random_index])","8ceee292":"id_test = df_test['id'].tolist()\nX_dense      = corpus2dense(bow_corpus, num_terms=keep_terms, num_docs=df_train.shape[0]).T\nX_dense_test = corpus2dense(bow_corpus_test, num_terms=keep_terms, num_docs=df_test.shape[0]).T","c84625af":"lin, col = X_dense.shape\nprint('linhas  -', lin)\nprint('colunas -', col)","8683cbee":"# training and test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","c2243d1e":"# return X_train, X_test, y_train, y_test from a dense_matrix\ndef get_x_y(dense_matrix, target):\n    X = dense_matrix\n    y = target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=26)\n    return X_train, X_test, y_train, y_test","74adf990":"y_dense = df_train['target'].tolist()\nX_train, X_test, y_train, y_test = get_x_y(X_dense, y_dense)","e87d265d":"# machine learning algorithm\nfrom sklearn.linear_model import LogisticRegression","c25d811a":"# trainning and test\ndef train_test(class_model, X_train, X_test, y_train, y_test):\n    model = class_model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return model, accuracy_score(y_test, y_pred)\n\n# print everything\ndef print_train_test(class_model, X_train, X_test, y_train, y_test):\n    model, acc = train_test(class_model, X_train, X_test, y_train, y_test)\n    display(model)\n    display('score: %.4f' % acc)\n    return model","0fb6590e":"%%time\n# train, test model\nclass_model = LogisticRegression(random_state=26, max_iter=100)\nmodel = print_train_test(class_model, X_train, X_test, y_train, y_test)","7b9b722e":"model = class_model.fit(X_dense, y_dense)","50ba81f0":"def gerenate_submission(model, X, id_test, file_name):\n    target  = model.predict(X)\n    submission = {'id':id_test, 'target':target}\n    df_submission = pd.DataFrame(submission)\n    df_submission.to_csv(file_name, index=False)","e02e4a56":"gerenate_submission(model, X_dense_test, id_test, 'submission.csv')","740220b5":"# visualization\nfrom matplotlib import pyplot","09d4400b":"# get importance\nimportance = model.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %s, Score: %.5f' % (dictionary.get(i),v))\n    if i >= 10:\n        break","10195d95":"# plot feature importance\ninit_feature = 0\nmax_feature = 500\n\nfor i in range(3):\n    pyplot.bar([x for x in range(init_feature, max_feature)], importance[init_feature:max_feature])\n    pyplot.title('Importance from %d to %d' % (init_feature, max_feature))\n    pyplot.show()\n    init_feature += max_feature\n    max_feature += max_feature","0f3d385e":"Executa o pr\u00e9-processamento em toda base de treino.","587fab24":"Seleciona um texto aleat\u00f3rio.","cea0cafd":"Constro\u00ed tuplas de _(palavra, contagem)_ para cada tweet.","3246145d":"Treinando o modelo, utilizando todo o dataset de treinamento.","4c91fff0":"# NLP - Detec\u00e7\u00e3o de Tweets de Desastres\n\nEste notebook realiza uma classifica\u00e7\u00e3o de tweets (texto livre) utilizando Processamento de Linguagem Natural (Natural Language Processing - NLP) na competi\u00e7\u00e3o [Natural Language Processing with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/). Esta competi\u00e7\u00e3o tem como prop\u00f3sito identificar quais mensagens do Twitter, chamadas de tweets, s\u00e3o claramente avisos de um \"desastre\" - tento em vista que pessoas est\u00e3o utilizando o Twitter para anunciar uma emerg\u00eancia que est\u00e3o observando em tempo real. Sendo assim, ag\u00eancias buscam monitorar tweets a fim de identificar esses desastres e ativamente atender o chamado, antes mesmo de receber uma liga\u00e7\u00e3o direta.\n\n> **Resumidamente**. Tarefa de classifica\u00e7\u00e3o bin\u00e1ria de texto, _i.e.,_ \u00e9 um desastre ou n\u00e3o \u00e9.   \n> Conte\u00fado voltado para n\u00edvel intermedi\u00e1rio da \u00e1rea de Aprendizado de M\u00e1quina e Ci\u00eancia de Dados!\n\n<a id=\"top\"><\/a>\n## Conte\u00fado\n\n> **Nota**. Alguns c\u00f3digos e outputs ser\u00e3o ocultados, a fim de facilitar a visualiza\u00e7\u00e3o dos dados e dar destaque para o conte\u00fado mais importante.\n\nO notebook est\u00e1 organizado como segue:\n\n- [Dados](#data) - Tratamendo dos dados.\n  - Pr\u00e9-processamento do texto\n- [Vetoriza\u00e7\u00e3o](#feature)\n  - Utilizando BOW \n- [Classifica\u00e7\u00e3o](#classification) dos tweets.\n  - [Regress\u00e3o Log\u00edstica](#reg) - Classifica\u00e7\u00e3o com Regress\u00e3o Log\u00edstica.\n  - [Explainability](#exp) - Simples visualiza\u00e7\u00e3o da import\u00e2ncia das palavras.","9b0c8777":"## Bag of Words (BOW)\n\nBag-of-words (BOW) \u00e9 uma representa\u00e7\u00e3o simplificada usada no Processamento de Linguagem Natural (NLP). Consiste em considerar cada palavra como um atributo do documento, neste caso um tweet. Logo, cada tweet se torna um vetor com a contagem de vezes que a respectiva palavra (atributo do vetor) aparece no documento.\n\n> **Nota**. Os vetores ser\u00e3o gigantestos, pois, usualmente, possuem muitas palavras unicas nos tweets.","3f7ee0de":"Verificando um dicion\u00e1rio BOW de um tweet aleat\u00f3rio.","b00f9309":"## Conjunto de Treinamento e Valida\u00e7\u00e3o","d7d44944":"-----\n<a id=\"feature\"><\/a>\n# Vetoriza\u00e7\u00e3o\n\nVetoriza\u00e7\u00e3o consiste em transformar os texto em vetores, a fim de poder aplicar m\u00e9todos de Aprendizado de M\u00e1quina.   \nNeste estudo de caso, tilizaremos o BOW - melhor explicado a seguir.\n\n> Visualiza\u00e7\u00e3o da import\u00e2ncia das palavras ser\u00e1 abordado ap\u00f3s a classifica\u00e7\u00e3o.\n\n\n[Voltar para o Topo](#top)","3bb07419":"Tuplas de exemplo.","e19508b2":"<a id=\"exp\"><\/a>\n## Explainability\n\nEstudar qual a import\u00e2ncia das features para o modelo.\n\nNeste estudo explora-se:\n\n- pacote `sklearn`, listando os pesos da Regress\u00e3o Log\u00edstica","04ca7602":"-----\n<a id=\"data\"><\/a>\n# Dados\n\n- Importando as bibliot\u00e9cas\n- Carregamento dos dados\n- Pr\u00e9-processamento dos dados\n\n[Voltar para o Topo](#top)","758574d2":"## Pr\u00e9-Processamento dos Dados\n\n- Remove caracteres especiais - Links, @name, #hashtags.\n- Transforma as palavras em tokens.\n- Lematiza\u00e7\u00e3o - Reduz palavras para o lemma correspondente, de acordo com o significado no dicion\u00e1rio (an\u00e1lise vocabular e morfol\u00f3gica).\n- Stemiza\u00e7\u00e3o - Reduz palavras flexionadas (ou \u00e0s vezes derivadas) ao seu radical (stem).\n- Vetoriza\u00e7\u00e3o dos dados - BOW, TFIDF","1bdacf3d":"Quais s\u00e3o as caracter\u00edsticas dos nossos dados?\n\n- Quantos valores nulos temos?","e6f68c63":"**Nota**. Aparentemente, ap\u00f3s as primeiras 1500 palavras o modelo n\u00e3o extrai informa\u00e7\u00f5es \u00fateis dos documentos para a sua classifica\u00e7\u00e3o.","cc9325c1":"## Carregando os Dados","19fc216f":"Criando as matrizes densos, isto \u00e9 a constru\u00e7\u00e3o de uma grande matriz com cada tweet e cada palavra dentro do dicion\u00e1rio.","b2bc6eb6":"### Import\u00e2ncia dos Atributos\n\nImprimir a import\u00e2ncia de algumas palavras, bem como visualizar dos demais atributos.   \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression) - Coeficiente das caracter\u00edsticas na fun\u00e7\u00e3o de decis\u00e3o.","b156b26a":"Testando a remo\u00e7\u00e3o de caracteres especiais.","53e2d2ff":"## Importando as Bibliot\u00e9cas","53b52cab":"Busca as palavras unicas, chamados de tokens, do conjunto de dados de treinamento.","d04fff93":"-----\n<a id=\"classification\"><\/a>\n# Classifica\u00e7\u00e3o\n\nNesta se\u00e7\u00e3o utilizamos a Regress\u00e3o Log\u00edstica como algoritmo de classifica\u00e7\u00e3o.\n\n\n[Voltar para o Topo](#top)","921d5b3b":"<a id=\"reg\"><\/a>\n## Regress\u00e3o Log\u00edstica\n\n> Aumentar o n\u00famero de itera\u00e7\u00f5es da Regress\u00e3o n\u00e3o mudou o resultado.","8e52e2ab":"Gerando o arquivo de submiss\u00e3o da competi\u00e7\u00e3o. _Fun\u00e7\u00e3o ocultada._","fd6e465c":"Testa o pr\u00e9-processamento do texto.","c93d03ab":"Filtra os tokens que\n\n- Aparecem em menos de 10 tweets (valor absoluto); or\n- Mais de 85% dos tweets (fra\u00e7\u00e3o de todos os tweets).\n- Por fim, mantenha os 3,000 tokens mais frequentes."}}