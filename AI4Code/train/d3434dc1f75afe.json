{"cell_type":{"6973d57d":"code","ee0ef6ba":"code","ffe5709a":"code","16f20631":"code","4360ae61":"code","c2413e45":"code","95aed762":"code","ed90a531":"code","cc29172a":"code","2910926f":"markdown","f93bfd95":"markdown","ef60082c":"markdown","e5e64781":"markdown","d28ec775":"markdown","b4e08b58":"markdown","1fedb9e2":"markdown"},"source":{"6973d57d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee0ef6ba":"import numpy as np\nimport pandas as pd\n\n## Data load\nX_train_csv = pd.read_csv(\"..\/input\/ecommerce-shipping-data-competition-form\/X_train.csv\")\ny_train_csv = pd.read_csv(\"..\/input\/ecommerce-shipping-data-competition-form\/y_train.csv\")\nX_test_csv = pd.read_csv(\"..\/input\/ecommerce-shipping-data-competition-form\/X_test.csv\")\ny_test_csv = pd.read_csv(\"..\/input\/ecommerce-shipping-data-competition-form\/test_label\/y_test.csv\")\n\n## EDA \ndef EDA(name, df) :\n    print(\"== ===================================================\")\n    print(\"== EDA : \", name)\n    print(\"== ===================================================\")\n    \n    print(\">>> all column info\")\n    print(df.columns)\n    \n    print(\">>> include object column info\")\n    print(df.select_dtypes(include=object).columns)\n    for col in df.select_dtypes(include=object).columns :\n        print(\"-- ---------------------------------------------------\")\n        print(col, \"-\", df[col].nunique(), \" : \", df[col].unique())\n        print(df[col].value_counts())\n        print(\"null check : \", df[col].isnull().sum())\n        \n    print(\">>> exclude object column info\")\n    print(df.select_dtypes(exclude=object).columns)\n    for col in df.select_dtypes(exclude=object).columns :\n        print(\"-- ---------------------------------------------------\")\n        print(col, \"-\", df[col].unique()[:10])\n        print(df[col].describe())\n        print(\"null check : \", df[col].isnull().sum())\n    \n    \n    print(\"== ===================================================\")\n    print(df.isnull().sum())\n    print(df.info())\n    print(df.describe())\n    print(\"== ===================================================\\n\\n\")\n    \nEDA(\"X_train_csv\", X_train_csv)\nEDA(\"y_train_csv\", y_train_csv)\nEDA(\"X_test_csv\", X_test_csv)","ffe5709a":"## User defined variable\ntarget_col = \"Reached.on.Time_Y.N\"\ndrop_col = [\"ID\"]\nis_scaled = True\n\n# Customer_care_calls $7 > 7 essencial transform\n\"\"\"\n'ID' - drop\n'Warehouse_block' - \ubc94\uc8fc A B C D F\n'Mode_of_Shipment' - \ubc94\uc8fc\n'Customer_care_calls' - \ubc94\uc8fc \uc5f0\uc18d\uc73c\ub85c \ubcc0\uacbd\ud544\uc694 - $ \ube7c\uace0 int\ub85c\n'Customer_rating' - \uc5f0\uc18d\n'Cost_of_the_Product' - \uc5f0\uc18d \n'Prior_purchases' - \uc5f0\uc18d\n'Product_importance' - \ubc94\uc8fc\n'Gender' - \ubc94\uc8fc\n'Discount_offered' - \uc5f0\uc18d\n'Weight_in_gms' - \uc5f0\uc18d\n\"\"\"\ndef data_preprocessing(df_X, df_X_sub) :\n    df_X['Customer_care_calls'] = df_X['Customer_care_calls'].str.replace(\"$\", \"\", regex=True).astype(int)\n    df_X_sub['Customer_care_calls'] = df_X_sub['Customer_care_calls'].str.replace(\"$\", \"\", regex=True).astype(int)\n    print(\" > Customer_care_calls check --------------------------\")\n    print(df_X.Customer_care_calls.value_counts())\n    print(df_X_sub.Customer_care_calls.value_counts())\n    print(\" > Customer_care_calls type check ---------------------\")\n    print(df_X.info())\n    print(df_X_sub.info())\n    \n    X_count = df_X.shape[0]\n    X_sub_count = df_X_sub.shape[0]\n    \n    print(\" > One-Hot encoding pre count check -------------------\")\n    print(X_count)\n    print(X_sub_count)\n    df_dum = pd.get_dummies(pd.concat([df_X, df_X_sub], axis=0))\n    \n    print(\" > One-Hot encoding after concat count check ----------------\")\n    print(len(df_dum))\n\n    df_X = df_dum[:X_count]\n    df_X_sub = df_dum[X_count:]\n    print(\" > One-Hot encoding split count check -----------------\")\n    print(len(df_X))\n    print(len(df_X_sub))\n    \n    print(\" > One-Hot encoding type check ---------------------\")\n    print(df_X.info())\n    print(df_X_sub.info())\n    \n    return df_X, df_X_sub\n\nX = X_train_csv.drop(drop_col, axis=1)\ny = y_train_csv[target_col]\nX_sub = X_test_csv.drop(drop_col, axis=1)\n\nX, X_sub = data_preprocessing(X, X_sub)","16f20631":"## HoldOut\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n\n## Regulation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_scaled_train = X_train\nX_scaled_test = X_test\nX_scaled_sub = X_sub\n\nif is_scaled :\n    X_scaled_train = scaler.transform(X_train)\n    X_scaled_test = scaler.transform(X_test)\n    X_scaled_sub = scaler.transform(X_sub)","4360ae61":"## model - RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_scaled_train, y_train)\n\n## train predict\npred_train = model.predict(X_scaled_train)\nprob_train = model.predict_proba(X_scaled_train)\n## test predict\npred_test = model.predict(X_scaled_test)\nprob_test = model.predict_proba(X_scaled_test)\n## submission predict\npred_sub = model.predict(X_scaled_sub)\nprob_sub = model.predict_proba(X_scaled_sub)\n\n## evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, accuracy_score\nprint(\"\\n\\n>>> train -----------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_train, pred_train))\nprint(\"f1_score       : \", f1_score(y_train, pred_train))\nprint(\"roc_auc_score  : \", roc_auc_score(y_train, prob_train[:,1]))\nprint(confusion_matrix(y_train, pred_train))\nprint(classification_report(y_train, pred_train))\n\nprint(\"\\n\\n>>> test ------------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_test, pred_test))\nprint(\"f1_score       : \", f1_score(y_test, pred_test))\nprint(\"roc_auc_score  : \", roc_auc_score(y_test, prob_test[:,1]))\nprint(confusion_matrix(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\n\nprint(\"\\n\\n>>> sub creation ----------------------------------------\")\ndf_sub = pd.DataFrame({\"ID\":X_test_csv['ID'], \"Reached.on.Time_Y.N\":prob_sub[:,1], \"Reached.on.Time_Y.N.pred\":pred_sub})\ndf_sub.to_csv(\"y_submission.csv\", index=False)\n\ndf_subr = pd.read_csv(\".\/y_submission.csv\")\ndisplay(df_subr.head(5))\n\ny_sub = y_test_csv['Reached.on.Time_Y.N']\nprob_subr = df_subr['Reached.on.Time_Y.N']\npred_subr = df_subr['Reached.on.Time_Y.N.pred']\n\nprint(\"\\n\\n>>> sub -------------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_sub, pred_subr))\nprint(\"f1_score       : \", f1_score(y_sub, pred_subr))\nprint(\"roc_auc_score  : \", roc_auc_score(y_sub, prob_subr))\nprint(confusion_matrix(y_sub, pred_subr))\nprint(classification_report(y_sub, pred_subr))","c2413e45":"## cross valid \nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nkfold = KFold(n_splits=10)\nscore = cross_val_score(RandomForestClassifier(random_state=42), X_scaled_train, y_train, cv=kfold, scoring=\"roc_auc\")\nprint(score.mean())\nprint(score)\n# help(GridSearchCV)","95aed762":"# help(RandomForestClassifier)","ed90a531":"## GridSearchCV\n\"\"\"\ngrid_param = {\"n_estimators\":range(100, 1000, 100), \"min_samples_split\":range(1, 10, 1), \"min_samples_leaf\":range(1,5,1), \"max_features\":[\"log2\", \"sqrt\", \"auto\"]}\ngrid_search = GridSearchCV(model, grid_param)\ngrid_search.fit(X_scaled_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.score(X_scaled_test, y_test))\n\"\"\"","cc29172a":"## Enclosure.03 FeatureImportances\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nftr_importances_values = model.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n\nplt.figure(figsize=(8,6))\nplt.title(\"Top 20 Feature Importances\")\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","2910926f":"## Enclosure.02 GridSearchCV","f93bfd95":"## Part.02 Data Preprocessing","ef60082c":"## Enclosure.03 FeatureImportances","e5e64781":"## Enclosure.01 Cross_val","d28ec775":"## Part.03 Data Holdout & Data Normalization","b4e08b58":"## Part.01 Data Load & EDA","1fedb9e2":"## Part.04 Data Modeling & Evaluation"}}