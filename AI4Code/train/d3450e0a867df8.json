{"cell_type":{"953ffd6a":"code","b1a976ce":"code","d42511c2":"code","76b23416":"code","998f627c":"code","477d8e3d":"code","a81cf6b6":"code","67456088":"code","cd9538d1":"code","8cded811":"code","a54ba033":"code","93191c8b":"code","cea5c9de":"code","81163415":"code","d8f72716":"code","00261b14":"code","01a901e1":"code","c6392d98":"code","c843309a":"code","1dde6641":"code","3ba45c7f":"code","c772cc07":"code","cfb8280d":"code","a0b7d1c1":"code","86bf2d0e":"code","c5d9c888":"code","ed9fdfb9":"code","ee26a5e0":"code","3d5f551a":"code","98f9af23":"code","d35bbe06":"code","b94ccf1c":"code","ef0c5395":"code","4c1664ab":"code","73a882a0":"code","82025923":"code","bd439a08":"code","6638a8ee":"code","a7be267d":"code","0a5cad07":"code","57b71342":"code","80e62cfb":"code","5dd354af":"code","95ca8875":"markdown","453b6006":"markdown","a796a16d":"markdown","2be5cd99":"markdown","729158cf":"markdown","8cd83e74":"markdown","f4b06181":"markdown","68e9df06":"markdown","8bf2187a":"markdown","6d1145f5":"markdown","1c19bf1b":"markdown","a6a08510":"markdown","dccf3298":"markdown","1056679c":"markdown","ae6f5c70":"markdown","317d57fb":"markdown","13e6f1bb":"markdown","d710880c":"markdown","b458a604":"markdown","ca9b25cc":"markdown","ba78e12c":"markdown","72514343":"markdown","fbda8213":"markdown","820c1818":"markdown","68bad7f8":"markdown","9423fa96":"markdown","394d489c":"markdown","5b525741":"markdown","ed2fdf52":"markdown","08092acf":"markdown","58f9d57b":"markdown","2052c368":"markdown","be3b8c9a":"markdown","41933f5d":"markdown","aec8c806":"markdown","1a77517f":"markdown","6cf74c7f":"markdown","8224bfe7":"markdown","add30578":"markdown","2b092863":"markdown"},"source":{"953ffd6a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('dark')\nsns.set_palette('Set2')\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier","b1a976ce":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ndata = pd.concat([train_data, test_data]).reset_index().drop(['index'], axis=1)","d42511c2":"data['Surname'] = data['Name'].apply(lambda x: x.split(',')[0])","76b23416":"# New Ticket_id column\ndata['Ticket_id'] = 'new_col'\n# Initialize Ticket_id = Pclass + Ticket + Fare + Embarked\ndef ticket_id(row):\n    row['Ticket_id'] = str(row.Pclass) + '-' + str(row.Ticket)[:-1] + '-' + str(row.Fare) + '-' + str(row.Embarked)\n    return row\n\ndata = data.apply(ticket_id, axis='columns')","998f627c":"# New Group_id column\ndata['Group_id'] = 'new_col2'\n# Initialize Group_id = Surname + Ticket_id\ndef group_id(row):\n    row['Group_id'] = str(row.Surname) + '-' + str(row.Ticket_id)\n    return row\n\ndata = data.apply(group_id, axis='columns')","477d8e3d":"# creation of the Title feature\ndata['Title'] = 'man'\ndata.loc[data.Sex == 'female', 'Title'] = 'woman'\ndata.loc[data['Name'].str.contains('Master'), 'Title'] = 'boy'","a81cf6b6":"data.loc[data.Title == 'man', 'Group_id'] = 'noGroup'\n# New column with WC frequency\ndata['WC_count'] = data.loc[data.Title != 'man'].groupby('Group_id')['Group_id'].transform('count')\n# assign noGroup to every unique value\ndata.loc[data.WC_count <=1, 'Group_id'] = 'noGroup'","67456088":"cols = ['PassengerId', 'Survived', 'Name', 'Title', 'Ticket_id','Group_id']\ndata.loc[(data.Ticket_id == '1-1696-134.5-C') & (data.Title != 'man'), cols]","cd9538d1":"indices = []\ncount = 0\nfor i in range(0,1309):\n    if (data.loc[i,'Title'] != 'man') & (data.loc[i,'Group_id'] == 'noGroup'):\n        data.loc[i,'Group_id'] = data.loc[(data['Ticket_id'] == data.loc[i, 'Ticket_id']) & (data.Title != 'man'), 'Group_id'].iloc[0]\n        if (data.loc[i, 'Group_id'] != 'noGroup'):\n            indices.append(i)\n            count += 1\nprint('{:d} passengers were added to an existing group'.format(count))","8cded811":"cols = ['PassengerId', 'Survived', 'Name', 'Title', 'Group_id']\ndata.loc[indices, cols]","a54ba033":"number_of_groups = data.loc[data.Group_id != 'noGroup', 'Group_id'].nunique()\nprint('Number of groups found: {:d}'.format(number_of_groups))\nnumber_of_WCG_passengers = data.loc[data.Group_id != 'noGroup', 'Group_id'].count()\nprint('\\nNumber of passengers in a group: {:d}'.format(number_of_WCG_passengers))\ncomposition = data.loc[data.Group_id != 'noGroup','Title'].value_counts()\nprint('\\nComposition of the groups:')\nprint(composition.to_string())","93191c8b":"data['WCSurvived'] = data.loc[(data.Title != 'man') & (data.Group_id != 'noGroup')].groupby('Group_id').Survived.transform('mean')","cea5c9de":"cols = ['PassengerId', 'Survived', 'WCSurvived', 'Name', 'Title', 'Group_id']\ndata.loc[data.Group_id == 'Sage-3-CA. 234-69.55-S', cols]","81163415":"print('WCSurvived all data values:')\nprint(data.WCSurvived.value_counts().to_string())\nplt.figure(figsize=(7,5))\nf = sns.countplot(y=data.WCSurvived)","d8f72716":"data.loc[(data.WCSurvived==0.75) | (data.WCSurvived==0.5), cols].sort_values(by='Group_id')","00261b14":"# Get the family names using set difference\ntest_groups = set(data[891:1309].Group_id.unique()) - set(data[0:891].Group_id.unique())\ndata.loc[data.Group_id.isin(test_groups), cols].sort_values(by='Group_id')","01a901e1":"fig, ax = plt.subplots(1,2,figsize=(12,6))\nfig.suptitle('Woman-child-groups analysis', fontsize=14)\na = sns.barplot(x='Pclass', y='Survived', data=data[data.Group_id != 'noGroup'], ax=ax[0]).set_ylabel('Survival rate')\nb = sns.barplot(x='Embarked', y='Survived', data=data[data.Group_id != 'noGroup'], ax=ax[1]).set_ylabel('Survival rate')","c6392d98":"# Assign WCSurvived = 0 to 3rd class test families, else 1\ndata.loc[data.Group_id.isin(test_groups), 'WCSurvived'] = 0\ndata.loc[(data.Group_id.isin(test_groups)) & (data.Pclass != 3), 'WCSurvived'] = 1","c843309a":"print('WCSurvived test values:')\nprint(data[891:1309].WCSurvived.value_counts().to_string())","1dde6641":"# Set everyone to 0\ndata.loc[891:1308, 'Predict'] = 0\n# Set women to 1, completing the gender model\ndata.loc[891:1308, 'Predict'][(data.Sex == 'female')] = 1\n# Change WCG women with WCSurvived=0 to 0\ndata.loc[891:1308,'Predict'][(data.Sex == 'female') & (data['WCSurvived'] == 0)] = 0\n# Change WCG boys with WCSurvived=1 to 1, completing the WCG + gender model\ndata.loc[891:1308, 'Predict'][(data.Title == 'boy') & (data['WCSurvived'] == 1)] = 1\n# With this, the three group members with non-integer WCSurvived are not changed from the gender model","3ba45c7f":"print('The following 8 males are predicted to live:')\ncols = ['PassengerId', 'Name', 'Title', 'Group_id']\ndata[891:1309][cols].loc[(data.Title == 'boy') & (data.Predict == 1)]","c772cc07":"print('The following 15 females are predicted to die:')\ndata[891:1309][cols].loc[(data.Title == 'woman') & (data.Predict == 0)]","cfb8280d":"print('The remaining 258 males are predicted to die')\nprint('and the remaining 137 females are predicted to live')","a0b7d1c1":"output = pd.DataFrame({'PassengerId': data[891:1309].PassengerId, 'Survived': data[891:1309].Predict.astype('int')})\noutput.to_csv('WCG_gender.csv', index=False)\nprint('WCG_gender submission was successfully saved!')\nprint('Submission is loading... you scored 81,6%!')","86bf2d0e":"# Assign np.NaN to zero-fares\ndef fix_fare(row):\n    if row.Fare == 0:\n        row.Fare = np.NaN\n    return row\nprint('The following {:d} passengers have a zero Fare:'.format(data[data.Fare==0].shape[0]))\ncols = ['PassengerId', 'Survived', 'Pclass','Fare', 'Name']\ndata.loc[data.Fare==0, cols]","c5d9c888":"fig, ax = plt.subplots(1,2,figsize=(12,8))\nfig.suptitle('Removing zero fares: before and after', fontsize=14)\na = sns.swarmplot(x='Pclass', y='Fare', data=data, ax=ax[0])\nax[0].axhline(y=2, color='r')\n# Apply the fix_fare function \ndata = data.apply(fix_fare, axis='columns')\nax[1].axhline(y=2, color='r')\nb = sns.swarmplot(x='Pclass', y='Fare', data=data, ax=ax[1])","ed9fdfb9":"# Calculate Ticket frequency and divide Fare by it\ndata['Ticket_freq'] = data.groupby('Ticket')['Ticket'].transform('count')\ndata['Pfare'] = data['Fare'] \/ data['Ticket_freq']","ee26a5e0":"fig, ax = plt.subplots(1,2,figsize=(12,8))\nfig.suptitle('Fare and Pfare compared', fontsize=14)\na = sns.swarmplot(x='Pclass', y='Fare', data=data, ax=ax[0])\nb = sns.swarmplot(x='Pclass', y='Pfare', data=data, ax=ax[1])","3d5f551a":"# Isolating adult males in train and test set\ntrain_male = data[0:891].loc[(data.Sex=='male') & (data.WCSurvived.isnull())]\ntest_male = data[891:1309].loc[(data.Sex=='male') & (data.WCSurvived.isnull())]","98f9af23":"fig, ax = plt.subplots(2,2,figsize=(12,12))\nfig.suptitle('Adult males EDA', fontsize=14)\nsns.barplot(x='Pclass', y='Survived', data=train_male, ax=ax[0][0])\nax[0][0].axhline(y=train_male.Survived.mean(), color='r')\nsns.barplot(x='Embarked', y='Survived', data=train_male, ax=ax[0][1])\nax[0][1].axhline(y=train_male.Survived.mean(), color='r')\nsns.swarmplot(x='Pclass', y='Pfare', hue='Survived', data=train_male, ax=ax[1][0])\nax[1][0].axhline(y=25, color='y')\nax[1][0].axhline(y=32, color='y')\na = sns.swarmplot(y='Age', x='Pclass', hue='Survived', data=train_male, ax=ax[1][1])","d35bbe06":"x1 = train_male.loc[train_male['Survived']==1, 'Pfare']\nx0 = train_male.loc[train_male['Survived']==0, 'Pfare']\ny1 = train_male.loc[train_male['Survived']==1, 'Age']\ny0 = train_male.loc[train_male['Survived']==0, 'Age']\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nfig.suptitle('Age and Pfare distributions with hue Survived', fontsize=14)\nsns.distplot(x1, bins=30, label = 'Survived', ax = ax[0], color = 'c')\nsns.distplot(x0, bins=25, label = 'Not survived', ax = ax[0], color = 'y')\nax[0].set_xlim(-5, 70)\nax[0].legend()\nsns.distplot(y1, bins=20, label = 'Survived', ax = ax[1], color = 'g')\nsns.distplot(y0, bins=20, label = 'Not survived', ax = ax[1], color = 'r')\nax[1].legend()\nfig.show()","b94ccf1c":"cols = ['PassengerId', 'Name', 'Pfare', 'Pclass', 'Embarked']\ny_m = train_male['Survived']\nfeatures = ['Pfare', 'Pclass', 'Embarked']\nX_m = train_male[features]\n\nnumerical_cols = ['Pfare']\ncategorical_cols = ['Pclass', 'Embarked']\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\nprecision_m = []\nrecall_m = []\n\nfor k in range(1,18):\n    pipeline1 = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', KNeighborsClassifier(n_neighbors=k))\n    ])\n    precision_m.append(cross_val_score(pipeline1, X_m, y_m, cv=15, n_jobs=-1, scoring='precision').mean())\n    recall_m.append(cross_val_score(pipeline1, X_m, y_m, cv=15, n_jobs=-1, scoring='recall').mean())\n    \nk_range = range(1,18)\nplt.figure(figsize=(7,5))\nplt.plot(k_range, precision_m, label='15-fold precision')\nplt.plot(k_range, recall_m, label='15-fold recall')\nplt.axhline(y=0.5, color='r')\nplt.xlabel('Value of k for KNN')\nplt.title('Precision and recall by number of neighbors', fontsize=14)\nplt.legend()\nplt.show()","ef0c5395":"m1 = KNeighborsClassifier(n_neighbors=1)\nm2 = KNeighborsClassifier(n_neighbors=3)\nm3 = KNeighborsClassifier(n_neighbors=7)\n# Preprocessing is the same as before\nmale_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('voting',VotingClassifier([\n        ('m1', m1), ('m2', m2), ('m3', m3)]))\n])\nprint('15-fold precision of the ensemble: {:.3f}'.format(\n    cross_val_score(male_pipeline, X_m, y_m, cv=15, n_jobs=-1, scoring='precision').mean()))\nprint('15-fold recall of the ensemble: {:.3f}'.format(\n    cross_val_score(male_pipeline, X_m, y_m, cv=15, n_jobs=-1, scoring='recall').mean()))\nprint('15-fold accuracy of the ensemble: {:.3f}'.format(\n    cross_val_score(male_pipeline, X_m, y_m, cv=15, n_jobs=-1).mean()))\n# Fit model and make predictions\nmale_pipeline.fit(X_m, y_m)\nlearn_train_m = male_pipeline.predict(X_m)\nX_test_m = test_male[features]\npredictions_m = male_pipeline.predict(X_test_m)\nprint('\\nThe following 9 adult males are predicted to live:')\ntest_male.loc[(predictions_m==1), cols]","4c1664ab":"fig, ax = plt.subplots(1,3,figsize=(15,8))\nfig.suptitle('Fun comparison of train set vs test set', fontsize=14)\nax[0].set_title('Real train set')\nax[0].set_ylim(top=60)\nsns.swarmplot(x=X_m.Pclass, y=X_m.Pfare, hue=y_m, ax=ax[0])\nax[1].set_title('Ensemble learns the train set')\nax[1].set_ylim(top=60)\nsns.swarmplot(x=X_m.Pclass, y=X_m.Pfare, hue=learn_train_m,  ax=ax[1])\nax[2].set_title('Ensemble predicts the test set')\nax[2].set_ylim(top=60)\na = sns.swarmplot(x=test_male.Pclass, y=test_male.Pfare, hue=predictions_m,  ax=ax[2])","73a882a0":"data.loc[891:1308, 'Predict'][(data.Sex=='male') & (data.WCSurvived.isnull())] = predictions_m\noutput = pd.DataFrame({'PassengerId': data[891:1309].PassengerId, 'Survived': data[891:1309].Predict.astype('int')})\noutput.to_csv('WCG_male.csv', index=False)\nprint('WCG_male submission was successfully saved!')\nprint('Submission is loading... you scored 82,3%!')","82025923":"train_female = data[0:891].loc[(data.Sex=='female')  & (data.WCSurvived.isnull())]\ntest_female = data[891:1309].loc[(data.Sex=='female') & (data.WCSurvived.isnull())]","bd439a08":"fig, ax = plt.subplots(2,2,figsize=(12,12))\nfig.suptitle('Non-WCG females EDA', fontsize=14)\nsns.barplot(x='Pclass', y='Survived', data=train_female, ax=ax[0][0])\nax[0][0].axhline(y=train_female.Survived.mean(), color='r')\nsns.barplot(x='Embarked', y='Survived', data=train_female, ax=ax[0][1])\nax[0][1].axhline(y=train_female.Survived.mean(), color='r')\nsns.swarmplot(x='Pclass', y='Pfare', hue='Survived', data=train_female, ax=ax[1][0])\nax[1][0].set_ylim(top=70)\nax[1][0].axhline(y=7, color='y')\nax[1][0].axhline(y=10, color='y')\na = sns.swarmplot(y='Age', x='Pclass', hue='Survived', data=train_female, ax=ax[1][1])","6638a8ee":"w1 = train_female.loc[train_female['Survived']==1, 'Pfare']\nw0 = train_female.loc[train_female['Survived']==0, 'Pfare']\nz1 = train_female.loc[train_female['Survived']==1, 'Age']\nz0 = train_female.loc[train_female['Survived']==0, 'Age']\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nfig.suptitle('Age and Pfare distributions with hue Survived', fontsize=14)\nsns.distplot(w1, bins=35, label = 'Survived', ax = ax[0], color = 'c')\nsns.distplot(w0, bins=15, label = 'Not survived', ax = ax[0], color = 'y')\nax[0].set_xlim(-5, 60)\nax[0].legend()\nsns.distplot(z1, bins=12, label = 'Survived', ax = ax[1], color = 'g')\nsns.distplot(z0, bins=10, label = 'Not survived', ax = ax[1], color = 'r')\nax[1].legend()\nfig.show()","a7be267d":"from sklearn.metrics import make_scorer, precision_score, recall_score\n# We set zero_division=0 to avoid raising errors\ncustom_precision = make_scorer(precision_score, pos_label=0, zero_division=0)\ncustom_recall = make_scorer(recall_score, pos_label=0)","0a5cad07":"y_f = train_female['Survived']\nX_f = train_female[features]\nprecision_f = []\nrecall_f = []\n# Preprocessing is always the same...\nfor k in range(1,18):\n    pipeline2 = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', KNeighborsClassifier(n_neighbors=k))\n    ])\n    # We use 9-fold because the train size is smaller\n    # and 198\/9 = integer\n    precision_f.append(cross_val_score(pipeline2, X_f, y_f, cv=9, n_jobs=-1, scoring=custom_precision).mean())\n    recall_f.append(cross_val_score(pipeline2, X_f, y_f, cv=9, n_jobs=-1, scoring=custom_recall).mean())\n    \nplt.figure(figsize=(7,5))\nplt.plot(k_range, precision_f, label='9-fold precision')\nplt.plot(k_range, recall_f, label='9-fold recall')\nplt.axhline(y=0.5, color='r')\nplt.xlabel('Value of k for KNN')\nplt.title('Precision and recall by number of neighbors', fontsize=14)\nplt.legend()\nplt.show()","57b71342":"f1 = KNeighborsClassifier(n_neighbors=4)\nf2 = KNeighborsClassifier(n_neighbors=9)\nf3 = KNeighborsClassifier(n_neighbors=11)\n# Preprocessing pipelines are the same as before\nfemale_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('voting', VotingClassifier([\n        ('f1', f1), ('f2', f2), ('f3', f3)]))\n])\nprint('9-fold precision of the ensemble: {:.3f}'.format(\n    cross_val_score(female_pipeline, X_f, y_f, cv=9, scoring=custom_precision).mean()))\nprint('9-fold recall of the ensemble: {:.3f}'.format(\n    cross_val_score(female_pipeline, X_f, y_f, cv=9, scoring=custom_recall).mean()))\nprint('9-fold accuracy of the ensemble: {:.3f}'.format(\n    cross_val_score(female_pipeline, X_f, y_f, cv=9).mean()))\n# Preprocessing of training data, fit model\nfemale_pipeline.fit(X_f, y_f)\nlearn_train_f = female_pipeline.predict(X_f)\nX_test_f = test_female[features]\npredictions_f = female_pipeline.predict(X_test_f)\nprint('\\nThe following 6 non-WCG females are predicted to die:')\ntest_female.loc[(predictions_f==0), cols]","80e62cfb":"fig, ax = plt.subplots(1,3,figsize=(15,8))\nfig.suptitle('Fun comparison of train set vs test set', fontsize=14)\nax[0].set_title('Real train set')\nax[0].set_ylim(top=55)\nsns.swarmplot(x=X_f.Pclass, y=X_f.Pfare, hue=y_f, ax=ax[0])\nax[1].set_title('Ensemble learns the train set')\nax[1].set_ylim(top=55)\nsns.swarmplot(x=X_f.Pclass, y=X_f.Pfare, hue=learn_train_f,  ax=ax[1])\nax[2].set_title('Ensemble predicts the test set')\nax[2].set_ylim(top=55)\na = sns.swarmplot(x=test_female.Pclass, y=test_female.Pfare, hue=predictions_f,  ax=ax[2])","5dd354af":"data.loc[891:1308, 'Predict'][(data.Sex=='female') & (data.WCSurvived.isnull())] = predictions_f\noutput = pd.DataFrame({'PassengerId': data[891:1309].PassengerId, 'Survived': data[891:1309].Predict.astype('int')})\noutput.to_csv('WCG_male_female.csv', index=False)\nprint('WCG_male_female was successfully saved!')\nprint('Submission is loading... you scored 82,8%!')","95ca8875":"Now let's perform a short analysis to see what features might be useful considering.  \nIn red I plotted the average survival rate for adult males in the train data.","453b6006":"And here we are, 11 passengers were added to a group. Let's see them.","a796a16d":"All these people have the same `Pclass`, `Fare`, `Embarked` and `Ticket` number, but two of them are considered part of any group: this is what we are going to fix.  \nSo let's search for these alone women (or children) which are most likely part of an already existing group.  \nWe loop through all the passengers whose `Group_id` is 'noGroup' and who are not adult males: if we find that their `Ticket_id` value occurs in an existing group, we change the passenger's `Group_id` entry.","2be5cd99":"For non-WCG females, we definitely see that `Pclass` will be determinant another time and that we have to focus on third-class because the survival rates elsewhere are really high.  \nWe see there is a decent concentration of green dots in the third class with `Pfare` between 7 and 10, so this may be our best chance to guess some females who died.  \nLet's see `Age` and `Pfare` distributions to understand their importance.","729158cf":"Now we have something like this: if a group has both passengers in the train and test data, the second ones have an associated `WCSurvived` value that is the mean of the `Survived` values in the train data for that particular group.","8cd83e74":"We first load train and test data and join the two dataframes as it will be much easier to perform all our next operations.","f4b06181":"It looks like 9 of those 11 passengers are in the test data, so there is definitely the possibility to find other females who died, nice!  \nJust to understand how many passengers are part of a group, let's make a quick analysis.","68e9df06":"As we did before, let's run a quick analysis to see if we can use more features or if `Age` is again not useful.","8bf2187a":"We now assign the label 'noGroup' to every man and count the frequency for each `Group_id` element of the dataframe in the new `WC_count` column.  \nWe will then assign 'noGroup' also to every woman or child whose `Group_id` value is unique (i.e. with frequency one) in the entire dataframe.","6d1145f5":"We see that an even or odd number of neighbors makes a significant difference in the recall score, while the precision has a nice trend almost independent from the parity.  \nIt is interesting to see that even when the recall is approximately zero, the models maintain a non-zero precision (even though they are pretty useless).  \nLet's select our three values for k in the ensemble to be 1, 3 and 7.  \nThe reason is that the first two are supposed to detect most of the male survivors with a decent recall while the latter is a pretty good model on his own that will improve the total precision.  \nI know this is a completely naive approach to an ensemble but forgive me for this!","1c19bf1b":"## Submission to Kaggle\nLet's change the adult male predictions of the ensemble from the gender model and submit.  \nAn advantage of the new 100% public leaderboard is that we have now the possibility to count exactly how many correct predictions we make.","a6a08510":"# Scoring over 83%, is it really possible?  \nTwo years have passed since Chris Deotte pushed the top score of the Titanic competition at 82,8% (note that this score is evaluated with the new 100% public leaderboard, previously it was 85,1%, for more info check the discussion [here](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/179147)).  \nFor sure he did a great job and inspired lots of people, but is there a way to improve on his work?  \nThis notebook finds some adult male survivors with a simple approach, so definitely yes... but can we score over 83%?  \nIf you are new to the competition, I recommend you to play for a while with it and come back later, otherwise let's get started!\n![](attachment:WCG%20pipeline-min.jpg)","dccf3298":"Now a great idea, which definitely helps: adding relatives to the groups we identified so far.  \nWhat characteristics do these people share with the other group members?  \nWell, they have a different `Surname` but all the other entries are equal, so they have the same `Ticket_id`.  \nLet's see an example.","1056679c":"## Make WCG predictions and submission to Kaggle\nThere are 74 passengers part of a woman-child-group in the test dataset.  \nLet's apply these two prediction rules on top of the gender model which simply predicts that all men (including children) die while all women live: \n* Predict die for all females whose entire family, excluding adult males, all die as indicated by the `WCSurvived` column equaling 0  \n* Predict live for all boys whose entire family, excluding adult males, all live as indicated by the `WCSurvived` column equaling 1","ae6f5c70":"Next, we modify the `Fare` column another time to create the `Pfare` feature that is just a passenger's `Fare` divided by his ticket frequency.  \nThis mostly helps in reducing the range of the `Fare` feature and also fixes some outliers.  \nAgain, here you find the difference before and after: note the scale difference on the y-axis.","317d57fb":"Now we submit the predictions to the leaderboard: at the time I first tried doing this, the old leaderboard with only 50% of test data was still in use.  \nHowever, now that I am publishing the notebook, there is the new one that uses all of the test data to calculate the score and it appears that they are lower as much as 2% (reason explained [here](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/179147)).","13e6f1bb":"# Woman-Child-Groups (WCG)\nIf we want to get the most accurate Titanic model ever, for sure we have to start with the best component available: let's code the final version of the WCG model developed by Chris in the first part of the [Titanic Mega Model](https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210).  \nIf you haven't already, check that notebook out because it is really clear and contains one dotplot visualization that explains the idea behind the grouping models.  \nJust with the WCG model, we will achieve 81,6% (confirmed to be 83,7% in the old public leaderboard before the change) and we will then work our way to the top.  \nThe first thing to do is extract the surname from the `Name` column, as it will be essential in engineering a `Group_id` feature that precisely identifies families.  ","d710880c":"Oh, what a strange plot!  \nWe see the effect of parity is huge, that it takes a while to increase accuracy over 0.5 and that we have to lose almost half of the recall to gain that.  \nThis time, precision and recall also go to zero at the same time which is a cool thing.  \nI'll try to select the KNN models with k values of 4, 9, 11 with a similar logic as before: we take the high recall of the first, we combine it to the high precision of the second and we add the third that has a little bit of both.  \nLet's see how it goes...","b458a604":"Nice, as good as we expected!  \nThis extremely simple woman-child-group model scores 81,6% in the new and current leaderboard (341 correct predictions) which is really impressive.  \nConsidering that the gender submission scores 76,5% (which is 320 correct predictions) and this model changes 23 of them, it means 22\/23 are correct!  \nWith the old leaderboard, I confirm that it was 83,7% so definitely not bad.\n# Non-WCG passengers\nFor the second part of this notebook, we will focus on non-WCG passengers, who are adult males on one side and females who are not part of an existing group on the other.  \n## Fixing the Fare column\nBefore starting we will do a couple of things.  \nFirst of all, we fix rows with a `Fare` amount of 0 assigning them a null value that will be imputed later when we come to modeling.  \nAs pointed out by Erik Bruin [here](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting), the information might be correct (a quick research showed for example that some of them were part of the guarantee group), but the zero-fares might confuse the algorithms as most of them are within the 1st and 2nd class passengers.  \nIn this notebook, I actually ended up using KNNs so this preprocessing makes no difference, but if I wanted to use something else this may help and that's why I leave it.","ca9b25cc":"We notice `Age` is again not really astonishing while `Pfare` is the one which will help us the most.  \nLet's try not to use `Age` and hope for the best: we already found some male survivors so we are more than happy with the result.\n## Let's build an ensemble, again\nWe will make our predictions through an ensemble of three models, using KNNs just for fun.  \nI encourage you to fork this notebook and try your models if you don't like this choice!  \nSince the classes are very imbalanced another time (the survivors are 4.5 times the deads), accuracy is not a good metric here: for example, cross-validation accuracy for the predict all live model is 0.818!  \nSince sklearn metrics use 1 as the default value for positive samples, we have to create our custom metrics that use 0 as the positive value.","ba78e12c":"Here you can see the difference before and after removing zero fares.  ","72514343":"As you can see, the range is reduced from over 500 to 125, the second class is now more compact and the third class lost most of its outliers with really high fares.  \nWith these two small tweaks, we are now ready to go.\n## Explore adult males\nAdult males are the most difficult category to make predictions for.  \nMost of the models we see here on Kaggle predict that all of them die and if not, usually the results are not satisfactory.  \nHere my idea seems to work, even if not with magnificent results: let's dive into it.","fbda8213":"Again, we don't expect our model to have a high recall but we would like to have a decent precision.  \nNote that with respect to these two custom metrics, the predict all live model has a score of 0.0 and 0.0!  \nLet's vary the number of neighbors in a KNN model and see how these two quantities change.","820c1818":"Here is a summary of which passengers are predicted to live or die from the previous prediction rules.","68bad7f8":"We notice the `Age` distributions mostly overlap, while the `Pfare` ones do not (note the different height).  \nSo, is it leaving out `Age` the correct choice? I think yes.\n## Let's build an ensemble\nWhat information can we extract from the `Pfare`, `Pclass`, `Embarked` columns?  \nAre they enough to detect some male survivors?  \nLet's build an ensemble of three models and we will see!  \nI used only K-Nearest Neighbors just to have fun: it may not be the best choice (and I am pretty sure about that) but I encourage you to fork this notebook and try your models.  \nSince the classes are very imbalanced (the deads are more than 5 times the survivors), accuracy might not be a good metric here: for example, cross-validation accuracy for the predict all die model is 0.84!  \nWe are interested in precision and recall instead, i.e. respectively how many of the samples predicted as survivors actually lived and how many survivors are captured by our positive predictions.\n![](attachment:precision-recall-min.jpg)\n*Image by Walber - Own work, CC BY-SA 4.0, [link](https:\/\/commons.wikimedia.org\/w\/index.php?curid=36926283).*  \n\nSince the survivors are very spread out, we don't expect our model to have a high recall but we would like to have a decent precision for sure.  \nNote that with respect to these two metrics, the predict all die model has a score of 0.0 and 0.0!  \nLet's see how these two metrics vary in a KNN model when we change the number of neighbors.","9423fa96":"Now we divide the passengers into three categories: men, women and children (only male ones).  \nFrom the latter two, WCG will be created and we will assign a survival rate to the group so we can make predictions for each member in case he\/she is in the test data.","394d489c":"We draw some interesting conclusions: `Pclass` is definitely useful and also `Embarked`, even though these two features are not independent (see why if you didn't know this).  \nWhat about `Pfare`? We see there is a nice concentration of orange dots in the first class with `Pfare` between 25 and 32 (almost 50%), so this may be our best chance to guess some male survivors.  \nWhat about `Age` instead? We recognize that the value of `Pclass` is the most discriminative because `Age` alone is very confusing (try tracing two horizontal lines as we did for `Pfare`).  \nLet's see the distributions of the two numerical features to understand their influence on the target variable.","5b525741":"We can understand the power of this approach now if we look at all the values that `WCSurvived` assumes.","ed2fdf52":"Wow! Our score increased again and it is now 82,8% which is 346 passengers, 2 more correct predictions than the WCG + adult males model!  \nUnfortunately, this is exactly the top score previously available so we were not able to make a breakthrough over 83%... or did we?  \nWell, we found some male survivors, that's a great win for me because this is something no one has ever succeeded doing and also indirectly is pushing the top score over its limit.      \nIf I had ensembled different models for non-WCG females (maybe including `Age` too), I could have probably scored over 83% but that's just an idea I leave here for the readers!  \n# Conclusion\nFirst of all, I would like to thank [Chris](https:\/\/www.kaggle.com\/cdeotte) for his amazing Titanic tutorials as well as [Erik](https:\/\/www.kaggle.com\/erikbruin) for his careful family analysis and `Fare` tricks which helped a bit.      \nSecondly, thank you for reading my notebook!  \nI hope you liked it: this was my attempt to push the top score higher and even if I did not succeed, it was still a nice journey.  \nI had to look at the problem from another perspective and not using the `Age` column ended up being a way to find some male survivors, so at least I made an important contribution.  \nThis notebook proves another time that the two tasks of predicting adult male survivors and females not in a group who died are very different and it is really difficult to do both well with the same approach.  \nI believe it is possible to increase the public score again ensembling this model with others, mostly to correct some of the female predictions: I think the maximum score achievable could be around 350 passengers, which is 83,7%.  \nThat being said, I finish this notebook with a short summary of what we did.\n# Summary of what we did\nThis notebook proved that male survivors can be found with simple elements and thus the old top scores have the potential to be improved, especially if we use the WCG model as a strong baseline.  \nHere's the pipeline we followed to reach top 1% in the competition and an histogram of Titanic LB scores in September 2020.\n![](attachment:Titanic_LB.png)\nFirst, the passengers (train + test) were divided into four groups:\n1. Adult males - 60% of the population (A)\n1. Young boys (with Master title) - 5% of the population (B)\n1. Females that have children, sisters, or mothers (brothers and husbands are optional) - 13% of the population (C)\n1. Females that have brothers, or husbands (no children, no sisters, no mothers) or are traveling alone - 22% of the population (D)\n\nNext, we built 3 classifiers:\n1. **WCG** on top of the **gender model** to classify (B) + (C)\n1. **Ensemble_1** of KNNs to classify (A)\n1. **Ensemble_2** of KNNs to classify (D)\n\nFinally we assembled the models. Here are the public scores:\n1. **WCG + gender model** scores 81,6% which is 341 correct predictions\n1. **WCG + ensemble_1 + all females live** scores 82,3% which is 344 correct predictions\n1. **WCG + ensemble_1 + ensemble_2** scores 82,8% which is 346 correct predictions\n\nSee you soon in the next one and good luck passing 83%!  \nPlease let me know down in the comments your suggestions or if you find errors in the code!","08092acf":"From this analysis, we immediately see that woman-child-groups traveling in `Pclass` 1 or 2 mostly survived and woman-child-groups in `Pclass` 3 mostly perished.  \nSince the Gibsons had `Pclass` 1 and `Embarked` C while the Klasens, Peacocks, and van Billiards had `Pclass` 3 and `Embarked` S, our best bet is to assume that the Gibsons survived and the other three families died.","58f9d57b":"The unique values count confirms that generally the assumption that these groups lived or perished together is absolutely correct: that is the real power of the WCG model.  \nOnly 11 people have a non-integer `WCSurvived` value: the Allisons and the Asplunds.  \nWe see that most of them are in the train data, so for the only three people in the test set we will just use the gender model and hope for the best.  ","2052c368":"Wow, we scored 82,3% which is 344 passengers, 3 more correct predictions than the WCG + gender model!  \nIt means 6 out of our 9 male predictions were correct: we finally found a way to guess some male survivors, yeah!\n## Explore non-WCG females\nNon-WCG females are a very wide group which contains solo females but also women traveling with their spouse.  \nA way to isolate them from the dataframe is of course select null `WCSurvived` entries from the female subpopulation.","be3b8c9a":"Wow, the 9-fold precision for the ensemble is 0.571, at least it did not go down!  \nThe recall isn't great but it's ok, also this one is a difficult task and we don't expect a lot more than this.      \nI'm surprised that we made only 6 predictions, but it is what it is with such a small test set (only 97 rows).  \nAs I did before, here I compare the train set, how the ensemble learns it and its predictions for the test data.  \nThanks to this plot, we can also see that non-WCG females in the third class are not distributed as in the train data and this may explain why we made only a few predictions.  ","41933f5d":"It's actually pretty funny to see how the ensemble learns the train set but hey, if it works I keep it!\n## Final Submission\nNow it's time to see if we have a good model that predicts which females die.  \nHopefully, we have the same increment as before and have the best Titanic model ever!  \nLet's change these female predictions from the previous model and submit to Kaggle.","aec8c806":"Now, this is the total number of passengers in the test set by `WCSurvived` value: we are finally ready to make predictions.","1a77517f":"These families are the Gibsons, Klasens, Peacocks, van Billiards.  \nWe need to make predictions for them so let's explore which woman-child-groups live or die.","6cf74c7f":"For each passenger, let's now concatenate the strings `Surname + Pclass + Ticket + Fare + Embarked` to have an alphanumeric one under the name `Group_id`.  \nAs proved in [this](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting) notebook by Erik Bruin, sometimes tickets of passengers traveling together differ in their last digit, so let's also remove the last digit to be consistent and identify the largest number of groups (for example, the `Ticket` value 'CA. 2343' will become 'CA. 234').  \nThe other features such as `SibSp` and `Parch` are purposely not included in the `Group_id` because they contain errors and are not equal for every family component.  \nThe `Cabin` column is not included either because too much data is missing.  \nLet's write a group_id function that does exactly what we need.  \nWe pass through a `Ticket_id` helper column that we will reuse later.","8224bfe7":"We are not done yet as there are some family groups with all members in the test data.  \nFor this reason, they have an unknown family survival rate, but we will assume that every component of the family underwent the same fate as our WCG hypothesis.","add30578":"The 15-fold precision of the ensemble is 0.562, slightly better than 0.5 and in between the original precisions of the three models.  \nI know finding some male survivors is a difficult task, so obtaining this >0.5 accuracy is a good result for me.  \nUnfortunately, the 15-fold recall of the ensemble is only 0.318 but at least it did not go down from the original recalls of the three models.  \nThat is a consequence of the sparsity of the survivors, especially in the second and third classes where our model was not able to extract useful patterns (except one, Mr. Ling Hee in the third class!).  \nSince we made 9 predictions, there is a concrete possibility that we find some male survivors this time!  \nJust for fun, here I compare the train set, how the ensemble learns it and its predictions for the test data.","2b092863":"So we found 80 woman-child-groups, for a total of 230 passengers: 171 women and 59 boys.  \nLet's move on to the central aspect of this approach: calculate the average survival rate of each group.  \nHere is how it is done: we simply group by `Group_id` and evaluate the mean for the known `Survived` values."}}