{"cell_type":{"10f0322b":"code","b5010a1b":"code","3c481425":"code","5410747e":"code","abd84508":"code","276ba938":"code","02a617fd":"code","b4b3421e":"code","26f17188":"code","b3ae9ffb":"code","ad7004c1":"code","f98054b6":"code","93b14f36":"code","3057f35e":"code","5a3eabf1":"code","bed3b072":"code","1bdd91dc":"code","ece72b3d":"code","bcc98407":"code","1bb7ecc2":"code","035e781c":"code","1159d36f":"code","28356594":"code","df085e71":"code","781d210d":"code","9bfd2979":"code","c86e1c33":"code","c8170266":"code","25ba21c4":"code","7ef6f3f6":"code","47d1139f":"code","65166f6e":"code","708c66b8":"code","599c2055":"code","cd83826c":"markdown","35c235e1":"markdown","2ca9ffe2":"markdown","906ef924":"markdown","d0f69b63":"markdown","e60cc18b":"markdown","de352aa7":"markdown","4cb1cf70":"markdown","96377dde":"markdown","b718f1dd":"markdown","ae057516":"markdown","d7db9311":"markdown","2db5f1df":"markdown","aa0ef0a2":"markdown","a327dfde":"markdown","83128e64":"markdown","08246190":"markdown","46461504":"markdown","fbff83c2":"markdown"},"source":{"10f0322b":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nimport numpy as np\nimport math\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score \nfrom sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom itertools import product\nimport statsmodels.api as sm\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import LSTM\n\nfrom itertools import cycle\nimport plotly.offline as py\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nplt.style.use('seaborn-darkgrid')","b5010a1b":"# Google Drive Root Path\n# root_path = '\/content\/drive\/MyDrive\/Datasets_For_Working\/Bitcoin_2015-2021-30-Oct-2021\/3_pull.csv'\n\n# Local Machine Root Path\n# root_path = '.\/input\/btc_ohlc_Oct_2015_to_Oct_2021.csv'\n\n# Kaggle Root Path\nroot_path = '\/kaggle\/input\/bitcoin-historic-prices-from-oct2015-to-oct2021\/btc_ohlc_Oct_2015_to_Oct_2021.csv'\n\n\n# btc_input_df = pd.read_csv(root_path, nrows=500)\nbtc_input_df = pd.read_csv(root_path)\nbtc_input_df.tail()","3c481425":"btc_input_df.shape","5410747e":"btc_input_df.describe()","abd84508":"btc_input_df.info()","276ba938":"btc_input_df_datetype = btc_input_df.astype({'time': 'datetime64'})","02a617fd":"btc_input_df_datetype.info()","b4b3421e":"print('Null Values:',btc_input_df_datetype.isnull().values.sum())\nprint('If any NA values:', btc_input_df_datetype.isnull().values.any())","26f17188":"btc_input_df_datetype.tail()","b3ae9ffb":"btc_input_df_datetype.head()","ad7004c1":"btc_input_df_datetype.set_index(\"time\").close.plot(figsize=(24,7), title=\"Bitcoin Weighted Price\")","f98054b6":"plt.figure(figsize=(15,12))\nplt.suptitle('Lag Plots', fontsize=22)\n\nplt.subplot(3,3,1)\npd.plotting.lag_plot(btc_input_df_datetype['close'], lag=1) #minute lag\nplt.title('1-Minute Lag')\n\nplt.subplot(3,3,2)\npd.plotting.lag_plot(btc_input_df_datetype['close'], lag=60) #hourley lag\nplt.title('1-Hour Lag')\n\nplt.subplot(3,3,3)\npd.plotting.lag_plot(btc_input_df_datetype['close'], lag=1440) #Daily lag\nplt.title('Daily Lag')\n\nplt.subplot(3,3,4)\npd.plotting.lag_plot(btc_input_df_datetype['close'], lag=10080) #weekly lag\nplt.title('Weekly Lag')\n\nplt.subplot(3,3,5)\npd.plotting.lag_plot(btc_input_df_datetype['close'], lag=43200) #month lag\nplt.title('1-Month Lag')\n\nplt.legend()\nplt.show()","93b14f36":"btc_input_df_datetype['date'] = pd.to_datetime(btc_input_df_datetype['time'],unit='s').dt.date\n\ndisplay(btc_input_df_datetype.head())\n\ngroup = btc_input_df_datetype.groupby('date')\n\nbtc_closing_price_groupby_date = group['close'].mean()","3057f35e":"display(btc_closing_price_groupby_date.head(10))\n\nprint(\"Length of btc_closing_price_groupby_date :\", len(btc_closing_price_groupby_date))","5a3eabf1":"prediction_days = 60\n\n# Set Train data to be uplo ( Total data length - prediction_days )\ndf_train= btc_closing_price_groupby_date[:len(btc_closing_price_groupby_date)-prediction_days].values.reshape(-1,1)\n\n\n# Set Test data to be the last prediction_days (or 60 days in this case)\ndf_test= btc_closing_price_groupby_date[len(btc_closing_price_groupby_date)-prediction_days:].values.reshape(-1,1)","bed3b072":"df_test.shape\n","1bdd91dc":"chosen_col = 'Close'\n\nfig, ax = plt.subplots(1, figsize=(13, 7))\nax.plot(df_train, label='Train', linewidth=2)\nax.plot(df_test, label='Test', linewidth=2)\nax.set_ylabel('Price USD', fontsize=14)\nax.set_title('', fontsize=16)\nax.legend(loc='best', fontsize=16)","ece72b3d":"scaler_train = MinMaxScaler(feature_range=(0, 1))\nscaled_train = scaler_train.fit_transform(df_train)\n\nscaler_test = MinMaxScaler(feature_range=(0, 1))\nscaled_test = scaler_test.fit_transform(df_test)","bcc98407":"def dataset_generator_lstm(dataset, look_back=5):\n    # A \u201clookback period\u201d defines the window-size of how many\n    # previous timesteps are used in order to predict\n    # the subsequent timestep. \n    dataX, dataY = [], []\n    \n    for i in range(len(dataset) - look_back):\n        window_size_x = dataset[i:(i + look_back), 0]\n        dataX.append(window_size_x)\n        dataY.append(dataset[i + look_back, 0]) # this is the label or actual y-value\n    return np.array(dataX), np.array(dataY)\n\ntrainX, trainY = dataset_generator_lstm(scaled_train)\n\ntestX, testY = dataset_generator_lstm(scaled_test)\n\nprint(\"trainX: \", trainX.shape)\nprint(\"trainY: \", trainY.shape)\nprint(\"testY: \", testX.shape)\nprint(\"testY\", testY.shape)","1bb7ecc2":"print(\"trainX: \", trainX)\n# print(\"trainY: \", trainY)\n# print(\"testY: \", testX)\n# print(\"testY\", testY)","035e781c":"print(trainX.shape)\nprint(testX.shape)","1159d36f":"trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1 ))\n\nprint(\"Shape of trainX: \", trainX.shape)\nprint(\"Shape of testX: \", testX.shape)","28356594":"print(\"trainX: \", trainX)\nprint(\" ********** \")\nprint(\"testX: \", testX)","df085e71":"# First checking the values for input_shape = (trainX.shape[1], trainX.shape[2])\n# Note - `input_shape` of LSTM Model - `input_shape` is supposed to be (timesteps, n_features).\n\nprint(\"trainX.shape[1] - i.e. timesteps in input_shape = (timesteps, n_features) \", trainX.shape[1])\nprint(\"trainX.shape[2] - i.e. n_features in input_shape = (timesteps, n_features) \", trainX.shape[2])","781d210d":"regressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\n# You must set return_sequences=True when stacking LSTM layers so that the second LSTM layer\n# has a compatible n-dimensional sequence input.\n# This hyper parameter should be set to False (which is the default value) for the last layer\n# and true for the other previous layers.\n\nregressor.add(LSTM(units = 128, activation = 'relu',return_sequences=True, input_shape = (trainX.shape[1], trainX.shape[2])))\nregressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 64, input_shape = (trainX.shape[1], trainX.shape[2])))\n# regressor.add(LSTM(units = 64, return_sequences = True, input_shape = (trainX.shape[1], trainX.shape[2])))\nregressor.add(Dropout(0.2))\n# Note - If I plan to add 3-rd or 4-th layers of LSTM then \n# I must set return_sequences=True in the 2-nd layer above\n# so that the 3-rd LSTM layer has a compatible n-dimensional sequence input.\n\n\n# Adding a third LSTM layer and some Dropout regularisation\n# regressor.add(LSTM(units = 64, return_sequences = True, input_shape = (trainX.shape[1], trainX.shape[2])))\n# regressor.add(Dropout(0.2))\n\n\n# Adding a fourth LSTM layer and some Dropout regularisation\n# regressor.add(LSTM(units = 64, input_shape = (trainX.shape[1], trainX.shape[2])))\n# regressor.add(Dropout(0.2))\n\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\nregressor.summary()\n","9bfd2979":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Compiling the LSTM\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\ncheckpoint_path = 'my_best_model.hdf5'\n\ncheckpoint = ModelCheckpoint(filepath=checkpoint_path, \n                             monitor='val_loss',\n                             verbose=1, \n                             save_best_only=True,\n                             mode='min')\n\n\nearlystopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\ncallbacks = [checkpoint, earlystopping]\n\n\nhistory = regressor.fit(trainX, trainY, batch_size = 32, epochs = 300, verbose=1, shuffle=False, validation_data=(testX, testY), callbacks=callbacks)","c86e1c33":"from tensorflow.keras.models import load_model\n\nmodel_from_saved_checkpoint = load_model(checkpoint_path)","c8170266":"plt.figure(figsize=(16,7))\nplt.plot(history.history['loss'], label='train')\n\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","25ba21c4":"# Transformation to original form and making the predictions\n\n# predicted_btc_price_test_data = regressor.predict(testX)\npredicted_btc_price_test_data = model_from_saved_checkpoint.predict(testX)\n\npredicted_btc_price_test_data = scaler_test.inverse_transform(predicted_btc_price_test_data.reshape(-1, 1))\n\ntest_actual = scaler_test.inverse_transform(testY.reshape(-1, 1))","7ef6f3f6":"plt.figure(figsize=(16,7))\n\nplt.plot(predicted_btc_price_test_data, 'r', marker='.', label='Predicted Test')\n\nplt.plot(test_actual, marker='.', label='Actual Test')\n\nplt.legend()\nplt.show()","47d1139f":"# Transformation to original form and making the predictions\n\npredicted_btc_price_train_data = model_from_saved_checkpoint.predict(trainX)\n\npredicted_btc_price_train_data = scaler_train.inverse_transform(predicted_btc_price_train_data.reshape(-1, 1))\n\ntrain_actual = scaler_train.inverse_transform(trainY.reshape(-1, 1))","65166f6e":"plt.figure(figsize=(16,7))\n\nplt.plot(predicted_btc_price_train_data, 'r', marker='.', label='Predicted Train')\n\nplt.plot(train_actual, marker='.', label='Actual Train')\n\nplt.legend()\nplt.show()","708c66b8":"rmse_lstm_test = math.sqrt(mean_squared_error(test_actual, predicted_btc_price_test_data))\n\nprint('Test RMSE: %.3f' % rmse_lstm_test)\n\n# With 2 Layers + Dropout + lookback=5 => I got - Test RMSE: 1666.162  => This seems best\n","599c2055":"rmse_lstm_train = math.sqrt(mean_squared_error(train_actual, predicted_btc_price_train_data))\n\nprint('Test RMSE: %.3f' % rmse_lstm_train)\n\n# With 2 Layers + Dropout + lookback=5 => I got - Test RMSE: 1047.916  => This seems best","cd83826c":"# Convert 'time' column from object dtype to datetime dtype\n\nFrom above I can see that the 'time' column is being treated as an object rather than as dates. To fix this, I will use the (pd.to_datetime() function which converts the arguments to dates.","35c235e1":"## Plot line graph to show Loss Numbers relative to the epoch","2ca9ffe2":"# LSTM (Long Short-Term Memory) Mechanism\n\nThe Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.\n\nUnlike other recurrent neural networks, the network\u2019s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.\n\nIn the Keras deep learning library, LSTM layers can be created using the LSTM() class.\n\nCreating a layer of LSTM memory units allows you to specify the number of memory units within the layer.\n\nEach unit or cell within the layer has an internal cell state, often abbreviated as \u201cc\u201c, and outputs a hidden state, often abbreviated as \u201ch\u201c.\n\nThe Keras API allows you to access these data, which can be useful or even required when developing sophisticated recurrent neural network architectures, such as the encoder-decoder model.\n\n\n## `input_shape` of LSTM Model - `input_shape` is supposed to be (timesteps, n_features).\n\n    input_shape = (95000,360)\n\nKeras LSTM takes and input with shape of (n_examples, n_times, n_features) and your layers input has to have this shape\n\nTo give you an example: if I'm observing the amount of rain and the temperature each hour for 24h in order to predict the weather (1 = good, 0 = bad), and I do that for 365 days, I will have 365 samples, each of which will have 24 timesteps, and 2 variables (one for rain, one for temperature), so my input is going to have the shape (365, 24, 2), and input_shape = (24, 2)\n\n\n## `return_sequences=True`\n\nThe original LSTM model is comprised of a single hidden LSTM layer followed by a standard feedforward output layer.\n\nThe Stacked LSTM is an extension to this model that has multiple hidden LSTM layers where each layer contains multiple memory cells.\n\nYou must set return_sequences=True when stacking LSTM layers so that the second LSTM layer has a compatible n-dimensional sequence input. \n\n#### To stack LSTM layers, we need to change the configuration of the prior LSTM layer to output a compatible n-dim array as input for the subsequent layer.\n\nWe can do this by setting the return_sequences argument on the layer to True (defaults to False). This will return one output for each input time step and provide the compatible n-dim array.\n\n## Why Increase Depth of an LSTM ?\n\nStacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique.\n\nIt is the depth of neural networks that is generally attributed to the success of the approach on a wide range of challenging prediction problems.\n\nAdditional hidden layers can be added to a Multilayer Perceptron neural network to make it deeper. The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction. For example, from lines to shapes to objects.\n\nA sufficiently large single hidden layer Multilayer Perceptron can be used to approximate most functions. Increasing the depth of the network provides an alternate solution that requires fewer neurons and trains faster. Ultimately, adding depth it is a type of representational optimization.\n\nGiven that LSTMs operate on sequence data, it means that the addition of layers adds levels of abstraction of input observations over time. In effect, chunking observations over time or representing the problem at different time scales.\n\nStacked LSTMs are now a stable technique for challenging sequence prediction problems. \n\n# Final LSTM ","906ef924":"## Let\u2019s take a look at the normalized window for trainX.\n\n```\n[[[0.00000000e+00]\n  [1.44159882e-04]\n  [2.79473948e-04]\n  [5.34851927e-04]\n  [4.90695552e-04]]\n\n [[1.44159882e-04]\n  [2.79473948e-04]\n  [5.34851927e-04]\n  [4.90695552e-04]\n  [4.21819458e-04]]\n\n [[2.79473948e-04]\n  [5.34851927e-04]\n  [4.90695552e-04]\n  [4.21819458e-04]\n  [7.33745838e-04]]... ]\n\n```\nAbove are the first three entries. We can see that the five time steps immediately prior to the one we are trying to predict move in a stepwise motion.\n\nConsider the last value of the first window which is => 4.90695552e-04","d0f69b63":"-------------------------------\n\n# For LSTM Reshape input ( trainX and testX ) to be 3-D of [samples, time steps, features]\n\n### First check the current shape of trainX and testX","e60cc18b":"# Mix Max Scaling of Data post Train-Test Split\n\nScaling must be done after the data has been split into training and test sets \u2014 with each being scaled separately. \n\nA common mistake when first using the LSTM is to first normalize the data before splitting the data.\n\nThe reason this is erroneous is that the normalization technique will use data from the test sets as a reference point when scaling the data as a whole. This will inadvertently influence the values of the training data, essentially resulting in data leakage from the test sets.\n","de352aa7":"# Transformation of time series into supervision problems\n\nTime series data needs to be prepared before training a supervised learning model (such as LSTM neural network). For example, a unary time series is represented as an observation vector:\n\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n```\n\nThe supervised learning algorithm requires data to be provided as a set of samples, where each sample has an input component X and an output component y. The model will learn how to map input to output from the provided examples.\n\n\nThe time series must be converted into samples with input and output components .  For a univariate time series problem that is interested in one-step forecasting, you can use the observations at the previous time point as input and the observations at the current time point as the output. For example, the above 10-step univariate sequence can be expressed as a supervised learning problem, the input is 3 time steps, and the output is 1 time step, as shown below:\n\nX *******   Y\n\n===============\n \n[1, 2, 3],  [4]\n\n[2, 3, 4],  [5]\n\n[3, 4, 5],  [6]","4cb1cf70":"# LSTM Prediction using trainX and plotting line graph against Actual trainY","96377dde":"# Train Test Split","b718f1dd":"<h1 style=\"font-size:250%; font-family:cursive; color:#ff6666;\"><b>\ud835\uddd5\ud835\uddf6\ud835\ude01\ud835\uddf0\ud835\uddfc\ud835\uddf6\ud835\uddfb \ud835\udde3\ud835\uddff\ud835\uddf6\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddff\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\udddf\ud835\udde6\ud835\udde7\ud835\udde0 - Detailed Video in my YouTube Channel<\/b><a id=\"0\"><\/a><\/h1>\n\n<a href=\"https:\/\/bit.ly\/3mXnKGH\"><h1 style=\"font-size:250%; font-family:cursive; color:#ff6666;\"><b>Link to my YouTube Video<\/b><\/h1><\/a>\n\n[![IMAGE ALT TEXT](https:\/\/imgur.com\/ZtAvye5.png)](https:\/\/bit.ly\/3mXnKGH \"\ud835\uddd5\ud835\uddf6\ud835\ude01\ud835\uddf0\ud835\uddfc\ud835\uddf6\ud835\uddfb \ud835\udde3\ud835\uddff\ud835\uddf6\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddff\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddd7\ud835\uddf2\ud835\uddf2\ud835\uddfd \ud835\udddf\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udddf\ud835\udde6\ud835\udde7\ud835\udde0\")","ae057516":"# RMSE - Test Data","d7db9311":"# Lag Plot\n\nLag plot are used to observe the autocorrelation. These are crucial when we try to correct the trend and stationarity and we have to use smoothing functions. Lag plot helps us to understand the data better.","2db5f1df":"# RMSE - Train Data","aa0ef0a2":"# For LSTM I need to reshape input to be a 3D Tensor of [samples, time steps, features]\n\n    X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n\n**Samples** - This is the len(dataX), or the amount of data points you have.\n\n**Time steps** - A sample contains multiple time steps, that is, the width of the sliding window (according to the above example, the time step is 3). Note here that it is distinguished from the sliding step of the sliding window. This is equivalent to the amount of time steps you run your recurrent neural network. If you want your network to have memory of 60 characters, this number should be 60. For this notebook, I am using the window-size to be 5.\n\n**Features** - this is the amount of features in every time step. If you are processing pictures, this is the amount of pixels. In this case I have 1 feature (the price of Bitcoin) per time step.\n\n ### According to the documentation and the source code, the Keras LSTM input data must be in the form: [batch_size, timesteps, input_dim].\n\n In Keras, the number of time steps is equal to the number of LSTM cells. This is what the word \u201ctime steps\u201d means in the 3D tensor of the shape [batch_size, timesteps, input_dim].\n\n\n### So to emphasize again, the input to every LSTM layer must be three-dimensional.\n\nThe three dimensions of this input are:\n\nSamples. One sequence is one sample. A batch is comprised of one or more samples.\n\nTime Steps. One time step is one point of observation in the sample.\n\nFeatures. One feature is one observation at a time step.\n\nThis means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.\n\nWhen defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. You can do this by specifying a tuple to the \u201cinput_shape\u201d argument.\n\n-----------------------\n\n### Example of LSTM With Single Input Sample where you have one sequence of multiple time steps and one feature.\n\nFor example, this could be a sequence of 10 values:\n\n```\n0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\n\n```\n\n1. We can define this sequence of numbers as a NumPy array.\n\n2. We can then use the reshape() function on the NumPy array to reshape this one-dimensional array into a three-dimensional array with 1 sample, 10 time steps, and 1 feature at each time step.\n\nThe reshape() function when called on an array takes one argument which is a tuple defining the new shape of the array. We cannot pass in any tuple of numbers; the reshape must evenly reorganize the data in the array.\n\n3. Once reshaped, we can print the new shape of the array.\n\nPutting all of this together, the complete example is listed below.\n\n```py\nfrom numpy import array\ndata = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\ndata = data.reshape((1, 10, 1))\nprint(data.shape)\n\n```\nRunning the example prints the new 3D shape of the single sample.\n\n```\n(1, 10, 1)\n\n```\nThis data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 1).\n\n```py\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 1)))\nmodel.add(Dense(1))\n\n```\n\n----------------------------------","a327dfde":"We can see that there is a positive correlation for minute, hour and daily lag plots. \n\nCorrelation decreases drastically with Weekly Lag and absolutely no correlation for month lag plots.\n\nIt makes sense to re-sample our data atmost at the Daily level, thereby preserving the autocorrelation as well.","83128e64":"# Making data ready for LSTM - pd.to_datetime(unit='s) and then groupby('date')\n\n## pd.to_datetime unit='s'\n\nThe unit of the arg (D,s,ms,us,ns) denote the unit, which is an integer or float number. This will be based off the origin. Example, with unit=\u2019ms\u2019 and origin=\u2019unix\u2019 (the default), this would calculate the number of milliseconds to the unix epoch start.\n\nSay you pass an int as your arg (like 20203939), with unit, you\u2019ll be able specify what unit your int is away from the origin. In the example here, if we set unit=\u2019s\u2019, this means pandas will interpret 20203939 as 20,203,939 seconds away from the origin. Available units are [D,s,ms,us,ns]. \n[Source](https:\/\/www.dataindependent.com\/pandas\/pandas-to-datetime\/)","08246190":"# Dataset Generator for LSTM\n\nWe will frame the problem to take a window of the last so many number of days of data to predict the current days data.\n\nTo achieve this, we will define a new function named `dataset_generator_lstm()` that will split the input sequence into windows of data appropriate for fitting a supervised learning model, like an LSTM\n\n\nFor example, if the sequence was:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\n\nThen the samples for training the model will look like:\n\n```\n\nInput \t\t\t\t| Output\n1, 2, 3, 4, 5 \t\t6\n2, 3, 4, 5, 6 \t\t7\n3, 4, 5, 6, 7 \t\t8\n\n```\n\nLSTMs expect each sample in the dataset to have two dimensions; the first is the number of time steps (in the above case it is 5), and the second is the number of observations per time step (in this case it is 1).\n\nBecause it is a regression type problem, we will use a linear activation function in the output layer and optimize the mean squared error loss function. We will also evaluate the model using the mean squared error (MAE) metric.\n\n## Define look_back period\n\nA \u201clookback period\u201d defines how many previous timesteps are used in order to predict the subsequent timestep. \n\nFor example if I set the lookback period is to 5, that that means that I am using the time steps at t-4, t-3, t-2, t-1, and t to predict the value at time t+1.\n\nFor my case below, I will be using a one-step prediction model.\n\n### Lookback period\n\n\n```py\n\nlookback = 5\n\nX_train, Y_train = dataset_generator_lstm(train, lookback)\n\nX_val, Y_val = dataset_generator_lstm(val, lookback)\n\n```\n\n### Quick note on Python slice notation for slicing an array\n\n- [1:5] is equivalent to \"from 1 to 5\" (5 not included)\n\n- [1:] is equivalent to \"1 to end\"\n\n- [len(a):] is equivalent to \"from length of a to end\"\n\nIn the below function I will generate the train_X array and train_y array for feeding into the LSTM","46461504":"# LSTM Predictions using testX and plotting line graph against Actual testY\n\nDue to scaling step done earlier with MinMaxScaler the predicted scale is between 0 and 1. \nNow, I need to transfer this scale to the original data scale (real value). for example:[0.58439621 0.58439621 0.58439621 ... 0.81262134 0.81262134 0.81262134], the pred answer transfer to :[250 100 50 60 .....]\nSo here I am going to use `inverse_transform` to Scale back the data to the original representation.","fbff83c2":"### And now reshape trainX and testX "}}