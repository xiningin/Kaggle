{"cell_type":{"eef4b426":"code","0fd3e90e":"code","f17568d2":"code","6b3e1780":"code","9875f842":"code","5b3aa551":"code","2f7eb480":"code","e1d00687":"code","3ab3e711":"code","7f8bf5e2":"code","cc8c61f6":"code","3abbfc1b":"code","cfb854a8":"code","afa71ef7":"code","7d4cf683":"code","a1bf176e":"code","75eecb24":"code","8503f1c4":"code","6d5b507a":"code","cf9bcd94":"code","9e994464":"code","6beedd03":"markdown","a6bb5e8f":"markdown","dca5396a":"markdown","6ab190cf":"markdown","0ae3f1f2":"markdown","b018719d":"markdown","4067754a":"markdown","74986379":"markdown","113e5f39":"markdown","d8c08534":"markdown","7310d306":"markdown"},"source":{"eef4b426":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fd3e90e":"data=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","f17568d2":"data.info()","6b3e1780":"data.head()#first five rows","9875f842":"data.describe()","5b3aa551":"print(data.loc[:,\"target\"].value_counts())#loc is label-based, which means that we have to specify the name of the rows and columns that we need to filter out\nsns.countplot(data[\"target\"])","2f7eb480":"x=data.drop([\"target\"],axis=1)\ny=data[\"target\"].values","e1d00687":"#Normalization\nX=(x-np.min(x))\/(np.max(x)-np.min(x))","3ab3e711":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=2)","7f8bf5e2":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\npredicton=knn.predict(x_test)\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","cc8c61f6":"score_list=[]\nfor i in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"K values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","3abbfc1b":"from sklearn.linear_model import LogisticRegression\nlogisticR= LogisticRegression()\nlogisticR.fit(x_train,y_train)","cfb854a8":"print(\"Train Accuracy: {}\".format(logisticR.score(x_train,y_train)))\nprint(\"Test Accuracy: {}\".format(logisticR.score(x_test,y_test)))","afa71ef7":"#SVM\nfrom sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(x_train,y_train)\n","7d4cf683":"print(\"Train Accuracy: {}\".format(svm.score(x_train,y_train)))\nprint(\"Test Accuracy: {}\".format(svm.score(x_test,y_test)))","a1bf176e":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train, y_train)\n","75eecb24":"print(\"Train Accuracy: {}\".format(nb.score(x_train,y_train)))\nprint(\"Test Accuracy: {}\".format(nb.score(x_test,y_test)))","8503f1c4":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n","6d5b507a":"print(\"Train Accuracy: {}\".format(dt.score(x_train,y_train)))\nprint(\"Test Accuracy: {}\".format(dt.score(x_test,y_test)))","cf9bcd94":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=100)#n_estimators is number of tree which we use\nrfc.fit(x_train,y_train)","9e994464":"print(\"Train Accuracy: {}\".format(rfc.score(x_train,y_train)))\nprint(\"Test Accuracy: {}\".format(rfc.score(x_test,y_test)))","6beedd03":"So we can see that 3 is best k value to knn algorithm.","a6bb5e8f":"Support Vector Machine","dca5396a":"The describe() method will do a quick statistical summary for every numerical column, as shown above.","6ab190cf":"Decision Tree Classification","0ae3f1f2":"Random Forest Classification","b018719d":"P(A|B)=(P(A|B)*P(A))\/P(B)\n\nP(A|B):Likelihood,\nP(A): Prior probability,\nP(B): marginal likelihood\n\n","4067754a":"K-NEAREST NEIGHBORS (KNN)","74986379":"We can see that our data has 14 features and 303 sample. 13 features are integer and 1 feature is float.","113e5f39":"Logistic Regression\n","d8c08534":"Naive Bayes Classification","7310d306":"Now we should find the best k value to best accuracy."}}