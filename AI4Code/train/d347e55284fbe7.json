{"cell_type":{"2d5824a3":"code","51c39887":"code","00c30ed7":"code","12e809a5":"code","7269dd0e":"code","60019888":"code","ec00cf28":"code","c06c1e50":"code","2b3f5664":"code","0f94befc":"code","1120d2bc":"code","b4caf06f":"code","f8cb71f5":"code","576725a9":"code","d58e6177":"code","ce0b642a":"code","530dc8ea":"code","f1e268ab":"code","d8d56f7a":"code","3293505f":"code","6210986c":"code","069f1aaa":"code","231e9fd2":"code","d63136e2":"code","ab2ca1ed":"code","80139f84":"code","33e6e561":"code","f0e18601":"code","84c6fd41":"code","f5421572":"code","907fcd30":"code","e69bb9e1":"code","62308b7c":"code","367621d3":"code","7c20a269":"code","5b675470":"code","8e0d7052":"code","4e1c2854":"code","d1770e87":"code","a433dfb2":"code","b84fb4e3":"markdown","81287e00":"markdown","4b61749c":"markdown","620ae936":"markdown","568bede9":"markdown","1972441d":"markdown","3a74b7e7":"markdown","980767b1":"markdown","632d8e46":"markdown","4ef36355":"markdown","01c88ea2":"markdown","a2031dde":"markdown","b8aecfaf":"markdown","cf4aa802":"markdown","cf2b0fbe":"markdown","2db5da0f":"markdown","c01468c2":"markdown","bde4f1ac":"markdown","3ebdf6d0":"markdown","4ba8650b":"markdown","659b5fd0":"markdown","10a790a3":"markdown","38f3abd3":"markdown","3175402a":"markdown","a125233f":"markdown","88571ec2":"markdown","df81db71":"markdown","3204f2ce":"markdown","20dd8655":"markdown","7ffe214b":"markdown","f4ca92cb":"markdown","d186be4e":"markdown","9cb09cc0":"markdown","a68304b8":"markdown","df167441":"markdown","73485445":"markdown","709e457f":"markdown","36ef4acb":"markdown","15c69b17":"markdown","272b0ec6":"markdown"},"source":{"2d5824a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #charting\nfrom scipy.stats import mode #statistics for slope\nfrom sklearn.metrics import mean_squared_error #error metric to optimise when we build a model\nfrom math import sqrt #Other math functions\nimport plotly.express as px #alternative charting function\nimport lightgbm as lgb #popular model choice\nimport seaborn as sns #alternative charting function\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51c39887":"#change display when printing .head for example the default settings only displays limited number of columns\npd.set_option('display.max_columns', 500)","00c30ed7":"#The data we will be using is a foreign exhange rates dataset kindly provided to Kaggle. \nforex_df = pd.read_csv('\/kaggle\/input\/foreign-exchange-rates-per-dollar-20002019\/Foreign_Exchange_Rates.csv',engine = 'python')\n","12e809a5":"#let's go ahead and preview the top or bottom n rows\nforex_df.tail(6)","7269dd0e":"#you can see that the currencies are 'objects' which effectively means they are strings. We will need to convert later on to numeric to enable calculations.\nforex_df.info()","60019888":"#What is the number of rows, and columns in this dataset?\nforex_df.shape","ec00cf28":"#create a list of all the currency columns in the dataset\ncurrency_list = ['AUSTRALIA - AUSTRALIAN DOLLAR\/US$','EURO AREA - EURO\/US$','NEW ZEALAND - NEW ZELAND DOLLAR\/US$','UNITED KINGDOM - UNITED KINGDOM POUND\/US$','BRAZIL - REAL\/US$','CANADA - CANADIAN DOLLAR\/US$','CHINA - YUAN\/US$','HONG KONG - HONG KONG DOLLAR\/US$','INDIA - INDIAN RUPEE\/US$','KOREA - WON\/US$','MEXICO - MEXICAN PESO\/US$','SOUTH AFRICA - RAND\/US$','SINGAPORE - SINGAPORE DOLLAR\/US$','DENMARK - DANISH KRONE\/US$','JAPAN - YEN\/US$','MALAYSIA - RINGGIT\/US$','NORWAY - NORWEGIAN KRONE\/US$','SWEDEN - KRONA\/US$','SRI LANKA - SRI LANKAN RUPEE\/US$','SWITZERLAND - FRANC\/US$','TAIWAN - NEW TAIWAN DOLLAR\/US$','THAILAND - BAHT\/US$']\n#cleanse data\nfor c in currency_list:\n    #ffill simply takes the previous row and applies it to the next row. We have conditioned this to only be applied to non numeric data.\n    forex_df[c] = forex_df[c].where(~forex_df[c].str.isalpha()).ffill()\n    #we then want to convert the currency columns into numeric so that we can apply functions to it.\n    forex_df[c] = pd.to_numeric(forex_df[c], errors='coerce') ","c06c1e50":"#Let's check that this actually did what we intended.\nforex_df.tail()","2b3f5664":"#let's check that the columns are now numeric, yep that worked!\nforex_df.info()","0f94befc":"#generate features\n\n# time features\nforex_df['date'] = pd.to_datetime(forex_df['Time Serie'])\nforex_df['year'] = forex_df['date'].dt.year\nforex_df['month'] = forex_df['date'].dt.month\nforex_df['week'] = forex_df['date'].dt.week\nforex_df['day'] = forex_df['date'].dt.day\nforex_df['dayofweek'] = forex_df['date'].dt.dayofweek\n\n","1120d2bc":"# lag features\nforex_df['lag_t1'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(1))\nforex_df['lag_t3'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(3))\nforex_df['lag_t7'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'].transform(lambda x: x.shift(7))\n","b4caf06f":"# lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR\/US$\"]:\n    forex_df['lag_t1_%s' % c] = forex_df[c].transform(lambda x: x.shift(1))","f8cb71f5":"# ratio lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR\/US$\"]:\n    forex_df['lag_t1_ratio_%s' % c] = forex_df['lag_t1']  \/ forex_df['lag_t1_' + c] ","576725a9":"forex_df.tail()","d58e6177":"#rolling features\n#mean\nforex_df['rolling_mean_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).mean()\nforex_df['rolling_mean_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).mean()\nforex_df['rolling_mean_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).mean()\nforex_df['rolling_mean_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).mean()\nforex_df['rolling_mean_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).mean()\nforex_df['rolling_mean_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).mean()\n\n#max\nforex_df['rolling_max_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).max()\nforex_df['rolling_max_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).max()\nforex_df['rolling_max_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).max()\nforex_df['rolling_max_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).max()\nforex_df['rolling_max_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).max()\nforex_df['rolling_max_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).max()\n\n#min\nforex_df['rolling_min_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).min()\nforex_df['rolling_min_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).min()\nforex_df['rolling_min_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).min()\nforex_df['rolling_min_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).min()\nforex_df['rolling_min_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).min()\nforex_df['rolling_min_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).min()\n\n#standard deviation\nforex_df['rolling_std_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).std()\nforex_df['rolling_std_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).std()\nforex_df['rolling_std_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).std()\nforex_df['rolling_std_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).std()\nforex_df['rolling_std_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).std()\nforex_df['rolling_std_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).std()\n\n#median\nforex_df['rolling_med_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).median()\nforex_df['rolling_med_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).median()\nforex_df['rolling_med_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).median()\nforex_df['rolling_med_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).median()\nforex_df['rolling_med_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).median()\nforex_df['rolling_med_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).median()","ce0b642a":"# exponential moving averages\nforex_df['rolling_ema_t1_t7'] = forex_df['lag_t1'].ewm(span=7,adjust=False).mean()\nforex_df['rolling_ema_t1_t14'] = forex_df['lag_t1'].ewm(span=14,adjust=False).mean()\nforex_df['rolling_ema_t1_t28'] = forex_df['lag_t1'].ewm(span=28,adjust=False).mean()\nforex_df['rolling_ema_t1_t90'] = forex_df['lag_t1'].ewm(span=90,adjust=False).mean()\nforex_df['rolling_ema_t1_t180'] = forex_df['lag_t1'].ewm(span=180,adjust=False).mean()\nforex_df['rolling_ema_t1_t360'] = forex_df['lag_t1'].ewm(span=360,adjust=False).mean()","530dc8ea":"#Take a quick look at the data over time now that we have some features to compare against:\n# This is a relatively easy method to plot multiple values on a line chart plus it allows you to dynamically interact with the chart\ndf_long=pd.melt(forex_df, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR\/US$', 'rolling_ema_t1_t7', 'rolling_mean_t1_t7', 'rolling_ema_t1_t360', 'rolling_med_t1_t360'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","f1e268ab":"#round the value to 0 decimals\nforex_df['lag_t1_round_0'] = forex_df['lag_t1'].round(0)\nforex_df['lag_t3_round_0'] = forex_df['lag_t3'].round(0)\nforex_df['lag_t7_round_0'] = forex_df['lag_t7'].round(0)\n\n#get the decimal place\nforex_df['lag_t1_dec'] = forex_df['lag_t1'] - forex_df['lag_t1_round_0']\nforex_df['lag_t3_dec'] = forex_df['lag_t3'] - forex_df['lag_t3_round_0']\nforex_df['lag_t7_dec'] = forex_df['lag_t7'] - forex_df['lag_t7_round_0']\n\n#round the value to 1 decimals, as the rounded value to 0 decimals is nearly always 1 in the case of AUD\/USD\nforex_df['lag_t1_round_1'] = forex_df['lag_t1'].round(1)\nforex_df['lag_t3_round_1'] = forex_df['lag_t3'].round(1)\nforex_df['lag_t7_round_1'] = forex_df['lag_t7'].round(1)","d8d56f7a":"#rolling mode of rounded figure\nforex_df['lag_t1_mode_7'] = forex_df['lag_t1_round_1'].rolling(window=7,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_14'] = forex_df['lag_t1_round_1'].rolling(window=14,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_28'] = forex_df['lag_t1_round_1'].rolling(window=28,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_90'] = forex_df['lag_t1_round_1'].rolling(window=90,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: mode(x)[0])","3293505f":"#frequency of mode\n","6210986c":"#ranges\nforex_df['rolling_range_t1_t7'] = forex_df['rolling_max_t1_t7'] - forex_df['rolling_min_t1_t7']\nforex_df['rolling_range_t1_t14'] = forex_df['rolling_max_t1_t14'] - forex_df['rolling_min_t1_t14']\nforex_df['rolling_range_t1_t28'] = forex_df['rolling_max_t1_t28'] - forex_df['rolling_min_t1_t28']\nforex_df['rolling_range_t1_t90'] = forex_df['rolling_max_t1_t90'] - forex_df['rolling_min_t1_t90']\nforex_df['rolling_range_t1_t180'] = forex_df['rolling_max_t1_t180'] - forex_df['rolling_min_t1_t180']\nforex_df['rolling_range_t1_t360'] = forex_df['rolling_max_t1_t360'] - forex_df['rolling_min_t1_t360']","069f1aaa":"#coefficient of variation - the ratio of standard deviation to mean\nforex_df['rolling_coefvar_t1_t7'] =  forex_df['rolling_std_t1_t7'] \/ forex_df['rolling_mean_t1_t7']\nforex_df['rolling_coefvar_t1_t14'] = forex_df['rolling_std_t1_t14'] \/ forex_df['rolling_mean_t1_t14']\nforex_df['rolling_coefvar_t1_t28'] = forex_df['rolling_std_t1_t28'] \/ forex_df['rolling_mean_t1_t28']\nforex_df['rolling_coefvar_t1_t90'] = forex_df['rolling_std_t1_t90'] \/ forex_df['rolling_mean_t1_t90']\nforex_df['rolling_coefvar_t1_t180'] = forex_df['rolling_std_t1_t180'] \/ forex_df['rolling_mean_t1_t180']\nforex_df['rolling_coefvar_t1_t360'] = forex_df['rolling_std_t1_t360'] \/ forex_df['rolling_mean_t1_t360']","231e9fd2":"#ratio of change to standard deviation\n#I like this because if the currency is normally volatile (high std dev), then a change in the rolling mean may be normal. \n#On the other hand if the currency is not normally volatile (low std dev), then it adds weight to any changes observed\nforex_df['rolling_meanstd_t1_t14'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t14']) \/ forex_df['rolling_std_t1_t14']\nforex_df['rolling_meanstd_t1_t28'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t28']) \/ forex_df['rolling_std_t1_t28']\nforex_df['rolling_meanstd_t1_t90'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t90']) \/ forex_df['rolling_std_t1_t90']\nforex_df['rolling_meanstd_t1_t180'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t180']) \/ forex_df['rolling_std_t1_t180']\nforex_df['rolling_meanstd_t1_t360'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t360']) \/ forex_df['rolling_std_t1_t360']","d63136e2":"#cardinality\nforex_df['lag_t1_card_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: np.unique(x).shape[0])\nforex_df['lag_t1_card_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: np.unique(x).shape[0])","ab2ca1ed":"#moving average crossover trends, 1 = positive, 0 = negative\nforex_df['lag_t1_trend_7'] = np.where(forex_df['lag_t1'] >= forex_df['rolling_ema_t1_t7'],1,0)\nforex_df['lag_t1_trend_14'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t14'],1,0)\nforex_df['lag_t1_trend_28'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t28'],1,0)\nforex_df['lag_t1_trend_90'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t90'],1,0)\nforex_df['lag_t1_trend_180'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t180'],1,0)\nforex_df['lag_t1_trend_360'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t360'],1,0)","80139f84":"#number of crossovers last n days\nforex_df['lag_t1_no_crossover_7'] = forex_df['lag_t1_trend_7'].rolling(window=7,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_14'] = forex_df['lag_t1_trend_14'].rolling(window=14,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_28'] = forex_df['lag_t1_trend_28'].rolling(window=28,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_90'] = forex_df['lag_t1_trend_90'].rolling(window=90,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_180'] = forex_df['lag_t1_trend_180'].rolling(window=180,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_360'] = forex_df['lag_t1_trend_360'].rolling(window=360,min_periods=1).sum()","33e6e561":"#decay","f0e18601":"#slope or 1st derivative\nforex_df['lag_t1_slope_7'] = forex_df['lag_t1'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_slope_14'] = forex_df['lag_t1'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_slope_28'] = forex_df['lag_t1'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_slope_90'] = forex_df['lag_t1'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_slope_180'] = forex_df['lag_t1'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_slope_360'] = forex_df['lag_t1'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values","84c6fd41":"#2nd derivative, slope of the 1st derivative, again for detecting trend changes\nforex_df['lag_t1_deriv2_7'] = forex_df['lag_t1_slope_7'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_deriv2_14'] = forex_df['lag_t1_slope_7'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_deriv2_28'] = forex_df['lag_t1_slope_7'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_deriv2_90'] = forex_df['lag_t1_slope_7'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_deriv2_180'] = forex_df['lag_t1_slope_7'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_deriv2_360'] = forex_df['lag_t1_slope_7'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values\n","f5421572":"forex_df.shape","907fcd30":"#We have a heap of features:\nlist(forex_df.columns)","e69bb9e1":"\n#Create a list of the features to drop, as previously mentioned we can't use the feature from today else it would cause target leakage - the model knows something that it can't know in advance.\nuseless_cols = ['Unnamed: 0', \n                \"date\", \n                'AUSTRALIA - AUSTRALIAN DOLLAR\/US$',\n                'Time Serie', \n                'EURO AREA - EURO\/US$',\n                 'NEW ZEALAND - NEW ZELAND DOLLAR\/US$',\n                 'UNITED KINGDOM - UNITED KINGDOM POUND\/US$',\n                 'BRAZIL - REAL\/US$',\n                 'CANADA - CANADIAN DOLLAR\/US$',\n                 'CHINA - YUAN\/US$',\n                 'HONG KONG - HONG KONG DOLLAR\/US$',\n                 'INDIA - INDIAN RUPEE\/US$',\n                 'KOREA - WON\/US$',\n                 'MEXICO - MEXICAN PESO\/US$',\n                 'SOUTH AFRICA - RAND\/US$',\n                 'SINGAPORE - SINGAPORE DOLLAR\/US$',\n                 'DENMARK - DANISH KRONE\/US$',\n                 'JAPAN - YEN\/US$',\n                 'MALAYSIA - RINGGIT\/US$',\n                 'NORWAY - NORWEGIAN KRONE\/US$',\n                 'SWEDEN - KRONA\/US$',\n                 'SRI LANKA - SRI LANKAN RUPEE\/US$',\n                 'SWITZERLAND - FRANC\/US$',\n                 'TAIWAN - NEW TAIWAN DOLLAR\/US$',\n                 'THAILAND - BAHT\/US$']\n\n#define train columns to use in model\ntrain_cols = forex_df.columns[~forex_df.columns.isin(useless_cols)]\n\n#Let's simply use historical data up until Oct 2019\nx_train = forex_df[forex_df['date'] <= '2019-10-31']\n#The variable we want to predict is AUD to USD rate.\ny_train = x_train['AUSTRALIA - AUSTRALIAN DOLLAR\/US$']\n\n#The LGBM model needs a train and validation dataset to be fed into it, let's use Nov 2019\nx_val = forex_df[(forex_df['date'] > '2019-10-31') & (forex_df['date'] <= '2019-11-30')]\ny_val = x_val['AUSTRALIA - AUSTRALIAN DOLLAR\/US$']\n\n#We shall test the model on data it hasn't seen before or been used in the training process\ntest = forex_df[(forex_df['date'] > '2019-12-01')]\n\n#Setup the data in the necessary format the LGB requires\ntrain_set = lgb.Dataset(x_train[train_cols], y_train)\nval_set = lgb.Dataset(x_val[train_cols], y_val)\n \n\n","62308b7c":"#Set the model parameters\nparams = {\n        \"objective\" : \"regression\", # regression is the type of business case we are running\n        \"metric\" :\"rmse\", #root mean square error is a standard metric to use\n#         \"force_row_wise\" : True,\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n#         \"sub_feature\" : 0.8,\n#         \"sub_row\" : 0.75,\n#         \"bagging_freq\" : 1,\n        \"lambda_l1\" : 0.1, #lambda_l1 worked better than l2 in this case, as we have high number of features this makes sense (L1 or Lasso reduces some terms to 0 weight, whereas L2 or ridge includes all)\n#         \"nthread\" : 4\n        'verbosity': 1, \n#         'num_iterations' : 300,\n        'num_leaves': 100, # minimum number of leaves in each boosting round\n        \"min_data_in_leaf\": 25, #minimum amount of data in the leaf nodes or last value of the tree\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n#         \"max_bin\" = \n        \"sub_sample\" : 0.025, #sampling feature to reduce overfitting\n#         \"boosting\":\"dart\",\n}","367621d3":"#Run the model\nm_lgb = lgb.train(params, train_set, num_boost_round = 2500, valid_sets = [train_set, val_set], verbose_eval = 50)","7c20a269":"#plot feature importance\nfeature_imp = pd.DataFrame({'Value':m_lgb.feature_importance(),'Feature':train_cols})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","5b675470":"#generate predictions on test data\ny_pred = m_lgb.predict(test[train_cols])\ntest['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred'] = y_pred","8e0d7052":"test.head()","4e1c2854":"#view the test data in chart form\ndf_long=pd.melt(test, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR\/US$', 'AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","d1770e87":"#RSME metric\nrms = sqrt(mean_squared_error(test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$'], test['AUSTRALIA - AUSTRALIAN DOLLAR\/US$_pred']))","a433dfb2":"rms","b84fb4e3":"### 4. Generate Features","81287e00":"We may want to add in a ratio as the raw value for GBP may not be as stable as a ratio value","4b61749c":"### 7. Model Interpretation and Performance","620ae936":"Mode is often overlooked, don't forget to give it a try when tackling your next problem","568bede9":"### Summary","1972441d":"This notebook is useful for absolute beginners who are new to Python, through to intermediate users who I will introduce some more advanced features. The code deliberately avoids functions where possible to keep things easy to read for all levels.","3a74b7e7":"Cardinality is the number of unique values, e.g. [0,0,0,1,1] has 2 unique values","980767b1":"#### 4.5 Trends","632d8e46":"#### 4.3 Rolling Features","4ef36355":"### 6.Model","01c88ea2":"If the shorter moving average crosses over a longer one, it could be a trend indicator","a2031dde":"#### 4.1 Date Features","b8aecfaf":"So let's get straight into it!","cf4aa802":"You can see that there is 'ND' in the above example for 25th December, markets closed for Christmas, so we could either drop this or apply some cleansing to enable us to convert to a numeric field.","cf2b0fbe":"So let's go ahead and create a bunch of rolling features across different days and metrics","2db5da0f":"We will also use other countries values from yesterday to aid in predicting today's data","c01468c2":"#### 7.1 Model Interpretation","bde4f1ac":"#### 4.2 Lag features","3ebdf6d0":"### 5. Prepare Dataset for Model","4ba8650b":"Purpose is to practice generating features on timeseries data on an easy dataset that does not run into memory issues.\nI am only going to predict 1 currency, the AUD\/USD rate, and use other countries as inputs. \nOtherwise you could just use pd.melt to stack the countries in order to generate predictions for all, if you want to see an example of this let me know in the comments.","659b5fd0":"### 3. Import & Cleanse Data","10a790a3":"#### 4.4 Other Features - Decimals, Rounding, Mode, Coefficient of Variation","38f3abd3":"We can see that the model generally captures trends well, there will always be spikes and volatility that is hard to predict.","3175402a":"Light GBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n    Faster training speed and higher efficiency.\n\n    Lower memory usage.\n\n    Better accuracy.\n\n    Support of parallel and GPU learning.\n\n    Capable of handling large-scale data.\n","a125233f":"Thanks for viewing this notebook if you made it this far, I hope you got some ideas and please share in comments below any suggestions or questions you may have to improve and learn on your journey!","88571ec2":"It's interesting to try decimal or rounded features if you remember, it can often give a small boost to results","df81db71":"#### 5.1 Drop unnecessary columns","3204f2ce":"#### 7.2 Predictions on Test Data for out of time performance","20dd8655":"#### 7.3 Model Performance","7ffe214b":"### 2. Install Packages","f4ca92cb":"#### 3.1 Cleanse data and Convert to Numeric Format","d186be4e":"Exponential moving averages provide more weight to recent values, which in finance are generally useful. ","9cb09cc0":"note that the value for 24th Dec is carried forward to 25th Dec for those countries, but not for others that did not have this holiday.","a68304b8":"This shows how many times the feature was used by the model. \n<br>We can see that the model did like a lot of the slope and derivative type features generated at the end.\n<br>The model also liked the ratio of other currencies a fair amount. NB when I ran with these excluded it did not make much difference to performance. \n","df167441":"We need to shift the data by 1 or more days, so that we can use yesterday's data to predict today and so on. Later on we will remove today's data as we don't want to cause 'data leakage' whereby our model has information available to it that is not known at the time, which would not help it to work in practice.","73485445":"Light GBM will be used to build this model.","709e457f":"### 1. Timeseries Practice with Forex Dataset","36ef4acb":"About the dataset, the data is foreign exchange rates from 2000 to 2019. \nThis data is reasonably clean and requires minimal data preparation and cleansing. We want to be able to make a prediction on the daily AUD\/USD rate at the end of this notebook.\n","15c69b17":"With rolling features you can set min_periods as 1, that way you don't lose that much data (if you were to drop nulls in a subsequent step) as for example a 7 day rolling average the previous 6 days would be 'n\/a'. This may help especially for longer lags e.g.365 days.","272b0ec6":"Generally with timeseries there are two main approaches. One being getting the data into a stationary format, accounting for the trend and seasonality. You then apply more traditional models you may have come across such as 'ARIMA', 'GARCH' etc. In this case, we won't be doing that, but will be applying machine learning models to the data directly without the need for this step which is simpler."}}