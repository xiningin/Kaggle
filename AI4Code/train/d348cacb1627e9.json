{"cell_type":{"7504981a":"code","6029fbea":"code","95417d6e":"code","e6fc92ca":"code","6a7ed7c2":"code","229a5de3":"code","bb121149":"code","14ff017d":"code","47f00259":"code","09967aa6":"code","844190a1":"code","4f9f9e2f":"code","05ae2ecb":"code","7a69733e":"code","5c69b72c":"code","1e88923c":"code","a5382f15":"code","7ef98bfa":"markdown","2ef1b60e":"markdown","86659733":"markdown","a5112acd":"markdown","75d7e5bf":"markdown","050c5f88":"markdown","654bb48b":"markdown","8eaf9b86":"markdown","b199e646":"markdown","5d5cb354":"markdown","28d5b78f":"markdown","6186c83a":"markdown","67707eec":"markdown","a8f8e46b":"markdown","7b55faec":"markdown","0de60950":"markdown","b94f9b9f":"markdown","c7caa016":"markdown","2a97e8a9":"markdown","3ac37167":"markdown","b7d793fa":"markdown","82ebd2f4":"markdown","ceaf869f":"markdown","1d56c9a1":"markdown","b7f5df15":"markdown","d4e7837b":"markdown"},"source":{"7504981a":"# Para tener auto-completion\n%config Completer.use_jedi = False\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import Sequential, layers, Input\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6029fbea":"# CELDA 2\n\n# Cargar del dataset para entrenamiento\ndf = pd.read_csv('..\/input\/https-www-kaggle-com-c-dcic-ia-2021-proyecto-ad\/train.csv')\n\nprint(\"Cantidad de ejemplos: {}\\nCantidad de columnas: {}\".format(df.shape[0], df.shape[1]))\n\ndf.head(5)","95417d6e":"# CELDA 3\n\n# Vemos informaci\u00f3n b\u00e1sica sobre las columnas del dataset\nprint(df.info(memory_usage = False))\n\nprint(\"-\"*50)\n\n# Columnas que tienen valores nulos (NaN)\nprint(\"Columnas con valores nulos:\")\nfor column in df.columns:\n    if df[column].isna().sum() > 0:\n        print(\"-> {} => {} nulos\".format(column, df[column].isna().sum()))\n\nprint(\"-\"*50)\n\n# Informaci\u00f3n sobre columnas categ\u00f3ricas\nprint(\"Columnas categ\u00f3ricas:\")\nfor column in df.columns:\n    if df[column].dtype == 'object':\n        values = df[column].unique()\n        print(\"-> {} => {} valores distintos\".format(column, len(df[column].unique())))\n\nprint(\"-\"*50)\n\n# Distribuci\u00f3n de los ejemplos (filas del dataset) para cada resultado del match\nprint(\"Distribuci\u00f3n de target (cantidad de ejemplos):\")\nprint(df['match_result'].value_counts())","e6fc92ca":"# CELDA 4\n\n# Informaci\u00f3n de los atributos num\u00e9ricos\ndf.describe()","6a7ed7c2":"# CELDA 5\n\n# Informaci\u00f3n de los atributos categoricos\ndf.describe(include = ['object'])","229a5de3":"# CELDA 6\n\n# Informacion de los posibles valores de los atributos categoricos\n\n# df['country'].value_counts() # Nominal\n# df['league'].value_counts() # Nominal\n# df['season'].value_counts() # Nominal\n# df['home_team'].value_counts() # Nominal\n# df['away_team'].value_counts() # Nominal\n\n# df['home_team_buildUpPlaySpeedClass'].value_counts() # Ordinal\ndf['home_team_buildUpPlayPositioningClass'].value_counts() # Nominal\n# df['home_team_chanceCreationShootingClass'].value_counts() # Ordinal\n# df['home_team_chanceCreationPositioningClass'].value_counts() # Nominal\n# df['home_team_defenceTeamWidthClass'].value_counts() # Ordinal\n# df['home_team_defenceDefenderLineClass'].value_counts() # Nominal\n\n# df['away_team_buildUpPlaySpeedClass'].value_counts() # Ordinal\n# df['away_team_buildUpPlayPositioningClass'].value_counts() # Nominal\n# df['away_team_chanceCreationShootingClass'].value_counts() # Ordinal\n# df['away_team_chanceCreationPositioningClass'].value_counts() # Nominal\n# df['away_team_defenceTeamWidthClass'].value_counts() # Ordinal\n# df['away_team_defenceDefenderLineClass'].value_counts() # Nominal","bb121149":"# CELDA 7\n\n# Obtenemos el valor promedio de las columnas home_team_buildUpPlayDribbling y away_team_buildUpPlayDribbling\nmean_home_team_buildUpPlayDribbling = df['home_team_buildUpPlayDribbling'].mean()\nmean_away_team_buildUpPlayDribbling = df['away_team_buildUpPlayDribbling'].mean()\n\nprint(mean_home_team_buildUpPlayDribbling)\nprint(mean_away_team_buildUpPlayDribbling)\nprint('-'*50)\n\n# Completamos los nulos de las columnas con los valores obtenidos.\n\ndf['home_team_buildUpPlayDribbling'] = df['home_team_buildUpPlayDribbling'].fillna(mean_home_team_buildUpPlayDribbling)\ndf['away_team_buildUpPlayDribbling'] = df['away_team_buildUpPlayDribbling'].fillna(mean_away_team_buildUpPlayDribbling)\n\nprint(\"Verificando valores nulos:\")\nprint(df.isna().sum())","14ff017d":"# CELDA 8\n\nmapeo = {'Slow' : 0, 'Balanced' : 1, 'Fast' : 2}\n# Modificamos la columna home_team_buildUpPlaySpeedClass y away_team_buildUpPlaySpeedClass a partir del mapeo anterior\ndf['home_team_buildUpPlaySpeedClass'] =  df['home_team_buildUpPlaySpeedClass'].map(mapeo)\ndf['away_team_buildUpPlaySpeedClass'] =  df['away_team_buildUpPlaySpeedClass'].map(mapeo)\n\nmapeo = {'Little' : 0, 'Normal' : 1, 'Lots' : 2}\n# Modificamos la columna home_team_chanceCreationShootingClass y away_team_chanceCreationShootingClass a partir del mapeo anterior\ndf['home_team_chanceCreationShootingClass'] =  df['home_team_chanceCreationShootingClass'].map(mapeo)\ndf['away_team_chanceCreationShootingClass'] =  df['away_team_chanceCreationShootingClass'].map(mapeo)\n\nmapeo = {'Narrow' : 0, 'Normal' : 1, 'Wide' : 2}\n# Modificamos la columna home_team_defenceTeamWidthClass y away_team_defenceTeamWidthClass a partir del mapeo anterior\ndf['home_team_defenceTeamWidthClass'] =  df['home_team_defenceTeamWidthClass'].map(mapeo)\ndf['away_team_defenceTeamWidthClass'] =  df['away_team_defenceTeamWidthClass'].map(mapeo)","47f00259":"# CELDA 9\n\n# Verificamos que se hayan aplicado los mapeos definidos en la celda anterior\n\n# df[['home_team_buildUpPlaySpeedClass', 'away_team_buildUpPlaySpeedClass']]\n# df[['home_team_chanceCreationShootingClass', 'away_team_chanceCreationShootingClass']]\n# df[['home_team_defenceTeamWidthClass', 'away_team_defenceTeamWidthClass']]","09967aa6":"# CELDA 10\n\n# OneHotEncoder\ndf = pd.get_dummies(data = df, columns = ['home_team_buildUpPlayPositioningClass', \n                                          'home_team_chanceCreationPositioningClass',\n                                          'home_team_defenceDefenderLineClass',\n                                          'away_team_buildUpPlayPositioningClass', \n                                          'away_team_chanceCreationPositioningClass',\n                                          'away_team_defenceDefenderLineClass'])\n\n\n\n# df.describe(include = ['object'])\nprint(df.info(memory_usage = False))","844190a1":"# CELDA 11\n\ncolumnas_eliminar = ['id', 'country', 'league', 'season', 'home_team', 'away_team']\n\ndf = df.drop(columns = columnas_eliminar)\n\nprint(\"Cantidad de ejemplos: {}\\nCantidad de columnas: {}\".format(df.shape[0], df.shape[1]))\ndf.head(3)","4f9f9e2f":"# CELDA 12\n\nprint(df.info(memory_usage = False))","05ae2ecb":"# CELDA 13\n\n# Separamos la columna target en una nueva variable. El nombre es 'y' ya que es el resultado de una funci\u00f3n y = f(x)\ny = np.asarray(df['match_result'])\nprint(\"Etiquetas: {}\".format(y))\nprint(\"-\"*30)\n\n# Eliminamos la columna 'match_result' del dataframe ya que el resultado del partido no predice el resultado del partido, es decir no tiene informaci\u00f3n \u00fatil para predecir.\ndf = df.drop(columns = ['match_result'])\n\n'''\nSi bien no es necesario hacer el encoding para el \u00c1rbol de Decisi\u00f3n, pero de todas lo hacemos ya que ser\u00e1 de utilidad para la RNA \ny de esta manera somos uniformes a la hora de realizar el pre-procesamiento de los datos para ambos algoritmos de aprendizaje.\n'''\n# OneHotEncoder\nenc = OneHotEncoder(handle_unknown = 'ignore')\n\n# La funci\u00f3n fit_transform devuelve una transformaci\u00f3n de los datos que recibe. \n# Los transforma generando una lista de listas, donde cada sub-lista esta codificando uno de los 3 posibles valores (-1, 0, 1).\ny_enc = enc.fit_transform(y.reshape(-1, 1)).toarray()\n\nprint(\"Etiquetas (one-hot encoding):\\n{}\".format(y_enc))\nprint(\"-\"*30)\n\nprint(\"Mapeo del encoder:\")\nfor val, codif in zip(enc.categories_[0].tolist(), enc.transform(enc.categories_[0].reshape(-1, 1)).toarray()):\n    print(\"Label: {} ==> Codificaci\u00f3n One-Hot ==> {}\".format(val, codif))","7a69733e":"# CELDA 14\n\n# X son todos los atributos que no son el target (es decir, match_result)\n# Y es la columna target (match_result)\n\ntest_size = 0.2\n\nX_train, X_val, y_train, y_val = train_test_split(df, y_enc, test_size = test_size, random_state = 10)\n\nprint(\"Datos de entrenamiento:\\n\\tCantidad de ejemplos (filas): {}\\n\\tCantidad de atributos (columnas): {}\\n\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"Datos de validaci\u00f3n:\\n\\tCantidad de ejemplos (filas): {}\\n\\tCantidad de atributos (columnas): {}\".format(X_val.shape[0], X_val.shape[1]))","5c69b72c":"# CELDA 15\n\n# Cada profundidad nos dar\u00e1 un \u00e1rbol diferente.\nprofundidades = [1, 3, 8, 12, 15, 18, 20, 23, 25, 27, 35, 50, 100]\n\nfor prof in profundidades:\n    # Para cada profundidad, se ajusta un AD con esa profundidad m\u00e1xima.\n    clf = DecisionTreeClassifier(max_depth = prof)\n    \n    # Se ajusta el AD con los datos de entrenamiento\n    clf = clf.fit(X_train, y_train)\n    \n    # Se calcula un resultado usando el conjunto de validaci\u00f3n\n    predictions = clf.predict(X_val)\n    print(\"Con profundidad = {}, el resultado obtenido es: {:.3} (F1-score)\\n\".format(prof,\n        f1_score(np.argmax(y_val, axis = 1), np.argmax(predictions, axis = 1), average = 'macro')))","1e88923c":"# CELDA 16\n\n# Codificaremos los resultados de un partido de la siguiente forma: -1 gana visitante, 0 empate, 1 gana local.\nlista_resultados = ['Derrota', 'Empate', 'Victoria']\n\nMAX = 20\n\n# Creamos AD con profundidad m\u00e1xima MAX\nclf = DecisionTreeClassifier(max_depth = MAX)\n\n# Ajustamos el AD con los datos de entrenamiento\nclf = clf.fit(X_train, y_train)\n\n# Informaci\u00f3n sobre el AD aprendido\nprint(\"Profundidad del \u00e1rbol: {}\".format(clf.get_depth()))\nprint(\"Cantidad de nodos (internos + hojas): {}\".format(clf.tree_.node_count))\nprint(\"Cantidad de hojas: {}\".format(clf.get_n_leaves()))\nprint()\n\n# Predicciones sobre el conjunto de validaci\u00f3n\npredictions = clf.predict(X_val)\n\n# Reporte de clasificaci\u00f3n\nprint(classification_report(np.argmax(y_val, axis = 1), np.argmax(predictions, axis = 1), target_names = lista_resultados))","a5382f15":"# CELDA 17\n\ndf = pd.read_csv('..\/input\/https-www-kaggle-com-c-dcic-ia-2021-proyecto-ad\/test.csv')\n\n\n# Separamos las IDs de los ejemplos de test para generar la submission, de esa manera podemos indexar cada partido con su resultado\nids = df['id'] # IDs de los ejemplos del conjunto de test\n\n\n# Completar los valores nulos\nmean_home_team_buildUpPlayDribbling = df['home_team_buildUpPlayDribbling'].mean()\nmean_away_team_buildUpPlayDribbling = df['away_team_buildUpPlayDribbling'].mean()\n\ndf['home_team_buildUpPlayDribbling'] = df['home_team_buildUpPlayDribbling'].fillna(mean_home_team_buildUpPlayDribbling)\ndf['away_team_buildUpPlayDribbling'] = df['away_team_buildUpPlayDribbling'].fillna(mean_away_team_buildUpPlayDribbling)\n\n\n# Codificacion de las variables categoricas ordinales\nmapeo = {'Slow' : 0, 'Balanced' : 1, 'Fast' : 2}\n# Modificamos la columna home_team_buildUpPlaySpeedClass y away_team_buildUpPlaySpeedClass a partir del mapeo anterior\ndf['home_team_buildUpPlaySpeedClass'] =  df['home_team_buildUpPlaySpeedClass'].map(mapeo)\ndf['away_team_buildUpPlaySpeedClass'] =  df['away_team_buildUpPlaySpeedClass'].map(mapeo)\n\nmapeo = {'Little' : 0, 'Normal' : 1, 'Lots' : 2}\n# Modificamos la columna home_team_chanceCreationShootingClass y away_team_chanceCreationShootingClass a partir del mapeo anterior\ndf['home_team_chanceCreationShootingClass'] =  df['home_team_chanceCreationShootingClass'].map(mapeo)\ndf['away_team_chanceCreationShootingClass'] =  df['away_team_chanceCreationShootingClass'].map(mapeo)\n\nmapeo = {'Narrow' : 0, 'Normal' : 1, 'Wide' : 2}\n# Modificamos la columna home_team_defenceTeamWidthClass y away_team_defenceTeamWidthClass a partir del mapeo anterior\ndf['home_team_defenceTeamWidthClass'] =  df['home_team_defenceTeamWidthClass'].map(mapeo)\ndf['away_team_defenceTeamWidthClass'] =  df['away_team_defenceTeamWidthClass'].map(mapeo)\n\n\n# Codificacion de las variables categoricas nominales\ndf = pd.get_dummies(data = df, columns = ['home_team_buildUpPlayPositioningClass', \n                                          'home_team_chanceCreationPositioningClass',\n                                          'home_team_defenceDefenderLineClass',\n                                          'away_team_buildUpPlayPositioningClass', \n                                          'away_team_chanceCreationPositioningClass',\n                                          'away_team_defenceDefenderLineClass'])\n\n\n# Eliminaci\u00f3n de columnas irrelevantes\ncolumnas_eliminar = ['id', 'country', 'league', 'season', 'home_team', 'away_team']\ndf = df.drop(columns = columnas_eliminar)\n\n\n# Realizamos las predicciones utilizando el ultimo AD creado\npredictions = clf.predict(df)\npredictions = np.argmax(predictions, axis = 1)\npredictions = [x - 1 for x in predictions]\n\n\n    \n# Generar archivo de submission\nsubmission = pd.DataFrame({\n    'id': ids,\n    'match_result': predictions\n})\n\n\n# Guardamos el dataframe como un archivo csv.\nsubmission.to_csv('my_submission.csv', index = False)\n\n\n# Vemos las primeras l\u00edneas del archivo.\nsubmission.head()","7ef98bfa":"La funci\u00f3n np.argmax() codifica el match_result en [0, 1, 2] en vez de [-1, 0, 1]. Pero como lo hace tanto con **y_val** como con **predictions**, entonces las predicciones se siguen correspondiendo con los datos a validar.","2ef1b60e":"## Lectura: columna target\n\nAntes de entrenar un clasificador debemos **codificar la columna target** que en nuestro caso ser\u00e1 **match_result**.\n\nEn primer lugar, separamos la columna target del dataframe y luego la codificamos usando una **representaci\u00f3n One-Hot encoding**. Si bien realizar la codificaci\u00f3n no es necesario para un \u00c1rbol de Decisi\u00f3n, pero s\u00ed es necesario para una RNA, por lo tanto realizamos la codificaci\u00f3n ahora as\u00ed luego el pre-procesamiento de los datos para una RNA seguir\u00e1 los mismos pasos que para el AD.","86659733":"Para comenzar, lo que haremos ser\u00e1 **Importar** el dataset, utilizando el m\u00e9todo **read_csv()** de la libreria pandas pas\u00e1ndole por par\u00e1metro el archivo *train.csv*. El dataset se importa como una tabla de datos en pandas, que se denomina *Dataframe*.","a5112acd":"# Secci\u00f3n 2: Pre-Procesamiento de los datos\n\nLo que haremos a continuaci\u00f3n ser\u00e1:\n1. Trabajar con los valores nulos\n2. Convertir atributos categ\u00f3ricos en num\u00e9ricos\n3. Borrar aquellas columnas que no ayudan a descubrir un patr\u00f3n en los datos por diferentes motivos (por ejemplo toman un valor \u00fanico).","75d7e5bf":"En la celda anterior podemos ver que si bien no tenemos valores nulos, pero la columna 'season' tiene un \u00fanico valor posible, por lo cu\u00e1l habr\u00eda que eliminarla ya que no va a ayudar al algoritmo de aprendizaje a descubrir un patron en los datos. Esto se har\u00e1 en la secci\u00f3n de **Pre-procesamiento de los datos**","050c5f88":"### Transformando variables categ\u00f3ricas nominales\n\nLo que debemos hacer con las **variables categ\u00f3ricas nominales** es representarlas de forma num\u00e9rica en lugar de su forma de String, pero sin tener en cuenta ning\u00fan orden entre los valores.\n\nComo pudimos ver en las celdas 4 y 5, las variables nominales de inter\u00e9s en nuestro dataset son:\n\n1. home_team_buildUpPlayPositioningClass - 2 valores\n2. home_team_chanceCreationPositioningClass - 2 valores\n3. home_team_defenceDefenderLineClass - 2 valores\n4. away_team_buildUpPlayPositioningClass - 2 valores\n5. away_team_chanceCreationPositioningClass - 2 valores\n6. away_team_defenceDefenderLineClass - 2 valores\n\nPara hacer la transformaci\u00f3n utilizaremos **One-hot encoding**. Lo que haremos es utilizar la funci\u00f3n **get_dummies()** de la libreria **pandas** a la cual le pasamos por par\u00e1metro el dataset actual y todas aquellas columnas (variables) categ\u00f3ricas nominales sobre las cuales deseamos aplicar la transformaci\u00f3n. Como resultado, obtenemos un nuevo dataset que habr\u00e1 generado una columna por cada posible valor de cada variable nominal, colocando un 1 en cada columna que tiene el valor de cada variable nominal, y un 0 en las demas.","654bb48b":"En la celda anterior podemos ver que todos tienen desviaci\u00f3n estandar (std) distinta de 0, por lo que no hay columnas num\u00e9ricas que tengan valores \u00fanicos","8eaf9b86":"## 2. Variables Categ\u00f3ricas\n\nUna variable es categ\u00f3rica si toma valores de un conjunto finito de posibilidades. Sus valores suelen ser Strings, con lo cual debemos transformarlas a una representaci\u00f3n num\u00e9rica para que sirva como entrada al \u00c1rbol de Decisi\u00f3n.\n\nUna variable categ\u00f3rica puede ser de dos tipos:\n* **Nominal**: las categor\u00edas no tienen un orden inherente.\n* **Ordinal**: las categor\u00edas s\u00ed tienen un orden inherente, que puede ayudar a reconocer un patr\u00f3n en los datos.\n\n  \nLas variables categ\u00f3ricas que nos interesan son aquellas que tienen pocos valores diferentes, ya que codificar una variable categ\u00f3rica que tenga muchos valores diferentes dificulta el aprendizaje de un patr\u00f3n en los datos. Por esta raz\u00f3n representaremos de forma num\u00e9rica solamente a aquellas variables categ\u00f3ricas que aporten informaci\u00f3n significativa y que posean pocos valores diferentes posibles (ya que podr\u00eda ser que una variable tenga pocos valores diferentes pero que no sea considerada de inter\u00e9s, como por ejemplo la liga). Las variables categ\u00f3ricas sobre las cu\u00e1les no haremos la transformaci\u00f3n ser\u00e1n eliminadas del dataset m\u00e1s adelante, junto al motivo de por qu\u00e9 son eliminadas.\n\n\nPara obtener una representaci\u00f3n num\u00e9rica de una variable categ\u00f3rica podemos emplear dos estrategias:\n\n1. **One-hot encoding**: genera una columna por cada valor posible de la variable, poniendo un 1 en la columna adecuada dependiendo del valor (y cero en las dem\u00e1s). Se usa con variables nominales.\n\n2. **Ordinal encoding**: representa cada valor de la variable categ\u00f3rica con un n\u00famero entero, manteniendo el orden entre los valores.\nSe usa con variables ordinales.","b199e646":"### Transformando variables categ\u00f3ricas ordinales\n\nLo que debemos hacer con las **variables categ\u00f3ricas ordinales** es representarlas de forma num\u00e9rica en lugar de su forma de String, manteniendo el orden que exist\u00eda entre ellas.\n\n\nComo pudimos ver en las celdas 4 y 5, las variables ordinales de inter\u00e9s de nuestro dataset son:\n \n 1. home_team_buildUpPlaySpeedClass - Valores: 'Slow', 'Balanced', 'Fast'\n 2. home_team_chanceCreationShootingClass - Valores: 'Little', 'Normal', 'Lots'\n 3. home_team_defenceTeamWidthClass - Valores: 'Narrow', 'Normal', 'Wide'\n 4. away_team_buildUpPlaySpeedClass - Valores: 'Slow', 'Balanced', 'Fast'\n 5. away_team_chanceCreationShootingClass - Valores: 'Little', 'Normal', 'Lots'\n 6. away_team_defenceTeamWidthClass - Valores: 'Narrow', 'Normal', 'Wide'\n \n \nPara hacer la transformaci\u00f3n utilizaremos **Ordinal encoding**. Lo que haremos es definir un **mapeo** para cada una de las variables ordinales mencionadas anteriormente, asignando un n\u00famero a cada valor categ\u00f3rico posible de forma tal que se mantega el orden de las categor\u00edas originales. Luego aplicamos el mapeo definido a la variable ordinal que corresponda, para cambiar los Strings por valores num\u00e9ricos.","5d5cb354":"## Separando un conjunto de validaci\u00f3n\n\nNecesitamos una porci\u00f3n de los datos para verificar el funcionamiento del modelo aprendido. Este conjunto de datos se denomina **conjunto de validaci\u00f3n**. Un clasificador **no aprende de estos datos**, sino que se usan para verificar lo aprendido.\n\n\nDestinaremos un 20% de los datos totales para el entrenamiento, ya que si destinamos una menor cantidad (por ejemplo menos del 10%) el resultado obtenido en esos datos podr\u00eda no representar correctamente el funcionamiento del AD. Y si destinamos una mayor cantidad, el modelo podr\u00eda no aprender patrones generales de los datos y dar un mal resultado.","28d5b78f":"Ahora que todas las variables son de tipo num\u00e9rico y sin nulos, ya casi estamos en condiciones de usar los datos para poder crear el \u00c1rbol de Decisi\u00f3n.","6186c83a":"Si la columna target tiene sus resultados desbalanceados en el y_train (por ejemplo, tenemos muchos casos de empates pero muy pocos casos de victorias locales), entonces la medida de accuracy aunque sea muy alta podr\u00eda no ser de utilidad, ya que por ejemplo podr\u00edamos estar prediciendo muy bien los empates, pero predecir mal todas las victorias locales, pero como tenemos muchos empates entonces el accuracy ve que acertamos en la mayor\u00eda de las veces totales. Lo que hace F1 es regular este desbalance, y dar un resultado  teniendo en cuenta que hay muchos empates y pocas victorias locales.","67707eec":"# Secci\u00f3n 5 - Submit de Resultados","a8f8e46b":"En la celda anterior podemos ver que las columnas **home_team_buildUpPlayDribbling** y **away_team_buildUpPlayDribbling** tienen valores nulos, lo cu\u00e1l es un problema que explicaremos m\u00e1s adelante, junto a qu\u00e9 medidas tomaremos para solucionarlo.","7b55faec":"# Secci\u00f3n 3 - Desarrollo de un \u00c1rbol de Decisi\u00f3n","0de60950":"En la celda anterior, podemos ver que en lugar de tener 35 columnas ahora tenemos 41 columnas en total, ya que como codificamos 6 variables nominales, y cada una de esas variables ten\u00eda 2 valores posibles, entonces se agrego una columna m\u00e1s por cada una, lo que nos da un total de 6 columnas extra. De esta forma, podemos tener 2 columnas por cada variable nominal que nos permita codificar el valor. Esto es una forma de confirmar que el **one-hot encoding** se realiz\u00f3 correctamente.\n\n\nEn el siguiente paso, eliminaremos todas aquellas variables (num\u00e9ricas y categ\u00f3ricas) que no sean de utilidad para encontrar un patr\u00f3n en los datos.\n\nLas variables categ\u00f3ricas (ya sea ordinal o nominal) que no fueron transformadas ser\u00e1n eliminadas. Se explicar\u00e1 la raz\u00f3n de por qu\u00e9 fueron eliminadas, es decir, el motivo de por qu\u00e9 no ser\u00e1n de utilidad para encontrar un patr\u00f3n en los datos. Estas variables no fueron transformadas justamente porque iban a ser eliminadas, por lo que hubiese sido innecesario.\n\n","b94f9b9f":"### Ajustando un \u00c1rbol de Decisi\u00f3n\n\nLo que haremos en la celda siguiente ser\u00e1 tomar la profundidad del AD que obtuvo el mejor resultado en la celda anterior y almacenarla en la variable MAX.\n\nCon esta profundidad crearemos un nuevo AD con profundidad m\u00e1xima MAX que ser\u00e1 ajustado usando los datos de entrenamiento X_train y sus etiquetas y_train.\n\nLuego haremos predicciones sobre los datos de validaci\u00f3n (X_val) usando el AD entrenado.\n\nFinalmente, mostraremos los resultados obtenidos con estas predicciones. Para ello, compararemos las predicciones con las etiquetas de los datos de validaci\u00f3n (y_val)","c7caa016":"## 1. Valores nulos\n\nLa informaci\u00f3n anterior nos muestra que hay 2 columnas que toman valores nulos:\n\n* home_team_buildUpPlayDribbling - 39 nulos\n* away_team_buildUpPlayDribbling - 39 nulos\n\nLa alternativa que escogeremos para eliminarlos es completar los valores nulos de la siguiente manera:\n\n* *home_team_buildUpPlayDribbling* - Completamos los nulos con el valor promedio de la variable.\n* *away_team_buildUpPlayDribbling* - Completamos los nulos con el valor promedio de la variable.\n","2a97e8a9":"### 3. Eliminaci\u00f3n de columnas (atributos) irrelevantes\n\nDe la informaci\u00f3n que obtuvimos en la secci\u00f3n 1, podemos observar que las siguientes columnas no son relevantes por diferentes motivos y podemos eliminarlas:\n* **columna 'id'**\n* **columna 'country'**\n* **columna 'league'**\n* **columna 'season'**\n* **columna 'home_team'**\n* **columna 'away_team'**","3ac37167":"En la celda siguiente, a trav\u00e9s de la funci\u00f3n value_counts(), podemos ver los posibles valores y la cantidad de ocurrencia de los valores de una determinada variable categ\u00f3rica, y de esa manera inferir si se trata de una variable categ\u00f3rica ordinal o nominal. Hacer esto ser\u00e1 esencial para el **pre-procesamiento de los datos** a la hora de tener que decidir que tipo de encoding hacer, como veremos m\u00e1s adelante.","b7d793fa":"# Secci\u00f3n 1: Carga y Visualizaci\u00f3n de los datos","82ebd2f4":"**An\u00e1lisis de los datos:** \n\nEn las siguientes celdas de c\u00f3digo vamos a obtener m\u00e1s informaci\u00f3n acerca de los datos con los que estamos trabajando. En particular buscamos conocer:\n\n1. Para cada columna del dataframe, el tipo de la misma.\n\n En pandas, el tipo de un dato puede ser num\u00e9rico (*int64*, *float64*), categ\u00f3rico (*object*) o de tipo fecha (*datetime*). Para el \u00c1rbol de decisi\u00f3n vamos a necesitar que todas sus entradas sean num\u00e9ricas.\n\n2. **Aquellas columnas que tienen valores nulos**.\n\n3. La cantidad de valores que toman las **variables categ\u00f3ricas**.\n\n4. La cantidad de ejemplos para cada clase en la columna objetivo *match_result*.","ceaf869f":"### Probando distintos \u00c1rboles de Decisi\u00f3n\n\nLo que haremos a continuaci\u00f3n ser\u00e1 probar distintos \u00c1rboles de Decisi\u00f3n, asignando distintas profundidades a cada \u00e1rbol. Para evaluar qu\u00e9 tan bueno fue un determinado \u00c1rbol de Decisi\u00f3n, lo que haremos ser\u00e1 evaluarlo mediante una **m\u00e9trica**. Una m\u00e9trica es una funci\u00f3n matem\u00e1tica que nos permitir\u00e1 medir la performance del AD.  La m\u00e9trica que utlizaremos ser\u00e1 **F1**.","1d56c9a1":"En la siguiente celda, verificaremos que todas las columnas resultantes sean de tipo num\u00e9rico.","b7f5df15":"Un \u00e1rbol de decisi\u00f3n tiene las siguientes caracter\u00edsticas:\n\n* Los nodos internos se etiquetan con atributos que caracterizan a los ejemplos.\n* Los arcos salientes de un nodo etiquetado con un atributo A se etiquetan con cada valor posible de A.\n* Las hojas marcan una decisi\u00f3n o etiqueta de respuesta.\n\nEn las celdas siguientes presentamos una manera de construir un \u00e1rbol de decisi\u00f3n que \u00fanicamente recibe datos num\u00e9ricos. De ah\u00ed la importancia de haber transformado o eliminado las variables categ\u00f3ricas. Utilizaremos de entrenamiento (X_train) y obtendremos resultados sobre los datos de validaci\u00f3n (X_val).","d4e7837b":"Hasta ahora no hemos utilizado los datos del conjunto de test que brinda la competencia. Vamos a usarlos ahora para **calcular predicciones y enviarlas a la competencia**.\n\nPara los datos de test hay que hacer el **mismo pre-procesamiento que hicimos con los datos de entrenamiento**. \n\n\nLa \u00fanica diferencia que hay con el pre-procesamiento anterior es que no debemos separar un conjunto de validaci\u00f3n, ya que el funcionamiento del AD ya fue verificado, lo que haremos ahora ser\u00e1 **ponerlo en pr\u00e1ctica** con los datos del conjunto de test (en el archivo test.csv). Por lo tanto, esta vez haremos uso de **todos** los datos de entrenamiento (test.csv) para crear nuestro \u00e1rbol de decisi\u00f3n, tomando como profundidad del \u00e1rbol aquella profunidad que nos di\u00f3 un mejor resultado en la Secci\u00f3n 3 para predecir los datos que separamos en el conjunto de validaci\u00f3n. \n\n\nExplicar que es necesario hacer la transformacion de [0, 1, 2] a [-1, 0, 1] ya que test.csv tiene los resultados en [-1, 0, 1] a diferencia de y_val que los tenia en [0, 1, 2]\n\nDebido a que los resultados de las predicciones de los datos de test estan codificados como [-1, 0, 1], entonces a la lista de valores que devuelve **np.argmax()** si debemos transformarla, ya que esta \u00faltima tendr\u00e1 sus resultados utilizando los valores [0, 1, 2]."}}