{"cell_type":{"f5a46a9f":"code","9fd00992":"code","fabd1abf":"code","5026ced6":"code","57c952fc":"code","c7dec7f4":"code","9e4079d1":"code","1b1fead2":"code","d7841b14":"code","e71ec84d":"code","606b1c82":"code","4faaea16":"markdown","b7c9bbf1":"markdown","e6109fd3":"markdown","b9fe650a":"markdown","74dc202b":"markdown","efc8dd91":"markdown","c8690b8f":"markdown"},"source":{"f5a46a9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fd00992":"from typing import Union, Optional\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_probability as tfp\nimport numpy as np","fabd1abf":"@tf.function\ndef sparsemoid(inputs: tf.Tensor):\n    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n\n@tf.function\ndef identity(x: tf.Tensor):\n    return x","5026ced6":"class ODST(tf.keras.layers.Layer):\n    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.):\n        super(ODST, self).__init__()\n        self.initialized = False\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n    \n    def build(self, input_shape: tf.TensorShape):\n        feature_selection_logits_init = tf.zeros_initializer()\n        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)        \n        \n        feature_thresholds_init = tf.zeros_initializer()\n        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)\n        \n        log_temperatures_init = tf.ones_initializer()\n        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)\n        \n        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n        bin_codes = (tf.reshape(indices, (1, -1)) \/\/ tf.reshape(offsets, (-1, 1)) % 2)\n        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'),\n                                 trainable=False)\n        \n        response_init = tf.ones_initializer()\n        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'),\n                                 trainable=True)\n                \n    def initialize(self, inputs):        \n        feature_values = self.feature_values(inputs)\n        \n        # intialize feature_thresholds\n        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n                                                      self.threshold_init_beta)\n                         .sample([self.n_trees * self.depth]))\n        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n        \n        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n        \n        \n        # intialize log_temperatures\n        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n        \n        \n        \n    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n        # ^--[in_features, n_trees, depth]\n\n        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n        # ^--[batch_size, n_trees, depth]\n        \n        return feature_values\n        \n    def call(self, inputs: tf.Tensor, training: bool = None):\n        if not self.initialized:\n            self.initialize(inputs)\n            self.initialized = True\n            \n        feature_values = self.feature_values(inputs)\n        \n        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n\n        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n        # ^--[batch_size, n_trees, depth, 2]\n\n        bins = sparsemoid(threshold_logits)\n        # ^--[batch_size, n_trees, depth, 2], approximately binary\n\n        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n        # ^--[batch_size, n_trees, depth, 2 ** depth]\n\n        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n        # ^-- [batch_size, n_trees, 2 ** depth]\n\n        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n        # ^-- [batch_size, n_trees, units]\n        \n        return tf.reduce_sum(response, axis=1)","57c952fc":"class NODE(tf.keras.Model):\n    def __init__(self, units: int = 1, n_layers: int = 1, link: tf.function = tf.identity, n_trees: int = 3, depth: int = 4, threshold_init_beta: float = 1., feature_column: Optional[tf.keras.layers.DenseFeatures] = None):\n        super(NODE, self).__init__()\n        self.units = units\n        self.n_layers = n_layers\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n        self.feature_column = feature_column\n        \n        if feature_column is None:\n            self.feature = tf.keras.layers.Lambda(identity)\n        else:\n            self.feature = feature_column\n        \n        self.bn = tf.keras.layers.BatchNormalization()\n        self.ensemble = [ODST(n_trees = n_trees,\n                              depth = depth,\n                              units = units,\n                              threshold_init_beta = threshold_init_beta) \n                         for _ in range(n_layers)]\n        \n        self.link = link\n        \n        \n    def call(self, inputs, training=None):\n        X = self.feature(inputs)\n        X = self.bn(X, training=training)\n        \n        for tree in self.ensemble:\n            H = tree(X)\n            X = tf.concat([X, H], axis=1)\n            \n        return self.link(H)","c7dec7f4":"CATEGORICAL_COLUMNS = ['line_stat', 'serv_type', 'serv_code',\n                       'bandwidth', 'term_reas_code', 'term_reas_desc',\n                       'with_phone_service', 'current_mth_churn']\nNUMERIC_COLUMNS = ['contract_month', 'ce_expiry', 'secured_revenue', 'complaint_cnt']\n\ndf = pd.read_csv('\/kaggle\/input\/broadband-customers-base-churn-analysis\/bbs_cust_base_scfy_20200210.csv').assign(complaint_cnt = lambda df: pd.to_numeric(df.complaint_cnt, 'coerce'))\ndf.loc[:, NUMERIC_COLUMNS] = df.loc[:, NUMERIC_COLUMNS].astype(np.float32).pipe(lambda df: df.fillna(df.mean())).pipe(lambda df: (df - df.mean())\/df.std())\ndf.loc[:, CATEGORICAL_COLUMNS] = df.loc[:, CATEGORICAL_COLUMNS].astype(str).applymap(str).fillna('')\ndf = df.groupby('churn').apply(lambda df: df.sample(df.churn.value_counts().min()))\ndf.head()","9e4079d1":"from sklearn.model_selection import train_test_split\n\n\nX, y = (df\n           .loc[:, NUMERIC_COLUMNS]\n           .astype('float32')\n           .join(pd.get_dummies(df.loc[:, CATEGORICAL_COLUMNS])),\n           df.churn == 'Y')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X.to_numpy(), y.to_numpy(), train_size=250000, test_size=250000)","1b1fead2":"node = NODE(n_layers=2, units=1, depth=2, n_trees=2, link=tf.keras.activations.sigmoid)\nnode.compile(optimizer='adam', loss='bce')\n\nnode.fit(x=X_train, y=y_train, validation_split=0.25, shuffle=True, batch_size=100, epochs=10)","d7841b14":"node.summary()","e71ec84d":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_train, node.predict(X_train) > 0.5)","606b1c82":"accuracy_score(y_valid, node.predict(X_valid) > 0.5)","4faaea16":"# Data\nAs an example dataset, we will be looking at a customer churn classification problem for broadband internet customers. The aim of this notebook is not to explore many complicated approaches to featur engineering but to explore the inner workings of tabnet. The main aim in choosing a dataset was for it to be reasonably large, at around 510125 observations, and to have mixed categorical, count and continuous data, as it common to tabular datasets. The only operations we performed to clean and resample the data was to ensure there was class balance before finding a random even train-validation split. ","b7c9bbf1":"To simplify Neural Oblivious Trees, they are essentially a lookup table, with a series of thresholds in the rows and columns. These thresholds are learned and made sparse with the sparsemax activation function. One difficulty in implementing Neural Oblivious Trees is in initializing the thresholds so that data is balanced across the splits when training starts. This involves sampling from the Beta distribution and finding a corresponding percentile. \n\nOne major advantage of NODE is that leafs effectively serve as embedding layers, allowing leaves to easily learn multiclass classification problems. To ensemble models, authors recommend using summing across trees at a particular layer and using that vector as input to later tree ensembles. This is an interesting proposition of this approach which sets it apart from out traditional ensembles. ","e6109fd3":"# Validation\nWe opted to initialize our models with 2 ensemble layers of 2 trees at a depth of 1. We used binary cross-entropy and a sigmoid activation function along with a small batch size of 100 samples. Unlike the original implementation with uses dropout at leaf nodes, we regularize our model with BatchNormalization provided an added beenfit in stabilizing our intitialization. This was shown to improve the stability of our model and the speed of our convergence in our initial experiments. Dropout may be included late in our work. ","b9fe650a":"\"Wouldn't it be great it CatBoost was differentiable...\" - Researchers from Yandex  \n\nFor Kagglers CatBoost has taken the competition space by storm, earning its stripes on sparse tabular data. CatBoost is fast and robust, with sensible defaults out-the-box; but wouldn't it be great if CatBoost was differentiable. Work on tree-like neural networks has a [long body of literature](http:\/\/www.cs.cornell.edu\/~oirsoy\/softtree.html). And many important authors have put their names to research looking to apply there methods to model [distillation and explainability](https:\/\/arxiv.org\/abs\/1711.09784). What makes CatBoost special is its use of [Oblivious Trees](https:\/\/www.youtube.com\/watch?v=8o0e-r0B5xQ). Unlike other tree method that consider every leaf at every depth, Oblivious trees ensure a feature is reused across all trees of a certain depth.  In [Neural Oblivious Decision Ensemble (NODE)](https:\/\/arxiv.org\/abs\/1909.06312) authors borrow attention breakthroughs from [EntMax](https:\/\/arxiv.org\/pdf\/1909.00015.pdf), to make soft oblivious decision trees more closely resemble our traditional greedy learners. By imagining trees as a set of attention mechanisms and allowing differentiable compute, these models steal performance from embeddings, shared feature representation and advances in pre-training, to achieve promising results on tabular data problems. \n\n![node](https:\/\/github.com\/Qwicen\/node\/raw\/master\/images\/densetree.png)","74dc202b":"# NODE\nUnlike in my previous work on TabNet, NODE authors provide a high quality and elegent [pytorch implementation](https:\/\/github.com\/Qwicen\/node) for use by the open-source community. While I would recommend users experiment first hand with the implementation, I opted to copy this implementation to Tensorflow 2.0 to allow users the ability to incorperate this architecture into existing code bases. ","efc8dd91":"You can see interestingly from these experiments how well NODE generalizes.  ","c8690b8f":"While NODE shares many properties of trees, the model appears highly parameters cause of how it stores information on splits and its gradient optimization. "}}