{"cell_type":{"59117cf1":"code","98bcb31f":"code","ac9c9e64":"code","980960ac":"code","1a31b10d":"code","d2872f7f":"code","3e95ca95":"code","64daf15d":"code","d8d765e7":"code","c4f8680f":"code","531d30be":"code","878ec9f5":"code","65b99bf3":"code","21f94d69":"code","db579374":"code","7a5dca75":"markdown","cd7bc270":"markdown","9ac0a7bf":"markdown","a88ac794":"markdown","78308f26":"markdown","293c1922":"markdown","3082f297":"markdown","3adad521":"markdown","5259e7a8":"markdown","6ce70646":"markdown","8178ec2e":"markdown","0bd6f019":"markdown","64883b2b":"markdown","46f0bd20":"markdown","1e8e5cf9":"markdown","3602955b":"markdown","0607c7d9":"markdown","ce032901":"markdown","46b434c2":"markdown"},"source":{"59117cf1":"################### Importing Libraries ######################\nimport pandas as pd\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.info()","98bcb31f":"############ Count number of Categorical and Numerical Columns ######################\ntrain_df = train_df.drop(columns=['Loan_ID']) ## Dropping Loan ID\ncategorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']\n\nprint(categorical_columns)\nnumerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\nprint(numerical_columns)\n","ac9c9e64":"### Data Visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfig,axes = plt.subplots(4,2,figsize=(12,15))\nfor idx,cat_col in enumerate(categorical_columns):\n    row,col = idx\/\/2,idx%2\n    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col])\n\n\nplt.subplots_adjust(hspace=1)","980960ac":"fig,axes = plt.subplots(1,3,figsize=(17,5))\nfor idx,cat_col in enumerate(numerical_columns):\n    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx])\n\nprint(train_df[numerical_columns].describe())\nplt.subplots_adjust(hspace=1)","1a31b10d":"#### Encoding categrical Features: ##########\ntrain_df_encoded = pd.get_dummies(train_df,drop_first=True)\ntrain_df_encoded.head()","d2872f7f":"########## Split Features and Target Varible ############\nX = train_df_encoded.drop(columns='Loan_Status_Y')\ny = train_df_encoded['Loan_Status_Y']\n\n################# Splitting into Train -Test Data #######\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)\n############### Handling\/Imputing Missing values #############\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\nimp_train = imp.fit(X_train)\nX_train = imp_train.transform(X_train)\nX_test_imp = imp_train.transform(X_test)\n","3e95ca95":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\n\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_train)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","64daf15d":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\n\nfor depth in range(1,20):\n    tree_clf = DecisionTreeClassifier(max_depth=depth)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n    \n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)\n","d8d765e7":"\nimport graphviz \nfrom sklearn import tree\n\ntree_clf = tree.DecisionTreeClassifier(max_depth = 3)\ntree_clf.fit(X_train,y_train)\ndot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())\ngraph = graphviz.Source(dot_data)\ngraph","c4f8680f":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\nmin_samples_leaf = []\nimport numpy as np\nfor samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    min_samples_leaf.append(samples_leaf)\n    \n\nTuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\nTuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n\nplot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","531d30be":"from sklearn.metrics import confusion_matrix\ntree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_test_imp)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","878ec9f5":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)\nrf_clf.fit(X_train,y_train)\ny_pred = rf_clf.predict(X_train)\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())\n","65b99bf3":"y_pred = rf_clf.predict(X_test_imp)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","21f94d69":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\n\ntrain_accuracies = []\ntrain_f1_scores = []\ntest_accuracies = []\ntest_f1_scores = []\nthresholds = []\n\n#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\nfor thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n    logreg_clf = LogisticRegression(solver='liblinear')\n    logreg_clf.fit(X_train,y_train)\n    \n    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n\n    train_acc = accuracy_score(y_train,y_pred_train)\n    train_f1 = f1_score(y_train,y_pred_train)\n    \n    y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n    \n    test_acc = accuracy_score(y_test,y_pred_test)\n    test_f1 = f1_score(y_test,y_pred_test)\n    \n    train_accuracies.append(train_acc)\n    train_f1_scores.append(train_f1)\n    test_accuracies.append(test_acc)\n    test_f1_scores.append(test_f1)\n    thresholds.append(thresh)\n    \n    \nThreshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\nThreshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n\nplot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","db579374":"thresh = 0.4 ### Threshold chosen from above Curves\ny_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\ny_pred = (y_pred_test_thresh > thresh).astype(int) \nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","7a5dca75":"Random Forest gives same results as Decision Tree Classifier.\nFinally, we will try Logistic Regression Model by sweeping threshold values.","cd7bc270":"From above plot, we will choose Min_Samples_leaf to 35 to improve test accuracy. \n\nLet's use this Decision Tree classifier on unseen test data and evaluate __Test Accuracy, F1 Score and Confusion Matrix__","9ac0a7bf":"Logistic Regression Confusion matrix is very similar to Decision Tree and Random Forest Classifier.\nIn this analysis, we did extensive analysis of input data and were able to achieve Test Accuracy of  __86 %__\n\n__If you like this Kernel please do Upvote !!__\n","a88ac794":"Hello !\nI am Ajay and  in this kernel we will explore loan approval dataset from Analytics Vidya competition.\n<br>Please __Upvote__ this kernel if you like content.\n### Problem Statement:\n\n__About Company__ <br>\nDream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n\n__Problem__ <br>\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n\n#### Dataset Description:\n\n| Variable | Description | \n|------|------|\n| Loan_ID | Unique Loan ID | \n| Gender | Male\/ Female | \n| Married | Applicant married (Y\/N) | \n| Dependents | Number of dependents | \n| Education | Applicant Education (Graduate\/ Under Graduate) | \n| Self_Employed | Self employed (Y\/N) | \n| ApplicantIncome | Applicant income | \n| CoapplicantIncome | Coapplicant income | \n| LoanAmount | Loan amount in thousands | \n| Loan_Amount_Term | Term of loan in months | \n| Credit_History | credit history meets guidelines | \n| Property_Area | Urban\/ Semi Urban\/ Rural | \n| Loan_Status | Loan approved (Y\/N) | \n","78308f26":"#### Overfitting Problem\nWe can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.\n\n#### First let's try tuning 'Max_Depth' of tree\n","293c1922":"### Preprocessing Data:\nInput data needs to be pre-processed before we feed it to model. Following things need to be taken care:\n1. Encoding Categorical Features.\n2. Imputing missing values","3082f297":"#### Random Forest: Test Data Evaluation","3adad521":"For Numercical Columns, there is no significant relation to Loan approval status.\n","5259e7a8":"### Model 1: Decision Tree Classifier\n","6ce70646":"From above tree, we could see that some of the leafs have less than 5 samples hence our classifier might overfit.\nWe can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 3","8178ec2e":"### Model 2: Random Forest Classifier\n","0bd6f019":"Logistic Regression does slightly better than Decision Tree and Random Forest.\n<br> Based on the above Test\/Train curves, we can keep threshold to 0.4. <br>\nNow Finally let's look at Logistic Regression Confusion Matrix","64883b2b":"### Model 3: Logistic Regression","46f0bd20":"##### Observations:\n1. We can see there are total 13 columns including target variable, all of them are self explanatory. \n2. We also see some missing values, lets take stock of missing columns and what are the possible values for categorical and numerical columns \n","1e8e5cf9":"#### Plots above convey following things about the dataset:\n1. Loan Approval Status: About 2\/3rd of applicants have been granted loan.\n2. Sex: There are more Men  than Women (approx. 3x) \n3. Martial Status: 2\/3rd of the population in the dataset is Marred; Married applicants are more likely to be granted loans.\n4. Dependents: Majority of the population have zero dependents and are also likely to accepted for loan.\n5. Education: About 5\/6th of the population is Graduate and graduates have higher propotion of loan approval\n6. Employment: 5\/6th of population is not self employed.\n7. Property Area: More applicants from Semi-urban and also likely to be granted loans.\n8. Applicant with credit history are far more likely to be accepted.\n9. Loan Amount Term: Majority of the loans taken are for 360 Months (30 years).\n\nNow, let's also analyze Numerical Columns:","3602955b":"#### Analyze values assigned to columns ","0607c7d9":"####  Mis-classifications\nIt can be seen that majority of the misclassifications are happening because of Loan Reject applicants being classified as Accept.\n\nLet's look into Random Forest Classifier if it can reduce mis-classifications","ce032901":"From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score\nOptimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7\n#### Visulazing Decision Tree with Max Depth = 3","46b434c2":"# Loan Approval Prediction: \n### EDA + Decision Tree, Random Forest & Logistic Regression Modeling"}}