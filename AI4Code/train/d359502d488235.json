{"cell_type":{"a9a0ecf5":"code","2d1d2197":"code","e7bba295":"code","2ab6ee43":"code","c21224d5":"code","238a8c1a":"code","30615d52":"code","6baf33d3":"code","1469403c":"code","0d7e57d7":"code","0527acaa":"code","cd60c42d":"code","23dc49a0":"markdown","e83341ae":"markdown","c71b868b":"markdown","a8759778":"markdown","13361610":"markdown","c90f9b3a":"markdown","ff11bedd":"markdown","74d36363":"markdown","b9cee7dd":"markdown"},"source":{"a9a0ecf5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d1d2197":"# plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# ML Libraries\nfrom xgboost import XGBRegressor\n# Omptimization Library\nimport optuna\n# Sk-Learn metrics and model selection\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer","e7bba295":"X_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\nX_test  = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsub     = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","2ab6ee43":"y_train = X_train.loss\n\nX_train.drop(labels=['id', 'loss'], axis=1, inplace=True)\n\nX_test.drop( labels=['id'], axis=1, inplace=True)","c21224d5":"root_mean_square_error = make_scorer(mean_squared_error, greater_is_better=False, **{'squared' : False})","238a8c1a":"sKF = KFold(n_splits=4, shuffle=True, random_state=3141)","30615d52":"def Objective(trial):\n    # The Paramameter Dict\n    param_grid = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 100, 5000),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 15),\n        'learning_rate'   : trial.suggest_float('learning_rate',0.0001, 2, log=True),\n        'gamma'           : trial.suggest_float('gamma', 0, 1),\n        'subsample'       : trial.suggest_float('subsample', 0.2, 1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1),\n        'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 100),\n        'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 100),\n        'tree_method'     : 'gpu_hist',\n        'random_state'    : 3141,\n        'verbosity'       : 1,\n    }\n    # Form xgb model with param dict fed.\n    regr  = XGBRegressor(**param_grid)\n    # mean of 4 Kfold shuffeled neg RMSE \n    SCORE = cross_val_score(regr,X_train,y_train, scoring=root_mean_square_error, cv=sKF).mean()\n    return SCORE","6baf33d3":"# Create a Optuna stydy. for maximizing neg RMSE\nstudy = optuna.create_study(direction='maximize')","1469403c":"%%time\n\n# Forming Param Dict\nfinal_param_list = {\n    'n_estimators'    : 3493,\n    'max_depth'       : 7,\n    'learning_rate'   : 0.006848867349350412,\n    'gamma'           : 0.6737971005366712,\n    'subsample'       : 0.6101403456639767,\n    'colsample_bytree': 0.6053687986044185,\n    'reg_alpha'       : 87.3846454719566,\n    'reg_lambda'      : 65.78414200721654,\n    'tree_method'     : 'gpu_hist',\n    'random_state'    : 3141,\n    'verbosity'       : 1,\n} # best_scaore : 7.838040400806658\n\nfinal_param_list_2 = {\n    'n_estimators'    : 2496,\n    'max_depth'       : 9,\n    'learning_rate'   : 0.01107053835431393,\n    'gamma'           : 0.028684498045221676,\n    'subsample'       : 0.7037432500659588,\n    'colsample_bytree': 0.2755633622133354,\n    'reg_alpha'       : 99.26591991981083,\n    'reg_lambda'      : 91.6583006084785,\n    'tree_method'     : 'gpu_hist',\n    'random_state'    : 2021,\n    'verbosity'       : 1,\n}\n\n# Form class instance of xgb regr\nregr_model = XGBRegressor(**final_param_list)\n\n# fit Data on training Dataset\nregr_model.fit(X_train, y_train)","0d7e57d7":"%%time\n\n# Predict target on test data\ny_pred = regr_model.predict(X_test)","0527acaa":"# Form FataFrame\nsub.loss = y_pred","cd60c42d":"sub.to_csv('submission.csv', index=False)","23dc49a0":"# Model Creation\n## Make Custom Root Mean Squares Error scorer","e83341ae":"# Import Relavant Libraries","c71b868b":"## Import Data","a8759778":"## Make Shuffled Kfold With Random Seed For Repeatablity","13361610":"## Optimization of XGBoostRegressor Params with Optuna","c90f9b3a":"# Best Params From Optimization\nstudy.best_params","ff11bedd":"## From Model with Optimal Parameters","74d36363":"## Make train, test DataFrame and target Series","b9cee7dd":"%%time\n\n# Time Out in 2 hrs.\nstudy.optimize(Objective, n_trials=1_000, timeout=5*60*60)"}}