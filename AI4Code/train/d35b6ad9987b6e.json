{"cell_type":{"073d8a8a":"code","d9c08458":"code","152f4d3b":"code","9b4830ec":"code","339a5502":"code","7693e5af":"code","b2bde96c":"code","63434620":"code","916dfd2e":"code","acca9235":"code","49e23277":"code","36230fba":"code","3645baca":"code","2c63007e":"code","39fdd490":"code","5f8c0e24":"code","10c9491e":"code","9d73a17d":"code","761af7ef":"code","f155a18d":"code","6d67995d":"code","817d5bbb":"code","8655cc50":"code","93c5d9b8":"code","88430b76":"code","205f7e6d":"code","d7898c8b":"code","5494ec94":"code","b3701436":"code","4a427fe9":"markdown","89ecdac7":"markdown","68c5c9df":"markdown","504415f7":"markdown"},"source":{"073d8a8a":"#It is a good practice to import all the modules required for training the model.\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import DataLoader, TensorDataset, random_split","d9c08458":"DATASET_URL = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00242\/ENB2012_data.xlsx\" #The url of the dataset is given here\nDATA_FILENAME = \"Energyefficiency.csv\" #the name of the saved downloaded file.\ndownload_url(DATASET_URL, root='.',filename=DATA_FILENAME) #Don't forget to add the directory(any name where the downloaded file is saved) at the root parameter.","152f4d3b":"# To read the data from the excel file(.xlsx means excel),we use pandas dataframe where dataframe is like row by column which can be visualized below.\ndataframe = pd.read_excel(DATA_FILENAME) \ndataframe.head(5) #It is used to get the first 5 rows of the dataframe.","9b4830ec":"num_rows = len(dataframe)\nprint(num_rows)","339a5502":"num_cols = sum(1 for i in dataframe.columns)\nprint(num_cols)","7693e5af":"input_cols = [i for i in dataframe.columns if(i!='Y1' and i!='Y2')] #the column titles of the input variables\nprint(len(input_cols))","b2bde96c":"output_cols = ['Y1','Y2'] #the column titles of output\/target variable(s)\nprint(len(output_cols))","63434620":"# Visualizing the distribution of target values in a graph.\nplt.plot(dataframe['Y1'],'r')\nplt.plot(dataframe['Y2'],'b')\nplt.legend(['heating load','cooling load'])\nplt.show()","916dfd2e":"import numpy as np\ndef dataframe_to_arrays(dataframe):\n    # Make a copy of the original dataframe as we may need the original one if anything goes wrong which is a good practice.\n    dataframe1 = dataframe.copy(deep=True)\n    # Extract input & outupts as numpy arrays of datatype float32 as the model expects the data to be float instead of double.\n    inputs_array = dataframe1[input_cols].to_numpy().astype(np.float32)\n    targets_array = dataframe1[output_cols].to_numpy().astype(np.float32)\n    return inputs_array, targets_array","acca9235":"inputs_array, targets_array = dataframe_to_arrays(dataframe)\ninputs_array, targets_array","49e23277":"# We should convert the numpy arrays into tensor as the model expects tensors.\ninputs = torch.from_numpy(inputs_array)\ntargets = torch.from_numpy(targets_array)","36230fba":"#let's confirm that the datatype is float\ninputs.dtype, targets.dtype","3645baca":"# Let's now create the dataset with inputs and targets combined to create batches at next step\ndataset = TensorDataset(inputs, targets)","2c63007e":"# It is necessary to split the data into train and validation in order to evaluate the model on untouched data while training the model.\ntorch.manual_seed(16)#this will make the random same everytime we run this notebook which will help us evaluate a particular value as we do below .\nval_percent = 0.1 # between 0.1 and 0.2 is good.\nval_size = int(num_rows * val_percent)\ntrain_size = num_rows - val_size\n\n\ntrain_ds, val_ds = random_split(dataset,[train_size,val_size]) # Use the random_split function to split dataset into 2 parts of the desired length.","39fdd490":"#Usually we take batch_size as a power of 2(i.e,16,32,64,...) as it improves the speed of the traing convergence(i.e,loss is reduced to minimum soon)\nbatch_size = 128","5f8c0e24":"train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) #pin_memory enables the data to be loaded into pinmemory which speedups the GPU loading action.\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) # num_works are used when the dataset is large to make the loading fat as it is done parallely by 4 systems.","10c9491e":"#Let's have a look at the first batch.\nfor xb, yb in train_loader:\n    print(\"inputs:\", xb)\n    print(\"targets:\", yb)\n    break","9d73a17d":"input_size = len(input_cols)\noutput_size = len(output_cols)","761af7ef":"class EfficiencyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #input layer\n        self.linear1=nn.Linear(input_size,128)  # we have to pass the number of inputs(number of input columns) and the number of hidden units\n        #hidden layers\n        self.linear2=nn.Linear(128,64) #previous layer hidden units count and the current layers hidden units count\n        self.linear3=nn.Linear(64,32) # make sure that the previous layer hidden units count and the current layer hidden units count are equal.\n        self.linear4=nn.Linear(32,16)\n        self.linear6=nn.Linear(16,8)\n        #output layer\n        self.linear5=nn.Linear(8,output_size) # here we need to pass previous layers hidden units count and number of outputs(number of output columns) as we mentioned above.\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1) #As the model expects the features to flattened(i.e, all the features are stacked vertically),view()will do this for us where -1 makes the model to rest features as 2nd dimension.\n        # Apply layers & activation functions\n        out= self.linear1(out) #It is an inbuilt-method for linear regression i.e  z= w*x +b where w is the weight and b is the bias\n        out=F.relu(out)  #As we can know from the previous post that linear relation can't make a good prediction , now we are using a non-linear activation function called rectified linear units (Relu)\n        out= self.linear2(out)\n        out=F.relu(out)\n        out= self.linear3(out)\n        out=F.relu(out)\n        out= self.linear4(out)\n        out=F.relu(out)\n        out= self.linear6(out)\n        out=F.relu(out)\n        out=self.linear5(out)  # here we used 5 layers including 3 hidden layers.\n        return out\n    def training_step(self, batch):\n        inputs, targets = batch \n        # Generate predictions\n        out = self(inputs)  #which returns the predicted values      \n        # Calcuate loss\n        loss = F.l1_loss(out,targets) #L1 loss(i.e ((1\/num_rows)*(predicted-target)^2)^1\/2) which is used to calculate the loss. Although there are lot loss functions, l1_loss will be more suitable for linear reg.\n        return loss\n    \n    def validation_step(self, batch):\n        inputs, targets = batch\n        # Generate predictions\n        out = self(inputs)\n        # Calculate loss\n        loss = F.l1_loss(out,targets)                           # l1_loss is calculated for validation set   \n        return {'val_loss': loss.detach()}  \n        #detach() is used to remove this from computational graph as it may confuse the backprop since it is the loss of validation set to clear the memory whic will make training fast.\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combining losses for all the batches to compute a single loss value\n        return {'val_loss': epoch_loss.item()}\n    \n    def epoch_end(self, epoch, result, num_epochs):\n        # Print result every 20th epoch where epoch represents a complete traversal through the training examples.\n        if (epoch+1) % 2 == 0 or epoch == num_epochs-1:\n            print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch+1, result['loss'],result['val_loss']))","f155a18d":"torch.cuda.is_available() #This gives whether the gpu is available or not in form of boolean(true or false)\ndef get_default_device():\n    \"\"\"To pick GPU if available, else CPU so that the program won't crash if gpu not available\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\ndevice=get_default_device()\nprint(device)\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device as everything should be present in the respective device \"\"\"\n    if isinstance(data, (list,tuple)): # isinstance checks whether the input is of the desired type like it is list or tuple and will go next only if the condition is passed.\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True) #thus the data is moved according to the device selected\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device .Don't worry if you dont't get it,just do as instructed\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n#The above class will perform the functions when the instance is called.","6d67995d":"#Let's now create the instances for the below data.\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","817d5bbb":"model = to_device(EfficiencyModel(),device) #Initializing the model and also moving the model to the device","8655cc50":"#once the model is initialized ,all the w's(weights) and b's(biases) are initialized at random which are then updated by calculating the derivative of the loss fuction with respect to both w and b respectively.\n#This is how the model is made to learn the relations between inputs and targets.\nlist(model.parameters())","93c5d9b8":"def evaluate(model, val_loader):\n    #To evaluate the validation set\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr) #optimizer which is used update the parameters. Stochastic Gradient Descent is mostly used for linear reg. problems.\n    for epoch in range(epochs):\n        # Training Phase \n        losses=[]\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            losses.append(loss) #the training losses are stored for every batch for future visualizations.\n            loss.backward() # Backpropagation(i.e, derivative(slope) calculation) is made by the model.\n            optimizer.step() #It updates the weights and biases with the derivatives and learning rate(i.e, w=w-lr*dw)\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['loss']= torch.stack(losses).mean()  #The mean of training losses is stored for each epoch.\n        model.epoch_end(epoch, result, epochs)\n        history.append(result) #appending the results for visualization purposes.\n    return history","88430b76":"result = evaluate(model,val_loader) # Use the the evaluate function to evaluate the model.\nprint(result)","205f7e6d":"epochs = 150 #number of epochs to run the model.\nlr = 0.5e-3  #learning rate\nmodel = to_device(EfficiencyModel(),device)\nmodel.load_state_dict(torch.load(\"savedmodel\/1\")) #To load the model and predict\nmodel.eval()\n#If you want run comment the above 2 lines and uncomment the below line\n#history = fit(epochs, lr, model, train_loader, val_loader)","d7898c8b":"val_loss = evaluate(model,val_loader)\nprint(val_loss)\n#Let's visualize the validation loss for the epochs\nplt.plot([i['loss'] for i in history],'r')\nplt.plot([i['val_loss'] for i in history],'b')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"losses\")\nplt.legend(['loss','val_loss'])\nplt.show()","5494ec94":"def predict_single(input, target, model):\n    predictions = model(input.unsqueeze(0))     \n    prediction = predictions[0].detach() #detach() is done to remove it from the computational graphs i.e, its dervatives are no longer calculated.\n    print(\"Input:\", input)\n    print(\"Target:\", target)\n    print(\"Prediction:\", prediction)","b3701436":"#Let's check the predictions of the model by passing a value from the validation set(you can also try by passing a tensor of your wish as torch.tensor([...]))\nprint(val_ds[4])\nval=DeviceDataLoader(val_ds[4],device) #we have to move the data to the device to avoid errors.\ninput, target = val\npredict_single(input, target, model)","4a427fe9":"## Step 4: Train the model to fit the data\n\nTo train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem.","89ecdac7":"## Step 2: Prepare the dataset for training\n\nWe need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays.","68c5c9df":"## Step 3: Create a Linear Regression Model\n\nOur model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment).","504415f7":"## Step 1: Downloading and Exploring the Dataset"}}