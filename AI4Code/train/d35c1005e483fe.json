{"cell_type":{"abf211bd":"code","7f07e783":"code","40b0eb78":"code","4e691dac":"code","0993e79e":"code","8ab2d21d":"code","b99da557":"code","ea62c81e":"code","6e4bc5f6":"code","7a51afbb":"code","58d23d2f":"code","c92cb82f":"code","96106aa7":"code","516019e3":"code","da47b776":"code","b931524e":"code","9b975c8f":"code","f1bebb36":"code","9e2f607a":"code","71a35072":"code","d6405c8f":"code","fa59b682":"code","d7424623":"code","0beb3337":"code","89a119ca":"code","b008837d":"code","28b83119":"code","9d47b735":"code","3a86bc27":"code","4645ab8e":"code","5a2719d4":"code","3b4c3c16":"code","4c51c6b6":"code","cd4335e6":"code","7fbe6fdf":"code","f6bf27cd":"code","de6b101e":"code","e9cb100b":"code","687eccce":"code","dc8e400d":"code","647b55f4":"code","0893b1a8":"code","a81ded17":"code","ddbc9e6b":"code","f6d20c15":"code","990de13f":"code","e901b5f9":"code","f1df84e3":"code","676c222d":"code","bd527213":"code","1d0f1242":"code","e7d0355e":"code","229c605c":"code","bdb08c22":"code","6a85ccf5":"code","5af569bb":"code","c8927b3e":"code","3952ea28":"code","1ca82340":"code","0060d5bc":"code","c65199b2":"markdown","1e733795":"markdown","647d8675":"markdown","8bea0297":"markdown","2fc52446":"markdown","a2925bc5":"markdown","46643208":"markdown","7905f9c5":"markdown","eb74afe0":"markdown","4b2fb837":"markdown","1e37b9d5":"markdown","e3ecaa6c":"markdown","1427ca8a":"markdown","b5bf601a":"markdown","26b8c665":"markdown","a61b638c":"markdown","c728cd5a":"markdown","c4d89276":"markdown"},"source":{"abf211bd":"import pandas as pd","7f07e783":"true_news_data = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","40b0eb78":"true_news_data['IS_FAKE'] = 0 # Creating default column","4e691dac":"true_news_data.head()","0993e79e":"fake_news_data = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","8ab2d21d":"fake_news_data['IS_FAKE'] = 1 # Creating default column","b99da557":"fake_news_data.head()","ea62c81e":"news_data = pd.concat([true_news_data,fake_news_data]) # Concating the dataframes","6e4bc5f6":"news_data = news_data.sample(frac=1.0).reset_index(drop=True) # Shuffling the data\n\nnews_data.head()","7a51afbb":"news_data.IS_FAKE.value_counts() # proprtion of data","58d23d2f":"news_data.subject.groupby(news_data['IS_FAKE']).value_counts()","c92cb82f":"!pip install texthero # Installing TextHero\n!pip install -U spacy","96106aa7":"import texthero as hero # Importing TextHero","516019e3":"fake_news_data.loc[0,'text']\n\n# From below data we can confirm that our data is uncleaned as it has numbers, bracket, special characters.\n# Thus, this data cannot be directly provided to ML model","da47b776":"news_data.isnull().sum()\n\n# Since, our data has no null values we can skip this step","b931524e":"news_data['text'] = hero.lowercase(news_data['text'])\n\n# Converting text column to lower case","9b975c8f":"news_data.head()","f1bebb36":"news_data['text'] = hero.remove_digits(news_data['text']) # To remove digits","9e2f607a":"news_data['text'] = hero.remove_punctuation(news_data['text']) # To remove punctuations","71a35072":"news_data['text'] = hero.remove_diacritics(news_data['text']) # To remove diacritics","d6405c8f":"# To remove any form of brackets\n\nnews_data['text'] = hero.remove_brackets(news_data['text'])\nnews_data['text'] = hero.remove_angle_brackets(news_data['text'])\nnews_data['text'] = hero.remove_curly_brackets(news_data['text'])\nnews_data['text'] = hero.remove_round_brackets(news_data['text'])\nnews_data['text'] = hero.remove_square_brackets(news_data['text'])","fa59b682":"hero.top_words(news_data['text'])[:10]\n\n# From below data we can see that there are many unwanted words, we will remove these words","d7424623":"news_data['text'] = hero.remove_stopwords(news_data['text'])","0beb3337":"hero.top_words(news_data['text'])[:10] # Stop words have been removed, but there still seems to have unwanted characters","89a119ca":"news_data['text'] = hero.remove_punctuation(news_data['text'])","b008837d":"news_data['text'] = hero.remove_whitespace(news_data['text'])","28b83119":"news_data['text'] = news_data['text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2 and word.isalpha() and word != 'reuters']))\n\n# In above step we are only considering words that has more than 2 length and contains only alphabets","9d47b735":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')","3a86bc27":"news_data['text'] = news_data['text'].apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(word) for word in x.split()]))","4645ab8e":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(news_data.loc[news_data.IS_FAKE == 0,\"text\"].values))\n    \nfig = plt.figure(\n    figsize = (10, 10),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","5a2719d4":"wordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(news_data.loc[news_data.IS_FAKE == 1,\"text\"].values))\n    \nfig = plt.figure(\n    figsize = (10, 10),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","3b4c3c16":"from sklearn.feature_extraction.text import TfidfVectorizer","4c51c6b6":"Tfidf_vect = TfidfVectorizer()\nTrain_content_tfidf = Tfidf_vect.fit_transform(news_data['text'])","cd4335e6":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score","7fbe6fdf":"SKF = StratifiedKFold(n_splits=5, shuffle=True)","f6bf27cd":"print(f' Accuracy of Logistic Regression  : {round(cross_val_score(LogisticRegression(),Train_content_tfidf,news_data[\"IS_FAKE\"],cv=SKF,scoring=\"accuracy\").mean()*100,2)}%')","de6b101e":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(Train_content_tfidf,news_data['IS_FAKE'],test_size=0.2)","e9cb100b":"LR = LogisticRegression()\nLR.fit(X_train,Y_train)\nY_pred = LR.predict(X_test)","687eccce":"from sklearn import metrics\n\nprint(f' Accuracy Score : {round(metrics.accuracy_score(Y_test,Y_pred)*100,2)}%')\nprint(metrics.classification_report(Y_test,Y_pred))","dc8e400d":"import seaborn as sns\n\nsns.heatmap(metrics.confusion_matrix(Y_test,Y_pred), annot=True, fmt='d',cmap='YlGnBu')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.show()","647b55f4":"import tensorflow as tf\nfrom tensorflow import keras","0893b1a8":"MAX_NB_WORDS = 10000\n\ntokenizer = keras.preprocessing.text.Tokenizer(MAX_NB_WORDS) # Selecting top 10000 words\ntokenizer.fit_on_texts(news_data['text'])\ntrain_data = tokenizer.texts_to_sequences(news_data['text'])\n\nword_index = tokenizer.word_index","a81ded17":"print(train_data[:2]) # Data is tokenized","ddbc9e6b":"len(word_index) # length of tokenized dictionary","f6d20c15":"sns.histplot([len(x) for x in train_data],bins=1000)\n\n# Since most of the text data is less than 1000 words, we will select sequences to be approximate 600","990de13f":"MAX_SEQUENCE_LENGTH = 600\nEMBEDDING_DIM = 30","e901b5f9":"train_data = keras.preprocessing.sequence.pad_sequences(train_data, maxlen=MAX_SEQUENCE_LENGTH)\n# Padding to make equal length of 600 sequences","f1df84e3":"train_data.shape","676c222d":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(train_data,news_data['IS_FAKE'],test_size=0.2)","bd527213":"X_train.shape,X_test.shape,Y_train.shape,Y_test.shape","1d0f1242":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n\n# Below layers are not required.\n\n# model.add(keras.layers.Conv1D(100, 4, activation='relu')) \n# model.add(keras.layers.MaxPooling1D(4))\n\nmodel.add(keras.layers.LSTM(units=128))\n\nmodel.add(keras.layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","e7d0355e":"model.fit(X_train, Y_train, epochs=5,batch_size=1000, validation_split=0.1,verbose=1)","229c605c":"print(model.evaluate(X_test,Y_test, verbose=1))","bdb08c22":"import numpy as np\n\ny_predicted = model.predict(X_test)\ny_predicted_labels = [np.round(i) for i in y_predicted]","6a85ccf5":"sns.heatmap(tf.math.confusion_matrix(labels=Y_test,predictions=y_predicted_labels), annot=True, fmt='d',cmap='YlGnBu')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.show()","5af569bb":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128))) # Only added Bi-directional layer\n\nmodel.add(keras.layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","c8927b3e":"model.fit(X_train, Y_train, epochs=5,batch_size=1000, validation_split=0.1,verbose=1)","3952ea28":"print(model.evaluate(X_test,Y_test, verbose=1))","1ca82340":"y_predicted = model.predict(X_test)\ny_predicted_labels = [np.round(i) for i in y_predicted]","0060d5bc":"sns.heatmap(tf.math.confusion_matrix(labels=Y_test,predictions=y_predicted_labels), annot=True, fmt='d',cmap='YlGnBu')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.show()","c65199b2":"## Tokenizing","1e733795":"<img src='https:\/\/drive.google.com\/uc?export=view&id=1QxVs7yMvhNw5spf1SM5SpZ-crrD3JojG' height=350 >\n\nWe will be using TextHero library, it has pre-coded function to clean, transform and visualise our data.\n\nCheckout this library [here](https:\/\/texthero.org\/).","647d8675":"# ML Model","8bea0297":"## 4. REMOVE STOP WORDS","2fc52446":"# LSTM Model","a2925bc5":"# Conclusion\n\n1.  For this dataset we can use LSTM or LogisticRegression\n2.  We can try to add more layers to improve accuracy in LSTM\n3.  We can try to select optimal numbers of frequent words while tokenizing","46643208":"## 5. REMOVE WHITE SPACES","7905f9c5":"<img src='https:\/\/drive.google.com\/uc?export=view&id=1ASmFC7vxUx0EFvtp2RJCT6DQdZhhg2Ab' height =350 >\n\nIn this notebook we have fake and real news data, we will clean this data and create LSTM model to predict which class does a particular news belong.","eb74afe0":"## 2. CONVERT TO LOWER CASE","4b2fb837":"## 8. TF-IDF","1e37b9d5":"## 6. REMOVE UNWANTED DATA","e3ecaa6c":"## 3. REMOVE NON-ALPHA TEXT(DIGITS,PUNCTUATIONS,DIACRITICS)","1427ca8a":"### Visualizing top words from Fake news","b5bf601a":"# Data Pre-Processing\nWe will use texthero builtin functions to clean our data as below,","26b8c665":"## 1. FILL NULL VALUES","a61b638c":"## 7. LEMMATIZATION","c728cd5a":"# Bi-directional LSTM","c4d89276":"### Visualizing top words from True news"}}