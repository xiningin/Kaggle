{"cell_type":{"95e2cd72":"code","0edeab76":"code","db8c52ae":"code","e13fd2fb":"code","9ba1f2a6":"code","ab80d06a":"code","6ceb8e76":"code","0a73a28e":"code","3bbdc380":"code","7a48954b":"code","b29b613a":"code","24d00b5d":"code","144e2ae5":"code","8eb9a25c":"code","4ec9d483":"code","60369510":"code","3ba56b2a":"code","e2709812":"code","fa3c27dc":"code","cebbe6a2":"markdown"},"source":{"95e2cd72":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0edeab76":"#Loading Train and Test Data\ndf_train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ndf_test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(df_train.shape[0],df_train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(df_test.shape[0],df_test.shape[1]))","db8c52ae":"df_train.head()","e13fd2fb":"df_train[\"month\"] = df_train[\"first_active_month\"].dt.month\ndf_test[\"month\"] = df_test[\"first_active_month\"].dt.month\ndf_train[\"year\"] = df_train[\"first_active_month\"].dt.year\ndf_test[\"year\"] = df_test[\"first_active_month\"].dt.year\ndf_train['elapsed_time'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days\ndf_test['elapsed_time'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days\ndf_train.head()","9ba1f2a6":"df_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_2'])\ndf_test = pd.get_dummies(df_test, columns=['feature_1', 'feature_2'])\ndf_train.head()","ab80d06a":"df_hist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\ndf_hist_trans.head()","6ceb8e76":"df_hist_trans = pd.get_dummies(df_hist_trans, columns=['category_2', 'category_3'])\ndf_hist_trans['authorized_flag'] = df_hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_hist_trans['category_1'] = df_hist_trans['category_1'].map({'Y': 1, 'N': 0})\ndf_hist_trans.head()","0a73a28e":"def aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","3bbdc380":"import gc\nmerch_hist = aggregate_transactions(df_hist_trans, prefix='hist_')\ndel df_hist_trans\ngc.collect()\ndf_train = pd.merge(df_train, merch_hist, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_hist, on='card_id',how='left')\ndel merch_hist\ngc.collect()\ndf_train.head()","7a48954b":"df_new_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\ndf_new_trans.head()","b29b613a":"df_new_trans = pd.get_dummies(df_new_trans, columns=['category_2', 'category_3'])\ndf_new_trans['authorized_flag'] = df_new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_new_trans['category_1'] = df_new_trans['category_1'].map({'Y': 1, 'N': 0})\ndf_new_trans.head()","24d00b5d":"merch_new = aggregate_transactions(df_new_trans, prefix='new_')\ndel df_new_trans\ngc.collect()\ndf_train = pd.merge(df_train, merch_new, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_new, on='card_id',how='left')\ndel merch_new\ngc.collect()\ndf_train.head()","144e2ae5":"target = df_train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in df_train.columns if c not in drops]\nfeatures = list(df_train[use_cols].columns)\ndf_train[features].head()","8eb9a25c":"print(df_train[features].shape)\nprint(df_test[features].shape)","4ec9d483":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.0041,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","60369510":"from sklearn.metrics import mean_squared_error\nval_score = np.sqrt(mean_squared_error(target, oof))\nval_score","3ba56b2a":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(clf, max_num_features=30, height=0.5, ax=ax, title='Feature importance', xlabel='Feature importance', ylabel='Features')\nplt.show()","e2709812":"sorted(list(zip(clf.feature_importance(), features)), reverse=True)","fa3c27dc":"df_sub = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\ndf_sub[\"target\"] = predictions\ndf_sub.to_csv(\"sub_val_{}.csv\".format(val_score), index=False)","cebbe6a2":"In this Kernel, I work off of https:\/\/www.kaggle.com\/rooshroosh\/simple-data-exploration-with-python-lb-3-760 but include `category_1`, `category_2`, and `category_3` from the transactions tables, which end up being very important features when aggregated by `mean`. Hopefully this shows how you can incorporate more categorical data into your kernel."}}