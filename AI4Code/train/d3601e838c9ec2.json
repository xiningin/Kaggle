{"cell_type":{"f5086da4":"code","d94b862d":"code","49ab1fbf":"code","92afb29f":"code","248df7d8":"code","569c9c6a":"code","7545e741":"code","72fed036":"code","b79db423":"code","a6e38e83":"code","e2c1c681":"code","45abe021":"code","9300178f":"code","23667b90":"code","7def4a3e":"code","1bf60e96":"code","c7b73811":"code","d8cd1698":"code","96acbc41":"code","8a66bead":"code","29492c13":"code","0b22f261":"code","df51c21e":"code","b22f4780":"markdown","ad4c53e0":"markdown","f91126fe":"markdown","55583c05":"markdown","822507c1":"markdown","19436bd0":"markdown","13ab3f8e":"markdown","044ffa29":"markdown","99e102b0":"markdown","83b310ed":"markdown","be8d00aa":"markdown","02cdad54":"markdown","aad642e2":"markdown","6266cce9":"markdown","154e0f1a":"markdown","091caee7":"markdown"},"source":{"f5086da4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d94b862d":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report)\nimport plotly.graph_objs as go\nimport plotly.offline as py","49ab1fbf":"# Reading the dataset\nattritionData=pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\nattritionData.head(5)","92afb29f":"print((attritionData.Attrition.value_counts()\/len(attritionData.index))*100)\nsns.countplot(x='Attrition',data=attritionData)","248df7d8":"sns.countplot(x='BusinessTravel',hue='Attrition',data=attritionData,palette='RdBu_r')","569c9c6a":"import plotly.express as px\nattDataBasedonAge=attritionData.groupby(['Age','Attrition']).apply(lambda x:x['Age'].count()).reset_index(name='Counts')\npx.line(attDataBasedonAge,x='Age',y='Counts',color='Attrition',title='Agewise Counts of People in an Organization')","7545e741":"attDataBasedonAgeGender=attritionData.groupby(['Age','Attrition','Gender']).apply(lambda x:x['Age'].count()).reset_index(name='Counts')\nattDataBasedonAgeGender['AttrGen']=attDataBasedonAgeGender.Attrition+\"_\"+attDataBasedonAgeGender.Gender\npx.line(attDataBasedonAgeGender,x='Age',y='Counts',color='AttrGen',title='Age & Gender Counts of People in an Organization')\n","72fed036":"attDataBasedonOverTime=attritionData.groupby(['OverTime','Attrition']).apply(lambda x:x['OverTime'].count()).reset_index(name='Counts')\n\nax=sns.barplot(data=attDataBasedonOverTime.reset_index(), x='OverTime', y='Counts',hue='Attrition')\nax.set(xlabel='Overtime group Defined as \"NO\" and \"YES\" ', ylabel='Count of Each category')","b79db423":"attritionDataBasedonStock=attritionData.groupby(['StockOptionLevel','Attrition']).apply(lambda x:x['StockOptionLevel'].count()).reset_index(name='Counts')\nattritionDataBasedonStock['StockTrueFalse']=attritionDataBasedonStock[['StockOptionLevel']].apply(lambda x:x.map({0:'No',1:'Yes',2:'Yes',3:'Yes'}))\n\nax=sns.barplot(data=attritionDataBasedonStock, x='StockTrueFalse', y='Counts',hue='Attrition',ci=None)\nax.set(xlabel='Stock Defined as \"NO\" and \"YES\" ', ylabel='Count of Each category')","a6e38e83":"attritionDataBasedonJobSatisfaction=attritionData.groupby(['JobSatisfaction','Attrition']).apply(lambda x:x['JobSatisfaction'].count()).reset_index(name='Counts')\n\nattritionDataBasedonJobSatisfaction['JobSatisfactionLevel']=attritionDataBasedonJobSatisfaction[['JobSatisfaction']].apply(lambda x:x.map({1:'Low',2:'Medium',3:'High',4:'Very High'}))\nax=sns.barplot(data=attritionDataBasedonJobSatisfaction, x='JobSatisfactionLevel', y='Counts',hue='Attrition',ci=None)\nax.set(xlabel='Stock Defined as \"NO\" and \"YES\" ', ylabel='Count of Each category')","e2c1c681":"corr_matirx=attritionData.corr()\nplt.figure(figsize=(20,12))\nax = sns.heatmap(corr_matirx[corr_matirx>0.4], annot = True, cmap=\"Set1\")\ntop, bottom = ax.get_ylim()\nax.set_ylim(top+0.5, bottom-0.5)","45abe021":"attritionData01=attritionData.copy()\n# Encoding the Target column\nattritionData01['AttritionTarget']=attritionData01[['Attrition']].apply(lambda x:x.map({'Yes':1,'No':0}))","9300178f":"# Empty list to store columns with categorical data\n\ncategoricalColumns = []\nfor col, value in attritionData01.iteritems():\n    if value.dtype == 'object':\n        categoricalColumns.append(col)\n\n# Store the numerical columns in a list numerical\nnumericalColumns = attritionData01.columns.difference(categoricalColumns)\n\nprint(\"Total number of numerical columns are:- \",len(numericalColumns))\nprint(\"Total number of categorical columns are:- \",len(categoricalColumns))","23667b90":"attrition_catgorical = attritionData01[categoricalColumns]\nattrition_catgorical = attrition_catgorical.drop(['Attrition'], axis=1) # Dropping the target column\nattrition_catgorical = pd.get_dummies(attrition_catgorical,drop_first=True)\n\nattrition_numerical = attritionData01[numericalColumns]\nattritionDatset_final = pd.concat([attrition_numerical, attrition_catgorical], axis=1)\nattritionDatset_final.head(3)","7def4a3e":"# Splitting the data for test and train\nX = attritionDatset_final.drop(\"AttritionTarget\",axis=1)   \ny = attritionDatset_final[\"AttritionTarget\"]\n\n# Split data into train and test sets as well as for validation and testing\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size= 0.80,random_state=0)","1bf60e96":"# We still have imbalanceed data where the percentage of people stayin the organisation is more thant the percentage of the employee leaving the organisation\n(y_train.value_counts()\/len(y_train.index))*100\n","c7b73811":"# we will use SMOTE technique to Balance out the data\n\nsm = SMOTE(random_state=0)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n\n# After balancing the data Here is how the Balanced percentage of Data looks like\n(y_train_res.value_counts()\/len(y_train_res.index))*100","d8cd1698":"seed = 0   # We set our random seed to zero for reproducibility\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 1000, \n    'max_features': 0.3,\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'random_state' : seed,\n    'verbose': 1\n}\n\nrf = RandomForestClassifier(**rf_params)\nrf.fit(X_train_res, y_train_res)\nrf_predictions = rf.predict(X_test)\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test, rf_predictions)))\nprint(\"=\"*80)\nprint(classification_report(y_test, rf_predictions))","96acbc41":"trace = go.Scatter(\n    y = rf.feature_importances_,\n    x = attritionDatset_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        color = rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = attritionDatset_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","8a66bead":"plt.figure(figsize=(15,40))\nfeat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nfeat_importances.nlargest(len(X.columns)).sort_values().plot(kind='barh', align='center')","29492c13":"import xgboost\nclassifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.5, gamma=0.4, learning_rate=0.1,\n       max_delta_step=0, max_depth=6, min_child_weight=7, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)","0b22f261":"classifier.fit(X_train_res,y_train_res)\nprediction=classifier.predict(X_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test, prediction)))\nprint(\"=\"*80)\nprint(classification_report(y_test, prediction))","df51c21e":"plt.figure(figsize=(15,40))\nfeat_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeat_importances.nlargest(len(X.columns)).sort_values().plot(kind='barh', align='center')","b22f4780":"### As per the Graph we get to see Employees who work overtime are the one who likely to leave the orgnisation than compared to others.","ad4c53e0":"### Feature Engineering\nLets encode categorical featues to build the predictive models","f91126fe":"### From the below Visualization we get to see that people who has job satisfaction level as \"High\" also has got hit attrition followed by \"Low\" job satisfaction level. This also indicates how the data was recorded for job satisfaction since if employee thinks it get tracked then they might shows the False postive indications.","55583c05":"### Using XGBoost","822507c1":"### From the below line graph we can see people in the age group of 28 and 29 are more likely to quit than any other age group","19436bd0":"### Implementing the model to predict attrition in 5 mins,Note book is crisp and clear  if you are intrested know the description and conclusion you can refer my blog below.\n\nBlog url:- https:\/\/www.devonblog.com\/software-development\/artificial-intelligence\/attrition-prediction-using-machine-learning\/","13ab3f8e":"=====================================================================","044ffa29":"#### Conclusion:-\n- We were able to achieve 87% of accuracy on test data using XGboost.\n- Please refer my blog for conclusion results.\n\nhttps:\/\/www.devonblog.com\/software-development\/artificial-intelligence\/attrition-prediction-using-machine-learning\/","99e102b0":"### Building model using Random Forest(RF)","83b310ed":"### As per the Graph we see Employees who dont have stock option in the company are most likely to leave the organization.These employees can be a junior employees or mid level employees who might not have stock options","be8d00aa":"### Below Heat or corelation map tells how the features are linked to other features. we can see\n- Job level is directly proportional to Total working years.\n- Montly Income is directly proportional to the Total working years.\n- Stock option is Directly proptional to Job level.\n- Performance Rating is Directly proportional to percentage salary hike.","02cdad54":"## We will use SMOTE technique to Balance out the data","aad642e2":"### Finding the important Features\/Columns from Random forest ","6266cce9":"### When we segregate based on Gender and attrition, we see female of age \"29\" group is having most attrition and male of age \"28\" group has most attrition than compared to any other age group.","154e0f1a":"### As per this ficiton datset we see people who \"Travel Rarely\" are the one who have quitted more,","091caee7":"### It's Quiet imbalance dataset, where 83.87% data have attrition label has \"No\" and only 16.12% data has got attrition rate has \"yes\"."}}