{"cell_type":{"4a96e69c":"code","34723684":"code","50884610":"code","22447b84":"code","d0d9f2b7":"code","40d08682":"code","2d296af8":"code","70c090da":"code","b98ff43a":"code","9a3f2c6f":"code","389b80ea":"code","006fe98e":"code","9fb33a13":"code","027c2f2b":"code","af74e5d4":"code","a32317c5":"code","81bb1cef":"code","85948612":"code","416162f9":"code","483e20de":"code","255273a2":"code","edcfd6e3":"code","604c9cac":"code","f56790f2":"code","a13c32f0":"code","45b81e9e":"code","bdd0b820":"code","0a3e413b":"code","a6134b71":"code","6c7379a3":"code","89d2032a":"code","6e13e5dc":"code","0f3f00ca":"code","e5d07137":"code","b57e5b57":"code","9408ef68":"code","7bb92cb7":"code","ab586d63":"code","35331cdb":"code","5396d27e":"code","3cbb1a69":"code","553a0db5":"code","3d11f983":"code","0036aee0":"code","f7014629":"code","2bf0e224":"code","94b02f25":"code","ef2e0481":"code","495178b9":"code","285b1da3":"code","241b9d2f":"code","ae53f82a":"code","ac3f1522":"code","794ca5fe":"code","fab3e913":"code","760619a0":"code","96e07258":"code","3e13964c":"code","156df02f":"code","0e260d0c":"code","74c197af":"code","a5024b2a":"code","9ee44395":"code","0958692d":"code","2c319b83":"code","c58be9fa":"code","ee5b636f":"code","b2d20f2f":"code","b9fbf362":"code","39e7b8c3":"markdown","e07f99de":"markdown","2994950e":"markdown","4b2639a9":"markdown","71319ee6":"markdown","564a27c5":"markdown","4bf38680":"markdown","50f0b0b2":"markdown","75e4e4ef":"markdown","0898b28c":"markdown","1931ad5d":"markdown","17e9dd8a":"markdown","ef8826b2":"markdown","fa6c91d3":"markdown","d914c89d":"markdown","14d5c82d":"markdown","a77e36bb":"markdown","49cd6d20":"markdown","4d205733":"markdown","0ce6bab4":"markdown","db8eed71":"markdown","30a62338":"markdown","fe75b7a4":"markdown","2b1a7450":"markdown","27b9a3b5":"markdown","eb7429e3":"markdown","078df780":"markdown","8c862ab9":"markdown","e062b51f":"markdown","7a02adcb":"markdown","c49875dd":"markdown","d2d4c373":"markdown","a8ca15c3":"markdown","322109a9":"markdown","b9335d68":"markdown","e44e7401":"markdown","72f43d8f":"markdown","88dfd506":"markdown","ae5edb96":"markdown","babe4f1f":"markdown"},"source":{"4a96e69c":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pylab as pl\n\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier \nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, validation_curve, cross_val_score\nfrom sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, roc_curve, accuracy_score, confusion_matrix, log_loss, plot_roc_curve, auc, precision_recall_curve, classification_report\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom eli5.sklearn import PermutationImportance\nimport eli5\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport pickle\nimport warnings\nimport numpy as np\nfrom sklearn import linear_model, metrics, pipeline, preprocessing\nfrom sklearn.impute import KNNImputer\n\nwarnings.filterwarnings(\"ignore\")","34723684":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","50884610":"raw_train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\nraw_test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\n# sub = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv')","22447b84":"raw_train.head()","d0d9f2b7":"raw_test.head()","40d08682":"set(raw_train.enrollee_id)&set(set(raw_test.enrollee_id))","2d296af8":"graf = sns.countplot(y=\"target\", data=raw_train, alpha=0.8)\nplt.xlabel('Number of Data', fontsize=12)\nplt.ylabel('target', fontsize=12)\n\nplt.show()","70c090da":"plt.figure(figsize=[15,17])\nfft=[\"gender\", \"relevent_experience\", \"education_level\", \"major_discipline\", \"experience\", \"company_size\", \"company_type\", \"target\"]\nn=1\nfor f in fft:\n    plt.subplot(4,2,n)\n    sns.countplot(x=f, hue='relevent_experience', edgecolor=\"black\", alpha=0.7, data=raw_train)\n    sns.despine()\n    plt.title(\"Countplot of {}  by relevent_experience\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()\n\n\n    \nplt.figure(figsize=[15,4])\nsns.countplot(x='experience', hue='education_level',edgecolor=\"black\", alpha=0.7, data=raw_train)\nsns.despine()\nplt.title(\"Countplot of experience by education_level\")\nplt.show()","b98ff43a":"education_graf = sns.countplot(x='education_level', alpha=0.8, data=raw_train)\nplt.ylabel(\"Number of Data\", fontsize=12)\nplt.xlabel(\"Education level\", fontsize=10)\nplt.show()","9a3f2c6f":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\ncd = raw_train['city_development_index'].value_counts().reset_index()\ncd.columns = [\n    'city_development_index', \n    'count'\n]\ncd['city_development_index'] = cd['city_development_index'].astype(str) + '-'\ncd = cd.sort_values(['count']).tail(50)\n\nfig = px.bar(\n    cd, \n    x='count', \n    y='city_development_index', \n    orientation='h', \n    title='Count: City development index', \n    width=1000,\n    height=900 \n)\n\nfig.show()\n\nf, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(raw_train[\"city_development_index\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of city_development_index\")\nplt.show()","389b80ea":"sns.boxplot(x='target', y='city_development_index', data=raw_train)\nplt.show()","006fe98e":"plt.figure(figsize=[10, 5])\nsns.boxplot(x='company_size', y='city_development_index', data=raw_train)\nplt.xlabel('Company size')\nplt.ylabel('City development index')\nplt.show()","9fb33a13":"sns.displot(x='training_hours',\n            hue='target',\n            data=raw_train,\n            stat=\"probability\")\nplt.show()","027c2f2b":"raw_data_age = raw_train[['enrollee_id', 'education_level', 'experience', 'last_new_job']]\\\n    .groupby(['education_level', 'experience', 'last_new_job']).sum('enrollee_id').reset_index()","af74e5d4":"raw_data_age[:5]","a32317c5":"def get_age_category(x, y):\n    if ((x in ['1', '2', '3', '4', '5', '6', '<1', '7', '8']) &\n            ((y in ['Phd']) | (y in ['Masters']) | (y in ['Graduate']))):\n        return 'Adult'\n    elif ((x in ['1', '2', '3', '<1']) & (y != 'Phd') & (y != 'Masters') &\n          (y != 'Graduate')):\n        return 'Young'\n    elif (x in ['9', '10', '11', '12', '13', '14', '15']):\n        return 'Middle'\n    elif (x in ['16', '17', '18', '19', '20', '>20']):\n        return 'Old'","81bb1cef":"raw_train['age'] = raw_train.apply(lambda row: get_age_category(row['experience'], row['education_level']), axis=1)","85948612":"raw_train.age.value_counts()","416162f9":"corr=raw_train.corr()[\"target\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","483e20de":"import missingno\nmissingno.heatmap(raw_train, cmap=\"RdYlGn\", figsize=(10,5), fontsize=12);","255273a2":"cols_object = list(raw_train.dtypes[raw_train.dtypes =='object'].index)\nt = 0\nfor i in cols_object:\n    cols_object[t] = raw_train[i].unique()  \n    cols_object[t] = [i for i in cols_object[t] if i is not np.nan]\n    t = t+1\n    \ncols_object[-1] = [i for i in cols_object[-1] if i is not  None]","edcfd6e3":"print(cols_object)","604c9cac":"cols = list(raw_train.dtypes[raw_train.dtypes =='object'].index)\n\nprint(cols)","f56790f2":"names = {\n    'city': 0, \n    'gender': 1, \n    'relevent_experience': 2, \n    'enrolled_university': 3, \n    'education_level': 4, \n    'major_discipline': 5, \n    'experience': 6, \n    'company_size': 7, \n    'company_type': 8, \n    'last_new_job': 9, \n    'age': 10\n}\n\n_dict = {key: {cols_object[names[key]][i]: i for i in range(len(cols_object[names[key]]))} for key in cols}\n\nfor key in cols:\n    raw_train[key] = raw_train[key].map(_dict[key]) ","a13c32f0":"raw_train.info()","45b81e9e":"print(\"Any missing sample in training set:\",raw_train.isnull().values.any())","bdd0b820":"raw_train.isna().mean()[raw_train.isna().mean() > 0] * 100","0a3e413b":"missing_cols = raw_train.columns[raw_train.isna().any()].tolist()\n\nmissing_cols","a6134b71":"#dataframe having features with missing values\ndf_missing = raw_train[['enrollee_id'] + missing_cols]\n\n#dataframe having features without missing values\ndf_non_missing = raw_train.drop(missing_cols, axis = 1)","6c7379a3":"knn = KNNImputer(n_neighbors = 3)\nX = np.round(knn.fit_transform(df_missing))\ndf_missing = pd.DataFrame(X, columns = df_missing.columns)","89d2032a":"#Let's join both dataframes\ntrain = pd.merge(df_missing, df_non_missing, on = 'enrollee_id')","6e13e5dc":"train","0f3f00ca":"raw_test.head()","e5d07137":"raw_test['age'] = raw_test.apply(lambda row: get_age_category(row['experience'], row['education_level']), axis=1)","b57e5b57":"raw_test.age.value_counts()","9408ef68":"for key in cols:\n    raw_test[key] = raw_test[key].map(_dict[key]) \n\n\nraw_test","7bb92cb7":"print(\"Any missing sample in test set:\",raw_test.isnull().values.any(), \"\\n\")","ab586d63":"raw_test.isna().mean()[raw_test.isna().mean() > 0] * 100","35331cdb":"#dataframe having features with missing values\ndf_missing_test = raw_test[['enrollee_id'] + missing_cols]\n\n#dataframe having features without missing values\ndf_non_missing_test = raw_test.drop(missing_cols, axis = 1)","5396d27e":"knn = KNNImputer(n_neighbors = 3)\nX = np.round(knn.fit_transform(df_missing_test))\ndf_missing_test = pd.DataFrame(X, columns = df_missing_test.columns)","3cbb1a69":"test = pd.merge(df_missing_test, df_non_missing_test, on = 'enrollee_id')","553a0db5":"print(\"Any missing sample in train set:\",train.isnull().values.any(), \"\\n\")\nprint(\"Any missing sample in test set:\",test.isnull().values.any(), \"\\n\")\n","3d11f983":"from sklearn.model_selection import train_test_split\ny = train['target']\nX = train.drop(columns=['target', 'enrollee_id', 'city_development_index', 'training_hours'])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=9)\n","0036aee0":"from xgboost import XGBClassifier\nclf_XGB = XGBClassifier()\n\nclf_XGB.fit(X_train, y_train, eval_metric='logloss')\n\ny_train_pred = clf_XGB.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf_XGB.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\n\nprint('Model params:')\nprint(clf_XGB.get_params())\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")\n\nfpr, tpr, _ = roc_curve(y_val, y_val_pred_pos)","f7014629":"def get_scores(report_df, model, X_val, y_val, name):\n\n    report = pd.DataFrame(columns={'ROC-AUC'}, data=[0])\n    report['ROC-AUC'] = roc_auc_score(y_val,\n                                      model.predict_proba(X_val)[:, 1])\n    report['F1'] = f1_score(y_val, model.predict(X_val))\n    report['precision_0'] = precision_score(\n        y_val, model.predict(X_val), pos_label=0)\n    report['precision_1'] = precision_score(\n        y_val, model.predict(X_val), pos_label=1)\n    report['recall_0'] = recall_score(\n        y_val, model.predict(X_val), pos_label=0)\n    report['recall_1'] = recall_score(\n        y_val, model.predict(X_val), pos_label=1)\n\n    report.index = [name]\n    report_df = report_df.append(report)\n    return report_df\n","2bf0e224":"df_report = pd.DataFrame()\ndf_report = get_scores(df_report, clf_XGB, X_val,\n                       y_val, 'XGBClassifier KNN')","94b02f25":"df_report","ef2e0481":"def plot_auc_curve(fpr, tpr, auc):\n    plt.figure(figsize = (16,6))\n    plt.plot(fpr,tpr,'b+',linestyle = '-')\n    plt.fill_between(fpr, tpr, alpha = 0.5)\n    plt.ylabel('True Postive Rate')\n    plt.xlabel('False Postive Rate')\n    plt.title(f'ROC Curve Having AUC = {auc}')","495178b9":"plot_auc_curve(fpr, tpr, auc_test)","285b1da3":"# funtion to plot learning curves\n\ndef plot_learning_cuve(model, X, Y):\n    \n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 11)\n    train_loss, test_loss = [], []\n    \n    for m in range(200,len(x_train),200):\n        \n        model.fit(x_train.iloc[:m,:], y_train[:m], eval_metric='logloss', verbose=False)\n        y_train_prob_pred = model.predict_proba(x_train.iloc[:m,:])\n        train_loss.append(log_loss(y_train[:m], y_train_prob_pred))\n        \n        y_test_prob_pred = model.predict_proba(x_test)\n        test_loss.append(log_loss(y_test, y_test_prob_pred))\n        \n    plt.figure(figsize = (15,8))\n    plt.plot(train_loss, 'r-+', label = 'Training Loss')\n    plt.plot(test_loss, 'b-', label = 'Test Loss')\n    plt.xlabel('Number Of Batches')\n    plt.ylabel('Log-Loss')\n    plt.legend(loc = 'best')\n\n\n\n    plt.show()","241b9d2f":"plot_learning_cuve(XGBClassifier(), X, y)","ae53f82a":"sns.countplot(y, edgecolor = 'black')\nplt.show()","ac3f1522":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state = 402)\nX_smote, y_smote = smote.fit_resample(X, y)\n\n\nsns.countplot(y_smote, edgecolor = 'black')\nplt.show()","794ca5fe":"X_train, X_val, y_train, y_val = train_test_split(X_smote, y_smote, test_size = 0.2 ,random_state = 42)\n\nclf_XGB_smote = XGBClassifier()\n\nclf_XGB_smote.fit(X_train, y_train, eval_metric='logloss')\n\ny_train_pred = clf_XGB_smote.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf_XGB_smote.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nfpr, tpr, _ = roc_curve(y_val, y_val_pred_pos)\n\n\nprint('Model params:')\nprint(clf_XGB_smote.get_params())\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")","fab3e913":"df_report = get_scores(df_report, clf_XGB_smote, X_val,\n                       y_val, 'XGBClassifier KNN(SMOTE)')\n","760619a0":"df_report","96e07258":"plot_learning_cuve(XGBClassifier(), X_smote, y_smote)","3e13964c":"from catboost import CatBoostClassifier\n\nX_train, X_val, y_train, y_val = train_test_split(X_smote, y_smote, test_size = 0.2 ,random_state = 42)\n\nCB_CLASS = CatBoostClassifier(iterations=100,\n                           learning_rate=0.1,\n                           depth=8,loss_function='Logloss',\n                             custom_loss=['AUC', 'Accuracy'],\n                             )\n\nCB_CLASS.fit(X_smote, y_smote,\n            eval_set=(X_val, y_val),\n            verbose=False)\n\ny_train_pred = CB_CLASS.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = CB_CLASS.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint('Model is fitted:' + str(CB_CLASS.is_fitted()))\nprint('Model params:')\nprint(CB_CLASS.get_params())\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")","156df02f":"df_report = get_scores(df_report, CB_CLASS, X_val,\n                       y_val, 'CB_CLASS KNN(SMOTE)')\n\ndf_report","0e260d0c":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = clf_XGB_smote, X = X_smote, y = y_smote, verbose=False, cv = 5)\nprint(\"Accuracy:{:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation:{:.2f} %\".format(accuracies.std()*100))","74c197af":"df_report","a5024b2a":"CB_CLASS = CatBoostClassifier()\n\nparameters = {'depth'         : [5, 7, 9],\n              'learning_rate' : [0.01, 0.05, 0.1],\n               'iterations'    : [30, 50, 100]\n                 }\ngrid = GridSearchCV(estimator=CB_CLASS, param_grid = parameters, cv = 5, n_jobs=-1)\ngrid.fit(X_smote, y_smote, verbose=False)\nbest_param = grid.best_estimator_\n\nprint(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)","9ee44395":"CB_CLASS = CatBoostClassifier(iterations=100,\n                           learning_rate=0.1,\n                           depth=9,loss_function='Logloss')\n\nCB_CLASS.fit(X_smote, y_smote,\n            eval_set=(X_val, y_val),\n            verbose=False)\n\ny_train_pred = CB_CLASS.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = CB_CLASS.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint('Model is fitted:' + str(CB_CLASS.is_fitted()))\nprint('Model params:')\nprint(CB_CLASS.get_params())\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")","0958692d":"df_report = get_scores(df_report, CB_CLASS, X_val,\n                       y_val, 'CB_CLASS GRID')\n\ndf_report","2c319b83":"features =[\"city\", \"city_development_index\", \"gender\", \"relevent_experience\", \"enrolled_university\", \"education_level\", \"major_discipline\", \"experience\", \"company_size\", \"company_type\", \"last_new_job\", \"training_hours\", \"age\"]\ntarget = 'target'","c58be9fa":"#Make predictions using XGB\ny_predict_XGB = clf_XGB_smote.predict(X_val)\n\ny_predict_XGB","ee5b636f":"#Make predictions using CB\ny_predict_CB = CB_CLASS.predict(X_val)\n\ny_predict_CB","b2d20f2f":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_val, y_predict_XGB)\nmetrics.auc(fpr, tpr)","b9fbf362":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_val, y_predict_CB)\nmetrics.auc(fpr, tpr)","39e7b8c3":"# Measure AUC\n\n\nThe AUC is an estimate of the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. Reference : https:\/\/www.kdnuggets.com\/2010\/09\/pub-is-auc-the-best-measure.html#:~:text=www.riceanalytics.com-,The%20area%20under%20the%20curve%20(AUC)%20that%20relates%20the%20hit,a%20randomly%20chosen%20negative%20instance.","e07f99de":"# Feature Engineering","2994950e":"# CatBoostClassifier","4b2639a9":"People with PhD and extensive experience cannot be in a group under 20 years old because this  PhD and Master's degree are not for young specialists.\n\nLet's take an approximate time frame for age categories, if there is more data, it can be made more accurate than focusing on education and experience.\n\n- young < 20 y.o.\n- adult - 20-40 y.o.\n- middle - 40-60 y.o.\n- old - >60 y.o.","71319ee6":"Let's check maches of ID in train and test data.","564a27c5":"It is a tool to find out how much a machine model benefits from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, it will not benefit much from more training data.","4bf38680":"Let's plot AUC Curve.","50f0b0b2":"# Testing Data","75e4e4ef":"Big companies usually are placed in cities with a high rating of **city development index**.","0898b28c":"The probability of a new job search increases when **city development index** is lower.","1931ad5d":"# City development index\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Lubna_Hasan\/publication\/24115086\/figure\/tbl4\/AS:668624478019607@1536423906970\/Calculation-of-CDI-by-UN-HABITAT-GUIP-Index-Formula.png\" width=\"600\">","17e9dd8a":"Let's try to increase data in balanced manner using Synthetic Minority Oversampling Technique (SMOTE)","ef8826b2":"# Training hours","fa6c91d3":"## Features\n\n* enrollee_id : Unique ID for enrollee\n* city: City code\n* citydevelopmentindex: Developement index of the city (scaled)\n* gender: Gender of enrolee\n* relevent_experience: Relevent experience of enrolee\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of enrolee\n* major_discipline :Education major discipline of enrolee\n* experience: Enrolee total experience in years\n* company_size: No of employees in current employer's company\n* company_type : Type of current employer\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n* target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n\n","d914c89d":"Here I try to measure correlation in data using Correlation coefficients.\n\nCorrelation coefficientsare used to measure how strong a relationship is between two variables.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n \n* 1 indicates a strong positive relationship.\n* -1 indicates a strong negative relationship.\n* A result of zero indicates no relationship at all.","14d5c82d":"I think that having relevant experience is important if you want to change jobs. Let\/s look on the data in this view.","a77e36bb":"# Correlation in Data","49cd6d20":"# Prediction","4d205733":"* 0 \u2013 Not looking for job change, \n* 1 \u2013 Looking for a job change\n\nWe have a big imbalance in data.  ","0ce6bab4":"Below we are plotting heatmap showing nullity correlation between various columns of dataset.\n\nThe nullity correlation ranges from -1 to 1.\n\n* -1 - Exact Negative correlation represents that if the value of one variable is present then the value of other variables is definitely absent.\n* 0 - No correlation represents that variables values present or absent do not have any effect on one another.\n* 1 - Exact Positive correlation represents that if the value of one variable is present then the value of the other is definitely present.","db8eed71":"# Model","30a62338":"# Hyperparameters of model","fe75b7a4":"# Training Data","2b1a7450":"There is a point that young students are inclined to look for a new job, but we do not have a person's age in the dataset, but we can potentially designate it using several features.","27b9a3b5":"I choose to use a k-nearest neighbour method for missing values.To do this, divide the dataframe into columns with and without missing values.","eb7429e3":"# Bar plots","078df780":"I note that these are the largest groups:\n- men\n- students\n- STEM specialization, maybe people have more choice\n- current company type PVT LTD\n- over 20 years of experience","8c862ab9":"# Prerpocessing","e062b51f":"As we can see model is overfitting the data, we can do various things to resolve this problem like we can increase data set size in balanced manner and we can also tune hyperparameters of model.","7a02adcb":"There is a high variance problem and I need to make more training data.","c49875dd":"# Cross validation","d2d4c373":"# Learning curve","a8ca15c3":"# Fill missing values","322109a9":"to be continued :)","b9335d68":"I choose to use KNN method for defining missing values.","e44e7401":"**Training hours** don't provide new correlations from the data.","72f43d8f":"# Education level","88dfd506":"Here I find unique names of columns of objects and create functions that convert all values into numbers.","ae5edb96":"# Oversampling (SMOTE)","babe4f1f":"# Target"}}