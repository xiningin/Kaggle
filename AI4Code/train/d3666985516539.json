{"cell_type":{"b914bc64":"code","4f53cab5":"code","96d7cce2":"code","0204f69b":"code","e2ea57cb":"code","20ccb5a1":"code","5501f3f0":"code","7c862564":"code","8805cbad":"code","650496d0":"code","1011e599":"code","02b9aef5":"code","420ac1d7":"code","67c4bb37":"code","d6d28163":"code","512450b0":"code","43039d22":"code","195a56b2":"code","8eb7afe7":"code","ff8a994d":"code","88648114":"code","4e2463fd":"code","9ccc8a1a":"code","8a49087f":"code","15086225":"code","0170f071":"code","19784507":"code","819b10b9":"code","2cd90f5a":"code","47534440":"code","d1f9ee25":"code","a41c2c42":"code","ce648752":"code","a93c4e62":"code","345eb3e2":"code","5a81ea6e":"code","38fcd91b":"markdown","763a39c0":"markdown","3f6a4d04":"markdown","ae0bcc01":"markdown","370c97fd":"markdown","4e4916f1":"markdown","b80257bd":"markdown"},"source":{"b914bc64":"#\n# Import Libraries\n#\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport math\nimport time\nimport nltk\n#nltk.download('wordnet')\n\nfrom scipy import stats\n\nfrom textblob import TextBlob, Word\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nstart_time = time.time()\n\n#\n# Read data from CSV file and save to DataFrame then print the data frame\n#\nfile_path_1 = r'..\/input\/fake-and-real-news-dataset\/Fake.csv'\ndf_data_1 =  pd.read_csv(file_path_1)  \n\n\n#\n# Inspect Data Set\n#\n#print('\\n\\nFake News Data: Display')\n#display(df_data_1)\n\n#print('\\n\\nFake News Data: Dimensions')\n#display(df_data_1.shape)\n\nprint('\\n\\nFake News Data: Information')\ndisplay(df_data_1.info())\n\nprint('\\n\\nFake News Data: Description')\ndisplay(df_data_1.describe())\n\nprint('\\n\\nFake News Data: Head')\ndisplay(df_data_1.head())\n\n\n#\n# Read data from CSV file and save to DataFrame then print the data frame\n#\nfile_path_2 = r'..\/input\/fake-and-real-news-dataset\/True.csv'\ndf_data_2 =  pd.read_csv(file_path_2)  \n\n\n#\n# Inspect Data Set\n#\n#print('\\n\\nReal News Data: Display')\n#display(df_data_2)\n\n#print('\\n\\nReal News Data: Dimensions')\n#display(df_data_2.shape)\n\nprint('\\n\\nReal News Data: Information')\ndisplay(df_data_2.info())\n\nprint('\\n\\nReal News Data: Description')\ndisplay(df_data_2.describe())\n\nprint('\\n\\nReal News Data: Head')\ndisplay(df_data_2.head())","4f53cab5":"#\n# Function to clean HTML characters that might still be in the text\n# Source: https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n#\ndef CleanHTMLText(Text):\n    Text = Text.str.replace('(<br\/>)', '')\n    Text = Text.str.replace('(<a).*(>).*(<\/a>)', '')\n    Text = Text.str.replace('(&amp)', '')\n    Text = Text.str.replace('(&gt)', '')\n    Text = Text.str.replace('(&lt)', '')\n    Text = Text.str.replace('(\\xa0)', ' ')\n    return Text\n\n\n#\n# Convert title and text columns to string\n#\ndf_data_1['title'] = df_data_1['title'].astype(str)\ndf_data_1['text'] = df_data_1['text'].astype(str)\n\ndf_data_2['title'] = df_data_2['title'].astype(str)\ndf_data_2['text'] = df_data_2['text'].astype(str)\n\n\n#\n# Remove leading and trailing white charcaters\n#\ndf_data_1['title'] = df_data_1['title'].str.strip()\ndf_data_1['text'] = df_data_1['text'].str.strip()\n\ndf_data_2['title'] = df_data_2['title'].str.strip()\ndf_data_2['text'] = df_data_2['text'].str.strip()\n\n\n#\n# Remove duplicate spaces, tabs, new line characters and conver them to single space\n#\ndf_data_1['title'] = df_data_1['title'].apply(lambda t: ' '.join(t.split()))\ndf_data_1['text'] = df_data_1['text'].apply(lambda t: ' '.join(t.split()))\n\ndf_data_2['title'] = df_data_2['title'].apply(lambda t: ' '.join(t.split()))\ndf_data_2['text'] = df_data_2['text'].apply(lambda t: ' '.join(t.split()))\n\n\n#\n# Clean HTML characters \/ tags from title and text\n#\ndf_data_1['title'] = CleanHTMLText(df_data_1['title'])\ndf_data_1['text'] = CleanHTMLText(df_data_1['text'])\n\ndf_data_2['title'] = CleanHTMLText(df_data_2['title'])\ndf_data_2['text'] = CleanHTMLText(df_data_2['text'])\n\n\n#\n# Add a new column to record the label (Fake \/ real).  1 for Fake and 0 for Real\n#\ndf_data_1['label'] = 1  # Fake\n\ndf_data_2['label'] = 0  # Real\n\n\n#\n# Check if there are any duplicate records based on title and text\n#\nlen_orig = len(df_data_1.index)\ndf_data_1 = df_data_1.drop_duplicates(subset = ['title','text'])\nlen_new = len(df_data_1.index)\nif len_orig != len_new:\n    print('\\n\\nFake News Data: No. of duplicate records that were removed based on title and text columns = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nFake News Data: No duplicate records based on title and text columns')\n\nlen_orig = len(df_data_2.index)\ndf_data_2 = df_data_2.drop_duplicates(subset = ['title','text'])\nlen_new = len(df_data_2.index)\nif len_orig != len_new:\n    print('\\n\\nReal News Data: No. of duplicate records that were removed based on title and text columns = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nReal News Data: No duplicate records based on title and text columns')\n\n    \n#\n# Merge both data frames into one\n#\ndf_data_all = pd.concat([df_data_1, df_data_2])\n\n\n#\n# Convert title and text columns to string\n#\ndf_data_all['title'] = df_data_all['title'].astype(str)\ndf_data_all['text'] = df_data_all['text'].astype(str)\n\n\n#    \n# Check if there are any duplicate records after merger\n#\nlen_orig = len(df_data_all.index)\ndf_data_all = df_data_all.drop_duplicates(subset = ['title','text'])\nlen_new = len(df_data_all.index)\nif len_orig != len_new:\n    print('\\n\\nAll News Data: No. of duplicate records that were removed based on title and text columns = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nAll News Data: No duplicate records based on title and text column')\n\n\n#\n# Check if there are any records with NaN (text column)\n#                           \nlen_orig = len(df_data_all.index)\ndf_data_all = df_data_all.dropna(subset = ['text'])\nlen_new = len(df_data_all.index)\nif len_orig != len_new:\n    print('\\n\\nAll News Data: No. of NaN records that were removed from text column = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nAll News Data: No records with NaN in text column')\n\n\n#\n# Check if there are any records with empty string\n#\nnan_value = float('NaN')\ndf_data_all['text'] = df_data_all['text'].replace('', nan_value)                             \nlen_orig = len(df_data_all.index)\ndf_data_all = df_data_all.dropna(subset = ['text'])\nlen_new = len(df_data_all.index)\nif len_orig != len_new:\n    print('\\n\\nAll News Data: No. of empty string records that were removed from text column = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nAll News Data: No empty string records in text column')\n\n\n#\n# Inspect Data Set\n#\n#print('\\n\\nAll News Data: Display')\n#display(df_data_all)\n\n#print('\\n\\nAll News Data: Dimensions')\n#display(df_data_all.shape)\n\nprint('\\n\\nAll News Data: Information')\ndisplay(df_data_all.info())\n\nprint('\\n\\nAll News Data: Description')\ndisplay(df_data_all.describe())\n\nprint('\\n\\nAll News Data: Head')\ndisplay(df_data_all.head())","96d7cce2":"#\n# Function to get count of upper case words in a string\n# Source: https:\/\/github.com\/Adarsh4052\/100daysofmlcode\/blob\/master\/Day%205-6%20-%20Fake%20%26%20Real%20News%20Classifier.ipynb\n#\ndef CountAllUpperCaseLetterWords(t):\n    upper_list = []\n    for word in t.split():\n        if word.isupper():\n            upper_list.append(word)\n    return len(upper_list)\n\n\n#\n# Function to get count of upper case words in a string\n#\ndef CountAllLowerCaseLetterWords(t):\n    lower_list = []\n    for word in t.split():\n        if word.islower():\n            lower_list.append(word)\n    return len(lower_list)\n\n\n#\n# Function to get average word length in a string\n# Source: https:\/\/github.com\/Adarsh4052\/100daysofmlcode\/blob\/master\/Day%205-6%20-%20Fake%20%26%20Real%20News%20Classifier.ipynb\n#\ndef AvgWordLength(t):\n    words = t.split()\n    return ( sum( len(word) for word in words ) \/ len(words))\n\n\n#    \n# Check if there are any duplicate records after merger and clean up based on text and label\n#    \nlen_orig = len(df_data_all.index)\ndf_data_all = df_data_all.drop_duplicates(subset = ['text','label'])\nlen_new = len(df_data_all.index)\nif len_orig != len_new:\n    print('\\n\\nAll News Data: No. of duplicate records that were removed based on text and label columns = ', \n          len_orig - len_new)\nelse:\n    print('\\n\\nAll News Data: No duplicate records based on text and label columns')\n\n\n#\n# Merge title and text into one column\n#\ndf_data_all['titleandtext'] = df_data_all['title'] + ' ' + df_data_all['text']\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].astype(str)\n\n\n#\n# Add columns for word count in title and text\n#\ndf_data_all['title_word_count'] = df_data_all['title'].str.split().str.len()\ndf_data_all['text_word_count'] = df_data_all['text'].str.split().str.len()\n\n\n#\n# Calculate the length for title and text\n#\ndf_data_all['title_length'] = df_data_all['title'].apply(len)\ndf_data_all['text_length'] = df_data_all['text'].apply(len)\n\n\n#\n# Calculate the number of sentences\n#\ndf_data_all['title_sentence_count'] = df_data_all['title'].str.split('.').str.len()\ndf_data_all['text_sentences_count'] = df_data_all['text'].str.split('.').str.len()\n\n\n#\n# Calculate the average word count per sentence\n#\ndf_data_all['title_sentence_avg_words'] = df_data_all['title_word_count'] \/ df_data_all['title_sentence_count']\ndf_data_all['text_sentences_avg_words'] = df_data_all['text_word_count'] \/ df_data_all['text_sentences_count']\n\n\n#\n# Calculate the number of question marks\n#\ndf_data_all['title_question_marks'] = df_data_all['title'].str.count('\\?')\ndf_data_all['text_question_marks'] = df_data_all['text'].str.count('\\?')\n\n\n#\n# Calculate the number of exclamation marks\n#\ndf_data_all['title_exclamation_marks'] = df_data_all['title'].str.count('!')\ndf_data_all['text_exclamation_marks'] = df_data_all['text'].str.count('!')\n\n\n#\n# Use TextBlob to calculate sentiment polarity of title and text\n# The polarity has a range of [-1,1]\n# Source: https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n#\ndf_data_all['title_polarity'] = df_data_all['title'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf_data_all['text_polarity'] = df_data_all['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n\n\n#\n# Add count of words of capitalized words, all lower case words, all upper case words and other words\n# Count all other words (that are no all uppercase or all lowercase)\n# Also calculate % of each category\n#\ndf_data_all['title_lcase_count'] = df_data_all['title'].apply(lambda t: CountAllLowerCaseLetterWords(t))\ndf_data_all['text_lcase_count'] = df_data_all['text'].apply(lambda t: CountAllLowerCaseLetterWords(t))\ndf_data_all['title_ucase_count'] = df_data_all['title'].apply(lambda t: CountAllUpperCaseLetterWords(t))\ndf_data_all['text_ucase_count'] = df_data_all['text'].apply(lambda t: CountAllUpperCaseLetterWords(t))\ndf_data_all['title_other_case_count'] = df_data_all['title_word_count'] - df_data_all['title_lcase_count'] - df_data_all['title_ucase_count'] \ndf_data_all['text_other_case_count'] = df_data_all['text_word_count'] - df_data_all['text_lcase_count'] - df_data_all['text_ucase_count'] \n\ndf_data_all['title_lcase_pct'] = df_data_all['title_lcase_count'] \/ df_data_all['title_word_count']\ndf_data_all['text_lcase_pct'] = df_data_all['text_lcase_count'] \/ df_data_all['text_word_count']\ndf_data_all['title_ucase_pct'] = df_data_all['title_ucase_count'] \/ df_data_all['title_word_count']\ndf_data_all['text_ucase_pct'] = df_data_all['text_ucase_count'] \/ df_data_all['text_word_count']\ndf_data_all['title_other_case_pct'] = df_data_all['title_other_case_count'] \/ df_data_all['title_word_count']\ndf_data_all['text_other_case_pct'] = df_data_all['text_other_case_count'] \/ df_data_all['text_word_count']\n\n\n#\n# Add average word length for titles and text\n#\ndf_data_all['title_avg_word_length'] = df_data_all['title'].apply(lambda t: AvgWordLength(t))\ndf_data_all['text_avg_word_length'] = df_data_all['text'].apply(lambda t: AvgWordLength(t))\n\n\n# \n# Rearrange data frame columns\n#\ndf_data_all = df_data_all[['title', 'text', 'titleandtext', 'title_word_count', 'text_word_count', 'title_length', 'text_length', \n                           'title_avg_word_length', 'text_avg_word_length', 'title_polarity', 'text_polarity',\n                           'title_lcase_count', 'text_lcase_count', 'title_ucase_count', 'text_ucase_count', 'title_other_case_count', \n                           'text_other_case_count', 'title_lcase_pct', 'text_lcase_pct', 'title_ucase_pct', 'text_ucase_pct', \n                           'title_other_case_pct', 'text_other_case_pct', 'title_sentence_count', 'text_sentences_count', \n                           'title_sentence_avg_words', 'text_sentences_avg_words', 'title_question_marks', 'text_question_marks', \n                           'title_exclamation_marks', 'text_exclamation_marks', 'label']]\n\n\n#\n# Describe the data set.\n#\nprint('\\n\\nAll News Data: Description')\ndisplay(df_data_all[['title','text', 'titleandtext']].describe())\ndisplay(df_data_all[['title_word_count', 'text_word_count', 'title_length', 'text_length', \n                     'title_avg_word_length', 'text_avg_word_length', 'title_polarity', 'text_polarity']].describe())\ndisplay(df_data_all[['title_lcase_count', 'text_lcase_count', 'title_ucase_count', 'text_ucase_count', 'title_other_case_count', \n                     'text_other_case_count', 'title_lcase_pct', 'text_lcase_pct', 'title_ucase_pct', 'text_ucase_pct', \n                     'title_other_case_pct', 'text_other_case_pct']].describe())\ndisplay(df_data_all[['title_sentence_count', 'text_sentences_count', 'title_sentence_avg_words', 'text_sentences_avg_words', \n                     'title_question_marks', 'text_question_marks', 'title_exclamation_marks', 'text_exclamation_marks']].describe())","0204f69b":"#\n# Clean the text column that will be passed to the TFDIFVectorizer\n# Source: https:\/\/github.com\/Adarsh4052\/100daysofmlcode\/blob\/master\/Day%205-6%20-%20Fake%20%26%20Real%20News%20Classifier.ipynb\n#\n\n\n#\n# Chane all words to lowercase\n#\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].apply(lambda t: ' '.join(word.lower() for word in t.split()))\n\n\n#\n# Remove stop words from the text\n#\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].apply(lambda t: ' '.join(word for word in t.split() if word not in STOPWORDS))\n\n\n#\n# Remove punctuation from text\n#\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].str.replace('[^\\w\\s]' , '')\n\n\n#\n# remove numbers\n#\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].apply(lambda t: ' '.join(word for word in t.split() if not word.isnumeric()))\n\n# Remove frequent words\n# all_words = ' '.join(df_data_all['titleandtext'] ).split()\n# # let's keep the threshold of 28 K which almost equal to number of data instances in the dataset\n# freq_words = pd.Series(all_words).value_counts()[:20]\n# # remove freq_words\n# df_data_all['titleandtext'] = df_data_all['titleandtext'].apply( lambda t: ' '.join( word for word in t.split() if word not in freq_words)) \n\n# remove rare words\n# all_words = ' '.join( df_data_all['titleandtext'] ).split()\n# rare_words = pd.Series( all_words ).value_counts()[ -200000 : ]\n# rare_words.sort_values\n# # remove rare_words\n# df_data_all['titleandtext'] = df_data_all['titleandtext'].apply( lambda t: ' '.join( word for word in t.split() if word not in rare_words))\n\n\n#\n# lemmatization\n#\ndf_data_all['titleandtext'] = df_data_all['titleandtext'].apply(lambda t: ' '.join([Word(word).lemmatize() for word in t.split()]))\n\n\n#\n# Create new DF for titletext and label.  This will be used to TFDIF\n#\ndf_titletext = pd.DataFrame({'titleandtext': df_data_all['titleandtext'], \n                             'label': df_data_all['label']})","e2ea57cb":"#\n# Data Exploration - display count for each label (dependent variable)\n# Source: https:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n# Source: https:\/\/stackoverflow.com\/questions\/31749448\/how-to-add-percentages-on-top-of-bars-in-seaborn\n#\ndef DisplayLabelBreakdown(header, data, label_list, label_col_name):\n    print('\\n\\n' + header + ': Record count by Label')\n    print(data[label_col_name].value_counts())\n\n    plt.figure(figsize = (7, 5), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    ax = sns.countplot(x = label_col_name, data = data, alpha = 0.7)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() \/ 2.,\n                height + 3,\n                '{:1.2f}'.format(height \/ len(data) * 100) + '%',\n                ha = 'center')\n    ax.set_xticklabels(label_list)\n    plt.suptitle(header + ': Record count by Label', fontsize = 16)\n    plt.xlabel('Label', fontsize = 14)\n    plt.ylabel('Count', fontsize = 14)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n    plt.show()\n\n\nlabel_dict = {0: 'Real',\n              1: 'Fake'}\n\n\n#\n# Plot record breakdown by label\n#\nDisplayLabelBreakdown('All Newa Data', df_data_all, label_dict.values(), 'label')\n\n\n#\n# Print numeric feature description by label\n#\nprint('\\n\\nAll Newa Data: Description')\ndisplay(df_data_all[['title_word_count', 'text_word_count', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_length', 'text_length', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_polarity', 'text_polarity', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_avg_word_length', 'text_avg_word_length', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_lcase_count', 'text_lcase_count', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_ucase_count', 'text_ucase_count', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_other_case_count', 'text_other_case_count', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_lcase_pct', 'text_lcase_pct', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_ucase_pct', 'text_ucase_pct', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_other_case_pct', 'text_other_case_pct', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_sentence_count', 'text_sentences_count', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_sentence_avg_words', 'text_sentences_avg_words', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_question_marks', 'text_question_marks', 'label']].groupby('label').describe())\ndisplay(df_data_all[['title_exclamation_marks', 'text_exclamation_marks', 'label']].groupby('label').describe())","20ccb5a1":"#\n# Function to Plot histogram, density plot and box plot for one independat variable with binary classes\n# Source: https:\/\/towardsdatascience.com\/histograms-and-density-plots-in-python-f6bda88f5ac0\n# Source: https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n#\ndef PlotBinaryClassCharts(header, data, feature_col_name, label_col_name, x_axis_label, label_list):\n    #\n    # Plot bivariate histogram by label\n    #\n    fig, ax = plt.subplots(ncols = 3, figsize = (21, 5), facecolor = 'lightgrey')\n    fig.suptitle(header, fontsize = 18)\n    sns.set(style = 'darkgrid', palette = 'hls')\n    data[data[label_col_name] == 0][feature_col_name].hist(ax = ax[0], alpha = 0.7, bins = 30, label = label_list[0])\n    data[data[label_col_name] == 1][feature_col_name].hist(ax = ax[0], alpha = 0.7, bins = 30, label = label_list[1])\n    ax[0].set_title('Histogram by Label', size = 16)\n    ax[0].set_xlabel(x_axis_label, fontsize = 14)\n    ax[0].set_ylabel('Count', fontsize = 14)\n    ax[0].tick_params(axis = 'both', labelsize = 12)\n#    ax[0].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n#    ax[0].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(float(x), ',.1f')))\n    ax[0].legend()\n    \n    \n    #\n    # Plot bivariate density plot by label\n    #\n    for key in label_list:\n        try:\n            sns.distplot(data[feature_col_name][data[label_col_name] == key], \n                         hist = True, \n                         kde = True,\n                         kde_kws = {'linewidth': 1},\n                         label = label_list[key],\n                         bins = 30,\n                         ax = ax[1])\n        except Exception as e:\n            print('\\n\\nDensity plot for ' + header + ' error: ' + str(e))\n    ax[1].set_title('Density Plot by Label', size = 16)\n    ax[1].set_xlabel(x_axis_label, fontsize = 14)\n    ax[1].set_ylabel('Density', fontsize = 14)\n    ax[1].tick_params(axis = 'both', labelsize = 12)\n#    ax[1].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(float(x), ',.2f')))\n#    ax[1].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(float(x), ',.1f')))\n    ax[1].legend()\n    \n    \n    #\n    # Plot bivariate box plot by label\n    #\n    data_to_plot = [data[data[label_col_name] == 0][feature_col_name], data[data[label_col_name] == 1][feature_col_name]]\n    bp = ax[2].boxplot(data_to_plot)\n    plt.xticks([1, 2], [label_list[0], label_list[1]])\n    ax[2].set_title('Box Plot by Label', size = 16)\n    ax[2].set_xlabel('Label', fontsize = 14)\n    ax[2].set_ylabel(x_axis_label, fontsize = 14)\n    ax[2].tick_params(axis = 'both', labelsize = 12)\n#    ax[2].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(float(x), ',.1f')))\n    plt.show()\n    \n    \n#\n# Plot vivarate chars for original features\n#\nPlotBinaryClassCharts('Title Word Count', df_data_all, 'title_word_count', 'label', 'Word Count', label_dict)\nPlotBinaryClassCharts('Title Length', df_data_all, 'title_length', 'label', 'Length', label_dict)\nPlotBinaryClassCharts('Title Average Word Length', df_data_all, 'title_avg_word_length', 'label', 'Average Word Length', label_dict)\nPlotBinaryClassCharts('Title Lowercase Word Count', df_data_all, 'title_lcase_count', 'label', 'Lower Case Word Count', label_dict)\nPlotBinaryClassCharts('Title Lowercase Word Percentage', df_data_all, 'title_lcase_pct', 'label', 'Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Title Uppercase Word Count', df_data_all, 'title_ucase_count', 'label', 'Uppercase Word Count', label_dict)\nPlotBinaryClassCharts('Title Uppercase Word Percentage', df_data_all, 'title_ucase_pct', 'label', 'Uppercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Title Non Uppercase\/Lowercase Word Count', df_data_all, 'title_other_case_count', 'label', 'Non Uppercase\/Lowercase Word Count', label_dict)\nPlotBinaryClassCharts('Title Non Uppercase\/Lowercase Word Percentage', df_data_all, 'title_other_case_pct', 'label', 'Non Uppercase\/Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Title Polarity', df_data_all, 'title_polarity', 'label', 'Polarity', label_dict)\nPlotBinaryClassCharts('Title Sentence Count', df_data_all, 'title_sentence_count', 'label', 'Sentence Count', label_dict)\nPlotBinaryClassCharts('Title Average Words per Sentence', df_data_all, 'title_sentence_avg_words', 'label', 'Average Words per Sentence', label_dict)\nPlotBinaryClassCharts('Title Question Mark Count', df_data_all, 'title_question_marks', 'label', 'Question Mark Count', label_dict)\nPlotBinaryClassCharts('Title Exclamation Mark Count', df_data_all, 'title_exclamation_marks', 'label', 'Exclamation Mark Count', label_dict)\n\nPlotBinaryClassCharts('Text Word Count', df_data_all, 'text_word_count', 'label', 'Word Count', label_dict)\nPlotBinaryClassCharts('Text Length', df_data_all, 'text_length', 'label', 'Length', label_dict)\nPlotBinaryClassCharts('Text Average Word Length', df_data_all, 'text_avg_word_length', 'label', 'Average Word Length', label_dict)\nPlotBinaryClassCharts('Text Lowercase Word Count', df_data_all, 'text_lcase_count', 'label', 'Lower Case Word Count', label_dict)\nPlotBinaryClassCharts('Text Lowercase Word Percentage', df_data_all, 'text_lcase_pct', 'label', 'Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Text Uppercase Word Count', df_data_all, 'text_ucase_count', 'label', 'Uppercase Word Count', label_dict)\nPlotBinaryClassCharts('Text Uppercase Word Percentage', df_data_all, 'text_ucase_pct', 'label', 'Uppercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Text Non Uppercase\/Lowercase Word Count', df_data_all, 'text_other_case_count', 'label', 'Non Uppercase\/Lowercase Word Count', label_dict)\nPlotBinaryClassCharts('Text Non Uppercase\/Lowercase Word Percentage', df_data_all, 'text_other_case_pct', 'label', 'Non Uppercase\/Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Text Polarity', df_data_all, 'text_polarity', 'label', 'Polarity', label_dict)\nPlotBinaryClassCharts('Text Sentence Count', df_data_all, 'text_sentences_count', 'label', 'Sentence Count', label_dict)\nPlotBinaryClassCharts('Text Average Words per Sentence', df_data_all, 'text_sentences_avg_words', 'label', 'Average Words  per Sentence', label_dict)\nPlotBinaryClassCharts('Text Question Mark Count', df_data_all, 'text_question_marks', 'label', 'Question Mark Count', label_dict)\nPlotBinaryClassCharts('Text Exclamation Mark Count', df_data_all, 'text_exclamation_marks', 'label', 'Exclamation Mark Count', label_dict)","5501f3f0":"#\n# list of numeric columns excluding label\n#\nnumeric_columns = df_data_all[df_data_all.columns[0:-1]].select_dtypes([np.number]).columns\n\n\n#\n# Function to plot Histogram\n#\ndef PlotHistogramAll(header, data, numeric_col_list):   \n    fig = plt.figure(figsize = (21, 21), facecolor = 'lightgrey')\n    ax = fig.gca()\n    sns.set(style = 'darkgrid', palette = 'hls')\n    data[numeric_col_list].hist(ax = ax, alpha = 0.7, bins = 30)\n    fig.suptitle(header, fontsize = 18)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.show()\n\n\n#\n# Plot histogram of all features before transforming to normal distribution\n#\nPlotHistogramAll('All News Data: Histogram Before Normalization and Scaling', df_data_all, numeric_columns)\n\n\n#\n# Create a new dataframe for normalized and scaled data\n#\ndf_data_all_norm = df_data_all.copy()\n\n\n#\n# Loop through each column and determine the skewness to understand whether it is normally \n# distributed or not.\n# If skew >= 1 then it is positive highly skewed and it can be converted to normal distribution \n# by applying log10(x + 1)\n# If skew <= -1 then it is negative highly skewed and it can be converted to normal distribution \n# by applying log10(max(x+1) - x)\n# If skew > -1 and <= -0.5 then it negative modertely skewed and it can be converted to normal \n# distribution by applying sqrt(max(x+1) - x)\n# If skew >= 0.5 and < 1 then it is positve moderately skewed and it can be converted to normal \n# distribution by applying sqrt(x)\n# Else it is normally distributed\n# Source: https:\/\/www.datanovia.com\/en\/lessons\/transform-data-to-normal-distribution-in-r\/\n#\nskew_list = df_data_all_norm[numeric_columns].skew(axis = 0, skipna = True)\nprint('\\n\\nSkew Summary:')\nfor key, value in skew_list.items():\n    orig_data = df_data_all_norm[key]\n    new_data = None\n    msg = key\n    if value >= 1:\n        #Positive highly skewed\n        #new_data = np.log10(orig_data)\n        new_data = np.log10(orig_data + 1)\n        msg = msg + ': is Positive highly skewed. Skew = '\n    elif value <= -1:\n        #Negative highly skewed\n        new_data = np.log10(max(orig_data + 1) - orig_data)\n        msg = msg +': is Negative highly skewed. Skew = '\n    elif value > -1 and value <= -0.5:\n        #Negative moderately skewed\n        new_data = np.sqrt(max(orig_data + 1) - orig_data)\n        msg = msg +': is Negative moderately skewed. Skew = '\n    elif value >= 0.5 and value < 1:\n        #Positive moderately skewed\n        new_data = np.sqrt(orig_data)\n        msg = msg +': is Positive moderately skewed. Skew = '\n    else:\n        new_data = orig_data\n        msg = msg +': No change since it is normally ditributed. Skew = '    \n    df_data_all_norm[key] = new_data\n    msg = msg + str(round(value, 3))\n    print('\\t' + msg)\n\n    \n#\n# Check if there are any records with NaN, Inf, -Inf after transformation and remove them\n#                           \nlen_orig = len(df_data_all_norm.index)\ndf_data_all_norm = df_data_all_norm[~df_data_all_norm.isin([np.nan, np.inf, -np.inf]).any(1)]\nlen_new = len(df_data_all_norm.index)\nif len_orig != len_new:\n    print('\\n\\nAll News Data - Normalized and Scaled: No. of NaN, INF, -INF records that were removed from numerical columns = ', \n          len_orig - len_new, '\\n\\n')\nelse:\n    print('\\n\\nAll News Data - Normalized and Scaled: No records with NaN, INF, -INF records that were removed from numerical columns\\n\\n')\n\n    \n#\n# Plot histogram of all features after transforming to normal distribution\n#\nPlotHistogramAll('All News Data: Histogram After Normalization and Scaling', df_data_all_norm, numeric_columns)\n\n\n#\n# Scale the normalized data\n#\nscaler = StandardScaler()\ndf_data_all_norm[numeric_columns] = scaler.fit_transform(df_data_all_norm[numeric_columns])","7c862564":"#\n# Remove the outliers from the data set by using stats.zscore\n# Source: https:\/\/github.com\/pandas-dev\/pandas\/issues\/15111\n#\ndef RemoveOutliers(header, data, numeric_col_list):\n    # Remove outliers by only keeping records with zscore below 3 only.  Anything >= 3 is an outlier \n    # and will be removed\n    df_temp = data[(np.abs(stats.zscore(data[numeric_col_list])) < 3).all(axis = 1)]\n    if len(data) != len(df_temp):\n        num_outliers_removed = len(data) - len(df_temp)\n        print('\\n\\n' + header + ': No. of outlier records to be removed =', num_outliers_removed)\n        data = df_temp\n        print('\\n\\n' + header + ': Description after removing outliers:')\n        display(data.describe())\n    else:\n        print('\\n\\n' + header + ': No outlier were records')\n    return data\n\n\n#\n# Remove outliers\n#\ndf_data_all = RemoveOutliers('All News Data', df_data_all, numeric_columns)\ndf_data_all_norm = RemoveOutliers('All News Data - Normalized and Scaled', df_data_all_norm, numeric_columns)","8805cbad":"#\n# Plot bivariate charts for Normalized and Scaled Data\n#\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Word Count', df_data_all_norm, 'title_word_count', 'label', 'Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Length', df_data_all_norm, 'title_length', 'label', 'Length', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Average Word Length', df_data_all_norm, 'title_avg_word_length', 'label', 'Average Word Length', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Lowercase Word Count', df_data_all_norm, 'title_lcase_count', 'label', 'Lower Case Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Lowercase Word Percentage', df_data_all_norm, 'title_lcase_pct', 'label', 'Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Uppercase Word Count', df_data_all_norm, 'title_ucase_count', 'label', 'Uppercase Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Uppercase Word Percentage', df_data_all_norm, 'title_ucase_pct', 'label', 'Uppercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Non Uppercase\/Lowercase Word Count', df_data_all_norm, 'title_other_case_count', 'label', 'Non Uppercase\/Lowercase Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Non Uppercase\/Lowercase Word Percentage', df_data_all_norm, 'title_other_case_pct', 'label', 'Non Uppercase\/Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Polarity', df_data_all_norm, 'title_polarity', 'label', 'Polarity', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Sentence Count', df_data_all_norm, 'title_sentence_count', 'label', 'Sentence Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Average Words per Sentence', df_data_all_norm, 'title_sentence_avg_words', 'label', 'Average Words per Sentence', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Question Mark Count', df_data_all_norm, 'title_question_marks', 'label', 'Question Mark Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Title Exclamation Mark Count', df_data_all_norm, 'title_exclamation_marks', 'label', 'Exclamation Mark Count', label_dict)\n\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Word Count', df_data_all_norm, 'text_word_count', 'label', 'Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Length', df_data_all_norm, 'text_length', 'label', 'Length', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Average Word Length', df_data_all_norm, 'text_avg_word_length', 'label', 'Average Word Length', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Lowercase Word Count', df_data_all_norm, 'text_lcase_count', 'label', 'Lower Case Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Lowercase Word Percentage', df_data_all_norm, 'text_lcase_pct', 'label', 'Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Uppercase Word Count', df_data_all_norm, 'text_ucase_count', 'label', 'Uppercase Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Uppercase Word Percentage', df_data_all_norm, 'text_ucase_pct', 'label', 'Uppercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Non Uppercase\/Lowercase Word Count', df_data_all_norm, 'text_other_case_count', 'label', 'Non Uppercase\/Lowercase Word Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Non Uppercase\/Lowercase Word Percentage', df_data_all_norm, 'text_other_case_pct', 'label', 'Non Uppercase\/Lowercase Word Percentage', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Sentence Count', df_data_all_norm, 'text_sentences_count', 'label', 'Sentence Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Average Words per Sentence', df_data_all_norm, 'text_sentences_avg_words', 'label', 'Average Words per Sentence', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Question Mark Count', df_data_all_norm, 'text_question_marks', 'label', 'Question Mark Count', label_dict)\nPlotBinaryClassCharts('Normalized and Scaled without outliers: Text Exclamation Mark Count', df_data_all_norm, 'text_exclamation_marks', 'label', 'Exclamation Mark Count', label_dict)","650496d0":"#\n# Check the correlation of independent variables by plotting a heat map\n# Check if there is any correlation between independaet variables and drop highly correlated variables\n# Source: https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/\n# Source: https:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n#\ndef RemoveCorrelatedFeatures(header, data, numeric_col_list):\n    # Find the correlation matrix for independent variables\n    corr_matrix = data[numeric_col_list].corr()\n    \n    # Plot a heat map for the correlation matrix\n    plt.figure(figsize = (14, 14), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    ax = sns.heatmap(\n        corr_matrix, \n        vmin = -1, vmax = 1, center = 0,\n        cmap = sns.diverging_palette(20, 220, n = 50, s = 50, l = 50),\n        square = True)\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation = 45,\n        horizontalalignment = 'right')\n    plt.title(header +': Correlation Heatmap', fontsize = 18)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.show()\n    \n    # Set a threshold for correlation.  Any feature with corr greater than this threshold will be dropped\n    corr_threshold = 0.75\n\n    # Create correlation matrix.  Take absolute values only\n    corr_matrix = corr_matrix.abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n    \n    # Find index of feature columns with correlation greater than Correlation Threshold\n    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n    \n    if len(to_drop) > 0:\n        # Display highly correlated features\n        print('\\n\\n' + header + ': Correlated features to be dropped:')\n        print(to_drop)\n        \n        # Drop features \n        data = data.drop(data[to_drop], axis = 1)\n\n        # Inspect Data Set after removing correlated columns\n        print('\\n\\n' + header + ': Information after removing correlated columns:')\n        display(data.info())\n        print('\\n\\n' + header + ': Description after removing correlated columns:')\n        display(data.describe())\n        print('\\n\\n' + header + ': Head after removing correlated columns:')\n        display(data.head())\n    else:\n        print('\\n\\n' + header + ': No correlated features to be dropped')\n    return data\n    \ndf_data_all = RemoveCorrelatedFeatures('All News Data without outliers', df_data_all, numeric_columns)\ndf_data_all_norm = RemoveCorrelatedFeatures('All News Data - Normalized and Scaled without outliers', df_data_all_norm, numeric_columns)\n\n#\n# RFE: Recursive Feature Elimination\n# SourceL https:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n#\n\n# data_final_vars=data_final.columns.values.tolist()\n# y=['y']\n# X=[i for i in data_final_vars if i not in y]from sklearn.feature_selection import RFE\n# from sklearn.linear_model import LogisticRegressionlogreg = LogisticRegression()rfe = RFE(logreg, 20)\n# rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n# print(rfe.support_)\n# print(rfe.ranking_)\n\n# cols=[ list columns to be removed ] \n# X=os_data_X[cols]\n# y=os_data_y['y']","1011e599":"#\n# Display records breakdown per label\n#\nDisplayLabelBreakdown('All News Data without outliers', df_data_all, label_dict.values(), 'label')\nDisplayLabelBreakdown('All News Data - Normalized and Scaled without outliers', df_data_all_norm, label_dict.values(), 'label')","02b9aef5":"#\n# Function to vectorize the words and return the top n words\n# Source: https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n#\ndef get_top_n_words(corpus, n = None, stop_words = None, bigram = False):\n    if bigram:\n        vec = CountVectorizer(stop_words = stop_words, ngram_range = (2, 2)).fit(corpus)\n    else:\n        vec = CountVectorizer(stop_words = stop_words).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n\n\n#\n# Create a dictionry to store all word frequencies\n#\nall_word_dict = {'Real News Title: All words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title']),\n                 'Real News Title: All words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'], \n                                                                                   stop_words = 'english'),\n                 'Fake News Title: All words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title']),\n                 'Fake News Title: All words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'],\n                                                                                   stop_words = 'english'),\n                 'Real News Text: All words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text']),\n                 'Real News Text: All words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'],\n                                                                                   stop_words = 'english'),\n                 'Fake News Text: All words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text']),\n                 'Fake News Text: All words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'],\n                                                                                   stop_words = 'english')}\n\n\n#\n# Create a dictionry to store top 30 word frequencies\n#\ntop_word_dict = {'Real News Title: Top 30 words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'], n = 30),\n                 'Real News Title: Top 30 words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'], n = 30, \n                                                                                   stop_words = 'english'),\n                 'Fake News Title: Top 30 words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'], n = 30),\n                 'Fake News Title: Top 30 words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'], n = 30,\n                                                                                   stop_words = 'english'),\n                 'Real News Text: Top 30 words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'], n = 30),\n                 'Real News Text: Top 30 words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'], n = 30,\n                                                                                   stop_words = 'english'),\n                 'Fake News Text: Top 30 words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'], n = 30),\n                 'Fake News Text: Top 30 words without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'], n = 30,\n                                                                                   stop_words = 'english')}","420ac1d7":"#\n# Function to display multiple word clouds in one diagram for word frequencies that are stored in a dictionary\n# Source: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n# Source: https:\/\/stackoverflow.com\/questions\/54076679\/how-to-generate-wordclouds-next-to-each-other-in-python\n#\ndef ShowWordClouds(word_dict, bigram = False):\n    n_word_clouds = len(word_dict)\n    n_cols = 2\n    n_rows = np.ceil(n_word_clouds \/ n_cols)\n    plt.figure(figsize = (21, 5 * n_rows), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    i = 1\n    for key in word_dict:\n        if 'stop' in key.lower():\n            stopwords = STOPWORDS\n        else:\n            stopwords = None\n        if bigram:\n            word_cloud = WordCloud(\n                width = 1750,\n                height = 1000,\n                background_color = 'seashell',\n                stopwords = STOPWORDS).generate_from_frequencies(dict(word_dict[key]))\n        else:\n            word_cloud = WordCloud(\n                width = 1750,\n                height = 1000,\n                background_color = 'seashell',\n                stopwords = STOPWORDS).generate(str(word_dict[key]))\n        plt.subplot(n_rows, n_cols, i).set_title(key, fontdict = {'fontsize': 16})\n        plt.tight_layout(pad = 0)\n        plt.imshow(word_cloud, interpolation = 'bilinear')\n        plt.axis('off')\n        i = i + 1\n    plt.show()\n    \n    \n#\n# Display WordCloud for Top 30 words for all combinations\n#\nShowWordClouds(all_word_dict)","67c4bb37":"#\n# Function to display multiple word clouds in one diagram for word frequencies that are stored in a dictionary\n# Source: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n# Source: https:\/\/stackoverflow.com\/questions\/54076679\/how-to-generate-wordclouds-next-to-each-other-in-python\n#\ndef ShowWordFrequencyCharts(word_dict):\n    n_word_clouds = len(word_dict)\n    n_cols = 2\n    n_rows = int(np.ceil(n_word_clouds \/ n_cols))\n    fig, ax = plt.subplots(ncols = n_cols, nrows = n_rows, figsize = (21, 5 * n_rows), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    i = 1\n    for key in word_dict:\n        common_words = dict(word_dict[key])\n        names = list(common_words.keys())\n        values = list(common_words.values())\n        plt.subplot(n_rows, n_cols, i).set_title(key, fontdict = {'fontsize': 16})\n        plt.tight_layout(pad = 0)\n        plt.bar(range(len(common_words)), values, tick_label = names, color = 'skyblue', edgecolor = 'grey')\n        plt.xlabel('Word', fontsize = 14)\n        plt.ylabel('Count', fontsize = 14)\n        plt.xticks(rotation = 60, fontsize = 12)\n        plt.yticks(fontsize = 12)\n        plt.subplot(n_rows, n_cols, i).get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        i = i + 1\n    plt.show()\n\n\n#\n# Display Word Count for Top 30 words for all combinations\n#\nShowWordFrequencyCharts(top_word_dict)","d6d28163":"#\n# Create a dictionry to store all bigram frequencies\n#\nall_bigram_dict = {'Real News Title: All bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'],\n                                                                      bigram = True),\n                 'Real News Title: All bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'],\n                                                                                       stop_words = 'english',\n                                                                                       bigram = True),\n                 'Fake News Title: All bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'],\n                                                                    bigram = True),\n                 'Fake News Title: All bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'],\n                                                                                       stop_words = 'english',\n                                                                                       bigram = True),\n                 'Real News Text: All bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'],\n                                                                   bigram = True),\n                 'Real News Text: All bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'],\n                                                                                      stop_words = 'english',\n                                                                                      bigram = True),\n                 'Fake News Text: All bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'],\n                                                                   bigram = True),\n                 'Fake News Text: All bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'],\n                                                                                      stop_words = 'english',\n                                                                                      bigram = True)}\n\n\n#\n# Create a dictionry to store top 30 bigram frequencies\n#\ntop_bigram_dict = {'Real News Title: Top 30 bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'], \n                                                                         n = 30,\n                                                                         bigram = True),\n                 'Real News Title: Top 30 bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['title'], \n                                                                                          n = 30,\n                                                                                          stop_words = 'english',\n                                                                                          bigram = True),\n                 'Fake News Title: Top 30 bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'],\n                                                                       n = 30,\n                                                                       bigram = True),\n                 'Fake News Title: Top 30 bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['title'],\n                                                                                          n = 30,\n                                                                                          stop_words = 'english',\n                                                                                          bigram = True),\n                 'Real News Text: Top 30 bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'],\n                                                                      n = 30,\n                                                                      bigram = True),\n                 'Real News Text: Top 30 bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 0]['text'],\n                                                                                         n = 30,\n                                                                                         stop_words = 'english',\n                                                                                         bigram = True),\n                 'Fake News Text: Top 30 bigrams': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'],\n                                                                      n = 30,\n                                                                      bigram = True),\n                 'Fake News Text: Top 30 bigrams without stop words': get_top_n_words(corpus = df_data_all[df_data_all['label'] == 1]['text'],\n                                                                                         n = 30,\n                                                                                         stop_words = 'english',\n                                                                                         bigram = True)}","512450b0":"#\n# Display WordCloud for Top 30 bigrams for all combinations\n#\nShowWordClouds(all_bigram_dict, bigram = True)","43039d22":"#\n# Display Word Count for Top 30 biagrams for all combinations\n#\nShowWordFrequencyCharts(top_bigram_dict)\n\nprint('Execution Time: --- %s seconds ---' % round((time.time() - start_time), 2))","195a56b2":"#\n# Split the data frame into two data frames: One data frame from independent variables and \n# one for the dependant variable\n#\n\n\n#\n# Data after outlier removal\n#\nnumeric_columns = df_data_all[df_data_all.columns[0:-1]].select_dtypes([np.number]).columns\ndf_X = pd.DataFrame(df_data_all[numeric_columns])\ndf_Y = pd.DataFrame(df_data_all[df_data_all.columns[-1]])\n\n\n#\n# Data after outlier removal, transformation and scaling\n#\nnumeric_columns = df_data_all_norm[df_data_all_norm.columns[0:-1]].select_dtypes([np.number]).columns\ndf_X_norm = pd.DataFrame(df_data_all_norm[numeric_columns])\ndf_Y_norm = pd.DataFrame(df_data_all_norm[df_data_all_norm.columns[-1]])","8eb7afe7":"#\n# Split the data into train and test data frames \n# Prepare train \/ test split data (non-stratified and strartified)\n#\nsplit_pct = 0.8\n# Train\/Test: Non-stratified\ndf_X_train, df_X_test, df_Y_train, df_Y_test = train_test_split(\n                                                    df_X,\n                                                    df_Y,\n                                                    train_size = split_pct,\n                                                    random_state = 10)\n# Train\/Test: Stratified\ndf_X_train_strat, df_X_test_strat, df_Y_train_strat, df_Y_test_strat = train_test_split(\n                                                    df_X,\n                                                    df_Y,\n                                                    train_size = split_pct,\n                                                    stratify = df_Y,\n                                                    random_state = 10)\n# Train\/Test Normalized and Scaled: Non-stratified\ndf_X_norm_train, df_X_norm_test, df_Y_norm_train, df_Y_norm_test = train_test_split(\n                                                    df_X_norm,\n                                                    df_Y_norm,\n                                                    train_size = split_pct,\n                                                    random_state = 10)\n# Train\/Test Normalized and Scaled: Stratified\ndf_X_norm_train_strat, df_X_norm_test_strat, df_Y_norm_train_strat, df_Y_norm_test_strat = train_test_split(\n                                                    df_X_norm,\n                                                    df_Y_norm,\n                                                    train_size = split_pct,\n                                                    stratify = df_Y_norm,\n                                                    random_state = 10)\n\n\n#\n# Use TFIDF to calculate the Term Frequency and Inverse Document Frequency\n# This is to be applied to the titletext\n# Source: https:\/\/data-flair.training\/blogs\/advanced-python-project-detecting-fake-news\/\n#\ndf_X_tfidf = df_titletext['titleandtext']\ndf_Y_tfidf = df_titletext['label']\n\ndf_X_tfidf_train, df_X_tfidf_test, df_Y_tfidf_train, df_Y_tfidf_test = train_test_split(\n                                                    df_X_tfidf,\n                                                    df_Y_tfidf,\n                                                    train_size = split_pct,\n                                                    random_state = 10)\n# Train\/Test Normalized and Scaled: Stratified\ndf_X_tfidf_train_strat, df_X_tfidf_test_strat, df_Y_tfidf_train_strat, df_Y_tfidf_test_strat = train_test_split(\n                                                    df_X_tfidf,\n                                                    df_Y_tfidf,\n                                                    train_size = split_pct,\n                                                    stratify = df_Y_tfidf,\n                                                    random_state = 10)\n\n# Fit and teansforma all data\ntfidf_vect = TfidfVectorizer(stop_words = 'english', max_df = 0.7, analyzer = 'word', sublinear_tf = True, use_idf = True, smooth_idf = True)\ndf_X_tfidf = tfidf_vect.fit_transform(df_titletext['titleandtext'])\n\n# Fit and transform train set, transform test set - non stratified\ndf_X_tfidf_train = tfidf_vect.fit_transform(df_X_tfidf_train)\ndf_X_tfidf_test = tfidf_vect.transform(df_X_tfidf_test)\n# Fit and transform train set, transform test set - stratified\ndf_X_tfidf_train_strat = tfidf_vect.fit_transform(df_X_tfidf_train_strat)\ndf_X_tfidf_test_strat = tfidf_vect.transform(df_X_tfidf_test_strat)","ff8a994d":"#\n# calculate the fpr and tpr for all thresholds of the classification then plot ROC Curve\n# Source: https:\/\/stackoverflow.com\/questions\/25009284\/how-to-plot-roc-curve-in-python\n#\ndef PlotROCCurve(TestY, Predicted):\n    roc_auc = roc_auc_score(TestY, Predicted)\n    fpr, tpr, thresholds = roc_curve(TestY, Predicted)\n    plt.figure(figsize = (7, 5), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    plt.plot(fpr, tpr, label = 'AUC = %0.3f' % roc_auc)\n    plt.plot([0, 1], [0, 1],'c--')\n    plt.xlim([-.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize = 14)\n    plt.ylabel('True Positive Rate', fontsize = 14)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.title('Receiver Operating Characteristic', fontsize = 16)\n    plt.legend(loc = 'lower right')\n    plt.show()\n    \n\n#\n# Function to plot Tuning score (ROC AUC)\n#\ndef PlotTuningAccuracy(name, param_list, score_history_train, score_history_test, opt_param, opt_score):\n    plt.figure(figsize = (7, 5), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    if isinstance(opt_param, str):\n        plt.plot(param_list, score_history_train, label = 'Train')\n        plt.plot(param_list, score_history_test, label = 'Test - AUC = %0.3f' % opt_score + ' @ ' + name + ' = ' + opt_param)\n    else:\n        plt.plot(param_list, score_history_train,  label = 'Train')\n        plt.plot(param_list, score_history_test, label = 'Test - AUC = %0.3f' % opt_score + ' @ ' + name + ' = %0.3f' % opt_param)\n    plt.xlabel(name, fontsize = 14)\n    plt.ylabel('ROC AUC Score', fontsize = 14)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.plot(opt_param, opt_score, 'or')\n    plt.ylim([-0.05, 1.05])\n    plt.title('Tuning Hyperparameter: ' + name, fontsize = 16)\n    plt.legend(loc = 'lower right')\n    plt.show()\n    \n\n#\n# Function to plot Tconfusion matrix\n#  \ndef PlotConfusionMatrix(TestY, Predicted, label_list):    \n    cm = confusion_matrix(TestY, Predicted)\n    plt.figure(figsize = (7, 5), facecolor = 'lightgrey')\n    sns.set(style = 'darkgrid', palette = 'hls')\n    ax = sns.heatmap(cm, annot = True, cmap = sns.diverging_palette(20, 220, n = 200), fmt = 'd')\n    ax.set_xlabel('Predicted labels', fontsize = 14)\n    ax.set_ylabel('True labels', fontsize = 14)\n    ax.xaxis.set_ticklabels(label_list)\n    ax.yaxis.set_ticklabels(label_list)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.title('Confusion Matrix', fontsize = 16)\n    plt.show()","88648114":"#\n# Create a function to Run Support Vector Machine Classifier and tune it - this is for Train\/Test Set\n# The hyperparameters to be tuned: C, gamma\n# Display Classification Report, ROC Curve, Confusion Matrix \n# Source: https:\/\/data-flair.training\/blogs\/advanced-python-project-detecting-fake-news\/\n#\n@ignore_warnings(category = ConvergenceWarning)\ndef RunSVCTestTrain(model_name, TrainX, TestX, TrainY, TestY, SummaryTable, label_list):\n    \n    func_start_time = time.time()\n    lbl = model_name + ': ' + SummaryTable.at['Validation Method', model_name]\n    opt_params = {'gamma': None,\n                  'C': None}\n#                  'kernel': None}\n    \n    \n    #\n    # Tune gamma to maximize ROC\n    #\n    param_values = [0.1, 1, 10, 100]\n    score_history_train = []\n    score_history_test = []\n    for param_value in param_values:\n        svc = SVC(gamma = param_value)\n        svc.fit(TrainX, TrainY.values.ravel())\n        score_history_train.append(roc_auc_score(TrainY, svc.predict(TrainX)))\n        Predicted = svc.predict(TestX)\n        score_history_test.append(roc_auc_score(TestY, Predicted))\n    # Optimal values\n    opt_params['gamma'] = param_values[np.argmax(score_history_test)]\n    opt_auc_roc = np.amax(score_history_test)\n    print('\\n\\nOptimal gamma value = ', opt_params['gamma'])\n    print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n    # Plot the ROC AUC Score\n    PlotTuningAccuracy('gamma', param_values, score_history_train, score_history_test, opt_params['gamma'], opt_auc_roc)\n    \n    \n    #\n    # Tune C to maximize ROC\n    #\n    param_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n    score_history_train = []\n    score_history_test = []\n    for param_value in param_values:\n        svc = SVC(gamma = opt_params['gamma'],\n                  C = param_value)\n        svc.fit(TrainX, TrainY.values.ravel())\n        score_history_train.append(roc_auc_score(TrainY, svc.predict(TrainX)))\n        Predicted = svc.predict(TestX)\n        score_history_test.append(roc_auc_score(TestY, Predicted))\n    # Optimal values\n    opt_params['C'] = param_values[np.argmax(score_history_test)]\n    opt_auc_roc = np.amax(score_history_test)\n    print('\\n\\nOptimal C value = ', opt_params['C'])\n    print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n    # Plot the ROC AUC Score\n    PlotTuningAccuracy('C', param_values, score_history_train, score_history_test, opt_params['C'], opt_auc_roc)\n    \n    \n#     #\n#     # Tune kernel to maximize ROC\n#     #\n#     param_values = ['linear', 'rbf', 'poly']\n#     score_history_train = []\n#     score_history_test = []\n#     for param_value in param_values:\n#         svc = SVC(gamma = opt_params['gamma'],\n#                   C = opt_params['C'],\n#                   kernel = param_value)\n#         svc.fit(TrainX, TrainY.values.ravel())\n#         score_history_train.append(roc_auc_score(TrainY, svc.predict(TrainX)))\n#         Predicted = svc.predict(TestX)\n#         score_history_test.append(roc_auc_score(TestY, Predicted))\n#     # Optimal values\n#     opt_params['kernel'] = param_values[np.argmax(score_history_test)]\n#     opt_auc_roc = np.amax(score_history_test)\n#     print('\\n\\nOptimal kernel value = ', opt_params['kernel'])\n#     print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n#     # Plot the ROC AUC Score\n#     PlotTuningAccuracy('kernel', param_values, score_history_train, score_history_test, opt_params['kernel'], opt_auc_roc)\n    \n    \n    #\n    # Create SVC based on optimal hyperparatmers \n    #\n    svc = SVC(gamma = opt_params['gamma'],\n              C = opt_params['C'])\n#              kernel = opt_params['kernel'])\n    \n    # Train SVC\n    svc = svc.fit(TrainX, TrainY.values.ravel())\n    \n    # Predict the class for the Test data\n    Predicted = svc.predict(TestX)\n\n    # Calculate error, MSE, RMSE\n#    mse = mean_squared_error(np.array(TestY, dtype = np.float32), np.array(Predicted, dtype = np.float32))\n#    rmse = math.sqrt(mse)\n    accuracy = accuracy_score(TestY, Predicted)\n    precision, recall, fscore, support = precision_recall_fscore_support(TestY, Predicted, average = None)\n    roc_auc = roc_auc_score(TestY, Predicted)\n    \n    # Display classification report\n    print('\\n\\nClassification Report for Optimal Model:')\n    cs_r = classification_report(TestY, Predicted, target_names = label_list)\n    print(cs_r)\n\n    # Plot ROC AUC \n    PlotROCCurve(TestY, Predicted)\n    \n    #Display confusion matrix\n    PlotConfusionMatrix(TestY, Predicted, label_list)\n    \n    # Update the summary table by including the results for this model\n    SummaryTable.at['Feature Count', model_name] = TrainX.shape[1]\n    SummaryTable.at['Total Records', model_name] = TrainX.shape[0] + TestY.shape[0]\n    SummaryTable.at['Optimal C', model_name] = opt_params['C']\n    SummaryTable.at['Optimal gamma', model_name] = opt_params['gamma']\n#    SummaryTable.at['Optimal kernel', model_name] = opt_params['kernel']\n    SummaryTable.at['Accuracy Score', model_name] = round(accuracy, 3)\n    SummaryTable.at['ROC AUC Score', model_name] = round(roc_auc, 3)\n    SummaryTable.at['Precision - Class 0', model_name] = round(precision[0], 3) \n    SummaryTable.at['Precision - Class 1', model_name] = round(precision[1], 3)\n    SummaryTable.at['Recall - Class 0', model_name] = round(recall[0], 3) \n    SummaryTable.at['Recall - Class 1', model_name] = round(recall[1], 3) \n    SummaryTable.at['F1-Score - Class 0', model_name] = round(fscore[0], 3) \n    SummaryTable.at['F1-Score - Class 1', model_name] = round(fscore[1], 3) \n    SummaryTable.at['Support - Class 0', model_name] = round(support[0], 3) \n    SummaryTable.at['Support - Class 1', model_name] = round(support[1], 3) \n    SummaryTable.at['Execution Time', model_name] = round((time.time() - func_start_time), 2)\n    display(SummaryTable)\n    \n    print('Function Execution Time: --- %s seconds ---' % round((time.time() - func_start_time), 2))\n\n    \n#\n# Create a function to Run SVC and tune it - this is for KFolds\n# The hyperparameters to be tuned: C, gamma\n# Display Classification Report, ROC Curve, Confusion Matrix \n# Source: https:\/\/data-flair.training\/blogs\/advanced-python-project-detecting-fake-news\/\n# Source: https:\/\/medium.com\/@haydar_ai\/learning-data-science-day-22-cross-validation-and-parameter-tuning-b14bcbc6b012\n# Source: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_multi_metric_evaluation.html\n#\n@ignore_warnings(category = ConvergenceWarning)\ndef RunSVCKFolds(model_name, DataX, DataY, Stratified, SummaryTable, label_list):\n    \n    func_start_time = time.time()\n    lbl = model_name + ': ' + SummaryTable.at['Validation Method', model_name]\n    opt_params = {'gamma': None,\n                  'C': None}\n#                  'kernel': None}\n    \n    \n    #\n    # create cross validation object and get splits\n    #\n    if Stratified:\n        cv = StratifiedKFold(n_splits = 5, random_state = 10, shuffle = True)\n    else:\n        cv = KFold(n_splits = 5, random_state = 10, shuffle = True)\n    \n    cv.get_n_splits(DataX, DataY.values.ravel())\n        \n        \n    #\n    # Prepare hyperparamter ranges and scoring\n    #\n    C_List = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n    gamma_list = [0.1, 1, 10, 100]\n#    kernel_list = ['linear', 'rbf', 'poly']\n    parameter_grid = {'C': C_List,\n                      'gamma': gamma_list}\n#                      'kernel': kernel_list}\n    scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n    \n    \n    #\n    # Create classifer object and run grid search to find optimal paramters\n    #\n    svc = SVC()\n    \n    grid_search = GridSearchCV(svc, \n                               param_grid = parameter_grid, \n                               cv = cv,  \n                               scoring = scoring, \n                               refit = 'AUC', \n                               return_train_score = True)\n    \n    grid_search.fit(DataX, DataY.values.ravel())\n    \n    print('\\n\\nBest parameters: {}'.format(grid_search.best_params_))\n    opt_params['C'] = grid_search.best_params_.get('C')\n    opt_params['gamma'] = grid_search.best_params_.get('gamma')\n#    opt_params['kernel'] = grid_search.best_params_.get('kernel')\n    \n    \n    #\n    # Create SVC based on optimal hyperparatmers \n    #\n    svc = grid_search.best_estimator_\n    \n    # Predict the class for the data\n    Predicted = svc.predict(DataX)\n    \n    # Calculate error, MSE, RMSE\n#    mse = mean_squared_error(np.array(TestY, dtype = np.float32), np.array(Predicted, dtype = np.float32))\n#    rmse = math.sqrt(mse)\n    accuracy = accuracy_score(DataY, Predicted)\n    precision, recall, fscore, support = precision_recall_fscore_support(DataY, Predicted, average = None)\n    roc_auc = roc_auc_score(DataY, Predicted)\n    \n    # Display classification report\n    print('\\n\\nClassification Report for Optimal Model:')\n    cs_r = classification_report(DataY, Predicted, target_names = label_list)\n    print(cs_r)\n\n    # Plot ROC AUC \n    PlotROCCurve(DataY, Predicted)\n    \n    #Display confusion matrix\n    PlotConfusionMatrix(DataY, Predicted, label_list)\n    \n    # Update the summary table by including the results for this model\n    SummaryTable.at['Feature Count', model_name] = DataX.shape[1]\n    SummaryTable.at['Total Records', model_name] = DataX.shape[0]\n    SummaryTable.at['Optimal C', model_name] = opt_params['C']\n    SummaryTable.at['Optimal gamma', model_name] = opt_params['gamma']\n#    SummaryTable.at['Optimal kernel', model_name] = opt_params['kernel']\n    SummaryTable.at['Accuracy Score', model_name] = round(accuracy, 3)\n    SummaryTable.at['ROC AUC Score', model_name] = round(roc_auc, 3)\n    SummaryTable.at['Precision - Class 0', model_name] = round(precision[0], 3) \n    SummaryTable.at['Precision - Class 1', model_name] = round(precision[1], 3)\n    SummaryTable.at['Recall - Class 0', model_name] = round(recall[0], 3) \n    SummaryTable.at['Recall - Class 1', model_name] = round(recall[1], 3) \n    SummaryTable.at['F1-Score - Class 0', model_name] = round(fscore[0], 3) \n    SummaryTable.at['F1-Score - Class 1', model_name] = round(fscore[1], 3) \n    SummaryTable.at['Support - Class 0', model_name] = round(support[0], 3) \n    SummaryTable.at['Support - Class 1', model_name] = round(support[1], 3) \n    SummaryTable.at['Execution Time', model_name] = round((time.time() - func_start_time), 2)\n    display(SummaryTable)\n    \n    print('Function Execution Time: --- %s seconds ---' % round((time.time() - func_start_time), 2))\n\n    \n#\n# Create a dataframe to store the results of different models\n#\ndf_SVC_summary_1 = pd.DataFrame({\n    'SVC_01':['Support Vector Classifier','Train\/Test','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'SVC_02':['Support Vector Classifier','Train\/Test - Stratified','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'SVC_03':['Support Vector Classifier','Train\/Test','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'SVC_04':['Support Vector Classifier','Train\/Test - Stratified','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]})\n\ndf_SVC_summary_1.index = ['Algorithm Name',\n                          'Validation Method',\n                          'Outliers Removed',\n                          'Normalized and Scaled',\n                          'Feature Type',\n                          'Feature Count',\n                          'Total Records',\n                          'Optimal gamma',\n                          'Optimal C',\n#                          'Optimal kernel',\n                          'Accuracy Score',\n                          'ROC AUC Score',\n                          'Precision - Class 0',\n                          'Precision - Class 1',\n                          'Recall - Class 0',\n                          'Recall - Class 1',\n                          'F1-Score - Class 0',\n                          'F1-Score - Class 1',\n                          'Support - Class 0',\n                          'Support - Class 1',\n                          'Execution Time']","4e2463fd":"RunSVCTestTrain(model_name = 'SVC_01',\n                TrainX = df_X_train,\n                TestX = df_X_test,\n                TrainY = df_Y_train,\n                TestY = df_Y_test,\n                SummaryTable = df_SVC_summary_1,\n                label_list = label_dict.values())","9ccc8a1a":"RunSVCTestTrain(model_name = 'SVC_02',\n                TrainX = df_X_train_strat,\n                TestX = df_X_test_strat,\n                TrainY = df_Y_train_strat,\n                TestY = df_Y_test_strat,\n                SummaryTable = df_SVC_summary_1,\n                label_list = label_dict.values())","8a49087f":"RunSVCTestTrain(model_name = 'SVC_03',\n                TrainX = df_X_norm_train,\n                TestX = df_X_norm_test,\n                TrainY = df_Y_norm_train,\n                TestY = df_Y_norm_test,\n                SummaryTable = df_SVC_summary_1,\n                label_list = label_dict.values())","15086225":"RunSVCTestTrain(model_name = 'SVC_04', \n                TrainX = df_X_norm_train_strat,\n                TestX = df_X_norm_test_strat,\n                TrainY = df_Y_norm_train_strat,\n                TestY = df_Y_norm_test_strat,\n                SummaryTable = df_SVC_summary_1,\n                label_list = label_dict.values())","0170f071":"#\n# Create a function to Run Random Forest Classifier and tune it - this is for Train\/Test Set\n# The hyperparameters to be tuned: max_depth, max_features, bootstrap\n# Display Classification Report, ROC Curve, Confusion Matrix \n# Source: https:\/\/data-flair.training\/blogs\/advanced-python-project-detecting-fake-news\/\n# Source: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n#\n@ignore_warnings(category = ConvergenceWarning)\ndef RunRFTTestTrain(model_name, TrainX, TestX, TrainY, TestY, SummaryTable, label_list):\n    \n    func_start_time = time.time()\n    lbl = model_name + ': ' + SummaryTable.at['Validation Method', model_name]\n    opt_params = {'max_depth': None,\n                  'max_features': None,\n                  'bootstrap': None}\n#                  'min_samples_leaf': None,\n#                  'min_samples_split': None}\n    \n    \n    #\n    # Tune max_depth to maximize ROC\n    #\n    param_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n    score_history_train = []\n    score_history_test = []\n    for param_value in param_values:\n        rft = RandomForestClassifier(max_depth = param_value)\n        rft.fit(TrainX, TrainY.values.ravel())\n        score_history_train.append(roc_auc_score(TrainY, rft.predict(TrainX)))\n        Predicted = rft.predict(TestX)\n        score_history_test.append(roc_auc_score(TestY, Predicted))\n    # Optimal values\n    opt_params['max_depth'] = param_values[np.argmax(score_history_test)]\n    opt_auc_roc = np.amax(score_history_test)\n    print('\\n\\nOptimal max_depth value = ', opt_params['max_depth'])\n    print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n    # Plot the ROC AUC Score\n    PlotTuningAccuracy('max_depth', param_values, score_history_train, score_history_test, opt_params['max_depth'], opt_auc_roc)\n    \n    \n    #\n    # Tune max_features to maximize ROC\n    #\n    param_values = ['auto', 'sqrt']\n    score_history_train = []\n    score_history_test = []\n    for param_value in param_values:\n        rft = RandomForestClassifier(max_depth = opt_params['max_depth'],\n                  max_features = param_value)\n        rft.fit(TrainX, TrainY.values.ravel())\n        score_history_train.append(roc_auc_score(TrainY, rft.predict(TrainX)))\n        Predicted = rft.predict(TestX)\n        score_history_test.append(roc_auc_score(TestY, Predicted))\n    # Optimal values\n    opt_params['max_features'] = param_values[np.argmax(score_history_test)]\n    opt_auc_roc = np.amax(score_history_test)\n    print('\\n\\nOptimal max_features value = ', opt_params['max_features'])\n    print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n    # Plot the ROC AUC Score\n    PlotTuningAccuracy('max_features', param_values, score_history_train, score_history_test, opt_params['max_features'], opt_auc_roc)\n    \n    \n    #\n    # Tune bootstrap to maximize ROC\n    #\n    param_values = [True, False]\n    score_history_train = []\n    score_history_test = []\n    for param_value in param_values:\n        rft = RandomForestClassifier(max_depth = opt_params['max_depth'],\n                  max_features = opt_params['max_features'],\n                  bootstrap = param_value)\n        rft.fit(TrainX, TrainY.values.ravel())\n        score_history_train.append(roc_auc_score(TrainY, rft.predict(TrainX)))\n        Predicted = rft.predict(TestX)\n        score_history_test.append(roc_auc_score(TestY, Predicted))\n    # Optimal values\n    opt_params['bootstrap'] = param_values[np.argmax(score_history_test)]\n    opt_auc_roc = np.amax(score_history_test)\n    print('\\n\\nOptimal bootstrap value = ', opt_params['bootstrap'])\n    print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n    # Plot the ROC AUC Score\n    PlotTuningAccuracy('bootstrap', param_values, score_history_train, score_history_test, opt_params['bootstrap'], opt_auc_roc) \n    \n\n#     #\n#     # Tune min_samples_leaf to maximize ROC\n#     #\n#     param_values = [1, 2, 4]\n#     score_history_train = []\n#     score_history_test = []\n#     for param_value in param_values:\n#         rft = RandomForestClassifier(max_depth = opt_params['max_depth'],\n#                   max_features = opt_params['max_features'],\n#                   bootstrap = opt_params['bootstrap'],\n#                   min_samples_leaf = param_value)\n#         rft.fit(TrainX, TrainY.values.ravel())\n#         score_history_train.append(roc_auc_score(TrainY, rft.predict(TrainX)))\n#         Predicted = rft.predict(TestX)\n#         score_history_test.append(roc_auc_score(TestY, Predicted))\n#     # Optimal values\n#     opt_params['min_samples_leaf'] = param_values[np.argmax(score_history_test)]\n#     opt_auc_roc = np.amax(score_history_test)\n#     print('\\n\\nOptimal min_samples_leaf value = ', opt_params['min_samples_leaf'])\n#     print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n#     # Plot the ROC AUC Score\n#     PlotTuningAccuracy('min_samples_leaf', param_values, score_history_train, score_history_test, opt_params['min_samples_leaf'], opt_auc_roc) \n\n\n#     #\n#     # Tune min_samples_split to maximize ROC\n#     #\n#     param_values = [2, 5, 10]\n#     score_history_train = []\n#     score_history_test = []\n#     for param_value in param_values:\n#         rft = RandomForestClassifier(max_depth = opt_params['max_depth'],\n#                   max_features = opt_params['max_features'],\n#                   bootstrap = opt_params['bootstrap'],\n#                   min_samples_leaf = opt_params['min_samples_leaf'],\n#                   min_samples_split = param_value)\n#         rft.fit(TrainX, TrainY.values.ravel())\n#         score_history_train.append(roc_auc_score(TrainY, rft.predict(TrainX)))\n#         Predicted = rft.predict(TestX)\n#         score_history_test.append(roc_auc_score(TestY, Predicted))\n#     # Optimal values\n#     opt_params['min_samples_split'] = param_values[np.argmax(score_history_test)]\n#     opt_auc_roc = np.amax(score_history_test)\n#     print('\\n\\nOptimal min_samples_split value = ', opt_params['min_samples_split'])\n#     print('ROC AUC at optimal value = ', round(opt_auc_roc, 3))\n#     # Plot the ROC AUC Score\n#     PlotTuningAccuracy('min_samples_split', param_values, score_history_train, score_history_test, opt_params['min_samples_split'], opt_auc_roc) \n\n\n    #\n    # Create RFT based on optimal hyperparatmers \n    #\n    rft = RandomForestClassifier(max_depth = opt_params['max_depth'],\n             max_features = opt_params['max_features'],\n             bootstrap = opt_params['bootstrap'])\n#             min_samples_leaf = opt_params['min_samples_leaf'],\n#             min_samples_split = opt_params['min_samples_split'])\n    \n    # Train RFT\n    rft = rft.fit(TrainX, TrainY.values.ravel())\n    \n    # Predict the class for the Test data\n    Predicted = rft.predict(TestX)\n\n    # Calculate error, MSE, RMSE\n#    mse = mean_squared_error(np.array(TestY, dtype = np.float32), np.array(Predicted, dtype = np.float32))\n#    rmse = math.sqrt(mse)\n    accuracy = accuracy_score(TestY, Predicted)\n    precision, recall, fscore, support = precision_recall_fscore_support(TestY, Predicted, average = None)\n    roc_auc = roc_auc_score(TestY, Predicted)\n    \n    # Display classification report\n    print('\\n\\nClassification Report for Optimal Model:')\n    cs_r = classification_report(TestY, Predicted, target_names = label_list)\n    print(cs_r)\n\n    # Plot ROC AUC \n    PlotROCCurve(TestY, Predicted)\n    \n    #Display confusion matrix\n    PlotConfusionMatrix(TestY, Predicted, label_list)\n    \n    # Update the summary table by including the results for this model\n    SummaryTable.at['Feature Count', model_name] = TrainX.shape[1]\n    SummaryTable.at['Total Records', model_name] = TrainX.shape[0] + TestY.shape[0]\n    SummaryTable.at['Optimal max_depth', model_name] = opt_params['max_depth']\n    SummaryTable.at['Optimal max_features', model_name] = opt_params['max_features']\n    SummaryTable.at['Optimal bootstrap', model_name] = opt_params['bootstrap']\n#    SummaryTable.at['Optimal min_samples_leaf', model_name] = opt_params['min_samples_leaf']\n#    SummaryTable.at['Optimal min_samples_split', model_name] = opt_params['min_samples_split']\n    SummaryTable.at['Accuracy Score', model_name] = round(accuracy, 3)\n    SummaryTable.at['ROC AUC Score', model_name] = round(roc_auc, 3)\n    SummaryTable.at['Precision - Class 0', model_name] = round(precision[0], 3) \n    SummaryTable.at['Precision - Class 1', model_name] = round(precision[1], 3)\n    SummaryTable.at['Recall - Class 0', model_name] = round(recall[0], 3) \n    SummaryTable.at['Recall - Class 1', model_name] = round(recall[1], 3) \n    SummaryTable.at['F1-Score - Class 0', model_name] = round(fscore[0], 3) \n    SummaryTable.at['F1-Score - Class 1', model_name] = round(fscore[1], 3) \n    SummaryTable.at['Support - Class 0', model_name] = round(support[0], 3) \n    SummaryTable.at['Support - Class 1', model_name] = round(support[1], 3) \n    SummaryTable.at['Execution Time', model_name] = round((time.time() - func_start_time), 2)\n    display(SummaryTable)\n    \n    print('Function Execution Time: --- %s seconds ---' % round((time.time() - func_start_time), 2))\n\n    \n#\n# Create a function to Run RFT and tune it - this is for KFolds\n# The hyperparameters to be tuned: max_depth, max_features, bootstrap\n# Display Classification Report, ROC Curve, Confusion Matrix \n# Source: https:\/\/data-flair.training\/blogs\/advanced-python-project-detecting-fake-news\/\n# Source: https:\/\/medium.com\/@haydar_ai\/learning-data-science-day-22-cross-validation-and-parameter-tuning-b14bcbc6b012\n# Source: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_multi_metric_evaluation.html\n#\n@ignore_warnings(category = ConvergenceWarning)\ndef RunRFTKFolds(model_name, DataX, DataY, Stratified, SummaryTable, label_list):\n    \n    func_start_time = time.time()\n    lbl = model_name + ': ' + SummaryTable.at['Validation Method', model_name]\n    opt_params = {'max_depth': None,\n                  'max_features': None,\n                  'bootstrap': None}\n#                  'min_samples_leaf': None,\n#                  'min_samples_split': None}\n    \n    \n    #\n    # create cross validation object and get splits\n    #\n    if Stratified:\n        cv = StratifiedKFold(n_splits = 5, random_state = 10, shuffle = True)\n    else:\n        cv = KFold(n_splits = 5, random_state = 10, shuffle = True)\n    \n    cv.get_n_splits(DataX, DataY.values.ravel())\n        \n        \n    #\n    # Prepare hyperparamter ranges and scoring\n    #\n    max_depth_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n    max_features_list = ['auto', 'sqrt']\n    bootstrap_list = [True, False]\n#    min_samples_leaf_list = [1, 2, 4]\n#    min_samples_split = [2, 5, 10]\n    parameter_grid = {'max_depth': max_depth_list,\n                      'max_features': max_features_list,\n                      'bootstrap': bootstrap_list}\n#                      'min_samples_leaf': min_samples_leaf_list,\n#                      'min_samples_split': min_samples_split}\n    scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n    \n    \n    #\n    # Create classifer object and run grid search to find optimal paramters\n    #\n    rft = RandomForestClassifier()\n    \n    grid_search = GridSearchCV(rft, \n                               param_grid = parameter_grid, \n                               cv = cv,  \n                               scoring = scoring, \n                               refit = 'AUC', \n                               return_train_score = True)\n    \n    grid_search.fit(DataX, DataY.values.ravel())\n    \n    print('\\n\\nBest parameters: {}'.format(grid_search.best_params_))\n    opt_params['max_depth'] = grid_search.best_params_.get('max_depth')\n    opt_params['max_features'] = grid_search.best_params_.get('max_features')\n    opt_params['bootstrap'] = grid_search.best_params_.get('bootstrap')\n#    opt_params['min_samples_leaf'] = grid_search.best_params_.get('min_samples_leaf')\n#    opt_params['min_samples_split'] = grid_search.best_params_.get('min_samples_split')\n    \n    \n    #\n    # Create RFT based on optimal hyperparatmers \n    #\n    rft = grid_search.best_estimator_\n    \n    # Predict the class for the data\n    Predicted = rft.predict(DataX)\n    \n    # Calculate error, MSE, RMSE\n#    mse = mean_squared_error(np.array(TestY, dtype = np.float32), np.array(Predicted, dtype = np.float32))\n#    rmse = math.sqrt(mse)\n    accuracy = accuracy_score(DataY, Predicted)\n    precision, recall, fscore, support = precision_recall_fscore_support(DataY, Predicted, average = None)\n    roc_auc = roc_auc_score(DataY, Predicted)\n    \n    # Display classification report\n    print('\\n\\nClassification Report for Optimal Model:')\n    cs_r = classification_report(DataY, Predicted, target_names = label_list)\n    print(cs_r)\n\n    # Plot ROC AUC \n    PlotROCCurve(DataY, Predicted)\n    \n    #Display confusion matrix\n    PlotConfusionMatrix(DataY, Predicted, label_list)\n    \n    # Update the summary table by including the results for this model\n    SummaryTable.at['Feature Count', model_name] = DataX.shape[1]\n    SummaryTable.at['Total Records', model_name] = DataX.shape[0]\n    SummaryTable.at['Optimal max_depth', model_name] = opt_params['max_depth']\n    SummaryTable.at['Optimal max_features', model_name] = opt_params['max_features']\n    SummaryTable.at['Optimal bootstrap', model_name] = opt_params['bootstrap']\n#    SummaryTable.at['Optimal min_samples_leaf', model_name] = opt_params['min_samples_leaf']\n#    SummaryTable.at['Optimal min_samples_split', model_name] = opt_params['min_samples_split']\n    SummaryTable.at['Accuracy Score', model_name] = round(accuracy, 3)\n    SummaryTable.at['ROC AUC Score', model_name] = round(roc_auc, 3)\n    SummaryTable.at['Precision - Class 0', model_name] = round(precision[0], 3) \n    SummaryTable.at['Precision - Class 1', model_name] = round(precision[1], 3)\n    SummaryTable.at['Recall - Class 0', model_name] = round(recall[0], 3) \n    SummaryTable.at['Recall - Class 1', model_name] = round(recall[1], 3) \n    SummaryTable.at['F1-Score - Class 0', model_name] = round(fscore[0], 3) \n    SummaryTable.at['F1-Score - Class 1', model_name] = round(fscore[1], 3) \n    SummaryTable.at['Support - Class 0', model_name] = round(support[0], 3) \n    SummaryTable.at['Support - Class 1', model_name] = round(support[1], 3) \n    SummaryTable.at['Execution Time', model_name] = round((time.time() - func_start_time), 2)\n    display(SummaryTable)\n    \n    print('Function Execution Time: --- %s seconds ---' % round((time.time() - func_start_time), 2))\n\n    \n#\n# Create a dataframe to store the results of different models\n#\ndf_RFT_summary_1 = pd.DataFrame({\n    'RFT_01':['Random Forest','Train\/Test','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_02':['Random Forest','Train\/Test - Stratified','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_03':['Random Forest','Train\/Test','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_04':['Random Forest','Train\/Test - Stratified','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_05':['Random Forest','KFolds','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_06':['Random Forest','KFolds - Stratified','Yes','No','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_07':['Random Forest','KFolds','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_08':['Random Forest','KFolds - Stratified','Yes','Yes','Engineered Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]})\n\ndf_RFT_summary_1.index = ['Algorithm Name',\n                          'Validation Method',\n                          'Outliers Removed',\n                          'Normalized and Scaled',\n                          'Feature Type',\n                          'Feature Count',\n                          'Total Records',\n                          'Optimal max_depth',\n                          'Optimal max_features',\n                          'Optimal bootstrap',\n#                          'Optimal min_samples_leaf',\n#                          'Optimal min_samples_split',\n                          'Accuracy Score',\n                          'ROC AUC Score',\n                          'Precision - Class 0',\n                          'Precision - Class 1',\n                          'Recall - Class 0',\n                          'Recall - Class 1',\n                          'F1-Score - Class 0',\n                          'F1-Score - Class 1',\n                          'Support - Class 0',\n                          'Support - Class 1',\n                          'Execution Time']\n\n\n#\n# Create a dataframe to store the results of different models\n#\ndf_RFT_summary_2 = pd.DataFrame({\n    'RFT_09':['Random Forest','Train\/Test','Yes','N\/A','TF-IDF Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None],\n    'RFT_10':['Random Forest','Train\/Test - Stratified','Yes','N\/A','TF-IDF Features',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]})\n\ndf_RFT_summary_2.index = ['Algorithm Name',\n                          'Validation Method',\n                          'Outliers Removed',\n                          'Normalized and Scaled',\n                          'Feature Type',\n                          'Feature Count',\n                          'Total Records',\n                          'Optimal max_depth',\n                          'Optimal max_features',\n                          'Optimal bootstrap',\n #                         'Optimal min_samples_leaf',\n #                         'Optimal min_samples_split',\n                          'Accuracy Score',\n                          'ROC AUC Score',\n                          'Precision - Class 0',\n                          'Precision - Class 1',\n                          'Recall - Class 0',\n                          'Recall - Class 1',\n                          'F1-Score - Class 0',\n                          'F1-Score - Class 1',\n                          'Support - Class 0',\n                          'Support - Class 1',\n                          'Execution Time']","19784507":"RunRFTTestTrain(model_name = 'RFT_01',\n                TrainX = df_X_train,\n                TestX = df_X_test,\n                TrainY = df_Y_train,\n                TestY = df_Y_test,\n                SummaryTable = df_RFT_summary_1,\n                label_list = label_dict.values())","819b10b9":"RunRFTTestTrain(model_name = 'RFT_02',\n                TrainX = df_X_train_strat,\n                TestX = df_X_test_strat,\n                TrainY = df_Y_train_strat,\n                TestY = df_Y_test_strat,\n                SummaryTable = df_RFT_summary_1,\n                label_list = label_dict.values())","2cd90f5a":"RunRFTTestTrain(model_name = 'RFT_03',\n                TrainX = df_X_norm_train,\n                TestX = df_X_norm_test,\n                TrainY = df_Y_norm_train,\n                TestY = df_Y_norm_test,\n                SummaryTable = df_RFT_summary_1,\n                label_list = label_dict.values())","47534440":"RunRFTTestTrain(model_name = 'RFT_04', \n                TrainX = df_X_norm_train_strat,\n                TestX = df_X_norm_test_strat,\n                TrainY = df_Y_norm_train_strat,\n                TestY = df_Y_norm_test_strat,\n                SummaryTable = df_RFT_summary_1,\n                label_list = label_dict.values())","d1f9ee25":"RunRFTKFolds(model_name = 'RFT_05',\n             DataX = df_X,\n             DataY = df_Y,\n             Stratified = False,\n             SummaryTable = df_RFT_summary_1,\n             label_list = label_dict.values())","a41c2c42":"RunRFTKFolds(model_name = 'RFT_06',\n             DataX = df_X,\n             DataY = df_Y,\n             Stratified = True,\n             SummaryTable = df_RFT_summary_1,\n             label_list = label_dict.values())","ce648752":"RunRFTKFolds(model_name = 'RFT_07',\n             DataX = df_X_norm,\n             DataY = df_Y_norm,\n             Stratified = False,\n             SummaryTable = df_RFT_summary_1,\n             label_list = label_dict.values())","a93c4e62":"RunRFTKFolds(model_name = 'RFT_08', \n             DataX = df_X_norm,\n             DataY = df_Y_norm,\n             Stratified = True,\n             SummaryTable = df_RFT_summary_1,\n             label_list = label_dict.values())","345eb3e2":"RunRFTTestTrain(model_name = 'RFT_09',\n                TrainX = df_X_tfidf_train,\n                TestX = df_X_tfidf_test,\n                TrainY = df_Y_tfidf_train,\n                TestY = df_Y_tfidf_test,\n                SummaryTable = df_RFT_summary_2,\n                label_list = label_dict.values())","5a81ea6e":"RunRFTTestTrain(model_name = 'RFT_10',\n                TrainX = df_X_tfidf_train_strat,\n                TestX = df_X_tfidf_test_strat,\n                TrainY = df_Y_tfidf_train_strat,\n                TestY = df_Y_tfidf_test_strat,\n                SummaryTable = df_RFT_summary_2,\n                label_list = label_dict.values())","38fcd91b":"# Model Tuning and Training: Support Vector Classifier\nModels will be prepared for Feature Type 1 using the validation strategy which was defined above.<br>\nThe following hyperparamters will be tuned to achieve the highest ROC AUC score:\n* gamme\n* C","763a39c0":"# Data pre-processing\nThe first step of the machine learning pipeline is to read the CSV files, clean the data and create features. The following activities were performed for reading and cleaning the data:\n1.\tRead Fake.csv and True.csv\n2.\tReplace double spaces, tabs and new line character with single space in title and\/or text\n3.\tRemove any HTML tags that may have leaked into title and\/or text \n4.\tAssign label 1 to Fake.csv records and assign label 0 to True.csv\n5.\tMerge the data from Fake.csv and True.csv into a new dataframe (all data)\n6.\tRemove any duplicate records based on title and text from the new merged dataframe\n7.\tRemove any NA\/NaN text records from all data\n\nThe original data set has only two useful columns (title and text). In order to train a model, additional features were required. Two approaches were followed to create additional features and both approaches will be trained and assessed.<br>\n* Feature Type 1 \u2013 Engineered Features: The first approach was to create features manually by finding word counts, question mark counts, exclamation counts, number of sentences etc. which resulted in 28 new features.<br>\n* Feature Type 2 \u2013 TF-IDF: The second approach was to use Term Frequency \u2013 Inverse Document Frequency vectorizer to create a matrix which will be used to classify the articles based on the content.","3f6a4d04":"# News Classification using Support Vector Classifier and Random Forest","ae0bcc01":"# Validation Strategy\nIn order to assess the performance of the mdoels, the following validation methods will be applied to each individual algorithm:\n### Feature Type 1: Engineerd Features:\n* Train\/Test Split: 80%\/20% split\n1. Pre-processed data with removed outliers train\/test (random)\n2. Pre-processed data with removed outliers train\/test (stratified)\n3. Scaled, normalized pre-processed data with removed outliers train\/test (random)\n4. Scaled, normalized pre-processed data with removed outliers train\/test (stratified) \n* KFolds: 10 Folds\n1. Pre-processed data with removed outliers train\/test (random)\n2. Pre-processed data with removed outliers train\/test (stratified)\n3. Scaled, normalized pre-processed data with removed outliers train\/test (random)\n4. Scaled, normalized pre-processed data with removed outliers train\/test (stratified) \n\n### Feature Type 2: TF-IDF Features:\n* Train\/Test Split: 80%\/20% split\n1. New column (title + text) and label train\/test (random)\n2. New column (title + text) and label train\/test (stratified)\n* KFolds: 10 Folds\n1. New column (title + text) and label train\/test (random)\n2. New column (title + text) and label train\/test (stratified)\n\nThe performance of all validation methods for each algorithm will be summarized in a table.","370c97fd":"# Modelling\nTrain and tune the following models:\n* Support Vector Machine: Support Vector Classifier\n* Bagging: Random Forest Classifier","4e4916f1":"# Model Tuning and Training: Random Forest Classifier\nModels will be prepared for Feature Type 1 and Feature Type 2 using the validation strategy which was defined above.<br>\nThe following hyperparamters will be tuned to achieve the highest ROC AUC score:\n* max_depth\n* max_features\n* bootstrap","b80257bd":"# Exploratory Data Analys and Data Preparation\n\nExploratory data analysis is a critical step to understand the data and to determine whether further preparation\/processing is required:\n1.\tThe first activity was to check record count by label to see whether the classes were balanced or not. In this case, the records were balanced by having 55% \/ 45% split.\n2.\tPlot histogram, density plot and box plot per numerical feature grouped by label.  This is a sample plot:\n3.\tPlot a wordcloud and top 20 term frequency per title and\/or text group by label. This will help in determining the most common words per title and\/or text per label (real\/fake).  The plots are to be prepared with stopwords and without stopwords.  Stopwords are common words such as \u201cto\u201d, \u201cthe\u201d, \u201cin\u201d etc.\n\nAdditional data preparation is required to improve the performance of machine learning models. Different preparation activities are required based on the feature type:\n### Feature Type 1 \u2013 Engineered Features: \nSince these are pure numerical features based on count of words, sentences, specific characters etc further processing is required to ensure that the data meets the modelling requirements in terms of distribution, scaling etc. The following activities were performed:\n1.\tNormal Transformation: Check the skew value of each numeric feature and apply the appropriate transformation function source (https:\/\/www.datanovia.com\/en\/lessons\/transform-data-to-normal-distribution-in-r):<br>\n&emsp; Skew\tDistribution\tTransformation function<br>\n&emsp; skew >= 1 \tpositive highly skewed\tlog10(x + 1)<br>\n&emsp; skew <= -1 \tnegative highly skewed\tlog10(max(x+1) - x)<br>\n&emsp; skew > -1 and <= -0.5 \tnegative moderately skewed \tsqrt(max(x+1) - x)<br>\n&emsp; skew >= 0.5 and < 1 \tpositive moderately skewed\tsqrt(x)<br>\n2.\tScaling: Scale the data after normalization by using StandardScaler to have a mean of 0 and standard deviation of 1.\n3.\tRemoving outliers: remove outliers by calculating the zscore of all numeric features and removing records with zscore >= 3. This was applied to the pre-processed data frame and the dataframe which was normalized\/scaled:\n \n4.\tRemoving correlated features: Generate a heat map of all features and remove any feature with correlation value > 0.75. Since the remaining number of features was relatively low (ranging from 16 to 18) then all remaining features will be used for modelling.<br>\n&emsp; Pre-processed data frame with outliers removed: List of correlated columns that were removed ['title_length', 'text_length', 'text_lcase_count', 'title_other_case_count', 'text_other_case_count', 'title_lcase_pct', 'title_ucase_pct', 'title_other_case_pct', 'text_other_case_pct', 'text_sentences_count']<br>\n&emsp; Pre-processed data + normalized + scaled + outliers removed: List of correlated columns that were removed ['title_length', 'text_length', 'text_lcase_count', 'title_other_case_count', 'text_other_case_count', 'title_lcase_pct', 'title_ucase_pct', 'title_other_case_pct', 'text_other_case_pct', 'text_sentences_count', 'title_sentence_avg_words', 'title_exclamation_marks']<br>\n5.\tSince outliers were removed, it was necessary to check whether the labels were relatively balanced or not. \n \n6.\tTrain\/Test split: Split the pre-processed data with removed outliers and the scaled, normalized pre-processed data with removed outliers into train set (80%) and test set (20%). Prepare two train\/test sets per original data set (one regular and one stratified).  In total there will be 4 train\/test data sets.<br>\n\n### Feature Type 2 \u2013 TF-IDF: \nThe second approach was to use Term Frequency \u2013 Inverse Document Frequency vectorizer to create a matrix which will be used to classify the articles based on the content. The following activities were performed:\n1.\tCombine the title and text from original pre-processed data into one column \n2.\tChange all words in the new column (title + text) to lower case then remove all numbers and punctuation\n3.\tRemove all stop words from the new column.  Stopwords are a set of commonly used words in a language.  Some English stop words are \u201cis\u201d, \u201cthe\u201d, \u201cto\u201d, \u201cin\u201d, etc.\n4.\tLemmatize the new column which is the process of grouping together the inflected forms of a word. For instance, the word \u201cgoing\u201d, \u201cgoes\u201d, \u201cgo\u201d will be grouped as \u201cgo\u201d.\n5.\tTrain\/Test split: Split the new column (title + text) and label into train set (80%) and test set (20%). Prepare two train\/test sets (one regular and one stratified).  In total there will be 2 train\/test data sets.\n6.\tTF-IDF Matrix: Use TF-IDF Vectorizer to calculate the term frequency and inverse document frequency of both train \/ test data set.  The vectorizer will remove stop words, keep words that have document frequency of less than 70%.\n"}}