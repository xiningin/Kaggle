{"cell_type":{"c992f557":"code","85230b0a":"code","4b4fece3":"code","7cd76ecf":"code","4c913d55":"code","aabee8c5":"code","cb12cf37":"code","102525e9":"code","ed623f65":"code","57d9f802":"code","cf1bf07f":"code","14fcc664":"code","ebde3da7":"code","582d5708":"code","bb8d8340":"code","1cca9498":"code","5f811cf7":"code","f63d5859":"code","eae10396":"code","27042645":"code","c5cc2927":"code","2698bfdf":"code","7ec38e26":"code","b292bd25":"code","5656d0dd":"code","4e96bb13":"code","d71879f4":"code","94d1db2d":"code","fde2d1c5":"code","9ba720b9":"code","fea4eb27":"code","103cca12":"code","720cccb2":"code","06c6c432":"code","089a1020":"code","59c4191b":"markdown","f5bdec75":"markdown","662640e4":"markdown","abfe9021":"markdown","3e7a3632":"markdown","51905b13":"markdown","c8b7d6ac":"markdown","458f817a":"markdown","89ba3821":"markdown","3136a6a6":"markdown","b1ea3e8e":"markdown","18927611":"markdown","f6710cb5":"markdown","7ef59048":"markdown","25ecb247":"markdown","69adb12b":"markdown","3593b409":"markdown","2b334c94":"markdown","ec4af762":"markdown","4fdb2c0a":"markdown","6c28b316":"markdown","a3d37be1":"markdown","d1dc0935":"markdown","1be9e093":"markdown","4c86ac72":"markdown","99096fd9":"markdown","00d64279":"markdown","43b20be8":"markdown","43466222":"markdown","9fdaeeea":"markdown","227d24d6":"markdown"},"source":{"c992f557":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","85230b0a":"import pandas as pd\nimport numpy as np","4b4fece3":"responses = pd.read_csv( \"..\/input\/responses.csv\")\n","7cd76ecf":"responses.head()","4c913d55":"df = pd.DataFrame(responses)","aabee8c5":"df['House - block of flats'] = np.where(df['House - block of flats']==\"block of flats\",1,2)\n\ndf['Village - town'] = np.where(df['Village - town']==\"village\",1,2)\n\ndf['Left - right handed'] = np.where(df['Left - right handed']==\"right handed\",1,2)\n\ndf['Only child'] = np.where(df['Only child']==\"yes\",1,2)\n\ndf['Gender'] = np.where(df['Gender']==\"male\",1,2) #male=1, female=2\n\ndf['Internet usage'] = df['Internet usage'].astype('category')\ndf['Internet usage'] = df['Internet usage'].cat.codes + 1\n\ndf['Punctuality'] = df['Punctuality'].astype('category')\ndf['Punctuality'] = df['Punctuality'].cat.codes + 1\n\ndf['Lying'] = df['Lying'].astype('category')\ndf['Lying'] = df['Lying'].cat.codes + 1\n\ndf['Smoking'] = df['Smoking'].astype('category')\ndf['Smoking'] = df['Smoking'].cat.codes + 1\n\ndf['Alcohol'] = df['Alcohol'].astype('category')\ndf['Alcohol'] = df['Alcohol'].cat.codes + 1","cb12cf37":"df.head()","102525e9":"def cat_to_num(x):\n    if x==\"currently a primary school pupil\":\n        return 1\n    if x==\"primary schoool\":\n        return 2\n    if x==\"secondary school\":\n        return 3\n    if x==\"college\/bachelor degree\":\n        return 4\n    if x==\"masters degree \":\n        return 5\n    if x==\"doctorate degree \":\n        return 6\n    \n    \ndf['Education'] = df['Education'].apply(cat_to_num)","ed623f65":"df.head()","57d9f802":"df1 = pd.DataFrame(df)\ngen_df = df1.iloc[:,76:133]","cf1bf07f":"import missingno as msno\n\nmsno.matrix(gen_df)\nmsno.heatmap(gen_df)","14fcc664":"def myMedianimpute(data):\n    for i in data.columns:\n        data[i] = data[i].replace('?',np.nan).astype(float)\n        data[i] = data[i].fillna((data[i].median()))\n    return data\n\nmyMedianimpute(gen_df)","ebde3da7":"np.sum(gen_df.isna())","582d5708":"df2 = pd.concat([gen_df,df1.Gender],axis=1)\ndf2.columns","bb8d8340":"len(gen_df.columns)","1cca9498":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in  df2.columns:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    sns.countplot(y=i, data=df2, ax=ax[0])\n    sns.countplot(y=i, hue='Gender', data=df2, ax=ax[1])\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)","5f811cf7":"import matplotlib.pyplot as plt\nfrom scipy.stats import spearmanr\n\nspr_corr = gen_df.corr(method='spearman')\n\nfig = plt.figure(figsize=(25,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(spr_corr,cmap='coolwarm',vmin=-1,vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(gen_df.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(gen_df.columns)\nax.set_yticklabels(gen_df.columns)\nplt.show()","f63d5859":"def redundant_pairs(df):\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef top_correlations(df, method, n):\n    au_corr = df.corr(method = method).abs().unstack()\n    labels_to_drop = redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\ntop_corr = top_correlations(gen_df,'spearman',57)\ntop_corr\n","eae10396":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kmodes.kmodes import KModes\n","27042645":"per = np.array(gen_df)\n","c5cc2927":"km = KModes(n_clusters=500,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm1 = km.fit(per)\nm1.cluster_centroids_","2698bfdf":"m1.cost_","7ec38e26":"km = KModes(n_clusters=6,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm0 = km.fit(per)\nm0.cluster_centroids_\nm0.cost_","b292bd25":"mdl1 = m1.cluster_centroids_\nkm1 = KModes(n_clusters=250,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm2 = km1.fit(mdl1)\nm2.cost_","5656d0dd":"mdl2 = m2.cluster_centroids_\nkm2 = KModes(n_clusters=125,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm3= km2.fit(mdl2)\nm3.cost_","4e96bb13":"mdl3 = m3.cluster_centroids_\nkm3 = KModes(n_clusters=62,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm4= km3.fit(mdl3)\nm4.cost_","d71879f4":"mdl4 = m4.cluster_centroids_\nkm4 = KModes(n_clusters=31,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm5 = km4.fit(mdl4)\nm5.cost_","94d1db2d":"mdl4 = m4.cluster_centroids_\nkm4 = KModes(n_clusters=31,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm5 = km4.fit(mdl4)\nm5.cost_","fde2d1c5":"mdl5 = m5.cluster_centroids_\nkm5 = KModes(n_clusters=15,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm6 = km5.fit(mdl5)\nm6.cost_","9ba720b9":"mdl5 = m5.cluster_centroids_\nkm5 = KModes(n_clusters=15,max_iter=1000,init='Huang',n_init=2,\n            n_jobs=-1)\n\nm6 = km5.fit(mdl5)\nm6.cost_","fea4eb27":"mdl6 = m6.cluster_centroids_\nkm6 = KModes(n_clusters=7,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm7 = km6.fit(mdl6)\nm7.cost_","103cca12":"mdl7 = m7.cluster_centroids_\nkm7 = KModes(n_clusters=5,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm8 = km7.fit(mdl7)\nm8.cost_","720cccb2":"mdl8 = m8.cluster_centroids_\nkm8 = KModes(n_clusters=4,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm9 = km8.fit(mdl8)\nm9.cost_","06c6c432":"mdl9 = m9.cluster_centroids_\nkm9 = KModes(n_clusters=3,max_iter=1000,init='Cao',n_init=2,\n            n_jobs=-1)\n\nm10 = km9.fit(mdl9)\nm10.cost_","089a1020":"mdl9 = m9.cluster_centroids_\nmdl9","59c4191b":"Some interesting observations can be made from these frequency plots grouped by gender. For example, men tend to get angry more often (laughs). \nOn an average men show more tendency to display most of the traits shown here, it seems. Which makes it clear that personalities can be quite different based on gender (this is ofcourse obvious).","f5bdec75":"The second iteration itself reduces the cost from over 14000 to about 7000. \nLet's keep going to see what happens, this time using the cluster centroids of the second as input for the third. We'll keep doing this until we reach an acceptable value of the cost function.","662640e4":"Some very interesting stuff has come out:\n\n1.\tIn general, disorganised people lack awareness about their surroundings. They are not very sure what or when to do things. May be lazy. Often disinterested in as basic things as eating. Signs of frustration, selfishness visible. Get angry less often. This is one personality type. Lazy, confused and disorganized.\n\n2.\tReliability coincides with keeping promises. Socially outgoing people may have strong political opinions. Extroversion\n3. People with strong sense of responsibility may be god believers. Get angry more often.Maybe another personality type.  \n\n4.\tThose with less patience lie less often, but have low adaptation skills.\n\nI may do more on this data in the future. \n","abfe9021":"On the whole, we see the stronger correlated features actually do make sense to be so (eg: workaholism and prioritising workload). Inspite of this correlations may not really help that much in finding groups in personality. A very interesting pair is the last one. With respect to the real world, it is indeed something to consider.","3e7a3632":"I'm using KModes clustering here. The data is to be clustered with respect to the rows as I want all the features to  stay and cotribute towards finding personality types. But other than this I'll be doing something quite interesting as well, which ","51905b13":"Let's see the top correlations shall we ?","c8b7d6ac":"Let's go clustering!","458f817a":"checking if the imputation worked","89ba3821":"Now we select only the personality traits, general views on life,  features for our subsequent work","3136a6a6":"I have changed the initialisation method to 'Cao'. Why? let's see ","b1ea3e8e":"Loading the data onto a pandas dataframe","18927611":"Making another copy of the dataframe","f6710cb5":"Recoding the categorical features written in text to numerical one in case we use them later (Feature engineering)\nThe +1 is being used to make sure the coding begins with 1 instead of 0 which is the default","7ef59048":"Now we see why initially I chose something as high as 500. We wish to reduce the cost function value and look at the final clusters. \nHopefully those final clusters will reveal persoanlity types that make some sense.","25ecb247":"Now let's do something else for a change","69adb12b":"So we see that none of the invidual features have 30-60% missing values, in which case we'd have to remove that feature from consideration altogether. Here all the features can be retained. There are 57 features for gen_df.\n\nSince all the features are being retained, it is desirable that the missing values be imputed with some suitable value, such that we face no problems in analysis later on. For the imputation, the neutral response of 3 is chosen to be ideal, in order to introduce as little error as possible by doing so.","3593b409":"We know that there are missing values in the entire data. Let's try and visualize them in the part we've just taken.","2b334c94":"We see that the it has worked just fine.\n\nMoving on, let's try some visualizations (Exploratory data analysis)","ec4af762":"We see that there are some significant correlations if a threshold of 0.3 is considered. So low (0.3) is taken because in the domain of psychology,\ncorrelated features with even low correlations may play a significant role in our final outcomes. In other words, statistical significance may not necessarily give a very accurate picture of actual psychological significance.","4fdb2c0a":"Let's if any correlations can be spotted and whether they help us detect personality types","6c28b316":"Now we go into the modelling. Let's begin by importing the required libraries","a3d37be1":"We see that there is no further reduction, so 4 may be chosen as the optimum number of expected approximate personality types in the data we selected.\nAt each iteration I reduced the number of clusters by 50% approx so that it is not arbitrary. Various combinations may be attempted ofcourse.\nLet's check out if we found any feasible personality-types\/trait-groups in our final cluster.","d1dc0935":"Let's continue","1be9e093":"This value is quite acceptable. We may stop here. ","4c86ac72":"Next we convert our working dataframe into a numpy array","99096fd9":"As we see, the initial cost is quite high. We'll need to do something about that. Now comes the interesting part. Let's be very clear about something. We cannot have 500 personality types (I've given initial number of clusters = 500). Let's see what happens if we give much lesser, say 6 (it is unlikely that the number of personality types that can be found for the given data will exceed 10 at most).","00d64279":"But the way we reduce the value of the cost function will be quite interesting. It will leave great scope of automating the process that I'm going to use in order to find some optimum path. Let's see what I'm talking about","43b20be8":"We see the differences in the values of the cost function. Why so?\n\n'Huang' is a probabilistic method using the probabilities of the attributes to choose centroid points. As the clusters begin to form and take shape with each iteration, probablities of the individual attributes become less and less important. From here on 'Cao', which uses the density of distribution of the points to find the centroids makes mores sense. Thus 'Cao' begins to give better results. One may keep checking with both till one starts giving better results.","43466222":"Very interesting. Further decrease. But clearly it is still too high. Let's keep going to see what happens","9fdaeeea":"Importing libraries that we'll need for now. We'll import what we need as we go.","227d24d6":"Next 'Education' is dealt with. Randomly assigning the numbers is not the best way to go about. It is better to code the labels based on the level of education, starting from 1 for the lowest qualification and 6 for the highest"}}