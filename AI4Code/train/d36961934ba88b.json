{"cell_type":{"12fb40aa":"code","8e5ccee4":"code","fc0fce8b":"code","cf3f5902":"code","c809e9e3":"code","250866ad":"code","7f128b6d":"code","9ef11f05":"code","3e2b4846":"code","63ced08b":"code","33a45d0e":"code","2c5e789b":"code","d4673f35":"code","462700f6":"code","c6011943":"code","11331a21":"code","90b61941":"code","3e3261d7":"code","b8035a04":"code","98302ae7":"code","f0a307ee":"code","fc466231":"code","9af8ed8c":"code","fadad4ea":"code","9b713dc4":"code","397dad4f":"code","7b8b6182":"code","493dab16":"code","d21dadff":"code","e23e9baf":"code","6d474471":"code","9310bc86":"markdown","c578a47b":"markdown","77f1c827":"markdown","28201c1c":"markdown","702863f5":"markdown","3e1d8110":"markdown","8d55dc48":"markdown","5bda0197":"markdown","228a1da6":"markdown","6fa634c9":"markdown","354402d2":"markdown","de17b2e8":"markdown","d599bf75":"markdown","1e4a6a25":"markdown","3d5ec43e":"markdown","63c1bb30":"markdown","a3bd8fa7":"markdown","27ab93a8":"markdown","1a009fd6":"markdown","cfd4b2d2":"markdown","57250e6d":"markdown","17c39781":"markdown","a2e9d4de":"markdown","564e56a3":"markdown","24f92911":"markdown","31a2f81a":"markdown","53c727d6":"markdown","d8fd82b4":"markdown","f700c4cc":"markdown","1287099c":"markdown","f9c387ef":"markdown","01bcd31f":"markdown","44edd617":"markdown","df400501":"markdown","21953989":"markdown","8fc7c247":"markdown","3a230719":"markdown","ef141836":"markdown","d61cd02c":"markdown","6aff95e6":"markdown","17c83f26":"markdown","a9bf476d":"markdown","323fcb38":"markdown","fb44f570":"markdown"},"source":{"12fb40aa":"import pathlib\n\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport PIL\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.regularizers import l2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom glob import glob\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","8e5ccee4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))","fc0fce8b":"# Defining the path for train and test images\ndata_dir_train = pathlib.Path('\/kaggle\/input\/isic-skin-cancer-dataset\/Skin cancer ISIC The International Skin Imaging Collaboration\/Train\/')\ndata_dir_test = pathlib.Path('\/kaggle\/input\/isic-skin-cancer-dataset\/Skin cancer ISIC The International Skin Imaging Collaboration\/Test\/')","cf3f5902":"image_count_train = len(list(data_dir_train.glob('*\/*.jpg')))\nprint(image_count_train)\nimage_count_test = len(list(data_dir_test.glob('*\/*.jpg')))\nprint(image_count_test)","c809e9e3":"# getting the exact paths of each image in train data folder\npath_list = [x for x in glob(os.path.join(data_dir_train, '*', '*.jpg'))]\nprint(path_list)","250866ad":"# getting the classes of each image dataset based on the folder name as per the skin cancer name\nlesion_list = [os.path.basename(os.path.dirname(y)) for y in glob(os.path.join(data_dir_train,'*', '*.jpg'))]\nprint(lesion_list)","7f128b6d":"# creating a dictionary to prepare a dataframe\ndataframe_dict = dict(zip(path_list, lesion_list))\noriginal_df = pd.DataFrame(list(dataframe_dict.items()),columns = ['Path','Label'])","9ef11f05":"original_df.head()","3e2b4846":"original_df['Label'].value_counts()","63ced08b":"batch_size = 32\nimg_height = 180\nimg_width = 180","33a45d0e":"## getting the train dataset here using the keras library, with seed = 123, validation split = 20%, and image resize to 180x180\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  labels='inferred',\n  seed=123,\n  validation_split = 0.2,\n  subset = 'training',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","2c5e789b":"## getting the validation dataset here using the keras library with same setting as above\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  labels='inferred',\n  seed=123,\n  validation_split = 0.2,\n  subset = 'validation',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n","d4673f35":"# List out all the classes of skin cancer and store them in a list. \n# These correspond to the directory names in alphabetical order.\nclass_names = train_ds.class_names\nprint(class_names)\nnum_classes = len(class_names)","462700f6":"# Plot the graph to visualize the images from each of the 9 classes\nplt.figure(figsize=(12, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(num_classes):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\nplt.show()\n","c6011943":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","11331a21":"# Buildinf a sequential model with  rescaling and 3 conv layers and 2 dense layers with softmax as output activation\nmodel = Sequential([\n    # rescaling layer\n  layers.experimental.preprocessing.Rescaling(1.\/255,input_shape=(img_height,img_width,3)),\n    # 1st conv layer\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n    # maxpooling layer\n  layers.MaxPooling2D(),\n    # 2nd conv layer\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n    # maxpooling layer\n  layers.MaxPooling2D(),\n    # 3rd conv layer\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n     # maxpooling layer\n  layers.MaxPooling2D(),\n    # flatten\n  layers.Flatten(),\n    # 1st dense layer\n  layers.Dense(128, activation='relu'),\n    # output dense layer\n  layers.Dense(num_classes,activation='softmax')\n])","90b61941":"### Compiling the model using adam optimiser and sparse_categorical_crossentropy loss function\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","3e3261d7":"# View the summary of all layers\nmodel.summary()","b8035a04":"# training  the model with 20 epochs\nepochs = 20\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","98302ae7":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","f0a307ee":"# To handle the overfitting scenario above we can add some dynamic data augmenttaion like \n# - Random flipping\n# - Random rotaion\n# - Random zoom\n\n# this data augmentation layer will prevent the model from memorizing the train data and hence we can handle overfiting\n\ndata_augmentation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n  layers.experimental.preprocessing.RandomRotation(0.1),\n  layers.experimental.preprocessing.RandomZoom(0.1)\n])\n","fc466231":"# Plot graph to visualize how the augmentation strategy works for one instance of training image.\nplt.figure(figsize=(12, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(num_classes):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[0]])\n        plt.axis(\"off\")\nplt.show()","9af8ed8c":"# Buildinf a sequential model with  data augmentation, rescaling and 3 conv layers and 2 dense layers with softmax as output activation\nmodel = Sequential([\n    # data augmentation layer\n  data_augmentation,\n    # Rescaling layer\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n    # 1st conv layer\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n    # maxpoolig layer\n  layers.MaxPooling2D(),\n    # 2nd conv layer\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n    # maxpooling layer\n  layers.MaxPooling2D(),\n    # 3rd conv layer\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n    # maxpooling layer\n  layers.MaxPooling2D(),\n    # flatten\n  layers.Flatten(),\n    # 1st dense layer\n  layers.Dense(128, activation='relu'),\n    # output layer\n  layers.Dense(num_classes,activation='softmax')\n])","fadad4ea":"## compiling the model with adam optimizer and sparse_categorical_crossentropy as loss function\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","9b713dc4":"model.summary()","397dad4f":"## training the model with 20 epochs\nepochs = 20\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","7b8b6182":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","493dab16":"# creating the dictionary and dataframe of original train data to plot a bar graph to check the class imbalance\nimage_dict = {}\nfor classname in class_names:\n    image_dict[classname] = len(list(data_dir_train.glob(classname+'\/*.jpg')))   \nprint(image_dict)  ","d21dadff":"# creating dataframe containing count of images per class\ndf = pd.DataFrame()\ncounts = list(image_dict.values())\ndf['class'] = class_names\ndf['count'] = counts","e23e9baf":"df.head(10)","6d474471":"# Plotting the bar graph for each class\nplt.figure(figsize=(12, 10))\nsns.barplot(x = df['class'], y = df['count'], data = df)\nplt.xticks(rotation=80)\nplt.show()","9310bc86":"`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n\n`Dataset.prefetch()` overlaps data preprocessing and model execution while training.","c578a47b":"### Visualize Augmented Data","77f1c827":"#### Path list of all the train images , required to populate the dataframe","28201c1c":"### Compiling the model","702863f5":"### Create a dataset\n\nDefine some parameters for the loader:","3e1d8110":"## Model II with Data augmentation layer","8d55dc48":"The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.","5bda0197":"## Model I : Simple model","228a1da6":"#### List of skin cancer classnames","6fa634c9":"### Model Summary","354402d2":"### Visualizing training results","de17b2e8":"### Loading and reading Skin Cancer Data","d599bf75":"#### Clearly from above graph we can see that there is a class imbalance issue and the images are not uniformly distributed\n#### - seborrheic keratosis  class has the least number of samples, that is 77\n#### - pigmented benign keratosis, melanoma and basal cell carcinoma are the top 3 dominant classes wrt the sample count\n","1e4a6a25":"### Compile the model\nChoose an appropirate optimiser and loss function for model training ","3d5ec43e":"### Load using keras.preprocessing\n\nLet's load these images off disk using the helpful image_dataset_from_directory utility.","63c1bb30":"### Findings ","a3bd8fa7":"### Train the model","27ab93a8":"### Importing all the important libraries","1a009fd6":"This dataset has about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively.","cfd4b2d2":"### Create the model, compile and train the model","57250e6d":"Augmentor library by default adds specified number of images to each class in the same location as the input directory.\nTried using Augmentor in Kaggle but since kaggle does not support addition of extra files in the input directory, as input directory being write-protected, so dropping it.\n\nBut Surely the Augmentor library can fix the class - imbalance issue and increase the overall model performance","17c39781":"### Data Augmentation","a2e9d4de":"### Training the model","564e56a3":"### Train Dataset","24f92911":"### Model Summary","31a2f81a":"#### Cleraly we can see that the model is overfitting the train accuracy is 90% and the validation accuracy is only 52%\n#### One of the reasons of overfitting could be lack of sufficient images as we know that CNN model require huge amount of images to learn, hence it looks like the model is memorizing the train images and therefore leading to overfitting","53c727d6":"A complete solution executed on the google colab (https:\/\/colab.research.google.com\/) is available at\ngithub link:\nhttps:\/\/github.com\/pyrajiv\/CNN\n","d8fd82b4":"Use 80% of the images for training, and 20% for validation.","f700c4cc":"Augmentor stores the augmented images in the output sub-directory of each of the sub-directories of skin cancer types..","1287099c":"#### Lets find the distribution of classes in the training dataset.\n#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.","f9c387ef":"### Validation Dataset","01bcd31f":"#### There has been a considerable improvement now as compared to the previous model as we can see from the train and validation accuracy that the model overfitting has been handled, but the model performance is not so good only 59% train and 54% validation accuracy\n#### One reason could be class imbalance","44edd617":"### Visualizing the data\n#### Visualizing one instance of all the nine classes present in the dataset","df400501":"#### To Rectify class imbalance we can use the Augmentor library to add more images to the existing samples which can help to resolve the class imbalance issue.\n#### **Context:** You can use a python package known as `Augmentor` (https:\/\/augmentor.readthedocs.io\/en\/master\/) to add more samples across all classes so that none of the classes have very few samples.","21953989":"To use `Augmentor`, the following general procedure is followed:\n\n1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n3. Execute these operations by calling the `Pipeline\u2019s` `sample()` method.\n","8fc7c247":"### Findings from the graph and model history","3a230719":"### Create the model\n#### Create a CNN model, which can accurately detect 9 classes present in the dataset. Using ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). The RGB channel values are in the `[0, 255]` range. Hence, it is good to standardize values to be in the `[0, 1]`","ef141836":"### Plotting graph to detect class imbalance","d61cd02c":"### Visualizing the results","6aff95e6":"#### Paths for train and test images","17c83f26":"Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.","a9bf476d":"## Findings","323fcb38":"# CNN Skin Cancer Classification","fb44f570":"#### Verifying the test train image count"}}