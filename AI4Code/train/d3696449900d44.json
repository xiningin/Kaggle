{"cell_type":{"c39e1cd6":"code","0da0cb3f":"code","f7e241ec":"code","b40cfce4":"code","ab4aad33":"code","2eb8296d":"code","89b51fec":"code","1c049f47":"code","2fa440dc":"code","56f803ea":"code","519de09f":"code","19cb1dfa":"code","145bd75f":"code","97cc007f":"code","41526251":"code","7ca67c98":"code","71ed8483":"code","abe4553d":"code","a77dafbb":"code","c69a3298":"markdown","0355b5a4":"markdown","697aa57c":"markdown","4cbe7eb7":"markdown","7e61faf8":"markdown","c9a8cf4f":"markdown","e50542df":"markdown","2746ee62":"markdown","79a1b7a0":"markdown","8082e6b5":"markdown","37e7c6e0":"markdown","859c0949":"markdown","fb660d29":"markdown","f7620013":"markdown","d5119e27":"markdown","e8fb8d9b":"markdown","35801536":"markdown","112dad83":"markdown"},"source":{"c39e1cd6":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"https:\/\/gist.githubusercontent.com\/mancunian1792\/d174a34b29e09dfa100b3aa60afba707\/raw\/231e21d5bc873583279a29ad4ee1af2d50a31378\/data.csv\")","0da0cb3f":"df.head()","f7e241ec":"del df[\"elementid\"]","b40cfce4":"def convert_clicks_categorical(num):\n  if num <= 10:\n    return \"LOW\"\n  elif num > 10 and num <=100:\n    return \"MEDLOW\"\n  elif num > 100 and num <=250:\n    return \"MEDIUM\"\n  elif num > 250 and num <= 500:\n    return \"MEDHIGH\"\n  else:\n    return \"HIGH\"\n\ndf[\"clicks\"] = df[\"numclicks\"].apply(lambda x: convert_clicks_categorical(x))\ndel df[\"numclicks\"]","ab4aad33":"df.to_csv(\"data.csv\", index=False)","2eb8296d":"ab_testing_data = pd.read_csv(\"https:\/\/gist.githubusercontent.com\/mancunian1792\/06ec31e69b8c10d5de28b7c4949a6538\/raw\/2bee970d5559ff07706aa537d96474b9e0dbb7e7\/ab_testing_data.csv\")","89b51fec":"ab_testing_data.head()","1c049f47":"from graphviz import Digraph\ndag = Digraph()\ndag.node('T', 'Tag Name')\ndag.node('S', 'isVisible')\ndag.node('V', 'Version')\ndag.node('C', 'Clicks')\n\ndag.edges(['VT', 'VS', 'SC', 'TC'])\ndag\n","2fa440dc":"# Getting the cpt tables from bnlearn fit values\nV_alias = ['v1','v2','v3', 'v4', 'v5']\nT_alias = ['a','area', 'button', 'center', 'div', 'font', 'form', 'img', 'input', 'li', 'object', 'p', 'span', 'strong', 'ul']\nS_alias = ['False','True']\nC_alias = ['HIGH','LOW', 'MEDHIGH', 'MEDIUM', 'MEDLOW']","56f803ea":"!pip install torch==1.7.0\n!pip install pyro-ppl\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import Importance, EmpiricalMarginal\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np","519de09f":"import torch\nV_prob = torch.tensor([0.23, 0.194, 0.207, 0.190, 0.177])\nS_prob = torch.tensor([[0.16, 0.84], [0.19, 0.81], [0.22, 0.78], [0.08, 0.92], [0.11, 0.89]])\n\nT_prob = torch.tensor([[0.404, 0.014, 0.014, 0.0001, 0.289, 0.0001, 0.01, 0.029, 0.014, 0.043, 0.0001, 0.043, 0.115, 0.0001, 0.01], \n                       [0.446, 0.017, 0.017, 0.0002, 0.240, 0.0002, 0.017, 0.034, 0.017, 0.0002, 0.0002, 0.103, 0.086, 0.0002, 0.0174], \\\n                       [0.4503, 0.016, 0.016, 0.032, 0.209, 0.0002, 0.016, 0.032, 0.016, 0.0002, 0.016, 0.080, 0.080, 0.016, 0.016], \\\n                       [0.489, 0.017, 0.017, 0.0002, 0.244, 0.0002, 0.0177, 0.052, 0.0177, 0.0177, 0.0002, 0.070, 0.035, 0.0002, 0.0177], \\\n                       [0.507, 0.019, 0.019, 0.019, 0.169, 0.019, 0.019, 0.037, 0.019, 0.0002, 0.0002, 0.0754, 0.0754, 0.0002, 0.019]])\n\nC_prob = torch.tensor([[[0.0009, 0.996, 0.009, 0.009, 0.009], [0.03, 0.57, 0.007, 0.05, 0.33]],\n                       [[0.20, 0.001, 0.001, 0.20, 0.59], [0.20, 0.20, 0.20, 0.20, 0.20]], \\\n                       [[0.20, 0.20, 0.20, 0.20, 0.20], [0.001, 0.001, 0.001, 0.20, 0.797]], \\\n                       [[0.002, 0.9912, 0.002, 0.002, 0.002], [0.2, 0.2, 0.2, 0.2, 0.2]], \\\n                       [[0.0003, 0.998, 0.0003, 0.0003, 0.0003], [0.0001, 0.9602, 0.0001, 0.0001, 0.03932]], \\\n                       [[0.0064, 0.9741, 0.0064, 0.0064, 0.0064], [0.2, 0.2, 0.2, 0.2, 0.2]], \\\n                       [[0.2, 0.2, 0.2, 0.2, 0.2], [0.001, 0.796, 0.001, 0.001, 0.20]], \\\n                       [[0.002, 0.992, 0.002, 0.002, 0.002], [0.0009, 0.9962, 0.0009, 0.0009, 0.0009]], \\\n                       [[0.2, 0.2, 0.2, 0.2, 0.2], [0.2, 0.0013, 0.796, 0.00132, 0.00132]],\\\n                       [[0.2, 0.2, 0.2, 0.2, 0.2], [0.001, 0.993, 0.002, 0.002, 0.002]],\\\n                       [[0.0064, 0.974, 0.006, 0.006, 0.006],[0.2, 0.2, 0.2, 0.2, 0.2]],\\\n                       [[0.0011, 0.996, 0.001, 0.001, 0.001],[0.0004, 0.936, 0.0004, 0.0004, 0.0628]],\\\n                       [[0.2,0.2,0.2, 0.2, 0.2],[0.0003, 0.9157, 0.0003, 0.0003, 0.0835]],\\\n                       [[0.0065, 0.974, 0.0065, 0.0065, 0.0065],[0.2, 0.2, 0.2, 0.2, 0.2]],\\\n                       [[0.2, 0.2, 0.2, 0.2, 0.2],[0.0013, 0.9947, 0.0013, 0.0013, 0.0013]]])","19cb1dfa":"def model():\n    V = pyro.sample(\"V\", dist.Categorical(probs=V_prob))\n    T = pyro.sample(\"T\", dist.Categorical(probs=T_prob[V]))\n    S = pyro.sample(\"S\", dist.Categorical(probs=S_prob[V]))\n    C = pyro.sample(\"C\", dist.Categorical(probs=C_prob[T][S]))\n    return{'V': V,'S': S,'T': T,'C': C}","145bd75f":"conditioned_model_1 = pyro.condition(model, data={'C':torch.tensor(2), 'S': torch.tensor(1)})\n\nV_posterior = Importance(conditioned_model_1, num_samples=5000).run()\nV_marginal = EmpiricalMarginal(V_posterior,\"V\")\nV_samples = [V_marginal().item() for _ in range(5000)]\nV_unique, V_counts = np.unique(V_samples, return_counts=True)\n\nplt.bar(V_unique, V_counts\/5000, align='center', alpha=0.5)\nplt.xticks(V_unique, V_alias)\nplt.ylabel('Posterior Probability')\nplt.xlabel('Versions')\nplt.title('P(V | C = MEDHIGH, S= Visible) - Importance Sampling')","97cc007f":"intervention_condition = pyro.do(model, data={'C':torch.tensor(2), 'S': torch.tensor(1)})\n\nV_posterior = Importance(intervention_condition, num_samples=5000).run()\nV_marginal = EmpiricalMarginal(V_posterior,\"V\")\nV_samples = [V_marginal().item() for _ in range(5000)]\nV_unique, V_counts = np.unique(V_samples, return_counts=True)\n\nplt.bar(V_unique, V_counts\/5000, align='center', alpha=0.5)\nplt.xticks(V_unique, V_alias)\nplt.ylabel('Posterior Probability')\nplt.xlabel('Versions')\nplt.title('P(V | do(C = MEDHIGH, S= Visible) - Importance Sampling')","41526251":"conditioned_model_2 = pyro.condition(model, data={'C':torch.tensor(0)})\n\nT_posterior = Importance(conditioned_model_2, num_samples=5000).run()\nT_marginal = EmpiricalMarginal(T_posterior,\"T\")\nT_samples = [T_marginal().item() for _ in range(5000)]\nT_unique, T_counts = np.unique(T_samples, return_counts=True)\nplt.figure(figsize=(15,10))\nplt.bar(T_unique, T_counts\/5000, align='center', alpha=0.5)\nplt.xticks(T_unique, T_alias)\nplt.ylabel('Posterior Probability')\nplt.xlabel('Tag-Names')\nplt.title('P(T | C = HIGH) - Importance Sampling')","7ca67c98":"intervention_model_visible = pyro.do(model, data= {\"S\": torch.tensor(1)})\n\nC_posterior = Importance(intervention_model_visible, num_samples=5000).run()\nC_marginal = EmpiricalMarginal(C_posterior,\"C\")\nC_samples = [C_marginal().item() for _ in range(5000)]\nC_unique, C_counts = np.unique(C_samples, return_counts=True)\nplt.bar(C_unique, C_counts\/5000, align='center', alpha=0.5)\nplt.xticks(C_unique, C_alias)\nplt.ylabel('Posterior Probability')\nplt.xlabel('Click Rate')\nplt.title('P(C | do(Visible=True)) - Importance Sampling')","71ed8483":"intervention_model_visible_no = pyro.condition(model, data= {\"S\": torch.tensor(0)})\n\nC_posterior = Importance(intervention_model_visible_no, num_samples=5000).run()\nC_marginal = EmpiricalMarginal(C_posterior,\"C\")\nC_samples = [C_marginal().item() for _ in range(5000)]\nC_unique, C_counts = np.unique(C_samples, return_counts=True)\nplt.bar(C_unique, C_counts\/5000, align='center', alpha=0.5)\nplt.xticks(C_unique, C_alias)\nplt.ylabel('Posterior Probability')\nplt.xlabel('Click Rate')\nplt.title('P(C | do(Visible=False) - Importance Sampling')","abe4553d":"def causal_effect(val):\n  c_samples_visible = [\n    1 if intervention_model_visible()['C'] == val else 0\n    for _ in range(5000)\n  ]\n  c_samples_not_visible = [\n      1 if intervention_model_visible_no()['C'] == val else 0\n      for _ in range(5000)\n  ]\n\n  causal_effect = np.mean(c_samples_visible) - np.mean(c_samples_not_visible)\n  return causal_effect","a77dafbb":"for lvl in C_alias:\n  diff = causal_effect(C_alias.index(lvl))\n  print(f\"E(Click = {lvl} | do(Visible = True) - E(Click = {lvl} | do(Visible = False))) is {diff}\")","c69a3298":"### Datasource: https:\/\/scholarworks.montana.edu\/xmlui\/handle\/1\/3507\n\nThe dataset consists of web analytics data trying to capture the user click rate. The dataset consists of tagname, visisblity of the tag, num clicks and the different versions.","0355b5a4":"### **Why is causal inference important?**\n\n---\n\n\n\nThe normal machine learning process doesn't capture causality in the system. We make predictions through correlations. If a system is time variant and the data generation process changes, more often, the built machine learning becomes invalid. For example, say, we build a machine learning model to predict sales. We do a marketing campaign that boosts the sales (in that instance), our machine learning model, doesn't understand the generated data as its different to the ones that was used during training and results in bad predictions. Machine learning systems that encode causality, via causal inference allows us to model the data generating process and give structure to it.\n\nCausal inference models using DAGS are inherently bayesian and hence they give us the uncertainity of the predictions by default. We can estimate the effect of certain factors on the system. The three ladders of causal inference systems are Association (Inferring the probability of one event with evidence of another event), Intervention (Infer , If this - then what questions) and Counterfactuals (Alter the fact. Had i done this instead of reality, what would have happened. Twin -world scenario). The above questions are  difficult to answer via normal machine learning models.","697aa57c":"### What does this notebook contain\nThis notebook contains pyro + R code that takes an AB testing sample data and runs few causal queries. ","4cbe7eb7":"We take the conditional probabilities obtained and use them alongside pyro to answer few causal queries.","7e61faf8":"**Association Query 1**: Infer the probability of versions, given an evidence. This is an interesting query as we go against the direction of the DAG.\n\n**Evidence**: Click through is *MEDHIGH* and visibility of tag is *TRUE*\n","c9a8cf4f":"An interesting observation is that, from data, we see that the probability of the area tag in occuring is very low in 0.001 range. But given the evidence that the click rate is *HIGH* we see that the probability of the area tag goes up 10 fold.","e50542df":"### **Proposed DAG**\n\nIn Different versions, we can change the tag name (add\/edit\/delete) and make it visible or not visible. Hence, version is the root and describes the user changes to the website. The tag name and its visibility causes the clicks. Obviously, we are missing many other factors, like the content, the color or aesthetics. For a fully comprehendable model, we can consider them as latent variables and assume them to be sampled from a distribution. For now, lets just let them be.","2746ee62":"### Steps Followed.\n\nI chose to take a AB testing dataset and answer the following,\n\n\n1.   Fit data to a DAG\n2.   Get conditional probabilities and use them as prior probabilities.\n3.   Use inference algorithms to answer few causal queries.\n4.   Estimate causal effect for a use case.\n\n","79a1b7a0":"### Causal Effect Query\n\n**Does Setting visibility to TRUE for all elements make any effect ?**\n\n","8082e6b5":"We see that by making all the tags visible, the probability of MED-LOW is increased and we see a massive decrease in LOW probability. Hence, we can say that there is a ~15% chance for the  click rates to increase from the 0-10 range to 10-100 range by making all the tags visible.","37e7c6e0":"Now, we use some R magic, to fit the data into a bayesian network and get the conditional probabailities.","859c0949":"### This dataset was uploaded to git and used below.","fb660d29":"``` {r}\n# R code to be run separately.\nlibrary(bnlearn)\nlibrary(Rgraphviz)\nlibrary(dplyr)\nnet <- model2network(\"[version][isVisible|version][tagname|version][clicks|isVisible:tagname]\")\ngraphviz.plot(net)\n\ndata = read.csv(\"https:\/\/gist.githubusercontent.com\/mancunian1792\/06ec31e69b8c10d5de28b7c4949a6538\/raw\/2bee970d5559ff07706aa537d96474b9e0dbb7e7\/ab_testing_data.csv\")\n\nfit <- bn.fit(net, data, method=\"bayes\")\n\nfit\n```","f7620013":"### **Conditional Probabalities**","d5119e27":"**Intervention Query** - During intervention, the effect of the parent is negated. We try to negate the effect of visibility. What would happen if i make all the tags visible ? Hence, the effect of version disappears. What would be the \n\n**Evidence** - Visibility is True.","e8fb8d9b":"For simplicity sake, we remove element id. This is done as it is easy to analyse discrete variables than continous variables in causal inference. But, this variable is very important as it captures the content. We also discretize the number of clicks according to our own logic. (This is very well up for modification)","35801536":"**Association Query 2**: Infer the probability of tag names given evidence about click rate.\n\n**Evidence**: Click through is *HIGH*","112dad83":"### P.S - If you're interested in combining causal inference with deep learning architecture and want to read about it, please checkout this [Tutorial](https:\/\/linkinnation1792.gitbook.io\/causal-scene-generation\/) and [this 10 minute summary](https:\/\/medium.com\/swlh\/causal-generative-modelling-a-brief-tutorial-with-game-character-images-728d3450b600)"}}