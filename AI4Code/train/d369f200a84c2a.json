{"cell_type":{"a8cb0f5a":"code","d5e38788":"code","cb4998e7":"code","5ddc78d3":"code","e3336870":"code","509bd56c":"code","9612bcb3":"code","ecbf38f0":"code","8e1d3a80":"code","caacd110":"code","2b7b28d9":"code","1c0ba0c4":"code","f9c36be8":"code","dae37694":"code","2b21c1c2":"code","1c09aabe":"code","58b38f10":"code","0e050a61":"code","5afbc62a":"code","71272ce0":"code","5b658578":"code","abde459d":"code","076d1b40":"code","7e3dbf0d":"code","d0749094":"code","f6ab7c4f":"markdown","79fc85e3":"markdown","da4c3d43":"markdown","1d5592d5":"markdown","16cd4262":"markdown","0a228daa":"markdown","b8f73b94":"markdown","50c65d9a":"markdown","6ad9c609":"markdown","7a012bb5":"markdown","d19f1275":"markdown","d987f6c2":"markdown","47f3391d":"markdown"},"source":{"a8cb0f5a":"    import numpy as np\n    import torch\n    import pandas as pd\n    \n    import torchvision.transforms as transforms\n    import torch.nn as nn\n    import torch.nn.functional as F\n    \n    from torch.utils.data import DataLoader\n    from torch.utils.data import Dataset","d5e38788":"# random seed (for reproducibility)\nseed = 1234\n# set random seed for numpy\nnp.random.seed(seed)\n# set random seed for pytorch\ntorch.manual_seed(seed)","cb4998e7":"# reading csv files and train & test file paths\ndf_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsample_sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","5ddc78d3":"# get the image pixel values and labels\ntrain_labels = df_train.iloc[:, 0]\ntrain_images = df_train.iloc[:, 1:]\n\ntest_images = df_test.iloc[:, 0:]","e3336870":"# convert data to Tensors\ntransform = transforms.ToTensor()","509bd56c":"# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 20\n\n# custom dataset\nclass MNISTDataset(Dataset):\n    def __init__(self, images, labels=None, transforms=None):\n        self.X = images\n        self.y = labels\n        self.transforms = transforms\n         \n    def __len__(self):\n        return (len(self.X))\n    \n    def __getitem__(self, i):\n        data = self.X.iloc[i, :]\n        data = np.asarray(data).astype(np.uint8).reshape(28, 28, 1)\n        \n        if self.transforms:\n            data = self.transforms(data)\n            \n        if self.y is not None:\n            return (data, self.y[i])\n        else:\n            return data\n        \ntrain_data = MNISTDataset(train_images, train_labels, transform)\ntest_data = MNISTDataset(test_images, None, transform)\n\n# dataloaders\ntrain_loader = DataLoader(train_data,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=num_workers)\ntest_loader = DataLoader(test_data,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)","9612bcb3":"class ConvLayer(nn.Module):\n    \n    def __init__(self, in_channels=1, out_channels=256):\n        '''Constructs the ConvLayer with a specified input and output size.\n           These sizes has initial values from the paper.\n           param input_channel: input depth of an image, default value = 1\n           param output_channel: output depth of the convolutional layer, default value = 256\n           '''\n        super(ConvLayer, self).__init__()\n\n        # defining a convolutional layer of the specified size\n        self.conv = nn.Conv2d(in_channels, out_channels, \n                              kernel_size=9, stride=1, padding=0)\n\n    def forward(self, x):\n        \n        # applying a ReLu activation to the outputs of the conv layer\n        output = F.relu(self.conv(x)) # we will have dimensions (batch_size, 20, 20, 256)\n        return output","ecbf38f0":"class PrimaryCaps(nn.Module):\n    \n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n        '''Constructs a list of convolutional layers to be used in \n           creating capsule output vectors.\n           param num_capsules: number of capsules to create\n           param in_channels: input depth of features, default value = 256\n           param out_channels: output depth of the convolutional layers, default value = 32\n           '''\n        super(PrimaryCaps, self).__init__()\n\n        # creating a list of convolutional layers for each capsule I want to create\n        # all capsules have a conv layer with the same parameters\n        self.capsules = nn.ModuleList([\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                      kernel_size=9, stride=2, padding=0)\n            for _ in range(num_capsules)])\n    \n    def forward(self, x):\n        '''Defines the feedforward behavior.\n           param x: the input; features from a convolutional layer\n           return: a set of normalized, capsule output vectors\n           '''\n        # get batch size of inputs\n        batch_size = x.size(0)\n        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n        u = [capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules]\n        # stack up output vectors, u, one for each capsule\n        u = torch.cat(u, dim=-1)\n        # squashing the stack of vectors\n        u_squash = self.squash(u)\n        return u_squash\n    \n    def squash(self, input_tensor):\n        '''Squashes an input Tensor so it has a magnitude between 0-1.\n           param input_tensor: a stack of capsule inputs, s_j\n           return: a stack of normalized, capsule output vectors, v_j\n           '''\n        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n        scale = squared_norm \/ (1 + squared_norm) # normalization coeff\n        output_tensor = scale * input_tensor \/ torch.sqrt(squared_norm)    \n        return output_tensor","8e1d3a80":"def softmax(input_tensor, dim=1): # to get transpose softmax function # for multiplication reason s_J\n    # transpose input\n    transposed_input = input_tensor.transpose(dim, len(input_tensor.size()) - 1)\n    # calculate softmax\n    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n    # un-transpose result\n    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input_tensor.size()) - 1)","caacd110":"# dynamic routing\ndef dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n    '''Performs dynamic routing between two capsule layers.\n       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n       param u_hat: input, weighted capsule vectors, W u\n       param squash: given, normalizing squash function\n       param routing_iterations: number of times to update coupling coefficients\n       return: v_j, output capsule vectors\n       '''    \n    # update b_ij, c_ij for number of routing iterations\n    for iteration in range(routing_iterations):\n        # softmax calculation of coupling coefficients, c_ij\n        c_ij = softmax(b_ij, dim=2)\n\n        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n\n        # squashing to get a normalized vector output, v_j\n        v_j = squash(s_j)\n\n        # if not on the last iteration, calculate agreement and new b_ij\n        if iteration < routing_iterations - 1:\n            # agreement\n            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n            \n            # new b_ij\n            b_ij = b_ij + a_ij\n    \n    return v_j # return latest v_j","2b7b28d9":"# it will also be relevant, in this model, to see if I can train on gpu\nTRAIN_ON_GPU = torch.cuda.is_available()\n\nif(TRAIN_ON_GPU):\n    print('Training on GPU!')\nelse:\n    print('Only CPU available')","1c0ba0c4":"class DigitCaps(nn.Module):\n    \n    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6, \n                 in_channels=8, out_channels=16):\n        '''Constructs an initial weight matrix, W, and sets class variables.\n           param num_capsules: number of capsules to create\n           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n           param in_channels: number of capsules in previous layer, default value = 8\n           param out_channels: dimensions of output capsule vector, default value = 16\n           '''\n        super(DigitCaps, self).__init__()\n\n        # setting class variables\n        self.num_capsules = num_capsules\n        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\n        self.in_channels = in_channels # previous layer's number of capsules\n\n        # starting out with a randomly initialized weight matrix, W\n        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes, \n                                          in_channels, out_channels))\n\n    def forward(self, u):\n        '''Defines the feedforward behavior.\n           param u: the input; vectors from the previous PrimaryCaps layer\n           return: a set of normalized, capsule output vectors\n           '''\n        \n        # adding batch_size dims and stacking all u vectors\n        u = u[None, :, :, None, :]\n        # 4D weight matrix\n        W = self.W[:, None, :, :, :]\n        \n        # calculating u_hat = W*u\n        u_hat = torch.matmul(u, W)\n\n        # getting the correct size of b_ij\n        # setting them all to 0, initially\n        b_ij = torch.zeros(*u_hat.size())\n        \n        # moving b_ij to GPU, if available\n        if TRAIN_ON_GPU:\n            b_ij = b_ij.cuda()\n\n        # update coupling coefficients and calculate v_j\n        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n\n        return v_j # return final vector outputs\n    \n    def squash(self, input_tensor):\n        '''Squashes an input Tensor so it has a magnitude between 0-1.\n           param input_tensor: a stack of capsule inputs, s_j\n           return: a stack of normalized, capsule output vectors, v_j\n           '''\n        # same squash function as before\n        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n        scale = squared_norm \/ (1 + squared_norm) # normalization coeff\n        output_tensor = scale * input_tensor \/ torch.sqrt(squared_norm)    \n        return output_tensor","f9c36be8":"class Decoder(nn.Module):\n    \n    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n        '''Constructs an series of linear layers + activations.\n           param input_vector_length: dimension of input capsule vector, default value = 16\n           param input_capsules: number of capsules in previous layer, default value = 10\n           param hidden_dim: dimensions of hidden layers, default value = 512\n           '''\n        super(Decoder, self).__init__()\n        \n        # calculate input_dim\n        input_dim = input_vector_length * input_capsules\n        \n        # define linear layers + activations\n        self.linear_layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), # first hidden layer\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\n            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\n            )\n        \n    def forward(self, x):\n        '''Defines the feedforward behavior.\n           param x: the input; vectors from the previous DigitCaps layer\n           return: two things, reconstructed images and the class scores, y\n           '''\n        classes = (x ** 2).sum(dim=-1) ** 0.5\n        classes = F.softmax(classes, dim=-1)\n        \n        # find the capsule with the maximum vector length\n        # here, vector length indicates the probability of a class' existence\n        _, max_length_indices = classes.max(dim=1)\n        \n        # create a sparse class matrix\n        sparse_matrix = torch.eye(10) # 10 is the number of classes\n        if TRAIN_ON_GPU:\n            sparse_matrix = sparse_matrix.cuda()\n        # get the class scores from the \"correct\" capsule\n        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n        \n        # create reconstructed pixels\n        x = x * y[:, :, None]\n        # flatten image into a vector shape (batch_size, vector_dim)\n        flattened_x = x.contiguous().view(x.size(0), -1)\n        # create reconstructed image vectors\n        reconstructions = self.linear_layers(flattened_x)\n        \n        # return reconstructions and the class scores, y\n        return reconstructions, y","dae37694":"class CapsuleNetwork(nn.Module):\n    \n    def __init__(self):\n        '''Constructs a complete Capsule Network.'''\n        super(CapsuleNetwork, self).__init__()\n        self.conv_layer = ConvLayer()\n        self.primary_capsules = PrimaryCaps()\n        self.digit_capsules = DigitCaps()\n        self.decoder = Decoder()\n                \n    def forward(self, images):\n        '''Defines the feedforward behavior.\n           param images: the original MNIST image input data\n           return: output of DigitCaps layer, reconstructed images, class scores\n           '''\n        primary_caps_output = self.primary_capsules(self.conv_layer(images))\n        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\n        reconstructions, y = self.decoder(caps_output)\n        return caps_output, reconstructions, y","2b21c1c2":"# instantiate and print net\ncapsule_net = CapsuleNetwork()\n\nprint(capsule_net)\n\n# move model to GPU, if available \nif TRAIN_ON_GPU:\n    capsule_net = capsule_net.cuda()","1c09aabe":"class CapsuleLoss(nn.Module):\n    \n    def __init__(self):\n        '''Constructs a CapsuleLoss module.'''\n        super(CapsuleLoss, self).__init__()\n        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n\n    def forward(self, x, labels, images, reconstructions):\n        '''Defines how the loss compares inputs.\n           param x: digit capsule outputs\n           param labels: \n           param images: the original MNIST image input data\n           param reconstructions: reconstructed MNIST image data\n           return: weighted margin and reconstruction loss, averaged over a batch\n           '''\n        batch_size = x.size(0)\n\n        ##  calculate the margin loss   ##\n        \n        # get magnitude of digit capsule vectors, v_c\n        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n\n        # calculate \"correct\" and incorrect loss\n        left = F.relu(0.9 - v_c).view(batch_size, -1)\n        right = F.relu(v_c - 0.1).view(batch_size, -1)\n        \n        # sum the losses, with a lambda = 0.5\n        margin_loss = labels * left + 0.5 * (1. - labels) * right\n        margin_loss = margin_loss.sum()\n\n        ##  calculate the reconstruction loss   ##\n        images = images.view(reconstructions.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n\n        # return a weighted, summed loss, averaged over a batch size\n        return (margin_loss + 0.0005 * reconstruction_loss) \/ images.size(0)","58b38f10":"import torch.optim as optim\n\n# custom loss\ncriterion = CapsuleLoss()\n\n# Adam optimizer with default params\noptimizer = optim.Adam(capsule_net.parameters())","0e050a61":"def train(capsule_net, criterion, optimizer, \n          n_epochs, print_every=300):\n    '''Trains a capsule network and prints out training batch loss statistics.\n       Saves model parameters if *validation* loss has decreased.\n       param capsule_net: trained capsule network\n       param criterion: capsule loss function\n       param optimizer: optimizer for updating network weights\n       param n_epochs: number of epochs to train for\n       param print_every: batches to print and save training loss, default = 100\n       return: list of recorded training losses\n       '''\n\n    # track training loss over time\n    losses = []\n\n    # one epoch = one pass over all training data \n    for epoch in range(1, n_epochs+1):\n\n        # initialize training loss\n        train_loss = 0.0\n        \n        capsule_net.train() # set to train mode\n    \n        # get batches of training image data and targets\n        for batch_i, (images, target) in enumerate(train_loader):\n\n            # reshape and get target class\n            target = torch.eye(10).index_select(dim=0, index=target)\n\n            if TRAIN_ON_GPU:\n                images, target = images.cuda(), target.cuda()\n\n            # zero out gradients\n            optimizer.zero_grad()\n            # get model outputs\n            caps_output, reconstructions, y = capsule_net(images)\n            # calculate loss\n            loss = criterion(caps_output, target, images, reconstructions)\n            # perform backpropagation and optimization\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() # accumulated training loss\n            \n            # print and record training stats\n            if batch_i != 0 and batch_i % print_every == 0:\n                avg_train_loss = train_loss\/print_every\n                losses.append(avg_train_loss)\n                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n                train_loss = 0 # reset accumulated training loss\n        \n    return losses","5afbc62a":"# training for 5 epochs\nn_epochs = 5\nlosses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)","71272ce0":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.show()","5b658578":"def test(capsule_net, test_loader):\n    '''Prints out test statistics for a given capsule net.\n       param capsule_net: trained capsule network\n       param test_loader: test dataloader\n       return: returns last batch of test image data and corresponding reconstructions and all predictions\n       '''\n    preds = []\n    \n    capsule_net.eval() # eval mode\n\n    for batch_i, (images) in enumerate(test_loader):\n\n        batch_size = images.size(0)\n\n        if TRAIN_ON_GPU:\n            images = images.cuda()#, target, target.cuda()\n\n        # forward pass: compute predicted outputs by passing inputs to the model\n        caps_output, reconstructions, y = capsule_net(images)\n        \n        # convert output probabilities to predicted class\n        _, pred = torch.max(y.data.cpu(), 1)\n        \n        # concatenate predictions\n        preds += pred\n    \n    # convert to numpy list\n    preds = torch.stack(preds, dim=0).numpy()\n    # return last batch of capsule vectors, images, reconstructions and all the predictions\n    return preds, caps_output, images, reconstructions","abde459d":"# call test function and get reconstructed images\npreds, caps_output, images, reconstructions = test(capsule_net, test_loader)","076d1b40":"def display_images(images, reconstructions):\n    '''Plot one row of original MNIST images and another row (below) \n       of their reconstructions.'''\n    # convert to numpy images\n    images = images.data.cpu().numpy()\n    reconstructions = reconstructions.view(-1, 1, 28, 28)\n    reconstructions = reconstructions.data.cpu().numpy()\n    \n    # plot the first ten input images and then reconstructed images\n    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\n\n    # input images on top row, reconstructions on bottom\n    for images, row in zip([images, reconstructions], axes):\n        for img, ax in zip(images, row):\n            ax.imshow(np.squeeze(img), cmap='gray')\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)","7e3dbf0d":"# display original and reconstructed images, in rows\ndisplay_images(images, reconstructions)","d0749094":"# submission\nsample_sub.Label = preds\nsample_sub.to_csv('submission.csv', index=False)","f6ab7c4f":"Creation of the convolutional layer","79fc85e3":"If you want more details about the vector representation and dynamic routing I invite you to read Cezanne's blog which is very clear.","da4c3d43":"# Decoder","1d5592d5":"Let's look at the example presented in the [Cezanne Camacho's blog](https:\/\/cezannec.github.io\/Capsule_Networks\/).\nIn the following image, capsules will detect a cat\u2019s face. As shown in the image the capsule consists of neurals with properties like the position,orientation,width etc. Then we get a vector output with magnitude 0.9 which means we have 90% confidence that this is a cat face.\n\n![](https:\/\/cezannec.github.io\/assets\/capsules\/cat_face_1.png)\n\nIf we have changed one or more of these properties, like in the example the orientation of the cat\u2019s face. The CapsNet still detects the cat\u2019s face with 90% confidence(with magnitude 0.9) but there will be a change in the orientation( $\\theta$ )to indicate a change in the properties.\n\n![](https:\/\/cezannec.github.io\/assets\/capsules\/cat_face_2.png)","16cd4262":"# Encoder","0a228daa":"<a id='0'><\/a>\n# <p style=\"background-color:lightgray; font-family:newtimeroman; font-weight: 700; font-size:300%; text-align:center; border-radius: 50px 50px;\">Capsule Networks <\/p>","b8f73b94":"[Capsule Network](https:\/\/arxiv.org\/pdf\/1710.09829.pdf) is a little-known category of neural network. \n>Briefly explaining it, capsules are small groups of neurons where each neurons in a capsule represents various properties of a particular image part.\n\nThe main interest of this type of network is to address some issues of convolutional neural networks (CNN). such as for example :\n*  Relative position\n\n    CNN are unable to identify the position of one object relative to another, they can only identify whether or not an object exists in a certain region.\n\n![kndrck.co](https:\/\/i.imgur.com\/0ZyaPt3.png)\n\nFor example, both images are identified as a face by a CNN because all the key features are present regardless their position, while the capsule network (CapsNet) will not classify the one on the left as face because the key features are not positioned correctly with each other.","50c65d9a":"![image.png](attachment:8776406b-4d45-47a2-9d4f-2c478f4df05c.png)\nimage from the [original paper](https:\/\/arxiv.org\/pdf\/1710.09829.pdf) (Hinton et. al., 2017)","6ad9c609":"An advantage of CapsNet is that capsules represent the relationships between parts of a entire object by using dynamic routing to weight the connections between one layer of capsules and the next and by creating strong connections between spatially related object parts.\n\nThe output of each capsule is a vector, this vector has a magnitude and orientation.\n\n   >Magnitude: Indicate whether this particular part of the image is present or not. Basically, we can sum it up as the probability of the part's existence (it should be between 0 and 1).\n\n   >Oriantation : It changes if any of the properties of that particular image have changed.","7a012bb5":"A CapsNet is composed of two main parts :\n1. a convolutional encoder\n1. a fully-connected, linear decoder","d19f1275":"## *What are Capsule Networks ?*","d987f6c2":"# Capsule Network Architecture","47f3391d":"## *Referencies*\n\n* [Cezanne Camacho blog](https:\/\/cezannec.github.io\/Capsule_Networks\/)\n* [Kendrick](https:\/\/kndrck.co\/posts\/capsule_networks_explained\/#fn:1)\n* [Dynamic Routing Between Capsules](https:\/\/arxiv.org\/pdf\/1710.09829.pdf)"}}