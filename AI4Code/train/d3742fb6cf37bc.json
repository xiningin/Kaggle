{"cell_type":{"4fb62070":"code","e732760a":"code","41e94949":"code","f2212c8d":"code","6d92377e":"code","a7bf8134":"code","3df5f844":"code","90d080f8":"code","a1eac7eb":"code","360f304b":"code","fdf303c5":"code","568cc54b":"code","9b78b4ac":"code","10f7102c":"code","4bc72ff9":"code","e823cf00":"code","333d2b27":"code","8a8ede11":"code","bca74d70":"code","b10c61ef":"code","706dd512":"code","98964043":"code","ea6781f2":"code","7e1737bb":"code","502019dc":"code","e0e3c472":"code","d001c522":"code","1ea67f07":"code","f8a8ab05":"code","bcba1f09":"code","0f3aa975":"code","aabf1b10":"code","c6037ba1":"code","f836b84f":"code","99bde599":"code","16add6cd":"code","4e20de8a":"code","919c5b80":"code","889ab1ac":"code","5d152a1a":"code","efe00809":"code","e40696f2":"code","b548b90c":"code","d8bc8c4e":"code","65f8b8ed":"code","dc8a8f9c":"code","eb2ea8ef":"code","d94d2a33":"code","8aed0e47":"code","6dcd2e13":"code","8672774b":"code","9e3358e0":"code","8ec48929":"code","15c90443":"code","1e2b4d69":"code","4d9e382c":"code","45ef915a":"code","afb80bb9":"code","41ef00d3":"code","ca701601":"code","df995b5b":"code","2e7ef29d":"code","f5dde5c7":"code","479510fb":"code","0960e332":"code","f90f3179":"code","44430df1":"code","c7df1b10":"code","c538afcb":"code","424fdd26":"code","a79abf7c":"code","794faec1":"code","1e824ba9":"code","8a493ac0":"code","c2291608":"code","a91cbec6":"code","b547f6cf":"code","437b3f57":"code","a9d8527b":"code","2fb787b7":"code","a4e91598":"code","c4a28d4b":"code","cc0efd29":"code","61d61939":"code","37b6f1c3":"code","14dcba43":"code","9dbef1a0":"code","cf0fb7ff":"code","0a8f9047":"code","7660cd10":"code","9041efbf":"code","8ee2b503":"code","0b7707da":"code","4b955fbd":"code","a8ddc2d0":"code","b4c5c262":"code","bd6e98b1":"code","a3cbc79a":"code","7c8a499e":"code","d9df06bc":"code","f2d99426":"code","260df7c8":"code","c6e0d56a":"code","6bc91c2c":"code","7c95506c":"code","1446c842":"code","76220af7":"code","71cb3d91":"code","8bd83db0":"code","aa716c04":"code","86ba11a0":"code","d622ce44":"code","f0831ac7":"code","ed3a7772":"code","66635a6c":"code","48e362ea":"code","2dd48be9":"code","594cfe85":"code","1ed4a2ce":"code","64590c59":"code","2c69893f":"code","c02e60d8":"code","eb37963c":"markdown","9b1037b7":"markdown","ba73ed3e":"markdown","d20690a2":"markdown","3107d2c0":"markdown","cbec5e8d":"markdown","9eb11d0d":"markdown","a28f04c6":"markdown","286ef4ec":"markdown","d2adeb61":"markdown","3838a0f2":"markdown","71ed9e29":"markdown"},"source":{"4fb62070":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e732760a":"import numpy as np\nimport os\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\nfrom sklearn.neural_network import MLPClassifier\n# import pydot\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom time import perf_counter\nimport seaborn as sns\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))\n\nprint(\"All modules have been imported\")","41e94949":"image_dir = Path('..\/input\/diabetic-retinopathy-224x224-gaussian-filtered\/gaussian_filtered_images\/gaussian_filtered_images')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))","f2212c8d":"filepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head()","6d92377e":"level = []\nfor i in image_df['Label']:\n    if i=='No_DR':\n        level.append(0)\n    elif i=='Mild':\n        level.append(1)\n    elif i=='Moderate':\n        level.append(1)\n    elif i=='Severe':\n        level.append(1)\n    else:\n        level.append(1)","a7bf8134":"image_df['Level'] = level\nimage_df.head()","3df5f844":"X = []\nfor i in image_df['Filepath']:\n    image = cv2.imread(i)\n    X.append(image)\n    \nX = np.asarray(X)\ny = image_df['Level']\nY = np.asarray(y)","90d080f8":"# Y=to_categorical(Y,5)\nx_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.4, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.1, random_state=42)\nprint(len(x_train),len(x_val),len(x_test))","a1eac7eb":"# Defining our DNN Model\ndnn_model=Sequential()\ndnn_model.add(Dense(8, input_dim=2, kernel_initializer = 'uniform', activation = 'relu'))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu'))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(256, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(2,activation='softmax'))\ndnn_model.summary()","360f304b":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)","fdf303c5":"def classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    y_pred_train = [1 if x>0.5 else 0 for x in y_pred_train]\n    y_pred_val = [1 if x>0.5 else 0 for x in y_pred_val]\n    y_pred_test = [1 if x>0.5 else 0 for x in y_pred_test]\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy)) \n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n                          \n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n          \n    print(\"-\"*80)\n    print()","568cc54b":"def classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","9b78b4ac":"base_model= ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","10f7102c":"from sklearn.pipeline import make_pipeline\nfrom sklearn import pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n    \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n   \n    print('------------------------ Test Set Metrics------------------------')\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    \n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","4bc72ff9":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","e823cf00":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","333d2b27":"print(\"Performance Report:\")\ny_pred10=dnn_model.predict_classes(test_features)\ny_test10=[np.argmax(x) for x in test_y]\ny_pred_prb10=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test10, y_pred10),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test10, y_pred10, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test10,y_pred10, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test10, y_pred10, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test10, y_pred10),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test10, y_pred10,target_names=target))","8a8ede11":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","bca74d70":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","b10c61ef":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","706dd512":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","98964043":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","ea6781f2":"base_model= VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","7e1737bb":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","502019dc":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","e0e3c472":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","d001c522":"print(\"Performance Report:\")\ny_pred2=dnn_model.predict_classes(test_features)\ny_test2=[np.argmax(x) for x in test_y]\ny_pred_prb2=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test2, y_pred2),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test2, y_pred2, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test2,y_pred2, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test2, y_pred2, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test2, y_pred2),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test2, y_pred2,target_names=target))","1ea67f07":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","f8a8ab05":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","bcba1f09":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","0f3aa975":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","aabf1b10":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","c6037ba1":"base_model= VGG19(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","f836b84f":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","99bde599":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","16add6cd":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","4e20de8a":"print(\"Performance Report:\")\ny_pred1=dnn_model.predict_classes(test_features)\ny_test1=[np.argmax(x) for x in test_y]\ny_pred_prb1=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test1, y_pred1),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test1, y_pred1, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test1,y_pred1, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test1, y_pred1, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test1, y_pred1),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test1, y_pred1,target_names=target))","919c5b80":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","889ab1ac":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","5d152a1a":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","efe00809":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","e40696f2":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","b548b90c":"base_model= ResNet101(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","d8bc8c4e":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","65f8b8ed":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","dc8a8f9c":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","eb2ea8ef":"print(\"Performance Report:\")\ny_pred3=dnn_model.predict_classes(test_features)\ny_test3=[np.argmax(x) for x in test_y]\ny_pred_prb3=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test3, y_pred3),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test3, y_pred3, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test3,y_pred3, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test3, y_pred3, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test3, y_pred3),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test3, y_pred3,target_names=target))","d94d2a33":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","8aed0e47":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","6dcd2e13":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","8672774b":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","9e3358e0":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","8ec48929":"base_model= MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","15c90443":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","1e2b4d69":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","4d9e382c":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","45ef915a":"print(\"Performance Report:\")\ny_pred4=dnn_model.predict_classes(test_features)\ny_test4=[np.argmax(x) for x in test_y]\ny_pred_prb4=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test4, y_pred4),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test4, y_pred4, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test4,y_pred4, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test4, y_pred4, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test4, y_pred4),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test4, y_pred4,target_names=target))","afb80bb9":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","41ef00d3":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","ca701601":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","df995b5b":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","2e7ef29d":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","f5dde5c7":"base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","479510fb":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","0960e332":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","f90f3179":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","44430df1":"print(\"Performance Report:\")\ny_pred5=dnn_model.predict_classes(test_features)\ny_test5=[np.argmax(x) for x in test_y]\ny_pred_prb5=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test5, y_pred5),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test5, y_pred5, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test5,y_pred5, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test5, y_pred5, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test5, y_pred5),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test5, y_pred5,target_names=target))","c7df1b10":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","c538afcb":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","424fdd26":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","a79abf7c":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","794faec1":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","1e824ba9":"base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","8a493ac0":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","c2291608":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","a91cbec6":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","b547f6cf":"print(\"Performance Report:\")\ny_pred6=dnn_model.predict_classes(test_features)\ny_test6=[np.argmax(x) for x in test_y]\ny_pred_prb6=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test6, y_pred6),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test6, y_pred6, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test6,y_pred6, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test6, y_pred6, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test6, y_pred6),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test6, y_pred6,target_names=target))","437b3f57":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","a9d8527b":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","2fb787b7":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","a4e91598":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","c4a28d4b":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","cc0efd29":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","61d61939":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","37b6f1c3":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","14dcba43":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","9dbef1a0":"print(\"Performance Report:\")\ny_pred7=dnn_model.predict_classes(test_features)\ny_test7=[np.argmax(x) for x in test_y]\ny_pred_prb7=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test7, y_pred7),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test7, y_pred7, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test7,y_pred7, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test7, y_pred7, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test7, y_pred7),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test7, y_pred7,target_names=target))","cf0fb7ff":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","0a8f9047":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","7660cd10":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","9041efbf":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","8ee2b503":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","0b7707da":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","4b955fbd":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","a8ddc2d0":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","b4c5c262":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","bd6e98b1":"print(\"Performance Report:\")\ny_pred8=dnn_model.predict_classes(test_features)\ny_test8=[np.argmax(x) for x in test_y]\ny_pred_prb8=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test8, y_pred8),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test8, y_pred8, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test8,y_pred8, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test8, y_pred8, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test8, y_pred8),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test8, y_pred8,target_names=target))","a3cbc79a":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","7c8a499e":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","d9df06bc":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","f2d99426":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","260df7c8":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","c6e0d56a":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","6bc91c2c":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","7c95506c":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","1446c842":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","76220af7":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","71cb3d91":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","8bd83db0":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","aa716c04":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","86ba11a0":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","d622ce44":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","f0831ac7":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","ed3a7772":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","66635a6c":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","48e362ea":"train_y=to_categorical(y_train,2)\nval_y=to_categorical(y_val,2)\ntest_y=to_categorical(y_test,2)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","2dd48be9":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","594cfe85":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","1ed4a2ce":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","64590c59":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","2c69893f":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","c02e60d8":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","eb37963c":"# MobileNetV2","9b1037b7":"# InceptionResNetV2","ba73ed3e":"# DenseNet121","d20690a2":"# ResNet50","3107d2c0":"# MobileNet","cbec5e8d":"# DNN Model","9eb11d0d":"# DenseNet169","a28f04c6":"# InceptionV3","286ef4ec":"# ResNet101","d2adeb61":"# VGG-16","3838a0f2":"# XceptionNet","71ed9e29":"# VGG-19"}}