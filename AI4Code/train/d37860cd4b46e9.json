{"cell_type":{"b19d671b":"code","507b1172":"code","c24e9881":"code","eeedd445":"code","6e38ff1b":"code","ab259123":"code","0de2ab66":"code","09dc56a1":"code","b985a1f2":"code","22646128":"code","0f6e105e":"code","803be161":"code","442ed494":"code","8e82c326":"code","17cba407":"code","d5b11d5c":"code","56bf1c51":"code","b3724797":"code","2c5f8bb4":"code","e5170d6a":"code","35f014c5":"code","6f88823f":"code","43d88a77":"code","d15dc570":"code","e5b1debf":"code","982a5730":"code","fbfd6265":"code","ae6d5d19":"code","f5db1d4b":"code","287bee2c":"code","3ad7c41e":"code","fe665163":"code","773bbfff":"code","844acf68":"code","4d36df8d":"code","5b657ca2":"code","d9f1434a":"code","511a700b":"code","d85f58f0":"code","bba67619":"code","df636e80":"code","0d3af384":"code","f6da399e":"code","3649e493":"code","f84631c0":"code","606e4cd9":"code","ee48e6f2":"code","f8548684":"code","aa45f2eb":"code","e42981f3":"code","8131fe02":"markdown","e421857d":"markdown","5397376c":"markdown","7f92b884":"markdown","d4a01a83":"markdown","de0f455a":"markdown","693790bd":"markdown","4366cd9e":"markdown","18bb1e70":"markdown","d0bedfe9":"markdown","7c3339dd":"markdown","88e9110a":"markdown","5a6a08bc":"markdown","1969ee57":"markdown","69200559":"markdown","747035bf":"markdown","ffb1f76b":"markdown","5e4a8f6b":"markdown","9ba7458b":"markdown","2b121b6c":"markdown","ebf54403":"markdown","ebb5f08d":"markdown","98eeebf2":"markdown","a224d3a7":"markdown","bca51fba":"markdown","853e223b":"markdown","4d8c7b53":"markdown","43d742d8":"markdown","ad96d583":"markdown","44c83f0e":"markdown","856bc251":"markdown","bf5a3854":"markdown","74771946":"markdown"},"source":{"b19d671b":"!pip install seaborn --upgrade\nimport pandas as pd # data manipulation\nimport numpy as np # linear algebra\n\n# data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import qqplot # normality check\nimport plotly.express as px\nfrom sklearn.tree import plot_tree # decision tree \n\n# data preprocessing\nfrom imblearn.over_sampling import SMOTE # deal with imbalance data\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer # scale data\n\n# classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression # linear classification\nfrom sklearn.svm import LinearSVC, SVC # support vector machines\nfrom sklearn.tree import DecisionTreeClassifier # tree based\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,\\\nAdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\n\n# model evaluation and selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import classification_report, plot_roc_curve\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n\nimport os\nprint(os.listdir(\"..\/input\"))","507b1172":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","c24e9881":"df.shape   # dimensi data","eeedd445":"df.info()  # info struktur data","6e38ff1b":"df.age = df.age.astype('int64')  # Perbaiki tipe data usia ","ab259123":"# mempersiapkan data numerik dan kategori\nnumeric = ['age', 'creatinine_phosphokinase', \n           'ejection_fraction', 'platelets', \n           'serum_creatinine', 'time']\ncategorical = ['anaemia', 'diabetes', 'high_blood_pressure', \n               'sex', 'smoking']","0de2ab66":"df.isnull().sum()","09dc56a1":"target_count = df.DEATH_EVENT.value_counts()\ndeath_color = ['navy', 'crimson']\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(6, 5))\n    sns.countplot(data=df, x='DEATH_EVENT', palette=death_color)\n    for name , val in zip(target_count.index, target_count.values):\n        plt.text(name, val\/2, f'{round(val\/sum(target_count)*100, 2)}%\\n({val})', ha='center',\n                color='white', fontdict={'fontsize':13})\n    plt.xticks(ticks=target_count.index, labels=['No', 'True'])\n    plt.yticks(np.arange(0, 230, 25))\n    plt.show()","b985a1f2":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, numeric)):\n        plt.subplot(3, 3, i+1)\n        sns.histplot(data=df, x=name, color=col)\n    plt.suptitle('Histogram fitur Numerik', fontsize=15)","22646128":"fig, axes = plt.subplots(6, 2, figsize=(10, 20))\nplt.subplots_adjust(hspace=0.4)\naxes = axes.ravel()\nfor i, name, col in zip(np.arange(0, 14, 2), numeric, colors):\n    sns.boxplot(data=df, x=name, ax=axes[i], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=True)\n    sns.boxplot(data=df, x=name, ax=axes[i+1], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=False)\nplt.suptitle('Boxplot fitur Numerik sehubungan dengan kelas target\\n(dengan dan tanpa outlier) ', \n             fontsize=15)\nplt.show()","0f6e105e":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(12, 15))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, categorical)):\n        plt.subplot(3, 2, i+1)\n        sns.countplot(data=df, x=name, hue='DEATH_EVENT')","803be161":"X = df.iloc[:, :-1]\ny = df['DEATH_EVENT']\nprint(X.shape)\nprint(y.shape)","442ed494":"smote = SMOTE(random_state=2021, n_jobs=-1, k_neighbors=5)\nsmote.fit(X, y)\nX_smote, y_smote = smote.fit_resample(X, y)\nprint(X_smote.shape)\nprint(y_smote.shape)","8e82c326":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nqqplot(df.creatinine_phosphokinase, fit=True, line='45', ax=ax[0])\nax[0].set_title('sebelum transformasi')\nqqplot(np.log10(df.creatinine_phosphokinase), fit=True, line='45', ax=ax[1])\nax[1].set_title('setelah transformasi')\nplt.suptitle('plot qq untuk creatinine_phosphokinase', fontweight='bold')\nplt.show()","17cba407":"p = -1\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nqqplot(df.serum_creatinine, fit=True, line='45', ax=ax[0])\nax[0].set_title('sebelum transformasi')\n\nqqplot(df.serum_creatinine**p, fit=True, line='45', ax=ax[1])\nax[1].set_title('sesudah transformasi ')\n\nplt.suptitle('plot q-q untuk serum_creatinine', fontweight='bold')\nplt.show()","d5b11d5c":"pt = PowerTransformer(method='yeo-johnson')\nX_pt = pt.fit_transform(X_smote)\nX_pt","56bf1c51":"mm = MinMaxScaler()\nX_scaled = mm.fit_transform(X_pt)","b3724797":"pd.DataFrame(X_scaled, columns=X.columns).hist(figsize=(10, 10))\nplt.show()","2c5f8bb4":"rf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, \n                            class_weight='balanced', random_state=2021)\nrf.fit(X_scaled, y_smote)","e5170d6a":"feature_imp = pd.DataFrame(np.round(rf.feature_importances_*100, 2), index=X.columns, columns=['importance%'])\nfeature_imp = feature_imp.sort_values(by='importance%', ascending=False)\nfeature_imp.plot(kind='barh', figsize=(8, 5))\nplt.xlabel('percentage')\nplt.show()","35f014c5":"imp_features = feature_imp.index[:3]\nimp_features","6f88823f":"X_selected = pd.DataFrame(X_scaled, columns=X.columns)[imp_features]\nX_selected","43d88a77":"model_data = X_selected\nmodel_data['target'] = y_smote\nmodel_data","d15dc570":"px.scatter_3d(model_data, x='time', y='serum_creatinine', z='ejection_fraction', color='target')","e5b1debf":"X_train, X_test, y_train, y_test = train_test_split(model_data.drop(['target'], axis=1), \n                                                    model_data['target'], \n                                                    test_size=0.20, \n                                                    random_state=2021, \n                                                    stratify=model_data['target']\n                                                   )\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","982a5730":"knn_clf = KNeighborsClassifier(n_jobs=-1)\n\n# Penyetelan hiperparameter \n\nknn_params = {\n    'n_neighbors': np.arange(2, 12)\n}\nknn_cv = GridSearchCV(knn_clf, knn_params, scoring='f1', n_jobs=-1, cv=10)\nknn_cv.fit(X_train, y_train)","fbfd6265":"knn_cv.best_params_","ae6d5d19":"knn_train_pred = cross_val_predict(knn_cv, X_train, y_train, cv=10, n_jobs=-1)\nknn_train_pred","f5db1d4b":"print(classification_report(y_train, knn_train_pred, digits=4, target_names=['not gonna die', 'will die']))","287bee2c":"lr_clf = LogisticRegression(class_weight='balanced', random_state=2021, n_jobs=-1)","3ad7c41e":"# Penyetelan hyper-parameter menggunakan Pencarian Grid \nlr_params = {\n    'penalty': ['l1', 'l2', 'elasticnet'], \n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], \n    #'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\nlr_cv = GridSearchCV(lr_clf, lr_params, scoring='f1', cv=10, n_jobs=-1)\nlr_cv.fit(X_train, y_train)\n\n# Nilai parameter terbaik untuk mencapai skor F1 tertinggi \nlr_cv.best_params_","fe665163":"# Prediksi pada data pelatihan menggunakan prediksi nilai silang\nlr_train_pred = cross_val_predict(lr_cv, X_train, y_train, cv=10, n_jobs=-1)\n\n# Laporan klasifikasi\nprint(classification_report(y_train, lr_train_pred, digits=4, target_names=['not gonna die', 'will die']))","773bbfff":"lin_svm_clf = SVC(kernel='linear', class_weight='balanced', random_state=2021)\n\nparams = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nlin_svm_cv = GridSearchCV(lin_svm_clf, params, scoring='f1', n_jobs=-1, cv=10)\nlin_svm_cv.fit(X_train, y_train)\n\nlin_svm_cv.best_params_","844acf68":"lin_svm_train_pred = cross_val_predict(lin_svm_cv, X_train, y_train, cv=10, n_jobs=-1)\nprint(classification_report(y_train, lin_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","4d36df8d":"rbf_svm = SVC(kernel='rbf', class_weight='balanced')\n\nparams = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nrbf_svm_cv = GridSearchCV(rbf_svm, params, scoring='f1', n_jobs=-1, cv=10)\nrbf_svm_cv.fit(X_train, y_train)\n\nrbf_svm_cv.best_params_","5b657ca2":"rbf_svm_train_pred = cross_val_predict(rbf_svm_cv, X_train, y_train, cv=10, n_jobs=-1)\nprint(classification_report(y_train, rbf_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","d9f1434a":"dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021)\n\nparams = {\n    'criterion': ['gini', 'entropy'], \n    'max_depth': np.arange(2, 22, 2), # depth of tree\n    'min_samples_split': [2, 3, 4], # min. no. of samples a node must have before it splits \n    'min_samples_leaf': [1, 2, 3, 4] # min. non of samples a leaf node must have\n}\ndt_cv = GridSearchCV(dt_clf, params, scoring='f1', n_jobs=-1, cv=10)\ndt_cv.fit(X_train, y_train)\n\ndt_cv.best_params_","511a700b":"dt_train_pred = cross_val_predict(dt_cv, X_train, y_train, cv=10, n_jobs=-1)\n\nbest_dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021, \n                                    max_depth=4, criterion='entropy', min_samples_split=2, \n                                     min_samples_leaf= 1)\n\nbest_dt_clf.fit(X_train, y_train)","d85f58f0":"plt.figure(figsize=(15, 6))\nplot_tree(best_dt_clf, filled=True, \n          #feature_names=['time', 'serum_creatinine', 'ejection_fraction']\n         )\nplt.show()","bba67619":"rf = RandomForestClassifier(n_jobs=-1, random_state=2021, class_weight='balanced')\n\nparams = {\n    #'n_estimators': [100, 200, 300], \n    'max_depth': np.arange(2, 22, 1), \n    #'min_samples_split': [2, 3, 4], \n    #'min_samples_leaf': [1, 2, 3, 4], \n    'criterion': ['gini', 'entropy']\n}\nrf_cv = RandomizedSearchCV(rf, params, scoring='f1', n_jobs=-1, cv=10, random_state=2021, n_iter=20)\nrf_cv.fit(X_train, y_train)\n\nrf_cv.best_params_rf_train_pred = cross_val_predict(rf_cv, X_train, y_train, cv=10, n_jobs=-1, verbose=1)","df636e80":"rf_train_pred = cross_val_predict(rf_cv, X_train, y_train, cv=10, n_jobs=-1, verbose=1)","0d3af384":"print(classification_report(y_train, rf_train_pred, digits=4, target_names=['not gonna die', 'will die']))","f6da399e":"models = ['kNN', 'Logistic Regression', 'Linear SVM', 'Non-Linear SVM', \n          'Decision Tree', 'Random Forest']\nmodel_colors = sns.color_palette(\"Dark2\")\naccuracy = []\nrecall = []\nprecision = []\nf1 = []\nauc = []\npredictions = [knn_train_pred, lr_train_pred, lin_svm_train_pred, \n               rbf_svm_train_pred, dt_train_pred, rf_train_pred]\n\nfor model_pred in predictions:\n    accuracy.append(accuracy_score(y_train, model_pred))\n    precision.append(precision_score(y_train, model_pred))\n    recall.append(recall_score(y_train, model_pred))\n    f1.append(f1_score(y_train, model_pred))\n    auc.append(roc_auc_score(y_train, model_pred))","3649e493":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, accuracy, color=model_colors)\n    for m, a in zip(models, accuracy):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Accuracy percentage (%)')\n    plt.title('Model comparison on training data using Accuracy')\n    plt.show()","f84631c0":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, recall, color=model_colors)\n    for m, a in zip(models, recall):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Recall percentage (%)')\n    plt.title('Model comparison on training data using Recall')\n    plt.show()","606e4cd9":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, precision, color=model_colors)\n    for m, a in zip(models, precision):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Precision percentage (%)')\n    plt.title('Model comparison on training data using Precision')\n    plt.show()","ee48e6f2":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, f1, color=model_colors)\n    for m, a in zip(models, f1):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('F1 percentage (%)')\n    plt.title('Model comparison on training data using F1 score')\n    plt.show()","f8548684":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, auc, color=model_colors)\n    for m, a in zip(models, auc):\n        plt.text(m, a+0.01 , round(a, 3), ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('AUC')\n    plt.title('Model comparison on training data using AUC')\n    plt.show()","aa45f2eb":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nfor name, model in zip(models, best_models):\n    best_predictions = model.predict(X_test)\n    print(name.upper())\n    print(classification_report(y_test, best_predictions))\n    print(\"-------------------------------------------------------------\")","e42981f3":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    for name, model in zip(models, best_models):\n        best_predictions = model.predict(X_test)\n        fpr, tpr, thresholds = roc_curve(y_test, best_predictions)\n        plt.plot(fpr, tpr, linewidth=2, label=name)\n        plt.plot([0, 1], [0, 1], 'k--') \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Reccll)')\n    plt.legend()\n    plt.show()","8131fe02":"## 4.1 k-Nearest Neighbor Classifier\n\nMari kita mulai dengan pelajar yang sederhana dan malas di mana suatu objek diklasifikasikan berdasarkan suara pluralitas tetangganya. ","e421857d":"# 3. Manipulasi Data\n\n## 3.1 Pisahkan Fitur dan Kelas Target  ","5397376c":"## 2.5 Distribusi Fitur Numerik\n\n- Dengan memperhatikan grafik histogram variabel numeriknya, diketahui bahwa fitur `creatinine_phosphokinase` dan `serum_creatinine` sangat positif atau miring ke kanan ","7f92b884":"**Catatan:** Pendekatan serupa digunakan untuk semua pengklasifikasi di bawah ini.","d4a01a83":"## 2.6 Fitur Kategoris Distribusi Dengan Kelas Target","de0f455a":"Untuk penjelasan mengenai lebih lanjut mengenai fitur dataset silahkan kembali ke [Deskripsi Data](#des)","693790bd":"### Klasifikasi Non-Linear (Soft Margin)","4366cd9e":"## 4.3 Support Vector Machine (SVM)","18bb1e70":" ## 2.3 Missing Values\n\n- Tidak ada nilai yang hilang ","d0bedfe9":"### Klasifikasi SVM Linear (Hard Margin)\nSeperti yang terlihat dari plot 3D data. Masalahnya sepertinya tidak dapat dipisahkan menggunakan hard margin svm karena ada beberapa kebisingan di kedua kelas. ","7c3339dd":"## Perbandingan model pada data uji menggunakan Kurva ROC\n\n- kurva ROC, memplot tingkat positif sejati (nama lain untuk penarikan) terhadap tingkat positif palsu (FPR).\n- Sekali lagi ada trade-off: semakin tinggi recall (TPR), semakin banyak false positive (FPR) yang dihasilkan classifier. Garis putus-putus mewakili kurva ROC dari pengklasifikasi acak murni; pengklasifikasi yang baik tetap sejauh mungkin dari garis itu\n- Salah satu cara untuk membandingkan pengklasifikasi adalah dengan mengukur area di bawah kurva (AUC). Pengklasifikasi sempurna akan memiliki ROC AUC sama dengan 1, sedangkan pengklasifikasi acak murni akan memiliki ROC AUC sama dengan 0,5.\n- Hutan acak memiliki AUC tertinggi. ","88e9110a":"## 3.3 Normalisasi Data\n\nTerakhir, normalkan data menggunakan min-max scaler yang menskalakan data ke kisaran 0-1. Penskalaan diperlukan untuk ML juga seperti SVM, regresi logistik, knn yang sensitif terhadap penskalaan dan outlier (berlaku untuk masalah klasifikasi dan regresi). ","5a6a08bc":"Power Transformer dari paket sklearn-learn menyediakan dua metode untuk membuat distribusi seperti gaussian\n- Boxcox\n- Yeo-johnson\n\nKedua metode ini mencari nilai p yang tepat (seperti pada contoh di atas) agar distribusinya menjadi normal. Yeo-johnson adalah versi upgrade dari Boxcox karena berurusan dengan data dengan nilai negatif ","1969ee57":"# 5. Evaluasi Model\n\nRandom Forest telah mengungguli semua pengklasifikasi lainnya dalam akurasi, presisi, daya ingat, skor f1, dan skor AUC. ","69200559":"## 3.5  Seleksi Fitur menggunakan Random Forest  ","747035bf":"# 2. Persiapan Data\n\n## 2.1 Paket yang Dibutuhkan","ffb1f76b":"## 2.4 Check Variabel Targets \n\n- Kelas atau label target tidak seimbang ","5e4a8f6b":"## 4.4 Pohon Keputusan (Decision Tree) \n\n- Memerlukan sedikit persiapan data dan tidak memerlukan penskalaan atau pemusatan fitur.\n- Sederhana untuk dipahami dan diinterpretasikan.\n- Pohon Keputusan seringkali memiliki keputusan ortogonal (semua pemisahan tegak lurus terhadap sumbu), yang membuatnya sensitif terhadap rotasi set pelatihan. Sangat mungkin bahwa model tidak akan menggeneralisasi dengan baik. Salah satu cara untuk membatasi masalah ini adalah dengan menggunakan Analisis Komponen Utama yang seringkali menghasilkan orientasi yang lebih baik dari data pelatihan.\n- Masalah utama dengan Pohon Keputusan adalah bahwa mereka sangat sensitif terhadap variasi kecil dalam data pelatihan. \n- Random Forests dapat membatasi ketidakstabilan ini dengan membuat prediksi rata-rata pada banyak pohon. ","9ba7458b":"# 1. Latar Belakang\n\nPenyakit Lardiovaskular (Cardiovascular diseases) adalah penyebab kematian nomor 1 secara global, mengambil sekitar 17,9 juta jiwa setiap tahun, yang menyumbang 31% dari semua kematian di seluruh dunia. Gagal jantung adalah kejadian umum yang disebabkan oleh Penyakit Lardiovaskular dan kumpulan data ini berisi 12 fitur yang dapat digunakan untuk memprediksi kematian akibat gagal jantung.\n\nSebagian besar penyakit kardiovaskular dapat dicegah dengan mengatasi faktor risiko perilaku seperti penggunaan tembakau, diet tidak sehat dan obesitas, kurangnya aktivitas fisik, dan penggunaan alkohol yang berbahaya menggunakan strategi di seluruh populasi.Orang dengan penyakit kardiovaskular atau yang berada pada risiko kardiovaskular tinggi (karena adanya satu atau lebih faktor risiko seperti hipertensi, diabetes, hiperlipidemia, atau penyakit yang sudah ada) memerlukan deteksi dan manajemen dini di mana model pembelajaran mesin dapat sangat membantu. . \n\n## 1.1 Deskripsi Data\n\nDataset dari Davide Chicco, Giuseppe Jurman: \n\n**1. Age:** usia pasien (dalam tahun)\n\n**2. Anemia:** Penurunan sel darah merah atau hemoglobin\n\n**3. High blood pressure:** Jika pasien menderita hipertensi\n\n**4. Creatinine phosphokinase:** Tingkat enzim CPK dalam darah (mcg\/L)\n\n**5. Diabetes:** Jika pasien menderita diabetes\n\n**6. Ejection fraction:** Persentase darah yang meninggalkan jantung pada setiap kontraksi\n\n**7. Sex:** Wanita atau pria\n\n**8. Platelets:** Trombosit dalam darah (kiloplatelet\/mL)\n\n**9. Serum creatinine:** Tingkat kreatinin dalam darah (mg\/dL)\n\n**10. Serum sodium:** Tingkat natrium dalam darah (mEq\/L)\n\n**11. Smoking:** Jika pasien merokok\n\n**12. Time::** Periode tindak lanjut (dalam hari)\n\n**13. (target) death event:** Jika pasien meninggal selama masa tindak lanjut \n\n## 1.2 Deskripsi Masalah\n\n- Membuat model untuk memprediksi kemungkinan pasien meninggal karena gagal jantung.\n- Ini adalah masalah klasifikasi biner karena kelas target (Kematian) terdiri dari dua kelas Benar atau Salah ","2b121b6c":"# 6. Prediksi pada Data Uji","ebf54403":"# Kesimpulan\n\nModel yang terbaik untuk melakukan klasifikasi untuk dataset ini adalah KNN.\n\n","ebb5f08d":"## 3.2 Transformasi Data\n\nSelama EDA untuk fitur numerik, histogram dari beberapa fitur menunjukkan kemiringan. Beberapa fitur seperti `creatinine_phosphokinase` dan `serum_creatinine` sangat miring. Fitur miring seperti ini dapat dibuat lebih mirip Gaussian menggunakan transformasi daya atau transformasi log. Sebagai contoh:\n\n**1. creatinine_phosphokinase** menggunakan transformasi log dapat membuat data sesuai dengan normalitas. Dalam hal ini, log-transform menghilangkan atau mengurangi skewness karena data asli mengikuti distribusi log-normal atau kira-kira begitu. ","98eeebf2":"### Laporan Klasifikasi data pelatihan ","a224d3a7":"## 4.2 Regresi Logistik\n\nRegresi logistik menggunakan fungsi logit untuk menghitung probabilitas hasil, dalam kasus kami, kelas target `` ","bca51fba":"## 4.5 Random Forests\n\n- Ini adalah ansambel (kelompok prediktor) dari pohon keputusan.\n- Ia memiliki semua hyperparameters dari pohon keputusan ","853e223b":"## 3.2 Perbaiki Ketidakseimbangan Kelas menggunakan SMOTE\n\nSMOTE adalah teknik oversampling di mana sampel sintetis dihasilkan untuk kelas minoritas, dalam kasus ini, 1's ","4d8c7b53":"**Note:** Plot qq di atas menunjukkan pengaruh transformasi log pada kreatinin_fosfokinase. Plot QQ (atau plot kuantil-kuantil) adalah plot di mana sumbu sengaja diubah untuk membuat distribusi normal (atau Gaussian) muncul dalam garis lurus. Dengan kata lain, distribusi normal sempurna akan mengikuti garis dengan kemiringan = 1 dan intersep = 0. ","43d742d8":"### Prediksi pada data pelatihan menggunakan prediksi lintas Val\n\ncross_val_predict() melakukan validasi silang K-fold, tetapi alih-alih mengembalikan skor evaluasi, ia mengembalikan prediksi yang dibuat pada setiap lipatan tes (iterasi). Ini berarti Anda mendapatkan prediksi yang jelas untuk setiap instance dalam set pelatihan. ","ad96d583":" ## 2.2 Dataset dan Deskripsi Fitur ","44c83f0e":"# 4. Pemodelan Data dan Hyperparameter\n\nOleh karena itu, tujuannya sekarang adalah untuk memisahkan kedua kelas seperti yang ditunjukkan pada gambar di bawah ini\n\n**Catatan:** Semua pengklasifikasi telah disetel untuk memaksimalkan skor f1 alih-alih akurasi. Skor F1 adalah rata-rata harmonik dari recall dan presisi. Skor ini akan mendukung pengklasifikasi dengan presisi dan daya ingat yang sama. Saya bisa saja mencapai daya ingat atau presisi yang tinggi tetapi sayangnya, kita tidak dapat memilikinya dua arah karena meningkatkan presisi mengurangi daya ingat, dan sebaliknya. ","856bc251":"### Nilai Optimal untuk Memaksimalkan Kinerja\n\nOleh karena itu, skor f1 tertinggi dicapai dengan knn menggunakan 6 tetangga. ","bf5a3854":"**2. serum_creatinine** menggunakan transformasi resiprokal (p = -1). Transformasi ini memiliki efek radikal karena membalikkan urutan di antara nilai-nilai dari tanda yang sama, oleh karena itu, nilai yang lebih besar menjadi lebih kecil, dan sebaliknya.","74771946":"## 3.4  Distribusi Fitur Setelah Transformasi dan Penskalaan "}}