{"cell_type":{"5021ea7e":"code","f03a8b00":"code","e04a096e":"code","532647f4":"code","8cdf6263":"code","0ffe49f6":"code","de4f8249":"code","a95ba013":"code","120a5473":"code","91e2f407":"code","782b4c67":"code","11dc409c":"code","e4342fc6":"code","85e64da6":"code","267cb668":"code","7d8f0b9e":"markdown","61d6c77c":"markdown","5ea33617":"markdown","aab96b08":"markdown","855b695a":"markdown","f2afb80f":"markdown","5c753553":"markdown"},"source":{"5021ea7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom sklearn.model_selection import train_test_split # split train and test data \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f03a8b00":"class LinearRegression():\n    \"\"\" Apply Linear Regression\n        \n        Args: \n            lr : float. learning rate \n            iterations : int. Hom many iteration for training \n            \n    \"\"\"\n    def __init__(self,lr=0.05, iterations=100):\n        self.lr = lr \n        self.iterations = iterations\n        \n    def fit(self,x,y):\n        \"\"\" Fit the our model\n        \n            Args: \n                x : np.array, shape = [n_samples, n_features]. Training Data \n                y : np.array, shape = [n_samples, n_conclusion]. Target Values\n                \n            Returns: \n                self : object\n        \"\"\"\n        \n        self.cost_list = []\n        self.theta = np.zeros((x.shape[1],1))   \n        self.theta_zero = np.zeros((1,1))\n        m = x.shape[0]   # samples in the data\n        name_list=[]     # for plot x-axis name\n        \n        for i in range(self.iterations):  # Feed forward\n            \n            h_pred = np.dot(x,self.theta) + self.theta_zero    \n            error = h_pred - y \n            cost = 1\/(2*m)*(np.sum((error ** 2)))\n            gradient_vector = np.dot(x.T, error)\n            self.theta -= (self.lr\/m) * gradient_vector # Gradient Descent\n            self.theta_zero -= (self.lr\/m) * gradient_vector # Gradient Descent for theta zero\n            self.cost_list.append(cost)\n            name_list.append(i)\n        \n        plt.scatter(name_list,self.cost_list)\n    \n        return self\n    \n    def predict(self, x):\n        \"\"\" Predicts the value after the model has been trained.\n        \n            Args: \n                x: np.array, shape = [n_samples, n_features]. Training Data\n                \n            Returns: \n                Predicted value \n        \"\"\"\n        \n        self.y_pred = np.dot(x,self.theta) + self.theta_zero\n        \n        return self.y_pred\n        ","e04a096e":"def delete_comma_and_convert_float(df,column_name):\n    \"\"\" Delete comma that in the coumn's values \n    \n        Args:\n            df : dataframe.\n            column_name : string. \n    \"\"\"\n    index = df.columns.get_loc(column_name)\n    for i in range(len(df[column_name])):\n        value = df.iloc[i,index]\n        value_list = value.split(',')\n        if len(value_list) == 2:\n            new_value = float(''.join(value_list)) \/ 10\n            df.iloc[i,index] = new_value\n        else:\n            df.iloc[i,index] = float(value)","532647f4":"dataset = pd.read_csv(\"..\/input\/car-consume\/measurements.csv\")   # read data from file ","8cdf6263":"dataset.head(10)","0ffe49f6":"dataset.info()","de4f8249":"dropped_data = dataset.drop(['temp_inside', 'specials', 'AC', 'refill liters', 'refill gas'], axis=1) # drop some features","a95ba013":"delete_comma_and_convert_float(dropped_data, 'distance')\ndropped_data['distance'] = dropped_data['distance'].astype(float)\ndelete_comma_and_convert_float(dropped_data, 'consume')\ndropped_data['consume'] = dropped_data['consume'].astype(float)","120a5473":"dropped_data['gas_type'] = dropped_data['gas_type'].map({'SP98': 1, 'E10': 0})  # change 'gas_type' values to 1 and 0. String to int","91e2f407":"new_df = dropped_data[['distance','speed','temp_outside','gas_type','rain','sun','consume']]\nsorted_df = new_df = new_df.sort_values('consume')\ndataset_x = sorted_df.drop(['consume'],axis=1)\ndataset_y = sorted_df.consume.values","782b4c67":"plt.figure(figsize=(19,6))\n\nplt.subplot(161)\nplt.scatter(dataset_x['distance'],dataset_y)\nplt.subplot(162)\nplt.scatter(dataset_x['speed'],dataset_y)\nplt.subplot(163)\nplt.scatter(dataset_x['temp_outside'],dataset_y)\nplt.subplot(164)\nplt.scatter(dataset_x['gas_type'],dataset_y)\nplt.subplot(165)\nplt.scatter(dataset_x['rain'],dataset_y)\nplt.subplot(166)\nplt.scatter(dataset_x['sun'],dataset_y)\n\n\nplt.show()","11dc409c":"dataset_x = (dataset_x - np.min(dataset_x)) \/ (np.max(dataset_x) - np.min(dataset_x))","e4342fc6":"x_train, x_test, y_train, y_test = train_test_split(dataset_x, dataset_y, test_size= 0.2, random_state= 42)","85e64da6":"x_train_distance = x_train.distance.values\nx_train_distance = x_train_distance.reshape((len(x_train_distance), 1))\ny_train = y_train.reshape((len(y_train), 1))\nprint(x_train_distance.shape)\nprint(y_train.shape)","267cb668":"LinearRegression = LinearRegression()\nLinearRegression.fit(x_train_distance,y_train)","7d8f0b9e":"## Delete Comma and Convert Float ##","61d6c77c":"## Data Train and Test Split","5ea33617":"# Code #","aab96b08":"## Train","855b695a":"## Prepared Data for Training","f2afb80f":"# Libs\n## Model Libs ","5c753553":"## Normalization"}}