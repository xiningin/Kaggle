{"cell_type":{"0b71c7a1":"code","7b725186":"code","634189ad":"code","af699de1":"code","473de44b":"code","76da4ccc":"code","79de7145":"code","00257374":"code","a5736da8":"code","c88aeb4e":"code","73f29b73":"code","c1065268":"code","2a774dbe":"code","fec7baed":"code","87c43392":"code","4b211f80":"code","a98f87ad":"code","e42b816d":"code","7e637f9f":"code","31ac87d1":"code","919e177c":"code","3d4cf908":"code","e5e87160":"code","27cbc5fb":"code","40b26212":"code","4841cb30":"code","6a6c3f37":"code","df6fd9bb":"code","63dba3b5":"code","8044584c":"code","b1d3b8a5":"markdown"},"source":{"0b71c7a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob as gb\nimport os\nimport cv2","7b725186":"class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\nn_classes = 6","634189ad":"class_names[2]","af699de1":"print(class_names_label)","473de44b":"def load_data():\n    datasets = ['seg_train\/seg_train', 'seg_test\/seg_test']\n    size = (150, 150)\n    output = [] \n    for dataset in datasets:\n        directory = \"\/kaggle\/input\/intel-image-classification\/\" + dataset\n        images = []\n        labels = []\n        for folder in os.listdir(directory):\n            curr_label = class_names_label[folder]\n            for file in os.listdir(directory + \"\/\" + folder):\n                img_path = directory + \"\/\" + folder + \"\/\" + file\n                curr_img = cv2.imread(img_path)\n                curr_img = cv2.resize(curr_img, size)\n                images.append(curr_img)\n                labels.append(curr_label)\n        images = np.array(images, dtype='float32')\n        labels = np.array(labels, dtype='int32')\n\n        images = images \/ 255\n\n        output.append((images, labels))\n\n    return output","76da4ccc":"#%tensorflow_version 2.x  # this line is not required unless you are in a notebook\nimport tensorflow as tf\n\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nkeras = tf.keras","79de7145":"(train_images, train_labels),(test_images, test_labels) = load_data()","00257374":"train_images.shape","a5736da8":"# Let's look at a one image\nIMG_INDEX = 8710  # change this to look at other images\n\nplt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\nplt.xlabel(class_names[train_labels[IMG_INDEX]])\nplt.show()","c88aeb4e":"IMG_SIZE = 150","73f29b73":"IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","c1065268":"#base_model.summary()","2a774dbe":"base_model.trainable = False # freezing the trainable hyperparemetes of the existing model. pre-trained model MobileNet V2","fec7baed":"#base_model.summary()","87c43392":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()","4b211f80":"prediction_layer = keras.layers.Dense(6)","a98f87ad":"model = tf.keras.Sequential([\n  base_model,\n  global_average_layer,\n  prediction_layer\n])","e42b816d":"#model.summary()","7e637f9f":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=10, \n                    validation_data=(test_images, test_labels))","31ac87d1":"def plot_accuracy_loss(history):\n    \"\"\"\n        Plot the accuracy and the loss during the training of the nn.\n    \"\"\"\n    fig = plt.figure(figsize=(10,5))\n\n    # Plot accuracy\n    plt.subplot(221)\n    plt.plot(history.history['accuracy'],'bo--', label = \"acc\")\n    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_acc\")\n    plt.title(\"train_acc vs val_acc\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # Plot loss function\n    plt.subplot(222)\n    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n    plt.title(\"train_loss vs val_loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epochs\")\n\n    plt.legend()\n    plt.show()","919e177c":"plot_accuracy_loss(history)","3d4cf908":"test_loss = model.evaluate(test_images, test_labels)","e5e87160":"predpath = '..\/input\/intel-image-classification\/seg_pred\/'","27cbc5fb":"pred = []\nfiles = gb.glob(pathname= str(predpath + 'seg_pred\/*.jpg'))\nfor file in files: \n    image = cv2.imread(file)\n    image_array = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n    pred.append(list(image_array))","40b26212":"images = np.array(pred, dtype='float32')\nimages = images \/ 255","4841cb30":"len((images))","6a6c3f37":"plt.figure(figsize=(20,20))\nfor n , i in enumerate(list(np.random.randint(0,len(images),36))) : \n    plt.subplot(6,6,n+1)\n    plt.imshow(images[i])    \n    plt.axis('off')\n","df6fd9bb":"preds = model.predict(images) # contains the final probiblity distribution of the output classes for each of the image. 7301 images and 6 classes distribution.","63dba3b5":"len(preds)","8044584c":"plt.figure(figsize=(20,20))\nfor n , i in enumerate(list(np.random.randint(0,len(images),36))) : \n    plt.subplot(6,6,n+1)\n    plt.imshow(images[i])    \n    plt.axis('off')\n    x =np.argmax(preds[i]) # takes the maximum of of the 6 probabilites. \n    plt.title((class_names[x]))","b1d3b8a5":"> Now that we have our base layer setup, we can add the classifier. Instead of flattening the feature map of the base layer we will use a global average pooling layer that will average the entire 5x5 area of each 2D feature map and return to us a single 1280 element vector per filter."}}