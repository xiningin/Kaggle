{"cell_type":{"bd9d4bae":"code","f696c8b9":"code","cb73bbdd":"code","16a92f49":"code","5be5d2c2":"code","3902f456":"code","d4f7aeb0":"code","a0a91823":"code","2c17ed50":"code","3e5182b6":"code","e5cb3762":"code","1a7fc76a":"code","fffa573e":"code","bce278bc":"code","a173acfd":"code","c5d3bd7f":"code","755abd58":"code","23578309":"code","2d886c14":"markdown","9d03a916":"markdown","0d3dacc3":"markdown","ee5b747c":"markdown","7dbe1411":"markdown","1bb5ff49":"markdown","aead6b2a":"markdown","e68a36e9":"markdown","c25f9222":"markdown","5995bc2b":"markdown","a0c9e391":"markdown"},"source":{"bd9d4bae":"# Built-in imports\nimport os\nimport sys\nimport random\n\n# Image processing imports\nimport cv2\n\n# Machine learning imports\nimport numpy as np; print('NUMPY Version:{}'.format(np.__version__))\nimport pandas as pd; print('PANDAS Version:{}'.format(pd.__version__))\nimport tensorflow as tf; print('TENSORFLOW Version:{}'.format(tf.__version__))\n\n# Visualization imports\nimport matplotlib.pyplot as plt","f696c8b9":"AUTOTUNE = tf.data.experimental.AUTOTUNE  \nIMG_SIZE = 150\nBATCH_SIZE = 32\nEPOCHS = 50\n\nDATA_DIR = '..\/input\/car-damage-detection\/data1a\/'\ntrain_dir = os.path.join(DATA_DIR, 'training\/')\ntest_dir = os.path.join(DATA_DIR, 'validation\/')\n\n# Attempt at deterministic outputs(Reproducibility)\ndef seed_it_all(seed=7):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    #rng = np.random.default_rng(seed)\n    \nseed_it_all()","cb73bbdd":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    # On google colab (tpu_address = 'grpc:\/\/' + os.environ['COLAB_TPU_ADDR'], pass tpu_address as param in below fn)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    #If no TPU, uncomment below to check for GPU\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","16a92f49":"num_train_damaged = len(os.listdir(train_dir + '00-damage\/'))\nnum_train_whole = len(os.listdir(train_dir + '01-whole\/'))\n\nprint(\"Number of damaged train images:{}\".format(num_train_damaged))\nprint(\"Number of whole train images:{}\".format(num_train_whole))\nprint(\"Total train images:{}\".format(num_train_damaged + num_train_whole))\n\nprint('\\n')\n\nnum_test_damaged = len(os.listdir(test_dir + '00-damage\/'))\nnum_test_whole = len(os.listdir(test_dir + '01-whole\/'))\n\nprint(\"Number of damaged test images:{}\".format(num_test_damaged))\nprint(\"Number of whole test images:{}\".format(num_test_whole))\nprint(\"Total test images:{}\".format(num_test_damaged + num_test_whole))","5be5d2c2":"def display_multiple_img(image_dir, rows, cols):\n    \"\"\"\n    Function to Display Images from Dataset.\n    \n    parameters: image_dir(string) - Path of directory with images\n                rows(int) - No. of Rows in Output\n                cols(int) - No. of Columns in Output\n    \"\"\"\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8))\n    for ind,image_path in enumerate(os.listdir(image_dir)):\n        image = cv2.imread(image_dir + image_path, -1)\n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_xlabel(image.shape)\n        except:\n            continue;\n        \n    plt.tight_layout()\n    plt.show()","3902f456":"# Display damaged cars\ndisplay_multiple_img(train_dir + '00-damage\/', 3, 3)","d4f7aeb0":"# Display undamaged cars\ndisplay_multiple_img(train_dir + '01-whole\/', 3, 3)","a0a91823":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1\/255,\n                                  rotation_range=40,\n                                  width_shift_range=0.4,\n                                  height_shift_range=0.4,\n                                  shear_range=0.2,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\ntest_datagen = ImageDataGenerator(rescale=1\/255)\n\ntrain_dataset = train_datagen.flow_from_directory(train_dir,\n                                          target_size=(IMG_SIZE,IMG_SIZE),\n                                          batch_size = BATCH_SIZE,\n                                          class_mode = 'binary')\n                                         \ntest_dataset = test_datagen.flow_from_directory(test_dir,\n                                          target_size=(IMG_SIZE,IMG_SIZE),\n                                          batch_size = BATCH_SIZE,\n                                          class_mode = 'binary')\n\ntest_dataset.class_indices","2c17ed50":"def block(x, filters, kernel_size, repetitions, pool_size=2, strides=2):\n    for i in range(repetitions):\n        x = tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size, strides)(x)\n    return x","3e5182b6":"def get_model():\n    image_inputs = tf.keras.Input((IMG_SIZE, IMG_SIZE , 3))\n    x = block(image_inputs, 8, 3, 2)\n    x = block(x, 16, 3, 2)\n    x = block(x, 32, 3, 2)\n    x = block(x, 64, 3, 2)\n    x = block(x, 128, 3, 2)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    \n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[image_inputs], outputs=[output])\n    return model","e5cb3762":"model = get_model()","1a7fc76a":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=0.001)\nlr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', verbose=1, patience=2, factor=0.2, min_lr=0.0001)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1,save_best_only=True,mode='max')\n\ncallbacks = [early_stopping, lr_reduction, model_checkpoint]","fffa573e":"history = model.fit(train_dataset, \n                    validation_data=test_dataset,\n                    epochs=EPOCHS,\n                    callbacks=callbacks,\n                    batch_size=BATCH_SIZE)","bce278bc":"plt.plot(history.history['loss'], label='train loss') \nplt.plot(history.history['val_loss'], label='validation loss') \nplt.legend()\nplt.show()","a173acfd":"plt.plot(history.history['accuracy'], label='train accuracy') \nplt.plot(history.history['val_accuracy'], label='validation accuracy') \nplt.legend()\nplt.show()","c5d3bd7f":"print('Validation accuracy achieved', history.history['val_accuracy'][-2])","755abd58":"def predictImage(filename):\n    \n    img = cv2.imread(filename)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n    plt.imshow(img)\n    \n    Y = np.array(img)\n    X = np.expand_dims(Y,axis=0)\n    val = model.predict(X)\n    print(val)\n    if val < 0.5:\n        plt.xlabel(\"Car Damaged\",fontsize=30)\n    elif val >= 0.5:\n        plt.xlabel(\"Car Not Damaged\",fontsize=30)","23578309":"predictImage(\"..\/input\/car-damage-detection\/data1a\/validation\/01-whole\/0001.jpg\")","2d886c14":"## Config\n\nSetting up variables and accelarator","9d03a916":"## Imports","0d3dacc3":"* As seen in above plots, images are of different sizes, hence need to be reshaped. \n* We will carry out normalization and data augumentation using ImageDataGenerator below.\n","ee5b747c":"## Evaluate model","7dbe1411":"## Approach\n\n1. Module imports\n2. Configuration setup: Setting up required variables, seed and accelarator.\n3. Dataset exploration: Checking out sample images, their shapes, classwise distribution.\n4. Creating datasets: Used Keras ImageDataGenerators to create datasets directly from directories\n5. Define model: Define a CNN model using TF Functional API, compile it with metrics: accuracy.\n6. Train model: Train for 50 epochs, with EarlyStopping callback.\n7. Plot metrics: Plot accuracy, loss w.r.t epochs\n8. Evaluate model: Evaluate model on random test image.\n\n\nSteps taken to increase accuracy:\n1. Added image augumentations, only on training data\n2. Used ReduceLROnPlateu and EarlyStopping callbacks\n3. Increased patience of EarlyStopping to 5\n4. Decreased patience of ReduceLROnPlateau to 1\n5. Reduced EarlyStopping patience to 2","1bb5ff49":"## Define Model","aead6b2a":"## Plot metrics","e68a36e9":"## Train model","c25f9222":"## Dataset Exploration\nChecking out sample images, their shapes, classwise distribution.","5995bc2b":"**We can see that the dataset is balanced between both classes, in both the train and test sets.**","a0c9e391":"## Create Datasets"}}