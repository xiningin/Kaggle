{"cell_type":{"e6d33f77":"code","535554ab":"code","e6501bd4":"code","3d7f1331":"code","848d01b5":"code","25618851":"code","532852bb":"code","a95a26e3":"code","20829d14":"code","f57e4420":"code","0fbaff28":"code","ca8a01ff":"code","dc09b288":"code","9f22581a":"markdown","900ed2c7":"markdown","943fd48b":"markdown","017e6668":"markdown","a226e608":"markdown","3fde23fb":"markdown"},"source":{"e6d33f77":"import pandas as pd\nimport numpy as np","535554ab":"df=pd.read_csv(r\"..\/input\/phishing-website-detector\/phishing.csv\")","e6501bd4":"X= df.drop(columns='class')\nY=df['class']\nY=pd.DataFrame(Y)","3d7f1331":"X.describe()","848d01b5":"import seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\npd.value_counts(Y['class']).plot.bar()","25618851":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","532852bb":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=10, verbose=5)\nrfe_selector.fit(X_norm, Y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')\nprint(rfe_feature)","a95a26e3":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nchi_selector = SelectKBest(chi2, k=10)\nchi_selector.fit(X_norm, Y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\nprint(chi_feature)","20829d14":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), '1.25*median')\nembeded_lr_selector.fit(X_norm, Y)\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')\nprint(embeded_lr_feature)","f57e4420":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_norm = scaler.fit_transform(X)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=25)\nY_sklearn = pca.fit_transform(X_norm)","0fbaff28":"cum_sum = pca.explained_variance_ratio_.cumsum()\n\npca.explained_variance_ratio_[:10].sum()\n\ncum_sum = cum_sum*100\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.yticks(np.arange(0,110,10))\nplt.bar(range(25), cum_sum, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)\nplt.title(\"Around 95% of variance is explained by the First 25 colmns \");","ca8a01ff":"explained_variance=pca.explained_variance_ratio_\nprint(explained_variance.shape)\nprint(explained_variance.sum())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(25), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","dc09b288":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=20, n_iter=50, random_state=42)\nsvd.fit(X_norm)\nexplained_variance=svd.explained_variance_ratio_\nprint(explained_variance.shape)\nprint(svd.explained_variance_ratio_.sum())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(20), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","9f22581a":"# Check if data is blanced (Legit vs Phising)","900ed2c7":"# Feature Extraction\n## PCA\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html \nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.","943fd48b":"# Embeded\n## Logistic Regression\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectFromModel.html","017e6668":"# CHI2\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.chi2.html","a226e608":"# SVD","3fde23fb":"> # Recursive feature elimination\nresource: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html"}}