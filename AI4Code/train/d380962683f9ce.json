{"cell_type":{"bd1bb5f5":"code","d532c4ce":"code","019adb1f":"code","1de76528":"code","e350cc03":"code","40c80062":"code","76b7b555":"code","aae7093b":"code","a2c1bba7":"code","28b54462":"code","9faf5343":"code","76e75140":"code","44e4d86c":"code","9f4e2c78":"code","f73bb3d8":"code","617a46ba":"code","f4ced280":"code","24441a7c":"code","b332a4b7":"code","e906c553":"markdown","3e8b7e38":"markdown","9852d320":"markdown","90df8ffb":"markdown","bf178578":"markdown","85c259a8":"markdown","a93101b5":"markdown","8d31a977":"markdown"},"source":{"bd1bb5f5":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d532c4ce":"train=pd.read_csv('..\/input\/labeledTrainData.tsv',delimiter='\\t',quoting=3)\ntrain.head(4)","019adb1f":"train.shape","1de76528":"train.columns.values","e350cc03":"train[\"review\"][0]","40c80062":"from bs4 import BeautifulSoup\nexample=BeautifulSoup(train['review'][0])\nprint (train['review'][0])\nprint (example.get_text())","76b7b555":"import re\nletters_only=re.sub(\"[^a-zA-Z]\",\" \",example.get_text())\nprint(letters_only)","aae7093b":"lower_case=letters_only.lower()\nwords=lower_case.split()","a2c1bba7":"from nltk.corpus import stopwords\nprint (stopwords.words(\"english\"))","28b54462":"words=[w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","9faf5343":"def review_to_words(raw_review):\n    review_text=BeautifulSoup(raw_review).get_text()\n    letters_only=re.sub('[^a-zA-Z]',' ',review_text)\n    words=letters_only.lower().split()\n    stops=set(stopwords.words('english'))\n    meaningful_words=[w for w in words if not w in stops]\n    return(' '.join(meaningful_words))","76e75140":"clean_review=review_to_words(train['review'][0])\nprint(clean_review)","44e4d86c":"num_reviews=train['review'].size\nnum_reviews","9f4e2c78":"clean_train_review=[]\nfor i in range(0,num_reviews):\n    clean_train_review.append(review_to_words(train['review'][i]))","f73bb3d8":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer=CountVectorizer(analyzer='word',tokenizer=None,preprocessor = None, stop_words = None,max_features = 5000)\ntrain_data_features=vectorizer.fit_transform(clean_train_review)\ntrain_data_features=train_data_features.toarray()","617a46ba":"train_data_features.shape","f4ced280":"vcab=vectorizer.get_feature_names()\nprint(vcab)","24441a7c":"from sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=100)\nforest = forest.fit( train_data_features, train[\"sentiment\"] )","b332a4b7":"# Read the test data\ntest = pd.read_csv(\"..\/input\/testData.tsv\", header=0, delimiter=\"\\t\",quoting=3 )\n\n# Verify that there are 25,000 rows and 2 columns\nprint(test.shape)\n\n# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Use the random forest to make sentiment label predictions\nresult = forest.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )","e906c553":"**Random Forest**\nAt this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning!","3e8b7e38":"**Please UPVOTE for Encouragement**","9852d320":"**Data Cleaning and Text Preprocessing**\n\nFor removing HTML tages I am using beautiful soup library.","90df8ffb":"When considering how to clean the text, we should think about the data problem we are trying to solve. For many problems, it makes sense to remove punctuation. On the other hand, in this case, we are tackling a sentiment analysis problem, and it is possible that \"!!!\" or \":-(\" could carry sentiment, and should be treated as words. In this tutorial, for simplicity, we remove the punctuation altogether, but it is something you can play with on your own.\n\nSimilarly, in this tutorial we will remove numbers, but there are other ways of dealing with them that make just as much sense. For example, we could treat them as words, or replace them all with a placeholder string such as \"NUM\".\n\nTo remove punctuation and numbers, we will use a package for dealing with regular expressions, called re.","bf178578":"Finally, we need to decide how to deal with frequently occurring words that don't carry much meaning. Such words are called \"stop words\"; in English they include words such as \"a\", \"and\", \"is\", and \"the\". Conveniently, there are Python packages that come with stop word lists built in. Let's import a stop word list from the Python Natural Language Toolkit (NLTK). You'll need to install the library if you don't already have it on your computer; you'll also need to install the data packages that come with it, as follows:","85c259a8":"Creating Features from a Bag of Words (Using scikit-learn)\nNow that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n\nSentence 1: \"The cat sat on the hat\"\n\nSentence 2: \"The dog ate the cat and the hat\"\n\nFrom these two sentences, our vocabulary is as follows:\n\n{ the, cat, sat, on, hat, dog, ate, and }\n\nTo get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n\n{ the, cat, sat, on, hat, dog, ate, and }\n\nSentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n\nSimilarly, the features for Sentence 2 are: { 3, 1, 0, 0, 1, 1, 1, 1}\n\nIn the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n\nWe'll be using the feature_extraction module from scikit-learn to create bag-of-words features.","a93101b5":"Two elements here are new: First, we converted the stop word list to a different data type, a set. This is for speed; since we'll be calling this function tens of thousands of times, it needs to be fast, and searching sets in Python is much faster than searching lists.\n\nSecond, we joined the words back into one paragraph. This is to make the output easier to use in our Bag of Words, below. After defining the above function, if you call the function for a single review:","8d31a977":"We'll also convert our reviews to lower case and split them into individual words (called \"tokenization\" in NLP lingo)"}}