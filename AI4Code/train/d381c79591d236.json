{"cell_type":{"a3b37e9d":"code","b5d1cf4b":"code","fc447fef":"code","37793414":"code","a8ca4306":"code","0c235d1c":"code","84da8836":"code","5bddbecb":"code","1b327ba3":"code","a2c69d4d":"code","e50ab91b":"code","12ff4ab4":"code","2290a53c":"code","7afaf6b0":"code","142ebc3d":"code","80410e79":"code","f519edf3":"code","19e251cf":"code","0c20e185":"code","b012e335":"code","c770bbdd":"code","cd4b2348":"code","0d7821c9":"code","1f0cc23f":"code","e5a4892a":"code","3bd50d0f":"code","ac55bc26":"code","df4ec2a1":"code","e8898412":"code","d9c586d1":"code","561ad0dd":"code","f72f04fd":"code","75b551c3":"code","f6bd0bf5":"code","54bb6a2c":"code","9d825601":"code","46a9e4b3":"code","019de1fb":"code","5df2f554":"code","3d0213e8":"code","900418f5":"code","165f2f8c":"code","5aace56c":"code","9fce0a22":"code","3dfb86db":"code","6b9098df":"code","2ffb4da2":"code","e58a66f6":"code","cfe3fa2d":"code","82d5e0f3":"code","6a748fe8":"code","88169b26":"code","1f7ccef8":"code","3f0b7c0d":"code","b35095a0":"code","57c40f10":"code","bbb57e61":"code","aae26803":"code","c3cb2e6c":"code","06f21e4b":"code","fc8a2e21":"markdown","2b9fba78":"markdown","26fc7c05":"markdown","e2244658":"markdown","5444ea90":"markdown","2a8c8fbf":"markdown"},"source":{"a3b37e9d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n\nimport missingno as msno\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore') \n\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","b5d1cf4b":"df_train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\n\n# separate dataset","fc447fef":"df_train.head(10)","37793414":"df_train.describe()\n# statistical figures representation of data of train data","a8ca4306":"df_train.columns\n# attribute of each column","0c235d1c":"for col in df_train.columns:\n    msg = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_train[col].isnull().sum() \/ df_train[col].shape[0]))\n    print(msg)\n    \n    # The process of identifying the missing value of each column","84da8836":"msno.matrix(df=df_train.iloc[:, :], figsize=(8,8), color=(0.1, 0.6, 0.8))\n\n# msno.matrix creates the same matrix as shown below \n# empty space NULL data ","5bddbecb":"msno.bar(df=df_train.iloc[:, :], figsize=(8,8), color=(0.1, 0.6, 0.8))\n\n# It makes with the graph of the bar type.","1b327ba3":"f, ax = plt.subplots(1,2, figsize = (18,8))\n\ndf_train['Survived'].value_counts().plot.pie(explode = [0, 0.1], autopct = '%1.1f%%', ax=ax[0], shadow = True)\n# It draws a series-type pieplot. \nax[0].set_title('Pie plot - Survived')\n# Set the title for the first plot\nax[0].set_ylabel('')\n# Set the ylabel for the first plot\nsns.countplot('Survived', data = df_train, ax=ax[1])\n#  Draw a count plot.\nax[1].set_title('Count plot - Survived')\n# Set the title for the count plot\nplt.show()\n\n# The ratio of survival 0 to 1 of the train set is shown graphically.","a2c69d4d":"df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).count()\n\n# Tie with groupby and count how many counts.","e50ab91b":"pd.crosstab(df_train['Pclass'], df_train['Survived'], margins = True).style.background_gradient(cmap='Pastel1')\n# margin show total","12ff4ab4":"df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).mean().sort_values(by='Survived', ascending = False).plot.bar()\n\n# It means Survival rate","2290a53c":"y_position = 1.02\nf, ax = plt.subplots(1, 2, figsize= (18,8))\ndf_train[\"Pclass\"].value_counts().plot.bar(color = [\"#CD7F32\", \"#FFDF00\", \"#D3D3D3\"], ax = ax[0])\nax[0].set_title(\"Number of passengers By Pclass\")\nax[0].set_ylabel(\"Count\")\nsns.countplot(\"Pclass\", hue = \"Survived\", data = df_train, ax = ax[1])\nax[1].set_title(\"Pclass: Survived vs Dead\", y = y_position)\nplt.show()\n     \n# The number of passengers and the survival rate according to the Passenger Class can be known.3 class (I think it is economy) was the most on board, and FirstClass passengers had the highest survival rate.","7afaf6b0":"print(\"the oldest passenger : {:.1f} years\".format(df_train[\"Age\"].max()))\nprint(\"the youngest passenger : {:.1f} years\".format(df_train[\"Age\"].min()))\nprint(\"average of passengers age : {:.1f} years\".format(df_train[\"Age\"].mean()))","142ebc3d":"fix, ax = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[df_train[\"Survived\"] == 1][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Survived\"] == 0][\"Age\"], ax=ax)    \nplt.legend([\"Survived == 1\", \"Survived == 0\"])\nplt.show()\n\n# kdeplot is used to estimate the distribution of data.","80410e79":"fix, ax = plt.subplots(1, 1, figsize = (9, 7))\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 1][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 2][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 3][\"Age\"], ax=ax)\nplt.xlabel(\"Age\")\nplt.title(\"Age Distribution within classes\")\nplt.legend([\"1st Class\", \"2nd Class\", \"3rd Class\"])\nplt.show()                       \n\n# In this situation, if you use histogram, you can use kde because you can not overlap\n# The Age distribution according to the Class can be known.","f519edf3":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 1)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 1)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"1st Class\")\nplt.show()\n\n# Age distribution of non-survival people with first class\n# Age distribution of survival people with first class","19e251cf":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 2)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 2)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"2nd Class\")\nplt.show()\n\n# Age distribution of non-survival people with second class\n# Age distribution of survival people with second class","0c20e185":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 3)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 3)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"3rd Class\")\nplt.show()\n\n# Age distribution of non-survival people with third class\n# Age distribution of survival people with third class","b012e335":"chage_age_range_survival_ratio = []\ni = 80\nfor i in range(1,81):\n    chage_age_range_survival_ratio.append(df_train[df_train[\"Age\"] < i][\"Survived\"].sum()\/len(df_train[df_train[\"Age\"] < i][\"Survived\"])) # i\ubcf4\ub2e4 \uc791\uc740 \ub098\uc774\uc758 \uc0ac\ub78c\ub4e4\uc774 \uc0dd\uc874\ub960\n\nplt.figure(figsize = (7, 7))\nplt.plot(chage_age_range_survival_ratio)\nplt.title(\"Survival rate change depending on range of Age\", y = 1.02)\nplt.ylabel(\"Survival rate\")\nplt.xlabel(\"Range of Age(0-x)\")\nplt.show()\n    \n# The younger the age, the higher the probability of survival, The older the age, the less the probability of survival.","c770bbdd":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\nsns.violinplot(\"Pclass\",\"Age\", hue = \"Survived\", data = df_train, scale = \"count\", split = True, ax=ax[0])\nax[0].set_title(\"Pclass and Age vs Survived\")\nax[0].set_yticks(range(0, 110, 10))\n\nsns.violinplot(\"Sex\", \"Age\", hue = \"Survived\", data = df_train, scale = \"count\", split = True, ax=ax[1])\nax[1].set_title(\"Sex and Age vs Survived\")\nax[1].set_yticks(range(0, 110, 10))\n\nplt.show()\n\n# Based on age, the survival rate according to Pclass and the survival rate according to gender can be seen at a glance.\n# As a result, the better the Pclass, the higher the survival rate and the higher the survival rate of women than men.","cd4b2348":"f, ax = plt.subplots(1, 1, figsize=(7,7))\ndf_train[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"], as_index=True).mean().sort_values(by=\"Survived\",\n                                                                                         ascending = False).plot.bar(ax=ax)","0d7821c9":"f, ax = plt.subplots(2, 2, figsize=(20,15))\nsns.countplot(\"Embarked\", data = df_train, ax=ax[0,0])\nax[0,0].set_title(\"(1) No. of Passengers Boared\")\n\nsns.countplot(\"Embarked\", hue = \"Sex\", data = df_train, ax=ax[0,1])\nax[0,1].set_title(\"(2) Male-Female split for Embarked\")\n\nsns.countplot(\"Embarked\", hue = \"Survived\", data = df_train, ax=ax[1,0])\nax[1,0].set_title(\"(3) Embarked vs Survived\")\n\nsns.countplot(\"Embarked\", hue = \"Pclass\", data = df_train, ax=ax[1,1])\nax[1,1].set_title(\"(4) Embarked vs Pclass\")\n\nplt.subplots_adjust(wspace = 0.4, hspace = 0.5) \nplt.show()\n\n# As a result, the survival rate is high because the people on board C have a lot of first class and many women.","1f0cc23f":"df_train[\"FamilySize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"]+1\ndf_test[\"FamilySize\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]+1\n\n# Create a new feature, \"FamilySize\".","e5a4892a":"df_train[\"FamilySize\"].head(5)","3bd50d0f":"print(\"Maximum size of Family: \", df_train[\"FamilySize\"].max())\nprint(\"Minimum size of Family: \", df_train[\"FamilySize\"].min())","ac55bc26":"f, ax = plt.subplots(1, 3, figsize = (40, 10))\nsns.countplot(\"FamilySize\", data = df_train, ax = ax[0])\nax[0].set_title(\"(1) No. of Passenger Boarded\", y = 1.02)\n\nsns.countplot(\"FamilySize\", hue = \"Survived\", data = df_train, ax = ax[1])\nax[1].set_title(\"(2) Survived countplot depending of FamilySize\")\n\ndf_train[[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"], as_index = True).mean().sort_values(by = \"Survived\",\n                                                                                                      ascending = False).plot.bar(ax = ax[2])\nax[2].set_title(\"(3) Survived rate depending on FamilySize\", y = 1.02)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.5)\nplt.show()\n\n# The first plot is the number of passengers according to the number of family members (1 to 11), the second plot is the number of survivors according to the number of family members, and the third plot is the survival rate according to the number of family members.\n# The family with four families has the highest survival rate.","df4ec2a1":"f, ax = plt.subplots(1, 1, figsize = (8,8))\ng = sns.distplot(df_train[\"Fare\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"Fare\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\n# The skewness tells us how asymmetric the distribution is.","e8898412":"# Log to get rid of the skewness.\n\ndf_train[\"Fare\"] = df_train[\"Fare\"].map(lambda i:np.log(i) if i>0 else 0)","d9c586d1":"df_train[\"Fare\"].head()","561ad0dd":"f, ax = plt.subplots(1, 1, figsize = (8,8))\ng = sns.distplot(df_train[\"Fare\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"Fare\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\n# normal approximation","f72f04fd":"# First, fill NULL data.\n\ndf_train[\"Age\"].isnull().sum()","75b551c3":"# train, test Two data sets are combined and statistics is confirmed.\n# (Use concat: A function that builds a dataset on a dataset)\ndf_all = pd.concat([df_train,df_test])\ndf_all.shape","f6bd0bf5":"df_train.shape\n\n# Of the 891 rows, only two have no missing value, so replace it with the most frequent value.","54bb6a2c":"df_train[\"Embarked\"].fillna(\"S\", inplace = True)\n\n# fillna fills the missing value value with the designated value.\n# In the EDA process, S is the most common, so it replaces.","9d825601":"df_train[\"Age_Categ\"] = 0\ndf_test[\"Age_Categ\"] = 0\n\n# create a new feature","46a9e4b3":"def category_age(x):\n    if x < 10:\n        return 0\n    elif x < 20:\n        return 1\n    elif x < 30:\n        return 2\n    elif x < 40:\n        return 3 \n    elif x < 50:\n        return 4\n    elif x < 60: \n        return 5\n    elif x < 70: \n        return 6\n    else:\n        return 7\n    \n# Make a function for apply use. ","019de1fb":"df_train[\"Age_Categ\"] = df_train[\"Age\"].apply(category_age)\ndf_test[\"Age_Categ\"] = df_test[\"Age\"].apply(category_age)\n\n# By using the apply function, the information of the age column categorized is added to train and test set.","5df2f554":"# Since categorizing Age, the unnecessary Age column is deleted.\n\ndf_train.drop([\"Age\"], axis = 1 ,inplace = True)\ndf_test.drop([\"Age\"], axis = 1, inplace = True)","3d0213e8":"df_train[\"Embarked\"].value_counts()","900418f5":"df_train[\"Embarked\"] = df_train[\"Embarked\"].map({\"C\" : 0, \"Q\" : 1, \"S\" : 2})\ndf_test[\"Embarked\"] = df_test[\"Embarked\"].map({\"C\" : 0, \"Q\" : 1, \"S\" : 2})","165f2f8c":"df_train[\"Sex\"].unique()","5aace56c":"df_train[\"Sex\"] = df_train[\"Sex\"].map({\"female\" : 0, \"male\" : 1})\ndf_test[\"Sex\"] = df_test[\"Sex\"].map({\"female\" : 0, \"male\" : 1})","9fce0a22":"heatmap_data = df_train[[\"Survived\", \"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"FamilySize\", \"Age_Categ\"]]","3dfb86db":"colormap = plt.cm.PuBu\nplt.figure(figsize=(10, 8))\nplt.title(\"Person Correlation of Features\", y = 1.05, size = 15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths = 0.1, vmax = 1.0,\n           square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 16})\n\n\n# Correlation coefficient analysis shows whether there are overlapping features and which features show correlation.","6b9098df":"df_train = pd.get_dummies(df_train, columns = [\"Embarked\"], prefix = \"Embarked\")\ndf_test = pd.get_dummies(df_test, columns = [\"Embarked\"], prefix = \"Embarked\")","2ffb4da2":"df_train.drop([\"PassengerId\", \"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)\ndf_test.drop([\"PassengerId\", \"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)","e58a66f6":"kfold = StratifiedKFold(n_splits=10)","cfe3fa2d":"df_train[\"Survived\"] = df_train[\"Survived\"].astype(int)\n\nY_train = df_train[\"Survived\"]\n\nX_train = df_train.drop(labels = [\"Survived\"],axis = 1)","82d5e0f3":"# ExtraTrees\n\nExtC = ExtraTreesClassifier()\n\n# Search Grid for Optimal Parameters\n\nex_param_grid = {\"max_depth\": [None],\n                \"max_features\": [1,2,10],\n                \"min_samples_split\": [2, 3, 10],\n                \"min_samples_leaf\": [1,3,10],\n                \"bootstrap\": [False],\n                \"n_estimators\": [100, 300],\n                \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC, param_grid = ex_param_grid, cv = kfold, scoring = \"accuracy\",\n                     n_jobs = 4, verbose = 1)\n\ngsExtC.fit(X_train, Y_train)\nExtC_best = gsExtC.best_estimator_\n\ngsExtC.best_score_","6a748fe8":"# Grid Search Optimization for Five Models\n    \n# LightGBM\nLGBM = LGBMClassifier()\n\nlgbm_param_grid = {\"max_depth\" : [40, 50, 60],\n                 \"min_child_samples\": [10, 20],\n                  \"num_leaves\" : [20, 30],\n                 \"n_estimators\": [500,1000],\n                 \"learning_rate\": [0.01, 0.1, 0.2, 0.3]}\n\ngsLGBMC = GridSearchCV(LGBM,param_grid = lgbm_param_grid, cv = kfold, scoring = \"accuracy\",\n                       n_jobs = 4, verbose = 1)\ngsLGBMC.fit(X_train, Y_train)\nlgbm_best = gsLGBMC.best_estimator_\n\ngsLGBMC.best_score_","88169b26":"# XGBoost\n\nXGB = XGBClassifier()\nxgbc_param_grid={\n            'silent':[True],\n            'max_depth':[6,8],\n            'min_child_weight':[3,5],\n            'gamma':[0,1,2],\n            'n_estimators':[100, 300]}\n\ngsXGBC = GridSearchCV(XGB,param_grid = xgbc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsXGBC.fit(X_train,Y_train)\n\nXGBC_best = gsXGBC.best_estimator_\n\n# Best score\ngsXGBC.best_score_","1f7ccef8":"# RandomForestClassifier\nRFC = RandomForestClassifier()\n\n# Search Grid for Optimal Parameters\nrf_param_grid = {\"max_depth\": [None],\n                \"max_features\": [3,10],\n                \"min_samples_split\": [3,10],\n                \"min_samples_leaf\": [2,10],\n                \"n_estimators\": [100,300],\n                \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC, param_grid = rf_param_grid, cv=kfold, scoring = \"accuracy\", n_jobs = 4,\n                    verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\nRFC_best = gsRFC.best_estimator_\n\ngsRFC.best_score_","3f0b7c0d":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsLGBMC.best_estimator_,\"LGBM mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsXGBC.best_estimator_,\"XGB mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)","b35095a0":"votingC = VotingClassifier(estimators = [(\"rfc\", RFC_best), (\"extc\", ExtC_best),\n                                        (\"lgbm\", lgbm_best), (\"xgb\", XGBC_best)], voting = \"soft\", n_jobs = 4)\n\nvotingC = votingC.fit(X_train, Y_train)","57c40f10":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")","bbb57e61":"df_test[\"Fare\"].fillna(\"35.6271\", inplace = True)\nX_test = df_test.values","aae26803":"prediction = votingC.predict(X_test)","c3cb2e6c":"submission[\"Survived\"] = prediction","06f21e4b":"submission.to_csv(\".\/The_first_submission.csv\", index = False)","fc8a2e21":"# EDA","2b9fba78":"# Submission","26fc7c05":"# Ensemble","e2244658":"# Data Check","5444ea90":"# Feature Engineering","2a8c8fbf":"![](http:\/\/imgbntnews.hankyung.com\/bntdata\/images\/photo\/201802\/46e751857d79621b9b2c0422b13c57d1.jpg)\n\n***It's a kernel for beginners who are first introduced.***\n\n***It's based on the existing kernels and tried to explain them as easily as possible.***\n\n***This kernel can achieve the top 28 percent and I hope it will help beginners a lot.***"}}