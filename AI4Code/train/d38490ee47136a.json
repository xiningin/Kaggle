{"cell_type":{"457519e5":"code","a6b65476":"code","3fb32225":"code","2e56ed6a":"code","7ed51b26":"code","fa3a63bc":"code","43b57899":"code","4f8cc191":"code","60aedb89":"code","a02bc249":"code","e77b0ee4":"code","9af42b35":"markdown","a3dc4423":"markdown","808f47c7":"markdown","33cbc799":"markdown","14f6e750":"markdown","dff68a81":"markdown","fecb987e":"markdown","5396a668":"markdown","eb2bfd5c":"markdown"},"source":{"457519e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6b65476":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","3fb32225":"import matplotlib.pyplot as plt # plotting\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error","2e56ed6a":"df.head()","7ed51b26":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\ntarget = 'CPM'","fa3a63bc":"# check feature correlation\ncorr = df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True)\nplt.show()","43b57899":"# drop total revenue and every feature correlated with it\n# also drop constant features, date, target\n# save these features to list to drop the after preprocessing\ncolumns_to_drop = ['total_revenue', 'total_impressions', 'viewable_impressions', 'measurable_impressions', \n                   'revenue_share_percent', 'integration_type_id', 'date', target]","4f8cc191":"# use border date as split time for train and test\n# also filter outliers (values > 0 and 95 quantile)\nborder = '2019-06-21'\ndate = pd.to_datetime(df['date'])\ndf_train = df[(date <= border) & (df['CPM'] >= 0) & (df['CPM'] < df['CPM'].quantile(0.95))]\ndf_test = df[(date > border) & (df['CPM'] >= 0) & (df['CPM'] < df['CPM'].quantile(0.95))]\n\ny = df_train[target]\nX = df_train.drop(columns=columns_to_drop)\ny_test = df_test[target]\nX_test = df_test.drop(columns=columns_to_drop)\n\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)","60aedb89":"# from sklearn.model_selection import GridSearchCV\n\n# # gridsearch with cross validation for lgbm\n# lgb = LGBMRegressor(random_state=7)\n# gs = GridSearchCV(estimator=lgb,\n#                   param_grid={\n#                       'max_depth': [6, 10],\n#                       'n_estimators': [100, 200, 500, 1000]\n#                   },\n#                   scoring='neg_mean_squared_error',\n#                   cv=3)\n# %time gs.fit(X_train, y_train)\n# print('best parameters:', gs.best_params_)\n# print('best score:', gs.best_score_)\n\n# # train with best params\n# model = LGBMRegressor(random_state=7, **gs.best_params_)\n# %time model.fit(X, y);","a02bc249":"model = LGBMRegressor(random_state=7, n_estimators=1000, max_depth=10)\n%time model.fit(X, y);\ntrain_score = mean_squared_error(y, model.predict(X))\nprint('train score: ', train_score)","e77b0ee4":"test_score = mean_squared_error(y_test, model.predict(X_test))\nprint('test score: ', test_score)","9af42b35":"#  Objective and Theory\u00b6","a3dc4423":"Filter data + train-test split","808f47c7":"Removing the transaction id related data","33cbc799":"Step 1: Simply fit booster with given parameters","14f6e750":"Step 0: Grid search for best parameters","dff68a81":"# Data descriptive analysis and Preprocessing\u00b6","fecb987e":"## Train and test model","5396a668":"The main idea in this note book will be to first calculate the CPM which is the calculated based on the revenue generaed by the publisher(which is 1% of the actual revenue) thus calculating the CPM.\n\nOnce the CPM is calculated keeping that as a target variable we will try to create a model which will try to predict the reverse price ( We will take CPM roughly as the reverse price because any advertisers will try to reduce the cost and increase the value which they generate from Inventory and without the reverse price it will be most often [Rationally] lower than the actual Intrinsic value to the advertisers.)\n\nNow once we have the reverse price algorithm we know that any advertisers will be willing to go around 20% above the value in order to secure the slot if he sees value in the slot.","eb2bfd5c":"Step 2: Observe score on test"}}