{"cell_type":{"ed73ec3c":"code","19beb491":"code","e12c8f18":"code","46b93c50":"code","1259a51b":"code","7b54f09f":"code","01e4744a":"code","4bd19d88":"code","57af7112":"code","bba276ae":"code","0a3dc6c3":"code","93cec5fd":"code","b474b073":"code","14528ee2":"code","aa38e474":"code","37662067":"code","e681e2f1":"code","0a532812":"code","bb0ff4d9":"code","73009967":"code","165179b5":"code","4e71d2c8":"code","378e8f17":"code","5d96e075":"code","4c96bf65":"code","f6fb6d7c":"code","ba7848a9":"code","55494715":"markdown","819e8128":"markdown","1732a469":"markdown","c83e3ed7":"markdown","d0ef29ea":"markdown","951f51e9":"markdown","245732ce":"markdown","0792cbde":"markdown","61c5a7ab":"markdown","1ebfa889":"markdown","cd0070b0":"markdown","89221196":"markdown","d95f1dd3":"markdown","1368b745":"markdown","479c9139":"markdown","9b369a5f":"markdown","3917eeec":"markdown","1ee8113f":"markdown","144e3a25":"markdown","f9b46791":"markdown","28c5bba7":"markdown","91fd6c3f":"markdown","687ef8a7":"markdown"},"source":{"ed73ec3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n%matplotlib inline\n\nfrom math import sqrt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Analysis imports\nfrom pandas.plotting import lag_plot\nfrom pylab import rcParams\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom pandas import DataFrame\nfrom pandas import concat\n\n# Modelling imports\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom fbprophet import Prophet\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense\nfrom keras.layers import Dropout\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.layers.core import Activation\nimport tensorflow as tf\nfrom keras.initializers import glorot_uniform\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\n\nimport random\nfrom numpy.random import seed\n\nimport statsmodels.api as sm\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')","19beb491":"def show_graph(train, test=None, pred=None, title=None):\n    \n    fig = plt.figure(figsize=(20, 5))\n\n    # entire data\n    ax1 = fig.add_subplot(121)\n    ax1.set_xlabel('Dates')\n    ax1.set_ylabel('Price')\n    ax1.plot(train.index, train['Price'], color='green', label='Train price')\n    if test is not None:\n        ax1.plot(test.index, test['Price'], color='red', label='Test price')\n    if pred is not None:\n        if 'yhat' in pred.columns:\n            ax1.plot(pred.index, pred['yhat'], color = 'blue', label = 'Predicted price')\n            ax1.fill_between(pred.index, pred['yhat_lower'], pred['yhat_upper'], color='grey', label=\"Band Range\")\n        else:\n            ax1.plot(pred.index, pred['Price'], color='blue', label='Predicted price')\n    ax1.legend()\n    if title is not None:\n        plt.title(title + ' (Entire)')\n    plt.grid(True)\n\n    # zoom data\n    period=50\n    period=int(0.2*len(train))\n    ax2 = fig.add_subplot(122)\n    ax2.set_xlabel('Dates')\n    ax2.set_ylabel('Price')\n    ax2.plot(train.index[-period:], train['Price'].tail(period), color='green', label='Train price')\n    if test is not None:\n        ax2.plot(test.index, test['Price'], color='red', label='Test price')\n    if pred is not None:\n        if 'yhat' in pred.columns:\n            ax2.plot(pred.index, pred['yhat'], color = 'blue', label = 'Predicted price')\n            ax2.fill_between(pred.index, pred['yhat_lower'], pred['yhat_upper'], color='grey', label=\"Band Range\")\n        else:\n            ax2.plot(pred.index, pred['Price'], color='blue', label='Predicted price')\n    ax2.legend()\n    if title is not None:\n        plt.title(title + ' (Recent ' + str(period) + ')')\n    plt.grid(True)\n\n    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()\n    \ndef make_future_dates(last_date, period):\n    prediction_dates=pd.date_range(last_date, periods=period+1, freq='B')\n    return prediction_dates[1:]\n\ndef calculate_accuracy(forecast, actual, algorithm):\n    mse  = round(mean_squared_error(actual, forecast),4)\n    mae  = round(mean_absolute_error(actual, forecast),4)\n    rmse = round(sqrt(mean_squared_error(actual, forecast)),4)\n    return ({'algorithm':algorithm, 'mse':mse, 'mae':mae, 'rmse': rmse})","e12c8f18":"def get_data_from_EIA_local():\n    df = pd.read_csv(\"..\/input\/cushing-ok-wti-spot-price-fob\/Cushing_OK_WTI_Spot_Price_FOB_20200706.csv\", header=4, parse_dates=[0])\n    df.columns=[\"Date\", \"Price\"]\n    df.set_index('Date', inplace=True)\n    df.sort_index(inplace=True)\n    return df","46b93c50":"df_org=get_data_from_EIA_local()\ndata=df_org['2019-07-06':'2020-07-06'].copy()\ndata.Price[\"2020-04-20\"]=(data.Price[\"2020-04-17\"] + data.Price[\"2020-04-21\"]) \/ 2\n\nacc_sum=[]\ndf_preds=pd.DataFrame({\"Date\":make_future_dates('2020-07-06',34)})\ndf_preds=df_preds.set_index('Date', drop=True)\n\n# Display OIL price\nplt.figure(figsize=(10,5))\nplt.xlabel('Dates')\nplt.ylabel('Price')\nplt.plot(data['Price']);\nplt.grid(True)\nplt.show()","1259a51b":"# Show LAG\nfig = plt.figure(figsize=(10, 6))\nlag_plot(data['Price'], lag=5)\nplt.title('Lag')\nplt.grid(True)\nplt.legend();\n\n# Show Diff\ndata_diff = data - data.shift() \ndata_diff = data_diff.dropna()\nplt.figure(figsize=(10, 6))\nplt.title('Diff')\nplt.grid(True)\nplt.plot(data_diff);\n\nfig = plt.figure(figsize=(8, 6))\n\n# Show ACF\nax1 = fig.add_subplot(211)\nsm.graphics.tsa.plot_acf(data_diff, lags=40, ax=ax1)\n\n# Show PACF\nax2 = fig.add_subplot(212)\nsm.graphics.tsa.plot_pacf(data_diff, lags=40, ax=ax2)\n\nplt.tight_layout()","7b54f09f":"result = seasonal_decompose(data.Price[-1000:], model='additive', freq=30)\nplt.figure(figsize=(16,10))\nfig = result.plot()\nplt.show()","01e4744a":"values = DataFrame(data['Price'].values)\ndataframe = concat([values.shift(1),values.shift(5),values.shift(10),values.shift(30), values], axis=1)\ndataframe.columns = ['t', 't+1', 't+5', 't+10', 't+30']\nresult = dataframe.corr()\nprint(result)","4bd19d88":"adf_result = sm.tsa.stattools.adfuller(data['Price'].values, autolag ='AIC')\nadf = pd.Series(adf_result[0:4], index = ['Test Statistic', 'p-\u3000\u3000value', '#Lags Used', 'Number of Observations Used'])\nprint(adf)","57af7112":"split = int(0.80*len(data))\ntrain_data, test_data = data[0:split], data[split:]\nshow_graph(train_data,test_data,title='Train & Test')","bba276ae":"def evaluate_arima_model(train, test, order, maxlags=8, ic='aic'):\n    # feature Scaling\n    stdsc = StandardScaler()\n    train_std = stdsc.fit_transform(train.values.reshape(-1, 1))\n    test_std = stdsc.transform(test.values.reshape(-1, 1))\n    # prepare training dataset\n    history = [x for x in train_std]\n    # make predictions\n    predictions = list()\n    # rolling forecasts\n    for t in range(len(test_std)):\n        # predict\n        model = ARIMA(history, order=order)\n        model_fit = model.fit(maxlags=maxlags, ic=ic, disp=0)\n        yhat = model_fit.forecast()[0]\n        # invert transformed prediction\n        predictions.append(yhat)\n        # observation\n        history.append(test_std[t])\n    # inverse transform\n    predictions = stdsc.inverse_transform(np.array(predictions).reshape((-1)))\n    # calculate mse\n    mse = mean_squared_error(test, predictions)\n    return predictions, mse\n\ndef evaluate_arima_models(train, test, p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    pdq = list(itertools.product(p_values, d_values, q_values))\n    for order in pdq:\n        try:\n            predictions, mse = evaluate_arima_model(train, test, order)\n            if mse < best_score:\n                best_score, best_cfg = mse, order\n            print('Model(%s) mse=%.3f' % (order,mse))\n        except:\n            continue\n    print('Best Model(%s) mse=%.3f' % (best_cfg, best_score)) \n    return best_cfg\n\ndef predict_arima_model(train, period, order, maxlags=8, ic='aic'):\n    # Feature Scaling\n    stdsc = StandardScaler()\n    train_std = stdsc.fit_transform(train.values.reshape(-1, 1))\n    # fit model\n    model = ARIMA(train_std, order=order)\n    model_fit = model.fit(maxlags=maxlags, ic=ic, disp=0)\n    # make prediction\n    yhat = model_fit.predict(len(train), len(train) + period -1, typ='levels')\n    # inverse transform\n    yhat = stdsc.inverse_transform(np.array(yhat).flatten())\n    return yhat","0a3dc6c3":"model_name='AR Model'\n\n# evaluate parameters\np_values = range(1, 4)\nd_values = [0]\nq_values = [0]\n#evaluate_arima_models(train_data['Price'], test_data['Price'], p_values, d_values, q_values)\n\n# predict test period with best parameter\npredictions, mse = evaluate_arima_model(train_data['Price'], test_data['Price'],(1, 0, 0))\ndf_pred = pd.DataFrame({'Price':predictions},index=test_data.index)\n\n# calculate performance metrics\nacc = calculate_accuracy(predictions, test_data['Price'], model_name)\nprint(acc)\nacc_sum.append(acc)\n\n# show result\nshow_graph(train_data,test_data,df_pred,title=model_name+'\\nTest period prediction')\n\n# predict future period with best parameter\nforecast_out = 34\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\npredictions = predict_arima_model(data,len(future_dates),(1, 0, 0))\ndf_pred = pd.DataFrame({'Price':predictions},index=future_dates)\n\n# show result\nshow_graph(data,None,df_pred,title=model_name+'\\nFuture period prediction')\n\ndf_preds[model_name] = df_pred['Price']","93cec5fd":"model_name='MA Model'\n\n# evaluate parameters\np_values = [0]\nd_values = [0]\nq_values = range(1, 4)\n#evaluate_arima_models(train_data['Price'], test_data['Price'], p_values, d_values, q_values)\n\n# predict test period with best parameter\npredictions, mse = evaluate_arima_model(train_data['Price'], test_data['Price'],(0, 0, 1))\ndf_pred = pd.DataFrame({'Price':predictions},index=test_data.index)\n\n# calculate performance metrics\nacc = calculate_accuracy(predictions, test_data['Price'], model_name)\nprint(acc)\nacc_sum.append(acc)\n\n# show result\nshow_graph(train_data, test_data, df_pred, title=model_name + '\\nTest period prediction')\n\n# predict future period with best parameter\nforecast_out = 34\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\npredictions = predict_arima_model(data,len(future_dates),(0, 0, 1))\ndf_pred = pd.DataFrame({'Price':predictions},index=future_dates)\n\n# show result\nshow_graph(data,None,df_pred,title=model_name+'\\nFuture period prediction')\n\ndf_preds[model_name] = df_pred['Price']","b474b073":"model_name='ARMA Model'\n\n# evaluate parameters\np_values = range(0, 1, 2)\nd_values = [0]\nq_values = range(0, 1, 2)\n#evaluate_arima_models(train_data['Price'].tail, test_data['Price'], p_values, d_values, q_values)\n\n# predict test period with best parameter\n#predictions, mse = evaluate_arima_model(train_data['Price'], test_data['Price'],(1, 0, 1))\n#df_pred = pd.DataFrame({'Price':predictions},index=test_data.index)\n\n## calculate performance metrics\n#acc = calculate_accuracy(predictions, test_data['Price'], model_name)\n#print(acc)\n#acc_sum.append(acc)\n\n# show result\n#show_graph(train_data, test_data, df_pred, title=model_name + '\\nTest period prediction')\n\n# predict future period with best parameter\nforecast_out = 34\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\npredictions = predict_arima_model(data,len(future_dates),(2, 0, 1))\ndf_pred = pd.DataFrame({'Price':predictions},index=future_dates)\n\n# show result\nshow_graph(data,None,df_pred,title=model_name+'\\nFuture period prediction')\n\ndf_preds[model_name] = df_pred['Price']","14528ee2":"model_name='ARIMA Model'\n\n# evaluate parameters\np_values = [1, 2, 4, 6, 8, 10]\nd_values = range(0, 3)\nq_values = range(1, 3)\n#evaluate_arima_models(train_data['Price'], test_data['Price'], p_values, d_values, q_values)\n\n# predict test period with best parameter\npredictions, mse = evaluate_arima_model(train_data['Price'], test_data['Price'],(2, 1, 1))\ndf_pred = pd.DataFrame({'Price':predictions},index=test_data.index)\n\n# calculate performance metrics\nacc = calculate_accuracy(predictions, test_data['Price'],model_name)\nprint(acc)\nacc_sum.append(acc)\n\n# show result\nshow_graph(train_data, test_data, df_pred, title=model_name + '\\nTest period prediction')\n\n# predict future period with best parameter\nforecast_out = 34\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\npredictions = predict_arima_model(data,len(future_dates),(2, 1, 1))\ndf_pred = pd.DataFrame({'Price':predictions},index=future_dates)\n\n# show result\nshow_graph(data,None,df_pred,title=model_name+'\\nFuture period prediction')\n\ndf_preds[model_name] = df_pred['Price']","aa38e474":"model_name='Facebook Prophet'\n\ndef predict_prophet(train,period):\n    # create model\n    prop = Prophet(growth='logistic',\n                    n_changepoints=40,\n                    changepoint_range=1,\n                    changepoint_prior_scale=0.5,\n                    weekly_seasonality=False,\n                    yearly_seasonality=False\n                  )\n    # prepare training dataset\n    ph_df_train = pd.DataFrame({'y':train['Price'].values, 'ds':train.index})\n    ph_df_train['cap'] = 100\n    ph_df_train['floor'] = 0\n    prop.fit(ph_df_train)\n    # create future dates\n    future_prices = prop.make_future_dataframe(periods=period, freq = 'd')\n    future_prices['cap'] = 100\n    future_prices['floor'] = 0\n    # predict prices\n    forecast = prop.predict(future_prices)\n    predicted=forecast[-period:]\n    return predicted\n\n# predict test period\npredictions = predict_prophet(train_data,len(test_data))\npredictions.index = test_data.index\n\n# calculate performance metrics\nacc = calculate_accuracy(predictions['yhat'], test_data['Price'], model_name)\nprint(acc)\nacc_sum.append(acc)\n\n# show result\nshow_graph(train_data, test_data, predictions, title=model_name + '\\nTest period prediction')\n\n# predict future period\nforecast_out = 34\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\npredictions = predict_prophet(data,len(future_dates))\npredictions.index = future_dates\n\n# show result\nshow_graph(data,None,predictions,title=model_name+'\\nFuture period prediction')\n\ndf_preds[model_name] = predictions['yhat']","37662067":"def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\ndef create_lstm_data(train,test,look_back):\n    train_lstm = train\n    test_lstm = test\n    train_gen = TimeseriesGenerator(train_lstm, train_lstm, length=look_back, batch_size=20)     \n    test_gen = TimeseriesGenerator(test_lstm, test_lstm, length=look_back, batch_size=1)\n    return train_gen, test_gen\n\ndef create_lstm_model(neurons, activ_func=\"linear\",\n                dropout=0.10, loss=\"mean_squared_error\", optimizer=\"adam\"):\n    set_random_seed(20200715)\n    model = Sequential()\n    \n    model.add(LSTM(neurons,\n                   input_shape=(look_back,1),\n                   kernel_initializer=glorot_uniform(seed=20200715)\n                  ))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=1,\n                   kernel_initializer=glorot_uniform(seed=20200715)\n                  ))\n    model.add(Activation(activ_func))\n\n    model.compile(loss=loss, optimizer=optimizer)\n    return model\n\ndef predict_lstm_model(data, period, model):\n    prediction_list = data[-look_back:]\n    \n    for _ in range(period):\n        x = prediction_list[-look_back:]\n        x = x.reshape((1, look_back, 1))\n        out = model.predict(x)[0][0]\n        prediction_list = np.append(prediction_list, out)\n        \n    prediction_list = prediction_list[look_back-1:]\n    prediction_list = prediction_list[1:]\n    return prediction_list\n\ndef show_lstm_history(history):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss,  label='Training loss')\n    plt.title('validation loss')\n    plt.legend()\n    plt.show()","e681e2f1":"# feature Scaling\nstdsc = StandardScaler()\ntrain_lstm = stdsc.fit_transform(train_data.values.reshape(-1, 1))\ntest_lstm = stdsc.transform(test_data.values.reshape(-1, 1))\n\n# create data\nlook_back = 7\ntrain_gen, test_gen = create_lstm_data(train_lstm, test_lstm, look_back)\n\n# create model\nmodel = create_lstm_model(300)\nmodel.summary()\n\n# training\nhistory = model.fit_generator(train_gen, epochs=100, verbose=1, shuffle=False)\nshow_lstm_history(history)","0a532812":"model_name='LSTM'\n\n# predict test period\nprediction = model.predict_generator(test_gen)\n\n# inverse transform\nprediction = stdsc.inverse_transform(prediction.reshape((-1)))\ndf_pred = pd.DataFrame({'Price':prediction},index=test_data[:len(prediction)].index)\n\n# calculate performance metrics\nacc = calculate_accuracy(prediction, test_data[:len(prediction)], model_name)\nprint(acc)\nacc_sum.append(acc)\n\n# show result\nshow_graph(train_data,test_data,df_pred,title=model_name+'\\nTest period prediction')\n\n# predict future period\nforecast_out = 34\ntrain_lstm = stdsc.transform(data.values.reshape(-1, 1))\nprediction = predict_lstm_model(train_lstm, forecast_out, model)\n\n# inverse transform\nprediction = prediction.reshape((-1))\nprediction = stdsc.inverse_transform(np.array(prediction).flatten())\n\n# show result\nfuture_dates = make_future_dates(data.index[-1], forecast_out)\ndf_pred = pd.DataFrame({'Price':prediction},index=future_dates)\nshow_graph(data, None, df_pred, title=model_name + '\\nFuture period prediction')\n\ndf_preds[model_name] = df_pred['Price']","bb0ff4d9":"def prepare_data(data2, forecast_out, test_size):\n    label = np.roll(data2, -forecast_out).reshape((-1))\n    X = data2; \n    X_lately = X[-forecast_out:]\n    X = X[:-forecast_out] \n    y = label[:-forecast_out] \n    if test_size == 0:\n        X_train, X_test , Y_train, Y_test = X, np.empty(0), y, np.empty(0)\n    else:\n        X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size,shuffle=False) \n    return [X_train, X_test , Y_train, Y_test , X_lately];","73009967":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\nfrom sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, RidgeCV\nfrom sklearn.linear_model import TheilSenRegressor, RANSACRegressor, HuberRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_decomposition import PLSRegression\n\nreg_dict = {\"LinearRegression\": LinearRegression(),\n            #\"Ridge\": Ridge(),\n            \"Lasso\": Lasso(),\n            \"ElasticNet\": ElasticNet(), \n            #\"Polynomial_deg2\": Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression())]),\n            #\"Polynomial_deg3\": Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression())]),\n            #\"Polynomial_deg4\": Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', LinearRegression())]),\n            #\"Polynomial_deg5\": Pipeline([('poly', PolynomialFeatures(degree=5)),('linear', LinearRegression())]),\n            #\"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=3),\n            #\"DecisionTreeRegressor\": DecisionTreeRegressor(),\n            #\"RandomForestRegressor\": RandomForestRegressor(),\n            #\"SVR_rbf\": SVR(kernel='rbf', C=1e3, gamma=0.1, epsilon=0.1, degree=3),\n            \"SVR_linear\": SVR(kernel='linear', C=1e3, gamma=0.1, epsilon=0.1, degree=3),\n            #\"GaussianProcessRegressor\": GaussianProcessRegressor(),\n            #\"SGDRegressor\": SGDRegressor(),\n            #\"MLPRegressor\": MLPRegressor(hidden_layer_sizes=(10,10), max_iter=100, early_stopping=True, n_iter_no_change=5),\n            #\"ExtraTreesRegressor\": ExtraTreesRegressor(n_estimators=100), \n            ##\"PLSRegression\": PLSRegression(n_components=34),\n            #\"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(max_iter=100, tol=1e-3),\n            \"TheilSenRegressor\": TheilSenRegressor(random_state=0),\n            \"RANSACRegressor\": RANSACRegressor(random_state=0),\n            #\"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(),\n            #\"AdaBoostRegressor\": AdaBoostRegressor(random_state=0, n_estimators=100),\n            #\"BaggingRegressor\": BaggingRegressor(base_estimator=SVR(), n_estimators=10),\n            #\"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n            #\"VotingRegressor\": VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor(n_estimators=10))]),\n            #\"StackingRegressor\": StackingRegressor(estimators=[('lr', RidgeCV()), ('svr', LinearSVR())], final_estimator=RandomForestRegressor(n_estimators=10)),\n            #\"ARDRegression\": ARDRegression(),\n            \"HuberRegressor\": HuberRegressor(),\n            }","165179b5":"for reg_name, reg in reg_dict.items():\n\n    # prepare data\n    forecast_out = 34\n    x_train, x_test, y_train, y_test, X_lately = prepare_data(data,forecast_out,0.2)\n\n    # feature Scaling\n    stdsc = StandardScaler()\n    x_train_std = stdsc.fit_transform(x_train)\n    y_train_std = stdsc.transform(y_train.reshape(-1, 1))\n    x_test_std = stdsc.transform(x_test)\n    y_test_std = stdsc.transform(y_test.reshape(-1, 1))\n\n    # create and train model\n    reg.fit(x_train_std, y_train_std)\n\n    # Predict test period\n    prediction = reg.predict(x_test_std)\n\n    # inverse transform\n    prediction = stdsc.inverse_transform(prediction.reshape((-1)))\n\n    # calculate performance metrics\n    acc = calculate_accuracy(prediction, y_test, reg_name)\n    print(acc)\n    acc_sum.append(acc)\n\n    # show result\n    future_dates1 = data.index[forecast_out:forecast_out+len(x_train)]\n    future_dates2 = data.index[forecast_out+len(x_train):]\n    df_train = pd.DataFrame({'Price':y_train},index=future_dates1)\n    df_test = pd.DataFrame({'Price':y_test},index=future_dates2)\n    df_pred = pd.DataFrame({'Price':prediction},index=future_dates2)\n    show_graph(df_train,df_test,df_pred,title=reg_name+'\\nTest period prediction')\n\n\n    # prepare data\n    forecast_out = 34\n    x_train, x_test, y_train, y_test, X_lately = prepare_data(data,forecast_out,0.0)\n\n    # feature Scaling\n    stdsc = StandardScaler()\n    x_train_std = stdsc.fit_transform(x_train)\n    y_train_std = stdsc.transform(y_train.reshape(-1, 1))\n    X_lately_std = stdsc.transform(X_lately)\n\n    # create and train model\n    reg.fit(x_train_std, y_train_std)\n\n    # Predict future period\n    prediction = reg.predict(X_lately_std)\n\n    # inverse transform\n    prediction = stdsc.inverse_transform(prediction.reshape((-1)))\n\n    # show result\n    future_dates = make_future_dates(data.index[-1], forecast_out)\n    df_pred = pd.DataFrame({'Price':prediction},index=future_dates)\n    show_graph(data,None,df_pred,title=reg_name+'\\nFuture period prediction')\n\n    df_preds[reg_name] = df_pred['Price']","4e71d2c8":"df_sum = pd.DataFrame(acc_sum)\ndf_sum = df_sum.sort_values('mae', ascending=True)\ndf_sum = df_sum.reset_index(drop=True)\ndf_sum","378e8f17":"df_preds","5d96e075":"# display\nplt.figure(figsize=(16, 8))\nplt.plot(data.index[-100:], data['Price'].tail(100),label=\"Train\")\nfor col in df_preds.columns:\n    plt.plot(df_preds.index[-len(df_preds):], df_preds[col][-len(df_preds):],label=col)\n    \nplt.vlines([data.index[-1]], 0, 60, \"red\", linestyles='dashed')\nplt.text([data.index[-1]], 60, 'Today', backgroundcolor='white', ha='center', va='center')\nplt.vlines([data.index[-1-75]], 0, 60, \"red\", linestyles='dashed')\nplt.text([data.index[-1-75]], 60, '75 days before', backgroundcolor='white', ha='center', va='center')\nplt.vlines([df_preds.index[-1]], 0, 60, \"red\", linestyles='dashed')\nplt.text([df_preds.index[-1]], 60, '34 days after', backgroundcolor='white', ha='center', va='center')\n\nplt.title('Predictions (Raw Price)')\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.legend(loc='lower right',ncol=1)\nplt.grid(True)\nplt.show()","4c96bf65":"# ma75 calculation \ndf_ma75 = df_org.copy() \nfor col in df_preds.columns:\n    df_ma75[col] = df_org['Price'].copy() \ndf_ma75 = pd.concat([df_ma75, df_preds])\ndf_ma75 = df_ma75.rolling(75).mean()\ndf_ma75[-len(df_preds):]","f6fb6d7c":"def disp_all(mode='entire'):\n    plt.figure(figsize=(16, 8))\n    for col in df_ma75.columns:\n        if col != \"Price\":\n            plt.plot(df_ma75.index[-len(df_preds):], df_ma75[col][-len(df_preds):],label=col)\n\n    if mode is 'entire':\n        plt.plot(df_ma75.index[-100:], df_ma75['Price'].tail(100),label=\"Train-MA75\")\n        plt.plot(data.index[-100:], data['Price'].tail(100),label=\"Train-Raw\")\n        plt.vlines([data.index[-1]], 0, 60, \"red\", linestyles='dashed')\n        plt.text([data.index[-1]], 60, 'Today', backgroundcolor='white', ha='center', va='center')\n        plt.vlines([data.index[-1-75]], 0, 60, \"red\", linestyles='dashed')\n        plt.text([data.index[-1-75]], 60, '75 days before', backgroundcolor='white', ha='center', va='center')\n        plt.vlines([df_ma75.index[-1]], 0, 60, \"red\", linestyles='dashed')\n        plt.text([df_ma75.index[-1]], 60, '34 days after', backgroundcolor='white', ha='center', va='center')\n\n    plt.title('Predictions (MA75 Price '+mode+')')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend(loc='best',ncol=2)\n    plt.grid(True)\n    plt.show()\n    \ndisp_all('entire')\ndisp_all('zoom')","ba7848a9":"template = pd.read_csv('..\/input\/ntt-data-global-ai-challenge-06-2020\/sampleSubmission0710_updated.csv', header=0, parse_dates=[0])\ntemplate.drop(\"Price\",axis=1,inplace=True)\n\nfor col in df2.columns:\n    submission = pd.merge(template, df2[col], on='Date', how='left')\n    submission.rename(columns={col: 'Price'},inplace=True)\n    if submission[\"Price\"].isnull().any():\n        #print(\"[Warning] NaN found in \" + col +  \" (\"+str(submission[\"Price\"].isnull().sum()) + \")\")\n        submission[\"Price\"].fillna(submission[\"Price\"].mean(),inplace=True)\n    submission[\"Price\"] = submission[\"Price\"].round(9)\n    submission.to_csv(\"submission_\" + col + \".csv\", index=False)","55494715":"### ADF test","819e8128":"# AR\nARIMA(p) is AR model.","1732a469":"# Creating submission file","c83e3ed7":"# Evaluate performance metrics","d0ef29ea":"# ARIMA\nARIMA(p, d, q) is ARIMA model.","951f51e9":"# Visualize prediction results of each model","245732ce":"# Data Analysis","0792cbde":"# Importing the libraries.","61c5a7ab":"# MA\nARIMA(q) is MA model.","1ebfa889":"# LSTM","cd0070b0":"# Facebook Prophet","89221196":"# Train-Test Split","d95f1dd3":"# AR,MA,ARMA,ARIMA,Prophet,LSTM + MultiReg Models with WTI dataset\nI will try to predict with using various time series forecasting models.","1368b745":"Thank you for your reading!\nGood luck!","479c9139":"### Seasonal decompose","9b369a5f":"### Autocorrelation plot","3917eeec":"## Raw price","1ee8113f":"# Common function","144e3a25":"# ARMA\nARIMA(p, q) is ARMA model.","f9b46791":"## MA75 Price\nsee also:<br>\nhttps:\/\/www.kaggle.com\/sajikim\/moving-average-example-in-python","28c5bba7":"# Regression models","91fd6c3f":"# Common function for AR,MA,ARMA,ARIMA","687ef8a7":"# Loading WTI oil dataset"}}