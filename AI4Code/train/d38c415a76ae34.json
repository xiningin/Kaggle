{"cell_type":{"30a14a3d":"code","1c210e97":"code","4a9f0aaf":"code","886e2a7a":"code","7a70f9e1":"code","3f5468c8":"code","71c4c39e":"code","4f0f9abd":"code","460d5f42":"code","2db00ddc":"code","aa7b49a0":"markdown","6b35bcfa":"markdown","b754ecb0":"markdown","e240e373":"markdown","a604ce6e":"markdown","639e0e5d":"markdown","8c8d5c06":"markdown","831baeb4":"markdown","6a2d85d5":"markdown","f32131fd":"markdown","cf728304":"markdown","6779a45c":"markdown","23b749a9":"markdown","8c351ae5":"markdown","37affa88":"markdown","48160039":"markdown","7e5dcf5d":"markdown","16ea1bc3":"markdown","36bd32b4":"markdown","69c268ce":"markdown"},"source":{"30a14a3d":"import numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import make_blobs\nimport random","1c210e97":"# create blobs\ndata = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=.9, )\n\n# create scatter plot\nplt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')","4a9f0aaf":"import scipy.spatial.distance as metric\n\ndef euclidean_dist(A, B):\n    return metric.euclidean(A, B)\n\ndef plot(data,k,index,centroids,orig_centroids):\n    input = []\n    for i in range(len(index)):\n        for j in index[i]:\n            input.append(int(j))\n            \n    colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n    j=0\n    for i in input:\n        plt.scatter(data[0][j,0], data[0][j,1], marker=\"x\", color=colors[i], s=150, linewidths=5)\n        j+=1\n    ## New centroids\n    for centroid in range(len(centroids)):\n        plt.scatter(centroids[centroid][0],centroids[centroid][1],marker=\"o\", color=\"k\", s=150, linewidths=5)\n    # Original Clusters\n    for centroid in range(len(orig_centroids)):\n        plt.scatter(orig_centroids[centroid][0],orig_centroids[centroid][1],marker=\"D\", color=\"DarkBlue\", s=150, linewidths=5)\n","886e2a7a":"class K_Means:\n    def __init__(self,k,data,centeriod_init=None):\n        self.k = k\n        self.data = data  \n        self.centeriod_init = centeriod_init\n        \n    def initialise_centroids(self,centeriod_init,k,data):\n        ## 3 ways to initialize centroides\n        if(self.centeriod_init == 'random'): \n            initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n            self.centroids = data[initial_centroids]\n        elif(self.centeriod_init == 'firstk'):\n            self.centroids = data[:k]\n        else:\n            for i in range(self.k):\n                self.centroids.append(i%self.k)\n        return self.centroids    \n \n    def fit(self,data):\n        m = np.shape(data)[0]\n        cluster_assignments = np.mat(np.zeros((m,2)))\n        \n        cents = self.initialise_centroids(self.centeriod_init,self.k,data)\n        \n        # Preserve original centroids\n        cents_orig = cents.copy()\n        changed = True\n        num_iter = 0\n        \n        while changed and num_iter<100:\n            changed = False \n            # for each row in the dataset\n            for i in range(m):\n                # Track minimum distance and vector index of associated cluster\n                min_dist = np.inf\n                min_index = -1 \n                #calculate distance \n                for j in range(self.k):\n                    dist_ji = euclidean_dist(cents[j,:],data[i,:])\n                    if(dist_ji < min_dist):\n                        min_dist = dist_ji\n                        min_index = j \n                    # Check if cluster assignment of instance has changed\n                    if cluster_assignments[i, 0] != min_index: \n                        changed = True\n\n                # Assign instance to appropriate cluster\n                cluster_assignments[i, :] = min_index, min_dist**2\n\n            # Update centroid location\n            for cent in range(self.k):\n                points = data[np.nonzero(cluster_assignments[:,0].A==cent)[0]]\n                cents[cent,:] = np.mean(points, axis=0)\n    \n            # Count iterations\n            num_iter += 1\n            #print(num_iter)\n\n         # Return important stuff when done\n        return cents, cluster_assignments, num_iter, cents_orig","7a70f9e1":"# Perform k-means clustering with centroids initialize='rondom'\nkmeans = K_Means(k=3,data = data[0],centeriod_init='random')\ncentroids, cluster_assignments, iters, orig_centroids = kmeans.fit(data[0])\nindex = cluster_assignments[:,0] ## This has the cluster assignment 0,1,.... \ndistance = cluster_assignments[:,1]  ## This has the distance from their respective centroides for evaluation purposes \nk=3\nplot(data,k,index,centroids,orig_centroids)","3f5468c8":"# Perform k-means clustering with centroids initialize='firstk'\nkmeans = K_Means(k=3,data = data[0],centeriod_init='firstk')\ncentroids, cluster_assignments, iters, orig_centroids = kmeans.fit(data[0])\nindex = cluster_assignments[:,0] ## This has the cluster assignment 0,1,.... \ndistance = cluster_assignments[:,1]  ## This has the distance from their respective centroides for evaluation purposes \nk=3\nplot(data,k,index,centroids,orig_centroids)","71c4c39e":"costs = []\nfor i in range(10):\n    kmeans = K_Means(k=i,data = data[0],centeriod_init='firstk')\n    centroids, cluster_assignments, iters, orig_centroids = kmeans.fit(data[0])\n    distance = cluster_assignments[:,1]  ## This has the distance from their respective centroides for evaluation purposes \n    cost = sum(distance)\/(2*len(data[0]))\n    cost = np.array(cost)\n    cost =  cost.item()\n    costs.append(cost)\n    \nx = np.arange(10)\nplt.plot(x,costs)\nplt.title(\"Elbow curve\")\nplt.xlabel(\"K -->\")\nplt.ylabel(\"Dispersion\")","4f0f9abd":"costs = []\nfor p in range(10):\n    kmeans = K_Means(k=p,data = data[0],centeriod_init='random')\n    centroids, cluster_assignments, iters, orig_centroids = kmeans.fit(data[0])\n    X = data[0]\n    dist_ji = 0\n    a = 0\n    s=0\n    for i in range(len(data[0])):\n        for j in range(p):\n            dist_ji += euclidean_dist(centroids[j,:],X[i,:])\n            #print(dist_ji)\n    dist_ji -= sum(cluster_assignments[:,1])\/len(data[0])\n    a = sum(cluster_assignments[:,1])\/(len(data[0])-1)\n    s = (dist_ji - a)\/max(dist_ji,a)\n    s = np.array(s)\n    s =  s.item()\n    costs.append(s)\nx = np.arange(10)\nplt.plot(x,costs)\nplt.title(\"Silhoutte Score\")\nplt.xlabel(\"K -->\")\nplt.ylabel(\"Dispersion\")","460d5f42":"costs = []\nindexs = []\nClusters = []\ncentroid = []\norig_centroid = []\nfor i in range(10):\n    kmeans = K_Means(k=i,data = data[0],centeriod_init='random')\n    centroids, cluster_assignments, iters, orig_centroids = kmeans.fit(data[0])\n    \n    centroid.append(centroids)\n    orig_centroid.append(orig_centroids)\n    index = cluster_assignments[:,0] ## This has the cluster assignment 0,1,.... \n    indexs.append(cluster_assignments[:,0])\n    distance = cluster_assignments[:,1]  ## This has the distance from their respective centroides for evaluation purposes \n\n    cost = sum(distance)\/(2*len(data[0]))\n    cost = np.array(cost)\n    cost =  cost.item()\n    costs.append(cost)\n\nx = np.arange(10)\nplt.plot(x,costs)\nplt.title(\"Elbow curve\")\nplt.xlabel(\"K -->\")\nplt.ylabel(\"Dispersion\")","2db00ddc":"indexs = np.array(indexs)\ncentroid = np.array(centroid)\norig_centroid = np.array(orig_centroid)\n#index = index.item()\nClusters.append(index)\n\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,4,1)\nplot(data,1,indexs[1],centroid[1],orig_centroid[1])\nplt.xlabel(\"K = 1\")\nplt.subplot(1,4,2)\nplot(data,2,indexs[2],centroid[2],orig_centroid[2])\nplt.xlabel(\"K = 2\")\nplt.subplot(1,4,3)\nplot(data,3,indexs[3],centroid[3],orig_centroid[3])\nplt.xlabel(\"K = 3\")\nplt.subplot(1,4,4)\nplot(data,4,indexs[4],centroid[4],orig_centroid[4])\nplt.xlabel(\"K = 4\")","aa7b49a0":"## Silhouette Score","6b35bcfa":"## But how do we define or find the number ok 'K'..\n\n## This can be found out by Elbow curve or Silhoutte Score. ","b754ecb0":"### The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). ... The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.","e240e373":"## Perform k-means clustering with centroids initialize='rondom'","a604ce6e":"## K_Means Main Function - ","639e0e5d":"## <font color='Light Blue'>Clustering is the task of grouping together a set of objects in a way that objects in the same cluster are more similar to each other than to objects in other clusters. Similarity is a metric that reflects the strength of relationship between two data objects. Clustering is mainly used for exploratory data mining. It has manifold usage in many fields such as machine learning, pattern recognition, image analysis, information retrieval, bio-informatics, data compression, and computer graphics.\n\n## <font color='Light Blue'> However, this post tries to unravel the inner workings of K-Means, a very popular clustering technique. ","8c8d5c06":"## To Re-Iterate the point lets see how clusters are formed, So lets see plots with different values of 'K'  ","831baeb4":"## Now let's see its practical Implementation.","6a2d85d5":"# <font color='DarkBlue'> Kmeans from Scratch with Silhoutte and elbow curve","f32131fd":"## Conclusion:- \n### Stay tuned for notebooks on Algorithms from scratch..Do comment and UpVote if the was helpful to you.\ud83d\ude03\ud83d\ude4f","cf728304":"## In the above plots we can see that our model can classify data points into different clusters quite good. ","6779a45c":"## Note:- The blue markers are Original centroides black ones are the new centroides..","23b749a9":"## These will be global and reusable functions","8c351ae5":"# Perform k-means clustering with centroids initialize='firstk'","37affa88":"## Do comment and UpVote if this is helpful to you.\ud83d\ude03\ud83d\ude4f","48160039":"## Elbow curve","7e5dcf5d":"### In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.","16ea1bc3":"![2441A13F555FFC892E.png](attachment:2441A13F555FFC892E.png)","36bd32b4":"## Steps followed -- \n### Algorithm                  \n\n### 1.Clusters the data into k groups where k is predefined.\n\n### 2.Select k points at random as cluster centers.\n\n### 3.Assign objects to their closest cluster center according to the Euclidean distance function.\n\n### 4.Calculate the centroid or mean of all objects in each cluster.\n\n### 5.Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds. Know more about K-clustering from seo company. ","69c268ce":"### Its clear that at k = 3 the inter-cluster distance is actually forming valid clusters."}}