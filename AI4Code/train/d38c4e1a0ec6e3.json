{"cell_type":{"c95baf25":"code","9408d619":"code","6b9752de":"code","2fe2aaeb":"code","ba095dd3":"code","452f555b":"code","a4447066":"code","9040a09c":"code","8e237561":"code","741067ab":"code","7f9926b8":"code","36578e77":"code","c2aeb6f7":"code","979a8bb6":"code","a2e58a71":"code","560ee74e":"code","cddb0766":"code","816fb5e8":"code","e028cf8e":"code","f0fcd707":"code","4df80b77":"code","4a61c1b1":"code","87e9d9d3":"code","c30e3d97":"code","3de5bcaf":"code","39ab9db6":"code","2d1bba94":"code","1751b643":"code","647aa4e8":"code","aeb4fa12":"code","2b48277b":"code","22a7770a":"code","43d47aa8":"code","2016633a":"code","cd987fe0":"code","b630aae2":"code","a1053f5a":"code","16d2d2d1":"code","8be8d9d3":"code","4779e162":"code","191bfed0":"code","f48f0d52":"code","9b35a25b":"code","ee3a35fb":"code","7181306f":"code","1d984ec2":"code","4cae6a95":"code","bdaed36b":"code","303f863b":"code","edfab550":"code","2fb12c6b":"code","a29d61e9":"code","dce3a1bd":"code","262bf4b4":"code","d9af15ce":"code","0ee02b61":"code","cc68b1f2":"code","a4a9478a":"code","79392ce6":"code","3a3a0067":"code","62f7170e":"code","56508c62":"code","2b48977b":"code","5b3ad05f":"code","419fa357":"code","ab44dad5":"code","16492095":"code","fa80ee49":"code","b830c1d2":"code","c1f095ba":"code","489c761c":"code","fa2f645a":"code","69762527":"code","5791dc8b":"code","be83c501":"code","4a60bfd3":"code","d5122b8b":"code","3f5f6b91":"code","d4bb0998":"code","7b50cd33":"code","7cf9fadd":"code","931c419b":"code","5987f37b":"code","f86738c7":"code","2fe6f419":"code","f991b192":"code","b4da8494":"code","048a3bc8":"code","f1a25a43":"code","6df8c6d2":"code","a463c64d":"code","69e28f22":"code","25bbd630":"code","0ad2774d":"code","ace046d6":"code","70e13ec9":"code","9d598218":"code","08800e98":"code","4dbcf8f9":"code","6d2f55f5":"code","e6c07562":"code","fe2096d0":"code","5bb14594":"code","5b58a659":"code","e9c3fc28":"code","ff0ed37a":"code","609fcec0":"markdown","918a87b5":"markdown","ca3d38e0":"markdown","34011198":"markdown","e17fa46e":"markdown","7ced367f":"markdown","dfc22fb3":"markdown","533e9c1b":"markdown","1fcf3f0a":"markdown","d7068184":"markdown","1e0b7335":"markdown","450b01d0":"markdown","7312e810":"markdown","4f0f0320":"markdown","300ae216":"markdown","936e3ba1":"markdown","f016e90f":"markdown","fe4ee247":"markdown","adbcab59":"markdown","8ad50921":"markdown","b047efba":"markdown","1d9c4c4e":"markdown","dae4ff48":"markdown","10ebb0ed":"markdown","7c90c2b8":"markdown","de304015":"markdown","b9480e7f":"markdown","70001a18":"markdown","10bccdec":"markdown","4c4ebd2f":"markdown","bbbfb02a":"markdown","66f80069":"markdown","75f153f5":"markdown","2e6fd2f5":"markdown","6fe6656f":"markdown","5dd1023e":"markdown","cd9da749":"markdown","ac824efb":"markdown","799ea782":"markdown","767c8bbd":"markdown","7f0db94a":"markdown","4b827ac1":"markdown","5142f16b":"markdown","3f0dbf41":"markdown","be675a2d":"markdown","da71e64e":"markdown","be51d50b":"markdown","7298e19f":"markdown","e9966629":"markdown","feaae489":"markdown","124cbed3":"markdown","19b1707d":"markdown","fc7f9ef7":"markdown","a57a7245":"markdown","f999a109":"markdown"},"source":{"c95baf25":"'''\n    Here we have all the important libraries for reading a CSV file or plotting the graphs.\n    We will be importing the model_selection for splitting the data to trained and test data.\n    \n    statsmodels will be imported to get the statistics of the data frame.\n    StandardScaler library wil be used to do the scaling of the data.\n    \n    we will be importing the LogisticRegression libraries to build the models.\n    RFE library will help us in important featuer selections\n    \n    Variance_inflation_factor library will help us in determining the multiclooinearity\n    confusion metrics ill help us in determining lots of factors like accuracy\/precision etc\n    through the matrix\n    \n'''\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","9408d619":"# reading the data\n'''\n    Here we are reading the data from the csv file present in the below path. From the evaluation perspective,\n    we just have to change the below path to the required csv path\n'''\npath  = r'..\/input\/leads-scoring\/Leads.csv'\n\nleads = pd.read_csv(path)","6b9752de":"'''\n    Here we are creating a copy of the dataset, which can be used at the end.\n'''\nleads_copy = leads.copy(deep = True)","2fe2aaeb":"# viewing the first few records\nleads.head()","ba095dd3":"'''\n    Understanding the data\n'''\n\nleads.info()","452f555b":"'''\n    checking the shape of the data\n'''\n\nleads.shape","a4447066":"'''\n    describe will help us in determining the max, min, count, quartiles etc for each column.\n    This helps us in understanding the column range, type of column mean, standard deviation, outliers, etc\n'''\n\nleads.describe()","9040a09c":"'''\n    Checking for the percentages of null values in each column\n'''\nround(leads.isnull().sum()\/len(leads) * 100,2)","8e237561":"'''\n    It is mentioned that the value 'Select' needs to be handled because it is as good as Null. \n    So, converting the value Select as Null\n'''\n\nleads.replace('Select', np.nan, inplace= True)\nleads.head()","741067ab":"'''\n    Rechecking the Null percentages after Select has been made Null\n'''\n\nprint(round(leads.isnull().sum()\/len(leads) * 100,2))","7f9926b8":"'''\n    Dropping the columns 'Lead Profile','How did you hear about X Education' as they have very high percentage\n    of missing values \n'''\n\nleads = leads.drop(['Lead Profile','How did you hear about X Education'], axis = 1)","36578e77":"'''\n    Check for the counts of distinct values present in Lead Quality column\n'''\nleads['Lead Quality'].value_counts()","c2aeb6f7":"'''\n    Converting the missing values in the column Lead Quality column to \"Not Sure\".\n'''\n\nleads['Lead Quality'] = leads['Lead Quality'].replace(np.nan, 'Not Sure')\nleads['Lead Quality'].value_counts()","979a8bb6":"plt.figure(figsize=[10,8])\n\nplt.subplot(2,2,1)\nsns.countplot(leads['Asymmetrique Profile Index'])\n\nplt.subplot(2,2,2)\nsns.countplot(leads['Asymmetrique Profile Score'])\n\nplt.subplot(2,2,3)\nsns.countplot(leads['Asymmetrique Activity Index'])\n\nplt.subplot(2,2,4)\nsns.countplot(leads['Asymmetrique Activity Score'])\n\nplt.show()","a2e58a71":"'''\n    Doing a pairplot inorder to check the correlation of columns 'Asymmetrique Activity Score', \n    'Asymmetrique Profile Score', 'Converted'.\n'''\nsns.pairplot(leads[['Asymmetrique Activity Score', 'Asymmetrique Profile Score','Converted']])\nplt.show()","560ee74e":"'''\n    Converting the data to a pivot table for better understanding of the data. Now, we can clealry see how the values in the column\n    Asymmetrique Activity Index are present against the \"Converted\" values\n\n'''\ndf_pivot = pd.pivot_table(data = leads, columns = 'Converted', index = 'Asymmetrique Activity Index',values = 'Prospect ID', aggfunc='count')\ndf_pivot","cddb0766":"'''\n    Plotting the pivot table in a graph to make it more readable. \n'''\ndf_pivot.unstack('Asymmetrique Activity Index').plot(kind='bar')\nplt.show()","816fb5e8":"'''\n    Converting the data to a pivot table for better understanding of the data. Now, we can clealry see how the values in the column\n    Asymmetrique Profile Index are present against the \"Converted\" values\n\n'''\ndf_pivot = pd.pivot_table(data = leads, columns = 'Converted', index = 'Asymmetrique Profile Index',values = 'Prospect ID', aggfunc='count')\ndf_pivot","e028cf8e":"'''\n    Plotting the pivot table in a graph to make it more readable. \n'''\ndf_pivot.unstack('Asymmetrique Profile Index').plot(kind='bar')\nplt.show()","f0fcd707":"'''\n    Converting the data to a pivot table for better understanding of the data. Now, we can clealry see how the values in the column\n    Asymmetrique Activity Score are present against the \"Converted\" values\n\n'''\ndf_pivot = pd.pivot_table(data = leads, columns = 'Converted', index = 'Asymmetrique Activity Score',values = 'Prospect ID', aggfunc='count')\ndf_pivot","4df80b77":"'''\n    Plotting the pivot table in a graph to make it more readable. \n'''\ndf_pivot.unstack('Asymmetrique Activity Score').plot(kind='bar')\nplt.show()","4a61c1b1":"'''\n    Converting the data to a pivot table for better understanding of the data. Now, we can clealry see how the values in the column\n    Asymmetrique Profile Score are present against the \"Converted\" values\n\n'''\ndf_pivot = pd.pivot_table(data = leads, columns = 'Converted', index = 'Asymmetrique Profile Score',values = 'Prospect ID', aggfunc='count')\ndf_pivot","87e9d9d3":"'''\n    Plotting the pivot table in a graph to make it more readable. \n'''\ndf_pivot.unstack('Asymmetrique Profile Score').plot(kind='bar')\nplt.show()","c30e3d97":"'''\n   Looking at the correlation of the Scores with the values in Converted column\n'''\nleads[['Asymmetrique Activity Score', 'Asymmetrique Profile Score','Converted']].corr()","3de5bcaf":"# dropping the above mentioned columns\nleads = leads.drop(['Asymmetrique Activity Score', 'Asymmetrique Profile Score','Asymmetrique Activity Index', 'Asymmetrique Profile Index'], axis = 1)","39ab9db6":"'''\n    Finding the percentage of null columns post removing few columns and imputing of other columns\n'''\nround(leads.isnull().sum()\/len(leads) * 100,2)","2d1bba94":"# checking the frequency of values in the City column, so it can be handled appropriately\nprint(leads['City'].value_counts())","1751b643":"# replacing the missing values with the mode i.e Mumbai\nleads.City.replace(np.nan,'Mumbai',inplace=True)","647aa4e8":"# checking the frequency of the values\nprint(leads['Specialization'].value_counts())","aeb4fa12":"# replcaing null values with Others\nleads.Specialization.replace(np.nan,'Others',inplace=True)","2b48277b":"# checking the frequency of the values\nprint(leads['Tags'].value_counts())","22a7770a":"# replacing the missing values with 'Will revert after reading the email'\nleads.Tags.replace(np.nan,'Will revert after reading the email',inplace=True)","43d47aa8":"# checking the frequency of the values\nprint(leads['What matters most to you in choosing a course'].value_counts())","2016633a":"# replacing the null values\nleads['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects',inplace=True)","cd987fe0":"# checking the frequency of the values\nprint(leads['What is your current occupation'].value_counts())","b630aae2":"# replacing the null values\nleads['What is your current occupation'].replace(np.nan,'Unemployed',inplace=True)","a1053f5a":"# checking the frequency of the values\nprint(leads['Country'].value_counts())","16d2d2d1":"# replacing the null values\nleads.Country.replace(np.nan,'India',inplace=True)","8be8d9d3":"'''\n    Now, we have handled most of the data and have done all the imputations on the remaining columns as well. \n    Checking the null percentage again to see if the data has been cleaned\n'''\nround(leads.isnull().sum()\/len(leads) * 100,2)","4779e162":"'''\n    Dropping the remianing columns which have around 1% of null data, deleting these many records will not impact much.\n'''\nleads = leads.dropna()","191bfed0":"leads.shape","f48f0d52":"'''\n    Checking the null values again\n'''\nround(leads.isnull().sum()\/len(leads) * 100,2)","9b35a25b":"# plotting the pairplots of the numeric columns to see how they affect each other\nsns.pairplot(leads)\nplt.show()","ee3a35fb":"'''\n    Printing the heatmap of the entire data to find out the relation\/dependency of each column.\n'''\nsns.heatmap(leads.corr(),annot = True)\nplt.show()","7181306f":"'''\n Plotting the countplot to see map the frequency of the values in the columns with respect to the values in the column Converted \n'''\nplt.figure(figsize = (30,20))\n\nplt.subplot(2,3,1)\nsns.countplot(x = 'Do Not Email',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'Lead Origin',hue = 'Converted', data = leads)\nplt.xticks(rotation = 45)\n\nplt.subplot(2,3,3)\nsns.countplot(x = 'Lead Source',hue = 'Converted', data = leads)\nplt.xticks(rotation = 45)\n\nplt.subplot(2,3,4)\nsns.countplot(x = 'Do Not Call',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,5)\nsns.countplot(x = 'TotalVisits',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,6)\nsns.countplot(x = 'Last Notable Activity',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.show()","1d984ec2":"'''\n Plotting the countplot to see map the frequency of the values in the columns with respect to the values in the column Converted\n \n'''\n\nplt.figure(figsize = (20,16))\n\nplt.subplot(2,3,1)\nsns.countplot(x = 'Page Views Per Visit',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'Country',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,3)\nsns.countplot(x = 'Specialization',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,4)\nsns.countplot(x = 'What is your current occupation',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,5)\nsns.countplot(x = 'What matters most to you in choosing a course',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.show()","4cae6a95":"'''\nPlotting the countplot to see map the frequency of the values in the columns with respect to the values in the column Converted\n'''\nplt.figure(figsize = (20,16))\n\nplt.subplot(2,3,1)\nsns.countplot(x = 'Search',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'Magazine',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,3)\nsns.countplot(x = 'Newspaper Article',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,4)\nsns.countplot(x = 'X Education Forums',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,5)\nsns.countplot(x = 'Newspaper',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,6)\nsns.countplot(x = 'Digital Advertisement',hue = 'Converted', data = leads)\n\nplt.show()","bdaed36b":"'''\nPlotting the countplot to see map the frequency of the values in the columns with respect to the values in the column Converted\n\n'''\nplt.figure(figsize = (30,40))\n\nplt.subplot(2,3,1)\nsns.countplot(x = 'Through Recommendations',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'Receive More Updates About Our Courses',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,3)\nsns.countplot(x = 'Tags',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,4)\nsns.countplot(x = 'Lead Quality',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,5)\nsns.countplot(x = 'Update me on Supply Chain Content',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,6)\nsns.countplot(x = 'Get updates on DM Content',hue = 'Converted', data = leads)\n\nplt.show()","303f863b":"'''\nPlotting the countplot to see map the frequency of the values in the columns with respect to the values in the column Converted\n\n'''   \nplt.figure(figsize = (20,16))\n\nplt.subplot(2,3,1)\nsns.countplot(x = 'City',hue = 'Converted', data = leads)\nplt.xticks(rotation = 90)\n\nplt.subplot(2,3,2)\nsns.countplot(x = 'I agree to pay the amount through cheque',hue = 'Converted', data = leads)\n\n\nplt.subplot(2,3,3)\nsns.countplot(x = 'A free copy of Mastering The Interview',hue = 'Converted', data = leads)\n\nplt.show()","edfab550":"plt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'TotalVisits', data = leads)\n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'Total Time Spent on Website',hue = 'Converted', data = leads)\n\nplt.subplot(2,3,3)\nsns.boxplot(x = 'Page Views Per Visit',hue = 'Converted', data = leads)\n\nplt.show()","2fb12c6b":"'''\n    Defining a function in which a dataframe and a column(for which we want to remove the outliers) are passed. \n    The dataframe returned is sans the outliers. The range of values being returned are between 0.05 and 0.97 percentiles\n'''\ndef remove_outliers(df,col):\n    \n    Q1 = df[col].quantile(0.05)\n    Q3 = df[col].quantile(0.97)\n    df = df[(df[col] >=Q1) &(df[col] <=Q3)]\n    \n    return df","a29d61e9":"'''\n    \n    Taking a backup of the existing data frame so, that we can use the actual one, in case we want the DF with no \n    outlier treatment.\n    \n'''\n\nleads_outlier = leads.copy(deep = True)\nlen(leads_outlier)","dce3a1bd":"'''\n    Here the outliers for three columns TotalVisits, Total Time Spent on Website and Page Views Per Visit are removed\n'''\nleads_outlier = remove_outliers(leads_outlier,'TotalVisits')\nleads_outlier = remove_outliers(leads_outlier,'Total Time Spent on Website')\nleads_outlier = remove_outliers(leads_outlier,'Page Views Per Visit')","262bf4b4":"'''\n\n   Checking the number of records left post the outlier treatment\n\n'''\n\nlen(leads_outlier)","d9af15ce":"'''\n   \n   Plotting the boxplots after removing the outliers from the columns TotalVisits, Total Time Spent on Website and Page Views Per Visit\n   \n'''\nplt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'TotalVisits', data = leads_outlier)\n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'Total Time Spent on Website',hue = 'Converted', data = leads_outlier)\n\nplt.subplot(2,3,3)\nsns.boxplot(x = 'Page Views Per Visit',hue = 'Converted', data = leads_outlier)\n\nplt.show()","0ee02b61":"# Since the outliers were treated in another dataframe, updating the original dataframe with the cleansed one\nleads = leads_outlier.copy(deep = True)\nleads.head()","cc68b1f2":"leads.shape","a4a9478a":"'''\n    Converting the binary variables to 1 and 0 and using the map function to do the same.\n'''\n# the list of variables having binary value only\nvarlist =  ['Do Not Email', 'Do Not Call','Newspaper Article', 'X Education Forums', 'Newspaper','Digital Advertisement','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque','A free copy of Mastering The Interview','Search']\n\n# Defining the map function to change Yes to 1 and No to 0\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\nleads[varlist] = leads[varlist].apply(binary_map)\nleads = leads.drop(varlist, axis =1 )","79392ce6":"# checking how the dataframe looks like after the mapping is done\nleads.head()","3a3a0067":"'''\n    Converting all the categorical columns to the dummy variables.\n\n'''\n# creating a list of the columns\nvar_list = ['Lead Origin', 'Lead Source','Country','Specialization', 'What is your current occupation',\n            'What matters most to you in choosing a course', 'Magazine', 'Last Activity',\n            'Through Recommendations','Tags', 'Lead Quality','City','Last Notable Activity']\n\ndummy1 = pd.get_dummies(leads[var_list], drop_first=True)\n\n# Adding the results to the master dataframe\nleads = pd.concat([leads, dummy1], axis=1)\nleads = leads.drop(var_list, axis =1 )","62f7170e":"leads.shape","56508c62":"# The features are assigned to X\nX = leads.drop(['Prospect ID','Converted','Lead Number'], axis=1)\nX.head()","2b48977b":"# the target is assigned to y\ny = leads['Converted']\n\ny.head()","5b3ad05f":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","419fa357":"scaler = StandardScaler()\n\n# the original numerical columns are stored in another variable. \n# All the other columns are either converted using binary mapping method or are dummy variables and \n# therefore do not require standardization since standardscaler is being used here\ncont_varlist=['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nX_train[cont_varlist] = scaler.fit_transform(X_train[cont_varlist])\n\nX_train.head()","ab44dad5":"X_train[cont_varlist].describe()","16492095":"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","fa80ee49":"logreg = LogisticRegression()\nrfe = RFE(logreg, 14)\nrfe = rfe.fit(X_train, y_train)","b830c1d2":"rfe.support_","c1f095ba":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","489c761c":"col = X_train.columns[rfe.support_]\ncol","fa2f645a":"X_train_sm_2 = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres2 = logm2.fit()\nres2.summary()","69762527":"col3 = col.drop('Tags_number not provided', 1)\ncol3","5791dc8b":"X_train_sm_3 = sm.add_constant(X_train[col3])\nlogm3 = sm.GLM(y_train,X_train_sm_3, family = sm.families.Binomial())\nres3 = logm3.fit()\nres3.summary()","be83c501":"col4 = col3.drop('Lead Source_Welingak Website', 1)\ncol4","4a60bfd3":"X_train_sm_4 = sm.add_constant(X_train[col4])\nlogm4 = sm.GLM(y_train,X_train_sm_4, family = sm.families.Binomial())\nres4 = logm4.fit()\nres4.summary()","d5122b8b":"'''\n    Created a function to calculate and return VIF. \n'''\n\ndef calculate_vif(df, col):\n    vif = pd.DataFrame()\n    vif['Features'] = df[col].columns\n    vif['VIF'] = [variance_inflation_factor(df[col].values, i) for i in range(df[col].shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","3f5f6b91":"'''\n calculating VIF using the function created above\n'''\n\ncalculate_vif(X_train,col4)","d4bb0998":"y_train_pred = res4.predict(X_train_sm_4)\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob_model':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","7b50cd33":"# ro begin with, the threshold can be given as 0.5\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_prob_model.map(lambda x: 1 if x > 0.5 else 0)","7cf9fadd":"y_train_pred_final.head()","931c419b":"'''\n    This method will help us in getting the confusion matrix, accuracy score, sensitivity, specificty values also.\n    This method will also help us in finding the precision and recall.\n'''\ndef calculate_all_metrics(df,col_conv,col_pred):\n    confusion = metrics.confusion_matrix(df[col_conv], df[col_pred])\n    print(\"Confusion matrix obtained is \\n{val}\".format(val=confusion))\n    print(\"\\nAccuracy score obtained is {val}\".format(val = metrics.accuracy_score\n                                                    (df[col_conv], df[col_pred])))\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print(\"\\nSensitivity for the above confusion matrix obtained is = \", TP \/ float(TP+FN))\n    print(\"\\nSpecificity for the above confusion matrix obtained is = \", TN \/ float(TN+FP))\n    print(\"\\nFalse Positive Rate for the above confusion matrix obtained is = \", FP\/ float(TN+FP))\n    print (\"\\nPrecision for the above confusion matrix obtained is = \", TP \/ float(TP+FP))\n    print(\"\\nRecall for the above confusion matrix obtained is = \", TP \/ float(TP+FN))\n    print (\"\\nNegative predictive value for the above confusion matrix obtained is = \", TN \/ float(TN+ FN))","5987f37b":"calculate_all_metrics(y_train_pred_final,'Converted','Predicted')","f86738c7":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","2fe6f419":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob_model, drop_intermediate = False )","f991b192":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob_model)","b4da8494":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob_model.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","048a3bc8":"# Calculating the accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","f1a25a43":"# plotting a graph for the various probability values\n\nplt.figure(figsize=(15,10))\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.vlines(x=0.27, ymax=1, ymin=0, colors=\"black\", linestyles=\"--\")\nplt.vlines(x=0.3, ymax=1, ymin=0, colors=\"y\", linestyles=\"--\")\nplt.show()","6df8c6d2":"y_train_pred_2 = pd.DataFrame({'Converted':y_train.values, 'Converted_prob_model':y_train_pred})\ny_train_pred_2['Prospect ID'] = y_train.index\ny_train_pred_2.head()","a463c64d":"# using 0.27 as the index\ny_train_pred_2['Predicted'] = y_train_pred_2.Converted_prob_model.map(lambda x: 1 if x > 0.27 else 0)\ny_train_pred_2.head()","69e28f22":"'''\n\n    Calculating all the metrics after getting a final data frame.\n    \n'''\ncalculate_all_metrics(y_train_pred_2,'Converted','Predicted')","25bbd630":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_2.Converted, y_train_pred_2.Converted_prob_model, drop_intermediate = False )\ndraw_roc(y_train_pred_2.Converted, y_train_pred_2.Converted_prob_model)","0ad2774d":"X_test[cont_varlist] = scaler.transform(X_test[cont_varlist])\nX_test = X_test[col4]\nX_test_sm = sm.add_constant(X_test)\ny_test_pred = res4.predict(X_test_sm)\ny_test_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\ny_test_df['Prospect ID'] = y_test_df.index\n\ny_test_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n\ny_test_pred_final = pd.concat([y_test_df, y_test_pred_1],axis=1)\ny_test_pred_final.head()","ace046d6":"y_test_pred_final= y_test_pred_final.rename(columns={ 0 : 'Converted_prob'})\ny_test_pred_final.head()","70e13ec9":"y_test_pred_final['final_predicted'] = y_test_pred_final.Converted_prob.map(lambda x: 1 if x > 0.27 else 0)\ny_test_pred_final.head()","9d598218":"'''\n    find out all the metrics like precision,recall, accuracy,confusion matrix etc for the final data frame\n'''\ncalculate_all_metrics(y_test_pred_final,'Converted','final_predicted')","08800e98":"'''\n    Conversion rate\n'''\n# people getting converted actually\nl1 = len(y_test_pred_final[y_test_pred_final['Converted']==1])\n\n# people getting converted as per the model\nl2 = len(y_test_pred_final[y_test_pred_final['final_predicted']==1])\n\n# calculating and printing the conversion rate\nprint (\"Conversion rate as per the model is {0}\" .format(round(100*(l2\/l1),2)))","4dbcf8f9":"'''\n    this final_df will help us in determining the prospect id for the people who will be predicted (by the model) \n    to be converted as a customer\n'''\nfinal_df = y_test_pred_final[y_test_pred_final['final_predicted']==1]\nfinal_df.head()","6d2f55f5":"print(\"Total number of records in the final dataframe are \", len(final_df))\nfinal_df[\"LeadScore\"] = final_df[\"Converted_prob\"] * 100\nfinal_df.head()","e6c07562":"final_df.describe()","fe2096d0":"'''\n    Here we are finding the count of the records based on the some intervals.\n    eg. count of people who will be converted(according to the model) and have probability of getting converted is\n    greater than 0.8 and less than 0.9 etc\n'''\ndf_greater_90 = list(final_df['Converted_prob'].between(0.9, 1, inclusive=False)).count(True)\ndf_in_80_90 =list( final_df['Converted_prob'].between(0.8, 0.9, inclusive=False)).count(True)\ndf_in_70_80 =list( final_df['Converted_prob'].between(0.7, 0.8, inclusive=False)).count(True)\ndf_in_60_70 =list( final_df['Converted_prob'].between(0.6, 0.7, inclusive=False)).count(True)\ndf_in_50_60 =list( final_df['Converted_prob'].between(0.5, 0.6, inclusive=False)).count(True)\ndf_in_40_50 =list( final_df['Converted_prob'].between(0.4, 0.5, inclusive=False)).count(True)\ndf_in_30_40 =list( final_df['Converted_prob'].between(0.3, 0.4, inclusive=False)).count(True)\ndf_in_20_30 =list( final_df['Converted_prob'].between(0.2, 0.3, inclusive=False)).count(True)\ndf_in_10_20 =list( final_df['Converted_prob'].between(0.1, 0.2, inclusive=False)).count(True)\ndf_in_0_10 = list(final_df['Converted_prob'].between(0, 0.1, inclusive=False)).count(True)\n\n\nprint('\\nnumber of people getting converted with probability greater than 0.9 is {val}'.format(val = df_greater_90))\nprint('\\nnumber of people getting converted with probability in range 0.8 - 0.9 is {val}'.format(val = df_in_80_90))\nprint('\\nnumber of people getting converted with probability in range 0.7 - 0.8 is {val}'.format(val = df_in_70_80))\nprint('\\nnumber of people getting converted with probability in range 0.6 - 0.7 is {val}'.format(val = df_in_60_70))\nprint('\\nnumber of people getting converted with probability in range 0.5 - 0.6 is {val}'.format(val = df_in_50_60))\nprint('\\nnumber of people getting converted with probability in range 0.4 - 0.5 is {val}'.format(val = df_in_40_50))\nprint('\\nnumber of people getting converted with probability in range 0.3 - 0.4 is {val}'.format(val = df_in_30_40))\nprint('\\nnumber of people getting converted with probability in range 0.2 - 0.3 is {val}'.format(val = df_in_20_30))\nprint('\\nnumber of people getting converted with probability in range 0.1 - 0.2 is {val}'.format(val = df_in_10_20))\nprint('\\nnumber of people getting converted with probability in range 0.0 - 0.1 is {val}'.format(val = df_in_0_10))","5bb14594":"df_hot_leads = final_df[final_df['Converted_prob'] >= 0.8]\ndf_others = final_df[final_df['Converted_prob'] < 0.8]","5b58a659":"df_hot_leads.head()","e9c3fc28":"df_others.head()","ff0ed37a":"print(\"Number of hot leads is \", df_hot_leads.shape[0])","609fcec0":"Since none of the values present in the Specialization column seem to be good enough to impute the data, the null data can be imputed with the value \"Others\"","918a87b5":"**The conclusions drawn from the above pivot table and graph are synonymous with the ones made for Asymmetrique Activity Index","ca3d38e0":"The dataframe has been updated\n\nUpdating the categorical columns - mapping binary variables and creating dummy ones as needed","34011198":"* As evident, the mean is alomost 0 and the standard deviation is almost one. So scaling has been done correctly\n\n# Step 5 - Data Modelling\n* Building the model","e17fa46e":"# Step 1 - Reading and understanding the data","7ced367f":"@Author : Garima and Tushar","dfc22fb3":"In the Tags column, the value \"Will revert after reading the email\" has the highest value count and seems to be safe to replace the null value with it since this is not only the mode of the data but also a pretty logical response","533e9c1b":"* Scaling the data","1fcf3f0a":"The statistics calculated for cut-off being 0.5 look good, but it still makes sense to calculate the them for other cut-off values and see what is the most appropriate one","d7068184":"* Again, like the Index columns, even the Scores do not seem to be pretty conclusive although some values show affinity towards 0 and some towards 1,but not quite conclusively.","1e0b7335":"Evidently, the columns in the final model created do not show high signs of multicollinearity. Therefore, the model created can be used for further steps","450b01d0":"Univariate analysis of the data","7312e810":"*  How 'City' is related with Converted - A sizable portion of the population belongs to Mumbai. But the conversion rate pretty much seems to follow the same pattern for all the values in the City column. Therefore, a solid conclusion cannot really be drawn for this column\n   \n*  How 'I Agree to pay the amount through cheque' is related with Converted - The entire data has the value \"No\" in this column. Therefore, it does not make sense to draw any inference\n     \n*  How 'A free copy of Mastering The Interview' is related with Converted - Unlike expected, the conversion rate seems to be a tad bit better when the copy of mastering the interview is not available vs when it is avaible. Maybe there are more factors to be looked at rather than sole analysis of this column","4f0f0320":"* Around 40% of the city data is missing, the following options available to handle them:\n1. delete the column - the drawback of this approach is loss of data\n2. impute the column with the mode of the column values - this seems to be relevant for our case\n3. impute the column with another value - does not seem relevant because it is not easy to decide what other value to impute the missing values with","300ae216":"There are no more null values in the data. \n# Step 3 - EDA","936e3ba1":"The model now looks neat with a few variables, all of which are significant. Checking the VIF now makes sense, this will ensure that multicollinearity among the variables is low","f016e90f":"* Apparently, for each value of \"Asymmetrique Activity Index\", there are good number of values present as Converted = 0 and Converted = 1\n* The biggest take away from this is that when the \"Asymmetrique Activity Index\" being Low, the Converted column is more likely to be 0 than     However, this is not a solid conclusion because the number of records are not quite large.\n* Therefore, this kind of distribution is not a good predicted of the target variable","fe4ee247":"Predicting the target variable for the train dataset","adbcab59":"From the above plot we can see that with the help of vertical lines, the black line is giving us the best result. And this line is generated at prob = 0.27, hence we will use this as the cut off to find out the results","8ad50921":"# > Steps to be followed\n*  Step 1 - Read and understanding the data \n*  Step 2 - Data cleaning : handling missing\/Null values, correcting incorrect values\n*  Step 3 - EDA - pairplots, heatmap, Univariate analysis of the data\n*  Step 4 - Data Preparation - handling outliers, creating dummy variables, test train split, rescaling the data\n*  Step 5 - Data Modelling -  building the model, evaluating the model on train data , calculating the metrics\n*  Step 6 - Predicting and evaluating the model on test data\n*  Step 7 - Business recommendations","b047efba":"* Problem Statement\n> X Education requires us to build a model that assigns a lead score to each of the leads such that the customers with higher lead score \nhave a higher conversion chance and the customers with lower lead score have a lower conversion chance. The conversion rate in the input \ndataset is 30% and the ballpark of the target lead conversion rate needs to be around 80%. The CEO will have the team focus on the leads \nthat the model results as promising to make the \u201clead to customer\u201d process more efficient. Therefore, the Business objective is to select \nthe most promising leads \u2013 the ones that are highly likely to convert into paying customers.","1d9c4c4e":"The column \"Tags_number not provided\" has p-value of 1 and is therefore highly insignificant. Thus, this column needs to be dropped and the model needs to be updated","dae4ff48":"As evident from the above graphs, there are a lot of outliers present in the columns - TotalVisitors, Total Time Spent on Website and Page Views Per Visit","10ebb0ed":"* How 'Through Recommendations' is related with Converted - There are very less people who have joined the institute based on the recommendations.\n    \n* How 'Receive More Updates about our Course' is related with Converted - The entire data is \"No\". Therefore, does not make sense to draw any conclusion for this column\n     \n* How 'Tags' is related with Converted - \n       *  Will revert after receiving the email has the highest conversion rate.\n       *  People with the tag Lost to EINS have almost negligible people who are not getting converted.\n       *  Almost everyone with the Tag closed by Horizon has joined the course.\n       \n* How 'Lead Quality' is related with Converted - \n        *  Lead Quality = Not Sure -> conversion rate does not look good.\n        *  \"Low in Relevance\", \"Might be\" and \"High in Relevance\"  have higher conversion rate.\n            \n*  How 'Update me on Supply Chain Content' is related with Converted - The entire data has the value \"No\" in this column. Therefore, it does not make sense to draw any inference","7c90c2b8":"* From the graph, it seems that\n    * Total visits is positively related to Pages views per visit\n    * Total time spent on website is positively related to Converted and TotalVisits.    \n","de304015":"The column \"Lead Source_Welingak Website\" has p-value of 1 and is therefore highly insignificant. Thus, this column needs to be dropped and the model needs to be updated","b9480e7f":"Since most of the data belongs to India, we can replace Null with India in the Country column","70001a18":"From the graph, it is not very easy to make apparent conclusions on the correlations","10bccdec":"Most of the data shows the value \"Unemployed\" in the column \"What is your current occupation\". Also, it seems to be practical and logical to replace the null values as \"Unemployed\" in this column","4c4ebd2f":"All the statistics look good in the final model.\n\n# Step 6 - Predicting and evaluating the model on test data","bbbfb02a":"* The model created gives pretty good results for the test data as well. Accuracy of over 90% implies that in 90% of the cases the model can predict the \"converted\" value correctly. \n* A low False Positive Rate of 0.06 implies that in only 6% of the cases the model predicts that a lead would be converted while in reality it would not be converted. \n* A high Negative predictive value indicates that in 90% of the cases the model rightly predicts that the lead would not be converted and it is actually not converted. \n* A high value here would save a lot of time of the marketing team since they would not approach these leads that would eventually anyway not become customers 88% precision implies that the model correctly predicts conversion of a lead in 88% of the cases\n\nTherefore, this model satisfies the requirements and can thus be used to predict if a lead would be converted or not","66f80069":"*  In case of a logistic regression model, the target value predicted by the model is a probabilty. Based one some calculations, a cut-off is identified basis which the target variable is predicted by the model. \n\n* In the present scenario, the organization is looking to identify cases that have high likelihood of getting converted. Another way to look at this is to identify that leads that will not be converted so the marketing team would not approach them. \n* One of the ways to chunk the final dataframe created is to divide it based on the range of the probability predicted by the logistic regression model. \n* Based on these ranges, a minimum weighted average of the probabilty can be calculate. Obviously, it would make most sense to approach the leads with high probability. \n* Should the marketing team have more time, they can start approaching the other leads in the decreasing order of this probability","75f153f5":"the above summary has all the columns. Clearly, the model does not require so many of them. Therefore it makes sense to use the RFE method to build the model","2e6fd2f5":"As evident, a lot of null values have been handled now. The ones remaining are quite low but will still need to be handled.","6fe6656f":"*  The \"count\" row indicates that the columns TotalVisits, Page Views Per Visit, Asymmetrique Activity Score, Asymmetrique Profile Score have null values\n*  Apparently TotalVisits has outlier(s) becuase the max value is very large compared to other statistics. So is the case with Total Time Spent on Website, Page Views Per Visit\t ","5dd1023e":"* The dataset has a lot of columns with significant percentage of values missing. These columns will be handled one after the other in the decreasing order of null percentage.\n* The columns 'Lead Profile', and 'How did you hear about X Education' have 74% and 78% of values missing respectively. \n* Imputing such huge number of values will introduce a bias in the data, therefore, it seems reasonable to drop these columns instead\n","cd9da749":"> As such, from the above few cells, it can be concluded that these four columns have over 45% of the data missing. \n* The column Asymmetrique Profile Index has very few records in the category \"Low\". The difference in the counts for High and Medium is not quite high.\n* At 15 and 18, the values in Asymmetrique Profile Score see a spike while the other columns show a staggered behaviour\n* The column Asymmetrique Profile Index has very few records in the category \"Low\". This is closely followed by \"High\" while the category \"Medium\" has a substantially high count. \n*  At 14, 15 and 13, the values in Asymmetrique Activity Score see a spike while the other columns show a staggered behaviour","ac824efb":"*****From the pairplots above, it is not very easy to say for which values of Asymmetrique Activity Score (or Asymmetrique Profile Score), the columns Converted would certainly be 1 or 0.Therefore, neither of these columns seem to show a strong behaviour in being able to predict the value of \"Converted\"","799ea782":"Typically, one would want to pursue a course to improve their career prospects. Therefore, replacing the null values with \"Better Career Prospects\" in the column \"What matters most to you in choosing a course\" makes a lot of sense","767c8bbd":"The next column in line is - What is your current occupation","7f0db94a":"# Step 2- Data Cleaning","4b827ac1":"*  How 'Page Views Per Visit' is related with Converted - Pages views Per Visit also has some correlations with people getting Converted but the graph doesn't seem to be helpful in infering it clearly\n   \n*  How 'Country' is related with Converted - Almost the entire data is from India, so drawing an inference on relationship between the columns Country and Converted does not make sense\n   \n*  How 'Specialization' is related with Converted - Imapct of Specialization on Converted is similar across all the provided specializations.So the data is not very conclusive\n   \n*  How 'What is your current occupation' is related with Converted - While the people who are unemployed do not have a high Conversion rate, the working professionals have high conversion rate.\n   \n*  How 'What matters most to you in choosing a course' is related with Converted - when purpose of What matters most to you in choosing a course is Better Carrier Prospects, we have lot of people who are getting converted\n     ","5142f16b":"The next column - What matters most to you in choosing a course","3f0dbf41":"* How 'Do Not Email' is related with Converted - It seems that people who have received email are more likely to get converted in comparision with the ones who do not receive email.\n      \n*  How 'Lead Origin' is related with Converted - If the Lead Origin is Lead Ad Form, then the lead is highly likely to get converted. \n      \n*  How 'Lead Source' is related with Converted - There is a 30-40% conversion rate when the Lead Source is\n        * Google\n        * Direct Traffic\n        * Organic Search\n      Almost all the people who have come from the `Reference` or 'Welingak Website' get converted\n      \n* How 'Do Not Call' is related with Converted - There is about 40% conversion of people who receive call from X Education\n    \n*  How 'TotalVisits' is related with Converted - Unlike the expectation, the conversion rate does not seem to be cirectly proportional to the TotalVisits\n    \n*  How 'Last Notable Activity' is related with Converted - If Last Notable Activity is -:\n         *  Modified - chance of getting converted under 30 %\n         *  Email Opened - chance of getting converted is about 60%.\n         *  SMS Sent -  chances of getting converted are pretty high. ","be675a2d":"****This graph adds to the observations made from the pivot table","da71e64e":"# Step 7 - Recommendations\n\n* The final model built is 90.14% accurate on the test data. This means that the model is rightly able to predict the value of the \"Converted\" variable 90 times out of 100 times. \n* It is note worthy that this does not mean this is the conversion ratio because accuracy also includes predicting the negative cases(i.e the lead does not get converted) correctly.\n* The sensitivity of the model is 83.41% for the test data. This means that the model is rightly able to predict the positive cases (i.e when the lead actually gets converted) 83 times out of 100 times\n\n* Since, the final dataframe has been categorized into various levels of probabilities -:\n       * If the organization wants to focus only on hot leads then we will give them the prospect id's of people who come under the\n         probability range (0.8,1) i.e df_hot_leads dataframe, which will 786 people per the model created\n       * If the organization wants all the possible leads which will get converted, we will give them all the records created in final_df, i.e\n         860 records","be51d50b":"*  How 'Search' is related with Converted - Search = No for almost the entire data. And no conclusion can be drawn from it\n   \n*  How 'Magazine' is related with Converted - If Magazine is No, then again people who are getting converted is very high.\n   \n*  How 'Newspaper' is related with Converted - Here also, if it is not in Newspaper article, the chance of converting is high.\n   \n*  How 'What matters most to you in choosing a course' is related with Converted - X Education Forums, Newspaper , Digital Advertisement, they    do not have significant impact over conversion of people because the provided candidates have not referred any of them. \n","7298e19f":"* The column 'Lead Quality' has over 51% missing values. Since this column specifies the intuition of the employee working on the lead, the missing values can be replaced with 'Not sure' because any other value could skew the data in its own favour.\u00b6","e9966629":"Plotting the values, and generating the value counts for the columns - Asymmetrique Activity Index, Asymmetrique Profile Index, Asymmetrique Activity Score, Asymmetrique Profile Score","feaae489":"A few columns are seen to have a lot of null values. For instance, the column How did you hear about X Education has over 78% Null values, LeadProfile column has just under 75% null values, Lead Quality has over 51% null values. While these two columns top the list, Asymmetrique Activity Index, Asymmetrique Profile Index, Asymmetrique Activity Score, Asymmetrique Profile Score follow closely.","124cbed3":"As evident, except for one case, all the outliers have been taken care of. So the data is good to be proceeded with","19b1707d":"# Step 4 - Data Preparation\nChecking and handling the outliers","fc7f9ef7":"The City column has the higest percentage of missing data now","a57a7245":"Since there does not seem to be a lot correlation between each of the columns Asymmetrique Activity Score, Asymmetrique Profile Score, Asymmetrique Activity Index, Asymmetrique Profile Index and the Converted column. Therefore, it makes sense to drop these columns","f999a109":"About 700 records are removed during the treatment of outliers. This amounts to less than 8% of data removal which is not a bad idea in the present scenario"}}