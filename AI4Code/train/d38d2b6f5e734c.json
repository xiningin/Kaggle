{"cell_type":{"6a7e7ccd":"code","6586ba3e":"code","91a5d6e9":"code","4dfde9c4":"code","6263c7c5":"code","9d4ec3fa":"code","61442116":"code","39e45c04":"code","0c58a72a":"code","7c126489":"code","01208a13":"code","57e3a9b0":"code","222ebd9c":"code","1742f7ac":"code","36c066d9":"code","e24dcb00":"code","eda0ba63":"code","5ae13bd2":"markdown","f3d32f3b":"markdown","b00de35b":"markdown"},"source":{"6a7e7ccd":"# imports\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split","6586ba3e":"input_path = Path('\/kaggle\/input\/tabular-playground-series-nov-2021\/')\nlist(input_path.iterdir())","91a5d6e9":"train_df = pd.read_csv(input_path\/'train.csv')\ntest_df = pd.read_csv(input_path\/'test.csv')\nsubmission_df = pd.read_csv(input_path\/'sample_submission.csv')\n\ntrain_df.shape, test_df.shape","4dfde9c4":"submission_df.head(3)","6263c7c5":"train_df.head(3)","9d4ec3fa":"# null values\ntrain_df.isnull().sum().sum(), test_df.isnull().sum().sum()","61442116":"# duplicates check\nlen(train_df) - len(train_df.drop(['id', 'target'], axis=1).drop_duplicates())","39e45c04":"test_id = test_df.loc[:, 'id']\ntrain_target = train_df.loc[:, 'target']\ntrain_df.drop(['id', 'target'], axis=1, inplace=True)","0c58a72a":"train_target_counts = train_target.value_counts()\nlabels = train_target_counts.index\ncounts = train_target_counts.values\n\nplt.bar(labels, counts)\nplt.xticks(labels)\nplt.show()","7c126489":"from xgboost import XGBClassifier\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, auc, roc_auc_score","01208a13":"X_train, X_test, y_train, y_test = train_test_split(train_df, train_target, test_size=0.25, stratify=train_target,\n                                                    shuffle=True, random_state=13)\nX_train.shape, X_test.shape","57e3a9b0":"def objective(trial):\n    \"\"\"\n    Objective function to tune XGBoost classifier\n    \"\"\"\n    params = {\n        'tree_method': 'gpu_hist',\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'eta': trial.suggest_float('eta', 1e-8, 1., log=True),\n        'gamma': trial.suggest_float('gamma', 1e-8, 1., log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 9, step=2),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'alpha': trial.suggest_float('alpha', 1e-8, 1., log=True),\n        'subsample': trial.suggest_float('subsample', 0.2, 1.),\n        'colsample_bytree': trial.suggest_float('colsample_bytree',0.2, 1.),\n        'use_label_encoder': False,\n    }\n    \n    # KFold split\n    skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n    cv_scores = []\n    \n    for train_ix, test_ix in skf.split(X_train, y_train):\n        X_train_k, X_test_k = X_train.iloc[train_ix], X_train.iloc[test_ix]\n        y_train_k, y_test_k = y_train.iloc[train_ix], y_train.iloc[test_ix]\n    \n        booster = XGBClassifier(**params)\n        booster.fit(X_train_k, y_train_k, eval_metric='auc', eval_set=[(X_test_k, y_test_k)], verbose=0, early_stopping_rounds=100)\n        preds = booster.predict_proba(X_test)\n        preds = preds[:, 1]\n        cv_scores.append(preds)\n    cv_score = np.mean(cv_scores, axis=0)\n    return roc_auc_score(y_test, cv_score)    \n    # return cv_score","222ebd9c":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50, timeout=600)","1742f7ac":"study.best_trial, study.best_params","36c066d9":"test_df_preds = []\nskf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\nfor train_ix, test_ix in skf.split(train_df, train_target):\n    X_train, X_test = train_df.iloc[train_ix], train_df.iloc[test_ix]\n    y_train, y_test = train_target.iloc[train_ix], train_target.iloc[test_ix]\n    # xgb model\n    xgb_model = XGBClassifier(**study.best_params, use_label_encoder=False, tree_method='gpu_hist',\n                              objective='binary:logistic', eval_metric='auc')\n    xgb_model.fit(X_train, y_train, verbose=0, eval_set=[(X_test, y_test)], early_stopping_rounds=100)\n    print(xgb_model.best_score)\n    test_preds = xgb_model.predict_proba(test_df.drop('id', axis=1))\n    test_preds = test_preds[:, 1]\n    test_df_preds.append(test_preds)","e24dcb00":"submission_df.target = np.mean(test_df_preds, axis=0)","eda0ba63":"submission_df.to_csv('submission.csv', index=False)","5ae13bd2":"## Model building","f3d32f3b":"## Data sanity check","b00de35b":"## EDA\n\n1. It is said that, all the variables are continuous\n2. Target variable is binary"}}