{"cell_type":{"a70a7417":"code","15e26815":"code","121da322":"code","2af23fc5":"code","d85f1901":"code","8f2e1aa5":"code","51ef470f":"code","41979338":"code","805100ba":"code","63862650":"code","b1d8c8b6":"code","f14240b2":"code","8c5649f6":"code","6dddb60a":"code","3b6fcaab":"code","2faf1b46":"code","95cbb2e5":"code","1dd1baea":"code","b1104dbe":"code","71650ea9":"code","ef70bbf8":"code","11610ddb":"code","bb2d1988":"code","92c11e27":"code","8741934d":"code","afeb1391":"code","b712ccb9":"markdown","36cfcd34":"markdown","ed8e0b9c":"markdown","43873321":"markdown","7264c3b6":"markdown","494682ce":"markdown","9e0b968c":"markdown","d83fa4e3":"markdown"},"source":{"a70a7417":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten,Dropout\nimport seaborn as sns\n","15e26815":"\n# Load the data\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ntrain.head(10)","121da322":"y = train.label\nX = train.loc[:,'pixel0':]\nX.head()","2af23fc5":"# Check the data\nX.isnull().any().describe()","d85f1901":"test.isnull().any().describe()","8f2e1aa5":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","51ef470f":"X= X \/ 255.0\ntest = test \/ 255.0","41979338":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)\n","805100ba":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_train)","63862650":"var_thres.get_support()","b1d8c8b6":"### Finding non constant features\nsum(var_thres.get_support())","f14240b2":"# Lets Find non-constant features \nlen(X.columns[var_thres.get_support()])","8c5649f6":"constant_columns = [column for column in X_train.columns\n                    if column not in X_train.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","6dddb60a":"for column in constant_columns:\n    print(column)","3b6fcaab":"from sklearn.datasets import load_boston","2faf1b46":"X_train.corr()","95cbb2e5":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","1dd1baea":"corr_features = correlation(X_train, 0.7)\nlen(set(corr_features))","b1104dbe":"corr_features","71650ea9":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","ef70bbf8":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","11610ddb":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))","bb2d1988":"from sklearn.feature_selection import SelectKBest","92c11e27":"#No we Will select the  top 5 important features\nsel_ten_cols = SelectKBest(mutual_info_classif, k=10)\nsel_ten_cols.fit(X_train, y_train)\nX_train.columns[sel_ten_cols.get_support()]","8741934d":"## Perform chi2 test\n### chi2 returns 2 values\n### Fscore and the pvalue\nfrom sklearn.feature_selection import chi2\nf_p_values=chi2(X_train,y_train)","afeb1391":"f_p_values","b712ccb9":"## 3.Feature Selection-Information gain - mutual information In Classification Problem Statements","36cfcd34":"Mutual Information MI Estimate mutual information for a discrete target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n\nInshort\n\nA quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n\nThe mutual information between two random variables X and Y can be stated formally as follows:\n\nI(X ; Y) = H(X) \u2013 H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits.","ed8e0b9c":"### 4.Fisher Score- Chisquare  Test For Feature Selection","43873321":"Compute chi-squared stats between each non-negative feature and class.\n\nThis score should be used to evaluate categorical variables in a classification task. This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic variables, so using this function \u201cweeds out\u201d the features that are the most likely to be independent of class and therefore irrelevant for classification. The Chi Square statistic is commonly used for testing relationships between categorical variables.\n\nIt compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories.","7264c3b6":"### 2. Feature Selection- With Correlation\nIn this step we will be removing the features which are highly correlated","494682ce":" # Split training ","9e0b968c":"# Normalize the data","d83fa4e3":"train.drop(corr_features,axis=1)"}}