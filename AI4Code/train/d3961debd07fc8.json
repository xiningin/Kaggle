{"cell_type":{"a6d457c5":"code","48389ff2":"code","1117fa69":"code","164cc1cc":"code","eb874b49":"code","2c64e6e4":"code","7b6608d1":"code","ab64c98e":"code","7facd873":"code","8af4b4e5":"code","96715bfb":"code","d82c5935":"code","303bb77e":"code","391e3d6c":"code","978e65ff":"code","19113218":"code","10f34a4c":"code","30e2440c":"code","f13cee0c":"code","be0150de":"code","97253502":"code","275c8485":"code","754a12fd":"code","d4352e5c":"markdown","f0d136f7":"markdown","15748baf":"markdown","c86156ed":"markdown","dd0d612a":"markdown","74e0d3aa":"markdown","d60fac43":"markdown","31502ac1":"markdown","a7148c5d":"markdown"},"source":{"a6d457c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48389ff2":"train = pd.read_csv('\/kaggle\/input\/novartis\/Train.csv')\ntrain.head(5)","1117fa69":"import datetime\n#train['month'] = pd.DatetimeIndex(train['DATE']).month\ntrain['year']  = pd.DatetimeIndex(train['DATE']).year","164cc1cc":"train['MULTIPLE_OFFENSE1']=train['MULTIPLE_OFFENSE']","eb874b49":"train = train.drop(['MULTIPLE_OFFENSE'],axis=1)","2c64e6e4":"train.head(4)","7b6608d1":"X = train.iloc[: ,2:-1].values\ny = train.iloc[: , -1].values\nX","ab64c98e":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\nimputer = imputer.fit(X)\nX = imputer.transform(X)","7facd873":"y","8af4b4e5":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","96715bfb":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)","d82c5935":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=1,\n                             n_estimators=190,\n                             criterion='gini',\n                            max_depth=25,\n                             min_samples_split=2,\n                             min_samples_leaf=1,\n                             bootstrap=True,\n                            n_jobs=-1,verbose=False)\n\nclf.fit(X,y)","303bb77e":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier()\nclassifier_xgb.fit(X,y)\n","391e3d6c":"#clf.score(X_test,y_test)","978e65ff":"test = pd.read_csv('\/kaggle\/input\/novartis\/Test.csv')\ntest.head()","19113218":"import datetime\n#test['month'] = pd.DatetimeIndex(test['DATE']).month\ntest['year']  = pd.DatetimeIndex(test['DATE']).year","10f34a4c":"test1 = test.iloc[:,2:]\ntest1.head()","30e2440c":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\nimputer = imputer.fit(test1)\ntest1 = imputer.transform(test1)","f13cee0c":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\ntest1 = sc_X.fit_transform(test1)","be0150de":"prediction = classifier_xgb.predict(test1)\nprediction","97253502":"test.head(2)","275c8485":"submission = pd.DataFrame({'INCIDENT_ID':test['INCIDENT_ID'],'MULTIPLE_OFFENSE':prediction})\nsubmission.head()","754a12fd":"filename = 'Novartis_submission_21.csv'\nsubmission.to_csv(filename, index = False)\nprint('saved file:',filename)","d4352e5c":"# **Novartis Data science challenge :**\n\n*To predict whether the server is hacked or not.*","f0d136f7":"# Random Forest Classifier","15748baf":"# Loading training dataset","c86156ed":"# XGB classifier from xgboost","dd0d612a":"# Importing python datetime library","74e0d3aa":"# Loading test data","d60fac43":"# Applying StandardScaler method of sklearn.preprocessing library to apply feature scaling","31502ac1":"# Applying SimpleImputer method of sklearn library to handle missing value","a7148c5d":"# Approach That I took:\n\n1. Initially i tried Logistic Regression and DecisionTress Classifier but didn't get high accuracy\n   then tried RandomForest Classifier and acheived acuuracy of 94%\n2. Then after adding month and year as features acheived accuracy of 97.4%\n3. Atlast i tried xgboost and acheived final accuracy of 99.15%"}}