{"cell_type":{"8ede51ab":"code","8eaa418b":"code","bd090c1c":"code","dcd789ab":"code","ef99214c":"code","1dbf8922":"code","fa28c93f":"code","112b80f3":"code","2f149881":"code","6a85ec0a":"code","d0d40114":"code","f0d512f0":"code","b1883e21":"code","29c02cdb":"code","ce971211":"code","aeb871ac":"code","17cc79fe":"code","02cdb073":"code","f60eba86":"code","9a64a8a2":"code","f208a011":"code","3a518fef":"code","19b5e77e":"code","8994ad9b":"code","7c5558b3":"code","fb8656f2":"code","c7e5e378":"markdown","6ffe905d":"markdown","61386de3":"markdown","2732c61f":"markdown","dde508ed":"markdown","09416a97":"markdown","41e988bd":"markdown","68bbea00":"markdown","4e1692ac":"markdown","6e937890":"markdown","3c524407":"markdown","4e805750":"markdown","210cf0ca":"markdown","d21aae6d":"markdown","93ffacfd":"markdown","4f688205":"markdown"},"source":{"8ede51ab":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n\n%matplotlib inline","8eaa418b":"data = pd.read_csv('..\/input\/churndata\/churndata_processed.csv')\ndata","bd090c1c":"data.columns","dcd789ab":"for x in data.columns:\n    print(x, len(data[x].unique()))","ef99214c":"data.dtypes","1dbf8922":"data.corr()","fa28c93f":"fig, ax = plt.subplots(figsize=(15,10)) \nsns.heatmap(data.corr())","112b80f3":"target='churn_value'\ndata[target].value_counts()","2f149881":"data[target].value_counts(normalize=True)","6a85ec0a":"from sklearn.model_selection import StratifiedShuffleSplit\n\n\nfeature_cols = [x for x in data.columns if x != target]\n\n\n# Split the data into two parts with 1500 points in the test data\n# This creates a generator\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1500, random_state=42)\n\n# Get the index values from the generator\ntrain_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data[target]))\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, target]\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, target]","d0d40114":"y_train.value_counts(normalize=True)","f0d512f0":"y_test.value_counts(normalize=True)","b1883e21":"# Suppress warnings about too few trees from the early models\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","29c02cdb":"from sklearn.ensemble import RandomForestClassifier\n\n\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\n\noob_list = list()\n\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    \n    # Use this to set the number of trees\n    RF.set_params(n_estimators=n_trees)\n\n    # Fit the model\n    RF.fit(X_train, y_train)\n\n    # Get the oob error\n    oob_error = 1 - RF.oob_score_\n    \n    # Store it\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n\nrf_oob_df","ce971211":"sns.set_context('talk')\nsns.set_style('white')\n\nax = rf_oob_df.plot(legend=False, marker='o', figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error');","aeb871ac":"from sklearn.ensemble import ExtraTreesClassifier\n\nEF = ExtraTreesClassifier(oob_score=True, \n                          random_state=42, \n                          warm_start=True,\n                          bootstrap=True,\n                          n_jobs=-1)\n\noob_list = list()\n\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    \n    # Use this to set the number of trees\n    EF.set_params(n_estimators=n_trees)\n    EF.fit(X_train, y_train)\n\n    # oob error\n    oob_error = 1 - EF.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\net_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n\net_oob_df","17cc79fe":"oob_df = pd.concat([rf_oob_df.rename(columns={'oob':'RandomForest'}),\n                    et_oob_df.rename(columns={'oob':'ExtraTrees'})], axis=1)\n\noob_df","02cdb073":"sns.set_context('talk')\nsns.set_style('white')\n\nax = oob_df.plot(marker='o', figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error');","f60eba86":"from sklearn.ensemble import RandomForestClassifier\n\nRF_200 = RandomForestClassifier(n_estimators=200\n          ,oob_score=True \n          ,random_state=42\n          ,n_jobs=-1)\n\nRF_200.fit(X_train,y_train)\noob_error200 = 1 - RF_200.oob_score_\noob_error200","9a64a8a2":"y_pred=RF_200.predict(X_test)","f208a011":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, roc_auc_score\n\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\nscore_df = [['accuracy', accuracy_score(y_test, y_pred)],\n            ['precision', precision_score(y_test, y_pred)],\n            ['recall', recall_score(y_test, y_pred)],\n            ['f1', f1_score(y_test, y_pred)],\n            ['auc', roc_auc_score(y_test, y_pred)]] \n\nscore_df=pd.DataFrame(score_df,columns=['Error metric','Measurement']).set_index('Error metric')\nscore_df","3a518fef":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","19b5e77e":"cm_RF200=confusion_matrix(y_test,y_pred)\ncm_RF200","8994ad9b":"disp = ConfusionMatrixDisplay(confusion_matrix=cm_RF200, display_labels=RF_200.classes_)\ndisp.plot(cmap='Blues')","7c5558b3":"from sklearn.metrics import roc_curve, precision_recall_curve\nsns.set_context('talk')\n\nfig, axList = plt.subplots(ncols=2)\nfig.set_size_inches(16, 8)\n\n# Get the probabilities for each of the two categories\ny_prob = RF_200.predict_proba(X_test)\n\n# Plot the ROC-AUC curve\nax = axList[0]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\nax.plot(fpr, tpr, linewidth=5)\n# It is customary to draw a diagonal dotted line in ROC plots.\n# This is to indicate completely random prediction. Deviation from this\n# dotted line towards the upper left corner signifies the power of the model.\nax.plot([0, 1], [0, 1], ls='--', color='black', lw=.3)\nax.set(xlabel='False Positive Rate',\n       ylabel='True Positive Rate',\n       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n       title='ROC curve')\nax.grid(True)\n\n# Plot the precision-recall curve\nax = axList[1]\n\nprecision, recall, _ = precision_recall_curve(y_test, y_prob[:,1])\nax.plot(recall, precision, linewidth=5)\nax.set(xlabel='Recall', ylabel='Precision',\n       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n       title='Precision-Recall curve')\nax.grid(True)\n\nplt.tight_layout()","fb8656f2":"feat=pd.DataFrame(RF_200.feature_importances_,index=feature_cols, columns=['Importance']).sort_values(by='Importance',ascending=False)\nax=feat.plot(kind='bar', figsize=(16,6))\nax.set(ylabel='Feature Importance')\nax.set(xlabel='Features')","c7e5e378":"Let's see how many unique values we have in each column of our dataframe:","6ffe905d":"### Now let's select the RandomForest model for 200 trees and calculate error metrics and confusion matrix on the test data set:","61386de3":"The feature importances plot. Satisfaction is the biggest predictor of customer churn.","2732c61f":"#  ExtraTreesClassifier\n### After building this model, we are going to compare out-of-bag errors for the two different types of models.","dde508ed":"Let's see how in the proportion of the classes in our label:","09416a97":"It would be better if we create a dataframe with both oob-errors as columns in order to be easier to plot both lines.","41e988bd":"# The ROC-AUC and precision-recall curves.","68bbea00":"Let's read our csv file, take into account the file we will use today was generated from a previous project in which we worked with KNN.","4e1692ac":"# Random Forest:","6e937890":"### Let's fit random forest models with a range of tree numbers, then evaluate the out-of-bag error for each of these.","3c524407":"# Splitting our dataset:","4e805750":"As we can see in the figure above **RandomForest error is lower**, therefore is the best model for our case of study. We could select number of trees = 200 as the model which gave us the lowest oob-error and compute its corresponding error metrics. ","210cf0ca":"### Hi, welcome to my project!, today we will build random forest and extra trees classifiers to predict customer churn. ","d21aae6d":"Note: Since the only thing changing in our model is the number of trees, the **warm_start** flag can be used so that the model just adds more trees to the existing model each time. Thus we should have to use the **set_params** method to update the number of trees.","93ffacfd":"# Computing error metrics for n=200:","4f688205":"Now let's plot the resulting oob errors as a function of the number of trees."}}