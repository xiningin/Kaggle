{"cell_type":{"ffaf4fcd":"code","d8a1455b":"code","6852ab0c":"code","8f5690c0":"code","703d9903":"code","fcf30664":"code","4df43557":"code","4bebc66f":"code","d0f05673":"code","2b268139":"code","4b403cb6":"code","c21f0aff":"code","7179b48d":"code","135446ef":"code","1980f9be":"code","9f166e54":"code","65a88447":"code","df62a80a":"code","e7ce56e7":"code","e0cb718d":"code","778eb38f":"code","c8b4a109":"code","64b34884":"code","1f70dabd":"code","030eaa7f":"code","15343cf2":"code","3e527334":"code","8bf83f16":"code","f30a4094":"code","b7e2f803":"code","c50202a6":"code","7e8a8875":"code","7adfe976":"code","63225770":"code","d33a0009":"code","1da2fff1":"markdown","a9a7a371":"markdown","9c1866f9":"markdown","807ad65f":"markdown","f077bbf5":"markdown","a7041470":"markdown","7db06267":"markdown","2afcd221":"markdown","3d5acd00":"markdown","3f7e5027":"markdown","5519deb6":"markdown","46bb96f6":"markdown","3c2c1a04":"markdown"},"source":{"ffaf4fcd":"from wordcloud import WordCloud\nfrom keras.models import Sequential\nfrom keras import layers\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nimport plotly.express as px\nfrom itertools import chain\nfrom collections import Counter","d8a1455b":"fake=pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal=pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","6852ab0c":"fake['target']=0\nreal['target']=1\ndf=pd.concat([fake,real])\ndf.head()","8f5690c0":"df['original'] = df['text'] + ' ' + df['title']\ndf.head()","703d9903":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.countplot(df['target'])","fcf30664":"plt.figure(figsize=(15,7))\nsns.heatmap(df.isnull());","4df43557":"plt.figure(figsize=(15,7))\nlabels=['fake news','real news']\ncolors = [\"SkyBlue\",\"PeachPuff\"]\nplt.pie(df['target'].value_counts(),labels=labels,colors=colors,\n        autopct='%1.2f%%', shadow=True, startangle=140) \nplt.show()","4bebc66f":"plt.figure(figsize=(15,7))\nsns.countplot(fake['subject'])","d0f05673":"plt.figure(figsize=(15,7))\nsns.countplot(real['subject'])\nplt.show()","2b268139":"plt.figure(figsize=(15,7))\nsns.countplot(df['subject'])","4b403cb6":"plt.figure(figsize=(15,15))\nlabels=['News','politics','Government News','left-news','US_News','Middle-east','politicsNews','worldnews']\ncolors = [\"SkyBlue\",\"PeachPuff\",'tomato','gray','lightyellow','pink']\nplt.pie(df['subject'].value_counts(),labels=labels,colors=colors,\n        autopct='%1.f%%', shadow=True, startangle=140) \nplt.show()","c21f0aff":"wordcloud = WordCloud(background_color=\"black\").generate(str(fake['title']))\nplt.figure(figsize=(15,7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","7179b48d":"wordcloud = WordCloud(background_color=\"black\").generate(str(real['title']))\nplt.figure(figsize=(15,7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","135446ef":"real=real.groupby(['date'])['target'].count()\nreal=pd.DataFrame(real)\nfig = px.line(real)\nfig.show()","1980f9be":"fake=fake.groupby(['date'])['target'].count()\nfake=pd.DataFrame(fake)\nfig = px.line(fake)\nfig.show()","9f166e54":"def data_preprocessing(data):\n    ###Here we remove all trash in our text\n    prep1 = data.lower()\n    prep2 = re.sub('https?:\/\/\\S+|www\\.\\S+', '', data)\n    prep3 = re.sub('\\\\W', ' ', data)\n    prep4 = re.sub('\\n', '', data)\n    prep5 = re.sub(' +', ' ', data)\n    prep6 = re.sub('^ ', '', data)\n    prep7 = re.sub(' $', '', data)\n    return data","65a88447":"df['original'].apply(data_preprocessing)","df62a80a":"split_words = df['original'].str.split()\nlist_split_words = list(chain(*split_words))\n\ncount_word= Counter(list_split_words)\ndf_ = pd.DataFrame(data=count_word, index=['count'])\nmost_frequency_words = df_.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\n\nplt.figure(figsize=(15,10))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=most_frequency_words)\nplt.xticks(rotation=90)","e7ce56e7":"print(stopwords.words('english'))","e0cb718d":"stop = set(stopwords.words('english'))\ndef remove_stopwords(data):\n    words = [word for word in data if word not in stop]\n    words= \"\".join(words).split()\n    words= [words.lower() for words in data.split()]\n    return words    ","778eb38f":"df['original'].apply(remove_stopwords)","c8b4a109":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef lemmatization(text):\n    lemmas = []\n    for word in text.split():\n        lemmas.append(lemmatizer.lemmatize(word))\n    return \" \".join(lemmas)","64b34884":"df['original'].apply(lemmatization)","1f70dabd":"x = df['original'].values\ny= df['target'].values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_to_index = tokenizer.word_index\nx = tokenizer.texts_to_sequences(x)\n\nvocab_size = len(word_to_index)+1\nmax_length = 10000\nembedding_dim = 16\nx = pad_sequences(x, maxlen=max_length)\n\n","030eaa7f":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=42)","15343cf2":"model=Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.MaxPooling1D(5))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])","3e527334":"history = model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),batch_size=128)","8bf83f16":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()","f30a4094":"predicted_value = model.predict(X_test)\naccuracy_value = roc_auc_score(y_test, predicted_value)\nprint(accuracy_value)","b7e2f803":"prediction = []\nfor i in range(len(predicted_value)):\n    if predicted_value[i].item() > 0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)\ncm = confusion_matrix(list(y_test), prediction)\nplt.figure(figsize = (10, 10))\nsns.heatmap(cm, annot = True,fmt='g')","c50202a6":"model_2 = Sequential()\nmodel_2.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel_2.add(layers.Conv1D(32, 5, activation='relu'))\nmodel_2.add(layers.MaxPooling1D(3))\nmodel_2.add(layers.Conv1D(32, 5, activation='relu'))\nmodel_2.add(layers.LSTM(64))\nmodel_2.add(layers.Dense(1,activation='sigmoid'))\nmodel_2.summary()\nmodel_2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])","7e8a8875":"history_2 = model_2.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),batch_size=128)","7adfe976":"acc = history_2.history['acc']\nval_acc = history_2.history['val_acc']\nloss = history_2.history['loss']\nval_loss = history_2.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()","63225770":"predicted_value = model_2.predict(X_test)\naccuracy_value = roc_auc_score(y_test, predicted_value)\nprint(accuracy_value)","d33a0009":"prediction = []\nfor i in range(len(predicted_value)):\n    if predicted_value[i].item() > 0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)\ncm = confusion_matrix(list(y_test), prediction)\nplt.figure(figsize = (10, 10))\nsns.heatmap(cm, annot = True,fmt='g')","1da2fff1":"Usually, texts contain different grammatical forms of the same word or the same root words. Lemmatization is used to reduce the encountered word forms to one (normal form).\n\nLemmatization is a process that uses morphological analysis and vocabulary to reduce a word to its canonical form-a lemma.","a9a7a371":"Let's look at the distribution of our data, the missing values (there are no missing values), what the news outlets write about (nothing surprising), and the ratio of each subject category.","9c1866f9":"# Introduction and import main libraries","807ad65f":"# Visualization part","f077bbf5":"# Building models ","a7041470":"Now I want to tell you about Zipf's law and why we need to remove stop words. Zipf's law is an empirical law of the frequency distribution of natural language words: if all the words of a language are ordered in descending order of their frequency of use, then the frequency of the nth word in such a list will be approximately inversely proportional to its ordinal number n. For example, the second-most-used word is about twice as common as the first, the third \u2014 three times less common than the first, and so on.","7db06267":"Hello everyone. I started learning  NLP from Lane Hobson's book \"Natural Language Processing in Action\", and \"Deep Learning with Python [2017] Francois Chollet\". So I decided to disassemble this dataset.","2afcd221":"If you liked this work, you can upvote. Advice for improvement are also welcome :)","3d5acd00":"As advised by Fran\u00e7ois Chollet, I used convolutional neural networks for text recognition. The first model uses only them, and the second one looks more interesting: one of the strategies to combine the speed and ease of convolutional networks with the sensitivity to the order of recurrent networks is to use a one-dimensional convolutional network to pre-process data before transmitting it to the recurrent network. The convolutional part will turn a long input sequence into a shorter sequence of high-level features (reducing its time-solution). And then the sequence of selected features is fed to the input of the recurrent part of the network. This technique is not often found in scientific articles and practical applications, perhaps because it is little known. However, it has a fairly high efficiency and deserves a wider distribution.","3f7e5027":"On the next step, we need to process the dataset. At this stage, first of all, you need to bring all the characters of the text to lowercase, remove punctuation marks, various non-letter characters and numbers","5519deb6":"We will also analyze the time series: mark the date on the X-axis, and group the number of news items published on a particular day on the y-axis. In my opinion, most of the fakes were published during the 2016 election.","46bb96f6":"Convolutional neural networks are used to expand the receptive field (the perception spot). That is, this is done in order to handle a broader context (longer patterns).","3c2c1a04":"# Data preprocessing"}}