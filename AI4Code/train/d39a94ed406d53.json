{"cell_type":{"bb275bd0":"code","e72bf457":"code","c595fb9f":"code","ccb81049":"code","cfe43ad0":"code","fe49a413":"code","e8fb031d":"code","898fbdc5":"code","81f70ab6":"code","66129b7e":"code","66922aef":"code","9782e7fa":"code","d2f667ba":"code","fa2ab679":"code","13b98723":"code","993ba456":"code","ea68a209":"code","278fd4b2":"code","7427136f":"code","2497a653":"code","3749e5d6":"code","8a5a56aa":"code","a2300098":"code","98004bbd":"code","3e2ea0ee":"code","788a6377":"code","883c2181":"code","5539e036":"code","7576cf0d":"code","fc964a9e":"code","2c7d9727":"code","b86f4e48":"code","564852e6":"code","4172f875":"code","8f99163e":"code","a026ae7e":"code","65fe974f":"code","32188749":"code","6011746b":"code","a9f47aa1":"code","8913e047":"code","996ae693":"code","c33fc4ac":"code","0fba0645":"code","47affcee":"code","26dbbdc5":"code","49d9c4b2":"code","e1630035":"code","6a5f4ca0":"code","4a1a8e5f":"code","2388f9e4":"code","26df58cd":"code","88a085eb":"code","bd49fdea":"code","12dedfa0":"code","4f7e1c6b":"code","8f459ed9":"code","468cd422":"markdown","92bc429d":"markdown","3e0ee515":"markdown","b7443436":"markdown","c35b0431":"markdown","22d9e47b":"markdown","7096aeda":"markdown","b1cdd204":"markdown","9324f666":"markdown","e9bed16c":"markdown","5e2e1c19":"markdown","ea7c9ce8":"markdown","aa5fcb94":"markdown","01009f3e":"markdown","9d9d9317":"markdown","473e9ba9":"markdown","27be1256":"markdown","8d5c8d67":"markdown","d835d7b2":"markdown","7d931e58":"markdown","e6e90af4":"markdown","9159c59a":"markdown","d55bbb61":"markdown","65a0cb3b":"markdown","de3800af":"markdown","92264b84":"markdown","7fab3bb6":"markdown","27afd003":"markdown","0e0648ec":"markdown","57911667":"markdown","5f003dad":"markdown","1d4b8f56":"markdown","91195ed9":"markdown","09be85c5":"markdown","68877c7f":"markdown","14b9670f":"markdown","9e29a941":"markdown","953563de":"markdown","0cb0140e":"markdown","1a4baf27":"markdown","c003cffc":"markdown","22e1c9a1":"markdown","b44b6381":"markdown","d6c89abc":"markdown","8f0ce8a2":"markdown","d1f53139":"markdown","cfd5bef0":"markdown"},"source":{"bb275bd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e72bf457":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter ","c595fb9f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","ccb81049":"# set display options\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',500)","cfe43ad0":"train.head()","fe49a413":"test.head()","e8fb031d":"train.shape, test.shape","898fbdc5":"train.select_dtypes(include=['object']).shape[1], train.select_dtypes(include=['float','int']).shape[1]","81f70ab6":" plt.subplots(figsize=(20,4))\nmissing_values = train.isna().sum().sort_values(ascending=False).plot(kind='bar')\nplt.ylabel('Missing Value Count')\nplt.tight_layout()","66129b7e":"# define a function for displaying the price in thousands and with the dollar symbol\ndef thousands(x, pos):\n    'The two args are the value and tick position'\n    return '$%1.0fK' % (x * 1e-3)","66922aef":"# plot the distributions of the sale price \n\nfrom scipy.stats import norm\n\nfig = plt.figure(figsize=(20,6))\n\nplt.subplot(1, 2, 1)\nax1 = sns.boxplot(x = train.SalePrice)\nformatter = FuncFormatter(thousands)\nax1.get_xaxis().set_major_formatter(formatter)\nplt.xlabel('Sale Price')\nplt.title('Sale Price - Train Data')\n\nplt.subplot(1, 2, 2)\nax2 = sns.distplot(train.SalePrice,fit=norm)\nformatter = FuncFormatter(thousands)\nax2.get_xaxis().set_major_formatter(formatter)\nplt.xlabel('Sale Price')\nplt.title('Sale Price - Train Data')\nplt.tight_layout()","9782e7fa":"fig,ax = plt.subplots(figsize=(6,4))\nax = sns.distplot(np.log(train['SalePrice']),fit=norm)\nplt.xlabel('Log (Sale Price)')\nplt.tight_layout()","d2f667ba":"# get the numercical columns\nnum_cols = train.select_dtypes(include=['float','int']).columns\nnum_cols","fa2ab679":"# plot the correlaton between sale price and other numerical variables\nplt.figure(figsize=(15,8))\ntrain.corr()['SalePrice'].sort_values(ascending = False).plot(kind='bar')\nplt.title('Sales Price Correlation Plot')\nplt.axhline(y=0.6, color='r', linestyle='-')\nplt.tight_layout()","13b98723":"fig = plt.figure(figsize=(18,8))\n\nplt.subplot(2, 3, 1)\nax1 = sns.regplot(x =train.OverallQual,y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('Overall Quality')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs Overall Quality')\nax1.get_yaxis().set_major_formatter(formatter)\n\nplt.subplot(2, 3, 2)\nax2 = sns.regplot(x =train.GrLivArea,y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('Greater Livng Area (SqFt)')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs Greater Living Area')\nax2.get_yaxis().set_major_formatter(formatter)\n\nplt.subplot(2, 3, 3)\nax3 = sns.regplot(x =train.GarageArea,y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('Garage Area (SqFt)')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs Garage Area')\nax3.get_yaxis().set_major_formatter(formatter)\n\nplt.subplot(2, 3, 4)\nax4 = sns.regplot(x =train.TotalBsmtSF,y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('Basement Area (SqFt)')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs Basement Area')\nax4.get_yaxis().set_major_formatter(formatter)\n\nplt.subplot(2, 3, 5)\nax5 = sns.regplot(x =train['1stFlrSF'],y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('1st Floor Area (SqFt)')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs 1st Floor Area')\nax5.get_yaxis().set_major_formatter(formatter)\n\nplt.subplot(2, 3, 6)\nax6 = sns.regplot(x =train.OverallCond,y=train.SalePrice, fit_reg=True)\nplt.tight_layout()\nplt.xlabel('Overall Condition')\nplt.ylabel('Sale Price')\nplt.title('Sale Price vs Overall Condition')\nax6.get_yaxis().set_major_formatter(formatter)","993ba456":"fig,ax = plt.subplots(figsize=(30, 15))\nax = sns.heatmap(train[num_cols].corr(), annot=True, linewidths=.5, fmt= '.1f')\nplt.xticks(fontsize=16, fontweight='bold',rotation=90)\nplt.yticks(fontsize=16, fontweight='bold',rotation=0)\nplt.tight_layout()","ea68a209":"# create a list of columns to remove\ncols_to_remove = ['Id','GarageCars','GarageYrBlt','TotRmsAbvGrd']","278fd4b2":"# create a list of categorical features\ncat_feats = list(train.select_dtypes(include=['object','category']).columns)","7427136f":"cat_feats.remove('Alley')\ncat_feats.remove('Fence')\ncat_feats.remove('PoolQC')\ncat_feats.remove('MiscFeature')\ncat_feats.remove('FireplaceQu')","2497a653":"# box plots for categorical features \nfig = plt.figure(figsize=(18,52))\ncolors = sns.color_palette(n_colors=(38))\nn = 1\n\nfor i in range(0,len(cat_feats) - 1):\n    plt.subplot(13, 3, n)\n    ax = sns.boxplot(x='SalePrice',y=cat_feats[i],data=train,color=colors[n])\n    formatter = FuncFormatter(thousands)\n    ax.get_xaxis().set_major_formatter(formatter)\n    n += 1\nplt.tight_layout()","3749e5d6":"# Label encode categiorical columns with levels\n# Alley\nalley_dict = {'Grvl':1,'Pave':2}\ntrain['Alley'] = train['Alley'].map(alley_dict).fillna(0)\ntest['Alley'] = test['Alley'].map(alley_dict).fillna(0)\n\n# LotShape\nlot_shape_dict = {'Reg':4, 'IR1':3,'IR2':2,'IR3':1}\ntrain['LotShape'] = train['LotShape'].map(lot_shape_dict).fillna(0)\ntest['LotShape'] = test['LotShape'].map(lot_shape_dict).fillna(0)\n\n# LandContour\nland_contour_dict = {'Lvl1':4,'Bnk':3,'HLS':2,'Low':1}\ntrain['LandContour'] =  train['LandContour'].map(land_contour_dict).fillna(0)\ntest['LandContour'] =  test['LandContour'].map(land_contour_dict).fillna(0)\n\n# Utilities\nutilities_dict = {'ELO':1,'NoSeWa':2,'NoSewr':3,'AllPub':4}\ntrain['Utilities'] = train['Utilities'].map(utilities_dict).fillna(0)\ntest['Utilities'] = test['Utilities'].map(utilities_dict).fillna(0)\n\n# LandSlope\nland_slope_dict = {'Gtl':3,'Mod':2,'Sev':1}\ntrain['LandSlope'] = train['LandSlope'].map(land_slope_dict).fillna(0)\ntest['LandSlope'] = test['LandSlope'].map(land_slope_dict).fillna(0)\n\n# External Qual\next_qual = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1}\ntrain['ExterQual'] = train['ExterQual'].map(ext_qual).fillna(0)\ntest['ExterQual'] = test['ExterQual'].map(ext_qual).fillna(0)\n\n# External Cond\ntrain['ExterCond'] = train['ExterCond'].map(ext_qual).fillna(0)\ntest['ExterCond'] = test['ExterCond'].map(ext_qual).fillna(0)\n\n# BsmtQual\ntrain['BsmtQual'] = train['BsmtQual'].map(ext_qual).fillna(0)\ntest['BsmtQual'] = test['BsmtQual'].map(ext_qual).fillna(0)\n\n# BsmtCond\ntrain['BsmtCond'] = train['BsmtCond'].map(ext_qual).fillna(0)\ntest['BsmtCond'] = test['BsmtCond'].map(ext_qual).fillna(0)\n\n# BmstExposure\nbsmt_expo_dict = {'Gd':4,'Av':3,'Mn':2,'No':1}\ntrain['BsmtExposure'] = train['BsmtExposure'].map(bsmt_expo_dict).fillna(0)\ntest['BsmtExposure'] = test['BsmtExposure'].map(bsmt_expo_dict).fillna(0)\n\n# BsmtFinType1\nbsmt_fin_1 = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1}\ntrain['BsmtFinType1'] = train['BsmtFinType1'].map(bsmt_fin_1).fillna(0)\ntest['BsmtFinType1'] = test['BsmtFinType1'].map(bsmt_fin_1).fillna(0)\n\n# BsmtFinType2\ntrain['BsmtFinType2'] = train['BsmtFinType2'].map(bsmt_fin_1).fillna(0)\ntest['BsmtFinType2'] = test['BsmtFinType2'].map(bsmt_fin_1).fillna(0)\n\n# HeatingQC\ntrain['HeatingQC'] = train['HeatingQC'].map(ext_qual).fillna(0)\ntest['HeatingQC'] = test['HeatingQC'].map(ext_qual).fillna(0)\n\n# kitchenQual\ntrain['KitchenQual'] = train['KitchenQual'].map(ext_qual).fillna(0)\ntest['KitchenQual'] = test['KitchenQual'].map(ext_qual).fillna(0)\n\n# Functional\nfunctional_dict = {'Typ':8,'Min1':7,'Min2':6,'Mod':5,'Maj1':4,'Maj2':3,'Sev':2,'Sal':1}\ntrain['Functional'] = train['Functional'].map(functional_dict).fillna(0)\ntest['Functional'] = test['Functional'].map(functional_dict).fillna(0)\n\n#FirePlaceQu\ntrain['FireplaceQu'] = train['FireplaceQu'].map(ext_qual).fillna(0)\ntest['FireplaceQu'] = test['FireplaceQu'].map(ext_qual).fillna(0)\n\n# GarageFinish\ngarage_dict = {'Fin':3,'Rfn':2,'Unf':1}\ntrain['GarageFinish'] = train['GarageFinish'].map(garage_dict).fillna(0)\ntest['GarageFinish'] = test['GarageFinish'].map(garage_dict).fillna(0)\n\n# GarageQual\ntrain['GarageQual'] = train['GarageQual'].map(ext_qual).fillna(0)\ntest['GarageQual'] = test['GarageQual'].map(ext_qual).fillna(0)\n\n# GarageCond\ntrain['GarageCond'] = train['GarageCond'].map(ext_qual).fillna(0)\ntest['GarageCond'] = test['GarageCond'].map(ext_qual).fillna(0)\n\n# Paved Drive\npaved_drive_dict = {'Y':3,'P':2,'N':1}\ntrain['PavedDrive'] = train['PavedDrive'].map(paved_drive_dict).fillna(0)\ntest['PavedDrive'] = test['PavedDrive'].map(paved_drive_dict).fillna(0)\n\n# PoolQC\ntrain['PoolQC'] = train['PoolQC'].map(ext_qual).fillna(0)\ntest['PoolQC'] = test['PoolQC'].map(ext_qual).fillna(0)\n\n# Fence\nfence_dict = {'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1}\ntrain['Fence'] = train['Fence'].map(fence_dict).fillna(0)\ntest['Fence'] = test['Fence'].map(fence_dict).fillna(0)","8a5a56aa":"# fill Lot Frontage with mode values. \nlotFrontage_missing = train['LotFrontage'].mode()[0]\ntrain['LotFrontage'] = train['LotFrontage'].fillna(lotFrontage_missing)\ntest['LotFrontage'] = test['LotFrontage'].fillna(lotFrontage_missing)\n\n# fill MasVnrType and MasVnrArea\ntrain['MasVnrType'] = train['MasVnrType'].fillna('None') # add None as a category\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(0)\n\ntest['MasVnrType'] = test['MasVnrType'].fillna('None') # add None as a category\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)\n\n# fill missing in electrical with Mixed\ntrain['Electrical'] = train['Electrical'].fillna('Mix')\ntest['Electrical'] = test['Electrical'].fillna('Mix')\n\n# fill missing in Garage Type\ntrain['GarageType'] = train['GarageType'].fillna('None')\ntest['GarageType'] = test['GarageType'].fillna('None')\n\n# fill missing in Misc Feature\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None')\ntest['MiscFeature'] = test['MiscFeature'].fillna('None')","a2300098":"# Convert street and central air to binary  - since there are only 2 values\ntrain['Street'] = np.where(train['Street'] == 'Pave',1,0)\ntest['Street'] = np.where(test['Street'] == 'Pave',1,0)\n\ntrain['CentralAir'] = np.where(train['CentralAir'] == 'Y',1,0)\ntest['CentralAir'] = np.where(test['CentralAir'] == 'Y',1,0)","98004bbd":"target_encode_feats = ['MSSubClass', 'MSZoning','LotConfig','Neighborhood','Condition1','Condition2','BldgType','HouseStyle',\n                       'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','Electrical',\n                       'GarageType','MiscFeature','SaleType','SaleCondition']\n\n# calculate global mean\nglobal_mean = train.SalePrice.mean()\nweight = 100\n\nfor feature in target_encode_feats:\n    agg = train.groupby(feature)['SalePrice'].agg(['mean','count'])\n    means = agg['mean']\n    counts = agg['count']\n    smooth = (counts * means + weight * global_mean)\/(counts + weight)\n    train.loc[:,feature] = train[feature].map(smooth)\n    test.loc[:,feature] = test[feature].map(smooth)","3e2ea0ee":"train['Total_Porch_Area'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']\ntest['Total_Porch_Area'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch']\n\ntrain['Total_Bath'] = train['BsmtFullBath'] + train['BsmtHalfBath'] * 0.5 + train['FullBath'] + train['HalfBath'] * 0.5\ntest['Total_Bath'] = test['BsmtFullBath'] + test['BsmtHalfBath'] * 0.5 + test['FullBath'] + test['HalfBath'] * 0.5\n\ntrain['Build_Sold_Diff'] = train['YrSold'] - train['YearBuilt']\ntest['Build_Sold_Diff'] = test['YrSold'] - test['YearBuilt']","788a6377":"train['Has_Basement'] = np.where(train['TotalBsmtSF'] > 0,1,0)\ntest['Has_Basement'] = np.where(test['TotalBsmtSF'] > 0,1,0)\n\ntrain['Has_Pool'] = np.where(train['PoolArea'] > 0,1,0)\ntest['Has_Pool'] = np.where(test['PoolArea'] > 0,1,0)\n\ntrain['Has_Garage'] = np.where(train['GarageArea'] > 0,1,0)\ntest['Has_Garage'] = np.where(test['GarageArea'] > 0,1,0)\n\ntrain['Has_Fireplace'] = np.where(train['Fireplaces'] > 0,1,0)\ntest['Has_Fireplace'] = np.where(test['Fireplaces'] > 0,1,0)\n\ntrain['Has_1stFloor'] = np.where(train['1stFlrSF'] > 0,1,0)\ntest['Has_1stFloor'] = np.where(test['1stFlrSF'] > 0,1,0)\n\ntrain['Has_2ndFloor'] = np.where(train['2ndFlrSF'] > 0,1,0)\ntest['Has_2ndFloor'] = np.where(test['2ndFlrSF'] > 0,1,0)","883c2181":"num_feats = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\n             'LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch',\n            'PoolArea','MiscVal','Total_Porch_Area','Build_Sold_Diff']\n\nfrom scipy.stats import skew\nskew_dict = {}\n\nfor feats in num_feats:\n    skew_dict[feats] = train[feats].skew()\n\nskew_df = pd.DataFrame(list(skew_dict.items()),columns = ['Feature','Skew'])    \n\nfig,ax = plt.subplots(figsize=(12,6))\nax = sns.barplot(x='Feature',y='Skew',data=skew_df.sort_values(by='Skew',ascending=False), color='b')\nplt.axhline(y=1,color='r')\nplt.title('Skew for Features')\nplt.xticks(rotation=90)\nplt.tight_layout()","5539e036":"num_feats = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','1stFlrSF',\n             'LowQualFinSF','GrLivArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch',\n            'PoolArea','MiscVal','Total_Porch_Area']\n\nfrom scipy import stats\n\nskew_dict = {}\nlambdas = {}\n\nfor feats in num_feats:\n    train[feats] = train[feats] + 1\n    train[feats], fitted_lambda = stats.boxcox(train[feats])\n    lambdas[feats] = fitted_lambda\n\nfor feats in num_feats:\n    skew_dict[feats] = train[feats].skew()\n\nskew_df = pd.DataFrame(list(skew_dict.items()),columns = ['Feature','Skew'])    \n\nfig,ax = plt.subplots(figsize=(12,6))\nax = sns.barplot(x='Feature',y='Skew',data=skew_df.sort_values(by='Skew',ascending=False), color='b')\nplt.axhline(y=1,color='r')\nplt.title('Skew for Features')\nplt.xticks(rotation=90)\nplt.tight_layout()","7576cf0d":"# apply box-cox transformations to the test data\nfor feat in num_feats:\n    test[feat] = test[feat] + 10\n    test[feat] = stats.boxcox(test[feat], lambdas[feat])","fc964a9e":"for feature in target_encode_feats:\n    train[feature] = train[feature]\/1000\n    test[feature] = test[feature]\/1000","2c7d9727":"# convert sale price log\ntrain['SalePrice'] = np.log(train['SalePrice'])","b86f4e48":"cols_to_drop = ['Id','GarageCars','GarageYrBlt','TotRmsAbvGrd','YearRemodAdd','GarageYrBlt','MoSold']\ntrain = train.drop(cols_to_drop,axis=1)\ntest = test.drop(cols_to_drop,axis=1)","564852e6":"# check final dimensions of the data\ntrain.shape, test.shape","4172f875":"# create X and Y  arrays \nX = train.drop(['SalePrice'], axis=1)\ny = train['SalePrice'].values","8f99163e":"# create train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)","a026ae7e":"# Define parameters to tune\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","65fe974f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE","32188749":"num_trees_score = []\nfor n in n_estimators:\n    model = RandomForestRegressor(n_estimators = n)\n    model.fit(X_train,y_train)\n    num_trees_score.append(np.sqrt(MSE(np.exp(y_test), np.exp(model.predict(X_test)))))","6011746b":"fig,ax = plt.subplots(figsize=(6,4))\nsns.lineplot(x=n_estimators, y=num_trees_score)\nplt.xlabel('Number of Trees')\nplt.ylabel('Root Mean Square Error')\nplt.tight_layout()","a9f47aa1":"max_depth_score = []\nfor n in max_depth:\n    model = RandomForestRegressor(max_depth = n)\n    model.fit(X_train,y_train)\n    max_depth_score.append(np.sqrt(MSE(np.exp(y_test), np.exp(model.predict(X_test)))))","8913e047":"fig,ax = plt.subplots(figsize=(6,4))\nsns.lineplot(x=max_depth, y=max_depth_score)\nplt.xlabel('Max Depth')\nplt.ylabel('Root Mean Square Error')\nplt.tight_layout()","996ae693":"min_samples_score = []\nfor n in min_samples_split:\n    model = RandomForestRegressor(min_samples_split = n)\n    model.fit(X_train,y_train)\n    min_samples_score.append(np.sqrt(MSE(np.exp(y_test), np.exp(model.predict(X_test)))))","c33fc4ac":"fig,ax = plt.subplots(figsize=(6,4))\nsns.lineplot(x=min_samples_split, y=min_samples_score)\nplt.xlabel('Min Sample Split')\nplt.ylabel('Root Mean Square Error')\nplt.tight_layout()","0fba0645":"min_samples_leaf_score = []\nfor n in min_samples_leaf:\n    model = RandomForestRegressor(min_samples_leaf = n)\n    model.fit(X_train,y_train)\n    min_samples_leaf_score.append(np.sqrt(MSE(np.exp(y_test), np.exp(model.predict(X_test)))))","47affcee":"fig,ax = plt.subplots(figsize=(6,4))\nsns.lineplot(x=min_samples_leaf, y=min_samples_leaf_score)\nplt.xlabel('Min Samples Leaf')\nplt.ylabel('Root Mean Square Error')\nplt.tight_layout()","26dbbdc5":"rf = RandomForestRegressor()\n\n#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, \n                            #   verbose=2, random_state=42, n_jobs = -1)\n\n#rf_random.fit(X_train, y_train)","49d9c4b2":"#rf_random.best_params_","e1630035":"rf_best = RandomForestRegressor(n_estimators= 800,min_samples_split= 2,min_samples_leaf=1,max_features='sqrt',\n max_depth= 70,bootstrap= False)\n\nrf_best.fit(X_train,y_train)\n\npreds_train = rf_best.predict(X_train)\npreds_test = rf_best.predict(X_test)\n\nprint('Train RMSE: ',np.sqrt(MSE(np.exp(y_train), np.exp(preds_train))))\nprint('Test RMSE: ',np.sqrt(MSE(np.exp(y_test), np.exp(preds_test))))","6a5f4ca0":"from boruta import BorutaPy\n\nfeature_names = X.columns\n\nboruta_selector = BorutaPy(rf_best, n_estimators = 'auto', random_state = 0)\nboruta_selector.fit(np.array(X_train), np.array(y_train))\n\nboruta_ranking = boruta_selector.ranking_\nselected_features = np.array(feature_names)[boruta_ranking <= 2]\nselected_features","4a1a8e5f":"len(selected_features)","2388f9e4":"from sklearn.ensemble import GradientBoostingRegressor\n\n# define gradient boost parameters for tuning \ngb_random_grid = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],'n_estimators':[5,10,50,100,500,1000,2000], 'max_depth':[2,4,6,10]}\n\n# instantiate Gradient Boost Regressor\ngb = GradientBoostingRegressor()\n\n#random search  \ngb_rs =  RandomizedSearchCV(estimator = gb, param_distributions = gb_random_grid, cv = 3, \n                            scoring='neg_root_mean_squared_error',random_state=42, n_jobs = -1)\n\ngb_rs.fit(X_train,y_train)","26df58cd":"gb_rs.best_params_","88a085eb":"best_model = gb_rs.best_estimator_\n\ngb_train_preds = best_model.predict(X_train)\ngb_test_preds = best_model.predict(X_test)\n\nprint('Train RMSE: ',np.sqrt(MSE(np.exp(y_train), np.exp(gb_train_preds))))\nprint('Test RMSE: ',np.sqrt(MSE(np.exp(y_test), np.exp(gb_test_preds))))","bd49fdea":"from xgboost.sklearn import XGBRegressor\n\n# define XGB parameters for tuning \nxgb_random_grid = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],'n_estimators':[5,10,50,100,500,1000,2000], \n                   'max_depth':[2,4,6,10],'min_child_weight':[6,8,10,12],'gamma':[i\/10.0 for i in range(0,5)]}\n\n# instantiate XGBoost Regressor\nxgb = XGBRegressor()\n\n#random search  \nxgb_rs =  RandomizedSearchCV(estimator = xgb, param_distributions = xgb_random_grid, cv = 3, \n                            scoring='neg_root_mean_squared_error',random_state=42, n_jobs = -1)\n\nxgb_rs.fit(X_train,y_train)","12dedfa0":"xgb_rs.best_params_","4f7e1c6b":"xgb_best_model = xgb_rs.best_estimator_\n\nxgb_train_preds = xgb_best_model.predict(X_train)\nxgb_test_preds = xgb_best_model.predict(X_test)\n\nprint('Train RMSE: ',np.sqrt(MSE(np.exp(y_train), np.exp(xgb_train_preds))))\nprint('Test RMSE: ',np.sqrt(MSE(np.exp(y_test), np.exp(xgb_test_preds))))","8f459ed9":"from mlxtend.regressor import StackingRegressor\n\nreg1 = GradientBoostingRegressor(n_estimators = 500, max_depth= 2, learning_rate= 0.05)\nreg_meta = XGBRegressor(n_estimators= 2000,min_child_weight=10,max_depth= 6,learning_rate= 0.1,gamma= 0.1)\nreg2 = RandomForestRegressor(n_estimators= 800,min_samples_split= 2,min_samples_leaf=1,max_features='sqrt',\n max_depth= 70,bootstrap= False)\n\nreg_stack = StackingRegressor(regressors=[reg1,reg2],\n                             meta_regressor = reg_meta,\n                              use_features_in_secondary = True)\n\nreg_stack.fit(X_train, y_train)\n\ntrain_preds = reg_stack.predict(X_train)\ntest_preds = reg_stack.predict(X_test)\n\nprint('Train RMSE: ',np.sqrt(MSE(np.exp(y_train), np.exp(train_preds))))\nprint('Test RMSE: ',np.sqrt(MSE(np.exp(y_test), np.exp(test_preds))))","468cd422":"So what did the fancy correlation analysis tell us.\n\nWe have hardly 6 features that are highly correlated with the sale price (assumng a baseline correlation of 0.6). Features like overall quality, greater living area and garage area are highly correlated as expected. Both the Garagecars and GarageArea are shown in this plot. This is obvious since a 3 car garage will have more area than a 2 car garage. Duh! We will remove the highly correlated independent variables as well from the analysis at a later stage. \n\nThe total basement footage is also highly correlated with the sale price. Wonder if this is because it contributes to the overall area that automatically results in a higher price. Maybe we can further combine the areas to create one single variable. We will deal with those problems later.\n\nThe most suprising of 'em all is the overall condition. There seems to be no correlation or a very slight negative correlation between the overall condition and the sale price. One would assume that a higher rating will yield a higher price. This particular variable requires further exploration. \n\nAs a next step we will explore the individual relationship between the sale price and the the highty correlated variables.","92bc429d":"### 4.5 Model Stacking","3e0ee515":"# 5. Summary \n\nThis was an attempt to set up an end-to-end workflow for the Boston Housing Prices datatset. The following steps were done:\n* Initial EDA of the data to understand the relationships between the dependent and independent variables.\n* Feature Enigneering to add some new varaiables.\n* Feature Transformations such as mean target encoding and box-cox transformations. \n* Build baseline model and study the impact of different hyper parameters on the error.\n* Feature selection from baseline model.\n* Tried model stacking as well, with no major impact.\n\nThis is only the beginning and a work in progress :-) There is a lot more to get done. Better feature engineering and transformations. More hyper-parameter tuning and try to reduce overfitting and improve the error. I will keep working on this.\n\nHope you enjoyed! Bricks and Bouquets are welcome. Upvote if you think this deserves one!\n","b7443436":"### 3.5 Add New Features\n\nWe will add the following numerical features:\n* Total Porch Area \n* Total Bathrooms\n* Time difference between built and sold\n\nWe will add the followinng binary features:\n* Has_Pool\n* Has_Garage\n* Has_FirePlace\n* Has_1stFloor\n* Has_2ndFloor\n* Has_Basement","c35b0431":"### 3.4 Mean Target Encoding - Smoothing means","22d9e47b":"And finally drop the columns not needed, a few we decided to remove earlier","7096aeda":"Before we explore the categorical features, we need to deal with missing values in the following columns:\n* Alley\n* Fence\n* PoolQC\n* MiscFeature\n* FireplaceQu\n\nThese were the features that had the maximum number of missing values. The data descriptions clearly indicate why the valuses are missing. These are features that are simply not available in the house. For example, FirePlaceQu will be populated only if the fireplace is availbale. So it may be enough just to use the fireplace column and drop the Fire place quality column. Similarily, we can conert the MiscFeature column into binary - whether feature is available or not.\n\nSo for now let's remove these columns from the analysis. We can come back and include if absolutely needed. This is a typical iterative process in the machine learning pipeline.","b1cdd204":"Before that, just for kicks lets see what a log transform to the sale price does.","9324f666":"The median value for the sale price is approxiamtely 150K. The IQR is between 120K to 220K. The distribution is right skewed, with few outliers. The maximum price in the train data set is approximately 750K. We will probably scale the data (divide by 100,000) or similar to deal with the outliers. Of course this will not matter for tree based models such as Random Forest and Gradient Boosting. However, the plan is to try linear models and compare the results as well.\n\nNext, we will look at the sales price relationship with the the categorical and numerical features individually. ","e9bed16c":"We do see some strong positive correlations between the identified variables. However, there are some outliers in the data that can cause these strong positive correlations. Next, we will look at the correlations between the independent numerical variables.\n\nA point to note is that some features like the Overall Quality and Overall Cond can be analyzed as categorical variables. Though, these are analyzed as numercial variables here, the findings still remain the same.","5e2e1c19":"We will also transform the columns that were mean encoded. This we will just divide by 1000. \nAnd we will convert the target column - Sale Price to a log scale. ","ea7c9ce8":"Few columns such as PoolQc, MiscFeature, Alley, Fence, and FireplaceQue have a more than 50% missing values and can be dropped from the analysis. Other columns such as LotFrontage, GarageCond etc. have very few missing values and we will make a call on how to impute those values or if we need to drop them. Overall the dataset is well populated with only 22% of the columns with missing data. ","aa5fcb94":"Now we have only 6 features that have a skew more than 1 after the box-cox transformations.","01009f3e":"### 3.1 Label Encode - Categorical Columns with Levels","9d9d9317":"#### 4.1.3 Min Samples Split","473e9ba9":"#### 4.1.1 Number of Trees","27be1256":"# 3. Feature Engineering\n\nThis is one of the most important aspects in the machine learning pipeline. As the age old adage goes - **'Garbage In Garbage Out'.** So it becomes critcal to apply proper transformations etc. to the input data. Let's get cracking!\n\nFollowing are some of the methods used:\n* Transform numerical columns. Though this is not required for tree-based models, I still like scaling them. \n* Few of the categorical columns clearly have levels and will be encoded accordingly\n* One hot encoding for other categorical features without levels and mean encoding for features with high levels\n* Log transform of thee target column - we already saw the benefit during the EDA","8d5c8d67":"#### 4.1.5 Randomized Search CV","d835d7b2":"### 4.2 Feature Selection","7d931e58":"## 2.1  Sale Price\n\nAs a first step we will look at the sale price, our target variable.","e6e90af4":"### 3.2 Fix Missing Values","9159c59a":"Some of the variables like MSZoning, Street, Neighbourhood, Condition1, Condition2, ExterQual, BsmtQual and CentralAir have differences in sale price purely based on the median values. There is no significant difference in some of the other variables.\n\nNow that we have some idea about which variables are important for the sale price we will fix missing values and apply feature transformations to get the data ready for the model.","d55bbb61":"## 2.2 Numerical Features\n\nNow we will explore the relationship between the sale price and numerical features. We can spend our time exploring all numerical features. Maybe some people have the time, but not me. I always like finding shortcuts to make my life easier (ask my wife!). In the real world as well, we generally will not have a lot of time to explore all features. Trust me, your manager will be bugging you for results constantly.\n\nThere are two ways of approaching this:\n* use domain knowledge to make educated guesses on what variables you need to look at first. You can always come back for more after you have built your baseline models and verified initial assumptuions\n* analyze only the top of few independent variables that are highly correlated with the dependent variable.\n\nThis will speed up other aspects of data exploration as well, since we will have a fewer variables to clean etc. and get the data model ready. \n \nSince I do not have any real knowledge on the Boston Housing market I will be basing all my further analysis on the correlations.","65a0cb3b":"# 4. ML Models","de3800af":"### 3.6 Transform Numerical Features\n\nWe need to check the distributions for numerical features (especially the area features) and apply appropriate transformations.","92264b84":"### 4.1 Baseline Model \n\nFirst we will apply a baseline model - Random Forest. We will tune this to get reasonable RMSE errors (our chosen metric). We will also use this to select the required features for further models.","7fab3bb6":"As a first step we will plot the effect of tuning individual parameters on the root mean square error. \n\nAfter that We will apply a randomized search with a 3-fold cross-validation and tune the major parameters.","27afd003":"### 3.3 Binary Encoding for selected features","0e0648ec":"As expected we see that the GarageArea and GarageCars have a strong positive correlation (0.9) and one of the them can be dropped. We will drop the GarageCars and keep the GarageArea feature. The garage year built and overall year built are also strongly correlated (0.8).\n\nWe will use a threshold of 0.8 to remove features. This threshold is arbitary and will vary from problem to problem. Also, this is a way to reduce the dimensionality of the data. \n\nBased on the threshold of the following numercial features will be removed:\n* GarageCars\n* GarageYrBlt\n* TotRmsAbvGrd","57911667":"We have explored the data, undertstood the variables, applied the transformations and are ready for the final stage - predictive modeling. Usually, in my ML projects the predective model is the second to the last stage. I typically tend to explore the model results using a SHAP analysis etc. ","5f003dad":"The XGB and Gradient Boost models have a Test RMSE similar to the baseline model. However, they do not overfit as severly as the Random Forest model. Model stacking is peforming to the meta regressor.","1d4b8f56":"## 1.1 Missing Values","91195ed9":"Now that we have a tuned model as a baseline, we will select the required features for other high performance models. I prefer using the Boruta package and select the features based on the rank. I have written an other kernel explaning the package [here.](https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection)","09be85c5":"## 2.3 Categorical Features\nNow we will explore the relationship of some of the categorical variables with the sale price.","68877c7f":"#### 4.1.2 Max Depth","14b9670f":"Excellent! The log transform has made the distribution nearly normal. There are other transformations we can adopt such as square root, box cox or higher degree transformations. The right method really depends on the data and can be acheived with a few trials. ","9e29a941":"# 2. Feature Exploration","953563de":"#### 4.1.4 Min Samples Leaf","0cb0140e":"### 4.4 XGBoost","1a4baf27":"Based on the train and test RMSE, the model seems to severly overfit.","c003cffc":"A quick look at the training data shows that there are 1,460 rows and 81 columns. And the test dataset has 1,459 rows and 80 columns, since the sale price is what is neeeded to be predicted.","22e1c9a1":"As we can see there are plenty of features that have a Skew value greater than 1, which indicates that these have long tails. We will apply the box-cox transformations (my favorite) for these features and check the skew.","b44b6381":"This kernel analyzes the Boston Housing Price data. This is how the kernel has been structured.\n1. Data Overview\n2. Feature Exploration  \n   2.1 Sale Price  \n   2.2 Numerical Features   \n   2.3 Categorical Features  \n3. Feature Engineering  \n   3.1 Label Encoding  \n   3.2 Fix Missing Values  \n   3.3 Binary Encoding  \n   3.4 Mean Target Encoding  \n   3.5 Add New Features  \n   3.6 Transform Numerical Features  \n4. ML Models  \n   4.1 Baseline Model  \n   4.2 Feature Selection  \n   4.3 Gradient Boosting  \n   4.4 XGBoost  \n   4.5 Model Stacking  \n5. Summary","d6c89abc":"# 1. Data Overview ","8f0ce8a2":"We will use MLXtend for model stacking as final step. ","d1f53139":"We have 32 features from the feature selection process out of the 82 total features. We know the model overfits, but this is only a baseline model. We will use these features for other high performing models like Gradient Boosting, XGBoost and LightGBM.","cfd5bef0":"### 4.3 Gradient Boosting"}}