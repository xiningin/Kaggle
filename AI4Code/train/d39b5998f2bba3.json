{"cell_type":{"b9e000cc":"code","a595dd5a":"code","1db17cf7":"code","336ed1b1":"code","087e4f8e":"code","534626cd":"code","88730333":"code","07311cf1":"code","8c2e98d8":"code","4abe8e7a":"code","b1247434":"code","c16a6899":"code","a4ba4731":"code","002c9626":"code","93648b52":"code","33f1c42a":"code","cdebdc9a":"code","1db4a7cf":"code","39813fc5":"code","5d710d2c":"code","13d788a5":"code","345ae8c0":"code","da60fbde":"code","0e9ff99f":"code","e5479a83":"code","424bee6b":"code","4ee90cd5":"code","ff118fc8":"code","4070b403":"code","cdd0c299":"code","09fa5365":"code","5ba13d75":"code","dfbcb1c0":"code","d176e41b":"code","4e80d105":"code","0e7a15bb":"code","f1e3233f":"markdown","122363d2":"markdown","d90eb02d":"markdown","d6ff3b82":"markdown","1d7c5082":"markdown","dfaf247a":"markdown","abc4d4fa":"markdown","19fddc8d":"markdown","1155d638":"markdown","5d3d81c0":"markdown","f4f1e8a9":"markdown","f57bd825":"markdown","c262317d":"markdown","7c8538bd":"markdown","820e4334":"markdown","5baac9b3":"markdown","553ab5d7":"markdown","769a4749":"markdown","a82a35ea":"markdown","bb82d1da":"markdown","ebe4bb42":"markdown","a4e9e963":"markdown","00b4b381":"markdown","9d0df262":"markdown","43e2dfaf":"markdown","4306250b":"markdown","baa4dcf9":"markdown","68471291":"markdown","c60e0b5c":"markdown","316ccac7":"markdown","da2a5175":"markdown","74f6da8c":"markdown","b1f8f607":"markdown","67156a3d":"markdown","33b65829":"markdown","8d157dd5":"markdown"},"source":{"b9e000cc":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Visualisation\nimport seaborn as sns          \nimport plotly.express as px\nimport plotly.io as pio\n\n%matplotlib inline\n\n!conda install -c conda-forge -y lifelines\n!conda install -c conda-forge -y scikit-learn\n\nfrom itertools import product\nfrom lifelines import KaplanMeierFitter\nfrom collections import Counter\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, cross_validate\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import GridSearchCV, HalvingGridSearchCV, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay\n\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nimport shap\n","a595dd5a":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf = pd.DataFrame(data)\ndf","1db17cf7":"df.info()","336ed1b1":"df = df.rename(columns={\"DEATH_EVENT\": \"died\"})","087e4f8e":"categorical_cols = df.loc(axis=1)['anaemia','diabetes','high_blood_pressure','sex','smoking','died']\nnumerical_cols = df.loc(axis=1)['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']","534626cd":"sns.set_context(\"notebook\", font_scale=1.25)\n\nfig, ax = plt.subplots(2,3, figsize=(18,14))\n[sns.countplot(data=df, x=cat, hue=\"died\", ax=ax.flatten()[i]) \n for i,cat in enumerate(categorical_cols.drop('died',axis=1))]\nax.flatten()[-1].set_visible(False) # Remove the 6th empty plot\n[y_ax.set_ylim(0,140) for y_ax in ax.flatten()]\nplt.show()","88730333":"seaborn_cols = ['#3f75a0', '#d88138']\nfig = px.parallel_categories(categorical_cols, color=\"died\", color_continuous_scale=seaborn_cols)\nfig.show()","07311cf1":"fig = px.parallel_coordinates(df, color=\"died\", color_continuous_scale=seaborn_cols)\nfig.show()","8c2e98d8":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(18,20))\n[sns.histplot(data=df, x=num, hue=\"died\", kde=True, ax=ax.flatten()[i]) for i,num in enumerate(numerical_cols)]\nax.flatten()[-1].set_visible(False)\nplt.show()","4abe8e7a":"def km_fits(data, hue=None, split_points=None):\n    \n    if hue in categorical_cols.columns:\n        \n        range_hue = np.unique(data[hue])\n        \n        X = [data[data[hue]==x]['time'] for x in range_hue]\n        Y = [data[data[hue]==y]['died'] for y in range_hue]\n        fit_label = [str(hue + ': ' + str(range_hue_i)) for range_hue_i in range_hue]\n        \n        \n    elif hue in numerical_cols.columns:\n        \n        bins = pd.cut(x=data[hue],bins=split_points)\n        range_hue = np.unique(bins)\n        hue_group = str(hue) + \"_group\"\n        data[hue_group] = pd.cut(x=data[hue], bins=split_points)\n        \n        X = [data[data[hue_group] == bin_range]['time'] for bin_range in range_hue]      \n        Y = [data[data[hue_group] == bin_range]['died'] for bin_range in range_hue]        \n        fit_label = [str(str(range_hue_i).replace(',',' -').replace(']',')')) for range_hue_i in range_hue]        \n        data.drop(hue_group, axis=1, inplace=True)\n        \n    fits = [KaplanMeierFitter().fit(x_i, event_observed = y_i, label=fit_label[i]) for i,(x_i, y_i) in enumerate(zip(X,Y))]\n    \n    return fits","b1247434":"fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(18,10))\nplt.tight_layout(pad=5.0)\n\nfor i,feat in enumerate(categorical_cols.drop('died',axis=1).columns):\n    cat_fits = km_fits(df, hue=feat)\n\n    [x.plot(title=feat, ylabel=\"Survival Function\", xlabel=\"Days\",\n            ylim=(0,1.1), xlim=(0,290),\n            ci_alpha=0.1, ax=ax.flatten()[i]) for x in cat_fits]\n\nax.flatten()[-1].set_visible(False)\nfig.suptitle(\"Kaplan Meier Estimates for Categorical Variables \", fontsize=16.0)\nplt.show()","c16a6899":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(18,18))\n\nplt.tight_layout(pad=5.0)\n\nsplit_points = [[30.0,60.0,80.0,100.0],3,[0,30.0,45.0,np.inf],3,3,3,3]\n\nfor i,feat in enumerate(numerical_cols.columns):\n    con_fits = km_fits(df, hue=feat,split_points=split_points[i])\n\n    [x.plot(title=feat, ylabel=\"Survival Function\", xlabel=\"Days\",\n            ylim=(0,1.1), xlim=(0,290), ci_alpha=0.1, \n            ax=ax.flatten()[i]) for x in con_fits]\n\nax.flatten()[-1].set_visible(False)\nax.flatten()[-2].set_visible(False)\n\nfig.suptitle(\"Kaplan Meier Estimates for Continuous Variables \", fontsize=16.0, y=1.0)\n\nplt.show()","a4ba4731":"EF_fits = km_fits(df, hue=\"ejection_fraction\",split_points=[0,30.0,45.0,np.inf])\nsex_fits = km_fits(df, hue=\"sex\")\n\nstyles=['-','--','-.']\n\nfix, ax = plt.subplots(1,2, figsize=(12,5))\nplt.tight_layout(pad=2.5)\n\n[x0.plot(kind='line',ax=ax[0], ci_alpha=0.01, xlabel=\"Days\", ylabel=\"Survival Function\",\n         color='k', xlim=(0,290), ylim=(0,1.1), style=styles[i]) for i,x0 in enumerate(EF_fits)]\n\n[x1.plot(kind='line',ax=ax[1], ci_alpha=0.01, xlabel=\"Days\", ylabel=\"Survival Function\", \n         color='k', xlim=(0,290), ylim=(0,1.1), style=styles[j]) for j,x1 in enumerate(sex_fits)]\n\nax[0].legend(loc=3, labels=['EF<=30', '30<EF<=45','EF>45'])\nax[1].legend(loc=3, labels=['Women','Men'])\nax[0].set_title('(a)', y=1.025)\nax[1].set_title('(b)', y=1.025)\nplt.show()","002c9626":"numerical_cols.describe().loc(axis=0)['min','max']","93648b52":"df.isnull().any()","33f1c42a":"plt.figure(figsize=(12,10))\nfig = sns.heatmap(data=round(df.corr(),2), annot=True, annot_kws={\"fontsize\":13})\nplt.show()","cdebdc9a":"scaler = StandardScaler()\n\ndf_scaled = df.copy()\ndf_scaled.loc(axis=1)[numerical_cols.columns] = scaler.fit_transform(df.loc(axis=1)[numerical_cols.columns]) ","1db4a7cf":"RANDOM_STATE = 2\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=RANDOM_STATE)\n\nfeat_set = ['Leakage', 'Baseline']\n\n# create list of models to evaluate\nmodels = [LogisticRegression(), \n          RandomForestClassifier(random_state=RANDOM_STATE), \n          XGBClassifier(verbosity=0, use_label_encoder = False, \n                        random_state=RANDOM_STATE, eval_metric='logloss'),\n          GradientBoostingClassifier(random_state=RANDOM_STATE),\n          SVC(kernel='sigmoid'),\n          QuadraticDiscriminantAnalysis()]\n\nmodel_names = [mod.__class__.__name__ for mod in models]\n\nmod_cols = ['Name', \n            'Parameters',\n            'Time']\n\ndf_mod = pd.DataFrame(columns=mod_cols)\n\nfor i in range(len(feat_set)):\n\n    # Features\n    \n    if (i==0):\n        X = df_scaled.drop('died',axis=1)\n    else:\n        X = df_scaled.drop(['time','died'],axis=1)\n\n     \n    y = df_scaled['died']\n    \n    for j,model in enumerate(models):\n\n        # evaluate model\n        cv_results = cross_validate(model, X, y, cv=cv, scoring=\"f1\", return_train_score = True)\n        df_mod.loc[j + len(models)*i , 'Parameters'] = str(model.get_params())\n        df_mod.loc[j + len(models)*i, 'Name'] = model.__class__.__name__\n        df_mod.loc[j + len(models)*i, 'Time'] = cv_results['fit_time'].mean()\n        df_mod.loc[j + len(models)*i, 'Train Accuracy'] = cv_results['train_score'].mean()\n        df_mod.loc[j + len(models)*i, 'Test Score'] = cv_results['test_score'].mean()\n        df_mod.loc[j + len(models)*i, 'feat_set'] = feat_set[i]\n    ","39813fc5":"df_mod.loc(axis=1)['Name','Train Accuracy','Test Score','feat_set'].sort_values('Test Score', ascending=False)","5d710d2c":"fig = px.bar(data_frame = df_mod.sort_values('Test Score', ascending=True),\n             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n             color_discrete_sequence=px.colors.qualitative.D3,\n             template = \"plotly_white\")\nfig.show()","13d788a5":"counter = Counter(y) \n\n# transform the dataset\noversample = SMOTE()\nX_sm, y_sm = oversample.fit_resample(X, y)\ndf_sm = X_sm.copy()\ndf_sm['died'] = y_sm\nsmote_counter = Counter(y_sm)\n\n## Visualise the oversampling ## \nfig = plt.figure(figsize=(16,8))\nax1 = fig.add_subplot(1, 2, 1, projection='3d')\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\nax = [ax1, ax2]\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    ax1.scatter(X.loc(axis=0)[row_ix][\"platelets\"],\n                    [row_ix], \n                    X.loc(axis=0)[row_ix][\"age\"],\n                    label=str(label),\n                    alpha=0.75)\n                    \nfor label_sm, _ in smote_counter.items():\n    row_ix_sm = np.where(y_sm == label_sm)[0]\n    ax2.scatter(X_sm.loc(axis=0)[row_ix_sm][\"platelets\"], \n                    [row_ix_sm],\n                    X_sm.loc(axis=0)[row_ix_sm][\"age\"],\n                    label=str(label_sm), alpha=0.75)\n\nfor axi in ax:\n    axi.set_zlim(-2,2)\n    axi.set_xlim(-2,2)\n    axi.set_ylim(0,400)\n    axi.set_xticks([-2,-1,0,1,2])\n    axi.set_yticks([0,100,200,300,400])\n    axi.set_zticks([-2,-1,0,1,2])\n    \nax1.set_title(\"Original Data\")\nax2.set_title(\"SMOTE Data\")\nplt.legend()\nplt.show()","345ae8c0":"df_mod_sm = df_mod.copy()\n\nfor model in models:\n\n    cv_results = cross_validate(model, X_sm, y_sm, cv=cv, scoring=\"f1\", return_train_score = True)\n    \n    # Adding 1 to the max index instead of appending so I can pass everything as a dict()\n    df_mod_sm.loc(axis=0)[df_mod_sm.index.values.max()+1] = {\n            'Name':model.__class__.__name__,\n            'Parameters':str(model.get_params()),\n            'Time':cv_results['fit_time'].mean(),\n            'Train Accuracy':cv_results['train_score'].mean(),\n            'Test Score':cv_results['test_score'].mean(),\n            'feat_set':'SMOTE'\n             }","da60fbde":"fig = px.bar(data_frame = df_mod_sm.sort_values('Test Score', ascending=True),\n             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n             color_discrete_sequence=px.colors.qualitative.D3,\n             template = \"plotly_white\")\nfig.show()","0e9ff99f":"top_model_names = df_mod_sm.sort_values('feat_set', ascending=False).sort_values('Test Score', ascending=False)['Name'][:3].values\ntop_models = [m for m in models if m.__class__.__name__ in top_model_names]\n\n# Initialize a DataFrame to contain the importances of each feature for each model\ndf_imp = pd.DataFrame(index=range(0,len(X_sm.columns)*len(top_models)), columns=['feature','model','importance'])\n\n# len_feat will allow us to populate the features for each model\nlen_feat = int(len(X_sm.columns))\n\nfor i in range(len(top_models)):\n    results = permutation_importance(top_models[i].fit(X_sm, y_sm), X_sm, y_sm, scoring=\"f1\", \n                                n_repeats=10, n_jobs=None, \n                                random_state=RANDOM_STATE)   \n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'importance'] = (results['importances_mean'])\n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'model'] = top_models[i].__class__.__name__\n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'feature'] = X_sm.columns","e5479a83":"fig = px.bar(data_frame=df_imp.sort_values('importance'), \n             x=\"importance\", \n             y=\"feature\", \n             color=\"model\",\n             barmode=\"group\",\n             orientation=\"h\",\n             template = \"plotly_white\"\n             )\nfig.show()","424bee6b":"shap.initjs()\nexplainer = shap.TreeExplainer(top_models[2].fit(X_sm, y_sm))\nshap_values = explainer.shap_values(X_sm)\nshap.summary_plot(shap_values, features=X_sm, feature_names=X_sm.columns)","4ee90cd5":"shap.summary_plot(shap_values, features=X_sm, \n                        feature_names=X_sm.columns, \n                        plot_type=\"bar\")","ff118fc8":"feats = []\n\ndf_mod_sm_f = df_mod_sm[df_mod_sm['Name'].isin(top_model_names)].copy()\n\nfor j in [1,6]:\n    feats.append(X_sm.columns[np.argsort(np.abs(shap_values).mean(0))][::-1][:j+1].values)\n\nfor i,feat in enumerate(feats):    \n    for model in top_models:\n        cv_results = cross_validate(model, X_sm.loc(axis=1)[feat], y_sm, cv=cv, \n                                    scoring=\"f1\", return_train_score = True)\n\n        # Adding 1 to the max index instead of appending so I can pass everything as a dict()\n        df_mod_sm_f.loc(axis=0)[df_mod_sm_f.index.values.max()+1] = {\n                'Name':model.__class__.__name__,\n                'Parameters':str(model.get_params()),\n                'Time':cv_results['fit_time'].mean(),\n                'Train Accuracy':cv_results['train_score'].mean(),\n                'Test Score':cv_results['test_score'].mean(),\n                'feat_set': f\"SMOTE {len(feat)}-Feature\"\n                 }","4070b403":"fig = px.bar(data_frame = df_mod_sm_f.sort_values('Test Score', ascending=True),\n             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n             color_discrete_sequence=px.colors.qualitative.D3,\n             template = \"plotly_white\")\nfig.show()","cdd0c299":"rf_param_space = {\"n_estimators\" : [2000, 5000],\n                  \"criterion\":[\"gini\"], \n                  \"max_depth\":[9, 18],\n                  \"min_samples_split\": [2, 6]}\n\nrandom_forest = RandomForestClassifier(random_state=RANDOM_STATE)\n\nrf_gscv = HalvingGridSearchCV(estimator=random_forest, param_grid=rf_param_space,\n                   cv = 10, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n\n# rf_gscv = GridSearchCV(estimator=random_forest, param_grid=rf_param_space,\n#                    cv = cv, n_jobs=-1, scoring = \"f1\", verbose = 1)\n\nprint('Running GridSearchCV for Random Forest Classifier...')\nrf_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n\nprint(\"best estimator: \", rf_gscv.best_estimator_)\nprint(\"best parameters: \", rf_gscv.best_params_)\nprint(\"best score: \",rf_gscv.best_score_)","09fa5365":"gb_param_space = {\"n_estimators\" : [1000, 2500],\n                  \"subsample\" : [0.5, 1.0],\n                  \"max_depth\":[9, 18],\n                  \"learning_rate\" : [0.025, 0.75]}\n\ngrad_boost = GradientBoostingClassifier(random_state=RANDOM_STATE)\n\ngb_gscv = HalvingGridSearchCV(estimator=grad_boost, param_grid=gb_param_space,\n                   cv = 10, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n\n# gb_gscv = GridSearchCV(estimator=grad_boost, param_grid=gb_param_space,\n#                    cv = cv, n_jobs=-1, scoring = \"f1\", verbose = 1)\n\nprint('Running GridSearchCV for Gradient Boosting Classifier...')\ngb_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n\nprint(\"best estimator: \", gb_gscv.best_estimator_)\nprint(\"best parameters: \", gb_gscv.best_params_)\nprint(\"best score: \", gb_gscv.best_score_)","5ba13d75":"xgb_param_space = {\"n_estimators\":[100, 500],\n                   \"subsample\" : [0.5, 1.0],\n                   \"max_depth\" : [9, 18],\n                   \"eta\" : [0.25, 0.75]}\n\nxgboost = XGBClassifier(use_label_encoder=False, random_state=RANDOM_STATE, eval_metric='logloss')\n\n# xgb_gscv = HalvingGridSearchCV(estimator=xgboost, param_grid=xgb_param_space,\n#                    cv = cv, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n\nxgb_gscv = GridSearchCV(estimator=xgboost, param_grid=xgb_param_space,\n                   cv = 10, n_jobs=-1, scoring = \"f1\", verbose = 1)\n\n# ------- Uncomment code below to run grid search for XGBoost -------\n# print('Running GridSearchCV for XGBoost Classifier...')\n# xgb_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n\n# print(\"best estimator: \", xgb_gscv.best_estimator_)\n# print(\"best parameters: \", xgb_gscv.best_params_)\n# print(\"best score: \", xgb_gscv.best_score_)\n\n# xgb_gscv.best_params_['use_label_encoder'] = False\n# xgb_gscv.best_params_['eval_metric'] = 'logloss'\n# xgb_gscv.best_params_['random_state'] = RANDOM_STATE\n# tuned_xgb = XGBClassifier(**xgb_gscv.best_params_)","dfbcb1c0":"rf_gscv.best_params_['random_state'] = RANDOM_STATE\ngb_gscv.best_params_['random_state'] = RANDOM_STATE\n\ntuned_rf = RandomForestClassifier(**rf_gscv.best_params_)\ntuned_gb = GradientBoostingClassifier(**gb_gscv.best_params_)\ntuned_xgb = XGBClassifier(verbosity=0, use_label_encoder = False, \n                        random_state=RANDOM_STATE, eval_metric='logloss')\n\ntuned_models = [tuned_rf, tuned_gb, tuned_xgb] \n\ndf_final = pd.DataFrame(index = range(len(tuned_models)) ,\n                        columns = ['Name',\n                                   'Parameters',\n                                   'Time', \n                                   'F1 Score', \n                                   'Precision', \n                                   'Recall',\n                                   'Accuracy',\n                                   'roc_auc',\n                                   'fpr',\n                                   'tpr'\n                                  ])\n\nfor i, model in enumerate(tuned_models):\n    cv_results = cross_validate(model, X_sm.loc(axis=1)[feats[1]], y_sm, cv=cv, \n                                scoring=[\"f1\", \"precision\", \"recall\", \"roc_auc\", \"accuracy\"],\n                                return_train_score = True)\n\n    # Adding 1 to the max index instead of appending so I can pass everything as a dict()\n    df_mod_sm_f.loc(axis=0)[df_mod_sm_f.index.values.max()+1] = {\n            'Name':model.__class__.__name__,\n            'Parameters':str(model.get_params()),\n            'Time':cv_results['fit_time'].mean(),\n            'Train Accuracy':cv_results['train_accuracy'].mean(),\n            'Test Score':cv_results['test_f1'].mean(),\n            'feat_set': f\"SMOTE {len(feats[1])}-Feature (Tuned)\"\n             }\n    \n    # Gather data needed for ROC Curves\n    y_pred = cross_val_predict(model, X_sm.loc(axis=1)[feats[1]], y_sm, cv=10, method='predict_proba')  \n    fpr, tpr, thresholds = roc_curve(y_sm, y_pred[:,1])\n    roc_auc = auc(fpr, tpr)\n    \n    # Adding 1 to the max index instead of appending so everything can pass as a dict()\n    df_final.loc(axis=0)[i] = {\n            'Name':model.__class__.__name__,\n            'Parameters':str(model.get_params()),\n            'Time':cv_results['fit_time'].mean(),            \n            'F1 Score':cv_results['test_f1'].mean(),\n            'Precision':cv_results['test_precision'].mean(),\n            'Recall':cv_results['test_recall'].mean(),\n            'Accuracy':cv_results['test_accuracy'].mean(),\n            'roc_auc':roc_auc,\n            'fpr':fpr,\n            'tpr':tpr\n             }","d176e41b":"fig = px.bar(data_frame = df_mod_sm_f.sort_values('Test Score', ascending=True),\n             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n             color_discrete_sequence=px.colors.qualitative.D3,\n             template = \"plotly_white\")\nfig.show()","4e80d105":"lw=3.5\nplt.figure(figsize=(8,6))\n\n[plt.plot(df_final['fpr'][i], df_final['tpr'][i], \n          label = f\"{df_final['Name'][i]} - AUC:{df_final['roc_auc'][i]:.2f}\",\n          linewidth=lw, alpha=0.75) for i in range(len(df_final))]\n\nplt.plot([0,1],[0,1],'--k', alpha=0.5)\nplt.xlim(0,1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.ylim(0,1)\nplt.legend()\nplt.show()","0e7a15bb":"df_final_melt = df_final.melt(value_vars = [\n    \"F1 Score\", \n    \"Precision\", \n    \"Recall\", \n    \"roc_auc\", \n    \"Accuracy\"], id_vars=\"Name\")\n\nfig = px.bar(data_frame = df_final_melt.sort_values('value', ascending=True),\n             x=\"variable\", y=\"value\", color=\"Name\", barmode=\"group\",\n             color_discrete_sequence=px.colors.qualitative.Plotly,\n             template = \"plotly_white\")\nfig.show()","f1e3233f":"### Creating <br>\nNo new features will be created in this iteration. Performance will be measured with the original feature set.","122363d2":"## Model comparison & Leakage comparison using Repeated Stratified K-Fold Cross Validation","d90eb02d":"### Numerical Columns\nConstruct Kaplan-Meier curves for each of the numerical variables.","d6ff3b82":"### Feature Correlation","1d7c5082":"### XGBoost Hyperparameter using Grid Search\nHyperparameter tuning for XGBoost was inhibitively slow and resulted in a decrease in F1 score. Therefore, default hyperparameters are used and the .fit statement in the code below in commented out.","dfaf247a":"### Feature importance using SHAP (SHapley Additive exPlanations):","abc4d4fa":"### Converting \n* All of the categorical data is binary in nature so there is no need for one-hot encoding. \n* Features will be scaled in the modelling section below.\n","19fddc8d":"# Import","1155d638":"### Evaluate the impact of SMOTE on the models","5d3d81c0":"_____________\n## Insights from the EDA <br>\n### Categorical Data: \n- Those with **high blood pressure** appear to have an **increased risk of death**. This is reasonable as hypertension is a major risk factor for heart failure _(Levy D, Larson MG, Vasan RS, Kannel WB, Ho KKL. The Progression From Hypertension to Congestive Heart Failure. JAMA. 1996;275(20):1557\u20131562. doi:10.1001\/jama.1996.03530440037034)_.\n- **Anaemia** appears to be associated with **increased risk of death**. Again, this is reasonable as anaemia is a common comorbidity of heart failure _(Shah R, Agarwal AK. Anemia associated with chronic heart failure: current concepts. Clin Interv Aging. 2013;8:111-122. doi:10.2147\/CIA.S27105)_ \n- However, as demonstrated in the Kaplan Meier curves, hypertension and anaemia do carry increased risk of death but the confidence intervals are wide enough that these are not statistically significant. <br>\n\n\n### Numerical Data: \n- A low value for **time** is **highly correlated with death**. This is due to the follow up period being cut short due to death of the patient. It is obvious however, that the number of days is not what killed the patient (other than the relentless and inevitable march of time), it is merely that the number of days are cut short in those who were lost to follow up due to death. In this sense time is not a clinically useful variable for screening patients e.g <br> _Doctor: \"Go home, and if you don't come back in 3 months theres a good chance you have died\"._ **More on time later in this notebook.**\n- A lower **ejection fraction** seems to have some correlation with **increased death**. This seems reasonable as typically a low stroke volume is associated with congestinve heart failure. Stroke volume, along with end diastolic volume make up the equation for ejection fraction. $ EF = {SV\/EDV} $ Where EF is the ejection fraction, SV is the stroke volume, EDV is the end diastolic volume. \n- **Increased age** appears to play a slight role in **increased death**. \n- Elevated **serum_creatinine** appears to indicate **inreased death**\n- Creatinine_phosphokinase, platelets, and serum_sodium do **not** appear to have any significant influence in predicting patient outcome\n<br>\n\n### Survival Analysis\n- The predictive power of the ejection fraction is shown very well through the use of the Kaplan Meier curves.\n- As expected, age also shows a strong correlation with survival, as made apparent through the Kaplan Meier curves.\n- Serum creatinine is also strongly correlated with survival, however the confidence intervals are quite wide.\n_______\n**Note**: This dataset comes from the paper _Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone_ (Chicco & Jurman, 2020) which took the dataset from this original study _Survival analysis of heart failure patients: A cases tudy_ (Ahmad et al. 2017). Therefore, we know _a priori_ that ejection fraction and creatinine alone are likely the two most important features. However, feature importance varies across models, therefore, independant analysis of feature importance is still required. https:\/\/doi.org\/10.1186\/s12911-020-1023-5\n_______\n\n\n","f4f1e8a9":"# If you liked this Kernel please consider giving it an upvote.\n## I would love to hear any feedback you have about anything in the Kernel, questions, suggestions for improvements, leave a comment!","f57bd825":"# Data Cleaning","c262317d":"________\n### **IMPORTANT NOTE REGARDING TARGET LEAKAGE:**\n\nThe barplot above highlights the importance of **correct feature selection**. Time cannot be a feature of the model because it is **not available at the time of prediction**. The **time variable is a direct result of the death event** so to use the time variable in the model would introduce **target leakage**. <br>\nA concrete example would be as follows: A patient comes to you hoping to receive a heart failure prediction using your model. You say: \"sure\" and collect their data to make a prediction. However, you come to the time column, what do you put here? Number of days since... when? The model we build must be useful and accurate, therefore **we cannot use time as a feature.**\n________","7c8538bd":"### Categorical Columns \nConstruct Kaplan-Meier curves for each of the categorical variables.","820e4334":"**The parallel coordinates plot below is scary at first but it is an extremely efficient way to interrogate a lot of data. It shows the distribution of continuous variables, and the interactivity allows for many questions to be qualitatively answered easily, i.e. what is the outcome for those with age>90? And difference in outcome between genders? etc.**","5baac9b3":"### We can even reproduce the figures from the original manuscript _(Ahmad et al. 2017)_ to validate our methods. <br>\n![Ahmad_et_al.png](attachment:32d8fced-b01f-424c-a57c-faf6d09691b1.png)","553ab5d7":"# EDA","769a4749":"# Modelling\n## Feature Scaling","a82a35ea":"## ...and the winner is!\n**RandomForestClassifier**\n* The clinical application of this model means it should be optimised to **minimize the number of false negatives predictions** and the best metric to go by in that respect is Recall (TP \/ TP + FN). **Recall is highest in the RandomForestClassifier**. \n* Every other metric is also highest in the RandomForestClassifier","bb82d1da":"### Random Forest Classifier Hyperparameter Tuning using Grid Search","ebe4bb42":"### The original research article had an F1 score of 0.547 using the Random Forest, this is close to the F1 score of our Random Forest. <br> Lets see if we can improve the result!","a4e9e963":"There are no NaNs or empty values in the dataset.","00b4b381":"## __Numerical columns__","9d0df262":"## Categorical Columns","43e2dfaf":"### Completing <br>\nCheck for any missing values or NaNs in the data.","4306250b":"## Using Synthetic Minority Oversampling TEchnique (SMOTE) to improve the imbalanced dataset","baa4dcf9":"* Clearly **_high_blood_pressure_, _smoking_, _diabetes_, _anaemia_ are the least important features** across both methods of determining feature importance (permutation & SHAP).\n* **These will be dropped** from the analysis and the resultant change in performance will be noted.","68471291":"* A similar F1 score is achieved using only 7 of the original features compared to the full set of features. \n* Using only 2 features results in a marginal decrease in the F1 score compared to using 7 features.","c60e0b5c":"### Run the models using 2 feature sets: \n* **mean absolute SHAP value >0.1**  \n* **mean absolute SHAP value >0.6**","316ccac7":"## Determine Feature Importance\n\nSelecting appropriate features can result in improvements in model metrics through eliminating noisy features. It has the added benefit of making the model easier to understand and more explainable. \n\n### Feature importance using permutation feature importance:","da2a5175":"Some quick googling tells me the ranges below are all within a physiological or pathophysiological range so there is no reason to suspect any incorrect data.","74f6da8c":"# Conclusion\n* **SMOTE** significantly improved the dataset which resulted in much **higher performance** from the model.\n* **Hyperparameter tuning** resulted in very modest improvements and takes far more time than SMOTE.\n* **Feature selection** simplified the model whilst maintaining performance, what's not to love?","b1f8f607":"## Survival Analysis using Kaplan-Meier plots<br>\n First we need a function that will allow us to quickly plot Kaplan Meier curves for any feature.","67156a3d":"### XGBoost, Random Forest, and Gradient Boost are the most accurate classifiers for this data using default hyperparameters. <br>\n__Moving forward I will only consider these models.__\n<br>\n","33b65829":"### Gradient Boosing Classifier Hyperparameter Tuning using Grid Search","8d157dd5":"### Correcting\nCheck data for any obvious errors such as age=650 <br>"}}