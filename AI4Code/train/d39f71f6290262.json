{"cell_type":{"97b3bb47":"code","3d8b866c":"code","1317c5ea":"code","d9a703bc":"code","1355d15e":"code","ce812cdb":"code","45e24920":"code","69ed156a":"code","b372bc25":"markdown","f8284ff9":"markdown","50685a89":"markdown","e264a778":"markdown","0535b912":"markdown","0f547a7b":"markdown"},"source":{"97b3bb47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import FreqDist, bigrams\nimport plotly.graph_objects as go\nimport re\nfrom wordcloud import WordCloud, ImageColorGenerator\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d8b866c":"usopen = pd.read_csv('..\/input\/women-us-open-2021-tweets\/US_Open_2021_Tweets.csv', parse_dates = ['date'])\nusopen.head()","1317c5ea":"def processtext(text):\n    text = text.lower()\n    text = re.sub('http:\/\/|t.co|http','',text)\n    text = re.sub('&lt;\/?.*?&gt;',' &lt;&gt; ',text)\n    text=re.sub('(\\\\W)+',' ',text)\n    text = re.sub('[.;:!\\'?,\\\"()\\[\\]]', '', text)\n    \n    return text\nusopen['text']=usopen['text'].apply(lambda x: processtext(x))\nstop = set(stopwords.words('english'))\nusopen['text_no_stop']= usopen['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n","d9a703bc":"def lemmamytext(corpus):\n    lemma = WordNetLemmatizer()\n    return [' '.join([lemma.lemmatize(word) for word in x.split()])for x in corpus]\nusopen['text_no_stop']=lemmamytext(usopen['text_no_stop'])\n","1355d15e":"tk = TweetTokenizer()\nusopen['tokenized']=usopen['text_no_stop'].apply(tk.tokenize)\nusopen.head()","ce812cdb":"words  = []\nfor word in usopen['tokenized']:\n    words +=word\nfd = FreqDist(words)\nthirty = fd.most_common(30)\nthirty = pd.Series(dict(thirty))\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=thirty.index, y = thirty.values, hovertext = thirty.values))\nfig.update_layout(title = 'Most Common Words')\nfig.show()","45e24920":"\nfig, ax = plt.subplots(figsize = (30,30))\nwc = WordCloud(width = 2000, height = 2000).generate(' '.join(usopen['text_no_stop']))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","69ed156a":"bigram = bigrams(words)\nbigram_fd = FreqDist(bigram)\nbigram_sorted = {k:v for k,v in sorted(bigram_fd.items(), key=lambda item:item[1], reverse=True)}\nbigram_joined = {'_'.join(k):v for k,v in sorted(bigram_fd.items(),key = lambda item:item[1],reverse = True)}\nbigram_freqdist = pd.Series(dict(bigram_joined))\nfig = go.Figure()\nfig.add_trace(go.Bar(x=bigram_freqdist.index[0:30],y=bigram_freqdist.values[:30],hovertext = bigram_freqdist.values))\nfig.update_layout(title = 'Most Common Bigrams')\nfig.show()","b372bc25":"Interestingly, one of the most common bigrams is 'Piers Morgan' - the 'Piers' having been lemmatized to 'Pier'. This is probably as a result of Piers Morgan's reaction earlier when Raducanu (the US Open winner) had pulled out of Wimbledon, and Piers Morgan subsequently commenting after her US Open victory that he had always supported her.\n\nThat's all for now, but I will add to this notebook soon!","f8284ff9":"The next diagram shows a Word Cloud of the most common words and phrases, with the size of the words corresponding to their frequency in the corpus.","50685a89":"Now, we will plot a bar chart of the 20 most common words in the US Open dataset. ","e264a778":"# Tennis US Open Tweets\n\nIn this notebook, I'll do some basic data analysis and visualisation of the tweets in the wake of Emma Raducanu's victory in the 2021 US Open. I'll be updating this notebook and hopefully coming up with more advanced details as we go along this journey.","0535b912":"It's nice to see that many of the tweets have a congratulatory tone, as can be seen by the prominence of said positive words such as 'inspiration', 'brilliant', etc.\n\nThe next graph shows the most common bigrams (series of two words) that are present.","0f547a7b":"In order to prepare the text for analysis, we have to take the following steps:\n**1. Lowercase the text**\nThis is to standardise the text for ease of further analysis.\n\n**2. Remove special characters and punctuations**\nSome examples of special characters include the 'and' symbol and the full stop.\n\n**3. Remove stopwords**\nEnglish has a list of stopwords that are commonly used (e.g. 'a', 'has', 'of'), and this will hinder our ability to analyse the data.\n\n**4. Lemmatize the text**\nLemmatization refers to returning a word to its 'root'. One example would be the conversion of the word 'best' to 'good', or 'returning' to 'return'.\n\n**5. Tokenize the text**\nThis is the last step required before analysis of the text can be done."}}