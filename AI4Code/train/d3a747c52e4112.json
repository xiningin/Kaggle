{"cell_type":{"d51c0555":"code","72facf40":"code","9d9aafa7":"code","b253b9e0":"code","e0bf5426":"code","452a8256":"code","3f3d2253":"code","936c4369":"code","21723c0b":"code","46142b9d":"code","3c42acf4":"code","2ba0680c":"code","53d14ebe":"code","118babf1":"code","a333ac57":"code","d7955982":"code","bc36d74b":"code","fcdf5712":"code","9ab0528a":"code","c995db10":"markdown","17ceffe4":"markdown","e04c0a79":"markdown","7a98186a":"markdown","1dea8874":"markdown","5952216c":"markdown","8ccf52d3":"markdown","d672cd71":"markdown","0e65bd14":"markdown"},"source":{"d51c0555":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72facf40":"import tensorflow as tf","9d9aafa7":"import os\nimport random\nimport numpy as np\nfrom tqdm import tqdm","b253b9e0":"from skimage.io import imread,imshow\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt","e0bf5426":"# importing required modules \nfrom zipfile import ZipFile \n  \n# specifying the zip file name \nfile_name = \"..\/input\/data-science-bowl-2018\/stage1_train.zip\"\n  \n# opening the zip file in READ mode \nwith ZipFile(file_name, 'r') as zip: \n    # printing all the contents of the zip file \n     \n  \n    # extracting all the files \n    print('Extracting all the files now...') \n    zip.extractall(\"stage1_train\") \n    print('Done!') ","452a8256":"# importing required modules \nfrom zipfile import ZipFile \n  \n# specifying the zip file name \nfile_name = \"..\/input\/data-science-bowl-2018\/stage1_test.zip\"\n  \n# opening the zip file in READ mode \nwith ZipFile(file_name, 'r') as zip: \n    # printing all the contents of the zip file \n     \n  \n    # extracting all the files \n    print('Extracting all the files now...') \n    zip.extractall(\"stage1_test\") \n    print('Done!') ","3f3d2253":"TRAIN_PATH=\"stage1_train\/\"\nTEST_PATH='stage1_test\/'","936c4369":"train_ids=next(os.walk(TRAIN_PATH))[1]\ntest_ids=next(os.walk(TEST_PATH))[1]","21723c0b":"IMG_HEIGHT=128\nIMG_WIDTH=128\nIMG_CHANNELS=3","46142b9d":"X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n\nprint('Resizing training images and masks')\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n    path = TRAIN_PATH + id_\n    img = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img  #Fill empty X_train with values from img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '\/masks\/'))[2]:\n        mask_ = imread(path + '\/masks\/' + mask_file)\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n                                      preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)  \n            \n    Y_train[n] = mask   \n\n# test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Resizing test images') \nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')","3c42acf4":"image_x=random.randint(0,len(train_ids))\nimshow(X_train[image_x])\nplt.show()\nimshow(np.squeeze(Y_train[image_x]))\n\nplt.show()","2ba0680c":"seed=42\nnp.random.seed=seed","53d14ebe":"\n#Build the model\ninputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = tf.keras.layers.Lambda(lambda x: x \/ 255)(inputs)\n\n#Contraction path\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n \nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n \nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n \nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = tf.keras.layers.Dropout(0.3)(c5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n#Expansive path \nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\nc6 = tf.keras.layers.Dropout(0.2)(c6)\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n \nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\nc7 = tf.keras.layers.Dropout(0.2)(c7)\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n \nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\nc8 = tf.keras.layers.Dropout(0.1)(c8)\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n \nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\nc9 = tf.keras.layers.Dropout(0.1)(c9)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n \noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","118babf1":"#callbacks\ncallbacks=[tf.keras.callbacks.EarlyStopping(patience=2,monitor='val_loss'),\n           tf.keras.callbacks.TensorBoard(log_dir='logs')]","a333ac57":"results=model.fit(X_train,Y_train,validation_split=0.1,batch_size=16,epochs=30,callbacks=callbacks)","d7955982":"predict_test=model.predict(X_test,verbose=1)","bc36d74b":"predict_test_t=(predict_test>0.5).astype(np.uint8)","fcdf5712":"imshow(X_test[1])\nplt.show()\nimshow(np.squeeze(predict_test_t[1]))\nplt.show()","9ab0528a":"imshow(X_test[2])\nplt.show()\nimshow(np.squeeze(predict_test_t[2]))\nplt.show()","c995db10":"## Training the model","17ceffe4":"## Applying the segmentation on the test images using our trained model.","e04c0a79":"### Introduction:\nU-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.\n\nThe U-Net architecture stems from the so-called \u201cfully convolutional network\u201d first proposed by Long and Shelhamer.\n\nThe main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. What's more, a successive convolutional layer can then learn to assemble a precise output based on this information.\n\nOne important modification in U-Net is that there are a large number of feature channels in the upsampling part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and yields a u-shaped architecture. The network only uses the valid part of each convolution without any fully connected layers. To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.\n\nSource: https:\/\/en.wikipedia.org\/wiki\/U-Net","7a98186a":"## Having a look at our training images and the segmented images related to it respectively","1dea8874":"## Building a U-Net","5952216c":"## Unzipping the files","8ccf52d3":"## Performing preprocessing, Loading the data, and Getting data ready for our model","d672cd71":"![image.png](attachment:image.png)","0e65bd14":"## Importing important libraries"}}