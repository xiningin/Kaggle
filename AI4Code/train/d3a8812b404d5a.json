{"cell_type":{"b371758d":"code","335c1766":"code","8cdfa994":"code","264f7843":"code","7a2f9e4e":"code","0738f3da":"code","49696c9a":"code","2eb272ac":"code","93337cb8":"code","5f775943":"code","b0ecb7d1":"code","7996c8ed":"code","6de2aad7":"code","736a5292":"code","43668a62":"code","746b8934":"code","ab96be26":"code","715028c5":"code","eec2100c":"code","8cde2fb2":"code","02985e25":"code","2a505259":"code","1ea8963f":"code","57e5e1bd":"code","0f48a3c2":"code","d4d4d3a5":"code","73927bfd":"code","6472c826":"code","2aa4ee16":"code","a6df79c8":"code","1bbbb5a6":"code","a02a6c96":"code","75471f28":"code","f06b3743":"code","43cbec13":"code","dc199e21":"code","2ffbab75":"code","739665ed":"code","c9043d12":"code","21841554":"code","18bf2007":"code","a516ad1e":"code","aa5a7f6c":"code","01cef7e1":"code","09defa65":"code","2ff266cf":"code","9efa25d1":"code","70ddf89f":"code","89b13119":"code","5f843c90":"markdown","37d318aa":"markdown","b44b3dfa":"markdown","61ef3ded":"markdown","c1f02001":"markdown","12b1cf74":"markdown","e4ae443e":"markdown","601de168":"markdown","a7f132b0":"markdown","8a2cf1f6":"markdown","dce83e6b":"markdown","054ee5f9":"markdown","131a9292":"markdown","390ae2a8":"markdown","6cdba2f6":"markdown","f92c3ff9":"markdown","2d4ac935":"markdown","00bb312a":"markdown","16510320":"markdown","928d1601":"markdown","f707746d":"markdown","7075f2b4":"markdown","b38e5a7b":"markdown","b3983705":"markdown","050872c4":"markdown","e65176a1":"markdown","390a9f42":"markdown","34aa49e5":"markdown","484dd1e0":"markdown","77958956":"markdown","388b7e1d":"markdown"},"source":{"b371758d":"#import the packages\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\")) #list the files in the input directory\ndf=pd.read_csv('..\/input\/hmeq.csv') #import the dataset\ncolumnNames = pd.Series(df.columns.values) # to check the columns\/variables\/features present in our data set","335c1766":"df.head(5) # to see the first 5 rows of dataset\n","8cdfa994":"df.shape #to look at the shape of the dataset","264f7843":"#descriptive statistics\ndescription= df.describe(include='all') # to get the basic summary of all the numeric columns and frequency distribution of all the categorical columns.\ndescription","7a2f9e4e":"data_types=df.dtypes #to print data types for each variable\ndata_types","0738f3da":"MissingData=df.isnull().sum().rename_axis('Variables').reset_index(name='Missing Values') # the isnull() returns 1 if the value is null\nMissingData","49696c9a":"#dropping rows that have missing data\ndf.dropna(axis=0, how='any', inplace=True)\ndf\n","2eb272ac":"#Frequency distribution of target variable \"BAD\" and visualizing the target variable\ndf[\"BAD\"].value_counts().plot.bar(title='BAD')","93337cb8":"#visualizing the categorical variable REASON\nREASON_count= df[\"REASON\"].value_counts().rename_axis('REASON').reset_index(name='Total Count')\ndf[\"REASON\"].value_counts().plot.bar(title='REASON')\n","5f775943":"#visualizing the categorical variable JOB\nJOB_count= df[\"JOB\"].value_counts().rename_axis('JOB').reset_index(name='Total Count')\ndf[\"JOB\"].value_counts().plot.bar(title='JOB')","b0ecb7d1":"# visualizing numeric variables using seaborn\nf, axes = plt.subplots(3, 3, figsize=(25,25))\nsns.distplot( df[\"LOAN\"] , color=\"skyblue\", ax=axes[0, 0])\nsns.distplot( df[\"DEBTINC\"] , color=\"olive\", ax=axes[0, 1])\nsns.distplot( df[\"MORTDUE\"] , color=\"orange\", ax=axes[0, 2])\nsns.distplot( df[\"YOJ\"] , color=\"yellow\", ax=axes[1, 0])\nsns.distplot( df[\"VALUE\"] , color=\"pink\", ax=axes[1, 1])\nsns.distplot( df[\"DELINQ\"] , color=\"red\", ax=axes[1, 2])\nsns.distplot( df[\"DEROG\"] , color=\"green\", ax=axes[2, 0])\nsns.distplot( df[\"CLAGE\"] , color=\"gold\", ax=axes[2, 1])\nsns.distplot( df[\"CLNO\"] , color=\"teal\", ax=axes[2, 2])","7996c8ed":"\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='JOB vs BAD', figsize=(4,4))\n","6de2aad7":"REASON=pd.crosstab(df['REASON'],df['BAD'])\nREASON.div(REASON.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='REASON vs BAD', figsize=(4,4))","736a5292":"# visualizing numeric variables using seaborn\nf, axes = plt.subplots(3, 3, figsize=(25,25))\nsns.distplot( df[\"LOAN\"] , color=\"skyblue\", ax=axes[0, 0])\nsns.distplot( df[\"DEBTINC\"] , color=\"olive\", ax=axes[0, 1])\nsns.distplot( df[\"MORTDUE\"] , color=\"orange\", ax=axes[0, 2])\nsns.distplot( df[\"YOJ\"] , color=\"yellow\", ax=axes[1, 0])\nsns.distplot( df[\"VALUE\"] , color=\"pink\", ax=axes[1, 1])\nsns.distplot( df[\"DELINQ\"] , color=\"red\", ax=axes[1, 2])\nsns.distplot( df[\"DEROG\"] , color=\"green\", ax=axes[2, 0])\nsns.distplot( df[\"CLAGE\"] , color=\"gold\", ax=axes[2, 1])\nsns.distplot( df[\"CLNO\"] , color=\"teal\", ax=axes[2, 2])","43668a62":"\ndfWithBin = df.copy()\nbins=[0,15000,25000,90000] \ngroup=['Low','Average','High'] \ndfWithBin['LOAN_bin']=pd.cut(df['LOAN'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['LOAN_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Realtionship between Amount of Loan requested and the target variable BAD')\nplt.xlabel('LOAN')\nP= plt.ylabel('Percentage')","746b8934":"bins=[0,47000,92000,400000] \ngroup=['Low','Average','High'] \ndfWithBin['MORTDUE_bin']=pd.cut(dfWithBin['MORTDUE'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['MORTDUE_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Realtionship between the Amount due on existing mortgage and the target variable BAD')\nplt.xlabel('MORTDUE')\nP= plt.ylabel('Percentage')","ab96be26":"bins=[0,68000,120000,860000] \ngroup=['Low','Average','High'] \ndfWithBin['VALUE_bin']=pd.cut(dfWithBin['VALUE'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['VALUE_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Realtionship between the value of the current property and the target variable BAD')\nplt.xlabel('VALUE')\nP= plt.ylabel('Percentage')","715028c5":"bins=[0,3,15] \ngroup=['Low','High'] \ndfWithBin['DELINQ_bin']=pd.cut(dfWithBin['DELINQ'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['DELINQ_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Relationship of Number of Delinquent credit lines with the target variable')\nplt.xlabel('DELINQ')\nP= plt.ylabel('Percentage')","eec2100c":"bins=[0,2,15] \ngroup=['Low','High'] \ndfWithBin['DEROG_bin']=pd.cut(dfWithBin['DEROG'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['DEROG_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Relationship of Number of major derogatory reports with the target variable')\nplt.xlabel('DEROG')\nP= plt.ylabel('Percentage')","8cde2fb2":"bins=[0,120,230,1170] \ngroup=['Low','Average','High'] \ndfWithBin['CLAGE_bin']=pd.cut(dfWithBin['CLAGE'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['CLAGE_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Relationship  of Age of oldest tradeline in months with the target variable')\nplt.xlabel('CLAGE')\nP= plt.ylabel('Percentage')","02985e25":"bins=[0,40,204] \ngroup=['Low','High'] \ndfWithBin['DEBTINC_bin']=pd.cut(dfWithBin['DEBTINC'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['DEBTINC_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Debt to Income ratio realtionship with target variable')\nplt.xlabel('DEBTINC')\nP= plt.ylabel('Percentage')","2a505259":"#Create Correlation matrix\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","1ea8963f":"#encoding\ndf=pd.get_dummies(df, columns=['REASON','JOB'])\ndf\n","57e5e1bd":"# Extract independent and target variables\nX = df.drop(['BAD'], axis=1)\ny = df['BAD']","0f48a3c2":"#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = pd.DataFrame(sc_X.fit_transform(X), columns=X.columns)","d4d4d3a5":"#RFE with the logistic regression algorithm to select the top 4 features. \n#import classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nmodel = LogisticRegression()\nrfe = RFE(model, 4)\nfit = rfe.fit(X, y)\nno_of_features = fit.n_features_\nsupport_features = fit.support_\nranking_features = fit.ranking_\nprint(\"Num Features: %d\" % (no_of_features))\nprint(\"Selected Features: %s\" % (support_features))\nprint(\"Feature Ranking: %s\" % (ranking_features))\nX_sub = X.iloc[:,support_features] #updated X with the top 4 features","73927bfd":"#splitting the data into test and train for logistic regression\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sub,y,random_state = 0) # default 25% test data\n#import logistic regresiion model\nfrom sklearn.linear_model import LogisticRegression\n# create model (estimator) object\nclassifier = LogisticRegression()\n# fit model to training data\nclassifier.fit(X_train,y_train)\n#classifier performance on test set\nclassifier.score(X_test,y_test)\n# make predictions\ny_pred = classifier.predict(X_test)\ny_score= classifier.predict_proba(X_test)","6472c826":"\n#import performance measure tools\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, classification_report\nacs=accuracy_score(y_test,y_pred)\nrs=recall_score(y_test,y_pred, average='macro') \nps=precision_score(y_test,y_pred, average='macro') \nprint(\"accuracy score : \",acs)\nprint(\"precision score : \",rs)\nprint(\"recall score : \",ps)\n#print(\"Accuracy : %s\" % \"{0:.3%}\".format(acs))\nprint(classification_report(y_test, y_pred))","2aa4ee16":"import itertools\ndef plot_confusion_matrix(cm,classes=[0,1],title='Confusion matrix without normalization', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a6df79c8":"# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\nplt.show()\n","1bbbb5a6":"#ROC plot\nfrom sklearn.metrics import roc_curve, auc\ndef plot_roc(y_test, y_score):\n    fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\n\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(\"ROC plot for loan defaulter prediction\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n#Plot ROC\nplot_roc(y_test, y_score)","a02a6c96":"# KFOLD\nfrom statistics import mean, stdev\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate\nlogreg = LogisticRegression()\nkf = KFold(n_splits=5,shuffle=True,random_state=1)\nkf_scores = []\nxmat = X_sub.as_matrix()\nymat = y.as_matrix()\nfor train_index, test_index in kf.split(xmat):\n    X_train, y_train=xmat[train_index], ymat[train_index]\n    logreg.fit(X_train, y_train)\n    y_predicted=logreg.predict(X_test)\n    kf_scores.append(accuracy_score(y_test, y_predicted))\n\nprint(kf_scores)","75471f28":"#import KNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel= KNeighborsClassifier()\n#parameters of gridsearch for KNN:n_neighbors, weights\nparam_dict= {'n_neighbors':range(3,11,2),\n             'weights':['uniform','distance'],\n                       'p':[1,2,3,4,5]\n                       }\n#since this is binary classifier use odd for list of neighbours , with the range function we get k=3,5,7,9\n#uniform means that every near neighbour will get the same weightage whether k =3,5,7 or 9\n#p means the manhatten or eucledean or higher power distances\n#grid search to find the best parameters\nfrom sklearn.model_selection import GridSearchCV\nbest_model = GridSearchCV(model, param_dict, cv=5) \nbest_model.fit(X_sub, y)\nbest_model.best_params_ #{'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\nbest_model.best_score_  #best_score will give the mean score of 5 cv's which is 0.9304399524375743\nypred = best_model.predict(X_sub)\ny_score= best_model.predict_proba(X_sub)","f06b3743":"# import Performance measure\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nacs=accuracy_score(y,ypred) \nrs=recall_score(y,ypred, average='macro') \nps=precision_score(y,ypred, average='macro') \nprint(\"accuracy score : \",acs)\nprint(\"precision score : \",rs)\nprint(\"recall score : \",ps)","43cbec13":"# Compute confusion matrix for KNN\ncm = confusion_matrix(y, ypred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\nplt.show()","dc199e21":"#Plot ROC for KNN\nplot_roc(y,y_score)","2ffbab75":"#import SVM classifier\nfrom sklearn.svm import SVC\nmodel=SVC(probability=True)\nparam_dict = {'kernel':['rbf','poly'],'degree': [1,2,3], 'C':[0.5,0.75,1],'gamma': [0.01, 0.1, 1]}\n#parameters for grid search for SVM are kernel,degree,gamma and C\nfrom sklearn.model_selection import GridSearchCV\n#best_model = GridSearchCV(model, param_dict, cv=5, scoring= 'precision') \nbest_model = GridSearchCV(model, param_dict, cv=5) \nbest_model.fit(X_sub, y)\nbest_model.best_params_ # {'C': 1, 'degree': 1, 'gamma': 0.1, 'kernel': 'rbf'}\nbest_model.best_score_ # 0.9316290130796671\nypred = best_model.predict(X_sub)\ny_score= best_model.predict_proba(X_sub)","739665ed":"# import Performance measure for SVM\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nacs=accuracy_score(y,ypred) \nrs=recall_score(y,ypred, average='macro') \nps=precision_score(y,ypred, average='macro')\nprint(\"accuracy score : \",acs)\nprint(\"precision score : \",rs)\nprint(\"recall score : \",ps)","c9043d12":"#compute Confusion Matrix for SVC\ncm = confusion_matrix(y, ypred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\nplt.show()","21841554":"#Plot ROC for SVC\nplot_roc(y,y_score)","18bf2007":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier()\nparam_dict={'min_samples_split' : range(10,500,20),'max_depth': np.arange(3, 10)}\nclf=GridSearchCV(model,param_dict, cv=5)\nbest_model.fit(X, y)\nbest_model.best_params_ # \nbest_model.best_score_ # 0.9316290130796671\nypred = best_model.predict(X)\ny_score= best_model.predict_proba(X)\n# import Performance measure\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nacs=accuracy_score(y,ypred) \nrs=recall_score(y,ypred, average='macro')  \nps=precision_score(y,ypred, average='macro') \nprint(\"accuracy score : \",acs)\nprint(\"precision score : \",rs)\nprint(\"recall score : \",ps)","a516ad1e":"# Compute confusion matrix for Decision Tree\ncm = confusion_matrix(y, ypred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)","aa5a7f6c":"#Plot ROC for Decision Tree\nplot_roc(y,y_score)","01cef7e1":"#feature importance of decision tree\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 0) #\nfrom sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train, y_train) # train the data first for calculating the feature importance\nfeatimp = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint (featimp)\nfeatimp.plot(kind='bar', title='Feature Importance of Decision Tree Model')\n","09defa65":"#import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_jobs=-1)\nparam_dict = { 'n_estimators':[5,10,15],\n               'max_depth':[50,60,70],\n               'criterion': ['gini','entropy']\n              }\n#parameters for grid search for Randomforest are n_estimators, max depth and criterion\nfrom sklearn.model_selection import GridSearchCV\nbest_model = GridSearchCV(model, param_dict, cv=5) \nbest_model.fit(X, y)\nbest_model.best_params_ #{'criterion': 'gini', 'max_depth': 60, 'n_estimators': 10}\nbest_model.best_score_  #0.9200356718192628\nypred = best_model.predict(X)\ny_score= best_model.predict_proba(X)","2ff266cf":"# import Performance measure\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nacs=accuracy_score(y,ypred) \nrs=recall_score(y,ypred, average='macro') \nps=precision_score(y,ypred, average='macro')\nprint(\"accuracy score : \",acs)\nprint(\"precision score : \",rs)\nprint(\"recall score : \",ps)","9efa25d1":"#compute confusion matrix for Random Forest\ncm = confusion_matrix(y, ypred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)","70ddf89f":"#Plot ROC for Random Forest\nplot_roc(y,y_score)","89b13119":"#feature importance of random forest\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 0) #\nfrom sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_jobs=-1)\nmodel.fit(X_train, y_train) # train the data first for calculating the feature importance\nfeatimp = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint (featimp)\nfeatimp.plot(kind='bar', figsize=(15,7.5),title='Feature Importance of Random Forest Model')","5f843c90":"Data Preprocessing\n==","37d318aa":"Understanding the data\n==","b44b3dfa":"**Univariate Analysis**\n\nIn this analysis we examine each variable individually. For categorical features we can use bar plots which calculates the total count of each category in a particular variable. For numerical features we can use histograms or probability density plots to look at the distribution of the variable.","61ef3ded":"**Data Normalization**\n\nsome of the columns in X have large variance in the numeric data in them so standard scaling is done to normalize them","c1f02001":"KNN\n==\n\n","12b1cf74":"Problem statement\n==\n\nIt is important for Bank and Lenders to ensure that they don\u2019t give loans to people who are likely to default. The Home Equity data set contains information related to home equity for about six thousand loans given out in past. This data contains various information regarding customer\u2019s situation at the time of loan and also contains a column 'BAD' which indicates whether the customer later on defaulted on the loan. We can use this data set along with the variable \u2018BAD\u2019 to train machine learning models which would help us predict the likelihood of someone defaulting the loan in future based on their current situation.\n\nThis notebook provides an analysis of the Home Equity dataset and proposes the best model to use for predicting people who may default by analysing their current situation. This problem can be categorized as binary classification problem as the model will predict whether a person would default.\n","e4ae443e":"Hypothesis Generation\n==\n\nSome of the factors which can affect the BAD (target variable for this loan defaulter problem) are:\n* Mortgage due: Applicants with less amount left on mortgage should have higher chances of not defaulting on future loans\n* Loan amount: Applicants with less amount of loan requested should have higher chances of not defaulting on future loan\n* Value of current property: If the value of the current property is less then there should be less chances of being a defaulter on future loans\n* Reason: The reason for taking out the loan might also have an impact on the chance of going in default\n* Number of delinquent loans: If a person has already been delinquent in past for a number of times then there would be higher chances of default\n* Number of derogatory reports: Derogatory reports are always a negative sign on the credit history and any number in this column should indicate high chance of defaulting in future\n* Credit line age: This indicates the age of oldest credit approved and the older this is the less likely it will be for the customer to default\n* Debt to income ratio: If a person has high debt with respect to income then it would be difficult for that person to pay back more debt so a high number in this would indicate a defaulter","601de168":"Conclusion\n==\n\n* Explored different supervised learning algorithms for classification problems such as logistic regression, KNN, SVM, Decision Tree, Random Forest\n* Explored how RFE and Feature Importance gives the best contributing variables in predicting the target variable\n* Explored how hyper parameter tuning is done using GridSearchCV to achieve good accuracy \n* logistic Regression produces good accuracy score of 0.906 after doing the feature selection but it did not perform well on other performance metrics(precision and recall)\n* KNN  has a good accuracy score but recall score is not good\n* SVM also has a good accuracy score but recall score is still bad \n* Overall analysis shows that random forest model worked very well compared to other models as it has a highest recall value of about 0.99. Hence random forest dominated over all the models in this classification problem.","a7f132b0":"The RFE returns the top 4 features\/independent variables as: DEROG,DELINQ,CLAGE,DEBTINC\n\nTherefore, X= DEROG,DELINQ,CLAGE,DEBTINC\n                     Y= BAD\n\nIn random forest model and decision trees, variable\/feature selection is done automatically by splitting the top nodes based on the most important features of the data","8a2cf1f6":"Random Forest Classifier\n==","dce83e6b":"**Categorical and Target Variable Relationship**","054ee5f9":"The mean validation accuracy for the logistic regression model is  0.909","131a9292":"It shows that the dataset has 5960 rows(observations) and 13 columns (variables\/features)","390ae2a8":"Decision Tree Classifier\n==","6cdba2f6":"SVC\n==","f92c3ff9":"As we can see that the accuracy is above 90% in all the models which gives the impression that all these models are best in predicting the outcome of loan. This is because the target variable is a bit unbalanced(skewed) as zeros are about 80% and ones are about 20%. Hence it is important to consider other performance metrics,such as precision and recall, to evaluate the model.\nIn this problem False Negative (FN) means that we predicted that a person would not default whereas that person would default. So, it is important to reduce the number of FNs otherwise the models would end up allowing loans for people who are likely to default. On the other hand, False Positive (FP) mean that the model predicts a person to be default who may not default. This is not a major problem as at the end of the day the lender will still be protected from the risk of losing capital. In summary, for this problem we need to reduce the number of FN and to achieve that we must pick a model which has a high Recall as according to the following equation high recall means less FN:\n\nRecall = TP \/ (TP + FN), where TP = True Positive and FN = False Negative\n\nFrom the above table, we can see that the random forest model was the most reliable compared to other models as it has the highest recall value of 0.99 and an accuracy of above 90%.\n","2d4ac935":" We can use scikit-learn (sklearn) for making different models. It requires the target variable in a separate dataset ","00bb312a":"Logistic Regression\n==\n\nLogistic regression or the logit model is one of the classification techniques and is defined as a type of regression model where the target variable is binary for example having only two values such as default \/not default, fraud \/not fraud etc.In logistic regression, the probability of a binary outcome is predicted.\nFor building the logistic regression model split the dataset into train and test. Also used KFold cross validation to randomly split the entire dataset into\u00a0k-folds","16510320":"Missing Values\n==","928d1601":"These stacked bar plots show:\n\n* The proportion of defaulters or non defaulters in each category of categorical variables\n* The proportion for defaulters having Sales job is higher\n* There is nothing significant that we can infer from the number of defaulters having a different reason for taking out loan","f707746d":"It shows that 20% are defaulters and 80% are non defulters","7075f2b4":"Exploratory Data Analysis (EDA)\n==","b38e5a7b":"Feature Selection using RFE for Logistic Regression Model\n==\n\nFeature selection is a process of selecting the features in the data that contribute the most to predict the target attribute. The Recursive Feature Elimination (RFE) method is a feature selection approach. It works by recursively removing attributes and building a model on remaining attributes. Some of its benefits are:\n* reduces overfitting\n* Improves accuracy\n* Reduces training time\n\n","b3983705":"Performance\/Evaluation metrics of the models\n==\n\n**Confusion Matrix**\n\nA confusion matrix is a summary of prediction results on a classification problem. A confusion matrix for binary classification shows the four different outcomes: true positive, false positive, true negative, and false negative. The actual values form the columns, and the predicted values (labels) form the rows.\n\n![image.png](attachment:image.png)\n\n**Accuracy**\n\n\u2022\tTrue Positive - Targets which are actually true(Y) and we also predicted them true(Y)\n\u2022\tTrue Negative - Targets which are actually false(N) and we also predicted them false(N)\n\u2022\tFalse Positive - Targets which are actually false(N) but we predicted them true(T)\n\u2022\tFalse Negative - Targets which are actually true(T) but we predicted them false(N)\nWe can calculate the accuracy of the model using these values of confusion matrix. The accuracy is given by:\n\nAccuracy= TP + TN \/ (TP + TN + FP + FN)\n\n**Precision **\n\nPrecision is a measure of correctness achieved in true prediction i.e. it calculates out of observations labeled as true, how many of them are actually true.Precision is a good measure to determine, when the costs of False Positive is high.\n\nPrecision = TP \/ (TP + FP)\n\n**Recall**\n\nRecall is a measure of actual observations which are predicted correctly i.e. it calculates how many observations of true class are labeled correctly. It is also known as \u2018Sensitivity\u2019. Precision is a good measure to determine, when the costs of False Positive is high.\n\nRecall = TP \/ (TP + FN)\n\n**Specificity**\n\nSpecificity is a measure of how many observations of false class are labeled correctly.\n\nSpecificity = TN \/ (TN + FP)\n\nSpecificity and Sensitivity both plays an important role in deriving ROC curve.\n\n**ROC curve**\n\nReceiver Operating Characteristic(ROC) summarizes the model\u2019s performance by calculating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity).\nThe area under curve (AUC) also called concordance index is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model.\n","050872c4":"**Encoding**\n\nAs many of the Machine Learning models takes only the numerical values as input so we have to convert categorical columns to numeric\nDummy variable turns categorical variables into a series of 0 and 1, making them much easier to compare","e65176a1":"Feature Importance\n==\n* Tree methods mathematically determine the splits which effectively helps in distinguishing the classes\n* feature_importances_ is an inbuilt attribute that comes with Tree Based Classifiers\n* returns an array of scores of each feature\u2019s importance in determining the splits\n* the higher the score more important is the feature towards the target variable\n","390a9f42":"**Bivariate Analysis**\n\nIn this analysis we explore the relationship of each variable with respect to the target variable. We can also check our hypothesis using bivariate analysis.","34aa49e5":"**Numeric and Target Variable Relationship**","484dd1e0":"Model Comparison on Evaluation Metrics\n==\n\n**Model**|**Accuracy**|**Precision**|**Recall **\n---|---|---|---\n**Logistic Regression**|0.906064209274673|0.8944531696173614|0.5801866256457886\n**KNN**|0.9402497027348394|0.9430006397421091|0.6740208877284595\n**SVC**|0.9354934601664685|0.9669308137762878|0.6383333333333333\n**Random Forest**|0.9925683709869203|0.9583333333333333|0.9959533829718356\n**Decision Tree** |0.9268727705112961|0.59|0.9628398791540785\n\n\n\n\n\n","77958956":"This correlation matrix shows the correlation between all the numeric variables.\n\nThe interesting correlation that we can see is that BAD (target variable) is positively correlated with DELINQ, DEROG and DEBTINC","388b7e1d":"K-Fold Cross Validation for Logistic Regression\n==\n\nValidation is a technique of reserving a particular sample of a dataset on which you do not train the model. It helps in analyzing how robust the model is to the unseen data\nK-Folds Cross Validation is a validation technique in which we split our data into k different subsets (or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set.\n"}}