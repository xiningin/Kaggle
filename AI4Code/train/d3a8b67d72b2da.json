{"cell_type":{"464157b3":"code","ae161857":"code","2353874d":"code","d4c4c339":"code","5c0d2747":"code","b7445862":"code","6b92acc0":"code","9bbd7818":"code","f650e043":"code","34f204c6":"code","cdedcde8":"code","330aa0cd":"code","7aea7cc2":"code","99f02075":"code","41e2ce40":"code","820b7030":"code","0c7868b5":"code","3f220a17":"code","f5ca7620":"code","0420b07c":"code","17520045":"code","5481777f":"code","d714fc8c":"code","500da84f":"code","6adb3e2f":"code","23d6920d":"code","442d3cf2":"code","524fb746":"code","a0e0f394":"code","942d52b3":"code","18ad0b62":"code","0e9d8bc2":"markdown","716c8cde":"markdown","a7361bdd":"markdown","d448785a":"markdown","4d88b015":"markdown","a54fe34d":"markdown","f36c43c0":"markdown","0275aba7":"markdown","f4d96117":"markdown","873d1fca":"markdown","c42d5bb1":"markdown","d676c999":"markdown","58b97172":"markdown","783da355":"markdown","5067c683":"markdown","38b001ad":"markdown","c954283d":"markdown","78570915":"markdown","72d30ddb":"markdown"},"source":{"464157b3":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\nimport pandas as pd","ae161857":"iteration, records = 0, 2\nfor record in tf.compat.v1.python_io.tf_record_iterator(\"..\/input\/higgs-boson\/training\/shard_00.tfrecord\"):\n    iteration+=1\n    print('==== Record ', iteration, ' ====')\n    print(tf.train.Example.FromString(record))\n\n    if iteration == records:\n        break","2353874d":"#Useful dataframe that will store our features and label currently in a tfrecord\nfeature_label_df = pd.DataFrame()\nfeature_label_df['label'] = None\n\n# a simple limit on the number of records we would like to inspect, feel free to adjust\niteration, records = 0, 100 \n\n#Let's iterate over each tfrecord \nfor record in tf.compat.v1.python_io.tf_record_iterator(\"..\/input\/higgs-boson\/training\/shard_00.tfrecord\"):\n    iteration+=1\n    #print('==== Record ', iteration, ' ====')\n    #print(tf.train.Example.FromString(record))\n\n    #parse each tfrecord\n    example = tf.train.Example()\n    example.ParseFromString(record)\n    \n    #looking at the output of the cell above, we are interested in the features.bytes_list entry\n    string = example.features.feature['features'].bytes_list.value[0]\n\n    #convert it to an int or float, see note above\n    feature_tensor = tf.io.decode_raw(string, tf.float16) #tf.uint8\n    \n    #print('==== Features ', iteration, ' ====')\n    #print(feature_tensor)\n    \n    #append current decoded record to our hepful dataframe\n    feature_label_df = feature_label_df.append(pd.DataFrame(feature_tensor).transpose()).reset_index(drop=True)\n    \n    ##looking at the output of the cell above, we are alsoe interested in the label.float_list field as our target, i.e presence or not of Higgs boson \"decay products\"\n    label = example.features.feature['label'].float_list.value[0]\n    #print('==== Label ', iteration, ' ====')\n    #print(label)\n    feature_label_df.loc[iteration-1, 'label'] = label\n    \n    if iteration == records:\n        break","d4c4c339":"#Inspect our helpful dataframe\nfeature_label_df","5c0d2747":"#Inspect class balance\nfeature_label_df.label.value_counts().sort_index()","b7445862":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","6b92acc0":"# TF 2.3 version\n# Detect and init the TPU\n# try: # detect TPUs\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n#     strategy = tf.distribute.TPUStrategy(tpu)\n# except ValueError: # detect GPUs\n#     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n# TF 2.2 version\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","9bbd7818":"strategy.__dir__()","f650e043":"# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [12,7]\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)","34f204c6":"# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature    #Configuration for parsing a fixed-length input feature.\nAUTO = tf.data.experimental.AUTOTUNE         #See Note 5 below \ud83d\ude00","cdedcde8":"# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","330aa0cd":"def make_decoder(feature_description):\n    \n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    \n    return decoder","7aea7cc2":"def load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    \n    return dataset","99f02075":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size \/\/ batch_size\nvalidation_steps = validation_size \/\/ batch_size\n\n# For model.compile\nsteps_per_execution = steps_per_epoch","41e2ce40":"print(dataset_size, validation_size, training_size, batch_size, steps_per_epoch, validation_steps, steps_per_execution)","820b7030":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\n\ndecoder = make_decoder(feature_description)","0c7868b5":"decoder","3f220a17":"data_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '\/training' + '\/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '\/validation' + '\/*.tfrecord')","f5ca7620":"ds_train = load_dataset(train_files, decoder, ordered=False)\n\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\n\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","0420b07c":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make","17520045":"with strategy.scope():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[28])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    outputs = layers.Dense(1)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='sigmoid',\n    )","5481777f":"wide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)","d714fc8c":"early_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","500da84f":"history = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[early_stopping, lr_schedule],\n)","6adb3e2f":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","23d6920d":"wide.summary()","442d3cf2":"tf.keras.utils.plot_model(wide, show_shapes=True)","524fb746":"deep.summary()","a0e0f394":"tf.keras.utils.plot_model(deep, show_shapes=True)","942d52b3":"wide_and_deep.summary()","18ad0b62":"tf.keras.utils.plot_model(wide_and_deep, show_shapes=True)","0e9d8bc2":"# Wide and Deep Neural Networks #\n\nA *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n\n[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer.\n\n## Note 2 \ud83d\ude00\nThe differences of all 3 network types\n\n<a href=\"https:\/\/arxiv.org\/pdf\/1606.07792.pdf\"><img src=https:\/\/1.bp.blogspot.com\/-Dw1mB9am1l8\/V3MgtOzp3uI\/AAAAAAAABGs\/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB\/s1600\/image04.png width=1000px><\/a>","716c8cde":"## Note 6 \ud83d\ude00\n\nLet us inspect these 3 models and their architecture**","a7361bdd":"We've collected some hyperparameters here to make experimentation easier. Fork this notebook by [**clicking here**](https:\/\/www.kaggle.com\/kernels\/fork\/12171965) to try it yourself!","d448785a":"## Note 1 \ud83d\ude00\nLet us inspect a couple of records from the training dataset  \nUseful links:   \nTFRecord and tf.train.Example: https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord  \nHow to inspect the structure of a TFRecord file: https:\/\/stackoverflow.com\/questions\/55861893\/how-to-inspect-the-structure-of-a-tfrecord-file-in-tensorflow-1-13","4d88b015":"# Model #\n\nNow that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n","a54fe34d":"# Training #\n\nDuring training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch.","f36c43c0":"# References #\n\n- Baldi, P. et al. *Searching for Exotic Particles in High-Energy Physics with Deep Learning*. (2014) ([arXiv](https:\/\/arxiv.org\/abs\/1402.4735))\n- Cheng, H. et al. *Wide & Deep Learning for Recommender Systems*. (2016) ([arXiv](https:\/\/arxiv.org\/abs\/1606.07792))\n- *What Exactly is the Higgs Boson?* Scientific American. (1999) [(article)](https:\/\/www.scientificamerican.com\/article\/what-exactly-is-the-higgs\/)]\n\n[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer.","0275aba7":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum) to chat with other Learners.*","f4d96117":"## Note 4 \ud83d\ude00\n```make_decoder(feature_description)``` returns a function, in this case ```decoder(example)``` which takes ```example``` as input when called. This concept is also used in decorators in Python. \n\nHere is a beginner friendly article on them: https:\/\/realpython.com\/inner-functions-what-are-they-good-for\/#closures-and-factory-functions","873d1fca":"Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n\n# Load Data #\n\nThe dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training.","c42d5bb1":"# Searching for the Higgs Boson #\n\nThe Standard Model is a theory in particle physics that describes some of the most basic forces of nature. One fundamental particle, the Higgs boson, is what accounts for the *mass* of matter. First theorized in the 1964, the Higgs boson eluded observation for almost fifty years. In 2012 it was finally observed experimentally at the Large Hadron Collider. These experiments produced millions of gigabytes of data.\n\nLarge and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n\n<a href=\"https:\/\/home.cern\/science\/physics\/higgs-boson\"><img src=\"https:\/\/cds.cern.ch\/record\/1630222\/files\/Candidate Higgs Events in ATLAS and CMS.png\" width=400px><\/a>","d676c999":"Both the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https:\/\/www.kaggle.com\/docs\/tpu) (TPUs), an accelerator ideal for large workloads.\n\n![image.png](attachment:image.png)","58b97172":"## Note 3 \ud83d\ude00\nLet us inspect ```strategy``` object","783da355":"## Note 5 \ud83d\ude00\n\n```AUTO = tf.data.experimental.AUTOTUNE```\n\nGPUs and TPUs can radically reduce the time required to execute a single training step. Achieving peak performance requires an efficient input pipeline that delivers data for the next step before the current step has finished. The tf.data API helps to build flexible and efficient input pipelines.\nhttps:\/\/www.tensorflow.org\/guide\/data_performance\n\n<a href=\"https:\/\/www.tensorflow.org\/guide\/data_performance\"><img src=https:\/\/www.tensorflow.org\/guide\/images\/data_performance\/prefetched.svg width=1000px><\/a>\n\nThe tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the tf.data runtime to tune the value dynamically at runtime.\n\nNote that the prefetch transformation provides benefits any time there is an opportunity to overlap the work of a \"producer\" with the work of a \"consumer.\"","5067c683":"# The Collision Data #\n\nThe collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead obesrve the behavior of the particles they decay into, their \"decay products\".\n\nThe *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.","38b001ad":"## Note 7 \ud83d\ude00\n\nLet us monitor TPU and CPU usage during training\n\n<img src=https:\/\/storage.googleapis.com\/kaggle-media\/tpu\/tpu_monitor.png width=300px>","c954283d":"## Note 1 continued \ud83d\ude00\n## We can also try and convert the features and label from TFRecord format to uint6 or float16 datatypes\n## Note that without knowing the exact initial datatype before it was encoded as TFRecord, decoding it will not be accurate.\n## For example, below when we create the feature_tensor object, its size will depend on the datatype we will use to decode: tf.unit8 will give us 122 features while tf.float16 will return 61 features","78570915":"The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n\n# Setup #\n\nIn addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them.","72d30ddb":"## See Note 4 \ud83d\ude00 above"}}