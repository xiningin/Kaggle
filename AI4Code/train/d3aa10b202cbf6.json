{"cell_type":{"8ab0136b":"code","5fe04886":"code","e2f92fe6":"code","9640e402":"code","870ba716":"code","67cf4b53":"code","ceea2e19":"code","3adef344":"code","1d0a147e":"code","c1cb1ab4":"code","16df6708":"code","454b0fb7":"code","cad9f5bc":"code","58efc89d":"code","05ef07cc":"code","e3aefd76":"code","1cdd65b7":"code","92695be5":"code","6003c8d7":"code","328bf2e0":"code","0e2dec07":"code","91fda775":"code","391b3494":"code","7287555e":"code","edf3cbef":"markdown","191b46c3":"markdown","a12019c6":"markdown","a2630ad6":"markdown","413c2d09":"markdown","6695c75f":"markdown","7d52f8e0":"markdown","816ddffe":"markdown","c8bd6706":"markdown","1c47b3cf":"markdown","8614df78":"markdown","81761f6e":"markdown","70b12772":"markdown","90577cd2":"markdown","15103ebc":"markdown"},"source":{"8ab0136b":"import time\nstart_time = time.time()","5fe04886":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","e2f92fe6":"!pip install tensorflow-text==2.0.0 --user","9640e402":"import tensorflow as tf\nimport tensorflow_hub as hub","870ba716":"import tensorflow_text as text","67cf4b53":"#it helps to print full tweet , not a part\npd.set_option('display.max_colwidth', -1)\n","ceea2e19":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","3adef344":"def clean(text):\n    text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n    text = re.sub(r\"RT \", \" \", text) # remove RT\n    # remove all characters if not in the list [a-zA-Z#@\\d\\s]\n    text = re.sub(r\"[^a-zA-Z#@\\d\\s]\", \" \", text)\n    text = re.sub(r\"[0-9]\", \" \", text) # remove numbers\n    text = re.sub(r\"\\s+\", \" \", text) # remove extra spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    return text","1d0a147e":"train.text = train.text.apply(clean)\ntest.text = test.text.apply(clean)","c1cb1ab4":"train['text'][50:70]","16df6708":"use = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\")","454b0fb7":"X_train = []\nfor r in tqdm(train.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_train.append(review_emb)\n\nX_train = np.array(X_train)\ny_train = train.target.values\n\nX_test = []\nfor r in tqdm(test.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_test.append(review_emb)\n\nX_test = np.array(X_test)","cad9f5bc":"train_arrays, test_arrays, train_labels, test_labels = train_test_split(X_train,\n                                                                        y_train,\n                                                                        random_state =42,\n                                                                        test_size=0.05)","58efc89d":"def svc_param_selection(X, y, nfolds):\n    Cs = [1.07]\n    gammas = [2.075]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=8)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search\n\nmodel = svc_param_selection(train_arrays,train_labels, 5)","05ef07cc":"model.best_params_","e3aefd76":"pred = model.predict(test_arrays)","1cdd65b7":"cm = confusion_matrix(test_labels,pred)\ncm","92695be5":"accuracy = accuracy_score(test_labels,pred)\naccuracy","6003c8d7":"test_pred = model.predict(X_test)\nsubmission['target'] = test_pred.round().astype(int)\n#submission.to_csv('submission.csv', index=False)","328bf2e0":"train_df_copy = train\ntrain_df_copy = train_df_copy.fillna('None')\nag = train_df_copy.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n\nag.sort_values('Disaster Probability', ascending=False).head(20)","0e2dec07":"count = 2\nprob_disaster = 0.9\nkeyword_list_disaster = list(ag[(ag['Count']>count) & (ag['Disaster Probability']>=prob_disaster)].index)\n#we print the list of keywords which will be used for prediction correction \nkeyword_list_disaster","91fda775":"ids_disaster = test['id'][test.keyword.isin(keyword_list_disaster)].values\nsubmission['target'][submission['id'].isin(ids_disaster)] = 1","391b3494":"submission.to_csv(\"submission.csv\", index=False)\nsubmission.head(10)","7287555e":"print(\"--- %s seconds ---\" % (time.time() - start_time))","edf3cbef":"#### Accuracy and confusion matrix","191b46c3":"### Some words about Universal Sentence Encoders and Transformer","a12019c6":"### Using keywords for better prediction.","a2630ad6":"Load the multilingual encoder module.","413c2d09":"Please, upvote, if you like","6695c75f":"The keywords are used for the corretion.","7d52f8e0":"A Universal Sentence Encoders encode sentencies to fixed length vectors (The size is 512 in the case of the Multilingual Encoder). The encoders are pre trained on several different tasks: (research article) https:\/\/arxiv.org\/pdf\/1803.11175.pdf. And a use case: https:\/\/towardsdatascience.com\/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15 <br>\nTwo architectures are in use in the encoders: Transformer and Deep Averaging Networks.\nTransformer use \"self attention mechanism\" that learns contextual relations between words and (depending on model) even subwords in a sentence. Not only a word , but it position in a sentence is also taking into account (like positions of other words). There are different ways to implement the intuitive notion of \"contextual relation between words in a sentence\" ( so, different ways to construct \"representation space\" for the contextual words relation). If the several \"ways\" are implemented in a model in the same time: the term \"multi head attention mechanism\" is used.<br>\nTransformers have 2 steps. Encoding: read the text and transform it in vector of fixed length, and decoding: decode the vector (produce prediction for the task). For example: take sentence in English, encode, and translate (decode) in sentence in German.<br>\nFor our model we need only encoding mechanism: sentencies are encoded in vectors and supplied for classification to Support Vector Machine.<br>\nGood and intuitive explanation of the Transformer: http:\/\/jalammar.github.io\/illustrated-transformer\/ ; The original and quite famous now paper \"Attention is all you need\": (research article)\nhttps:\/\/arxiv.org\/pdf\/1706.03762.pdf. More about multi head attention: (research article)\nhttps:\/\/arxiv.org\/pdf\/1810.10183.pdf. How Transformer is used in BERT: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.<br>\n\nThe Multilingual Universal Sentence Encoder:(research articles) https:\/\/arxiv.org\/pdf\/1810.12836.pdf; https:\/\/arxiv.org\/pdf\/1810.12836.pdf;\nExample code: https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\nThe Multilingual Encoder uses very interesting Sentence Piece tokenization to make a pretrained vocabulary: (research articles) https:\/\/www.aclweb.org\/anthology\/D18-2012.pdf; https:\/\/www.aclweb.org\/anthology\/P18-1007.pdf.<br>\n\nAbout the text preprocessing and importance of its coherence with the text preprocessing that is conducted for pretraining + about the different models of text tokeniation:\n\nvery good article:\nhttps:\/\/mlexplained.com\/2019\/11\/06\/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp\/.<br>\n\nBelow the encoding is applied to every sentence in train.text and test.text columns and the resulting vectors are saved to lists.<br>","816ddffe":"Here I follow https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement The idea is that some keywords with very high probability (sometimes = 1) signal about disaster (or usual) tweets. It is possible to add the extra 'keyword' feature to the model, but the simple approach also works. I make correction for the disaster tweets prediction to the model basing on the \"disaster\" keywords.","c8bd6706":"### Data loading","1c47b3cf":"### Training and Evaluating","8614df78":"### Data preprosessing","81761f6e":"### Make Support Vector Machine prediction.","70b12772":"I did experiments with different parameters and text preprocessing within the model configuration. It gives many \"clever\" variants with a good score. But the very model is attractive by its simplicity.","90577cd2":"The results from :\nhttps:\/\/www.kaggle.com\/ihelon\/starter-nlp-svm-tf-idf ;\nhttps:\/\/www.kaggle.com\/dmitri9149\/svm-expm-v0\/edit\/run\/27808847 ;\nhttps:\/\/www.kaggle.com\/rerere\/disaster-tweets-svm ;\nshow the Support Vector Machine works quite well for the Real or Not ? (disaster) Tweets classification with with TF-ID for tokenization.<br>\nIn https:\/\/www.kaggle.com\/gibrano\/disaster-universal-sentences-encoder-svm the Multilingual Universal Sentence Encoder is used for sentence encoding. Here I follow the work in using the Multilingual Universal Sentence Encoder (from tensorflow_hub).<br>\nThe approach from https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement is applied for final filtering of the results basing on the 'keywords'.<br>\n\nThe resulting model is quite simple and relativelly fast (700....900 seconds execution time without GPU). This makes the model suitable for experiments with different parameters and text preprocessing.","15103ebc":"How the text looks like after the cleaning."}}