{"cell_type":{"dbe28c37":"code","9991202c":"code","82ae56f9":"code","1370aa37":"code","6453e8b4":"code","621f7b82":"code","1b72550c":"code","84818771":"code","36c8c859":"code","d468df33":"code","5ab388b6":"code","7fce5831":"code","d4c40ed1":"code","1f5b1ef4":"code","124d15c3":"markdown","ffe2cc6f":"markdown","383b915a":"markdown","5835c064":"markdown","876ae34b":"markdown","1a9042d1":"markdown","b8063414":"markdown"},"source":{"dbe28c37":"import numpy as np # linear algebra\nfrom numpy.random import seed\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\n\nimport tensorflow as tf #deep learning with keras\nfrom tensorflow.keras import regularizers\n\nfrom matplotlib import pyplot\n\nimport re","9991202c":"#ensure reproducibility\nseed(1)\ntf.random.set_seed(2)","82ae56f9":"#global variables\n\n#data paths\ntrain_path = \"..\/input\/titanic\/train.csv\"\ntest_path = \"..\/input\/titanic\/test.csv\"\noutput_prediction_path = \"submission_titanic_v29_20200719.csv\"\n\n#preprocessing\nlist_col_remove = [\"Ticket\",\"Cabin\"]\nlist_categ_col_to_encode = [\"Cabin_letter\", \"Cabin_first_digit\", \"Embarked\", \"Title\"]\nlist_ordinal_categ_col_to_encode = ['Fare_buckets', 'Age_buckets', 'Age_Class_buckets']\nspecial_remove_later = [\"Name\"]\ntarget_col = 'Survived'\ncomponents_PCA = 'full' #'full': no PCA applied, 'high': 90% of the total number of columns, 'medium': 75%, 'low': 50%, 'very_low': 33%\ntest_split_ratio = 0.25\n\n#deep learning model\n##initialization\nnumber_nodes_initial = 4096 #4096\nregul_L1 = 0.016 #0.02\nregul_L2 = 0.014 #0.015\ndrop_out = 0.2\n##training\nepochs_max = 1000 #not much improvements past this point: 1000\nepoch_max_full = 5 #after training on the train set, train on the validation set (to see all data)\npatience_eval = int(round(epochs_max\/1)) #change if we want to have an actual early stopping (e.g. 40)\nbatch_size = 15\ncheckpoint_filepath = '.mdl_wts.hdf5'\ninitial_learning_param = 0.0001","1370aa37":"#create needed arrays for training and predicting\nX_train, X_test, y_train, y_test, preprocessed_test_df = \\\n    create_train_val_predict_arrays(train_path,test_path, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, \\\n                                    special_remove_later, target_col, components_PCA, test_split_ratio)","6453e8b4":"#build and train model\nmodel = model_initialization(number_nodes_initial, X_train, regul_L1, regul_L2, drop_out, initial_learning_param)\nhistory, model = model_training(model, X_train, y_train, X_test, y_test, epochs_max, patience_eval, batch_size, checkpoint_filepath, initial_learning_param)","621f7b82":"#Adjust model with full data (OPTIONAL)\n# X_full = np.concatenate((X_train, X_test))\n# y_full = np.concatenate((y_train, y_test))\n# model.fit(X_full, y_full, epochs=epoch_max_full)","1b72550c":"#evaluate model\ntrain_acc, test_acc, confusion_matrix_results, classification_report_results = model_evaluation(model, X_train, y_train, X_test, y_test)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], test_acc[1]*100))\nprint(confusion_matrix_results)\nprint(classification_report_results)\n\nplot_history('accuracy')\n# plot_history('loss')","84818771":"#export model\nexport_pred(model, preprocessed_test_df, output_prediction_path)","36c8c859":"def add_features(df_prep, target_col, list_col_remove, n_ticket, tick_surv):\n    '''\n    combines all the functions below\n    '''\n    for function_adding_information in [cabin_information, name_information, family_information, fare_information]:\n        df_prep = function_adding_information(df_prep)\n        \n    df_prep = ticket_information(df_prep, target_col, n_ticket, tick_surv)\n    #df_prep = age_information(df_prep, list_col_remove, list_categ_col_to_encode)\n \n    #remove non-numerical remaining columns\n    df_prep = df_prep.drop(list_col_remove, axis=1)\n    \n    return df_prep\n\ndef cabin_information(df_prep):\n    '''\n    add first letter and first digit information wherever possible\n    '''\n    #feature to know whether the user has a cabin or not\n    df_prep[\"Has_cabin\"] = df_prep[\"Cabin\"].isna().astype(int)\n    \n    #feature to know the length of the cabin string (some users have multiple cabins) -> useless\n    #df_prep[\"Cabin_length\"] = df_prep[\"Cabin\"].str.len().fillna(0)\n    \n    #first letter of the cabin\n    df_prep[\"Cabin_letter\"] = df_prep[\"Cabin\"].astype(str).str[0]\n\n    #cabin digits (first digit may mean the floor on the boat)\n    #Note: the number of digits may mean the passenger was far at the end -> NEXT STEP\n    df_prep[\"Cabin_digits\"] = df_prep[\"Cabin\"].astype(str).str[1:]\n\n    #get index of users who have a cabin digit after the first letter (most users who have a non-NaN value do)\n    index_with_cabin_digits = df_prep.loc[df_prep['Cabin_digits'].str.isdigit(), :].index\n\n    #initialize the column\n    df_prep[\"Cabin_first_digit\"] = \"n\"\n    #fill in the column for the users with cabin digits\n    df_prep.loc[index_with_cabin_digits, \"Cabin_first_digit\"] = df_prep.loc[index_with_cabin_digits,'Cabin_digits'].astype(str).str[0]\n    df_prep.drop('Cabin_digits', axis=1, inplace=True)\n    \n    return df_prep\n\ndef name_information(df_prep):\n    '''\n    adds title information using regex from the name\n    '''\n    # Create a new feature Title, containing the titles of passenger names\n    df_prep['Title'] = df_prep['Name'].apply(get_title)\n    # Group all non-common titles into one single grouping \"Rare\"\n    regex_mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Rare', 'Sir': 'Rare', 'Rev': 'Rare',\n               'Don': 'Mr', 'Mme': 'Mrs', 'Jonkheer': 'Rare', 'Lady': 'Mrs',\n               'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Rare'}\n    df_prep.replace({'Title': regex_mapping}, inplace=True)\n    # Mapping titles\n    title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4, \"None\":5}\n    df_prep['Title'] = df_prep['Title'].map(title_mapping)\n    df_prep['Title'] = df_prep['Title'].fillna(5)\n\n    return df_prep\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n            return title_search.group(1)\n    return \"None\"\n\ndef family_information(df_prep):\n    '''\n    adds 2 columns about how big the user's family is on board and whether he is alone or not\n    '''\n    # Create new feature FamilySize as a combination of SibSp and Parch\n    df_prep['FamilySize'] = df_prep['SibSp'] + df_prep['Parch'] + 1\n\n    # Create new feature IsAlone from FamilySize\n    df_prep['isFamily'] = 1\n    df_prep.loc[df_prep['FamilySize'] == 1, 'isFamily'] = 0\n    \n    return df_prep\n\ndef fare_information(df_prep):\n    #special column for passengers who had a free tickets\n    df_prep['Fare_free'] = 0\n    index_free_fare = df_prep[df_prep['Fare'] == 0].index\n    df_prep.loc[index_free_fare, 'Fare_free'] = 1\n    \n    #for these 'outliers', we change the value to NaN (handled just below)\n    df_prep.loc[index_free_fare, 'Fare'] = np.nan\n    \n    #fill in missing values with median fare (not average as the ticket prices are not continuous)\n    df_prep['Fare'] = df_prep['Fare'].fillna(df_prep['Fare'].median())\n    \n    #we transform the fare feature as it doesn't follow a normal distribution\n    df_prep['Fare'] = np.log(np.log(df_prep['Fare']))\n#     df_prep['Fare'] = np.log(df_prep['Fare'])\n    \n    #Create fare buckets:\n    df_prep['Fare_buckets'] = 0\n    df_prep.loc[ df_prep['Fare'] <= 7.91, 'Fare_buckets'] = 0\n    df_prep.loc[(df_prep['Fare'] > 7.91) & (df_prep['Fare'] <= 14.454), 'Fare_buckets'] = 1\n    df_prep.loc[(df_prep['Fare'] > 14.454) & (df_prep['Fare'] <= 31), 'Fare_buckets'] = 2\n    df_prep.loc[ df_prep['Fare'] > 31, 'Fare_buckets'] = 3\n    df_prep['Fare_buckets'] = df_prep['Fare_buckets'].astype(int)\n    \n    return df_prep\n\ndef get_train_ticket_information(train_df, target_col):\n    '''\n    make sure a ticket in test that we have data on train is better assigned\n    We need to do this before the preprocessing happens as we will use these mappings in the preprocessing \n    '''\n    \n    boy = (train_df['Name'].str.contains('Master')) | ((train_df['Sex']==0) & (train_df['Age']<13))\n    female = (train_df['Sex']==1)\n    boy_or_female = boy | female   \n    # no. females + boys on ticket\n    n_ticket = train_df[boy_or_female].groupby('Ticket')[target_col].count()\n    # survival rate amongst females + boys on ticket\n    tick_surv = train_df[boy_or_female].groupby('Ticket')[target_col].mean()\n    \n    return n_ticket, tick_surv\n\ndef ticket_information(df_prep, target_col, n_ticket, tick_surv):\n    '''\n    types of ticket (i.e. location on the boat) and info about people on tickets (boy and female)\n    Note: there is no empty ticket in either train or test set\n    '''\n    #get index of users who have an integer as ticket (some users have text included in their ticket ID)\n    index_with_ticket_digits = df_prep.loc[df_prep['Ticket'].str.isdigit(), :].index\n    #create a column equal to 1 (about 3\/4 of all passengers) if integer and 0 if contains text as well\n    df_prep[\"Ticket_integer\"] = 0\n    df_prep.loc[index_with_ticket_digits, \"Ticket_integer\"] = 1\n\n    # if ticket exists in training data, fill NTicket with no. women+boys on that ticket in the training data\n    df_prep = ticket_replace(df_prep, n_ticket, 'NTicket')\n    # if ticket exists in training data, fill TicketSurv with women+boys survival rate in training data  \n    df_prep = ticket_replace(df_prep, tick_surv, 'TicketSurv')\n    \n    return df_prep\n\ndef ticket_replace(df_prep, replace_param, new_col_name):\n    '''\n    if ticket exists in training data, fill the new column with women+boys count \/ survival rate (from training data)\n    otherwise TicketSurv=0\n    '''\n    df_prep[new_col_name] = df_prep['Ticket'].replace(replace_param)\n    df_prep.loc[~df_prep['Ticket'].isin(replace_param.index),new_col_name] = 0\n    df_prep[new_col_name] = df_prep[new_col_name].fillna(0)\n    \n    return df_prep\n\n\ndef age_information(df_prep, special_remove_later, threshold):\n    \n    #fill in missing values\n#     df_prep['Age'] = df_prep['Age'].fillna(df_prep['Age'].median())\n    df_prep = age_fill_na(df_prep, special_remove_later)\n    \n    #add \"boy\" column (women and children usually would survive more)\n    df_prep['Boy'] = (df_prep['Name'].str.contains('Master')) | ((df_prep['Sex']==0) & (df_prep['Age']<13))\n    \n    #Create Age buckets\n    df_prep['Age_buckets'] = 0\n    df_prep.loc[ df_prep['Age'] <= 16, 'Age_buckets'] = 5\n    df_prep.loc[(df_prep['Age'] > 16) & (df_prep['Age'] <= 32), 'Age_buckets'] = 1\n    df_prep.loc[(df_prep['Age'] > 32) & (df_prep['Age'] <= 48), 'Age_buckets'] = 2\n    df_prep.loc[(df_prep['Age'] > 48) & (df_prep['Age'] <= 64), 'Age_buckets'] = 3\n    df_prep.loc[ df_prep['Age'] > 64, 'Age_buckets'] = 4\n    \n    #add Age*Class buckets\n    df_prep['Age_Class_buckets'] = df_prep['Age_buckets'] * df_prep['Pclass']\n    \n    #one-hot encoding on these -> NOT IN THIS VERSION FOR THE 'ORDINAL' CATEGORICAL COLUMNS\n    df_prep = do_all_one_hot_encodings(df_prep,['Age_buckets','Age_Class_buckets'],threshold)\n    \n    return df_prep\n\ndef age_fill_na(df_prep, special_remove_later):\n    '''\n    use linear regression model to fill missing Age values\n    '''\n    list_col_usable = [col for col in df_prep.columns if col not in special_remove_later]\n#     print(list_col_usable)\n\n    index_nan = df_prep[df_prep['Age'].isna()].index\n    index_no_nan = df_prep[~df_prep['Age'].isna()].index\n    \n    # Create linear regression object\n    regr = linear_model.LinearRegression()\n    # Train the model\n    regr.fit(df_prep.loc[index_no_nan, list_col_usable].drop('Age', axis = 1), df_prep.loc[index_no_nan, 'Age'])\n    # Make predictions\n    df_prep.loc[index_nan, 'Age'] = regr.predict(df_prep.loc[index_nan, list_col_usable].drop('Age', axis = 1))\n\n    return df_prep\n\ndef sex_renaming(df_prep):\n    '''\n    replace male, female respectively by 1 and 0 (there is no missing values in the \"Sex\" column)\n    '''\n#     print(df_prep[\"Sex\"])\n    df_prep[\"Sex\"] = df_prep[\"Sex\"].replace({\"male\": 1, \"female\": 0})\n    \n    return df_prep","d468df33":"def handle_missing_values(df_prep):\n    '''\n    handle missing valuesfor the numerical columns (it is handled after the one hot encoding for the categorical columns)\n    add a column wth the previously missing value data\n    #impute with mean (as most of the remaining columns here do not have many missing values)\n    '''\n    list_col_with_missing_values = ['Has_cabin','Cabin_length','FamilySize','isFamily']\n    list_actual_col_missing_values = [col for col in df_prep.columns if col in list_col_with_missing_values]\n    for col in list_actual_col_missing_values:\n        #fill in the missing values with the mean\n        df_prep[col] = df_prep[col].fillna(df_prep[col].mean())\n        \n    #add a missing column before filling in the missing values\n    list_add_na_col = ['Age']\n    list_actual_add_na_col = [col for col in df_prep.columns if col in list_add_na_col]\n    for col in list_actual_add_na_col:\n        missing_col_name = col + \"_na\"\n        df_prep[missing_col_name] = 0\n        index_missing_values_for_col = df_prep[df_prep[col].isnull()].index\n        df_prep.loc[index_missing_values_for_col, missing_col_name] = 1\n\n    return df_prep\n\ndef do_all_one_hot_encodings(df_prep,list_categ_col_to_encode,threshold):\n    '''\n    one-hot encode the necessary columns (all the categorical columns with at least 3 significant values)\n    '''\n    for categ_col in list_categ_col_to_encode:\n        df_prep = one_hot_significant(df_prep,categ_col,threshold)\n    \n    return df_prep\n\ndef one_hot_significant(df_prep,col_to_encode,threshold):\n    '''\n    we build a column for values with at least 'threshold' 10 instances\n    '''\n    #get the list of significant (enough) values to encode\n    val_count = df_prep[col_to_encode].value_counts()\n    list_one_hot = [val for val in val_count.index if val_count[val] >= threshold and val != \"n\"] #avoid NaN values\n\n    #get users who have values within this list\n    index_users_val_in_list = df_prep[df_prep[col_to_encode].isin(list_one_hot)].index\n\n    #proceed to one-hot encode for these users\n    dummy_temp = pd.get_dummies(df_prep.loc[index_users_val_in_list,col_to_encode], prefix=col_to_encode)\n\n    #join with the original df and fill in the users not in this index with values of 0\n    df_prep = pd.merge(df_prep, dummy_temp, left_index = True, right_index = True, how=\"left\")\n    df_prep.drop(col_to_encode, axis=1, inplace=True)\n    list_one_hot_new_columns = [col for col in df_prep.columns if col_to_encode in col]\n    df_prep[list_one_hot_new_columns] = df_prep[list_one_hot_new_columns].fillna(0)\n    \n    return df_prep\n\ndef apply_PCA(df_train, df_test, target_col, components_PCA='medium'):\n    '''\n    apply PCA on both training and test set to remove potential multi-colinearity\n    '''\n    \n    #determines the number of components used based on the user desired reduction\n    if components_PCA == 'full':\n        return df_train, df_test\n        \n    elif components_PCA == 'high':\n        no_component = int(round(len(df_test.columns) * 0.9))\n    \n    elif components_PCA == 'low':\n        no_component = int(round(len(df_test.columns) * 0.5))\n    \n    elif components_PCA == 'very_low':\n        no_component = int(round(len(df_test.columns) * 0.33))\n        \n    else: #when 'medium', i.e. default\n        no_component = int(round(len(df_test.columns) * 0.75))\n    \n    # Separating out the features\n    train = df_train.iloc[:,1:].values\n    test = df_test.values\n\n    pca = PCA(n_components = no_component)\n    principalComponents_train = pca.fit_transform(train)\n    \n    principalComponents_test = pca.transform(test)\n    \n#     print(\"PCA variance explained with {} variables:\".format(no_component), sum(pca.explained_variance_ratio_))\n    \n    principalDf_train = pd.DataFrame(data = principalComponents_train, index = df_train.index)\n    finalDf_train = pd.concat([df_train[[target_col]], principalDf_train], axis = 1)\n    \n    finalDf_test = pd.DataFrame(data = principalComponents_test, index = df_test.index)\n    \n    return finalDf_train, finalDf_test","5ab388b6":"def create_train_val_predict_arrays(train_path, test_path, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, \\\n                                    special_remove_later, target_col, components_PCA, test_split_ratio):\n    '''\n    outputs the preprocessed training, validation and test sets based on original data\n    '''\n    #imports the original training and test sets\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    \n    #preprocess training and test set\n    preprocessed_train_df, preprocessed_test_df = preprocessing_train_test_df(train_df, test_df, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, \\\n                                                                              special_remove_later, target_col, components_PCA)\n\n    #create 'train' and 'test' (i.e. validation) sets based on the original training data\n    X = preprocessed_train_df.iloc[:,1:].values\n    y = preprocessed_train_df.iloc[:,0].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split_ratio, random_state=42)\n\n    return X_train, X_test, y_train, y_test, preprocessed_test_df\n\n\ndef preprocessing_train_test_df(train_df, test_df, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, special_remove_later, target_col, components_PCA):\n    '''\n    creates the preprocessed versions of the training and test sets\n    '''\n    #get n_ticket, tick_surv from the train data (to make sure a ticket in test that we have data on train is better assigned)\n    n_ticket, tick_surv = get_train_ticket_information(train_df, target_col)\n    \n    #get the preprocessed df\n    preprocessed_train_df = preprocessing_df(train_df, target_col, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, special_remove_later, n_ticket, tick_surv)\n    preprocessed_test_df = preprocessing_df(test_df, target_col, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode,special_remove_later, n_ticket, tick_surv,threshold=0)\n\n    #remove the columns from preprocessed_test_df which are not in preprocessed_train_df\n    list_col_test_and_train = [col for col in preprocessed_test_df.columns if col in preprocessed_train_df.columns]\n    preprocessed_test_df = preprocessed_test_df[list_col_test_and_train]\n\n    #handle the missing columns from preprocessed_test_df in preprocessed_train_df (add column and fill in with value of 0)\n    list_col_train_not_test = [col for col in preprocessed_train_df.columns if col not in preprocessed_test_df.columns and col != target_col]\n    for missing_col in list_col_train_not_test:\n        preprocessed_test_df[missing_col] = 0\n        \n    #scale the data on train set and apply it on the test set. \n    #Note: RobustScaler should handle outliers better when scaling\n#     scaler = MinMaxScaler()\n    scaler = RobustScaler()\n    preprocessed_train_df[preprocessed_test_df.columns] = scaler.fit_transform(preprocessed_train_df[preprocessed_test_df.columns])\n    preprocessed_test_df[preprocessed_test_df.columns] = scaler.transform(preprocessed_test_df[preprocessed_test_df.columns])\n    \n    #PCA (to remove potential multi-colinearity)\n    preprocessed_train_df, preprocessed_test_df = apply_PCA(preprocessed_train_df, preprocessed_test_df, target_col, components_PCA)\n        \n    return preprocessed_train_df, preprocessed_test_df\n\n\ndef preprocessing_df(df, target_col, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, special_remove_later, n_ticket, tick_surv, threshold=10):\n    '''\n    preprocesses a df\n    '''\n    #set index as the passenger ID\n    df_prep = df.set_index(\"PassengerId\")\n    \n    #enriches data with additional features\n    df_prep = add_features(df_prep, target_col, list_col_remove, n_ticket, tick_surv)\n\n    #rename necessary values to numerical\n    df_prep = sex_renaming(df_prep)\n\n    #categorical column handling with one-hot encoding\n    list_actual_col_encode = [col for col in df_prep.columns if col in list_categ_col_to_encode]\n    df_prep = do_all_one_hot_encodings(df_prep,list_actual_col_encode,threshold)\n\n    #handle other missing values\n    df_prep = handle_missing_values(df_prep)\n    \n    #special handling for age (including missing values, one hot encoding for Age bucket)\n    df_prep = age_information(df_prep, special_remove_later, threshold)\n    \n    #ordinal column handling with one-hot encoding\n    list_actual_ord_col_encode = [col for col in df_prep.columns if col in list_ordinal_categ_col_to_encode]\n    df_prep = do_all_one_hot_encodings(df_prep,list_actual_ord_col_encode,threshold)\n    \n    #remove remaining useless columns\n    df_prep.drop(special_remove_later, axis=1, inplace=True)\n\n\n    return df_prep","7fce5831":"def model_initialization(number_nodes_initial, X_train, regul_L1, regul_L2, drop_out, initial_learning_param): \n    '''\n    define the model: deep learning classification model\n    '''\n    \n    #define layers used in the DNN\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(number_nodes_initial,activation='relu', kernel_regularizer=regularizers.l1_l2(l1=regul_L1, l2=regul_L2), input_shape=(X_train.shape[1],)),\n        tf.keras.layers.Dropout(drop_out),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(number_nodes_initial\/2,activation='relu', kernel_regularizer=regularizers.l2(regul_L2)),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n\n    ])\n    \n    # optimizer_model = tf.keras.optimizers.RMSprop(initial_learning_param)\n    optimizer_model = tf.keras.optimizers.Adam(initial_learning_param)\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer_model, metrics=['accuracy'])\n\n    return model\n\n\ndef model_training(model, X_train, y_train, X_test, y_test, epochs_max, patience_eval, batch_size, checkpoint_filepath, initial_learning_param):\n    '''\n    train the model\n    '''\n    #early stopping\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience = patience_eval, restore_best_weights=True)\n    \n    #save \n    mcp_save = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min')\n\n    #training\n    history = model.fit(X_train, y_train, epochs=epochs_max, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[es, mcp_save])\n    \n    # The model weights (that are considered the best) are loaded into the model.\n    model.load_weights(checkpoint_filepath)\n    \n    return history, model","d4c40ed1":"def plot_history(parameter):\n    '''\n    plot training history\n    '''\n    pyplot.plot(history.history[parameter], label='train')\n    pyplot.plot(history.history['val_' + parameter], label='test')\n    pyplot.legend()\n    pyplot.show()\n    \n    return None\n\ndef model_evaluation(model, X_train, y_train, X_test, y_test):\n    '''\n    evaluate model based on training and validation sets\n    '''\n    \n    train_acc = model.evaluate(X_train, y_train, verbose=0)\n    test_acc = model.evaluate(X_test, y_test, verbose=0)\n\n    Y_pred = model.predict(X_test)\n    y_pred = np.around(Y_pred,0).astype(int)\n    \n    #confusion matrix (rows: predicted, columns: actual)\n    confusion_matrix_results = confusion_matrix(y_test, y_pred)\n    classification_report_results = classification_report(y_test, y_pred)\n    \n    return train_acc, test_acc, confusion_matrix_results, classification_report_results","1f5b1ef4":"def export_pred(model, preprocessed_test_df, output_prediction_path):\n    '''\n    export actual submission\n    '''\n    \n    #predict from the original test set (after it has been preprocessed the same way as the original training set)\n    Y_pred_final = model.predict(preprocessed_test_df.values)\n    y_pred_final = np.around(Y_pred_final,0).astype(int)\n    \n    #handle potential nan values \/ values with issues (there should not be any)\n    y_pred_final[y_pred_final < 0] = 0 \n    y_pred_final[y_pred_final > 1] = 1 \n    \n    #export actual submission\n    final_output_df = pd.DataFrame(y_pred_final, index = preprocessed_test_df.index, columns=[target_col])\n    final_output_df.to_csv(output_prediction_path)\n    \n    return None","124d15c3":"## Modeling","ffe2cc6f":"### 3. apply and get all required datasets","383b915a":"### 2. for entire dataset","5835c064":"# FUNCTIONS\n## Preprocessing\n### 1. per feature","876ae34b":"## Exporting","1a9042d1":"## Evaluating","b8063414":"# Score & Ranking\n\n- Score: 0.79665\n- Public Ranking: between top 8% and top 9%"}}