{"cell_type":{"a69f5848":"code","48cc2917":"code","43c96417":"code","6804bebd":"code","031cce12":"code","d7a23741":"code","42a75285":"code","e9d4545d":"code","6f7e4f83":"code","bcfb630c":"code","f56e38da":"code","6ae0d168":"code","55f898fd":"code","921e687e":"code","caf9e042":"code","faff5f15":"code","3e0735e0":"code","e84b9fbe":"code","abe9e5cf":"markdown"},"source":{"a69f5848":"!pip install transformers","48cc2917":"import os\nimport random\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nfrom transformers import BertConfig, BertTokenizer, BertModel, BertPreTrainedModel, AdamW, WarmupLinearSchedule\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr","43c96417":"SEED = 2019\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(SEED)","6804bebd":"ROOT = '..\/input\/google-quest-challenge\/'\n\ndata_df = pd.read_csv(ROOT+'train.csv')\ntest_df = pd.read_csv(ROOT+'test.csv')\nsubmission_df = pd.read_csv(ROOT+'sample_submission.csv')","031cce12":"N_FOLDS = 5\n\nkf = KFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\nsplits = kf.split(data_df)\n\ndef get_fold(fold):    \n    for i, (train_index, valid_index) in enumerate(splits):\n        if i == fold:\n            return data_df.iloc[train_index], data_df.iloc[valid_index]\n        \ntrain_df, valid_df = get_fold(0)","d7a23741":"pretrained_weights = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)","42a75285":"QLEN = 150\nALEN = 359\n\nNUM_LABEL = 30\n\ndef encode_text(questions, answers):\n    \n    X = []\n    \n    for q, a in tqdm(zip(questions, answers)):\n        q_en = tokenizer.encode(q, add_special_tokens=False, max_length=QLEN)\n        q_en = q_en + [0]*(QLEN-len(q_en))\n        \n        a_en = tokenizer.encode(a, add_special_tokens=False, max_length=ALEN)\n        a_en = a_en + [0]*(ALEN-len(a_en))\n        \n        X.append(tokenizer.build_inputs_with_special_tokens(q_en, a_en))\n        \n    return np.array(X)\n\ntrain_X = encode_text(train_df['question_body'], train_df['answer'])\ntrain_y = train_df[train_df.columns[-NUM_LABEL:]].values\n\nvalid_X = encode_text(valid_df['question_body'], valid_df['answer'])\nvalid_y = valid_df[valid_df.columns[-NUM_LABEL:]].values\n\ntest_X = encode_text(test_df['question_body'], test_df['answer'])","e9d4545d":"BATCH_SIZE = 12\nEPOCHS = 15\nLEARNING_RATE = 2e-5\nACCUM_STEPS = 1\nEVAL_STEPS = 500\nMAX_STEPS = -1","6f7e4f83":"train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_X, dtype=torch.long), torch.tensor(train_y, dtype=torch.float))\ntrain_sampler = torch.utils.data.RandomSampler(train_dataset)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\nvalid_dataset = torch.utils.data.TensorDataset(torch.tensor(valid_X, dtype=torch.long), torch.tensor(valid_y, dtype=torch.float))\nvalid_sampler = torch.utils.data.SequentialSampler(valid_dataset)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)","bcfb630c":"class BertForMultiLabelClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertForMultiLabelClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):                     \n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        \n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n            outputs = (loss,) + outputs\n\n        return outputs # (loss), logits, (hidden_states), (attentions)","f56e38da":"config = BertConfig.from_pretrained(pretrained_weights, num_labels=NUM_LABEL)\nmodel = BertForMultiLabelClassification.from_pretrained(pretrained_weights, config=config)","6ae0d168":"def evaluate(model):\n    preds = None\n    eval_loss = 0.0\n    nb_eval_steps = 0\n        \n    for batch in valid_loader:\n        model.eval()\n        batch = tuple(t.to(device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'labels': batch[1]}\n            outputs = model(**inputs)\n            \n            tmp_eval_loss, logits = outputs[:2]\n            eval_loss += tmp_eval_loss.mean().item()\n\n        nb_eval_steps += 1\n        \n        if preds is None:\n            preds = torch.sigmoid(logits).detach().cpu().numpy()\n        else:\n            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n\n    eval_loss = eval_loss \/ nb_eval_steps\n    \n    corr = 0\n    for i in range(NUM_LABEL):\n        corr += spearmanr(valid_y[:, i], preds[:, i]).correlation\/NUM_LABEL\n\n    return eval_loss, corr","55f898fd":"device = torch.device('cuda')\nmodel = model.to(device)\n\nmodel.zero_grad()\n\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\nif MAX_STEPS > 0:\n    t_total = MAX_STEPS\nelse:\n    t_total = len(train_loader) \/\/ ACCUM_STEPS * EPOCHS\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=1e-8)\nscheduler = WarmupLinearSchedule(optimizer, warmup_steps=1500, t_total=t_total)","921e687e":"global_step = 0\ntr_loss, logging_loss = 0.0, 0.0\n\ntq = tqdm(range(EPOCHS))\ntb_writer = SummaryWriter()\n\nbest_corr = -1.0\noutput_model_file = 'pytorch_model.bin'\n\nlossf = None\nstep_list = []\ntrain_loss_list = []\n\nvalid_loss, valid_corr = None, None\nvalid_loss_list, valid_corr_list = [], []\n\nfor _ in tq:    \n    tk0 = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n    \n    for step, batch in tk0:\n        model.train()\n        batch = tuple(t.to(device) for t in batch)\n        \n        inputs = {'input_ids': batch[0], 'labels': batch[1]}\n\n        outputs = model(**inputs)        \n        loss = outputs[0] # model outputs are always tuple in transformers (see doc)\n\n        if ACCUM_STEPS > 1:\n            loss = loss \/ ACCUM_STEPS\n\n        loss.backward()\n        tr_loss += loss.item()\n        \n        if (step + 1) % ACCUM_STEPS == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n            \n            global_step += 1\n            \n            if EVAL_STEPS > 0 and (global_step + 1) % EVAL_STEPS == 0:\n                valid_loss, valid_corr = evaluate(model)\n\n                tb_writer.add_scalar('train_loss', (tr_loss - logging_loss)\/EVAL_STEPS, global_step)\n                train_loss_list.append((tr_loss - logging_loss)\/EVAL_STEPS)                        \n\n                tb_writer.add_scalar('valid_loss', valid_loss, global_step)\n                tb_writer.add_scalar('valid_corr', valid_corr, global_step)\n\n                valid_loss_list.append(valid_loss)\n                valid_corr_list.append(valid_corr)\n\n                step_list.append(global_step)\n                logging_loss = tr_loss            \n\n                if valid_corr > best_corr:\n                    torch.save(model.state_dict(), output_model_file)\n                    best_corr = valid_corr\n                    \n        lossf = 0.98*lossf + 0.02*loss.item() if lossf else loss.item()        \n        tk0.set_postfix(loss=lossf, valid_loss=valid_loss, valid_corr=valid_corr)\n        \n        if MAX_STEPS > 0 and global_step > MAX_STEPS:\n            tk0.close()\n            break\n    \n    tq.set_postfix(loss=lossf, valid_loss=valid_loss, valid_corr=valid_corr)\n\n    if MAX_STEPS > 0 and global_step > MAX_STEPS:\n        tq.close()\n        break","caf9e042":"plt.plot(step_list, train_loss_list, label='train_loss')\nplt.plot(step_list, valid_loss_list, label='valid_loss')\nplt.plot(step_list, valid_corr_list, label='valid_corr')\nplt.legend()\nplt.ylabel('loss, corr')\nplt.xlabel('step')\nplt.show()","faff5f15":"test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_X, dtype=torch.long))\ntest_sampler = torch.utils.data.SequentialSampler(test_dataset)\ntest_loader = torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)","3e0735e0":"model_state_dict = torch.load('pytorch_model.bin')\nmodel = BertForMultiLabelClassification.from_pretrained(pretrained_weights, config=config, state_dict=model_state_dict)\nmodel.to(device)\n\npreds = None\n\nfor batch in tqdm(test_loader, desc=\"testing\"):\n    model.eval()\n    batch = tuple(t.to(device) for t in batch)\n\n    with torch.no_grad():\n        inputs = {'input_ids': batch[0]}\n        outputs = model(**inputs)\n        logits = outputs[0]\n\n    if preds is None:\n        preds = torch.sigmoid(logits).detach().cpu().numpy()\n    else:\n        preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n\npreds_df = pd.DataFrame(data=preds, columns=submission_df.columns[-NUM_LABEL:])\nsubmission_df = pd.concat([test_df['qa_id'], preds_df], axis=1)\n\nsubmission_df.to_csv('submission.csv', index=False)","e84b9fbe":"submission_df","abe9e5cf":"This is a very basic BERT baseline. I use HuggingFace's Transformers. If there are any bugs, please let me know."}}