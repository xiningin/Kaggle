{"cell_type":{"f618a9f2":"code","be7e6d88":"code","9429701c":"code","d3ec4402":"code","fc68d607":"code","f0d45d03":"code","e62e20f5":"code","7f53a8b6":"code","ed1f0834":"code","4dab623a":"code","a7f25b29":"code","dd2229e0":"code","b317ebb5":"code","474fa273":"code","75d65cca":"code","d3bf0472":"code","027dd38c":"code","bcd6b390":"markdown","f91b5951":"markdown","c6633a4e":"markdown"},"source":{"f618a9f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\nimport spacy\nimport gensim\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport nltk; nltk.download('stopwords')\nimport seaborn as sns\nimport operator\nimport pickle\nfrom nltk.corpus import stopwords\n# Any results you write to the current directory are saved as output.","be7e6d88":"sys.path.insert(0, \"..\/\")\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\njson_filenames = glob.glob(f'{root_path}\/**\/*.json', recursive=True)","9429701c":"def return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            doc_id = data['paper_id']\n            row['doc_id'] = doc_id\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [abst['text'] for abst in data['abstract']]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n    \ncorona_df = return_corona_df(json_filenames, corona_df, 'b')","d3ec4402":"# stop words\nstop_words = stopwords.words(\"english\")\nstop_words.extend(['et', 'al'])","fc68d607":"def sent_to_words(sentence):\n    \"\"\"\n    divides sentence into words and removes punctuations\n    \"\"\"\n    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n\ndef remove_stopwords(doc):\n    \"\"\"\n    removes stopwords\n    \"\"\"\n    return [word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words]\n\ndef get_bigrams(text, bigram_model):\n    \"\"\"\n    get bigrams\n    \"\"\"\n    return bigram_model[text]\n\ndef preprocess_text(text, bigram_model):\n    data_words = list(sent_to_words(text))\n    data_words = remove_stopwords(data_words)\n    data_words = get_bigrams(data_words, bigram_model)\n    return data_words","f0d45d03":"bigram_model = gensim.models.phrases.Phrases(corona_df['text_body'].to_string(), min_count=1, threshold=2)\n# clean text\ncorona_df['preprocessed_text'] = corona_df['text_body'].apply(preprocess_text, args=(bigram_model,))\ncorona_df.to_csv('corona_preprocessed.csv', index = False, header=True)","e62e20f5":"# load preprocessed csv\n#corona_df = pd.read_csv('corona_preprocessed.csv')","7f53a8b6":"# train Word2Vec model\nmodel = gensim.models.Word2Vec(corona_df['preprocessed_text'], size=100, window=5, \n                 min_count=1, workers=4)\nmodel.save(\"corona_word2vec.model\")","ed1f0834":"model = gensim.models.Word2Vec.load(\"corona_word2vec.model\")","4dab623a":"# Visulaize Word Embeddings\nfrom sklearn.manifold import TSNE\n\nkeys = ['coronavirus', 'incubation', 'asymptomatic', 'transmission', 'materials',\n       'infection', 'diagnostics', 'model', 'morbidities']\n\nembedding_clusters = []\nword_clusters = []\nfor word in keys:\n    embeddings = []\n    words = []\n    for similar_word, _ in model.most_similar(word, topn=10):\n        words.append(similar_word)\n        embeddings.append(model[similar_word])\n    embedding_clusters.append(embeddings)\n    word_clusters.append(words)\n\n\n\nembedding_clusters = np.array(embedding_clusters)\nn, m, k = embedding_clusters.shape\ntsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\nembeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n\n\ndef tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n    plt.figure(figsize=(20, 20))\n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n        plt.scatter(x, y, c=color, alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n                         textcoords='offset points', ha='right', va='bottom', size=10)\n    plt.legend(loc=4)\n    plt.title(title)\n    plt.grid(True)\n    if filename:\n        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ntsne_plot_similar_words('Similar words', keys, embeddings_en_2d, word_clusters, 0.7,\n                        'similar_words.png')","a7f25b29":"# create index to word dictionary\nid2word = gensim.corpora.Dictionary(corona_df['preprocessed_text'])\n\n# create indexed text\ncorpus = [id2word.doc2bow(text) for text in corona_df['preprocessed_text']]","dd2229e0":"n_topics = 10\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=555)","b317ebb5":"corona_df['lda_label'] =[max(lda_model[c], key=operator.itemgetter(1))[0] for c in corpus]","474fa273":"pickle.dump(lda_model, open('lda_model.pk', 'wb'))\nlda_model = pickle.load(open('lda_model.pk', 'rb'))","75d65cca":"def get_topics(model, topic_range, topn):\n    word_dict = {};\n    for i in topic_range:\n        words = model.show_topic(i, topn);\n        word_dict['Topic # ' + '{:02d}'.format(i)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict);\n\nn_words = 10\nget_topics(lda_model, range(0,n_topics), n_words)","d3bf0472":"# human topic annotation\nlda_dictionary = {0: 'experiments', 1: 'cells', 2: 'prep_latin', 3 : 'protein', 4: 'model', \n                  5: 'treatment', 6:'genetics', 7:'infection', 8:'risks', 9:'symptoms'}\n\ndef transform(num):\n    return lda_dictionary[num]\n\ncorona_df['lda_label_string'] = corona_df['lda_label'].apply(transform)","027dd38c":"ax = sns.countplot(x=\"lda_label_string\",data=corona_df)\nax.set_title('Number of text body per topic');\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');","bcd6b390":"**Preprocess Data**","f91b5951":"**Word2Vec on Corona Dataset**","c6633a4e":"**LDA - Topic Modeling**"}}