{"cell_type":{"b57707b0":"code","210b7b46":"code","37f600c1":"code","8e82582a":"code","b726dac9":"code","03a25222":"code","96aca840":"code","2d948b68":"code","64e6ba51":"code","f6a79629":"code","72e184c4":"code","2e366f4d":"code","aaa0b039":"code","32b04a4e":"code","44defdf0":"code","c345cfef":"code","11505cd3":"code","b9261182":"code","32b06411":"code","be850c39":"code","751f897b":"code","8488c9de":"code","662794d2":"code","e64923d5":"code","446bb400":"code","c9ad8d2b":"code","cddb7315":"code","adbefc8e":"code","b1ac02c8":"code","000472ab":"code","1bf97030":"code","e07f890b":"code","b4dd0ea6":"code","6fe5632e":"code","83bbd5aa":"code","9858d80d":"markdown","32df4f5f":"markdown","6eef5dd5":"markdown","f08874b4":"markdown","d6c4627e":"markdown","ddf13caf":"markdown","d01bac50":"markdown","dd790933":"markdown","d2ea96cd":"markdown","eb015652":"markdown","0ba3e702":"markdown","8c90102c":"markdown","1132a9a0":"markdown","d0221241":"markdown","2b6c03e9":"markdown","7591aa93":"markdown","1403dbef":"markdown","6e7bf979":"markdown","30dbccb4":"markdown","4d794ec1":"markdown","3200ebe5":"markdown","edec016e":"markdown","db828d19":"markdown","304af971":"markdown","a1f3106c":"markdown","a9a9b518":"markdown","fec22c92":"markdown","43bc0b58":"markdown"},"source":{"b57707b0":"!pip install seaborn==0.11.0  \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re","210b7b46":" data = pd.read_csv('..\/input\/imdb-new-dataset\/imdb_database.csv')","37f600c1":"data.head(2)","8e82582a":"data.tail(2)","b726dac9":"data.info()","03a25222":"# Renaming Movie Revenue, Time duration and Number of Votes features\ndata.rename(columns={\"Movie Revenue (M$)\":\"Box Office\",\"Time Duration (min)\":\"Length\", \"Number of Votes\":\"Votes\"}, inplace=True)","96aca840":"# Remove Parenthesis from dates\ndata[\"Movie Date\"] = data[\"Movie Date\"].str.extract(r'([\\d]{4})')\ndata[\"Serie Date\"] = data[\"Serie Date\"].str.extract(r'([\\d]{4})')\n\n# Convert Movie Type to list\ndata[\"Movie Type\"] = data[\"Movie Type\"].str.split(\",\").apply(lambda x: [y.strip() for y in x])\n\n# Convert \"None\" string to NaN\nmapping = {\"None\":np.nan}\ndata.replace(to_replace=[\"None\", None], value=np.nan, inplace=True)\n\n# Convert features to right type\ndata[\"Length\"] = data[\"Length\"].apply(lambda x: int(x) if type(x) != type(np.nan) else x)\ndata[\"Box Office\"] = data[\"Box Office\"].apply(lambda x: float(x) if type(x) != type(np.nan) else x)\ndata[\"Movie Date\"] = data[\"Movie Date\"].apply(lambda x: int(x) if type(x) != type(np.nan) else x)\ndata[\"Serie Date\"] = data[\"Serie Date\"].apply(lambda x: int(x) if type(x) != type(np.nan) else x)\ndata[\"Metascore\"] = data[\"Metascore\"].apply(lambda x: int(x) if type(x) != type(np.nan) else x)\ndata[\"Score\"] = data[\"Score\"].apply(lambda x: int(x) if type(x) != type(np.nan) else x)","2d948b68":"data.info()","64e6ba51":"data.describe(include=[\"float64\",\"int64\"]).round(1)","f6a79629":"f, ax = plt.subplots(figsize=(18,6), ncols=2)\nsns.despine(f)\nsns.histplot(\n    data.Score,\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    binwidth=0.1,\n    kde=False,\n    ax=ax[0]\n)\nsns.histplot(\n    data.Metascore,\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    binwidth=1,\n    kde=False\n)\nplt.show()","72e184c4":"f, ax = plt.subplots(figsize=(22,8), ncols=2)\nsns.despine(f)\nsns.histplot(\n    data[\"Votes\"].loc[data[\"Votes\"] < 20000],\n    binwidth=0.09,\n    log_scale=True,\n    ax=ax[0]\n)\nsns.histplot(\n    data[\"Votes\"].loc[data[\"Votes\"] > 20000],\n    binwidth=15000,\n    log_scale=[False, False]\n)\nax[0].set_ylabel(\"Count (Total = 179900)\")\nax[0].set_title(\"Distribution of Votes (< 20 000 votes)\")\n\nax[1].set_ylabel(\"Count (Total = 10000)\")\nax[1].set_xlabel(\"Votes (in million)\")\nax[1].set_title(\"Distribution of Votes (> 20 000 votes)\")\nplt.show()","2e366f4d":"f, ax = plt.subplots(figsize=(12,8))\nsns.despine(f)\nsns.histplot(\n    data[\"Length\"].loc[data[\"Length\"] < 300],\n    binwidth=3,\n    log_scale=[False, False]\n)\nax.set_ylabel(\"Count (Total = 179389)\")\nax.set_title(\"Distribution of film duration inferior to 300 minutes\")\nplt.show()","aaa0b039":"f, ax = plt.subplots(figsize=(12,8))\nsns.despine(f)\nsns.histplot(\n    data[\"Box Office\"].loc[data[\"Box Office\"] < 1000000000],\n    binwidth=0.18,\n    log_scale=[True, False]\n)\nax.set_ylabel(\"Count (Total = 18946)\")\nax.set_title(\"Log-Distribution of Box Office\")\nplt.show()","32b04a4e":"data.describe(include=[\"object\"])","44defdf0":"mapping = {\"Tous publics\":\"G-rated\", \"Tous publics\":\"G-rated\", \"Tous publics avec avertissement\":\"G-rated\", \"Approved\":\"G-rated\", \"7\":\"G-rated\", \"G\":\"G-rated\", \"12 avec avertissement\":\"PG13\", \"PG-13\":\"PG13\", \"13\":\"PG13\", \"12\":\"PG13\", \"10\":\"PG\", \"TV-PG\":\"PG\", \"M\/PG\":\"PG\", \"GP\":\"PG\", \"16\":\"R-rated\", \"R\":\"R-rated\", \"Not Rated\":\"Not Rated\", \"NC-17\":\"NC17\", \"Unrated\":\"Not Rated\", \"18\":\"NC17\", \"(Banned)\":\"Banned\", \"X\":\"NC17\"}\n\ndata[\"Restriction\"] = data.Restriction.map(mapping)\n\ndata = data.loc[data[\"Restriction\"] != \"Banned\"]","c345cfef":"data[\"Restriction\"].unique()","11505cd3":"f, ax = plt.subplots(figsize=(15,14), nrows = 3, sharex=True, sharey=False)\nsns.set_context( rc={\"font.size\":12,\"axes.titlesize\":12,\"axes.labelsize\":10,})  \nsns.despine(offset=0, trim=False, bottom=True, left=False)\nsns.boxplot(x=\"Restriction\", y=\"Box Office\", palette=\"viridis\",\n            data=data, ax=ax[0])\nsns.boxplot(x=\"Restriction\", y=\"Score\", palette=\"viridis\",\n            data=data, ax=ax[1])\nsns.boxplot(x=\"Restriction\", y=\"Metascore\", palette=\"viridis\",\n            data=data)\nax[0].set_xlabel(\"\")\nax[1].set_xlabel(\"\")\nax[2].set_xlabel(\"Restriction\")\nax[0].set_ylabel(\"Box Office in million dollars (N = 14024)\")\nax[1].set_ylabel(\"Score (N = 73257)\")\nax[2].set_ylabel(\"Metascore (N = 13814)\")\nplt.xticks(rotation= 45)\nplt.tight_layout()","b9261182":"data.loc[:,['Restriction','Box Office', 'Score', 'Metascore']].groupby([\"Restriction\"]).agg([\"count\",\"mean\",\"median\",\"std\"]).round(1)","32b06411":"g = sns.lmplot(x=\"Score\", y=\"Metascore\", hue=\"Restriction\", data=data,\n               palette=\"viridis\", col=\"Restriction\", x_estimator=np.mean, col_wrap=4, height=4)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle(\"Linear Plot of Score and Metascore, grouped by Restriction\")\nplt.show()","be850c39":"data.corr()","751f897b":"import plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=data[\"Movie Date\"],\n                                y=data['Box Office'],\n                                mode='markers',\n                                marker_color=data['Box Office'],\n                                text=data['Movie Name'])) # hover text goes here\nfig.show()","8488c9de":"data_lstm = data.copy()\ndata_lstm.drop_duplicates(subset=[\"Description\",\"Restriction\"], inplace=True)\n# # 5 categories \nmapping = {\"G-rated\":\"Grated\", \"PG13\":\"PG13\", \"PG\":\"PG\", \"Not Rated\":\"Unrated\", \"NC17\":\"NC17\", \"R-rated\":\"Rrated\"}\ndata_lstm[\"Restriction\"] = data_lstm.Restriction.map(mapping)\n# Drop NaN\ndata_lstm.dropna(subset=[\"Restriction\"], inplace=True)\n\n\n# Remove \"see full sumary\" option in description\ndata_lstm[\"Description\"] = data_lstm[\"Description\"].apply(lambda x: x.strip(\"See full summary \u00bb\"))\ndata_lstm[\"Description\"] = data_lstm[\"Description\"].apply(lambda x: x.strip())\ndata_lstm[\"Description\"] = data_lstm[\"Description\"].apply(lambda x: x.strip(\"See full summary\"))\n","662794d2":"data_lstm[\"Restriction\"].value_counts()","e64923d5":"data_lstm[\"Restriction\"].unique()","446bb400":"data_lstm.loc[data_lstm.Description.apply(lambda x: len(x.split(' '))) > 20, \"Description\"].count()","c9ad8d2b":"data_lstm.loc[data_lstm.Description.apply(lambda x: len(x.split(' '))) > 40, \"Description\"].count()","cddb7315":"data_lstm.loc[data_lstm.Description.apply(lambda x: len(x.split(' '))) > 75, [\"Description\"]].value_counts()","adbefc8e":"data_lstm = data_lstm.loc[data_lstm.Description.apply(lambda x: len(x.split(' '))) > 20, :]","b1ac02c8":"f = open(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\")","000472ab":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef tokenize_lemma_stopwords(text):\n    \"\"\"[Standard text preprocessing using tokenization and lemmatization]\n\n    Args:\n        text ([str]): [Take a text as an input. Can be apply in a DataFrame using apply functions]\n\n    Returns:\n        [str]: [Return cleaned text]\n    \"\"\"\n\n    # Initialize \n    wordnet_lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer()\n    sw = set(stopwords.words('english'))\n\n\n    text = text.replace(\"\\n\", \" \")\n    # split string into words (tokens)\n    tokens = nltk.tokenize.word_tokenize(text.lower())\n\n    # keep strings with only alphabets\n    tokens = [t for t in tokens if t.isalpha()]\n\n    # put words into base form\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n    # tokens = [stemmer.stem(t) for t in tokens]\n\n    # remove short words, they're probably not useful\n    tokens = [t for t in tokens if len(t) > 2]\n    tokens = [t for t in tokens if t not in sw] # remove stopwords\n    cleaned_text = \" \".join(tokens)\n\n    return cleaned_text\n\n\ndef basic_stopwords(text):\n    \"\"\"Basic NLP cleaning. Remove stop words from the corpus and return a list of cleaned text\"\"\"\n    sw = set(stopwords.words('english'))\n    text = text.replace(\"\\n\", \" \")\n    # split string into words (tokens)\n    tokens = nltk.tokenize.word_tokenize(text.lower())\n\n    # keep strings with only alphabets\n    tokens = [t for t in tokens if t.isalpha()]\n\n    # remove short words, they're probably not useful\n    # tokens = [t for t in tokens if len(t) > 2]\n    # tokens = [t for t in tokens if t not in sw] # remove stopwords\n    cleaned_text = \" \".join(tokens)\n    \n    return cleaned_text\n\n\ndef dataCleaning(df, tokenize=True):\n    \"\"\"Call either the strong or basic NLP cleaning function\"\"\"\n    data = df.copy()\n    if tokenize:\n        data[\"Description\"] = data[\"Description\"].apply(tokenize_lemma_stopwords)\n    else: \n        data[\"Description\"] = data[\"Description\"].apply(basic_stopwords)\n    return data","1bf97030":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef create_embedding_matrix(filepath, word_index, embedding_dim):\n    \"\"\"\n    Create the embedded matrix using Glove Stanford dataset of embedded words\n    Return [array]: an array of embedded words\n    \"\"\"\n    \n    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    file = open(filepath, encoding='utf-8')\n    for line in file:\n        word, *vector = line.split()\n        if word in word_index:\n            idx = word_index[word] \n            embedding_matrix[idx] = np.array(\n                vector, dtype=np.float32)[:embedding_dim]\n    file.close()\n\n    return embedding_matrix\n\n\ndef data_preprocessing(df, length=150, vocab_size=5000):\n    \"\"\"\n    Remove NaN, tokenize, split into train\/test set, prepare data, \n    convert word to sequence of numbers, convert labels to class-number, prepare embedded words\n    \n    Return [arrays]: multiples arrays containing numbers\n    \"\"\"\n    \n    max_length = length\n    vocab_size = vocab_size\n    trunc_type = 'post'\n    padding_type = 'post'\n    oov_tok = 'OOV'\n\n    print(\"\\nParameters:\")\n    print(\"   Max padding length:\", length)\n    print(\"   Vocabulary size:\", vocab_size)\n    print(\"   Using oov: True\\n\")\n    print(\"-Preparing data\")\n\n    print(\"-Removing NaN Values from the restriction feature\")\n    # We take the dataframe without restriction_NaN because our goal is to predict those NaN using text classification method\n    # cleanedData = df.loc[(df.Restriction == \"PG13\") | (df.Restriction == \"Grated\") | (df.Restriction == \"Rrated\"), [\"Restriction\",\"Description\"]].reset_index(drop=True)\n    cleanedData = df.loc[df[\"Restriction\"].notna(), [\"Restriction\",\"Description\"]].reset_index(drop=True)\n    cleanedData = dataCleaning(cleanedData, tokenize=False)\n\n    print(\"--Splitting into train-test set\")\n    # Splitting into train-test dataset\n    cleanedTrainData , cleanedTestData = train_test_split(cleanedData, test_size=0.3, random_state=46, shuffle=True)\n    cleanedValData, cleandedTestData = train_test_split(cleanedTestData, test_size=0.2, random_state=46, shuffle=True)\n\n    print(\"---Preparing labels, train\/test\/val Restriction (labels) and description (articles)\")\n    # Preparing arrays\n    labels = cleanedData[\"Restriction\"]\n    # Train arrays\n    train_articles = cleanedTrainData[\"Description\"].values\n    train_labels = cleanedTrainData[\"Restriction\"]\n    # Validation arrays\n    validation_articles = cleanedValData[\"Description\"].values\n    validation_labels = cleanedValData[\"Restriction\"]\n    # Test arrays \n    test_articles = cleandedTestData[\"Description\"].values\n    test_labels = cleandedTestData[\"Restriction\"]\n\n    print(\"----Tokenizing and padding description data\")\n    # Description Tokenizer\n    tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', char_level=False)\n    tokenizer.fit_on_texts(train_articles)\n    word_index = tokenizer.word_index\n\n    print(\"-----Tokenizing and padding labels data\")\n    # Label Tokenizer\n    label_tokenizer = Tokenizer()\n    label_tokenizer.fit_on_texts(labels)\n\n    print(\"------Converting words to sequence of numbers\")\n    # Train ,validation and test label sequence\n    training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n    validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n    test_labels_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))\n\n\n    print(\"-------Padding train\/test\/val data\")\n    # Train, validation and test padding\n    train_sequences = tokenizer.texts_to_sequences(train_articles)\n    validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n    test_sequences = tokenizer.texts_to_sequences(test_articles)\n\n    train_padded = pad_sequences(train_sequences, \n                                padding=padding_type, truncating=trunc_type)\n    max_length = len(train_padded[0])\n    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, \n                                padding=padding_type, truncating=trunc_type)\n    test_padded = pad_sequences(test_sequences, maxlen=max_length, \n                                padding=padding_type, truncating=trunc_type)\n\n    print(\"--------Reseting index\")\n    # Put sequence to start at index 0\n    training_label_seq = training_label_seq - 1\n    validation_label_seq = validation_label_seq - 1\n    test_labels_seq = test_labels_seq - 1\n\n    print(\"---------Preparing embedded words\")\n    embedding_matrix = create_embedding_matrix(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\", \n                       word_index, embedding_dim=100)\n\n    print(\"Returning: Labels, embedded matrix, X_train, Y_train, X_val, Y_val, X_test, Y_test\")\n    list_variables = [labels, embedding_matrix, train_padded, training_label_seq, \n                      validation_padded, validation_label_seq, test_padded, \n                      test_labels_seq]\n    \n    return list_variables","e07f890b":"# Execute all the function that we prepared earlier with this\nlength = 75\nvocab_size = 10000\nlabels, embedding_matrix, train_padded, training_label_seq, validation_padded, validation_label_seq, test_padded, test_label_seq = data_preprocessing(data_lstm, length, vocab_size)","b4dd0ea6":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, SpatialDropout1D, Dropout\nfrom tensorflow.keras.optimizers import Adam, Nadam, RMSprop\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import Sequential\n\nfrom sklearn.utils import class_weight\n\n# Do not set all gpu memory at start\ntry:\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n  # Invalid device or cannot modify virtual devices once initialized.\n  pass\n\n\n\n# Class weights\nweights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(training_label_seq),\n                                                 np.concatenate(training_label_seq))\nweights = {i : weights[i] for i in range(len(np.unique(training_label_seq)))}\n\n# Embedding dimensions\ninput_dims = len(embedding_matrix)\noutput_dims = 100\nmax_length = len(train_padded[0])\n\n# Model construction\nlstm_outs = 200\ndrop_rates = 0.2\nrecur_act = \"sigmoid\"\n\n# Optimizer\nlearning_rate_optimizer = 0.001\noptimizer = \"Adam\"\n\n# Regularizer\nregularizer_opt = \"l1\"\nregularizer_rate_regularize = 0.001\n\n# Model Fit\nepoch_opt = 80\nbatch = 32\n\n\n\n# Define model keras\ndef model_keras(input_dim, output_dim, input_length, units, recurrent_dropout, \n                learning_rate_optimizer, optimizer, kernel_regularizer, \n                regularizer_rate, embedding_matrix, reccurent_activation):\n\n    # Model construction\n    model = Sequential([\n        Embedding(input_dim=input_dim, output_dim=output_dim, \n                  input_length=input_length, \n                  weights = [embedding_matrix], \n                  trainable=False),\n        Bidirectional(LSTM(units=units, kernel_regularizer = kernel_regularizer, \n                           recurrent_dropout=recurrent_dropout, \n                           recurrent_activation=reccurent_activation)),\n        Dense(len(labels.unique()), activation='softmax'),\n    ])\n\n    # Compilation\n    model.compile(loss='sparse_categorical_crossentropy', \n                  optimizer=optimizer, \n                  metrics=['accuracy'])\n\n    return model\n\n\n# Call model\nmodel = model_keras(input_dim = input_dims, \n                    output_dim = output_dims, \n                    input_length = max_length, \n                    units = lstm_outs, \n                    recurrent_dropout = drop_rates, \n                    learning_rate_optimizer = learning_rate_optimizer, \n                    optimizer = optimizer, \n                    kernel_regularizer = regularizer_opt,\n                    regularizer_rate = regularizer_rate_regularize,\n                    embedding_matrix = embedding_matrix, \n                    reccurent_activation = recur_act)\nmodel.summary()\n\n\n# Early Stopping\nes = EarlyStopping(monitor = 'val_loss',\n                   min_delta = 0,\n                   patience = 30,\n                   verbose = 1,\n                   restore_best_weights = False)\n\n# CheckPoint\nmcp_save = ModelCheckpoint('\/model\/.mdl_wts.hdf5', \n                           save_best_only=True, \n                           monitor='val_loss', \n                           mode='min')\n\n# Reduce learning rate on plateau\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', \n                                   factor=0.1, \n                                   patience=7, \n                                   verbose=1, \n                                   min_delta=1e-5, \n                                   mode='min')\n\n# Model Fit\nprint(\"\\n\\nEpoch generation\",\"_\"*48)\nwith tf.device('\/gpu:0'):\n    history = model.fit(x = train_padded, \n                    y = training_label_seq, \n                    epochs = epoch_opt, \n                    batch_size = batch, \n                    class_weight = weights, \n                    validation_data = (validation_padded, validation_label_seq), \n                    verbose = 2, \n                    use_multiprocessing = True, \n                    callbacks = [es, mcp_save, reduce_lr_loss])\n\n# Load the best model\nmodel = load_model(\".mdl_wts.hdf5\")","6fe5632e":"train_acc = model.evaluate(train_padded, training_label_seq, verbose=0)\ntest_acc = model.evaluate(validation_padded, validation_label_seq, verbose=0)\nprint('----Model mesure----\\nTrain:\\n   acc: %.3f\\nTest:\\n   acc: %.3f' % (train_acc[1], test_acc[1]))","83bbd5aa":"def plot_graphs(history, string):\n    plt.figure(figsize=(10,6))\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","9858d80d":"**Finally lets take a look to the features with string element**","32df4f5f":"### Keras and Tensorflow (WIP)","6eef5dd5":"### Now the fun part : calling the LSTM neural network  \n\nHere, we used :\n* an embedding layer with trainable parameters\n* a bidirectionnal LSTM neural network with 200 nodes (unidirectional count),Adam optimizer, a sigmoid recurrent activation function, L1 regularization, and recurrent dropout (as it is recommanded when using LSTM because of the way this neural network work - see: Chen et al. (2017), Gal & Ghahramani (2016), Hoch & Schmidhuber (1997), Labach, Salehinejad & Valaee (2019), Semeniuta, Severyn & Barth (2016), Wu & Gu (2015))\n* a dense layer with softmax activation (to get acces to our multiple label classes)  \n* The sparse categorical crossentropy loss function, well used for multi-label classification\n\nThe classes where weighted as some had more observations than other (from ~700 to ~33 000). We monitored the accuracy and the loss value of the training dataset and check the result with the validation dataset. A unique test dataset with labels was then used at the end of the process to confirm the accuracy of the model, and avoid the pitfall of using only train and test set (as test set can be contaminated when tweeking the model). EarlyStopping and ReducingLearningRateOnPLateau methods were used to improve the training and stop it when necessary. ","f08874b4":"Here, we just want a quick grasp to our data. How many rows, how many features ? What type ? How many NaN values ? Where ?  \nFor that, we'll use several pandas methods like `.info()`, or `.describe` as well as some plots. ","d6c4627e":"Now let's see if this restriction is correlated to the variable we try to predict: ","ddf13caf":"# R\u00e9f\u00e9rences \n\n\n1. Chen, G., Ye, D., Xing, Z., Chen, J., & Cambria, E. (2017). Ensemble application of convolutional and recurrent neural networks for multi-label text categorization. 2017 International Joint Conference on Neural Networks (IJCNN), 2377\u20112383. https:\/\/doi.org\/10.1109\/IJCNN.2017.7966144\n2. Gal, Y., & Ghahramani, Z. (2016). A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. arXiv:1512.05287 [stat]. http:\/\/arxiv.org\/abs\/1512.05287\n3. Hoch, S., & Schmidhuber, J. (1997). Long Short-Term Memory.pdf. Neural Computation, 9(8), 1735\u20111780.\n4. Labach, A., Salehinejad, H., & Valaee, S. (2019). Survey of Dropout Methods for Deep Neural Networks. arXiv:1904.13310 [cs]. http:\/\/arxiv.org\/abs\/1904.13310\n5. McInnes, L., Healy, J., & Melville, J. (2020). UMAP\u202f: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv:1802.03426 [cs, stat]. http:\/\/arxiv.org\/abs\/1802.03426\n6. Semeniuta, S., Severyn, A., & Barth, E. (2016). Recurrent Dropout without Memory Loss. arXiv:1603.05118 [cs]. http:\/\/arxiv.org\/abs\/1603.05118\n7. Wu, H., & Gu, X. (2015). Towards dropout training for convolutional neural networks. Neural Networks, 71, 1\u201110. https:\/\/doi.org\/10.1016\/j.neunet.2015.07.007\n8. Yan, Y., Wang, Y., Gao, W.-C., Zhang, B.-W., Yang, C., & Yin, X.-C. (2018). LSTM\u202f: Multi-Label Ranking for Document Classification. Neural Processing Letters, 47(1), 117\u2011138. https:\/\/doi.org\/10.1007\/s11063-017-9636-0\n9. Zhang, Y., Tuo, M., Yin, Q., Qi, L., Wang, X., & Liu, T. (2020). Keywords extraction with deep neural network model. Neurocomputing, 383, 113\u2011121. https:\/\/doi.org\/10.1016\/j.neucom.2019.11.083\n\n\n\n\n","d01bac50":"# Initial Look at the data  ","dd790933":"First, let's check our movies-descriptions that has a low amount of words. We saw earlier that some had the sentence 'Add a plot' in it. It might be a good idea to remove those unwanted description from our analysis. We therefore check the number of sentences in `Description` that have less than 20\/40 words.","d2ea96cd":"The score, metascore and number of votes seems a bit skewed to the right. Remember, skew to the right, also known as negative skewness means that our data tend not to follow a normal distribution. It is important to check that, as some algorithm tend to deal well with normal data but not with skewed ones. Let's see with a graph. ","eb015652":"Our dataset contains **189900 rows** (or entries as the method says) and **14 features** with 3 different types: \n* `object` (usually string)\n* `int` (integer)\n* `float` (or double in other languages) ","0ba3e702":"How many descriptions got more than 40 words ?","8c90102c":"Now let's take a look at the time-lenght of films. The length of a film could impact the note given to a movie as well as impact the box office. Who would pay to see a movie that ends after 20 minutes, right ?  \nWe also want to see if we have shorts or really long films, which could drastically impact our later analysis... \n  \nApparently, we got long and short films. We'll se later if we need to clean that up, but we need to keep it in mind. ","1132a9a0":"What about the distribution of the variable we try to predict : the box office. ","d0221241":"Now lets take a short look to some descriptions","2b6c03e9":"How many descriptions got more than 20 words ?","7591aa93":"# Cleaning Data\n","1403dbef":"First, lets convert objects type that should be int for float to their correct type.\n* convert \"None\" string values to NaN\n* Remove parenthesis from movie date and serie date\n* Convert Movie Date, Serie Date, Metascore, Time Duration (min) to float  \n\nLet's also use this time to put our Movie type in List form. We'll need that later on. ","6e7bf979":"When using LSTM neural networks, we need to convert words to embedded vectors. To simplify, we need to convert words into a sequence of numbers that the model can process. For this, we want to remove NaN values, remove stopwords, tokenize the sequence and embbed it. ","30dbccb4":"**Now lets clean the description feature from unwanted stopwords \/ tokenize it \/ Embbed it**","4d794ec1":"Many of the restrictions are identical with typo or different name. After checking with USA and Europe laws, we can come up with those categories (cf. https:\/\/en.wikipedia.org\/wiki\/Motion_picture_content_rating_system\")\n","3200ebe5":"Metascore and Score are Higly correlated. We can see them with a lm plot. To assess their relationship, we also used Restriction as a grouping variable. As we can see, their regression line are almost reproducing a 1-to-1 ratio (which would be expected if we had a correlation of 1 or close to 1). Which mean there is a **high risk** of collinearity, and one of those variable should be dropped in further analysis. ","edec016e":"# Acquire data  \n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the dataset into Pandas DataFrame.","db828d19":"Now a quick description of our data-types, number of rows, features:","304af971":"Lets keep only descriptions with more than 20 words in it. ","a1f3106c":"Finally, let's look at our correlation table: \n* Score and metascore are closely correlated (r = 0.758)\n* Box Office and Vote are correlated (r = 0.576)\n* Box Office and Score are correlated (r = 0.49)\n* Box Office and length are correlated (r = 0.524)\n\nWe also have 1 correlation that could be checked : \n* Box office and Serie Date. Does this mean that recent Series made less money than older series ?","a9a9b518":"# Importing standard libraries","fec22c92":"First, let's take a look at our 2 first and last data. ","43bc0b58":"# Feature engineering\n\nNow its te time to deal with our data for real. As we saw with our previous data exploration, we need to deal with many features: \n* Choosing between score and metascore as they are higly correlated and we might encounter collinearity problem usign both of them. We'll also need to standardize, remove outliers and so on. \n* Fix our `Restriction feature`. \n    * It seems that G-Rated movies tend to have a broader and higher box office income than other movies. Which mean it could be a good idea to implement it into our final analysis. But we have 18k observations for our Box office Y-feature, and only  14k observations for our Restriction X-feature. We therefore got 2 choices. Either we accept to loose 4000 observations on our Y-feature to match our X-feature. Or we can try to \"guess\" the rating of those movies with some kind of technics. We'll also need to drop irrelevant categories or merge them into an \"other\" category. \n    * This lead to an other \"problem\" --> How do we guess those ratings ? Well, we could check by hand for each of the 4000 movies, and hope to find information online. Or we could use the latest progress in deeplearning classification algorithms and guess the rating using an other feature of the dataset. I'll let you guess which one.... The Movie description feature of course !\n* So now, we also need to fix our Description feature. Remove short descriptions (like the description \"Add a plot\", which is not relevent for us), lemmatize, tokenize, and pad our data. (i'll explain later what those terms means but for now, see it as a cleaning and normalizing procedure)\n* Choosing limits for our movie length. We do not want to keep series, nor anime, nor film that are too long and could be categorized as independent and\/or experimental movie (one of them got a length of more than 1 day !)\n* For director and actors, we might use OneHot encoding technic to transform those categorical into numerical variables. We'll need to dig the relationship between those features and the box office feature, but we can already suppose that well-known actors (or actors with experience) as well as well-known directors will impact, in some extend, our box office. And maybe well-known directors only produces movies with well-knows actors in it. In which case we could keep only one of those 2 features. "}}