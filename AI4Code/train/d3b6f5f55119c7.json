{"cell_type":{"17066dbb":"code","50279a03":"code","70dc8be2":"code","336c3e9b":"code","1fd105de":"code","57fd34d6":"code","bc80c657":"code","19fcfa78":"code","b1c5c123":"code","5a31af95":"code","4e4c5fd6":"code","bf2d363d":"code","d3e9fb44":"code","8ec176a2":"code","069fd634":"code","12e4679f":"code","ecd48b65":"code","32876dab":"code","53edf095":"code","a7f196c4":"code","c95059d1":"code","5ef65eae":"code","47dc9173":"code","23b33baa":"code","b4cd2681":"code","006038b5":"code","9a2942a4":"code","ab92d8eb":"code","8b87f7e0":"code","cbe8fc5c":"code","0f10c55d":"code","12185814":"code","2037a0f5":"code","31915630":"code","1e26dedf":"code","8e844b31":"code","25e107b6":"code","cb303572":"code","82cfe004":"code","8c4516d3":"code","6afd4815":"code","b943e72a":"code","2f30e857":"code","5a9bebc5":"code","03bc32cc":"code","12ebc6b8":"code","b40cb16e":"code","34c89f75":"code","4b384da6":"code","b64b6a3c":"code","8ef87784":"code","4b3a7f08":"code","0333d6d5":"code","cdb8e414":"code","ba86ec4f":"code","1c2f34ed":"code","636c5ffd":"code","70cef8c2":"code","3f3e37ae":"code","24070d2b":"code","eaca7270":"code","c0c93c66":"code","deb50933":"code","dc3f42a1":"code","f557f281":"code","c5d21caa":"code","937dc948":"code","7a7d7ee7":"code","4b3e77ff":"code","ce012e3a":"code","0684f8ae":"code","441066dc":"code","9ebe908a":"code","f0eff4e2":"code","230ba80d":"code","7fc65d80":"code","1ca962a9":"code","db051cf9":"code","de1e6734":"code","62ecc926":"code","5a4db9b3":"code","442d314b":"code","e9ca5744":"code","2dee88fc":"code","e4b6775f":"code","1e6e6bad":"code","909e274a":"code","02bc06a9":"code","ddab9be2":"code","c6b03047":"markdown","0e26ec91":"markdown","8b0bdc19":"markdown","2d5a1b50":"markdown","22635a10":"markdown","4248d9f3":"markdown","d28fc103":"markdown","d2990816":"markdown","0e81fe07":"markdown","b1fe1203":"markdown","f28b9cb3":"markdown","ff1e1c67":"markdown","05ef3912":"markdown","6cff4f90":"markdown","40453588":"markdown","7e680815":"markdown","89d7e7a8":"markdown","d180edfc":"markdown","9fce3819":"markdown","95412ec4":"markdown","dfbdc313":"markdown","ef2b8478":"markdown","507fe927":"markdown","b268a54e":"markdown","3701a3d6":"markdown","0261a249":"markdown","dde7bdb7":"markdown","6a0d7993":"markdown","129881a6":"markdown","245eeac0":"markdown","7362082b":"markdown","88712f15":"markdown","ed293db7":"markdown","0b087d8c":"markdown","e63eba11":"markdown","52c86a82":"markdown","a18e6fc0":"markdown","80811a2a":"markdown","fc2a5293":"markdown","cf6d085e":"markdown","79f59872":"markdown","391b9975":"markdown","71723f1a":"markdown","ca8e056e":"markdown","bbfd3e94":"markdown","0bc0d2b3":"markdown","399b3dbd":"markdown","df32d153":"markdown","855e6cd2":"markdown","05fa6049":"markdown","674944d9":"markdown","a7e1eab6":"markdown","9471f68b":"markdown","99c1a167":"markdown","3520f729":"markdown","6699ed50":"markdown","0e319448":"markdown","39bbad71":"markdown","cd333ca1":"markdown","21b785e3":"markdown","ef156726":"markdown","2b12e605":"markdown","258ab33f":"markdown","082d1047":"markdown","ec18822c":"markdown","5a3df3cb":"markdown","dcc34374":"markdown","01c0a295":"markdown","ad56341b":"markdown","8872be91":"markdown","ae4f23b0":"markdown","2cf6251e":"markdown","a3f46567":"markdown","c15f0454":"markdown","d756d112":"markdown","3dd8a31c":"markdown","04553ae9":"markdown","b9d7b320":"markdown","fcc1fe7c":"markdown","316c2fa1":"markdown","ac860aeb":"markdown","102c254e":"markdown","669e1319":"markdown","0dc6a631":"markdown","2c9542c6":"markdown","2d7a3518":"markdown","5b973e31":"markdown","72ed07dd":"markdown","25737ff1":"markdown","fe7b75c3":"markdown","229b77f7":"markdown","7c5f211f":"markdown","8bf2b43b":"markdown","509db4cf":"markdown","a7d74aae":"markdown","f1591a19":"markdown","844d6b3a":"markdown","e403212a":"markdown","81b4d62d":"markdown","b6a644b2":"markdown","5b2c2ef2":"markdown","de433d42":"markdown","94431dea":"markdown"},"source":{"17066dbb":"# Kaggle's Python 3 environment comes with many helpful analytics libraries installed\n# Below we import the modules we will need for our analysis, visualisation and submission of our results\n\nimport pandas as pd # data processing as a 'dataframe' (table) and .CSV file input\/output\nimport matplotlib.pyplot as plt # module used for plotting graphs\n# the below line ensures graphs display within this notebook\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split # used for splitting the data ahead of applying machine learning models\nfrom sklearn import tree # contains the `DecisionTreeClassifier()` machine learning model, as well as other useful tools","50279a03":"# We convert the provided data into Pandas dataframe objects (tables, essentially)\n\ntestandtrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv') # we know whether these passengers survived, we'll build our ML models on these data\nvalidation_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') # these are the passengers that we must estimate whether they survived or not\n\n# We will not split the 'testandtrain_df' into the separate 'test' and 'train' sets until the data has been cleaned","70dc8be2":"print(\"Training dataset (rows, columns):\", testandtrain_df.shape)\nprint(\"Validation dataset (rows, columns):\", validation_df.shape)","336c3e9b":"testandtrain_df.sample(10) # takes a random sample of rows from the dataframe","1fd105de":"validation_df.sample(10) # takes a random sample of rows from the dataframe","57fd34d6":"# first we look at the \"train.csv\" data, where 'Survived' is provided\n\ntestandtrain_df.info() # provides a basic summary of the data in the table\n\n# we can see below we have data for all 891 passengers on most features, but not all (Age, Cabin and Embarked)","bc80c657":"# and below we look at the \"test.csv\" data, where 'Survived' is missing (to be estimated by us)\n\nvalidation_df.info() # provides a basic summary of the data in the table\n\n# again we have data for all 418 passengers on most features, but not all (Age, Fare, and Cabin)","19fcfa78":"# the below \".describe()\" method provides a statistical summary of the data in the table\n\ntestandtrain_df.describe(include =\"all\") # note we add 'include = \"all\"' to avoid ignoring null\/non-numeric rows\/cols","b1c5c123":"# the below \".describe()\" method provides a statistical summary of the data in the table\n\nvalidation_df.describe(include =\"all\") # note we add 'include = \"all\"' to avoid ignoring null\/non-numeric rows\/cols","5a31af95":"validation_df[validation_df.Fare.isna()] # Filtering the \"validation_df\" by the rows with null\/NaN values in the Fare column","4e4c5fd6":"validation_df.corr()[\"Fare\"][:] # checks the Pearson correlation between 'Fare' and all other numeric columns\n\n# Unsurprisingly, it shows a decent (negative) correlation with 'Pclass'.","bf2d363d":"# for completion, we check whether this relationship also holds for the \"train.csv\" data, which it does\n\ntestandtrain_df.corr()[\"Fare\"][:]","d3e9fb44":"# first we build a new concatenated dataframe from copies of the original two\n\ntemp1_df = pd.concat([testandtrain_df.copy(), validation_df.copy()], sort = False)\n\n\n# then we delete all features except 'Fare' and 'Pclass' and delete rows with null data (should be just one)\n\ntemp1_df = temp1_df.filter([\"Pclass\", \"Fare\"])\ntemp1_df = temp1_df.dropna()\n\n# to check this has worked correctly we look at the shape of the remaining dataframe\n# it should have 1,308 (891 + 417) rows and 2 columns\n\nprint(\"Combined dataframe of all passengers except those with missing 'Fare' values\\n\", \"Shape (rows, columns):\", temp1_df.shape)","8ec176a2":"avg_fares = temp1_df.groupby('Pclass').mean()\navg_fares","069fd634":"validation_df.loc[152, \"Fare\"] = avg_fares.loc[3, 'Fare']","12e4679f":"# to confirm success, we can look again at the passenger's data and see that Fare is now filled\n\nvalidation_df[validation_df.index == 152]","ecd48b65":"testandtrain_df[testandtrain_df.Embarked.isna()] # Filtering the \"testandtrain_df\" by the rows with 'null' values in the Embarked column","32876dab":"# Below we observe the embarkation port of all the passengers (both datasets combined)\ntestandtrain_df.Embarked.value_counts() + validation_df.Embarked.value_counts()","53edf095":"testandtrain_df.loc[61, \"Embarked\"] = \"S\"\ntestandtrain_df.loc[829, \"Embarked\"] = \"S\"","a7f196c4":"# these columns would take significant work to be useful in our model\n\nfor df in [testandtrain_df, validation_df]:\n    df.drop(columns = ['Cabin', 'Ticket'], inplace = True) # inplace = True ensures the original dataframes are updated","c95059d1":"# this column is arbitrary and won't help training the ML model\n\ntestandtrain_df.drop(columns = ['PassengerId'], inplace = True) # inplace = True ensures the original dataframes are updated","5ef65eae":"testandtrain_df.info()","47dc9173":"validation_df.info()","23b33baa":"# first a reminder of the current dataframe format, looking at the 'Name' value in particular\n\ntestandtrain_df.sample(10)","b4cd2681":"# for each dataframe, we go row-by-row, extract the title, then add it to a new column 'Title'\n\nfor df in [testandtrain_df, validation_df]:\n    for row in df.index:\n        # take the text after \", \" and before \". \"\n        df.loc[row, 'Title'] = df.loc[row, 'Name'].split(\", \")[1].split(\". \")[0]","006038b5":"# check both dataframes to see if this has had the desired effect...\n\ntestandtrain_df.sample(10)","9a2942a4":"# check both dataframes to see if this has had the desired effect...\n\nvalidation_df.sample(10)","ab92d8eb":"# again we create a temporary combined dataframe, then group-by the 'Title'\ntemp2_df = pd.concat([testandtrain_df.copy(), validation_df.copy()], sort = False)\n\nprint(\"Number of unique titles (in both dataframes):\", temp2_df.Title.nunique()) # shows the number of unique values\nprint(\"Number of null titles (in both dataframes):\", temp2_df.Title.isna().sum()) # shows the number of null values\n\n# then we count the rows (by arbitrarily counting the 'Name' values)\ntemp2_df.groupby(\"Title\").count().Name","8b87f7e0":"prof_titles = [\"Capt\", \"Col\", \"Dr\", \"Major\", \"Rev\"] # military or professional titles\nother_titles = [\"Don\", \"Dona\", \"Jonkheer\", \"Lady\", \"Mlle\", \"Mme\", \"Ms\", \"Sir\", \"the Countess\"] # nobility\/foreign titles\n\nfor df in [testandtrain_df, validation_df]:\n    for row in df.index:\n        if df.loc[row, 'Title'] in prof_titles:\n            df.loc[row, 'Title'] = \"Professional\"\n        elif df.loc[row, 'Title'] in other_titles:\n            df.loc[row, 'Title'] = \"Other\"\n        # otherwise, leaves title unchanged (for Miss\/Mrs\/Master\/Mr)","cbe8fc5c":"# Below we show the count and average survival rate (0 to 1) of passengers in each title group (out of 889 total)\n# We utilise the aggregator function `.agg()` which applies the (name, function) to each row\ntestandtrain_df.groupby(\"Title\").Survived.agg([(\"Count\", \"count\"), (\"Survival (mean)\", \"mean\")], axis = \"rows\")","0f10c55d":"for df in [testandtrain_df, validation_df]:\n    df.drop(columns = ['Name'], inplace = True)","12185814":"# Again we create a temporary combined dataframe and group-by the 'Title'\n# Then we calculate the mean age for each title and assign the result to \"title_ages\"\n\ntemp3_df = pd.concat([testandtrain_df.copy(), validation_df.copy()], sort = False)\ntitle_ages = temp3_df.groupby(\"Title\").Age.mean()\ntitle_ages","2037a0f5":"# for both dataframes, we now fill any null 'Age' values using the above average \"title_ages\"\n\nfor df in [testandtrain_df, validation_df]:\n    for row in df.index:\n        if pd.isna(df.loc[row, 'Age']): # if 'Age' is null value (\"NaN\")\n            df.loc[row, 'Age'] = title_ages[df.loc[row, 'Title']] # then set to average for that passenger's title, as above","31915630":"# Below the first `sum()` counts columns with NaN values, the second sums the indiviual rows\n\nprint(\"Total null values in both dataframes:\", testandtrain_df.isna().sum().sum() + validation_df.isna().sum().sum())","1e26dedf":"testandtrain_df.corr()[\"Survived\"][:] # the Pearson correlation between survival and other numeric features\n\n# for both SibSp and Parch the correlation is near 0","8e844b31":"# for both dataframes, we create the 'Family_Onboard' feature\n# as well as then dropping the 'SibSp' and 'Parch' features\n\nfor df in [testandtrain_df, validation_df]:\n    df[\"Family_Onboard\"] = df[\"SibSp\"] + df[\"Parch\"]\n    df.drop(columns = ['SibSp', 'Parch'], inplace = True)","25e107b6":"testandtrain_df.sample(10)","cb303572":"validation_df.sample(10)","82cfe004":"testandtrain_df[testandtrain_df[\"Fare\"] == 0]","8c4516d3":"validation_df[validation_df[\"Fare\"] == 0]","6afd4815":"print(\"Training dataset (rows, columns):\", testandtrain_df.shape)\nprint(\"Validation dataset (rows, columns):\", validation_df.shape)\n\n# We still have 891 + 418 = 1,309 passengers in total.","b943e72a":"temp4_df = pd.concat([testandtrain_df.copy(), validation_df.copy()], sort = False) # create a temporary combined dataframe\ntemp4_df = temp4_df[temp4_df.Fare != 0] # filter out zero fare rows\ntemp4_df.shape # we check we have 1,292 with non-zero fares (1309 minus the 17 zero fare passengers above)\n\n# for those that notice we have an extra column, we should, this is just because the dataframes have 1 distinct column each\n# so the combined dataframe takes all the unique columns and fills the rows with \"NaN\" where necessary","2f30e857":"# below we use plt.hist to create a histogram with fare values colour-coded by passenger class\n# we will not manually select 'bins' here, so they will be automatically chosen reflecting the data spread\n\nfig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (20,10)) # we'll present charts in a 2x2 grid\nax0, ax1, ax2, ax3 = axes.flatten()\n\n# First chart (all passenger classes together)\nax0.hist(temp4_df[temp4_df[\"Pclass\"] == 1].Fare, color = \"red\", alpha = 0.4, label = \"Pclass 1\") # alpha <1 makes the bars partially transparent\nax0.hist(temp4_df[temp4_df[\"Pclass\"] == 2].Fare, color = \"green\", alpha = 0.8, label = \"Pclass 2\")\nax0.hist(temp4_df[temp4_df[\"Pclass\"] == 3].Fare, color = \"blue\", alpha = 0.6, label = \"Pclass 3\")\n\nax0.set_title(\"Histogram of fares (non-crew) - all passenger classes\")\nax0.legend()\nax0.set_xlabel(\"Fare (US$)\")\nax0.set_ylabel(\"Number of passengers\")\n\n# Second chart (passenger class 1)\nax1.hist(temp4_df[temp4_df[\"Pclass\"] == 1].Fare, color = \"red\", bins = 40)\nax1.set_title(\"Histogram of fares (non-crew) - passenger class 1\")\nax1.set_xlabel(\"Fare (US$)\")\nax1.set_ylabel(\"Number of passengers\")\n\n# Third chart (passenger class 2)\nax2.hist(temp4_df[temp4_df[\"Pclass\"] == 2].Fare, color = \"green\")\nax2.set_title(\"Histogram of fares (non-crew) - passenger class 2\")\nax2.set_xlabel(\"Fare (US$)\")\nax2.set_ylabel(\"Number of passengers\")\n\n# Fourth chart (passenger class 3)\nax3.hist(temp4_df[temp4_df[\"Pclass\"] == 3].Fare, color = \"blue\")\nax3.set_title(\"Histogram of fares (non-crew) - passenger class 3\")\nax3.set_xlabel(\"Fare (US$)\")\nax3.set_ylabel(\"Number of passengers\")\n\nplt.show()","5a9bebc5":"# Below we use a manual `for` loop to 'bin' the data so we can easily apply custom labelling, for clarity\n# However note there do exist functions for automating this process\n\nfareGroups = [\"Crew\", \"0_10\", \"10_20\", \"20_50\", \"50_100\", \"over100\"] # names for each 'bin'\/group\nfareRanges = [-1, 0, 10, 20, 50, 100, 1000] # limits for each 'bin' as decided above - e.g. \"20_50\" = (20, 50]\n\nfor df in [testandtrain_df, validation_df]:\n    df[\"FareBinned\"] = pd.cut(df.Fare, fareRanges, labels = fareGroups)","03bc32cc":"# Checking our new column has appeared as desired\n\ntestandtrain_df.sample(10)","12ebc6b8":"# Checking our new column has appeared as desired\nvalidation_df.sample(10)","b40cb16e":"temp4_df = pd.concat([testandtrain_df.copy(), validation_df.copy()], sort=False)\nprint(\"Number of passengers in each bin\/grouping (out of 1,309 - both dataframes):-\\n\")\ntemp4_df.groupby(\"FareBinned\").Fare.count() # we arbitrarily take Fare as both dataframes have values in those columns","34c89f75":"# Next, we check whether there is a clear correlation between the fare groupings we've chosen and survival\n# As we only have 'Survived' data for the training data, we do it just for that dataframe\n\ntestandtrain_df.groupby(\"FareBinned\").Survived.agg([(\"Count\", \"count\"), (\"Survival (mean)\", \"mean\")], axis = \"rows\")\n\n# The data suggests quite clear correlations in the data - especially for the crew\/low-fare passengers and the highest fare passengers","4b384da6":"# As the above was successful, we can now drop the \"Fare\" column\n\nfor df in [testandtrain_df, validation_df]:\n    df.drop(columns = [\"Fare\"], inplace = True)","b64b6a3c":"plt.hist(temp4_df.Age) # we can re-use the temporary combined dataframe from above\n\nplt.title(\"Histogram of passenger ages\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Number of passengers\")\n\nplt.show()","8ef87784":"# Next, we will look quickly at whether there is a clear correlation between age and survival\n# As we only have Survived data for the training data, we do it just for that dataframe\n\nplt.scatter(testandtrain_df.Age, testandtrain_df.Survived)\n\nplt.title(\"Survival of passengers by Age (years)\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Survival (1 = yes, 0 = no)\")\n\nplt.show()\n\n# The chart below doesn't show a strong correlation\n# Except perhaps for the very oldest passengers - significantly fewer survived","4b3a7f08":"plt.hist(temp4_df.Family_Onboard) # we can re-use the temporary combined dataframe from above\n\nplt.title(\"Histogram of number of family members onboard\")\nplt.xlabel(\"Total family members onboard\")\nplt.ylabel(\"Number of passengers\")\n\nplt.show()","0333d6d5":"# Next, we will check whether there is a clear correlation between the number of family members onboard and survival\n# As we only have Survived data for the training data, we do it just for that dataframe\n\ntestandtrain_df.groupby(\"Family_Onboard\").Survived.agg([(\"Count\", \"count\"), (\"Survival (mean)\", \"mean\")], axis = \"rows\")","cdb8e414":"ageGroups = [\"infant\",\"child\",\"teenager\",\"young_adult\",\"adult\",\"senior\"]\nageRanges = [0,5,12,19,30,58,100] # so an infant is <= 5, a child is > 5 and <=12, etc.\n \nfamilyGroups = [\"0\",\"1\",\"2\",\"3\",\"4+\"] # the number of family members a passenger has onboard\nfamilyRanges = [-1,0,1,2,3,20]\n\nfor df in [testandtrain_df, validation_df]:\n    df[\"AgeBinned\"] = pd.cut(df.Age, ageRanges, labels = ageGroups)\n    df[\"FamilyBinned\"] = pd.cut(df.Family_Onboard, familyRanges, labels = familyGroups)","ba86ec4f":"# Checking our new column has appeared as desired\ntestandtrain_df.sample(10)","1c2f34ed":"# Checking our new column has appeared as desired\nvalidation_df.sample(10)","636c5ffd":"# As the above was successful, we can now drop the \"Age\" and \"Family_Onboard\" columns\n\nfor df in [testandtrain_df, validation_df]:\n    df.drop(columns = [\"Age\", \"Family_Onboard\"], inplace = True)","70cef8c2":"testandtrain_df.sample(10)","3f3e37ae":"validation_df.sample(10)","24070d2b":"for df in [testandtrain_df, validation_df]:\n    df['Sex'] = df['Sex'].map({\"female\": 1, \"male\": 0})","eaca7270":"# the columns to be replaced with dummy variables for each unique possible value\ndummy_cols = [\"Pclass\", \"Embarked\", \"Title\", \"FareBinned\", \"AgeBinned\", \"FamilyBinned\"]\n\n# the prefixes for the respective dummy columns\ndummy_prefixes = [\"Pclass\", \"Port\", \"Title\", \"Fare\", \"Age\", \"Family\"]\n\ntestandtrain_df = pd.get_dummies(testandtrain_df, columns = dummy_cols, prefix = dummy_prefixes)\nvalidation_df = pd.get_dummies(validation_df, columns = dummy_cols, prefix = dummy_prefixes)","c0c93c66":"testandtrain_df.columns # the dataframes now have 31 columns, all binary values","deb50933":"testandtrain_df.sample(5)","dc3f42a1":"validation_df.columns # the dataframes now have 31 columns, all binary values\n# (except PassengerId in the validation dataframe)","f557f281":"validation_df.sample(5)","c5d21caa":"testandtrain_df.corr()[\"Survived\"][:].sort_values(ascending = False)","937dc948":"print(\"Survival % split for train.csv data\")\ntestandtrain_df.Survived.value_counts(normalize = True) # return the % of all values in this Series\/column","7a7d7ee7":"X = testandtrain_df.iloc[:, 1:] # all rows, all columns except the first ('Survived')\ny = testandtrain_df.iloc[:, 0] # all rows, only the first column ('Survived')\n\n# we next (randomly) split the data into a 70%\/30% split train\/test\n# random_state = (integer) fixes the random selection for later comparison (otherwise it will be a new random selection every time)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 0)\n# stratification on 'y' selected - as explained above","4b3e77ff":"X_train # 70% of the passengers from \"train.csv\", all features\/columns except 'Survived'","ce012e3a":"y_train # same passengers as above, a single feature\/column showing whether they survived","0684f8ae":"X_test # remaining 30% of the passengers from \"train.csv\", all features\/columns except 'Survived'","441066dc":"y_test # same passengers as above, a single feature\/column showing whether they survived","9ebe908a":"# we create an instance of the model with random_state fixed (to any integer) so we can recreate the result if we make changes\ndt_model = tree.DecisionTreeClassifier(random_state = 0)\n\n# next we fit the model to our training data\ndt_model.fit(X_train, y_train)","f0eff4e2":"print(\"Train Accuracy:\", dt_model.score(X_train, y_train))\nprint(\"Test Accuracy:\", dt_model.score(X_test, y_test))","230ba80d":"# changing the test:train split ratio to see impact to model accuracy\n\ntrain_accuracy = []\ntest_accuracy = []\ntrial_range = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n\nfor test_split in trial_range:\n    # for each test_split we run a temporary Decision Tree\n    temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, stratify = y, test_size = test_split, random_state=1)\n    temp_dt = tree.DecisionTreeClassifier(random_state = 0)\n    temp_dt.fit(temp_X_train, temp_y_train)\n    train_accuracy.append(temp_dt.score(temp_X_train, temp_y_train))\n    test_accuracy.append(temp_dt.score(temp_X_test, temp_y_test))\n\ntesttrainsplit_df = pd.DataFrame({\"test_split\": trial_range, \"train_acc\": train_accuracy, \"test_acc\": test_accuracy})\n\nplt.figure(figsize = (20,5))\nplt.plot(testtrainsplit_df[\"test_split\"], testtrainsplit_df[\"train_acc\"], marker=\"o\")\nplt.plot(testtrainsplit_df[\"test_split\"], testtrainsplit_df[\"test_acc\"], marker=\"o\")\nplt.xlabel(\"Ratio of test data (vs train data)\")\nplt.xticks(trial_range)\nplt.ylabel(\"Model accuracy score\")\nplt.title(\"Decision Tree model accuracy with different test:train ratios\")\nplt.legend()","7fc65d80":"train_accuracy = []\ntest_accuracy = []\ntrial_range = range(1,11)\n\nfor depth in trial_range:\n    temp_dt = tree.DecisionTreeClassifier(random_state = 0, max_depth = depth)\n    temp_dt.fit(X_train, y_train)\n    train_accuracy.append(temp_dt.score(X_train, y_train))\n    test_accuracy.append(temp_dt.score(X_test, y_test))\n    \nmaxdepth_df = pd.DataFrame({\"max_depth\": trial_range, \"train_acc\": train_accuracy, \"test_acc\": test_accuracy})\nmaxdepth_df","1ca962a9":"plt.figure(figsize=(10,5))\nplt.plot(maxdepth_df[\"max_depth\"], maxdepth_df[\"train_acc\"], marker = \"o\")\nplt.plot(maxdepth_df[\"max_depth\"], maxdepth_df[\"test_acc\"], marker = \"o\")\nplt.xlabel(\"Maximum depth of tree\")\nplt.xticks(range(0,max(trial_range) + 1))\nplt.ylabel(\"Model accuracy score\")\nplt.title(\"Decision Tree model accuracy with varying 'max_depth'\")\nplt.legend()","db051cf9":"# based on the above, we will fix max_depth for future iterations\nmax_depth_fixed = 4","de1e6734":"train_accuracy = []\ntest_accuracy = []\ntrial_range = range(2,51)\n\nfor leaves in trial_range:\n    # we use max_depth_fixed as decided above\n    temp_dt = tree.DecisionTreeClassifier(random_state = 0, max_depth = max_depth_fixed, max_leaf_nodes = leaves)\n    temp_dt.fit(X_train, y_train)\n    train_accuracy.append(temp_dt.score(X_train, y_train))\n    test_accuracy.append(temp_dt.score(X_test, y_test))\n    \nmaxleaves_df = pd.DataFrame({\"max_leaf_nodes\": trial_range, \"train_acc\": train_accuracy, \"test_acc\": test_accuracy})\n\nplt.figure(figsize = (20,5))\nplt.plot(maxleaves_df[\"max_leaf_nodes\"], maxleaves_df[\"train_acc\"], marker = \"o\")\nplt.plot(maxleaves_df[\"max_leaf_nodes\"], maxleaves_df[\"test_acc\"], marker = \"o\")\nplt.xlabel(\"Maximum number of leaf nodes\")\nplt.xticks(range(0,max(trial_range) + 2, 2))\nplt.ylabel(\"Model accuracy score\")\nplt.title(\"Decision Tree model accuracy with varying 'max_leaf_nodes'\")\nplt.legend()","62ecc926":"# based on the above, we will not set max_leaf_nodes for future iterations, we will revert to default 'None'\n\ntrain_accuracy = []\ntest_accuracy = []\ntrial_range = range(2,51)\n\nfor samples in trial_range:\n    # we keep the max_depth_fixed set to the variable we decided above\n    temp_dt = tree.DecisionTreeClassifier(random_state = 0, max_depth = max_depth_fixed, min_samples_leaf = samples)\n    temp_dt.fit(X_train, y_train)\n    train_accuracy.append(temp_dt.score(X_train, y_train))\n    test_accuracy.append(temp_dt.score(X_test, y_test))\n    \nminsamples_df = pd.DataFrame({\"min_samples_leaf\": trial_range, \"train_acc\": train_accuracy, \"test_acc\": test_accuracy})\n\nplt.figure(figsize = (20,5))\nplt.plot(minsamples_df[\"min_samples_leaf\"], minsamples_df[\"train_acc\"], marker = \"o\")\nplt.plot(minsamples_df[\"min_samples_leaf\"], minsamples_df[\"test_acc\"], marker = \"o\")\nplt.xlabel(\"Minimum samples in terminal nodes\")\nplt.xticks(range(0, max(trial_range) + 2, 2))\nplt.ylabel(\"Model accuracy score\")\nplt.title(\"Decision Tree model accuracy with varying 'min_samples_leaf'\")\nplt.legend()","5a4db9b3":"dt_model = tree.DecisionTreeClassifier(random_state = 0, max_depth = max_depth_fixed, min_samples_leaf = 10)\ndt_model.fit(X_train, y_train)\n\nprint(\"Train Accuracy:\", dt_model.score(X_train, y_train))\nprint(\"Test Accuracy:\", dt_model.score(X_test, y_test))","442d314b":"plt.figure(figsize = (25,10)) # the plot requires a decent amount of space, note if you have adjusted the max_depth beyond 3-4 it can get very hard to fit legibly!\n\n# feature_names takes the columns from our dataframe\n# class_names allows you to label the final classifications ('Survived' in this case)\n# `filled = True` means the boxes will be coloured in line with it's classification\n# (mostly survived = dark blue, mostly deceased = dark orange, 50:50 = white)\n\nmytree = tree.plot_tree(dt_model,\n                feature_names = testandtrain_df.columns[1:], \n                class_names = [\"Deceased\", \"Survived\"], \n                filled = True, fontsize = 12)","e9ca5744":"from sklearn.linear_model import LogisticRegression\n\nregression = LogisticRegression(solver = 'lbfgs')\nregression.fit(X_train, y_train)\n\nprint(\"Logistic Regression model\")\nprint(\"=\"*25)\nprint(\"Train Accuracy:\", regression.score(X_train, y_train))\nprint(\"Test Accuracy:\", regression.score(X_test, y_test))","2dee88fc":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, y_train)\n\nprint(\"Random Forest Classifier model\")\nprint(\"=\"*30)\nprint(\"Train Accuracy:\", random_forest.score(X_train, y_train))\nprint(\"Test Accuracy:\", random_forest.score(X_test, y_test))","e4b6775f":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nprint(\"XGB Classifier model\")\nprint(\"=\"*20)\nprint(\"Train Accuracy:\", xgb.score(X_train, y_train))\nprint(\"Test Accuracy:\", xgb.score(X_test, y_test))","1e6e6bad":"validation_df # the data is model-ready EXCEPT for the 'PassengerId' column - so we need to pass it into the model without that column","909e274a":"# so we create a copy of the dataframe without the first column, which we'll then feed into the model\nX_validate = validation_df.drop(\"PassengerId\", axis = 1)","02bc06a9":"# then we ask the model to predict the 'Survived' for each passenger based on those features (as learnt from the training data)\ndt_model.fit(X, y)\nprediction = dt_model.predict(X_validate)\nprediction","ddab9be2":"# finally, we need to have a .csv file for submission to Kaggle\n# so we create a dataframe adding the PassengerId data to the survival predicton - then convert to .csv\noutput = pd.DataFrame({'PassengerId': validation_df.PassengerId, 'Survived': prediction})\noutput.to_csv('my_submission.csv', index = False) # filename is used as required by the Kaggle competition","c6b03047":"## Conclusions on 'Fare' splits\nThe frequency data above suggests that we could effectively group 'Fare' data with the following groups\/'bins':-\n* **\\$0** - as discussed previously, we'll have a 'Crew' column for zero fare passengers\n* **<\\$10** - a large portion of Pclass 3 and a small number of Pclass 2\n* **\\$10**-**\\$20** - a relatively even mix of Pclass 3 and 2, some 1\n* **\\$20**-**\\$50** - a mix of all three classes\n* **\\$50**-**\\$100** - mostly Pclass 1, but some from 2 and 3\n* **>\\$100** - Pclass 1 only, a big range but low frequency so grouping will protect against potential outliers (but we assume its not erroneous)\n\n### As a sanity check we'll see how these groupings split the data.","0e26ec91":"<a id=\"section5\"><\/a>\n# Section 5: Grouping\/binning data - 'Age', 'Fare' and 'Family_Onboard'\n\n### Despite being numerical, 'Age', 'Fare' and 'Family_Onboard' are better thought of as groups\/categories\nI believe that instead of letting the models treat 'Fare' and 'Age' as continuous values, its likely that correlations relate to groupings.\n\nI.e. I suspect that chances of survival are related to being in an age group or fare group. Age is a mathematically continuous concept technically but it can have cultural 'categorical' significance that may be more important here.\n\nFor example, if someone under the age of 13 asks for a lifeboat space, they may stand more chance as they are a 'child' - but a 12 year old may not have a direct \"advantage\" over an 11 year old. Actual age and perceived age are also two different things!\n\nThe downside to this method, is that it then completely depends on what boundaries\/groupings\/'bins' we choose to use - so trial and error may be necessary.","8b0bdc19":"### 'Embarked', 'Title', 'Pclass', 'FareBinned', 'AgeBinned' and 'FamilyBinned' - adding dummy variable columns\nAs mentioned earlier, these columns are trickier as there are >2 possible options, respectively.\n\nTherefore, in order to work in machine learning models best - we want to replace these columns with binary columns representing the choices.\n\nE.g. There will be a column representing the \"Mrs\" title - it will have a 1 value for anyone with that title, and 0 for all other titles.\n\nThereby allowing ML models to calculate mathematically the correlation with that title vs other factors.","2d5a1b50":"### Sorting the titles into groups\nMachine Learning algorithms won't learn much useful from features with very limited frequency.\n\nWe therefore need to group the data in some way.\n\nThe titles reveal partial information about gender and social class - but those are already captured in other features.\n\nTherefore, for the titles outside of the top 4 (Miss, Mrs, Master, Mr), we will split into two groups:-\n1. Professionals = military titles, doctor, reverend\n2. Other = nobility titles, non-English titles, other titles","22635a10":"<a id=\"section3_2\"><\/a>\n## 3.2 Filling in the missing 'Embarked' values\nFrom the above we have seen that there are two null 'Embarked' values in the `testandtrain_df` table.","4248d9f3":"* [BACK to Table of Contents](#toc_section)","d28fc103":"Therefore we will estimate the missing 'Fare' value by looking at the average fares in each passenger class (across both datasets).","d2990816":"### First we remind ourselves of the current state of the tables","0e81fe07":"## This notebook is NOT for you, if:-\n### * **you're an intermediate-advanced coder\/data analyst\/statistician**\nI am very much a beginner and the aim was to avoid more complexity and practice the basics - which is hopefully useful for other beginners.\n\nHowever feedback is very welcome if you spot mistakes\/potential improvements!\n\n### * **you're looking for a notebook to help you get a top score**\nThis notebook achieves a respectable score (~78%) and accuracy of the model is after all the whole point of the exercise! But having an easy-to-follow method\/implementation for learning is more my intention.\n\nHence why I use just one model with manual parameter tweaking - no ensemble\/stacking or more complex techniques - which are the key to higher scores.\n\n### * **you're a COMPLETE beginner at coding\/Kaggle\/machine learning**\nThis notebook doesn't try to explain EVERYTHING from scratch, so I am assuming you know basic coding\/Python (3) and the broad concept of machine learning algorithms. Specifically, I'm not going to explain what a Decision Tree IS but I WILL show how to implement and adjust one in this format.\n\nIf you're reading this, you're probably not a COMPLETE beginner with any of the above - but just in case, definitely start here instead: https:\/\/www.kaggle.com\/c\/titanic and https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial.","b1fe1203":"* [BACK to Table of Contents](#toc_section)","f28b9cb3":"### Now we're ready to split the data so it is ready for a machine learning model\n\n**Note**: below we use a 70%:30% split between the training set\/testing set without explaining why 70:30 is chosen - just so you can first see how the split works.\n\nWe experiment with different splits in the parameter tweaking section found here: [Adjusting the test:train split ratio](#section8_1).","ff1e1c67":"<a id=\"section9_2\"><\/a>\n## How to actually get your score\nI remember it took me a while to figure out clearly how to actually submit the file and receive a score!\n\nSo in case it helps anyone else who is stuck, here are the extra steps after saving the output .csv file (above).\n\n1. Hit 'Save Version' in the top right corner of the screen\n1. Select 'Save & Run All (Commit)\n1. KEY STEP: Go to 'Advanced Settings' and select 'Save output for this version' (or 'Always')\n1. Hit 'Save'\n\n--- Now the output file is saved, and can be submitted from the 'homepage'\/'Viewer' of your notebook\n\n5. Click the number next to 'Save Version' in the top right (to 'Show versions')\n1. Hit 'Go to Viewer' in the top right\n1. Find the 'Output' shortcut on the right side of the screen\n1. This will take you to your \"my_submission.csv\" (unless you called it something else in the code above)\n1. Hit 'Submit'! (In a recent update, there many be multiple steps here, but Kaggle explains it all clearly from here!)","05ef3912":"### Family Onboard spread and correlation with survival","6cff4f90":"### So how much data is initially empty\/null?\n * **Age**: ~20% of passengers in both the 'train.csv' and 'test.csv' - we will fill this in somehow\n * **Fare**: just 1 passenger in the 'test.csv' - we will fill this in somehow\n * **Cabin**: ~78% of passengers in both the 'train.csv' and 'test.csv' - most of the data is missing, so we will ignore this feature\n * **Embarked**: just 2 passengers in the 'train.csv' - we will fill this in somehow","40453588":"<a id=\"section6_1\"><\/a>\n## 6.1 Using dummy variables\nFor 'Sex' the data is already binary, as the only options (for this problem) were \"female\" or \"male\".\n\nTherefore we can simply map these values to a 1 or 0 respectively - then it is ML model ready.\n\nI won't change the column name from \"Sex\", but you could now think of it as \"Female\" with a 1 for yes and 2 for no.","7e680815":"### pandas.get_dummies\nLuckily, there's a function that takes care of this for us.\n\nWe will use `pd.get_dummies()` to create some new columns representing:-\n* **'Pclass'** - 1, 2 and 3\n* **'Embarked'** - Port of Embarkation (C = Cherbourg, Q = Queenstown and S = Southampton)\n* **'Title'** - Mrs, Miss, Mr, Master, Professional and Other\n* **'FareBinned'** - Exactly 0 ('Crew'), between \\$0**-**10, \\$10**-**20, \\$20**-**50, \\$50**-**100 and over \\$100\n* **'AgeBinned'** - infant (0-4 yrs old), child (5-12),teenager (13-19), young adult (20-30), adult (31-58) and senior (over 58)\n* **'FamilyBinned'** - other family members the passenger has onboard (0, 1, 2, 3, 4+)","89d7e7a8":"* [BACK to Table of Contents](#toc_section)","d180edfc":"### Conclusion\nFrom the above we can see the model accuracy improves significantly at max_depth of 2-3, for both training and test data.\n\nIt continues to increase for the training data - though it lowers for the test data at 5 - i.e. potential overfitting.\n\nTherefore we will choose to limit it at 3 (though there are arguments for using other values).","9fce3819":"<a id=\"section9\"><\/a>\n# Section 9: Model solution and (how to) submit to Kaggle\nBelow I will show how to produce a 'solution' dataframe and then how to submit that to Kaggle and receive a score.\n\nWe've trained a (Decision Tree) model to estimate whether a passenger has survived or not based on a set of specific features (in the form of binary columns).\n\nThe competition requires us to estimate the survival of the passengers in the `validation_df` (originally the \"test.csv\").\n\nTo this point we have converted the data into the format we needed - except that we've left the 'PassengerId' column in, which the model doesn't use.","95412ec4":"Now we can show the average (mean) 'Fare' values grouped by each 'Pclass'.","dfbdc313":"Overall, besides the 0 minimum and 512 maximum fare values, there don't appear to be further (immediately obvious) outliers.\n\nFor completeness, we check the `validation_df` data too. It provides a similar picture.","ef2b8478":"Next we will look at the 'Fare' values for a combined dataframe, excluding the zero fare (i.e. crew) rows we saw above.","507fe927":"* [BACK to Table of Contents](#toc_section)","b268a54e":"* [BACK to Table of Contents](#toc_section)","3701a3d6":"### Point to note: stratification\nIn the `train_test_split()` function below you will notice we \"stratify\" the data with `y` i.e. the survival rate of the passengers.\n\nAs a reminder, tragically, less than 40% of passengers survived overall.","0261a249":"<a id=\"toc_section\"><\/a>\n# Table of Contents\n* [Introduction](#intro)\n\n* [Table of Contents](#toc_section)\n\n* [SECTION 1: IMPORT MODULES AND DOWNLOAD RAW DATA](#section1)\n\n\n* [SECTION 2: EXPLORING THE DATA](#section2)\n    1. [Previewing the data](#section2_1)\n    1. [Initial observations](#section2_2)\n    1. [Checking for null and non-numeric data](#section2_3)\n    1. [Previewing the data statistically - in particular looking for outliers](#section2_4)\n\n    \n* [SECTION 3: CLEANING AND FIXING NULL DATA](#section3)\n    1. [Filling in the missing 'Fare' value](#section3_1)\n    1. [Filling in the missing 'Embarked' values](#section3_2)\n    1. [Remove unused features - 'Cabin', 'Ticket', 'PassengerId'](#section3_3)\n\n\n* [SECTION 4: FEATURE ENGINEERING](#section4)\n    1. [Creating the 'Title' feature](#section4_1)\n    1. [Estimating the missing 'Age' values](#section4_2)\n    1. [Converting 'SibSp' and 'Parch' to more useful data - 'Family_Onboard'](#section4_3)\n\n\n* [SECTION 5: GROUPING\/BINNING DATA](#section5)\n    1. [Grouping 'Fare' values](#section5_1)\n    1. [Grouping 'Age' and 'Family_Onboard' values](#section5_2)\n\n\n* [SECTION 6: CATEGORISING DATA (NUMERICALLY)](#section6)\n    1. [Using dummy variables](#section6_1)\n    1. [Looking at simple correlations with 'Survived'](#section6_2)\n\n\n* [SECTION 7: DATA FITTING FOR MACHINE LEARNING](#section7)\n\n\n* [SECTION 8: DECISION TREE MODEL](#section8)\n    1. [Adjusting the test:train split ratio](#section8_1)\n    1. [Adjusting the maximum depth](#section8_2)\n    1. [Adjusting the maximum leaf nodes](#section8_3)\n    1. [Adjusting the minimum leaf samples](#section8_4)\n    1. [Finalised tree (with visualisation)](#section8_5)\n\n\n* [Bonus Section: Comparing accuracy vs other models](#bonus_section)\n\n\n* [SECTION 9: Model solution and (how to) submit to Kaggle](#section9)\n    1. [How to actually get your score](#section9_2)","dde7bdb7":"Data is limited for family onboard of 4+, but you can see that families of that size are highly likely not to have survived.\n\nSurvival chances are lower for lone passengers too. While the strongest survival chances were for passengers that had 3 family members onboard.","6a0d7993":"<a id=\"intro\"><\/a>\n# Introduction - a machine learning template for relative beginners (such as myself!)\nThis notebook showcases my first Kaggle competition\/machine learning notebook.\n\nAs I am still learning, any feedback is more than welcome!\n\nThe aim of this notebook is to show how to go from a basic understanding of Python, machine learning and Kaggle competitions - to producing a complete (Decision Tree) machine learning model - and submitting to Kaggle for a decent score!","129881a6":"### So from the above, we shall choose the following groupings\/bins\nNot a precise scientific solution - you can use trial and error to adjust any of the below. Or as mentioned before, once you're more comfortable with machine learning techniques, apply a more advanced statistical (possibly automated) methodology.","245eeac0":"<a id=\"section8_3\"><\/a>\n## 8.3 Adjusting the tree model's maximum leaf nodes","7362082b":"## This notebook MIGHT be for you, if:-\n### * **you want to keep it SIMPLE - one ML model, limited visualisation, no ensembling or advanced techniques**\nThis notebook only uses 3 main Python modules:\n1. **pandas** - for all the data processing\n1. **sklearn** - for the machine learning model and related tools (incl. how to visualise it)\n1. **matplotlib** - for (very basic) data visualisation during the data exploration phase.\n\nTHAT'S IT!\n\n### * **you're familiar with basic data analysis\/ML concepts but want to see a clear and detailed example**\nCore concepts of this notebook:\n1. **data exploration** - checking for null data\/outliers and methods for deleting\/fixing them\n1. **feature engineering** - extracting data and converting features into a ML-model ready format\n1. **grouping\/categorising data** - 'binning'\/grouping data, binary classification\n1. **Decision Tree machine learning** - full implementation with some manual parameter adjustment and visualisation of the result\n\n*Note - the majority of this notebook isn't about specifically implementing a Decision Tree model - it can be a template for prepping the data for most other models too!*\n\n\n### * **you're after a clear & detailed notebook - from initial data download to full submission**\nThis notebook was definitely written with the idea of explaining all the steps taken in the implementation of a machine learning project and Kaggle competition submission.\n\nI also often print out what's happening behind the scenes as we go, so you can easily follow all the changes and (hopefully) see why each step is taken.\n\nHOWEVER - there is a full TABLE OF CONTENTS with links - so feel free to skip through the notebook quickly too!","88712f15":"### Final part - delete the 'Name' column\nNow that we've extracted the 'Title' information we won't be using the 'Name' data any further.","ed293db7":"### Sanity check before moving on\nWe should now have two tables with **no null data** EXCEPT in 'Age' (dealt with in the next section).\n\nWe removed two columns ('Cabin' and 'Ticket') from both tables and in addition one more from the `testandtrain_df` table ('PassengerId').\n\nSo we should have 9 columns in both tables now.\n\nThe only difference being that the 1st column is 'Survived' for the training data and it is 'PassengerId' in the validation data.\n\nWe kept all rows so it should still show 891 passengers in the `testandtrain_df` and 418 in `validation_df`.","0b087d8c":"### Sanity check - there should now be no null\/'NaN' values in either table\nBelow we do a count of any null values in both dataframes and sum for a total - it should now equal 0.","e63eba11":"## Adjusting the parameters\nBelow, for the purposes of seeing what happens behind-the-scenes, we will show how the accuracy of the model changes as we adjust some of the above default parameters.\n\n**Note**: there unsurprisingly exist functions to do parameter adjustment automatically - for multiple models simultaneously!\n\nThe below is done as a learning exercise primarily and to show the importance\/impact of certain parameter tweaks.\n\nI don't explore changing everything, I've just taken a selection of the most obvious parameters. For example I haven't changed `criterion` - hopefully you've read up on what it means at least - but FYI it doesn't make much difference in this example!","52c86a82":"* [BACK to Table of Contents](#toc_section)","a18e6fc0":"## Download raw data\nData is provided by Kaggle and to begin we download the data into the initial (`pandas`) 'dataframe' tables.\n\n* ***\"train.csv\"*** = the passengers that we know whether they survived or not\n* ***\"test.csv\"*** = the passengers we'll be estimating survival and submitting our answers to Kaggle\n\n***Note*** that unlike the default name assignment Kaggle provides, I will call the \"test.csv\" file \"*validation_df*\" and the \"train.csv\" will be called '*testandtrain_df*'.\n\nThis is because we will later split the \"train.csv\" data into separate 'test' and 'train' sets. This is because we want to hide some of the 'answers' from the model so we can measure how well our model works on data it hasn't seen yet - BEFORE we send our solution to Kaggle.","80811a2a":"* [BACK to Table of Contents](#toc_section)","fc2a5293":"<a id=\"section4_3\"><\/a>\n## 4.3 Converting 'SibSp' and 'Parch' to more useful data - 'Family_Onboard'\nA quick look at basic correlations between 'Survived' and other factors at this stage, shows no real correlation for 'SibSp' (siblings + spouses) nor 'Parch' (parents + children).","cf6d085e":"<a id=\"section2\"><\/a>\n# Section 2: Exploring the data\n<a id=\"section2_1\"><\/a>\n## 2.1 Previewing the data\n*(If you're already very familiar with this challenge and the dataset - skip ahead to [Section 3](#section3)!)*\n\nBelow we will look at the shape of the data and preview the columns\/rows for clarity of the format.\n\n| Column | Description |\n| :--- | ------- |\n| **PassengerId** | Arbitrary passenger unique integer identifier |\n| **Survived** | Survival (0 = No, 1 = Yes) |\n| **Pclass** | Passenger ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) |\n| **Sex** | Gender (male\/female) |\n| **Age** | Age in years |\n| **SibSp** | Number of siblings \/ spouses aboard |\n| **ParCh** | Number of parents \/ children aboard |\n| **Ticket** | Ticket number\/code |\n| **Fare** | Ticket fare in US dollars |\n| **Cabin** | Cabin number\/code |\n| **Embarked** | Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) |","79f59872":"### Conclusion\nThe models work best overall without a limit to the maximum number of leaf nodes - unless we overfit to very specific values - the default is therefore fine on this parameter.","391b9975":"The default parameter values are shown above - we will see what happens when we adjust them just below.\n\nBut first we will see how this model predicts survival on the training data, and the unseen test data.","71723f1a":"So for 'Pclass', for example, each passenger will have a new column created for each of the potential options - Pclass_1, Pclass_2 and Pclass_3.\n\nAnd so for each passenger, exactly one of those columns will have a '1' representing its respective passenger class - and two '0' values in the other columns.\n\nThe `.get_dummies()` can do this for all our chosen columns in one go - and deletes the old columns automatically too. Pretty neat!","ca8e056e":"* [BACK to Table of Contents](#toc_section)","bbfd3e94":"<a id=\"section6\"><\/a>\n# Section 6: Categorising data (numerically)\nMany machine learning models require all the data to be in numerical form.\n\nThere is also a benefit (often) to having 'normalised'\/'standardised' data - converting the data to a small range of number of a similar 'size' so that ML models don't accidentally give one feature a higher 'weight' than another (unless you want it to).\n\nTo solve both of these issues in one go we will convert our features to consider binary data.\n\nOne column where that will be particularly simple is: 'Sex' - here we can use '1' and '0' to represent whether the passenger \"*Is female?*\".\n\nSimilarly for categorical\/grouped data with more than 2 options, e.g.'Title', for each option we'll create a feature\/column that determines whether this passenger has that title '1' or not '0'.\n\nThen we can observe simple correlation data with each feature - and we can begin using ML models.\n\nEven data such as 'Pclass', which is already numerical, should in fact be categorical - otherwise a model might treat Pclass of 3 with triple 'weight'\/'importance' as Pclass 1. And whilst they are numbered, their significance is not necessarily linear.\n","0bc0d2b3":"* [BACK to Table of Contents](#toc_section)","399b3dbd":"* [BACK to Table of Contents](#toc_section)","df32d153":"The above (from training data only) shows that \"Mr.\" and professional titles were likely NOT to survive.\n\nWhile the most likely titles to survive were \"Mrs\", \"Other\" and \"Miss\". Master has only a smaller positive correlation with survival.\n\nThough we note the low frequency for \"Other\" and \"Professional\" in drawing these conclusions.","855e6cd2":"<a id=\"section2_4\"><\/a>\n## 2.4 Previewing the data statistically - in particular looking for outliers\nBesides looking for null data - it's aslo useful to initially preview some of the statistical information.\n\nThe below `describe()` method provides a basic statistical summary of the data in the table.\n\nWe notice that the table shows seemingly sensible min\/max values (where it is not 'NaN').\n\nExcept possibly for 'Fare' as the minimum 0 seems strange and the max 512 could be an outlier (we'll look at them again later).\n\nThe table also shows how varied the 'Ticket' data might be, as it has 681 unique values overall.","05fa6049":"\nAnd it isn't intuitive (to me, at least!) that grouping the family member infomation in this way, is useful. Should a sibling and a spouse have equal weighting? Or a parent and a child? It strikes me it would be useful to have the data completely split, but we can't (easily) extract that data now.\n\nInstead, we will gather the data together and create a new feature\/column of \"Family_Onboard\" - a simple sum of 'SibSp' and 'Parch' to show how many of this passenger's family members in total are also onboard.\n\nPerhaps single travellers could find spare lifeboat seats more easily, or on the contrary perhaps priority was always given to big families? (In the following section, we will group\/'bin' the results)","674944d9":"So if we split the data completely randomly, there is a chance that, for example, a significantly higher portion of survivors end up in the 'test' data - and then the model wouldn't get to \"train\" on those factors. Or vice versa and it would \"overfit\" survivors and be worse at understanding those that didn't survive.\n\nThe model will work best if it has a similar ratio of survivors\/non-survivors in the training\/test samples. I do note, that I am assuming that the validation data also has a similar ratio - which we don't know, but it would make sense to assume it's similar to the overall average.\n\nSo stratifying the data to `y` means that we're telling the function to maintain a similar ratio of Survival in both the train data and the test data when it does the random splitting.\n\nWhen I first ran the model, I didn't do this, and adding this feature made a **significant improvement** to my scores.","a7e1eab6":"And we use this to fill in the missing 'Fare' value. \n\nAs it is just one value, it can easily be hard-coded directly.","9471f68b":"<a id=\"section3\"><\/a>\n# Section 3: Cleaning and fixing null data (except 'Age')\nBased on the information gathered in the intial preview of the data in the previous section, we will first:-\n* fill the **Fare** and **Embarked** data for any passengers with null\/missing data in both dataframes.\n\n* remove the **Cabin** column as it is missing for most passengers and difficult to estimate accurately.\n\n* remove the **Ticket** column as it complex to analyse and as we have socio-economic infomation from other features ('Fare' and 'Pclass').\n\n* for the \"train.csv\" data ONLY, we will remove **PassengerID** (as its arbitrary) though we keep it for \"test.csv\" to submit our results.\n\n### Note: 'Age' column fixed in Section 4\nIn [Section 4](#section4) we will fill the **Age** column with the help of some 'feature engineering', as well as convert others columns to numeric\/categorical data, and finally delete all unnecessary columns.","99c1a167":"<a id=\"section1\"><\/a>\n# Section 1: Import modules and download data\n\n*(If you're already very familiar with this challenge and the dataset - skip ahead to [Section 3](#section3)!)*","3520f729":"There are no null values, which is helpful. But there are many titles with a low frequency.","6699ed50":"## Conclusion\nYou can probably spot that some of the tree's findings are redundant. E.g. the entire right side of the tree (in my version at least) concludes that the passenger did not survive UNLESS they were a Mr. that travelled in 1st class, paid between USD 50-100 AND embarked in Port C.\n\nYou could think that a better estimation could come about from a higher maximum depth to the tree (though we checked this above) - but keep an eye on the samples in each node. When you start to draw conclusions from a small number of examples, you could risk overfitting to the training data.\n\nTo see how to submit this solution to Kaggle for the Titanic Competition - see [SECTION 9: Model solution and (how to) submit to Kaggle](#section9).","0e319448":"\nI believe, given safety procedures and 'duty' - being a crew member could have had a significant impact on survival potential.\n\nTherefore we will want to consider seperating out '0' fare passengers from simply low value fare passengers in our grouping.","39bbad71":"* [BACK to Table of Contents](#toc_section)","cd333ca1":"* [BACK to Table of Contents](#toc_section)","21b785e3":"* [BACK to Table of Contents](#toc_section)","ef156726":"We can now preview what the average survival rate of our chosen title groupings are (for training data only, of course) and double-check the group sizes.","2b12e605":"* [BACK to Table of Contents](#toc_section)","258ab33f":"<a id=\"section8_4\"><\/a>\n## 8.4 Adjusting the tree model's minimum leaf samples","082d1047":"### And how are the non-zero 'Fare' values spread? Are the high values outliers?\nIf, for example, the fare data was very clearly split in-line with the passenger class data, then it might not add incremental information beyond the 'crew'\/non-crew assumption discussed above.\n\nHowever we will see that this is not the case and the data offers potentially incremental information.\n\nFirst as a quick reminder we'll consider the shape of the overall dataframes at this point.","ec18822c":"Despite the above chart suggesting there's not a strong correlation between age and survival (directly) - we will keep and group the age data.\n\nThis is because, as discussed before, we know that age categories\/groupings could still have had an effect (in combination with other factors).","5a3df3cb":"<a id=\"section4_1\"><\/a>\n## 4.1 Creating the 'Title' feature\nLooking at the format of the 'Name' shows that the person's title can be extracted.\n\nWe do this below and create a new 'Title' column - we will then delete the 'Name' column.","dcc34374":"## Thank you so much for checking out my notebook!\nI hope it was helpful and\/or interesting.\n\nAny comments\/feedback are gratefully received.\n\nHappy learning!","01c0a295":"* [BACK to Table of Contents](#toc_section)","ad56341b":"* [BACK to Table of Contents](#toc_section)","8872be91":"<a id=\"section3_3\"><\/a>\n## 3.3 Remove unused features - 'Cabin', 'Ticket', 'PassengerId'\nBelow we drop the features we won't use, in both dataframes.\n\nAs discussed above, we will drop **Cabin** and **Ticket** columns in both datasets due to missing data and complexity.\n\nWe will also remove **PassengerId**  from the `train.csv` data as it is arbitrary and therefore  won't help with the ML models.\n\nBut we need to keep PassengerId in `test.csv` data as we use it when we submit so Kaggle can check our accuracy.","ae4f23b0":"### So based on the training model - what is our machine learning solution?\n\n*Note the below explanation only works with the precise data I have used - any changes and this may not match your model.*\n\nYou can now see how our model - which has been calculated based on the training data - will estimate whether a new passenger will survive:-\n\n* Is their title \"Mr\"?\n    1. Yes ->\n        * Were they in passenger class 1?\n            1. Yes ->\n                * Was their fare between USD 50-100?\n                    1. Yes ->\n                        * Did they embark at Port C?\n                            1. Yes = Survived\n                            1. No = Deceased\n                    1. No ->\n                        * Was their fare between USD 20-50?\n                            1. Yes = Deceased\n                            1. No = Deceased\n            1. No ->\n                * Was their fare between USD 50-100?\n                    1. Yes = Deceased\n                    1. No ->\n                         * Was their fare between USD 20-50?\n                            1. Yes = Deceased\n                            1. No = Deceased\n    1. No ->\n        * Were they in passenger class 3?\n            1. Yes ->\n                * Did they have at least 4 family members onboard?\n                    1. Yes ->\n                        * Was their sex male?\n                            1. Yes = Deceased\n                            1. No = Deceased\n                    1. No ->\n                         * Did they embark at Port S?\n                            1. Yes = Survived\n                            1. No = Survived\n            1. No ->\n                * Did they have a professional title?\n                    1. Yes = Deceased\n                    1. No ->\n                         * Was their fare between USD 50-100?\n                            1. Yes = Survived\n                            1. No = Survived","2cf6251e":"### Age spread and correlation with survival","a3f46567":"* [BACK to Table of Contents](#toc_section)","c15f0454":"* [BACK to Table of Contents](#toc_section)","d756d112":"<a id=\"section3_1\"><\/a>\n## 3.1 Filling in the missing 'Fare' value\nFrom the above we have seen that there is a single null 'Fare' value in the `validation_df` table.","3dd8a31c":"* [BACK to Table of Contents](#toc_section)","04553ae9":"### Conclusion\nThe model accuracy, both on the training and test data, remains fairly consistent after having a 15\/85 or 20\/80 split of test\/train.\n\nI therefore will stick with the 30\/70 split we have used to fit the \"dt_model\" - but this is always worth examining to avoid over\/underfitting.","b9d7b320":"<a id=\"section8_5\"><\/a>\n## 8.5 Finalised Decision Tree model parameters\nBased on the above work we conclude that the default parameters are best on both the maximum leaf nodes and the minimum number of samples.\n\nSo we only include the adjusted 'max_depth' as this had a significant difference on our results.","fcc1fe7c":"The model now achieves similar scores for both the 'test' data and the 'train' data - suggesting that it is not overfitting or underfitting either.\n\nThis indicates it should stand a good chance of achieving a (hopefully) \"similar\" score with unseen data - though certainly no guarantee!","316c2fa1":"<a id=\"section6_2\"><\/a>\n## 6.2 Looking at simple correlations with 'Survived'\nNow that we have all the data in numeric form, we can see the simple Pearson's correlation for the `train.csv`\/`testandtrain_df` passengers.\n\nIt shows that, looking simplistically at single factors alone, the strongest positive correlations of survival were for female passengers. A weaker but still notable correlation exists for 1st class passengers, more expensive tickets and small families.\n\nMen with the specific title of \"Mr.\" were likely not to survive. Weaker negative correlations can also be seen for 3rd class passengers, tickets of <$10 (but non-zero) and solo travellers.\n\nWe would expect these factors to show up prominently in our Decision Tree (or any ML algorithm) at the end - so try to recall them (or skip ahead to [SECTION 8](#section8)).","ac860aeb":"<a id=\"section2_2\"><\/a>\n## 2.2 Initial observations\n### 'Name' contains some potentially useful data:\n**Spouse\/family ties** For \"Mrs.\" women, the data gives the husband's name in full - I initially thought this could be used to match husbands and wives and see if both being onboard correlated to survival. I also wondered whether finding other members of the same family by surname and linking their survival to each other in some way would be relevant.\n\nHowever we must recall that the overall dataset we are provided by Kaggle is only a **subset** of the total passengers aboard - so not finding a husband's name in our data does NOT confirm that he wasn't onboard. There is still likely some good potential analysis that could be done here but we will ignore this information as it is far more complicated to do than I initially hoped.\n\n**Title** - \"Mr.\", \"Mrs.\", \"Doctor\" etc - likely useful and worth extracting in some format.\n    \n### Ticket seems complex given the various (non-numeric) formats\nSo I am therefore choosing to delete it entirely. I believe that the combination of Fare and Passenger Class will already capture some of the socio-economic information the ticket number\/code provides.\n\nHowever I note that with stronger data analysis capabilities than I currently possess, useful information could likely be extracted - in particular in relation to the passenger's Fare and Cabin, where that is missing.\n\n### Need to handle null and non-numeric data\nWe should check how many **null** (*\"NaN\"*) values there are in the columns - e.g. Cabin might not be useful for that reason.\n\nAnd a lot of data is non-numeric, given we will be using machine learning techniques that require numeric data, these will need to be converted or removed.\n\nFurthermore, the concept of this challenge seems to lend itself to **categorising data**, even numeric data. I believe survival was likely determined by how passengers were 'grouped' into class\/age ranges etc. Rather than having a more continuous\/linear correlation.","102c254e":"<a id=\"section5_2\"><\/a>\n## 5.2 Grouping 'Age' and 'Family_Onboard' values\nAs with 'Fare' above, we will group the 'Age' and 'Family_Onboard' data into groups\/buckets\/bins.\n\nFirst we'll chart the spread of the data so that we can decide on appropriate splits.","669e1319":"<a id=\"section4_2\"><\/a>\n## 4.2 Estimating the missing 'Age' values\nNow that we have created the new 'Title' feature, we will use the average age for each title to estimate missing 'Age' data.","0dc6a631":"<a id=\"section8_2\"><\/a>\n## 8.2 Adjusting the tree model's maximum depth","2c9542c6":"* [BACK to Table of Contents](#toc_section)","2d7a3518":"<a id=\"section4\"><\/a>\n# Section 4: Feature engineering - 'Title', 'Age' and 'Family_Onboard'\n* The **'Age'** column is now the only column with some missing data.\n\n* We will estimate this feature in relation to a person's title (e.g. \"Mrs\", \"Master\") which is contained within 'Name'.\n\n* Naturally we must first extract this data, and create a new feature **'Title'** (we will then delete 'Name').\n\n* Finally we will convert 'SibSp' and 'Parch' to a more useful metric - **'Family_Onboard'**","5b973e31":"* [BACK to Table of Contents](#toc_section)","72ed07dd":"<a id=\"section7\"><\/a>\n# Section 7: Data fitting for machine learning\nFirst we split the `train.csv`\/`testandtrain_df` data for feeding into the ML models.\n\nWe will purposefully only train on a portion of the data - to avoid our models 'overfitting' (i.e. \"memorising\" the full \"train.csv\" dataset and not being good at estimating unseen new data.\n\nThe ultimate purpose when we use the ML models to estimate the unseen `test.csv`\/`validation_df` data.\n\nWe use the conventional notation of `y = f(X)` i.e. `y` is our \"target\" ('Survived': 1 = yes, 0 = no) which we want to model as a 'function of'\/'dependent on' `X`, our list of features\/columns.\n\n","25737ff1":"### Conclusion\nThe models work best with a low limit to the minimum number of terminal nodes - so we will set that below in the final model.","fe7b75c3":"<a id=\"section2_3\"><\/a>\n## 2.3 Checking for null and non-numeric values\nDecision Trees, as well as many other machine learning algorithms, require full datasets i.e. for all passengers and features\/columns. We will also require all data to be numeric - we can't model the text information until we convert it somehow (see [SECTION 6](#section6)).\n\nSo now we will look at some overviews\/statistics of the data, so we know what will need fixing.\n\nWe can also then decide whether any missing data can be estimated or if certain passengers (rows) or features (columns) should be deleted.","229b77f7":"* [BACK to Table of Contents](#toc_section)","7c5f211f":"### It worked, but...\nHow many titles are there exactly, and has this created any null values?","8bf2b43b":"### Conclusion\nA solid score! But the much stronger training data score could actually indicate a bit of overfitting.\n\nWe would rather have more similar scores and higher for the test data - that's giving us the best indications of how it might perform with the validation data!","509db4cf":"<a id=\"section8_1\"><\/a>\n## 8.1 Adjusting the test:train split ratio","a7d74aae":"### Plotting the data\n**Please note**: I'm no expert on data visualisation and I've created the below through trial and error.\n\nThe aim of the below charts is to help decide on\/visualise the spread of the data - but there is of course far more that can be done here.","f1591a19":"* [BACK to Table of Contents](#toc_section)","844d6b3a":"<a id=\"bonus_section\"><\/a>\n# Bonus Section: Comparing accuracy and applying other models\n\nI know I promised to only use one model in this notebook - so feel free to ignore this section! - but I thought it was important to provide some context to the Decision Tree scores AND to show that the groundwork used up to this point can also easily be applied to other models.\n\nBut as this is a little off-topic bonus, there's no detailed explanation of the models and I'm not tweaking any parameters - just using basic defaults (I have specified a couple of parameters to the default, only to avoid FutureWarning messages).\n\nAnd therefore these scores should be compared only to the initial scores of the Decision Tree - PRE parameter-tweaking.\n\n#### Overall - the Decision Tree is slightly less accurate on the unseen TEST data than these other methods, pre-tweaking, but not drastically.","e403212a":"<a id=\"section8\"><\/a>\n# Section 8: Decision Tree model\nAs mentioned in the introduction, I don't intend to explain what a Decision Tree model IS - but how to apply one and tweak one (manually).\n\nCreating the model, after all the work done so far, is thankfully quite easy.\n\n### Why use the Decision Tree rather than any other model especially?\nAs it was one of the simpler models for me to understand, personally - and it is one of the easiest to understand the 'solution' and visualise.\n\nBut the work done above can also be used for many other models too! See the [Bonus Section](#bonus_section).","81b4d62d":"## Decision Tree visualised and explained\nA recent update in the `sklearn` module means you can now quite easily plot your Decision Tree to truly understand how it works.\n\n\n### How to read the node visualisation (in this binary example)\n- **[Feature condition]** used to decide the child nodes e.g. if 'Title_Mr' <= 0.5 (i.e. = 0), left child, else (Title_Mr = 1) right child)\n- **gini** Gini impurity (the lower, the better) - i.e. how 'impure' the samples in this node are in classification (in this example, of whether they Survived)\n- **samples** in this case, how many passengers are at this node (filtered by the conditions of the previous parent nodes in this path so far)\n- **value** in this model, the first figure is number of passengers (out of \"samples\" in this node) where 'Survived' is \"0\" and the second is those that survived i.e. 'Survived' is \"1\"\n- **class** as this is a classification problem, we can give names (in this example I have gone with \"Deceased\" and \"Survived\") and the model displays at each node which label fits the majority of passengers at that node. At the terminal child nodes - this selection is how the Decision Tree estimates the answer.","b6a644b2":"<a id=\"section5_1\"><\/a>\n## 5.1 Grouping 'Fare' values\n### 'Fare' values of 0 ($) indicates crew membership\nWe had earlier noticed ([Section 2.4](#section2_4)) that some of the passenger 'Fare' data showed 0.\n\nLooking at the specific passengers, it doesn't seem to be that, for example, children went for free.\n\nInstead, it is all men, without family, that embarked in the same place.\n\nI believe it is fair to assume therefore that these people were members of the crew.","5b2c2ef2":"* [BACK to Table of Contents](#toc_section)","de433d42":"In order to estimate (simply) a value for the passenger missing this data point, we will look at what (numeric) factors directly correlate highest with 'Fare' below.\n\n*(Note: once we are more accomplished at machine learning, we could in fact use a machine learning mini-model here to estimate this in a more sophisticated way!)*","94431dea":"Based on the above, we will simply assign the null values to \"S\" as this is by far the most common value.\n\nAs it is just two passengers, we will do this manually and directly."}}