{"cell_type":{"6c7a6783":"code","2cc6e262":"code","4f0b0bfd":"code","6afbe564":"code","621bd38b":"code","b3bc6aec":"code","4f2b66b6":"code","e429fa21":"code","92dfd3fc":"code","750353dc":"code","d05f03c1":"code","8e53c809":"code","d6c366ac":"code","2fa974b5":"code","530b397a":"code","d41241ea":"code","63fc69dc":"code","9f1c37f6":"code","f1e10a10":"code","7b9ceb77":"code","2bdabb41":"code","7f68af9e":"code","a4fa2624":"markdown","370f33d0":"markdown","01b14466":"markdown","efa87278":"markdown","d7e82bc4":"markdown","1c026e37":"markdown","4dbb3bc6":"markdown","c94d073f":"markdown","eb0e6652":"markdown","fd76aebd":"markdown","1909a5ba":"markdown","a6820aa1":"markdown","7293f2b9":"markdown","1adeb7e3":"markdown","8d1b3342":"markdown"},"source":{"6c7a6783":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom itertools import cycle\nplt.style.use('ggplot')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","2cc6e262":"# Look at the data names and size\n!ls -Flash --color ..\/input\/jigsaw-toxic-severity-rating\/","4f0b0bfd":"val = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\ncomments = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nss = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\nprint(f'Validation Data csv is of shape: {val.shape}')\nprint(f'Comments csv is of shape: {comments.shape}')\nprint(f'Sample submission csv is of shape: {ss.shape}')","6afbe564":"# Top 5 \"Less Toxic\" Comments.\nval['less_toxic'].value_counts() \\\n    .to_frame().head(5)","621bd38b":"# Top 5 \"More Toxic\" Comments.\nval['more_toxic'].value_counts() \\\n    .to_frame().head(5)","b3bc6aec":"all_comments = pd.concat([val['less_toxic'],\n                          val['more_toxic']]) \\\n    .reset_index(drop=True)\n\nax = pd.DataFrame(index=range(1,19)) \\\n    .merge(all_comments.value_counts() \\\n           .value_counts().to_frame(),\n           left_index=True, right_index=True, how='outer').fillna(0) \\\n    .astype('int').rename(columns={0:'Comment Frequency'}) \\\n    .plot(kind='bar',\n          figsize=(12, 5))\nplt.xticks(rotation=0)\nax.set_title('Comment Frequency in Val Dataset', fontsize=20)\nax.set_xlabel('Comment Occurance')\nax.set_ylabel('Number of Comments')\nax.legend().remove()\nplt.show()\n","4f2b66b6":"ax = val['worker'].value_counts() \\\n    .plot(kind='hist', bins=50,\n          color=color_pal[1], figsize=(12, 5))\nax.set_title('Frequeny of Worker in Val Set', fontsize=20)\nax.set_xlabel('Rows in Validation set for a Worker')","e429fa21":"# The most commonly occuring comment.\nall_comments.value_counts() \\\n    .to_frame().rename(columns={0:'Total Comment Count'}) \\\n    .head()","92dfd3fc":"# The least common comment.\nall_comments.value_counts() \\\n    .to_frame().rename(columns={0:'Total Comment Count'}) \\\n    .tail()","750353dc":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\nval['comment_pair_ordered'] = val['less_toxic'] + ' : ' + val['more_toxic']\n# The most common pair\nval['comment_pair_ordered'] \\\n    .value_counts().value_counts() \\\n    .plot(kind='bar', title='Ordered Comment Pairs',\n          color=color_pal[4], ax=ax1)\nax1.tick_params(axis='x', rotation=0)\nax1.set_ylabel('Occurance')\nax1.set_xlabel('Number of times Pair is Found in Dataset')\n\n\n# Comment Pairs in a standard alphabetical order\nval['comment_pair_not_ordered'] = val[['less_toxic','more_toxic']] \\\n    .apply(lambda x: ':'.join(np.sort(list(x))), axis=1)\nval['comment_pair_not_ordered'].value_counts().value_counts() \\\n    .sort_index() \\\n    .plot(kind='bar', title='Unordered Comment Pairs', ax=ax2,\n          color=color_pal[5])\nax2.tick_params(axis='x', rotation=0)\nax2.set_xlabel('Number of times Unordered Pair is Found in Dataset')\nplt.show()","d05f03c1":"comments['text'].isin(all_comments).mean()","8e53c809":"val_order_dict = val['comment_pair_ordered'].value_counts().to_dict()\nval['n_agreements'] = val['comment_pair_ordered'].map(val_order_dict)","d6c366ac":"val['agreement'] = val['n_agreements'].map({1: 'Reviewer Disagreed',\n                         2: 'Agreed with One Reviwer',\n                         3: 'All Three Reviewers Agreed'})\nax = val['agreement'].value_counts().plot(kind='bar', color=color_pal[5],\n                                         figsize=(12, 5))\nax.tick_params(axis='x', rotation=0)\nax.set_title('Worker Agreement', fontsize=16)\nplt.show()","2fa974b5":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n# Reviewers with the most disagreements\nval.query('n_agreements == 1')['worker'].value_counts(ascending=True) \\\n    .tail(20) \\\n    .plot(kind='barh', title='Reviewers with the Most Disagreements', ax=ax1)\n\n# Reviewers with the most disagreements\nval.query('n_agreements == 3')['worker'].value_counts(ascending=True) \\\n    .tail(20) \\\n    .plot(kind='barh', title='Reviewers with the Most Agreements', ax=ax2,\n         color=color_pal[1])\nplt.show()","530b397a":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nval['worker'].value_counts().to_frame().merge(\n    val.query('n_agreements == 1')['worker'].value_counts().to_frame(),\n    left_index=True, right_index=True\n).rename(columns={'worker_x':'Number of Reviews',\n                  'worker_y':'Number of Disagreements'}) \\\n    .plot(x='Number of Reviews', y='Number of Disagreements',\n          kind='scatter', title='Worker Reviews vs Disagreements', ax=ax1)\n\nval['worker'].value_counts().to_frame().merge(\n    val.query('n_agreements == 3')['worker'].value_counts().to_frame(),\n    left_index=True, right_index=True\n).rename(columns={'worker_x':'Number of Reviews',\n                  'worker_y':'Number of Disagreements'}) \\\n    .plot(x='Number of Reviews', y='Number of Disagreements',\n          kind='scatter', title='Worker Reviews vs Agreements', ax=ax2, color=color_pal[2])\nplt.show()","d41241ea":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nnon_toxic_comments = val['less_toxic'].value_counts() \\\n    .to_frame().head(1000)\nnon_toxic_text = ' '.join(non_toxic_comments.index.tolist())\n\ntoxic_comments = val['more_toxic'].value_counts() \\\n    .to_frame().head(1000)\ntoxic_text = ' '.join(toxic_comments.index.tolist())\n\n\nwordcloud = WordCloud(max_font_size=50, max_words=100,width=500, height=500,\n                      background_color=\"white\") \\\n    .generate(non_toxic_text)\n\n\nwordcloud2 = WordCloud(max_font_size=50, max_words=100,width=500, height=500,\n                      background_color=\"black\") \\\n    .generate(toxic_text)\n\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,15))\n\nax1.imshow(wordcloud, interpolation=\"bilinear\")\nax1.axis(\"off\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax2.axis(\"off\")\nax1.set_title('Non Toxic Comments', fontsize=25)\nax2.set_title('Toxic Comments', fontsize=25)\nplt.show()","63fc69dc":"# A cleaned dataset from the first jigsaw competition\ntox = pd.read_csv('..\/input\/cleaned-toxic-comments\/train_preprocessed.csv')\n\n# Reference: https:\/\/www.kaggle.com\/prateekarma\/logistic-regression-with-feature-engineering\n# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\npp_comments_to_score = []\n\nfor sentance in comments.text:\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    pp_comments_to_score.append(sentance.strip())","9f1c37f6":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn import svm\nfrom scipy.sparse import hstack\n\n\n# Setup a config\ncfg = {\n    'MIN_DF_TFIDF' : 15,\n    'MAX_FEATURES_TFIDF' : None,\n    'MIN_DF_COUNT': 15,\n    'MIN_FEATURES_COUNT': None,\n}\n\n# Create combined comments\ncombined_comments = pp_comments_to_score + tox.comment_text.tolist()\n# Encode the text with the TFIDF vectorizor\nvectorizer = TfidfVectorizer(min_df=cfg['MIN_DF_TFIDF'],\n                             max_features=cfg['MAX_FEATURES_TFIDF'])\nvectorizer.fit_transform(combined_comments)\n\ntrain_comments_tfidf = vectorizer.transform(tox.comment_text)\ncomments_to_score_tfidf = vectorizer.transform(pp_comments_to_score)\nprint(\"Shape of training after tfidf \", train_comments_tfidf.shape)\nprint(\"Shape of test after tfidf\", comments_to_score_tfidf.shape)\n\n\nvectorizer = CountVectorizer(min_df=cfg['MIN_DF_COUNT'],\n                             max_features=cfg['MIN_FEATURES_COUNT'])\nvectorizer.fit(combined_comments)\ntrain_comments_bow = vectorizer.transform(tox.comment_text)\ncomments_bow = vectorizer.transform(pp_comments_to_score)\n\nfeature_names_comments_bow_one_hot = vectorizer.get_feature_names()\nprint(\"Shape of matrix after bag of words\",train_comments_bow.shape)\nprint(\"Shape of matrix after bag of words\",comments_bow.shape)\n\nx_train_stack = hstack([\n    train_comments_tfidf,\n    train_comments_bow\n])\n\nx_test_stack = hstack([\n    comments_to_score_tfidf, \n    comments_bow\n])","f1e10a10":"# Linear Regression\nss_lr = ss.copy()\nlr = LinearRegression()\nlr.fit(x_train_stack, tox['toxicity'].values)\nss_lr['score'] = lr.predict(x_test_stack)\nss_lr['score'] = ss_lr['score'].rank(method='first')\nss_lr.to_csv('submission.csv', index=False)","7b9ceb77":"# Logistic Regression Model\nss_logr = ss.copy()\nlogr = LogisticRegression(solver='liblinear')\nlogr.fit(x_train_stack, tox['toxicity'].values)\nss_logr['score'] = logr.predict(x_test_stack)\nss_logr['score'] = ss_logr['score'].rank(method='first')\nss_logr.to_csv('submission-logistic.csv', index=False)","2bdabb41":"ss_blend = ss.copy()\nss_blend['score_lr'] = ss_lr['score']\nss_blend['score_logr'] = ss_logr['score']\nss_blend['score'] = ss_blend[['score_lr','score_logr']].mean(axis=1)\nss_blend['score'] = ss_blend['score'].rank(method='first')\nss_blend[['comment_id','score']].to_csv('submission-blend.csv', index=False)","7f68af9e":"ss_blend.plot(kind='scatter',\n              x='score_lr', y='score_logr',\n              figsize=(12, 12),\n              title='Logistic vs Linear Model Predictions')\nplt.show()","a4fa2624":"## Blend","370f33d0":"## Lets take a look at the data.\nWe are provided 3 csv files:\n- `validation_data.csv` - This contains pairs of rankings not from comments_to_score. It gives us an idea of how the rankings were applied. We also can learn about the annotators from this dataset.\n- `comments_to_score.csv` (aka test set)- for each comment text in this file, we need to rank these in order of toxicity.\n- `sample_submission.csv` - a sample submission file.\n","01b14466":"# Jigsaw\/Conversation AI\n## Toxic Comment Severity\n\n![toxic](https:\/\/miro.medium.com\/max\/2400\/1*j6Ys0UcwbXIFoOnm5Zydkg.png)\n\nEDA notebook, exploring the data provided for the Jigaw toxic Comment Severity competition.\n\nIn this competition we are given data from the **Wikipedia Talk page comments** dataset - and are asked to rank comments in order of toxicity.\n\nThe evaluation metric is **Average Agreement with Annotators (AAA?)** Where we must match *ranking* of the comment with that of annotators.\n\n# Follow my Twitch live coding Streams...\nThis notebook was created during a live coding stream on twitch. You can watch the video and follow for future videos here: https:\/\/www.twitch.tv\/medallionstallion_\nDuring these streams I enjoy interacting with viewers, come and ask questions.","efa87278":"## Repeated Pairs in Validation Set\nHow much workers agree and\/or disagree.\n1. Comment pairs occur in the same order 1, 2 or 3 times - but never more.\n2. When we take the comments and undo the ordering (sort them alphabetically - we find that the pairs **almost always** occur 3 times)","d7e82bc4":"## Comment occurance in the validation set.\nHow often to comments even appear in the validation set? What is the distribution, and what are the top\/least occuring comments?\n\nSome thing to note:\n1. Comments tend to occur in multiples of 3 (3, 6, 9, etc.)\n2. Most workers only score a small ammount of comments. However there are workers who score much more than the rest of the population (200+ pairs)","1c026e37":"## Validation Data\nIn this dataset we have three columns. The worker identifier - which is unique for the person ordering the pair of comments. Two columns `less_toxic` and `more_toxic` show the comments as the worker has ordered them.","4dbb3bc6":"# Comments to Grade\n- Do they appear in the validation data? Yes 100% of the **public** `all_comments` also appear in the validation data. The private data may be a different story.","c94d073f":"# Wordclouds of Toxic and Non-Toxic Comments.","eb0e6652":"# Simple Baseline Model using TFIDF and Linear Regression\n- We use a cleaned version of the dataset from the first jigsaw competition.\n- First we scrub the text dataset using some helper code\n- Then we convert text into vectorized representation using tfidf and bag of words.\n- We train logistic regression and linear regression on the `toxicity` feature.","fd76aebd":"# Compare Model Predictions\n- Both models are able to pull the REALLY Toxic comments to the top!","1909a5ba":"## Lets look at disagreement count vs. total label reviews","a6820aa1":"# Where do labelers disagree the most?\nWe now know that pairs occur three times in the validation dataset. This leads us to ask the question... are there any \"workers\" who disagree more than others?\n\n- We can create a new columns `n_agreements` to see for each row how many times the three workers had the same order for the given pair.","7293f2b9":"## Logistic Regression Model","1adeb7e3":"## Linear Regression Model","8d1b3342":"## Comments most and lest commonly ranked `less_toxic` and `more_toxic`"}}