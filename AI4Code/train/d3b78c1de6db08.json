{"cell_type":{"386dab95":"code","e2f95977":"code","ba88a474":"code","4ee2e31c":"code","ee8a2a0a":"code","283d8207":"code","f5e03a39":"code","b57cd30c":"code","0f2d511a":"code","8b237f40":"code","f9a351f3":"code","39e6ac08":"code","60e91342":"code","17162fcd":"code","e8c6fd29":"code","6dfc3cf8":"code","fbbda806":"code","2011e803":"code","7d8fcbe4":"code","35d28318":"code","fc1e9200":"code","543f2fc4":"code","a4f37fd3":"code","7952e5bb":"code","1a40591d":"code","f89af828":"code","a1932909":"code","a7f84fd1":"code","39503082":"code","c15b81b3":"code","8cf81e71":"code","9de79172":"code","3722c7f0":"code","b681db22":"code","a3b06472":"code","9df53896":"code","be3d7f55":"code","bde0d78d":"code","8e054b44":"code","b739be2e":"code","1f1f588c":"code","7d365bed":"code","8eeab8c2":"code","d9a6ed17":"markdown","5a23c135":"markdown","223ceb16":"markdown","a1d36eba":"markdown","7f6bb749":"markdown","0da4b689":"markdown","7c96dd11":"markdown","72bab537":"markdown","264806de":"markdown","ea76dc2b":"markdown","aedb1718":"markdown","0ffcfa28":"markdown","97d20028":"markdown","09b0664a":"markdown","18137442":"markdown","1d73e9fb":"markdown"},"source":{"386dab95":"! pip install nltk\n! pip install wordcloud","e2f95977":"# General packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n# NLP packages\nimport nltk\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\n\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nimport re\n\nfrom pylab import rcParams\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrcParams['figure.figsize'] = 14, 6\nplt.style.use('ggplot')","ba88a474":"hotel_reviews = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\nhotel_reviews.head(3)","4ee2e31c":"## Getting the number of words by splitting them by a space\nwords_per_review = hotel_reviews.Review.apply(lambda x: len(x.split(\" \")))\nwords_per_review.hist(bins = 100)\nplt.xlabel('Review Length (words)')\nplt.ylabel('Frequency')\nplt.show()","ee8a2a0a":"print('Average words:', words_per_review.mean())\nprint('Skewness:', words_per_review.skew())","283d8207":"percent_val = 100 * hotel_reviews['Rating'].value_counts()\/len(hotel_reviews)\npercent_val","f5e03a39":"percent_val.plot.bar()\nplt.show()","b57cd30c":"# Mapping the ratings\nhotel_reviews['Sentiment_rating'] = np.where(hotel_reviews.Rating > 3,1,0)\n\n## Removing neutral reviews \nhotel_reviews = hotel_reviews[hotel_reviews.Rating != 3]\n\n# Printing the counts of each class\nhotel_reviews['Sentiment_rating'].value_counts()","0f2d511a":"hotel_reviews.Sentiment_rating.value_counts().plot.bar()\nplt.show()","8b237f40":"hotel_reviews['reviews_text_new'] = hotel_reviews['Review'].str.lower()","f9a351f3":"from nltk import word_tokenize\n\n# Word tokenization example:\nword_tokenize(\"DPhi Bootcamp rules. It is awesome :D\")","39e6ac08":"# For reviews not converted to lower case\ntoken_lists = [word_tokenize(each) for each in hotel_reviews['Review']]\ntokens = [item for sublist in token_lists for item in sublist]\nprint(\"Number of unique tokens then: \",len(set(tokens)))\n\n# For reviews converted to lower case\ntoken_lists_lower = [word_tokenize(each) for each in hotel_reviews['reviews_text_new']]\ntokens_lower = [item for sublist in token_lists_lower for item in sublist]\nprint(\"Number of unique tokens now: \",len(set(tokens_lower)))","60e91342":"nltk.pos_tag(tokens_lower[:10])\n","17162fcd":"tagged_words= nltk.pos_tag(tokens_lower)","e8c6fd29":"from nltk.corpus import sentiwordnet as swn, wordnet as wn\nfrom nltk.corpus import wordnet","6dfc3cf8":"\n\n def analyze_sentiment_sentiwordnet_lexicon(tokens_lower, verbose=False):\n\n    # tokenize and POS tag text tokens\n    pos_score = neg_score = token_count = obj_score = 0\n    # get wordnet synsets based on POS tags\n    # get sentiment scores if synsets are found\n    for word, tag in tagged_words:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        # if senti-synset is found        \n        if ss_set:\n            # add scores for all found synsets\n            pos_score += ss_set.pos_score()\n            neg_score += ss_set.neg_score()\n            obj_score += ss_set.obj_score()\n            token_count += 1\n    \n    # aggregate final scores\n    final_score = pos_score - neg_score\n    norm_final_score = round(float(final_score) \/ token_count, 2)\n    final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n    if verbose:\n        norm_obj_score = round(float(obj_score) \/ token_count, 2)\n        norm_pos_score = round(float(pos_score) \/ token_count, 2)\n        norm_neg_score = round(float(neg_score) \/ token_count, 2)\n        # to display results in a nice table\n        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n                                         norm_neg_score, norm_final_score]],\n                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                             ['Predicted Sentiment', 'Objectivity',\n                                                              'Positive', 'Negative', 'Overall']], \n                                                             labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        display(sentiment_frame)\n        \n    return final_sentiment","fbbda806":"### Selecting non alpha numeric charactes that are not spaces\nspl_chars = hotel_reviews['reviews_text_new'].apply(lambda review: \n                                                     [char for char in list(review) if not char.isalnum() and char != ' '])\n\n## Getting list of list into a single list\nflat_list = [item for sublist in spl_chars for item in sublist]\n\n## Unique special characters\nset(flat_list)","2011e803":"review_backup = hotel_reviews['reviews_text_new'].copy()\nhotel_reviews['reviews_text_new'] = hotel_reviews['reviews_text_new'].str.replace(r'[^A-Za-z0-9]+', ' ')\n\n","7d8fcbe4":"print(\"- Old Review -\")\nprint(review_backup.values[7])\nprint(\"\\n- New Review -\")\nprint(hotel_reviews['reviews_text_new'][8])","35d28318":"hotel_reviews.head(5)","fc1e9200":"from nltk.corpus import stopwords\n\nprint('Available languages for NLTK v.3.4.5: ')\nprint(stopwords.fileids())","543f2fc4":"noise_words = []\neng_stop_words = stopwords.words('english')\neng_stop_words","a4f37fd3":"stop_words = set(eng_stop_words)\nwithout_stop_words = []\nstopword = []\nsentence = hotel_reviews['reviews_text_new'][0]\nwords = nltk.word_tokenize(sentence)\n\nfor word in words:\n    if word in stop_words:\n        stopword.append(word)\n    else:\n        without_stop_words.append(word)\n\nprint('-- Original Sentence --\\n', sentence)\nprint('\\n-- Stopwords in the sentence --\\n', stopword)\nprint('\\n-- Non-stopwords in the sentence --\\n', without_stop_words)","7952e5bb":"def stopwords_removal(stop_words, sentence):\n    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]\n\nhotel_reviews['reviews_text_nonstop'] = hotel_reviews['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))\nhotel_reviews[['reviews_text_new','reviews_text_nonstop']]","1a40591d":"print(\"- Old Review -\")\nprint(hotel_reviews['reviews_text_new'][6])\nprint(\"\\n- New Review -\")\nprint(hotel_reviews['reviews_text_nonstop'][6])","f89af828":"from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n\nword_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n# First Word tokenization\nnltk_tokens = word_tokenize(word_data)\n#Next find the roots of the word\nfor w in nltk_tokens:\n       print (\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))","a1932909":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nword_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\nnltk_tokens = nltk.word_tokenize(word_data)\nfor w in nltk_tokens:\n       print (\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))","a7f84fd1":"hotel_reviews[['Review','Rating','Sentiment_rating']].head(5)","39503082":"from nltk import ngrams\n\nsentence = 'A bird in the hand worths two in the bush'\n\nfor n in range(1, 6):\n    print(str(n) + '-grams:\\n', list(ngrams(sentence.split(), n)))","c15b81b3":"# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nX = vec.fit_transform(hotel_reviews['reviews_text_new'])\ndf = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\ndf.head()","8cf81e71":"### Creating a python object of the class CountVectorizer\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n                             stop_words=noise_words, # List of stopwords\n                             ngram_range=(1,1)) # number of n-grams\n\nbow_data = bow_counts.fit_transform(hotel_reviews['reviews_text_new'])","9de79172":"bow_data","3722c7f0":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n                                                                    hotel_reviews['Sentiment_rating'], # Target variable\n                                                                    test_size = 0.2, # 20% test size\n                                                                    random_state = 0) # random state for replication purposes","b681db22":"y_test_bow.value_counts()\/y_test_bow.shape[0]","a3b06472":"### Training the model \nlr_model_all = LogisticRegression() # Logistic regression\nlr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n\n## Predicting the output\ntest_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n\n## Calculate key performance metrics\nprint(\"F1 score: \", f1_score(y_test_bow, test_pred_lr_all))","9df53896":"### Changes with respect to the previous code\n### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)\n### 2. Including the stopwords in the bag of words features\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize,\n                             ngram_range=(1,4))\n\nbow_data = bow_counts.fit_transform(hotel_reviews.reviews_text_new)","be3d7f55":"# Notice the increase in features with inclusion of n-grams\nbow_data","bde0d78d":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n                                                                    hotel_reviews['Sentiment_rating'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0)","8e054b44":"# Defining and training the model\nlr_model_all_new = LogisticRegression(max_iter = 200)\nlr_model_all_new.fit(X_train_bow, y_train_bow)\n\n# Predicting the results\ntest_pred_lr_all = lr_model_all_new.predict(X_test_bow)\n\nprint(\"F1 score: \", f1_score(y_test_bow,test_pred_lr_all))","b739be2e":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n### Creating a python object of the class CountVectorizer\ntfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n                               stop_words=noise_words, # List of stopwords\n                               ngram_range=(1,1)) # number of n-grams\n\ntfidf_data = tfidf_counts.fit_transform(hotel_reviews['reviews_text_new'])","1f1f588c":"tfidf_data","7d365bed":"X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n                                                                            hotel_reviews['Sentiment_rating'],\n                                                                            test_size = 0.2,\n                                                                            random_state = 0)","8eeab8c2":"### Setting up the model class\nlr_model_tf_idf = LogisticRegression()\n\n## Training the model \nlr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)\n\n## Prediciting the results\ntest_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)\n\n## Evaluating the model\nprint(\"F1 score: \",f1_score(y_test_bow, test_pred_lr_all))","d9a6ed17":"## n-grams\n\n","5a23c135":"\n\n1. Converting words to lower\/upper case\n2. Removing special characters\n3. Removing stopwords and high\/low-frequency words\n4. Stemming\/lemmatization\n\n### 1. Converting words to lower\/upper case\n\n","223ceb16":"# POS\n","a1d36eba":"# Sentiment Analysis of Amazon's customer reviews\n","7f6bb749":"### 4. Stemming & lemmatization","0da4b689":"## Importing the packages for data analysis\n\n","7c96dd11":"# Applying logistic regression","72bab537":"### 2. Removing special characters","264806de":"## Pre-processing","ea76dc2b":"### Bag-of-words","aedb1718":"# Divide into training and test sets:","0ffcfa28":"### 3. Removing stop words","97d20028":"# # word tokenization","09b0664a":"## Reading the data\n\n","18137442":"## TF-IDF model","1d73e9fb":"## Building a machine learning model"}}