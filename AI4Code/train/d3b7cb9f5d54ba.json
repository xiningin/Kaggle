{"cell_type":{"1a1b2819":"code","9a41adb6":"code","f7183fc3":"code","b978fda0":"code","b53e6b01":"code","e27c73ad":"code","9354308c":"code","6d9c6aea":"code","678fc096":"code","96d25f9b":"code","d6e843f8":"code","f93ff7a1":"code","184272c1":"code","3adee3b6":"code","a9089607":"code","143fde92":"code","ae573acc":"markdown","1c877a53":"markdown","46188956":"markdown","b685d5ac":"markdown","0d973e0b":"markdown","32de0fc0":"markdown","e9426e3f":"markdown"},"source":{"1a1b2819":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline\nimport scipy\npd.options.display.max_colwidth=300","9a41adb6":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nprint(df_val.shape)\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nprint(df_sub.shape)","f7183fc3":"# df_temp = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n\n# # Give 2X Weight To Toxic Comments, followed by 3X Weight to Severe Toxic Comments\n# df_temp['severe_toxic'] = df_temp.severe_toxic * 1.5\n# df_temp['insult'] = df_temp.insult * 0.1\n# df_temp['identity_hate'] = df_temp.identity_hate * 1.5\n# df_temp['toxic'] = df_temp.toxic * 1\n# df_temp['threat'] = df_temp.threat * 1.5\n# df_temp['obscene'] = df_temp.obscene * 0.16\n\n# df_temp['y'] = df_temp[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].mean(axis=1)\n\n# print(\"severe_toxic\",np.mean(df_temp[df_temp['severe_toxic']==1.5]['y']))\n# print(\"toxic\",np.mean(df_temp[df_temp['toxic']==1]['y']))\n# print(\"insult\",np.mean(df_temp[df_temp['insult']==0.1]['y']))\n# print(\"identity_hate\",np.mean(df_temp[df_temp['identity_hate']==1.5]['y']))\n# print(\"threat\",np.mean(df_temp[df_temp['threat']==1.5]['y']))\n# print(\"obscene\",np.mean(df_temp[df_temp['obscene']==0.16]['y']))","b978fda0":"# Give 2X Weight To Toxic Comments, followed by 3X Weight to Severe Toxic Comments\ndf['severe_toxic'] = df.severe_toxic * 1.5\ndf['insult'] = df.insult * 0.1\ndf['identity_hate'] = df.identity_hate * 1.5\ndf['toxic'] = df.toxic * 1\ndf['threat'] = df.threat * 1.5\ndf['obscene'] = df.obscene * 0.16\n\ndf['y'] = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].mean(axis=1)","b53e6b01":"print(\"severe_toxic\",np.mean(df[df['severe_toxic']==1.5]['y']))\nprint(\"toxic\",np.mean(df[df['toxic']==1]['y']))\nprint(\"insult\",np.mean(df[df['insult']==0.1]['y']))\nprint(\"identity_hate\",np.mean(df[df['identity_hate']==1.5]['y']))\nprint(\"threat\",np.mean(df[df['threat']==1.5]['y']))\nprint(\"obscene\",np.mean(df[df['obscene']==0.16]['y']))","e27c73ad":"# Remove All Columns Leaving Comment Tex and Y\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})","9354308c":"# Reduce Rows with No Toxicity as our key focus area is the reviews with toxicity\ndf = pd.concat([df[df.y>0] , \n                df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\nprint(df.shape)","6d9c6aea":"df['y'].describe()","678fc096":"pipeline = Pipeline(\n    [\n        (\"vect\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        (\"clf\", Ridge()),\n    ]\n)","96d25f9b":"# Train the pipeline\npipeline.fit(df['text'], df['y'])","d6e843f8":"p1 = pipeline.predict(df_val['less_toxic'])\np2 = pipeline.predict(df_val['more_toxic'])","f93ff7a1":"# Prev Version: 65.82\nf'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}'","184272c1":"# df_val['p1'] = p1\n# df_val['p2'] = p2\n# df_val['diff'] = np.abs(p2 - p1)\n\n# df_val['correct'] = (p1 < p2).astype('int')\n\n# df_val_incorrect_preds_l = pd.DataFrame()\n# df_val_incorrect_preds_l['text'] = df_val[df_val['correct']==0]['less_toxic']\n# df_val_incorrect_preds_l['y'] = 0.2\n\n# df_val_incorrect_preds_m = pd.DataFrame()\n# df_val_incorrect_preds_m['text'] = df_val[df_val['correct']==0]['more_toxic']\n# df_val_incorrect_preds_m['y'] = 0.7\n\n# df_v2 = pd.concat([df , df_val_incorrect_preds_l, df_val_incorrect_preds_m])\n\n# print(df_v2.shape)\n\n# Train the pipeline with v2 data\n# pipeline.fit(df['text'], df['y'])\n\n# p1 = pipeline.predict(df_val['less_toxic'])\n# p2 = pipeline.predict(df_val['more_toxic'])\n\n# f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}'","3adee3b6":"# Predict using pipeline\n\nsub_preds = pipeline.predict(df_sub['text'])\n\ndf_sub['score'] = sub_preds","a9089607":"# Rank the predictions \n\ndf_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\nprint(df_sub['score'].rank().nunique())","143fde92":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","ae573acc":"# Import Dependencies","1c877a53":"# Validate the pipeline ","46188956":"# Modeling","b685d5ac":"## Correct the rank ordering","0d973e0b":"# Read Jigsaw Toxic Comment Classification DataBase","32de0fc0":"Experiment : Assing More Toxic Comments Weight to ","e9426e3f":"# Predict on test data "}}