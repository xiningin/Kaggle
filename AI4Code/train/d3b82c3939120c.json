{"cell_type":{"70c79e97":"code","c0dacef2":"code","7aeab8e8":"code","21963d23":"code","8515a637":"code","69759ae7":"code","a26da163":"code","96ae33b6":"code","940df464":"code","74b676bc":"code","cc411c2d":"markdown","3d03c4d8":"markdown","da1ac986":"markdown","b2a8a2cf":"markdown","c5b0d075":"markdown","c4867d93":"markdown","f0caecca":"markdown","a198153f":"markdown","02cfe517":"markdown","9981169d":"markdown","ec321006":"markdown","aca54738":"markdown","8b8edde9":"markdown","da7fe364":"markdown","683fd162":"markdown","2b6a9e7e":"markdown","a8c1e75f":"markdown","5b233ec9":"markdown","9df4aace":"markdown","9ffb98ca":"markdown","19e8a969":"markdown","1e740057":"markdown","8354fe9d":"markdown"},"source":{"70c79e97":"# Import the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport re\nfname = \"\/kaggle\/input\/laptop-price-prediction-using-specifications\/LaptopPricePrediction.csv\"\ndf_orig = pd.read_csv(fname)","c0dacef2":"# Clean it\ndf = df_orig.copy()\n\n# Drop unamed col\ndf = df.drop(list(df.filter(regex=\"Unnamed.*\")),axis=1)\n#print(df.dtypes)\n\n# Convert price to a number\ndf.Price = df.Price.str.replace(r'[^0-9]','',regex=True).astype('int')\n\n# Convert storage to number\n# Remove the secondary storage\ndf.Storage = df.Storage.str.split('|').str[0]\ndf.Storage = np.where(df.Storage.str.split(' ').str[0] == \"M.2\",\"256 GB SSD\",df.Storage)\ndf['Storage_GB'] = np.where(\n    df.Storage.str.split(' ').str[1] == \"TB\",\n    pd.to_numeric(df.Storage.str.split(' ').str[0],errors=\"coerce\")*1024,\n    pd.to_numeric(df.Storage.str.split(' ').str[0],errors=\"coerce\")\n)\nstr_med = df.Storage_GB.mean()\ndf.Storage_GB = df.Storage_GB.fillna(value=str_med)\ndf['Storage_Type'] = np.where(df.Storage.str.contains('SSD'),\"SSD\",\"HDD\")\ndf = df.drop('Storage',axis=1)\n\n# Convert display size to number\ndf.Display = pd.to_numeric(\n    df.Display.str.split(' ').str[0].replace(r'[^0-9.]','',regex=True),\n    errors=\"coerce\"\n)\n# Fix the inches, convert to cm\ndf.loc[df.Display == 15.6,\"Display\"] = 39.624 # 15.6 * 2.54\n# Fill NAs with mean\ndisp_med = df.Display.mean()\ndf.Display = df.Display.fillna(value=disp_med)\n\n# Convert RAM to number\ndef ram_type(x):\n    x = str(x)\n    if \"DDR3\" in x:\n        return \"DDR3\"\n    elif \"DDR4\" in x:\n        return \"DDR4\"\n    else:\n        return \"DDR4\"\n\ndf[\"RAM_Type\"] = df.RAM.apply(ram_type)\ndf.RAM = pd.to_numeric(df.RAM.str.split(' ').str[0],errors=\"coerce\")\n# Fix MB values\ndf.RAM = df.RAM.apply(lambda x: x if x<100 else x\/1024)\nram_med = df.RAM.mean()\ndf.RAM = df.RAM.fillna(value=ram_med)\n#print(df.RAM.describe())\n\n# Convert Brand Name and Model Name\ndf['Brand_Name'] = df.Name.str.split(' ').str[0]\n#df['Model_Name'] = df.Name.str.split(' ').str[0:2].str.join('_')\ndf = df.drop('Name',axis=1)\n\n# Convert CPU name\ndef cpu_conv(x):\n    x = str(x)\n    #print(\"***\",x)\n    cpu_name = \"\"\n    cpu_gen = \"\"\n    if re.search('[0-9]{1,2}[a-z]{2} Gen',x):\n        cpu_gen = re.search('([0-9]{1,2}[a-z]{2} Gen)',x).group(1)\n    if re.search('i[0-9]',x):\n        cpu_name = re.search('(i[0-9])',x).group(1)\n    elif re.search('Ryzen [0-9]',x):\n        cpu_name = re.search('(Ryzen [0-9])',x).group(1)\n    elif re.search('A[0-9]',x):\n        cpu_name = re.search('(A[0-9])',x).group(1)\n    elif re.search('AMD \\w+ ',x):\n        cpu_name = re.search('(AMD \\w+) ',x).group(1)\n    elif re.search(\"Intel \\w+ \",x):\n        cpu_name = re.search('(Intel \\w+) ',x).group(1)\n    else:\n        cpu_name = \"Unknown\"\n    cpu = f\"{cpu_name.replace(' ','_')}_{cpu_gen.replace(' ','_')}\"\n    #cpu = cpu_name.replace(' ','_')\n    #print(cpu)\n    return cpu\ndef cpu_brand(x):\n    x = str(x)\n    brand = \"Other\"\n    if \"Core i\" in x:\n        brand = \"Intel\"\n    elif \"Intel\" in x:\n        brand = \"Intel\"\n    elif \"AMD\" in x:\n        brand = \"AMD\"\n    return brand\ndf[\"CPU_Brand\"] = df.Processor.apply(cpu_brand)\ndf.Processor = df.Processor.apply(cpu_conv)\n\n# Convert OS name\nos_dict = {\n    \"64 bit Windows 10 Operating System\": \"win10\",\n    \"Windows 10 Operating System\": \"win10\",\n    \"Mac OS Operating System\": \"mac\",\n    \"Pre-installed Genuine Windows 10 Operating System (Includes Built-in Security, Free Automated Updates, Latest Features)\":\n        \"win10_sec\",\n    \"DOS Operating System\":\"dos\",\n    \"64 bit Chrome Operating System\": \"chrome\"\n}\ndf[\"Operating System\"] = df[\"Operating System\"].apply(lambda x: os_dict.get(x, np.nan))\n\n# Convert warranty\ndef warr_conv(x):\n    x = str(x).lower()\n    time = 0\n    if \"ne-year\" in x:\n        time = 12\n    elif \"year\" in x:\n        time = int(x.split(' ')[0])*12\n    elif \"month\" in x:\n        time = int(x.split(' ')[0])\n    return time\ndf.Warranty = df.Warranty.apply(warr_conv)\n\nprint(df.head())","7aeab8e8":"# encode the data\n# One-hot encode low-cardinality data\nif 'Storage_Type' in df and 'CPU_Brand' in df and 'RAM_Type' in df:\n    one_hot = pd.get_dummies(df['Storage_Type'],drop_first=True)\n    df = df.drop('Storage_Type',axis=1).join(one_hot)\n\n    one_hot = pd.get_dummies(df['CPU_Brand'],drop_first=True)\n    df = df.drop('CPU_Brand',axis=1).join(one_hot)\n    \n    one_hot = pd.get_dummies(df['RAM_Type'],drop_first=True)\n    df = df.drop('RAM_Type',axis=1).join(one_hot)\n\n# Target encode other data\ncols = [col for col in df.columns if df[col].dtype == \"O\"]\nfor col in cols:\n    mean_labels = df.groupby([col]).Price.mean()\n    #print(mean_labels.to_dict())\n    df[col] = df[col].map(mean_labels.to_dict())\n#print(df)","21963d23":"from matplotlib import pyplot as plt\nplt.hist(df.Price)\nplt.title(\"Price histogram\")\nplt.show()\nplt.hist(df.RAM)\nplt.title(\"RAM histogram\")\nplt.show()\nplt.hist(df.Display)\nplt.title(\"Display histogram\")\nplt.show()\nplt.scatter(df.Brand_Name, df.Price)\nplt.title(\"Price vs Brand\")\nplt.show()\nplt.scatter(df.RAM, df.Price)\nplt.title(\"Price vs RAM\")\nplt.show()\nplt.scatter(df.Processor, df.Price)\nplt.title(\"Price vs Processor\")\nplt.show()\nplt.scatter(df.Display, df.Price)\nplt.title(\"Price vs Display\")\nplt.show()\nplt.scatter(df.Brand_Name*df.RAM,df.Price)\nplt.title(\"Price vs Brand*RAM\")\nplt.show()\nplt.scatter(df.Processor*df.RAM, df.Price)\nplt.title(\"Price vs Proc*RAM\")\nplt.show()\nplt.scatter(df.Processor*df.RAM*df.Brand_Name,df.Price)\nplt.title(\"Price vs Proc*RAM*Brand\")\nplt.show()\nplt.scatter(df.Processor*df.RAM*df.Brand_Name*df[\"Operating System\"],df.Price)\nplt.title(\"Price vs Proc*RAM*Brand*OS\")\nplt.show()","8515a637":"print(f\"Summary of Price values: {df.Price.describe()}\")\ndf[\"Proc_Brand_RAM\"] = (df.Processor*df.RAM*df.Brand_Name)\ndf = df.drop(\"Unnamed: 0\",axis=1,errors=\"ignore\")\nprint(\"Linear correlations:\")\nall_corr = df.corr(method=\"pearson\")\ncorrs = (all_corr.filter(['Price'])\n      .query('abs(Price) > 0')\n      .sort_values(by='Price',ascending=False,key=abs,axis=0)\n      .assign(r2 = lambda x: x.Price**2)\n)\nprint(corrs)\n# Find RMSE from R^2\n# R^2 = 1 - SSR\/SST \n# R^2 - 1 = -SSR\/SST\n# -R^2 + 1\n# 1 - R^2 = SSR\/SST\n# SST*(1-R^2) = SSR\n# Use definition of Mean Squared Error (MSE)\n# MSE = (1\/n)*SSR\n# RMSE = sqrt((1\/n)*SSR)\n# RMSE = sqrt((1\/n)*SST*(1-R^2))\nn = df.Price.size\np = len(df.columns) - 1\ncorr_matrix = all_corr.drop([\"Price\"],axis=1).drop([\"Price\"],axis=0).to_numpy()\nprice_corr = all_corr.filter(['Price']).query('Price != 1').to_numpy()\ntotal_r2 = (price_corr.transpose() @ np.linalg.inv(corr_matrix) @ price_corr).item()\n#total_r2 = 1 - ((1 - total_r2)*(n-1)\/(n-p-1))\nsst = sum(np.square(df.Price - df.Price.mean()))\nrmse = np.sqrt((1\/n)*sst*(1-total_r2))\nprint(f\"Ideal R^2: {total_r2}\")\nprint(f\"Ideal RMSE: {rmse}\")","69759ae7":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\ny = df.Price\nX = df.drop('Price',axis=1,errors=\"ignore\")#.filter([\"Brand\",\"RAM\",\"Processor\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Linear model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n# Random forest\nrf_params = {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 5}\nrf = RandomForestRegressor(n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_features=\"sqrt\",max_depth=5, random_state=1)\nrf.fit(X_train, y_train)\n# Gradient Boosting\ngb_params = {'n_estimators': 200, 'min_samples_split': 100, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_depth': 15}\ngb = GradientBoostingRegressor(n_estimators=200, min_samples_split=100, min_samples_leaf=10, max_features=\"sqrt\",max_depth=15, random_state=1)\ngb.fit(X_train, y_train)\n# Extra Trees\net_params = {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 5}\net = ExtraTreesRegressor(n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_features=\"sqrt\",max_depth=5, random_state=1)\net.fit(X_train, y_train)\n# Voting regressor\n#vr = VotingRegressor([('lm',lm),('rf',rf),('gb',gb),('et',et)])\nvr = VotingRegressor([('lm',lm),('rf',rf)])\nvr.fit(X_train, y_train)\n\nmodels = [lm, rf, gb, et, vr]","a26da163":"for model in models:\n    xval = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n    xvm = xval.mean()\n    xvs = xval.std()\n    mname = str(model).split('(')[0]\n    print(f\"{mname}: Mean={xvm:.2f}, Stdev={xvs:.2f}, 95% CI=({xvm-2*xvs:.2f},{xvm+2*xvs:.2f})\")","96ae33b6":"rf_tune = False\ngb_tune = False\net_tune = False\nfor model in models:\n    mname = str(model).split('(')[0]\n    #print(f\"{mname}: {model.get_params()}\\n\")\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]\n# bootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\nrg2 = random_grid.copy()\n#rg2['gamma']\nif rf_tune:\n    rs = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 4)\n    rs.fit(X_train, y_train)\n    print(rs.best_params_)\n    print(rs.best_score_)\nif gb_tune:\n    rs = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 4)\n    rs.fit(X_train, y_train)\n    print(rs.best_params_)\n    print(rs.best_score_)\nif et_tune:\n    rs =  RandomizedSearchCV(estimator = et, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 4)\n    rs.fit(X_train, y_train)\n    print(rs.best_params_)\n    print(rs.best_score_)","940df464":"for model in models:\n    mname = str(model).split('(')[0]\n    y_pred = model.predict(X_test)\n    rmse = mean_squared_error(y_test,y_pred,squared=False)\n    mae = mean_absolute_error(y_test,y_pred)\n    y_m = y.mean()\n    print(f\"{mname}:\")\n    print(\"RMSE: \" + str(rmse))\n    print(\"RMSE ($): \" + str(rmse*0.013))\n    print(\"RMSE (%): \" + str(rmse\/y_m*100))\n    print(\"MAE: \" + str(mae))\n    print(\"MAE ($): \" + str(mae*0.013))\n    print(\"MAE (%): \" + str(mae\/y_m*100))\n    print(\"R^2 (training): \" + str(r2_score(y_train,model.predict(X_train))))\n    print(\"R^2: \" + str(r2_score(y_test,y_pred)))\n    print(\"\\n\")","74b676bc":"import scipy as sc\nimport seaborn as sb\n# Test normality of residuals\ny_pred = lm.predict(X_test)\nres = (y_test - y_pred)\n#res = (y_train - lm.predict(X_train))\nmu = np.mean(res)\ns = np.std(res)\nskew = sc.stats.skew(res)\nkurt = sc.stats.kurtosis(res,fisher=True)\nif abs(kurt) > 7 or abs(skew) > 2:\n    print(\"Residuals likely non-normal\")\nelse:\n    print(\"Residuals likely close to normal\")\nprint(f\"Mu: {round(mu,2)}, Sigma: {round(s,2)}, skew: {round(skew,2)}, ex. kurt: {round(kurt,2)}\")\nsc.stats.probplot(res,plot=plt);\nplt.show()\nplt.scatter(res, y_pred)\nplt.title(\"Residuals vs Fitted\")\nplt.show()\n#sb.kdeplot(data=res,bw_adjust=0.2)","cc411c2d":"## 4.1 Train models <a id=\"_41\"><\/a>","3d03c4d8":"## 4.4 Model evalulation on testing data <a id=\"_44\"><\/a>","da1ac986":"# 4 Regression models <a id=\"_4\"><\/a>","b2a8a2cf":"The QQ plot, as well as skewness and kurtosis values, show that the residuals are close to being normally distributed. There is some pattern in the residuals vs fitted plot, meaning that the errors are not independent, and do not have a constant variance. This means that a linear model probably isn't the best fit for the data. However, the model still has good predictive performance compared with the other models, so there is not reason to stop using it.","c5b0d075":"## 3.1 Graphical summaries <a id=\"_31\"><\/a>","c4867d93":"# 1 Introduction <a id=\"introduction\"><\/a>","f0caecca":"## 4.5 Checking assumptions of linear regression <a id=\"_45\"><\/a>","a198153f":"The data seems to have high variance (highly spread out), so it may be difficult to predict prices from these features. The data points seem spread out from a best fit line (high residual values). The interaction between processor, brand, and RAM seems to be a good predictor, as well as the brand and RAM alone.","02cfe517":"## 3.2 Numerical summaries <a id=\"_32\"><\/a>","9981169d":"# 5 Discussion <a id=\"_5\"><\/a>","ec321006":"The cross-validation shows that the best performing model is the Linear Regression, with the voting regressor in second place, with slightly lower variance than the Linear Regression.","aca54738":"# 3 Exploring the data <a id=\"_3\"><\/a>","8b8edde9":"## 4.3 Tuning hyperparameters <a id=\"_43\"><\/a>","da7fe364":"## 2.1 Cleaning the data <a id=\"_21\"><\/a>","683fd162":"Final evaluation on the training data shows that the Linear Regression is the most accurate model, with an R^2 of 0.52, the same as the 0.52 based on the original linear correlation, and with a RMSE of 30,597, which is 38% of the mean price.","2b6a9e7e":"Table of Contents\n* [1 Introduction](#introduction)\n* [2 Preprocessing the data](#_2)\n    * [2.1 Cleaning the data](#_21)\n    * [2.2 Encoding the data](#_22)\n* [3 Exploring the data](#_3)\n    * [3.1 Graphical summaries](#_31)\n    * [3.2 Numerical summaries](#_32)\n* [4 Regression models](#_4)\n    * [4.1 Train models](#_41)\n    * [4.2 Cross-validation](#_42)\n    * [4.3 Tuning hyperparameters](#_43)\n    * [4.4 Model evaluation on testing data](#_44)\n    * [4.5 Checking assumptions of linear regression](#_45)\n* [5 Discussion](#_5)\n   ","a8c1e75f":"The R^2 value using the Pearson (linear) correlation coefficient for the data is 0.52. This means the predictors explain 52% of the variance of the target (price). Root Mean Squared Error can be calculated from this R^2 value, and we get a value of about 31,187. This is the goal R^2 and RMSE value for ML models. The R^2 value of 0.52 corresponds to a R (correlation coefficient) value of about 0.72, which is a moderate linear relationship. Other correlation coefficients, such as Spearman and Kendall (these assess non-linear correlations, usually with ranked data) showed lower values.\n\nThe strongest correlations with Price besides Price itself are the interaction between Processor, Brand, and RAM, then RAM, Brand Name, the Operating System, and the Processor individually. The RAM for example, explains 32.9% of the variance of the computer price, and the brand name explains 29.2% of the variance.\n\nKnowing these features, we can create models to predict the price of a laptop based on these features.","5b233ec9":"# 2 Preprocessing the data <a id=\"_2\"><\/a>","9df4aace":"Linear regression generally assumes that the residuals are independent and follow a normal distribution and constant variance over all independent variable values.","9ffb98ca":"The best price predicting model (Linear Regression) on the testing data got an R^2 of about 0.521 on the testing data, with a Root Mean Square Error of 30,597, or about a 38% error on average. The second best model, the Voting Regressor (based on Linear Regression and Random Forest), had an R^2 of 0.518, and about the same performance as Linear Regression on cross-validation (mean R^2 of 0.45 vs 0.46), and with a slightly lower standard deviation of 0.11 compared to the 0.13 for Linear Regression.\n\nThe features in the data with the strongest linear correlations with Price are RAM (r=0.57), Brand Name (r=0.54), Operating System (r=0.25), and Processor (r=0.24). RAM and Brand Name are by far the strongest of the correlations with price. RAM has a positive correlation with price. Warranty (r=0.02), the presence of an SSD (r=0.05), and rating (r=0.01) have a small positive correlation with price. Having an Intel Processor (r=-0.05), DDR4 memory (r=-0.04), more storage (r=-0.02), and a larger display (r=-0.03) have a negative correlation with price.","19e8a969":"## 2.2 Encoding the data <a id=\"_22\"><\/a>\nWe are encoding most categorical variables using Target Encoding because many variables have high cardinality (many different categories). For binary variables we can just one-hot encode.","1e740057":"In this notebook, we will try to predict the price of a laptop using different specifications about the laptop.","8354fe9d":"## 4.2 Cross-validation <a id=\"_42\"><\/a>"}}