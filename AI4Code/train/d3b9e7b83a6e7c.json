{"cell_type":{"524e04ab":"code","1570d443":"code","fc16b355":"code","c1f0ac3c":"code","ee6ea23e":"code","cc2ab88b":"code","48bd3e69":"code","54212228":"code","0d1faf92":"code","5ee763a4":"code","1ca53953":"code","8a47b2aa":"code","4a637938":"code","f138c4c9":"code","6edc306a":"code","d79bdd17":"code","2dc57ad0":"code","3ec82ecb":"code","ad2fe050":"code","2088ee1c":"code","f17a8e5d":"code","0bbdcca4":"code","a03efbd1":"code","2018f236":"code","3d098f30":"code","53e17126":"code","92d45a23":"code","ac1a80c8":"code","673d304a":"code","eb454f62":"code","b24f4ea4":"code","4f9c45ba":"code","9f6474ce":"code","16819e60":"code","748435fb":"code","1c687338":"code","c6a24e9e":"code","a93ee6a4":"code","5c987766":"code","4191d721":"code","2e23c139":"code","d0865b66":"code","4ea22a16":"code","87466818":"code","b09435c6":"code","d287154d":"code","e3c94454":"code","b4560be6":"code","12e3761d":"code","b060ebc3":"markdown","a151ef8f":"markdown","3e8bf653":"markdown","a8ebc52e":"markdown","7c11c62c":"markdown","28295b88":"markdown","2c2954a9":"markdown","c86ba5c1":"markdown","af452e61":"markdown","74e76fb9":"markdown","26a08a77":"markdown","b38e0b6f":"markdown","cde9daf1":"markdown","5d56b33f":"markdown","db4925a9":"markdown","aaa38b8d":"markdown","b0658369":"markdown","151a1a00":"markdown","d99c6119":"markdown","7c10ac12":"markdown","cba590d8":"markdown","759d16e3":"markdown","874965cd":"markdown","93db73cb":"markdown","31b9f875":"markdown","6ec874b8":"markdown","e1594a78":"markdown","82038114":"markdown","60834f64":"markdown","bb695781":"markdown","7bb366ac":"markdown","0a91ab22":"markdown","4537d611":"markdown","fa7dfcc0":"markdown","736cfe93":"markdown","ec7a0d02":"markdown","36a8d9f2":"markdown","5c2dfa7d":"markdown"},"source":{"524e04ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1570d443":"import pandas as pd\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mno\nimport numpy as np\nimport math\n%matplotlib inline\nsns.set_style('whitegrid')","fc16b355":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c1f0ac3c":"train_df.head()","ee6ea23e":"train_df.shape","cc2ab88b":"test_df.shape","48bd3e69":"mno.matrix(train_df)\nplt.show()","54212228":"mno.matrix(test_df)\nplt.show()","0d1faf92":"df = pd.concat([train_df, test_df])\ndf.shape","5ee763a4":"df = df.dropna(subset=['Embarked'])\ndf.shape","1ca53953":"df['Fare'] = df['Fare'].fillna(float(8.05)) #8.05 is the mode for the fare column. Can be calculated using df['Fare'].mode()\ndf.isna().sum()","8a47b2aa":"sns.countplot(x='Survived', data=df.iloc[0:889, :], palette='GnBu')\nplt.show()","4a637938":"sns.countplot(x='Survived', data=df.iloc[0:889, :], palette='GnBu', hue='Sex')\nplt.show()","f138c4c9":"sns.countplot(x='Survived', data=df.iloc[0:889, :], palette='GnBu', hue='Pclass')\nplt.show()","6edc306a":"sns.boxplot(x='Pclass', y='Age', data=df, palette='GnBu')\nplt.show()","d79bdd17":"Age = list(df.Age)\nPclass = list(df.Pclass)\nfor i in range(len(Age)):\n    if math.isnan(Age[i]) == True:\n        if Pclass[i] == 1:\n            Age[i] = 39\n        elif Pclass[i] == 2:\n            Age[i] = 29\n        else:\n            Age[i] = 25\ndf['Age'] = Age\nprint(df.isna().sum())","2dc57ad0":"PassengerId = list(test_df['PassengerId'])","3ec82ecb":"df = df.drop(['Cabin','Ticket','PassengerId'], axis=1)","ad2fe050":"df.columns","2088ee1c":"profile = ProfileReport(df, title=\"Titanic Dataset Report\")\nprofile.to_file(output_file=\"TitanicDatasetReport.html\")","f17a8e5d":"sns.swarmplot(x=\"Pclass\", data=df, y=\"Fare\", hue=\"Survived\", palette=\"GnBu\")\nplt.show()","0bbdcca4":"sns.swarmplot(x='Survived', y=\"Fare\", data=df.iloc[0:889,:], palette=\"GnBu\")\nplt.show()","a03efbd1":"sns.swarmplot(x='Sex', y=\"Fare\", hue=\"Survived\", data=df.iloc[0:889,:], palette=\"GnBu\")\nplt.show()","2018f236":"sns.distplot(df.iloc[0:889,4:5], kde=False, bins=10, color=\"darkred\")\nplt.show()","3d098f30":"sns.countplot(x=\"SibSp\", data=df.iloc[0:889,:], palette=\"GnBu\", hue=\"Survived\")\nplt.show()","53e17126":"sns.countplot(x=\"Parch\", data=df.iloc[0:889,:], palette=\"GnBu\", hue=\"Survived\")\nplt.show()","92d45a23":"sns.scatterplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", style=\"Sex\", data=df.iloc[0:889,:], palette=\"GnBu\")\nplt.show()","ac1a80c8":"sns.countplot(x=\"Embarked\", hue=\"Survived\", data=df.iloc[0:889,:], palette=\"GnBu\")\nplt.show()","673d304a":"sns.countplot(x=\"Embarked\", hue=\"Pclass\", data=df.iloc[0:889,:], palette=\"GnBu\")\nplt.show()\ndf.head()","eb454f62":"Sex = list(df['Sex'])\nfor i in range(len(Sex)):\n          if Sex[i] == 'male':\n               Sex[i] = 1\n          elif Sex[i] == 'female':\n               Sex[i] = 0\ndf['Sex'] = Sex\nEmbarked = list(df['Embarked'])\nfor i in range(len(Embarked)):\n    if Embarked[i] == 'S':\n        Embarked[i] = 1\n    elif Embarked[i] == 'C':\n        Embarked[i] = 2\n    elif Embarked[i] == 'Q':\n        Embarked[i] = 3\ndf['Embarked'] = Embarked","b24f4ea4":"Names = list(df['Name'])\nTitles = []\nfor i in range(len(Names)):\n    title = Names[i].split(',')[1].split()[0]\n    Titles.append(title)\ndf['Title'] = Titles\nprint(df['Title'].value_counts())\ndf['Title'].loc[df['Title']=='the'] = 'Miss.'\ndf['Title'].loc[df['Title']=='Ms.'] = 'Miss.'\nprint(df['Title'].unique())","4f9c45ba":"columns = ['Survived', 'Pclass','Title', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Embarked']\ndf = df[columns]\ndf = df.drop('Name', axis=1)\ndf.head()","9f6474ce":"sns.countplot(x='Title', data=df.iloc[0:889,:], hue='Survived', palette='GnBu')\nplt.xticks(rotation=90)\nplt.show()","16819e60":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['Title'] = labelencoder.fit_transform(df['Title'])\ndf.head()","748435fb":"columns = list(df.columns)\ncolumns.remove('Sex')\ncolumns\nheatmap = sns.heatmap(df[columns].iloc[0:889,:].corr(), cmap='GnBu')\nheatmap.set_title('Correlation Matrix')\nplt.xticks(rotation=90)\nplt.show()","1c687338":"train_df = df.iloc[0:889,:]\ntest_df = df.iloc[889:,1:]\ncols = list(train_df.columns)\ncols.remove('Survived')\ncols.append('Survived')\ntrain_df = train_df[cols]\nprint(test_df.isna().sum())","c6a24e9e":"train_df.columns","a93ee6a4":"from sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.ensemble import GradientBoostingClassifier as gbdt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import plot_tree\n\nX = train_df.iloc[:,0:8]\ny = train_df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \n\ndef optimize(model, params):\n    clf = GridSearchCV(model, params, cv=5, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    print(clf.best_params_)\n    return clf\n    \ndef validation(model, cv):\n    model.fit(X_train, y_train)\n    scores = cross_val_score(model, X_test, y_test, cv=cv)\n    print(scores)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    return None\n    \ndef make_preds(model):\n    preds = model.predict(test_df)\n    print('Predictions made successfully!')\n    return preds","5c987766":"def plotfeatures(model, X_train):\n    feature_importance = model.feature_importances_\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X_train.keys()[sorted_idx])\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance')\n    plt.show()","4191d721":"params={\n    'criterion':['gini','entropy'],\n    'class_weight':[None,'balanced'],\n    'max_depth': [2,4,6,8,10,12],\n    'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n    'min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True),\n    'max_features': list(range(1,X_train.shape[1])),\n    \n}\ndtcmodel = dtc()\nvalidation(dtcmodel, 5)\nplt.figure(figsize=(18,10))\nplot_tree(dtcmodel, filled=True)\nplt.show()","2e23c139":"optimized_dtcmodel = optimize(dtcmodel, params)","d0865b66":"dt_preds = make_preds(optimized_dtcmodel)","4ea22a16":"plt.figure(figsize=(18,10))\nplot_tree(optimized_dtcmodel.best_estimator_, filled=True)\nplt.show()","87466818":"params = {\n    'criterion' :['gini', 'entropy'],\n    'max_depth': [50,60,70,80,90,100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100,200,300,400,500,600,700,800,900,1000],\n}\nrf_model = rf()\nvalidation(rf_model, 5)","b09435c6":"optimized_rfmodel = optimize(rf_model, params)","d287154d":"rf_preds = make_preds(optimized_rfmodel)","e3c94454":"parameters = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":[10]\n    }\ngbdtmodel = gbdt()\nvalidation(gbdtmodel, 5)\noptimized_gbdtmodel = optimize(gbdtmodel, params)","b4560be6":"gbdt_preds = make_preds(gbdtmodel)","12e3761d":"submission = pd.DataFrame({'PassengerId':PassengerId,'Survived':gbdt_preds.astype(int)})\nfilename = 'Titanic Predictions Revised(Demo) - Decision Trees.csv'\n\nsubmission.to_csv(filename,index=False)","b060ebc3":"All females who paid more than 200 dollars survived. The survival rate for females is pretty good.","a151ef8f":"# EDA(Continued)","3e8bf653":"The graph above shows the Age distribution.","a8ebc52e":"As you can see, the columns age and cabin have a lot of missing values. There also are 2 missing values in the embarked column and 1 in the fare column which can be dropped.\nTo handle the missing values in the train and the test dataset, we will first join both of these datasets since both of them are to be dealt with missing values.","7c11c62c":"# Feature Engineering\n\nTill now in the analysis we did, the name column wasn't useful at all. That is because everyone had a unique name, but even during that time, people were referred using Mr.\/Master or Mrs.\/Miss. So let's create a feature that will extract this information from the name column and let's see if this feature is important. The new feature that we will be creating would be a categorical feature.","28295b88":"As you can see, we made a title column which tells us the title of the person. You might not understand some titles but they are just the french or spanish form of Miss or Mr. What we will do is keep them as it is till we train our model and validate it. After that is done, we will replace these french and spanish form of words to english and again train our model, see if the accuracy improves or not. Now lets do some EDA for this feature.","2c2954a9":"Pclass had a little or moderate effect on the survival rate. All the people in Pclass = 1 would have been nearest to the emergency boats and were saved first. Now let's see the mean age of people in all the Pclass.","c86ba5c1":"From here, we can see that for Pclass = 1, mean age is 39, 29 for Pclass = 2 and 25 for Pclass = 3. Pclass might be the feature that must have affected age the most and therefore we will replace the missing values in age with these values for the specific Pclass.\n\nI saw this in a video by krish naik, the link will be mentioned below.\n\nLink: https:\/\/www.youtube.com\/watch?v=P_iMSYQnqac","af452e61":"We replaced the embarked column with numerical value. Replacing the S,C and Q values with 1,2 and 3 doesn't change the feature type. Embarked still is a categorical feature. Same goes for the Sex Column. We did this so that we can plot a correlation heatmap.","74e76fb9":"Now let's look at Gradient Boosted Decision Trees.","26a08a77":"According to this plot, all the female survivors were the ones who paid a fare > 100 with 2 or 3 exceptions.","b38e0b6f":"# Decision Trees","cde9daf1":"Finally, we will draw a heatmap to see the correlation between different features.","5d56b33f":"As you can see, we have filled up all the nan values in the age column. Now we have 1014 missing values left in the cabin column. The total size of the dataframe is (1307, 12). The number of missing values is very large in the cabin column and thus we will drop the cabin column. We will also drop the ticket column and the passengerid column as they are just unique features.","db4925a9":"****With this, we end the EDA and as you can see, we gained a lot of information from all the visualizations.****","aaa38b8d":"First, we will import all the necessary libraries.","b0658369":"# Missing Numbers\n\nNow lets see which columns having missing numbers and form up a strategy to either drop those rows with missing numbers, or fill them up. To see which columns have missing numbers, we will use the missingno library which will help us recogonize the columns with missing values using data viz.","151a1a00":"After running the above optimize function for the gradient boosting classifier, we get the following as best parameters:\n{'criterion': 'friedman_mse', 'learning_rate': 0.15, 'loss': 'deviance', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 0.13636363636363638, 'n_estimators': 10, 'subsample': 1.0}","d99c6119":"# Gradient Boosted Decision Trees","7c10ac12":"# EDA\n\nNow, let's do some data viz and understand the rest of the features in this dataset.","cba590d8":"# Pandas Report\n\nLets use the pandas-profiling library to generate a report of the dataset that will help us get more insights.","759d16e3":"For gradient boosting, you can see that the sex feature was the most important in the predictions. The title feature that we created also came in handy.","874965cd":"# **Machine Learning**\n\nOne step we perfrom before machine learning is Feature Scaling, but for tree-based models, we don't need feature scaling. So we will first train our dataset using tree-based models and then, we will scale the features and use other machine learning models. At the end, we will try to improve every model using hyperparameter optimization, select the best model and make our submission.\n","93db73cb":"We have a missing fare value in the test dataset which needs to be filled. Lets just fill it with the mode of the fare column.","31b9f875":"Lets divide our dataframe back into train and test dataset.","6ec874b8":"We get a very complex decision tree. Let's optimize this.","e1594a78":"We can see that there are some missing values in this dataset.","82038114":"# Titanic survival prediction","60834f64":"# Submission","bb695781":"What we did above is known as label encoding. It is basically what we did with the sex column(replaced female and male with 0 and 1 respectively). Label encoding is done before training the model during the feature preprocessing. Here, we are doing it now for plotting a correlation heatmap as seaborn heatmaps only take numerical values\nNote: The values of the Title column are now numerical but it is still a categorical feature.","7bb366ac":"As you can see, we have two datasets, train.csv and test.csv. Now lets just see the top 5 values of the train dataset and the shape for both of these datasets.","0a91ab22":"# Filling up the missing values","4537d611":"Now let's import the data","fa7dfcc0":"Many form S = Southampton did not survive even though a lot of them were from Pclass = 1","736cfe93":"An increase of almost 20% in accuracy after hyperparameter optimization! The best paramters for random forest classifier are: \n{'criterion': 'entropy', 'max_depth': 60, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 400}","ec7a0d02":"From this graph, we can see that many people with same type of fare like the survived people have not survived. Another insight that we get from the two graphs above are that only people from Pclass 1 travelled with a fare > 100 and the survival rate of people with fare > 100 is good.","36a8d9f2":"We dropped the 2 rows with nan values in the embarked columns. Now let's do some EDA to see how to deal with the missing values in the age column. For age, we cannot just straight away select the mean and fill in the missing values since a lot of missing values are there and some or the other feature does affect the age parameter in some way.","5c2dfa7d":"# Random Forest "}}