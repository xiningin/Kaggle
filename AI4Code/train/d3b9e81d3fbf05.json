{"cell_type":{"0beb43de":"code","d190078c":"code","5f7bbd75":"code","fd6c8b18":"code","6f66364b":"code","9686f6d9":"code","9e75ed22":"code","69275649":"code","f3fa1d12":"code","29b52ca2":"code","55d49d94":"code","8d6c35c5":"code","42531abb":"code","33dfe158":"code","bb0ecf74":"code","ac072f84":"code","cc1b4833":"code","35713be9":"code","fb00eb79":"code","58cdfb01":"code","963e4651":"code","e6131ff4":"code","86da70e2":"code","af254cc3":"code","5f3775fd":"code","9563bf75":"code","61bdd450":"code","728a1947":"code","a4ebe050":"code","747a94b2":"code","6cb3ef7e":"code","45e03e2c":"code","d74e9fe3":"code","b5fffe19":"code","56f9c6c6":"code","3e995922":"code","9a2e119e":"code","9857ac5c":"code","c3d4e678":"code","d526e6b3":"code","23ba484c":"code","57079458":"code","55349ebb":"code","2d5c53e3":"code","f8bfd513":"code","c4432c27":"code","12a9040d":"code","b25dfffb":"code","74080af7":"code","ed9ad7e6":"code","480488b2":"code","319f59ae":"code","304a9258":"code","ef3f77d7":"code","b0c12a87":"code","b1b79e5a":"code","cd39a0b9":"code","b2aad66d":"code","0619251e":"code","a301e0a2":"code","12e66823":"code","1e0432d6":"code","f0a4c83a":"code","af10eebf":"code","1d7ead5e":"code","6d341802":"code","7aef5a14":"code","e8f1264b":"code","04ed59c6":"code","132d32d2":"code","239afe70":"code","29a8ee7d":"code","e52f5846":"code","a47dd797":"code","76a8323e":"code","0017df4c":"code","ce02c02a":"code","b38063db":"code","778f1aac":"code","cb94a2d8":"code","2e668eda":"code","77a674bf":"code","e24d89ff":"code","f641e86c":"code","ff4fdd19":"code","7fcb6b64":"code","f416ef4f":"code","f653e991":"code","9871f225":"code","cf494955":"code","9f58e4fe":"code","71bd8f4d":"code","64d29e9a":"code","376c71ed":"code","98f08dd1":"code","5c3ccd6c":"code","9230d56f":"code","c9c3743d":"code","20961fd3":"code","2c3da59e":"code","f5635f89":"code","de24ab8b":"code","17213da9":"code","4365d381":"code","7db5e9dd":"code","5c0fe117":"code","cb3eb1a2":"code","61828aae":"code","441614c0":"code","292e0d06":"code","883119c3":"markdown","5d04b012":"markdown","45e407e7":"markdown","0e63817b":"markdown","4131d801":"markdown","23cc76e9":"markdown","5bf66187":"markdown","5c8efacd":"markdown","87fec182":"markdown","a83c8c7d":"markdown","b81fb005":"markdown","a10579c4":"markdown","7eec799b":"markdown","51e4a61e":"markdown","d21af382":"markdown","59f386ea":"markdown","d231e9e5":"markdown","e50f3777":"markdown"},"source":{"0beb43de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d190078c":"# Importing required Libraries\n\n# import numpy as np\n# import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","5f7bbd75":"train_data=pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')\ntest_data=pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx')","fd6c8b18":"pd.set_option('display.max_columns', None)","6f66364b":"train_data.head(5)","9686f6d9":"train_data.info()","9e75ed22":"train_data.shape","69275649":"train_data[\"Airline\"].unique()","f3fa1d12":"train_data.isnull().sum()","29b52ca2":"indexNames = train_data[ train_data['Airline'] == \"Trujet\" ].index\n\ntrain_data.drop(indexNames , inplace=True)","55d49d94":"print(train_data.shape)\ntrain_data[\"Airline\"].unique()","8d6c35c5":"train_data.dropna(inplace = True)","42531abb":"train_data.isnull().sum()","33dfe158":"train_data.shape","bb0ecf74":"train_data['Journey_day'] = pd.to_datetime(train_data['Date_of_Journey'], format = \"%d\/%m\/%Y\").dt.day","ac072f84":"train_data['Journey_month'] = pd.to_datetime(train_data['Date_of_Journey'], format = \"%d\/%m\/%Y\").dt.month","cc1b4833":"train_data.head(3)","35713be9":"train_data.drop([\"Date_of_Journey\"],axis = 1, inplace = True)","fb00eb79":"# Extracting the Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting the minutes.\ntrain_data[\"Deep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","58cdfb01":"train_data.head(3)","963e4651":"# Extracting hours\ntrain_data['Arrival_hour'] = pd.to_datetime(train_data['Arrival_Time']).dt.hour\n\n# Extracting minutes\ntrain_data['Arrival_min'] = pd.to_datetime(train_data['Arrival_Time']).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use.\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","e6131ff4":"train_data.head(3)","86da70e2":"duration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:  #check if duration contains only hrs or min\n        if 'h' in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"  # Add 0 min in last\n        else:\n            duration[i] = \"0h \" + duration[i]  # Add 0 hrs in front\n            \nduration_hours = []\nduration_mins = []\n\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))","af254cc3":"print(len(duration_hours))\nprint(len(duration_mins))","5f3775fd":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data['Duration_hours'] = duration_hours\ntrain_data['Duration_mins'] = duration_mins","9563bf75":"# Drop \"Duration\" as it is of no use\n\ntrain_data.drop([\"Duration\"], axis = 1, inplace = True)","61bdd450":"train_data.head(3)","728a1947":"train_data['Airline'].value_counts()","a4ebe050":"# Airline vs Price\n\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect=3)","747a94b2":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\nAirline = pd.get_dummies(Airline, drop_first = True)\nAirline.head(5)","6cb3ef7e":"Airline.shape","45e03e2c":"train_data[\"Source\"].value_counts()","d74e9fe3":"# Source Vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind = \"boxen\", height = 6, aspect = 3)","b5fffe19":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\n\nSource = train_data[[\"Source\"]]\nSource = pd.get_dummies(Source, drop_first = True)\nSource.head(5)","56f9c6c6":"train_data[\"Destination\"].value_counts()","3e995922":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\nDestination = pd.get_dummies(Destination, drop_first = True)\nDestination.head(5)","9a2e119e":"train_data[\"Route\"]","9857ac5c":"train_data.head(3)","c3d4e678":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","d526e6b3":"train_data[\"Total_Stops\"].value_counts()","23ba484c":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n","57079458":"train_data.head(3)","55349ebb":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","2d5c53e3":"data_train.head(3)","f8bfd513":"# Drop Airline, Source and Destination columns as they are of no use now.\n\ndata_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","c4432c27":"print(data_train.shape)\ndata_train.head(3)","12a9040d":"data_train.head(3)","b25dfffb":"data_train.shape","74080af7":"test_data.head(5)","ed9ad7e6":"test_data.info()","480488b2":"test_data.dropna(inplace = True)","319f59ae":"test_data.isnull().sum()","304a9258":"test_data[\"Airline\"].unique()","ef3f77d7":"# Date of jouney\n\n# Extracting day\n\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.day\n","b0c12a87":"test_data.head(3)","b1b79e5a":"# Extracting month\n\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\n","cd39a0b9":"test_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","b2aad66d":"test_data.head(3)","0619251e":"# Depature time\n\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis=1, inplace = True)","a301e0a2":"test_data.head(3)","12e66823":"# Arrival Time\n\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data[\"Arrival_Time\"]).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data[\"Arrival_Time\"]).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","1e0432d6":"test_data.head(3)","f0a4c83a":"# Duration\n\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"\n        else:\n            duration[i] = \"0h \" + duration[i]\n            \nduration_hours = []\nduration_mins = []\n\nfor i in range(len(duration)):\n    duration_hours.append(duration[i].split(sep = \"h\")[0])\n    duration_mins.append(duration[i].split(sep = \"m\")[0].split()[-1])\n    \n# Adding these as columns\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins","af10eebf":"test_data.drop([\"Duration\"], axis=1, inplace = True)","1d7ead5e":"test_data.head(3)","6d341802":"# Airline\n\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first = True)","7aef5a14":"# Source\n\nSource = pd.get_dummies(test_data[\"Source\"], drop_first = True)","e8f1264b":"# Destination\n\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)","04ed59c6":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntest_data.drop([\"Route\", \"Additional_Info\"], axis=1, inplace = True)","132d32d2":"# Label Encoding for Total Stops\n\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","239afe70":"# Concatinate the above columns\n\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\nprint(test_data.shape)\nprint(Airline.shape)\nprint(Source.shape)\nprint(Destination.shape)","29a8ee7d":"data_test.drop([\"Airline\", \"Source\", \"Destination\"], axis=1, inplace = True)","e52f5846":"print(data_test.shape)\ndata_test.head(3)","a47dd797":"data_train.shape","76a8323e":"data_train.columns","0017df4c":"# Excluding the target variable - \"Price\"\n\nX = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Deep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head(5)","ce02c02a":"X.shape","b38063db":"y = data_train.iloc[:,1]\ny.head()","778f1aac":"# Finds correlation between Independent and Dependent attributes.\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\nplt.show()","cb94a2d8":"# Using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","2e668eda":"print(selection.feature_importances_)","77a674bf":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","e24d89ff":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","f641e86c":"# Fitting Random Forest Regressor\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","ff4fdd19":"# Predicting Y values on X_test\n\ny_pred = reg_rf.predict(X_test)","7fcb6b64":"print(list(X_test.columns))","f416ef4f":"print(list(data_test.columns))","f653e991":"test_pred = pd.DataFrame(reg_rf.predict(data_test))\ntest_pred.to_excel(\"C:\\\\Users\\\\Bhuvan PC\\\\Downloads\\\\flight_test_pred.xlsx\",\"Sheet1\")\n","9871f225":"# Calculating score on Training splitted data\n\nreg_rf.score(X_train, y_train)","cf494955":"# Calculating score on Testing splitted data\n\nreg_rf.score(X_test, y_test)","9f58e4fe":"sns.distplot(y_test - y_pred)\nplt.show()","71bd8f4d":"plt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","64d29e9a":"from sklearn import metrics","376c71ed":"print(\"MAE : \", metrics.mean_absolute_error(y_test, y_pred))\nprint(\"MSE : \", metrics.mean_squared_error(y_test, y_pred))\nprint(\"RMSE : \", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","98f08dd1":"# By Standardizing : RMSE \/(max(DV) - min(DV))\n\n2094.3372231200883\/(max(y)-min(y))","5c3ccd6c":"metrics.r2_score(y_test, y_pred)","9230d56f":"from sklearn.model_selection import RandomizedSearchCV","c9c3743d":"# Randomized Search CV\n\n# Number of tress in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\n# Number of features to consider at every split.\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree.\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n\n# minimum number of samples required to split a node.\nmin_samples_split = [2, 5, 10, 15, 100]\n\n# minimum number of samples required at each leaf node.\nmin_samples_leaf = [1, 2, 5, 10]","20961fd3":"# Create a Random grid.\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","2c3da59e":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\n\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid, scoring='neg_mean_squared_error', n_iter =10, verbose = 2, random_state = 42, n_jobs = 1)","f5635f89":"rf_random.fit(X_train, y_train)","de24ab8b":"rf_random.best_params_","17213da9":"prediction = rf_random.predict(X_test)","4365d381":"plt.figure(figsize = (8,8))\nsns.distplot(y_test - prediction)\nplt.show()","7db5e9dd":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","5c0fe117":"print('MAE :', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE :', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE :', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","cb3eb1a2":"import pickle\n\n# Open a file, where you want to store the data\nfile = open('flight_rf.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","61828aae":"model = open('flight_rf.pkl', 'rb')\nforest = pickle.load(model)","441614c0":"y_prediction = forest.predict(X_test)","292e0d06":"metrics.r2_score(y_test, y_prediction)","883119c3":"### Departure time is when a plane leaves the gate. Similar to Date_of_Journey we can extract values from Dep_Time","5d04b012":"# Don't forget to upvote and comment if you like my model\n\n**Flight Fare Prediction**\n![](https:\/\/static01.nyt.com\/images\/2020\/01\/28\/business\/28itineraries\/merlin_119708615_015bf26f-808a-41cf-8a8c-5ec3db569f3b-mobileMasterAt3x.jpg)","45e407e7":"## Hyperparameter Tuning\n\n1. Choose following method for hyperparameter tuning\n    . RandomizedSearchCV --> Fast\n    . GridSearchCV\n2. Assign hyperparameters in form of dictionery\n3. Fit the model\n4. Check best paramters and best score","0e63817b":"## Fitting model using Random Forest\n\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n2. If needed do scaling of data\n        . Scaling is not done in Random forest\n3. Import model\n4. Fit the data\n5. Predict w.r.t X_test\n6. In regression check RSME Score\n7. Plot graph","4131d801":"### Arrival time is when the plane pulls up to the gate.Similar to Date_of_Journey we can extract values from Arrival_Time","23cc76e9":"## Reading Test and Train data sets","5bf66187":"## Save the model to reuse it again","5c8efacd":"## Problem Statement\n\nFlight ticket prices can be something hard to guess, today we might see a price, check out the price of the same flight tomorrow, it will be a different story. We might have often heard travelers saying that flight ticket prices are so unpredictable. As data scientists, we are gonna prove that given the right data anything can be predicted. Here you will be provided with prices of flight tickets for various airlines between the months of March and June of 2019 and between various cities. Size of training set: 10683 records\n\nSize of test set: 2671 records\n\nFEATURES: Airline: The name of the airline.\n\nDate_of_Journey: The date of the journey\n\nSource: The source from which the service begins.\n\nDestination: The destination where the service ends.\n\nRoute: The route taken by the flight to reach the destination.\n\nDep_Time: The time when the journey starts from the source.\n\nArrival_Time: Time of arrival at the destination.\n\nDuration: Total duration of the flight.\n\nTotal_Stops: Total stops between the source and destination.\n\nAdditional_Info: Additional information about the flight\n\nPrice: The price of the ticket","87fec182":"### Time taken by plane to reach destination is called Duration. It is the differnce betwwen Departure Time and Arrival time. Assigning and converting Duration column into list","a83c8c7d":"## EDA on Test data","b81fb005":"### Removing \"Trujet\" Airline from train as its not providing any services now.","a10579c4":"* ### From graph we can see that Jet Airways Business have the highest Price.\n* ### Apart from the first Airline almost all are having similar median","7eec799b":"## Categorical Data","51e4a61e":"### Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.","d21af382":"# Importing dataset\n\nSince data is in form of excel file we have to use pandas read_excel to load the data\nAfter loading it is important to check the complete information of data as it can indication many of the hidden infomation such as null values in a column or a row\nCheck whether any null values are there or not. if it is present then following can be done,\nImputing data using Imputation method in sklearn\nFilling NaN values with mean, median and mode using fillna() method\nDescribe data --> which can give statistical analysis","59f386ea":"## Handling Categorical Data\n\nOne can find many ways to handle categorical data. Some of them categorical data are,\n\n* **Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case.\n* **Ordinal data** --> data are in order --> **LabelEncoder** is used in this case.","d231e9e5":"## EDA\n\nFrom description we can see that Date_of_Journey is a object data type,\\ Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas to_datetime to convert object data type to datetime dtype.\n\n* dt.day method will extract only day of that date\n* dt.month method will extract only month of that date","e50f3777":"# Feature Selection\nFinding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\n\n* **heatmap**\n* **feature_importance_**\n* **SelectKBest**"}}