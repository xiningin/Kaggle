{"cell_type":{"e9bcb2dc":"code","8342125e":"code","d4bbda58":"code","5490295c":"code","2233aff3":"code","f0a5db65":"code","9e119aba":"code","9371b308":"code","fbc96079":"code","46a0ee61":"code","7f3f625b":"code","41783255":"code","14d89ac5":"code","fbd6f377":"code","e2e03a1d":"code","730de78d":"code","8fd5205d":"code","0d19b3f0":"code","b39d91d0":"code","b6031188":"code","997e4499":"code","f619c30f":"code","49e7bd3b":"code","bfc720ba":"code","f03be926":"code","0e670617":"code","3df40c0e":"code","bf63d081":"code","f29dc816":"code","d40922a3":"code","2ae82af0":"code","5313e91a":"code","b00d9d3b":"code","083e6b82":"code","5628cc55":"code","381ee2ed":"code","cff0d1b7":"code","11822fca":"code","3b199d75":"code","6387f53a":"code","3c2000ff":"markdown","ac2e8404":"markdown","66ebc658":"markdown","42ce0e1a":"markdown","ad922141":"markdown","7e7990eb":"markdown","1e876f95":"markdown","916db515":"markdown","6de107fc":"markdown","d957c744":"markdown","48caca05":"markdown","2f416a61":"markdown","7b40665b":"markdown","00fcffc3":"markdown","cc176c55":"markdown","411aa663":"markdown","0840d9ba":"markdown"},"source":{"e9bcb2dc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc,os,sys\nimport re\nimport random\n\nfrom sklearn import metrics, preprocessing\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, LeaveOneGroupOut\nfrom sklearn.decomposition import PCA, KernelPCA, NMF\nfrom sklearn.cluster import KMeans\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('darkgrid')\n\npd.options.display.float_format = '{:,.3f}'.format\n\nprint(os.listdir(\"..\/input\"))","8342125e":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(42)","d4bbda58":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5490295c":"%%time\ntrain_id = pd.read_csv('..\/input\/train_identity.csv')\ntrain_trn = pd.read_csv('..\/input\/train_transaction.csv')\ntest_id = pd.read_csv('..\/input\/test_identity.csv')\ntest_trn = pd.read_csv('..\/input\/test_transaction.csv')\n\nprint(train_id.shape, test_id.shape)\nprint(train_trn.shape, test_trn.shape)","2233aff3":"id_cols = list(train_id.columns.values)\ntrn_cols = list(train_trn.drop('isFraud', axis=1).columns.values)\n\nX_train = pd.merge(train_trn[trn_cols + ['isFraud']], train_id[id_cols], how='left')\nX_train = reduce_mem_usage(X_train)\nX_test = pd.merge(test_trn[trn_cols], test_id[id_cols], how='left')\nX_test = reduce_mem_usage(X_test)\n\nX_train_id = X_train.pop('TransactionID')\nX_test_id = X_test.pop('TransactionID')\ndel train_id,train_trn,test_id,test_trn\n\nall_data = X_train.append(X_test, sort=False).reset_index(drop=True)","f0a5db65":"_='''\ncorr_matrix = all_data.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ncolumns_to_drop = [c for c in upper.columns if any(upper[c] > 0.98)]\ndel upper\n\nprint('drop columns:', columns_to_drop)\nall_data.drop(columns_to_drop, axis=1, inplace=True)\n'''","9e119aba":"def encode_loop(df, col, drop=True):\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col] \/ df[col].max())\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col] \/ df[col].max())\n    if drop:\n        df.drop(col, axis=1, inplace=True)\n    return df","9371b308":"ccols = [f'C{i}' for i in range(1,15)]\ndcols = [f'D{i}' for i in range(1,16)]\nmcols = ['M1','M2','M3','M5','M6','M7','M8','M9']","fbc96079":"all_data['_log_dist_1_2'] = np.log1p(np.where(all_data['dist1'].isna(), all_data['dist2'], all_data['dist1']))","46a0ee61":"import datetime\n\nSTART_DATE = '2017-11-30'\n#START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrandate = all_data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\nall_data['_days'] = all_data['TransactionDT'] \/\/ (24*60*60)\n#all_data['_weekday'] = trandate.dt.dayofweek.astype(str)\nall_data['_hour'] = trandate.dt.hour\nall_data = encode_loop(all_data, '_hour')\n#all_data['_day'] = trandate.dt.day\n#all_data['_year_month'] = trandate.dt.year.astype(str) + '_' + trandate.dt.month.astype(str)\nall_data['_weekday__hour'] = trandate.dt.dayofweek.astype(str) + '_' + trandate.dt.hour.astype(str)","7f3f625b":"all_data['_P_emaildomain__ProductCD'] = all_data['P_emaildomain'] + '__' + all_data['ProductCD']\nall_data['_card3__card5'] = all_data['card3'].astype(str) + '__' + all_data['card5'].astype(str)","41783255":"all_data['_uid1'] = (all_data['_days'] - all_data['D1']).astype(str) + '__' + all_data['P_emaildomain'].astype(str)\nall_data['_uid2'] = all_data['card1'].astype(str) + '__' + all_data['addr1'].astype(str) + '__' + all_data['_uid1']\n\n# lag previous transaction\ngroup_key = ['_uid2']\nall_data = all_data.assign(\n        _day_lag_uid2 = all_data['TransactionDT'] - all_data.groupby(group_key)['TransactionDT'].shift(1)\n        #,_amount_lag_uid2 = all_data['TransactionAmt'] - all_data.groupby(group_key)['TransactionAmt'].shift(1)\n        #,_amount_ema5_uid2 = all_data.groupby(group_key)['TransactionAmt'].apply(lambda x: x.ewm(span=5).mean())\n        ,_amount_lag_pct_uid2 = np.abs(all_data.groupby(group_key)['TransactionAmt'].pct_change())\n)\n_='''\ngroup_key = ['_uid1']\nall_data = all_data.assign(\n        _day_lag_uid1 = all_data['TransactionDT'] - all_data.groupby(group_key)['TransactionDT'].shift(1)\n        ,_amount_lag_uid1 = all_data['TransactionAmt'] - all_data.groupby(group_key)['TransactionAmt'].shift(1)\n        ,_amount_ema5_uid1 = all_data.groupby(group_key)['TransactionAmt'].apply(lambda x: x.ewm(span=5).mean())\n        ,_amount_lag_pct_uid1 = np.abs(all_data.groupby(group_key)['TransactionAmt'].pct_change())\n)\n'''","14d89ac5":"all_data['_amount_decimal'] = ((all_data['TransactionAmt'] - all_data['TransactionAmt'].astype(int)) * 1000).astype(int)\nall_data['_amount_decimal_len'] = all_data['TransactionAmt'].apply(lambda x: len(re.sub('0+$', '', str(x)).split('.')[1]))\nall_data['_amount_fraction'] = all_data['TransactionAmt'].apply(lambda x: float('0.'+re.sub('^[0-9]|\\.|0+$', '', str(x))))","fbd6f377":"def values_agg(df, periods, columns, aggs=['max']):\n    for period in periods:\n        for col in columns:\n            if col in df.columns:\n                new_col = f'{col}_{period}'\n                grouped_col = df.groupby([period])[col]\n                for a in aggs:\n                    df[f'_{a}_{new_col}'] = df[period].map(grouped_col.agg(a).to_dict())\n    return df\n\nall_data = values_agg(all_data, ['_uid2'], ccols)\n#all_data = values_agg(all_data, ['_uid2'], dcols)","e2e03a1d":"amt_cols = ['_uid2','_P_emaildomain__ProductCD']\n\nall_data = values_agg(all_data, amt_cols, ['TransactionAmt'], aggs=['max','mean','var'])","730de78d":"for f in ccols + amt_cols:\n    vc = all_data[f].value_counts(dropna=False)\n    all_data[f'_count_full_{f}'] = all_data[f].map(vc)","8fd5205d":"all_data['_all_na'] = all_data.isna().sum(axis=1).astype(np.int8)\n#all_data['_addr_na'] = all_data[['addr1','addr2','dist1','dist2']].isna().sum(axis=1).astype(np.int8)","0d19b3f0":"all_data['_ccols_nonzero'] = all_data[ccols].apply(lambda x: len(x.to_numpy().nonzero()[0]), axis=1)\nall_data['_ccols_sum'] = all_data[ccols].sum(axis=1).astype(np.int8)\nall_data['_ccols_0_bin'] = ''\nfor c in ccols:\n    all_data['_ccols_0_bin'] += (all_data[c] == 0).astype(int).astype(str)\n\nall_data.drop(ccols, axis=1, inplace=True)","b39d91d0":"#all_data['_D1_eq_D2'] = np.where(all_data['D1'] == all_data['D2'].fillna(0),'1','0')\n#all_data['_D3_eq_D5'] = np.where(all_data['D3'] == all_data['D5'],'1','0')\n#all_data['_D8_na'] = np.where(np.isnan(all_data['D8']),'1','0')\n\nall_data['_dcol_na'] = all_data[dcols].isna().sum(axis=1).astype(np.int8)\nall_data['_dcols_na_bin'] = ''\nfor c in dcols:\n    all_data['_dcols_na_bin'] += all_data[c].isna().astype(int).astype(str)\n\n# diff date threshold\nfor f in ['D1','D2']:\n    #all_data[f] = all_data[f].fillna(0) - all_data['_days'].apply(lambda x: np.min([154,x]))\n    all_data[f] = all_data[f].fillna(0) - all_data['_days']\n\nfor f in ['D3','D4','D5','D6','D7','D10','D11','D12','D13','D14','D15']:\n    all_data[f] = all_data[f].fillna(0) - all_data['_days']\n\n#all_data['_dcol_max'] = all_data[dcols].fillna(0).max(axis=1).astype(np.int8)\n    \n# time feature\n#all_data['D9'] = (all_data['D9'] * 24)\n#all_data['_D9_na'] = all_data['D9'].isna().astype(np.int8)\n\n#all_data.drop(dcols, axis=1, inplace=True)","b6031188":"#all_data['_mcol_sum'] = all_data[mcols].sum(axis=1).astype(np.int8)\n#all_data['_mcol_na'] = all_data[mcols].isna().sum(axis=1).astype(np.int8)\nall_data['_mcols_na_bin'] = ''\nfor c in mcols:\n    all_data['_mcols_na_bin'] += all_data[c].isna().astype(int).astype(str)","997e4499":"vcols = [f'V{i}' for i in range(1,340)]\n\nsc = preprocessing.MinMaxScaler()\n\ndec = PCA(n_components=2, random_state=42) #0.99\nvcol_dec = dec.fit_transform(sc.fit_transform(all_data[vcols].fillna(-1)))\n\nall_data['_vcols_dec0'] = vcol_dec[:,0]\nall_data['_vcols_dec1'] = vcol_dec[:,1]\nall_data['_vcols_na'] = all_data[vcols].isna().sum(axis=1).astype(np.int8)\n\nfor f in ['V144','V145','V150','V151','V159','V160','V307']:\n    vcols.remove(f)\n\nall_data['_vcols_sum'] = all_data[vcols].sum(axis=1).astype(np.int8)\n\nall_data.drop(vcols, axis=1, inplace=True)","f619c30f":"_='''\ncnt_day = cnt_day \/ cnt_day.mean()\nall_data['_pct_trns_day'] = all_data['_days'].map(cnt_day.to_dict())\n'''\ncnt_day = all_data['_days'].value_counts()\nall_data['_count_trns_day'] = all_data['_days'].map(cnt_day.to_dict())\n\n#daily_cols = ['C1','C13'] # + amt_cols + dcols\ndaily_cols = ccols\nfor f in daily_cols:\n    if f in all_data.columns:\n        val_day = all_data[f].astype(str) + '__' + all_data['_days'].astype(str)\n        vc = val_day.value_counts(dropna=True)\n        all_data[f'_count_day_{f}'] = val_day.map(vc)\n        all_data[f'_count_pct_day_{f}'] = all_data[f'_count_day_{f}'] \/ all_data['_count_trns_day']\n\nall_data.drop('_count_trns_day', axis=1, inplace=True)","49e7bd3b":"# drop low impotance features\n_='''\n'''\ncolumns_to_drop = ['id_07','id_08','id_10','id_12','id_16',\n                   'id_21','id_22','id_23','id_24','id_25',\n                   'id_26','id_27','id_28','id_29','id_32',\n                   'id_34','id_35','id_36','id_37']\nall_data.drop(columns_to_drop, axis=1, inplace=True)","bfc720ba":"many_same_values_columns = [c for c in all_data.drop('isFraud', axis=1).columns if all_data[c].value_counts(normalize=True).values[0] >= 0.98]\ncolumns_to_drop = list(many_same_values_columns)\nprint('drop columns:', columns_to_drop)\nall_data.drop(columns_to_drop, axis=1, inplace=True)","f03be926":"# drop feature\nall_data.drop(['_days'], axis=1, inplace=True)\nall_data.drop(['TransactionDT'], axis=1, inplace=True) #,'card3','card5'\n\nuid = all_data['_uid2']\n\ndrop_card_cols = ['_uid1','_uid2','card1','card2']\n#all_data.drop(drop_card_cols, axis=1, inplace=True)","0e670617":"_='''\n'''\ncat_cols = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_emaildomain','R_emaildomain',\n            'M1','M2','M3','M4','M5','M6','M7','M8','M9','DeviceType','DeviceInfo'] + [f'id_{i}' for i in range(12,39)]\n\n# to str type\nfor i in cat_cols:\n    if i in all_data.columns:\n        all_data[i] = all_data[i].astype(str)\n        #all_data[i].fillna('unknown', inplace=True) # need for category-type\n\n# factorize\nenc_cols = []\nfor i, t in all_data.loc[:, all_data.columns != 'isFraud'].dtypes.iteritems():\n    if t == object:\n        enc_cols.append(i)\n        all_data[i] = pd.factorize(all_data[i])[0]\n        #all_data[i] = all_data[i].astype('category')\nprint(enc_cols)","3df40c0e":"print('features:', all_data.shape[1])","bf63d081":"X_train = all_data[all_data['isFraud'].notnull()]\nX_test = all_data[all_data['isFraud'].isnull()].drop('isFraud', axis=1)\nY_train = X_train.pop('isFraud')\n\nuid_train = uid[all_data['isFraud'].notnull()]\nuid_fraud = uid_train[Y_train == 1]\nuid_test = uid[all_data['isFraud'].isnull()]\n\ntrain_group = trandate[:len(X_train)].dt.month\n\ndel uid\ndel all_data","f29dc816":"_='''\npseudo = X_test[uid_test.isin(uid_fraud).values]\npseudo['isFraud'] = 1\n\nY_train = pd.concat([Y_train, pseudo.pop('isFraud')], axis=0)\nX_train = pd.concat([X_train, pseudo], axis=0)\np_group = pd.Series([train_group[0]] * len(pseudo))\ntrain_group = pd.concat([train_group, p_group], axis=0)\nprint(len(pseudo))\n'''","d40922a3":"cat_cols = ['card1','card2','card3','card5','addr1','_uid1','_uid2','_P_emaildomain__ProductCD','_card3__card5']\nfor f in cat_cols:\n    if f in X_train.columns:\n        train_set = set(X_train[f])\n        test_set = set(X_test[f])\n        tt = train_set.intersection(test_set)\n        print(f, '-', 'train:%.3f'%(len(tt)\/len(train_set)), ',test:%.3f'%(len(tt)\/len(test_set)))\n        X_train[f] = X_train[f].map(lambda x: -999 if x not in tt else x)\n        X_test[f] = X_test[f].map(lambda x: -999 if x not in tt else x)","2ae82af0":"import lightgbm as lgb\nfrom imblearn.datasets import make_imbalance\n\ndef scale_minmax(preds):\n    return (preds - preds.min()) \/ (preds.max() - preds.min())\n\ndef pred_lgb(X_train, Y_train, X_test, params, num_iterations=1000):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n \n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    dtrain = lgb.Dataset(X_train[:312574], label=Y_train.iloc[:312574])\n    dvalid = lgb.Dataset(X_train[-175998:], label=Y_train[-175998:])\n\n    clf = lgb.train(params, dtrain, 5000, \n                    valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                    verbose_eval=1000, early_stopping_rounds=200)\n    feature_importances['fold_0'] = clf.feature_importance()\n    oof_preds = clf.predict(X_train)\n    sub_preds = clf.predict(X_test)\n    \n    return oof_preds, sub_preds, feature_importances\n\n# predict with lightgbm, KFold \ndef pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=5):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    kf = KFold(n_splits=nfolds, shuffle=False, random_state=42)\n    for fold, (train_index, test_index) in enumerate(kf.split(X_train, Y_train)):\n        dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=1000, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n        oof_preds[test_index] = clf.predict(X_train.iloc[test_index])\n        sub_preds += clf.predict(X_test) \/ nfolds\n    \n    return oof_preds, sub_preds, feature_importances","5313e91a":"# predict with lightgbm, LeaveOneGroupOut \ndef pred_lgb_LOGO(X_train, Y_train, X_test, groups, params, undersampling=False):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    nfolds = groups.nunique()\n\n    kf = LeaveOneGroupOut()\n    for fold, (train_index, test_index) in enumerate(kf.split(X_train, Y_train, groups)):\n        if undersampling:\n            size1 = sum(Y_train.iloc[train_index]==1)\n            X_train_us, Y_train_us = make_imbalance(\n                    X_train.iloc[train_index].fillna(-9999), Y_train.iloc[train_index], \n                    sampling_strategy={0:size1*5, 1:size1}, random_state=42)\n            X_train_us = pd.DataFrame(X_train_us, columns=X_train.columns)\n            dtrain = lgb.Dataset(X_train_us, label=Y_train_us)\n        else:\n            dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=1000, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n        oof_preds[test_index] = clf.predict(X_train.iloc[test_index])\n        sub_preds += clf.predict(X_test) \/ nfolds\n    \n    return oof_preds, sub_preds, feature_importances","b00d9d3b":"# predict with lightgbm, LeaveOneGroupOut \ndef pred_lgb_LOGO_2(X_train, Y_train, X_test, groups, params):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    nfolds = groups.nunique()\n    print('groups:', nfolds)\n\n    kf = LeaveOneGroupOut()\n    pred_count = 0\n    for fold, (tt_index, out_index) in enumerate(kf.split(X_train, Y_train, groups)):\n        train_index = tt_index[tt_index < out_index[0]]\n        test_index = tt_index[tt_index > out_index[len(out_index) - 1]]\n        print(f'fold[{fold+1}] - {len(tt_index)} (train:{len(train_index)}, valid:{len(test_index)})')\n        if len(train_index) == 0 or len(test_index) == 0: # or len(train_index) < len(test_index)\n            print('- skip')\n            continue\n        \n        pred_count += 1\n        \n        dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=500, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(pred_count)] = clf.feature_importance()\n        oof_preds += clf.predict(X_train)\n        sub_preds += clf.predict(X_test) \n\n    oof_preds = oof_preds \/ pred_count\n    sub_preds = sub_preds \/ pred_count \n        \n    return oof_preds, sub_preds, feature_importances","083e6b82":"%%time\n\nparams={\n        #'boosting':'dart', # dart (drop out trees)\n        'learning_rate': 0.01,\n        'objective': 'binary',\n        'boost_from_average': False,\n        'is_unbalance': False,\n        'metric': 'auc',\n        'num_threads': -1,\n        'num_leaves': 256,\n        'max_bin': 256,\n        'verbose': 1,\n        'random_state': 42,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 1,\n        'feature_fraction': 0.60\n    }\n\n#oof_preds, sub_preds, feature_importances = pred_lgb(X_train, Y_train, X_test, params)\n#oof_preds, sub_preds, feature_importances = pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=3)\noof_preds, sub_preds, feature_importances = pred_lgb_LOGO(X_train, Y_train, X_test, train_group, params) #, undersampling=True\n#oof_preds, sub_preds, feature_importances = pred_lgb_LOGO_2(X_train, Y_train, X_test, train_group, params)","5628cc55":"fpr, tpr, thresholds = metrics.roc_curve(Y_train, oof_preds)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","381ee2ed":"# Add pseudo labeled data\n_='''\n#X_test_p1 = X_test[(sub_preds <= 0.00001)].copy()\n#X_test_p1['isFraud'] = 0\n#print(X_test_p1.shape)\n#Y_train = pd.concat([Y_train, X_test_p1.pop('isFraud')], axis=0)\n#X_train = pd.concat([X_train, X_test_p1], axis=0)\n\nX_test_p2 = X_test[(sub_preds >= 0.99)].copy()\nX_test_p2['isFraud'] = 1\nprint(X_test_p2.shape)\nY_train = pd.concat([Y_train, X_test_p2.pop('isFraud')], axis=0)\nX_train = pd.concat([X_train, X_test_p2], axis=0)\n\nY_train.reset_index(drop=True, inplace=True)\nX_train.reset_index(drop=True, inplace=True)\n\noof_preds, sub_preds, feature_importances = pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=3)\n\nfpr, tpr, thresholds = metrics.roc_curve(Y_train, oof_preds)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\n'''","cff0d1b7":"folds = [c for c in feature_importances.columns if c.startswith('fold')]\nfeature_importances['average'] = feature_importances[folds].mean(axis=1)\nfeature_importances.sort_values(by='average', ascending=False, inplace=True)\n#feature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(13, 13))\nsns.barplot(data=feature_importances.head(50), x='average', y='feature');\nplt.title('50 TOP feature importance');","11822fca":"feature_importances.sort_values(by='average', ascending=False)['feature'].values","3b199d75":"submission = pd.DataFrame()\nsubmission['TransactionID'] = X_test_id\nsubmission['isFraud'] = sub_preds\n\nsubmission.loc[uid_test.isin(uid_fraud).values, 'isFraud'] = 1\n\nsubmission.to_csv('submission.csv', index=False)","6387f53a":"np.mean(sub_preds)","3c2000ff":"# Load data","ac2e8404":"## Vx feature","66ebc658":"## Prepare","42ce0e1a":"## Count encoding","ad922141":"## Aggregate feature","7e7990eb":"# Predict","1e876f95":"## Mx feature","916db515":"## Combine feature","6de107fc":"## Total feature count","d957c744":"## New date feature","48caca05":"## Encode category feature","2f416a61":"## New amount feature","7b40665b":"## Drop feature","00fcffc3":"## Cx feature","cc176c55":"## Id feature","411aa663":"## Dx feature","0840d9ba":"# Feature engineering"}}