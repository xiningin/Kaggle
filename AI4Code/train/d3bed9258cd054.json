{"cell_type":{"c2f75101":"code","32eb8f80":"code","737289c8":"code","0f37fe17":"code","ae7b7499":"code","248571bf":"code","a03d1b9b":"code","ea90f9e2":"code","f51d7f81":"code","455e8406":"code","0c25467e":"code","9f824ed2":"code","a5097edb":"code","73e9a956":"code","e82016e5":"code","b556f7b3":"code","78181afd":"code","fa30d658":"code","0035eb5a":"code","d3c61631":"code","3df5405b":"code","5a68c2f6":"code","9d78088b":"code","c166194e":"code","d7b14456":"code","718d59d1":"code","147ab5e5":"code","3d9ee1e2":"code","6ae73f5a":"code","07533032":"code","1c15e867":"code","b7e59540":"code","1ef012c0":"code","4ec9a05b":"code","cbbaf32e":"code","a08cf6ae":"code","4bdd343c":"code","1f625b49":"code","51bacaae":"code","57e4c45c":"code","9aa64c03":"code","4bdd731c":"code","eb654aca":"code","410f6c40":"code","afa98862":"code","1f4dd42e":"code","8f47d9a2":"code","b5f02bc2":"code","385ac646":"code","1dd4146d":"code","0695d03e":"code","82de5511":"code","add6cfd0":"code","876e3956":"code","fe76d37a":"code","6022f10c":"code","01aae469":"code","ca633be6":"code","1fa9bc93":"markdown","8c938e2a":"markdown","62da3523":"markdown","1978a74a":"markdown","d9c40863":"markdown","914e99ce":"markdown","5d0dec75":"markdown","2573009b":"markdown","262fdb28":"markdown","6f71e448":"markdown","98b3e195":"markdown","89fe043f":"markdown","b86c52f0":"markdown","b142e074":"markdown","282ae3cd":"markdown","b62cf64a":"markdown","d2ddeffb":"markdown","2987d59c":"markdown","39886e40":"markdown"},"source":{"c2f75101":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32eb8f80":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","737289c8":"print(train.shape)\nprint(test.shape)\nprint(submission.shape)","0f37fe17":"display(train.head)\ndisplay(test.head)\ndisplay(submission.head)","ae7b7499":"## remove #tags,@words and links from the text field\nimport re,string\n\ndef strip_links(text):\n    link_text = re.sub('http:\/\/\\S+|https:\/\/\\S+', '', text)\n    return link_text\n    \n\ndef strip_all_entities(text):\n    entity_prefixes = ['@','#']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)","248571bf":"train['text_1']=train['text'].apply(lambda x:strip_links(x))\ntest['text_1']=test['text'].apply(lambda x:strip_links(x))","a03d1b9b":"train['text_2']=train['text_1'].apply(lambda x:strip_all_entities(x))\ntest['text_2']=test['text_1'].apply(lambda x:strip_all_entities(x))","ea90f9e2":"print(test['text'][3260])\nprint(test['text_1'][3260])\nprint(test['text_2'][3260])","f51d7f81":"train['text_2'] = train['text_2'].apply(lambda x: x.lower())\ntrain['text_3'] = train['text_2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","455e8406":"test['text_2'] = test['text_2'].apply(lambda x: x.lower())\ntest['text_3'] = test['text_2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","0c25467e":"print(test['text'][3260])\nprint(test['text_3'][3260])","9f824ed2":"print(train[ train['target'] == 1].size)\nprint(train[ train['target'] == 0].size)","a5097edb":"## For train\nl=[]\nfor i in range(len(train)):\n    l.append(len([w for w in train.loc[i,'text_3'].split(' ')]))","73e9a956":"## For test\nl1=[]\nfor i in range(len(test)):\n    l1.append(len([w for w in test.loc[i,'text_3'].split(' ')]))","e82016e5":"### Add a column of length of tokens\ntrain['token_cnt']=l\ntest['token_cnt']=l1","b556f7b3":"train['token_cnt'].max()## Maximumm tokens in train set =34\ntest['token_cnt'].max()## Maximumm tokens in train set =32","78181afd":"train.drop(['keyword','location','text','text_1','text_2','token_cnt'],axis=1,inplace=True)","fa30d658":"test.drop(['keyword','location','text','text_1','text_2','token_cnt'],axis=1,inplace=True)","0035eb5a":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    train['text_3'],\n    train['target'],\n    test_size=0.2, \n    random_state=123\n)","d3c61631":"from tensorflow.python.keras.preprocessing.text import Tokenizer","3df5405b":"### Declare the vocabulary size for word embedding\ntop_words = 1000\nt = Tokenizer(num_words=top_words) # num_words -> Vocablury size\nt.fit_on_texts(X_train.tolist())","5a68c2f6":"X_train = t.texts_to_sequences(X_train.tolist())\nX_val = t.texts_to_sequences(X_val.tolist())","9d78088b":"from tensorflow.python.keras.preprocessing import sequence\nmax_review_length = 50","c166194e":"X_train = sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\nX_val = sequence.pad_sequences(X_val, maxlen=max_review_length, padding='post')","d7b14456":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","718d59d1":"#Check if embeddings have been downloaded\n!ls -l","147ab5e5":"#unzip the file, we get multiple embedding files. We can use either one of them\n!unzip glove.6B.zip","3d9ee1e2":"!ls -l","6ae73f5a":"from gensim.scripts.glove2word2vec import glove2word2vec","07533032":"#Glove file - we are using model with 50 embedding size\nglove_input_file = 'glove.6B.50d.txt'\n\n#Name for word2vec file\nword2vec_output_file = 'glove.6B.50d.txt.word2vec'\n\n#Convert Glove embeddings to Word2Vec embeddings\nglove2word2vec(glove_input_file, word2vec_output_file)","1c15e867":"!ls -l","b7e59540":"### We will extract word embedding for which we are interested in; the pre trained has 400k words each with 50 embedding vector size.\nfrom gensim.models import Word2Vec, KeyedVectors","1ef012c0":"# Load pretrained Glove model (in word2vec form)\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)","4ec9a05b":"#Embedding length based on selected model - we are using 50d here.\nembedding_vector_length = 50","cbbaf32e":"#Initialize embedding matrix\nembedding_matrix = np.zeros((top_words + 1, embedding_vector_length))\nprint(embedding_matrix.shape)","a08cf6ae":"for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n    if i > (top_words+1):\n        break\n    try:\n        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix[i] = embedding_vector\n    except:\n        pass","4bdd343c":"embedding_matrix[3]","1f625b49":"#Initialize model\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","51bacaae":"model.add(tf.keras.layers.Embedding( top_words+ 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=max_review_length) #Number of words in each review\n         )","57e4c45c":"model.output","9aa64c03":"model.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.LSTM(256)) #RNN State - size of cell state and hidden state\nmodel.add(tf.keras.layers.Dropout(0.2))","4bdd731c":"model.output","eb654aca":"model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n#Compile the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","410f6c40":"model.summary()","afa98862":"model.fit(X_train,y_train,\n          epochs=10,\n          batch_size=32,          \n          validation_data=(X_val, y_val))","1f4dd42e":"x = model.get_layer('lstm').output\nmodel2 = tf.keras.Model(model.input, x)","8f47d9a2":"### Declare the vocabulary size for word embedding\ntop_words = 1000\nt = Tokenizer(num_words=top_words) # num_words -> Vocablury size\nt.fit_on_texts(train['text_3'].tolist())\ntrain_seq  = t.texts_to_sequences(train['text_3'].tolist())\ntest_seq  = t.texts_to_sequences(test['text_3'].tolist())\n\ntrain_seq = sequence.pad_sequences(train_seq,maxlen=max_review_length,padding='post')\ntest_seq = sequence.pad_sequences(test_seq, maxlen=max_review_length, padding='post')","b5f02bc2":"train_seq","385ac646":"model.fit(train_seq,train['target'],\n          epochs=10,\n          batch_size=32)","1dd4146d":"## train for 10 more rounds\nmodel.fit(train_seq,train['target'],\n          epochs=20,\n          initial_epoch=10,\n          batch_size=32)","0695d03e":"test_pred=model.predict(test_seq)","82de5511":"test_pred=test_pred.reshape((3263,))","add6cfd0":"test['prediction']=test_pred","876e3956":"test['target']=np.where(test['prediction']>0.5,1,0)","fe76d37a":"test['target'].value_counts()","6022f10c":"test.drop(['text_3','prediction'],axis=1,inplace=True)","01aae469":"test","ca633be6":"test.to_csv('sample_submission.csv',index=False)","1fa9bc93":"### Split the data of train into trainn and validation","8c938e2a":"### Drop unnecessary columns","62da3523":"### Get embedding from loaded pretrained model ","1978a74a":"### Generate the word index for train and test","d9c40863":"### Add LSTM layers with 256 cell and hidden state size","914e99ce":"### Cleaning Part","5d0dec75":"### Check the length of tokens","2573009b":"### Remove hyperlinks","262fdb28":"### Load pretrained embedding model from gensim","6f71e448":"### Train the Model","98b3e195":"### Append to test data","89fe043f":"### Build the Tokenizer","b86c52f0":"### Remove hashtags and mentions if any","b142e074":"### Check the number of 0's and 1's in tweets","282ae3cd":"### Train the model on the whole train set","b62cf64a":"### Remove special characters","d2ddeffb":"### Pad the sequence of tokens","2987d59c":"### Store the output of lstm","39886e40":"### Intitate the model"}}