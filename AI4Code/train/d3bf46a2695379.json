{"cell_type":{"61728201":"code","85ad5469":"code","4230c0c7":"code","8d386a90":"code","ad8afce1":"code","d45c69bd":"code","7c3f8818":"code","d3314687":"code","c0586edc":"code","a9065b02":"code","1e525df0":"code","fdffb753":"code","97e4ce60":"code","6711f62c":"code","7f548fb9":"code","3449018d":"code","4751ab95":"code","4bd5cb7e":"code","49d7297d":"code","9183e5cf":"code","694ff3be":"code","134cc2ba":"code","5dafe896":"code","c9572115":"code","d84a06c9":"code","1933c29a":"code","f6db848c":"code","c580a305":"code","9b152c91":"code","7671e897":"code","ae85c9da":"code","29fe890c":"code","1937d38e":"code","fd5c5b46":"code","126c86a2":"code","b91bbad3":"code","354e6caf":"code","901fa81d":"code","22c4cc57":"code","4f73a665":"code","c819ae8c":"code","ae4705fd":"code","3c37ad93":"code","7efca211":"code","83521113":"code","ec553520":"code","c9c5a7ed":"code","0ea5c3cb":"code","2b2b7111":"code","7508c4cd":"code","7eadfd2e":"code","c2a6d31c":"code","bf19fd51":"code","cba04314":"code","bd55cd5f":"code","431ff6f6":"code","f19fa212":"code","a782d9fd":"code","5795b8b2":"markdown","85125d9f":"markdown","6f0b5d9e":"markdown","dccb7394":"markdown","22acc6c8":"markdown","1ea62384":"markdown","379ab718":"markdown","345ec3d2":"markdown","56b7defd":"markdown","8c6d8cb5":"markdown","2e95f62c":"markdown","fa278f2d":"markdown","b346643e":"markdown","ce235022":"markdown","1977da97":"markdown","ac8d83db":"markdown","4e80e442":"markdown","bc69dfa9":"markdown","40ac5465":"markdown","7e668c91":"markdown","5aaf738a":"markdown","8199b611":"markdown","7727234c":"markdown","cfa25b4c":"markdown","2da081c5":"markdown","54d20409":"markdown","f19e1404":"markdown","9f04c1c4":"markdown","542b5b74":"markdown","f224fa4e":"markdown","f5332d78":"markdown","58008a0c":"markdown","eeb06110":"markdown","2d1bf4b8":"markdown","ecca51f8":"markdown","3341a43c":"markdown"},"source":{"61728201":"# pip install comet_ml","85ad5469":"# import comet_ml in the top of your file\n# from comet_ml import Experiment\n\n# # Initialise comet experiment\n# experiment = Experiment(api_key=\"NWLadblAxMd1YoXYmNsNiVX36\",\n#                         project_name=\"nlp-machine-learning\", workspace=\"pilasande\")","4230c0c7":"# Python packages\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\n# Wordcount visualizations\nfrom wordcloud import WordCloud\n# NLP\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom spacy import displacy\nfrom bs4 import BeautifulSoup\n# sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\n# Optimization\nfrom sklearn.model_selection import GridSearchCV\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score,accuracy_score\n# Warnings\nimport warnings","8d386a90":"train = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')\nsample = pd.read_csv('..\/input\/climate-change-belief-analysis\/sample_submission.csv')\ndata = pd.read_csv('..\/input\/mbti-type\/mbti_1.csv')\nmbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', 'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', 'J':'Judging', 'P': 'Perceiving'}","ad8afce1":"# view data\ntrain.head()","d45c69bd":"# Data summary and checking for nulls\ntrain.info()","7c3f8818":"# label the stance indicated by the class\nlabels_dict = {-1: 'Agnostic',0: 'Neutral',1: 'Believer',2: 'News'}\n# Replace class values with tweet stance:\ntrain.replace({'sentiment': labels_dict}, inplace=True)","d3314687":"# Document Corpus\nraw_corpus = [statement.lower() for statement in train.message]","c0586edc":"def word_count(df,Corpus):\n    \"\"\"Output graph of most frequent words in each class\n       given a dataframe with a class column and a corpus \"\"\"\n    fig, axs = plt.subplots(2,2, figsize=(16,8),)\n    fig.subplots_adjust(hspace = 0.5, wspace=.2)\n    axs = axs.ravel()\n    for index, stance in enumerate(df.sentiment.unique()):\n        corpus = np.array(Corpus)[df[df.sentiment == stance].index.values]\n        corpus = ' '.join(corpus).split(' ')\n        word_counts = {}\n        for word in corpus:\n            if word in word_counts.keys():\n                word_counts[word] += 1\n            else:\n                word_counts[word] = 1\n        word_val_pair = []\n        for word,word_freq in word_counts.items():\n            word_val_pair.append((word,word_freq))\n        word_val_pair.sort(key = lambda x: x[1],reverse=True)\n        words = []\n        frequency = []\n        for word_val in word_val_pair[:10]:\n            words.append(word_val[0])\n            frequency.append(word_val[1])\n        axs[index].set_title(f'{stance}',fontsize=15)\n        axs[index].bar(x=words,height=frequency,edgecolor='k')\n    ","a9065b02":"word_count(train,raw_corpus)","1e525df0":"# Word Cloud\ndef word_cloud(input_df,Corpus):\n    \"\"\"Function output the wordcloud of a class given\n       a dataframe with a sentiment column and a corpus\"\"\"\n    df = input_df.copy()\n    fig, axs = plt.subplots(2,2, figsize=(16,8))\n    fig.subplots_adjust(hspace = 0.5, wspace=.2)\n    axs = axs.ravel()\n    for index, stance in enumerate(df.sentiment.unique()):\n        corpus = np.array(Corpus)[df[df.sentiment == stance].index.values]\n        corpus = ' '.join(corpus)\n        word_cloud = WordCloud(background_color='white', max_font_size=80).generate(corpus)\n        axs[index].set_title(f'{stance}',fontsize=15)\n        axs[index].imshow(word_cloud,interpolation='bilinear')\n        axs[index].axis('off')\n","fdffb753":"word_cloud(train,raw_corpus)","97e4ce60":"def hashtags(input_df,Corpus):\n    \"\"\"Function output the wordcloud of a class given\n       a dataframe with a sentiment column and a corpus\"\"\"\n    df = input_df.copy()\n    fig, axs = plt.subplots(2,2, figsize=(16,8))\n    fig.subplots_adjust(hspace = 0.5, wspace=.2)\n    axs = axs.ravel()\n    for index, stance in enumerate(df.sentiment.unique()):\n        corpus = list(np.array(Corpus)[df[df.sentiment == stance].index.values])\n        for line in range(len(corpus)):\n            corpus[line] = ' '.join([word for word in corpus[line].split() if word.startswith('#')])\n        corpus = ' '.join([word for word in corpus if word])\n        corpus = re.sub(r\"[,.\\\"!@#$%^&*(){}?\/;`~:<>+=-]\", \"\", corpus)\n        word_cloud = WordCloud(background_color='black', max_font_size=80).generate(corpus)\n        axs[index].set_title(f'{stance}',fontsize=15)\n        axs[index].imshow(word_cloud,interpolation='bilinear')\n        axs[index].axis('off')","6711f62c":"hashtags(train,raw_corpus)","7f548fb9":"green_terms = ['biofuels','photovoltaic',\n               'cap-and-trade','pollution',\n               'carbon dioxide','renewable energy',\n               'carbon footprint','solar',\n               'carbon offsets','wind energy',\n               'carbon tax','carcinogen',\n               'clean energy','clean tech', \n               'climate bill','climate change',\n               'corporate social responsibility',\n               'cradle to cradle','ecolabel',\n               'energy','fossil fuels',\n               'green economy','green roof',\n               'green-collar','greenhouse',\n               'cycle assessment','wind power','green',\n               'carbon','dioxide']\ndef green_speak(input_df,Corpus):\n    \"\"\"Function output the wordcloud of a class given\n       a dataframe with a sentiment column and a corpus\"\"\"\n    df = input_df.copy()\n    fig, axs = plt.subplots(2,2, figsize=(16,8))\n    fig.subplots_adjust(hspace = 0.5, wspace=.2)\n    axs = axs.ravel()\n    for index, stance in enumerate(df.sentiment.unique()):\n        corpus = np.array(Corpus)[df[df.sentiment == stance].index.values]\n        corpus = ' '.join(corpus)\n        corpus = re.sub(r\"[,.\\\"!@#$%^&*(){}?\/;`~:<>+=-]\", \"\", corpus)\n        word_dict = {}\n        for term in green_terms:\n            if term in corpus:\n                word_dict[term] = corpus.count(term)\n        word_cloud = WordCloud(background_color='black', max_font_size=80).generate_from_frequencies(word_dict)\n        axs[index].set_title(f'{stance}',fontsize=15)\n        axs[index].imshow(word_cloud,interpolation='bilinear')\n        axs[index].axis('off')","3449018d":"green_speak(train,raw_corpus)","4751ab95":"imbalance = sns.barplot(x = train.sentiment.value_counts(), y= train.sentiment.value_counts().index)\nimbalance.set(title = 'Class distribution in data',xlabel='sentiment counts')\nplt.show()","4bd5cb7e":"def boxplot(input_df):\n    df = input_df.copy()\n    df.tweet_length = df.message.apply(lambda x: len(x))\n    plot = sns.boxplot(x=df.tweet_length,y=df.sentiment)\n    plot.set(xlabel='tweet_lenght')\n    return(plot)","49d7297d":"boxplot(train)\nplt.show()","9183e5cf":"# Set stopwords\nadded_stop_words = ['rt','dm']\nstop_words = set(stopwords.words(\"english\")+added_stop_words)\nremoved_stop_words = ['not','do']\nfor i in removed_stop_words:\n    stop_words.remove(i)","694ff3be":"# Define Cleanig function\ndef cleaning_fun(tweet):\n    \"\"\"This function takes a tweet and extracts important text\"\"\"\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+','',tweet) # Remove URLs\n    tweet = re.sub(r'<.*?>','',tweet) # Remove html tags\n    tweet = re.sub(r'abc|cnn|fox|sabc','news',tweet) # Replace tags with news\n    tweet = re.sub(r'climatechange','climate change',tweet)\n#   Tokenize tweet\n    tokenizer = TreebankWordTokenizer()\n    tweet = tokenizer.tokenize(tweet)\n    tweet = [word for word in tweet if word.isalnum()] #Remove punctuations\n#   Remove numbers\n    tweet = [word for word in tweet if not any(c.isdigit() for c in word)]\n#   Replace News if news is in the words\n    tweet = ['news' if 'news' in word else word for word in tweet]\n#   Replace word with trump if trump is in the word\n    tweet = ['trump' if 'trump' in word else word for word in tweet]\n#   Remove stop words\n    tweet = ' '.join([word for word in tweet if word not in stop_words])\n    return(tweet)","134cc2ba":"# Add clean tweets column to train data\ntrain['clean_tweets'] = train.message.apply(lambda x: cleaning_fun(x))\ntrain.head(3)","5dafe896":"# cleaned corpus\nclean_corpus = [cleaning_fun(tweet) for tweet in raw_corpus]","c9572115":"word_count(train,clean_corpus)","d84a06c9":"word_cloud(train,clean_corpus)","1933c29a":"def NER(corpus):\n    nlp = spacy.load('en_core_web_sm')\n    seperator=','\n    y=[]\n    doc=nlp(seperator.join(clean_corpus[:90]))\n    for entity in doc.ents:\n        y.append(entity.text)\n    word_cloud = WordCloud(background_color='white', max_font_size=80).generate(seperator.join(y))\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(word_cloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show()\nNER(clean_corpus)","f6db848c":"def MBTI(input_df,train):\n    df = input_df.copy()\n    for i in range(len(df)):\n        df.posts[i] = BeautifulSoup(df.posts[i], \"lxml\").text\n        df.posts[i] = re.sub(r'\\|\\|\\|', r' ', df.posts[i])\n        df.posts[i] = re.sub(r'http\\S+', r'<URL>', df.posts[i])\n    np.random.seed(1)\n    tfidf2 = CountVectorizer(ngram_range=(1, 1), stop_words='english',lowercase = True, max_features = 5000)\n    model_lr = Pipeline([('tfidf1', tfidf2), ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005,max_iter=300))])\n    warnings.filterwarnings(\"ignore\")\n    model_lr.fit(df.posts, df.type)\n    separator = ', '\n    a=separator.join(train.query(\"sentiment=='News'\")['clean_tweets'][:1000].values.tolist())\n    b=separator.join(train.query(\"sentiment=='Believer'\")['clean_tweets'][:1000].values.tolist())\n    c=separator.join(train.query(\"sentiment=='Neutral'\")['clean_tweets'][:1000].values.tolist())\n    d=separator.join(train.query(\"sentiment=='Agnostic'\")['clean_tweets'][:1000].values.tolist())\n    k=[a,b,c,d]\n    pred_all = model_lr.predict(k)\n    return (pred_all)\nMBTI(data,train)","c580a305":"#instantiate and vectorize corpus\ncount_vectorizer = CountVectorizer(ngram_range=(1,2))\ncount_vectorizer.fit(clean_corpus)","9b152c91":"#Included for pickling to web app (not used in actual model) \ntfvectorizer = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\ntfvectorizer.fit(clean_corpus)","7671e897":"# Define feature and target variables\nX = train.clean_tweets\ny = train.sentiment","ae85c9da":"# Tran test split data\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","29fe890c":"# Vectorize test and train set\nX_train = count_vectorizer.transform(X_train)\nX_test = count_vectorizer.transform(X_test)","1937d38e":"from sklearn.linear_model import SGDClassifier\n# Define Classification Models to be tested with default parameters\nmodels = {'LogisticReg': LogisticRegression(multi_class='ovr',\n                                            class_weight='balanced',\n                                            max_iter=1000),\n          'LinearSVC': LinearSVC(),\n          'RandomForest': RandomForestClassifier(n_estimators=5)}","fd5c5b46":"perfomance_df = pd.DataFrame()\nfor name in models.keys():\n    scores = cross_val_score(models[name], X_train, y_train, cv=5, scoring='f1_weighted')\n    mean_score = round(scores.mean(),2)\n    mean_stddev = round(scores.std(),3)\n    temp = pd.DataFrame({'weighted_f1_avg':mean_score,'deviation':mean_stddev}, index=[name])\n    perfomance_df = pd.concat([perfomance_df, temp])\nprint(perfomance_df.sort_values('weighted_f1_avg', ascending=False))","126c86a2":"# Validation of Models\nval_df = pd.DataFrame()\nfor name in models.keys():\n    models[name].fit(X_train,y_train)\n    y_pred = models[name].predict(X_test)\n    eval_score = f1_score(y_test,y_pred,average='weighted')\n    eval_score = round(eval_score,2)\n    temp = pd.DataFrame({'weighted_f1_avg':eval_score}, index=[name])\n    val_df = pd.concat([val_df, temp])\nprint(val_df.sort_values('weighted_f1_avg', ascending=False))","b91bbad3":"chosen_model = 'LogisticReg'","354e6caf":"y_pred = models[chosen_model].predict(X_test)","901fa81d":"def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(5,5)):\n    \"\"\"\n    Generate matrix plot of confusion matrix with pretty annotations.\n    The plot image is saved to disk.\n    args: \n      y_true:    true label of the data, with shape (nsamples,)\n      y_pred:    prediction of the data, with shape (nsamples,)\n      filename:  filename of figure file to save\n      labels:    string array, name the order of class labels in the confusion matrix.\n                 use `clf.classes_` if using scikit-learn models.\n                 with shape (nclass,).\n      ymap:      dict: any -> string, length == nclass.\n                 if not None, map the labels & ys to more understandable strings.\n                 Caution: original y_true, y_pred and labels must align.\n      figsize:   the size of the figure plotted.\n    \"\"\"\n    if ymap is not None:\n        y_pred = [ymap[yi] for yi in y_pred]\n        y_true = [ymap[yi] for yi in y_true]\n        labels = [ymap[yi] for yi in labels]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%' % (p)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%' % (p)\n    cm = pd.DataFrame(cm, index=labels, columns=labels)\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n# Code source : https:\/\/gist.github.com\/hitvoice\/36cf44689065ca9b927431546381a3f7","22c4cc57":"cm_analysis(y_test,y_pred,models[chosen_model].classes_)","4f73a665":"classification_report(y_test, y_pred)","c819ae8c":"# Comet log parameters\n# experiment.log_metric('weighted_f1',f1_score(y_test,y_pred,average='weighted'))\n# experiment.log_confusion_matrix(y_test, y_pred)","ae4705fd":"parameters = {'C':[10,5,1],\n              'multi_class': ['ovr','multinomial']}\ngrid_search = GridSearchCV(models[chosen_model], parameters,scoring='f1_weighted')\ngrid_search.fit(X_train, y_train)\nsearch_params = grid_search.best_params_","3c37ad93":"best_logistic_model = LogisticRegression(multi_class=search_params['multi_class'],\n                                         class_weight='balanced',\n                                         max_iter=1000,\n                                         C = search_params['C'])\nscore = cross_val_score(best_logistic_model, X_train, y_train, cv=5, scoring='f1_weighted')\nmean_score = round(score.mean(),2)\nprint(mean_score)","7efca211":"best_logistic_model.fit(X_train,y_train)\ny_pred = best_logistic_model.predict(X_test)","83521113":"# Confussion Matrix\ncm_analysis(y_test,y_pred,best_logistic_model.classes_)","ec553520":"#classifiaction report\nprint(classification_report(y_test, y_pred))","c9c5a7ed":"# Comet log parameters\nexperiment.log_metric('weighted_f1',f1_score(y_test,y_pred,average='weighted'))\nexperiment.log_confusion_matrix(y_test, y_pred)","0ea5c3cb":"#End Comet experiment\nexperiment.end()","2b2b7111":"# Clean test file\ntest.message = test.message.apply(lambda x: cleaning_fun(x))","7508c4cd":"test_X = count_vectorizer.transform(test.message)","7eadfd2e":"test_pred = best_logistic_model.predict(test_X)","c2a6d31c":"output = pd.DataFrame({'tweetid':test.tweetid,'sentiment':test_pred})","bf19fd51":"# Replace original labels\nnew_dict = {'Agnostic':-1,'Neutral':0,'Believer':1,'News':2}\noutput.replace({'sentiment':new_dict},inplace=True)","cba04314":"output.head()","bd55cd5f":"output.to_csv('my_submission.csv', index=False)","431ff6f6":"1. https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/#textbloblemmatizerwithappropriatepostag <br>\n2. https:\/\/gist.github.com\/hitvoice\/36cf44689065ca9b927431546381a3f7<br>\n3. https:\/\/www.kaggle.com\/lbronchal\/what-s-the-personality-of-kaggle-users","f19fa212":"# Pickle Vectorizers\nimport pickle\nVectorizers = {'countvec':count_vectorizer,'tfidf':tfvectorizer}\nfor filename,item in Vectorizers.items():\n    outfile = open(f'{filename}.pkl','wb')\n    pickle.dump(item,outfile)\n    outfile.close()","a782d9fd":"#Pickle Models\nfor filename,item in models.items():\n    outfile = open(f'{filename}.pkl','wb')\n    pickle.dump(item,outfile)\n    outfile.close()","5795b8b2":"It can be seen from the confission matrix that the model perfoms relatively well for the classes of Believers and news tweets, Our model appears to be biased towards the believers class with the class containing the highest numbers of false positives\/false negatives (i.e 29.5% of agnostic,32.9% of Neutral and 15.3% of news tweets were all classified as Believers)","85125d9f":"The plot indicates that news and neutral tweeps tend to write shorter messages as compared to the agnostics and believers, neutral tweets have a higher variation (with some being very short and others very long), it is also quite possibly harder to extract sentiment from shorter tweets as they do not give enough context.","6f0b5d9e":"The model could be especially useful to businesses looking into the Lifestyles Of Health and Sustainability (LOHAS) market.Exploratory data analysis revealed that tweets classified as Believers or Agnostics were strategic and logical individuals. Carefully constructed arguments as well as objective evidence is the way to go when marketing green products to these groups.","dccb7394":"# Import Data <a class=\"anchor\" id=\"import_data\"><\/a>","22acc6c8":"Once best parameters are found from gridsearch, the model is then initialised with the best parameters and a cross validation is perfomed once again.","1ea62384":"# References <a class=\"anchor\" id=\"9th-bullet\"><\/a>","379ab718":"## Green Speak Terms <a class=\"anchor\" id=\"green_speak\"><\/a>","345ec3d2":"The following models were tested:\n- Logistic Regression Model <br>\nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. The idea in logistic regression is to cast the problem in the form of a generalized linear regression model.\nMulticlass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross-entropy loss.\n- Linear Support Vector Classifier<br>\nSupport vector machine algorithm in which each data item is plotted as a point in n-dimensional space (where n is number of features) with the value of each feature being the value of a particular coordinate. Then, classification is perfomed by finding the hyper-plane that differentiates the two classes. Linear SVC then uses linear support vectors as the between classes.\n- Random Forest<br>\nRandomforest is an ensemle method using decision trees, bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.","56b7defd":"# Model Selection <a class=\"anchor\" id=\"model_selection\"><\/a>","8c6d8cb5":"# Submission File Preparation <a class=\"anchor\" id=\"8th-bullet\"><\/a>","2e95f62c":"From initial preprocessing the following lines were added to the cleaning function:<br>\n<code>tweet = re.sub(r'climatechange','climate change',tweet)<\/code><br>\n<code>tweet = re.sub(r'abc|cnn|fox|sabc','news',tweet)<\/code><br>\n<code>tweet = ['news' if 'news' in word else word for word in tweet]<\/code><br>\n<code>tweet = ['trump' if 'trump' in word else word for word in tweet]<\/code><br>","fa278f2d":"A grid search over five folds was used on the model to tune the C parameter for an optimized model.","b346643e":"## Effect of lemmitazation\nThe following packages were used for lemmitazation:<br>\n<code>from nltk.corpus import wordnet<\/code><br>\n<code>from nltk import pos_tag<\/code><br>\n<code>from nltk.stem import WordNetLemmatizer<\/code><br>\n#Define tagging function to be used with lemmatization<br>\n<code>def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)<\/code><br>\nwith the following in the cleaning_fun function:<br>\n<code>lemm = WordNetLemmatizer()\ntweet = [lemm.lemmatize(word, get_wordnet_pos(word)) for word in tweet]\n<\/code>\nOverall it was found that lemmatization offered no improvement in model perfomance and lead to poorer recall values in 'Neutral' and 'Agnostic' classes.<br>\nCode source[https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/#textbloblemmatizerwithappropriatepostag].","ce235022":"The plots between classes show the 10 most used words in each class, it can be seen that climate is the most used word in the data sets in 3 of the four clases, the plots also consist of common english words which offer little insights, futher cleaning must be done to remove these words.","1977da97":"This section aims to clean the raw data into the most important text articles, after cleaning the raw data a visualization similar to the one on the EDA step is shown. The data is cleaned using the <code>cleaning_fun<\/code> function which applies the use of regular expressions, list comprehensions and nltk packages (tokenization and stop words) to remove text which is deemed non-significant for sentiment analysis.","ac8d83db":"## Model Training and Validation","4e80e442":"## Parameter Tuning","bc69dfa9":"Based on perfomance on cross validation the logistic Regression model was chosen","40ac5465":"**Meaning of Clases**<br>\n<table align='left' style='width:50%'>\n    <tr>\n        <th style='width:10%' align=\"center\">Class<\/th>\n        <th style='width:30%' align=\"center\">Sentiment<\/th>\n        <th style='width:5%' align='center'>Stance<\/th>\n    <\/tr>\n    <tr>\n        <td>-1<\/td>\n        <td>The tweet does not believe in man made climate change<\/td>\n        <td>Agnostic<\/td>\n    <\/tr>\n    <tr>\n        <td>0<\/td>\n        <td>The tweet neither supports nor refutes the belief of man made climate change<\/td>\n        <td>Neutral<\/td>\n    <\/tr>\n    <tr>\n        <td>1<\/td>\n        <td>The tweet supports the belief of man made climate change<\/td>\n        <td>Believer<\/td>\n    <\/tr>\n    <tr>\n        <td>2<\/td>\n        <td>The tweet links to factual news on climate change<\/td>\n        <td>News<\/td>\n    <\/tr>\n<\/table>","7e668c91":"## Tweet length distribution <a class=\"anchor\" id=\"tweet_lenght\"><\/a>","5aaf738a":"# Import Packages <a class=\"anchor\" id=\"import_packages\"><\/a>","8199b611":"## Vectorize Corpus and Split Data <a class=\"anchor\" id=\"vectorize_corpus\"><\/a>","7727234c":"The data shows imbalanced data with 59% of the sample being in the Believer class and only 8% being in the Agnostics class, this may lead to a biased model. This hypothesis was tested by oversampling the minority classes to half the number of the Believers class and further undersampling the believers class to half its size in the train set, the results yeilded a decrease in model perfomance.","cfa25b4c":"## Hashtag analysis <a class=\"anchor\" id=\"hashtag_analysis\"><\/a>\nIn this section we look at the hashtags association between the classes, a word count is perfomed and the data visalized in the form of a wordcloud.","2da081c5":"The following section sets out to do analysis on the raw data and prime non essential elements for improved perfomance of the model(s).","54d20409":"# Exploratory Data Analysis <a class=\"anchor\" id=\"exploratory data analysis\"><\/a>","f19e1404":"## Class Imbalance <a class=\"anchor\" id=\"class imbalance\"><\/a>\nThis section aims to look at class imbalance of the given dataset","9f04c1c4":"# Conclusion","542b5b74":"# Data Preprocessing <a class=\"anchor\" id=\"data_preprocessing\"><\/a>","f224fa4e":"# Pickle Files Preparation <a class=\"anchor\" id=\"10th-bullet\"><\/a>","f5332d78":"# Insights and Observations <a class=\"anchor\" id=\"6th-bullet\"><\/a>","58008a0c":"## Class WordCount <a class=\"anchor\" id=\"class_word_count\"><\/a>\nThe aim is to calculate the frequency of words in each class and find the most used words for each class.","eeb06110":"# TABLE OF CONTENTS\n* [Introduction](#intoduction)\n* [Import Packages](#import_packages)\n* [Import Data](#import_data)\n* [Exploratory Data Analysis](#exploratory_data)\n* [Data Preprocessing](#data_preprocessing)\n* [Model Selection](#model_selection)\n* [Insights](#insights)\n* [Conlusion](#conclusion)\n* [Kaggle Submission File](#kaggle_submission_file)\n* [References](#references)\n* [Pickled files](#pickled_files)\n","2d1bf4b8":"With the continued popularity of online social networking,companies have started focusing their marketing efforts oninteractive media such as Twitter and Facebook. These media channels, as well as others, are helping companies to better engage with their customers\/consumers than traditional marketing methods can. In addition, the data science space has revolutionzed the process of market research through innovative methods. Given that there is so much corporate activity, media attention, and consumer involvement been directed toward sustaining the planet,improving the lives of people around the world, and protecting the ability of future generations to meet their own needs, the focus of this notebook is to accurately clasify belief in anthropogenic climate change, a hot topic, within a dataset of tweets.","ecca51f8":"**Model Metrics on test data**","3341a43c":"# Introduction <a class=\"anchor\" id=\"introduction\"><\/a>"}}