{"cell_type":{"37999c18":"code","e61963da":"code","bb30f039":"code","9163ac81":"code","a7168e81":"code","205134d2":"code","1bb9dbd0":"code","1777093e":"code","d820ac55":"code","1c9b72d7":"code","c5356695":"code","02d5d988":"code","9fc51afc":"code","6bf52d9b":"code","4a3c4095":"code","24297d5f":"code","213aa545":"code","6d3df85f":"code","cffd52e5":"code","449f697c":"code","98e9508f":"code","d13a5292":"code","ac88f6d1":"code","d35053c1":"code","e6a9383b":"code","7a7a8411":"code","5738efff":"code","a4f017fc":"code","2eacd324":"code","2d737e05":"code","ec5788e6":"code","8f41c819":"code","2ab6c1a3":"code","cf18c42b":"code","c3a99ab6":"code","e74f5f21":"code","0e46e6ab":"code","63db7775":"code","c2fdecd6":"code","167989f9":"code","55f680b4":"code","1ecf9ede":"code","c77ae62d":"code","780e4f97":"code","1d6be673":"code","a7bd4164":"code","00b8284a":"code","a5d0db03":"code","01fe3383":"code","e9657c70":"code","eda5ce32":"code","4696a528":"code","166773c3":"code","9ed5a898":"code","08ebc979":"code","a0c862d3":"code","16614782":"code","1e25bd45":"code","dc0d92af":"code","e5c1cf8e":"code","79d52813":"code","508089c3":"code","33cd1216":"code","8922f4a0":"markdown","3931f978":"markdown","62980372":"markdown","4042c994":"markdown","b7c5d66f":"markdown","a7f208a1":"markdown","b5fe4f84":"markdown","f05664f9":"markdown","3c270798":"markdown","c1f30bba":"markdown","9fcb74c3":"markdown","6b1791f0":"markdown","04a25836":"markdown","9a874c32":"markdown","dfdf7fa8":"markdown","c5174937":"markdown","3713a68d":"markdown","d453b9c2":"markdown","d5e9c516":"markdown","47fd56dd":"markdown","a3bdddf7":"markdown","67b26d0c":"markdown","c0bb4fbe":"markdown"},"source":{"37999c18":"!pip install visualkeras","e61963da":"import plotly.offline as pyo\npyo.init_notebook_mode()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2 \nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn import preprocessing\nimport random\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bb30f039":"def Create_Directory_DataFrame():\n    df =pd.DataFrame(columns=['Class','Location'])\n    basedir = '..\/input\/leapgestrecog\/leapGestRecog\/'\n    for folder in os.listdir(basedir):\n        for Class in os.listdir(basedir+folder+'\/'):\n            for location in os.listdir(basedir+folder+'\/'+Class+'\/'):\n                df = df.append({'Class':Class,'Location':basedir+folder+'\/'+Class+'\/'+location},ignore_index=True)\n    df = df.sample(frac = 1) \n    return df","9163ac81":"df = Create_Directory_DataFrame()\nprint(df.shape)\ndf.head()","a7168e81":"count = 1\nf = plt.figure(figsize=(50,13))\nfor Class in df['Class'].unique():\n    seg = df[df['Class']==Class]\n    address =  seg.sample().iloc[0]['Location']\n    img = cv2.imread(address,0)\n    ax = f.add_subplot(2, 5,count)\n    ax = plt.imshow(img)\n    ax = plt.title(Class,fontsize= 30)\n    count = count + 1\nplt.suptitle(\"Hand Sign Images\", size = 32)\nplt.show()","205134d2":"img.shape","1bb9dbd0":"w , h= 64,64\nfinal_class = 10","1777093e":"from tqdm import tqdm\ntrain_image = []\nfor location in tqdm(df.iloc[:]['Location']):\n    img = cv2.imread(location,0)\n    img = cv2.resize(img, (w,h), interpolation = cv2.INTER_AREA)\n    img = img.reshape(w,h,1)\n    train_image.append(img)\nX = np.array(train_image)","d820ac55":"from sklearn.preprocessing import OneHotEncoder\ny = np.array(df.iloc[:]['Class'])\ny = y.reshape(y.shape[0],1)\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(y)\nprint(enc.categories_)","1c9b72d7":"y = enc.transform(y).toarray()","c5356695":"print('Data   :   '+str(X.shape))\nprint('Output :   '+str(y.shape))","02d5d988":"print(X[0].reshape(w,h))","9fc51afc":"plt.figure(figsize=(25,8))\nplt.imshow(X[66].reshape(w,h))\nplt.title(enc.inverse_transform(y[0].reshape(1,10))[0][0],size = 20)\nplt.show()","6bf52d9b":"y[0]","4a3c4095":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)","24297d5f":"print('Train data    :'+str(X_train.shape))\nprint('Test data     :'+str(X_test.shape))\nprint('Train Output  :'+str(y_train.shape))\nprint('Test Output   :'+str(y_test.shape))","213aa545":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ]\n    )\n    return block\ndef dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    return block\ndef build_model(act , final_class , w , h ):\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(w , h , 1)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(final_class, activation='sigmoid')\n    ])\n    return model","6d3df85f":"def wrap(Training_Output_Results , Opt , Act ,  history):\n    epoch  = len(history.history['loss'])\n    epochs = list(np.arange(1,epoch + 1,1))\n    Optimizer = np.repeat(Opt,epoch).tolist()\n    Activation = np.repeat(Act,epoch).tolist()\n    cumiliated_res = {}\n    cumiliated_res['Epochs']=epochs\n    cumiliated_res['Optimizer']=Optimizer\n    cumiliated_res['Activation_Function']=Activation\n    cumiliated_res['Train_Loss']=history.history['loss']\n    cumiliated_res['Train_Accuracy']=history.history['accuracy']\n    cumiliated_res['Train_Precision']=history.history['precision']\n    cumiliated_res['Train_Recall']=history.history['recall']\n    cumiliated_res['Val_Loss']=history.history['val_loss']\n    cumiliated_res['Val_Accuracy']=history.history['val_accuracy']\n    cumiliated_res['Val_Precision']=history.history['val_precision']\n    cumiliated_res['Val_Recall']=history.history['val_recall']\n    convertDictionary = pd.DataFrame(cumiliated_res)\n    Training_Output_Results = Training_Output_Results.append(convertDictionary)\n    return Training_Output_Results","cffd52e5":"Optimisers = ['RMSprop','Adam','Adadelta','Adagrad']\nActivation_function =['relu','sigmoid','softmax','tanh','softsign','selu','elu']","449f697c":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Optimizer','Activation_Function','Train_Loss','Train_Accuracy','Train_Precision','Train_Recall',                                             'Val_Loss','Val_Accuracy','Val_Precision','Val_Recall'])\ndef Optimise_verify(Training_Output_Results):\n    for opt in Optimisers:\n        model = build_model(Activation_function[0], final_class , w , h)\n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall')\n        ]  \n        model.compile(\n                optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=10, validation_split=0.3, batch_size=15,verbose=0,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , opt,Activation_function[0],history)\n        print('---------------------Round for '+opt+' Completed-----------------------------------------')\n    return Training_Output_Results\n    \n    \nTraining_Output_Results = Optimise_verify(Training_Output_Results)","98e9508f":"Training_Output_Results=Training_Output_Results.sample(frac = 1) \nprint(Training_Output_Results.shape)\nTraining_Output_Results.to_csv('Optimizer_64*64_data.csv', index = False) \nTraining_Output_Results.head()","d13a5292":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Optimizer','Activation_Function','Train_Loss','Train_Accuracy','Train_Precision','Train_Recall',\n                                              'Val_Loss','Val_Accuracy','Val_Precision','Val_Recall'])\ndef Activation_verify(Training_Output_Results):\n    for act in Activation_function:\n        model = build_model(act,final_class,w,h)\n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall')\n        ]  \n        model.compile(\n                optimizer=Optimisers[0],\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=10, validation_split=0.3, batch_size=15,verbose=0,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , Optimisers[0],act,history)\n        print('---------------------Round for '+act+' Completed-----------------------------------------')\n    return Training_Output_Results\n    \n    \nTraining_Output_Results = Activation_verify(Training_Output_Results)","ac88f6d1":"Training_Output_Results=Training_Output_Results.sample(frac = 1) \nprint(Training_Output_Results.shape)\n\nTraining_Output_Results.to_csv('Activation_64*64_data.csv', index = False)\nTraining_Output_Results.head()","d35053c1":"opt = pd.read_csv('.\/Optimizer_64*64_data.csv')\nact = pd.read_csv('.\/Activation_64*64_data.csv')","e6a9383b":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"black\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\npyo.iplot(scatterplot, filename = 'Opt_train_acc')","7a7a8411":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_loss')","5738efff":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Precision\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Precision',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_prec')","a4f017fc":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Recall\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Recall',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_recall')","2eacd324":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_acc')","2d737e05":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Precision\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Precision',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_prec')","ec5788e6":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Recall\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Recall',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_recall')","8f41c819":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_loss')","2ab6c1a3":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.area(\n    data_frame=act,\n    x=\"Epochs\",\n    y=\"Train_Accuracy\",\n    color=\"Activation_Function\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Act_train_acc')","cf18c42b":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.area(\n    data_frame=act,\n    x=\"Epochs\",\n    y=\"Train_Loss\",\n    color=\"Activation_Function\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Act_train_loss')","c3a99ab6":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.area(\n    data_frame=act,\n    x=\"Epochs\",\n    y=\"Train_Precision\",\n    color=\"Activation_Function\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Precision',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Act_train_prec')","e74f5f21":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.area(\n    data_frame=act,\n    x=\"Epochs\",\n    y=\"Train_Recall\",\n    color=\"Activation_Function\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Recall',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Act_train_rec')","0e46e6ab":"import plotly.graph_objects as go\ntab_opt = opt[opt['Epochs']==10]\nfinal_col = np.delete(tab_opt.columns[0:], [0,2])\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(final_col),\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=[tab_opt.Optimizer , tab_opt.Train_Loss,tab_opt.Train_Accuracy,tab_opt.Train_Precision,tab_opt.Train_Recall,tab_opt.Val_Loss,tab_opt.Val_Accuracy,tab_opt.Val_Precision,tab_opt.Val_Recall],\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.show()","63db7775":"import plotly.graph_objects as go\nty =opt[opt['Epochs']==10].iloc[:,3:]\nnm = ty.columns\nty = ty.values.tolist()\ndata = []\n\nfor j in range(len(nm)):\n        lt = []\n        for i in range(len(Optimisers)):\n            lt.append(ty[i][j])\n            \n        data.append(go.Bar(name = nm[j],x=Optimisers, y=lt))\nfig = go.Figure(data=data)\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","c2fdecd6":"import plotly.graph_objects as go\ntab_opt = act[act['Epochs']==10]\nfinal_col = np.delete(tab_opt.columns[0:], [0,1])\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(final_col),\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=[tab_opt.Activation_Function , tab_opt.Train_Loss,tab_opt.Train_Accuracy,tab_opt.Train_Precision,tab_opt.Train_Recall,tab_opt.Val_Loss,tab_opt.Val_Accuracy,tab_opt.Val_Precision,tab_opt.Val_Recall],\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.show()","167989f9":"import plotly.graph_objects as go\nty =act[act['Epochs']==10].iloc[:,3:]\nnm = ty.columns\nty = ty.values.tolist()\ndata = []\n\nfor j in range(len(nm)):\n        lt = []\n        for i in range(len(Activation_function)):\n            lt.append(ty[i][j])\n            \n        data.append(go.Bar(name = nm[j],x=Activation_function, y=lt))\nfig = go.Figure(data=data)\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","55f680b4":"def Plot(history , name , model):\n    model.save(name+'.h5')\n    epochs = range(1,len(history.history['loss']) + 1)\n    epochs = list(epochs)\n    fig = make_subplots(rows=2, cols=4,subplot_titles=(\"Train Loss\", \"Train Accuracy\" , \"Train Precision\",\"Train Recall\", \"Validation Loss\", \"Validation Accuracy\",\n                                                      \"Validation Precision\",\"Validation Recall\"))\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['loss']), row=1, col=1)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['accuracy']), row=1, col=2)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['precision']), row=1, col=3)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['recall']), row=1, col=4)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_loss']), row=2, col=1)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_accuracy']), row=2, col=2)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_precision']), row=2, col=3)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_recall']), row=2, col=4)\n    fig.update_layout(showlegend=False,height=1000, width=1200, title_text=name)\n    pyo.iplot(fig, filename = 'Act_train_rec')","1ecf9ede":"model = build_model('relu', final_class ,w , h)\nMETRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall')\n]  \nmodel.compile(\n                optimizer='RMSprop',\n                loss='categorical_crossentropy',\n                metrics=METRICS\n        )\nhistory = model.fit(X_train, y_train, epochs=50, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)","c77ae62d":"Plot(history , 'final_model',model)","780e4f97":"import visualkeras\nvisualkeras.layered_view(model)","1d6be673":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes=True)","a7bd4164":"from matplotlib import pyplot\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\nfilters, biases = model.layers[1].get_weights()\n# normalize filter values to 0-1 so we can visualize them\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) \/ (f_max - f_min)\n# plot first few filters\nn_filters, ix = 6, 1\nfor i in range(n_filters):\n\t# get the filter\n\tf = filters[:, :, :, i]\n\t# plot each channel separately\n\tfor j in range(3):\n\t\t# specify subplot and turn of axis\n\t\tax = pyplot.subplot(n_filters,3 , ix)\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# plot filter channel in grayscale\n\t\tpyplot.imshow(f[:, :, j])\n\t\tix += 1\n# show the figure\npyplot.show()","00b8284a":"from matplotlib import pyplot\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\nfilters, biases = model.layers[1].get_weights()\n# normalize filter values to 0-1 so we can visualize them\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) \/ (f_max - f_min)\n# plot first few filters\nn_filters, ix = 6, 1\nfor i in range(n_filters):\n\t# get the filter\n\tf = filters[:, :, :, i]\n\t# plot each channel separately\n\tfor j in range(3):\n\t\t# specify subplot and turn of axis\n\t\tax = pyplot.subplot(n_filters,3 , ix)\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# plot filter channel in grayscale\n\t\tpyplot.imshow(f[:, :, j],cmap='gray')\n\t\tix += 1\n# show the figure\npyplot.show()","a5d0db03":"plt.figure(figsize=(25,8))\nplt.imshow(X[66].reshape(w,h))\nplt.title(enc.inverse_transform(y[0].reshape(1,10))[0][0],size = 20)\nplt.show()","01fe3383":"from keras.models import Model\nfrom matplotlib.pyplot import figure\nfrom numpy import expand_dims\ndef image_transform_gray(image):\n    img = expand_dims(image, axis=0)\n    model1 = Model(inputs=model.inputs, outputs=model.layers[0].output)\n    feature_maps = model1.predict(img)\n    figure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\n    square = 4\n    ix = 1\n    for _ in range(square):\n        for _ in range(square):\n            # specify subplot and turn of axis\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # plot filter channel in grayscale\n            pyplot.imshow(feature_maps[0, :, :, ix-1],cmap='gray')\n            ix += 1\n    # show the figure\n    pyplot.show()\ndef image_transform(image):\n    img = expand_dims(image, axis=0)\n    model1 = Model(inputs=model.inputs, outputs=model.layers[0].output)\n    feature_maps = model1.predict(img)\n    figure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\n    square = 4\n    ix = 1\n    for _ in range(square):\n        for _ in range(square):\n            # specify subplot and turn of axis\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # plot filter channel in grayscale\n            pyplot.imshow(feature_maps[0, :, :, ix-1])\n            ix += 1\n    # show the figure\n    pyplot.show()","e9657c70":"image_transform(X[66])\nimage_transform_gray(X[66])","eda5ce32":"plt.figure(figsize=(25,8))\nplt.imshow(X[56].reshape(w,h))\nplt.title(enc.inverse_transform(y[0].reshape(1,10))[0][0],size = 20)\nplt.show()","4696a528":"image_transform(X[56])\nimage_transform_gray(X[56])","166773c3":"plt.figure(figsize=(25,8))\nplt.imshow(X[1566].reshape(w,h))\nplt.title(enc.inverse_transform(y[0].reshape(1,10))[0][0],size = 20)\nplt.show()","9ed5a898":"image_transform(X[1566])\nimage_transform_gray(X[1566])","08ebc979":"y_pred = model.evaluate(X_test , y_test,verbose =1)","a0c862d3":"y_pred[1]","16614782":"history.history['loss'][49]\nimport plotly.graph_objects as go\n\n\nfig = go.Figure(data=[\n    go.Bar(name = 'Accuracy',x=['Training','Validation','Real World Data'], y=[history.history['accuracy'][49] ,history.history['val_accuracy'][49],y_pred[1] ]),\n    go.Bar(name = 'Precision',x=['Training','Validation','Real World Data'], y=[history.history['precision'][49] ,history.history['val_precision'][49],y_pred[2] ]),\n    go.Bar(name = 'Loss',x=['Training','Validation','Real World Data'], y=[history.history['loss'][49] ,history.history['val_loss'][49],y_pred[0] ]),\n\n])\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.update_yaxes(type = \"log\")\npyo.iplot(fig, filename = 'Act_train_rec')","1e25bd45":"model.predict(X_test)","dc0d92af":"y_prediction = model.predict(X_test)\ndef binary_classify(y_pred):\n    for inp in y_pred:\n        maximum = 0\n        index = 0\n        for i in range(10):\n            if(maximum != max(maximum,inp[i])):\n                maximum = max(maximum,inp[i])\n                index = i\n            inp[i] = 0\n        inp[index]=1\n    return y_pred\ny_prediction  = binary_classify(y_prediction)","e5c1cf8e":"def create_result(y):\n    y_final = []\n    for i in range(y.shape[0]):\n        y_final.append(enc.inverse_transform(y[i].reshape(1,10))[0][0])\n    return y_final \ndef remove_none(y , y_pred):\n    index = []\n    for i in range(len(y)-1,0,-1):\n        if y_pred[i] == None :\n            del y[i]\n            del y_pred[i]\n        \n    return y , y_pred\ndef label_encode(y , y_pred):\n    le = preprocessing.LabelEncoder()\n    le.fit(y_pred)\n    print(le.classes_)\n    y = le.transform(y)\n    y_pred = le.transform(y_pred)\n    return y , y_pred\n\ny_class_result = create_result(y_prediction)\ny_class_desired = create_result(y_test)","79d52813":"y_label_desired , y_label_result = label_encode(y_class_desired , y_class_result) ","508089c3":"from sklearn.metrics import classification_report\ntn = []\nfor cat in enc.categories_[0].reshape(10,1):\n    tn.append(cat[0])\ntarget_names = tn\nprint(classification_report(y_label_desired, y_label_result, target_names=target_names))","33cd1216":"count = 1\nf = plt.figure(figsize=(20,24))\nfor i in range(20):\n    ind = random.sample(list(y_label_result),1)[0]\n    img = X_test[ind]\n    Class = str(y_class_desired[ind]) + '  vs  '+str(y_class_result[ind])\n    ax = f.add_subplot(5, 4,count)\n    ax = plt.imshow(img.reshape(w,h))\n    ax = plt.title(Class,fontsize= 11)\n    count = count + 1\nplt.suptitle(\"Hand Sign Images\", size = 32)\nplt.show()","8922f4a0":"RMSprop 0.064\nAdamgrad 0.0634\nagain a close call to distinguish","3931f978":"# Activation Function","62980372":"# Visualize Data","4042c994":"# Ploting","b7c5d66f":"Adam 99.9\nRMSprop 99.6\nAgain a close boundary","a7f208a1":"# Segmentation in Traing and Test Data Sets","b5fe4f84":"# Model\n\n1. To make our model more modular and easier to understand, let's define some blocks. As we're building a convolution neural network, we'll create a convolution block and a dense layer block.\n2. The following method will define the function to build our model for us. The Dropout layers are important as they \"drop out,\" hence the name, certain nodes to reduce the likelikhood of the model overfitting. We want to end the model with a Dense layer of one node, as this will be the output that determines if an X-ray shows an image of pneumonia.\nSince there are only two possible labels for the image, we will be using the binary_crossentropy loss. When we fit the model, identify the class weights. Because we are using a TPU, training will be relatively quick.\n\nFor our metrics, we want to include precision and recall as they will provide use with a more informed picture of how good our model is. Accuracy tells us what fractions are the labels are correct. Since our data is not balanced, accuracy might give a skewed sense of a good model (i.e. a model that always predicts PNEUMONIA will be 74% accurate but is not a good model).\n\nPrecision is the number of true positives (TP) over the sum of TP and false positives (FP). It shows what fraction of labeled positives are actually correct.\n\nRecall is the number of TP over the sum of TP and false negatves (FN). It shows what fraction of actual positives are correct.","f05664f9":"# Activation Analytics","3c270798":"# Data Creation","c1f30bba":"object : the model to train.      \n-> X : our training data. Can be Vector, array or matrix      \n-> Y : our training labels. Can be Vector, array or matrix       \n-> Batch_size : it can take any integer value or NULL and by default, it will\nbe set to 32. It specifies no. of samples per gradient.      \n-> Epochs : an integer and number of epochs we want to train our model for.      \n-> Verbose : specifies verbosity mode(0 = silent, 1= progress bar, 2 = one\nline per epoch).      \n-> Shuffle : whether we want to shuffle our training data before each epoch.      \n-> steps_per_epoch : it specifies the total number of steps taken before\none epoch has finished and started the next epoch. By default it values is set to NULL.","9fcb74c3":"# Test The Results","6b1791f0":"Both adam and Rmsprop miserably fail the recall test and Adagrad does best with 88.8%","04a25836":"# Final Model","9a874c32":"Optimizer : RMSprop\nActivation : Relu","dfdf7fa8":"# Samples","c5174937":"# Fetch Necessary Libraries","3713a68d":"Adam and RMSprop shows best accuracies namely 93.2 and 98.5 %","d453b9c2":"# Installing Model Visualizer","d5e9c516":"# Optimization","47fd56dd":"# Optimizer Analytics","a3bdddf7":"# Define Constraints","67b26d0c":"In validation RMS 99.4 and Adam 66.4 this creates a good margin to distinguish both","c0bb4fbe":"# Loading data from file"}}