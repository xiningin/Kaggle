{"cell_type":{"8394cde1":"code","9f191405":"code","7273d228":"code","a333a11c":"code","5f1d77f9":"code","fc12520a":"code","ea480681":"code","7c98219a":"code","fd0a880b":"code","52904eff":"code","10e5534b":"code","09006464":"code","80b2eff5":"code","3d9bdf8d":"code","552337b0":"code","79ab6b7b":"code","21c173d9":"code","a478d7ba":"code","49c6ea09":"code","54501b2e":"code","cde46270":"code","5da65a62":"code","715d9448":"code","d8fd5cd6":"code","a890ba3a":"code","db200b2e":"code","878d1ad9":"code","5cf78e4e":"code","fc8e72af":"code","c51659e5":"code","15ac0c8a":"code","20a8f464":"code","59408cb2":"code","3fd685e6":"code","d81396be":"code","5c772a8d":"code","90927622":"code","59c1a9fa":"code","b3912b89":"code","11d0d5a8":"code","c07af4cb":"code","d0693513":"code","9d681faf":"code","27095e99":"code","b86c4cf8":"code","2d54fe22":"code","01238bca":"code","4b5b8de8":"code","00a866f0":"code","c6d86ebf":"code","b655b0c0":"code","1ad35a94":"code","34b0dbbb":"code","273ec7d1":"code","6bd1e2ae":"code","af4faef5":"code","e35f610a":"code","fdd987e6":"code","e90c3261":"code","e552ac51":"code","c6ee4d7d":"code","10248848":"code","b0703fa5":"code","0984af37":"code","15aa5d13":"code","77230d27":"markdown","5f62548e":"markdown","f3e3d154":"markdown","2c3e85fb":"markdown","ef62cead":"markdown","9f1c4626":"markdown","8f32e410":"markdown","8cdca8c0":"markdown","a2301092":"markdown","291c3db6":"markdown","f6775674":"markdown","feb344f5":"markdown","ee52910f":"markdown","f8c538f1":"markdown","dab0fe5d":"markdown","657c9fd1":"markdown","146f9454":"markdown","f3f3525e":"markdown","5ebd2452":"markdown","d8b89578":"markdown","73655e80":"markdown","49d52fea":"markdown","491e0c0c":"markdown","c84c992b":"markdown","e07c4b2d":"markdown","4ae40670":"markdown","e4a09857":"markdown","11d65dc2":"markdown","eb6e62a5":"markdown","55c03872":"markdown","700ee6df":"markdown","314357c7":"markdown","426b9565":"markdown","4c236393":"markdown","2d4cebf7":"markdown"},"source":{"8394cde1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom collections import Counter\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport os\nprint(os.listdir(\"..\/input\"))","9f191405":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape, test.shape","7273d228":"train.head()","a333a11c":"train.describe()","5f1d77f9":"train.info()","fc12520a":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() ","ea480681":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","7c98219a":"plt.figure(figsize=(30,8))\nsns.heatmap(train.corr(),cmap='coolwarm',annot = True)\nplt.show()","fd0a880b":"corr = train.corr()\nplt.subplots(figsize=(13,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","52904eff":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","10e5534b":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","09006464":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));","80b2eff5":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","3d9bdf8d":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() ","552337b0":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","79ab6b7b":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() ","21c173d9":"def outlier_detect(feature, data):\n    outlier_index = []\n\n    for each in feature:\n        Q1 = np.percentile(data[each], 25)\n        Q3 = np.percentile(data[each], 75)\n        IQR = Q3 - Q1\n        min_quartile = Q1 - 1.5*IQR\n        max_quartile = Q3 + 1.5*IQR\n        outlier_list = data[(data[each] < min_quartile) | (data[each] > max_quartile)].index\n        outlier_index.extend(outlier_list)\n        \n    outlier_index = Counter(outlier_index)\n    #If there are three or more outlier data features we must delete them. (n)\n    outlier_data = list(i for i, n in outlier_index.items() if n > 3)\n    return outlier_data","a478d7ba":"outlier_data = outlier_detect(['OverallQual','GrLivArea','SalePrice'], train)\ntrain.loc[outlier_data]","49c6ea09":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","54501b2e":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","cde46270":"#missing data\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","5da65a62":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","715d9448":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","d8fd5cd6":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_features = handle_missing(all_features)","a890ba3a":"#checking for the missing data\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","db200b2e":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","878d1ad9":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","5cf78e4e":"# Normalize skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","fc8e72af":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","c51659e5":"all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","15ac0c8a":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","20a8f464":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","59408cb2":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","3fd685e6":"all_features.shape","d81396be":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]","5c772a8d":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape","90927622":"# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA","59c1a9fa":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","b3912b89":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","11d0d5a8":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\n#Decision Tree Regression\ndtreg = DecisionTreeRegressor(random_state = 100)\n\n#Random Forest Regression\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 0)","c07af4cb":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","d0693513":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","9d681faf":"score = cv_rmse(dtreg)\nprint(\"dtreg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['dtreg'] = (score.mean(), score.std())","27095e99":"score = cv_rmse(rfr)\nprint(\"rfr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rfr'] = (score.mean(), score.std())","b86c4cf8":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","2d54fe22":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","01238bca":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","4b5b8de8":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","00a866f0":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))","c6d86ebf":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, train_labels)","b655b0c0":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, train_labels)","1ad35a94":"print('Svr')\nsvr_model_full_data = svr.fit(X, train_labels)","34b0dbbb":"print('Ridge')\nridge_model_full_data = ridge.fit(X, train_labels)","273ec7d1":"print('RandomForest')\nrf_model_full_data = rf.fit(X, train_labels)","6bd1e2ae":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, train_labels)","af4faef5":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","e35f610a":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","fdd987e6":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","e90c3261":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","e552ac51":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))","c6ee4d7d":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression_New_Improvement.csv\", index=False)","10248848":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_regression2.csv\", index=False)","b0703fa5":"from sklearn.model_selection import train_test_split\nscaler = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(X, train_labels, test_size=0.3, random_state=42)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","0984af37":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","15aa5d13":"plt.figure(figsize=(8,6))\nplt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap='plasma')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","77230d27":"# Submit predictions","5f62548e":"# Remove Outliers","f3e3d154":"# Fit the models","2c3e85fb":"# Welcome to my notebook for predicting the House Price Competition\n* Here, I want to show how a data scientist would go through all the necessary steps of the data science journey","ef62cead":"The SalePrice is now normally distributed, excellent!","9f1c4626":"* Clearly by using these two components we can easily separate these two classes.\n\n* Interpreting the components\n* Unfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n\n* The components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:\n\n# Note:\n\n* Principal Component Analysis:\n* Used in exploratory data analysis (EDA)\n* Visualize genetic distance and relatedness between populations.\n\n# Method:\n\n* Eigenvalue decomposition of a data covariance (or correlation) matrix\n* Singular value decomposition of a data matrix (After mean centering \/ normalizing ) the data matrix for each attribute.\n# Output\n\n* Component scores, sometimes called factor scores (the transformed variable values)\n* loadings (the weight)\n* Data compression and information preservation\n\n* Visualization\n* Noise filtering\n* Feature extraction and engineerin","8f32e410":"great! we did take care of the missing values","8cdca8c0":"# Feature Alaysis","a2301092":"# Fix skewed features","291c3db6":"As you see the sale price value is right skewed. We need to make this normal distributed.","f6775674":"# Recreate training and test sets","feb344f5":"# Feature transformations\nLet's create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won't be able to reliably tell if log(feature) or feature^2 is a predictor of the SalePrice.","ee52910f":"# Encode categorical features","f8c538f1":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data.\n* We can apply a log(1+x) tranform to fix the skew.","dab0fe5d":"# The Goal\n* Each row in the dataset describes the characteristics of a house.\n* Our goal is to predict the SalePrice, given these features.\n* Our models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses will affect our score equally.","657c9fd1":"# Blend models and get predictions","146f9454":"# EDA Processes","f3f3525e":"# Train models\n* Get cross validation scores for each model","5ebd2452":"# Create interesting features\nlet's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house.","d8b89578":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I'll use for making the final predictions.","73655e80":"Numerically encode categorical features because most models can only handle numerical features.","49d52fea":"# Setup cross validation and define error metrics","491e0c0c":"Here's a brief version of what you'll find in the data description file.\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","c84c992b":"We can now move through each of the features above and impute the missing values for each of them.","e07c4b2d":"* What is the data trying to say to us ? We need to analyse the data. \n* Analysing data is the most important thing to understand what the data is telling us.","4ae40670":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data.","e4a09857":"# Buidling the Model","11d65dc2":"# Identify the best performing model","eb6e62a5":"# We'll be trying to predict hause price with regression models.\n\n* Let's get started!\n\n# Check out the data\n* We've been able to get some data from your neighbor for housing prices as a csv set, let's get our environment ready with the libraries we'll need and then import the data!","55c03872":"# Let's import the necessary libraries","700ee6df":"* Let's plot how SalePrice relates to some of the features in the datase","314357c7":"# Data Visualization","426b9565":"# Principal Component Analysis\n* Dimension Reduction Techniques\n\nAs we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance.","4c236393":"# Missing Values","2d4cebf7":"* Key features of the model training process in this kernel:\n* Cross Validation: Using 12-fold cross-validation\n* Models: On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n* Stacking: In addition, I trained a meta StackingCVRegressor optimized using xgboost\n* Blending: All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions."}}