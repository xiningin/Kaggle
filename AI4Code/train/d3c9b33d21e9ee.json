{"cell_type":{"243845cf":"code","e70b7add":"code","19515af8":"code","db9c939a":"code","aeb46344":"code","79542b63":"code","1679e4dd":"code","e9aa1848":"code","51b74f31":"code","4cf572e1":"code","462209e3":"code","f5857e99":"code","872770da":"code","6e422af3":"code","ba4ffe69":"code","c9b7b523":"code","f74bb096":"code","bd48d08e":"code","e88fad8d":"code","d31c8949":"code","7cf79476":"code","893c46bd":"code","1eb740ba":"code","35cbfb34":"code","e18f97e6":"code","533e740d":"code","5ad8e83f":"code","256dc13c":"code","4cabbb9d":"code","d5650b39":"code","a389491d":"code","c075ac2e":"code","1cdc44f2":"code","c2b59b4e":"code","02faa004":"code","4e550661":"code","7bc7f05f":"code","69962ebd":"code","67e23a8e":"code","66e7d379":"code","e88ad305":"code","ee7a697a":"code","e43c408c":"code","96fe9af4":"code","76d8fc7b":"code","3065c889":"code","f6435a3a":"code","f836f40e":"code","2d9ba1b1":"code","d77fbc82":"code","a7459500":"code","73fce7f5":"code","f37669c5":"code","96773392":"code","8997fdca":"code","6a6c01a1":"code","1495213e":"code","0520c93f":"code","730e9142":"code","5cefe087":"code","f910f848":"code","dbee8281":"code","1db758ea":"code","e21059c6":"code","5997dbf1":"code","314afbff":"code","8dc81c50":"code","d7104200":"code","c568c9bd":"code","44f15829":"code","03ac4135":"code","86e0cfa3":"code","ff3695ed":"code","40adee8a":"code","2ce9d93b":"code","6fce45ab":"code","566e26c2":"code","8ab32e5f":"code","54ed5684":"code","1ba142ab":"code","e3178365":"code","70b0b38d":"code","7ed6af2e":"code","a715d9af":"code","6066c6f2":"code","485a1f2a":"code","abdb335e":"code","ccdc28da":"code","1f9c471d":"code","d321c183":"code","da3b1aba":"code","a69698f5":"code","38f8f55a":"code","1567e227":"code","ed85a843":"code","e2d1897a":"code","6b2b01a4":"code","668b378f":"code","afcb357d":"code","d6e2ce0a":"code","99c1ee62":"code","ee1d10ad":"code","1b034942":"code","63371032":"code","6cdfb1e8":"code","7c81b2eb":"code","5db04200":"code","c55474fe":"code","94ba224b":"code","baf19d1c":"markdown","16f886d8":"markdown","3198e2ce":"markdown","4389e936":"markdown","38d9299b":"markdown","2767cae4":"markdown","d720e36d":"markdown","3a0f74ba":"markdown","50082920":"markdown","16e066bc":"markdown","b617ef94":"markdown"},"source":{"243845cf":"!pip install ecg-plot\nimport physionet_challenge_utility_script as pc\nimport ecg_plot\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import plot_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers.merge import concatenate\nfrom scipy import optimize\nfrom scipy.io import loadmat\nimport os\n%load_ext autoreload\n%autoreload\n%reload_ext autoreload","e70b7add":"gender, age, labels, ecg_filenames = pc.import_key_data(\"\/kaggle\/input\/\")\necg_filenames = np.asarray(ecg_filenames)","19515af8":"SNOMED_scored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_scored.csv\", sep=\";\")\nSNOMED_unscored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_unscored.csv\", sep=\";\")\ndf_labels = pc.make_undefined_class(labels,SNOMED_unscored)","db9c939a":"y , snomed_classes = pc.onehot_encode(df_labels)","aeb46344":"pc.plot_classes(snomed_classes, SNOMED_scored,y)","79542b63":"y_all_comb = pc.get_labels_for_all_combinations(y)\nprint(\"Total number of unique combinations of diagnosis: {}\".format(len(np.unique(y_all_comb))))","1679e4dd":"folds = pc.split_data(labels, y_all_comb)","e9aa1848":"pc.plot_all_folds(folds,y,snomed_classes)","51b74f31":"order_array = folds[0][0]","4cf572e1":"def shuffle_batch_generator(batch_size, gen_x,gen_y): \n    np.random.shuffle(order_array)\n    batch_features = np.zeros((batch_size,5000, 12))\n    batch_labels = np.zeros((batch_size,snomed_classes.shape[0])) #drop undef class\n    while True:\n        for i in range(batch_size):\n\n            batch_features[i] = next(gen_x)\n            batch_labels[i] = next(gen_y)\n            \n        yield batch_features, batch_labels\n\ndef generate_y_shuffle(y_train):\n    while True:\n        for i in order_array:\n            y_shuffled = y_train[i]\n            yield y_shuffled\n\n\ndef generate_X_shuffle(X_train):\n    while True:\n        for i in order_array:\n                #if filepath.endswith(\".mat\"):\n                    data, header_data = pc.load_challenge_data(X_train[i])\n                    X_train_new = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n                    X_train_new = X_train_new.reshape(5000,12)\n                    yield X_train_new","462209e3":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n    min_delta=0.0001, cooldown=0, min_lr=0\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', mode='max', verbose=1, patience=2)","f5857e99":"def thr_chall_metrics(thr, label, output_prob):\n    return -pc.compute_challenge_metric_for_opt(label, np.array(output_prob>thr))","872770da":"def load_challenge_data(filename):\n    x = loadmat(filename)\n    data = np.asarray(x['val'], dtype=np.float64)\n    new_file = filename.replace('.mat','.hea')\n    input_header_file = os.path.join(new_file)\n    with open(input_header_file,'r') as f:\n        header_data=f.readlines()\n    return data, header_data","6e422af3":"def generate_validation_data(ecg_filenames, y,test_order_array):\n    y_train_gridsearch=y[test_order_array]\n    ecg_filenames_train_gridsearch=ecg_filenames[test_order_array]\n\n    ecg_train_timeseries=[]\n    for names in ecg_filenames_train_gridsearch:\n        data, header_data = load_challenge_data(names)\n        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n        ecg_train_timeseries.append(data)\n    X_train_gridsearch = np.asarray(ecg_train_timeseries)\n\n    X_train_gridsearch = X_train_gridsearch.reshape(ecg_filenames_train_gridsearch.shape[0],5000,12)\n\n    return X_train_gridsearch, y_train_gridsearch","ba4ffe69":"def compute_modified_confusion_matrix(labels, outputs):\n    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n    # are the labels and the columns are the outputs.\n    num_recordings, num_classes = np.shape(labels)\n    A = np.zeros((num_classes, num_classes))\n\n    # Iterate over all of the recordings.\n    for i in range(num_recordings):\n        # Calculate the number of positive labels and\/or outputs.\n        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n        # Iterate over all of the classes.\n        for j in range(num_classes):\n            # Assign full and\/or partial credit for each positive class.\n            if labels[i, j]:\n                for k in range(num_classes):\n                    if outputs[i, k]:\n                        A[j, k] += 1.0\/normalization\n\n    return A","c9b7b523":"def plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, val_fold, threshold, snomedclasses):\n    df_cm = pd.DataFrame(compute_modified_confusion_matrix(generate_validation_data(ecg_filenames,y,val_fold)[1], (y_pred>threshold)*1), columns=snomedclasses, index = snomedclasses)\n    df_cm = df_cm.fillna(0)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    df_norm_col=(df_cm-df_cm.mean())\/df_cm.std()\n    plt.figure(figsize = (36,14))\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_norm_col, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt=\".2f\",cbar=False)# font size","f74bb096":"ann_model = Sequential()\nann_model.add(Dense(50, activation='relu', input_shape=(5000,12)))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(GlobalAveragePooling1D())\nann_model.add(Dense(27, activation='sigmoid'))","bd48d08e":" ann_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","e88fad8d":"ann_model.summary()","d31c8949":"plot_model(ann_model)","7cf79476":"batchsize = 10\nann_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), callbacks=[reduce_lr,early_stop])","893c46bd":"plt.plot(ann_model.history.history['accuracy'])\nplt.plot(ann_model.history.history['Precision'])\nplt.plot(ann_model.history.history['val_accuracy'])\nplt.plot(ann_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","1eb740ba":"plt.plot(ann_model.history.history['loss'])\nplt.plot(ann_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","35cbfb34":"plt.plot(ann_model.history.history['Recall'])\nplt.plot(ann_model.history.history['Precision'])\nplt.plot(ann_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","e18f97e6":"y_pred = ann_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","533e740d":"init_thresholds = np.arange(0,1,0.05)","5ad8e83f":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","256dc13c":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","4cabbb9d":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_ann.png\", dpi=100)","d5650b39":"lenet_5_model=Sequential()\n\nlenet_5_model.add(Conv1D(filters=6, kernel_size=3, padding='same', input_shape=(5000,12)))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(Activation('relu'))\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(Conv1D(filters=16, strides=1, kernel_size=5))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(Activation('relu'))\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(GlobalAveragePooling1D())\n\nlenet_5_model.add(Dense(64, activation='relu'))\n\nlenet_5_model.add(Dense(32, activation='relu'))\n\nlenet_5_model.add(Dense(27, activation = 'sigmoid'))","a389491d":" lenet_5_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","c075ac2e":"lenet_5_model.summary()","1cdc44f2":"plot_model(lenet_5_model)","c2b59b4e":"batchsize = 10\nlenet_5_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), callbacks=[reduce_lr,early_stop])","02faa004":"plt.plot(lenet_5_model.history.history['accuracy'])\nplt.plot(lenet_5_model.history.history['Precision'])\nplt.plot(lenet_5_model.history.history['val_accuracy'])\nplt.plot(lenet_5_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","4e550661":"plt.plot(lenet_5_model.history.history['loss'])\nplt.plot(lenet_5_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","7bc7f05f":"plt.plot(lenet_5_model.history.history['Recall'])\nplt.plot(lenet_5_model.history.history['Precision'])\nplt.plot(lenet_5_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","69962ebd":"y_pred = lenet_5_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","67e23a8e":"init_thresholds = np.arange(0,1,0.05)","66e7d379":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","e88ad305":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","ee7a697a":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_lenet5.png\", dpi=100)","e43c408c":"alexNet_model=Sequential()\n\nalexNet_model.add(Conv1D(filters=96, kernel_size=11, strides=4, input_shape=(5000,12)))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(Activation('relu'))\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=256, kernel_size=5, padding='same'))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(Activation('relu'))\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=384, padding='same', kernel_size=3))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(Activation('relu'))\nalexNet_model.add(Conv1D(filters=384, kernel_size=3))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(Activation('relu'))\nalexNet_model.add(Conv1D(filters=256, kernel_size=3))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(Activation('relu'))\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(GlobalAveragePooling1D())\nalexNet_model.add(Dense(128, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(128, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(27, activation='sigmoid'))","96fe9af4":" alexNet_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","76d8fc7b":"alexNet_model.summary()","3065c889":"plot_model(alexNet_model)","f6435a3a":"batchsize = 10\nalexNet_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), callbacks=[reduce_lr,early_stop])","f836f40e":"plt.plot(alexNet_model.history.history['accuracy'])\nplt.plot(alexNet_model.history.history['Precision'])\nplt.plot(alexNet_model.history.history['val_accuracy'])\nplt.plot(alexNet_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","2d9ba1b1":"plt.plot(alexNet_model.history.history['loss'])\nplt.plot(alexNet_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","d77fbc82":"plt.plot(alexNet_model.history.history['Recall'])\nplt.plot(alexNet_model.history.history['Precision'])\nplt.plot(alexNet_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","a7459500":"y_pred = alexNet_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","73fce7f5":"init_thresholds = np.arange(0,1,0.05)","f37669c5":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","96773392":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","8997fdca":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_alexnet.png\", dpi=100)","6a6c01a1":"vgg_16_model=Sequential()\n\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same',  input_shape=(5000,12)))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Activation('relu'))\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(GlobalAveragePooling1D())\nvgg_16_model.add(Dense(256, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(128, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(27, activation='sigmoid'))","1495213e":" vgg_16_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","0520c93f":"vgg_16_model.summary()","730e9142":"plot_model(vgg_16_model)","5cefe087":"batchsize = 10\nvgg_16_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]))","f910f848":"plt.plot(vgg_16_model.history.history['accuracy'])\nplt.plot(vgg_16_model.history.history['Precision'])\nplt.plot(vgg_16_model.history.history['val_accuracy'])\nplt.plot(vgg_16_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","dbee8281":"plt.plot(vgg_16_model.history.history['loss'])\nplt.plot(vgg_16_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","1db758ea":"plt.plot(vgg_16_model.history.history['Recall'])\nplt.plot(vgg_16_model.history.history['Precision'])\nplt.plot(vgg_16_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","e21059c6":"y_pred = vgg_16_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","5997dbf1":"init_thresholds = np.arange(0,1,0.05)","314afbff":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","8dc81c50":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","d7104200":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_vgg16.png\", dpi=100)","c568c9bd":"def identity_block(X, f, filters):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n    \n    X = Conv1D(filters = F1, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(filters = F2, kernel_size = f, activation='relu', strides = 1, padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(filters = F3, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n\ndef convolutional_block(X, f, filters, s = 2):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n\n    X = Conv1D(F1, 1, activation='relu', strides = s)(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(F2, f, activation='relu', strides = 1,padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(F3, 1, strides = 1)(X)\n    X = BatchNormalization()(X)\n\n    X_shortcut = Conv1D(F3, 1, strides = s)(X_shortcut)\n    X_shortcut = BatchNormalization()(X_shortcut)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n\ndef ResNet50(input_shape):\n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding1D(3)(X_input)\n    \n    X = Conv1D(64, 7, strides = 2)(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n    X = identity_block(X, 3, [64, 64, 256])\n    X = identity_block(X, 3, [64, 64, 256])\n\n    X = convolutional_block(X, f = 3, filters = [128,128,512], s = 2)\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n\n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n    X = identity_block(X, 3, [512, 512, 2048])\n    X = identity_block(X, 3, [512, 512, 2048])\n\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n    \n    X = GlobalAveragePooling1D()(X)\n    X = Dense(27,activation='sigmoid')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","44f15829":"resNet50_model = ResNet50(input_shape = (5000,12))","03ac4135":" resNet50_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","86e0cfa3":"resNet50_model.summary()","ff3695ed":"plot_model(resNet50_model)","40adee8a":"batchsize = 10\nresNet50_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]))","2ce9d93b":"plt.plot(resNet50_model.history.history['accuracy'])\nplt.plot(resNet50_model.history.history['Precision'])\nplt.plot(resNet50_model.history.history['val_accuracy'])\nplt.plot(resNet50_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","6fce45ab":"plt.plot(resNet50_model.history.history['loss'])\nplt.plot(resNet50_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","566e26c2":"plt.plot(resNet50_model.history.history['Recall'])\nplt.plot(resNet50_model.history.history['Precision'])\nplt.plot(resNet50_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","8ab32e5f":"y_pred = resNet50_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","54ed5684":"init_thresholds = np.arange(0,1,0.05)","1ba142ab":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","e3178365":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","70b0b38d":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_resnet50.png\", dpi=100)","7ed6af2e":"def inception_block(prev_layer):\n    \n    conv1=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv1=BatchNormalization()(conv1)\n    conv1=Activation('relu')(conv1)\n    \n    conv3=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv3=BatchNormalization()(conv3)\n    conv3=Activation('relu')(conv3)\n    conv3=Conv1D(filters = 64, kernel_size = 3, padding = 'same')(conv3)\n    conv3=BatchNormalization()(conv3)\n    conv3=Activation('relu')(conv3)\n    \n    conv5=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv5=BatchNormalization()(conv5)\n    conv5=Activation('relu')(conv5)\n    conv5=Conv1D(filters = 64, kernel_size = 5, padding = 'same')(conv5)\n    conv5=BatchNormalization()(conv5)\n    conv5=Activation('relu')(conv5)\n    \n    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n    convmax=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(pool)\n    convmax=BatchNormalization()(convmax)\n    convmax=Activation('relu')(convmax)\n    \n    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n    \n    return layer_out\n\ndef inception_model(input_shape):\n    X_input=Input(input_shape)\n    \n    X = ZeroPadding1D(3)(X_input)\n    \n    X = Conv1D(filters = 64, kernel_size = 7, padding = 'same')(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n    \n    X = Conv1D(filters = 64, kernel_size = 1, padding = 'same')(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    \n    X = inception_block(X)\n    X = inception_block(X)\n    \n    X = MaxPool1D(pool_size=7, strides=2, padding='same')(X)\n    \n    X = GlobalAveragePooling1D()(X)\n    X = Dense(27,activation='sigmoid')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='Inception')\n    \n    return model","a715d9af":"inception_model = inception_model(input_shape = (5000,12))","6066c6f2":" inception_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","485a1f2a":"inception_model.summary()","abdb335e":"plot_model(inception_model)","ccdc28da":"batchsize = 10\ninception_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]))","1f9c471d":"plt.plot(inception_model.history.history['accuracy'])\nplt.plot(inception_model.history.history['Precision'])\nplt.plot(inception_model.history.history['val_accuracy'])\nplt.plot(inception_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","d321c183":"plt.plot(inception_model.history.history['loss'])\nplt.plot(inception_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","da3b1aba":"plt.plot(inception_model.history.history['Recall'])\nplt.plot(inception_model.history.history['Precision'])\nplt.plot(inception_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","a69698f5":"y_pred = resNet50_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","38f8f55a":"init_thresholds = np.arange(0,1,0.05)","1567e227":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","ed85a843":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","e2d1897a":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_inception.png\", dpi=100)","6b2b01a4":"lstm_model = Sequential()\nlstm_model.add(LSTM(64, input_shape=(5000,12), return_sequences=True))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(32, activation = 'relu'))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Dense(27, activation = 'sigmoid'))","668b378f":" lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","afcb357d":"lstm_model.summary()","d6e2ce0a":"plot_model(lstm_model)","99c1ee62":"batchsize = 10\nlstm_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]))","ee1d10ad":"plt.plot(lstm_model.history.history['accuracy'])\nplt.plot(lstm_model.history.history['Precision'])\nplt.plot(lstm_model.history.history['val_accuracy'])\nplt.plot(lstm_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","1b034942":"plt.plot(lstm_model.history.history['loss'])\nplt.plot(lstm_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","63371032":"plt.plot(lstm_model.history.history['Recall'])\nplt.plot(lstm_model.history.history['Precision'])\nplt.plot(lstm_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","6cdfb1e8":"y_pred = lstm_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","7c81b2eb":"init_thresholds = np.arange(0,1,0.05)","5db04200":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","c55474fe":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","94ba224b":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_lstm.png\", dpi=100)","baf19d1c":"# Results","16f886d8":"# RNN","3198e2ce":"# CNN","4389e936":"# LSTM","38d9299b":"# Inception","2767cae4":"# Lenet-5","d720e36d":"# Simple ANN","3a0f74ba":"# ResNet50","50082920":"# AlexNet","16e066bc":"# VGG-16","b617ef94":"\"Accuracy\" and \"Precision\" are overlapping in the graph above."}}