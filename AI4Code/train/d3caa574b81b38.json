{"cell_type":{"67b87350":"code","df44ee20":"code","ad0ccd24":"code","ca2b155e":"code","c92d7174":"code","1981168f":"code","9b81c1fe":"code","bf05b714":"code","2ed1ed17":"code","ac0406ef":"code","61342f15":"code","ceff77ac":"code","57b1a98f":"code","695fa42b":"code","05910a4c":"code","c7dba98c":"code","25c2844d":"code","9cd54350":"code","ca4ecf47":"markdown"},"source":{"67b87350":"!pip install torchsummary","df44ee20":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom numba import njit\nfrom multiprocessing import Pool\nfrom sklearn.metrics import r2_score\n\nimport torch\nimport wandb\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchsummary import summary\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\nimport matplotlib.pyplot as plt\nimport random","ad0ccd24":"train_targets = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntrain_targets['row_id'] = train_targets['stock_id'].astype(str) + '-' + train_targets['time_id'].astype(str)\ntrain_targets = train_targets[['row_id','target']].set_index(\"row_id\")\ntrain_files = glob(\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*\")","ca2b155e":"@njit\ndef fill_array(book_data, filled_data):\n    filled_data[0] = book_data[0]\n    last_read_idx = 0\n    for row_idx in range(1, 600):\n        if int(book_data[last_read_idx + 1][1]) == row_idx:\n            last_read_idx += 1\n        filled_data[row_idx] = book_data[last_read_idx]\n        filled_data[row_idx][1] = row_idx","c92d7174":"@njit\ndef process_groups(dataset, stock_id):\n    ret_lis = []\n    last_split_pos = 0\n    filled_data = np.zeros((600, dataset.shape[1]), dtype=np.float32)\n    for split_pos in np.nonzero(np.diff(dataset[:,0]))[0]:\n        data_split = dataset[last_split_pos:split_pos]\n        fill_array(data_split, filled_data)\n        ret_lis.append((dataset[last_split_pos][0], filled_data[:, 2:].copy()))\n        last_split_pos = split_pos\n    data_split = dataset[last_split_pos:]\n    fill_array(data_split, filled_data)\n    ret_lis.append((dataset[last_split_pos][0], filled_data[:, 2:].copy()))\n    return ret_lis","1981168f":"def process_single_stock(file_path):\n    book = pd.read_parquet(file_path, engine=\"pyarrow\").sort_values([\"time_id\", \"seconds_in_bucket\"]).to_numpy(dtype=np.float32)\n    grouped_data_list = [(data, train_targets.loc[f\"{file_path.split('=')[1]}-{int(time_id)}\", \"target\"]) for time_id, data in process_groups(book, int(file_path.split('=')[1]))]\n    return grouped_data_list","9b81c1fe":"epochs = 40\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsplit_size = 0.1\nlr = 3e-3\nlr_gamma = 0.9\nbetas = (0.5, 0.5)","bf05b714":"train_files_partitioned = []\ntrain_files_partitioned.append(train_files[:len(train_files)\/\/4])\ntrain_files_partitioned.append(train_files[len(train_files)\/\/4:len(train_files)\/\/2])\ntrain_files_partitioned.append(train_files[len(train_files)\/\/2:3 * len(train_files)\/\/4])\ntrain_files_partitioned.append(train_files[3 * len(train_files)\/\/4:])\nval_size = int(split_size * len(train_files_reduced))\ntrain_set = train_files_reduced[val_size:]\nvalid_set = train_files_reduced[:val_size]\nprint(\"Number of stocks used for training: \", len(train_set))\nprint(\"Number of stocks used for validation: \", len(valid_set))","2ed1ed17":"stored_list = []\nfor stock_file in train_set:\n    stored_list += process_single_stock(stock_file)","ac0406ef":"random.shuffle(stored_list)\ntrainX, trainy = zip(* stored_list)\ntrainy = np.array(trainy, dtype=np.float32).reshape(-1, 1)\ntrainX = np.array(trainX, dtype=np.float32)","61342f15":"def rmspe(y_true, y_pred):\n    return torch.sqrt(100 * torch.mean(((y_true - y_pred) \/ y_true) ** 2))","ceff77ac":"class VolatilityCNN(nn.Module):\n    def __init__(self):\n        super(VolatilityCNN, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv1d(8, 11, 3, stride=5),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Conv1d(11, 13, 3, stride=5),\n            nn.LeakyReLU(negative_slope=0.1)\n        )\n        self.pooling = nn.MaxPool1d(5)\n        self.ff = nn.Sequential(\n            nn.Flatten(),\n#             nn.Linear(52, 16),\n#             nn.LeakyReLU(negative_slope=0.1),\n            nn.Linear(52, 1)\n        )\n    \n    def forward(self, x):\n        return self.ff(self.pooling(self.block1(x)))","57b1a98f":"model = VolatilityCNN().to(device)\nsummary(model, (8, 600))\n\n# criterion = nn.MSELoss()\ncriterion = rmspe\noptimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_gamma)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=3e-6, factor=0.1)","695fa42b":"input_normalizer = StandardScaler()\noutput_normalizer = StandardScaler()","05910a4c":"# input_normalizer.fit(trainX.reshape(-1, 8))\ntrainX = input_normalizer.fit_transform((trainX.reshape(-1, 8))).reshape(*trainX.shape)\ntrainy = output_normalizer.fit_transform(trainy)\ntrainX = torch.tensor(trainX).permute(0, 2, 1).to(device)\ntrainy = torch.tensor(trainy).to(device)","c7dba98c":"for epoch in range(epochs):\n    model.train()\n    train_running_loss = 0\n    train_r2 = 0\n#     for train_stock_file in stored_list:\n    optimizer.zero_grad()\n#         trainX, trainy = zip(*[(input_normalizer.fit_transform(X), y) for X, y in process_single_stock(train_stock_file)])\n#     trainX , trainy = train_stock_file[0], train_stock_file[1]\n#         print(trainy)\n#     trainX = torch.tensor(trainX).unsqueeze(0).permute(0, 2, 1).to(device)\n#     trainy = torch.tensor(trainy).to(device).type(torch.float32)\n    output = model(trainX)\n\n    scaler_c1 = torch.tensor(output_normalizer.scale_).to(device)\n    scaler_c2 = torch.tensor(output_normalizer.mean_).to(device)\n    train_loss = criterion(trainy, output)\n    train_loss.backward()\n    optimizer.step()\n    print(train_loss.item())\n\n#     train_running_loss += loss.item()\n#     train_r2 += r2_score(trainy.cpu(), output.detach().cpu())\n#     random.shuffle(train_set)\n    \n#     model.eval()\n#     val_running_loss = 0\n#     val_r2 = 0\n#     for val_stock_file in valid_set:\n#         valX, valy = zip(*process_single_stock(val_stock_file))\n#         valX = torch.tensor(np.array(valX)).permute(0, 2, 1).to(device)\n#         valy = torch.tensor(valy).reshape(-1, 1).to(device)\n#         output = model(valX)\n        \n#         scaler_c1 = torch.tensor(output_normalizer.scale_).to(device)\n#         scaler_c2 = torch.tensor(output_normalizer.mean_).to(device)\n#         #  * scaler_c1 + scaler_c2\n#         val_loss = criterion(valy, output)\n#         val_running_loss += val_loss.item()\n#         val_r2 += r2_score(valy.cpu(), output.detach().cpu())\n    scheduler.step()\n    \n#     print(f\"Iteration {epoch}, Train RMSPE: {train_loss.item()}, Val RMSPE: {val_running_loss \/ len(valid_set)}, Train R2: {r2_score(trainy.cpu(), output.detach().cpu())}, Val R2: {val_r2 \/ len(valid_set)}\")","25c2844d":"# def process_stock(test_queries):\n#     features_set = process_single_stock(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id=\" + str(test_queries[\"stock_id\"][0]))\n#     features_dset = pd.DataFrame(features_set, columns=feature_columns)\n#     features_dset.insert(loc=features_dset.shape[1] - 3, column=\"stock_target_mean\", value=features_dset['stock_id'].map(feature_set.stocks_target_mean_val).values)\n#     testing_data = test_queries.merge(features_dset, how=\"left\", on=[\"time_id\", \"stock_id\"]).fillna(method=\"ffill\")\n#     testing_data_cleaned = testing_data.drop(columns=[\"time_id\", \"stock_id\"])\n    \n#     with torch.no_grad():\n#         X = torch.tensor(testing_data_cleaned.iloc[:, 1:].to_numpy(dtype=np.float32), dtype=torch.float32)\n#         model_out = model(X)\n#         model_out_scaled = feature_set.inverse_scale_transform(model_out).cpu().numpy().reshape(-1, )\n#     test_queries[\"target\"] = model_out_scaled\n#     test_queries_cleaned = test_queries.drop(columns=[\"time_id\", \"stock_id\"])\n#     return test_queries_cleaned","9cd54350":"testing_file = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\ntesting_file = testing_file.groupby(\"stock_id\").apply(process_stock)\ntesting_file = testing_file.fillna(0.001)\ntesting_file.to_csv(\"submission.csv\", index=False)","ca4ecf47":"## Submission"}}