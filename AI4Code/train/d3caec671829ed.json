{"cell_type":{"28ee59be":"code","954bf53d":"code","46d06a17":"code","cb1d7d5e":"code","077d3471":"code","a33efc07":"code","0d6e9d26":"code","10c51e22":"code","adfbced2":"code","72589c47":"code","0a715f22":"code","6d3115aa":"code","968f831f":"code","96f2ec7b":"code","3c013e07":"code","b462a2f8":"code","43dba48b":"code","97594170":"code","25918b14":"code","3a2eda36":"code","2652c45d":"code","61cb349e":"markdown","3048f23d":"markdown","3cb81e5a":"markdown","97136754":"markdown","ed54f224":"markdown","419a7dc5":"markdown","124bce8c":"markdown","49d890c5":"markdown"},"source":{"28ee59be":"# @title Licensed under the MIT License\n\n# Copyright (c) 2021 Katherine Crowson\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.","954bf53d":"import torch\n# Check the GPU status\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n!nvidia-smi","46d06a17":"diffusion_model = \"256x256_diffusion_uncond\"","cb1d7d5e":"if diffusion_model == '256x256_diffusion_uncond':\n    !wget --continue 'https:\/\/openaipublic.blob.core.windows.net\/diffusion\/jul-2021\/256x256_diffusion_uncond.pt'\nelif diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n    !wget --continue 'https:\/\/the-eye.eu\/public\/AI\/models\/512x512_diffusion_unconditional_ImageNet\/512x512_diffusion_uncond_finetune_008100.pt'\nmodel_path = \".\/\"","077d3471":"!git clone https:\/\/github.com\/openai\/CLIP\n!git clone https:\/\/github.com\/crowsonkb\/guided-diffusion\n!pip install -e .\/CLIP\n!pip install -e .\/guided-diffusion\n!pip install lpips datetime\n!pip install timm","a33efc07":"import gc\nimport io\nimport math\nimport sys\nfrom IPython import display\nimport lpips\nfrom PIL import Image, ImageOps\nimport requests\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom tqdm.notebook import tqdm\nsys.path.append('.\/CLIP')\nsys.path.append('.\/guided-diffusion')\nimport clip\nfrom guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random","0d6e9d26":"# https:\/\/gist.github.com\/adefossez\/0646dbe9ed4005480a2407c62aac8869\n\ndef interp(t):\n    return 3 * t**2 - 2 * t ** 3\n\ndef perlin(width, height, scale=10, device=None):\n    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n    wx = 1 - interp(xs)\n    wy = 1 - interp(ys)\n    dots = 0\n    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n\ndef perlin_ms(octaves, width, height, grayscale, device=device):\n    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n    for i in range(1 if grayscale else 3):\n        scale = 2 ** len(octaves)\n        oct_width = width\n        oct_height = height\n        for oct in octaves:\n            p = perlin(oct_width, oct_height, scale, device)\n            out_array[i] += p * oct\n            scale \/\/= 2\n            oct_width *= 2\n            oct_height *= 2\n    return torch.cat(out_array)\n\ndef create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n    out = perlin_ms(octaves, height, width, grayscale)\n    if grayscale:\n        out = TF.resize(size=(side_x, side_y)[::-1], img=out.unsqueeze(0))\n        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n    else:\n        out = out.reshape(-1, 3, out.shape[0]\/\/3, out.shape[1])\n        out = TF.resize(size=(side_x, side_y)[::-1], img=out)\n        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n\n    out = ImageOps.autocontrast(out)\n    return out","10c51e22":"def fetch(url_or_path):\n    if str(url_or_path).startswith('http:\/\/') or str(url_or_path).startswith('https:\/\/'):\n        r = requests.get(url_or_path)\n        r.raise_for_status()\n        fd = io.BytesIO()\n        fd.write(r.content)\n        fd.seek(0)\n        return fd\n    return open(url_or_path, 'rb')\n\n\ndef parse_prompt(prompt):\n    if prompt.startswith('http:\/\/') or prompt.startswith('https:\/\/'):\n        vals = prompt.rsplit(':', 2)\n        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n    else:\n        vals = prompt.rsplit(':', 1)\n    vals = vals + ['', '1'][len(vals):]\n    return vals[0], float(vals[1])\n\ndef sinc(x):\n    return torch.where(x != 0, torch.sin(math.pi * x) \/ (math.pi * x), x.new_ones([]))\n\ndef lanczos(x, a):\n    cond = torch.logical_and(-a < x, x < a)\n    out = torch.where(cond, sinc(x) * sinc(x\/a), x.new_zeros([]))\n    return out \/ out.sum()\n\ndef ramp(ratio, width):\n    n = math.ceil(width \/ ratio + 1)\n    out = torch.empty([n])\n    cur = 0\n    for i in range(out.shape[0]):\n        out[i] = cur\n        cur += ratio\n    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n\ndef resample(input, size, align_corners=True):\n    n, c, h, w = input.shape\n    dh, dw = size\n\n    input = input.reshape([n * c, 1, h, w])\n\n    if dh < h:\n        kernel_h = lanczos(ramp(dh \/ h, 2), 2).to(input.device, input.dtype)\n        pad_h = (kernel_h.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n        input = F.conv2d(input, kernel_h[None, None, :, None])\n\n    if dw < w:\n        kernel_w = lanczos(ramp(dw \/ w, 2), 2).to(input.device, input.dtype)\n        pad_w = (kernel_w.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n        input = F.conv2d(input, kernel_w[None, None, None, :])\n\n    input = input.reshape([n, c, h, w])\n    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n\nclass MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cutn, skip_augs=False):\n        super().__init__()\n        self.cut_size = cut_size\n        self.cutn = cutn\n        self.skip_augs = skip_augs\n        self.augs = T.Compose([\n            T.RandomHorizontalFlip(p=0.5),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomAffine(degrees=15, translate=(0.1, 0.1), interpolation=TF.InterpolationMode.BILINEAR),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomGrayscale(p=0.35),\n        ])\n\n    def forward(self, input):\n        input = T.Pad(input.shape[2]\/\/4, fill=0)(input)\n        sideY, sideX = input.shape[2:4]\n        max_size = min(sideX, sideY)\n\n        cutouts = []\n        for ch in range(cutn):\n            if ch > cutn - cutn\/\/4:\n                cutout = input.clone()\n            else:\n                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size\/max_size), 1.))\n                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n                offsety = torch.randint(0, abs(sideY - size + 1), ())\n                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n\n            if not self.skip_augs:\n                cutout = self.augs(cutout)\n            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n            del cutout\n\n        cutouts = torch.cat(cutouts, dim=0)\n        return cutouts\n\n\ndef spherical_dist_loss(x, y):\n    x = F.normalize(x, dim=-1)\n    y = F.normalize(y, dim=-1)\n    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n\n\ndef tv_loss(input):\n    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n\n\ndef range_loss(input):\n    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n\ndef unitwise_norm(x, norm_type=2.0):\n    if x.ndim <= 1:\n        return x.norm(norm_type)\n    else:\n        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel\/weight tensor\n        # might need special cases for other weights (possibly MHA) where this may not be true\n        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n\ndef adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    for p in parameters:\n        if p.grad is None:\n            continue\n        p_data = p.detach()\n        g_data = p.grad.detach()\n        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n        clipped_grad = g_data * (max_norm \/ grad_norm.clamp(min=1e-6))\n        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n        p.grad.detach().copy_(new_grads)","adfbced2":"save_dir = \".\/\"\n\ndef regen_perlin():\n    if perlin_mode == 'color':\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n    elif perlin_mode == 'gray':\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n    else:\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n\n    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n    del init2\n    return init.expand(batch_size, -1, -1, -1)\n\ndef do_run():\n    loss_values = []\n \n    if seed is not None:\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n \n    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs)\n    target_embeds, weights = [], []\n \n    for prompt in text_prompts:\n        txt, weight = parse_prompt(prompt)\n        txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n        target_embeds.append(txt)\n        weights.append(weight)\n \n    for prompt in image_prompts:\n        path, weight = parse_prompt(prompt)\n        img = Image.open(fetch(path)).convert('RGB')\n        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n        batch = make_cutouts(TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n        embed = clip_model.encode_image(normalize(batch)).float()\n        target_embeds.append(embed)\n        weights.extend([weight \/ cutn] * cutn)\n \n    target_embeds = torch.cat(target_embeds)\n    weights = torch.tensor(weights, device=device)\n    if weights.sum().abs() < 1e-3:\n        raise RuntimeError('The weights must not sum to 0.')\n    weights \/= weights.sum().abs()\n \n    init = None\n    if init_image is not None:\n        init = Image.open(fetch(init_image)).convert('RGB')\n        init = init.resize((side_x, side_y), Image.LANCZOS)\n        init = TF.to_tensor(init).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device).mul(2).sub(1)\n \n    cur_t = None\n    def cond_fn(x, t, y=None):\n        with torch.enable_grad():\n            x = x.detach().requires_grad_()\n            n = x.shape[0]\n            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n            x_in_grad = torch.zeros_like(x_in)\n            *_, h, w = x_in.shape\n            xi = torch.cat((x_in[..., :h], x_in, x_in[..., -h:]), dim=-1)\n            for i in range(cutn_batches):\n                clip_in = normalize(make_cutouts(xi.add(1).div(2)))\n                image_embeds = clip_model.encode_image(clip_in).float()\n                dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n                dists = dists.view([cutn, n, -1])\n                losses = dists.mul(weights).sum(2).mean(0)\n                loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n                x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] \/ cutn_batches\n            tv_losses = tv_loss(xi)\n            range_losses = range_loss(out['pred_xstart'])\n            sat_losses = torch.abs(xi - xi.clamp(min=-1,max=1)).mean()\n            loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n            if init is not None and init_scale:\n                init_losses = lpips_model(x_in, init)\n                loss = loss + init_losses.sum() * init_scale\n            x_in_grad += torch.autograd.grad(loss, x_in)[0]\n            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n   \n        adaptive_clip_grad([x])\n        magnitude = grad.square().mean().sqrt()\n        return grad * magnitude.clamp(max=clamp_max) \/ magnitude\n \n    if model_config['timestep_respacing'].startswith('ddim'):\n        sample_fn = diffusion.ddim_sample_loop_progressive\n    else:\n        sample_fn = diffusion.p_sample_loop_progressive\n \n    original_target_embeds = target_embeds.clone()\n    \n    \n    class NewModel(object):\n        def __init__(self, x):\n            self.x = x\n        \n        def __getattr__(self, x):\n            return getattr(self.x, x)\n        \n        def __call__(self, x, *args, **kwargs):\n            i = random.randrange(x.shape[-1])\n            return self.x(x.roll(i, -1), *args, **kwargs).roll(-i, -1)\n    \n    \n    model_ = NewModel(model)\n    for i in range(n_batches):\n        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n\n        if fuzzy_prompt:\n            target_embeds = original_target_embeds.clone() +  torch.randn_like(target_embeds).cuda() * rand_mag\n\n        if perlin_init:\n            init = regen_perlin()\n \n        if model_config['timestep_respacing'].startswith('ddim'):\n            samples = sample_fn(\n                model_,\n                (batch_size, 3, side_y, side_x),\n                clip_denoised=clip_denoised,\n                model_kwargs={},\n                cond_fn=cond_fn,\n                progress=True,\n                skip_timesteps=skip_timesteps,\n                init_image=init,\n                randomize_class=randomize_class,\n                eta=eta,\n                # cond_fn_with_grad=True,\n            )\n        else:\n            samples = sample_fn(\n                model_,\n                (batch_size, 3, side_y, side_x),\n                clip_denoised=clip_denoised,\n                model_kwargs={},\n                cond_fn=cond_fn,\n                progress=True,\n                skip_timesteps=skip_timesteps,\n                init_image=init,\n                randomize_class=randomize_class,\n                # cond_fn_with_grad=True,\n            )\n\n        for j, sample in enumerate(samples):\n            display.clear_output(wait=True)\n            cur_t -= 1\n            if j % display_rate == 0 or cur_t == -1:\n                for k, image in enumerate(sample['pred_xstart']):\n                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n                    current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n                    filename = f'progress_batch{i:05}_iteration{j:05}_output{k:05}_{current_time}.png'\n                    image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n                    image.save(save_dir + filename)\n                    image.save(\"out.png\")\n                    display.display(display.Image(save_dir + filename))\n#                     if google_drive and cur_t == -1:\n#                         image.save('\/content\/drive\/MyDrive\/' + filename)\n \n        plt.plot(np.array(loss_values), 'r')","72589c47":"# timestep_respacing = 'ddim50' # Modify this value to decrease the number of timesteps.\ntimestep_respacing = '50'\ndiffusion_steps = 1000\n\nmodel_config = model_and_diffusion_defaults()\nif diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': diffusion_steps,\n        'rescale_timesteps': True,\n        'timestep_respacing': timestep_respacing,\n        'image_size': 512,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n    })\nelif diffusion_model == '256x256_diffusion_uncond':\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': diffusion_steps,\n        'rescale_timesteps': True,\n        'timestep_respacing': timestep_respacing,\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n    })\nside_x, side_y = 512, 256  # hack\n\nmodel, diffusion = create_model_and_diffusion(**model_config)\nmodel.load_state_dict(torch.load(f'{model_path}{diffusion_model}.pt', map_location='cpu'))\nmodel.requires_grad_(False).eval().to(device)\nfor name, param in model.named_parameters():\n    if 'qkv' in name or 'norm' in name or 'proj' in name:\n        param.requires_grad_()\nif model_config['use_fp16']:\n    model.convert_to_fp16()","0a715f22":"clip_model = clip.load('ViT-B\/16', jit=False)[0].eval().requires_grad_(False).to(device)\nclip_size = clip_model.visual.input_resolution\nnormalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\nlpips_model = lpips.LPIPS(net='vgg').to(device)","6d3115aa":"text_prompts = [\n    # \"an abstract painting of 'ravioli on a plate'\",\n    # 'cyberpunk wizard',\n    'the gateway to eternal dread (painting)',\n]\n\nimage_prompts = [\n    # 'mona.jpg',\n]\n\nclip_guidance_scale = 25000 # 5000 - Controls how much the image should look like the prompt.\ntv_scale = 500 # 500 - Controls the smoothness of the final output.\nrange_scale = 100 # 100 - Controls how far out of range RGB values are allowed to be.\nsat_scale = 0 # 0 - Controls how much saturation is allowed. From nshepperd's JAX notebook, though not sure if it's doing anything right now...\ncutn = 16 # 16 - Controls how many crops to take from the image. Increase for higher quality.\ncutn_batches = 2 # 2 - Accumulate CLIP gradient from multiple batches of cuts [Can help with OOM errors \/ Low VRAM]\n\ninit_image = None # None - URL or local path\ninit_scale = 0 # 0 - This enhances the effect of the init image, a good value is 1000\nskip_timesteps = 10 # 0 - Controls the starting point along the diffusion timesteps\n\n# Try this option for random natural-looking noise in place of an init image:\nperlin_init = True # False - Option to start with random perlin noise\nperlin_mode = 'mixed' # 'mixed' ('gray', 'color')\nif init_image is not None: # Can't combine init_image and perlin options\n  perlin_init = False\n\nskip_augs = False # False - Controls whether to skip torchvision augmentations\nrandomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\nclip_denoised = False # False - Determines whether CLIP discriminates a noisy or denoised image\nclamp_max = 0.05 # 0.05\n\nfuzzy_prompt = False # False - Controls whether to add multiple noisy prompts to the prompt losses\nrand_mag = 0.05 # 0.1 - Controls the magnitude of the random noise\neta = 0.5 # 0.0 - DDIM hyperparameter\n\nparallax_level = 0.1  # Increase for more depth but also more artifacts","968f831f":"display_rate = 1\nn_batches = 1 # 1 - Controls how many consecutive batches of images are generated\nbatch_size = 1 # 1 - Controls how many images are generated in parallel in a batch\n\n# seed = 0\nseed = random.randint(0, 2**32) # Choose a random seed and print it at end of run for reproduction\n\ntry:\n    gc.collect()\n    torch.cuda.empty_cache()\n    do_run()\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print('seed', seed)\n    gc.collect()\n    torch.cuda.empty_cache()","96f2ec7b":"import torch\n# Check the GPU status\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n!nvidia-smi","3c013e07":"model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\nmidas = torch.hub.load(\"intel-isl\/MiDaS\", model_type)\nmidas.to(device)\nmidas.eval()\nmidas_transforms = torch.hub.load(\"intel-isl\/MiDaS\", \"transforms\")\nif model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform","b462a2f8":"from PIL import Image\n\nim = Image.open(\"out.png\")\nim","43dba48b":"import numpy as np\nfrom matplotlib import pyplot as plt\n\n\nimg = np.asarray(im)\nimg = np.concatenate((img, img, img), axis=1)\nwith torch.no_grad():\n    depth = midas(transform(img).to(device))\n\n    depth = torch.nn.functional.interpolate(\n        depth.unsqueeze(1),\n        size=img.shape[:2],\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()\ndepth = depth.detach().cpu().numpy().swapaxes(0, -1).swapaxes(0, 1)\ndepth -= depth.min()\ndepth \/= depth.max()\nplt.imshow(depth)\nplt.show()","97594170":"from numba import njit, prange\nimport cv2\n\n\n# @njit\n# def rowarp(i, d, s=0.1)\n\n\ntry:\n    parallax_level\nexcept NameError:\n    parallax_level = 0.1\n\n\n@njit  # (parallel=True)\ndef rewarp(i, d, s=parallax_level, u=4):\n#     for f in prange(i_.shape[0]):\n#         i, d = i_[f], d_[f]\n    l = i.shape[0], i.shape[1] \/\/ u, i.shape[2]\n    result = np.zeros(l)\n    mask = np.zeros(l[:-1])\n    count = np.zeros(l[:-1])\n    there = np.zeros(l[:-1])\n    h = l[1]\/\/2\n    for y in prange(i.shape[0]):\n        for x in range(i.shape[1]):\n            offset = x - h\n            m = d[y, x]\n            x_ = int(offset * (m * s + 1)) + h\n            x_ \/\/= u\n            x_ = min(max(x_, 0), l[1]-1)\n            if m > mask[y, x_] \/ max(1, count[y, x_]) - .015756258325:\n                result[y, x_] += i[y, x]\n                mask[y, x_] += m\n                count[y, x_] += 1\n                there[y, x_] += 1\n    there[:, h-1:h+1] = 1\n    result = result \/ np.maximum(count, 1).reshape(count.shape + (1,))\n    result = result.astype(np.uint8)\n    return result, there\n\n\ndef warp(i, d, u=64):\n    d = d[:, i.shape[1]:i.shape[1]*2]\n    orig = i.shape\n    s = i.shape[1] * u, i.shape[0]\n    i = cv2.resize(i, s)\n    d = cv2.resize(d, s)\n    result, there = rewarp(i, d, u=u)\n    result = cv2.resize(result, (orig[1], orig[0]))\n    for _ in range(16):\n        result[there == 0] = cv2.blur(result, (17, 1))[there == 0]\n    return result\n\n\nplt.imshow(warp(np.asarray(im), depth))\nplt.show()","25918b14":"from tqdm.auto import trange\n\n\nfps = 60\n\nmove_speed = 512\nrepeats = 2\nout_file = \"start.mp4\"\nwriter = cv2.VideoWriter(out_file, cv2.VideoWriter_fourcc(*'mp4v'), fps  # 30\n                         , im.size)\n\nimage = np.asarray(im)\nfor i in trange(int(move_speed * repeats)):\n    r = int(i \/ im.size[0] * move_speed)\n    result = np.roll(image, r, 1)\n    d = np.roll(depth, r, 1)\n    writer.write(cv2.cvtColor(warp(result, d), cv2.COLOR_BGR2RGB))\nwriter.release()","3a2eda36":"!ffmpeg -y -i start.mp4 -vf format=yuv420p out.mp4 > \/dev\/null 2> \/dev\/null","2652c45d":"from IPython import display\nfrom base64 import b64encode\n\n\ndef display_video(path):  \n    mp4 = open(path,'rb').read()   \n    data_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\n    display.display(\n      display.HTML(\n      \"\"\"\n          <video width=512 controls>\n                <source src=\"%s\" type=\"video\/mp4\">\n          <\/video>\n      \"\"\" % data_url\n           )   \n    )\n\ndisplay_video(\"out.mp4\")","61cb349e":"# Load MiDaS model","3048f23d":"# Load Diffusion and CLIP models","3cb81e5a":"# 3D","97136754":"# Settings","ed54f224":"# Define necessary functions","419a7dc5":"# Diffuse!","124bce8c":"# Generates images from text prompts with CLIP guided diffusion.\n\nBy Katherine Crowson (https:\/\/github.com\/crowsonkb, https:\/\/twitter.com\/RiversHaveWings). It uses either OpenAI's 256x256 unconditional ImageNet or Katherine Crowson's fine-tuned 512x512 diffusion model (https:\/\/github.com\/openai\/guided-diffusion), together with CLIP (https:\/\/github.com\/openai\/CLIP) to connect text prompts with images.\n\nKatherine's original notebook can be found here:\nhttps:\/\/colab.research.google.com\/drive\/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA\n\n---\n\nModified by Daniel Russell (https:\/\/github.com\/russelldc, https:\/\/twitter.com\/danielrussruss) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.\n\n**Update**: Sep 19th 2021\n\nFurther improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.\n\n**Update**: Sep 25th 2021\n\nModified some of the params to work a bit better than before! (I think?)\n\n**Update**: Oct 7th 2021\n\nThe random perlin noise will now regenerate with each consecutive batch. This is relevant when using perlin_init=True and n_batches > 1.\n\n**Update**: Oct 26th 2021\n\nWhen using fuzzy_prompt, prompts now get fuzzed with each consecutive batch, rather than created multiple fuzzed copies before training.\n\nChanged default timestep_respacing ('ddim50' -> '50'). Reduced default tv_loss_weight (1500 -> 500).\n\nUsing a batch_size > 1 works with perlin and image init options now. (was previously only working with random init)\n\nUpgraded train function to the faster `cond_fn_with_grad` version.\n\n**Update**: Oct 27th 2021\n\nRollback `cond_fn_with_grad` changes, was causing OOM and speed issues for some users? Seemed fine on P100 and 3090, not sure what's going on here.\n\n**Update**: Nov 4th 2021, by @nev#4905 (https:\/\/github.com\/neverix)\n\nAdded infinite tiling, adapted for Kaggle!\n\nLater: Added 3D animation","49d890c5":"# Install and import dependencies"}}