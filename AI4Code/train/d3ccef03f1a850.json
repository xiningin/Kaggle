{"cell_type":{"ec5f2a39":"code","773c7716":"code","ba1d970d":"code","2b124034":"code","ffaec477":"code","2c0c80a9":"code","ec8fb3b9":"code","d9e86e1f":"code","51504a19":"code","6bafb23c":"code","0b53f80e":"code","b4be6e49":"code","fe75ea6a":"code","01a6dcb2":"code","1d21aae6":"code","3b5bff4a":"code","416c3e57":"code","0ccacb1e":"code","9281ed17":"code","d6b2ebf7":"code","6e178203":"code","59aec327":"code","3d765814":"code","a9c6a4dc":"code","c1e84ba4":"code","6bac0c5e":"code","4c106096":"code","6ce72303":"code","c1119293":"code","a75a0a0f":"code","1ba45341":"code","45891197":"code","78edd8d5":"code","8b73d482":"code","fba654d3":"code","3918dce0":"code","07dbaa3a":"code","5af5c4dc":"code","c967e748":"code","7c6aa11e":"code","8abd9725":"code","922ff58c":"code","f83d26a2":"code","34d4edb5":"code","1e14ebe0":"code","161fd18e":"code","8f7c06e6":"code","58e26151":"code","c1f3b9a2":"code","7f133df6":"code","2b3f8ae5":"code","6d5af43a":"code","5ae43035":"code","06715aa0":"code","f994f420":"code","25734aef":"markdown","29d922a0":"markdown","234ac04d":"markdown","351f3e04":"markdown","82263624":"markdown","3a947c13":"markdown","adb88332":"markdown","86593a48":"markdown","421ce7a3":"markdown","ba2df781":"markdown","4cb4c5e5":"markdown","b4a8428f":"markdown"},"source":{"ec5f2a39":"#Packages in use\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","773c7716":"#reading the data\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","ba1d970d":"display(train.head())\nprint('**************************************************************************************************')\ndisplay(test.head())","2b124034":"#defining functions to concat and devide train and test dataset\n#to avoid doing data cleaning and manupulation twice\n\ndef conct(train,test):\n    return pd.concat([train,test],sort=True).reset_index(drop=True)\n\ndef devide(df):\n    return df.loc[:890],df.loc[891:].drop(['Survived'],axis=1)","ffaec477":"df=conct(train,test)","2c0c80a9":"# No of rows and columns in train dataset\ntrain.shape","ec8fb3b9":"# No of rows and columns in test dataset\ntest.shape","d9e86e1f":"# null values preseent in train dataset\ntrain.isnull().sum()","51504a19":"# null values present in test dataset \ntest.isnull().sum()","6bafb23c":"#Age\n#Age varies with pclass\n#people with higher passenger class are older than that of people in lower passenger class\n#Also age varies with sex\n#grouping age with pclass and sex and finding the median to fill the missing values","0b53f80e":"train.groupby(['Pclass','Sex'])['Age'].median()","b4be6e49":"df['Age']=df.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.median()))","fe75ea6a":"#Fare(Present in the test data set)\ndf.loc[df['Fare'].isnull()]","01a6dcb2":"# Passenger class 3, embarked S,travelled alone\n#fill the value with median fare of passengers falling under\n#the same criteria\nmissingFare=df.loc[(df['Pclass']==3)&(df['Embarked']==\"S\")&(df['SibSp']==0)]['Fare'].median()","1d21aae6":"missingFare\ndf.loc[df['Fare'].isnull(),'Fare']=missingFare","3b5bff4a":"# the cabin variable has a lot of missing variables \n#but it is an imprtant predictor when considering the\n#Structure of the titanic ship\n#hence we use cabin to create a new variable \"deck\" from cabin\n#The misssing values are given 'M'","416c3e57":"#keep all first letteres of cabin \ndf['Deck']=df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')","0ccacb1e":"df[['Deck','Survived']].groupby('Deck')['Survived'].mean().plot(kind='bar',figsize=(15,7))","9281ed17":"#grouping the deccks as ABC,DE,FG and M\ndf[df['Deck']=='T']","d6b2ebf7":"df.loc[339,'Deck']='A'\ndf['Deck']=df['Deck'].replace(['A','B','C'],'ABC')\ndf['Deck']=df['Deck'].replace(['D','E'],'DE')\ndf['Deck']=df['Deck'].replace(['F','G'],'FG')\n\ndf['Deck'].value_counts()","6e178203":"# the 2 null values in embarked are found out to be 'S'\n# By searchin their respective names on google\ndf.loc[df['Embarked'].isnull(),\"Embarked\"]='S'","59aec327":"df.isnull().sum()# all null values are well dealt with","3d765814":"# creating a new column Title from name which shows the socio economic status of an individual\ntitles = set()\nfor name in train['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nprint(titles)","a9c6a4dc":"Title_Dictionary = {\"Capt\": \"Officer\",\"Col\": \"Officer\",\"Major\": \"Officer\",\"Jonkheer\": \"Royalty\",\"Don\": \"Royalty\",\"Sir\" : \"Royalty\",\"Dr\": \"Officer\",\"Rev\": \"Officer\",\"the Countess\":\"Royalty\",\"Mme\": \"Mrs\",\"Mlle\": \"Miss\",\"Ms\": \"Mrs\",\"Mr\" : \"Mr\",\"Mrs\" : \"Mrs\",\"Miss\" : \"Miss\",\"Master\" : \"Master\",\"Lady\" : \"Royalty\"}","c1e84ba4":"df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\ndf['Title'] = df.Title.map(Title_Dictionary)\ndf.head()","6bac0c5e":"df1,df2=devide(df)# deviding the datasets","4c106096":"df1=df1.drop(['Name','Ticket','Cabin','PassengerId'], axis=1)\ndf1.head()","6ce72303":"#converting categorical features into numeric values\ndf1.Sex=df1.Sex.map({'female':0, 'male':1})\ndf1.Embarked=df1.Embarked.map({'S':0, 'C':1, 'Q':2,})\ndf1.Title=df1.Title.map({'Mr':0, 'Miss':1, 'Mrs':2,'Master':3,'Officer':4,'Royalty':5})\ndf1.Deck=df1.Deck.map({'FG':0,'DE':1,'ABC':2,'M':3})","c1119293":"df1.head()","a75a0a0f":"df1.isnull().sum()# Data is cleaned","1ba45341":"df1.Age = (df1.Age-min(df1.Age))\/(max(df1.Age)-min(df1.Age))\ndf1.Fare = (df1.Fare-min(df1.Fare))\/(max(df1.Fare)-min(df1.Fare))","45891197":"df1.describe()","78edd8d5":"from sklearn.model_selection import train_test_split","8b73d482":"X_train, X_test, y_train, y_test = train_test_split(\n    df1.drop(['Survived'], axis=1),\n    df1.Survived,\n    test_size= 0.2,\n    random_state=0,\n    stratify=df1.Survived\n)","fba654d3":"# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\nY_pred = clf.predict(X_test)\naccuracy_score(y_test, Y_pred)","3918dce0":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test, Y_pred)\ncm","07dbaa3a":"sns.heatmap(cm,annot=True)","5af5c4dc":"#Test data set\ndf2.head()","c967e748":"titles = set()\nfor name in df2['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nprint(titles)","7c6aa11e":"df2['Title'] = df2['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\ndf2['Title'] = df2.Title.map(Title_Dictionary)\ndf2.head()","8abd9725":"# dropping unwanted columns\ndf2=df2.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)","922ff58c":"# Converting categorical feature to numeric\ndf2.Sex=df2.Sex.map({'female':0, 'male':1})\ndf2.Embarked=df2.Embarked.map({'S':0, 'C':1, 'Q':2,'nan':'nan'})\ndf2.Title=df2.Title.map({'Mr':0, 'Miss':1, 'Mrs':2,'Master':3,'Officer':4,'Royalty':5})\ndf2.Deck=df2.Deck.map({'FG':0,'DE':1,'ABC':2,'M':3})\ndf2.head()","f83d26a2":"# Checking for null values\ndf2.isnull().sum()","34d4edb5":"# Null value in the title column\ndf2[df2.Title.isnull()]","1e14ebe0":"df2=df2.fillna(2)","161fd18e":"# Data is cleaned to have no null value\ndf2.isnull().sum()","8f7c06e6":"# cleaned dataset\ndf2.head()","58e26151":"# feature scaling\ndf2.Age = (df2.Age-min(df2.Age))\/(max(df2.Age)-min(df2.Age))\ndf2.Fare = (df2.Fare-min(df2.Fare))\/(max(df2.Fare)-min(df2.Fare))","c1f3b9a2":"# test dataset\ndf2.head()","7f133df6":"pred = clf.predict(df2)","2b3f8ae5":"pred","6d5af43a":"pred1=pred.astype(int)","5ae43035":"pred1","06715aa0":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": pred1\n    })\n","f994f420":"# visualizing predicted values\nsns.countplot(x='Survived', data=submission)","25734aef":"#### AGE","29d922a0":"## EDA","234ac04d":"### FARE","351f3e04":"# Cleaning data","82263624":"# Feature scaling","3a947c13":"# Data modelling","adb88332":"### Dealing with the null values","86593a48":"### Embarked","421ce7a3":"# Confusion matrix","ba2df781":"# Prediction","4cb4c5e5":"### CABIN","b4a8428f":"# Feature Engineering"}}