{"cell_type":{"71a9f214":"code","4d8938e3":"code","fbd3a29c":"code","7261aba3":"code","1f792ce3":"code","230c224b":"code","be9267d7":"code","2c1db815":"code","528357de":"markdown","23a3d7a0":"markdown","53ecf756":"markdown","8b3a6235":"markdown","0a071d58":"markdown","06947c0b":"markdown"},"source":{"71a9f214":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d8938e3":"path = '\/kaggle\/input\/applied-ml-microcourse-telco-churn'\n\ncustomer = pd.read_csv('{}\/customer.csv'.format(path))\ncontract = pd.read_csv('{}\/contract.csv'.format(path))\ncustomer.head()","fbd3a29c":"contract.head()","7261aba3":"customer.info()","1f792ce3":"contract[contract['customerID'].duplicated()].shape","230c224b":"print(customer[customer['customerID'].duplicated()])\nprint('Customer file shape is: {}'.format(customer.shape))\nprint('Contract file shape is: {}'.format(contract.shape))","be9267d7":"data = customer.merge(contract, on='customerID')\ndata.head()","2c1db815":"data['churn'] = np.where(data['EndDate'].isna(), 0, 1)\nprint('Merged data has shape: {}'.format(data.shape))\ndata.head()","528357de":"It is common for a single customer to have multiple contracts, for instance one for each contract term.  However, in this data set we can see that the each customer only has one contractID associated with it.  To verify this, you can use the duplicated() function.  This is applied within the data frame subset operation, and will return all duplicate rows on the given column.  In our case, it returns an empty data frame indicating that there are no duplicated customers.\n\nCheck to see if there are any duplicate customerIDs on the contract","23a3d7a0":"Let us now start creating our feature set.   We will merge the customer and contract data sets together and define our churn label.  Note that the customerID is the common linking field for these two data sets, and is unique across both files.  Always check this first as duplicate rows can cause problems\n\nEnsure that customerID is unique on customer and check the size of both data sets.","53ecf756":"## Defining the entity to model\n\nLet's have a look at some data for the churn use case.  Here are two files extracted from the data lake.  We have a customer entity file and a contract entity file.  Download the files, then we'll load them into pandas in python and have a look. ","8b3a6235":"# 4.2 Preparing data for churn modelling","0a071d58":"## Defining the prediction label\n\nLet's have a look at the contract data again and see if there is enough information to construct a churn label.  The file has the following fields:\n\n- Contract: contract term, either one year, month-to-month or two years\n- MonthlyCharges: contracted amount to charge per month\n- StartDate: notice that contract seem to roll over when the initial term expires and the start date is not updated\n- EndDate: this is only populated for one contract in the top 10.  This looks like it is filled if the contract has been terminated","06947c0b":"In this notebook, we will load the data sets and create our churn indicator"}}