{"cell_type":{"09b99d4d":"code","4da32334":"code","ad575e39":"code","6d9707c5":"code","7e500d63":"code","a2560831":"code","5a47e049":"code","2428b86a":"code","25fdeddb":"code","29d71acb":"code","7eb65a4d":"code","3c965978":"code","ffee04d4":"code","48a903e2":"code","1d36940a":"code","68ec630f":"code","685211ad":"code","47b19ddc":"code","f38fe98b":"code","3b579749":"code","bb3a1704":"code","132df015":"markdown","93915c95":"markdown","bdae1c94":"markdown","cb291b9b":"markdown","c07d3ce1":"markdown","f7ee3c44":"markdown","c72eaa6c":"markdown","f8638421":"markdown","adf2aa6d":"markdown","de4c9bdb":"markdown"},"source":{"09b99d4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4da32334":"train = pd.read_csv( '..\/input\/clickbait-dataset\/clickbait_data.csv')\n\ntrain.head()","ad575e39":"train.shape","6d9707c5":"#checking for nan values\n\ntrain = train.fillna('')\n\ntrain.shape","7e500d63":"import re\nfrom string import punctuation\n\ndef process_text1(headline):\n    \n    result = headline.replace('\/','').replace('\\n','')\n    result = re.sub(r'[0-9]+','number', result)   # we are substituting all kinds of no. with word number\n    result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)  # \\w matches one word\/non word character\n    result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n    \n    result = ''.join(word for word in result if word not in punctuation)  # removes all characters such as \"!\"#$%&'()*+, -.\/:;<=>?@[\\]^_`{|}~\"\n    result = re.sub(r' +', ' ', result).lower().strip()\n    return result\n\n#-------------------------------------------------------OR------------------------------------------------------------------------------------\n\ndef process_text2(headline):\n    \n    result2 = re.sub(r'[0-9]+','number', headline)\n    result2 = re.sub('[^a-zA-Z]',\" \", result)\n    result2 = result2.lower()\n    \n    # we are splitting the messages into word list on the basis of spaces\n    result2 = result2.split()\n    return result2","a2560831":"# removing the stopwords\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words(\"english\")\n\ndef cnt_stopwords(headline):\n    \n    result1 = headline.split()\n    num1 =  len([word for word in result1 if word in stop])\n    \n    return num1","5a47e049":"# Clickbait headlines also tend to contain more informal writing than non-clickbait headlines. As such, they may contain many more contractions, occurrences of slang, etc\n\ncontractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n                'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n                'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n                'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n                'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n                'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n                'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n                'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n                'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n                'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n                'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n                'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n                'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n                'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n                'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n                'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n                'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n                'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n                'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n\ndef cnt_contract(headline):\n    \n    result2 = headline.split()\n    num2 = len([word for word in result2 if word in contractions])\n    return num2","2428b86a":"# It is often the case that clickbait headlines are stated in the form of a questions, which begins with question words\n\nquestion_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n        'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n        'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n        'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n\n\ndef question_word(headline):\n    \n    result3 = headline.lower().split()\n    \n    if result3[0] in question_words:\n        return 1\n    else:\n        return 0","25fdeddb":"# Lastly, a function is defined to check the part-of-speech (i.e., noun, verb, adjective, etc.) of each word in a headline. It\u2019s possible that non-clickbait headlines contain noun\n\ndef pos_tags(headline):\n    \n    result4 = headline.split()\n    \n    non_stop = [word for word in result4 if word not in stopwords.words(\"english\")]\n    pos = [part[1] for part in nltk.pos_tag(non_stop)]\n    pos = \" \".join(pos)\n    return pos","29d71acb":"import nltk\n\ntrain['processed_headline']     = train['headline'].apply(process_text1)\ntrain['question'] = train['headline'].apply(question_word)\n\ntrain['num_words']       = train['headline'].apply(lambda x: len(x.split()))\ntrain['part_speech']     = train['headline'].apply(pos_tags)\ntrain['num_contract']    = train['headline'].apply(cnt_contract)\ntrain['num_stop_words']  = train['headline'].apply(cnt_stopwords)\ntrain['stop_word_ratio'] = train['num_stop_words']\/train['num_words']\ntrain['contract_ratio']  = train['num_contract']\/train['num_words']","7eb65a4d":"train","3c965978":"train = train.drop(columns = ['num_contract','num_stop_words'])\n\ntrain","ffee04d4":"from sklearn.model_selection import train_test_split\n\ndf_train,df_test = train_test_split(train, test_size=0.25, random_state=0)","48a903e2":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\"\"\"\n 1) strip_accents: \u2018ascii\u2019 is a fast method that only works on characters that have an direct ASCII mapping. \u2018unicode\u2019 is a slightly slower method that works on any characters\n 2) analyzer: Whether the feature should be made of word or character n-grams.\n 3) token_pattern: regexp selects tokens of 2 or more alphanumeric characters. \n 4) min_df: ignore terms that have a document frequency strictly lower than the given threshold.\n 5) use_idf: Enable inverse-document-frequency reweighting.\n 6) Smooth idf: weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once.\n 7) sublinear_tf: Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n\"\"\"\n\ntfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),use_idf=1, smooth_idf=1, sublinear_tf=1)\n\n\"\"\"\n\n1) fit_transform -> Learn vocabulary and idf, return document-term matrix,This is equivalent to fit followed by transform.\n\n2) transform -> Transform documents to document-term matrix,Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform)\n\n\"\"\"\n\nX_train_headline = tfidf.fit_transform(df_train['processed_headline'])\nX_test_headline  = tfidf.transform(df_test['processed_headline'])","1d36940a":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\n\"\"\"\nThe idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\nmean : u\nstd. deviation : s\ndata = [x1,x2....]\n\nprocesses every element as:\nx1 = (x1-u)\/s\n...\n\n\"\"\"\n\ncv = CountVectorizer()\n\nX_train_pos = cv.fit_transform(df_train['part_speech'])\nX_test_pos = cv.transform(df_test['part_speech'])\n\n\n\nsc = StandardScaler(with_mean = False) # taking the mean on sparse matrix will not work, becoz centering them creates dense matrix which is too large to fit in memory.\n\nX_train_pos_sc = sc.fit_transform(X_train_pos)\nX_test_pos_sc = sc.transform(X_test_pos)\n","68ec630f":"# removing those rows which are already used up\n\nX_train_val = df_train.drop( columns = ['headline','clickbait','processed_headline','part_speech']).values\nX_test_val = df_test.drop( columns = ['headline','clickbait','processed_headline','part_speech']).values\n\n# scaling all the above values\nsc = StandardScaler()\nX_train_val_sc = sc.fit_transform(X_train_val)\nX_test_val_sc  = sc.transform(X_test_val)","685211ad":"Y_train = df_train['clickbait'].values\nY_test = df_test['clickbait'].values","47b19ddc":"\"\"\"\nLastly, we can combine the new tf-idf vectors with the scaled engineered features and store them as sparse arrays.\nThis helps to save memory as the tf-idf vectors are extremely large, but are composed mostly of zeros.\n\nsparse matrix\n\nWhen a sparse matrix is represented with a 2-dimensional array, we waste a lot of space to represent that matrix.\nFor example, consider a matrix of size 100 X 100 containing only 10 non-zero elements.\nIn this matrix, only 10 spaces are filled with non-zero values and remaining spaces of the matrix are filled with zero.\nThat means, totally we allocate 100 X 100 X 2 = 20000 bytes of space to store this integer matrix.\n\n1)Triplet Representation (Array Representation)  -  In this representation, we consider only non-zero values along with their row and column index values. In this representation, the 0th row stores the total number of rows, total number of columns and the total number of non-zero values in the sparse matrix.\n                                                    For example, consider a matrix of size 3*4 containing 4 number of non-zero values.\n                                                    \n                                                    0 0 0 9                                  rows   col    non-zero-values\n                                     given matrix:  2 0 6 0       triplet representation:     3      4          4\n                                                    0 0 0 3                                   0      3          9\n                                                                                              1      0          2\n                                                                                              1      2          6                \n                                                                                              2      3          3\n                                                                                              \n e.g - it 1st row shows total row,col, non-0 val then in 2nd row 9 val is present in 0th row and 3rd col in this way...\n2)Linked Representation\n\"\"\"\n\nfrom scipy import sparse\n\nX_train = sparse.hstack([X_train_val_sc, X_train_headline, X_train_pos_sc]).tocsr()\nX_test  = sparse.hstack([X_test_val_sc, X_test_headline, X_test_pos_sc]).tocsr()\n\n# tocsr() - Returns a copy of this matrix in Compressed Sparse Row format.","f38fe98b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#Regularization is applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting and C is inverse of regularization\n\nparam_grid  = [{'C': np.linspace(90,100,20)}]\ngrid_cv = GridSearchCV(LogisticRegression(), param_grid, scoring='accuracy', cv=5, verbose=1) # No. of K-folds(cv) = 5\ngrid_cv.fit(X_train, Y_train)\n\nprint(grid_cv.best_params_)\nprint(grid_cv.best_score_)","3b579749":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(penalty='l2', C=93.684210526315795)\nmodel = model.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\n\n\n","bb3a1704":"# 1. accuracy score\naccuracy = accuracy_score(Y_test, Y_pred)\nprint('accuracy :',accuracy*100,\"%\")\n\n\n# 2. Classification report\nprint(classification_report(Y_test, Y_pred))","132df015":"3. **Applying functions to data**","93915c95":"In these same training data set we would perform the k-fold cross validation using gridsearchCV to find the best combination values for\nlogistic regression parameters","bdae1c94":"1.  **Loading the dataset**","cb291b9b":"**Converting the headlines text into numerical data by TF-IDF**","c07d3ce1":"**Preparing our response matrix**","f7ee3c44":"**Combining the feature matrix **","c72eaa6c":"5. **Preparing feature and response matrix**","f8638421":"2. **Feature Engineering**","adf2aa6d":"4. **Splitting the dataset into test and train**","de4c9bdb":"**Next, we can use CountVectorizer to count the number of part-of-speech occurrences in each headline. These counts are then scaled using StandardScaler.**"}}