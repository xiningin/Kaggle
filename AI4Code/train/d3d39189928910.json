{"cell_type":{"105d745e":"code","b5129fb7":"code","50e77844":"code","65e22aaa":"code","e7332af3":"code","b8d0a63e":"code","dd57b443":"code","5e0ab4d9":"code","795b5656":"code","1b650c31":"code","516cc1df":"code","2f661647":"code","fa03a29c":"code","a281af8a":"code","37f4e73f":"code","8764698c":"code","137ad3e7":"code","63ec29c0":"code","79239fc9":"code","6f5f9882":"code","674c9e3c":"code","8ba399fa":"code","c3a06550":"code","69496fad":"code","75ab61ab":"code","ba8519f9":"code","7fdf7fa9":"code","522e712d":"code","5d6fe7d0":"code","9a6de784":"code","8ff189fb":"code","89dd2ce7":"code","db490e7c":"code","df9eaa7d":"code","b6009e1f":"code","0766c7dc":"code","c02ff5fb":"code","2e73c5dc":"code","55817681":"code","2317009b":"code","3c7f9ad8":"code","bc9f348d":"code","1ddd45cb":"code","b48468c3":"code","82a18265":"code","304626f6":"code","30d4963d":"code","e9e29c41":"code","9ed5c5c6":"code","ffa963f1":"code","54845d03":"code","eca47792":"code","58d55a94":"code","7dcd5b33":"code","781bee82":"code","a8bafa20":"code","5e3e5615":"code","48fae170":"code","05263a80":"code","3d0e0200":"code","0eb3c7e3":"code","5d8a4af6":"code","ed390233":"code","28b56720":"code","3d542aae":"code","657d0a4e":"code","1f39c5e6":"code","6d88d76c":"code","6ec58857":"code","6e65cf86":"code","0eaa0dce":"code","b722e1fa":"code","cbfa9684":"code","2f9c3e26":"code","177d735b":"code","596d4c41":"code","46d68e35":"code","0fb324c2":"markdown","a4c4844e":"markdown","c314965a":"markdown","11c15d64":"markdown","8e0b7796":"markdown","b8542c30":"markdown","bc6dbfa0":"markdown","75059c29":"markdown","9014e9bd":"markdown","0a00ecfc":"markdown","ba9a7472":"markdown","59e94ce4":"markdown","2939b3dc":"markdown","33294ecb":"markdown","29859071":"markdown","6877713c":"markdown"},"source":{"105d745e":"#Importing Libraries for data analysis\n\n# Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features\n\n# For feature selection\n# Ref: http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\n\n# Data processing\n#  Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Splitting data\nfrom sklearn.model_selection import train_test_split\n\n# Decision tree modeling\nfrom sklearn.tree import  DecisionTreeClassifier as dt\n\n# RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n# Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc\nimport os, time, gc\n","b5129fb7":"\n\n# Read train\/test files\nheart = pd.read_csv(\"..\/input\/heart.csv\")\n\nheart.head(5)\nheart.shape         ## 303 x 14","50e77844":"#  Split into Test and Training Data\nX_train, X_test, y_train, y_test = train_test_split(\n        heart.drop('target', 1), \n        heart['target'], \n        test_size = 0.3, \n        random_state=10\n        ) ","65e22aaa":"# Look at data\nX_train.head(2)\nX_train.shape                        # 212 x 13\nX_test.shape                         # 91 x 13\n\ny_test.shape                        # 91 x \ny_train.shape                       # 212 x\n\n# Data types\nX_train.dtypes.value_counts()   # All afeatures re integers \n","e7332af3":"# Target classes are almost balanced\nheart.target.value_counts()\n","b8d0a63e":"# Check if there are Missing values? None\nX_train.isnull().sum().sum()  # 0\nX_test.isnull().sum().sum()   # 0","dd57b443":"#  Feature 1: Row sums of features  More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)","5e0ab4d9":"# Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)","795b5656":"#  Check if tmp_train is same as train or is a view\n#     of train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\ntmp_train._is_view                # False","1b650c31":"# Check if 0 has been replaced by NaN\ntmp_train.head(1)\ntmp_test.head(1)\n","516cc1df":"# Feature 2 : For every row, how many features exist\n#                that is are non-zero\/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","2f661647":"# Similary create other statistical features\n#    Feature 3\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)\n","fa03a29c":"# Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()","a281af8a":"# So what do we have finally\nX_train.shape                \nX_train.head(1)\nX_test.shape                 \nX_test.head(2)","37f4e73f":"# Before we proceed further, keep target feature separately\ntarget = y_train\ntarget.tail(2)","8764698c":"# Store column names of our data somewhere\n#     We will need these later (at the end of this code)\ncolNames = X_train.columns.values\ncolNames\n","137ad3e7":"\n# Random projection is a fast dimensionality reduction feature\n#     Also used to look at the structure of data\n","63ec29c0":"#  Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\n","79239fc9":"\ntmp.shape     # 303 X 21\n","6f5f9882":"# Transform tmp t0 numpy array\n\ntmp = tmp.values\ntmp.shape       # 303 X 21\n","674c9e3c":"#  Let us create 8 random projections\/columns\nNUM_OF_COM = 8","8ba399fa":"#  Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)","c3a06550":"# fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])","69496fad":"#  Look at some features\nrp[: 5, :  3]\n","75ab61ab":"#  Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(8)]\nrp_col_names\n","ba8519f9":"# Before clustering, scale data\n#  Create a StandardScaler instance\nse = StandardScaler()\n# fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# \ntmp.shape               ","7fdf7fa9":"#  Perform kmeans using 13 features.\n#     No of centroids is no of classes in the 'target'\ncenters = target.nunique()    \ncenters               ","522e712d":"# Begin clustering\nstart = time.time()\n\n# First create object to perform clustering\nkmeans = KMeans(n_clusters=centers,\n                n_jobs = 4)         \n\n# Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)\/60.0      ","5d6fe7d0":"# Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size \n\n","9a6de784":"# Cluster labels are categorical. So convert them to dummy\n","8ff189fb":"#  Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)\n","89dd2ce7":"# Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     \n                                          \n","db490e7c":"# Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    \n\n","df9eaa7d":"# We will use the following as names of new two columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names\n","b6009e1f":"\n#  Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n\n","0766c7dc":"# Consider only first 8 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 8])\n\n\ndf.shape     # 303 X 36\n","c02ff5fb":"#  Generate some names for these 36 columns\npoly_names = [ \"poly\" + str(i)  for i in range(36)]\npoly_names\n\n","2e73c5dc":"\n\n# Append now all generated features together\n# Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape       \n","55817681":"#   If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])       \n\n\ntmp.shape          \n","2317009b":"# Combine train and test into X and y to split compatible datasets\nX = tmp\nX.shape     \n","3c7f9ad8":"# Combine y_train and y_test into y to split into compatible datasets later\ny = pd.concat([y_train,y_test],\n                axis = 0,            \n                ignore_index = True\n                )\ny.shape        ","bc9f348d":"# Delete tmp - as a good programming practice\ndel tmp\ngc.collect()\n","1ddd45cb":"# Split the feature engineered data into new training and test dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3)","b48468c3":"# \nX_train.shape   \n","82a18265":"X_test.shape    ","304626f6":"# Decision tree classification\n# Create an instance of class\nclf1_dt = dt(min_samples_split = 5,\n         min_samples_leaf= 3\n        )","30d4963d":"start = time.time()\n# Fit\/train the object on training data\n#      Build model\nclf1_dt = clf1_dt.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60                     \n","e9e29c41":"#  Use model to make predictions\nclasses1_dt = clf1_dt.predict(X_test)","9ed5c5c6":"#  Check accuracy\n(classes1_dt == y_test).sum()\/y_test.size      ","ffa963f1":"#  Instantiate RandomForest classifier\nclf1_rf = rf(n_estimators=50)\n","54845d03":"# Fit\/train the object on training data\n#      Build model\n\nstart = time.time()\nclf1_rf = clf1_rf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60                    \n","eca47792":"# Use model to make predictions\nclasses1_rf = clf1_rf.predict(X_test)","58d55a94":"#  Check accuracy\n(classes1_rf == y_test).sum()\/y_test.size      ","7dcd5b33":"#  Get feature importance\nclf1_rf.feature_importances_        \nclf1_rf.feature_importances_.size   ","781bee82":"# To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       \n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      ","a8bafa20":"# So how many columns?\nlen(colNames)           ","5e3e5615":"#  Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\n                   \"importance\": clf1_rf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }\n                 ).sort_values(by = \"importance\", ascending=False)","48fae170":"feat_imp.shape                  \nfeat_imp.head(13)","05263a80":"# Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\n","3d0e0200":"# Select top 13 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex = feat_imp.index.values[:13]\nnewindex","0eb3c7e3":"# Use these top 13 columns for classification\n#   Create DTree classifier object\nclf2_dt = dt(min_samples_split = 5, min_samples_leaf= 3)","5d8a4af6":"# Train the object on data\nstart = time.time()\nclf2_dt = clf2_dt.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)\/60                     ","ed390233":"#  Make prediction\nclasses2_dt = clf2_dt.predict(X_test[: , newindex])","28b56720":"#  Accuracy?\n(classes2_dt == y_test).sum()\/y_test.size ","3d542aae":"# Create RForest classifier object\nclf2_rf = rf(n_estimators=500)","657d0a4e":"# Traion the object on data\nstart = time.time()\nclf2_rf = clf2_rf.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)\/60                     ","1f39c5e6":"# Make prediction\nclasses2_rf = clf2_rf.predict(X_test[: , newindex])\n","6d88d76c":"# Accuracy?\n(classes2_rf == y_test).sum()\/y_test.size  \n","6ec58857":"# Select top  columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex2 = feat_imp.index.values[:10]\nnewindex2","6e65cf86":"#   Create DTree classifier object\nclf3_dt = dt(min_samples_split = 5, min_samples_leaf= 3)\n","0eaa0dce":"# Train the object on data\nstart = time.time()\nclf3_dt = clf3_dt.fit(X_train[: , newindex2], y_train)\nend = time.time()\n(end-start)\/60                     \n","b722e1fa":"#  Make prediction\nclasses3_dt = clf3_dt.predict(X_test[: , newindex2])\n","cbfa9684":"# Accuracy?\n(classes3_dt == y_test).sum()\/y_test.size\n","2f9c3e26":"#  Create RForest classifier object\n# increasing the number of estimators to 300 from 50...\nclf3_rf = rf(n_estimators=500)\n","177d735b":"# Train the object on data\nstart = time.time()\nclf3_rf = clf3_rf.fit(X_train[: , newindex2], y_train)\nend = time.time()\n(end-start)\/60                     \n","596d4c41":"#  Make prediction\nclasses3_rf = clf3_rf.predict(X_test[: , newindex2])\n","46d68e35":"#  Accuracy?\n(classes3_rf == y_test).sum()\/y_test.size \n","0fb324c2":"# Feature creation using kmeans \n\n","a4c4844e":"## Notice the number of derived parameters making into the top important features\n*******************","c314965a":"# 2.0 Set working directory and read file","11c15d64":"# Model building \n","8e0b7796":"## Notice the K-means parameters... they're found to be most relevant!!\n***********","b8542c30":"# concatenate all features  ","bc6dbfa0":"# Creating Interaction features \n# Using Polynomials","75059c29":"## Final data for model creation and testing\n### X_train: Training Data with new features\n### y_train: expected output for training data\n\n### X_test: test data with new features\n### y_test: expected output for test data","9014e9bd":"**Objectives of this kernal\n\nLearning Feature Engineering for classification . \nLearning use of Decision Tree. \nLearning use of Random Forests\n**\n","0a00ecfc":"# Feature selection ","ba9a7472":"## Using top 16 features for classification as per newindex2","59e94ce4":"# 3. Feature Engineering #","2939b3dc":"#  Using Statistical Numbers \n","33294ecb":" ****************************************\n## Using feature importance given by model\n****************************************\n","29859071":"# Feature creation Using Random Projections ","6877713c":"# 1. Import"}}