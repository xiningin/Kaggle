{"cell_type":{"61ea0f23":"code","dcd6eea6":"code","81bca3bc":"code","100f160a":"code","3e1e536c":"code","01b21a22":"code","b8c7697e":"code","a0cd975e":"code","98ddba3f":"code","9bd217f4":"code","303986b8":"code","2d6b6735":"code","a093433f":"code","4e5900f9":"code","4b29d7c3":"code","65fa860a":"code","daef8ceb":"code","3ede1c9c":"code","83fb3598":"code","90ff12da":"code","f33f9ae7":"code","6cdad9fe":"code","216e214e":"code","d0b88167":"code","ad49868c":"code","a40fb153":"markdown","70a7a566":"markdown","a884b607":"markdown","dc5f0ad6":"markdown","4820d507":"markdown","b5034739":"markdown","236cfd9f":"markdown","1e92fe9b":"markdown","e6a00ce8":"markdown","4661df77":"markdown"},"source":{"61ea0f23":"import pandas as pd\nimport numpy as np\nfrom itertools import combinations\nimport glob\n\nimport concurrent.futures\nfrom tqdm import tqdm\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","dcd6eea6":"# Plots display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 14})\n\n# Model paths: single person pose classification\nmodel_path = 'https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/3'  # Larger version\nmodel_path_light = 'https:\/\/tfhub.dev\/google\/movenet\/singlepose\/lightning\/3'  # Lighter version\n\n# Directory containing 107 subfolders with images\ndata_directory_path = '..\/input\/yoga-pose-image-classification-dataset\/dataset'\n\n# TensorFlow settings\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_SIZE = 256\nIMG_SIZE_LIGHT = 192\nBATCH_SIZE = 32","81bca3bc":"# 17 keypoints in the model output\nkp_descriptions = [\n    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n]","100f160a":"def get_image(path: str, flip=False) -> tf.Tensor:\n    \"\"\"Function loads image from a file and resizes it.\n    :param path: Path to image file\n    :param flip: Optional argument indicating whether to flip the image left to right\n    :return: Tensor with resized image data\n    \"\"\"\n    image = tf.expand_dims(tf.compat.v1.image.decode_jpeg(\n        tf.io.read_file(path), channels=3), axis=0)\n    if flip:\n        image = tf.image.flip_left_right(image)\n    # Resize and pad the image to keep the aspect ratio and fit the expected size\n    image = tf.cast(tf.image.resize_with_pad(image, IMG_SIZE_LIGHT, IMG_SIZE_LIGHT), dtype=tf.int32)\n    return image\n\n\ndef configure_for_performance(ds: tf.data.Dataset) -> tf.data.Dataset:\n    \"\"\"Function applies batch() and prefetch() functions\n    to the dataset to optimize data processing.\n    :param ds: TensorFlow Dataset object\n    :return Batched TensorFlow Dataset object\n    \"\"\"\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE).cache()\n    return ds\n\n\ndef movenet_inference(model: tf.keras.Model, path: str, flip=False) -> tf.Tensor:\n    \"\"\"Function transforms an image to a tensor of shape [1, 17, 3]\n    containing y and x coordinates and confidence level for 17 keypoints.\n    :param model: movenet model in tensorflow format\n    :param path: Path to image file\n    :param flip: Optional argument indicating whether to flip the image left to right\n    :return: Tensor with data for 17 keypoints\n    \"\"\"\n    keypoints = model.inference_fn(get_image(path, flip))[0]\n    return keypoints\n\n\ndef movenet_inference_flat(model: tf.keras.Model, path: str, flip=False) -> np.array:\n    \"\"\"Function transforms an image to a numpy array shape [51,]\n    containing y and x coordinates and confidence level for 17 keypoints.\n    :param model: movenet model in tensorflow format\n    :param path: Path to image file\n    :param flip: Optional argument indicating whether to flip the image left to right\n    :return: 1D array with 51 values\n    \"\"\"\n    keypoints = model.inference_fn(get_image(path, flip))[0][0].numpy().flatten()\n    return keypoints\n\n\ndef movenet_serving(model: tf.keras.Model, path: str, flip=False) -> tf.Tensor:\n    \"\"\"Function transforms an image to a tensor of shape [1, 17, 3]\n    containing y and x coordinates and confidence level for 17 keypoints.\n    :param model: Movenet model in tensorflow format\n    :param path: Path to image file\n    :param flip: Optional argument indicating whether to flip the image left to right\n    :return: Tensor with data for 17 keypoints\n    \"\"\"\n    keypoints = model(get_image(path, flip))['output_0'][0]\n    return keypoints\n\n\ndef movenet_serving_flat(model: tf.keras.Model, path: str, flip=False) -> np.array:\n    \"\"\"Function transforms an image to a numpy array shape [51,]\n    containing y and x coordinates and confidence level for 17 keypoints.\n    :param model: Movenet model in tensorflow format\n    :param path: Path to image file\n    :param flip: Optional argument indicating whether to flip the image left to right\n    :return: 1D array with 51 values\n    \"\"\"\n    keypoints = model(get_image(path, flip))['output_0'][0][0].numpy().flatten()\n    return keypoints\n\n\ndef display_images():\n    \"\"\"Function displays 100 examples of images\n    belonging to different classes in 5 x 5 grids.\n    \"\"\"\n    nrows = ncols = 5\n    class_n = 0\n\n    for pose in classes_df['pose'].unique():\n\n        if class_n > 100:\n            break\n\n        class_n += 1\n\n        if class_n % 25 == 1:\n            fig = plt.gcf()\n            fig.set_size_inches(ncols * 2.7, nrows * 2.7)\n\n        image_path = classes_df[classes_df['pose'] == pose]['path'].iloc[0]\n        image = mpimg.imread(image_path)\n        plt.subplot(nrows, ncols, class_n % 25 + 1)\n        plt.imshow(image)\n        plt.axis('Off')\n        plt.title(pose)\n\n        if class_n % 25 == 1 and class_n != 1:\n            plt.show()\n\n\ndef display_keypoints(path: str, points: tf.Tensor):\n    \"\"\"Function displays an image with marked keypoints.\n    :param path: Path to the image file\n    :param points: Tensor of keypoints in the shapes of [17, 3],\n    where 0-th column contains y coordinates, 1st column - x coordinates,\n    2nd column - confidence score\n    \"\"\"\n    # Convert coordinates from range [0, 1] to absolute positions\n    y = points[:, 0] * IMG_SIZE_LIGHT\n    x = points[:, 1] * IMG_SIZE_LIGHT\n\n    # Read image from file and resize with padding\n    image = tf.compat.v1.image.decode_jpeg(tf.io.read_file(path), channels=3)\n    image = tf.image.resize_with_pad(image, IMG_SIZE_LIGHT, IMG_SIZE_LIGHT)\n    image = tf.cast(image, dtype=tf.uint8)\n\n    plt.imshow(image)\n    plt.scatter(x, y)\n    plt.axis('Off')\n    plt.show()\n\n\ndef distance(coordinates: np.array) -> tuple:\n    \"\"\"Function calculates distance between two keypoints\n    described by x and y coordinates relative to image size.\n    :param coordinates: Array with 4 values [x coordinate of the 1st keypoint,\n    y coordinate of the 1st keypoint, x coordinate of the 2nd keypoint,\n    y coordinate of the 2nd keypoint]\n    :return: Tuple with 3 values [Euclidean distance between two points,\n    distance between x coordinates, distance between y coordinates]\n    \"\"\"\n    x_1, y_1, x_2, y_2 = coordinates\n    hor_dist = abs(x_1 - x_2)\n    vert_dist = abs(y_1 - y_2)\n    dist = np.sqrt(hor_dist ** 2 + vert_dist ** 2)\n    return dist, hor_dist, vert_dist\n\n\ndef is_higher(coordinates: np.array) -> int:\n    \"\"\"Function identifies relative positions\n    of two y coordinates in vertical direction.\n    :param coordinates: Array with 2 values [y coordinate of the 1st keypoint,\n    y coordinate of the 2nd keypoint]\n    :return: Binary value (1 - if the 1st coordinate is higher than 2nd,\n    0 - if the 1st coordinate is lower than 2nd coordinate)\n    \"\"\"\n    y_1, y_2 = coordinates\n    res = int((y_1 - y_2) > 0)\n    return res\n\n\ndef add_pos_features(df: pd.DataFrame, drop_scores=False) -> pd.DataFrame:\n    \"\"\"Function creates positional features based on keypoints.\n    :param df: DataFrame with keypoints (x and y coordinates)\n    :param drop_scores: Optional argument specifying whether to drop confidence scores\n    :return: Updated DataFrame\n    \"\"\"\n    # Distance between left and right points in pairs of limbs\n    # relative to image size (Euclidean, horizontal and vertical)\n    for point_type in ('elbow', 'wrist', 'knee', 'ankle'):\n        d = np.apply_along_axis(\n            distance, 1, df[[\n                f'left_{point_type}_x', f'left_{point_type}_y',\n                f'right_{point_type}_x', f'right_{point_type}_y'\n            ]].values)\n        df[f'{point_type}s_dist'], df[f'{point_type}s_hor_dist'], \\\n        df[f'{point_type}s_vert_dist'] = d.transpose()\n\n    # Distance between specific keypoint pairs\n    for point_1, point_2 in [('wrist', 'ankle'), ('wrist', 'knee'),\n                             ('wrist', 'hip'), ('wrist', 'elbow'),\n                             ('wrist', 'shoulder'), ('wrist', 'ear'),\n                             ('ankle', 'hip'), ('ankle', 'ear'),\n                             ('elbow', 'knee'), ('knee', 'hip')]:\n        for side_1 in ('left', 'right'):\n            for side_2 in ('left', 'right'):\n                d = np.apply_along_axis(\n                    distance, 1, df[[\n                        f'{side_1}_{point_1}_x', f'{side_1}_{point_1}_y',\n                        f'{side_2}_{point_2}_x', f'{side_2}_{point_2}_y'\n                    ]].values)\n                df[f'{side_1}_{point_1}_{side_2}_{point_2}_dist'], \\\n                df[f'{side_1}_{point_1}_{side_2}_{point_2}_hor_dist'], \\\n                df[f'{side_1}_{point_1}_{side_2}_{point_2}_vert_dist'] = d.transpose()\n\n    # Relative upper \/ lower positions of specific keypoints (binary values: 0\/1)\n    for point_1, point_2 in combinations(['ear', 'hip', 'knee', 'ankle', 'wrist', 'elbow'], 2):\n        for side_1 in ('left', 'right'):\n            for side_2 in ('left', 'right'):\n                df[f'{side_1}_{point_1}_{side_2}_{point_2}'] = np.apply_along_axis(\n                    is_higher, 1, df[[\n                        f'{side_1}_{point_1}_y', f'{side_2}_{point_2}_y'\n                    ]].values)\n\n    if drop_scores:\n        columns = filter(lambda x: x.find('score') == -1, df.columns)\n        df = df[columns]\n\n    print('Positional features added. DataFrame shape:', df.shape)\n\n    return df\n\n\ndef get_model(n_features: int, n_classes, dense_neurons: list) -> tf.keras.Model:\n    \"\"\"Function creates a Densely Connected Neural Network.\n    :param n_features: Number of input features\n    :param n_classes: Number of classes\n    :param dense_neurons: List contains number of neurons in dense layers\n    :return: Compiled tensorflow model\n    \"\"\"\n    model = tf.keras.Sequential(\n        [tf.keras.layers.Input(shape=(n_features,))]\n    )\n\n    for n_neurons in dense_neurons:\n        model.add(tf.keras.layers.Dense(\n            n_neurons, activation='relu',\n            kernel_regularizer=tf.keras.regularizers.l2(0.001))\n        )\n\n    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n    model.summary()\n\n    return model\n\n\ndef train_model(model: tf.keras.Model,\n                train_ds: tf.data.Dataset,\n                val_ds: tf.data.Dataset) -> tf.keras.Model:\n    \"\"\"Function trains the model and evaluates it's performance,\n    displays training metrics.\n    :param model: Untrained model\n    :param train_ds: Training Dataset containing input data and labels\n    :param val_ds: Validation Dataset containing input data and labels\n    :return: Trained model\n    \"\"\"\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                  patience=5,\n                                                  restore_best_weights=True)\n\n    history = model.fit(train_ds, validation_data=val_ds,\n                        epochs=100, verbose=2, callbacks=[early_stop],\n                        use_multiprocessing=True, workers=-1)\n\n    loss, acc = model.evaluate(valid_ds)\n    print(f'Validation loss: {loss}\\nValidation accuracy: {acc}')\n    plot_history(history)\n\n    return model\n\n\ndef plot_history(hist):\n    \"\"\"Function plots a chart with training and validation metrics.\n    :param hist: Tensorflow history object from model.fit()\n    \"\"\"\n    # Losses and metrics\n    loss = hist.history['loss']\n    val_loss = hist.history['val_loss']\n    acc = hist.history['sparse_categorical_accuracy']\n    val_acc = hist.history['val_sparse_categorical_accuracy']\n\n    # Epochs to plot along x axis\n    x_axis = range(1, len(loss) + 1)\n\n    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n\n    ax1.plot(x_axis, loss, 'bo', label='Training')\n    ax1.plot(x_axis, val_loss, 'ro', label='Validation')\n    ax1.set_title('Categorical crossentropy')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    ax2.plot(x_axis, acc, 'bo', label='Training')\n    ax2.plot(x_axis, val_acc, 'ro', label='Validation')\n    ax2.set_title('Categorical accuracy')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()","3e1e536c":"# Create a DataFrame with 3 columns: image path, class name and class label\nimage_paths = glob.glob(f'{data_directory_path}\/*\/*.*')\nclasses_df = pd.DataFrame({'path': list(image_paths)})\nclasses_df['pose'] = classes_df['path'].apply(lambda x: x.split('\/')[-2])\nclasses_df['label'] = LabelEncoder().fit_transform(classes_df['pose'])\nprint(f'DataFrame shape: {classes_df.shape}')\nprint(classes_df.head())\n\n# Balance of classes\nbalance = classes_df['pose'].value_counts()\nbalance.hist(bins=10)\nplt.xlabel('Number of images per class')\nplt.ylabel('Frequency')\nplt.title('Class Distribution')\nplt.show()","01b21a22":"# Display examples of images\ndisplay_images()","b8c7697e":"# Pretrained model for pose classification\nhub_model = hub.load(model_path_light)\nmovenet = hub_model.signatures['serving_default']","a0cd975e":"# Model inference for a single image\nexample_path = classes_df['path'][0]\nexample_image = get_image(example_path)\noutputs = movenet(example_image)\nprint(outputs)","98ddba3f":"# Output is a dictionary with [1, 1, 17, 3] tensor.\n# For each image the model extracts 17 keypoints: y and x coordinates and confidence level.\nkeypoints = outputs['output_0']\nprint(keypoints)","9bd217f4":"# Display keypoints superimposed on the image\ndisplay_keypoints(path=example_path, points=keypoints[0][0])","303986b8":"# The same result can be achieved using inference_fn()\nprint(hub_model.inference_fn(example_image))","2d6b6735":"# We can convert model output into a 1D array with 51 columns\n# representing triplets of values: x, y, confidence\nprint(hub_model.inference_fn(example_image)[0][0].numpy().flatten())","a093433f":"# Inference time for 50 images using inference_fn() and multiprocessing\nstart = time.perf_counter()\nkeypoints = []\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = [executor.submit(movenet_inference_flat, hub_model, path) for path in classes_df['path'].head(50)]\n    for f in tqdm(concurrent.futures.as_completed(results)):\n        keypoints.append(f.result())\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')","4e5900f9":"# Inference time for 50 images using default serving method and multiprocessing\nstart = time.perf_counter()\nkeypoints = []\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = [executor.submit(movenet_serving_flat, movenet, path) for path in classes_df['path'].head(50)]\n    for f in tqdm(concurrent.futures.as_completed(results)):\n        keypoints.append(f.result())\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')","4b29d7c3":"# Inference time for 50 images processed consecutively\nstart = time.perf_counter()\nkeypoints = []\n\nfor path in tqdm(classes_df['path'].head(50)):\n    img = movenet_inference_flat(hub_model, path)\n    keypoints.append(img)\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')","65fa860a":"# Convert all images into keypoints tensors of shape [51,] and create a list\nstart = time.perf_counter()\nkeypoints = []\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = [executor.submit(movenet_inference_flat, hub_model, path) for path in classes_df['path']]\n    for f in tqdm(concurrent.futures.as_completed(results)):\n        keypoints.append(f.result())\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')\n\nkeypoints_df = pd.DataFrame(keypoints)\nprint('Original keypoints extracted. Shape:', keypoints_df.shape)","daef8ceb":"# Repeat this operation for all images flipped left to right.\n# This will double total number of images in all classes.\nstart = time.perf_counter()\nkeypoints_fl = []\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = [executor.submit(movenet_inference_flat, hub_model, path, True) for path in classes_df['path']]\n    for f in tqdm(concurrent.futures.as_completed(results)):\n        keypoints_fl.append(f.result())\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')\n\n# Create a matrix of all keypoints n_samples x 51 columns\nkeypoints_fl = np.vstack([keypoints + keypoints_fl])\nkeypoints_fl_df = pd.DataFrame(keypoints_fl)\nprint('Original and flipped keypoints extracted. Shape:', keypoints_fl_df.shape)","3ede1c9c":"# Rename columns in the DataFrames according to the values\ncolumns = []\nfor point in kp_descriptions:\n    for value in ('y', 'x', 'score'):\n        columns.append(f'{point}_{value}')\n\nkeypoints_df.columns = columns\nkeypoints_fl_df.columns = columns\nprint(keypoints_df.head())","83fb3598":"# Create additional features based on distances between keypoints\n# and their position relative to each other.\n# x and y coordinates are values between 0 and 1\n# (positions relative to image frame).\nkeypoints_df = add_pos_features(keypoints_df, drop_scores=True)\nkeypoints_fl_df = add_pos_features(keypoints_fl_df, drop_scores=True)\nprint(keypoints_df.head())","90ff12da":"# Add target values to the DataFrame with keypoints for original images\nkeypoints_df['label'] = classes_df['label'].values\n\n# Add 2 sets of target values - for original and flipped images\nkeypoints_fl_df['label'] = np.hstack(\n    [classes_df['label'].values, classes_df['label'].values]\n)\n\n# Save the data for future use\nkeypoints_df.to_csv('keypoints.csv', index=False)\nkeypoints_fl_df.to_csv('keypoints_fl.csv', index=False)","f33f9ae7":"# Remove target values from the DataFrame\ny = keypoints_df.pop('label')\ny_fl = keypoints_fl_df.pop('label')\nn_features = len(keypoints_df.columns)\n\n# Split keypoints data and labels into train and validation sets\nx_train, x_valid, y_train, y_valid = train_test_split(\n    keypoints_df, y,\n    stratify=y, test_size=0.2\n)\n\nx_train_fl, x_valid_fl, y_train_fl, y_valid_fl = train_test_split(\n    keypoints_fl_df, y_fl,\n    stratify=y_fl, test_size=0.2\n)","6cdad9fe":"# Create tensorflow datasets from input data and labels\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n\ntrain_ds_fl = tf.data.Dataset.from_tensor_slices((x_train_fl, y_train_fl))\nvalid_ds_fl = tf.data.Dataset.from_tensor_slices((x_valid_fl, y_valid_fl))\n\n# Use prefetch() and cache() to speed up training\ntrain_ds = configure_for_performance(train_ds)\nvalid_ds = configure_for_performance(valid_ds)\n\ntrain_ds_fl = configure_for_performance(train_ds_fl)\nvalid_ds_fl = configure_for_performance(valid_ds_fl)","216e214e":"# Densely connected neural network\nmodel = get_model(\n    n_features=n_features,\n    n_classes=107,\n    dense_neurons=[256, 256, 128]\n)","d0b88167":"# Train the model on original images only\nmodel = train_model(model, train_ds, valid_ds)","ad49868c":"# Train other model on the original and flipped images\nmodel_fl = get_model(\n    n_features=n_features,\n    n_classes=107,\n    dense_neurons=[256, 256, 128]\n)\n\nmodel_fl = train_model(model_fl, train_ds_fl, valid_ds_fl)","a40fb153":"## Create model","70a7a566":"## Functions","a884b607":"Multiprocessing makes inference faster than processing every single image consecutively. Default serving and inference_fn() demonstrate almost identical speed and depending on the run one or another could be slightly faster. We will use inference_fn() for out purpose.","dc5f0ad6":"## Exploratory Data Analysis","4820d507":"## Settings","b5034739":"## Pretrained model inference\nWe will use pretrained model from Tensorflow Hub for feature extraction. For each image the model identifies location of 17 keypoints of a human body relative to the frame of the image (or video). The model was trained on images with only one person moving or standing. Most of the photos in the yoga pose data set are also single person images.\n\nModel is available in two versions:\n- \"Thunder\" version is described as more accurate but slower model\n- \"Lightning\" version is smaller in size and faster for inference","236cfd9f":"Original data contains about 6,000 images belonging to 107 classes. Number of images varies from 18 examples to 90 examples per class. Large number of classes and limited number of images does not allow training neural network from scratch. In this case transfer learning would be optimal approach: we need to find ready-to-use model that was trained for a similar task and tune it for the task at hand.","1e92fe9b":"# Yoga Poses Classifier\n\nImage classification model for yoga poses based on transfer learning techniques.\n\nAlgorithm:\n- Identify coordinates of 17 keypoints in the image using pretrained model\n- Produce new features based on distance between keypoints and their relative positions\n- Train densely connected neural network on keypoint coordinates and position features\n\nTo account for small number of images and class imbalance training is performed on original images and then repeated on original flipped images.\n\nPretrained models:\n- https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/3 - larger model for higher accuracy\n- https:\/\/tfhub.dev\/google\/movenet\/singlepose\/lightning\/3 - smaller model for faster inference\n\nPretrained model inputs: frame of video or an image represented as an int32 tensor of shape:\n- 256 x 256 x 3 for larger model (\"Thunder\")\n- 192 x 192 x 3 for smaller model (\"Lightning\")\n\nChannels order: RGB with values in range 0-255.\n\nOutputs: float32 tensor of shape (1, 1, 17, 3).\n\nThe first two channels of the last dimension represents the y and x coordinates normalized to image frame, i.e. range in (0.0, 1.0) of the 17 keypoints in the order of: nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle. The third channel of the last dimension represents the prediction confidence scores of each keypoint, also in the range (0.0, 1.0).","e6a00ce8":"## Creating Datasets\n\nWe create 2 sets of train and validation data:\n- Datasets based on original images (highly imbalanced classes)\n- Datasets based on original images and images flipped left to right","4661df77":"We have about 6,000 images in the data set. Converting all the images into keypoint tensors could take considerable time. Developers claim that the inference capacity of \"Lightning\" version is >50FPS. We will check actual speed of inference using default serving method and inference_fn() and also use multiprocessing."}}