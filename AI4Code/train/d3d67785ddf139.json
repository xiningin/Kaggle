{"cell_type":{"582c669e":"code","da0a6a1b":"code","2670b273":"code","cc7466fe":"code","77e78942":"code","6a488a92":"code","cabb4103":"code","eb341fb7":"code","fdf2d592":"code","46f08639":"code","6368fdc0":"markdown"},"source":{"582c669e":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding","da0a6a1b":"# define documents\ndocs = ['Well done!',\n        'Good work',\n        'Great effort',\n        'nice work',\n        'Excellent!',\n        'Weak',\n        'Poor effort!',\n        'not good',\n        'poor work',\n        'Could have done better.']","2670b273":"# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])","cc7466fe":"# integer encode the documents\nvocab_size = 500\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)","77e78942":"# pad documents to a max length of 4 words\nmax_length = 8\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\nprint(padded_docs)","6a488a92":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 10, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","cabb4103":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","eb341fb7":"# summarize the model\nprint(model.summary())","fdf2d592":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)","46f08639":"# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","6368fdc0":"Created based on Jason Brownlee's post:\nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/"}}