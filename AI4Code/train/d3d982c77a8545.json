{"cell_type":{"d2e0a39d":"code","05e0d60e":"code","b1393adb":"code","828b6e2b":"code","175fe1c1":"code","adaa4d62":"code","fa61004c":"code","c31beb13":"code","41aaf5f8":"code","35499b62":"code","d4634733":"code","a0c849e8":"code","a21d72f3":"code","8f9f2c5d":"code","655798c3":"code","aedfb193":"code","04022b71":"code","efa877f6":"code","53e5405f":"code","b96ff737":"code","e82c5de2":"code","d3e1cb60":"code","34c22f57":"code","9d777ae7":"code","cfd119e0":"code","dc9c01f8":"code","4bc1a8d0":"code","a2a813db":"code","a182f705":"code","83db0d1c":"code","11b446a2":"code","f94d434e":"code","6264c350":"code","202b8155":"code","23fa3c09":"markdown","336c0f6c":"markdown","1542b838":"markdown","9106601d":"markdown","f94e20f3":"markdown","f64d5e63":"markdown","4c070854":"markdown","c2b04f44":"markdown","866e0068":"markdown"},"source":{"d2e0a39d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05e0d60e":"import matplotlib.pyplot as plt \nimport seaborn as sns\nimport random\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\nimport tensorflow as tf \nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport os\nimport sys\n\nsys.path.append(\"..\/input\/iterstrat\/\")\nsys.path.append(\"..\/input\/rank-gauss\/\")\n\nfrom sklearn.metrics import log_loss","b1393adb":"!pip install ..\/input\/iterstrat\/iterative_stratification-0.1.6-py3-none-any.whl","828b6e2b":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","175fe1c1":"def seedAll(seed_value = 42):\n    np.random.seed(seed_value)\n    random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n\nseedAll(seed_value=42)","adaa4d62":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')","fa61004c":"GENES = [col for col in train_features if col.startswith('g-')]\nCELLS = [col for col in train_features if col.startswith('c-')]\nTARGETS = train_targets_scored.columns[1:]","c31beb13":"for col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","41aaf5f8":"def build(shape=None,out_cols=206):\n    model = tf.keras.models.Sequential([\n                L.InputLayer(input_shape=shape),\n                L.BatchNormalization(),\n                L.Dropout(0.3),\n                tfa.layers.WeightNormalization(L.Dense(480,kernel_initializer=\"he_normal\")),\n                L.BatchNormalization(),\n                L.Activation(tf.nn.leaky_relu),\n                L.Dropout(0.4),\n                tfa.layers.WeightNormalization(L.Dense(256,kernel_initializer=\"he_normal\")),\n                L.BatchNormalization(),\n                L.Activation(tf.nn.leaky_relu),\n                L.Dropout(0.2),\n                tfa.layers.WeightNormalization(L.Dense(out_cols,activation=\"sigmoid\",kernel_initializer=\"he_normal\"))\n            ])\n    model.compile(loss=\"binary_crossentropy\",optimizer = tfa.optimizers.AdamW(lr=0.001,weight_decay=1e-4),metrics = [\"binary_crossentropy\"])\n    \n    return model\n\ndef metric(y_true,y_predicted):\n\n    metrics=[]\n    for col in range(y_true.shape[1]):\n        metrics.append(log_loss(y_true[:,col],y_predicted[:,col],labels=[0,1]))\n\n    return np.mean(metrics)\n\ndef transfer_weight(model_source,model_dest):\n    for i in range(len(model_source.layers[:-1])):\n        model_dest.layers[i].set_weights(model_source.layers[i].get_weights())\n    return model_dest","35499b62":"def prepare_data(df,targets=train_targets_scored,test=False):\n    #df = df.drop('sig_id',axis=1)\n    if(not test):\n        targets = targets.iloc[df[df.cp_type==\"trt_cp\"].index,:]\n        #targets = targets.drop('sig_id',axis=1)\n        df = df[df.cp_type==\"trt_cp\"]\n        \n    df = df.drop('cp_type',axis=1)\n    df = pd.concat([pd.get_dummies(df[\"cp_time\"],drop_first=True),df.drop('cp_time',axis=1)],axis=1)\n    df[\"cp_dose\"] = df[\"cp_dose\"].map({'D1':0,'D2':1})\n        \n    if(not test):\n        return df,targets\n    return df","d4634733":"train_data,train_targets = prepare_data(train_features,test=False)\n\ntest_data = prepare_data(test_features,test=True)","a0c849e8":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","a21d72f3":"features = ['sig_id',48, 72, 'cp_dose', 'g-0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5', 'g-6', 'g-8', 'g-10', 'g-11', 'g-12', 'g-13', 'g-14', 'g-15', 'g-16', 'g-17', 'g-18', 'g-20', 'g-21', 'g-22', 'g-24', 'g-25', 'g-27', 'g-28', 'g-29', 'g-30', 'g-32', 'g-33', 'g-34', 'g-35', 'g-36', 'g-37', 'g-38', 'g-40', 'g-41', 'g-42', 'g-44', 'g-45', 'g-46', 'g-47', 'g-48', 'g-49', 'g-50', 'g-51', 'g-52', 'g-53', 'g-55', 'g-56', 'g-57', 'g-59', 'g-60', 'g-61', 'g-62', 'g-63', 'g-64', 'g-65', 'g-66', 'g-68', 'g-69', 'g-73', 'g-75', 'g-76', 'g-77', 'g-78', 'g-79', 'g-81', 'g-82', 'g-84', 'g-89', 'g-90', 'g-91', 'g-93', 'g-94', 'g-95', 'g-96', 'g-97', 'g-98', 'g-99', 'g-100', 'g-101', 'g-102', 'g-103', 'g-105', 'g-107', 'g-108', 'g-110', 'g-111', 'g-113', 'g-114', 'g-116', 'g-117', 'g-119', 'g-120', 'g-122', 'g-123', 'g-124', 'g-125', 'g-126', 'g-127', 'g-128', 'g-130', 'g-131', 'g-133', 'g-134', 'g-136', 'g-137', 'g-140', 'g-141', 'g-142', 'g-143', 'g-144', 'g-145', 'g-146', 'g-148', 'g-149', 'g-150', 'g-151', 'g-153', 'g-156', 'g-157', 'g-158', 'g-159', 'g-160', 'g-163', 'g-164', 'g-166', 'g-167', 'g-169', 'g-172', 'g-173', 'g-174', 'g-175', 'g-176', 'g-177', 'g-178', 'g-180', 'g-183', 'g-184', 'g-185', 'g-186', 'g-187', 'g-188', 'g-190', 'g-191', 'g-192', 'g-194', 'g-195', 'g-198', 'g-200', 'g-201', 'g-202', 'g-203', 'g-204', 'g-205', 'g-206', 'g-207', 'g-208', 'g-209', 'g-212', 'g-213', 'g-214', 'g-216', 'g-217', 'g-218', 'g-221', 'g-222', 'g-223', 'g-224', 'g-226', 'g-228', 'g-229', 'g-230', 'g-231', 'g-232', 'g-233', 'g-234', 'g-235', 'g-240', 'g-241', 'g-242', 'g-243', 'g-244', 'g-245', 'g-246', 'g-248', 'g-250', 'g-251', 'g-252', 'g-253', 'g-254', 'g-256', 'g-258', 'g-260', 'g-262', 'g-263', 'g-264', 'g-265', 'g-267', 'g-268', 'g-269', 'g-271', 'g-272', 'g-273', 'g-274', 'g-275', 'g-277', 'g-278', 'g-279', 'g-280', 'g-281', 'g-282', 'g-283', 'g-284', 'g-287', 'g-288', 'g-289', 'g-291', 'g-292', 'g-293', 'g-294', 'g-296', 'g-297', 'g-298', 'g-299', 'g-300', 'g-301', 'g-302', 'g-303', 'g-304', 'g-305', 'g-306', 'g-309', 'g-311', 'g-312', 'g-313', 'g-314', 'g-315', 'g-317', 'g-318', 'g-321', 'g-322', 'g-323', 'g-324', 'g-325', 'g-329', 'g-330', 'g-331', 'g-332', 'g-333', 'g-334', 'g-336', 'g-337', 'g-340', 'g-341', 'g-342', 'g-343', 'g-344', 'g-345', 'g-347', 'g-348', 'g-350', 'g-351', 'g-352', 'g-353', 'g-354', 'g-355', 'g-356', 'g-357', 'g-358', 'g-359', 'g-360', 'g-362', 'g-363', 'g-365', 'g-369', 'g-370', 'g-371', 'g-372', 'g-373', 'g-375', 'g-376', 'g-377', 'g-378', 'g-379', 'g-380', 'g-381', 'g-382', 'g-383', 'g-384', 'g-386', 'g-387', 'g-389', 'g-390', 'g-391', 'g-392', 'g-394', 'g-395', 'g-396', 'g-397', 'g-398', 'g-399', 'g-400', 'g-401', 'g-402', 'g-403', 'g-404', 'g-405', 'g-406', 'g-407', 'g-408', 'g-409', 'g-410', 'g-411', 'g-414', 'g-415', 'g-418', 'g-421', 'g-422', 'g-423', 'g-424', 'g-425', 'g-427', 'g-429', 'g-430', 'g-431', 'g-432', 'g-433', 'g-434', 'g-435', 'g-436', 'g-437', 'g-438', 'g-439', 'g-440', 'g-441', 'g-442', 'g-443', 'g-444', 'g-445', 'g-446', 'g-447', 'g-449', 'g-450', 'g-451', 'g-453', 'g-454', 'g-455', 'g-456', 'g-457', 'g-458', 'g-459', 'g-460', 'g-461', 'g-462', 'g-463', 'g-464', 'g-465', 'g-466', 'g-468', 'g-469', 'g-471', 'g-472', 'g-473', 'g-476', 'g-478', 'g-479', 'g-480', 'g-481', 'g-482', 'g-483', 'g-485', 'g-486', 'g-487', 'g-489', 'g-490', 'g-491', 'g-492', 'g-493', 'g-494', 'g-495', 'g-497', 'g-499', 'g-500', 'g-503', 'g-505', 'g-506', 'g-507', 'g-509', 'g-511', 'g-512', 'g-513', 'g-515', 'g-516', 'g-518', 'g-519', 'g-520', 'g-521', 'g-522', 'g-524', 'g-525', 'g-526', 'g-527', 'g-528', 'g-530', 'g-533', 'g-534', 'g-536', 'g-537', 'g-538', 'g-539', 'g-540', 'g-541', 'g-543', 'g-544', 'g-547', 'g-550', 'g-551', 'g-552', 'g-553', 'g-554', 'g-555', 'g-556', 'g-557', 'g-558', 'g-559', 'g-560', 'g-561', 'g-562', 'g-563', 'g-564', 'g-565', 'g-566', 'g-567', 'g-569', 'g-570', 'g-571', 'g-572', 'g-573', 'g-575', 'g-578', 'g-581', 'g-583', 'g-584', 'g-585', 'g-586', 'g-587', 'g-588', 'g-589', 'g-590', 'g-594', 'g-596', 'g-597', 'g-598', 'g-599', 'g-600', 'g-601', 'g-602', 'g-603', 'g-604', 'g-605', 'g-606', 'g-608', 'g-610', 'g-611', 'g-612', 'g-613', 'g-614', 'g-615', 'g-616', 'g-617', 'g-618', 'g-619', 'g-620', 'g-621', 'g-623', 'g-624', 'g-625', 'g-626', 'g-627', 'g-628', 'g-630', 'g-631', 'g-633', 'g-634', 'g-635', 'g-636', 'g-638', 'g-639', 'g-640', 'g-643', 'g-645', 'g-648', 'g-649', 'g-651', 'g-655', 'g-656', 'g-657', 'g-658', 'g-659', 'g-661', 'g-663', 'g-664', 'g-665', 'g-666', 'g-667', 'g-668', 'g-669', 'g-670', 'g-671', 'g-672', 'g-674', 'g-676', 'g-677', 'g-678', 'g-679', 'g-680', 'g-681', 'g-682', 'g-683', 'g-688', 'g-689', 'g-690', 'g-691', 'g-692', 'g-694', 'g-696', 'g-697', 'g-698', 'g-699', 'g-701', 'g-703', 'g-704', 'g-705', 'g-707', 'g-709', 'g-710', 'g-711', 'g-712', 'g-713', 'g-714', 'g-715', 'g-716', 'g-718', 'g-719', 'g-720', 'g-721', 'g-722', 'g-723', 'g-724', 'g-725', 'g-726', 'g-727', 'g-729', 'g-730', 'g-731', 'g-732', 'g-733', 'g-734', 'g-735', 'g-736', 'g-737', 'g-741', 'g-742', 'g-745', 'g-747', 'g-752', 'g-753', 'g-754', 'g-755', 'g-757', 'g-758', 'g-759', 'g-761', 'g-763', 'g-764', 'g-765', 'g-766', 'g-767', 'g-768', 'g-769', 'g-770', 'g-771', 'c-1', 'c-3', 'c-4', 'c-7', 'c-8', 'c-9', 'c-12', 'c-13', 'c-14', 'c-15', 'c-16', 'c-17', 'c-18', 'c-21', 'c-22', 'c-23', 'c-24', 'c-25', 'c-26', 'c-27', 'c-29', 'c-30', 'c-31', 'c-34', 'c-35', 'c-37', 'c-38', 'c-39', 'c-40', 'c-41', 'c-42', 'c-44', 'c-46', 'c-47', 'c-48', 'c-49', 'c-50', 'c-51', 'c-54', 'c-56', 'c-57', 'c-58', 'c-59', 'c-61', 'c-62', 'c-64', 'c-65', 'c-66', 'c-67', 'c-69', 'c-73', 'c-76', 'c-77', 'c-79', 'c-81', 'c-82', 'c-86', 'c-87', 'c-89', 'c-91', 'c-93', 'c-94', 'c-95', 'c-98', 'c-99']","8f9f2c5d":"train = train_data.loc[:,features]\ntest = test_data.loc[:,features]","655798c3":"train.head()","aedfb193":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\na ,b=fe_cluster(train_features,test_features)\ncols = [col for col in a.columns if col.startswith('clusters')]\n\ntrain = pd.concat([train,a.loc[train_features[train_features.cp_type==\"trt_cp\"].index,cols]],axis=1)\ntest = pd.concat([test,b.loc[:,cols]],axis=1)","04022b71":"def fe_stats(train, test):\n    train_ = train.copy()\n    test_ = test.copy()\n    features_g = list(train_features.columns[4:776])\n    features_c = list(train_features.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train.iloc[:,-15:], test.iloc[:,-15:]\n\na,b = fe_stats(train_features,test_features)\n","efa877f6":"train = pd.concat((train,a.loc[train_features[train_features.cp_type==\"trt_cp\"].index,:]),axis=1)\ntest = pd.concat((test,b),axis=1)","53e5405f":"n_comps = 600\npca = PCA(n_components = n_comps)\n\ntotal_data = train_features.append(test_features)\ntotal_data = total_data.loc[:,GENES]\n\npca.fit(total_data)\ntrain2 = pca.transform(train_features.loc[train_features[train_features.cp_type==\"trt_cp\"].index,GENES])\ntest2 = pca.transform(test_features.loc[:,GENES])\n\ntrain2 = pd.DataFrame(train2,columns=[f'pca-g_{i}' for i in range(train2.shape[1])])\ntest2 = pd.DataFrame(test2,columns=[f'pca-g_{i}' for i in range(test2.shape[1])])\n\ntotal = train2.append(test2)\nvt = VarianceThreshold(0.8)\nvt.fit(total)\n\ntrain2 = vt.transform(train2)\ntest2 = vt.transform(test2)\n\ntrain2 = pd.DataFrame(train2,columns=[f'pca-g_{i}' for i in range(train2.shape[1])])\ntest2 = pd.DataFrame(test2,columns=[f'pca-g_{i}' for i in range(test2.shape[1])])\n\ntrain = pd.concat((train.reset_index(drop=True),train2),axis=1)\ntest = pd.concat((test,test2),axis=1)","b96ff737":"n_comps = 50\npca = PCA(n_components = n_comps)\n\ntotal_data = train_features.append(test_features)\ntotal_data = total_data.loc[:,CELLS]\n\npca.fit(total_data)\ntrain2 = pca.transform(train_features.loc[train_features[train_features.cp_type==\"trt_cp\"].index,CELLS])\ntest2 = pca.transform(test_features.loc[:,CELLS])\n\ntrain2 = pd.DataFrame(train2,columns=[f'pca-c_{i}' for i in range(train2.shape[1])])\ntest2 = pd.DataFrame(test2,columns=[f'pca-c_{i}' for i in range(test2.shape[1])])\n\ntotal = train2.append(test2)\nvt = VarianceThreshold(0.8)\nvt.fit(total)\n\ntrain2 = vt.transform(train2)\ntest2 = vt.transform(test2)\n\ntrain2 = pd.DataFrame(train2,columns=[f'pca-c_{i}' for i in range(train2.shape[1])])\ntest2 = pd.DataFrame(test2,columns=[f'pca-c_{i}' for i in range(test2.shape[1])])\n\ntrain = pd.concat((train.reset_index(drop=True),train2),axis=1)\ntest = pd.concat((test,test2),axis=1)","e82c5de2":"drug_id = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\n\ndrug_id.head()","d3e1cb60":"features = train.columns[1:]","34c22f57":"train = train.merge(drug_id,on=\"sig_id\",how=\"inner\")\ntrain.head()","9d777ae7":"!pip install ..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl","cfd119e0":"from pytorch_tabnet.tab_model import TabNetRegressor\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch\nfrom pytorch_tabnet.metrics import Metric","dc9c01f8":"MAX_EPOCH = 100\n# n_d and n_a are different from the original work, 32 instead of 24\n# This is the first change in the code from the original\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = 42,\n    verbose = 10\n)","4bc1a8d0":"def torch_seed(seed=42):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","a2a813db":"def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id,seed in enumerate(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        torch_seed(seed)\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True)\n        tmp = train.groupby('drug_id')[TARGETS].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[TARGETS])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[TARGETS])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nDRUG_THRESH = 18\nSEEDS = [42,58,132,456,789]\nNFOLDS =7\n\ntrain_df = make_cv_folds(train.merge(train_targets_scored,on=\"sig_id\",how=\"inner\"), SEEDS, NFOLDS, DRUG_THRESH)\ntrain_df.head()","a182f705":"ss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss.loc[:,'sig_id'] = test_features['sig_id'].values\nss.iloc[:,1:]=0","83db0d1c":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","11b446a2":"histories = []\nscores = []\n\nfor idx,seed in enumerate(SEEDS):\n    torch_seed(seed=seed)\n    print(f\"Training seed {seed}\")\n    print('='*50)\n    \n    for fold in range(7):\n        print(f'\\nFold {idx}')\n        print('-'*50)\n        \n        X_train = train_df[train_df[f'kfold_{idx}']!=fold].loc[:,features]\n        Y_train = train_df[train_df[f'kfold_{idx}']!=fold].loc[:,TARGETS]\n        X_val = train_df[train_df[f'kfold_{idx}']==fold].loc[:,features]\n        Y_val = train_df[train_df[f'kfold_{idx}']==fold].loc[:,TARGETS]\n        \n        \n        model = TabNetRegressor(**tabnet_params)\n        \n        model.fit(\n            X_train = X_train.values,\n            y_train = Y_train.values.squeeze(),\n            eval_set = [(X_val.values, Y_val.values.squeeze())],\n            eval_name = [\"val\"],\n            eval_metric = [\"logits_ll\"],\n            max_epochs = MAX_EPOCH,\n            patience = 20,\n            batch_size = 1024, \n            virtual_batch_size = 32,\n            num_workers = 1,\n            drop_last = False,\n            # To use binary cross entropy because this is not a regression problem\n            loss_fn = F.binary_cross_entropy_with_logits\n        )\n          \n        histories.append(model.history)\n        \n        val_pred = model.predict(X_val.values)\n        val_pred = 1\/(1+np.exp(-val_pred))\n        \n        score = metric(Y_val.values,val_pred)\n        scores.append(score)\n        \n        print(f\"Validation Score: {score}\")\n        pred = model.predict(test.loc[:,features].values)\n        pred = 1\/(1+np.exp(-pred))\n        \n        ss.loc[:,TARGETS]+= pred\n        torch.save(model,f'.\/model.{idx}_{fold}.pt')","f94d434e":"ss.loc[:,TARGETS] \/= 7*len(SEEDS)\nss.loc[test_features[test_features.cp_type=='ctl_vehicle'].index,TARGETS] = 0","6264c350":"print(f'validation score : {np.mean(scores)}')\nplt.figure()\n\nfor history in histories:\n    plt.plot(history[\"val_logits_ll\"],color='red')\n    plt.plot(history[\"loss\"],color=\"green\")\nplt.title(\"Training Curve\")","202b8155":"ss.to_csv('.\/submission.csv',index=False)","23fa3c09":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#gauss-rank\" role=\"tab\" aria-controls=\"profile\">Applying GaussRankScaler<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#model-data\" role=\"tab\" aria-controls=\"profile\">Model Architecture and Data Preparation<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#feat_selection\" role=\"tab\" aria-controls=\"messages\">Feature selection<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#feat-eng\" role=\"tab\" aria-controls=\"messages\">Feature Engineering<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#transfer-model\" role=\"tab\" aria-controls=\"settings\">Training Model on non_scored_targets for transfer<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#feature-model\" role=\"tab\" aria-controls=\"settings\">Training Model with Transfered weights<span class=\"badge badge-primary badge-pill\">6<\/span><\/a> ","336c0f6c":"## Genes","1542b838":"# <a id=\"feature-model\"> Training Model with Transfered Weights<\/a>","9106601d":"# The Idea\n The Basic idea behind this notebook is to train a model on the non_scored_targets and then train a model on the scored targets using these weights. The models are trained with features selected using `permutation importance` algorithm. `The implementation of this algorithm is included in this notebook in the form of a class.`<hr>\n <a style=\"color:green\">The feature engineering ideas are adopted from<\/a> [this](https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-feature-engineering-0-01846) <a style=\"color:green\"> notebook by <\/a>[@kushal1506](https:\/\/www.kaggle.com\/kushal1506)","f94e20f3":"# <a>PCA<\/a>","f64d5e63":"## <a id=\"gauss-rank\"> Applying RankGauss<\/a>","4c070854":"## <a id=\"model-data\"> Model Architecture and Data Preparation<\/a>","c2b04f44":"# <a id=\"feat-eng\">Feature Engineering<\/a>\n## Ideas\n<ul>\n<li>The gene features and cell features in the original set( without selection) are clustered in groups of 35 and 5 respectively. The cluster label is taken as a new feature<\/li>\n<li> Statistical information about the gene and cell features are included as new feautures. This includes:<hr>\n    <ol>\n        <li>sum of the gene features, sum of the cell features, and sum of gene and cell features combined.<\/li>\n        <li>mean of the gene features, mean of the cell features, and mean of gene and cell features combined.<\/li>\n        <li>std of the gene features, std of the cell features, and std of gene and cell features combined.<\/li>\n        <li>skewness of the gene features, skewness of the cell features, and skewness of gene and cell features combined.<\/li>\n        <li>kurtosis of the gene features, kurtosis of the cell features, and kurtosis of gene and cell features combined.<\/li>\n    <\/ol>\n<\/li>\n<\/ul>","866e0068":"# <a id=\"feat_selection\"> Feature Selection<\/a>\nIn the following block of code, A model is defined and trained on the training set. After training a a feature selection algorithm, PermutationImportance  is run on the validation set using that model.<br><a style=\"color:green;\"> PermutationImportance has been implemented from scratch in the class written here.<\/a><br>\n<br>\n<pre>\n\nX_train,X_val,Y_train,Y_val = train_test_split(train_data,train_targets,test_size=0.2,random_state=101)\n\nmodel = build((X_train.shape[1],))\nmodel.summary()\n\nsave_weight = ModelCheckpoint('model.learned.hdf5',save_best_only=True,save_weights_only=True,monitor = 'val_loss',mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\nearly = EarlyStopping(monitor='val_loss',patience=5,mode='min')\n\nmodel.fit(X_train,Y_train,\n         epochs=30,\n         validation_data = (X_val,Y_val),\n         batch_size=128,\n         callbacks = [early,reduce_lr_loss,save_weight])\n\nmodel.load_weights('model.learned.hdf5')\n<\/pre>\n<pre>\nclass PermutationImportance:\n\n    def __init__(self,estimator,threshold):\n\n        self.model = estimator\n        self.threshold = threshold\n        pd.options.mode.chained_assignment = None\n       \n\n    def metric(self,y_true,y_predicted):\n\n        metrics=[]\n        for col in range(y_true.shape[1]):\n            metrics.append(log_loss(y_true[:,col],y_predicted[:,col],labels=[0,1]))\n\n        return np.mean(metrics)\n\n        \n\n    def fit(self,data_i,targets):\n\n        data = data_i.copy()\n        l = len(data.columns)\n\n        features = []\n\n        base_predictions = self.model.predict(data.values)\n        base_loss = self.metric(targets.values,base_predictions)\n\n        for idx,col in enumerate(data.columns):\n    \n            original = data.iloc[:,idx].copy()\n            suffle = np.copy(original.values)\n            np.random.shuffle(suffle)\n            \n            data.iloc[:,idx] = suffle            \n\n            predictions = self.model.predict_proba(data.values)\n\n            loss = self.metric(targets.values,predictions)\n\n            if loss>base_loss+self.threshold:\n\n                features.append(col)\n\n\n\n            data.iloc[:,idx] = original.values\n            \n            print(f'{(idx+1)*100\/l}  %\\r',end='',flush=True)\n\n        return features\n\n\npi = PermutationImportance(model,0.00000001)\nfeatures = pi.fit(X_val,Y_val)\n<\/pre>"}}