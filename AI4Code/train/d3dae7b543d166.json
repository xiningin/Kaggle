{"cell_type":{"6ac0a315":"code","0799c46f":"code","8a3ffcf7":"code","8f13e9ed":"code","320e7498":"code","0e8c4f79":"code","03be2666":"code","a071c6a4":"code","658de793":"code","e5ad27f1":"code","39624c32":"code","98a874b5":"code","d161ec79":"code","eb1a349b":"code","6f457932":"code","04fd1df5":"code","8cdea89f":"code","107b165f":"code","d547feb9":"code","1df77a59":"code","b73c4706":"code","8d8ca54e":"code","8e74dff8":"code","e201a4b8":"code","fa0989aa":"code","c880e123":"code","3ef8731c":"code","5c58589d":"code","27cd5e8f":"code","5bb354be":"code","b5b9cd49":"code","17f8ff50":"code","b24b5a29":"code","162bc6fc":"code","4401ebc7":"code","1d9ccbf0":"code","317c88a4":"code","114c0318":"code","58599989":"code","7a14faed":"code","c627655e":"code","4f2b03a8":"code","ed81000d":"code","e1e41b47":"code","41127334":"code","f90dfb07":"code","8aa6c14b":"code","2d6aac39":"code","67e8a9ee":"code","3f386dfe":"code","acf689a1":"code","b9c505d0":"code","7987aaed":"code","924eac29":"code","3bbfc014":"code","23c0b44d":"code","b2b3cffd":"code","b4515f51":"code","70bf59ee":"code","4079f3bc":"code","fcc4b53c":"code","3d6e029c":"code","3884c377":"code","59995251":"code","83885ab9":"code","c441ed0e":"code","d803826b":"code","d8647dd7":"code","9e7fbbfc":"markdown","a1c58bce":"markdown","1375f70b":"markdown","af1eef5a":"markdown"},"source":{"6ac0a315":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n","0799c46f":"columns = ['count', 'season', 'holiday', 'workingday', 'weather', 'temp',\n       'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']\n\ncols_normalize = ['temp','atemp','humidity','windspeed']","8a3ffcf7":"df = pd.read_csv('..\/input\/bike-sharing-demand\/train.csv',parse_dates=['datetime'])\ndf_test = pd.read_csv('..\/input\/bike-sharing-demand\/test.csv',parse_dates=['datetime'])","8f13e9ed":"df.head()","320e7498":"# We need to convert datetime to numeric for training.\n# Let's extract key features into separate numeric columns\ndef add_features(df):\n    df['year'] = df['datetime'].dt.year\n    df['month'] = df['datetime'].dt.month\n    df['day'] = df['datetime'].dt.day\n    df['dayofweek'] = df['datetime'].dt.dayofweek\n    df['hour'] = df['datetime'].dt.hour","0e8c4f79":"add_features(df)\nadd_features(df_test)","03be2666":"df[\"count\"] = df[\"count\"].map(np.log1p)","a071c6a4":"df.head(2)","658de793":"df_test.head(2)","e5ad27f1":"# Normalize the dataset\ntransformer = Normalizer()","39624c32":"# Normalization parameters based on Training\ntransformer.fit(df[cols_normalize])","98a874b5":"def transform_data(scaler, df, columns):\n    transformed_data = scaler.transform(df[columns])\n    df_transformed = pd.DataFrame(transformed_data, columns=columns)\n    \n    for col in df_transformed.columns:\n        df[col] = df_transformed[col]","d161ec79":"transform_data(transformer, df, cols_normalize)\ntransform_data(transformer, df_test, cols_normalize)","eb1a349b":"df.head(2)","6f457932":"df_test.head(2)","04fd1df5":"# Store Original train and test data in normalized form\ndf.to_csv('train_normalized.csv',index=False, columns=columns)\ndf_test.to_csv('test_normalized.csv',index=False)","8cdea89f":"df = pd.read_csv('.\/train_normalized.csv')\ndf_test = pd.read_csv('.\/test_normalized.csv')","107b165f":"df.head(2)","d547feb9":"df_test.head(2)","1df77a59":"# We are not going to use numeric features: 'temp','atemp','humidity','windspeed'\n# Instead, we are going to use new components (aka features) generated by PCA for model training and testing\ncolumns = ['count', 'season', 'holiday', 'workingday', 'weather','year', 'month', 'day', 'dayofweek','hour']\n\n# PCA Training\ncolums_for_pca = ['temp','atemp','humidity','windspeed']","b73c4706":"# Find PCA\npca = PCA(n_components=0.9) # Capture 90% total variation","8d8ca54e":"# Find new components\npca.fit(df[colums_for_pca])","8e74dff8":"# No. of PCA Components\nprint ('Variance: ', pca.n_components)\nprint ('No. of components to keep: ', pca.n_components_)","e201a4b8":"def transform_with_pca(pca, df, columns):\n    transformed_data = pca.transform(df[columns])\n    \n    tcols = []\n    for i in range(pca.n_components_):       \n        tcols.append('component_' + str(i))\n    \n    print ('components:',tcols)\n    df_transformed = pd.DataFrame(transformed_data, columns=tcols)\n    \n    for col in df_transformed.columns:\n        df[col] = df_transformed[col]\n    \n    df.drop(columns, inplace=True, axis=1)\n    \n    return tcols","fa0989aa":"new_cols = transform_with_pca(pca, df, colums_for_pca)","c880e123":"transform_with_pca(pca, df_test, colums_for_pca)","3ef8731c":"df.head(2)","5c58589d":"df_test.head(2)","27cd5e8f":"for col in new_cols:\n    columns.append(col)","5bb354be":"columns","b5b9cd49":"# Training = 70% of the data\n# Validation = 30% of the data\n# Randomize the datset\nnp.random.seed(5)\nl = list(df.index)\nnp.random.shuffle(l)\ndf = df.iloc[l]","17f8ff50":"rows = df.shape[0]\ntrain = int(.7 * rows)\ntest = int(.3 * rows)","b24b5a29":"rows, train, test","162bc6fc":"columns","4401ebc7":"# Write Training Set\ndf[:train].to_csv('bike_train_pca.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","1d9ccbf0":"# Write Validation Set\ndf[train:].to_csv('bike_validation_pca.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","317c88a4":"# Test Data has only input features\ndf_test.to_csv('bike_test_pca.csv',index=False)","114c0318":"# Write Column List\nwith open('bike_train_column_list_pca.txt','w') as f:\n    f.write(','.join(columns))","58599989":"# Install xgboost in notebook instance.\n#### Command to install xgboost\n!pip install xgboost==0.90","7a14faed":"%matplotlib inline\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb","c627655e":"column_list_file = 'bike_train_column_list_pca.txt'\ntrain_file = 'bike_train_pca.csv'\nvalidation_file = 'bike_validation_pca.csv'\ntest_file = 'bike_test_pca.csv'","4f2b03a8":"columns = ''\nwith open(column_list_file,'r') as f:\n    columns = f.read().split(',')","ed81000d":"columns","e1e41b47":"# Specify the column names as the file does not have column header\ndf_train = pd.read_csv(train_file,names=columns)\ndf_validation = pd.read_csv(validation_file,names=columns)","41127334":"df_train.head(2)","f90dfb07":"df_validation.head(2)","8aa6c14b":"df_train.iloc[:,1:-2].head(2)","2d6aac39":"X_train = df_train.iloc[:,1:] # Features: 1st column onwards \ny_train = df_train.iloc[:,0].ravel() # Target: 0th column\n\nX_validation = df_validation.iloc[:,1:]\ny_validation = df_validation.iloc[:,0].ravel()","67e8a9ee":"# XGBoost Training Parameter Reference: \n#   https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.md\nregressor = xgb.XGBRegressor(max_depth=5,eta=0.1,subsample=0.7,num_round=150,n_estimators=150)","3f386dfe":"regressor","acf689a1":"regressor.fit(X_train,y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])","b9c505d0":"eval_result = regressor.evals_result()","7987aaed":"training_rounds = range(len(eval_result['validation_0']['rmse']))","924eac29":"plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\nplt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\nplt.grid(True)\nplt.xlabel('Iteration')\nplt.ylabel('RMSE')\nplt.title('Training Vs Validation Error')\nplt.legend()","3bbfc014":"xgb.plot_importance(regressor)","23c0b44d":"def adjust_count(x):\n    if x < 0:\n        return 0\n    else:\n        return x","b2b3cffd":"# Prepare Data for Submission to Kaggle\ndf_test = pd.read_csv(test_file,parse_dates=['datetime'])","b4515f51":"df_test.head(2)","70bf59ee":"X_test =  df_test.iloc[:,1:] # Exclude datetime for prediction","4079f3bc":"X_test.head(2)","fcc4b53c":"result = regressor.predict(X_test)","3d6e029c":"result[:5]","3884c377":"# Convert result to actual count\ndf_test[\"count\"] = np.expm1(result)","59995251":"df_test.head()","83885ab9":"df_test[df_test[\"count\"] < 0]","c441ed0e":"df_test['count'] = df_test['count'].map(adjust_count)","d803826b":"df_test[df_test[\"count\"] < 0]","d8647dd7":"df_test[['datetime','count']].to_csv('predicted_count_pca.csv',index=False)","9e7fbbfc":"Store only the 4 numeric colums for PCA Training and Test\n\nData Needs to be normalized","a1c58bce":"<h2>Kaggle Bike Sharing Demand Dataset Preparation For PCA<\/h2>\n<h4>Use PCA to find new components to replace 'temp','atemp','humidity','windspeed' in both training and test datasets<\/h4>\n<h4>To download dataset, sign-in and download from this link: https:\/\/www.kaggle.com\/c\/bike-sharing-demand\/data<\/h4>\n<br>\nInput Features: ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'day', 'dayofweek','hour', <b>'pca components'<\/b>]<br>\nTarget Feature: [log1p('count')]<br>\nPCA Training: ['temp','atemp','humidity','windspeed']<br><br>\n\nObjective: <quote>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period (Ref: Kaggle.com)<\/quote>","1375f70b":"## Train  model using PCA Components\n###  Model is trained with XGBoost installed in notebook instance","af1eef5a":"## Training, Validation and Test Set\n### Target Variable as first column followed by input features\n### Training, Validation files do not have a column header"}}