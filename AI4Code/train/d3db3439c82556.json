{"cell_type":{"fbd4394d":"code","f92dbebe":"code","3593bdc0":"code","0938a005":"code","5ee06907":"code","82c34b12":"code","389c850b":"code","af667e20":"code","9fd46169":"code","b1a5a92c":"code","bfae7209":"code","6cd2b940":"code","768ef8c3":"code","b5b68b54":"code","1af3954b":"code","69c84c32":"code","c8d33f8e":"code","16cfdaf2":"code","7ba82bd7":"code","87bf067a":"code","49b8b14c":"code","cb1bfa6f":"code","ee738a6f":"code","6fe0c1f5":"code","618c208e":"code","f171a20c":"code","9ec2b4a7":"code","7b4a8050":"markdown","e28ecf99":"markdown","9bf2cbb2":"markdown","0ec224bd":"markdown","1acbf912":"markdown","017b4c2b":"markdown","5e908df7":"markdown","d1fda366":"markdown","99959ab3":"markdown","0bde496b":"markdown","962ab84b":"markdown","1c5a0954":"markdown","5c5d207d":"markdown","bbf12d7d":"markdown","d8683e1b":"markdown","b7000a9f":"markdown","b77f71f0":"markdown","7e0a1d2a":"markdown","100bf484":"markdown","bebb9124":"markdown","a1b3556e":"markdown","63a00007":"markdown","347c23d2":"markdown","9480041f":"markdown"},"source":{"fbd4394d":"#Import libraries\nimport pandas as pd\npd.options.display.max_columns = 500\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nwarnings.filterwarnings('ignore')\nfrom sklearn.datasets import load_boston","f92dbebe":"boston_dataset = load_boston()\nX = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n\nY=boston_dataset['target']\nXY = pd.concat([X,pd.Series(Y, name = 'target' )], axis=1)","3593bdc0":"XY.head(2)","0938a005":"print(u'- The rows numbers is: {}'.format(XY.shape[0]))\nprint(u'- The number of columns is: {}'.format(XY.shape[1]))","5ee06907":"#Usefull functions that you coud use for a lot of similar regresions works \n    \ndef relaciones_vs_target_reg(X, Y, return_type='axes'):\n    '''\n    Function that represents scatter plots of the variables\n    in X as a function of the variable Y\n    '''\n    fig_tot = (len(X.columns))\n    fig_por_fila = 4.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+5 ) )\n    c = 0 \n    for i, col in enumerate(X.columns):\n        plt.subplot(num_filas, fig_por_fila, i+1)\n        sns.regplot(x=X[col], y=Y)\n        plt.title( '%s vs %s' % (col, 'target') )\n        plt.ylabel('Target')\n        plt.xlabel(col)\n    plt.show()\n    \ndef represento_historico(historico):\n    hist = pd.DataFrame(historico.history)\n    hist['epoch'] = historico.epoch\n\n    plt.figure(figsize=(15,7))\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean absolute error [MAE]')\n    plt.plot(hist['epoch'], hist['mae'],\n           label='Training error')\n    plt.plot(hist['epoch'], hist['val_mae'],\n           label = 'Validation Error')\n    plt.title('MAE error in training and in test')\n    plt.ylim([0,5])\n    plt.legend()\n\n    plt.figure(figsize=(15,7))\n    plt.xlabel('Epoch')\n    plt.ylabel('Root mean square error [MSE]')\n    plt.plot(hist['epoch'], hist['mse'],\n           label='Training error')\n    plt.plot(hist['epoch'], hist['val_mse'],\n           label = 'Validation error')\n    plt.title('MSE error in training and in test')\n    plt.ylim([0,20])\n    plt.legend()\n    plt.show()\n\ndef hist_pos_neg_feat(x, y, density=0, nbins=11, targets=(0,1)):\n    '''\n    Represent the variables in x divided into two distributions\n    depending on its value of y is 1 or 0\n    '''\n    fig_tot = len(x.columns)\n    fig_tot_fila = 4.; fig_tamanio = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_tot_fila) )\n    plt.figure( figsize=( fig_tot_fila*fig_tamanio+2, num_filas*fig_tamanio+2 ) )\n    target_neg, target_pos = targets\n    for i, feat in enumerate(x.columns):\n        plt.subplot(num_filas, fig_tot_fila, i+1);\n        plt.title('%s' % feat)\n        idx_pos = y == target_pos\n        idx_neg= y == target_neg\n        represento_doble_hist(x[feat][idx_pos].values, x[feat][idx_neg].values, nbins, \n                   density = density, title=('%s' % feat))","82c34b12":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Box representation of the independent variables X')\nplt.ylabel('Values of the variable Y')\n_ = plt.xlabel('Variables names')","389c850b":"\n#plt.figure(figsize=(18,20))\n#n = 0\n#for i, column in enumerate(X.columns):\n#   n+=1\n#    plt.subplot(5, 5, n)\n#    sns.distplot(X[column], bins=30)\n#    plt.title('Distribuci\u00f3n var {}'.format(column))\n#plt.show()\n","af667e20":"relaciones_vs_target_reg(X, Y)","9fd46169":"matriz_correlaciones = XY.corr(method='pearson')\nn_ticks = len(XY.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), XY.columns, rotation='vertical')\nplt.yticks(range(n_ticks), XY.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Correlation Matrix Pearson method')","b1a5a92c":"correlaciones_target = matriz_correlaciones.values[ -1, : -1]\nindices_inversos =  abs(correlaciones_target[ : ]).argsort()[ : : -1]\ndiccionario = {}\nfor nombre, correlacion in zip( X.columns[indices_inversos], list(correlaciones_target[indices_inversos] ) ):\n    diccionario[nombre] = correlacion\npd.DataFrame.from_dict(diccionario, orient='index', columns=['Correlaci\u00f3n con la target'])","bfae7209":"obj_escalar = StandardScaler()\nX_estandarizado = obj_escalar.fit_transform(X)","6cd2b940":"X_train, X_test, Y_train, Y_test = train_test_split(X_estandarizado, Y, test_size=0.2, random_state=0)","768ef8c3":"def constructor_modelo():\n    # model definition\n    modelo = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)])\n    \n    # def optimizer\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n    \n    # compile model\n    modelo.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n    return modelo","b5b68b54":"model = constructor_modelo()","1af3954b":"model.fit(X_train, Y_train)","69c84c32":"X_train.shape[1]","c8d33f8e":"model.summary()","16cfdaf2":"example = X_train[:10]\nex_pred = model.predict(example)\nex_pred","7ba82bd7":"# I show one point for each completed epochs\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('=D')\n        print('.', end='')\n\nEPOCHS = 1000\nhistorico = model.fit(X_train, Y_train, \n                    epochs=EPOCHS,\n                    validation_split = 0.2, \n                    verbose=0,\n                    callbacks=[PrintDot()])","87bf067a":"hist = pd.DataFrame(historico.history)\nhist['epoch'] = historico.epoch\nhist.tail()","49b8b14c":"represento_historico(historico)","cb1bfa6f":"model = constructor_modelo()\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_mse', patience=10)\n\nhistory = model.fit(X_train, Y_train, \n                    epochs=EPOCHS,\n                    validation_split = 0.2, \n                    verbose=0, \n                    callbacks=[early_stop, PrintDot()])\n\nrepresento_historico(history)","ee738a6f":"Y_train_pred = model.predict(X_train)\nplt.title('Real values  vs predictions in train')\nplt.xlabel('Real Values')\nplt.ylabel('Predictions')\n_ = plt.plot(Y_train, Y_train_pred, '.', Y_train, Y_train, '-')","6fe0c1f5":"Y_test_pred = model.predict(X_test)\nplt.title('Real Values vs predictions in test')\nplt.xlabel('Real Values')\nplt.ylabel('Predictions')\n_ = plt.plot(Y_test, Y_test_pred, '.', Y_test, Y_test, '-')","618c208e":"plt.xlabel('Errores en train y en test')\nplt.ylabel('freqs')\nplt.hist(Y_train - Y_train_pred.flatten(), bins=21, label='Train')\nplt.hist(Y_test - Y_test_pred.flatten(), bins=21, label='Test')\n_ = plt.legend()","f171a20c":"error_mse_train = round(mean_squared_error(Y_train, Y_train_pred),2)\nerror_mse_test = round(mean_squared_error(Y_test, Y_test_pred),2)\nprint('El error cuadr\u00e1tico medio en train es: {}'.format(error_mse_train))\nprint('El error cuadr\u00e1tico medio en test es: {}'.format(error_mse_test))","9ec2b4a7":"# Pleas coment and upvote if you think that notbook was usefull. Sorry about my english but is not my native language","7b4a8050":"Train the model for 1000 epochs and record the training and validation accuracy in the history object.\n\nThe model is usually trained with more than one epoch. In this case we train the model with 1000 epochs. In addition, we make use of the ** callbacks ** parameter, which are a series of functions that are applied in training to obtain statistics or stop training according to an internal state.","e28ecf99":"In the graphs above you can see little improvement in the validation data from an epoch, in fact it even degrades. This is due to the fact that there is overfitting from a certain point on, since the error in training decreases but increases in validation.\n\nThere is a callback for the training method called EarlyStopping that stops training if after certain times there is no improvement in validation.\n\nMonitor is the value to monitor, which in this case can be one of the following:\n[loss, mae, mse, val_loss, val_mae, val_mse]\nPatience is the number of times without improvement before stopping training","9bf2cbb2":"## Histogram","0ec224bd":"# Keras-TensorFlow full explained notebook with really usefull functions to graph.","1acbf912":"### 1.-Data reading and initial analysis. Number of rows, Number of columns.","017b4c2b":"### To create a fully model in Keras, there are two parts to configure:\n\n1.- The definition of the model: in this case we have defined the model as a sequential model. This means that the layers are added one after another. We have defined four layers, three dense with 64 neurons each. The activation function is a relu. Another one is the output layer, which has a single neuron.\n\n2.- Model compilation: this requires defining a cost function (in this case the MSE) and an optimizer. In order to configure the optimizer parameters, it has been necessary to define it previously.","5e908df7":"We can predict some examples to check if the output is correct or some configuration needs to be changed:","d1fda366":"That looks good ","99959ab3":"In history an object has been saved with the training of the neural network. You can view the progress of the training by accessing its metrics and the loss saved with the .history attribute.","0bde496b":"## Variable vs target ratio:","962ab84b":"#NICE","1c5a0954":"### Visualizations and correlations.\n\nObtaining basic descriptions with the Describe () method.\nObtaining the dispersion of the variables using boxplot graphs.\nRepresentation of the distributions of the variables using histograms.\nRepresentation of the relationship between the independent and dependent variables.\nRepresentation of the correlation matrix.","5c5d207d":"## Boxplot:","bbf12d7d":"# I analyze errors:","d8683e1b":"If we print the summary of the model we can see the four layers it has and the parameters in each of the layers. For the first layer we would have a total of\n\n## \ud835\udc43\ud835\udc4e\ud835\udc5f\u00e1\ud835\udc5a = (64 \ud835\udc5b\ud835\udc52\ud835\udc62\ud835\udc5f\ud835\udc5c\ud835\udc5bs \u2217 13 \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60 \ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61) +64 values \ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60 = 896\n \nAnd analogously with the rest of the layers:","b7000a9f":"## Previous data standardization:\nNeural network models can converge and get a good solution even when the data is not standardized. However, to facilitate and speed up training, the data is often standardized.","b77f71f0":"The company asks us to run a regression model that infers the value of Boston homes in order to invest in a more profitable way.\n\nTo do this, the company has data on the value of homes based on their characteristics.","7e0a1d2a":"## Correlation matrix:","100bf484":"## Regression model with NN using Tensorflow and Keras:\nSklearn is used in the neural network classification. Although there is a way to perform regression with neural networks in sklearn (MLPRegressor), in this case we will use Tensorflow and Keras, which are the tools that are really used when a person advances in neural networks.\n\nThis is because Tensorflow gives us a flexibility that sklearn does not.","bebb9124":"Description of the problem:\nInference of the value of houses in Boston.\n\nThe goal of the problem is to predict the value of a home using its characteristics.\n\nWe will use the Boston dataset. This dataset is included within sklearn, although the original source is from the University of California Irvine (Url: https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/)\nDataset description:\nIt has a total of 13 predictor variables X and a continuous variable to predict Y.\n\nThe total number of samples is 506 houses.\n\nVariable information:\n1. CRIM: per capita crime rate by city\n2. ZN: proportion of parcels larger than 25,000 square feet.\n3. INDUS: proportion of acres of non-retail businesses by city\n4. CHAS: categorical variable Charles River (= 1 if the house borders the river; 0 does not limit)\n5. NOX: concentration of nitric oxides (parts per 10 million)\n6. RM: average number of rooms per dwelling\n7. AGE: proportion of owner-occupied units built prior to 1940\n8. DIS: weighted distances to five Boston job centers\n9. RAD: radial road accessibility index\n10. TAX: \\ $ 10,000 full value property tax rate\n11. PTRATIO: student-teacher ratio by location\n12. B: 1000 (Bk - 0.63) ^ 2 where Bk is the proportion of black people per town\n13. LSTAT:% minor state of the population\n14. MEDV: Average value of owner-occupied homes in \\ $ 1000's","a1b3556e":"The previously created model can be trained using the .fit () method. By default it uses a single epoch and all the data:","63a00007":"## Training the model","347c23d2":"## Train Test Split \u00a1","9480041f":"The follow graph shows an error here, but not in jupyter notebook.\nError : Selected KDE bandwidth is 0. Cannot estimate density.\nIf you know how to fix it, pleas let me know"}}