{"cell_type":{"0e3c295a":"code","244d4ea1":"code","11e86d15":"code","4f0393ad":"code","29580044":"code","ae3f42e1":"code","63fdab0d":"code","0d8a3acc":"code","6bb57df8":"code","d4e41655":"code","a9219a12":"code","0532877b":"code","149f7699":"code","55d2f947":"code","82d45da7":"code","6785f414":"code","29fc98e3":"code","dbe69554":"markdown","3e6f8ec8":"markdown","cf48176d":"markdown","8245d01d":"markdown","aecbad46":"markdown","8ed99cbc":"markdown"},"source":{"0e3c295a":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\norig_data = pd.read_csv(\"..\/input\/compresive_strength_concrete.csv\")\norig_data.head()","244d4ea1":"data = orig_data.copy()\ndata.shape","11e86d15":"data.info()","4f0393ad":"data.describe().T","29580044":"#Changing column headers to just keep component names\ndata.columns = [col[:col.find(\"(\")].strip() for col in data.columns]\ndata.head()","ae3f42e1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data[data.columns[:-1]],\n                                                    data[[data.columns[-1]]],\n                                                    test_size = .2,\n                                                    random_state = 1)","63fdab0d":"from pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nsm = scatter_matrix(x_train, figsize=(15,15), diagonal = 'kde')\n#Changing label rotation\n[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n#Changing offset for label\n[s.get_yaxis().set_label_coords(-0.5,0.5) for s in sm.reshape(-1)]\n#Hiding ticks\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","0d8a3acc":"import seaborn as sns\nsns.heatmap(x_train.corr().abs())\nplt.show()","6bb57df8":"#Scaling the features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(x_train)\nx_train_scaled = pd.DataFrame(scaler.transform(x_train),\n                              columns = x_train.columns)\nx_train_scaled.head()","d4e41655":"#We will save the model performance metrics in a DataFrame\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\nModel = []\nRMSE = []\nR_sq = []\ncv = KFold(5, random_state = 1)\n\n#Creating a Function to append the cross validation scores of the algorithms\ndef input_scores(name, model, x, y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=cv, \n                                               scoring='neg_mean_squared_error').mean()))\n    R_sq.append(cross_val_score(model, x, y, cv=cv, scoring='r2').mean())","a9219a12":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                              AdaBoostRegressor)\n\nnames = ['Linear Regression', 'Ridge Regression', 'Lasso Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor', \n         'Random Forest Regressor', 'Gradient Boosting Regressor',\n         'Adaboost Regressor']\nmodels = [LinearRegression(), Ridge(), Lasso(),\n          KNeighborsRegressor(), DecisionTreeRegressor(),\n          RandomForestRegressor(), GradientBoostingRegressor(), \n          AdaBoostRegressor()]\n\n#Running all algorithms\nfor name, model in zip(names, models):\n    input_scores(name, model, x_train_scaled, y_train)","0532877b":"evaluation = pd.DataFrame({'Model': Model,\n                           'RMSE': RMSE,\n                           'R Squared': R_sq})\nprint(\"FOLLOWING ARE THE TRAINING SCORES: \")\nevaluation","149f7699":"#tuning this base model\nGradientBoostingRegressor()","55d2f947":"#tuning for number of trees\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators':range(20,1001,10),\n              'max_depth':[10], #range(5,16,2), \n              'min_samples_split':[100], #range(200,1001,200), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=cv).fit(x_train_scaled, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","82d45da7":"#tuning the tree specific parameters\nparam_grid = {'n_estimators': [230],\n              'max_depth': range(10,31,2), \n              'min_samples_split': range(50,501,10), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=cv).fit(x_train_scaled, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","6785f414":"#now increasing number of trees and decreasing learning rate proportionally\nclf = GradientBoostingRegressor(random_state=1, max_depth=20, \n                                min_samples_split=170, n_estimators=230*2, \n                                learning_rate=0.2\/2)\nprint(\"R Squared:\",cross_val_score(clf, x_train_scaled, y_train, cv=cv, scoring='r2').mean())","29fc98e3":"#applying this model on test data\nx_test_scaled = pd.DataFrame(scaler.transform(x_test),\n                             columns = x_test.columns)\nclf = GradientBoostingRegressor(learning_rate=0.2\/2, max_depth=20,\n                                min_samples_split=170, n_estimators=230*2, \n                                random_state=1).fit(x_train_scaled, y_train)\nprint(\"Test RMSE: \", np.sqrt(mean_squared_error(y_test, clf.predict(x_test_scaled))))\nprint(\"Test R^2: \", r2_score(y_test, clf.predict(x_test_scaled)))","dbe69554":"Gradient Boosting Regressor has the lowest RMSE, highest R-Squared","3e6f8ec8":"No missing values, all numeric","cf48176d":"Little correlation of ~0.6 between Superplasticizer and Water (which is negative as evident from scatter matrix), but lets move forward as is.","8245d01d":"No high correlation between any two features. Lets verify with Heatmap.","aecbad46":"Rather than splitting training data further to validation set, we will perform cross validation in all our training models.","8ed99cbc":"**Since score improved, the best model is GradientBoostingRegressor with learning_rate= 0.2\/2, max_depth= 20, min_samples_split= 170, n_estimators= 230*2**"}}