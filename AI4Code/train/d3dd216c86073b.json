{"cell_type":{"858f4c37":"code","846b4d8b":"code","92b5f92f":"code","da11fdba":"code","37d49f73":"code","60349a55":"code","b232cdf5":"code","03c7f254":"code","e17fcea6":"code","98e87022":"code","41b957df":"code","36d30c28":"code","252fd4c7":"code","d3a0b5a0":"code","4109c739":"code","2cf92296":"code","0107fdf7":"code","4b45db6a":"code","4910bc51":"code","8d283c76":"code","79bd5e01":"code","66365188":"code","20702d39":"code","f30215ae":"code","3c15157a":"code","17971468":"code","aead63d9":"markdown","62b8fbb0":"markdown","3fe408e9":"markdown","958b308e":"markdown","88fdf58e":"markdown","49d69ced":"markdown","2db60973":"markdown","a04b7f81":"markdown","edac73a8":"markdown","f7ec8129":"markdown","6b97a00e":"markdown","ea3147a4":"markdown","03b66c24":"markdown"},"source":{"858f4c37":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\nfrom functools import wraps\nfrom contextlib import contextmanager\nfrom tqdm import tqdm\n\n\n@contextmanager\ndef timer(msg):\n    st = datetime.now()\n    yield\n    cost = datetime.now() - st\n    print(f'{msg} Done. It cost {cost}')\n\n\ndef clock(func):\n    @wraps(func)\n    def clocked(*args, **kwargs):\n        st = datetime.now()\n        res = func(*args, **kwargs)\n        cost_ = datetime.now() - st\n        print(f'{func.__name__} cost {cost_}')\n        return res\n    return clocked\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","846b4d8b":"# inference: https:\/\/www.kaggle.com\/fusioncenter\/residual-network-for-tabular-data","92b5f92f":"with timer('drop dumpliced'):\n    train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\n    test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')\n    submit_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","da11fdba":"need_columns = [i for i in train_df.columns if 'feat' in i] + ['target']\nwith timer('drop duplicate'):\n    print('Before drop duplicate train_df.shape:', train_df.shape)\n    train_df = train_df.drop_duplicates(subset=need_columns).reset_index(drop=True)\n    print('After drop duplicate train_df.shape:', train_df.shape)","37d49f73":"@clock\ndef low_freqence_detector(df, vars_to_agg, agg_threshold=0.999):\n    \"\"\"\n    \u5c06\u4f4e\u9891\u503c\u5f52\u4e3a\u4e00\u7c7b\n    \"\"\"\n    replace_dict = {}\n    for col in tqdm(vars_to_agg):\n        a_cumsum = df[col].value_counts(normalize=True).cumsum()\n        value_count_series = df[col].value_counts()\n        will_be_replaced_values = value_count_series[a_cumsum >= agg_threshold].index.tolist()\n        n = len(will_be_replaced_values)\n        replace_value = min(will_be_replaced_values)\n        tmp_dict = (will_be_replaced_values, replace_value)\n        replace_dict[col] = tmp_dict\n    return replace_dict\n\ndef aggregate_low_freq_values(df, replace_dict):\n    df_out = df.copy(deep=True)\n    for replace_feat in tqdm(replace_dict):\n        need_replaced_values = replace_dict[replace_feat][0]\n        replace_value = replace_dict[replace_feat][1]\n        df_out.loc[df[replace_feat].isin(need_replaced_values), replace_feat] = replace_value\n    return df_out\n\ndef quick_agg_low_freq_values(tr_df, te_df, vars_to_agg, agg_threshold=0.999):\n    c = pd.concat([tr_df[vars_to_agg], te_df[vars_to_agg]], ignore_index=True)\n    replace_dict = low_freqence_detector(c, vars_to_agg, agg_threshold)\n    return aggregate_low_freq_values(tr_df, replace_dict), aggregate_low_freq_values(te_df, replace_dict)","60349a55":"train_df, test_df = quick_agg_low_freq_values(\n    train_df, test_df,\n    [i for i in train_df.columns if 'feat' in i]\n)","b232cdf5":"# # \u589e\u52a0freq\n# frequnce_add_col = ['feature_60', 'feature_15', 'feature_28', 'feature_61', 'feature_62']\n# for col_ in tqdm(frequnce_add_col):\n#     dict_ = train_df[col_].value_counts(normalize=True).to_dict()\n#     train_df[f'{col_}_freq'] = train_df[col_].map(dict_)\n#     test_df[f'{col_}_freq'] = test_df[col_].map(dict_)","03c7f254":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Dense, Input, SpatialDropout1D, Conv1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import concatenate, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras import Model, backend, layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam, Adamax\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix","e17fcea6":"def tf2_logloss(obs, pre):\n    pre = tf.clip_by_value(pre, 0.0005, 1-0.0005)\n    return tf.keras.metrics.categorical_crossentropy(obs, pre)\n\ndef plot_heatmap(y_true, y_pred_prob):\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    conf = confusion_matrix(y_true, y_pred)\n    conf_p = conf\/ conf.sum(axis=0).reshape(1, -1)\n    conf_r = conf\/ conf.sum(axis=1).reshape(-1, 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    sns.heatmap(conf_p, annot=True, fmt='.2f', ax=axes[0])\n    axes[0].set_title('Percision')\n    sns.heatmap(conf_r, annot=True, fmt='.2f', ax=axes[1])\n    axes[1].set_title('Recall')\n    plt.show()\n\n\ndef embedding_residul_block(\n    max_cnt, \n    embed_size=3, \n    feature_nums=75,\n    max_len=1, \n    number_of_blocks=4,\n    output_shape=9\n):\n    _input = Input(shape=(feature_nums,), dtype='float32')\n    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=False)(_input)\n    _embed = Flatten()(_embed)\n    \n    block_list = []\n    for i in range(number_of_blocks):\n        if i == 0:\n            blocki = BatchNormalization()(_input)\n            blocki = Dropout(0.2)(blocki)\n            blocki = Dense(64, activation='relu')(blocki)\n            blocki = concatenate([blocki, _embed])\n            block_list.append(blocki)\n            continue\n\n        blocki_bf = block_list[i-1]\n        blocki_next = BatchNormalization()(blocki_bf)\n        blocki_next = Dropout(0.2)(blocki_next)\n        blocki_next = Dense(128, activation='relu')(blocki_next)\n        blocki_next = concatenate([blocki_next, block_list[0]])\n        block_list.append(blocki_next)\n\n    _output = block_list[-1]\n    _output = BatchNormalization()(_output)\n    _output = Dense(output_shape, activation='softmax')(_output)\n\n    model = Model(inputs=_input, outputs = _output)\n    \n    model.compile(loss = 'categorical_crossentropy',\n                  optimizer=Adam(lr = 0.01),\n                  metrics=['accuracy']) #[tf2_logloss]) #\n    return model","98e87022":"from sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import f1_score, log_loss\nfrom copy import deepcopy\n\n\nmodel_save_path='residual_nn.hdf5'\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.00001)\nmodel_checkpoint = ModelCheckpoint(\n    model_save_path,\n    save_best_only=True,\n    save_weights_only=True,\n)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=5)\n\ndef pred_57_f1(y_true_in, y_pred_proba_in):\n    y_true = deepcopy(y_true_in)\n    y_pred_proba = deepcopy(y_pred_proba_in)\n    mean_5 = np.percentile(y_pred_proba[:, 5], 65)\n    mean_7 = np.percentile(y_pred_proba[:, 7], 65)\n    y_pred_proba[ y_pred_proba[:, 5] <= mean_5, 5] = 0.0001\n    y_pred_proba[ y_pred_proba[:, 7] <= mean_7, 7] = 0.0001\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    y_pred57 = (y_pred == 5) | (y_pred == 7) \n    y_pred[~y_pred57]=0\n    \n    y_true57 = (y_true == 5) | (y_true == 7) \n    y_true[~y_true57]=0\n    \n    return f1_score(y_true, y_pred, average='macro'), y_pred_proba, log_loss(y_true_in, y_pred_proba )\n","41b957df":"print(tf.config.list_physical_devices())\nfor i in tf.config.list_physical_devices():\n    if 'GPU' in i[1]:\n        print (i[1],'\u53ef\u7528,GPU\u540d\u79f0: ',i[0])\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        print ('Turn on GPU')","36d30c28":"from sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.isotonic import IsotonicRegression\nlb = LabelEncoder()\n# print(train_df.columns)\nX = train_df.drop(columns = ['id', 'target']).values\ny = lb.fit_transform(train_df['target'].values)\ntest_array = test_df.drop(columns = ['id']).values\nnfold = 5\nepochs = 50\noutput_shape = 9\n# kf = StratifiedKFold(nfold)\npred_arr_list = []\nfor seed_ in [2021, 42, 1921]:\n    tf.random.set_seed(seed_)\n    kf = StratifiedKFold(nfold, shuffle=True, random_state=seed_)\n\n    for foldi, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold: {foldi} | seed{seed_}\")\n        tr_x, tr_y = X[tr_idx], y[tr_idx]\n        val_x, val_y = X[val_idx], y[val_idx]\n        nn_model = embedding_residul_block(\n            max_cnt=500, # 500\n            embed_size=3,#3, \n            feature_nums=tr_x.shape[1],\n            max_len=1, \n            number_of_blocks=4,#4,\n            output_shape=9\n        )\n    #     nn_model.summary()\n        nn_model.fit(tr_x, to_categorical(tr_y, output_shape),\n                         validation_data = (val_x, to_categorical(val_y, output_shape)),\n                         epochs = epochs,\n                         batch_size = 128, #128,\n                         shuffle=True,\n                         callbacks = [early_stopping, model_checkpoint, reduce_lr]\n                    )\n        # load best model\n        nn_model.load_weights(model_save_path)\n        pred = nn_model.predict(val_x)\n        pred_cp = deepcopy(pred)\n        for i in range(pred.shape[1]):\n            ir = IsotonicRegression()\n            ir.fit(pred[:, i], to_categorical(val_y, output_shape)[:, i])\n            pred_cp[:, i] = ir.predict(pred[:, i])\n\n        ir_loss = log_loss(val_y, pred_cp )\n        loss_o = log_loss(val_y, pred )\n        print(f'original loss: {loss_o:.3f}, ir_loss:{ir_loss:.3f}')\n        plot_heatmap(val_y, pred_cp)\n        if foldi == 0:\n            pred_out = nn_model.predict(test_array)\n            pred_array = pred_out\n        else:\n            pred_out = nn_model.predict(test_array)\n            pred_array += pred_out\n        pred_arr_list.append(pred_out)\n        print(\"-\"*50)","252fd4c7":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.preprocessing import minmax_scaling\n\ndef make_mi_scores(X, y, discrete_features):\n    \"\"\"\n    https:\/\/www.kaggle.com\/mehrankazeminia\/1-tps-jun-21-histgradient-catboost-nn\/output\n    \u4f30\u8ba1\u79bb\u6563\u76ee\u6807\u53d8\u91cf\u7684\u4e92\u4fe1\u606f\n    \"\"\"\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=123)    \n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)                         \n    mi_scores = mi_scores.sort_values(ascending=False)                          \n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","d3a0b5a0":"n = 1\nfor arr_ in pred_arr_list:\n    if n ==1:\n        pred_f = arr_\n        n += 1\n        continue\n    pred_f += arr_\n    n += 1\n\npred_f = pred_f\/n\n\nsubmit_df0 = submit_df.copy(deep=True)\nsubmit_df0.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred_f, 10**-15, 1-10**-15)","4109c739":"from lightgbm import LGBMClassifier\nlgb_params = {\n        'num_leaves': 10,\n        'min_data_in_leaf': 63,\n        'learning_rate': 0.05,\n        'min_sum_hessian_in_leaf': 8.140308692805194,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'boost_from_average':'false',\n        'subsample': 0.749948437333368,\n        'colsample_bytree': 0.6168504947710284,\n         'reg_alpha': 0.227796749807186,\n         'reg_lambda': 70.2792417704872,\n        'min_gain_to_split': 0.4758826409257615,\n        'max_depth': 14, \n        'n_jobs': -1,\n        'boosting_type': 'gbdt',\n        'metric':'multi_logloss',\n#         'early_stopping_round' : 100,\n        'n_estimators': 500,\n        'tree_learner': 'serial',\n    }\nmodel1 = LGBMClassifier(**lgb_params)\n\nmodel1.fit(X, y, verbose=50)\npred1 = model1.predict_proba(test_array)\nsubmit_df1 = submit_df.copy(deep=True)\nsubmit_df1.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred1, 10**-15, 1-10**-15)","2cf92296":"from catboost import CatBoostClassifier\nmodel2 = CatBoostClassifier(depth=8,\n                            iterations=1000,\n                            learning_rate=0.02,                            \n                            eval_metric='MultiClass',\n                            loss_function='MultiClass', \n                            bootstrap_type= 'Bernoulli',\n                            leaf_estimation_method='Gradient',\n                            random_state=123,\n                            task_type='GPU')   \n\nmodel2.fit(X, y, verbose=100)\npred2 = model2.predict_proba(test_array)\nsubmit_df2 = submit_df.copy(deep=True)\nsubmit_df2.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred2, 10**-15, 1-10**-15)","0107fdf7":"def generate(main, support, coeff):\n    g = main.copy()    \n    for i in main.columns[1:]:\n        \n        res = []\n        lm, Is = [], []        \n        lm = main[i].tolist()\n        ls = support[i].tolist()  \n        \n        for j in range(len(main)):\n            res.append((lm[j] * coeff) + (ls[j] * (1.- coeff)))            \n        g[i] = res\n        \n    return g\n\n\nsub = generate(submit_df1, submit_df2, 0.85)\nsub = generate(submit_df0, sub , 0.85)","4b45db6a":"class SklearnHelper:\n    def __init__(self, clf, seed, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        \n    def train(self, x_tr, y_tr):\n        self.clf.fit(x_tr, y_tr)\n    \n    def predict(self, x):\n        try:\n            pred = self.predict_proba(x)\n        except AttributeError:\n            pred = self.clf.predict(x)\n        return pred\n    \n    def predict_proba(self, x):\n        return self.clf.predict_proba(x)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n    def __repr__(self):\n        return str(self.clf)\n\n        \nfrom sklearn.model_selection import KFold\nntrain = train_df.shape[0]\nntest = test_df.shape[0]\nNFOLDS = 5\nSEED = 2021\nkf = KFold(n_splits= NFOLDS, shuffle=True ,random_state=42)\n\ndef get_oof(clf, x_train, y_train, x_test, class_nums):\n    oof_train = np.zeros((ntrain, class_nums))\n    oof_test = np.zeros((ntest, class_nums))\n    oof_test_skf = np.zeros((ntest, class_nums))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        y_te = y_train[test_index]\n\n        clf.train(x_tr, y_tr)\n        \n        pred = clf.predict(x_te)\n        oof_train[test_index] = pred\n        oof_test_skf += clf.predict(x_test)\n        \n        loss = log_loss(y_te, pred)\n        print(f\"{str(clf)} | fold {i} | Log loss: {loss}\")\n        print(\"-\"*50)\n\n    oof_test = oof_test_skf \/ NFOLDS\n    return oof_train, oof_test","4910bc51":"from catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n# lgb\nlgb_params = {\n        'num_leaves': 10,\n        'min_data_in_leaf': 63,\n        'learning_rate': 0.05,\n        'min_sum_hessian_in_leaf': 8.140308692805194,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'boost_from_average':'false',\n        'subsample': 0.749948437333368,\n        'colsample_bytree': 0.6168504947710284,\n         'reg_alpha': 0.227796749807186,\n         'reg_lambda': 70.2792417704872,\n        'min_gain_to_split': 0.4758826409257615,\n        'max_depth': 14, \n        'n_jobs': -1,\n        'boosting_type': 'gbdt',\n        'metric':'multi_logloss',\n#         'early_stopping_round' : 100,\n        'n_estimators': 500,\n        'tree_learner': 'serial',\n        'verbose': 0\n    }\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n\n# catboost\ncatboost_param = dict(depth=8,\niterations=1000,\nlearning_rate=0.02,                            \neval_metric='MultiClass',\nloss_function='MultiClass', \nbootstrap_type= 'Bernoulli',\nleaf_estimation_method='Gradient',\nrandom_state=123,\ntask_type='GPU')\n","8d283c76":"rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngbm = SklearnHelper(clf=LGBMClassifier, seed=SEED, params=lgb_params)\nctb = SklearnHelper(clf=CatBoostClassifier, seed=SEED, params=catboost_param)","79bd5e01":"\nrf_oof_train, rf_oof_test = get_oof(rf,X, y, test_array, class_nums=9) # Random Forest\n# ada_oof_train, ada_oof_test = get_oof(ada, X, y, test_array, class_nums=9) # AdaBoost \nlgb_oof_train, lgb_oof_test = get_oof(gbm,X, y, test_array, class_nums=9) # lgb\net_oof_train, et_oof_test = get_oof(et, X, y, test_array, class_nums=9) # Extra Trees\nctb_oof_train, ctb_oof_test = get_oof(ctb,X, y, test_array, class_nums=9) # catboost","66365188":"# #### view\n# import scipy.stats as sts\n# view_foucs5_bool = (train_df.target == 'Class_6') \n# view_foucs7_bool =(train_df.target == 'Class_8')\n\n# skew_5_7_diff_beyond_05 = []\n# for i in [i for i in train_df.columns if 'feat' in i]:\n#     print(i)\n#     train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i].hist()\n#     skew_5 = sts.skew(train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i])\n#     plt.title(f'{i}&class_5 total_mean: {train_df.loc[view_foucs5_bool, i].mean():.3f} \\\n#     | limit> 3 mean: {train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i].mean():.3f} | \\\n#     skew : {skew_5:.3f}')\n#     plt.show()\n#     train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i].hist()\n#     skew_7 = sts.skew(train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i])\n#     plt.title(f'{i}&class_7 total_mean: {train_df.loc[view_foucs7_bool, i].mean():.3f} \\\n#     | limit> 3 mean: {train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i].mean():.3f}| \\\n#     skew : {skew_7:.3f}')\n\n#     plt.show()\n#     if abs(skew_7-skew_5) > 0.5 :\n#         print(f'abs(skew_7-skew_5) > 0.5: {abs(skew_7-skew_5):.2f}')\n#         skew_5_7_diff_beyond_05.append(i)\n#     print('--'*25)","20702d39":"# skew_5_7_diff_beyond_05\n# # nd_cols = [i for i in train_df.columns if 'feat' in i]","f30215ae":"# from sklearn.preprocessing import LabelEncoder\n# from sklearn.linear_model import LogisticRegression\n# from lightgbm import LGBMClassifier\n# import matplotlib.pyplot as plt\n\n# lb = LabelEncoder()\n# X = train_df.loc[view_foucs5_bool | view_foucs7_bool, nd_cols].values\n# y = lb.fit_transform(train_df.loc[view_foucs5_bool | view_foucs7_bool, 'target'].values)\n# test_array = train_df.loc[view_foucs5_bool | view_foucs7_bool, nd_cols].values\n# nfold = 5\n# epochs = 50\n# output_shape = 2\n# kf = StratifiedKFold(nfold)\n# for foldi, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n#     print(f\"Fold: {foldi}\")\n#     tr_x, tr_y = X[tr_idx], y[tr_idx]\n#     val_x, val_y = X[val_idx], y[val_idx]\n#     lr = LGBMClassifier(is_unbalance=True)\n#     lr.fit(tr_x, tr_y)\n#     pred = lr.predict_proba(val_x)\n#     plot_heatmap(val_y, pred)\n#     if foldi == 0:\n#         pred_array = lr.predict_proba(test_array)\n#     else:\n#         pred_array += lr.predict_proba(test_array)\n#     print(\"-\"*50)","3c15157a":"from datetime import datetime\nnow_ = datetime.now().strftime('%Y%m%d_%H_%M')\n# submit_df.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n#     np.clip(pred_f, 10**-15, 1-10**-15)\n\n\n# submit_df = submit_df.fillna(0.0001)\ndisplay(submit_df0.head())\nsubmit_df0.to_csv(f'residul_nn_model_{now_}.csv',index=False)","17971468":"os.environ['KAGGLE_USERNAME'] = \"scchuy\" # username from the json file \nos.environ['KAGGLE_KEY'] = \"59c271f7739fbc0d21b8d2c5f8789670\"\n!kaggle competitions submit -c tabular-playground-series-jun-2021 -f .\/residul_nn_model_{now_}.csv -m \"Message\"","aead63d9":"## Model0 - tabluar-residual-nn","62b8fbb0":"# Load data","3fe408e9":"# Focus \nclass6(5) - class8(7)\n","958b308e":"# residual net model\n## residual blocks","88fdf58e":"## model train","49d69ced":"## Model2 - CatBoostClassifier","2db60973":"## Model1- LGBMClassifier","a04b7f81":"## drop dumplicates rows","edac73a8":"## Stacking models","f7ec8129":"# Model Merge","6b97a00e":"# Submit","ea3147a4":"## Aggregate low freq categries ","03b66c24":"# Stacking"}}