{"cell_type":{"7723483e":"code","32a66ee1":"code","9a2bb925":"code","cf4c2023":"code","a2ac7082":"code","ba43d8cd":"code","b98ba1e2":"code","4e6fdcfc":"code","d32002a5":"code","59325541":"code","bf71a9fc":"code","335fb651":"code","7c405d9a":"markdown","944ab5a8":"markdown","5583ff56":"markdown","f7226946":"markdown","cb01a2db":"markdown","aeeac4a6":"markdown","6e432020":"markdown","a5b1e531":"markdown","85bb40c7":"markdown","201432e9":"markdown","15018572":"markdown","84621dfc":"markdown","23b2de25":"markdown","87b91f51":"markdown","62c2659b":"markdown","f6f69b3e":"markdown","f4527fb9":"markdown","2c89966d":"markdown","f4884644":"markdown"},"source":{"7723483e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_excel('..\/input\/covid19\/dataset.xlsx')\ndata.columns = map(lambda x: x.lower().strip().replace('\\xa0', '_').replace(' ', '_'), data.columns)\ndata.head()","32a66ee1":"import missingno as msno\nmsno.matrix(data, labels=True, fontsize=9)","9a2bb925":"useful_data = data[['sars-cov-2_exam_result', 'hematocrit', 'hemoglobin', 'platelets', 'mean_platelet_volume', 'red_blood_cells', 'lymphocytes', 'mean_corpuscular_hemoglobin_concentration_(mchc)', 'leukocytes', 'basophils', 'mean_corpuscular_hemoglobin_(mch)', 'eosinophils', 'mean_corpuscular_volume_(mcv)', 'monocytes', 'red_blood_cell_distribution_width_(rdw)']].dropna()\n\ndata_length = len(data)\nuseful_data_length = len(useful_data)\nprint('Number of samples: {}'.format(data_length))\nprint('Number of useful samples: {}'.format(useful_data_length))\nprint('Useful portion of the dataset: {:.2f}%'.format((useful_data_length \/ data_length) * 100.0))","cf4c2023":"sns.countplot(x='sars-cov-2_exam_result', data=useful_data)","a2ac7082":"# Here we separate the data into a matrix X and a vector y (data and target)\nX = useful_data.loc[:, useful_data.columns != 'sars-cov-2_exam_result']\ny = useful_data[['sars-cov-2_exam_result']]\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nrus = RandomUnderSampler(random_state=42, sampling_strategy='majority')\nX_test, y_test = rus.fit_resample(X_test, y_test) # Here, we undersample the majority class of the test set","ba43d8cd":"from sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\ndecision_tree_classifier = tree.DecisionTreeClassifier(max_depth=3, class_weight='balanced')\ndecision_tree_scores = cross_val_score(decision_tree_classifier, X, y, cv=5)\nprint(\"Scores: {}\".format(decision_tree_scores))\nprint(\"Mean: %0.2f\" % decision_tree_scores.mean())\nprint(\"Standard deviation: %0.2f\" % decision_tree_scores.std())","b98ba1e2":"from sklearn.metrics import plot_confusion_matrix\n\ndecision_tree_classifier.fit(X_train, y_train)\nplot_confusion_matrix(decision_tree_classifier, X_test, y_test, normalize='true')","4e6fdcfc":"import graphviz\ndecision_tree_classifier.fit(X, y)\ndot_data = tree.export_graphviz(decision_tree_classifier, out_file=None, \n                     rotate=True,\n                     feature_names=X.columns,\n                     class_names=decision_tree_classifier.classes_,\n                     filled=True, rounded=True,  \n                     special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","d32002a5":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_classifier = RandomForestClassifier(n_estimators=30, max_depth=3, class_weight='balanced_subsample')\nrandom_forest_scores = cross_val_score(random_forest_classifier, X, y.values.ravel(), cv=5)\nprint(\"Scores: {}\".format(random_forest_scores))\nprint(\"Mean: %0.2f\" % random_forest_scores.mean())\nprint(\"Standard deviation: %0.2f\" % random_forest_scores.std())","59325541":"from sklearn.metrics import plot_confusion_matrix\n\nrandom_forest_classifier.fit(X_train, y_train)\nplot_confusion_matrix(random_forest_classifier, X_test, y_test, normalize='true')","bf71a9fc":"from imblearn.ensemble import BalancedRandomForestClassifier\n\nbalanced_random_forest_classifier = BalancedRandomForestClassifier(n_estimators=30, max_depth=3, sampling_strategy='majority')\nbalanced_random_forest_scores = cross_val_score(balanced_random_forest_classifier, X, y.values.ravel(), cv=5)\nprint(\"Scores: {}\".format(balanced_random_forest_scores))\nprint(\"Mean: %0.2f\" % balanced_random_forest_scores.mean())\nprint(\"Standard deviation: %0.2f\" % balanced_random_forest_scores.std())","335fb651":"from sklearn.metrics import plot_confusion_matrix\n\nbalanced_random_forest_classifier.fit(X_train, y_train)\nplot_confusion_matrix(balanced_random_forest_classifier, X_test, y_test, normalize='true')","7c405d9a":"## Tree visualization\n\nAs said earlier, the trained model may provide some good insights about the importance of the features. The plot of the tree is shown right below.","944ab5a8":"## Confusion matrix","5583ff56":"# Decision Tree\n\nA well know model that could be used to try to predict the class of new instances is the [decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree). The advantages of this model is that the resultant tree could give us some good insights on what are the features that contribute the most on predicting a class.\n\nIn the first approach, we're going to set different weights to the samples that belong to the minority class. This can be done setting the _class_weight_ parameter to \"balanced\".","f7226946":"# Finding a subset of useful samples\n\nAs other people who already analysed the provided dataset, a lot of samples have _null_ values. Thus, we need to find a subset of the dataset that has useful information.","cb01a2db":"## Confusion matrix","aeeac4a6":"# COVID-19 diagnosis prediction using machine learning\n\nAn _attempt_ to diagnose COVID-19 in patients using different models trained with patients' health markers.","6e432020":"## Confusion matrix","a5b1e531":"## Reading the dataset","85bb40c7":"# A problem: small number of samples\n\nAs we could see, a lot of samples were lost after the selection of a useful subset of the dataset. The major problem of this is that these samples probably don't reflect the patterns of a population, leading to models that trained based on a tiny part of the population.\n\nThis's a problem we need work with, thought. By now, we need to focus on generating models could be general enough the possibly predict a diagnose of COVID-19.","201432e9":"## A different approach\n\nA possible different approach could be training the model considering an undersample of the majority class, but, for now, I won't do this.\n","15018572":"### Separating the data and the classifications","84621dfc":"# Another problem: imbalanced data\n\n But there's another problem: the remaining samples are highly unbalanced (considering their classes). The reason why this is a problem is because, if this data is used without prior processing for traning a model, they will generate a bias, favoring one class over to another.\n\nA plot of the number of instances per class is shown right below.","23b2de25":"## Solving the imbalancing problem\n\nThere are different alternatives to tackle the problem of unbalanced data. We could, for example, _undersample_ the majority class, or even _oversample_ the minority class (which is probably not a good idea for this case), and then train the desired model. Another approach could be _change the \"class weights\"_ of one of the classes. \n\nAs the models are trained, the different approaches are going to be described too.","87b91f51":"# Models","62c2659b":"# Notes\n\n### About the models\n- These trained models are, probably, very limited to the patterns of the set of data that was used to train them. In other words, they're probably not general enough, although they can be used as an initial step, given that dataset is probably growing.\n\n### Possible improvements\n- Different test sets could be used to plot different confusion matrices. Then, we could better analyse the accuracy of the models considering the different classes\n- False-positives should be favored over false-negatives, since one of the utilities of these trained models could be to do triage of patients\n\n### About the data\n- More data instances could have **basic health markers** (blood and urine markers, for example)\n- A **chest X-ray dataset** could also be made available\n- **Commorbities** might be useful for predicting patient's future clinical conditions\n\n#### To be continued...","f6f69b3e":"Decision trees are prone to overfitting, specially when it's deep...","f4527fb9":"As we can see, the features the have greatest number of valid values are the ones related to tests of other diseases. Although, these information are not really useful to diagnose the presence of the SARS-COV2 in a person's body. Thus, the features that we can use are the ones related to blood markers. ","2c89966d":"# Random Forest\n\nAnother approach could be the use of a random forest, which is basically a set of trained decision trees trained with different subsamples of the dataset. In this case, we'll set the number of estimators (trees) to thirty, since the dataset is small. We set\n\n","f4884644":"## A different approach\n\nA possible different approach could be the usage of balanced random forests. With this approach, we could undersample the majority class for each decision tree that is used to form the random forest. We can do this by setting the _sampling_strategy_ parameter of the balanced random forest to _\"majority\"_."}}