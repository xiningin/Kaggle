{"cell_type":{"c8dc7974":"code","465b5b8a":"code","e2805be7":"code","9efc06b5":"code","13c0f281":"code","0cdbd81b":"code","60c04981":"code","257e8662":"code","16a53ae5":"code","1bb59972":"code","4e18b533":"code","65982216":"code","9e0965eb":"code","c71b703d":"code","ee7f12da":"code","148a6c0b":"code","fca643ee":"code","b1158feb":"code","f8396b41":"code","6924ed47":"code","2aa4fe76":"code","dc81f9f6":"code","9474e996":"code","822be184":"code","57ccf48b":"code","838d2d97":"code","38bbabf7":"code","ed4b6b5d":"code","62a2acb0":"code","be771e0d":"code","4d4fb343":"code","1cb185f4":"code","0be3fae3":"code","9a37fc4a":"code","24125ed4":"code","256e3e90":"code","761fd95b":"code","31f6c446":"code","47d544b5":"code","98a71beb":"code","bb02a36f":"code","ba975f25":"code","6d194908":"code","b31b9bc1":"code","e005cf6c":"code","529e40f8":"code","183a922a":"code","d3267c0d":"code","c06ad287":"code","0f12af8c":"code","8de6e152":"code","971b874d":"code","4cfdfad6":"code","c21c4aa2":"code","813f8a79":"code","f758b1c6":"code","01fa1cec":"code","5b6609d8":"code","349d4dc1":"code","e665de0d":"code","fee909a0":"code","48e54530":"code","302a8725":"code","6afc469f":"code","67593db8":"code","7f9c4e50":"code","05aec3dc":"code","71778cb1":"code","b2bc59d2":"code","5c3f6bca":"code","000ba772":"code","30e7c2fb":"code","eba60cf7":"code","8c73678a":"code","04b9113f":"code","7b7107b5":"code","5f8e3c2b":"code","0b8457b4":"code","24b2069b":"code","eb4c95cd":"code","4b88b948":"code","223c38a6":"code","3b09322a":"code","828a96d3":"code","2a2c8cf5":"code","47ff151d":"code","b72d40e4":"code","7609545a":"code","a55ca3dc":"code","fd8d20f3":"code","8eab5065":"code","ffd21d6d":"code","279da4b3":"code","819301d6":"code","8398bfe5":"code","dbf20868":"code","e3d8addc":"code","6fe80a68":"code","807de382":"code","d21da4fd":"code","aeb06390":"code","f2c7b60d":"code","ac6231cb":"code","51fe635c":"code","1a719f94":"code","6ec1e2f7":"code","f4a75c70":"code","76ac0577":"code","c662c28a":"code","f049db4d":"code","a62c16fb":"code","335a3e68":"code","9f31681c":"code","e4d7f554":"code","3f556b56":"code","b5678898":"code","38baef8f":"code","259109f8":"code","395d7c2c":"code","d08845e3":"code","f616d212":"code","ffe8fa14":"code","0fa0e4b9":"code","f927a3ff":"code","11ae4f14":"code","7b5827f0":"code","b49fe3d1":"code","8796c2ed":"code","86771f25":"code","98e4b62c":"code","1ff66d80":"code","35ad23ba":"code","4dc59d5b":"code","bac7308f":"code","6936fb40":"code","c8773e4a":"code","25024def":"code","3ed174b4":"markdown","7e30e1cc":"markdown","94df0e65":"markdown","36041489":"markdown","9583d703":"markdown","9cc783ca":"markdown","23499ffd":"markdown","db1d14ea":"markdown","a39f8587":"markdown","dec43392":"markdown","4cf85205":"markdown","8d96f5d5":"markdown","c27c0c9d":"markdown","44b78fb0":"markdown","6ceaecec":"markdown","3e7ab916":"markdown","9549f0bb":"markdown","f91aae6f":"markdown","6cf0dcee":"markdown","3b16ac18":"markdown","514ab485":"markdown","a5d302ac":"markdown","b8d868ce":"markdown","9e908d70":"markdown","5a3e986b":"markdown","bdf3a7a2":"markdown","1b01d945":"markdown","790aa7b9":"markdown","7472b248":"markdown","b7b7c931":"markdown","31afa1d3":"markdown","76541b25":"markdown","6870c5b0":"markdown","7a301411":"markdown","188798a8":"markdown","40ef8a9e":"markdown","e93a9ff7":"markdown","a79e6942":"markdown","87280d0f":"markdown","bac8e5e0":"markdown","3375d2ea":"markdown","ed4fe4a0":"markdown","00619331":"markdown"},"source":{"c8dc7974":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.pandas.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","465b5b8a":"import warnings\nwarnings.filterwarnings('ignore')\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","e2805be7":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","9efc06b5":"print('''\nThe dimensions of:\ntraining set: {}\ntest set: {}\n'''.format(train.shape, test.shape))","13c0f281":"features_with_na = [feature for feature in train.columns\n                  if train[feature].isnull().sum() >= 1]\n\nprint('The features having NaN values are:')\nfeatures_with_na","0cdbd81b":"import missingno as msn\nprint('The viz. below gives us a qualitative idea about the percent of missing values in each feature')\nmsn.bar(train[features_with_na], figsize = (15,10), color = 'orange')","60c04981":"for feature in features_with_na:\n    num_na = train[feature].isnull().sum()\n    tot = len(train)\n    print('**{}**: {}% missing values'.format(feature, round((num_na\/tot)*100, 3)))","257e8662":"for feature in features_with_na:\n    data = train.copy()\n    data[feature] = np.where(data[feature].isnull(),1,0)\n    \n    data.groupby(feature)['SalePrice'].median().plot.bar(color = ['blue', 'orange'])\n    plt.xlabel(feature + '(is 1 if NaN, 0 otherwise)')\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","16a53ae5":"dataset = train.copy()# make a copy of training set to work on\ndataset.drop('Id', axis = 1, inplace = True)","1bb59972":"dataset.head()","4e18b533":"# considering those as numerical features which do not have datatype as 'object'\nnum_features = [feature for feature in dataset.columns\n               if dataset[feature].dtypes != 'O']\nprint('Numerical features in our dataset are:')\nnum_features","65982216":"time_features = [feature for feature in num_features\n                if 'Yr' in feature or 'Year' in feature]\nprint('Datetime related features are:')\ntime_features","9e0965eb":"sns.set(style = 'darkgrid')\nfor feature in time_features:\n    if feature != 'YrSold':\n        data = dataset.copy()\n        data[feature] = data['YrSold'] - data[feature]\n        plt.figure(figsize = (10,7))\n        sns.regplot(data[feature], data['SalePrice'], scatter = True, marker = '*', scatter_kws = {'color': 'g'}, line_kws = {'color': 'r'})\n        plt.title(feature)\n        plt.ylabel('SalePrice')\n        plt.show()\n        \n    ","c71b703d":"disc_features = [feature for feature in num_features\n                if dataset[feature].nunique() < 25 and feature not in time_features]\nprint('The discrete numerical features in our dataset are:')\ndisc_features","ee7f12da":"for feature in disc_features:\n    data = dataset.copy()\n    sns.barplot(data[feature], data['SalePrice'])\n    plt.title(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","148a6c0b":"cont_features = [feature for feature in num_features\n                if feature not in disc_features + time_features]\nprint('Continuous numerical features in our dataset are:')\ncont_features","fca643ee":"\nfor feature in cont_features:\n    data = dataset.copy()\n    plt.figure(figsize = (10,6))\n    sns.distplot(data[feature], color ='orange')\n    plt.show()","b1158feb":"import scipy.stats as stat\nimport pylab\n\ndef plot_data(df, feature):\n    plt.figure(figsize = (10,6))\n    plt.subplot(2,1,1)\n    plt.hist(df[feature])\n    plt.title(feature)\n    plt.subplot(2,1,2)\n    stat.probplot(df[feature], dist = 'norm', plot = pylab)\n    plt.show()\n    ","f8396b41":"cont_feat_0 = [feature for feature in cont_features\n              if 0 in dataset[feature].unique()]\nprint('Continous features that have 0 in them:')\ncont_feat_0","6924ed47":"cont_features","2aa4fe76":"\nfor feature in cont_features:\n    data = dataset.copy()\n    if 0 not in data[feature].unique():\n        data[feature] = np.log(data[feature])\n        plot_data(data, feature)\n    ","dc81f9f6":"for feature in cont_features:\n    data = dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data['SalePrice'] = np.log(data['SalePrice'])\n        sns.regplot(data[feature], data['SalePrice'], scatter_kws = {'color': 'g'}, line_kws = {'color': 'red'})\n        plt.show()","9474e996":"for feature in cont_features:\n    data = dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data.boxplot(column = feature)\n        plt.show()\n        ","822be184":"cat_features = [feature for feature in dataset.columns\n              if dataset[feature].dtypes == 'O']\ncat_features","57ccf48b":"print('Number of unique categories per feature:\\n')\nfor feature in cat_features:\n    print('**{}** : {} unique features'.format(feature, data[feature].nunique()))","838d2d97":"for feature in cat_features:\n    data = dataset.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar(color = 'g')\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","38bbabf7":"cat_na = [feature for feature in cat_features\n         if dataset[feature].isna().sum() >= 1]\nprint('Categorical features that have missing values:\\n')\ncat_na","ed4b6b5d":"for feature in cat_na:\n    dataset[feature] = dataset[feature].fillna('Missing')","62a2acb0":"cont_num_na = [feature for feature in cont_features\n         if dataset[feature].isna().sum() >= 1]\nprint('Continuous numerical features that have missing values:\\n')\ncont_num_na","be771e0d":"disc_num_na = [feature for feature in disc_features\n         if dataset[feature].isna().sum() >= 1]\ndisc_num_na","4d4fb343":"for feature in cont_num_na:\n    dataset[feature] = dataset[feature].fillna(dataset[feature].median())\ndataset.isna().sum()","1cb185f4":"time_features","0be3fae3":"for feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    dataset[feature] = dataset['YrSold'] - dataset[feature]","9a37fc4a":"for feature in cont_features:\n    if 0 in dataset[feature].unique():\n        pass\n    else:\n        dataset[feature] = np.log(dataset[feature])","24125ed4":"feature_scale = [feature for feature in dataset.columns\n                if feature not in ['Id', 'SalePrice']]","256e3e90":"dataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].median())\ndataset.isna().sum().sort_values(ascending = False)","761fd95b":"dataset.isna().sum().sort_values(ascending = False)","31f6c446":"cont_pred = cont_features[:-1]\ncont_pred","47d544b5":"num_features\nnum_pred = num_features[:-1]\nnum_pred = num_pred[:-1]\nnum_pred","98a71beb":"dataset[time_features]","bb02a36f":"dataset['SalePrice']","ba975f25":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(dataset[num_features].drop(['SalePrice', 'YrSold'], axis = 1))\nsc = MinMaxScaler()\nsc.fit(dataset[['SalePrice']])","6d194908":"X_train = pd.concat([train['Id'].reset_index(drop = True), dataset[cat_features].reset_index(drop = True), pd.DataFrame(scaler.transform(dataset[num_features].drop(['SalePrice','YrSold'], axis = 1)), columns = num_pred), pd.DataFrame(sc.transform(dataset[['SalePrice']]), columns = ['SalePrice'])], axis = 1)","b31b9bc1":"num_features.remove('YrSold')","e005cf6c":"X_train.head()","529e40f8":"from sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder(handle_unknown = 'ignore')\noe.fit(X_train[cat_features])","183a922a":"X_f = pd.concat([train['Id'].reset_index(drop = True), X_train[num_features].reset_index(drop = True), pd.DataFrame(oe.transform(X_train[cat_features]), columns = cat_features)], axis = 1)","d3267c0d":"X_f.head()","c06ad287":"copy2 = X_f.copy()","0f12af8c":"from sklearn.ensemble import ExtraTreesRegressor\nmod = ExtraTreesRegressor(n_estimators=100)\nmod.fit(copy2.drop('SalePrice', axis = 1), copy2['SalePrice'])","8de6e152":"imp = pd.Series(mod.feature_importances_, index = copy2.drop('SalePrice', axis = 1).columns)","971b874d":"X_imp = imp.sort_values(ascending = False).head(20)","4cfdfad6":"X_selected = X_imp[:12].index.values","c21c4aa2":"X_f[X_selected]","813f8a79":"X_selected = X_f[X_selected]","f758b1c6":"X_selected.head()","01fa1cec":"y = X_f['SalePrice']","5b6609d8":"y.head()","349d4dc1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size = 0.2)\nX_train.shape, X_test.shape","e665de0d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics","fee909a0":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)","48e54530":"cvs = cross_val_score(lr, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by Linear Regression is {}'.format(cvs.mean()))","302a8725":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nclf = GridSearchCV(dtr, param_grid = {'criterion': ['mse', 'friedman_mse' 'mae', 'poisson']}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclf.fit(X_train,y_train)","6afc469f":"print('The best score by Decision Tree Regressor is given by the parameters {} which is {}'.format(clf.best_params_, clf.best_score_))","67593db8":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nrclf = GridSearchCV(rf, param_grid = {'n_estimators': [100,150], 'criterion': ['mse']}, cv = 10, scoring = 'neg_root_mean_squared_error')\nrclf.fit(X_train,y_train)","7f9c4e50":"print('The best score for Random Forest Regressor is given by the parameters {} which is {}'.format(rclf.best_params_, rclf.best_score_))","05aec3dc":"from sklearn.svm import SVR\n\nsvm = SVR()\nclfsvm = GridSearchCV(svm, param_grid = {'kernel': ['rbf', 'linear'], 'C': [0.1, 0.5, 1, 2]}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclfsvm.fit(X_train,y_train)","71778cb1":"print('The best score for SVM is given by the parameters {} which is {}'.format(clfsvm.best_params_, clfsvm.best_score_))","b2bc59d2":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor()\nclfknn = GridSearchCV(knn, param_grid = {'n_neighbors': [5,7,10], 'weights': ['unform', 'distance']}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclfknn.fit(X_train, y_train)\n","5c3f6bca":"print('The best score for KNN is given by the parameters {} which is {}'.format(clfknn.best_params_, clfknn.best_score_))","000ba772":"from sklearn.ensemble import AdaBoostRegressor\n\nada = AdaBoostRegressor()\n\nclfada = GridSearchCV(ada, param_grid = {'n_estimators': np.arange(50, 150, 20), 'learning_rate': np.arange(0.1, 1, 0.2)}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclfada.fit(X_train, y_train)","30e7c2fb":"print('The best score for AdaBoost is given by the parameters {} which is {}'.format(clfada.best_params_, clfada.best_score_))","eba60cf7":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\nclfgb = GridSearchCV(gb, param_grid = {'n_estimators': np.arange(100, 160, 20), 'learning_rate': np.arange(0.1, 1,0.2)}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclfgb.fit(X_train,y_train)","8c73678a":"print('The best score for GradientBoost is given by the parameters {} which is {}'.format(clfgb.best_params_, clfgb.best_score_))","04b9113f":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nclfxg = GridSearchCV(xgb, param_grid = {'n_estimators': np.arange(100, 160, 20), 'learning_rate': np.arange(0.1, 1,0.2)}, cv = 10, scoring = 'neg_root_mean_squared_error')\nclfxg.fit(X_train, y_train)","7b7107b5":"print('The best score for XGBoost is given by the parameters {} which is {}'.format(clfxg.best_params_, clfxg.best_score_))","5f8e3c2b":"X_test.shape, y_test.shape","0b8457b4":"cvs = cross_val_score(lr, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by Linear Regression is {}'.format(cvs.mean()))\n\n","24b2069b":"dt = DecisionTreeRegressor(criterion = 'mse')\ndts = cross_val_score(dt, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by Decision Tree Regressor is {}'.format(dts.mean()))","eb4c95cd":"rf = RandomForestRegressor(criterion = 'mse', n_estimators =  150)\nrfs = cross_val_score(rf, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by Random Forest Regressor is {}'.format(rfs.mean()))","4b88b948":"svm = SVR(C =  1, kernel = 'linear')\nsvms = cross_val_score(svm, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by SVM is {}'.format(svms.mean()))","223c38a6":"knn = KNeighborsRegressor(n_neighbors =  10, weights = 'distance')\nknns = cross_val_score(knn, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by KNN is {}'.format(knns.mean()))","3b09322a":"ada = AdaBoostRegressor(learning_rate =  0.6, n_estimators =  70)\nadas = cross_val_score(ada, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by AdaBoost is {}'.format(adas.mean()))","828a96d3":"gb = GradientBoostingRegressor(learning_rate = 0.2, n_estimators = 130)\ngbs = cross_val_score(gb, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by GradientBoost is {}'.format(gbs.mean()))","2a2c8cf5":"xg = XGBRegressor(learning_rate = 0.2, n_estimators = 100)\nxgs = cross_val_score(xg, X_test, y_test, cv = 10, scoring = 'neg_root_mean_squared_error')\nprint('Cross validation score given by XGBoost is {}'.format(xgs.mean()))","47ff151d":"models = ['Simple Regression', 'Decision Tree', 'Random Forest', 'KNN', 'AdaBoost', 'GradientBoost', 'XgBoost', 'SVM']\nscores = [cvs.mean(), dts.mean(), rfs.mean(), knns.mean(), adas.mean(), gbs.mean(), xgs.mean(), svms.mean()]\nscore_df = pd.DataFrame( data = scores ,index = models, columns = ['Score'])","b72d40e4":"score_df.sort_values(by = 'Score', ascending = False)","7609545a":"test.head()","a55ca3dc":"test1 = test.drop('Id', axis = 1)","fd8d20f3":"test1['YearBuilt'] = test1['YrSold'] - test1['YearBuilt']","8eab5065":"cat = [feature for feature in test1.columns\n      if test1[feature].dtype == 'O']\n\nnum = [feature for feature in test1.columns\n      if test1[feature].dtype != 'O']\n\ndisc = [feature for feature in num\n       if test1[feature].nunique() < 25]\n\ncont = [feature for feature in num\n       if feature not in disc]","ffd21d6d":"cat_na = [feature for feature in cat\n         if test1[feature].isna().sum() >= 1]\nprint('Categorical Features with NaN values:')\ncat_na","279da4b3":"for feature in cat_na:\n    test1[feature] = test1[feature].fillna('Missing')","819301d6":"disc_nan = [feature for feature in disc\n           if test1[feature].isna().sum() >= 1]\nprint('Discrete Numerical features with NaN values:')\ndisc_nan","8398bfe5":"for feature in disc_nan:\n    test1[feature] = test1[feature].fillna(test1[feature].mode())","dbf20868":"cont_notime = [feature for feature in cont\n               if feature not in time_features]\ncont_notime","e3d8addc":"for feature in cont_notime:\n    if 0 in test1[feature].unique():\n        pass\n    else:\n        test1[feature] = np.log(test1[feature])","6fe80a68":"t = pd.concat([test['Id'].reset_index(drop = True), test1[cat_features].reset_index(drop = True), pd.DataFrame(scaler.transform(test1[num_pred]), columns = num_pred)], axis = 1)","807de382":"t.head()","d21da4fd":"X_ftest = pd.concat([t['Id'].reset_index(drop = True), t[num_pred].reset_index(drop = True), pd.DataFrame(oe.transform(t[cat_features]), columns = cat_features)], axis = 1)","aeb06390":"X_ftest.head()","f2c7b60d":"selected_features = ['ExterQual','OverallQual','GrLivArea','GarageCars','FullBath','CentralAir','GarageType','BsmtQual','Fireplaces','1stFlrSF','YearBuilt','KitchenQual']","ac6231cb":"\ntestdata = X_ftest[selected_features]\ntestdata.columns","51fe635c":"testdata.isna().sum()","1a719f94":"testdata['GarageCars'] = testdata['GarageCars'].fillna(0.50)","6ec1e2f7":"testdata.isna().sum()","f4a75c70":"X_selected.head()","76ac0577":"y_predlr = lr.predict(testdata)","c662c28a":"rf.fit(X_selected, y)\ny_predrf = rf.predict(testdata)","f049db4d":"xg.fit(X_selected, y)\ny_predxgb = xg.predict(testdata)","a62c16fb":"gb.fit(X_selected, y)\ny_predgb = gb.predict(testdata)","335a3e68":"preds = [y_predlr, y_predrf, y_predxgb, y_predgb]","9f31681c":"y_predlrinv = sc.inverse_transform([y_predlr])","e4d7f554":"y_predlrinv","3f556b56":"y_predlefin = np.exp(y_predlrinv)","b5678898":"y_predlefin","38baef8f":"ylr = list(y_predlefin)","259109f8":"dflr = pd.DataFrame()","395d7c2c":"dflr['Id'] = test['Id']","d08845e3":"dflr['SalePrice'] = y_predlefin.T","f616d212":"dflr.head()","ffe8fa14":"dflr.to_csv('Linreg.csv', index = False)","0fa0e4b9":"y_predrfinv = sc.inverse_transform([y_predrf]) ","f927a3ff":"y_predrffin = np.exp(y_predrfinv)","11ae4f14":"dfrf = pd.DataFrame()\ndfrf['Id'] = test['Id']\ndfrf['SalePrice'] = y_predrffin.T","7b5827f0":"dfrf.head()","b49fe3d1":"dfrf.to_csv('RandomForest.csv', index = False)","8796c2ed":"y_predgbinv = sc.inverse_transform([y_predgb]) ","86771f25":"y_predgbfin = np.exp(y_predgbinv)","98e4b62c":"dfgb = pd.DataFrame()\ndfgb['Id'] = test['Id']\ndfgb['SalePrice'] = y_predgbfin.T","1ff66d80":"dfgb.head()","35ad23ba":"dfgb.to_csv('GB.csv', index = False)","4dc59d5b":"y_predxgbinv = sc.inverse_transform([y_predxgb]) ","bac7308f":"y_predxgbfin = np.exp(y_predxgbinv)","6936fb40":"dfxgb = pd.DataFrame()\ndfxgb['Id'] = test['Id']\ndfxgb['SalePrice'] = y_predxgbfin.T","c8773e4a":"dfxgb.head()","25024def":"dfxgb.to_csv('XGB.csv', index = False)","3ed174b4":"### Decision Trees","7e30e1cc":"### XgBoost","94df0e65":"## Model Building","36041489":"### Gradient Boosting","9583d703":"The above plots give us an idea about the relationship between the categorical features and the Sale Price of the house","9cc783ca":"## Numerical Features can be classified into two:\n 1. Discrete numerical features\n 2. Continuous numerical features","23499ffd":"### SVM","db1d14ea":"- We see that most of the distributions are right skewed, therefore need to perform log transformation on them to get a better fit.","a39f8587":"The above plots suggest that:\n1. The sale price of older homes is less than that of new homes.\n2. Recent remodelling or garage building has a positive impact on the sale price of the house. ","dec43392":"- All NaN values have been dealt with","4cf85205":"- The above plots give us a qualitative overview of the outliers present with respect to each continuous numerical feature.","8d96f5d5":"## Continuous Numerical Features","c27c0c9d":"- we can observe that our numerical features also contains those which are datetime related like **'YrSold'** etc.\n- These need to be separated.","44b78fb0":"Loading the data from the csv files into dataframes","6ceaecec":"We will consider a numerical feature continuous if it does not come under discrete or the datetime features.","3e7ab916":"# Outliers","9549f0bb":"# Categorical Features","f91aae6f":"### KNN","6cf0dcee":"- We will now split our training set into 2 parts to check how our models perform on unseen data.","3b16ac18":"- Now we will plot the distributions of the continuous features.","514ab485":"- All the NaN values have been dealt with, Now we need to standardize the numerical features and bring them between 0 and 1 so that our model performs good","a5d302ac":"Now let us look at the effect of these features on the Sale Price of the house:","b8d868ce":"### Multivariate Regression","9e908d70":"### Modifying the test data","5a3e986b":"- In most of the above features we see that the missing values has a relationship with the 'Sale Price' so we cannot just drop them, we will have to fill those NaN with suitable values. ","bdf3a7a2":"- For dealing with missing values in the numerical features:\n1. for continuous ones, we will replace them by the median of the feature.\n2. for discrete ones, the NaN will be replaced by the mode.","1b01d945":"# Feature Selection","790aa7b9":"- Log transforming the continuous features to remove skewness","7472b248":"- Now we have the predicted values but they need to be scaled up and tronsformed using the exponential function","b7b7c931":"- Now let us see how our models perform on the rest of 20% data they have not seen","31afa1d3":"We will consider a feature discrete if it takes less than 25 unique values and is not datetime related","76541b25":"# Missing Values","6870c5b0":"- The above plots show the relationship after the log transformation of the continuous numerical features.","7a301411":"### Adaboost","188798a8":"# Numerical Features\n- Now that we have taken an initial look at the missing values, let us look at the numerical features in our dataset.","40ef8a9e":"- The features with datatype as 'object'","e93a9ff7":"- We will select the right features.\n- Fill the missing values in categorical variables with 'Missing'\n- Fill the missing values in continuous numerical variables with median of that feature\n- Fill the missing values in Discrete numerical variables with mode of that feature\n- Deal with the time features\n- Log transform the continuous features\n- Groups the 'Rare' categoricals\n- Scaling of continuous features\n- Encoding categoricals","a79e6942":"## Discrete Numerical Features","87280d0f":"### Random Forest","bac8e5e0":"- 4 features have more than 80% missing values\n- Now let us see whether the missing values have a relationship with our target variable ( 'SalePrice' )","3375d2ea":"Some discrete numerical features like **'FirePlace', 'OverallQual'** show clear relationship with sale price of the house.","ed4fe4a0":"- Let us focus on the datetime feature and create 3 new features that give us:\n1. Number of years ago the house was built\n2. Number of years ago the house was remodelled\n3. How old is the Garage\n- and let us see their relationship with our target variable, that is, Sale Price.","00619331":"- Now we will deal with the missing values present in the categorical features by replacing NaN with 'Missing' "}}