{"cell_type":{"d295475c":"code","08266db7":"code","fe18703b":"code","d9ec093d":"code","17ddbd96":"code","43498f16":"code","0262802a":"code","13e11923":"code","b306c388":"code","630a31d4":"code","a99e6229":"code","57fb6d75":"code","07b16441":"code","2e7da7ee":"code","a85ac3b9":"code","83c04c22":"code","206431ec":"code","e3093f27":"code","483f6a90":"code","3693e697":"code","fc32bdd6":"code","28958a0c":"code","97936e3b":"code","0214bb20":"code","5ce01b58":"code","54454d99":"code","77e902fa":"code","9316174e":"code","d9fc3793":"code","064aceb1":"code","579b0862":"code","9100f53f":"code","9e10d830":"code","550a1165":"code","d2bda044":"code","976e1c67":"code","8e371a6c":"code","7b937c64":"code","26de785d":"code","b465e560":"code","0d9a8719":"code","23d85e32":"code","a4d9978e":"code","85fae2a3":"code","8f699872":"code","24ddf321":"code","02427d0a":"code","a0e5cc86":"code","62d888b9":"code","9513fcb3":"code","35df4532":"code","04af5e2e":"code","4aac7b9f":"code","c33409b8":"code","6907a008":"code","2ea71404":"code","beb83679":"code","6c8d59f4":"code","742b1b3b":"code","ce68f002":"code","10fd02d5":"code","e5e031e6":"code","c0de1182":"code","ed85a4ad":"code","1fd8f6e8":"code","2886705a":"code","2c81ae85":"code","58abdd4f":"code","6760aa5a":"code","f5b0acb0":"code","fb594f80":"code","5edc7cd8":"code","b1eb90fe":"code","eff8859d":"code","afc88b37":"code","aefb5788":"code","31bf0286":"code","922124e2":"code","ee63c5fe":"code","1ececd38":"code","09e681a2":"code","253d1a4f":"code","223b78ab":"code","716cb1cb":"code","b5f527b5":"code","c298c0cd":"code","0896a3bb":"code","b047eaa5":"code","5363334c":"code","5473bfa6":"code","b0681468":"code","e08d8233":"code","aeb32a9d":"code","bf4a71a6":"code","eddb6291":"code","b2c349ef":"code","780fd235":"code","c85b36f3":"code","5693d475":"code","e3c27ff3":"code","250a1be6":"code","4111e499":"code","87b7934f":"code","d6217386":"code","706536f9":"code","bc69f736":"code","73f2d432":"code","f856fba6":"code","7a65c82b":"markdown","020326b4":"markdown","5b979041":"markdown","19dcc23d":"markdown","5aa5547d":"markdown","9fa48b3c":"markdown","8ca3be08":"markdown","59b4f3bc":"markdown","c5d0f994":"markdown","dabd0cd0":"markdown","1326b39e":"markdown","122bb198":"markdown","9a2e51dc":"markdown","52bd4600":"markdown","78b1e7f0":"markdown","5c6b85fc":"markdown","9fd6e3e0":"markdown","66f66076":"markdown","01af9331":"markdown","f6b0d850":"markdown","d39f120a":"markdown","ed04c675":"markdown","84f328ad":"markdown","63adae8c":"markdown","39c69112":"markdown","8044f01f":"markdown","80186e90":"markdown"},"source":{"d295475c":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline","08266db7":"#Read and explore the dataset\n\nurl = 'https:\/\/drive.google.com\/file\/d\/1ZxD4eAR1DK_2ts0u4uxZ1bOtIMnrJ3JJ\/view?usp=sharing'\nfile_id = url.split('\/')[-2]\ndwn_url='https:\/\/drive.google.com\/uc?id=' + file_id\n\nmktdata = pd.read_csv(dwn_url)\nprint(mktdata.info())\nmktdata.head()","fe18703b":"# Save dataset in a new DataFrame and keep the original for future usage\nmktdata_E = mktdata\n\n#Noticed the column lable \" Income \" has 2 spaces so I will remove them\nmktdata_E.columns = mktdata_E.columns.str.strip(\" \")\n\n#reformat the Income variable to float \nmktdata_E['Income'] = mktdata_E['Income'].str.replace('$', '')\nmktdata_E['Income'] = mktdata_E['Income'].str.replace(',', '').astype('float')\n\nprint(mktdata_E['Dt_Customer'].head())\nmktdata_E.info()","d9ec093d":"#Noticed Income variable has 24 missing values, I will try to explore the variable to see which method will be used to impuse \nfeatures = ['Income']\n\nsns.set_style('dark')\nfor col in features:\n    plt.figure(figsize=(25,4))\n    plt.subplot(131)\n    sns.distplot(mktdata_E[col], label=\"skew: \" + str(np.round(mktdata_E[col].skew(),2)))\n    plt.legend()\n    plt.subplot(132)\n    sns.boxplot(mktdata_E[col])\n\n    plt.tight_layout()\n    plt.show()","17ddbd96":"#impute missing values in Income column with the median value\nmktdata_E['Income'] = mktdata_E['Income'].fillna(mktdata_E['Income'].median())","43498f16":"mktdata_E.info()","0262802a":"for col in ['AcceptedCmp1','AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']:\n  print(\"% customer accepted the campaign \" + col + \" is \" + str(mktdata_E[col].sum() \/ len(mktdata_E[col]) *100) + \" %\")","13e11923":"# The column Dt_Customer should be stored as date and time type\nmktdata_E['Dt_Customer'] = mktdata_E['Dt_Customer'].astype('datetime64[ns]')\n# Calculate the number of year as a customer for each row and store in a new column\nfrom datetime import datetime\nmktdata_E['Years_as_Cust'] = pd.DatetimeIndex(mktdata_E['Dt_Customer']).year\nmktdata_E['Years_as_Cust'] = 2020 - mktdata_E['Years_as_Cust']\nprint(mktdata['Years_as_Cust'].describe())","b306c388":"# Calculate the ages of customers\nmktdata_E['Ages'] = 2020 - mktdata_E['Year_Birth']\nprint(mktdata_E['Ages'].describe())","630a31d4":"# Calculate total amount of product purchased and save them into 'MntTotal'\ncolumns_Mnt = ['MntFruits', 'MntMeatProducts', 'MntFishProducts',\n       'MntSweetProducts', 'MntGoldProds']\nmktdata_E['MntTotal'] = mktdata_E[columns_Mnt].sum(axis=1)","a99e6229":"# Calculate total purchases and save them into 'TotalPurchases'\ncolumns_pur = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\nmktdata_E['TotalPurchases'] = mktdata_E[columns_pur].sum(axis=1)","57fb6d75":"# Assign new variable \"ChildHome\" with '0' means no children and '1' means having children\ndef f(row):\n    if row['Kidhome'] + row['Teenhome'] > 0:\n        val = 1\n    else:\n        val = 0\n    return val\n\n\nmktdata_E['ChildHome'] = mktdata_E.apply(f, axis=1)","07b16441":"# Assign a new variable \"UsedDeal\": '0' means Haven't purchased any deal, '1' means have purchased at least 1 deal\ndef d(row):\n    if row['NumDealsPurchases'] > 0:\n        val = 1\n    else:\n        val = 0\n    return val\nmktdata_E['UsedDeal'] = mktdata_E.apply(d, axis=1)","2e7da7ee":"mktdata_E.head()","a85ac3b9":"mktdata_E.describe()","83c04c22":"mktdata_E.info()","206431ec":"mktdata_E.columns.values","e3093f27":"# Select which columns to plot, I eliminate all categorical variables and binary variables\ncol_to_plot = ['Year_Birth', 'Income',\n       'Kidhome', 'Teenhome', 'Recency', 'MntWines',\n       'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n       'NumWebVisitsMonth',  'Ages', 'MntTotal',\n       'TotalPurchases']\n\n\n# subplots\nsns.set_style('dark')\nmktdata_E[col_to_plot].plot(subplots=True, layout=(5,5), kind='box', figsize=(25,20), patch_artist=True)\nplt.subplots_adjust(wspace=0.5);","483f6a90":"from scipy.stats import skew","3693e697":"def dist_plot(df, cols):\n  sns.set_style('dark')\n  plt.figure(figsize=(25,20))\n  for i, col in enumerate(cols):\n    ax = plt.subplot(5,6, i+1)\n    sns.kdeplot(df[col], ax=ax)\n    plt.text(0.5,0.9,\"skew= \" + str(round(skew(df[col]),2)), bbox=dict(facecolor='yellow', alpha=0.5), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n    plt.xlabel(col)\n  plt.show()","fc32bdd6":"  dist_plot(mktdata_E,col_to_plot)","28958a0c":"# Look for any correlation\nplt.figure(figsize=(25,12))\nsns.heatmap(mktdata_E.corr(), annot=True)\nplt.show()","97936e3b":"# Save the dataset into new DataFrame called mktdata_cap and continue with cap & floor the outliers\nmktdata_cap = mktdata_E\nfeatures = ['Year_Birth', 'Income', 'Recency', 'MntWines',\n       'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n       'NumWebVisitsMonth', 'Ages', 'MntTotal',\n       'TotalPurchases']","0214bb20":"def iqr_capping(df, cols, factor):\n    \n    for col in cols:\n        \n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        \n        iqr = q3 - q1\n        \n        upper_whisker = q3 + (factor*iqr)\n        lower_whisker = q1 - (factor*iqr)\n        \n        df[col] = np.where(df[col]>upper_whisker, upper_whisker,\n                 np.where(df[col]<lower_whisker, lower_whisker, df[col]))","5ce01b58":"q1 = mktdata_E['Income'].quantile(0.25)\nq3 = mktdata_E['Income'].quantile(0.75)\niqr = q3 - q1\nupper_whisker = q3 + (1.5*iqr)\nlower_whisker = q1 - (1.5*iqr)\nprint(\"Income upper_whisker = \" + str(round(upper_whisker,2)))\nprint(\"Income lower_whisker = \" + str(round(lower_whisker,2)))\n","54454d99":"q1 = mktdata_E['Ages'].quantile(0.25)\nq3 = mktdata_E['Ages'].quantile(0.75)\niqr = q3 - q1\nupper_whisker = q3 + (1.5*iqr)\nlower_whisker = q1 - (1.5*iqr)\nprint(\"Ages upper_whisker = \" + str(round(upper_whisker,2)))\nprint(\"Ages lower_whisker = \" + str(round(lower_whisker,2)))","77e902fa":"iqr_capping(mktdata_cap, features, 1.5)\n","9316174e":"sns.set_style('dark')\nmktdata_cap[col_to_plot].plot(subplots=True, layout=(5,5), kind='box', figsize=(25,20), patch_artist=True)\nplt.subplots_adjust(wspace=0.5);","d9fc3793":"dist_plot(mktdata_cap, col_to_plot)","064aceb1":"mktdata_cap","579b0862":"#Standardize skewed variables\nfrom sklearn.preprocessing import StandardScaler\ncolumns_scaled = ['Year_Birth', 'Income', 'Recency', 'MntWines',\n       'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n       'NumWebVisitsMonth',  'Ages', 'MntTotal',\n       'TotalPurchases', 'Years_as_Cust']\n\nscalar = StandardScaler()\nmktdata_scaled = pd.DataFrame(scalar.fit_transform(mktdata_cap[columns_scaled]), columns=columns_scaled, index=mktdata_cap.index)\nmktdata_scaled","9100f53f":"#Adding scaled columns back to dataset\nmktdata_scaled = pd.concat([mktdata_scaled, mktdata_cap.drop(axis=1, columns=columns_scaled)],axis=1)","9e10d830":"# Encoding categorical variables using pandas\ncat_columns = mktdata_scaled.select_dtypes(exclude = np.number)\nprint(\"Number of unique values per categorical feature:\\n\", cat_columns.nunique())\ndummies_cat = pd.get_dummies(cat_columns)","550a1165":"# Add to mktdat_scaled Dataframe and changed the name as mktdata_trf\nmktdata_trf = pd.concat([mktdata_scaled,dummies_cat], axis=1)\nmktdata_trf = mktdata_trf.drop(columns=cat_columns.columns)\nmktdata_trf = mktdata_trf.drop(['ID'], axis=1)\nmktdata_trf.info()","d2bda044":"mktdata_trf","976e1c67":"dist_plot(mktdata_trf,col_to_plot)","8e371a6c":"#Apply PCA \nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nX_red = pca.fit_transform(mktdata_trf)","7b937c64":"pca.n_components_","26de785d":"pcaSummary = pd.DataFrame({'std_deviation': np.sqrt(pca.explained_variance_),\n              'prop_of_variance' : pca.explained_variance_ratio_,\n              'cum_prop_var': np.cumsum(pca.explained_variance_ratio_)})\npcaSummary_tps = pcaSummary.transpose()\npcaSummary_tps.columns = ['PC{}'.format(i) for i in range(1,len(pcaSummary_tps.columns) + 1)]\npcaSummary_tps","b465e560":"pcacomponents_df = pd.DataFrame(pca.components_.transpose(), columns=pcaSummary_tps.columns,\n                                index=mktdata_trf.columns)","0d9a8719":"pcacomponents_df","23d85e32":"from sklearn.cluster import KMeans\n\nkmeans_models = [KMeans(n_clusters=k, random_state=23).fit(X_red) for k in range (1, 10)]\ninnertia = [model.inertia_ for model in kmeans_models]\n\nplt.plot(range(1, 10), innertia)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","a4d9978e":"from sklearn.metrics import silhouette_score\n\nsilhoutte_scores = [silhouette_score(X_red, model.labels_) for model in kmeans_models[1:4]]\nplt.plot(range(2,5), silhoutte_scores, \"bo-\")\nplt.xticks([2, 3, 4, 5])\nplt.title('Silhoutte scores vs Number of clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhoutte score')\nplt.show()","85fae2a3":"from sklearn.metrics import silhouette_score\n\nkmeans = KMeans(n_clusters=3, random_state=23)\nkmeans.fit(X_red)\n\nprint('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans.labels_)))","8f699872":"#adding cluster labels column into dataset\nmktdata_E['cluster_id'] = kmeans.labels_\nmktdata_trf['cluster_id'] = kmeans.labels_","24ddf321":"mktdata_trf = pd.get_dummies(mktdata_trf, columns=['cluster_id'])\nmktdata_trf","02427d0a":"mktdata_E.head()","a0e5cc86":"mktdata_E.groupby('cluster_id').mean()","62d888b9":"mktdata_trf","9513fcb3":"#change label name of cluster_id\nmktdata_E = mktdata.assign(cluster_cat=pd.cut(mktdata['cluster_id'], \n                               bins=[-1, 0.5, 1.5, 2.5], \n                               labels=['H&H', 'M&M', 'L&L']))\nmktdata_E.head()\n","35df4532":"# Partition and transform data before modeling \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nr_state = 3\n\n\n# isolate X and y variables, and perform train-test split\nX = mktdata_trf.drop(['Response'], axis=1)\ny = mktdata_trf['Response']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n","04af5e2e":"# Logistic regression model\nlored = LogisticRegression(C=1e9,solver='liblinear')\nlored.fit(X_train, y_train)\n\n# predictions\ny_pred_lored = lored.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Logistic regression model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_lored)))\nprint(\"Median value of target variable: \", y.median())","4aac7b9f":"# import the metrics class\nfrom sklearn import metrics\ncm = metrics.confusion_matrix(y_test, y_pred_lored)\ncm","c33409b8":"cls_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\nticks = np.arange(len(cls_names))\nplt.xticks(ticks, cls_names)\nplt.yticks(ticks, cls_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix\\n', fontsize = 16)\nplt.ylabel('Actual label',fontsize = 16)\nplt.xlabel('Predicted label',fontsize = 16)","6907a008":"# Print accuracy, Precision, Recall and F1 Score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_lored))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_lored, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_lored, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_lored, average=\"binary\"))","2ea71404":"#Create DataFrame of coefficient\ncoefficient = pd.DataFrame({'Feature': X_train.columns, 'coefficient' : lored.coef_[0]})\ncoefficient = coefficient.sort_values(by=['coefficient'],ascending=False)\n\n# plot the coefficient \nax = coefficient.plot.bar(x='Feature', y='coefficient', figsize=(25,10),rot=0)\nplt.xticks(rotation=90)","beb83679":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.model_selection import cross_val_score\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred_maxtree = clf.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Decision Tree model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_maxtree)))\nprint(\"Median value of target variable: \", y.median())","6c8d59f4":"print(\"Max depth= \" + str(clf.get_depth()))\nprint(\"Number of Leaves= \" + str(clf.get_n_leaves()))","742b1b3b":"importance = pd.DataFrame({'Feature': X_train.columns, 'Importance Score' : clf.feature_importances_})\nimportance = importance.sort_values(by=['Importance Score'],ascending=False)\n\n# plot feature importance\nax = importance.plot.bar(x='Feature', y='Importance Score', figsize=(25,10),rot=0)\nplt.xticks(rotation=90)","ce68f002":"cm = metrics.confusion_matrix(y_test, y_pred_maxtree)\ncm","10fd02d5":"cls_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\nticks = np.arange(len(cls_names))\nplt.xticks(ticks, cls_names)\nplt.yticks(ticks, cls_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix\\n', fontsize = 16)\nplt.ylabel('Actual label',fontsize = 16)\nplt.xlabel('Predicted label',fontsize = 16)","e5e031e6":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_maxtree))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_maxtree, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_maxtree, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_maxtree, average=\"binary\"))","c0de1182":"!pip install graphviz","ed85a4ad":"!pip install pydotplus","1fd8f6e8":"from sklearn.tree import export_graphviz\nfrom io import StringIO\nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = X_train.columns,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('Tree_mktdata.png')\nImage(graph.create_png())","2886705a":"# Create Optimal Decision Tree classifer object\nclf_op = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=2,\n                       min_samples_split=6)\n\n# Train Decision Tree Classifer\nclf_op = clf_op.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred_optree = clf_op.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Optimal Decision Tree model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_optree)))\nprint(\"Median value of target variable: \", y.median())","2c81ae85":"print(\"Max depth= \" + str(clf_op.get_depth()))\nprint(\"Number of Leaves= \" + str(clf_op.get_n_leaves()))","58abdd4f":"importance = pd.DataFrame({'Feature': X_train.columns, 'Importance Score' : clf_op.feature_importances_})\nimportance = importance.sort_values(by=['Importance Score'],ascending=False)\n\n# plot feature importance\nax = importance.plot.bar(x='Feature', y='Importance Score', figsize=(25,10),rot=0)\nplt.xticks(rotation=90)","6760aa5a":"cm = metrics.confusion_matrix(y_test, y_pred_optree)\ncls_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\nticks = np.arange(len(cls_names))\nplt.xticks(ticks, cls_names)\nplt.yticks(ticks, cls_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix\\n', fontsize = 16)\nplt.ylabel('Actual label',fontsize = 16)\nplt.xlabel('Predicted label',fontsize = 16)","f5b0acb0":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_optree))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_optree, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_optree, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_optree, average=\"binary\"))","fb594f80":"from sklearn.tree import export_graphviz\nfrom io import StringIO\nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(clf_op, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = X_train.columns,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('Tree_mktdata_op.png')\nImage(graph.create_png())","5edc7cd8":"from sklearn.ensemble import GradientBoostingClassifier\n\nclassifier_GBC = GradientBoostingClassifier(random_state = 3)\nclassifier_GBC.fit(X_train, y_train)\n\ny_pred_GBC = classifier_GBC.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Gradient Booster Classifier model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_GBC)))\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_GBC))\nprint(\"Precision:\",metrics\n      .precision_score(y_test, y_pred_GBC, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_GBC, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_GBC, average=\"binary\"))","b1eb90fe":"hgb = GradientBoostingClassifier(learning_rate=0.2, loss='exponential', max_depth=4,\n                           min_samples_split=4, n_estimators=300,\n                           random_state=3)\nhgb.fit(X_train, y_train)\ny_pred_hgb = hgb.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Gradient Booster Classifier model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_hgb)))\nprint(\"Median value of target variable: \", y.median())","eff8859d":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_hgb))","afc88b37":"cm = metrics.confusion_matrix(y_test, y_pred_hgb)\ncm","aefb5788":"cls_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\nticks = np.arange(len(cls_names))\nplt.xticks(ticks, cls_names)\nplt.yticks(ticks, cls_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix\\n', fontsize = 16)\nplt.ylabel('Actual label',fontsize = 16)\nplt.xlabel('Predicted label',fontsize = 16)","31bf0286":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_hgb))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_hgb, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_hgb, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_hgb, average=\"binary\"))","922124e2":"importance = pd.DataFrame({'Feature': X_train.columns, 'Importance Score' : hgb.feature_importances_})\nimportance = importance.sort_values(by=['Importance Score'],ascending=False)\n\n# plot feature importance\nax = importance.plot.bar(x='Feature', y='Importance Score', figsize=(25,10),rot=0)\nplt.xticks(rotation=90)","ee63c5fe":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras import optimizers\n# set Hyper parameters\nlearning_rate=0.01\nno_epochs=100\n","1ececd38":"# Model creation\n\nmodel = Sequential([\n    Flatten(input_shape=(52,)),\n    Dense(16, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])","09e681a2":"#Compile model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","253d1a4f":"# Fit model\nmodel.fit(X_train, y_train, epochs=no_epochs, batch_size=len(mktdata_trf),  verbose=2)\n","223b78ab":"y_pred_NN = model.predict(X_test)\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)\n","716cb1cb":"from sklearn.feature_selection import RFE","b5f527b5":"rfe = RFE(lored,n_features_to_select=40)\nrfe = rfe.fit(X_train, y_train.values)\nprint(rfe.support_)\nprint(rfe.ranking_)","c298c0cd":"X_train_sig = X_train.loc[:, [False, True, True, True, False, True, False, False, True, True, True, True\n                              , True, True, False, False, True, True, True, True, True, True, True, True\n                              , True, True, True, True, True, True, True, True, True, True, True, False\n                              , True, True, True, False, False, True, True, True, True, True, True, False\n                              , True, True, False, False]\n                          ]\nX_test_sig = X_test.loc[:, [False, True, True, True, False, True, False, False, True, True, True, True\n                              , True, True, False, False, True, True, True, True, True, True, True, True\n                              , True, True, True, True, True, True, True, True, True, True, True, False\n                              , True, True, True, False, False, True, True, True, True, True, True, False\n                              , True, True, False, False]\n                        ]","0896a3bb":"X_train_sig","b047eaa5":"# Logistic regression model\nlored = LogisticRegression(C=1e9, solver='liblinear')\nlored.fit(X_train_sig, y_train)\n\n# predictions\ny_pred = lored.predict(X_test_sig)\n\n# evaluate model using RMSE\nprint(\"Logistic regression model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(\"Median value of target variable: \", y.median())","5363334c":"from sklearn.metrics import precision_score, recall_score, f1_score\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average=\"binary\"))","5473bfa6":"sns.clustermap(X_train.corr())","b0681468":"import statsmodels.api as sm\nlog_reg = sm.GLM(y_train, X_train).fit()\nprint(log_reg.summary())\n","e08d8233":"rfe = RFE(lored,n_features_to_select=25)\nrfe = rfe.fit(X_train, y_train.values)\nprint(rfe.support_)\nprint(rfe.ranking_)\n","aeb32a9d":"X_train_sig = X_train.loc[:, [False, False, True, False, False, True , False, False, False, True, True, True\n                              , False, True, False, False, True, True, True, True, True, True, True, True\n                              , True, False, True, False, False, True, False, True, True, False, True , False\n                              , True, False, True, False, False, False, False, False, True, False, False, True \n                              , False, True, False, False]]\n\nX_test_sig = X_test.loc[:, [False, False, True, False, False, True , False, False, False, True, True, True\n                              , False, True, False, False, True, True, True, True, True, True, True, True\n                              , True, False, True, False, False, True, False, True, True, False, True , False\n                              , True, False, True, False, False, False, False, False, True, False, False, True \n                              , False, True, False, False]]","bf4a71a6":"log_reg = sm.Logit(y_train, X_train_sig).fit()\nprint(log_reg.summary())","eddb6291":"y_pred_24logit = log_reg.predict(X_test_sig)\n","b2c349ef":"# change the y_pred from probability to decrete value 0 and 1\nfor idx in y_pred_24logit.index :\n  if y_pred_24logit[idx] < 0.5:\n    y_pred_24logit[idx] = 0\n  else:\n    y_pred_24logit[idx] = 1","780fd235":"print(\"Logistic regression with 24 features model RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred_24logit)))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_24logit))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_24logit, average=\"binary\"))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_24logit, average=\"binary\"))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred_24logit, average=\"binary\"))","c85b36f3":"# Create dataframe mktdata_trf_sig to predict target with 24 features\nmktdata_trf_sig =mktdata_trf.drop(['Response'], axis=1).loc[:, [False, False, True, False, False, True , False, False, False, True, True, True\n                              , False, True, False, False, True, True, True, True, True, True, True, True\n                              , True, False, True, False, False, True, False, True, True, False, True , False\n                              , True, False, True, False, False, False, False, False, True, False, False, True \n                              , False, True, False, False]]\n","5693d475":"y_pred_sig = log_reg.predict(mktdata_trf_sig)\n\n# change the y_pred from probability to decrete value 0 and 1\nfor idx in y_pred_sig.index :\n  if y_pred_sig[idx] < 0.5:\n    y_pred_sig[idx] = 0\n  else:\n    y_pred_sig[idx] = 1\n\n\nmktdata_E['Response_pred'] = y_pred_sig\n","e3c27ff3":"CrosstabResult = pd.crosstab(index=mktdata_E['Response_pred'],columns=mktdata_E['cluster_cat'])\nCrosstabResult.loc['%Repsonse'] = (CrosstabResult.loc[1]\/ (CrosstabResult.loc[1]+CrosstabResult.loc[0]))*100\nprint(CrosstabResult)\n\n\nCrosstabResult.plot.bar(figsize=(7,4), rot=0)","250a1be6":"# Create data frame for H&H group who likely to response \"Yes\" to our next campaign\nmktdata_H = mktdata_E.loc[mktdata_E['cluster_id'] == 0]\nmktdata_H = mktdata_H.loc[mktdata_H['Response_pred'] == 1]\n\n# Create data frame for M&M group who likely to response \"No\" to our next campaign\nmktdata_L = mktdata_E.loc[mktdata['cluster_id'] == 2]\nmktdata_L = mktdata_L.loc[mktdata_L['Response_pred'] == 0]\n\n# Create data frame for M&M group who likely to response \"No\" to our next campaign\nmktdata_M = mktdata_E.loc[mktdata['cluster_id'] == 1]\nmktdata_M = mktdata_M.loc[mktdata_M['Response_pred'] == 0]\n","4111e499":"plt_a = mktdata.drop(['ID', 'Dt_Customer'], axis=1)\nplt_b = mktdata_H.drop(['ID', 'Dt_Customer'], axis=1)\n\n\nfrom matplotlib.ticker import PercentFormatter\nfig, axs = plt.subplots(8,4, figsize=(28, 40))\nfig.subplots_adjust(hspace = .3, wspace=.3)\n\ncount = 0\n\nfor count, item in enumerate(axs.reshape(-1)):\n  item.hist(plt_a.iloc[:,count], color = 'red',weights=np.ones(len(plt_a.iloc[:,count])) \/ len(plt_a.iloc[:,count]), alpha=0.6, bins=25, label=plt_a.columns[count])\n  item.hist(plt_b.iloc[:,count], color = 'blue',weights=np.ones(len(plt_b.iloc[:,count])) \/ len(plt_b.iloc[:,count]), alpha=0.6, bins=25, label=plt_b.columns[count])\n  plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n  #Title \n  item.text(0.5,0.9,plt_a.columns[count],transform=item.transAxes)\n\nfig.text(0.1,0.9, 'Everage: Red, High Income group: Blue',fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\nplt.show()","87b7934f":"plt_a = mktdata.drop(['ID', 'Dt_Customer'], axis=1)\nplt_b = mktdata_L.drop(['ID', 'Dt_Customer'], axis=1)\n\n\nfrom matplotlib.ticker import PercentFormatter\nfig, axs = plt.subplots(8,4, figsize=(28, 40))\nfig.subplots_adjust(hspace = .3, wspace=.3)\n\ncount = 0\n\nfor count, item in enumerate(axs.reshape(-1)):\n  item.hist(plt_a.iloc[:,count], color = 'red',weights=np.ones(len(plt_a.iloc[:,count])) \/ len(plt_a.iloc[:,count]), bins=25, alpha=0.6, label=plt_a.columns[count])\n  item.hist(plt_b.iloc[:,count], color = 'green',weights=np.ones(len(plt_b.iloc[:,count])) \/ len(plt_b.iloc[:,count]), bins=25, alpha=0.6, label=plt_b.columns[count])\n  plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n  #Title \n  item.text(0.5,0.9,plt_a.columns[count],transform=item.transAxes)\n\nfig.text(0.1,0.9, 'Everage: Red, Low Income group: Green',fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\nplt.show()","d6217386":"plt_a = mktdata.drop(['ID', 'Dt_Customer'], axis=1)\nplt_b = mktdata_M.drop(['ID', 'Dt_Customer'], axis=1)\n\n\nfrom matplotlib.ticker import PercentFormatter\nfig, axs = plt.subplots(8,4, figsize=(28, 40))\nfig.subplots_adjust(hspace = .3, wspace=.3)\n\ncount = 0\n\nfor count, item in enumerate(axs.reshape(-1)):\n  item.hist(plt_a.iloc[:,count], color = 'red',weights=np.ones(len(plt_a.iloc[:,count])) \/ len(plt_a.iloc[:,count]), bins=25, alpha=0.6, label=plt_a.columns[count])\n  item.hist(plt_b.iloc[:,count], color = 'green',weights=np.ones(len(plt_b.iloc[:,count])) \/ len(plt_b.iloc[:,count]), bins=25, alpha=0.6, label=plt_b.columns[count])\n  plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n  #Title \n  item.text(0.5,0.9,plt_a.columns[count],transform=item.transAxes)\n\nfig.text(0.1,0.9, 'Everage: Red, Medium income group: green',fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\nplt.show()","706536f9":"from sklearn import datasets\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","bc69f736":"false_positive_rate_lored, true_positive_rate_lored, threshold_lored = roc_curve(y_test, y_pred_lored)\nfalse_positive_rate_maxtree, true_positive_rate_maxtree, threshold_maxtree = roc_curve(y_test, y_pred_maxtree)\nfalse_positive_rate_optree, true_positive_rate_optree, threshold_optree = roc_curve(y_test, y_pred_optree)\nfalse_positive_rate_GBC, true_positive_rate_GBC, threshold_GBC = roc_curve(y_test, y_pred_GBC)\nfalse_positive_rate_hgb, true_positive_rate_hgb, threshold_hgb = roc_curve(y_test, y_pred_hgb)\nfalse_positive_rate_24logit, true_positive_rate_24logit, threshold_24logit = roc_curve(y_test, y_pred_24logit)\n","73f2d432":"print('roc_auc_score for Logistic Regression: ', roc_auc_score(y_test, y_pred_lored))\nprint('roc_auc_score for Maximal Tree: ', roc_auc_score(y_test, y_pred_maxtree))\nprint('roc_auc_score for Optimal Tree: ', roc_auc_score(y_test, y_pred_optree))\nprint('roc_auc_score for Gradient Boosting Classifer: ', roc_auc_score(y_test, y_pred_GBC))\nprint('roc_auc_score for Hyperparameter GBC: ', roc_auc_score(y_test, y_pred_hgb))\nprint('roc_auc_score for Logistic Regression with 24 features: ', roc_auc_score(y_test, y_pred_24logit))\n\n\n","f856fba6":"#Plot the curve\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate_lored, true_positive_rate_lored)\nplt.plot(false_positive_rate_maxtree, true_positive_rate_maxtree)\nplt.plot(false_positive_rate_optree, true_positive_rate_optree)\nplt.plot(false_positive_rate_GBC, true_positive_rate_GBC)\nplt.plot(false_positive_rate_hgb, true_positive_rate_hgb)\nplt.plot(false_positive_rate_24logit, true_positive_rate_24logit)\n\nplt.legend(['Logistic Regression', 'Maximal Tree', 'Optimal Tree', 'Gradient Boosting Classifer', 'Hyperparameter GBC', 'Logistic Regression with 24 features' ])\n\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","7a65c82b":"# **Based on the demographic of each group, I try to label them as their characteristics**\n\n1.   Cluster_id = 0: Low income & less spending = H&H\n2.   Cluster_id = 1: High income & Loyal = M&M\n1.   Cluster_id = 2: Medium income & medium speding = L&L ","020326b4":"# **Logistic regression**","5b979041":"**Tunning model**","19dcc23d":"So the H&H group is likely to response to our campaign and the L&L is the least likely to repsonse","5aa5547d":"# Analyze with labels of groups","9fa48b3c":"Code for log transformation\n\ncol_to_transform = mktdata.drop(columns=['ID', 'Education', 'Marital_Status', 'Dt_Customer', 'Kidhome', 'Teenhome',\n                                      'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', \n                                       'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain',\n                                     'Country'])\n\nfor i,col in enumerate(col_to_transform.columns):\n  mktdata[col] = np.log(1 + mktdata[col])","8ca3be08":"# **3. Applying Modeling to predict Campaign Response**","59b4f3bc":"# **2. Clustering Analysis**","c5d0f994":"#Code for Tunning Model\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngrid = {\n    'learning_rate' : [0.2, 0.3, 0.4, 0.5],\n    'n_estimators' : [300, 500, 700, 900],\n    'min_samples_split' : [3, 4, 5, 6],\n    'max_depth' : [2, 3, 4, 5],\n    'loss' : ['deviance', 'exponential']\n}\nrandom_cv = RandomizedSearchCV(estimator=classifier_GBC,\n                              param_distributions=grid,\n                              n_iter=20,\n                              n_jobs=-1,\n                              cv=5,\n                              verbose=7,\n                              random_state=10,\n                              scoring='accuracy')\nrandom_cv.fit(X_train, y_train)\n\nrandom_cv.best_estimator_\n\nGradientBoostingClassifier(learning_rate=0.2, loss='exponential', max_depth=4,\n                           min_samples_split=4, n_estimators=300,\n                           random_state=3)","dabd0cd0":"We can see outliners which could make skewness if we impute missing values by mean, instead, we can impute by median","1326b39e":"# ROC curves plotting.  ","122bb198":"# **4. Conclusion and Recommendation**\n\n1. H&H group are most likely to respond to the new campaign based on the prediction, while group L&L is less likely to respond \nWe need to shift our focus to H&H group instead of L&L group\n\n2. L&L group are most likely to say no and they significantly and negatively affected the total acceptance rate in the previous campaigns.\nWe should not target the L&L group in the next campaign \n\n3. The customers who are married or stay together with spouse or living in a household with kids are significantly and negatively affect the total acceptance rate\n\uf0e8 We should not target on the customers who are married or stay together with spouse or living in a household with kids\n\n4. It seems like the higher education the higher rate of responses \n\uf0e8we should focus more on the higher education group\n\n5. We should also focus on the customers that previously say \u201cyes\u201d to campaigns 3 and 5 and of course, the more loyal they are, the more possibility they would say \u201cyes\u201d to our next campaign.\n","9a2e51dc":"# **1. DATA CLEANING & ENGINEERING**\n","52bd4600":"# **Decision Tree**","78b1e7f0":"#Code for Tunning the Tree (I put it as a text bx since it would take time to run the GridSearch)\nfrom sklearn.model_selection import GridSearchCV\n\nparam_dict = {\"criterion\": ['gini', 'entropy' ],\n              \"max_depth\": range (1, 20),\n              \"min_samples_split\": range (1, 10),\n              \"min_samples_leaf\": range (1, 5)}\n\ngrid = GridSearchCV(clf,\n                    param_grid = param_dict,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(str(grid.best_estimator_))\nprint(grid.best_score_)\n\n#DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=2, min_samples_split=6)","5c6b85fc":"# **Identify the outliners & skewness**","9fd6e3e0":"# ** Tuning to get optimal Tree**","66f66076":"Analytic questions to be answered:\n\n1.\tAre there any separated segments in customers' geography, number of purchased products and channels of purchases? How would each segment contribute to the success of the next campaign?\n**==> Yes, we can segment our customers by income, amount of product purchased and the household with children or not. There are 3 groups: Hight income + loyal + have no children , middle income with medium amount of purchases + with children, and low income with low amount of purhases + with children.**\n\n2.\tIs there a significant relationship between geographical profile or purchase behaviour and the success of the next campaign? Can we predict the success rate?\n**==> Yes we can by apply the Gradient Boosting Classifier model and predict the acceptance rate of the next campaign for each customer and also each group. The H&H group have the highest rate of response toward the next campaign while the M&M and L&L are not likely to repsonse. H&H group tend to buy more grocery stuff from us such as meat, fruits, fish but they do not purchase much gold from us and they have the lowest rate of using deal in their purchases **\n\n\n\n","01af9331":"# Try logit model","f6b0d850":"# **Data Transformation**\nBefore applying any model we need to transform categorical variables into numerical variables using pandas","d39f120a":"# **Neural Network**","ed04c675":"# **Data Engineering**\nSince we don't apply time-series analysis so I will transform timedate variables to 'number of years' form\n\n*   Dt_Customer column stores the date and time a people registered as customer --> Convert to number of years as a customer\n*   Year_Birth column stores the birth date of each customer --> convert to ages\n*   Engineering the total amount of purchases variable\n*   Engineering the total amount of products variable\n\n*   Engineering the \"ChildHome\": '0' means no children, '1' means having children\n*   Engineering the \"UsedDeal\": '0' means Haven't purchased any deal, '1' means have purchased at least 1 deal\n\n\n\n\n\n\n","84f328ad":"## FInding the significant variables to include in the best model so far the logistic Regression model","63adae8c":"# **Apply Standard Scaler to reduce skewness**","39c69112":"<a href=\"https:\/\/www.kaggle.com\/mrain2\/marketing-campaign-prediction?scriptVersionId=82607048\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\"><\/a>","8044f01f":"# **Gradient Boosting Classifier**","80186e90":"## Maximal Tree\n"}}