{"cell_type":{"8779ebd3":"code","2efbaeff":"code","618b6c88":"code","71b18df6":"code","22c44121":"code","87336259":"code","0441622a":"code","0a5c34a8":"code","acd08221":"code","c2dc29d4":"code","8559f0ac":"code","6972624f":"code","358de325":"code","50c2126e":"code","6582148a":"code","044de3ed":"code","2f2c8355":"code","aa27c7a6":"code","ca1e5f8e":"code","af18036e":"code","34e87e59":"code","fdf55aa4":"code","6f99ee41":"code","48fe4075":"code","46d2f977":"code","0d6ed647":"code","eaa8b36b":"code","7bbf8917":"code","0338a551":"code","9fe6b0f6":"code","85d037fd":"code","dcccd810":"code","253f56b5":"code","4ba0558b":"code","a9f520ba":"code","f9d2f6aa":"code","00c2b846":"code","151509f4":"code","55ccbc5c":"code","b5a06e78":"code","f8e1632e":"code","2f63001d":"code","c32e1b04":"code","edd32b2d":"code","29bd5f8a":"code","af14986c":"code","c2e84215":"code","cff12b74":"code","5eb3778c":"code","30ebbdd5":"code","28ed209a":"code","f1b4073d":"code","ba3ab473":"code","f9577638":"code","0e9224d5":"code","89f5340c":"code","f6620388":"code","a12a4ae4":"code","dee39f4c":"code","48627dc1":"code","5f0b9361":"code","49381f15":"code","a2d90a40":"code","34b662d6":"code","186a6d45":"code","7320a1ce":"code","97d9809b":"code","79330890":"code","d7b019d3":"code","984dbfb6":"code","433ef9f1":"code","fb6a866a":"code","385475b5":"code","7ee97f28":"code","9fd4a407":"code","4bbdf7a7":"code","75f880a3":"code","43795763":"code","6da6f8ae":"code","c048116e":"code","5916f3a7":"code","0f9915c3":"code","cb21f7ff":"code","ba59c4ae":"code","3d6ff00e":"code","83df001a":"code","3304d03f":"code","29e41114":"code","5c11740c":"markdown","ad5f4c6a":"markdown","0b98c62b":"markdown","8aa67e1f":"markdown","9efbc591":"markdown","096253cb":"markdown","d97d0446":"markdown","23c88579":"markdown","61861c38":"markdown","e1d8199f":"markdown","55c25f1e":"markdown","863a0f60":"markdown","f0fca866":"markdown","bba2484b":"markdown","85f84058":"markdown","74efaa5d":"markdown","d60f659c":"markdown","98ff002a":"markdown","d74eeef9":"markdown","6f21311c":"markdown","7b055a94":"markdown","4a5d2a48":"markdown","27860ae0":"markdown","2026e75e":"markdown","cc7bcb57":"markdown","e5d6c236":"markdown","a53edacd":"markdown","1598ee86":"markdown","db7a7ad6":"markdown","eb069096":"markdown","d7e18631":"markdown","9ac7e648":"markdown","3336d3a6":"markdown","0c093d59":"markdown","3caa33a0":"markdown","213911c4":"markdown","15453ed0":"markdown","3a48c75a":"markdown","8dba677b":"markdown","12b4592d":"markdown","7ddf66f7":"markdown","6a5af70f":"markdown","ceca4a77":"markdown","f71c9b24":"markdown","23f514be":"markdown","caa52ab3":"markdown","02add02a":"markdown","ede8cfb1":"markdown","d44e7c57":"markdown","6517d98a":"markdown","c5e42f88":"markdown","b2abf7a7":"markdown","a448ae03":"markdown","3a4b045d":"markdown","25dca303":"markdown","358da8f7":"markdown","dd100580":"markdown","2f1e8558":"markdown","c13f3d75":"markdown","84c736b0":"markdown","7e334f90":"markdown","1b4d6f5e":"markdown","3c63632f":"markdown"},"source":{"8779ebd3":"#Loading packages and Data\n\nfrom IPython.display import Image\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom datetime import datetime\nfrom random import random\nfrom math import sqrt\nfrom numpy import concatenate\nfrom numpy import array\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal  import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n!pip install pyramid-arima\nfrom pyramid.arima import auto_arima\n\n\n#!pip install plotly==3.10.0\n\nfrom fbprophet import Prophet\n#from plotly.plotly import plot_mpl\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n","2efbaeff":"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npd.plotting.register_matplotlib_converters()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","618b6c88":"#Loading csv\ndata=pd.read_csv('..\/input\/smart-home-dataset-with-weather-information\/HomeC.csv')\n","71b18df6":"data.head()","22c44121":"data.info()","87336259":"data.tail()","0441622a":"# removing the truncated record\ndata=data[:-1]\ndata.shape","0a5c34a8":"#given that the time is in UNIX format, let's check \ntime = pd.to_datetime(data['time'],unit='s')\ntime.head()","acd08221":"#new daterange in increments of minutes\ntime_index = pd.date_range('2016-01-01 05:00', periods=len(data),  freq='min')  \ntime_index = pd.DatetimeIndex(time_index)\ndata['time']=time_index","c2dc29d4":"#changing column names before doing some calculation as they look weird with \"[kw]\"\ndata.columns=['time', 'use', 'gen', 'House overall', 'Dishwasher',\n       'Furnace 1', 'Furnace 2', 'Home office', 'Fridge',\n       'Wine cellar', 'Garage door', 'Kitchen 12',\n       'Kitchen 14', 'Kitchen 38', 'Barn', 'Well',\n       'Microwave', 'Living room', 'Solar', 'temperature',\n       'icon', 'humidity', 'visibility', 'summary', 'apparentTemperature',\n       'pressure', 'windSpeed', 'cloudCover', 'windBearing', 'precipIntensity',\n       'dewPoint', 'precipProbability']","8559f0ac":"data['gen'].head()","6972624f":"data['Solar'].head()","358de325":"(data['gen']-data['Solar']).value_counts()","50c2126e":"data=data.drop('gen',axis=1)","6582148a":"(data['House overall']-data['use']).value_counts()","044de3ed":"data=data.drop('House overall',axis=1)","2f2c8355":"#getting  hour, day,week, month from the date column\ndata['day']= data['time'].dt.day\ndata['month']= data['time'].dt.month\ndata['week']= data['time'].dt.week\ndata['hour']= data['time'].dt.hour","aa27c7a6":"import seaborn as sns\ndef visualize(label, cols):\n    fig,ax=plt.subplots(figsize=(14,8))\n    colour= ['red','green','blue','yellow']\n    for colour,col in zip(colour,cols):\n            data.groupby(label)[col].mean().plot(ax=ax,label=col,color=colour)\n    plt.legend()\n\n","ca1e5f8e":"visualize('hour',['Furnace 1','Furnace 2'])","af18036e":"visualize('day',['Furnace 1','Furnace 2'])","34e87e59":"visualize('month',['Furnace 1','Furnace 2'])","fdf55aa4":"data['Furnace']= data['Furnace 1']+data['Furnace 2']\ndata=data.drop(['Furnace 1','Furnace 2'], axis =1)","6f99ee41":"visualize('month',['Kitchen 12','Kitchen 14','Kitchen 38'])","48fe4075":"visualize('week',['Kitchen 12','Kitchen 14','Kitchen 38'])","46d2f977":"visualize('day',['Kitchen 12','Kitchen 14','Kitchen 38'])","0d6ed647":"visualize('hour',['Kitchen 12','Kitchen 14','Kitchen 38'])","eaa8b36b":"data['Kitchen 38'].describe()","7bbf8917":"fig,ax=plt.subplots(2,2,figsize=(15,10))\n\ndata.groupby('hour')['Kitchen 38'].mean().plot(ax=ax[0,0],color='green',label= 'kitchen 38')\ndata.groupby('day')['Kitchen 38'].mean().plot(ax=ax[0,1],color='green',label= 'kitchen 38')\ndata.groupby('week')['Kitchen 38'].mean().plot(ax=ax[1,0],color='green',label= 'kitchen 38')\ndata.groupby('month')['Kitchen 38'].mean().plot(ax=ax[1,1],color='green',label= 'kitchen 38')\n\n                                                     \n\nplt.legend()","0338a551":"data['icon'].value_counts()","9fe6b0f6":"data['summary'].value_counts()","85d037fd":"data.groupby('summary')['Solar'].sum()","dcccd810":"data=data.drop(['icon','summary'], axis =1)","253f56b5":"data['cloudCover'].dtypes","4ba0558b":"data['cloudCover'].head()","a9f520ba":"data['cloudCover'].value_counts()","f9d2f6aa":"data['cloudCover'].unique()","00c2b846":"data['cloudCover'].replace(['cloudCover'], method='bfill', inplace=True)\n","151509f4":"data['cloudCover'].unique()","55ccbc5c":"data['cloudCover']=data['cloudCover'].astype('float')","b5a06e78":"data.info()","f8e1632e":"data.index= data['time']\n#daily resampling\ndataD=data.resample('D').mean()","2f63001d":"dataD.info()","c32e1b04":"#hourly resampling\ndataH=data.resample('H').mean()","edd32b2d":"weathercols= ['temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nHousecols = ['Dishwasher','Furnace', 'Home office', 'Fridge','Wine cellar', 'Garage door', 'Kitchen 12','Kitchen 14', \n             'Kitchen 38', 'Barn', 'Well','Microwave', 'Living room']\nuseweather=['use','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nsolarweather=['Solar','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nusesolar=['use','Solar']","29bd5f8a":"\n# load dataset\ndef series_visualize(data, cols):\n    dataset = data[cols]\n    values = dataset.values\n    # specify columns to plot    \n    groups = [i for i in range(len(cols))]\n    j = 1\n# plot each column\n    plt.figure(figsize=(18,13))\n    for group in groups:\n        plt.subplot(len(groups), 1, j)\n        plt.plot(values[:, group])\n        plt.title(dataset.columns[group], y=0.5, loc='right')\n        j += 1\n    plt.show()","af14986c":"#series_visualize(dataH,Housecols)\nseries_visualize(dataD,Housecols)","c2e84215":"series_visualize(dataH,usesolar)","cff12b74":"series_visualize(dataD,usesolar)","5eb3778c":"datause=dataD.iloc[:,0].values\n#fig,ax=plt.subplots(figsize=(15,10))\nplt.rcParams['figure.figsize'] = (14, 9)\nseasonal_decompose(dataD[['use']]).plot()\nresult = adfuller(datause)\nplt.show()","30ebbdd5":"X= dataD.iloc[:,0].values\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","28ed209a":"# split data into train and tests\ntrain=dataD[dataD['month']<12].iloc[:,0]\ntest=dataD[dataD['month']>=12].iloc[:,0]\nprint(\"train has {} records, test has {} records\".format(len(train),len(test)))","f1b4073d":"fig,ax=plt.subplots(figsize=(18,6))\ntrain.plot(ax=ax)\ntest.plot(ax=ax)\nplt.show()","ba3ab473":"\n# fit model withweekly seasonality \nmodel = ExponentialSmoothing(train.values,seasonal='add',seasonal_periods=7)\nmodel_fit = model.fit()\n\n# make prediction\n\ny = model_fit.forecast(len(test))\ny_predicted=pd.DataFrame(y,index=test.index,columns=['Holtwinter'])\n\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Test')\nplt.plot(y_predicted, label='Holtwinter')\nplt.legend(loc='best')\nplt.show()","f9577638":"rms = sqrt(mean_squared_error(test,y_predicted))\nprint(rms)","0e9224d5":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n# Draw Plot\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.tolist(), lags=50,ax=axes[0])\ny=plot_pacf(train.tolist(), lags=50, ax=axes[1])\nplt.show()","89f5340c":"# first differencing\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.diff().dropna(), lags=50,ax=axes[0])\ny=plot_pacf(train.diff().dropna(), lags=50, ax=axes[1])\nplt.show()","f6620388":"seasonal=seasonal_decompose(dataD[['use']]).seasonal\n#fig.ax=plt.subplots(figsize=(16,5))\nseasonal.plot()\nseasonal.diff(1).dropna().plot(color='orange')\nseasonal.diff(7).dropna().plot(color='green')","a12a4ae4":"from statsmodels.tsa.statespace.sarimax import SARIMAX\ny_hat = test.copy()\nfit1 = SARIMAX(train.values, order=(1, 0, 2),seasonal_order=(0,1,1,7)).fit()\ny_pred  =  fit1.predict(dynamic=True)\ny = fit1.forecast(len(test),dynamic=True)\ny_predicted=pd.DataFrame(y,index=y_hat.index,columns=['sarima'])\n\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Test')\nplt.plot(y_predicted, label='SARIMA')\nplt.legend(loc='best')\nplt.show()","dee39f4c":"rms = sqrt(mean_squared_error(test,y_predicted))\nprint(rms)","48627dc1":"#building the model\nmodel = auto_arima(train,start_p=1,d=1,start_q=1,max_p=3,max_d=2,max_q=3,start_P=1,D=1,start_Q=1,\n                   max_P=2,max_D=1,max_Q=2,seasonal =True, m=7, max_order=5,stationary=False, trace=True, error_action='ignore', suppress_warnings=True)\nmodel.fit(train)\n\nforecast = model.predict(n_periods=len(test))\nforecast = pd.DataFrame(forecast,index = test.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","5f0b9361":"rms = sqrt(mean_squared_error(test,forecast))\nprint(rms)","49381f15":"def split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n\n","a2d90a40":"# define input sequence\nraw_seq = train[:307].values.tolist()\n# choose a number of time steps\nn_steps_in, n_steps_out = 28, 16\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model","34b662d6":"#LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)\n","186a6d45":"# demonstrate prediction\nx_input = train[307:].values\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","7320a1ce":"yhat=yhat.reshape(16,1)","97d9809b":"\nforecast = pd.DataFrame(yhat,index = test.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","79330890":"rms = sqrt(mean_squared_error(test,forecast))\nprint(rms)","d7b019d3":"new_train= pd.DataFrame(train)\nnew_train['ds']=new_train.index\nnew_train['y']=new_train['use']\nnew_train.drop(['use'],axis = 1, inplace = True)\nnew_train=new_train.reset_index()\nnew_train.drop(['time'],axis = 1, inplace = True)","984dbfb6":"#model\nm = Prophet(mcmc_samples=300, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='additive', \\\n           seasonality_prior_scale=0.4, weekly_seasonality=True, \\\n            daily_seasonality=False)\n\nm.fit(new_train)\nfuture = m.make_future_dataframe(periods=16)\n#prediction\nforecast = m.predict(future)","433ef9f1":"c=m.plot_components(forecast)\nplt.show()","fb6a866a":"d=m.plot(forecast)\nplt.show()","385475b5":"predictions=pd.DataFrame(forecast[335:]['yhat'])\npredictions.index=test.index\n\nfig,ax=plt.subplots(figsize=(15,8))\ntest.plot(ax=ax)\npredictions.plot(ax=ax)","7ee97f28":"len(train)","9fd4a407":"rms = sqrt(mean_squared_error(test,forecast[['yhat']][335:]))\nprint(rms)","4bbdf7a7":"temperature=dataD[dataD['month']<12].loc[:,'temperature']\nrain=dataD[dataD['month']<12].loc[:,'precipIntensity']\nwind=dataD[dataD['month']<12].loc[:,'windSpeed']\n\ntemperature=temperature.reset_index().drop('time',axis=1)\nrain=rain.reset_index().drop('time',axis=1)\nwind=wind.reset_index().drop('time',axis=1)\n\ntrain_regressor=pd.concat([new_train,temperature,rain,wind],axis=1)","75f880a3":"m = Prophet(mcmc_samples=300, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='additive', \\\n           seasonality_prior_scale=0.4, weekly_seasonality=True, \\\n            daily_seasonality=False)","43795763":"m.add_regressor('temperature', prior_scale=0.5, mode='additive')\nm.add_regressor('precipIntensity', prior_scale=0.5, mode='additive')\nm.add_regressor('windSpeed', prior_scale=0.5, mode='additive')","6da6f8ae":"m.fit(train_regressor)\nfuture = m.make_future_dataframe(periods=16)\n","c048116e":"testtemp=dataD.loc[:,'temperature']\ntestrain=dataD.loc[:,'precipIntensity']\ntestwind=dataD.loc[:,'windSpeed']\ntesttemp=testtemp.reset_index().drop('time',axis=1)\ntestrain=testrain.reset_index().drop('time',axis=1)\ntestwind=testwind.reset_index().drop('time',axis=1)\nfuture['temperature']=testtemp\nfuture['precipIntensity']=testrain\nfuture['windSpeed']=testwind","5916f3a7":"future.tail()","0f9915c3":"forecast = m.predict(future)","cb21f7ff":"f = m.plot_components(forecast)\n","ba59c4ae":"d=m.plot(forecast)\nplt.show()","3d6ff00e":"predictions=pd.DataFrame(forecast[335:]['yhat'])\npredictions.index=test.index\n\nfig,ax=plt.subplots(figsize=(15,8))\ntest.plot(ax=ax)\npredictions.plot(ax=ax)","83df001a":"rms = sqrt(mean_squared_error(test,forecast[['yhat']][335:]))\nprint(rms)","3304d03f":"maxlag=12\ntest = 'ssr_chi2test'\ndef grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df\n\ngrangers_causation_matrix(dataD[useweather], variables = useweather)  ","29e41114":"Image(\"\/kaggle\/input\/conclusion\/kagglextreme.PNG\")","5c11740c":"                                    Daily power usage and generation","ad5f4c6a":"Let us check how solar energy got produced in different days","0b98c62b":"Before building models, Let us check for datatypes that are not int or float","8aa67e1f":"Future Work: Understanding [fireTs](https:\/\/pypi.org\/project\/fireTS\/) and non-linear modelling of time-series.","9efbc591":"Also Let's check 'House overall' and 'use' ","096253cb":"### Load Forecasting \n***\n\nIn this Kernel, I delve into different aspects of Time-series forecasting and the problems enccountered while modelling Load forecasting using different Time-series techniques. \n\n\n**Contents:**\n\n[1. Data Understanding]('1')\n\n[2. Preprocessing Data]('2')\n\n[3. Univariate Time-series modelling]('3')  \n    [3.1 Holt-winters exponential smoothing]('3.1')  \n    [3.2 SARIMAX]('3.2')  \n    [3.3 Auto-ARIMA]('3.3')  \n    [3.4 LSTM]('3.4')  \n    [3.5 Facebook-Prophet]('3.5')  \n   \n[4. Multivariate Time-series modelling]('4')\n\n[5. Conclusion]('5')","d97d0446":" P-value < 0.05, the series is stationary ","23c88579":"Now we will check for 'cloudCover' column","61861c38":"### <div id= '3.3'>3.3 Auto ARIMA<\/div>","e1d8199f":"Now, we can start building Time-series models. As the trend is not linear. we start with Holtz-winters exponential smooting ","55c25f1e":"### <div id= '4'>4.Multi-Variate models<\/div>","863a0f60":"Load forecasting would be helpful to optimize energy consumption and plan household energy needs accordingly, saving solar energy and utilizing it optimally. Let us build univariate time-series models first!","f0fca866":"The advantages of [Prophet](https:\/\/facebook.github.io\/prophet\/) are, \nwe can model holiday effects, weekly, yearly seasonalities, Saturation checks etc. Prophet requires variables as 'ds' and 'y'. Also, we can add regressors with out much effort!","bba2484b":"Adding other variables to the series to get causal relationship effects","85f84058":"Let's check power generated from sources other than Solar","74efaa5d":"                                                            * * *","d60f659c":"Let us see what's happening with \"Kitchen 38\"","98ff002a":"###  <div id= '3.4'>3.4 LSTM<\/div>","d74eeef9":"We use  P is 1, d is 0 and q is 2 , because i'll be coservative at first,\n\n      1. ACF plots shows gradually decreasing to 0 with few lags above\n      2. PACF plot cuts off quicky at 1\n      3. After differencing once we see the series has negative spikes which means over differencing. so we choose d as 0\n        \nFor Seasonal terms,(P,D,Q)m - ","6f21311c":"#### ACF and PACF plots","7b055a94":"As these reports  are genererated by data acquisition system, we will  remove these variables, because the real temperature data will be enough for us instead of these variables.\n","4a5d2a48":"### **Resampling and Visualization**","27860ae0":"we see after 7 differences, seasonality got removed completely. And as a general rule D=1 and only we keep SMA and test with RMS","2026e75e":" ### <div id= '1'>1. Data Understanding<\/div>","cc7bcb57":" * It seems \"solar\" and \"gen\" are simillar columns. So we drop 'gen' column as it is the only power generated by Solar ","e5d6c236":"To conclude, Auto-Arima performed better than LSTM and Prophet. Due to limited amount of data, statsmodels outshined neural networks","a53edacd":"First, Lets us split sequence to prepare data in the required form for LSTM\n\n       given sequence [10, 20, 30, 40, 50, 60, 70, 80, 90] into\n            X,\t\t\ty\n        10, 20, 30\t\t40\n        20, 30, 40\t\t50\n        30, 40, 50\t\t60\n        ...","1598ee86":"Understanding the time-series would let us know whether the series is having linear or exponential trend, additive or multiplicative seasonality which aides us in using appropriate techniques for considering these effects.","db7a7ad6":"### Before building Multi-variate models, we will check for *Granger's Causaulity*","eb069096":"Ad-Fuller Test for stationarity","d7e18631":"Additive seasonality with no trend","9ac7e648":"### <div id= '2'>2. Preprocessing Data<\/div>\n\n**Feature Engineering**","3336d3a6":"### <div id= '3.1'>3.1 HOLTZ-WINTERS Exponential Smoothing <\/div>","0c093d59":"In the months of June, July,and August, \"office\", \"winecellar\", \"Fridge\" power consumption rose. And in December, January, February months Furnace's power consumption rose.","3caa33a0":"As there are lot of unique values, let us check what are they","213911c4":"### <div id= '3'>3.Univariate Models<\/div>","15453ed0":" Now, our dataset doesn't have any null values and no categorical variables. Now, we split our dataset for training and testing and start building time-series models and forecast load. We first resample on Day and forecast daily load. Later, we try to build multi-variate time-series models using other variables","3a48c75a":"We need to resample the data and convert the data into a time-series first as data is in minute steps. Resampling over day allows us to forecast the day wise load","8dba677b":"### <div id= '5'>5.Conclusion<\/div>","12b4592d":"To find (p,d,q) for ARIMA, we first need to plot Auto-correlation(ACF) and Partial Auto-correlation(PACF) plots. Because of seasonality, we use seasonal ARIMA model SARIMAX from statsmodels ","7ddf66f7":"The basis behind Vector AutoRegression is that each of the time series in the system influences each other. That is, you can predict the series with past values of itself along with other series in the system. So, we won't be able to model VAR for this problem because the variables failed Granger's Causaulity","6a5af70f":"we need to prepare dataset as a 3D matrix for LSTM from [samples, timesteps] to  [samples, timesteps, features] . Here, we have only 1 feature i.e., use,  and timesteps are the sequence of steps, here, we choose 28 timesteps and the output timesteps to predict as 16, because we need to validate on the Test set","ceca4a77":"We start with understanding the types of variables, length, different variable names, and  their spread. ","f71c9b24":"we need to impute 'cloudCover' with the nearest values as the records are taken in minute steps. We would use backward fill to replace","23f514be":"**VAR model**","caa52ab3":"Decomposing Time-series ","02add02a":"For now, we concentrate on Daily Load forecasting","ede8cfb1":"As expected clear, partly cloudy, drizzle, light rain days produced a lot more power than other days. Also the number of clear days outnumbered other days. So, this number would be large compared to other day's","d44e7c57":"As we could see, there are simillar names for variables. we first check their energy consumption patterns over the day, week, month and then if they look simillar, we will merge them to a single variable!","6517d98a":"There is consumption but very little comparing to other kitchens, we will keep them like that","c5e42f88":"#### Testing Stationarity and plotting Trend ","b2abf7a7":"Time has 1 record more than others. Let's check why?","a448ae03":"Here, Time step is in increments of seconds but specified as Minute time steps. So, we create a new daterange in increments of minute","3a4b045d":"                             Hourly power usage and hourly solar power generation ","25dca303":"As we see, the last record is truncated!","358da8f7":"### <div id= '3.2'>3.2 ARIMA <\/div>","dd100580":"'House overall' and 'use' are simillar columns. we drop 'House overall'","2f1e8558":"Now, we check for kitechs too","c13f3d75":"LSTM couldn't produce better predictions than Statistical models. Because, LSTMs need more data to tune their parameters. And, in our case, we have only 1 year data. Also, LSTM's are better at forecasting longterm not at shortterm","84c736b0":"### <div id= '3.5'>3.5 Facebook-Prophet<\/div>","7e334f90":"Furnace 2 power consumption is simillar to  Furnace 1, so we will combine both of them and make it as single variable representing Furnace power","1b4d6f5e":"***","3c63632f":" The usage is not caused by any of the variables. As p>=0.05, We couldn't reject the null hypothesis"}}