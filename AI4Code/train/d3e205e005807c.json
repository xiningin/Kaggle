{"cell_type":{"f94a644b":"code","c43a8a94":"code","1668c222":"code","72654450":"code","882bb8bd":"code","d0e60e9b":"code","9f2dba4a":"code","00d7f885":"code","13ee4d1f":"code","1e75d63b":"code","be8b7f7b":"code","f388e753":"code","4f8d1682":"code","f54a5fc3":"code","ad284d1c":"code","98c80e4d":"code","5ea1e3d8":"markdown","1a9b9ab5":"markdown","71d8c5fe":"markdown","95453368":"markdown","72d54b81":"markdown","726acde9":"markdown","ece92f0a":"markdown","628aa8dd":"markdown","ffeea8ad":"markdown","7f9196ff":"markdown","97e333b5":"markdown"},"source":{"f94a644b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c43a8a94":"def prepare_data(df):\n    df.keyword.fillna(\"unknown\", inplace = True)\n    df.location.fillna(\"unknown\", inplace = True)\n    df['clean_tweet'] =  df.text + ' location: ' + df.location + ' keyword: ' + df.keyword\n    ","1668c222":"df = pd.read_csv(os.path.join('\/kaggle\/input\/nlp-getting-started', 'train.csv'))\ndf = df.sample(frac = 1)\nprepare_data(df)\ntrain_df, val_df = np.split(df, [int(.9*len(df))])","72654450":"\n\ntraining_args = TrainingArguments(\n    output_dir= '\/kaggle\/output',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='\/kaggle\/output',            # directory for storing logs\n    logging_steps=10,\n)\n","882bb8bd":"model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","d0e60e9b":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntrain_encodings = tokenizer(train_df.clean_tweet.to_list(), truncation=True, padding=True)\nval_encodings = tokenizer(val_df.clean_tweet.to_list(), truncation=True, padding=True)\ntotal_encodings = tokenizer(df.clean_tweet.to_list(), truncation=True, padding=True)","9f2dba4a":"import torch\n\nclass TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","00d7f885":"train_ds = TweetDataset(train_encodings, train_df.target.to_list())\nval_ds = TweetDataset(val_encodings, val_df.target.to_list())\ntotal_ds = TweetDataset(total_encodings, df.target.to_list())","13ee4d1f":"trainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_ds,              # training dataset\n    eval_dataset=val_ds,                 # evaluation dataset\n)\ntrainer.train()","1e75d63b":"# one more try with the total set\ntrainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=total_ds,              # training dataset = complete set\n    eval_dataset=total_ds,               # evaluation dataset = complete set\n)\ntrainer.train()","be8b7f7b":"trainer.evaluate()","f388e753":"from transformers import pipeline\npl = pipeline('sentiment-analysis',  model=model, tokenizer=tokenizer, device=0)","4f8d1682":"print(pl('it is a sunny day'))\nprint(pl('o my god the airplane crashed!'))","f54a5fc3":"test_df = pd.read_csv(os.path.join('\/kaggle\/input\/nlp-getting-started', 'test.csv'))\nprepare_data(test_df)\npredictions = [ pl(test_df.loc[x, 'clean_tweet']) for x in test_df.index ]\npreds = [ 0 if x[0]['label'] == 'LABEL_0' else 1  for x in predictions ]","ad284d1c":"for i in range(10):\n    print(str(preds[i]) + '  ' + test_df.loc[i, 'clean_tweet'])","98c80e4d":"output = pd.DataFrame({'id': test_df.id, 'target': preds})\n\noutput.to_csv('hak_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","5ea1e3d8":"Now we define a default torch DataSet that will be used for training, data comes from the data frames","1a9b9ab5":"We need to create encodings for the tweets, but we can use the existing tokenizer","71d8c5fe":"Let's check the training loss.","95453368":"**Using a pretrained DISTILBERT from huggingface **\n\nThe idea is to take the model distilbert-base-uncased and fine tune it with the tweets.\nUses the pipeline API which is very easy to work with!\n\nSee https:\/\/huggingface.co\/transformers\/custom_datasets.html#seq-imdb on fine tuning a pretrained model.\n","72d54b81":"Check some of the samples","726acde9":"We preprocess the code to include the keywords and location in the tweet text..\n\nSteps\n*     Load the data\n*     Clean the null values in keyword and location\n*     append the keyword and location to the tweet and store it in a new column\n\nFirst we define a cleansing funtion","ece92f0a":"This looks fine,a sunny day is not catastrophic, but an airplace crash is.\nNow lets check the test set.","628aa8dd":"And now lets make some predictons, device=0 is required if you run the notebook on gpu","ffeea8ad":"Now we read in the tweet, shuffle it (seems to be sorted by keyword) and split it in 90% training and 10% validation data.","7f9196ff":"Now create the result file","97e333b5":"Now we finetune the pretrained distilbert with our tweets. Distilbert can be used for text classification.\n\n"}}