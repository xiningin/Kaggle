{"cell_type":{"10bfcf1f":"code","9ce0cce2":"code","d0ef3cec":"code","db52693b":"code","ffb05571":"code","1f52f778":"code","a705fd15":"code","818658f6":"code","b7d6aaf9":"code","a2419e77":"code","9d63abc7":"code","e919791e":"code","b1947f45":"code","20cb2b13":"code","826438a0":"code","1c22b403":"code","3da0f33f":"code","712f7e51":"code","a07f3b81":"code","7ec3acda":"code","a77e9fa9":"code","1f5154e9":"code","4e077b2e":"code","3ccbe5b0":"code","13791cc3":"code","3526a30b":"code","a48a88f1":"code","df27e5cd":"code","909de1b0":"code","65017b3f":"code","18384d34":"code","1c1aa810":"code","e91fd07c":"code","777bb92f":"code","35750583":"code","e17ab767":"code","6fc63d8f":"code","e59ff63d":"code","3448e71b":"code","5ba46759":"code","ad006704":"code","2d5029e2":"code","0cc4bdaf":"code","50d2b110":"code","092342df":"code","b718a0b2":"code","582ea910":"code","98c47690":"code","368eb36f":"code","4f259f87":"code","98174427":"code","86621f15":"code","31fc4097":"code","79670b4d":"code","b01a028f":"code","c945342a":"code","56b62332":"code","f1568279":"code","e2f5bf51":"code","bf5e1cc8":"code","23cd7881":"code","60629ca1":"code","4f0de571":"code","39d6edec":"code","94857f5f":"code","627a6d6b":"code","d93e63aa":"code","829aaf5d":"code","9d17229a":"code","68111d27":"code","5ec159d0":"code","57fb2d9c":"code","9923c666":"code","34fe4169":"code","651c4242":"code","54c9e920":"code","33ada99e":"code","d1a88006":"code","8be0db76":"code","ed28a1b3":"code","299878f6":"code","a14786fa":"code","e7c86f54":"code","d92acc4f":"code","352a36fd":"code","ba47e67e":"code","8aecc05d":"code","f7364014":"code","593266e3":"code","4498466d":"code","14010f0f":"code","e82d78ef":"code","c8bf7096":"code","c3a47c66":"code","e3c64f26":"code","1334f5dd":"code","7fd7670b":"markdown","dc32aada":"markdown","a5bcfbfa":"markdown","042f03c6":"markdown","d3578fb8":"markdown","6c6a2290":"markdown","becade4e":"markdown","9a755ee6":"markdown","9d02f3e8":"markdown","6f98d65b":"markdown"},"source":{"10bfcf1f":"# Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport datetime as dt\n\n# Impute rest of missing values using KNN imputier \nfrom sklearn.impute import KNNImputer\n\n# Dashboard visiualization\nfrom matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage, AnnotationBbox)\n\n# Stats models for linear regression\nimport statsmodels.api as sm\n\n# Clustering Algorithm\nfrom sklearn.cluster import KMeans\n\n# Linear regression sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9ce0cce2":"# Reading the dataset\ndf =pd.read_csv('\/kaggle\/input\/fifa19\/data.csv')","d0ef3cec":"# Reading the first rows of the data\npd.set_option(\"display.max_columns\", 89)\nprint(df.shape)\ndf.head(5)  ","db52693b":"# Change columns names\ncols_names = [x.lower().strip().replace(' ','_') for x in list(df.columns)]\ndf.columns = cols_names","ffb05571":"# Drop unnecessary columns ['unnamed:_0', 'photo', 'flag', 'club_logo']\ndf.drop(['unnamed:_0', 'photo', 'flag', 'club_logo'], axis=1, inplace=True)","1f52f778":"# Build function to convert currency string to decimal value\n\ndef currency_to_decimal(s):\n\n    \"\"\"\n    Function to convert currency string to decimal value\n    the function takes (s) input and converted to amount in decimals\n    \"\"\"\n    \n    str_length = len(s)\n    dott_location = s.find(\".\")\n    \n \n    if dott_location < 0:\n        dott_location = 0\n        \n    k_zeros = 3\n    m_zeros = 6\n    \n    if dott_location == 0:\n        decimal_after_dot = 0\n    else:\n        decimal_after_dot = str_length  - dott_location - 2\n    \n    end_with_k = k_zeros - decimal_after_dot\n    end_with_m = m_zeros - decimal_after_dot\n    \n\n\n    if s[-1] == 'M':\n        s1 = s[1:-1].replace('.', '') + (end_with_m * \"0\")\n    else:\n        s1 = s[1:-1].replace('.', '') + (end_with_k * \"0\")\n\n    \n    return s1\n\n","a705fd15":"# Apply function currency_to_decimal to cols ['value', 'wage', 'release_clause']\ncurrency_cols = ['value', 'wage', 'release_clause']\n\nfor i in currency_cols:\n    df[f'{i}_new'] = df[i].fillna('0')\n    df[f'{i}_new'] = df[f'{i}_new'].apply(currency_to_decimal)","818658f6":"# Create new variable, Covert column weight from bound to Kg\nbound_to_kg =  0.45359237\ndf['weight_kg'] = df['weight'].fillna('1111').apply(lambda x: x[:-3]).astype('float') * bound_to_kg\n\n# Impute missing values and assign it to mean weight\ndf['weight_kg'] = df['weight_kg'].replace(bound_to_kg, df['weight_kg'].mean())","b7d6aaf9":"# Create new variable, Covert column height from feet to cm\nfeet_to_cm = 30.48 \ndf['height_cm'] = df['height'].replace(\"[\\']\", '.', regex=True).astype(float) * feet_to_cm","a2419e77":"# Specify the columns of players postions \npositions_list = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', \n                  'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb']\n\n# Create new dataframe container for operation on players postions\ndf_positions = pd.DataFrame()\n\n# Adjust the players postions by removing the the + sign after rating\nfor i in positions_list:\n    df_positions[i] = df[i].fillna('111').apply(lambda x: x[:-2])","9d63abc7":"# Create list of columns to drop\ncols_to_drop = ['value', 'wage', 'release_clause', 'weight', 'height', 'ls', 'st',\n                'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm',\n                'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb']\n\n# Create new dataframe and remove the unnecessary parameters\ndf2 = df.drop(cols_to_drop, axis = 1)\n\n# Append df_positions to new dataframe df2\n\ndf2 = pd.concat([df2, df_positions], axis = 1)","e919791e":"df2.head()","b1947f45":"# Rearrange columns of df2\n\ncols_arranged = [\n        # Qualtative (Categorical) Variable \n       'id', 'name', 'nationality', 'club',\n       'special', 'preferred_foot', 'work_rate', 'body_type', 'real_face', 'position',\n       'jersey_number', 'joined', 'loaned_from', 'contract_valid_until',\n \n       # Quantative Variable \n       'weight_kg', 'height_cm',\n       'age', 'overall', 'potential','international_reputation', 'weak_foot', \n       'skill_moves', 'crossing', 'finishing', 'headingaccuracy', 'shortpassing', 'volleys',\n       'dribbling', 'curve', 'fkaccuracy', 'longpassing', 'ballcontrol',\n       'acceleration', 'sprintspeed', 'agility', 'reactions', 'balance',\n       'shotpower', 'jumping', 'stamina', 'strength', 'longshots',\n       'aggression', 'interceptions', 'positioning', 'vision', 'penalties',\n       'composure', 'marking', 'standingtackle', 'slidingtackle', 'gkdiving',\n       'gkhandling', 'gkkicking', 'gkpositioning', 'gkreflexes', \n    \n       'ls', 'st','rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm',\n       'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb','rcb', 'rb',\n    \n    \n       # Target Variable \n       'wage_new', 'release_clause_new', 'value_new']","20cb2b13":"# put the dataframe df2 in order\ndf2 = df2[cols_arranged].copy()","826438a0":"# Restore missing values in positions list\ndf2[positions_list] = df2[positions_list].astype('float').replace(1.0, np.nan)\ndf2.fillna(np.nan, inplace=True)","1c22b403":"# Missing values\npd.set_option(\"display.max_rows\", 100)\ndf2.isnull().sum().sort_values(ascending = False) ","3da0f33f":"# Visualizing missing values\nmsno.matrix(df2);","712f7e51":"# Checking the cols with the least amount of Nans\nindex_missing = df2[df2['jumping'].isnull()].index\n\n# Dropping missing values identified from above\ndf2.drop(index_missing, inplace = True)","a07f3b81":"# Investigating & Dealing wit column \"loaned_from\" for reason of missing values\ndf2['loaned_from'].value_counts()\n\n\"\"\"\nThe data values of the column are for the club name \nwhere a player is loaned temporarily to, so we will transform \nthe column to a binary column where it we define if a player is loaned or not\n\"\"\"\n# Create new varaible \"loaned\"\ndf2['loaned'] = df2['loaned_from'].apply(lambda x: 0 if pd.isnull(x) else 1)\n\n# Drop variable \"loaned_from\"\ndf2.drop('loaned_from', axis=1, inplace = True)","7ec3acda":"# Investigating & Dealing wit column \"contract_valid_until\" for reason of missing values\nindex_missing_contracts = df2[df2['contract_valid_until'].isnull()].index\n\n\"\"\"\nColumn includes player who are not contraced with any club, 85 rows\nAction: drop null values\n\"\"\"\n# Drop missing of \"contract_valid_until\" values from df \ndf2.drop(index_missing_contracts, axis=0, inplace = True)","a77e9fa9":"# Investigating & Dealing wit column \"joined\" for reason of missing values\ndf2[df2['joined'].isnull()]\n\"\"\"\nThe date when a player joined a club\nAction: Impute with the average duration for contracts\n\"\"\"\n\n# Adjust type for cols (joined, contract_valid_until)\ndf2['joined'] = pd.to_datetime(df2['joined'])\ndf2['contract_valid_until'] = pd.to_datetime(df2['contract_valid_until'])\n\n# Create new variable \"durations_with_club\"\ndf2['durations_with_club'] = df2['contract_valid_until'] - df2['joined']\n\n# Statistic description of column \"durations_with_club\"\ndf2['durations_with_club'].describe()","1f5154e9":"#Handling missing values in \"=joined\" column\n\n\"\"\"\nWe can see that 75% of all payers are having during of almost 4.5 years with thier clubs\ntherefore we can use this value to estimate the missing values \n\"\"\"\n\n#Estimating missings values for column joined\n#df2['joined'][df2['joined'].isnull()] = df2['contract_valid_until'] - dt.timedelta(4.5*365)\n\nind_joined_null = df2[df2['joined'].isnull()].index\ndf2.loc[ind_joined_null, 'joined'] = df2['contract_valid_until'] - dt.timedelta(4.5*365)\n\n\"\"\"\nFrom statistical description we can see that there are unlogical date values \nwhere the joined date is greater than the contract expiry date, as a way to mitigate\ndate errors we will assume that the minimum contract duration will be 90 days\nand will adjusted dates accordingly\n\"\"\"\n\n# Adjusted dates where columns \"joined\" and \"contract_valid_until\" mismatch\n#df2['joined'][df2['durations_with_club'] < dt.timedelta(90)] = df2['contract_valid_until'] - dt.timedelta(90)\n\nind_date_mismatch = df2[df2['durations_with_club'] < dt.timedelta(90)].index\ndf2.loc[ind_date_mismatch, 'joined'] = df2['contract_valid_until'] - dt.timedelta(90)\n\n# Recalculate in years column \"durations_with_club\"\n#df2.drop('durations_with_club', inplace=True, axis=1)\ndf2['durations_with_club'] = ((df2['contract_valid_until'] - df2['joined']).dt.days.astype('int') )\/ 365\n\n# Checking the dates statistcs to ensure everything is in order\n(df2['contract_valid_until'] - df2['joined']).describe()\n\n","4e077b2e":"# Ensure correct data types\n# Check dataframe information (types and counts)\ndf2.info()","3ccbe5b0":"# Change categorical variables data type to object\ncategorical_variables = df2.columns[:11]\n\nfor i in categorical_variables:\n    df2[i] = df2[i].astype('str')\n    \n    \n# Change quantative variables data type to float, int\nquantative_variables = df2.columns[13:]\n\nfor j in quantative_variables:\n    df2[j] = df2[j].astype('float')\n    \ndf2['jersey_number'] = df2['jersey_number'].astype('float').astype('int')\ndf2['loaned'] = df2['loaned'].astype('float').astype('int')","13791cc3":"# grab columns of numerical values\nnumerical_cols = [x for x in df2.columns if df2[x].dtype != 'O']\nnumerical_cols.remove('contract_valid_until') \nnumerical_cols.remove('joined') \n\n# Data for model fitting\nx= df2[numerical_cols]\nx = x.fillna(np.nan)\nx = x.values","3526a30b":"# Instantiate KNN imputer model\nimputer = KNNImputer(n_neighbors=5, add_indicator =True)\nimputer.fit(x)","a48a88f1":"# Predication result\nimputed = pd.DataFrame(imputer.transform(x))","df27e5cd":"# Identify columns of interest for predication\npositions_list2 = [f'{x}_indicator' for x in positions_list]\nimputed_columns = numerical_cols + positions_list2\nimputed.columns = imputed_columns\n\ncols_knn = positions_list + positions_list2","909de1b0":"# Create new dataframe of predication columns of interest \npredication_df = imputed[cols_knn]\npredication_df.head()","65017b3f":"# Visualization the KNN prediction result\nplt.figure(figsize = [20, 30])\n\nfor i in range(26):\n    plt.subplot(7, 4, i+1);\n    predication_df[positions_list[i]][predication_df[positions_list2[i]] == 0].plot(ls='', marker='.', alpha=.05,label='Real');\n    predication_df[positions_list[i]][predication_df[positions_list2[i]] == 1].plot(ls='', marker='.', alpha=.2,label='predicated');\n    plt.title(positions_list[i])\n    plt.xticks([0, (predication_df.shape[0] \/ 2), predication_df.shape[0]], [0,50,100])\n    plt.legend()","18384d34":"#Droping cleaned columns\ndf_cleaned = df2.drop(positions_list, axis=1)","1c1aa810":"# Merging the predication result to main df\ndf_cleaned = df_cleaned.reset_index(drop=True) # Reset Index \npredication_df = predication_df[positions_list] # Isolate columns of Interest\npredication_df = predication_df.reset_index(drop=True) # Reset Index \ndf_cleaned = pd.concat([df_cleaned, predication_df], axis=1) # Merge the 2 df","e91fd07c":"# Visualizating the result of the cleaning process\nmsno.matrix(df_cleaned)","777bb92f":"df_cleaned.head(2)","35750583":"df_cleaned.nationality.value_counts().head(10)","e17ab767":"# count of players in the dataset\nprint(f'Count of players in the dataset is: {df_cleaned.shape[0]} players')\n\n# count of features in the dataset\nprint(f'Count of features in the dataset is: {df_cleaned.shape[1]} features')\n\n# count of nationalities in the dataset\nprint(f'Count of nationalities in the dataset is: {df_cleaned.nationality.nunique()} nationalities')\n\n# count of clubs in the dataset\nprint(f'Count of clubs in the dataset is: {df_cleaned.club.nunique()} clubs')","6fc63d8f":"#Exploring quantative variables\ndf_cleaned.hist(figsize=[20,30]);\ndf_cleaned.describe()","e59ff63d":"# Exploring categorical variables\nfor i in df_cleaned.columns[2:11]:\n    \n    ind = df_cleaned[i].value_counts().index\n    val = df_cleaned[i].value_counts().values\n    \n    print(f'Variable: {i}')\n    print(df_cleaned[i].value_counts().describe())\n    print('.'*50)\n    \n    plt.figure(figsize = [20, 5])\n    plt.bar(ind, val);\n    plt.xticks(rotation=90)\n    plt.title(i)\n    \n","3448e71b":"# Heat map to check for correlation accross all variables in the dataset\nplt.figure(figsize=[15, 10])\nsns.heatmap(df_cleaned.corr(), cmap='Blues');","5ba46759":"#converting correlation matrix to df \ndf_corr = df_cleaned.corr()\ncorr_table = df_corr[abs(df_corr) >= 0.01].stack().reset_index()\ncorr_table = corr_table[corr_table['level_0'].astype(str)!=corr_table['level_1'].astype(str)]\ncorr_table['ordered-cols'] = corr_table.apply(lambda x: '-'.join(sorted([x['level_0'],x['level_1']])),axis=1)\ncorr_table = corr_table.drop_duplicates(['ordered-cols'])\n#corr_table.drop(['ordered-cols'], axis=1, inplace=True)\ncorr_table['abs_corr_val'] = abs(corr_table[0])\n\ncorr_table = corr_table.sort_values('abs_corr_val', ascending=False).reset_index()\n\ncorr_table.head()","ad006704":"# Quantifing correlation matrix results\ntotal = corr_table.shape[0]\nperfect_corr = (corr_table['abs_corr_val'] == 1).sum()\ncorr_above80 = (corr_table['abs_corr_val'] > 0.79).sum()\ncorr_above70 = (corr_table['abs_corr_val'] > 0.69).sum()\ncorr_above60 = (corr_table['abs_corr_val'] > 0.59).sum()\nAVG_correlation = corr_table['abs_corr_val'].mean()\n\nprint(f'Total combinations created by the correlation matrix is {total}')\nprint(f'There are {perfect_corr} variables which have a perfect 100% correlation between each others')\nprint(f'There are {corr_above80} variables which have correlation greater than 80% between each others')\nprint(f'There are {corr_above70} variables which have correlation greater than 70% between each others')\nprint(f'There are {corr_above60} variables which have correlation greater than 60% between each others')\nprint(f'Absolute average of correlation is {AVG_correlation: .2f} ')\n","2d5029e2":"# Visualizing correation dataframe based on strength on abstract level by ignoring direction\nplt.figure(figsize=[15, 6])\nplt.bar(corr_table.index, corr_table['abs_corr_val']);\nplt.axhline(0.8, c='r', ls='--', label='correlation > 80%');\nplt.axhline(0.7, c='orange', ls='--', label='correlation > 70%');\nplt.axhline(0.6, c='yellow', ls='--', label='correlation > 60%');\nplt.axvspan(-2, 22, color='r', alpha =0.5, label='Perfect correlation');\nplt.title('correlation matrix |Result abstracted|')\nplt.ylabel('correlation %')\nplt.legend();","0cc4bdaf":"# Splitting the features of the dataset into categories\nattack_list = ['crossing', 'finishing', 'headingaccuracy', 'shortpassing', 'volleys']\nskill_list = ['dribbling', 'curve', 'fkaccuracy', 'longpassing', 'ballcontrol']\nmovement_list = ['acceleration', 'sprintspeed', 'agility', 'reactions', 'balance']\npower_list = ['shotpower', 'jumping', 'stamina', 'strength', 'longshots']\nmentality_list = ['aggression', 'interceptions', 'positioning', 'vision', 'penalties', 'composure']\ndefence_list =['marking', 'standingtackle', 'slidingtackle']\ngk_list = ['gkdiving', 'gkhandling', 'gkkicking', 'gkpositioning', 'gkreflexes']\n\nplayer_list = ['id', 'name', 'nationality', 'club', 'special', 'loaned', 'preferred_foot',\n       'work_rate', 'body_type', 'real_face', 'position', 'jersey_number',\n       'joined', 'contract_valid_until', 'durations_with_club', 'weight_kg', 'height_cm', 'age',\n       'overall', 'potential', 'international_reputation', 'weak_foot',\n       'skill_moves']\n\nvalue_list = ['wage_new']\n\nposition_rate_list = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw',\n       'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm',\n       'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb']\ncols_list = [attack_list, skill_list, movement_list, power_list, mentality_list, defence_list,\n        gk_list, position_rate_list, player_list, value_list]\n","50d2b110":"len(position_rate_list)","092342df":"# Creating new variables compiling similar features\nvars_compiled = [attack_list, skill_list, movement_list, power_list, mentality_list, defence_list, gk_list] \nvars_name = ['attack', 'skill', 'movement', 'power', 'mentality', 'defence', 'gk']\n\nattack = list(df_cleaned[attack_list].mean(axis=1))\nskill = list(df_cleaned[skill_list].mean(axis=1))\nmovement = list(df_cleaned[movement_list].mean(axis=1))\npower = list(df_cleaned[power_list].mean(axis=1))\nmentality = list(df_cleaned[mentality_list].mean(axis=1))\ndefence = list(df_cleaned[defence_list].mean(axis=1))\ngk = list(df_cleaned[gk_list].mean(axis=1))","b718a0b2":"# Convert column \"work rate\" to numeric with hierarchy\nwork_rate_dict = {'Low\/ Low': 1, 'Low\/ Medium': 2, 'Low\/ High': 3,  'Medium\/ Low': 2,\n                  'Medium\/ Medium': 4, 'Medium\/ High':5, 'High\/ High':6, 'High\/ Medium': 5, 'High\/ Low': 3}\nwk_rate = list(df_cleaned['work_rate'].map(work_rate_dict))","582ea910":"# Convert columns positions to categorical with 4 distinct values (back, midfeild, front, goalkeeper)\nmid_list = ['RM', 'LAM', 'LM', 'RDM', 'CM', 'RAM', 'LDM', 'CAM', 'CDM', 'LCM']\nmid_list_lower = [x.lower() for x in mid_list]\n\nback_list = ['RWB', 'LWB', 'RB', 'LB', 'LCB', 'CB', 'RCB', 'RCM']\nback_list_lower = [x.lower() for x in back_list]\n\nfront_list = ['RF', 'ST', 'LW', 'LF', 'RS', 'LS', 'RW', 'CF']\nfront_list_lower = [x.lower() for x in front_list]\n\nmidfield = list(df_cleaned[mid_list_lower].mean(axis=1))\nback = list(df_cleaned[back_list_lower].mean(axis=1))\nfront = list(df_cleaned[front_list_lower].mean(axis=1))\n\n\npositin_dict = {'RM':'back', 'LAM':'back', 'LM':'back', 'RDM':'back', 'CM':'back', 'RAM':'back', \n                'LDM':'back', 'CAM':'back', 'CDM':'back', 'LCM':'back',\n                'RWB' : 'midfield', 'LWB' : 'midfield', 'RB' : 'midfield', 'LB' : 'midfield', \n                'LCB' : 'midfield', 'CB' : 'midfield', 'RCB' : 'midfield', 'RCM' : 'midfield',\n                'RF' : 'front', 'ST' : 'front', 'LW' : 'front', 'LF' : 'front', 'RS' : 'front',\n                'LS' : 'front', 'RW' : 'front', 'CF' : 'front', 'GK' : 'goalkeeper'}\n\npositi = list(df_cleaned['position'].map(positin_dict))\n","98c47690":"# Convert column \"preferred_foot\" to binary variable indicates right foot\nfoot_dict = {'Left':0, 'Right':1}\nright_foot = list(df_cleaned['preferred_foot'].map(foot_dict))","368eb36f":"# Adjust value of column \"body_type\", change body type to normal in inconsistent cells\nbody_dict = {'Normal':'normal', 'Lean':'lean', 'Stocky':'stocky', 'Messi':'normal', \n              'C. Ronaldo':'normal', 'Neymar':'normal', 'Courtois':'normal', \n              'PLAYER_BODY_TYPE_25':'normal', 'Shaqiri':'normal', 'Akinfenwa':'normal'}\n\nbody_type = list(df_cleaned['body_type'].map(body_dict))","4f259f87":"# Rescale columns ('international_reputation', 'weak_foot', 'skill_moves') to be between 0-100\n\ndf_cleaned['int_rept'] = df_cleaned['international_reputation'] * 20\ndf_cleaned['weak_ft'] = df_cleaned['weak_foot'] * 20\ndf_cleaned['skl_move'] = df_cleaned['skill_moves'] * 20","98174427":"df_new = df_cleaned.copy()","86621f15":"#Drop columns which has been transformed or don't have clear explanation.\ndrop_cols = ['special', 'real_face', 'preferred_foot', 'position', 'joined', 'contract_valid_until', \n             'work_rate', 'body_type', 'value_new', 'release_clause_new',\n            'international_reputation', 'weak_foot', 'skill_moves']\n\ndf_new = df_new.drop(drop_cols, axis=1)\ndf_new = df_new.drop(attack_list, axis=1)\ndf_new = df_new.drop(skill_list, axis=1)\ndf_new = df_new.drop(movement_list, axis=1)\ndf_new = df_new.drop(power_list, axis=1)\ndf_new = df_new.drop(mentality_list, axis=1)\ndf_new = df_new.drop(defence_list, axis=1)\ndf_new = df_new.drop(gk_list, axis=1)\ndf_new = df_new.drop(mid_list_lower, axis=1)\ndf_new = df_new.drop(back_list_lower, axis=1)\ndf_new = df_new.drop(front_list_lower, axis=1)","31fc4097":"#Merge new transformed variables\n\ntransformed_vars = [right_foot, body_type, attack, skill, movement, power, mentality, defence,\n                    gk, wk_rate, midfield, back, front, positi]\n\n\ndf_new['right_foot'] = right_foot\ndf_new['body_type'] = body_type\ndf_new['wk_rate'] = wk_rate\ndf_new['attack'] = attack\ndf_new['skill'] = skill\ndf_new['movement'] = movement\ndf_new['power'] = power\ndf_new['mentality'] = mentality\ndf_new['defence'] = defence\ndf_new['gk'] = gk\ndf_new['midfield'] = midfield\ndf_new['back'] = back\ndf_new['front'] = front\ndf_new['position'] = positi\n","79670b4d":"# Reorder & Renamce the columns of the df\n\ncols_order = ['id', 'name', 'nationality', 'club', 'durations_with_club', 'loaned', 'body_type',\n              'weight_kg','height_cm', 'age', 'right_foot', 'wk_rate', 'jersey_number', 'position', \n              'overall', 'potential', 'int_rept', 'weak_ft', 'skl_move', 'attack', 'skill', 'movement', \n              'power', 'mentality', 'defence', 'gk', 'midfield', 'back', 'front', 'wage_new']\n\ndf_new = df_new[cols_order].copy()\n\ncols_rename = ['id', 'name', 'nationality', 'club', 'duration', 'loaned', 'body_type',\n              'wgt_kg','hgt_cm', 'age', 'right_ft', 'wk_rate', 'jersey', 'position', \n              'overall', 'potential', 'int_rept', 'weak_ft', 'skl_move', 'attack', 'skill', 'movement', \n              'power', 'mentality', 'defence', 'gk', 'midfield', 'back', 'front', 'wage']\n\ndf_new.columns = cols_rename","b01a028f":"# A taste of the new df\nprint(df_new.shape)\ndf_new.head()","c945342a":"df_new.describe()","56b62332":"# Heat map to check for correlation accross all variables in the dataset\nplt.figure(figsize=[10, 10])\nsns.heatmap(df_new.corr(), cmap='Blues', cbar=False); #, annot=True, fmt=\".2f\", linewidths=.5","f1568279":"# Testing colinearity of new_df\n# converting correlation matrix to df \ndf_corr = df_new.corr()\ncorr_table = df_corr[abs(df_corr) >= 0.01].stack().reset_index()\ncorr_table = corr_table[corr_table['level_0'].astype(str)!=corr_table['level_1'].astype(str)]\ncorr_table['ordered-cols'] = corr_table.apply(lambda x: '-'.join(sorted([x['level_0'],x['level_1']])),axis=1)\ncorr_table = corr_table.drop_duplicates(['ordered-cols'])\n#corr_table.drop(['ordered-cols'], axis=1, inplace=True)\ncorr_table['abs_corr_val'] = abs(corr_table[0])\n\ncorr_table = corr_table.sort_values('abs_corr_val', ascending=False).reset_index()\n\ncorr_table.head()","e2f5bf51":"# Quantifing correlation matrix results\ntotal = corr_table.shape[0]\nperfect_corr = (corr_table['abs_corr_val'] == 1).sum()\ncorr_above80 = (corr_table['abs_corr_val'] > 0.79).sum()\ncorr_above70 = (corr_table['abs_corr_val'] > 0.69).sum()\ncorr_above60 = (corr_table['abs_corr_val'] > 0.59).sum()\nAVG_correlation = corr_table['abs_corr_val'].mean()\n\nprint(f'Total combinations created by the correlation matrix is {total}')\nprint(f'There are {perfect_corr} variables which have a perfect 100% correlation between each others')\nprint(f'There are {corr_above80} variables which have correlation greater than 80% between each others')\nprint(f'There are {corr_above70} variables which have correlation greater than 70% between each others')\nprint(f'There are {corr_above60} variables which have correlation greater than 60% between each others')\nprint(f'Absolute average of correlation is {AVG_correlation: .2f} ')\n","bf5e1cc8":"# statistics about players skills\nstats = df_new.describe()[1:]\ndef corr_matrix_func():\n    corr_matrix_vars = list(df_new.columns[14:-1])\n    sns.heatmap(stats[corr_matrix_vars], cmap='Blues', annot=True, cbar=False, lw=0.5)","23cd7881":"# Describing the statistics of skills variables\nnew_feat_list = list(df_new.columns[14:-1])\n\nnew_feat_list.remove('gk')\nnew_feat_list.remove('int_rept')\nnew_feat_list.remove('weak_ft')\nnew_feat_list.remove('skl_move')\n\nnew_feat = df_new[new_feat_list].stack()\nnew_feat_df = pd.DataFrame(new_feat).reset_index()\nnew_feat_df.columns = ['index', 'feature', 'score']\nnew_feat_df.drop('index', inplace=True, axis=1)","60629ca1":"# distribution of skills features=\ndef Players_skills_func():\n    sns.stripplot(x = \"feature\", y = \"score\", data = new_feat_df, color='k', alpha=0.1, size=1, jitter=0.35);\n    sns.boxplot(x = \"feature\", y = \"score\", data = new_feat_df, showmeans=True,\n                meanprops={\"marker\":\"o\", \"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"10\"});\n    plt.title(\"Players skills rating\");","4f0de571":"# Top and bottom 10 clubs in terms of average rates and wages\nclub_rate = df_new.groupby('club')['overall'].mean()\nclub_wage = df_new.groupby('club')['wage'].mean()\n\navg_df = pd.concat([club_rate, club_wage], axis = 1)\n\navg_df['wage1'] = (avg_df['wage']-min(avg_df['wage']))\/(max(avg_df['wage'])-min(avg_df['wage']))\navg_df['wage1'] = avg_df['wage1'] * 100\navg_df.head()\n\navg_df_10 = avg_df.drop('wage', axis=1)\ntop10 = avg_df_10.sort_values('overall', ascending=False).iloc[0:10,].sort_values('overall', ascending=True)\nbottom10 = avg_df_10.sort_values('overall', ascending=True).iloc[0:10,].sort_values('overall', ascending=True)\navg_bar = pd.concat([bottom10, top10])\n\navg_bar","39d6edec":"# polar graph attributes\nfeatures_list = list(df_new.columns[14:-5])\nfeatures_list.append(features_list[0])\nlabel_placement = np.linspace(0, 2*np.pi, len(features_list))\ny_polar = list(df_new[features_list].mean())\n\ndef polar_func():\n    #plt.subplot(projection = 'polar')\n    plt.plot(label_placement, y_polar)\n    line, labels = plt.thetagrids(np.degrees(label_placement), labels=features_list)","94857f5f":"# Dashboard\n\nfig = plt.figure(figsize=[29.7, 21]); \ngrid = fig.add_gridspec(100, 100);\nfig.tight_layout()\n\nfig.set_facecolor('white')\n\n# Plotting the dashboard header\ngrid_2 = grid[0:10, :]\nax1 = fig.add_subplot(grid_2);\nfifa_color = (0.13, 0.29, 0.85, 1) # Axis background color\nax1.set_facecolor(fifa_color)\nax1.set_xticks([]);\nax1.set_yticks([]);\n\ngrid_header = grid[0:10, :35]\nax2 = fig.add_subplot(grid_header);\nheader_photo = plt.imread('..\/input\/photoheader\/fi.jpg') # Reading Header\nax2.imshow(header_photo) \nax2.spines[['right', 'left', 'top', 'bottom']].set_visible(False)\nax2.text(2000, 350, \"FIFA 2019 DASHBOARD\", color='white', fontsize = 70, weight='bold'); \nax2.set_xticks([]);\nax2.set_yticks([]);\n\n\n# grid for statistics about players skills\ngrid_correlation1 = grid[22:42, 2:36]\nax4 = fig.add_subplot(grid_correlation1);\ncorr_matrix_func()\n\n# grid for histogram about players wages distribution\ngrid_wage = grid[22:42, 39:55]\nax5 = fig.add_subplot(grid_wage);\n\nsns.histplot(df_new['wage'][df_new['wage'] < 60000], kde=True, ax=ax5);\n#ax5.hist(df_new['wage'][df_new['wage'] < 60000], alpha=0.5, edgecolor='gray')\nax5.set_title('Wages Distribution');\nwages_dist = ((df_new['wage']<60000).sum() \/ df_new.shape[0]) * 100\nax5.text(30000, 3000, f'{wages_dist: .2f}% of players\\n under 60k wage');\n\n\n\n\n# grid for histogram about players ages distribution\ngrid_age = grid[22:42, 58:76]\nax6 = fig.add_subplot(grid_age);\nax6.set_title('Ages Distribution');\n#ax6.hist(df_new['age'], alpha=0.5)\nsns.histplot(df_new['age'], kde=True, ax=ax6);\n\n\n\n\n# grid for histogram about players Weights distribution\ngrid_weight = grid[22:30, 79:98]\nax7 = fig.add_subplot(grid_weight);\nax7.set_title('Weights Distribution');\n#ax7.hist(df_new['wgt_kg'], alpha=0.5)\nsns.histplot(df_new['wgt_kg'], kde=True, ax=ax7);\n\n\n# grid for histogram about players Heights distribution\ngrid_height = grid[34:42, 79:98]\nax8 = fig.add_subplot(grid_height);\nax8.set_title('Heights Distribution');\n#ax8.hist(df_new['hgt_cm'], alpha=0.5)\nsns.histplot(df_new['hgt_cm'], kde=True, ax=ax8);\n\n\n# grid for boxplot about players skills distribution\ngrid_skills_box = grid[75:100, 33:77]\nax9 = fig.add_subplot(grid_skills_box);\nPlayers_skills_func()\n\n# Grid for top and bottom 10 clubs in terms of average rates and wages\ngrid_clubs = grid[50:, 5:30]\nax10 = fig.add_subplot(grid_clubs);\navg_bar.plot.barh(ax=ax10);\nax10.axhline(9.4, c='r', ls='--');\n\nax10.axhspan(0, 9.4, color='red', alpha=0.05);\n\nax10.text(60, 5, \"Lowest 10 Clubs\", fontsize=20, color='red');\nax10.legend(loc='lower right');\nax10.set_title('Highest & Lowest clubs for average rates and wages');\n\n\n# Grid for top and bottom 10 clubs in terms of average rates and wages\ngrid_polar = grid[50:70, 30:50]\nax11 = fig.add_subplot(grid_polar, projection = 'polar');\npolar_func()\nax11.set_title('Average Rating overall');\n\n\n# Grid for Count of players per position on field\ngrid_ = grid[50:70, 53:80]\nax12 = fig.add_subplot(grid_);\nax12.set_title('Count of players per position on field');\nsns.countplot(x='position', data=df_new, ax=ax12, color='tab:blue', alpha=0.7);\nax12.spines[['right', 'top']].set_visible(False)\n\n\n# Grid for Player with Right foot and left foot\ngrid_1 = grid[50:70, 80:]\nax13 = fig.add_subplot(grid_1);\n\nplt.pie(df_new['right_ft'].value_counts(), startangle=90, labels=['Right foot', 'Left foot'], autopct='%1.1f%%');\ncircle =plt.Circle((0, 0), 0.6, color= 'white');\ncircle_fig = plt.gcf();\ncircle_fig.gca().add_artist(circle);\nax13.set_title('Player with Right foot and left foot');\n\n\n\n# Grid for Count of players per body type\ngrid_2 = grid[73:100, 82:]\nax14 = fig.add_subplot(grid_2);\nsns.countplot(y='body_type', data=df_new, ax = ax14, color='tab:blue', alpha=0.7);\nax14.set_title('Count of players per body type');\n\n\n# ====================================\n\ngrid_cards = grid[10:20, :]\nax3 = fig.add_subplot(grid_cards);\nax3.spines[['right', 'left', 'top', 'bottom']].set_visible(False)\nax3.set_xticks([]);\nax3.set_yticks([]);\n\nax3.text(0.14, 0.2, \"18\", color='dodgerblue', fontsize = 80, weight='bold'); \nax3.text(0.205, 0.2, \"k\", color='dodgerblue', fontsize = 40, weight='bold');\nax3.text(0.205, 0.52, \"Players\", color='dimgray', fontsize = 20, weight='bold');\nax3.text(0.14, 0.75, \"Almost\", color='silver', fontsize = 10, weight='bold'); \n\n\nax3.text(0.406, 0.2, \"651\", color='lightcoral', fontsize = 80, weight='bold'); \nax3.text(0.505, 0.52, \"Football\", color='dimgray', fontsize = 20, weight='bold');\nax3.text(0.505, 0.3, \"Clubs\", color='dimgray', fontsize = 20, weight='bold');\n \n    \nax3.text(0.696, 0.2, \"163\", color='teal', fontsize = 80, weight='bold'); \n#ax3.text(0.765, 0.52, \"Football\", color='dimgray', fontsize = 20, weight='bold');\nax3.text(0.795, 0.3, \"Nationalities\", color='dimgray', fontsize = 20, weight='bold');\n\n\n# Grid for Player icon\ngrid_icon = grid[11:19, 5:15]\nax_icon1 = fig.add_subplot(grid_icon);\nplayer_link = '..\/input\/iconss\/player5.png'\nplayer_icon = plt.imread(player_link) # Reading Header\nax_icon1.imshow(player_icon) \nax_icon1.spines[['right', 'left', 'top', 'bottom']].set_visible(False)\nax_icon1.set_xticks([]);\nax_icon1.set_yticks([]);  \n\n\n# Grid for club icon\ngrid_icon2 = grid[11:19, 33:43]\nax_icon2 = fig.add_subplot(grid_icon2);\nclub_link = '..\/input\/iconss\/club3.png'\nclub_icon = plt.imread(club_link) # Reading Header\nax_icon2.imshow(club_icon) \nax_icon2.spines[['right', 'left', 'top', 'bottom']].set_visible(False)\nax_icon2.set_xticks([]);\nax_icon2.set_yticks([]);  \n\n\n\n# Grid for nationality icon\ngrid_icon3 = grid[11:19, 63:73]\nax_icon3 = fig.add_subplot(grid_icon3);\nnation_link = '..\/input\/iconss\/nationality1.png'\nnation_icon = plt.imread(nation_link) # Reading Header\nax_icon3.imshow(nation_icon) \nax_icon3.spines[['right', 'left', 'top', 'bottom']].set_visible(False)\nax_icon3.set_xticks([]);\nax_icon3.set_yticks([]); \n\n\n    \nplt.savefig(\"fifa19_dashboard1.png\", bbox_inches='tight')\n\n","627a6d6b":"#create new df \"clubs\n#test for the relationship between average rating of player skills per club and how it effect it's player wage\n\nclubs = df_new.groupby('club')[['overall', 'wage']].mean()\n\nclubs.head()","d93e63aa":"# Visualizing the relationship between wage and skill rate\nplt.figure(figsize =[15, 5]);\n\n#plot the scatter points between skill rate and wage on normal scale\nplt.subplot(1,2, 1);\nplt.scatter(data=clubs, x='overall', y='wage', alpha=0.3);\nplt.title('Relation between skills and wage');\nplt.ylabel('wage');\nplt.xlabel('skill rate');\n\n#plot the scatter points between skill rate and wage on log10 scale\nplt.subplot(1,2, 2);\nplt.scatter(data=clubs, x='overall', y='wage', alpha=0.3);\nplt.yscale('log');\nplt.title('Relation between skills and wage on logarthimic scale');\nplt.ylabel('wage on log scale');\nplt.xlabel('skill rate');","829aaf5d":"# adding new columns, mapping the wage on log10 scale\nclubs['wage_log10'] = np.log10(clubs['wage'])\n\n# add intercept for calculating slope on the linear regression model with statsmodels \nclubs['intercept'] = 1\n\nclubs[['wage_log10', 'overall']].corr()","9d17229a":"# Fitting the model\nlm = sm.OLS(clubs['wage_log10'], clubs[['intercept', 'overall']])\nresult = lm.fit()\n\n# Extract summary statistics from model result\nresult.summary()","68111d27":"# making predication with the model\nclubs['prediction_log'] = result.predict(clubs[['intercept', 'overall']])\nclubs['prediction'] = 10 ** clubs['prediction_log']","5ec159d0":"# Visualizing the model predication trend result \nplt.figure(figsize =[15, 5]);\n\nplt.subplot(1,2, 1);\n#plot the scatter points between skill rate and wage on log10 scale\nplt.scatter(data=clubs, x='overall', y='wage_log10', alpha=0.3, label='data points');\nplt.title('Relation between skills and wage on logarthimic scale');\nplt.ylabel('wage on log scale');\nplt.xlabel('skill rate');\n\n#trend on log scale\nplt.plot(clubs['overall'], clubs['prediction_log'], color='r', label='prediction trend');\nplt.legend();\n\nplt.subplot(1,2, 2);\n#plot the scatter points between skill rate and wage on normal scale\nplt.scatter(data=clubs, x='overall', y='wage', alpha=0.3, label='data points');\nplt.title('Relation between skills and wage on normal scale');\nplt.ylabel('wage on log scale');\nplt.xlabel('skill rate');\n\n#trend on normal scale\nclubs2 = clubs.sort_values('prediction')\nplt.plot(clubs2.overall, clubs2.prediction, color='r', lw=2, label='prediction trend')\nplt.legend();","57fb2d9c":"clubs['rsme'] = np.sqrt(np.power(clubs['prediction_log'] - clubs['prediction_log'].mean(), 2))\n\nrsme = clubs['rsme'].mean()\n\nprint(f'Root squares means error for the predication of log10 wages is {rsme: .2f}')","9923c666":"clubs.sort_values('overall', ascending=False).head()","34fe4169":"# Group the teams skills means\nskl_rates = pd.DataFrame(df_new.groupby('club')['overall'].mean())\nskl_rates.sort_values('overall', ascending=False).head(6)","651c4242":"# Calculating the difference between the top 2 times mean skills rating\ndif = float(skl_rates.loc['Juventus'].values - skl_rates.loc['Napoli'].values)\n\ndif","54c9e920":"# Create new df for top 2 teams\ntop_clubs = df_new[(df_new['club'] == 'Napoli') | (df_new['club'] == 'Juventus')]\n\n# Visualizing the distribution of top 2 teams\nplt.figure(figsize=[12, 5]);\nplt.subplot(1, 2, 1);\nsns.scatterplot(x= np.arange(0, top_clubs.shape[0]) , y=top_clubs['overall'], hue=top_clubs['club']);\nplt.title('Scatter: Juventus Vs. Napoli');\n\nplt.subplot(1, 2, 2);\nplt.hist(top_clubs['overall'][top_clubs['club'] == 'Juventus'], alpha=0.5, label = 'Juventus');\nplt.hist(top_clubs['overall'][top_clubs['club'] == 'Napoli'], alpha=0.5, label = 'Napoli');\nplt.title('Distribution: Juventus Vs. Napoli');\nplt.legend();","33ada99e":"# Hypothesis test for best combinations of players\njuv_arr = np.empty(10000)\nnapo_arr = np.empty(10000)\ndiff_arr = np.empty(10000)\n\n\nfor i in range(10000):\n    juv_samp = np.random.choice(top_clubs['overall'][top_clubs['club'] == 'Juventus'], 11)\n    napo_samp = np.random.choice(top_clubs['overall'][top_clubs['club'] == 'Napoli'], 11)\n    juv_arr[i] = juv_samp.mean()\n    napo_arr[i] = napo_samp.mean()\n    diff_arr[i] = juv_samp.mean() - napo_samp.mean()\n","d1a88006":"# Visualizing the Hypothesis test result\nplt.figure(figsize=[12, 5]);\nplt.subplot(1, 2, 1);\nplt.title('Bootstraping Distribution')\nplt.hist(juv_arr, alpha=0.5, label = 'Juventus');\nplt.hist(napo_arr, alpha=0.5, label = 'Napoli');\nplt.legend()\n\n\nplt.subplot(1, 2, 2);\nplt.hist(diff_arr, alpha=0.5, label = 'diff distribution');\nplt.axvline(np.percentile(diff_arr, 97.5), color='red', ls='--', label = 'UCL');\nplt.axvline(np.percentile(diff_arr, 2.5), color='red', ls='--', label = 'LCL');\nplt.axvline(dif, color='green', ls='--', label = 'Actual Difference');\nplt.legend()\nplt.title('Hypothesis Test - Bootstraping Result')\n\n\n\n(diff_arr < dif).sum() \/ 10000","8be0db76":"# Convert categorical variable body_type to numeric \ndf_new[['type_lean', 'type_normal']]  = pd.get_dummies(df_new['body_type']).drop('stocky', axis = 1)","ed28a1b3":"# Convert categorical variable position to numeric \ndf_new[['position_back', 'position_front', 'position_midfield']]  = pd.get_dummies(df_new['position']).drop('goalkeeper', axis = 1)","299878f6":"# Drop categorical variable and keep only variables to be fed to ML clustering algorithm\ndf_clusters = df_new.drop(['id', 'name', 'nationality', 'club', 'body_type', 'position'], axis=1)","a14786fa":"# Checking the final result of data preparation \ndf_clusters.head()","e7c86f54":"# Testing for best number of clusters based on elbow technique\nn_clusts = np.arange(1, 10)\nelbow_list = []\nfor i in n_clusts:\n    club_model = KMeans(n_clusters = i)\n    club_model.fit(df_clusters)\n    elbow_list.append(club_model.inertia_)\n    print(club_model.inertia_)","d92acc4f":"# Visualizing the result\nplt.plot(n_clusts, elbow_list, marker='o');\nplt.title('Inertia per number of clusters');\nplt.annotate('best parameter', xy=(4, 1.2 * 10**12), xytext=(4.8, 4 * 10**12),\n            arrowprops=dict(facecolor='black', shrink=0.05));\nplt.ylabel('Inertia');\nplt.xlabel('Number of clusters');","352a36fd":"# Applying clustering algorithm\nkmeans_model = KMeans(n_clusters = 4)\nkmeans_model.fit(df_clusters)\n\nkmeans_clusters = kmeans_model.predict(df_clusters)","ba47e67e":"# Attaching the clusters to dataframe\ndf_clusters['h_clusters'] = kmeans_clusters","8aecc05d":"# Result of clustering in numeric\navg_clusters = df_clusters.groupby('h_clusters')[['overall', 'wage', 'skl_move',\n       'attack', 'skill', 'movement', 'power', 'mentality', 'defence']].mean()\n\navg_clusters","f7364014":"# Clustering visiualized\nplt.figure(figsize=[6, 6]);\n\nplt.plot(df_clusters['overall'][df_clusters['h_clusters']==0], df_clusters['wage'][df_clusters['h_clusters']==0], \n         marker='.', lw=0, alpha=0.2, label='Cluster 1');\nplt.plot(df_clusters['overall'][df_clusters['h_clusters']==1], df_clusters['wage'][df_clusters['h_clusters']==1], \n         marker='.', lw=0, alpha=0.2, label='Cluster 2');\nplt.plot(df_clusters['overall'][df_clusters['h_clusters']==2], df_clusters['wage'][df_clusters['h_clusters']==2], \n         marker='.', lw=0, alpha=0.2, label='Cluster 3');\n\nplt.plot(df_clusters['overall'][df_clusters['h_clusters']==3], df_clusters['wage'][df_clusters['h_clusters']==3], \n         marker='.', lw=0, alpha=0.2, label='Cluster 3');\n\n\nplt.plot(avg_clusters['overall'], avg_clusters['wage'], \n         c='k', marker='o', markersize=10, lw=0, label='Centroids', alpha=0.4)\n\n\nplt.title('Clustering Result');\nplt.xlabel('skill rate');\nplt.ylabel('wages');\nplt.legend();\n\nplt.text(77, 300000, 'Super Players');\nplt.text(72, 150000, 'Elite Players');\nplt.text(58, 60000, 'Above Average Players');\nplt.text(50, 21000, 'Average Players');\n#plt.yscale('log')","593266e3":"# Converting clusters column to dummy variable\ndf_clusters[['cluster_1', 'cluster_2', 'cluster_3']] = pd.get_dummies(df_clusters['h_clusters']).drop(3, axis = 1)\ndf_numeric = df_clusters.drop('h_clusters', axis = 1)","4498466d":"# Split data into Training and testing\nX = np.array(df_numeric.drop('wage',axis=1))\ny = np.array(df_numeric['wage']) \ny = np.log10(y)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state= 0, shuffle = True)","14010f0f":"# Training the model on training data and predicting on testing data\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nr2 = r2_score(y_test,y_pred)\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint('R2 Score is : {} | Root Mean Square Error is : {}'.format(r2,rmse))","e82d78ef":"# Model coefficients\ncoefs = pd.DataFrame({'variable':df_numeric.drop('wage', axis=1).columns, 'coefficient':lr.coef_}\n                    ).append({'variable': 'intercept', 'coefficient':lr.intercept_}, ignore_index=True).reset_index(drop=True)\n\ncoefs","c8bf7096":"# comparing predication result to test value array\ntest = pd.DataFrame(y_test)\npred = pd.DataFrame(y_pred)\npred_compare = pd.concat([test, pred], axis =1)\npred_compare.columns = ['test_log', 'pred_log']\npred_compare = pred_compare.sort_values('pred_log')\npred_compare['test'] = 10 ** pred_compare['test_log']\npred_compare['pred'] = 10 ** pred_compare['pred_log']\n#pred_compare['within_95_confidence'] = \n\n\n\npred_compare.head()\n\n\n\n\npred_compare.head()","c3a47c66":"ci = np.std(pred_compare['pred']) * 1.96\nwithin_95 = []\n\nfor i, v in pred_compare.iterrows():\n    if (v[2] >= v[3] - ci) and ((v[2] <= v[3] + ci)):\n        within_95.append(1)\n    else:\n        within_95.append(0)\n        \n","e3c64f26":"within_95_array = np.array(within_95)\n\nprint(f'Total observation in the test set is {len(within_95_array)}')\nprint(f'Number of correct predications within 95% confidence interval is {within_95_array.sum()}')\nprint(f'Number of wrong predications within 95% confidence interval is {len(within_95_array) - within_95_array.sum()}')\nprint(f'Predications accuracy is {within_95_array.sum() \/ len(within_95_array):0.3f}')","1334f5dd":"# visualizing the model predication result \n\nrange_x = np.arange(1, pred_compare.shape[0]+1)\nci = np.std(pred_compare['pred'])*1.96\nci_log = np.std(pred_compare['pred_log'])*1.96\n\nplt.figure(figsize=[20,5]);\n\nplt.subplot(1, 2, 2);\nplt.plot(range_x, pred_compare['test'], alpha=0.2, lw=0, marker='.', label = 'Actual Values');\nplt.plot(range_x, pred_compare['pred'], lw=2, c='r', label = 'Predicted Values');\nplt.fill_between(range_x, pred_compare['pred'] + ci, pred_compare['pred'] - ci,alpha = 0.1, color= 'red', label = 'Confidence at 95%');\nplt.title('Comparing actual test set to predicted values')\nplt.ylabel('Player Value')\nplt.xlim([0, pred_compare.shape[0]]);\nplt.legend(loc='upper left');\n\nplt.subplot(1, 2, 1);\nplt.plot(range_x, pred_compare['test_log'], alpha=0.2, lw=0, marker='.', label = 'Actual Values');\nplt.plot(range_x, pred_compare['pred_log'], lw=2, c='r', label = 'Predicted Values');\nplt.fill_between(range_x, pred_compare['pred_log'] + ci_log, pred_compare['pred_log'] - ci_log, \n                 alpha = 0.05, color= 'red', label = 'Confidence at 95%');\nplt.title('Comparing actual test set to predicted values - log scale')\nplt.ylabel('Player Value - log scale')\nplt.xlim([0, pred_compare.shape[0]]);\nplt.legend(loc='upper left');\n","7fd7670b":"<a id='conclusion'><\/a>\n## Conclusion\n\n\n- From intial exploration to the datset we find out that are very strong multicolinearity between variables in the dataset, thus to fix the issue and make the dataset more legable and comperhandable will be trimming and engineering the feature and reduce dimensionality from 89 dimension to 29 dimension\n- In regard to relationship between players skill rate and wages we find out that there a significant exponential relationship between the variables with 81% correlation between players skill rate and log<sub>10<\/sub> of players wages\n- In regard to the idea of having a dominant team, after conduting a hypothesis testing on the top 2 team with highest average players skill rate, we concluded that there is no one team which has a siginificance dominance when it comes to players skills\n- To better understand the types of players and football players in the dataset we applied ML clustering algorithm KMeans and after conducting exhaustive test on the right number of clusters in the dataset we found out that there are 4 segements in the data, we can call them as follow (Super players - Elite players - Above average players - Average players)\n- To estimate the players wage based on available variable, we have applied linear regression model and were able to predict the wages of the a test dataset at 95% confidence interval with accuracy 99.7%, with only 17 incorrect predication out of  +5.3K predications","dc32aada":"<a id='bestclub'><\/a>\n## Research Question 2: Is there a dominant team which has the best combinations of players?\n\n-  In an effort to identify the team with the best combination of players, we group the club by the avg skill rate and find out that `Juventus` and `Napoli` are the highest rated teams\n- So we conduct a hypothesis test to check which team has the best combinations of players:\n\n<br>\n\n$Hypothesis:$\n\n\n\n<center>\n    $H_0: \\mu_{J} = \\mu_{N} $ <br><br>\n    $H_1: \\mu_{J} \\neq \\mu_{N} $ \n<\/center>\n\n<br><br>\n$J$ : Average skills rate for Juventus players<br>\n$N$ : Average skills rate for Napoli players <br>\n$\\alpha$ = 5%\n    \n<br>\n  \n> After conducting 10k bootstrap samples we find $P$ of $\\mu_{J} = \\mu_{N}$ is at 49%, so at $\\alpha$ = 5% <br> <center> <b>We Fail to Reject the Null Hypothesis <\/b> <\/center>","a5bcfbfa":"<a id='intro'><\/a>\n\n## Introduction\n\nThe FIFA 2019 dataset includes information about almost 18K players from more than 160 distinct nationalities and distributed over more than 600 football clubs from around the world, for each player in the dataset there are +80 features describing the characterstics of the players like nationality, age, physical characterstics, wages, position on the pitch, skills and performance measures.\n\n\nWill be tackling the project by firstly clean and wrangle the data, then will be moving to basic exploration univariate and multivariate, then build a user friendly dashboard for audience convenience,after that will be performing dimensionality reduction, doing hypothesis testing, drawing statistical ineference and applying supervised and unsupervised machine learning models to cluster and segment the football clubs and players and will predict players wages with linear regression \n\n\n","042f03c6":"![alt text](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1679908\/2753430\/headerfifa19.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211028%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211028T143941Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=4ea958c7ca63f8e8d60098c77cdfc8a94bc5aabffa95b1e8dd7f429f3283f1f1eb36eb8fa62c5d97298d7b0d918cb6c9ee9035d9362ecd1c0915c0eabe5c8ef367367ab961a0b052d35c83d9b618725ae8c151a78d7aad7f57f3146926487d0e93fae04e5c6a499a4be157cf4ae5aecb818b934753cf7dc4f58bbccfb0011c800bebd96b0d48cd23e3c137a9211d319b706b438b533a0d889821d2682bf2c37a8bf64a2f1ae44cd9816e7290eb09801a7523ec1a461e83f8c1bc1b7ce433c5490bcf9da00e45299ac3070a43218af3185e5ac22eedd50f123b8513f6061c8205f263a4c75817c12936c1fc35b7e0b1bc0e32d51a6b4e5c3a3a8768fb5d29db95)\n\n## FIFA 2019\n\nThe project is part of the Advanced Data Analysis Nanodegree Program by Udacity a scholarship by (Egypt FWD) Future work is a digital initiative powered by Information Technology Industry Development Agency (ITIDA) to upskill Web, Data and Digital marketing tech skills for jobs of the future, to equip and train 100,000 young Egyptians for digital technologies and skills to remote work and local market opportunities.\n\n#### Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#clean\">Data Cleaning<\/a><\/li>\n<li><a href=\"#explore\">Data Exploration<\/a><\/li>\n<li><a href=\"#Dashboard\">Project Dashboard<\/a><\/li>\n<li><a href=\"#skillwage\">Research Question 1: What is the effect of players skills rate on wages?<\/a><\/li>\n<li><a href=\"#bestclub\">Research Question 2: Is there a dominant team which has the best combinations of players?<\/a><\/li>\n<li><a href=\"#segment\">Research Question 3: What is the best segmentation based on hierarchy of clubs and players?<\/a><\/li>\n<li><a href=\"#wage_predicaton\">Research Question 4: What is the predict wage per player considering all variables in the dataset? How accurate is the predication ?<\/a><\/li>\n<li><a href=\"#conclusion\">Conclusion<\/a><\/li>\n<\/ul>\n\n\n","d3578fb8":"<a id='Dashboard'><\/a>\n## Project Dashboard\n\n","6c6a2290":"<a id='clean'><\/a>\n## Data Cleaning\n\n1. Change columns names ---> convert to lower case, remove spaces and add underscores\n2. Drop cols [Photo, Flag, Club Logo] ---> as they contain urls for photos\n3. Cols [value, wage, Release Clasue] ---> to be adjusted from currency string to floats\n4. Column [Weight] ---> change from bound to Kg and remove string 'lbs'\n5. Column [Height] ---> change from inch to cm and remove string '\n6. Adjust columns of player postion ratings \n7. Rearrange columns of df2 in order ---> (qualtative variables, quantative and target variable)\n8. Checking and handling missing values\n    1. Imputing rows with all data missing\n    2. Treating columns \"Loaned\" which has the highest `Nans`\n    3. Dealing with missing contracts dates\n    4. Build machine learning clustering algorithem `KNN` to impute missing data from players postions rating columns","becade4e":"<a id='skillwage'><\/a>\n## Research Question 1: What is the effect of players skills rate on wages?\n\n- We can see that there is an expontontial relationship between skills rate and player wages, Therefore we transform the y-axis \"wage\" to a **log<sub>10<\/sub>** scale and we find out that there is a **strong positive linear correlation at 81%**\n- There is a significant relationship between skills and wages at  **$P>|t|$** = 0.0000\n- Based on the result of the linear regression model, we can denote that for every 1 unit of increase of player skill rating, there an increase of `0.0840` of log<sub>10<\/sub> the wage, at average ROOT SQUARE MEANS ERROR of 0.30","9a755ee6":"<a id='segment'><\/a>\n## Research Question 3: What is the best segmentation based on hierarchy of clubs and players?\n\n- To estimate the best segmentations for players and teams using all the parameters on the data set, we conduct a test on the data by exhausting several n_clusters and measure the inertia and plot on chart and found out the best parameters is **4 clusters**","9d02f3e8":"<a id='wage_predicaton'><\/a>\n## Research Question 4: What is the predict wage per player considering all variables in the dataset? How accurate is the predication ?","6f98d65b":"<a id='explore'><\/a>\n## Data Exploration\n\n### Observations\n1. The FIFA 2019 dataset includes list of 17,918 football players from 163 distinct nationalities and distributed over more than 600 football clubs from around the world\n2. The dataset contains 86 features describing the characterstics of each player like nationality, age, physical characterstics, wages, position on the pitch also skills and performance like:\n    - Attacking\n    - Ball handling skills\n    - Movement\n    - Power\n    - Mentality\n    - Defending\n    - Goalkeeping\n3. Most of the variables are normally distributed with a few features are bimodally distributed and lesser varaibles skewed either to left or to right\n4. Players who have postions in the center (Striker `ST` - Goal keeper`GK` - Center back `CB` - Center midfeild `CM`) are well represented in the dataset in contrast to players who plays at wings positions like (Left forward `LF` - Right forwad `RF` - Right attack midfield`RAM`- Right attack midfield `LAM`)\n5. Most of the players are right footed\n6. England has the highest representation in terms of count of players, generally most of the player are from Europe and South-America\n7. 75% of all the players in the dataset are under the age of 29, and maximum age observed is 45 years old\n8. Features in the dataset are highly correlated with each other, as observed there are 22 variables which correlates perfectly with each other, and there are almost 750 combinations of parameters which has correlation greater than 60%, that can be explained as denoted in point **No. 2**, as all the skills features in dataset are describing the above mentioned points (*Attacking, Ball handling skills, Movement, Power, Mentality, Defending and Goalkeeping*)\n\n\n### Actions\n1. Group the skills variables into main 7 category listed Above\n2. Combine the player position on the pitch, so to reduce the number of features from 26 to 4 distinct features (`front` - `midfield` - `back` - `goalkeeping`)\n3. Convert column `work rate` to numeric with hierarchy\n4. Convert column `positions` to categorical with 4 distinct values (back, midfeild, front, goalkeeper)\n5. Convert column `preferred_foot` to binary variable indicates right foot\n6. Adjust value of column `body_type`, change body type to normal in inconsistent cells\n7. Substitute columns (`joined`, `contract_valid_until`) with a new feature which measures the duration for the players with their clubs\n8. Drop columns (`special`, `real_face`) as they don't have clear explanation or interpretation\n9. Drop columns (`value`, `release_clause`) as they are representing same ratios between 3 variables wage, value and release cols, since we have data for only 1 year we will keep only `wage`\n10. Rescale columns `international_reputation`, `weak_foot`, `skill_moves` to be between 0-100\n\n<br>\n\n> As a result will be creating a new dataframe where I will *remove, group and combine to introduce a whole new sets of features* which should be more beneficial in terms of **analysis and prediction** , so it would be useful for answering the **research questions**\n\n<br>\n\n### Results\n1. Making the dataset more concise, legable and comperhandable by reducing the dataset dimensionality from **89 demonstions to 29 dimensions**, while in the same time maintaining all the added value attributes in the dataset \n2. Features multicolinearity has been reduced drastically from an absolute correlation average between all features of the dataset of **39% to 27%** and features which has correlation greater than **60% reduced from 747 variables combinations to 47 only**"}}