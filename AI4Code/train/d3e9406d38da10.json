{"cell_type":{"2b80eb5f":"code","a3bc8685":"code","3feeca91":"code","4c82340d":"code","0ed8cddc":"code","6328c271":"code","0d14ad4d":"code","80ef3f67":"code","3cfcc388":"code","1f685bc7":"code","c2103782":"code","ffdf43d3":"code","c157bc0c":"markdown"},"source":{"2b80eb5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3bc8685":"# See how much data there is...\nyear_files = sorted(os.listdir(\"..\/input\/gsod_all_years\/\"))\nprint(year_files[:5])\nprint(year_files[-5:])\nprint(len(year_files))","3feeca91":"# First, let's import and look at a single dataset\nimport tarfile\npathstem = \"..\/input\/gsod_all_years\/\"\nwith tarfile.open(pathstem + year_files[-1]) as tar:\n    i = 0\n    for tarinfo in tar:\n        #Credits: https:\/\/docs.python.org\/3\/library\/tarfile.html\n        print(tarinfo.name, \"is\", tarinfo.size, \"bytes in size and is\", end=\"\")\n        if tarinfo.isreg():\n            print(\"a regular file.\")\n        elif tarinfo.isdir():\n            print(\"a directory.\")\n        else:\n            print(\"something else.\")\n        i += 1\n        if i > 5: break # just show a couple of files\n    tar.extractall(path='.\/temp')","4c82340d":"station_files = sorted(os.listdir(\".\/temp\"))\nprint(station_files[-5:])\nimport gzip\nwith gzip.open(\".\/temp\/\" + station_files[0],'rb') as station_file:\n    i = 0\n    for line in station_file:\n        print(line)\n        i+=1\n        if i > 4: break\n    station_df = pd.read_csv(station_file, sep=r'\\s+')\nprint(station_df.head())\nprint(station_df.describe())\nprint(station_df.columns.values)\n\n# NOTE that the header is missing the COUNT fields (ref. readme.txt), we'll need to add them manually","0ed8cddc":"from datetime import datetime\ntoday = datetime.today()\ntoday = today.month*100+today.day\nprint(today)","6328c271":"import gzip\n\n# convert to human-readable temperatures, i.e., *C (:P)\ndef FtoC (Ftemp):\n    return (Ftemp - 32.) * 5.0\/9.0\n# extract fields of interest (\"STN\", \"YEAR\", \"MODA\", \"MIN\", \"TEMP\", \"MAX\") from a single .op.gz line\ndef process_opgz_line (line):\n    return [line[:5], line[14:18], line[18:22], line[110:116], line[24:30], line[102:108]]\n\n# process a single .op.gz file\ndef process_opgz (opgz_path, moda=None, verbose=True):\n    # read in the data\n    station_data = []\n    with gzip.open(opgz_path,'rb') as station_file:\n        station_contents = station_file.readlines()[1:]\n    columns = [\"STN\", \"YEAR\", \"MODA\", \"MIN\", \"TEMP\", \"MAX\"] # header\n    # let's extract the data from their character-wise position (seems safest, ref. readme.txt)\n    station_data += list(map(lambda line : [line[:5], line[14:18], line[18:22], line[110:116], line[24:30], line[102:108]], station_contents))\n    if verbose: print(station_data[:5])\n    station_df = pd.DataFrame(station_data, columns=columns)\n    # cast to the right type\n    for col in columns:\n        station_df[col] = pd.to_numeric(station_df[col], errors='coerce')\n    # select only the data for the current month and day number\n    if moda != None:\n        station_df.drop(station_df[station_df['MODA'] != moda].index, inplace=True)\n    for col in ['MIN', 'TEMP', 'MAX']:\n        # convert to human-readable temperatures, i.e., *C (:P)\n        station_df[col] = station_df[col].apply(FtoC)\n        # handle missing data\n        for col in ['MIN', 'TEMP', 'MAX']:\n            mask = (station_df[col] < -100.) | (station_df[col] > 100.)\n            station_df[col].mask(mask, inplace=True)\n    if verbose:\n        print(station_df.head())\n        print(station_df.describe())\n    return station_df\ndf = process_opgz(\".\/temp\/\" + station_files[0])\nprint(df.shape[0])","0d14ad4d":"# process a single year, i.e., .tar file\nimport tarfile\nfrom tqdm import tqdm\ndef process_tar (tar_path, moda=None, verbose=True):\n    print(\"Processing year data from file %s..\" % tar_path)\n    # extract the tarfile\n    print(' - extracting tarfile.. ', end='', flush=True)\n    with tarfile.open(tar_path) as tar:\n        if verbose:\n            i = 0\n            for tarinfo in tar:\n                #Credits: https:\/\/docs.python.org\/3\/library\/tarfile.html\n                print(tarinfo.name, \"is\", tarinfo.size, \"bytes in size and is\", end=\"\")\n                if tarinfo.isreg():\n                    print(\"a regular file.\")\n                elif tarinfo.isdir():\n                    print(\"a directory.\")\n                else:\n                    print(\"something else.\")\n                i += 1\n                if i > 5: break # just show a couple of files\n        tar.extractall(path='.\/temp')\n    print('done.', flush=True)\n    # process all the op.gz files\n    print(\" - processing .op.gz files.. \", flush=True)\n    year_df = pd.DataFrame(columns=[\"STN\", \"YEAR\", \"MODA\", \"MIN\", \"TEMP\", \"MAX\"])\n    station_files = sorted(os.listdir(\".\/temp\"))\n    for station_file in tqdm(station_files):\n        station_df = process_opgz(\".\/temp\/\"+station_file, moda=moda, verbose=False)\n        if station_df.shape[0] > 0:\n            year_df = year_df.append(station_df)\n    print('    done.', flush=True)\n    print(' - averaging.. ', end='', flush=True)\n    # average all data per day per station\n    year_df = year_df.groupby(['YEAR', 'MODA', 'STN'], as_index=False).mean().reset_index()\n    # average per day across all stations\n    year_df = year_df.groupby(['YEAR', 'MODA'], as_index=False).mean().reset_index()\n    year_df.drop(labels=['STN',], inplace=True, axis=1)\n    print('done.', flush=True)\n    if verbose: print(year_df.head())\n    print(\" - removing temporary files.. \", end='', flush=True)\n    ! rm -r '.\/temp'\n    print('done.', flush=True)\n    print(' - year %s done.' % tar_path, flush=True)\n    return year_df","80ef3f67":"# find the last moda of the dataset\nyear_files = sorted(os.listdir(\"..\/input\/gsod_all_years\/\"))\ndf = process_tar(\"..\/input\/gsod_all_years\/\" + year_files[-1], moda=None, verbose=False)\nprint(df.head())\nprint(df.describe())","3cfcc388":"today = df['MODA'].max()\nprint(today)","1f685bc7":"# now process all the data; sit back and enjoy :D\nyear_files = sorted(os.listdir(\"..\/input\/gsod_all_years\/\"))\ndf = pd.DataFrame(columns=[\"YEAR\", \"MODA\", \"MIN\", \"TEMP\", \"MAX\"])\n# in principle, we can do the entire century, but, to keep it short,\n# let's limit ourselves to the last 10 years (should take about an hour to process)\n#  - it's, frankly, not very informative :\/, but this is to keep this kernel able to run in an hour or so...\nfor year_file in year_files[-10:]:\n    df = df.append(process_tar(\"..\/input\/gsod_all_years\/\" + year_file, moda=today, verbose=False))\ndf.sort('YEAR', inplace=True)","c2103782":"# save the processed data for reuse\nimport pickle as pkl\nwith open('processed_data.pkl', 'wb') as f:\n    pkl.dump(df, f)\n! ls .","ffdf43d3":"# finally, let's plot the data\nprint(df.head())\nprint(df.describe())\n\nimport matplotlib.pyplot as plt\nplt.clf()\nplt.scatter(df['YEAR'], df['TEMP'], color='red', label='Avg')\nplt.plot(df['YEAR'], df['MIN'], 'k--', label='Min')\nplt.plot(df['YEAR'], df['MAX'], 'k--', label='Max')\nplt.grid()\nplt.title('Station-average surface temperature on %i' % today)\nplt.xlabel('Year')\nplt.ylabel('Temperature')\nplt.show()","c157bc0c":"Now, for the sake of the exercise, let us say that we are analysing the dataset from a climate scientist point of view (it would be cool to produce one of those plots concerning global warming). As such, the data points of interest would be mean, minimal, and maximal temperatures; or (ref. readme.txt) TEMP, MIN, and MAX fields. To simplify, lets ignore the spatial data for now and just average over all entries on a day-by-day basis (note that this is not equivalent to a proper, spatially weighted average). We will also need time stamps, YEAR and MODA.\n\nLet's write a couple of functions to extract these data, they should go something like follows:\n1. Extract one year tarfile into .\/temp\n2. Unzip a single file from it, read YEAR, MODA, TEMP, MIN, MAX from it into a dictionary indexed by [YEAR, MODA, STN], close the file\n3. Repeat for all the files within the extracted year\n4. Average TEMP, MIN, MAX per station to have three values indexed by [YEAR, MODA, STN] in the dictionary.\n5. Average TEMP, MIN, MAX per day to have three values indexed by [YEAR, MODA] in the dictionary.\n6. Delete the extracted year (rm -r temp)\n7. Repeat for all the years in the dataset.\n\nNOTE: Processing the entire dataset would be very costly and should, ultimately, be done in parallel. Therefore, let's just focus on a single day a year for the sake of the exercise."}}