{"cell_type":{"30f7774a":"code","0e05a02f":"code","7b656b55":"code","f3ab5e92":"code","6e0e18e5":"code","e1065d46":"code","e2e2f6a9":"code","4baa7422":"code","53704feb":"code","2f48e4bd":"code","6a4f41ea":"code","a72b141d":"code","b269f6ae":"code","a7f34c9b":"code","999abda5":"code","b5d039e6":"code","5d4c64f2":"code","e2b4872f":"code","a5fd0584":"code","0c523616":"code","9628cb6a":"code","7f301ba7":"code","3befbe0f":"code","fbce804e":"code","00f8fd33":"code","8dd0ae32":"code","39d1c1cc":"code","6e664f34":"code","81da892f":"code","ceccb3e0":"code","bfe6ae95":"code","3117745e":"code","7aad1ee8":"code","66675d15":"code","b8459bc3":"code","786c669f":"code","183eaa84":"code","7a43bea7":"code","8a2475a3":"code","02d2b12c":"markdown","c890df3c":"markdown","b98e3e6f":"markdown","1806712e":"markdown","ca0877d4":"markdown","f8003811":"markdown","b2b18bf4":"markdown","da801378":"markdown"},"source":{"30f7774a":"import gc\nimport os\nimport pickle\nimport random\nimport time\nfrom collections import Counter, defaultdict\nfrom functools import partial\nfrom pathlib import Path\nfrom psutil import cpu_count\nimport matplotlib.pyplot as plt\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\n#from skmultilearn.model_selection import iterative_train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastprogress import master_bar, progress_bar\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms","0e05a02f":"NUM_CLASSES = 80\ncheckpoint_file = 'model_best.h5'\nSIZE=128\nEPOCHS = 200 #150 for inception, 100 for xception\nBATCH_SIZE = 64\n\nLR = 4e-4\nTTA = 19 #Number of test-time augmentation\nPATIENCE = 5  #ReduceOnPlateau option\nLR_FACTOR = 0.25 #ReduceOnPlateau option\nCURATED_ONLY = True # use only curated data for training\nTRAIN_AUGMENT = True # use augmentation for training data?\nMODEL = 'inception' # choose among 'xception', 'inception', 'mobile', 'simple'\n\n# if use BCEwithLogits loss, use Activation = 'linear' only\nACTIVATION = 'linear' \n# ACTIVATION = 'softmax'\n# ACTIVATION = 'sigmoid'\n\n# LOSS = 'categorical_crossentropy'\n# LOSS = 'binary_crossentropy' \nLOSS = 'BCEwithLogits' ","7b656b55":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 520\nseed_everything(SEED)","f3ab5e92":"# from official code https:\/\/colab.research.google.com\/drive\/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n            retrieved_cumulative_hits[class_rankings[pos_class_indices]] \/\n            (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class \/ float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) \/\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) \/ np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class","6e0e18e5":"import tensorflow as tf\n\n# from https:\/\/www.kaggle.com\/ratthachat\/keras-cnn-with-lwlrap-evaluation\/edit\ndef tf_one_sample_positive_class_precisions(y_true, y_pred) :\n    num_samples, num_classes = y_pred.shape\n    \n    # find true labels\n    pos_class_indices = tf.where(y_true > 0) \n    \n    # put rank on each element\n    retrieved_classes = tf.nn.top_k(y_pred, k=num_classes).indices\n    sample_range = tf.zeros(shape=tf.shape(tf.transpose(y_pred)), dtype=tf.int32)\n    sample_range = tf.add(sample_range, tf.range(tf.shape(y_pred)[0], delta=1))\n    sample_range = tf.transpose(sample_range)\n    sample_range = tf.reshape(sample_range, (-1,num_classes*tf.shape(y_pred)[0]))\n    retrieved_classes = tf.reshape(retrieved_classes, (-1,num_classes*tf.shape(y_pred)[0]))\n    retrieved_class_map = tf.concat((sample_range, retrieved_classes), axis=0)\n    retrieved_class_map = tf.transpose(retrieved_class_map)\n    retrieved_class_map = tf.reshape(retrieved_class_map, (tf.shape(y_pred)[0], num_classes, 2))\n    \n    class_range = tf.zeros(shape=tf.shape(y_pred), dtype=tf.int32)\n    class_range = tf.add(class_range, tf.range(num_classes, delta=1))\n    \n    class_rankings = tf.scatter_nd(retrieved_class_map,\n                                          class_range,\n                                          tf.shape(y_pred))\n    \n    #pick_up ranks\n    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices)\n\n    # add one for division for \"presicion_at_hits\"\n    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n    \n    # generate tensor [num_sample, predict_rank], \n    # top-N predicted elements have flag, N is the number of positive for each sample.\n    sample_label = pos_class_indices[:, 0]   \n    sample_label = tf.reshape(sample_label, (-1, 1))\n    sample_label = tf.cast(sample_label, tf.int32)\n    \n    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))\n    retrieved_class_true_position = tf.concat((sample_label, \n                                               num_correct_until_correct), axis=1)\n    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n                                         retrieved_pos, \n                                         tf.shape(y_pred))\n    # cumulate predict_rank\n    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n\n    # find positive position\n    pos_ret_indices = tf.where(retrieved_class_true > 0)\n\n    # find cumulative hits\n    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n    correct_rank = tf.cast(correct_rank, tf.float32)\n\n    # compute presicion\n    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n\n    return pos_class_indices, precision_at_hits\n\ndef tf_lwlrap(y_true, y_pred):\n    num_samples, num_classes = y_pred.shape\n    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n    pos_flgs = tf.cast(y_true > 0, tf.int32)\n    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n    class_label = pos_class_indices[:,1]\n    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n                                                        class_label,\n                                                       num_classes)\n    labels_per_class = tf.cast(labels_per_class, tf.float32)\n    labels_per_class = tf.add(labels_per_class, 1e-7)\n    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n                                  tf.cast(labels_per_class, tf.float32))\n    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n    return out","e1065d46":"from keras import backend as k\ndef BCEwithLogits(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits=True), axis=-1)","e2e2f6a9":"dataset_dir = Path('..\/input\/freesound-audio-tagging-2019')\npreprocessed_dir = Path('..\/input\/fat2019_prep_mels1')","4baa7422":"csvs = {\n    'train_curated': dataset_dir \/ 'train_curated.csv',\n    #'train_noisy': dataset_dir \/ 'train_noisy.csv',\n    'train_noisy': preprocessed_dir \/ 'trn_noisy_best50s.csv',\n    'sample_submission': dataset_dir \/ 'sample_submission.csv',\n}\n\ndataset = {\n    'train_curated': dataset_dir \/ 'train_curated',\n    'train_noisy': dataset_dir \/ 'train_noisy',\n    'test': dataset_dir \/ 'test',\n}\n\nmels = {\n    'train_curated': preprocessed_dir \/ 'mels_train_curated.pkl',\n    'train_noisy': preprocessed_dir \/ 'mels_trn_noisy_best50s.pkl',\n    'test': preprocessed_dir \/ 'mels_test.pkl',  # NOTE: this data doesn't work at 2nd stage\n}","53704feb":"train_curated = pd.read_csv(csvs['train_curated'])\ntrain_noisy = pd.read_csv(csvs['train_noisy'])\nif CURATED_ONLY:\n    train_df = train_curated\nelse:\n    train_df = pd.concat([train_curated, train_noisy], sort=True, ignore_index=True)\ntrain_df.head()","2f48e4bd":"test_df = pd.read_csv(csvs['sample_submission'])\ntest_df.head()","6a4f41ea":"labels = test_df.columns[1:].tolist()\nlabels[:10]","a72b141d":"num_classes = len(labels)\nnum_classes","b269f6ae":"y_train = np.zeros((len(train_df), num_classes)).astype(int)\nfor i, row in enumerate(train_df['labels'].str.split(',')):\n    for label in row:\n        idx = labels.index(label)\n        y_train[i, idx] = 1\n\ny_train.shape","a7f34c9b":"with open(mels['train_curated'], 'rb') as curated, open(mels['train_noisy'], 'rb') as noisy:\n    x_train = pickle.load(curated)\n    if CURATED_ONLY == False:\n        x_train.extend(pickle.load(noisy))\n\nwith open(mels['test'], 'rb') as test:\n    x_test = pickle.load(test)\n    \nlen(x_train), len(x_test)","999abda5":"\nfor ii in range(5):\n    print(x_train[ii].shape) #x_train is of shape (TRAIN_NUM,128,LEN,3) [4D Tensor]\n    print(x_test[ii].shape,'\\n')  #x_test of shape (TEST_NUM,128,LEN,3) [4D Tensor]","b5d039e6":"from keras.layers import *\nfrom keras.models import Sequential, load_model, Model\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as preprocess_inception\nfrom keras.applications.mobilenet_v2 import MobileNetV2\nfrom keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobile\nfrom keras.applications.xception import Xception\nfrom keras.applications.xception import preprocess_input as preprocess_xception\n\nfrom keras.utils import Sequence\nfrom sklearn.utils import shuffle\ndef create_model_inception(n_out=NUM_CLASSES):\n\n    base_model =InceptionV3(weights=None, include_top=False)\n    \n    x0 = base_model.output\n    x1 = GlobalAveragePooling2D()(x0)\n    x2 = GlobalMaxPooling2D()(x0)\n    x = Concatenate()([x1,x2])\n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    \n    predictions = Dense(n_out, activation=ACTIVATION)(x)\n\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model","5d4c64f2":"def create_model_xception(n_out=NUM_CLASSES):\n\n    base_model = Xception(weights=None, include_top=False)\n    \n    x0 = base_model.output\n    x1 = GlobalAveragePooling2D()(x0)\n    x2 = GlobalMaxPooling2D()(x0)\n    x = Concatenate()([x1,x2])\n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n#     x = Dense(128, activation='relu')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.3)(x)\n    \n    predictions = Dense(n_out, activation=ACTIVATION)(x)\n\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model","e2b4872f":"def create_model_mobile(n_out=NUM_CLASSES):\n\n    base_model =MobileNetV2(weights=None, include_top=False)\n    \n    x0 = base_model.output\n    x1 = GlobalAveragePooling2D()(x0)\n    x2 = GlobalMaxPooling2D()(x0)\n    x = Concatenate()([x1,x2])\n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n#     x = Dense(128, activation='relu')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.25)(x)\n\n    \n    predictions = Dense(n_out, activation=ACTIVATION)(x)\n\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model","a5fd0584":"def conv_simple_block(x, n_filters):\n    \n    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    \n    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = AveragePooling2D()(x)\n\n    return x\n\ndef create_model_simplecnn(n_out=NUM_CLASSES):\n    \n    inp = Input(shape=(128,128,3))\n#     inp = Input(shape=(None,None,3))\n    x = conv_simple_block(inp,64)\n    x = conv_simple_block(x,128)\n    x = conv_simple_block(x,256)\n    x = conv_simple_block(x,512)\n    \n    x1 = GlobalAveragePooling2D()(x)\n    x2 = GlobalMaxPooling2D()(x)\n    x = Add()([x1,x2])\n\n    x = Dropout(0.2)(x)\n\n    x = Dense(128, activation='linear')(x)\n    x = PReLU()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    predictions = Dense(n_out, activation=ACTIVATION)(x)\n\n    model = Model(inputs=inp, outputs=predictions)\n    return model","0c523616":"'''Choose your model here'''\nif MODEL == 'xception':\n    preprocess_input = preprocess_xception\n    model = create_model_xception(n_out=NUM_CLASSES)\nif MODEL == 'inception':\n    preprocess_input = preprocess_inception\n    model = create_model_inception(n_out=NUM_CLASSES)\nif MODEL == 'mobile':\n    preprocess_input = preprocess_mobile\n    model = create_model_mobile(n_out=NUM_CLASSES)\nelse:\n    preprocess_input = preprocess_mobile\n    model = create_model_simplecnn(\n    n_out=NUM_CLASSES)\n\nprint(MODEL)\nmodel.summary()","9628cb6a":"# If you want, you can try more advanced augmentation like this\naugment_img = iaa.Sequential([\n    iaa.SomeOf((0,3),[\n#         iaa.ContrastNormalization((0.9, 1.1)),\n#         iaa.Multiply((0.9, 1.1), per_channel=0.2),\n        iaa.Fliplr(0.5),\n        iaa.GaussianBlur(sigma=(0, 0.1)),\n        iaa.Affine( # x-shift\n            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.0, 0.0)},\n        ),\n        iaa.CoarseDropout(0.1,size_percent=0.05) # see examples : https:\/\/github.com\/aleju\/imgaug\n            ])], random_order=True)\n\n\n# Or you can choose this simplest augmentation (like pytorch version)\n# augment_img = iaa.Fliplr(0.5)\n\n# This is my ugly modification; sorry about that\nclass FATTrainDataset(Sequence):\n\n    def getitem(image):\n        # crop 2sec\n\n        base_dim, time_dim, _ = image.shape\n        crop = random.randint(0, time_dim - base_dim)\n        image = image[:,crop:crop+base_dim,:]\n\n        image = preprocess_input(image)\n        \n#         label = self.labels[idx]\n        return image\n    def create_generator(train_X, train_y, batch_size, shape, augument=False, shuffling=False, test_data=False):\n        assert shape[2] == 3\n        while True:\n            if shuffling:\n                train_X,train_y = shuffle(train_X,train_y)\n\n            for start in range(0, len(train_y), batch_size):\n                end = min(start + batch_size, len(train_y))\n                batch_images = []\n                X_train_batch = train_X[start:end]\n                if test_data == False:\n                    batch_labels = train_y[start:end]\n                \n                for i in range(len(X_train_batch)):\n                    image = FATTrainDataset.getitem(X_train_batch[i])   \n                    if augument:\n                        image = FATTrainDataset.augment(image)\n                    batch_images.append(image)\n                    \n                if test_data == False:\n                    yield np.array(batch_images, np.float32), batch_labels\n                else:\n                    yield np.array(batch_images, np.float32)\n        return image\n    \n    def augment(image):\n\n        image_aug = augment_img.augment_image(image)\n        return image_aug","7f301ba7":"from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n                             \nfrom sklearn.model_selection import train_test_split\n\ncheckpoint = ModelCheckpoint(checkpoint_file, monitor='val_tf_lwlrap', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = False)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_tf_lwlrap', factor=LR_FACTOR, patience=PATIENCE, \n                                   verbose=1, mode='max', min_delta=0.0001, cooldown=2, min_lr=1e-5 )\n\ncsv_logger = CSVLogger(filename='..\/working\/training_log.csv',\n                       separator=',',\n                       append=True)\n\n\n# split data into train, valid\nx_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=SEED)\n\n# create train and valid datagens\ntrain_generator = FATTrainDataset.create_generator(\n    x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=TRAIN_AUGMENT, shuffling=True)\nvalidation_generator = FATTrainDataset.create_generator(\n    x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)\n\ncallbacks_list = [checkpoint, csv_logger, reduceLROnPlat]","3befbe0f":"train_steps = np.ceil(float(len(x_trn)) \/ float(BATCH_SIZE))\nval_steps = np.ceil(float(len(x_val)) \/ float(BATCH_SIZE))\ntrain_steps = train_steps.astype(int)\nval_steps = val_steps.astype(int)\nprint(train_steps, val_steps)\nprint(len(x_trn), BATCH_SIZE)\n","fbce804e":"print(LOSS)\nif LOSS=='BCEwithLogits':\n     model.compile(loss=BCEwithLogits,\n            optimizer=Adam(lr=LR),\n            metrics=[tf_lwlrap,'categorical_accuracy'])\nelse:\n    model.compile(loss=LOSS,\n            optimizer=Adam(lr=LR),\n            metrics=[tf_lwlrap,'categorical_accuracy'])\n","00f8fd33":"\n\nhist = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=validation_generator,\n    validation_steps=val_steps,\n    epochs=EPOCHS,\n    verbose=1,\n    callbacks=callbacks_list)","8dd0ae32":"print(K.eval(model.optimizer.lr))","39d1c1cc":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\nax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('categorical_accuracy')\nax[1].plot(hist.epoch, hist.history[\"categorical_accuracy\"], label=\"Train categorical_accuracy\")\nax[1].plot(hist.epoch, hist.history[\"val_categorical_accuracy\"], label=\"Validation categorical_accuracy\")\nax[0].legend()\nax[1].legend()","6e664f34":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('tf_lwlrap')\nax[0].plot(hist.epoch, hist.history[\"tf_lwlrap\"], label=\"Train lwlrap\")\nax[0].plot(hist.epoch, hist.history[\"val_tf_lwlrap\"], label=\"Validation lwlrap\")\nax[1].set_title('categorical_accuracy')\nax[1].plot(hist.epoch, hist.history[\"categorical_accuracy\"], label=\"Train categorical_accuracy\")\nax[1].plot(hist.epoch, hist.history[\"val_categorical_accuracy\"], label=\"Validation categorical_accuracy\")\nax[0].legend()\nax[1].legend()","81da892f":"model.load_weights(checkpoint_file)","ceccb3e0":"validation_generator = FATTrainDataset.create_generator(\n    x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)\n\npred_val_y = model.predict_generator(validation_generator,steps=val_steps,verbose=1)\nfor ii in range(TTA):\n    validation_generator = FATTrainDataset.create_generator(\n        x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False)\n\n    pred_val_y += model.predict_generator(validation_generator,steps=val_steps,verbose=1)\n\n'''Since the score is based on ranking, we do not need to normalize the prediction'''\n# pred_val_y = pred_val_y\/10\n","bfe6ae95":"train_generator = FATTrainDataset.create_generator(\n    x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=True, shuffling=False)\npred_train_y = model.predict_generator(train_generator,steps=train_steps,verbose=1)","3117745e":"import sklearn.metrics\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n      truth[nonzero_weight_sample_indices, :] > 0, \n      scores[nonzero_weight_sample_indices, :], \n      sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap","7aad1ee8":"print(pred_val_y.shape, y_val.shape)\nprint(np.sum(pred_val_y), np.sum(y_val))\n# for ii in range(len(y_val)):\n#     print(np.sum(pred_val_y[ii]), np.sum(y_val[ii]))","66675d15":"print(\"lwlrap from sklearn.metrics for training data =\", calculate_overall_lwlrap_sklearn(y_trn, pred_train_y))\nprint(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(y_val, pred_val_y\/10))\n\nscore, weight = calculate_per_class_lwlrap(y_val, pred_val_y)\nlwlrap = (score * weight).sum()\nprint('direct calculation of lwlrap : %.4f' % (lwlrap))","b8459bc3":"test_steps = np.ceil(float(len(x_test)) \/ float(BATCH_SIZE)).astype(int)\n","786c669f":"test_generator = FATTrainDataset.create_generator(\n    x_test, x_test, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False, test_data=True)\npred_test_y = model.predict_generator(test_generator,steps=test_steps,verbose=1)\n\nfor ii in range(TTA):\n    test_generator = FATTrainDataset.create_generator(\n        x_test, x_test, BATCH_SIZE, (SIZE,SIZE,3), augument=False, shuffling=False, test_data=True)\n    pred_test_y += model.predict_generator(test_generator,steps=test_steps,verbose=1)","183eaa84":"sort_idx = np.argsort(labels).astype(int)\n","7a43bea7":"print(sort_idx)","8a2475a3":"sample_sub = pd.read_csv('..\/input\/freesound-audio-tagging-2019\/sample_submission.csv')\ntest_Y_sort = pred_test_y[:, sort_idx]\nsample_sub.iloc[:, 1:] =  test_Y_sort\nsample_sub.to_csv('submission.csv', index=False)\n\nsample_sub.head()","02d2b12c":"### model","c890df3c":"# Introduction \nThis is my effort to do a *minimum* `Keras` replication with comparable baseline to the great kernel of @mhiro2 https:\/\/www.kaggle.com\/mhiro2\/simple-2d-cnn-classifier-with-pytorch (and further improved by @peining), which in turns use the excellent pre-processed data of @daisukelab https:\/\/www.kaggle.com\/daisukelab\/creating-fat2019-preprocessed-data) -- Note that to inference to the private data in stage-2, you have to preprocess data yourself.\n\nOne change I made in a Keras version, instead of a simple conv net, I decide to use a pre-defined architectures [trained from scratch] `MobileNetV2`, `InceptionV3` and `Xception` where you can choose in the kernel. Also, many ideas borrow from a nice kernel of @voglinio https:\/\/www.kaggle.com\/voglinio\/keras-2d-model-5-fold-log-specgram-curated-only , I also borrow the SoftMax+BCE loss & TTA ideas from Giba's kernel (BTW, we all know Giba without having to mention his user :).\n\n**UPDATE in V.17 : I add a simple CNN almost exactly the same as the pytorch baseline**\n\nI apologize that my code is not at all clean; some of the `pytorch` code is still here albeit not used.\n\n## Major Updates\n* V1 [CV680, LB574]\n* V4 [CV66x, LB576]\n* V5 [] Add image augmentation module\n* V9 [CV679] Add lwlrap TF metric (credit @rio114 : https:\/\/www.kaggle.com\/rio114\/keras-cnn-with-lwlrap-evaluation )\n* V11 [] Employ list of augmentations mentioned in https:\/\/github.com\/sainathadapa\/kaggle-freesound-audio-tagging\/blob\/master\/approaches_all.md\n* V16 [] Add BCEwithLogits (use only with ACTIVATION = 'linear')\n* V17 add SimpleCNN similar to the pytorch baseline\n* V20 add Curated-Only, Train-augment options\n\n\n**with BCEwithLogits and SimpleCNN, now this kernel should almost comparable to the pytorch baseline**\n\n### Minor Updates\n* V15[CV662]\n","b98e3e6f":"# Calculate Validation Score using TTA\nNote that we have to initiate validation_generation everytime before doing a new prediction as `model.fit_generator` will mis-index examples at the end of epoch (and you will get random score)","1806712e":"## Predict Test Data with TTA","ca0877d4":"### dataset","f8003811":"### imports","b2b18bf4":"### utils","da801378":"### train"}}