{"cell_type":{"7e7e3b85":"code","c3e6e293":"code","62f1ad8e":"code","143f8e0c":"code","cbd2044d":"code","b15c74d8":"code","b16b76b1":"code","76c81938":"code","b0c0040f":"code","d85d5e0e":"code","0fc11623":"code","ea50880a":"code","a43b36db":"code","b6de0ef3":"code","ae6be81c":"code","57fa84f1":"code","f43ac09b":"code","46e64967":"code","ea6930a9":"code","00690aa3":"code","5565ca2e":"code","9e1a180c":"code","b57a49c8":"code","897c2963":"code","bffb2881":"code","1f315c6d":"code","d83bbb8e":"code","4e909739":"code","9b52202e":"code","67b38a55":"code","79e73a45":"code","d37717fd":"code","3bc23bb9":"code","93fb3a66":"code","bf002ca1":"code","67aeb973":"code","bfe57f65":"markdown","795e8d85":"markdown","b51e2851":"markdown","1467e7a3":"markdown","7840c145":"markdown","8e455c21":"markdown","f9f11f4e":"markdown","ecf381bf":"markdown","6fa4f0c6":"markdown","39f9fe3b":"markdown","26421bf1":"markdown","c50a337e":"markdown"},"source":{"7e7e3b85":"import pandas as pd\nimport numpy as np\nfrom io import StringIO\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","c3e6e293":"csv_data = '''A,B,C,D\n   1.0,2.0,3.0,4.0\n   5.0,6.0,,8.0\n   10.0,11.0,12.0,'''\ndf = pd.read_csv(StringIO(csv_data))","62f1ad8e":"df.isnull().sum()","143f8e0c":"df.values","cbd2044d":"df.dropna()\n#deleting rows with missing values","b15c74d8":"df.dropna(axis=1)","b16b76b1":"# only drop rows where all columns are NaN\ndf.dropna(how='all')\n   # drop rows that have not at least 4 non-NaN values\ndf.dropna(thresh=4)\n   # only drop rows where NaN appear in specific columns (here: 'C')\ndf.dropna(subset=['C'])","76c81938":"imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n#Other options for the strategy parameter are median or most_frequent\nimr.fit(df)\nimurated_data = imr.transform(df.values)\nimurated_data","b0c0040f":"df = pd.DataFrame([\n    ['green', 'M', 10.1, 'class1'],\n    ['red', 'L', 13.5, 'class2'],\n    ['blue', 'XL', 15.3, 'class1']])\ndf.columns = ['color', 'size', 'price', 'classlabel']\ndf","d85d5e0e":"# size_mapping = {\n#     'XL':3,\n#     'M':2,\n#     'L':1\n# }\n# df['size'] = df['size'].map(size_mapping)\nsize_mapping = {\n    'XL': 3,\n    'L': 2,\n    'M': 1}\ndf['size'] = df['size'].map(size_mapping)\ndf","0fc11623":"#Encoding Class Labels\nclass_mapping = {label:idx for idx,label in enumerate(np.unique(df['classlabel']))}\nclass_mapping\n","ea50880a":"df['classlabel'] = df['classlabel'].map(class_mapping)","a43b36db":"#for inversing\ninv_class_mapping = {v:k for k,v in class_mapping.items()}\ndf['classlabel'] = df['classlabel'].map(inv_class_mapping)\n","b6de0ef3":"#we can use label encoder class from scikit learn to achieve the same\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['classlabel'].values)","ae6be81c":"class_le.inverse_transform(y)","57fa84f1":"X = df[['color','size','price']].values\nX[:,0] = class_le.fit_transform(X[:,0])\nX","f43ac09b":"ohe = OneHotEncoder(categorical_features=[0], sparse = False)\nohe.fit_transform(X)","46e64967":"#ohe.fit_transform(df).toarray()\n#df['classlabel'] = df['classlabel'].map(y)\n#df['color'] = class_le.fit_transform(df['color'])","ea6930a9":"pd.get_dummies(df[['color','size','price']])","00690aa3":"df_wine = pd.read_csv(\"..\/input\/winedata\/wine.csv\")","5565ca2e":"df_wine.columns = ['Class label', 'Alcohol','Malic acid', 'Ash','Alcalinity of ash', 'Magnesium','Total phenols', 'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity', 'Hue','OD280\/OD315 of diluted wines','Proline']\ndf_wine.columns","9e1a180c":"np.unique(df_wine['Class label'])","b57a49c8":"X,y = df_wine.iloc[:,1:], df_wine.iloc[:,0]\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3,random_state=0)\n","897c2963":"mms = MinMaxScaler()\nX_train_norm = mms.fit_transform(X_train)\nX_test_norm = mms.transform(X_test)","bffb2881":"stdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\nX_train_std","1f315c6d":"#lets try the L1 regularization with LogistiRegression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty = 'l1', C=0.1)\nlr.fit(X_train_std, y_train)\nprint(lr.score(X_train_std,y_train))\nprint(lr.score(X_test_std,y_test))","d83bbb8e":"lr.intercept_\n#Since we the  t the LogisticRegression object on a multiclass dataset, it uses the One-vs-Rest (OvR) approach by default where the  rst intercept belongs to the model that  ts class 1 versus class 2 and 3; the second value is the intercept of the model that  ts class 2 versus class 1 and 3; and the third value is the intercept of the model that  ts class 3 versus class 1 and 2, respectively:","4e909739":"lr.coef_\n#The weight array that we accessed via the lr.coef_ attribute contains three rows of weight coef cients, one weight vector for each class. Each row consists of 13 weights where each weight is multiplied by the respective feature in the 13-dimensional Wine dataset to calculate the net input","9b52202e":"import matplotlib.pyplot as plt\nfig = plt.figure()\nax = plt.subplot(111)\ncolors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'pink', 'lightgreen', 'lightblue', 'gray', 'indigo', 'orange']\nweights,param = [], []\nfor c in np.arange(-4,6,dtype=float):\n    #print(c)\n#     po = 10**c\n#     print(po)\n    lr = LogisticRegression(penalty='l1', C=10**c,random_state=0)\n    lr.fit(X_train_std,y_train)\n    weights.append(lr.coef_[1])\n    param.append(10**c)\nweights = np.array(weights)\nfor column,color in zip(range(weights.shape[1]),colors):\n    plt.plot(param,weights[:,column], label=df_wine.columns[column+1],color=color)\n\nplt.axhline(0, color='black', linestyle='--', linewidth=3)\nplt.xlim([10**(-5), 10**5])\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.xscale('log')\nplt.legend(loc='upper left')\nax.legend(loc='upper center',bbox_to_anchor=(1.38, 1.03),ncol=1, fancybox=True)\nplt.show()\n    ","67b38a55":"from sklearn.base import clone\nfrom itertools import combinations\nimport numpy as np\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score","79e73a45":"class SBS():\n    \n    def __init__(self, estimator, k_features,\n        scoring=accuracy_score,\n        test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n    def fit(self, X, y):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)\n        self.scores_ = [score]\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n            \n            for p in combinations(self.indices_, r=dim-1):\n                \n                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n            self.scores_.append(scores[best])\n            self.k_score_ = self.scores_[-1]\n            return self\n    def transform(self, X):\n        return X[:, self.indices_]\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        \n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score","d37717fd":"#Now, let's see our SBS implementation in action using the KNN classi er from scikit-learn:\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nknn = KNeighborsClassifier(n_neighbors = 2)\nsbs = SBS(knn, k_features=1)\nsbs.fit(X_train_std,y_train)","3bc23bb9":"k_feat = [len(k) for k in sbs.subsets_]\nplt.plot(k_feat, sbs.scores_,marker='o')\nplt.ylim([0.7, 1.1])\nplt.grid()\nplt.show()","93fb3a66":"#k5 = list(sbs.subsets_[8])\nprint(sbs.subsets_)","bf002ca1":"from sklearn.ensemble import RandomForestClassifier\nfeat_label = df_wine.columns[1:]\nforest = RandomForestClassifier(n_estimators = 1000, random_state = 0, n_jobs=-1)\nforest.fit(X_train,y_train) #in the random forest we dont need to standardize the data\nimportances = forest.feature_importances_\nindeces = np.argsort(importances)[::-1]\nfor f in range(X_train.shape[1]):\n    print(feat_label[indeces[f]], importances[indeces[f]])","67aeb973":"plt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]),importances[indeces],color='blue',align='center')\nplt.xticks(range(X_train.shape[1]), feat_label[indeces], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\nplt.show()","bfe57f65":"# plot the regularization path, \nwhich is the weight coef cients of the different features for different regularization strengths:","795e8d85":"### Standardization","b51e2851":"# Sequential Backword Selection\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational ef ciency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization. A classic sequential feature selection algorithm is Sequential Backward Selection (SBS), which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the classi er to improve upon computational ef ciency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting.\n\nThe idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to de ne criterion function J that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classi er after and before the removal of a particular feature. Then the feature\nto be removed at each stage can simply be de ned as the feature that maximizes\nthis criterion; or, in more intuitive terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding de nition of SBS, we can outline the algorithm in 4 simple steps:\n1. Initialize the algorithm with k = d , where d is the dimensionality of the full feature space Xd .\n2. Determine the feature x\u2212 that maximizes the criterion x\u2212 = arg max J (Xk \u2212 x) where x\u2208Xk .\n3. Remove the feature x\u2212 from the feature set: X := X \u2212 x\u2212;k := k \u22121.\nk-1 k\n4. Terminate if k equals the number of desired features, if not, go to step 2.","1467e7a3":"## Accessing feature importance with Random Forest","7840c145":"## Sparse solution with L1 regularization","8e455c21":"## Handling categorical data\nWhen we are talking about categorical data, we have to further distinguish between nominal and ordinal features. Ordinal features can be understood as categorical values that can be sorted or ordered. For example, T-shirt size would be an ordinal feature, because we can de ne an order XL > L > M. In contrast, nominal features don't imply any order and, to continue with the previous example, we could think of T-shirt color as a nominal feature since it typically doesn't make sense to say that, for example, red is larger than blue.","f9f11f4e":"### Eliminating samples or features with missing values","ecf381bf":"### Normalization","6fa4f0c6":"If we notice that a model performs much better on a training dataset than on the test dataset, this observation is a strong indicator for over tting. Over tting means that model  ts the parameters too closely to the particular observations in the training dataset but does not generalize well to real data\u2014we say that the model has a high variance. A reason for over tting is that our model is too complex for the given training data and common solutions to reduce the generalization error are listed\nas follows:\n\u2022 Collect more training data\n\u2022 Introduce a penalty for complexity via regularization\n\u2022 Choose a simpler model with fewer parameters\n\u2022 Reduce the dimensionality of the data\n","39f9fe3b":"# Performing one-hot encoding on nominal features","26421bf1":"## Sequential feature selection algorithms\nThere are two main categories of dimensionality reduction techniques: feature selection and feature extraction\n### feature selection,\nwe select a subset of the original features. \n### feature extraction, \nwe derive information from the feature set to construct a new feature subspace","c50a337e":"### mean imputation\nwe simply replace the missing value by the mean value of the entire feature column\nA convenient way to achieve this is by using the Imputer class from scikit-learn"}}