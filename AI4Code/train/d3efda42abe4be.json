{"cell_type":{"9cd665be":"code","fa30612c":"code","dc4ce07f":"code","a7436933":"code","7ccd53ba":"code","a9072c63":"code","8d78ba7d":"code","f198bdc5":"code","2ab9c8a9":"code","54be98d0":"code","2791f92a":"code","09edea30":"code","390bfef2":"code","cc075193":"code","8b09de66":"code","7736abff":"code","e9e5c181":"code","eae32bc1":"code","c7797625":"code","542e3135":"code","9b4623e9":"code","92e4ee2b":"code","44717580":"code","a66ff615":"code","c5ea1949":"code","5243249f":"code","87c45de2":"code","3cac08b9":"code","c12cf7bb":"code","33cbc2ba":"code","a2e7e58a":"code","7ab5e447":"code","db7404f1":"code","7fe931d0":"code","f95622d7":"code","361d0208":"code","dbeec719":"code","31061e2e":"code","8969fcad":"code","2f68adc7":"code","34fbc4ef":"code","9cc69de5":"code","032cc783":"code","f767a224":"code","8b04d506":"code","4952e172":"code","34afe01e":"code","6b4ee696":"code","0f1355e8":"code","f167a022":"code","16ba23b5":"code","30c90d58":"code","e9921912":"code","1dda8815":"code","e63ae354":"code","49f99378":"code","a93e5173":"markdown","42a79584":"markdown","b840fa4e":"markdown","5b150ab1":"markdown","8de77b38":"markdown","a2894a50":"markdown","3a7f0165":"markdown","38dc8c07":"markdown","098291e6":"markdown","d2023b57":"markdown","fb99e177":"markdown","a4ab7ed2":"markdown","ea3a94de":"markdown","b16294fe":"markdown","c5b15721":"markdown","ed6fac2e":"markdown","82551567":"markdown","2c254913":"markdown","2be8c86c":"markdown","06e8dc51":"markdown","17514cf4":"markdown","c218f5a2":"markdown","3176a759":"markdown","e8b1c315":"markdown"},"source":{"9cd665be":"import pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport requests\nfrom geopy.geocoders import Nominatim\nimport folium\nimport re\nimport json\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\nfrom sklearn.cluster import KMeans\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\n\nimport matplotlib.colors as colors","fa30612c":"# address = 'New Delhi'\n# geolocator = Nominatim(user_agent=\"ny_explorer\")\n# location = geolocator.geocode(address)\n# latitude = location.latitude\n# longitude = location.longitude\n# print('The geograpical coordinate of Toronto are {}, {}.'.format(latitude, longitude))","dc4ce07f":"latitude = 28.6141793\nlongitude = 77.2022662","a7436933":"#accessing the web page by Http request made by requests library\n# req = requests.get(\"https:\/\/en.wikipedia.org\/wiki\/Neighbourhoods_of_Delhi\").text\n# soup = BeautifulSoup(req, 'lxml')\n# div = soup.find('div', class_=\"mw-parser-output\" )\n# print(\"web Page Imported\")\n","7ccd53ba":"# #Code to extract the relevent data from the request object using beautiful soup\n# data = pd.DataFrame(columns=['Borough','Neighborhood'])\n# i=-1\n# flag = False\n# no=0\n# prev_borough = None\n# for child in div.children:\n#     if child.name:\n#         span = child.find('span')\n#         if span!=-1 and span is not None:\n#             try:\n#                 if span['class'][0] == 'mw-headline' and child.a.text!='edit':\n#                     prev_borough = child.a.text\n#                     i+=1\n#                     flag = True\n#                     continue\n#             except KeyError:\n#                 continue\n#         if child.name=='ul' and flag==True:\n#             neighborhood = []\n#             for ch in child.children:\n                \n#                 try:\n#                     data.loc[no]=[prev_borough,ch.text]\n#                     no+=1\n#                 except AttributeError:\n#                     pass\n#         flag = False\n# data[50:60]","a9072c63":"# lat_lng = pd.DataFrame(columns=['latitude','longitude'])\n# geolocator = Nominatim(user_agent=\"ny_explorer\")\n# for i in range(184):\n#     address = data['Neighborhood'].loc[i]+',New Delhi'\n#     try: \n#         location = geolocator.geocode(address)\n#         lat_lng.loc[i]=[location.latitude,location.longitude]\n#     except AttributeError:\n#         continue","8d78ba7d":"# df1 = data\n# df2 = lat_lng\n# delhi_neighbourhood_data = pd.concat([df1, df2], axis=1)\n# delhi_neighbourhood_data.to_csv(r'E:\\jupyter\\Coursera Practice\\delhi_dataSet.csv')","f198bdc5":"delhi_neighborhood_data = pd.read_csv(r'..\/input\/delhi_dataSet.csv')\ndelhi_neighborhood_data.dropna(inplace=True)\ndelhi_neighborhood_data.reset_index(inplace=True)\ndelhi_neighborhood_data.drop(['index','Unnamed: 0'], axis=1, inplace=True)\ndelhi_neighborhood_data.head()","2ab9c8a9":"delhiData = delhi_neighborhood_data\nmap_delhi = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# add markers to map\nfor lat, lng, label in zip(delhiData['latitude'], delhiData['longitude'], delhiData['Neighborhood']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_delhi)  \n    \nmap_delhi","54be98d0":"# CLIENT_ID = 'PVEHZMCGQRW1UTUDAKHLC0RTRNC205YZ2NJDZDPPJOHQV5VH' # your Foursquare ID\n# CLIENT_SECRET = 'XYAYEPCDCHKUT44EMD25OADY1UADBPQZEGVYH0IJRDEWKW1Q' # your Foursquare Secret\n# VERSION = '20180605' # Foursquare API version\n# radius = 1000\n# LIMIT = 200\n\n# print('Credentails Registered')","2791f92a":"# def getNearbyVenues(names, latitudes, longitudes, radius=500):\n    \n#     venues_list=[]\n#     for name, lat, lng in zip(names, latitudes, longitudes):\n#         print(name)\n            \n#         # create the API request URL\n#         url = 'https:\/\/api.foursquare.com\/v2\/venues\/explore?&client_id={}&client_secret={}&v={}&categoryId={}&ll={},{}&radius={}&limit={}'.format(\n#             CLIENT_ID, \n#             CLIENT_SECRET, \n#             VERSION,\n#             '4d4b7105d754a06374d81259',\n#             lat, \n#             lng, \n#             radius, \n#             LIMIT)\n            \n#         # make the GET request\n#         try:\n#             results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n#         except KeyError:\n#             continue\n        \n#         # return only relevant information for each nearby venue\n#         venues_list.append([(\n#             name, \n#             lat, \n#             lng, \n#             v['venue']['name'], \n#             v['venue']['location']['lat'], \n#             v['venue']['location']['lng'],  \n#             v['venue']['categories'][0]['name']) for v in results])\n\n#     nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n#     nearby_venues.columns = ['Neighborhood', \n#                   'Neighborhood Latitude', \n#                   'Neighborhood Longitude', \n#                   'Venue', \n#                   'Venue Latitude', \n#                   'Venue Longitude', \n#                   'Venue Category']\n    \n#     return(nearby_venues)","09edea30":"# delhi_venues = getNearbyVenues(names=delhiData['Neighborhood'],\n#                                    latitudes=delhiData['latitude'],\n#                                    longitudes=delhiData['longitude']\n#                                   )","390bfef2":"delhi_venues = pd.read_csv(r'..\/input\/restaurant_dataSet.csv')","cc075193":"map_res = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# add markers to map\nfor lat, lng, label in zip(delhi_venues['Venue Latitude'], delhi_venues['Venue Longitude'], delhi_venues['Venue']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=2,\n        popup=label,\n        color='red',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_res)  \n    \nmap_res","8b09de66":"# one hot encoding\ndelhi_onehot = pd.get_dummies(delhi_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\ndelhi_onehot['Neighborhood'] = delhi_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nfixed_columns = [delhi_onehot.columns[-1]] + list(delhi_onehot.columns[:-1])\ndelhi_onehot = delhi_onehot[fixed_columns]\n\ndelhi_onehot.head()","7736abff":"delhi_onehot.shape","e9e5c181":"#To be used while Generating Graphs\ndelhi_grouped = delhi_onehot.groupby('Neighborhood').mean().reset_index()\ndelhi_grouped.head()","eae32bc1":"for i in delhi_grouped.columns:\n    print(i,end=\", \")","c7797625":"delhi_grouped.shape","542e3135":"def return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]","9b4623e9":"num_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n\n# create a new dataframe\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = delhi_grouped['Neighborhood']\n\nfor ind in np.arange(delhi_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(delhi_grouped.iloc[ind, :], num_top_venues)\n\nneighborhoods_venues_sorted.head()","92e4ee2b":"# set number of clusters\nkclusters = 5\n\ndelhi_grouped_clustering = delhi_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(delhi_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]","44717580":"# add clustering labels\nneighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n\ndelhi_merged = delhiData\n\n# merge toronto_grouped with toronto_data to add latitude\/longitude for each neighborhood\ndelhi_merged = delhi_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n\ndelhi_merged.dropna(inplace=True)\ndelhi_merged.head() # check the last columns!","a66ff615":"# create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(delhi_merged['latitude'], delhi_merged['longitude'], delhi_merged['Neighborhood'], delhi_merged['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[int(cluster)-1],\n        fill=True,\n        fill_color=rainbow[int(cluster)-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters","c5ea1949":"clusterdata = pd.merge(delhi_onehot.groupby('Neighborhood').sum(),delhi_merged[['Neighborhood','Cluster Labels']],left_on='Neighborhood', right_on='Neighborhood',how='inner')\nclusterdata = clusterdata.iloc[:,1:].groupby('Cluster Labels').sum().transpose()\nclusterdata.head()","5243249f":"import seaborn as sns","87c45de2":"def plot_bar(clusternumber):\n    sns.set(style=\"whitegrid\",rc={'figure.figsize':(20,10)})\n    df = clusterdata[[clusternumber]].drop(clusterdata[[clusternumber]][clusterdata[clusternumber]==0].index)\n    chart = sns.barplot(x=df.index, y=clusternumber, data=df)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)","3cac08b9":"plot_bar(0)","c12cf7bb":"plot_bar(1)","33cbc2ba":"plot_bar(2)","a2e7e58a":"plot_bar(3)","7ab5e447":"plot_bar(4)","db7404f1":"delhi_venues.drop('Unnamed: 0',axis=1,inplace=True)","7fe931d0":"forheatmap=delhi_venues.copy()\nforheatmap=pd.merge(forheatmap,delhi_merged[['Neighborhood','Cluster Labels']],left_on='Neighborhood', right_on='Neighborhood',how='inner')\nforheatmap.drop(forheatmap[~forheatmap['Cluster Labels'].isin([1,2])].index, inplace=True)","f95622d7":"forheatmap.head()","361d0208":"from folium.plugins import HeatMap","dbeec719":"#heat map of all restaurants in selected Neighborhoods\nres_heat = folium.Map(location=[latitude, longitude], zoom_start=11)\nHeatMap(list(zip(forheatmap['Venue Latitude'],forheatmap['Venue Longitude'])),\n        min_opacity=0.2,\n        radius=10, blur=15,\n        max_zoom=1\n       ).add_to(res_heat)\nfor lat, lng, label in zip(forheatmap['Neighborhood Latitude'], forheatmap['Neighborhood Longitude'], forheatmap['Neighborhood']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=2,\n        popup=label,\n        color='red',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(res_heat)\nres_heat","31061e2e":"forindres = forheatmap[forheatmap['Venue Category']=='Indian Restaurant']\n\n# heat map for Indian Restaurants in the selected Neighborhoods\nres_heat_ind = folium.Map(location=[latitude, longitude], zoom_start=11)\nHeatMap(list(zip(forindres['Venue Latitude'],forindres['Venue Longitude'])),\n        min_opacity=0.2,\n        radius=10, blur=15,\n        max_zoom=1\n       ).add_to(res_heat_ind)\nfor lat, lng, label in zip(forindres['Neighborhood Latitude'], forindres['Neighborhood Longitude'], forindres['Neighborhood']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=2,\n        popup=label,\n        color='red',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(res_heat_ind)\nres_heat_ind","8969fcad":"count_all = forheatmap[['Neighborhood','Venue']].groupby('Neighborhood').count().sort_values(by='Venue')\ntarget_count = int(0.6*len(count_all))\nprint(count_all.iloc[target_count])\ncount_all.drop(count_all[count_all.Venue.values>7].index,inplace=True)\ncount_all.columns=['all count']\ncount_all.head()","2f68adc7":"count_ind = forheatmap[forheatmap['Venue Category']==\"Indian Restaurant\"][['Neighborhood','Venue']].groupby('Neighborhood').count().sort_values(by='Venue')\ntarget_count = int(0.3*len(count_ind))\nprint(count_ind.iloc[target_count])\ncount_ind.drop(count_ind[count_ind.Venue.values>1].index,inplace=True)\ncount_ind.columns = ['ind count']\ncount_ind.head()","34fbc4ef":"lowdensity = count_all.join(count_ind)\nlowdensity.index.values","9cc69de5":"temp_recommend = delhiData.copy()\ntemp_recommend.drop(temp_recommend[~temp_recommend['Neighborhood'].isin(lowdensity.index.values)].index, inplace=True)\ntemp_recommend.head()","032cc783":"#most popular neighborhoods\ntop_nei = delhi_venues[['Neighborhood','Venue']].groupby('Neighborhood').count().sort_values(by='Venue', ascending=False).head(3).index.values\ntop_nei","f767a224":"toplatlng = delhiData[['Neighborhood','latitude','longitude']][delhiData['Neighborhood'].isin(top_nei)].reset_index()\ntoplatlng","8b04d506":"from math import sin, cos, sqrt, atan2, radians\n\ndef distanceInKM(la1,lo1,la2,lo2):\n    # approximate radius of earth in km\n    R = 6373.0\n    \n    lat1 = radians(la1)\n    lon1 = radians(lo1)\n    lat2 = radians(la2)\n    lon2 = radians(lo2)\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    dis = R * c\n    return round(dis,4)\n\nprint(\"Result:\", distanceInKM(toplatlng.iloc[2]['latitude'],toplatlng.iloc[2]['longitude'],toplatlng.iloc[0]['latitude'],toplatlng.iloc[0]['longitude']))","4952e172":"temp_recommend.reset_index(inplace=True)","34afe01e":"temp_recommend.drop(columns=['index','Borough'], inplace=True)","6b4ee696":"temp_recommend.head()","0f1355e8":"for i in toplatlng.index:\n    temp_recommend[toplatlng.iloc[i]['Neighborhood']] = temp_recommend.apply(lambda x : distanceInKM(toplatlng.iloc[i]['latitude'],toplatlng.iloc[i]['longitude'],x['latitude'],x['longitude']),axis=1)","f167a022":"temp_recommend.head()","16ba23b5":"# top 5 neighborhoods near Connaught Place\nneiNearCP = temp_recommend.sort_values(by=['Connaught Place']).iloc[:,:3].head().set_index('Neighborhood')\nneiNearCP","30c90d58":"# top 5 neighborhoods near Hauz Khas Village\nneiNearHK = temp_recommend.sort_values(by=['Hauz Khas Village']).iloc[:,:3].head().set_index('Neighborhood')\nneiNearHK","e9921912":"# top 5 neighborhoods near Khirki Village\nneiNearKV = temp_recommend.sort_values(by=['Khirki Village']).iloc[:,:3].head().set_index('Neighborhood')\nneiNearKV","1dda8815":"final_recommend=neiNearCP.append(neiNearHK).append(neiNearKV).reset_index()\nfinal_recommend.drop_duplicates(inplace=True)\nfinal_recommend.reset_index(inplace=True)\nfinal_recommend.drop(columns=['index'],inplace=True)\nfinal_recommend","e63ae354":"final = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# add markers to map\nfor lat, lng, label in zip(final_recommend['latitude'], final_recommend['longitude'], final_recommend['Neighborhood']):\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(final)  \n    \nfinal","49f99378":"import pandas as pd\ndelhi_dataSet = pd.read_csv(\"..\/input\/delhi_dataSet.csv\")\nrestaurant_dataSet = pd.read_csv(\"..\/input\/restaurant_dataSet.csv\")","a93e5173":"### Summary\n* We have, as a result, generated to data Sets.\n* The first was the data set(delhi_data) that contained the borough, name, Latitude and Longitude of all the major Neighborhoods of Delhi\n* And, the second data set(delhi_venues) contained the geographical information pertinent to all the major restaurants in delhi\n<br><br>","42a79584":"### Normalization of the data for clustering","b840fa4e":"Based on definition of our problem, factors that will influence our decission are:\n* number of existing restaurants in the neighborhood (any type of restaurant)\n* number of and distance to Indian restaurants in the neighborhood, if any\n* distance of neighborhood from popular neighborhoods\n\nIn our project we will:\n* acquire the names and boroughs of the neighborhoods by scrapping a wikipedia page.\n* After we have got the names of all the neighborhoods, we will geocode them using the library geopy.geocoder (Nominatim).\n* Next, we use the foursquare API to find all types of restaurants within a 1000 meter radius for every neighborhood.\n\n","5b150ab1":"<h2>Clustering and Analysis<\/h2>\n<a name='analysis'><\/a>\n<br>\nOur goal here is to find the neighborhoods with low density of Indian restaurants. But, how will we decide which neighborhoods, currently operating on minimal number of Indian restaurants, have the potential for growth and which neighborhoods do not.\n<br><br>\nThe most intuitive idea would be to find neighborhoods that have similar patterns of restaurant trends.\n<br><br>\nThis can be achived by clustering the neighborhoods of the basis of the restaurant data we have acquired. Clustering is a predominant algorithm of unsupervised Machine Learning. It is used to segregate data entries in cluster depending of the similarity of their attributes, calculated by using the simple formula of euclidian distance.\n<br><br>\nWe can then analyze these clusters separately and use those clusters that show high trends of Indian Restaurants","8de77b38":"### Cluster Visualization","a2894a50":"## Table of contents\n* [Introduction: Business Problem](#introduction)\n* [Data Acquisition and preperation](#data)\n* [Clustering and Analysis](#analysis)\n* [Recommendation](#recommendation)\n* [Results and Discussion](#results)","3a7f0165":"Our Analysis was done on over 186 neighborhoods, containing over 848 restaurants within 2km radius of every neighborhood. We segragated these neighborhoods on the basis of types and amounts of restaurants. Five clusters were obtained, each having a unique collection of restaurants. Since, we were focused on finding optimal neighborhoods for opening Indian restaurants, we selected cluster 2 and 3 which had the highest number of Indian restaurants. The above actions left us with the only those neighborhoods that had a shared characteristics of and that had a high demand for Indian restaurants.\n<br><br>\nNext, we plotted a heat map for analysing the density of restaurants in the remaining neighborhoods. This allowed us to select neighborhoods that had few or no Indian restaurants and were not overcrowded by other kinds of restaurants. A total of 57 neighborhoods were left. After this, we found out the top three most popular neighborhoods(namely: Canaught Place, Hauz khas Village and Khirki Village), and the distance of every remaining neighborhoods from all three of them. Then, we extracted top 5 closest neighborhoods from each of three most popular neighborhoods mentioned above. Taking the union of the resulting three dataset we get 11 neighborhoods that satisfy all three conditons layed out in the business problem by the customer.\n<br><br>\nThe neighborhoods recommendation obtained here are not completely accurate. This is due to the limitations in the dataset used in the project. Due to lack of cross referencing sources, we may have missed a few neighborhoods from our consideration. The foursquare API does not contain, or does not rely, a comprehensive dataset about the restaurants present in delhi. Surely, in a city like Delhi with a population of over 19 million, there are much more restaurants than 848.","38dc8c07":"### Visualing the obtained data set","098291e6":"## Results and Discussions\n<a name='results'><\/a>","d2023b57":"## Recommendation\n<a name='recommendation'><\/a>","fb99e177":"Analysing the bar graphs we can clearly see that <b>clusters 1 and 2<\/b> have a high demand for Indian Restaurants","a4ab7ed2":"Now, we will add the last constraint i.e the neighborhood should be close to popular neighborhoods","ea3a94de":"now we will remove all neighborhoods with the following conditions:\n* Number of Indian restaurants >30%\n* Number of all restaurants >60% \n<br>\n\n'%' here refers to percentile","b16294fe":"### Analyzing the Clusters","c5b15721":"## Final Recommendation","ed6fac2e":"### In this section:\n* we will, first, analyze the density of the Indian Restaurants in generally for each neighborhood.\n* Then we will weed out the neighborhoods that in the highest 70 percentile of density\n* Find out the most popular neighborhoods\n* Will then try to find remaining neighborhoods that are close to them\n* Provide the a detailed recommendation of top 10 neighborhoods\n\n\n<br>\nNow, as clusters 1 and 2 have a maximum number of Indian Restaurants, we will focus our search on neighborhoods within these two clusters.\n\n### Why?\nWe know that when we were clustering the neighborhoods the data used contained the mean of all types of restaurants present in the particular neighborhood. Therefore, we can say that the neighborhoods are clustered on their restaurant trends.<br>\n<br>\nNow, clusters 2 and 3 may collectively have the highest number of indian restaurant but there will be some neighborhoods in these clusters which would have a demand for Indian Restaurants, as these neighborhoods are in the same cluster, but would not have enough supply.\n","82551567":"### geocoding every neighborhood","2c254913":"<h2>Data Acquisition and preperation<\/h2><a name='data'><\/a>","2be8c86c":"\n<center> <H1> Restaurant Location Recommendation Using Neighborhood Clustering <\/H1><\/center>","06e8dc51":"<h3> Foursquare <\/h3>\n\n'The Foursquare Places API provides location based experiences with diverse information about venues, users, photos, and check-ins. The API supports real time access to places, Snap-to-Place that assigns users to specific locations, and Geo-tag.'(wikipedia)\n\nHere we are using the explore api call and filtering the search only to find venues that are identified as restaurants.","17514cf4":"### Scrapping the Wikipedia Page\n\n\nThis section is commented out due to geopy api not working. The code is just fine otherwise. The dataset used in this notebook was made by this code","c218f5a2":"### Applying the clustering algorithm","3176a759":"<h3>Introduction: Business Problem<\/h3><a name='introduction'><\/a>\n<br>\nIn ths project will try to find an optimal location for a restaurant. Specifically, this report will be targeted to stakeholders interested in opening an Indian Cusine restaurant in Delhi, India. Finding a suitable location for restaurants in major cities like delhi proves to be a daunting task. Various factors such as over-saturation or no demand ,for the type of restaurant that the customer wants to open, effect the success or failure of the restaurant. Hence, customers can bolster their decisions using the descriptive and predictive capabilites of data science.\n<br><br>\nWe need to find locations(Neighborhood) that have a <b>potentially unfulfilled demand<\/b> for Indian Restaurant. Also, we need locations that have <b>low competition and are not already crowded<\/b>. We would also prefer location as close to popular city Neighborhood, assuming the first two conditions are met.\n<br><br>\nWe will use our data science powers to generate a few most promissing neighborhoods based on this criteira. Advantages of each area will then be clearly Expressed so that best possible final location can be chosen by stakeholders.\n<br>\n<br>\n<br>","e8b1c315":"Now that we have obtained the location of every neighborhood we can use the foursquare API to find the location of nearby restaurant"}}