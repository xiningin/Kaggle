{"cell_type":{"d96d5e37":"code","b119ee2c":"code","ca023872":"code","8633206b":"code","22acb0ae":"code","40367340":"code","96ea65fb":"code","6095adfe":"code","cb1b8b2b":"code","67fd27c8":"code","7e6dab8f":"code","d8a23b97":"code","98566e7a":"code","e147528e":"code","c1a1a36a":"code","878bc9c9":"code","02f165b0":"code","0df8d7e9":"code","1d0b0c9c":"code","4f791e40":"code","fcdc2447":"code","2b5ae577":"code","211b110f":"code","4fe4963e":"code","d92023ed":"code","ff9a4cd8":"code","56b26781":"code","adda88a0":"code","5d81f27c":"code","de5355b2":"code","780d1791":"code","a734626c":"markdown","2ea6ced7":"markdown","e9af89f0":"markdown","6e53ad92":"markdown"},"source":{"d96d5e37":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline","b119ee2c":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(10)","ca023872":"test.head(10)","8633206b":"len(tweet)","22acb0ae":"dedup = tweet.drop_duplicates('text')\nprint(f'{len(tweet) - len(dedup)} duplicate rows removed')","40367340":"fakes = dedup[dedup['target']==0]\nreals = dedup[dedup['target']==1]","96ea65fb":"fig, ax = plt.subplots(1,1,figsize=(5,5))\n\nax.bar('Fake events', len(fakes)) \nax.bar('Real events', len(reals)) \n\nax.set_title('Original Real \/ Fake distribution')\n\nplt.show()","6095adfe":"#from sklearn.model_selection import train_test_split\n#train, test = train_test_split(dedup, test_size=0.25, random_state=42)","cb1b8b2b":"#train_reals = train[train['target']==1]\n#train_fakes = train[train['target']==0]","67fd27c8":"#train_reals.head(10)","7e6dab8f":"import re\nimport string\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n","d8a23b97":"df=pd.concat([tweet,test])\ndf.shape","98566e7a":"#REMOVE URL\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n#train_reals['text']=train_reals['text'].apply(lambda x : remove_URL(x))\n#train_fakes['text']=train_fakes['text'].apply(lambda x : remove_URL(x))\n\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\n\n#print(train_fakes['text'][3230])\n#Providence Health &amp; Services: Emergency Services Supervisor - Emergency Department... (#Kodiak AK) http:\/\/t.co\/AQcSUSqbDy #Healthcare #Job","e147528e":"#REMOVE HTML TAG\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n#train_reals['text']=train_reals['text'].apply(lambda x : remove_html(x))\n#train_fakes['text']=train_fakes['text'].apply(lambda x : remove_html(x))\n\ndf['text']=df['text'].apply(lambda x : remove_html(x))","c1a1a36a":"#REMOVE EMOJI\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n#train_reals['text']=train_reals['text'].apply(lambda x: remove_emoji(x))\n#train_fakes['text']=train_fakes['text'].apply(lambda x: remove_emoji(x))\n\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))","878bc9c9":"#REMOVE PUNCTUATION\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#train_reals['text']=train_reals['text'].apply(lambda x : remove_punct(x))\n#train_fakes['text']=train_fakes['text'].apply(lambda x : remove_punct(x))\n\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","02f165b0":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus=create_corpus(df)","0df8d7e9":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","1d0b0c9c":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","4f791e40":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","fcdc2447":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","2b5ae577":"model=Sequential()\n\nembedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","211b110f":"model.summary()","4fe4963e":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","d92023ed":"print(test)","ff9a4cd8":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","56b26781":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","adda88a0":"history=model.fit(X_train,y_train,batch_size=4,epochs=20,validation_data=(X_test,y_test),verbose=2)","5d81f27c":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","de5355b2":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","780d1791":"sub.head()","a734626c":"**GloVe for Vectorization\u00b6**\n\nHere we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","2ea6ced7":"### Separate the labels and visualize the distribution","e9af89f0":"**DATA CLEANING**","6e53ad92":"* ### Remove duplicate rows (i.e. same tweet content)"}}