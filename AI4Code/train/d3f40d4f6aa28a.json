{"cell_type":{"c3a78cc2":"code","d63764b9":"code","f928f86f":"code","d4d62918":"code","10f7f27b":"code","0662413c":"code","09bde399":"code","c9ea5309":"code","d85cb6b0":"code","8ea13c2c":"code","07e2a480":"code","15b62e8e":"markdown","e98ac308":"markdown","8fe728e2":"markdown","ca3cce4b":"markdown","6631c828":"markdown"},"source":{"c3a78cc2":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\n\n# Skew the distribution a bit with a constantly growing trend. \n# For the little number, the data is still stationary according to the Dickey-Fuller test\nHB_GROW = 1\n# number of steps that model considers. \n# try to change this number to 5 and see the difference :)\nAR_ORDER = 50\n\ndata = np.random.rand(10000) ** 2\nseason1 = np.sin(np.arange(0, 10000 \/ 5, 0.2))\nseason2 = np.cos(np.arange(0, 10000 \/ 10, 0.1))\nnoise = np.random.randn(10000,)\nstationary_data = data + season1 + season2 + noise\ndata = stationary_data + np.linspace(0, HB_GROW, 10000)","d63764b9":"stats = adfuller(data)\nassert stats[1] < 0.05, \\\n       \"The data is not stationary, please decrease the HB_GROW\" # still stationary ?","f928f86f":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\nax1.title.set_text(\"Whole range\")\nax1.plot(data[:2000])\nax2.title.set_text(\"First 300 points\")\nax2.plot(data[:300]);","d4d62918":"# Autoregression order\np = AR_ORDER\ntrain = []\n# Max index should have been increased by 1 (len(data) - p + 1), \n# but last slice would never have the target value to predict.\n# So i just skipped it on the start\nmax_start_index = len(data) - p\nfor li in range(0, max_start_index):\n    sl = data[li:li+p]\n    train.append(sl)\n\ntrain = np.array(train)\nprint(f\"Train set consists of {train.shape[0]} examples {train.shape[1]} features each one\")\n\ntargets = data[p:]\n\nassert train.shape == (10000 - p, p)\nassert targets.shape == (10000 - p,)\n\n\n# Now we have smth like this (but with order 10, not 5)\n# train_seq   target\n# 1 2 3 4 5   6\n# 2 3 4 5 6   7\n# 3 4 5 6 7   8\n# Ensure that first target is the same as the last element in second training example \n# or the second from the end in second example, etc. (because slice is shifting repeatedly by 1)\nassert targets[0] == train[1, -1]\nassert targets[0] == train[2, -2]\nassert targets[0] == train[3, -3]","10f7f27b":"TRAIN_SET_SLICE = slice(None, 10000)\nVAL_SET_SLICE = slice(6000, 8000)\nTEST_SET_SLICE = slice(8000, 10000 - p)\nX_train, y_train = train[TRAIN_SET_SLICE, :], targets[TRAIN_SET_SLICE]\nX_val, y_val = train[VAL_SET_SLICE, :], targets[VAL_SET_SLICE]\nX_test, y_test = train[TEST_SET_SLICE, :], targets[TEST_SET_SLICE]\n\n# a bit of tests never hurts\nassert X_val.shape == (2000, p)\nassert y_test.shape == (2000 - p,)\nassert all(y_val == targets[6000:8000])","0662413c":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","09bde399":"inputs = Input(shape=(p,))\noutputs = Dense(1)(inputs)\nmodel = Model(inputs, outputs)\nopt = SGD(learning_rate=1e-3)\nmodel.compile(optimizer=opt, loss=\"mse\")\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(patience=10, factor=0.3, min_lr=1e-10)\nearly_stop = EarlyStopping(patience=20)","c9ea5309":"r = model.fit(X_train, \n              y_train, \n              validation_data=(X_val, y_val),\n              batch_size=32,\n              epochs=500, \n              verbose=False, \n              shuffle=True, callbacks=[reduce_lr, early_stop])","d85cb6b0":"plt.plot(r.history[\"loss\"], color=\"blue\")\nplt.plot(r.history[\"val_loss\"], color=\"orange\");","8ea13c2c":"model.evaluate(X_test, y_test)","07e2a480":"test_pred = model.predict(X_test)\nplt.plot(y_test[:100])\nplt.plot(test_pred[:100], color=\"green\");","15b62e8e":"<h1 style=\"text-align:center;color:brown;\">Univariate Time Series Autoregression<\/h1>\n\n<img src=\"https:\/\/idoraquel.s3.eu-central-1.amazonaws.com\/userimages\/Drawing-3.sketchpad.png\" \/>","e98ac308":"<h2 style=\"text-align:center;color:brown;\">\n    AR Model\n<\/h2>\n<p>In order to make it more intuitive, i won't import any blackbox models from package (although they are great :))<\/p>\n<p>Instead, we will construct it using simple Dense layer from the keras (tensorflow)<\/p>","8fe728e2":"<h2 style=\"text-align:center;color:brown;\">The goal of this notebook<\/h2>\n<p>\nThis notebook is for <b>beginners<\/b>. My main goal is to show how the time (or any other sequence) feature can be used in datasets.\n<\/p>\n\n<p>\nYou probably already know how the linear regression works.\n<\/p>\n\n<p><i><u>Linear regression:<\/u><\/i> Given number of features, you are updating the weights in order to minimize the error.<\/p>\n<p><i><u>Autoregression of order N:<\/u><\/i> Given sequence of target values over some time, we are modelling the same linear regression. But features now are the target values for the previous N time steps.<\/p> ","ca3cce4b":"Please have a look at the orange curve (validation loss over epochs). It is so \"nervous\" because the data is **not clearly stationary**.\n\nIn other words, the model can't converge because of amount of noise\n\nConsider *decreasing* HB_GROW to 0.1 or remove this line\n> \\+ np.linspace(0, HB_GROW, 10000)","6631c828":"<h2 style=\"text-align:center;color:brown;\">Validation &amp; test sets<\/h2>\n\n<p><i><u style=\"color:red\">Important:<\/u><\/i> When it comes to splitting the dataset, we always should consider the time. Namely, evaluate the data on the <b>only unseen<\/b> data. \nIn other words, in the future<\/p>"}}