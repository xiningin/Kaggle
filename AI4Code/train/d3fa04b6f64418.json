{"cell_type":{"6051c550":"code","3d94ee60":"code","239c1b9b":"code","83225a39":"code","b6c2d48d":"code","3b6de7af":"code","71842f8a":"code","c994efdb":"code","b6c37c06":"code","bc2c674c":"code","fbdd7bf1":"code","3a19b3ac":"code","4a91ab9c":"code","234b0c31":"code","143a46e0":"code","8bdc6bf4":"code","ab2697d2":"code","0d98cb46":"code","d4c8f3d4":"markdown","1318faf8":"markdown","5edcca87":"markdown","0aa95501":"markdown","fbb609ed":"markdown","911b46f8":"markdown","8f04e765":"markdown","d32b0a0a":"markdown","afc08f43":"markdown","fe027567":"markdown","2bbb3a3f":"markdown","320416bf":"markdown","d81a6d9a":"markdown","d5d67530":"markdown"},"source":{"6051c550":"# Importing necessary libraries\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport time\n","3d94ee60":"# Opening file, reading, eliminating whitespaces, and splitting by '\\n', which in turn creates list\nlabels = open('..\/input\/yolo-coco-data\/coco.names').read().strip().split('\\n')  # list of names\n\n# # Check point\n# print(labels)\n","239c1b9b":"# Defining paths to the weights and configuration file with model of Neural Network\nweights_path = '..\/input\/yolo-coco-data\/yolov3.weights'\nconfiguration_path = '..\/input\/yolo-coco-data\/yolov3.cfg'\n\n# Setting minimum probability to eliminate weak predictions\nprobability_minimum = 0.5\n\n# Setting threshold for non maximum suppression\nthreshold = 0.3\n","83225a39":"network = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)\n\n# Getting names of all layers\nlayers_names_all = network.getLayerNames()  # list of layers' names\n\n# # Check point\n# print(layers_names_all)\n","b6c2d48d":"# Getting only output layers' names that we need from YOLO algorithm\nlayers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names\n\n# Check point\nprint(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n","3b6de7af":"# Our image initially is in RGB format\n# But now we open it in BGR format as function 'cv2.imread' opens it so\nimage_input = cv2.imread('..\/input\/images-for-testing\/cat2.jpg')\n\n# Getting image shape\nimage_input_shape = image_input.shape\n\n# Check point\nprint(image_input_shape)  # tuple of (917, 1222, 3)\n","71842f8a":"# Showing RGB image but firstly converting it from BGR format\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()\n","c994efdb":"# The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob\n# from input image after mean subtraction, normalizing, and RB channels swapping\n# Resulted shape has number of images, number of channels, width and height\n# E.G.: blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n# Link: https:\/\/www.pyimagesearch.com\/2017\/11\/06\/deep-learning-opencvs-blobfromimage-works\/\nblob = cv2.dnn.blobFromImage(image_input, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n\n# Check point\nprint(image_input.shape)  # (917, 1222, 3)\nprint(blob.shape)  # (1, 3, 416, 416)\n","b6c37c06":"# Check point\n# Slicing blob and transposing to make channels come at the end\nblob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\nprint(blob_to_show.shape)  # (416, 416, 3)\n\n# Showing 'blob_to_show'\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 5.0)\nplt.imshow(blob_to_show)\nplt.show()","bc2c674c":"# Calculating at the same time, needed time for forward pass\nnetwork.setInput(blob)  # setting blob as input to the network\nstart = time.time()\noutput_from_network = network.forward(layers_names_output)\nend = time.time()\n\n# Showing spent time for forward pass\nprint('YOLO v3 took {:.5f} seconds'.format(end - start))\n","fbdd7bf1":"# Check point\nprint(type(output_from_network))  # <class 'list'>\nprint(type(output_from_network[0]))  # <class 'numpy.ndarray'>","3a19b3ac":"# Seed the generator - every time we run the code it will generate by the same rules\n# In this way we can keep specific colour the same for every class\nnp.random.seed(42)\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\n# Check point\nprint(colours.shape)  # (80, 3)\nprint(colours[0])  # [102 220 225]","4a91ab9c":"# Preparing lists for detected bounding boxes, obtained confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []","234b0c31":"# Getting spacial dimension of input image\nh, w = image_input_shape[:2]  # Slicing from tuple only first two elements\n\n# Check point\nprint(h, w)  # 917 1222","143a46e0":"for result in output_from_network:\n    # Going through all detections from current output layer\n    for detection in result:\n        # Getting class for current object\n        scores = detection[5:]\n        class_current = np.argmax(scores)\n\n        # Getting confidence (probability) for current object\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current > probability_minimum:\n            # Scaling bounding box coordinates to the initial image size\n            # YOLO data format keeps center of detected box and its width and height\n            # That is why we can just elementwise multiply them to the width and height of the image\n            box_current = detection[0:4] * np.array([w, h, w, h])\n\n            # From current box with YOLO format getting top left corner coordinates\n            # that are x_min and y_min\n            x_center, y_center, box_width, box_height = box_current.astype('int')\n            x_min = int(x_center - (box_width \/ 2))\n            y_min = int(y_center - (box_height \/ 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)","8bdc6bf4":"# It is needed to make sure the data type of the boxes is 'int'\n# and the type of the confidences is 'float'\n# https:\/\/github.com\/opencv\/opencv\/issues\/12789\nresults = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n# Check point\n# Showing labels of the detected objects\nfor i in range(len(class_numbers)):\n    print(labels[int(class_numbers[i])])\n\n# Saving found labels\nwith open('found_labels.txt', 'w') as f:\n    for i in range(len(class_numbers)):\n        f.write(labels[int(class_numbers[i])])\n","ab2697d2":"# Checking if there is at least one detected object\nif len(results) > 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Getting current bounding box coordinates\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n        # Preparing colour for current bounding box\n        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n\n        # Drawing bounding box on the original image\n        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n                      colour_box_current, 5)\n\n        # Preparing text with label and confidence for current bounding box\n        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n\n        # Putting text with label and confidence on the original image\n        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.5, colour_box_current, 5)","0d98cb46":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()\n","d4c8f3d4":"# \ud83d\uddbc\ufe0f Showing RGB image with bounding boxes and labels","1318faf8":"# \ud83c\udf93 Related course for classification tasks","5edcca87":"# \ud83d\udd02 Implementing non maximum suppression of given boxes and corresponding scores","0aa95501":"# \u27bf Going through all output layers after feed forward and answer from network","fbb609ed":"# \ud83d\uddc0 Loading input image from file","911b46f8":"# \u27b0 Implementing forward pass with our blob and only through output layers","8f04e765":"# \ud83d\uddc2\ufe0f Loading trained YOLO Objects Detector with the help of 'dnn' library from OpenCV","d32b0a0a":"# \ud83d\udcc2 Loading COCO class labels from file","afc08f43":"# \ud83e\udded Testing YOLO v3 - Objects Detection Algorithm\n* Using **'dnn'** OpenCV library for loading model of Neural Network from configuration file.\n\n* Using weights trained on COCO dataset of 80 classes.\n","fe027567":"# \ud83d\udca1 Getting blob from input image","2bbb3a3f":"# \ud83c\udf08 Colours for representing every detected object","320416bf":"# \ud83d\udce5 Importing needed libraries","d81a6d9a":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)","d5d67530":"# \ud83d\uddbd Drawing bounding boxes and labels"}}