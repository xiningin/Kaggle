{"cell_type":{"9cbb7353":"code","10b9a2e5":"code","f1acb27d":"code","d924ffa7":"code","7efa7737":"code","07462cf8":"code","9a61cc7d":"code","b890e5a7":"code","5dba5888":"code","184f1e2a":"code","9a51e5ee":"code","2e2cada7":"code","99967f45":"code","8450ace5":"code","bcb67d04":"code","9a1385b4":"code","6a29ab27":"markdown","ab7899cc":"markdown","b4aefca4":"markdown","464fa36d":"markdown","9799b7e7":"markdown","aa66ad02":"markdown","9fb0feaa":"markdown","8e2229fd":"markdown","2594c18f":"markdown","ca6cd14a":"markdown","0c011203":"markdown","50f8872a":"markdown","c9712d78":"markdown","f59d5170":"markdown","b30614a4":"markdown","ba8a55e0":"markdown","a2271053":"markdown"},"source":{"9cbb7353":"import pandas as pd\nreviews = pd.read_csv('..\/input\/olist_order_reviews_dataset.csv')\nreviews['num_letters'] = reviews.review_comment_message.str.count('[a-zA-Z]')\nreviews = reviews[reviews.num_letters > 0]  # Filter valid comments\nreviews.head(3)","10b9a2e5":"import re\nimport numpy as np\n# Nlp\nimport nltk\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.manifold import TSNE\n# Plots\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools, figure_factory\ninit_notebook_mode(connected=True)\n# Wordcloud\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef plot_wordcloud(text, stopwords, mask=None, max_words=200, max_font_size=100,\n                   title=None, title_size=40, image_color=False):\n\n    figure_size = (24, 16)\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=1200, \n                    height=300,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    plt.imshow(wordcloud);\n    plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n\nstopwords = nltk.corpus.stopwords.words('portuguese')\ncomments = reviews.review_comment_message.values\nplot_wordcloud(comments, stopwords, title=\"\")","f1acb27d":"from collections import defaultdict\n\ndef generate_ngrams(text, stopwords, n_gram=1):\n    token = [w.lower() for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n    # Remove stopwords and ponctuation\n    token = [t for t in token if re.search('[a-zA-Z]', t) and t not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Count words\nfreq_dict = defaultdict(int)\nfor sent in comments:\n    for word in generate_ngrams(sent, stopwords):\n        freq_dict[word] += 1\nwdf = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1],\n                   columns=[\"word\", \"word_count\"])\n\n# Count Trigrams\nfreq_dict = defaultdict(int)\nfor sent in comments:\n    for word in generate_ngrams(sent, stopwords, 3):\n        freq_dict[word] += 1\ntdf = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1],\n                   columns=[\"trigram\", \"trigram_count\"])\n\n# Sort and filter top 12\ntdf = tdf.sort_values(by='trigram_count', ascending=False).iloc[:12]\nwdf = wdf.sort_values(by='word_count', ascending=False).iloc[:12]\n\ntrace0 = go.Bar(\n    y=wdf.word.values,\n    x=wdf.word_count.values,\n    name='Number of words',\n    orientation='h',\n    marker=dict(color='rgb(49,130,189)')\n)\ntrace1 = go.Bar(\n    y=tdf.trigram.values,\n    x=tdf.trigram_count.values,\n    name='Number of trigrams',\n    orientation='h',\n    marker=dict(color='rgb(204,204,204)')\n)\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\n\nfig['layout'].update(\n    height=600, width=800,\n    title='Words and trigrams in reviews',\n    margin=dict(l=150, r=10, t=100, b=100),\n    legend=dict(orientation=\"h\")\n)\nfig['layout']['xaxis1'].update(domain=[0, 0.40])\nfig['layout']['xaxis2'].update(domain=[0.6, 1])\niplot(fig)","d924ffa7":"length = [len(text) for text in comments]\nnum_letters = [len(re.findall(r'[a-zA-Z]', text)) for text in comments]\nnum_commas = [text.count(',') for text in comments]\nnum_dots = [text.count('.') for text in comments]\n\nfig, axis = plt.subplots(1, 2, figsize=(12,4))\npl0 = sns.kdeplot(length, color='navy', label='Review length', ax=axis[0])\npl1 = sns.kdeplot(num_letters, color='orange', label='Number of letters', ax=axis[0])\npl2 = sns.kdeplot(num_dots, color='navy', label='Number of dots', ax=axis[1])\npl3 = sns.kdeplot(num_commas, color='orange', label='Number of commas', ax=axis[1])","7efa7737":"review_count = reviews.review_score.value_counts()\ntrace = go.Bar(x=review_count.index, y=review_count.values)\nlayout = go.Layout(title='Review scores distribution', height=360, width=800)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","07462cf8":"class ItemSelector(BaseEstimator, TransformerMixin):\n    \"\"\"For data grouped by feature, select subset of data at a provided key.\"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_):\n        return data_[self.key].values","9a61cc7d":"class RemoveStopwords(BaseEstimator, TransformerMixin):\n    \"\"\"Remove stopwords from list of tokens list.\"\"\"\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, comments):\n        stopwords = nltk.corpus.stopwords.words('portuguese')\n        stopwords.extend(['\u00e9', 'ok', 'ta', 't\u00e1', 'att', 'att.', 'sr', 'por\u00e9m',\n                          'produto', 'recomendo'])\n        return [[tk for tk in tokens if tk not in stopwords] for tokens in comments]","b890e5a7":"class CorrectSpelling(BaseEstimator, TransformerMixin):\n    \"\"\"Fix a few spelling mistakes in tokens.\"\"\"\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, comments):\n        mistakes_dict = {\n            'decpcionou': 'decepcionou', 't\u00f4': 'estou', 'to': 'estou',\n            'q': 'que', 'pq': 'porque', 'mt': 'muito', 'muiiita': 'muita',\n            'estaav': 'estava', 'acabento': 'acabamento', 'orrivel': 'horr\u00edvel',\n            'sert\u00f5es': 'certos', 'vcs': 'voc\u00eas', 'msg': 'mensagem', 'dta': 'data',\n            '\u00f1': 'n\u00e3o', 'n': 'n\u00e3o', 'grates': 'gr\u00e1tis', 'testa-lo': 'testar',\n            'superandoo': 'superando', 'atentimento': 'atendimento',\n            'cancelacem': 'cancelassem', 'msm': 'mesmo', 'protudo': 'produto',\n            'decrarar': 'declarar', 'trasporte': 'transporte', 'decpsionei': 'decepcionei',\n            'empuerada': 'empoeirada', 'recebie': 'recebi', 'superr': 'super',\n            'nao': 'n\u00e3o', 'mto': 'muito', 'tb': 'tamb\u00e9m', 'execelente': 'excelente',\n            'tao': 't\u00e3o', 'blz': 'beleza'\n        }\n        return [[mistakes_dict[tk] if tk in mistakes_dict else tk for tk in tokens]\n               for tokens in comments]","5dba5888":"class Stemmer(BaseEstimator, TransformerMixin):\n    \"\"\"Used to reduce words with the portuguese Snowball stemmer.\"\"\"\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, comments):\n        #stemmer_ = nltk.stem.snowball.SnowballStemmer('portuguese')\n        stemmer_ = nltk.stem.RSLPStemmer()\n        return [[stemmer_.stem(tk) for tk in tokens] for tokens in comments]","184f1e2a":"class Tokenize(BaseEstimator, TransformerMixin):\n    \"\"\"Class to tokenize comments.\"\"\"\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, comments):\n        tokenized_comments = list()\n        for text in comments:\n            # Tokenize and lower\n            tokens = [w.lower() for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n            # Filter tokens that doesn't have letters\n            tokens = [t for t in tokens if re.search('[a-zA-Z]', t)]\n            tokenized_comments.append(tokens)\n        return tokenized_comments","9a51e5ee":"def print_top_words_for_topic(components, feature_names, num_words):\n    \"\"\"Print top words for each topic in components vector.\"\"\"\n    for topic_idx, topic in enumerate(components):\n        message = \"Topic #%d: \" % (topic_idx + 1)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]])\n        print(message)\n\nnum_topics = 3\nrnd_state = 125","2e2cada7":"lsa_pipeline = Pipeline([\n    ('selector', ItemSelector(key='review_comment_message')),\n    ('tokenize', Tokenize()),\n    ('spelling', CorrectSpelling()),\n    ('stopwords', RemoveStopwords()),\n    ('stemming', Stemmer()),\n    ('tfidf', TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,\n                              analyzer='word', min_df=2, max_df=0.95)),\n    ('reduce', TruncatedSVD(n_components=num_topics, random_state=rnd_state))\n])\n\nlsa_matrix = lsa_pipeline.fit_transform(reviews)\n# Get words and components from pipeline transformers\nfeat_names = lsa_pipeline.get_params()['tfidf'].get_feature_names()\ncomponents = lsa_pipeline.get_params()['reduce'].components_\n# Print the 8 most important words for each topic\nprint_top_words_for_topic(components, feat_names, 8)","99967f45":"plsa_pipeline = Pipeline([\n    ('selector', ItemSelector(key='review_comment_message')),\n    ('tokenize', Tokenize()),\n    ('spelling', CorrectSpelling()),\n    ('stopwords', RemoveStopwords()),\n    ('stemming', Stemmer()),\n    ('tfidf', TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,\n                              analyzer='word', min_df=2, max_df=0.95)),\n    ('reduce', NMF(n_components=num_topics, random_state=rnd_state, solver='mu',\n                   beta_loss='kullback-leibler', alpha=0.1, l1_ratio=0.5))\n])\n\nplsa_matrix = plsa_pipeline.fit_transform(reviews)\n# Get words and components from pipeline transformers\nfeat_names = plsa_pipeline.get_params()['tfidf'].get_feature_names()\ncomponents = plsa_pipeline.get_params()['reduce'].components_\n# Print the 8 most important words for each topic\nprint_top_words_for_topic(components, feat_names, 8)","8450ace5":"lda_pipeline = Pipeline([\n    ('selector', ItemSelector(key='review_comment_message')),\n    ('tokenize', Tokenize()),\n    ('spelling', CorrectSpelling()),\n    ('stopwords', RemoveStopwords()),\n    ('stemming', Stemmer()),\n    ('countvec', CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,\n                                 analyzer='word', min_df=2, max_df=0.95)),\n    ('reduce', LatentDirichletAllocation(n_components=num_topics,\n                                         random_state=rnd_state))\n])\n\nlda_matrix = lda_pipeline.fit_transform(reviews)\n# Get words and components from pipeline transformers\nfeat_names = lda_pipeline.get_params()['countvec'].get_feature_names()\ncomponents = lda_pipeline.get_params()['reduce'].components_\n# Print the 8 most important words for each topic\nprint_top_words_for_topic(components, feat_names, 8)","bcb67d04":"def scatter_plot(arr, tsne_arr, threshold=0):\n    idx = np.amax(arr, axis = 1) >= threshold\n    colors = [row.argmax() for row in arr[idx]]\n    trace = go.Scattergl(x=tsne_arr[idx, 0], y=tsne_arr[idx, 1],\n                         mode='markers', marker=dict(color=colors))\n    iplot([trace])","9a1385b4":"lr = 150\n# LSA\nlsa_tsne = TSNE(n_components=2, learning_rate=lr).fit_transform(lsa_matrix)\nscatter_plot(lsa_matrix, lsa_tsne)\n\n# pLSA\nplsa_tsne = TSNE(n_components=2, learning_rate=lr).fit_transform(plsa_matrix)\nscatter_plot(plsa_matrix, plsa_tsne)\n\n# LDA\nlda_tsne = TSNE(n_components=2, learning_rate=lr).fit_transform(lda_matrix)\nscatter_plot(lda_matrix, lda_tsne, threshold=0.5)","6a29ab27":"<h2>4. Visualization with t-SNE<\/h2>\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It's not a linear model and does not retain distances but probabilities. For more information please refer to [this github page](https:\/\/lvdmaaten.github.io\/tsne\/).","ab7899cc":"<h2>5. Quick Conclusion<\/h2>\n\nThe challenge here is that most comments are related to delivery time or just praise\/criticism without a specific topic. However, we can see that the first topic was about delivery, while the second was mainly about price and quality service. The third topic was mixed, with words like quality, store and delivery again. Another issue is the difficulty to evaluate unsupervised methods, since we don't have a straightforward metric to measure it.","b4aefca4":"Some customers really like dots... Finally, let's check the review scores:","464fa36d":"<h3>1.1 Text Exploratory Analysis<\/h3>\n\nFor a complete exploration on all datasets you can check [this kernel](https:\/\/www.kaggle.com\/jsaguiar\/e-commerce-exploratory-analysis).","9799b7e7":"<h3>3.1 Latent Semantic Analysis (LSA)<\/h3>\n\nThe key idea of LSA is to map documents (and by symmetry terms) to a vector space of reduced dimensionality. The mapping is restricted to be linear and is based on a Singular Value Decomposition (SVD), which is also used in Principal Component Analysis. ","aa66ad02":"<h3>2.3 Stemming<\/h3>\n\n> Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. (Wikipedia)\n\nThe goal of stemming algorithms is to group related words and to separate unrelated words. More information about the portuguese stemmer can be find in [this link](https:\/\/snowballstem.org\/algorithms\/portuguese\/stemmer.html).","9fb0feaa":"<h3>2.2 Spelling mistakes<\/h3>\n\nThere are a few algorithms to correct spelling mistakes, but I'll be doing this manually.","8e2229fd":"<h2>2. Text Preprocessing<\/h2>\n\nThe first step is to transform comments in tokens. We also need to apply a few preprocessing steps like removing common words and correcting spelling mistakes. These steps must be implemented as classes and will be grouped using a sklearn pipeline in the next section. The first class is just for selecting columns in our data:","2594c18f":"<h2>1. Introduction<\/h2>\n\nThis dataset has 41,000 comments in portuguese and 100,000 review scores about ecommerce orders in Brazil. But what costumers are praising or complaining about? Delivering time? Packaging? Missing items? Product quality?\n\nAltough the kernel name, I'll be using topic extraction to find a few topics that represent the costumers opinions. With this approach there is no distance metric and we are not actually using a clustering algorithm, but a dimensionality reduction technique.\n\nAnother approach would be training a classifier to predict some category for each review, but this has a few problems:\n\n* Manually labelling thousands of comments for training\n* Different analysts could introduce bias\n* Fixed number of categories\n\nFirst let's have a quick look at the reviews dataset. I'm removing all reviews that doesn't have a comment through python regular expression.","ca6cd14a":"It seems that most comments are at least mentioning delivery time. Now let's have a look at some text statistics:","0c011203":"<h3>2.4 Tokenize<\/h3>\n\nReturn a list for each comment with tokens and remove the ones that doesn't have letters (like !! or ...)","50f8872a":"<h3>2.1 Stopwords<\/h3>\n\nRemoving common words like 'the' and 'is' usually improves the model since these words doesn't help to distinguish between documents. I'll be using the portuguese stopwords from NLTK with a few extra elements.","c9712d78":"<h3>3.2 Latent Dirichlet Allocation (LDA)<\/h3>\n\nLatent Dirichlet Allocation is a generative statistical model similar to pLSA. The difference is that LDA uses an [Dirichlet distribution](https:\/\/en.wikipedia.org\/wiki\/Dirichlet_distribution) to handle the per-document distribution, while in PLSA this is given by a hyperparameter. This distribution should also prevent overfitting (in theory).","f59d5170":"<h3>2.5 Vectorize<\/h3>\n\nFinally we need to convert words to numbers. I'll use two transformers that are already implemented in Sklearn library:\n\n<b>Count Vectorizer<\/b>\n\n> Count the number of words in each document (review) and return a matrix where each column is a word and each row is a review. To save memory, sklearn return a sparse matrix object (scipy.sparse.csr_matrix).\n\n<b>TF-IDF<\/b>\n\n> Term frequency\u2013inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a collection. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word (Wikipedia).\n\nBoth are considered bag-of-words representations, which doesn't consider the order and consequently the sentence meaning. However, it's possible to create good topic models using only the frequency of words in each document. ","b30614a4":"<h3>3.2 Probabilistic Latent Semantic Analysis (pLSA)<\/h3>\n\nThe crucial difference between pLSA and LSA is the objective function utilized to determine the optimal decomposition. In LSA, this is the L2-norm (or [Frobenius Norm](http:\/\/mathworld.wolfram.com\/FrobeniusNorm.html) or matrix norm), while pLSA relies on the likelihood function of multinomial sampling ([Kullback-Leibler divergence](https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence)). \n\nPLSA is a non-linear model based on probability distributions. The output is the distribution over a fixed vocabulary, where every topic has a probability for every word. It's important to note that the same word can have a high probability for multiple topics. For a detailed explanation please refer to the [original paper](http:\/\/www.iro.umontreal.ca\/~nie\/IFT6255\/Hofmann-UAI99.pdf).\n","ba8a55e0":"Let's have a look at the most common words and trigrams:","a2271053":"<h2>3. Topic Modeling<\/h2>\n\nTopic model is a type of statistical model for discovering the abstract topics that occur in a collection of documents. The number of topics is considered a hyperparameter, which I choose by looking at the topics that were being created. I've also tryed to use the perplexity metric for finding the optimal number, but it didn't work very well."}}