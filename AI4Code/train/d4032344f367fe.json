{"cell_type":{"f464f1f8":"code","4a503030":"code","dd376fcc":"code","401970cb":"code","c65daafc":"code","a38f1bf6":"code","5bbdc9d9":"code","b8f91376":"code","913dd58f":"code","c8446758":"code","460896fd":"code","19b02a6f":"code","f010e3c5":"code","4e0a7f36":"code","f0d03f1e":"code","0f9da0ea":"code","8c07256e":"code","f616fe87":"code","73b4130b":"code","07ef84bb":"code","f0ff92bd":"code","ceee397d":"code","b17f16ae":"code","a015fd33":"code","0b1df91d":"code","42d31318":"code","7920d806":"code","2fb6c1a2":"code","509aff54":"code","24b808b0":"code","ad82965c":"code","93f40c1b":"code","f902555f":"code","39a37f48":"code","e9a5c1b9":"code","b0ba71b8":"code","4ec58690":"code","8cc03151":"code","cc7ba0e2":"code","227e70a7":"code","25c96109":"code","0e6d939d":"code","6ec4a4f0":"code","27d9900f":"code","b1757a83":"code","28c0ebc3":"code","d899c5f9":"code","d77759ef":"code","9341fe8f":"code","51693053":"code","00959e9e":"code","581051c8":"code","f7c7bfd2":"code","3fbba192":"code","4a18fa1d":"code","8cd77e5c":"code","ddcea365":"code","06efb890":"code","9a789742":"code","7fd4b93a":"code","3d3bb591":"code","0c3f71bc":"code","77bf2af2":"code","7da73e42":"code","156657bf":"code","d9fff06c":"code","b6a0ef0a":"code","7511f4ae":"code","766781e1":"code","c7646d5c":"code","99a69fec":"code","5f9c6b6a":"code","2d14d4ea":"code","850dd50c":"code","a7d0b437":"code","92ef2bd5":"code","9dd5a8cb":"code","bf961fef":"code","f3859861":"code","aa022ce3":"code","8f453b1d":"code","22bfb72a":"code","d84c80b9":"code","a9b635a7":"code","cff16e66":"code","d3437ce5":"code","2db8fafd":"code","b61daadc":"code","4da3c0ca":"markdown","510c7670":"markdown","f023b435":"markdown","47cf9da8":"markdown","32765f0f":"markdown","27ec2fac":"markdown","98be044a":"markdown","2d8403e1":"markdown","33ed24ee":"markdown","0fff6cd4":"markdown","307d893d":"markdown","192cc942":"markdown","32b6ea88":"markdown","61577604":"markdown","6d0fda8a":"markdown","7e4edb55":"markdown","a844f160":"markdown","cb804db8":"markdown","99814ffd":"markdown","e81e1eba":"markdown","2bb323c5":"markdown","f2abbed5":"markdown","b34668bc":"markdown","06c702be":"markdown","3e46aee5":"markdown","25309c13":"markdown","4848b89a":"markdown","c1b1eae9":"markdown","ce01b338":"markdown","8e47d24c":"markdown","7d365728":"markdown","3586f267":"markdown","a5ab6b6a":"markdown","3d21197c":"markdown","49fef5d8":"markdown","6d7bc66e":"markdown","9c2dfbb3":"markdown","21c97c51":"markdown","164cca5d":"markdown","cef3677d":"markdown","d537cc29":"markdown","ee0af61f":"markdown","a2748ff1":"markdown","0cfa2aec":"markdown","d62a28b7":"markdown","7da52287":"markdown","0de76eee":"markdown","2a3b6f51":"markdown","83208d8d":"markdown","288e47e7":"markdown","f4e7ea68":"markdown","d1f9e0bc":"markdown","0da86094":"markdown","bff4b7d1":"markdown","0f5d5454":"markdown","1f0363db":"markdown","1ba7fe60":"markdown","9df0f0ae":"markdown","719824ad":"markdown","cfe1058b":"markdown","56a2e8f0":"markdown","2acd3238":"markdown","bed46352":"markdown","6eaad526":"markdown","7deff5ce":"markdown","ea01eb22":"markdown","634e67f7":"markdown","361d1a74":"markdown","5129014e":"markdown"},"source":{"f464f1f8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline","4a503030":"import os\nfrom zipfile import ZipFile\nfrom urllib.request import urlretrieve\n\ndataset_url = 'https:\/\/github.com\/JovianML\/opendatasets\/raw\/master\/data\/house-prices-advanced-regression-techniques.zip'\nurlretrieve(dataset_url, 'house-prices.zip')\nwith ZipFile('house-prices.zip') as f:\n    f.extractall(path='house-prices')\n    \nos.listdir('house-prices')","dd376fcc":"import pandas as pd\npd.options.display.max_columns = 200 ## dislaying the columns and max rows \npd.options.display.max_rows = 200\n\nprices_df = pd.read_csv('.\/house-prices\/train.csv')\nprices_df","401970cb":"\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Identify input and target columns\ninput_cols, target_col = prices_df.columns[1:-1], prices_df.columns[-1]\ninputs_df, targets = prices_df[input_cols].copy(), prices_df[target_col].copy()\n\n# Identify numeric and categorical columns\nnumeric_cols = prices_df[input_cols].select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = prices_df[input_cols].select_dtypes(include='object').columns.tolist()\n","c65daafc":"inputs_df","a38f1bf6":"# Impute and scale numeric columns\nimputer = SimpleImputer().fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])\n\nscaler = MinMaxScaler().fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])\n","5bbdc9d9":"inputs_df","b8f91376":"# One-hot encode categorical columns\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs_df[categorical_cols].fillna('Unknown'))\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\ninputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols].fillna('Unknown'))\n\n# Create training and validation sets\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(\n    inputs_df[numeric_cols + encoded_cols], targets, test_size=0.25, random_state=42)","913dd58f":"from sklearn.ensemble import RandomForestRegressor","c8446758":"# Create the model\n## Model is created using the 100 no of tress for random classsifier as default \ndef random_forest_clr(**param):\n  model = RandomForestRegressor(random_state=42,n_jobs=-1,n_estimators=50,**param).fit(train_inputs,train_targets)\n  print(' The training model accuracy score is {:.4f} \\n The validation model accuracy score is {:.4f} '.format(model.score(train_inputs,train_targets)*100,model.score(val_inputs,val_targets)*100))","460896fd":"random_forest_clr(max_depth=5)","19b02a6f":"random_forest_clr(max_features=0.8)","f010e3c5":"def test_random_forest_and_plot(param_name,param_values):\n  train_error,val_error=[],[]\n  for value in param_values:\n    param={param_name:value}\n    model=RandomForestRegressor(random_state=42,n_jobs=-1,**param).fit(train_inputs,train_targets)\n    train_error.append(1-model.score(train_inputs,train_targets))\n    val_error.append(1-model.score(val_inputs,val_targets))\n  plt.figure(figsize=(12,9))\n  plt.plot(param_values,train_error,'ro-')\n  plt.plot(param_values,val_error,'bo-')\n  plt.title(' The distribution of {} parameter'.format(param_name))\n  plt.legend(['training','validation'])","4e0a7f36":"test_random_forest_and_plot('max_depth',np.arange(2,20,3))","f0d03f1e":"test_random_forest_and_plot('n_estimators',np.arange(20,1000,50))","0f9da0ea":"test_random_forest_and_plot('min_samples_leaf',np.arange(1,50,2))","8c07256e":"test_random_forest_and_plot('min_samples_split',np.arange(0.05,1,0.05))","f616fe87":"test_random_forest_and_plot('max_features',np.arange(0.05,1,0.05))","73b4130b":"%%time\ntest_random_forest_and_plot('max_leaf_nodes',np.arange(2,100,2))","07ef84bb":"test_random_forest_and_plot('max_samples',np.arange(0.1,1,0.1))","f0ff92bd":"def random_forest_clr(**param):\n  model = RandomForestRegressor(random_state=42,n_jobs=-1,**param).fit(train_inputs,train_targets)\n  print(' The training model accuracy score is {:.4f} \\n The validation model accuracy score is {:.4f} '.format(model.score(train_inputs,train_targets)*100,model.score(val_inputs,val_targets)*100))","ceee397d":"random_forest_clr(max_depth=14,n_estimators=400,max_samples=0.8,max_features=0.8)","b17f16ae":"model=RandomForestRegressor(n_jobs=-1,n_estimators=400,random_state=42,max_depth=14,max_samples=0.8,max_features=0.8).fit(train_inputs,train_targets)","a015fd33":"model.estimators_[1]","0b1df91d":"from sklearn.tree import export_graphviz","42d31318":"import graphviz","7920d806":"figure=export_graphviz(model.estimators_[1],feature_names=numeric_cols+encoded_cols,filled=True,rounded=True)","2fb6c1a2":"graphviz.Source(figure)","509aff54":"feature_rf_imp = pd.DataFrame({\n    'feature': numeric_cols+encoded_cols,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False).head(10)","24b808b0":"feature_rf_imp","ad82965c":"plt.figure(figsize=(10,8))\nsns.barplot(data=feature_rf_imp, x='importance', y='feature');","93f40c1b":"from sklearn.neighbors import KNeighborsRegressor","f902555f":"# Create the model\n## Model is created using the 100 no of tress for random classsifier as default \ndef knn_clr(**param):\n  model = KNeighborsRegressor(n_jobs=-1,**param).fit(train_inputs,train_targets)\n  print(' The training model accuracy score is {:.4f} \\n The validation model accuracy score is {:.4f} '.format(model.score(train_inputs,train_targets)*100,model.score(val_inputs,val_targets)*100))","39a37f48":"knn_clr(n_neighbors=10)","e9a5c1b9":"knn_clr(n_neighbors=11,algorithm='kd_tree')","b0ba71b8":"knn_clr(n_neighbors=18,algorithm='ball_tree',leaf_size=12)","4ec58690":"def test_knn_and_plot(param_name,param_values):\n  train_error,val_error=[],[]\n  for value in param_values:\n    param={param_name:value}\n    model=KNeighborsRegressor(n_jobs=-1,**param).fit(train_inputs,train_targets)\n    train_error.append(1-model.score(train_inputs,train_targets))\n    val_error.append(1-model.score(val_inputs,val_targets))\n  plt.figure(figsize=(12,9))\n  plt.plot(param_values,train_error,'ro-')\n  plt.plot(param_values,val_error,'bo-')\n  plt.title(' The distribution of {} parameter'.format(param_name))\n  plt.legend(['training','validation'])","8cc03151":"%%time\ntest_knn_and_plot('n_neighbors',np.arange(2,20,1))","cc7ba0e2":"%%time\ntest_knn_and_plot('leaf_size',np.arange(2,100,2))","227e70a7":"knn_clr(n_neighbors=6)","25c96109":"model=KNeighborsRegressor(n_neighbors=6,n_jobs=-1).fit(train_inputs,train_targets)","0e6d939d":"model.n_features_in_","6ec4a4f0":"from xgboost import XGBRegressor","27d9900f":"from sklearn.metrics import confusion_matrix,classification_report","b1757a83":"## Lets define the helper model model \ndef xgboost_clfr(**param):\n  model=XGBRegressor(n_jobs=-1,random_state=42,**param).fit(train_inputs,train_targets)\n  print(' The training model accuracy score is {:.4f} \\n The validation model accuracy score is {:.4f} '.format(model.score(train_inputs,train_targets)*100,model.score(val_inputs,val_targets)*100))\n","28c0ebc3":"xgboost_clfr(max_depth=6)","d899c5f9":"xgboost_clfr(n_estimators=200,max_depth=5)","d77759ef":"def test_xgboost_and_plot(param_name,param_values):\n  train_error,val_error=[],[]\n  for value in param_values:\n    param={param_name:value}\n    model=XGBRegressor(random_state=42,n_jobs=-1,**param).fit(train_inputs,train_targets)\n    train_error.append(1-model.score(train_inputs,train_targets))\n    val_error.append(1-model.score(val_inputs,val_targets))\n  plt.figure(figsize=(12,9))\n  plt.plot(param_values,train_error,'ro-')\n  plt.plot(param_values,val_error,'bo-')\n  plt.title(' The distribution of {} parameter'.format(param_name))\n  plt.legend(['training','validation'])","9341fe8f":"test_xgboost_and_plot('max_depth',np.arange(2,20,3))","51693053":"test_xgboost_and_plot('learning_rate',np.arange(0.1,1,0.05))","00959e9e":"test_xgboost_and_plot('n_estimators',np.arange(10,1000,50))","581051c8":"test_xgboost_and_plot('subsample',np.arange(0.1,1,0.1))","f7c7bfd2":"test_xgboost_and_plot('colsmaple_bytree',np.arange(0.1,1,0.1))","3fbba192":"xgboost_clfr(max_depth=5,learning_rate=0.25,subsample=0.7)","4a18fa1d":"model=XGBRegressor(n_jobs=-1,random_state=42,max_depth=5,learning_rate=0.25,subsample=0.7).fit(train_inputs,train_targets)","8cd77e5c":"pred_xgb_train=model.predict(train_inputs)\npred_xgb_val=model.predict(val_inputs)","ddcea365":"xgb_feature=pd.DataFrame({'feature':numeric_cols+encoded_cols,'importance':model.feature_importances_}).sort_values('importance',ascending=False).head(10)","06efb890":"xgb_feature","9a789742":"plt.figure(figsize=(12,9))\nsns.barplot(data=xgb_feature,x='feature',y='importance')\nplt.title(' The top 10 important paraemetrs in predciting the result')\nplt.xticks(rotation=90);","7fd4b93a":"from lightgbm import LGBMRegressor","3d3bb591":"%%time\ndef light_gbm_clfr(**param):\n  model=LGBMRegressor(n_jobs=-1,random_state=42,**param).fit(train_inputs,train_targets)\n  print(' the training model score is {:.4f}% and \\ The validatin score is {:.4f}%'.format(model.score(train_inputs,train_targets)*100,model.score(val_inputs,val_targets)*100))","0c3f71bc":"%%time\nlight_gbm_clfr(n_estimators=200)","77bf2af2":"light_gbm_clfr(max_depth=8)","7da73e42":"def test_lightgbm_and_plot(param_name,param_values):\n  train_error,val_error=[],[]\n  for value in param_values:\n    param={param_name:value}\n    model=LGBMRegressor(random_state=42,n_jobs=-1,**param).fit(train_inputs,train_targets)\n    train_error.append(1-model.score(train_inputs,train_targets))\n    val_error.append(1-model.score(val_inputs,val_targets))\n  plt.figure(figsize=(12,9))\n  plt.plot(param_values,train_error,'ro-')\n  plt.plot(param_values,val_error,'bo-')\n  plt.title(' The distribution of {} parameter'.format(param_name))\n  plt.legend(['training','validation'])","156657bf":"##1. num_leaves\ntest_lightgbm_and_plot('num_leaves',np.arange(2,100,4))","d9fff06c":"%%time\ntest_lightgbm_and_plot('max_depth',np.arange(2,30,2))","b6a0ef0a":"test_lightgbm_and_plot('max_depth',np.arange(2,6,1))","7511f4ae":"test_lightgbm_and_plot('learning_rate',np.arange(0.1,1,0.1))","766781e1":"test_lightgbm_and_plot('subsample',np.arange(0.1,1,0.1))","c7646d5c":"test_lightgbm_and_plot('min_child_samples',np.arange(5,50,5))","99a69fec":"model_lightgbm=LGBMRegressor(num_leaves=10,max_depth=4,random_state=42,n_jobs=-1).fit(train_inputs,train_targets)","5f9c6b6a":"test_df = pd.read_csv('house-prices\/test.csv')","2d14d4ea":"test_df","850dd50c":"test_df[numeric_cols] = imputer.transform(test_df[numeric_cols])\ntest_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\ntest_df[encoded_cols] = encoder.transform(test_df[categorical_cols].fillna('Unknown'))","a7d0b437":"test_inputs = test_df[numeric_cols + encoded_cols]","92ef2bd5":"test_preds = model_lightgbm.predict(test_inputs)","9dd5a8cb":"submission_df = pd.read_csv('house-prices\/sample_submission.csv')","bf961fef":"submission_df","f3859861":"submission_df['SalePrice'] = test_preds","aa022ce3":"submission_df.to_csv('submission_lightgbm.csv', index=False)","8f453b1d":"from IPython.display import FileLink\nFileLink('submission6.csv') # Doesn't work on Colab, use the file browser instead to download the file.","22bfb72a":"def predict_input(model, single_input):\n    input_df = pd.DataFrame([single_input])\n    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].fillna('Unknown'))\n    return model.predict(input_df[numeric_cols + encoded_cols])[0]","d84c80b9":"sample_input = { 'MSSubClass': 20, 'MSZoning': 'RL', 'LotFrontage': 77.0, 'LotArea': 9320,\n 'Street': 'Pave', 'Alley': None, 'LotShape': 'IR1', 'LandContour': 'Lvl', 'Utilities': 'AllPub',\n 'LotConfig': 'Inside', 'LandSlope': 'Gtl', 'Neighborhood': 'NAmes', 'Condition1': 'Norm', 'Condition2': 'Norm',\n 'BldgType': '1Fam', 'HouseStyle': '1Story', 'OverallQual': 4, 'OverallCond': 5, 'YearBuilt': 1959,\n 'YearRemodAdd': 1959, 'RoofStyle': 'Gable', 'RoofMatl': 'CompShg', 'Exterior1st': 'Plywood',\n 'Exterior2nd': 'Plywood', 'MasVnrType': 'None','MasVnrArea': 0.0,'ExterQual': 'TA','ExterCond': 'TA',\n 'Foundation': 'CBlock','BsmtQual': 'TA','BsmtCond': 'TA','BsmtExposure': 'No','BsmtFinType1': 'ALQ',\n 'BsmtFinSF1': 569,'BsmtFinType2': 'Unf','BsmtFinSF2': 0,'BsmtUnfSF': 381,\n 'TotalBsmtSF': 950,'Heating': 'GasA','HeatingQC': 'Fa','CentralAir': 'Y','Electrical': 'SBrkr', '1stFlrSF': 1225,\n '2ndFlrSF': 0, 'LowQualFinSF': 0, 'GrLivArea': 1225, 'BsmtFullBath': 1, 'BsmtHalfBath': 0, 'FullBath': 1,\n 'HalfBath': 1, 'BedroomAbvGr': 3, 'KitchenAbvGr': 1,'KitchenQual': 'TA','TotRmsAbvGrd': 6,'Functional': 'Typ',\n 'Fireplaces': 0,'FireplaceQu': np.nan,'GarageType': np.nan,'GarageYrBlt': np.nan,'GarageFinish': np.nan,'GarageCars': 0,\n 'GarageArea': 0,'GarageQual': np.nan,'GarageCond': np.nan,'PavedDrive': 'Y', 'WoodDeckSF': 352, 'OpenPorchSF': 0,\n 'EnclosedPorch': 0,'3SsnPorch': 0, 'ScreenPorch': 0, 'PoolArea': 0, 'PoolQC': np.nan, 'Fence': np.nan, 'MiscFeature': 'Shed',\n 'MiscVal': 400, 'MoSold': 1, 'YrSold': 2010, 'SaleType': 'WD', 'SaleCondition': 'Normal'}","a9b635a7":"predicted_price = predict_input(model, sample_input)","cff16e66":"print('The predicted sale price of the house is ${}'.format(predicted_price))","d3437ce5":"import joblib","2db8fafd":"house_prices_rf = {\n    'model_lightgbm': model_lightgbm,\n    'model_xgboost':model,\n    'imputer': imputer,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}","b61daadc":"joblib.dump(house_prices_rf, 'house_prices_rf.joblib')","4da3c0ca":"## XGBoost\n\n<font size =3>**XGBOOST.**\n\nThe results of the regression problems are continuous or real values. Some commonly used regression algorithms are Linear Regression and Decision Trees. There are several metrics involved in regression like root-mean-squared error (RMSE) and mean-squared-error (MAE). These are some key members for XGBoost models, each plays their important roles.\n\nRMSE: It is the square root of mean squared error (MSE).\nMAE: It is an absolute sum of actual and predicted differences, but it lacks mathematically, that\u2019s why it is rarely used, as compared to other metrics.\nXGBoost is a powerful approach for building supervised regression models. The validity of this statement can be inferred by knowing about its (XGBoost) objective function and base learners.\n\nThe objective function contains loss function and a regularization term. It tells about the difference between actual values and predicted values, i.e how far the model results are from the real values. The most common loss functions in XGBoost for regression problems is reg:linear, and that for binary classification is reg:logistics.\n\nEnsemble learning involves training and combining individual models (known as base learners) to get a single prediction, and XGBoost is one of the ensemble learning methods. XGBoost expects to have the base learners which are uniformly bad at the remainder so that when all the predictions are combined, bad predictions cancels out and better one sums up to form final good predictions.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)","510c7670":"### Lets chart the top 10 most important features in the model","f023b435":"> **EXERCISE**: Change the sample input above and make predictions. Try different examples and try to figure out which columns have a big impact on the sale price. Hint: Look at the feature importance to decide which columns to try.","47cf9da8":"## KNN Neighbours\n\n`sklearn.neighbors `provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels.\n\nThe principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply \u201cremember\u201d all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree).\n\nDespite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n\nThe classes in sklearn.neighbors can handle either NumPy arrays or scipy.sparse matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.\n\nThere are many learning routines which rely on nearest neighbors at their core. One example is kernel density estimation, discussed in the density estimation section.\n\n![](https:\/\/bookdown.org\/tpinto_home\/Regression-and-Classification\/_main_files\/figure-html\/knn3and20-1.png)","32765f0f":"**As it can be seen the XGBoost Regresor is very fast in computing the prediction and also the accuracy score is high.**","27ec2fac":"<font size =4> 3. n_estimators","98be044a":"Best value for max_depth is 14.","2d8403e1":"# House Prices - Advanced Regression Techniques\n\n![](https:\/\/i.imgur.com\/3sw1fY9.jpg)\n\nIn this assignment, you'll continue building on the previous assignment to predict the price of a house using information like its location, area, no. of rooms etc. You'll use the dataset from the [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition on [Kaggle](https:\/\/kaggle.com). \n\nWe'll follow a step-by-step process:\n\n1. Download and prepare the dataset for training\n2. Train, evaluate and interpret a decision tree\n3. Train, evaluate and interpret a random forest\n4. Tune hyperparameters to improve the model\n5. Make predictions and save the model\n\nAs you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end and your machine learning model is trained properly without errors. \n\n\n","33ed24ee":"## Lets submit the submission using XGBoost model ","0fff6cd4":"First, we need to reapply all the preprocessing steps.","307d893d":"## Importing the Libraries","192cc942":"We can now submit this file to the competition: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/submissions\n\n![](https:\/\/i.imgur.com\/6h2vXRq.png)\n","32b6ea88":"### 1. n_neighbors","61577604":"## Making Predictions on the Test Set\n\nLet's make predictions on the test set provided with the data.","6d0fda8a":"Let's save our work before continuing.","7e4edb55":"## Lets use the LIGHT GBM\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n\n- Lower memory usage.\n\n- Better accuracy.\n\n- Support of parallel, distributed, and GPU learning.\n\n- Capable of handling large-scale data.","a844f160":"### Lets model with XGBOOST model using best hyper-parameter as\n- max_depth =5\n- learning_rate =0.25\n- subsample=0.7 ","cb804db8":"### Lets build the model with the best hyper-parameter\n\n1.max_depth = 14\n2.n_estimators = 400\n3.max_features = 0.8\n4.max_samples = 0.8. ","99814ffd":"<font size =4>**Let's also view and plot the feature importances.**","e81e1eba":"**The best value for max_features is 0.8.**","2bb323c5":"As it can be seen the  accuracy score between the training and validation set is comparable approx.5% and hence we can sat that the model is now overfit. So lets reduce the overfit by tuning the hyperparameter.","f2abbed5":"**The best option for max_depth is 4.**","b34668bc":"boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs","06c702be":"### **Conclusion**: Hence it can be seen that the accuracy score for this KNNeighbor model is less than the Random forest because in random forest there is randomisation of decsision tress with random hyperparamter hence reduces the overfit largely and the result can also be seen in the result. ","3e46aee5":"### 1. leaf_size","25309c13":"**Interpretation**\n\n1. As intuited `OverallQual`(Overall material and finish quality) parameters influences the most in estimating the sales price of house .\n\n2. `GrLivArea`(Above grade (ground) living area square feet) seems to be second most influential factor for sale price ,\n\n3. Similarly `GarageCars`(Size of garage in car capacity) also play a critical role in slae price.\n\n4. All the other remaining parametes also seems to have their slight part in sale price .","4848b89a":"Best value for n_estimators is 400.","c1b1eae9":"<font size =4> 3. min_samples_leaf","ce01b338":"Let's save it as a CSV file and download it.","8e47d24c":"### Making Predictions on Single Inputs","7d365728":"<font size =4> 1. Max_Depth","3586f267":"## Hyperparameter Tuning\n\nLet us now tune the hyperparameters of our model. You can find the hyperparameters for `RandomForestRegressor` here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\n\n<img src=\"https:\/\/i.imgur.com\/EJCrSZw.png\" width=\"480\">\n\nHyperparameters are use","a5ab6b6a":"<font size =4> **Lets increase the accuracy score using hyper-paramter tuning.**","3d21197c":"<font size =4> 7. max_samples","49fef5d8":"Let's begin by installing the required libraries.","6d7bc66e":"## Make a Submission\n\nTo make a submission, just execute the following cell:","9c2dfbb3":"Let's define a helper function `knn_clr` which can test the given value of one or more hyperparameters.","21c97c51":"### The best parameter for KNNEighbors is \n- n_neighbors=5.8","164cca5d":"### As it can be seen that that teh accuracy score of the lightGBM modle is 89.06% however for XGBoost model is 87.96% which is a significant increase in the score.","cef3677d":"<font size =4> 5. max_features","d537cc29":"## Reference\n\nI am very grateful to be part of the `JOVIAN` Community which has created several modules for the Data Science , Machine Learning and AI enthusisast you can check out there site [Jovian](https:\/\/jovian.ai\/learn\/data-analysis-with-python-zero-to-pandas)\n\n- One of the reason why the Jovian Community is best because they explained every modules in a very lucid manner which is very easy to understand.\n- The one of the astonishing thing about the jovian is most of their modules are free which is amazing.","ee0af61f":"n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs","a2748ff1":"<font size =4> **Lets reduce the overfit using hyper-paramter tuning.**","0cfa2aec":"**The best option for subsample is 0.7.**","d62a28b7":"Let's save our work before continuing.","7da52287":"We can now make predictions using our final model.","0de76eee":"<font size =4> Hence it can be seen the above best model is overfit by 7.4% however if we look at the accuracy score for the above model without using tuning it was max.88.9% but with tuning it is 89.62% which is a significant increment. ","2a3b6f51":"### Lets tune the hyper-parameter","83208d8d":"**The best option for num_leaves is 10.**","288e47e7":"### Model for best parameter using KNNeighbors","f4e7ea68":"## Lets check the random tree for best model ","d1f9e0bc":"Let's replace the values of the `SalePrice` column with our predictions.","0da86094":"<font size =4> 4. min_samples_split","bff4b7d1":"<font size =4> 6. max_leaf_nodes","0f5d5454":"**The bets option for n_neighbors is 5.8.**","1f0363db":"<font size =4> 1. Max_Depth","1ba7fe60":"## Data fields\nHere's a brief version of what you'll find in the data description file.\n\n- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: $Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale","9df0f0ae":"## Random Forests\n\n<font size =3>**A random forest regressor.**\n\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n\n![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/random-forest-algorithm2.png)","719824ad":"<font size =4> 4. subsample","cfe1058b":"**The best choice for max_samples is 0.8.**","56a2e8f0":"Let's define a helper function `random_forest_clr` which can test the given value of one or more hyperparameters.","2acd3238":"**Lets build the decision tree using graphviz and using 2nd tree estimators.**","bed46352":"### Saving the Model","6eaad526":"<font size =4> 2. n_estimators","7deff5ce":"<font size =4> 2. learning_rate","ea01eb22":"## Download and prepare the dataset for training","634e67f7":"As it can be seen the  accuracy score between the training and validation set is comparable exactly and hence we can say that the model is no overfit. So lets increase the accuracy score by tuning the hyperparameter.","361d1a74":"**The best option for learning_rate is 0.25.**","5129014e":"**The best option for max_depth = 5.**"}}