{"cell_type":{"634c2692":"code","2cbb3cc4":"code","903a30ba":"code","87e3de61":"code","195c6476":"code","3d09eb41":"code","8fa4b150":"code","3fffd9b0":"code","8e0b9e2d":"code","737c6f67":"code","1d011c5e":"markdown","bc4b1cd0":"markdown","98c9a2b0":"markdown"},"source":{"634c2692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))  \n        df=pd.read_json(os.path.join(dirname,filename),lines=True)\n\n# Any results you write to the current directory are saved as output.","2cbb3cc4":"df.category.unique()","903a30ba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.utils import shuffle,resample\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nimport os \n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))  \n        df=pd.read_json(os.path.join(dirname,filename),lines=True)\n\nb1=df['category']=='HEALTHY LIVING'\nb2=df['category']=='SPORTS'\nb3=df['category']==\"SCIENCE\"\nb=np.logical_or(b1,b2)\ndf_filter=df[np.logical_or(b,b3)]\n\ndf_filter.category.value_counts().plot(kind=\"bar\")\n\nencoder=LabelEncoder()\ndf_filter[\"label\"]=encoder.fit_transform(df_filter.category)\ndf_filter=shuffle(df_filter)\ndf_resample=df_filter[df_filter.category==\"HEALTHY LIVING\"] \ndf_resample_science=df_filter[df_filter.category==\"SCIENCE\"].sample(len(df_resample),replace=True)\ndf_resample_sport=df_filter[df_filter.category==\"SPORTS\"].sample(len(df_resample),replace=True)\ndf_resample=pd.concat([df_resample,df_resample_science])\ndf_resample=pd.concat([df_resample,df_resample_sport])\ndf_resample=shuffle(df_resample)\n\n\ntfidfvect=TfidfVectorizer(stop_words=\"english\",max_features=18000)\ndata=tfidfvect.fit_transform(df_resample.short_description)","87e3de61":"pip install yandex-translater","195c6476":"from yandex.Translater import Translater  \n\nlist_fr=[]\n \ndef translate_yendex_en_fr(s): \n    tr=Translater() \n    tr.set_key(\"API-KEY\")\n    tr.set_from_lang(\"en\")\n    tr.set_to_lang(\"fr\")\n    try:\n        tr.set_text(s)\n        f=tr.translate()\n        print(f)\n        return f\n    except Exception as e:\n        return None\n        \n#df_filter[\"short_description_fr\"]=df_filter[\"short_description\"].apply(lambda x:translate_yendex_en_fr(x))\nfor i in range(100):\n    list_fr.append(translate_yendex_en_fr(df_filter[\"short_description\"].iloc[i]))","3d09eb41":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.pipeline import Pipeline\n\nclfs={\"Multinomial Naivs Bayes\":MultinomialNB(),\"Random Forest Classifier\":RandomForestClassifier(),\n      \"GBM Classifier\":GradientBoostingClassifier(),\"MLP Classifier\":MLPClassifier(),\"XGB Classifier\":XGBClassifier()}\nx_train,x_test,y_train,y_test=train_test_split(data,df_resample.label,stratify=df_resample.label,test_size=0.2)","8fa4b150":"for k,clf in clfs.items(): \n    print(k)\n    clfs[k]=clfs[k].fit(x_train,y_train)","3fffd9b0":"from sklearn.metrics import confusion_matrix,classification_report\nimport seaborn as sns \n\ny_preds=[]\nclassification_reports=[]\nfor clf in clfs.values():\n    y_pred=clf.predict(x_test)\n    y_preds.append(y_pred)\n    classification_reports.append(classification_report(y_test,y_pred))","8e0b9e2d":"for clf_report,k in zip(classification_reports,clfs.keys()): \n    print(f\"classifcation report for {k}\")\n    print(clf_report)","737c6f67":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n#d={\"accuracy\":[],\"recall\":[],\"precision\":[],\"f1_score\":[],\"roc_auc\":[]}\nconfusions=[]\nfor y_pred,k in zip(y_preds,clfs.keys()): \n    sns.heatmap(confusion_matrix(y_pred,y_test))\n    plt.title(f\"Metrics Evaluation of{k}\",color=\"white\")\n    plt.show()","1d011c5e":"## Creating ML Models which Classifies articles into 3 distinct categories\n\nIn the Code below we will test the performance of a set of Machine Learning algorithms predefined in the SKlearn python library. \n<ul>\n    <li>Multinomial Naive Bayes<\/li>\n    <li>Random Forest Classifier<\/li>\n    <li>Gradient Boosting Classifier<\/li>\n    <li>Multi Layers Perceptron Classifier<\/li>\n    <li>Extreme Gradient Boosting Classifier (XGBoost)<\/li>\n<\/ul>\n\nThe metrics used to compare the performance of each algorithm are:\n<ul>\n    <li><strong>Accuracy: <\/strong> <em> (TP+TN)\/(TN+TP+FP+FN)<\/em><\/li>\n    <li><strong>Precision: <\/strong> <em> (TP)\/(TP+FN)<\/em><\/li>\n    <li><strong>Recall: <\/strong> <em> (TP)\/(TP+FP)<\/em><\/li>\n    <li><strong>F1-Score: <\/strong> which is the harmonic mean between Precision and Recall<em>2*(Recall*Precision)\/(Recall+Precision)<\/em><\/li>\n<\/ul>\nAll those metrics can be gathered all together using the classification_report from the SKlearn library. We will also use a confusion matrix to visualize all this.","bc4b1cd0":"### Creating A French DataBase fot the 3 categories\n\nFor this Task we will be using the translation api <strong>YenDex<\/strong>. We did just as a demo in case we want to build a French dataset from a set of articles written in English.\n","98c9a2b0":"From the above results, we can conclude that RandomForestClassifier and Multi-Layer Perceptron performed best. With more advanced hardware infrastructure we would have applied a Grid Search to perform a Hyper-Parameter tuning which may result to better performance."}}