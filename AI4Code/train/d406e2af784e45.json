{"cell_type":{"0646f1b6":"code","58dd5e70":"code","0086976c":"code","4c8498fc":"code","0305fefd":"code","d17f888b":"code","0e6a3c60":"code","535844bc":"markdown","92aea387":"markdown","929e805b":"markdown","c5e13600":"markdown","6c5cf525":"markdown","e7c5f082":"markdown","5fe08019":"markdown","2e608a53":"markdown"},"source":{"0646f1b6":"pip install pickledb","58dd5e70":"import pandas\nimport json\nimport csv\nimport tensorflow_hub as hub\nimport numpy as np\nimport pickle\nimport pickledb\nimport random\nimport os","0086976c":"\nclass EmbeddingStore:\n    embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")\n\n    def __init__(self, filename):\n        self.embedding_file = filename\n        self.all_docs_embeddings = self.read_embeddings()\n        if self.all_docs_embeddings is None:\n            print(\"all_docs initialized to None\")\n\n    def print_embeddings(self):\n        if self.all_docs_embeddings is None:\n            print(\"Not initialized\")\n        else:\n            print(self.all_docs_embeddings.shape)\n\n    def save_embeddings(self):\n        f = open(self.embedding_file, 'wb')\n        pickle.dump(self.all_docs_embeddings, f)\n\n    def read_embeddings(self):\n        try:\n            f = open(self.embedding_file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\n    def mergeInOrder(self, filename):\n        f = open(filename, 'rb')\n        try:\n            new_embeddings_ = pickle.load(f)\n        except EOFError:\n            new_embeddings_ = None\n        combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n        print(\"Merged embeddings = {}\".format(len(self.all_docs_embeddings)))\n        self.save_embeddings()\n\n    def add_embeddings(self, new_embeddings_):\n        if self.all_docs_embeddings is None:\n            combined_embeddings_ = new_embeddings_\n        else:\n            combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n\n    def dump(self):\n        self.save_embeddings()\n\n    def add_paras(self, new_paras_):\n        new_embeddings_ = self.embed(new_paras_)\n        self.add_embeddings(new_embeddings_)\n\n    def add_para(self, new_para_):\n        new_embeddings_ = self.embed([new_para_])\n        self.add_embeddings(new_embeddings_)\n\n    def add_para_new(self, new_para_):\n        t = tf.cast(t, tf.string)\n        t = tf.string_split([t], delimiter).values\n        e = self.embed(t)\n        e = tf.reduce_mean(e, axis=0)\n        self.add_embeddings(tf.squeeze(e))\n\n    def find_top_match(self, query):\n        #print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        #print(query_embedding_.shape) \n        #print(query_embedding_[0].shape)\n        #print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        #print(corr)\n        value = np.amax(corr)\n        pos = np.where(corr == value)\n        #print(pos)\n        #print(\"Match = {} at index {}\".format(value, pos[0][0]))\n        return pos[0][0]\n\n    def find_top_k_match(self, query, k):\n        print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        print(query_embedding_.shape)\n        print(query_embedding_[0].shape)\n        print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        print(corr)\n        values = np.argpartition(corr, -k)[-k:]\n        print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\n    def find_top_k_match_with_conf(self, query, k):\n        query_embedding_ = self.embed(query)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        indexes = np.argpartition(corr, -k)[-k:]\n        values = []\n        for i in indexes:\n            values.append((i,corr[i]))\n        #print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\nclass ParaStore:\n\n    def __init__(self, docsfilename, embeddingsfile):\n        self.docfile = docsfilename\n        self.embeddingstore = EmbeddingStore(embeddingsfile)\n        self.all_docs = self.read_para()\n        self.nos_docs = self.all_docs.totalkeys()\n        print(\"There are {} docs in the store\".format(self.nos_docs))\n\n    # Merge the pickledbs \n    def mergeInOrder(self, filename, embeddingsfile):\n        doc2 = pickledb.load(filename, False)\n        rangekey = doc2.totalkeys()\n        for i in range(rangekey):\n            para = doc2.get(str(i))\n            self.all_docs.set(str(self.nos_docs), para)\n            self.nos_docs +=1\n        self.all_docs.dump()\n        self.embeddingstore.mergeInOrder(embeddingsfile)\n\n    def save_para(self):\n        self.all_docs.dump()\n\n    def read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\n    # Changed this function to use the aid instead of text\n    def already_exists(self, new_para):\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if new_para[\"aid\"] == candidate[\"aid\"] and new_para[\"pid\"] == candidate[\"pid\"]:\n                print(\"({},{}) already in the doc store\".format(new_para[\"aid\"], new_para[\"pid\"]))\n            #if new_para[\"aid\"] == self.all_docs.get(key)[\"aid\"]:\n            #    print(\"{} already in the doc store\".format(new_para[\"aid\"]))\n                return True\n        return False\n\n    def add_para(self, new_para):\n        text = new_para[\"text\"]\n        if not self.already_exists(new_para):\n            self.all_docs.set(str(self.nos_docs), new_para)\n            #self.embeddingstore.add_para(text)\n            self.embeddingstore.add_para_new(text)\n            self.nos_docs = self.nos_docs+1\n            if self.nos_docs % 1000 == 0:\n                self.dump()\n\n    def add_paras(self, paras):\n        print(\"Adding {} paras\".format(len(paras)))\n        for i in range(len(paras)):\n            self.add_para(paras[i])\n\n    def dump(self):\n        self.all_docs.dump()\n        self.embeddingstore.dump()\n\n    def get_para(self, nos):\n        #return self.all_docs.get(str(nos+1))\n        return self.all_docs.get(str(nos))\n\n    def get_matching_para(self, query):\n        pos = self.embeddingstore.find_top_match(query)\n        match = self.get_para(pos)\n        return match\n\n    def get_matching_k_para(self, query, k):\n        positions = self.embeddingstore.find_top_k_match(query, k)\n        matches = {}\n        for i in positions:\n            match = self.get_para(i)\n            matches[str(i)] = match\n        return matches\n\n    def get_matching_k_para_with_conf(self, query, k):\n        positions = self.embeddingstore.find_top_k_match_with_conf(query, k)\n        #print(\"Input query = {}\".format(query))\n        matches = []\n        for pos in positions:\n            i = pos[0]\n            match = self.get_para(i)\n            matches.append((pos[0], pos[1], match))\n        return matches\n\n    def filter_k_randomly(self, matches, k):\n        kmatches = {}\n        print(\"Filtering {} from {} matches\".format(k, len(matches)))\n        if len(matches) <= k:\n            return matches\n        else:\n           indexes = random.sample(matches.keys(), k)\n        for i in indexes:\n            kmatches[i] = matches[i]\n        return kmatches\n\n    def applyFilter(self, filt, text):\n        for name in filt:\n            if text and name.lower() in text.lower():\n                return True\n        return False\n\n    def get_including_k_para(self, query, filt, k):\n        matches = {}\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if (query[0] in candidate[\"text\"]) and (self.applyFilter(filt, candidate[\"text\"])):\n                matches[key] = candidate\n        kmatches = self.filter_k_randomly(matches, k)\n        return kmatches","4c8498fc":"def openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef getTopKMatch(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para(query_vector, k)\n    return matches\n\ndef getTopKMatchWithConf(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    #print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para_with_conf(query_vector, k)\n    matches.sort(key = lambda x: x[1], reverse=True)\n    return matches\n\ndef findTitleURL(cord_uid):\n    df = pandas.read_csv(metadatafile)\n    cand = df.loc[df['cord_uid']== cord_uid]\n    title = cand[\"title\"].values[0]\n    url = cand[\"url\"].values[0]\n    return (title,url)\n\ndef findMatches(paragraph, paraFile, embeddingFile):\n    print(\"Paragraph = \" + paragraph)\n    topk = getTopKMatchWithConf(paraFile, embeddingFile, paragraph, 10)  \n    matches = []    \n    for cand in topk:\n        #print(cand)\n        match = {}\n        para = cand[2]\n        cord_uid = para[\"aid\"]\n        title, url = findTitleURL(cord_uid)\n        match[\"title\"] = title\n        match[\"url\"] = url\n        match[\"text\"] = para[\"text\"]\n        match[\"index\"] = str(cand[0])\n        matches.append(match)\n    return matches\n\ndef findSeed(taskFile, paraFile, embeddingFile):\n    seeddir = \"\/kaggle\/working\/seeds\/\"\n    if not os.path.isdir(seeddir):\n        os.mkdir(seeddir)\n    json_file = open(taskFile, 'r')\n    data = json.load(json_file)\n    keys = data.keys()\n    start = taskFile.find(\"Task\")\n    for key in sorted(keys):\n        \n        outfile = taskFile[start:taskFile.find(\"_query\")] + \"_\" + key + \"_match.csv\"\n        #print(\"Outputfile = {}\".format(outfile))\n        csvwriter = csv.writer(open(seeddir + outfile, 'w'))\n        paraText = data[key][\"paraText\"]\n        #print(paraText)\n        matches = findMatches(paraText, paraFile, embeddingFile)\n        for match in matches:\n            csvwriter.writerow([paraText, match[\"text\"], match[\"index\"], match[\"title\"], match[\"url\"]])\n            \n        \nparaFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.db\"\nembeddingFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.pkl\"\ntaskFile = \"\/kaggle\/input\/task-4u\/Task_4_query.json\"\nmetadatafile = \"\/kaggle\/input\/cord19metadata\/metadata.csv\"\n#metadatafile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\n\n\nfindSeed(taskFile, paraFile, embeddingFile)\n","0305fefd":"pip install MulticoreTSNE","d17f888b":"import pickle\nimport pickledb\nimport pandas\nimport sys\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy import sparse\nfrom scipy.sparse.linalg import eigs\nfrom MulticoreTSNE import MulticoreTSNE as TSNE\nfrom sklearn.manifold import SpectralEmbedding\n\n\n\ndef read_embeddings(file):\n        try:\n            f = open(file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\ndef read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\ndef get_para(self, nos):\n        return self.docs.get(str(nos))\n\ndef make_graph(x, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd):\n    '''\n    ***\n    \n    inputs: \n\n    x: the data (np.array)\n    args: args: the input arguments to the script\n    nrmd: if we want the graph matrix to be normalized\n    outputs:\n\n    a: the diffusion matrix (scipy.csr_matrix)\n\n    ***\n    '''\n    n = x.shape[0]\n    print(knnfile)\n    # construct kneighbors graph from data\n\n    if (id(knnfile) == id('')):\n        print('computing knn\\n')\n\n        start_time = time.time()\n        a = kneighbors_graph(x, n_neighbors, mode='distance')\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n        print(a.shape)\n        with open('knn'+str(n_neighbors), 'wb') as wfile:\n            pickle.dump(a,wfile)\n    else:\n        with open('knn'+str(n_neighbors), 'rb') as rfile:\n            a = pickle.load(rfile)                           \n    # simmetrize it \n\n#print(a[[1,2],:])\n    a = a + a.transpose()\n#a = np.expm1(-a)\n#a = a-a.sign()\n#if args.sparse_mode: a = a + sparse.eye(n)\n\n#print(a[[1,2],:])\n\n    if (nrmd):\n\n        # get the un-normalized weight matrix\n        norm = (a * sigma).max(axis=1).todense()\n\n        norm = 1. \/ norm\n        a = - a.multiply(norm)\n        a = a.expm1()\n        a = a - a.sign()\n        if sparse_mode: a = a + sparse.eye(n)\n\n        # get the normalized weight matrix\n        p = a.sum(axis=1)\n        p = 1. \/ p\n        a = a.multiply(p)\n        assert a.sum() == n # sanity check\n    return a\n\n\nclass diff_vecs:  \n    def __init__(self, name, roll):  \n        self.name = name  \n        self.vec = vec \n   \n\n\ndef diffuse_labels(y=None, train_indices=None, g=None, t=1, class_=1):\n    '''\n    ***\n    \n    inputs: \n\n    y: the labels (np.array)\n    train_indices: the indices of the train dataset (list)\n    g: the graph (scipy.csr_matrix)\n    t: how many diffusion steps (int)\n\n    outputs:\n\n    signal: the soft labels from diffusion (np.array)\n\n    ***\n    '''\n    n = len(y)\n\n    # get training data labels and normalize in [-1,1]\n    y = y[train_indices]\n    y = 2 * (y == class_).astype(np.float) - 1\n    # get the signal to diffuse\n    signal = np.zeros((n))\n    signal[train_indices] = y\n\n    # diffuse t times \n    for _ in range(t):\n        signal = g.dot(signal)\n        signal[train_indices] = y\n    return signal\n\ndef plot_diff(title,fig_ax, x, signal,training_idx,text,color):\n    print('Plotting results')\n    fig_ax.scatter(x[:, 0], x[:, 1], s=2**2)\n    heatmap = signal[signal>0]\n    cmap = 'bwr'\n\n\n    for ix in training_idx:\n        print(ix)\n        fig_ax.text(x[ix,0],x[ix,1], str(ix))\n\n    fig_ax.set_title(title, fontsize='x-large')\n    fig_ax.scatter(x[signal>0,0], x[signal>0,1],color = 'red')\n\n    fig_ax.scatter(x[training_idx,0], x[training_idx,1], marker='s',  color = color)\n\n    return(fig_ax)\n\n\ndef findPara(pos, docs):\n    para = docs.get(str(pos))  # uncomment this line if pos is an integer\n    return para['text']\n\n\n# Function to lookup title for notebook\ndef findTitle(pos, paraFile=None, metadataFile=None):\n    if not paraFile:\n        paraFile = \"\/kaggle\/input\/embeddings\/cord19.db\"\n    if not metadataFile:\n        metadataFile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\n        \n    docs = pickledb.load(paraFile, False)\n    para = docs.get(str(pos))\n    docid = para[\"aid\"]\n    metadata = pandas.read_csv(metadataFile)\n    cand = metadata[metadata[\"cord_uid\"] == docid]\n    title = cand[\"title\"].values[0]\n    return docid, title\n\n\ndef main(docs, data, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done):\n      \n    n = data.shape[0]\n    print(n)\n\n    print('starting TSNE')\n    print(TSNE_done)\n    if (TSNE_done == 0):\n#        \tprint('Doing TSNE_para2')\n            X_embedded = TSNE(n_jobs=4).fit_transform(data)\n            with open('tsne_para_para2', 'wb') as wfile:\n                pickle.dump(X_embedded,wfile)\n            plt.scatter(X_embedded[:, 0], X_embedded[:, 1])\n            plt.show()\n    else:\n            with open(TSNE_file, 'rb') as rfile:\n                X_embedded = pickle.load(rfile)\n\n\n    #creating the graph and intiating diffusion:\n    print('Creating graph, and starting diffusion process\\n')\n\n    g=make_graph(X_embedded, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd = 1)\n    df = pandas.read_csv(seedFile,header=None)\n    meds = ['task x']\n    titles = ['tasks xx']\n    colors = ['green','yellow','pink','black','brown','grey']\n    count = 0\n\n    #fig, ax_array = plt.subplots(1, len(meds), sharex=True, sharey=True)\n\n\n    print('diffusion loop')\n    a = []\n    for med in meds:\n        seed_idx = df.values.astype(int)\n        y = np.ones(n, dtype=int)\n        y.astype(int)\n        signal = diffuse_labels(y=y, train_indices=seed_idx, g=g, t=titer, class_=1)\n        a=np.where(signal>0)\n        sd_id_id = np.where((seed_idx) == a[0])\n\n\n        print('recoeverd results indices: '+str(a))\n\n        color = colors[count]\n        count = count+1\n    outfilename = seedFile[seedFile.find(\"Seedfile\"):seedFile.find('.')]\n    outF = open(\"\/kaggle\/working\/diffusion\/\"+ outfilename + \"_res\" +\".txt\", \"w\")\n    print('doing '+ str(seedFile))\n    # print out the interactions\n    for i in a[0]:\n        #print('i'+str(i))\n        #print(type(i))\n        #id, tit = findTitle(i, paraFile='cord19.db', metadataFile='metadata.csv')\n        para = findPara(i, docs)\n        outF.write('id: '+ str(i) + ' and paragraph: ' + para +  '\\n\\n')\n        #print('id: '+ str(i) + ' and paragraph: ' + para +  '\\n')\n        #print('id: '+ id + ' and title: ' + tit +  '\\n')\n\n    #sys.exit(0)\n    print('done')\n    \n# Data files\nparaFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.db\"\nembeddingFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.pkl\"\nmetadatafile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\nTSNE_file = \"\/kaggle\/input\/tsnepara\/tsne_para2\"\nseedDir = '\/kaggle\/input\/task4seeds'\n\n\n# Default parameters\nsigma = 1\nmode = 0\nsparse_mode = 0\nknnfile = ''\nn_neighbors = 4\nTSNE_done = 1\ntiter = 1\n\n\ndef processAllSeeds(dir, paraFile, embeddingFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done):\n    outputdir = \"\/kaggle\/working\/diffusion\/\"\n    if not os.path.isdir(outputdir):\n        os.mkdir(outputdir)\n    docs = pickledb.load(paraFile, False) \n    data = read_embeddings(embeddingFile)\n    for f in os.listdir(dir):\n        if f[0] == '.':\n            continue\n        seedFile = os.path.join(dir,f)\n        print(\"Invoking main with seedFile = {}\".format(seedFile))\n        main(docs, data, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done)\n\nprocessAllSeeds(seedDir, paraFile, embeddingFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done)","0e6a3c60":"import os\nimport csv\n\nresult_dir = \"\/kaggle\/input\/task4result\/\"\noutput_dir = \"\/kaggle\/working\/\"\n\ndef lookupTask4SubTasks(nos):\n    if nos == 1:\n        subtask = \"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\"\n    elif nos == 2:\n        subtask = \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\"\n    elif nos == 3:\n        subtask = \"Exploration of use of best animal models and their predictive value for a human vaccine.\"\n    elif nos == 4:\n        subtask = \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\"\n    elif nos == 5:\n        subtask = \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\"\n    elif nos == 6:\n        subtask = \"Efforts targeted at a universal coronavirus vaccine.\"\n    elif nos == 7:\n        subtask = \"Efforts to develop animal models and standardize challenge studies\"\n    elif nos == 8:\n        subtask = \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\"\n    elif nos == 9:\n        subtask = \"Approaches to evaluate risk for enhanced disease after vaccination\"\n    elif nos == 10:\n        subtask = \"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"\n    else:\n        subtask = \"Illegal subtask nos\"\n    \n    return subtask\n\ndef extractSubtask(infile):\n    index1 = infile.find(\"Subtask\")\n    start = index1 + len(\"Subtask\")\n    end = start+2\n    return infile[start:end]\n\ndef extractResultPara(line):\n    index1 = line.find(\"paragraph:\")\n    start = index1 + len(\"paragraph:\")\n    return line[start:].strip()\n\ndef text2CSV(infile, outfile):\n    csvwriter = csv.writer(open(outfile, 'w'))\n    in_file = open(infile, 'r')\n    subtasknos = extractSubtask(infile)\n    subtask_str = lookupTask4SubTasks(int(subtasknos)) \n    csvwriter.writerow([\"Subtask ID\", \"SubTask Description\", \"Matching Paragraph\"])\n    for line in in_file:\n        if len(line.strip()) <=0:\n            continue\n        match = extractResultPara(line)\n        csvwriter.writerow([subtasknos, subtask_str, match])\n        \n    \ndef convertDiffResultToCSV(dir, outdir):\n    for f in os.listdir(dir):\n        if f[0] == '.':\n            continue\n        infile = os.path.join(dir,f)\n        f = f[f.find('_')+1:]\n        filename = f.split('.')[0] + \".csv\"\n        outfile = os.path.join(outdir, filename)\n        print(\"Input file {} to Output file {}\".format(infile, outfile))\n        text2CSV(infile, outfile)\n\nconvertDiffResultToCSV(result_dir, \".\")","535844bc":"**Encoding**\n\nNext we show the code to compute the USE embeddings of all the paragraphs of the documents and the articles in the CORD-19 data set. This s a computationaly expensive step which takes 2-3 hours. To save time we have precomputed embeddings and uploaded them as input data source.","92aea387":"**DESCRIPTION**\n\nWe have built a pipeline of the following processing modules\n\nETL - extract a representation of each article in the CORD-19 data corpus,\n\nEncoding - compute the Universal Sentence Encoder (USE) encoding of the extracted representation of each article. (This is a computationaly expensive step which takes 2-3 hours. To save time we have precomputed embeddings.\nRepresentation - use the deep-encoding to create a contextual graph\n\nSeed Generation - use the sub-task or a related question to find top matching paragraphs from the CORD-19 data corpus, and sving them in the relevant input folder.\n\nGraph construction  - use dimension reduction via SVD and TSNE to efifcently constrct a semantic graph from the deep represntation.\n\nDiffusion - using the seed as the initial labelled and diffusing their labells on the semantic graph to find contextually similar documents. \n\nReporting - reporting for each sub-tasks all the relevant pargraphs found in the output csv files.","929e805b":"**Diffusion**\n\nThis part entails several steps in recovering the relevant paragraphs for a given sub-task:\n1. Read the deep-features embedded data\n2. Reduce the dimension of the data via SVD and t-SNE\n3. Construct a proximity graph between all the paragrphs respresentation\n4. Starting from the seed nodes diffuse their labels as a random-walk depending on the transition probabilities derived from the simialrity weihgts in the graph.\n5. aggregate all the nodes that have been reached from the seeds via the random-walk\n6. save the corresponding paragraph text of each of the reachable nodes.","c5e13600":"**Presentation and Output**","6c5cf525":"![image.png](attachment:image.png)","e7c5f082":"**GOAL**\n\nCOVID-19 Open Research Dataset Challenge has 10 Tasks where each task has several sub tasks. For each subtask or question related to the subtask we aim to find the most relevant paragraphs that contain answers. The tool can be used to find texts that are contextually associated with specific medical questions, and in partiular with the subtasks specified in the challenge. Moreover it can asnwer various other tasks such as: What is the target of a drug x? Which drugs have similar effect?  Does smoking increase fatality? Are pets susceptible to spill over? \nIn this submission we focus on the Theraputics task and provide our results in the attached files containing all the relvant paragraphs that contain answers to each sub-task specified within this task.","5fe08019":"**Seed Selection**\n\nThis is a manual step where an expert selects the paragraphs that best match the task description. To aid this step we have created a webinterface that allows the expert to go through the paragraphs returned by the seed generation step and select the ones that are best match to the task description. The user can find more matches by using the seeds as input to the seed generation phase. Example seed file for Task4 Subtask 7:\n\n..\/input\/task1seeds\/Seedfile_Task_4_Subtask07.csv\n\n330640\n992837\n947534\n\nThe selected seeds are then used as input to our diffusion step as described below.","2e608a53":"**Seed Generation**\n\nWe extracted the task and subtask descriptions and uploaded them as a data source (task-4-query). The following module take each task description (and subtasks within), compute their USE embeddings and finds top-10 semantically similar paragraphs in the USE embedding space using COSINE similarity. As demonstrated in the example below, this step itself yields interesting and relevant results for many of the subtasks."}}