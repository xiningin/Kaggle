{"cell_type":{"8ca781fc":"code","53f884e0":"code","d065ce63":"code","98be0228":"code","ac79f86c":"code","e169d22c":"code","a0da3ef8":"code","68eca8f4":"code","0b1e4b22":"code","d01e5ca7":"code","6427ccb9":"code","60ff7a80":"code","ac77c8e3":"code","8208065c":"code","88fe2416":"code","fe46ee64":"code","d4a46da8":"code","af2bfc10":"code","cf53d8a5":"code","0ab02406":"code","c750f5b0":"code","76384d21":"code","0684eae4":"code","d215dd24":"code","7a6ea8bc":"code","03b538e2":"code","cb2c2619":"code","141a6310":"code","a7bc717c":"code","9d85bbe1":"code","1e7b77ff":"code","e441c44c":"code","af14f99a":"code","51a8de1b":"code","520267bf":"code","a6357611":"code","1e7928cd":"code","21140de4":"code","1b754264":"code","21edf49b":"code","5be15f4f":"code","decbaebd":"code","a44d4522":"code","c9182826":"code","1c25c15b":"code","8d289cae":"code","9880f2d5":"code","f70052fd":"markdown","e85c67ff":"markdown","b0104091":"markdown","e7c55c03":"markdown","48d09639":"markdown","6847aa2b":"markdown","fbcb26a8":"markdown","0964c22e":"markdown","03feb7ce":"markdown","74210a27":"markdown","69ec5730":"markdown","85dcc47f":"markdown","15302726":"markdown","ca489232":"markdown","9078407e":"markdown","7cb1d811":"markdown","29223802":"markdown","81634d1d":"markdown","ce02434e":"markdown","7cad02b1":"markdown","cf2826d0":"markdown","b4f00d3b":"markdown","768f6c8f":"markdown","ddbf6933":"markdown","164a9eda":"markdown","83c8736f":"markdown","ee6d520b":"markdown","6d932bfb":"markdown","df6cfcbd":"markdown","0c8af821":"markdown","67844b18":"markdown","66f61e9b":"markdown","e62313ca":"markdown","86a54918":"markdown","3c1880b6":"markdown","cacb6dee":"markdown","d9132f60":"markdown","47f8eca8":"markdown","95fcab27":"markdown","ae38536c":"markdown","b6d225b9":"markdown","68221a79":"markdown","e387347c":"markdown","2ed37971":"markdown","12e28cd2":"markdown","cdf0b124":"markdown","eab4bdf1":"markdown","736d2621":"markdown","a71abec2":"markdown","dbd830a5":"markdown","1f66d12d":"markdown","bfde5191":"markdown","b9a13196":"markdown","2109d84f":"markdown","8e59e984":"markdown","e40563c3":"markdown","a311d98d":"markdown","ed3ca70a":"markdown","d65d055f":"markdown","ba9a309a":"markdown","0aab0dce":"markdown","e9b356f7":"markdown","21b1587e":"markdown","b0f26cb6":"markdown","17b2ab0f":"markdown","d58fa208":"markdown","a215ba36":"markdown","6d00af06":"markdown","c00c139e":"markdown","f1cd4971":"markdown"},"source":{"8ca781fc":"# Data Analysis\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Statistics\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n# ML\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xg\n\n# Another\nimport warnings\nwarnings.filterwarnings('ignore')","53f884e0":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d065ce63":"train_data.columns.values","98be0228":"print(train_data.head())\nprint('-'*20)\nprint(train_data.info())","ac79f86c":"# Now let's put each category in it's own dataframe and then collect them\n# I will make the code very easy to understand so don't worry if you're beginner\n\nfull = pd.DataFrame()\nmedium = pd.DataFrame()\nremove_me = pd.DataFrame()\n\nfeatures = train_data.columns.values\nnumber_of_houses = 1460 # try writing it train_data.shape[0]\n\nfor feature in features:\n    if train_data[feature].count() == number_of_houses:\n        full[feature] = train_data[feature]\n    elif train_data[feature].count() > number_of_houses * 0.5: # Actually, that mean it has more than 50% non-null values\n        medium[feature] = train_data[feature]\n    else:\n        remove_me[feature] = train_data[feature]","e169d22c":"print('Number of numerical features: ', end='')\nprint(len(train_data.select_dtypes(include=['number']).columns.values))\ntrain_data.describe(exclude=['O'])","a0da3ef8":"print('Number of categorical features: ', end='')\nprint(len(train_data.select_dtypes(include=['O']).columns.values))\ntrain_data.describe(include=['O'])","68eca8f4":"plt.hist(train_data['SalePrice'])\nplt.title('Sale Prices')\nplt.show()","0b1e4b22":"#1\ntrain_data = train_data.drop(['Id'], axis=1)\n\n#2\ntrain_data = train_data.drop(remove_me.columns.values, axis=1)\n\n#3\n\n# First let's create the important data we will use\nnumerical_data = train_data.select_dtypes(include=['number'])\ncategorical_data = train_data.select_dtypes(include=['object'])\n\n# we want to know the ratio of (values equals zero) \/ 1460\n# to each feature and if the feature has more than 50% ratio we will remove it\nfeature_zero_ratio = {feature:numerical_data.loc[numerical_data[feature] == 0, feature].count() \/ 1460 for feature in numerical_data.columns.values}\nfeature_zero_ratio","d01e5ca7":"for feature in numerical_data:\n    if feature_zero_ratio[feature] > 0.30:\n        numerical_data = numerical_data.drop([feature], axis=1)\n        train_data = train_data.drop([feature], axis=1)\n        if feature in medium:\n            medium = medium.drop([feature], axis=1)\n\ntrain_data.shape","6427ccb9":"print(numerical_data.columns.values)\nprint(len(numerical_data.columns.values))\n\ncorrmat = numerical_data.corr()\nfig, ax = plt.subplots(figsize=(12, 12))\nsns.set(font_scale=1.25)\nsns.heatmap(corrmat, vmax=.8, annot=True, square=True, annot_kws={'size': 8}, fmt='.2f')\nplt.show()","60ff7a80":"n = 10\nmost_largest_features = corrmat.nlargest(n, 'SalePrice')['SalePrice'].index\nzoomed_corrmat = np.corrcoef(numerical_data[most_largest_features].values.T)\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.set(font_scale=1)\nsns.heatmap(zoomed_corrmat, annot=True, square=True, fmt='.2f', annot_kws={'size':10}, yticklabels=most_largest_features.values, xticklabels=most_largest_features.values)\nprint(most_largest_features.values)","ac77c8e3":"sns.set()\nmost_largest_features = corrmat.nlargest(7, 'SalePrice')['SalePrice'].index\nsns.pairplot(numerical_data[most_largest_features.values], size=1.5)\nplt.show()\nprint(most_largest_features)","8208065c":"numerical_data = numerical_data.drop(['1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageYrBlt'], axis=1)\ntrain_data = train_data.drop(['1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageYrBlt'], axis=1)\nnumerical_data.columns.values","88fe2416":"corr_with_price = numerical_data.corr()\ncorr_with_price = corr_with_price.sort_values(by= 'SalePrice', ascending=False)\ncorr_with_price['SalePrice']","fe46ee64":"numerical_data.columns.values","d4a46da8":"numerical_have_missing = pd.DataFrame()\ncategorical_have_missing = pd.DataFrame()\n\n# Numerical\nfor feature in numerical_data.columns.values:\n    if feature in medium:\n        numerical_have_missing[feature] = numerical_data[feature]\n      \n    \n# Categorical\nfor feature in categorical_data.columns.values:\n    if feature in medium:\n        categorical_have_missing[feature] = categorical_data[feature]\n","af2bfc10":"print(numerical_have_missing.columns.values)\nprint('-'*30)\nprint(numerical_have_missing.info())","cf53d8a5":"sns.histplot(numerical_have_missing['LotFrontage'])\nplt.title('LotFrontage')\nplt.show()","0ab02406":"# Let's code it\nold_LotFrontage = list(numerical_have_missing['LotFrontage'].values)\nmissing_indices = list(numerical_have_missing.loc[numerical_have_missing['LotFrontage'].isnull(), 'LotFrontage'].index)\nrandom_values = [random.randint(60, 80) for _ in range( 1460 - numerical_have_missing['LotFrontage'].count() ) ]\nrandom_values_idx = 0\n\nfor missing_idx in missing_indices:\n        \n    old_LotFrontage[missing_idx] = random_values[random_values_idx]\n    random_values_idx += 1\n        \n\nnumerical_have_missing['LotFrontage'] = pd.Series(old_LotFrontage)\ntrain_data['LotFrontage'] = pd.Series(old_LotFrontage)\n\nprint(numerical_have_missing['LotFrontage'].count())\nprint(train_data['LotFrontage'].count())\n","c750f5b0":"print(len(categorical_have_missing.columns.values))\nprint('-'*30)\nprint(categorical_have_missing.columns.values)\nprint('-'*30)\nprint(categorical_have_missing.count())","76384d21":"train_data = train_data.drop(['FireplaceQu'], axis=1)\ncategorical_have_missing = categorical_have_missing.drop(['FireplaceQu'], axis=1)","0684eae4":"imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\nfor feature in categorical_have_missing:\n    \n    categorical_have_missing[feature] = imputer.fit_transform(categorical_have_missing[feature].values.reshape((-1, 1)))\n    train_data[feature] = imputer.fit_transform(train_data[feature].values.reshape((-1, 1)))","d215dd24":"print(len(categorical_have_missing.columns.values))\nprint('-'*30)\nprint(categorical_have_missing.columns.values)\nprint('-'*30)\nprint(categorical_have_missing.count())","7a6ea8bc":"plt.scatter(train_data['GrLivArea'], train_data['SalePrice'])\nplt.show()","03b538e2":"train_data[ (train_data['GrLivArea'] > 4000) & (train_data['SalePrice'] < 200000)].index","cb2c2619":"# Let's return Id and remove it later.\ntrain_data['Id'] = pd.Series(train_data.index)","141a6310":"train_data = train_data.drop( train_data[ (train_data['Id'] == 1298) | (train_data['Id'] == 523) ].index)","a7bc717c":"test_data = test_data[train_data.drop(['SalePrice'], axis=1).columns.values]\n# Delete Id again\ntrain_data = train_data.drop(['Id'], axis=1)","9d85bbe1":"plt.scatter(train_data['GrLivArea'], train_data['SalePrice'])\nplt.show()\nprint(train_data.shape)","1e7b77ff":"train_data = pd.get_dummies(train_data)","e441c44c":"sns.distplot(train_data['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","af14f99a":"train_data['SalePrice'] = np.log(train_data['SalePrice'])\nsns.distplot(train_data['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","51a8de1b":"sns.distplot(train_data['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","520267bf":"train_data['GrLiveArea'] = np.log(train_data['GrLivArea'])\nsns.distplot(train_data['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","a6357611":"train_data.shape","1e7928cd":"target = train_data['SalePrice']\ntrain_data = train_data.drop(['SalePrice'], axis=1)\n\nX, y = train_data, target","21140de4":"lin_reg = LinearRegression()\nlin_reg.fit(X, y)\npred = lin_reg.predict(X)\nprint(lin_reg.score(X, y))\nnp.sqrt(mean_squared_log_error(pred, y))","1b754264":"forest_reg = RandomForestRegressor()\nforest_reg.fit(X, y)\npred = forest_reg.predict(X)\nprint(forest_reg.score(X, y))\nnp.sqrt(mean_squared_log_error(pred, y))","21edf49b":"xg_reg = xg.XGBRegressor(objective ='reg:linear',\n                  n_estimators = 300, seed = 123)\nxg_reg.fit(X, y)\npred = xg_reg.predict(X)\nprint(xg_reg.score(X, y))\nnp.sqrt(mean_squared_log_error(pred, y))","5be15f4f":"print(test_data.head())\nprint('-'*20)\nprint(test_data.info())\n","decbaebd":"sns.histplot(numerical_have_missing['LotFrontage'])\nplt.title('LotFrontage')\nplt.show()","a44d4522":"# Let's code it\nold_LotFrontage = list(test_data['LotFrontage'].values)\nmissing_indices = list(test_data.loc[test_data['LotFrontage'].isnull(), 'LotFrontage'].index)\nrandom_values = [random.randint(60, 80) for _ in range( 1460 - test_data['LotFrontage'].count() ) ]\nrandom_values_idx = 0\n\nfor missing_idx in missing_indices:\n        \n    old_LotFrontage[missing_idx] = random_values[random_values_idx]\n    random_values_idx += 1\n        \n\ntest_data['LotFrontage'] = pd.Series(old_LotFrontage)\nprint(test_data['LotFrontage'].count())","c9182826":"imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\nfor feature in test_data.drop(['LotFrontage'], axis=1):\n    \n    test_data[feature] = imputer.fit_transform(test_data[feature].values.reshape((-1, 1)))\n    test_data[feature] = imputer.fit_transform(test_data[feature].values.reshape((-1, 1)))\n    \ntest_data = pd.get_dummies(test_data)\nprint(test_data.isnull().sum().max())\nprint(test_data.shape)","1c25c15b":"print(test_data.head())\nprint('-'*20)\nprint(test_data.info())","8d289cae":"\nrest = set(X.columns.values) - set(list(test_data.columns.values))\nfor feature in list(rest):\n    test_data[feature] = 0","9880f2d5":"ids = test_data['Id']\ntest_data = test_data.drop(['Id'], axis=1)\nourPred = xg_reg.predict(test_data)\nsubmission = pd.DataFrame({\n        \"Id\": ids,\n        \"SalePrice\": ourPred\n    })\nsubmission.to_csv('submission.csv', index=False)","f70052fd":"# **As we can see here there are a lot of features** \n**we can classify it by two ways:**\n   * number of non-null values\n   * categorical & numerical\n   \n   ","e85c67ff":"**LinearRegression**","b0104091":"*Now we want to remove those two houses who have those* **outliers**.","e7c55c03":"Let's do the same with '**GrLivArea**'","48d09639":"**XGBRegressor**","6847aa2b":"![istockphoto-493172515-612x612.jpg](attachment:32395a82-66a0-41da-ab55-c6ed083c890f.jpg)","fbcb26a8":"![Inkeddownload_LI.jpg](attachment:b36ee925-3e7a-4b82-858c-e8ff30d6f48d.jpg)","0964c22e":"![cartoon-thief-walking-carefully-vector-23539704.jpg](attachment:83b152d2-9bc3-4916-b248-137eac442153.jpg)","03feb7ce":"> # ***Normality (Some Statistics)***","74210a27":"*Let's go to the next step*","69ec5730":"**so we want remove houses which have the indices of [523, 1298]**","85dcc47f":"**RandomForestRegressor**","15302726":"**Our target**","ca489232":"*So the best Model is* **XGBRegressor**","9078407e":"> # ***Outliers !***","7cb1d811":"**let's try to reduce the number of features by know the meaningless features**","29223802":"![R.jpg](attachment:adc28a90-186b-4b4b-b20b-aaa52a479620.jpg)","81634d1d":"**Let's see what we have:**\n  * we knew the relationship between each feature and the target\n  * we knew also the relationship among the features\n  * we reduce the number of numerical features from 38 to 17\n  ","ce02434e":"**So we have just two numerical features with missing values**","7cad02b1":"**we can see that some features have a linear relationship so we can remove any of them let's remove the lowest one have relation with target:**\n  * GrLivArea and 1stFlrSF --> the removed one (1stFlrSF)\n  * GrLivArea and TotalBsmtSF --> the removed one (TotalBsmtSF)\n  * GarageCars and GarageArea --> the removed one (GarageArea)\n  * YearBuilt and GarageYrBlt --> the removed one (GarageYrBlt)","cf2826d0":"Same **problem**, so same **solution**","b4f00d3b":"> # ***ML***","768f6c8f":"**Last thing from these graphs and heatmaps: we will remove the nutral features**\n*     *Nutral Features : features have a relationship with 'SalePrice' in this range [-0.1, 0.2]*","ddbf6933":"**Now let's focus on categorical data with missing values**","164a9eda":"**Categorical**","83c8736f":"**let's begin with number of non-null values:**\n1. full (from it's name)\n2. medium (have some missing data)\n3. Remove_me (data have more than 50% missing data)\n\n**Note:**\n*I think there is forth category called funny which have one feature (PoolQC) if you don't understand me check it*","ee6d520b":"> # **Submission**","6d932bfb":"**According to those to histogram, why don't we give the missing values that:**\n  * *LotFrontage --> random value from 60 to 80*\n  \n","df6cfcbd":"Now as you can see we have to outliers and .... **what ?! really you don't see them**, *ooh man why are you strange, okay I will make it easy to you now see this.*","0c8af821":"*From the graphs above there is an outliers with 'GrLivArea'*, **what you will not go and check it !!**, *okay I will plot it again.* ","67844b18":"> # Explore","66f61e9b":"**Load Project Data**","e62313ca":"# **Now let's see what we have:**\n  * we knew the features that should be removed and put them in a dataframe called 'remove_me'\n  * we knew the features that we must complete them and put them in a dataframe called 'medium'\n  * we knew that categorical data have a small number of unique value and that will help us use get_dummies() method instead of other hard tools\n  * we knew that Id means nothing","86a54918":"**Let's zoom in to the most 10 largest relationship**","3c1880b6":"* **Histogram** - *Kurtosis and skewness.*\n* **Normal probability plot** - *Data distribution should closely follow the diagonal that represents the normal distribution.*","cacb6dee":"> # ***Final***","d9132f60":"***Now let's drop features having more than 0.30 ratio***","47f8eca8":"As we can see 'SalePrice' is **not normal distributed** has **positive skewness** and don't follow the diagonal line, \nBut we can solve that easily by take the log of it.","95fcab27":"# **So our data are prepared**\n*let's begin with numerical*","ae38536c":"*I will leave the resources of that in the comments, but just focus:*\n  * **Normality** - *When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.*\n\n  * **Homoscedasticity** - *I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.*","b6d225b9":"![istockphoto-524018986-612x612.jpg](attachment:cb54b716-0f1a-4dc2-9fe7-0a0e0fb0347e.jpg)","68221a79":"![pngtree-green-check-mark-icon-flat-style-png-image_1986021.jpg](attachment:7e83a49f-9964-40b8-aafb-427dbd6e47d4.jpg)","e387347c":"**All features' names we don't know will be in the 'data_description.txt' file, so do check it**","2ed37971":"**Now let's impute the missing data which is in the 'medium' we can do that with several ways:**\n * numerical\n   1. give it the value of the median\n   2. give it the value of the mean\n   3. give it a random value from (mean - std) to (mean + std)\n * categorical\n   1. give it the value of the mode (the most appeared value)\n   2. do some analysis more, then do step 1 to decrease the ratio of the wrong values","12e28cd2":"**Numerical**","cdf0b124":"![OIP.jpg](attachment:dd6431d3-b641-4e64-93e3-7104ac735cc6.jpg)","eab4bdf1":"***So there is no missing values any more***","736d2621":"**we will drop:**\n  1. id -> because it means nothing\n  2. all features in remove_me DataFrame\n  3. tricky features: the features that tells you that it hasn't a lot of missing values but most of it's values are equals to zero \n*(ex: when you tell me what is the area of your pool and I say zero)*","a71abec2":"# Ok, as you read this is a simple solution for predicting prices\n> **The Steps(Pipeline) we will do here:**\n>    1. *Clarify features' names and make some thoughts about them (we will see which of them may be true in EDA)*\n>    2. *Explore our data as quick lookup*\n>    3. *Exploratory Data Analysis (EDA)*\n>    4. *Make Some Decisions*\n>    5. *Clean and handle missing data*\n>    6. *Prepare our data for ML (Transform strings into categorical data but integers or floats)*\n>    7. *Normality*\n>    8. *ML*","dbd830a5":"![research-500px.jpg](attachment:de2d58e1-e1ce-4e74-8b1b-c1987ec2a621.jpg)","1f66d12d":"**Now we want to see the relationship between numerical data and the target**","bfde5191":"**Let's check it.**","b9a13196":"![1871787-escapegames24-connections-puzzle.jpg](attachment:efd5b7db-cce1-4239-b19e-82d8ee30d0a7.jpg)","2109d84f":"**Check the you-don't-know feature's name from that text file  '..\/input\/house-prices-advanced-regression-techniques\/data_description.txt'**","8e59e984":"*We will remove '**FireplaceQu**' as it has a lot of missing values and **impute** the last values with the **most frequent** value*","e40563c3":"**Now let's see the relations of the seven largest in graph (scatter plot)**","a311d98d":"![istockphoto-866429562-612x612.jpg](attachment:6afd72f2-3736-48b4-bdc1-55138e6b6dc9.jpg)","ed3ca70a":"![R (1).png](attachment:94d15678-64ce-48e7-bf0e-65eae671409d.png)","d65d055f":"# **Let's Go on** ","ba9a309a":"![OIP (1).jpg](attachment:5a32a85f-1f3f-4a40-bed2-e31612745f01.jpg)","0aab0dce":"![carpenter-clipart-diy-man-3.jpg](attachment:abf2d3ce-c20e-4055-ab3a-eeb23f649d16.jpg)","e9b356f7":"**now, let's classify them as numerical and categorical**:\n  * Numerical (refers to value)\n  * Categorical (string refers to string or number refers to string)","21b1587e":"> # ***Dummies (change the categorical into it's equivalent numerical)***","b0f26cb6":"> # Exploratory Data Analysis (EDA)","17b2ab0f":"![986.jpg](attachment:dd3318d1-abfd-4fec-9c27-b8f29f258a68.jpg)","d58fa208":"**Importing Libraries & Modules**","a215ba36":"> # Features' names quick look","6d00af06":"> # **Handling Missing Data**","c00c139e":"the coming kernels is for my submission, you can ignore","f1cd4971":"**Check again**"}}