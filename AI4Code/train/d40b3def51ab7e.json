{"cell_type":{"a60c244c":"code","b86d7357":"code","1f6ae393":"code","82bd7ff2":"code","7cdae487":"code","3605de4f":"code","cae53c9e":"code","026f7e2c":"code","780fc4a9":"code","21e995ae":"code","6187d055":"code","dfd962b4":"code","792e5a4f":"code","cda6d15a":"code","46964c37":"code","58b3aa0d":"code","fcb41214":"code","653282e6":"code","a06f5d16":"code","a176f12e":"code","c96cc053":"code","f6d85986":"code","11545a1c":"code","5316b68f":"code","e9a0ad87":"code","ca151458":"code","848c66b6":"code","2f39a957":"code","51b1820f":"code","c853ef86":"code","ebfd7d02":"code","6aef41b7":"code","35b7b23c":"code","dc708c39":"code","6e477fcc":"code","cbfcda5d":"code","5c2dd04e":"code","d0abd070":"code","cfe75f99":"markdown","40c8cf71":"markdown","8638f002":"markdown","3552e1e8":"markdown","590852e5":"markdown","479c6a06":"markdown","444a659e":"markdown","9a003063":"markdown","268ff914":"markdown","83395e3f":"markdown","5f55e7d1":"markdown","8baca476":"markdown","fa2c47e1":"markdown","6919a654":"markdown","eb25bd5d":"markdown","cf40bdcb":"markdown","0c7391d3":"markdown","3bc5d17a":"markdown","ade3c781":"markdown","eb545724":"markdown","c8b17274":"markdown","a2f9bf42":"markdown","6a911cc1":"markdown","57a2003b":"markdown","6267a6fa":"markdown","6bd6a5dc":"markdown","37a08b7c":"markdown","622b9095":"markdown","66b50fae":"markdown","b2d1f967":"markdown","4b598b98":"markdown","b5cc255a":"markdown","c78ea429":"markdown","13c71ed2":"markdown","9dc2b143":"markdown"},"source":{"a60c244c":"\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style = 'darkgrid') #\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 25, 12.5\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","b86d7357":"train.info()","1f6ae393":"categorical = []\nnumerical = []\nfor feature in test.columns:\n    if test[feature].dtype == object:\n        categorical.append(feature)\n    else:\n        numerical.append(feature)\ntrain[categorical].head()","82bd7ff2":"train[numerical].isnull().sum().sort_values(ascending = False).head(8)","7cdae487":"test[numerical].isnull().sum().sort_values(ascending = False).head(8)","3605de4f":"train[['meaneduc', 'SQBmeaned']].describe()","cae53c9e":"train['meaneduc'].fillna(train['meaneduc'].mean(), inplace = True)\ntrain['SQBmeaned'].fillna(train['SQBmeaned'].mean(), inplace = True)\n#the same for test\ntest['meaneduc'].fillna(test['meaneduc'].mean(), inplace = True)\ntest['SQBmeaned'].fillna(test['SQBmeaned'].mean(), inplace = True)\ntrain['rez_esc'].fillna(0, inplace = True)\ntrain['v18q1'].fillna(0, inplace = True)\ntrain['v2a1'].fillna(0, inplace = True)","026f7e2c":"sns.set(style = 'darkgrid')\nsns_plot = sns.palplot(sns.color_palette('Accent'))\nsns_plot = sns.palplot(sns.color_palette('Accent_d'))\nsns_plot = sns.palplot(sns.color_palette('CMRmap'))\nsns_plot = sns.palplot(sns.color_palette('Set1'))\nsns_plot = sns.palplot(sns.color_palette('Set3'))","780fc4a9":"target_values = train['Target'].value_counts()\ntarget_values = pd.DataFrame(target_values)\ntarget_values['Household_type'] = target_values.index\ntarget_values","21e995ae":"mappy = {4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"}\ntarget_values['Household_type'] = target_values.Household_type.map(mappy)\ntarget_values","6187d055":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\nax = sns.barplot(x = 'Household_type', y = 'Target', data = target_values, palette='Accent', ci = None).set_title('Distribution of Poverty in Households')","dfd962b4":"#Let's find out largest correlations and depict them\ncorrs = train.corr().abs()\ncorrs1 = corrs.unstack().drop_duplicates()\nstrongest = corrs1.sort_values(kind=\"quicksort\", ascending = False)\nstrongest1 = pd.DataFrame(strongest)\ntemp = strongest1.index.values\nfirst_cols = [i[0] for i in temp]\nsecond_cols = [j[1] for j in temp]\ntotal_cols_corr = list(set(first_cols[:20] + second_cols[:20]))\nstrongest.head(25)","792e5a4f":"corr = train[total_cols_corr].corr()\nsns.set(font_scale=1)\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nf, ax = plt.subplots(figsize=(25, 12.5))\nsns.heatmap(corr, cmap=cmap, annot=True, ax=ax, fmt='.2f')","cda6d15a":"train['v2a11'] = train.v2a1.apply(lambda x: np.log(x+1))\nsns.set(font_scale=1, style=\"darkgrid\")\nc =  sns.color_palette('spring_d')[4]\nsns_jointplot = sns.jointplot('age', 'meaneduc', data=train, kind='kde', color=c, size=6)","46964c37":"for i in range(1, 5):\n    sns.set(font_scale=1, style=\"white\")\n    c =  sns.color_palette('spring_d')[i]\n    sns_jointplot = sns.jointplot('age', 'meaneduc', data=train[train['Target'] == i], kind='kde', color=c, size=6, stat_func=None)","58b3aa0d":"def plot_distribution(df, var, target, **kwargs):\n    row = kwargs.get('row', None)\n    col = kwargs.get('col', None)\n    facet = sns.FacetGrid(df, hue = target, aspect = 4, row = row, col = col)\n    facet.map(sns.kdeplot, var, shade = True)\n    facet.set(xlim = (0, df[var].max()))\n    facet.add_legend()\n    plt.show()","fcb41214":"#select some columns\nnumerical1 = ['v2a11', 'meaneduc', 'overcrowding'] #monthly pay rent, mean education, overcrowd\nfor numy in numerical1:\n    plot_distribution(train, numy, 'Target')\n#In the first graph instead of 0's should be nulls(we changed these before). So there is no info about monthly rate payment for non vulnerable households ","653282e6":"f, ax = plt.subplots(figsize=(20, 10))\nsns.boxplot(x='Target', y = 'r4h3',ax = ax, data = train, hue = 'Target' )\nax.set_title('Number of men in households', size = 25)","a06f5d16":"f, ax = plt.subplots(figsize=(20, 10))\nsns.boxplot(x='Target', y = 'r4m3',ax = ax, data = train, hue = 'Target' )\nax.set_title('Number of women in households', size = 25)","a176f12e":"ninos = train.groupby(by = 'Target')['hogar_nin', 'Target'].sum()\nninos = pd.DataFrame(ninos)\nninos['mean_children'] = (ninos['hogar_nin']\/ninos['Target'])\nninos['Target1'] = ninos.index.map({4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"})\nsns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(16, 8))\nax = sns.barplot(x = 'Target1', y = 'mean_children', data = ninos, palette='Pastel1', ci = None).set_title('Mean number on children in different households')","c96cc053":"train['v2a1'].replace(0, np.nan, inplace = True)\ntrain[\"v2a1\"] = train.groupby(\"Target\").transform(lambda x: x.fillna(x.median()))\nrpd = pd.DataFrame([train['v2a1']\/train['hogar_total'], train['Target']]).T\nrpd['Target'] = rpd['Target'].map({4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"})\nrpd.groupby(by = 'Target').mean()","f6d85986":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(16, 8))\nax = sns.barplot(x = 'Target', y = 'Unnamed 0', data = rpd, palette='Pastel1',order = [\"Extereme Poverty\",\"Vulnerable\",\"Moderate Poverty\", \"NonVulnerable\"], ci = None).set_title('Montly rent payment per dweller')","11545a1c":"#visualization of feature importance of XGB below\n\n\nvaluez = ['meaneduc', 'age', 'qmobilephone','Target', 'r4t3', 'tamhog', 'escolari', 'overcrowding']\ntra = pd.melt(train[valuez], \"Target\", var_name=\"measurement\")\nf, ax = plt.subplots()\nsns.despine(bottom=True, left=True)\nsns.stripplot(x=\"value\", y=\"measurement\", hue=\"Target\",\n              data=tra, dodge=True, jitter=True,\n              alpha=.05, zorder=1)\nsns.pointplot(x=\"value\", y=\"measurement\", hue=\"Target\",\n              data=tra, dodge=.532, join=False, palette=\"dark\",\n              markers=\"x\", scale=1, ci=None)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles[4:], labels[4:], title=\"Target\",\n          handletextpad=0, columnspacing=1,\n          loc=\"lower right\", ncol=1, frameon=True)\n","5316b68f":"from sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nscaler1 = StandardScaler()\nX_scaled = scaler1.fit_transform(train[numerical])","e9a0ad87":"%%time\ntsne2d = TSNE(random_state=13012)\ntsne_representation2d = tsne2d.fit_transform(X_scaled)","ca151458":"%%time\ntsne3d = TSNE(n_components = 3, random_state = 666)\ntsne_representation3d = tsne3d.fit_transform(X_scaled)","848c66b6":"tsne_representation2d = pd.DataFrame(tsne_representation2d, columns = ['First_col', 'Second_col'])\ntsne_representation2d['Target'] = train.loc[:, 'Target']","2f39a957":"tsne_representation3d = pd.DataFrame(tsne_representation3d, columns = ['First_col', 'Second_col', 'Third_col'])\ntsne_representation3d['Target'] = train.loc[:, 'Target']","51b1820f":"sns.set(font_scale=1, style=\"darkgrid\") #CMRmap_r\nsns.lmplot( x=\"First_col\", y=\"Second_col\", data=tsne_representation2d, fit_reg=False, hue='Target', legend=False, palette=\"Set1\", size = 17)\nplt.legend(loc='lower right')","c853ef86":"from matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\nrcParams['figure.figsize'] = 30, 20\nfig = pyplot.figure()\nax = Axes3D(fig)\n\nax.scatter(tsne_representation3d.loc[:, 'First_col'], tsne_representation3d.loc[:, 'Second_col'], tsne_representation3d.loc[:, 'Third_col'], s = 29, c = tsne_representation3d.loc[:, 'Target'],\n          edgecolors = 'black')\nax.set_title('t-SNE visualization in 3 dimensions', size = 20)\npyplot.show()","ebfd7d02":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb","6aef41b7":"y = train['Target']\ntrain = train.drop(['Id', 'Target'] ,axis = 1)\ntrain = train.select_dtypes(exclude=['object'])\ntest = test.drop('Id',axis = 1)\ntest = test.select_dtypes(exclude=['object'])","35b7b23c":"y.value_counts()","dc708c39":"y = y - 1\nX_train, X_test, y_train, y_test = train_test_split(train, y, stratify = y, test_size = 0.3, random_state = 666)","6e477fcc":"y.value_counts()","cbfcda5d":"from sklearn.metrics import f1_score\ndef evaluate_macroF1(true_value, predictions):  \n    pred_labels = predictions.reshape(len(np.unique(true_value)),-1).argmax(axis=0)\n    f1 = f1_score(true_value, pred_labels, average='macro')\n    return ('macroF1', f1, True) \nparams = {\n        \"objective\" : \"multi:softmax\",\n        \"metric\" : evaluate_macroF1,\n        \"n_estimators\": 100,\n        'max_depth' : 9,\n        \"learning_rate\" : 0.23941,\n        'max_delta_step': 2,\n        'min_child_weight': 9,\n        'subsample': 0.72414,\n        \"seed\": 666,\n        'num_class': 4,\n        'silent': True\n    }\nxgbtrain = xgb.DMatrix(X_train, label=y_train)\nxgbval = xgb.DMatrix(X_test, label=y_test)\n\n\nwatchlist = [(xgbtrain, 'train'), (xgbval, 'valid')]\nevals_result = {}\nmodel = xgb.train(params, xgbtrain, 5000, \n                     watchlist,\n                    early_stopping_rounds=150, verbose_eval=100)\n#we don't need these for now\n#xgbtest = xgb.DMatrix(test)\n#p_test = model.predict(xgbtest, ntree_limit=model.best_ntree_limit)\n\n#p_test = p_test + 1","5c2dd04e":"xgb_fimp=pd.DataFrame(list(model.get_fscore().items()),columns=['feature','importance']).sort_values('importance', ascending=False)\nxgb_fimp1 = xgb_fimp.iloc[0:35]\n\nsns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(10, 15))\nax = sns.barplot(x = 'importance', y = 'feature', data = xgb_fimp1,palette='Accent', ci = None).set_title('Feature importance of XGBooost')","d0abd070":"from sklearn.metrics import classification_report\nXgb_test = xgb.DMatrix(X_test)\ny_pred = model.predict(Xgb_test, ntree_limit=model.best_ntree_limit)\nprint(classification_report(y_test, y_pred))\n","cfe75f99":"Let's explore NA values in data","40c8cf71":"## Seaborn","8638f002":"Most of train data is allocated around the age of 20 and mean education of 10 years.\nBut we didn't separate data by *Target*.\nLet's do that too.","3552e1e8":"## Loading libraries and data","590852e5":"Let's check out some of the living conditions for different households","479c6a06":"So, we have 2 ID columns, the other 3 columns can be converted into integers. Intuitively, \"no\" can be converted to 0, while \"yes\" to 1. But for now we won't do that","444a659e":"## Primary EDA","9a003063":"## Correlations","268ff914":"## If you like this kernel, please, upvote. It's not that hard for you and, moreover, it motivates me to work and share ideas with you, guys","83395e3f":"here we have a deal with skewed classes, that's why we need to use at least stratification in splitting data","5f55e7d1":"Other 3 columns we fill in with 0's temporarily","8baca476":"Work in progress... Stay tuned :)","fa2c47e1":"As a result, we might probably delete some columns without decreasing ROC as they are collinear( for instance, public, \"=1 electricity from CNFL,  ICE,  ESPH\/JASEC\" and coopele, =1 electricity from cooperative, correlation between these two is -.98) \n","6919a654":"According to the results of XGB and the graph above, algorithms can detect 4th class, but first three classes are difficult to detect as values in variables are almost the same. So the main approach is to generate **killer features**, which will help algorithms to separate first three classes and, therefore, reach 0.5+ F1-score","eb25bd5d":"The Numbers are close to each other between categories. Probably, these two variables(number of women and men in the household) won't have a large impact on target variable.","cf40bdcb":"Although t-SNE is a bit unstable(changing random state may change the pic.), from the first glance we can claim that there are some clusters, which can help us to separate class 4 from others. Let's look at 3d representation.","0c7391d3":"# Part 2. Models","3bc5d17a":"Let's map index\n","ade3c781":"# Hope it was informative for all of you. Thanks for your attention!","eb545724":"We can conclude, that most of the variables have types of float and integers. Only 5 columns are objects, let's explore which ones","c8b17274":"Good news is that the same columns with missing values are observed in test! The good thing is to check if we should fill in NAs with mean or median values.","a2f9bf42":"We can observe a huge difference in mean number of children in different types of households, consequently, the hypothesis about the mean number of children in poor households is true. Therefore,  we can create this feature in our data to increase score.","6a911cc1":".## Multi-Dimensional Reduction and Visualisation with t-SNE","57a2003b":"# Part 1. EDA","6267a6fa":"As an 'insight', we can claim that classes are skewed. And, probably, without any effort our algorithms will have large errors predicting Extreme Powerty.","6bd6a5dc":"Seaborn is a great library for visualization. You can choose many palettes, which makes the graphs visually nice. For instance, some of them.","37a08b7c":"The picture became much better. For NonVulnerables both mean years of education and age are higher and allocates around 10 and 20 respectively. It's useful to mention that variance of mean education for NonVulnerables is less than for others.\n\nFor Extreme Poors age in years is the least among these 4 categories.","622b9095":"Let's also check the hypothesis that Poorer households have more children","66b50fae":"We can actually conclude, that our data is pretty full, only in 5 columns we have observe Missing values. For now it's too early to make conclusion about dropping out first three colums as useful information may be contained there(We will explore the hypothesis using graphs). Moreover, LightGBM and XGBoost are able to handle missing values while training, so that's not a problem.\n\nWe can fill in NAs with mean or median values for the rest 2 cols.","b2d1f967":"According to the results, the main feature xgboost extracts is mean education. Then, we observe age, years of education of male head of household squared, overcrowding. ","4b598b98":"Let's build a **basic XGBoost model** and take a glance at feature importance ","b5cc255a":"### Living condition comparisons","c78ea429":"The point is that if we observe outliers in data, we should fill in NAs with median, otherwise it's ok to fill in with mean values. In the table, 50% is the median value, mean is mean :) Here it's fine to use mean values","13c71ed2":"We explored that Extreme poverty households tend to have more children than nonVulnerables. Let's dig deeper and find out how monthly rate payment per person differ. \nFirstly, as we have observed a lot of missing values in v2a1(Monthly rent payment),so necessary to say that there will be high bias. ","9dc2b143":"Now we see that a lot of observations of 4-th cluster are detached from others. Unfortunately, I didn't find out how to plot the same with *seaborn* and an attempt to plot it with *plotly* occured to be unsuccessful. If someone has any idea how to plot the same graph(but interactive), you are welcome to share."}}