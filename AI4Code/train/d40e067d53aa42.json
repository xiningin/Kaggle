{"cell_type":{"df035270":"code","ac2fde93":"code","f7217d42":"code","dd204c0a":"code","b265e0c6":"code","c4cf4198":"code","9d1ac001":"code","3646e1d1":"code","35c99c7c":"code","1a109c60":"code","f984cfa0":"code","2cb2fbfb":"code","5749b7d1":"code","72c4cc60":"code","35577aeb":"markdown","1e4cc2a3":"markdown","37097055":"markdown","2b997089":"markdown","01f71141":"markdown","1a802aa7":"markdown","95674838":"markdown","4504034c":"markdown","0190fcb6":"markdown","aafdf78f":"markdown","5390e8dd":"markdown","73d57e92":"markdown","5b5c00b8":"markdown","f67c7fb2":"markdown","2a6e85f3":"markdown","e6dd28b0":"markdown","b1e0427e":"markdown","11437f94":"markdown","2abb3fc7":"markdown"},"source":{"df035270":"# !pip install nb_black\n# %load_ext nb_black\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\n\n# Import data\ndata_path = \"..\/input\/kaggle-survey-2019\/\"\nmultiple_choice_responses_19 = pd.read_csv(\n    data_path + \"multiple_choice_responses.csv\", engine=\"c\", low_memory=False\n)\nquestions_only_19 = pd.read_csv(data_path + \"questions_only.csv\", engine=\"c\")","ac2fde93":"def plot_sankey(questions, threshold=0, diff=False):\n    \"\"\"This function plots a Sankey diagram from a list of two or more questions.\n\n    Parameters\n    ----------\n    questions: list of str\n        List of the questions to be plotted.\n    threshold: int\n        Relations below this value will be discarded. It's useful to simplify the diagram.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    def get_responses_and_series(question):\n        \"\"\"This function gets all the responses for a question and a Series por each response.\n\n        Notes\n        -----\n        They are single choice questions that has all the responses in a single Series.\n        This Series is broke down in a Series of each response.\n\n        The are multiple choice questions that has one Series per response.\n\n        Parameters\n        ----------\n        question: str\n            Question to be processed.\n\n        Returns\n        -------\n        responses: list of str\n            It is a list of all responses for that question.\n        series: DataFrame\n            It is a DataFrame with a Series por each response.\n        \"\"\"\n        # Initialize series DataFrame\n        series = pd.DataFrame()\n\n        # NORMAL QUESTION\n        # ---------------\n        # If the question is single choice, it is in the multiple_choice_responses columns\n        if question in multiple_choice_responses_19.columns:\n            # Get the list of responses sorted by the frequency in descending order\n            responses = (\n                multiple_choice_responses_19[question][1:].value_counts().index.tolist()\n            )\n            # Iterate the responses, breaking down the series for each response\n            for response in responses:\n                # Make a copy\n                response_series = multiple_choice_responses_19[question][1:].copy()\n                # Make null other responses\n                response_series[response_series != response] = np.NaN\n                # Add this response series to the series DataFrame\n                series = pd.concat(\n                    [series, response_series], axis=\"columns\", sort=False\n                )\n\n        # MULTIPLE CHOICE QUESTION\n        # ------------------------\n        # If the question is multiple choice, the question is followed by _Part_\n        # Get all columns with _Part_ and discard free text columns which have _TEXT\n        else:\n            # Check if the question is multiple choice\n            multiple_choice = multiple_choice_responses_19[1:].filter(\n                like=f\"{question}_Part_\", axis=\"columns\"\n            )\n            # Don't use multiple choice with text response\n            multiple_choice = multiple_choice.drop(\n                columns=list(multiple_choice.filter(like=\"_TEXT\"))\n            )\n            # Get the list of responses sorted by the frequency in descending order\n            responses = multiple_choice.describe().T[\"top\"].tolist()\n            #\n            series = multiple_choice\n\n        # Rename the series names\n        series.columns = responses\n\n        return responses, series\n\n    def diff_color(diff):\n        \"\"\"This function returns a greenish color if diff is greater than 0 \n        and a redish color if diff is lesser than 0.\n        \n        Notes\n        -----\n        First, the diff is scaled and transformed from (-inf\u2026+inf) to (-1\u20261).\n        Then, (-1\u20261) is transformed to an hex color, where 0 is light grey, 1 green and -1 red.\n\n        Parameters\n        ----------\n        diff: float\n            Difference in parts per unit of the number of real links compared with theorical neutral links.\n            0.1 means that real links are a 10% greater than an equilibrated weight\n            -0.1 means that real links are a 10% lesser than an equilibrated weight\n\n        Returns\n        -------\n        color: str\n            Color in hex format. In instance: \"#ff0000\"\n        \"\"\"\n        diff = diff \/ 4\n        diff = diff \/ (1 + abs(diff))\n        if diff >= 0:\n            color = f\"#{int((1-diff)*192 + diff*0):02x}{int((1-diff)*192 + diff*255):02x}{int((1-diff)*192 + diff*0):02x}\"\n        else:\n            diff = -diff\n            color = f\"#{int((1-diff)*192 + diff*255):02x}{int((1-diff)*192 + diff*0):02x}{int((1-diff)*192 + diff*0):02x}\"\n        return color\n\n    # Initialize variables\n    questions_index = 0\n    source_offset = 0\n    link = pd.DataFrame(columns={\"source\", \"target\", \"value\"})\n\n    # Get the question of the source side\n    source_question = questions[questions_index]\n    # Get the responses and series of the source side\n    source_responses, source_series = get_responses_and_series(source_question)\n    # Set the first part of the title\n    title = f\"\\n<br><b>{source_question}<\/b>: {questions_only_19[source_question].values[0]}\"\n    # Fill the node list with the nodes of the source side\n    node = source_responses.copy()\n\n    # For the remaining questions\n    while questions_index < (len(questions) - 1):\n        # Increment the questions index\n        questions_index += 1\n        # Initialize variables\n        link_partial = pd.DataFrame(columns={\"source\", \"target\", \"value\"})\n        # Get the question of the target side\n        target_question = questions[questions_index]\n        # Get the responses and series of the target side\n        target_responses, target_series = get_responses_and_series(target_question)\n        # Append the part of the title relative to the current question\n        title += f\"\\n<br><b>{target_question}<\/b>: {questions_only_19[target_question].values[0]}\"\n\n        # The target nodes will begin in the next position of the current node list\n        target_offset = len(node)\n        # The node list is extended with the target responses list\n        node += target_responses\n\n        # Iterate all the combinations between the source and target sides\n        for source_index, source_response in enumerate(source_responses):\n            for target_index, target_response in enumerate(target_responses):\n                # Value is the number of coincidences of the source\/target combination.\n                # A coincidence occurs when for the given combination and the same respondent\n                # there are responses in both sides.\n                value = (\n                    source_series[source_response].notna()\n                    & target_series[target_response].notna()\n                ).sum()\n                # Only relations with values greater than a threshold are processed\n                if value >= threshold:\n                    # Append the link DataFrame with the source, target and value fields.\n                    link_partial = link_partial.append(\n                        {\n                            \"source\": source_offset + source_index,\n                            \"target\": target_offset + target_index,\n                            \"value\": value,\n                        },\n                        ignore_index=True,\n                    )\n        # If diff is colored\n        if diff:\n            # Get the source sum of links for each node\n            source_size = (\n                link_partial[[\"source\", \"value\"]]\n                .groupby(\"source\")\n                .agg(np.sum)\n                .rename(columns={\"value\": \"count\"})\n            )\n\n            # Get the source sum of links for each node\n            target_size = (\n                link_partial[[\"target\", \"value\"]]\n                .groupby(\"target\")\n                .agg(np.sum)\n                .rename(columns={\"value\": \"count\"})\n            )\n\n            # Get the total links from source or target nodes (they are the same)\n            link_size = source_size.sum().values[0]\n\n            # Calculate the weights of links of a neutral and equilibrated net\n            base = (\n                source_size.dot(target_size.T)\n                .div(link_size)\n                .reset_index()\n                .melt(id_vars=\"source\")\n            )\n\n            # As the link_partial and base are not indexed with the same indices,\n            # the coupling has to be done by hand.\n            link_partial[\"base\"] = link_partial.apply(\n                lambda x: base[\n                    (base[\"target\"] == x[\"target\"]) & (base[\"source\"] == x[\"source\"])\n                ][\"value\"].values[0],\n                axis=\"columns\",\n            )\n\n            # Calculate the diff\n            link_partial[\"diff\"] = (\n                link_partial[\"value\"] - link_partial[\"base\"]\n            ) \/ link_partial[\"base\"]\n\n            # Get the color\n            link_partial[\"color\"] = link_partial[\"diff\"].apply(diff_color)\n\n        # Append the partial link to the general one\n        link = pd.concat([link, link_partial], sort=False)\n\n        # Prepare the target side to be the next source side\n        source_responses = target_responses\n        source_series = target_series\n        source_offset = target_offset\n\n    # If diff is colored\n    if diff:\n        color = link[\"color\"]\n    else:\n        color = None\n    # Create and show the diagram\n    fig = go.Figure(\n        data=[\n            go.Sankey(\n                node=dict(label=node),\n                link=dict(\n                    source=link[\"source\"],\n                    target=link[\"target\"],\n                    value=link[\"value\"],\n                    color=color,\n                ),\n            )\n        ]\n    )\n    fig.update_layout(title_text=f\"Sankey Diagram: {title}\", font_size=10, height=800)\n    fig.show()","f7217d42":"plot_sankey([\"Q18\", \"Q19\"], threshold=100, diff=True)","dd204c0a":"plot_sankey([\"Q14\", \"Q19\"], threshold=40, diff=True)","b265e0c6":"plot_sankey([\"Q14\", \"Q19\"], threshold=40, diff=True)","c4cf4198":"plot_sankey([\"Q14\", \"Q20\"], threshold=200, diff=True)","9d1ac001":"plot_sankey([\"Q18\", \"Q20\"], threshold=1000, diff=True)","3646e1d1":"plot_sankey([\"Q15\", \"Q23\"], threshold=40)","35c99c7c":"plot_sankey([\"Q31\", \"Q29\", \"Q30\"], threshold=150)","1a109c60":"plot_sankey([\"Q21\", \"Q22\"], threshold=0, diff=True)","f984cfa0":"plot_sankey([\"Q21\", \"Q24\"], threshold=1000, diff=True)","2cb2fbfb":"plot_sankey([\"Q12\", \"Q13\"], threshold=1000, diff=True)","5749b7d1":"plot_sankey([\"Q13\", \"Q14\"], threshold=200, diff=True)","72c4cc60":"plot_sankey([\"Q10\", \"Q11\"], threshold=0, diff=True)","35577aeb":"About the software recomendation, Business Intelligence software recommends R and SQL above the average. Advance statistical software recommends SQL. And basic statistical software C and C++.","1e4cc2a3":"# Introduction\n\nThe aim of this notebook is to have some curious insights of the data.\nOne usual way to present the data is to cross some few responses of a question with all the responses of other question, like what's the salary of the respondents who use that platform compared with the respondents who use the other platform.\n\nThe Sankey diagram lets compare all the responses of a question with all the responses of other question. The link between responses of one and another question is shown visually, and the number of respondents who share that two responses are available when hovering the flow.\n\nThe link is grey if the number or respondents of the link is proportional to the number of respondents of the source response and the target response. The more respondents in the link that it would be for a neutral distribution, The more greenish is the link. The green color indicates a positive trend. The same for the red color, but for a negative trend.\n\n# Table of contents\n\n- [Programming language](#Programming-language)\n- [Analyze data years vs machine learning years](#Analyze-data-years-vs-machine-learning-years)\n- [Cloud platforms and products](#Cloud-platforms-and-products)\n- [Hardware](#Hardware)\n- [Media sources vs platforms](#Media-sources-vs-platforms)\n- [Money](#Money)","37097055":"People that don't use CPUs or GPUs, uses more linear or logistic regression and Decision tress or random forests. Maybe they use pencil and paper?\n\nNeural networks are used more frequently with GPUs than with CPUs.","2b997089":"## Primary tool vs programming language\nThere is any relation between the primary tool used and the programming language?","01f71141":"## Visualization libraries vs primary tool vs programming languages\nThe visualization libraries, the primary tool and the programmin languages are related? And how?","1a802aa7":"As a general rule (and as expected), the experience analyzing data is larger than the experience with machine learning.","95674838":"# Analyze data years vs machine learning years\nWhat's the relation between the number of years analyzing data and the number of years of machine learning experience?","4504034c":"They are some libraries that are practically only related to one primary tool, as Shiny, Bokeh, D3.js, Leaflet \/ Folium and Geoplotlib.\n\nThey are other libraries that are used by several primary tools, as Matplotlib, Seabor, ggplot and plotly.\n\nThere are some other interesting insights:\n\n- Matplotlib and seaborn are used more frequently by Other. Maybe these libraries are used in software development.\n- ggplot is specially used in advanced statistical software.\n- Basic statistical software has a high proportion of None. Maybe as these software has its own visualization libraries.\n- Local development enviroments seldom use any visualization library.","0190fcb6":"People who uses Business Intelligence software uses more R and SQL than the usual.\n\nPeople who uses basic statistical software, cloud-based data software and other, uses less R.","aafdf78f":"# Programming language\n\nPeople who uses one programming language, recommend it's own language above the average?","5390e8dd":"# Cloud platforms and products\nWhat's the relation between the cloud computing platform and the cloud computing products?","73d57e92":"# Hardware\nWhat's the relation use of hardware and the use of a TPU? And the ML algorithm used?","5b5c00b8":"As expected, respondents who use TPU as a regular basis has used more than once a TPU, than the average.\n\nAlso, respondents who use TPU as a regular basis have a little positive trend to have used more than once a TPU.","f67c7fb2":"# Media sources vs platforms\nExist some relations between media sources vs platforms?\n\nIt's expected that people from some communities would use more one platform or another.","2a6e85f3":"There are some programming languages that practically only uses one visualization library. As C, C++, Java, Javascript and MATLAB that principally uses Matplotlib.\n\nThere are some visualization libraries that are only practically used by one programming language. As Bokeh with Python, and Shiny with R. It makes sense, as these libraries are only for that programming languages.\n\nIn proportion, Python uses more Seaborn and Plotly than ggplot and Matplotlib. And R uses more Shiny and ggplot than Seaborn and Matplotlib.\n","e6dd28b0":"Yes, generally people that uses a language, recommend its language. Like SQL, C++, Matlab and R.\n\nIt's interesting to see that people that uses Python recomend Python at quite the same proportion as the other languages do.\n\nPython users recommend less R and SQL, and R users recommend less Python.","b1e0427e":"Some positive trends could be see from respondents who use podcast, slack and hacker news that use Coursera more than the other options.\n\nAlso, Coursera is less used by respondents who are in blogs, Kaggle and YouTube.\n\nIt's expected that Kaggle users use more Kaggle Courses than other options, but it's not like that.","11437f94":"# Money\n\nPeople with higher salaries spend more on machine learning?","2abb3fc7":"Yes, it is clearly a correlation between the current yearly compensation and the money spent on machine learning."}}