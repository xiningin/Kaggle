{"cell_type":{"6b960b8f":"code","a6ea23fc":"code","f14815e4":"code","ab9d387c":"code","aa6df9c4":"code","d83d1c89":"code","5f93c20a":"code","4fa8266a":"code","3eddd841":"code","d6dcc034":"code","3bd06a9f":"code","a069ecfe":"code","6474b8e7":"code","0d2f05ee":"code","a4dcf9f6":"code","35908f83":"code","898e82fe":"code","65347b8b":"code","3bdc50b5":"code","fd11eb3d":"code","9aedc921":"code","55d09241":"code","a9c16bfe":"code","79009a9d":"code","003b4ab3":"code","09d0c28d":"markdown"},"source":{"6b960b8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gensim\nprint(os.listdir(\"..\/input\/embeddings\"))\n\n# Any results you write to the current directory are saved as output.","a6ea23fc":"import gensim","f14815e4":"from gensim.models import KeyedVectors\npath= '..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\nword2vec=KeyedVectors.load_word2vec_format(path,binary=True)","ab9d387c":"embeddings=gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","aa6df9c4":"len(word2vec['amazon'])","d83d1c89":"embeddings['amazon']","5f93c20a":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([embeddings['camera'],embeddings['quality']])","4fa8266a":"embeddings.most_similar('hyundai',topn=10)  ## similar word to hyundai","3eddd841":"embeddings.doesnt_match(['rahul','sonia','gandhi','sachin'])  ## getting the odd man out","d6dcc034":"embeddings.most_similar(positive=['king','women'],negative=['man'],topn=1)","3bd06a9f":"url=\"https:\/\/raw.githubusercontent.com\/skathirmani\/datasets\/master\/imdb_sentiment.csv\"\nimdb=pd.read_csv(url)","a069ecfe":"### document term matrix is created usings weight assign to words\n## as normal document term matrix has very high dimension\n# reducing the dimension by assigning weight","6474b8e7":"imdb.head(2)","0d2f05ee":"### it wil fail when word in docement does not match word in google","a4dcf9f6":"import nltk\ndocs_vectors=pd.DataFrame()\nstopwords=nltk.corpus.stopwords.words('english')  ### do not do stemming\nfor doc in imdb['review'].str.lower().str.replace('[^a-z ]',' '):\n    words=nltk.word_tokenize(doc)\n    words_clean=[word for word in words if word not in stopwords]\n    temp=pd.DataFrame()\n    for word in words_clean:     ### looping through allthe words in a document\n        try:\n            word_vec=pd.Series(embeddings[word])\n            temp=temp.append(word_vec,ignore_index=True)\n        except:\n            pass\n    temp_avg=temp.mean()        ### calculating the mean(column sum)\n    docs_vectors=docs_vectors.append(temp_avg,ignore_index=True)\ndocs_vectors.shape   ","35908f83":"docs_vectors ## vector representation of each word","898e82fe":"pd.isnull(docs_vectors).sum().sum()  ##nearly 2 rows is completely missing","65347b8b":"docs_vectors['sentiment']=imdb['sentiment']\ndocs_vectors=docs_vectors.dropna()","3bdc50b5":"docs_vectors.shape","fd11eb3d":"### cant use multinomial naive baise as it contain negative values","9aedc921":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier","55d09241":"train,test=train_test_split(docs_vectors,test_size=0.2,random_state=100)\ntrain_x=train.drop('sentiment',axis=1)\ntrain_y=train['sentiment']\ntest_x=test.drop('sentiment',axis=1)\ntest_y=test['sentiment']\n","a9c16bfe":"ab_model = AdaBoostClassifier(n_estimators=300,random_state=100)\nab_model.fit(train_x,train_y)\nab_pred =ab_model.predict(test_x)\naccuracy_score(test_y,ab_pred)","79009a9d":"ab_pred[:5]","003b4ab3":"gb_model = GradientBoostingClassifier(n_estimators=300,random_state=100)\ngb_model.fit(train_x,train_y)\ngb_pred =gb_model.predict(test_x)\naccuracy_score(test_y,gb_pred)","09d0c28d":"Using IMDb dataset"}}