{"cell_type":{"05ab0744":"code","7b7e4fdd":"code","62e2b885":"code","a67c35b1":"code","2242ff07":"code","1a86a6bd":"code","bd86820d":"code","5fba0d14":"code","72dbda84":"code","ae8e8984":"code","019b15a3":"code","9da2b628":"code","daa71bb9":"code","a3d4bfdc":"code","7151888b":"code","d5527809":"code","697bec9f":"code","d9053e4d":"code","d4d222c6":"code","cb807420":"code","516f83c1":"code","b712c658":"code","5c175db7":"code","f5568525":"code","3d58732b":"code","937f2415":"code","4a946272":"code","bec502e1":"code","1aa1056a":"code","5f38d2be":"code","d19859b7":"code","e5b68278":"code","4b82e191":"code","834ef7af":"code","33ad540d":"code","7d7f3897":"code","9352c4a5":"code","26110ebd":"code","4150cdbf":"code","5d8ba88c":"code","66ccd48b":"code","a5da0c6e":"code","7a6af1ea":"code","1d3fa934":"code","779cbac0":"code","82e2256f":"code","2edfa22b":"code","fd6921a9":"code","c501b44d":"code","a049010a":"code","fde51053":"code","08cbed8a":"code","2da55d6f":"code","bdf654a1":"markdown","82dd6975":"markdown","837040c3":"markdown","c20d15c0":"markdown","8c5ecfd6":"markdown","7cec58c6":"markdown","43a54840":"markdown","422052dc":"markdown","19601bfb":"markdown","15ac36b0":"markdown","07922e8e":"markdown","3d35dc90":"markdown","69746c07":"markdown","f0ff3d39":"markdown","8e3ea89d":"markdown"},"source":{"05ab0744":"from numpy.random import seed\nseed(101)\n#from tensorflow import set_random_seed\n#set_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n#import keras\n#from keras import backend as K\n\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","7b7e4fdd":"os.listdir('..\/input\/skin-cancer-mnist-ham10000')","62e2b885":"\n\n# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n \n #val_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)\n\n","a67c35b1":"df_data = pd.read_csv('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_metadata.csv')\n\ndf_data.head()","2242ff07":"# this will tell us how many images are associated with each lesion_id\ndf = df_data.groupby('lesion_id').count()\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head()","1a86a6bd":"# here we identify lesion_id's that have duplicate images and those that have only\n# one image.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\ndf_data['duplicates'] = df_data['lesion_id']\n# apply the function to this new column\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head()","bd86820d":"df_data['duplicates'].value_counts()","5fba0d14":"# now we filter out images that don't have duplicates\ndf = df_data[df_data['duplicates'] == 'no_duplicates']\n\ndf.shape","72dbda84":"# now we create a val set using df because we are sure that none of these images\n# have augmented duplicates in the train set\ny = df['dx']\n\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n\ndf_val.shape","ae8e8984":"df_val['dx'].value_counts()","019b15a3":"# This set will be df_data excluding all rows that are in the val set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# create a new colum that is a copy of the image_id column\ndf_data['train_or_val'] = df_data['image_id']\n# apply the function to this new column\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n\nprint(len(df_train))\nprint(len(df_val))","9da2b628":"df_train['dx'].value_counts()","daa71bb9":"df_val['dx'].value_counts()","a3d4bfdc":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","7151888b":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1')\nfolder_2 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        ","d5527809":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","697bec9f":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","d9053e4d":"# note that we are not augmenting class 'nv'\nclass_list = ['mel','bkl','bcc','akiec','vasc','df']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir\/train_dir\/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir\/train_dir\/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir\/train_dir\/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        #brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    \n    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)\/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","d4d222c6":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","cb807420":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","516f83c1":"# plots images with labels within jupyter notebook\n# source: https:\/\/github.com\/smileservices\/keras_utils\/blob\/master\/utils.py\n\ndef plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","b712c658":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","5c175db7":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\n","f5568525":"\ndatagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","3d58732b":"# create a copy of a mobilenet model\n\nmobile = tensorflow.keras.applications.mobilenet.MobileNet()","937f2415":"mobile.summary()","4a946272":"type(mobile.layers)","bec502e1":"# How many layers does MobileNet have?\nlen(mobile.layers)","1aa1056a":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\nx = Dropout(0.25)(x)\npredictions = Dense(7, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","5f38d2be":"model.summary()","d19859b7":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","e5b68278":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","4b82e191":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n\n","834ef7af":"# Get the labels that are associated with each index\nprint(valid_batches.class_indices)","33ad540d":"# Add weights to try to make the model more sensitive to melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","7d7f3897":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)\n","9352c4a5":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","26110ebd":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","4150cdbf":"os.listdir('..\/input\/skin-cancer-model')","5d8ba88c":"# Here the best epoch will be used.\n\n#model.load_weights('..\/input\/skin-cancer-model\/model.h5')\nmodel.load_weights('model.h5')\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","66ccd48b":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\nplt.title('Training and validation top3 accuracy')\nplt.legend()\n\n\nplt.show()","a5da0c6e":"# Get the labels of the test images.\n\ntest_labels = test_batches.classes","7a6af1ea":"# We need these to plot the confusion matrix.\ntest_labels","1d3fa934":"# Print the label associated with each class\ntest_batches.class_indices","779cbac0":"# make a prediction\npredictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)","82e2256f":"predictions.shape","2edfa22b":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n\n","fd6921a9":"test_labels.shape","c501b44d":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","a049010a":"test_batches.class_indices","fde51053":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","08cbed8a":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","2da55d6f":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","bdf654a1":"### Set Up the Generators","82dd6975":"### Create Train and Val Sets","837040c3":"### Copy the train images  into aug_dir","c20d15c0":"### Generate the Classification Report","8c5ecfd6":"### Train the Model","7cec58c6":"### Create a Confusion Matrix","43a54840":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.","422052dc":"### Plot the Training Curves","19601bfb":"### Visualize 50 augmented images","15ac36b0":"### Modify MobileNet Model","07922e8e":"### Create a train set that excludes images that are in the val set","3d35dc90":"### Transfer the Images into the Folders","69746c07":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. ","f0ff3d39":"### Evaluate the model using the val set","8e3ea89d":"### Create a stratified val set"}}