{"cell_type":{"87540bc4":"code","27bcf596":"code","fd98161f":"code","4d8516e6":"code","821055e8":"code","95b5880e":"code","c6b6db38":"code","31014c53":"code","a6f91be3":"code","a71f8ecb":"code","32bf7f44":"code","126f7db7":"code","f8234b6e":"code","639b1970":"code","3191a7b8":"code","532be6d8":"code","e0d0f25d":"code","4c19b08e":"code","a4f3897e":"code","6ef55ef1":"code","2fc1cb93":"code","d5e0669d":"code","df398503":"code","fa6fb2ce":"code","00377fb7":"code","a9c6dc39":"code","93103b71":"code","45c75610":"code","b0f34455":"code","17eca27a":"code","7f663fff":"code","1d4c5844":"code","1d5f3d79":"code","d9484ec4":"markdown","5ffd70b1":"markdown","1b545b73":"markdown","3f81b771":"markdown","b1c0fd6b":"markdown","b5c3c423":"markdown"},"source":{"87540bc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom matplotlib.pyplot import figure\nmpl.rc('axes',labelsize=14)\nmpl.rc('xtick',labelsize=12)\nmpl.rc('ytick',labelsize=12)\n#figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\nimport matplotlib.cm as cm\n\nfrom sklearn.model_selection import train_test_split\n\n\nimport warnings\n# Ignore useless warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# Avoid runtime error messages\npd.set_option('display.float_format', lambda x:'%f'%x)\n\nnp.random.seed(87)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27bcf596":"filepath = '..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv'\ndata = pd.read_csv(filepath)","fd98161f":"data.head(5)","4d8516e6":"data.describe()","821055e8":"#check for missing values:\nprint(data.isnull().sum())","95b5880e":"#Serial No seems like a useless column, lets save it incase we need it and drop it\ndata_serialNo = data['Serial No.']\ndata.drop('Serial No.', axis=1,inplace=True)\nprint(\"Dropped Serial No.\")\nprint(data.shape)\nprint(data.head(5))","c6b6db38":"print(data.dtypes)","31014c53":"# Looks like SOP, LOR, Research are actually categorical variables, Lets convert them to categorical variables\ntemp = [\"SOP\",\"LOR \",\"Research\",\"University Rating\"]\nfor label in temp:\n    data[label] = data[label].astype('object')\nprint(\"Converted to Category\")","a6f91be3":"#picking up numerical and categorical features:\nnumerical_features = data.dtypes[data.dtypes != \"object\"].index\ncategorical_features = data.dtypes[data.dtypes == \"object\"].index\n\nprint(numerical_features)\nprint(categorical_features)\nfor i,feature in enumerate(numerical_features,1):\n    print (i, feature, sep=\":\")\n\nfor j,feature in enumerate(categorical_features,1):\n    print (j, feature, sep=\"-\")\n\n#print(np.ceil((len(data.columns))\/3))","a71f8ecb":"#plotting all numeric features against GRE Score\nfig, ax = plt.subplots(figsize=(20,15))\nsns.set(style='darkgrid')\nlength = len(numerical_features)\nfor i,feature in enumerate(numerical_features,1):\n    plt.subplot(np.ceil(length\/2), 2, i) #nrows,ncols,index\n    sns.scatterplot(x = feature, y = 'GRE Score',data=data,alpha=0.7,hue=\"GRE Score\", palette ='nipy_spectral_r',linewidth=0.5, edgecolor='white')\n    #sns.regplot(x = feature, y=\"GRE Score\",data=data,color='orange')\n    plt.ylabel('GRE Score', fontsize=13)\n    plt.xlabel(feature, fontsize=13)\nplt.show()","32bf7f44":"#plotting distributions of all numeric features\nfig, ax = plt.subplots(figsize=(20,20))\nsns.set(style='darkgrid')\nlength = len(data.columns)\nfor i,feature in enumerate(numerical_features,1):\n    plt.subplot(np.ceil(length\/2), 2, i) #nrows,ncols,index\n    sns.distplot(data[feature], rug=True,color='green')\n    plt.xlabel(feature, fontsize=13)\n    plt.title(\"Distribution of {} variable\".format(feature))\nplt.show()","126f7db7":"#plotting distributions of all categorical features\nfig, ax = plt.subplots(figsize=(20,20))\nsns.set(style='darkgrid')\nlength = len(data.columns)\nfor i,feature in enumerate(categorical_features,1):\n    plt.subplot(np.ceil(length\/2), 2, i) #nrows,ncols,index\n    sns.scatterplot(x=feature,y=\"CGPA\",data=data,alpha=0.7,hue=\"Research\", palette ='nipy_spectral_r',linewidth=0.8, edgecolor='white')\n    #sns.regplot(x = feature, y=\"GRE Score\",data=data,color='orange')\n    plt.xlabel(feature, fontsize=13)\n    plt.title(\"{} variable\".format(feature))\nplt.show()","f8234b6e":"#plotting distributions of all categorical features\nfig, ax = plt.subplots(figsize=(20,20))\nsns.set(style='darkgrid')\nlength = len(data.columns)\nfor i,feature in enumerate(categorical_features,1):\n    plt.subplot(np.ceil(length\/2), 2, i) #nrows,ncols,index\n    sns.scatterplot(x=feature,y=\"GRE Score\",data=data,alpha=0.7,hue=\"Research\", palette ='nipy_spectral_r',linewidth=0.8, edgecolor='white')\n    #sns.regplot(x = feature, y=\"GRE Score\",data=data,color='orange')\n    plt.xlabel(feature, fontsize=13)\n    plt.title(\"{} variable\".format(feature))\nplt.show()","639b1970":"\n#Lets look at Correltion matrix between variables\ndata_temp = data.astype('float64')\n#print(data_temp)\ncorr_mat = data_temp.corr()\nself_corr = np.zeros_like(corr_mat)\nself_corr[np.triu_indices_from(self_corr)] = True\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap (corr_mat, square = True,cmap=\"viridis_r\",alpha=0.7,linewidth=0.5,edgecolor='white',fmt=\".2f\", annot=True,mask=self_corr)","3191a7b8":"corr_mat['Chance of Admit '].sort_values(ascending=False)","532be6d8":"print(data[numerical_features])","e0d0f25d":"#Log transforming all numerical features to avoid any skewness\n\nfrom scipy.stats import skew\nskewed_feats = data[numerical_features].apply(lambda x: skew(x.dropna())) #compute skewness\nprint(skewed_feats)\nskewed_feats = skewed_feats.index\nskewed_feats","4c19b08e":"#Applying log transform to all the skewed variables in the full dataset\ndata[skewed_feats] = np.log1p(data[skewed_feats])\nprint(\"Transformed all Numerical Features\")","a4f3897e":"#creating Dummy variables from the categorical Data\ndata = pd.get_dummies(data)\nprint(\"created dummy variables\")","6ef55ef1":"data.head(10)","2fc1cb93":"#Splitting data\n#chance of Admit is our target variable\nX = data.drop('Chance of Admit ',axis=1)\ny = data['Chance of Admit ']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, shuffle=False)\nprint(\"X_train: \",X_train.shape,\"X_test: \",X_test.shape,\"y_train: \",y_train.shape,\"y_test: \",y_test.shape)","d5e0669d":"#Import Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV, LinearRegression\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n","df398503":"#setup KFold with 10 splits\nkf = KFold(n_splits=10,random_state=23, shuffle=True)","fa6fb2ce":"#Define our RMSE function: Root Mean Squared Error\ndef rmse_cross_val (model, X=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return (rmse)\n\n#we have already scaled back our outliers, even then lets define our RMSLE function: Root Mean Squared log Error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y_train,y_pred))\nprint(\"Defined RMSE function\")","00377fb7":"#Linear Regression\nlreg = LinearRegression()\n\n#LightGBM Regressor\nlightgbm = LGBMRegressor(objective='regression',\n                         num_leaves=6,\n                         learning_rate=0.01,\n                         n_estimators=7000,\n                         max_bin=200,\n                         bagging_fractions=0.8,\n                         bagging_freq=4,\n                         feature_fractions=0.2,\n                         feature_fraction_seed=8,\n                         min_sum_hessian_in_leaf=11,\n                         verbose=-1,\n                         random_state=23)\n#XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective=\"reg:squarederror\",\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=22,\n                       reg_alpha=0.00006,\n                       random_state=23)\n                         \n    \n#RidgeRegressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(),RidgeCV(alphas=ridge_alphas, cv=kf))\n\n\n#supportvectorregressor\nsvr = make_pipeline(RobustScaler(),SVR(C=20,epsilon=0.008,gamma=0.0003))\n\n#gradientboostregressor\ngbr = GradientBoostingRegressor(n_estimators = 6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=23)\n#random forest regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                           max_depth=5,\n                           min_samples_split=5,\n                           min_samples_leaf=5,\n                           max_features=None,\n                           oob_score=True,\n                           random_state=23)\n\n#Stack all Models\n\nstack_gen = StackingCVRegressor(regressors=(lreg,xgboost,lightgbm,svr,ridge,gbr,rf),\n                                meta_regressor=svr,\n                                use_features_in_secondary=True)","a9c6dc39":"scores={}\nscore = rmse_cross_val(lreg)\nprint(\"Liner regression: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['lreg'] = (score.mean(),score.std())\nscore = rmse_cross_val(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['lgb'] = (score.mean(),score.std())\nscore = rmse_cross_val(xgboost)\nprint(\"XGBoost: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['xgboost'] = (score.mean(),score.std())\nscore = rmse_cross_val(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['svr'] = (score.mean(),score.std())\nscore = rmse_cross_val(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['ridge'] = (score.mean(),score.std())\nscore = rmse_cross_val(rf)\nprint(\"RandomForest: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['rf'] = (score.mean(),score.std())\nscore = rmse_cross_val(gbr)\nprint(\"Gradientboostingregressor: {:.4f} ({:.4f})\".format(score.mean(),score.std()))\nscores['gbr'] = (score.mean(),score.std())\n","93103b71":"print(\"Linear Regression\")\nlreg_model_full_data=lreg.fit(X_train,y_train)\n\nprint(\"lightgbm\")\nlgbm_model_full_data=lightgbm.fit(X_train,y_train)\n\nprint(\"xgboost\")\nxgbm_model_full_data=xgboost.fit(X_train,y_train)\n\nprint(\"svr\")\nsvr_model_full_data=svr.fit(X_train,y_train)\n\nprint(\"Ridge\")\nridge_model_full_data=ridge.fit(X_train,y_train)\n\nprint(\"RandomForest\")\nrf_model_full_data=rf.fit(X_train,y_train)\n\nprint(\"gradientBoosting\")\ngbr_model_full_data=gbr.fit(X_train,y_train)\n\nprint(\"stacked models\")\nstack_gen_model_full_data=stack_gen.fit(X_train,y_train)","45c75610":"def blended_predictions(X_train):\n    return ((0.1 * ridge_model_full_data.predict(X_train))+\n            (0.1 * svr_model_full_data.predict(X_train))+\n            (0.1 * gbr_model_full_data.predict(X_train))+\n            (0.1 * xgbm_model_full_data.predict(X_train))+\n            (0.1 * lgbm_model_full_data.predict(X_train))+\n            (0.1 * rf_model_full_data.predict(X_train))+\n            (0.1 * lreg_model_full_data.predict(X_train))+\n            (0.30 * stack_gen_model_full_data.predict(np.array(X_train))))","b0f34455":"#Getting final predictions\nblended_score = rmsle(y, blended_predictions(X_train))\nscores['blended'] = (blended_score,0)\nprint(\"RMSLE Score on training data:\")\nprint(blended_score)","17eca27a":"#Identifying best model\nf, ax = plt.subplots(figsize=(20, 15))\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'],color='green')\nfor i, score in enumerate(scores.values()):\n    ax.text(i,score[0]-0.001,'{:.6f}'.format(score[0]),horizontalalignment='left',size='large',color='purple',weight='semibold')\nplt.ylabel('Score(RMSE)',size=20)\nplt.xlabel('Model',size=20)\nplt.title(\"Scores of Models\",size=20)\nplt.show()\n","7f663fff":"#Generating final predictions\nblendPred = blended_predictions(X_test)","1d4c5844":"submission = pd.DataFrame()\nsubmission['actual'] = y_test\nsubmission['predictions'] = blendPred\nsubmission['residuals'] = submission['actual']-submission['predictions']\nsubmission['rmsescore']= np.absolute(submission['residuals']\/submission['actual']*100)\nsubmission = submission.reset_index()\nsubmission.drop('index',axis=1,inplace=True)","1d5f3d79":"pd.options.display.max_rows = 999\npd.set_option('display.float_format', lambda x: '% 2f' % x)\nsubmission.sort_values(by = ['rmsescore'])","d9484ec4":"### Training all the models","5ffd70b1":"* there is a largly clear indication that University which ahs higher rating takes majority of students with Research experience\n* At the same time, Strength of Letter of Recommendation though largely correlates with CGPA, there are exceptions to the rule\n* there is an indication that people with higher CGPA, also has better SOP, might make sense because higher CGPA suggests, more seriousness and hardworking students\n* There is a similar relation between GRE Scores and LOR,SOP","1b545b73":"* So it seems like people with higher GRE Scores also have higher CGPA, TOEFL Score and also has a higher chance of admission","3f81b771":"* Well, SVR is our best performing model, and simple Linear Regression is second best, beating, all other advanced models\n* so while stacking the models, I increased the weightage of  SVR","b1c0fd6b":"* So GRE Score, TOEFL Score, CGPA are top 3 correlated variables with a chance of admission","b5c3c423":"Generating rms scores for all our regressors"}}