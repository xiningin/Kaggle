{"cell_type":{"daf7b3bf":"code","7dd01ef0":"code","8e22a003":"code","757d461e":"code","289d769b":"code","69fe77f2":"code","e9e387f1":"code","203345b5":"code","c6dd0062":"code","d66b1408":"code","42d72fbd":"code","ea7c8b96":"code","d2a190e0":"code","4f47f814":"code","87442692":"code","ba9f4d7d":"code","6b380dc5":"code","6ad3917a":"code","8b246d81":"code","82a70286":"code","829b50c2":"code","5921f9c2":"code","3abc7910":"code","28e5ece7":"code","38d7bfb6":"markdown","3085e664":"markdown","627d666f":"markdown","6dc1f8b3":"markdown","9106dfc4":"markdown","465a79f1":"markdown","27d0b137":"markdown","f5329668":"markdown","e1f5c720":"markdown","0a5abbb6":"markdown","04b1f9e5":"markdown","d76ad439":"markdown","4cd84ca3":"markdown","6bc419d1":"markdown","f9169993":"markdown","206e7685":"markdown","70f21995":"markdown","fd055edf":"markdown","5ee338f7":"markdown","c6a9f6db":"markdown","3e9a6450":"markdown","419b112c":"markdown","f2c96d04":"markdown","e90dfeb6":"markdown","de60bd91":"markdown","98aea529":"markdown","c23ac3ae":"markdown","3213306a":"markdown","469b0b2e":"markdown","2b7487af":"markdown","9c8e7f7f":"markdown","4fa55468":"markdown","bb297c66":"markdown"},"source":{"daf7b3bf":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'): # get files directories\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nmissing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n\/a\", \"na\", \"--\",\" \"] # Taking into account Missing Values\n\ntitanic_train_file_path = '..\/input\/titanic\/train.csv'\ntitanic_train = pd.DataFrame(pd.read_csv(titanic_train_file_path, na_values = missing_value_formats))  # reading the training set\n\ntitanic_test_file_path = '..\/input\/titanic\/test.csv'\ntitanic_test = pd.read_csv(titanic_test_file_path, na_values = missing_value_formats)# reading the final testing set\n\ntitanic_train.head() # first 10 rows of the data\n\n","7dd01ef0":"titanic_train.describe()","8e22a003":"# Center the charts\n\nfrom IPython.core.display import HTML \nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","757d461e":"# Pie chart of Variable 'Survived'\n\n\nimport matplotlib.pyplot as plt\n\nlabels = 'Died', 'Survived'\nsizes = [titanic_train['Survived'].loc[titanic_train['Survived'] == 0].count()\/titanic_train['Survived'].count(), titanic_train['Survived'].loc[titanic_train['Survived'] == 1].count()\/titanic_train['Survived'].count()]\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Survived')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Survivors Ratio',  fontsize=13,fontweight='bold')\nplt.show()","289d769b":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\n\nplt.hist(titanic_train['Pclass'],weights=np.ones(len(titanic_train['Pclass'])) \/ len(titanic_train['Pclass']), bins=np.arange(1,5)-0.25 , width=0.5)\n\nplt.ylabel('Percentage of Passengers by Class',  fontsize=13,fontweight='bold')\nplt.xlabel('Class Types',  fontsize=13,fontweight='bold')\nplt.xticks(range(1,4))\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n\nplt.show()\n\n","69fe77f2":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndf=pd.crosstab(titanic_train['Pclass'],titanic_train['Survived']).apply(lambda r: r\/r.sum(), axis=1)\ndf.plot.bar()\nplt.ylabel('Percentage of Survivors by Class',  fontsize=13,fontweight='bold')\nplt.xlabel('Class Types',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\n\nL=plt.legend()\nL.get_texts()[0].set_text('Died')\nL.get_texts()[1].set_text('Survived')\nplt.show()\n\n","e9e387f1":"import matplotlib.pyplot as plt\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Female', 'Male'\nsizes = [titanic_train['Sex'].loc[titanic_train['Sex'] == \"female\"].count()\/titanic_train['Sex'].count(), titanic_train['Sex'].loc[titanic_train['Sex'] == \"male\"].count()\/titanic_train['Sex'].count()]\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Sex Ratio',  fontsize=13,fontweight='bold')\nplt.show()\n\n","203345b5":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib\n\n\ndf=pd.crosstab(titanic_train['Sex'],titanic_train['Survived']).apply(lambda r: r\/r.sum(), axis=1)\ndf.plot.bar()\nplt.ylabel('Percentage of Survivors by Sex',  fontsize=13,fontweight='bold')\nplt.xlabel('Sex',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\n\nL=plt.legend()\nL.get_texts()[0].set_text('Died')\nL.get_texts()[1].set_text('Survived')\nplt.show()\n\nplt.show()","c6dd0062":"titanic_train.groupby('Sex').Age.plot(kind='kde')\n\nplt.ylabel('Percentage of Passengers',  fontsize=13,fontweight='bold')\nplt.xlabel('Age',  fontsize=13,fontweight='bold')\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\nplt.xlim(xmin=0,xmax=100)\nplt.legend()\nplt.show()","d66b1408":"titanic_train.groupby('Pclass').Age.plot(kind='kde')\nplt.ylabel('Percentage of Passengers',  fontsize=13,fontweight='bold')\nplt.xlabel('Age',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\nplt.xlim(xmin=0,xmax=100 )\nplt.legend()\nL=plt.legend()\nL.get_texts()[0].set_text('1st Class')\nL.get_texts()[1].set_text('2nd Class')\nL.get_texts()[2].set_text('3rd Class')\n\n\nplt.show()","42d72fbd":"import matplotlib.pyplot as plt\n\nranges = [0,30,60,90,120,150,180,550]\n\n\ndf1=pd.DataFrame(titanic_train['Fare'].loc[titanic_train['Survived'] == 1].value_counts())\ndf1=df1.groupby(pd.cut(df1.index, ranges)).sum()\ndf1.rename(columns={'Fare': 'Survivors'})\ndf2=pd.DataFrame(titanic_train['Fare'].value_counts())\ndf2=df2.groupby(pd.cut(df2.index, ranges)).sum()\ndf2.rename(columns={'Fare': 'Survivors'})\n\ndf3=df1\/df2\ndf3.plot.bar(legend=None, color='orange')\nplt.xticks(rotation=60)\n\nplt.xlabel('Fare Range',  fontsize=13,fontweight='bold')\nplt.ylabel('Percentage of Survivors per Fare range',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n","ea7c8b96":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\ndf=pd.crosstab(titanic_train['SibSp'],titanic_train['Survived']).apply(lambda r: r\/r.sum(), axis=1)\ndf.plot.bar()\nplt.ylabel('Percentage of Survivors by SibSp',  fontsize=13,fontweight='bold')\nplt.xlabel('Number of Siblings \/ Spouses',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\nL=plt.legend()\nL.get_texts()[0].set_text('Died')\nL.get_texts()[1].set_text('Survived')\nplt.show()","d2a190e0":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\ndf=pd.crosstab(titanic_train['Parch'],titanic_train['Survived']).apply(lambda r: r\/r.sum(), axis=1)\ndf.plot.bar()\nplt.ylabel('Percentage of Survivors by Parch',  fontsize=13,fontweight='bold')\nplt.xlabel('Number  of Parents \/ Children',  fontsize=13,fontweight='bold')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.xticks(rotation=360)\nL=plt.legend()\nL.get_texts()[0].set_text('Died')\nL.get_texts()[1].set_text('Survived')\nplt.show()","4f47f814":"def convert(list): # convert function joins a list of strings into a single string\n      \n    # Converting integer list to string list \n    s = [str(i) for i in list] \n      \n    # Join list items using join() \n    res = int(\"\".join(s)) \n      \n    return(res) \n\nfor i in range(0,len(titanic_train['Cabin'])): # Encoding Cabin training column \n        \n    if isinstance(titanic_train['Cabin'][i], str):\n        \n        number = [ord(letter)  for letter in titanic_train['Cabin'][i]]\n        \n        titanic_train['Cabin'][i]=convert(number)\n\nfor i in range(0,len(titanic_test['Cabin'])): # Encoding Cabin Final testing column \n         \n    if isinstance(titanic_test['Cabin'][i],str):\n        \n        number = [ord(letter)  for letter in titanic_test['Cabin'][i]]\n        \n        titanic_test['Cabin'][i]=convert(number)\n\n    \nfor i in range(0,len(titanic_train['Ticket'])): # Encoding Ticket training column \n        \n    if isinstance(titanic_train['Ticket'][i],str ):\n        \n        number = [ord(letter)  for letter in titanic_train['Ticket'][i]]\n        \n        titanic_train['Ticket'][i] = convert(number)\n\nfor i in range(0,len(titanic_test['Ticket'])):  # Encoding Ticket Final testing column \n        \n    if isinstance(titanic_test['Ticket'][i], str):\n\n        number = [ord(letter)  for letter in titanic_test['Ticket'][i]]\n\n        titanic_test['Ticket'][i] = convert(number)\n\nfor i in range(0,len(titanic_train['Name'])): # Encoding Name training column\n        \n    if isinstance(titanic_train['Name'][i],str ):\n        \n        number = [ord(letter)  for letter in titanic_train['Name'][i]]\n        \n        titanic_train['Name'][i] = convert(number)\n\nfor i in range(0,len(titanic_test['Name'])): # Encoding Name Final testing column\n        \n    if isinstance(titanic_test['Name'][i], str):\n\n        number = [ord(letter)  for letter in titanic_test['Name'][i]]\n\n        titanic_test['Name'][i] = convert(number)\n\n# Converting Variables Types to Integers\n\ntitanic_train['Ticket']=pd.to_numeric(titanic_train['Ticket'], downcast= 'integer') \ntitanic_test['Ticket']=pd.to_numeric(titanic_test['Ticket'], downcast= 'integer')\n\ntitanic_train['Cabin']=pd.to_numeric(titanic_train['Cabin'], downcast='integer')\ntitanic_test['Cabin']=pd.to_numeric(titanic_test['Cabin'], downcast='integer')\n\ntitanic_train['Name']=pd.to_numeric(titanic_train['Name'], downcast='integer')\ntitanic_test['Name']=pd.to_numeric(titanic_test['Name'], downcast='integer')\n\n#Training dataset after encoding\n\ntitanic_train.describe()","87442692":"import pandas as pd \n\ntitanic_train['Sex']=titanic_train['Sex'].replace('female',0)\ntitanic_train['Sex']=titanic_train['Sex'].replace('male',1 )\n\ntitanic_test['Sex']=titanic_test['Sex'].replace('female',0)\ntitanic_test['Sex']=titanic_test['Sex'].replace('male',1 )\n\ntitanic_train['Embarked']=titanic_train['Embarked'].replace('C',1 )\ntitanic_train['Embarked']=titanic_train['Embarked'].replace('Q',2 )\ntitanic_train['Embarked']=titanic_train['Embarked'].replace('S',3 )\n\ntitanic_test['Embarked']=titanic_test['Embarked'].replace('C',1 )\ntitanic_test['Embarked']=titanic_test['Embarked'].replace('Q',2 )\ntitanic_test['Embarked']=titanic_test['Embarked'].replace('S',3 )\n","ba9f4d7d":"#Test Whether there is any Missing Values in the Datasets\n\ntitanic_train.Age.isna().any().any()","6b380dc5":"#Frequency of Missing Values for Variable 'Age'\n\ntitanic_train['Age'].isnull().sum() \/ len(titanic_train)","6ad3917a":"#Summary of missing Values per Feature for Training Dataset\n\nprint(titanic_train.isnull().sum())\nprint(titanic_test.isnull().sum())","8b246d81":"#Frequency of Missing Values for Variable 'Cabin'\n\ntitanic_train['Cabin'].isnull().sum() \/ len(titanic_train)","82a70286":"#X_train represent the Table of Features of the Training Set\nX_train = titanic_train.drop(['Survived','Name','PassengerId'], axis=1)\n\n#X_testfinal represent the Table of Features of the Testing  Set\nX_testfinal =titanic_test.drop(['Name','PassengerId'], axis=1)\n\n#Standardizing the Data\n\n\n#Code For KNN imputation\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=27)\nX_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\n\nimputer = KNNImputer(n_neighbors=20)\nX_testfinal = pd.DataFrame(imputer.fit_transform(X_testfinal),columns = X_testfinal.columns)\n\n#Summary of missing Values per Feature for Training Dataset\nprint(X_train.isnull().sum())","829b50c2":"#Summary of Missing Values After Filling for Testing Dataset\n\nprint(X_testfinal.isnull().sum())","5921f9c2":"# Code for Correlation Matrix\nimport seaborn as sns\n\nplt.figure(figsize=(15, 15))\nsns.heatmap(titanic_train.corr(), annot = True,square=True,cmap=plt.cm.Reds)\n","3abc7910":"\nY_train=titanic_train.Survived\n\nX_train_new = X_train\n    \nX_testfinal_new = X_testfinal\n    \nY_train_new=Y_train\n    \n### PCA Code\n\n        \nfrom sklearn.decomposition import PCA\npca = PCA(.999)\npca.fit(pd.concat([X_train_new, X_testfinal_new], axis=0))\nX_train_new2 = pd.DataFrame(pca.transform(X_train_new))\nX_testfinal_new2 = pd.DataFrame(pca.transform(X_testfinal_new))\nX_train_new = pd.concat([X_train_new2,X_train_new], axis=1)\nX_testfinal_new = pd.concat([X_testfinal_new2,X_testfinal_new], axis=1)\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=3)\npoly.fit(pd.concat([X_train_new,X_testfinal_new], axis=0))\nX_train_new = pd.DataFrame(poly.transform(X_train_new))\nX_testfinal_new = pd.DataFrame(poly.transform(X_testfinal_new))\nfrom sklearn.decomposition import PCA\npca = PCA(.999)\npca.fit(pd.concat([X_train_new, X_testfinal_new], axis=0))\nX_train_new2 = pd.DataFrame(pca.transform(X_train_new))\nX_testfinal_new2 = pd.DataFrame(pca.transform(X_testfinal_new))\nX_train_new = pd.concat([X_train_new2,X_train_new], axis=1)\nX_testfinal_new = pd.concat([X_testfinal_new2,X_testfinal_new], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(pd.concat([X_train_new, X_testfinal_new], axis=0)) \nX_train_new = pd.DataFrame(scaler.transform(X_train_new))\nX_testfinal_new = pd.DataFrame(scaler.transform(X_testfinal_new))\nimport numpy as np\nfrom numpy import arange\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nnp.random.seed(0)\nclf = Lasso()\n# Create space of candidate learning algorithms and their hyperparameters\nsearch_space = [{'alpha': arange(0.01,10,0.01),'max_iter':[100000]}]\n\nclf = GridSearchCV(clf, search_space, cv=5, verbose=1, n_jobs = -1)\nbestmodel=clf.fit(X_train_new, Y_train_new)\nsel_ = SelectFromModel(Lasso(alpha=bestmodel.best_params_['alpha'],max_iter=bestmodel.best_params_['max_iter']))\nsel_.fit(X_train_new, Y_train_new)\nX_train_new = pd.DataFrame(sel_.transform(X_train_new))\nX_testfinal_new = pd.DataFrame(sel_.transform(X_testfinal_new))\n","28e5ece7":"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_train_new)\nfrom sklearn.metrics import accuracy_score\n\nfrom bayes_opt import BayesianOptimization\n\ndef evaluate_model(n_estimators,max_depth,min_samples_split,min_samples_leaf):\n    acc=[]\n    for train_index, test_index in kf.split(X_train_new):\n        rf=RandomForestClassifier(random_state = 42, n_jobs = -1,verbose=1,n_estimators=int(n_estimators),max_depth=int(max_depth),min_samples_split=int(min_samples_split),min_samples_leaf=int(min_samples_leaf)).fit(X_train_new.iloc[train_index], Y_train_new.iloc[train_index])\n        acc.append(accuracy_score(Y_train_new.iloc[test_index],rf.predict(X_train_new.iloc[test_index])))\n    \n    return min(acc)\n# Number of trees in random forest\nn_estimators = (200,4000)\n# Number of features to consider at every split\n# Maximum number of levels in tree\nmax_depth = (10,200)\n# Minimum number of samples required to split a node\nmin_samples_split = (2,30)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (1,30)\n# Method of selecting samples for training each tree\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\noptimizer = BayesianOptimization(f = evaluate_model, pbounds = random_grid,random_state=42)\n\noptimizer.maximize(100,60)\nimport statistics \n\n# Fit the random search model\nfrom sklearn.metrics import accuracy_score\nacc=[]\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_train_new)\nfor train_index, test_index in kf.split(X_train_new):\n    \n\n    best_random = RandomForestClassifier(random_state = 42, n_jobs = -1,verbose=1,n_estimators=int(optimizer.max['params']['n_estimators']),max_depth=int(optimizer.max['params']['max_depth']),min_samples_split=int(optimizer.max['params']['min_samples_split']),min_samples_leaf=int(optimizer.max['params']['min_samples_leaf'])).fit(X_train_new.iloc[train_index], Y_train_new.iloc[train_index])\n\n    acc.append(accuracy_score(Y_train_new.iloc[test_index],best_random.predict(X_train_new.iloc[test_index])))\n\n     \nprint(statistics.mean(acc)) \n\n\nbest_random = RandomForestClassifier(random_state = 42, n_jobs = -1,verbose=1,n_estimators=int(optimizer.max['params']['n_estimators']),max_depth=int(optimizer.max['params']['max_depth']),min_samples_split=int(optimizer.max['params']['min_samples_split']),min_samples_leaf=int(optimizer.max['params']['min_samples_leaf'])).fit(X_train_new, Y_train_new)\n\ntest_preds2 = best_random.predict(X_testfinal_new)\ntest_preds2\n\n\n\noutput = pd.DataFrame({'PassengerId': titanic_test[\"PassengerId\"],'Survived': test_preds2})\noutput.to_csv('Submission.csv', index=False)\n\n","38d7bfb6":"# 4. Missing Values:\n \nBefore we move on with the analysis, we have another remark which is missing values in the variable Age. We can find that from the count of variable Age which is less than the count of Passengers or by simply running the following code:","3085e664":"For the Variables : \"PassengerId\", \"Survived\" , \"Pclass\", the statistics **mean** and **standard deviation** (**std**) are meaningless.\n\nWhat informations does this table brings:\n\nAt First, we can see that we have **891** passenger. their **mean** age is about **29.6** with a standard deviation of **14.5** years which means that for **+** and **- 28** years we capture more than **60%** of the population assuming that we a **Normal distribution** of Ages of the Titanic Population.\n\nBy continuing to read the table we get more information for example the **75%** quantile is equal to **38** years and that means that **75%** of the passengers have less than **38** years of age which make the Titanic population a quite young one.\n\nAt Second, from the columns of **SibSp** and **Parch** we see that both means are less than **1** which is normal and we can conclude that half of the population have one Sibling\/Spouse and **38%** of the population have **1** Parent\/Child.\n\nAt Last, We have an Important Variable which is **Fare**, this Variable have a high chance to be most explanatory Variable in the final model. Let's start by observing it's statistics: we can see from the mean and std that it has a very high standard deviation which means that there is a big disparity of Fare between the passengers and we can suspect that based on the median value compared to the max and the **75%** quantile. Anyway this will either be  confirmed or disconfirmed by the plots that we're going to make after this.\n\n**P.S: There are 64% male Against 36% female.**\n","627d666f":" # 5. Feature Selection:\n\nThe Objective here is to find the Best Variables to use in our model.","6dc1f8b3":"**Comment:**\n\nThe ratio of Survivors is Very low For Poor People Compared to Rich ones.\n","9106dfc4":"So in total we have three  variables which are presenting missing values one Numerical variable which is **Age** and another one wich is a String Variable: **Cabin** and finally **Embarked** variable which presents the lowest frequencey of missing values of only 2 out of 891.","465a79f1":"**Comment:**\n\nThe chart above represents the Percentage of Survivors and Dead Passengers in Each Class.\nWe can see that Class 1 and Class 3 represent opposite behaviours of Variable 'Survived' as the Majority of Passengers of Class 3 have Died Which is the opposite of what happened in Class 1.","27d0b137":"From the data we can spot many strong correlations such as **Corr(Pclass,Fare)=-0.549** also **Corr(SibSp,Parch)=0.414** but the conclusion is that Sex Variable has the highest correlation with the Survived Variable which is our Target.\n","f5329668":"* **Plotting The Variable 'Survived':** ","e1f5c720":" * First, Let's start by importing the DataSets and reading it.","0a5abbb6":"That returns True.\nLet's Calculate the missing values Frequency in Age Column:","04b1f9e5":"**Comment:**\n\nThe more Siblings\/Spouses you have the more is the chance that you die.","d76ad439":"**Comment:**\n\nThe Conclusion for the Above Chart is that the more Age we have the higher is the probability to be in a Higher Class.","4cd84ca3":"* **Plotting of Variable 'Sex':**","6bc419d1":"# 3. Encoding Categorical Variables:\n\nThe Variables '**Sex**' and '**Embarked**' are Categorical and of type string so they must be Coded into numbers:","f9169993":" # 1. Data Visualisation:\n\n\nNow, Let's move to the plots:\n\n","206e7685":"* **Plotting Of The Variable 'SibSp':**","70f21995":"This analysis is still poor. To have a better information let's bring out our correlation matrix:\n","fd055edf":"The mean Score of this model is **80%** but there is still room for improvement.","5ee338f7":"**Comment:** \n\nThe Pie Chart is a way of representing the ratio of Survivors in the Passengers Population of the Training Data.\nWe can see that the amount of survivors counts for about 38%.","c6a9f6db":"* **Plotting Of The Variable 'Age':**","3e9a6450":"* **Plotting Of The Variable 'Parch':**","419b112c":"The '**Cabin**' Variable presents a very high frequency of missing values: of every 10 rows there are more than 7 rows with missing Cabin value.\n\nTo solve the missing Values Problem we can try a KNN imputation of the 5 nearest Values of missing values,the problem that will remain unsolvable is the Cabin Variable missing values as there is no logic of replacement available.","f2c96d04":"So of every Ten rows 2 are having missing values in the variable **Age**.\n\nLet's run the code and get a Summary:\n\n","e90dfeb6":"* #                                       **Titanic Disaster Competition**\n\n\nThis is my first Competition at Kaggle.\nI will try to perform a good analysis of the data and build an appropriate model.\n\n\n*** Project Steps:**\n\n**1. Data Visualization: Bar Charts, Pie Charts, Density plots, Heatmap, etc... Using Matplotlib and Seaborn.**\n\n**2. Encoding String and Categorical Variables.**\n\n**3. Missing Values Imputation.**\n\n**4. Feature generation with PCA.**\n\n**5. Automatic Feature Selection with Lasso Regression & Cross Validation.**\n\n**6. Building a Random Forest Model & Tuning Hyper-parameters with Bayesian Optimization along with Cross Validation.**\n\n\n","de60bd91":" # 5. Random Forest Model:","98aea529":"**Comment:**\n\nFor the Female Sex the Majority Survived, While the Opposite happened for the Male Sex.","c23ac3ae":"**Comment:** \n\nThe Chart Above represents the Percentage of Passengers in each Class.\nWe can see that the amount of Passengers in Class 3 is the widest and it is the smallest in Class 2.","3213306a":"Following this Url: https:\/\/www.kaggle.com\/c\/titanic\/data , you'll find the Data Dictionnary which explains the meaning of variables names.\n\nInput variables:\n\n* **pclass**: Ticket class - Categorical - Values: 1 = 1st, 2 = 2nd, 3 = 3rd.\n* **sex**: Categorical - Values: male or female.\n* **Age**: Age in years - Numerical.\n* **sibsp**: # of siblings \/ spouses aboard the Titanic - Numerical.\n* **parch**: # of parents \/ children aboard the Titanic - Numerical.\n* **ticket**: Ticket number - Mix of Numbers and Characters.\n* **fare**: Passenger fare - Numerical.\n* **cabin**: Cabin number - Mix of Numbers and Characters.\n* **embarked**: Port of Embarkation - Categorical - Values: C = Cherbourg, Q = Queenstown, S = Southampton.\n\nTarget Variable:\n* **survival**: Categorical - Values: 0 = No, 1 = Yes.\n\n","469b0b2e":"* **Plotting of Variable 'Pclass':**","2b7487af":"# 2. Encoding String Type Variables:\n \nThe Variables 'Cabin', 'Ticket' and 'Name' are non Categorical String Variables and the first two of them might present a high prediction power so we will try to encode them into there respective Integer Unicodes:","9c8e7f7f":"* **Plotting Of The Variable 'Fare':**","4fa55468":"**Comment:**\n\nWe Conclude from the Chart Above That Age has identical Distribution By Sex in the Population of Passengers.","bb297c66":"**Comment:**\n\nThe more Parents\/Children you have the more is the chance that you've died on the Titanic."}}