{"cell_type":{"66dd4dab":"code","86eadf90":"code","21424c2e":"code","26fa3d02":"code","e15950e3":"code","9eb98a96":"code","bf4e8550":"code","94dac86a":"code","e79b84b4":"code","dac5824b":"code","cffbc6fb":"code","5e560f81":"code","e56de677":"code","f02deadb":"code","3a13a547":"code","598fe9ab":"code","7327442b":"code","e16c7999":"code","96cdb2e7":"code","7155c3cf":"code","398fbe09":"code","14ce4ba8":"code","d4a6eb07":"code","a9aa2341":"code","7d24ca3d":"code","39c3c259":"code","fecae8c9":"code","570e1730":"code","c2620ead":"code","f25ebf84":"code","e1e7419e":"code","a17fc31b":"code","c472aab3":"code","52e580e5":"code","dd10b284":"code","a48c3433":"code","ccbb6d6b":"code","9d2ffa33":"code","84e3444d":"code","cc0c4fdd":"code","f4b3eb84":"code","88202727":"code","649cbc11":"code","a4a4316d":"code","0036152b":"code","6b2bb1d0":"code","ef952fd3":"code","16df3d02":"code","d9fa4aae":"code","8d7fec01":"code","b02796c2":"code","99a69c15":"code","f45b1875":"code","98f3329f":"markdown","b24da485":"markdown","c7c5dbfc":"markdown","8e4b125a":"markdown","44fb68f7":"markdown","d7c096d4":"markdown","323c6f19":"markdown","9853f66a":"markdown","af0e9486":"markdown","984cc81d":"markdown","021bf0ba":"markdown","a41b34e2":"markdown","f7fec89f":"markdown","808420ad":"markdown","145de25b":"markdown","e95c5a37":"markdown","daa15ee0":"markdown","665627fb":"markdown","92a5d212":"markdown","bd28df2f":"markdown","a1581df2":"markdown","1ce9a334":"markdown","c500ab00":"markdown","5a9ab407":"markdown","5b3f4ac6":"markdown","fee18cc2":"markdown","ffa354a3":"markdown","e879b3d3":"markdown","df3f204e":"markdown","6dbc0473":"markdown","13c97951":"markdown","bb4c6216":"markdown","a03c2c7d":"markdown","ce6e7331":"markdown","53c0d051":"markdown","b5db4ad1":"markdown","ca04121e":"markdown","7c89b4c0":"markdown","3416015b":"markdown","63626048":"markdown","ed9fff6f":"markdown","4fc04891":"markdown","5adfbd70":"markdown","2476201e":"markdown","c6e0a351":"markdown","9a762b63":"markdown","f5e724f2":"markdown","d8111693":"markdown","d6f7090e":"markdown","6ee3f4c3":"markdown"},"source":{"66dd4dab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","86eadf90":"path = '\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/'\ntrain = pd.read_csv(path + 'aug_train.csv')\ntest = pd.read_csv(path + 'aug_test.csv')\nsample_submission = pd.read_csv(path + 'sample_submission.csv')","21424c2e":"print('Train shape:', train.shape)\ntrain.head()","26fa3d02":"train.info()","e15950e3":"df_null = pd.DataFrame(train.isnull().sum())\ndf_null = df_null.rename(columns={0:'Number of null values'})\ndf_null['Percentage null values'] = round(train.isnull().sum()\/train. enrollee_id.count()*100,2)\ndf_null","9eb98a96":"print('Test shape:', test.shape)\ntest.head()","bf4e8550":"test.info()","94dac86a":"\ndf_null = pd.DataFrame(test.isnull().sum())\ndf_null = df_null.rename(columns={0:'Number of null values'})\ndf_null['Percentage null values'] = round(test.isnull().sum()\/test. enrollee_id.count()*100,2)\ndf_null","e79b84b4":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(16,4))\naux = train['target'].value_counts().to_frame()\nplt.title('Frequency of Target')\naux.plot.bar(ax = axes)\nplt.show()","dac5824b":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.45)\n# Train\naxes[0].title.set_text('Frequency of the cities (represented >50)- Train set')\naux = train['city'].value_counts().to_frame()\ninde_50=list(aux[aux.city>50].index)\ndf_city_target = train.groupby('city')['target'].value_counts().to_frame().unstack()\ndf_city_target[df_city_target.index.isin(inde_50)].reindex(inde_50).plot.bar(ax = axes[0], stacked = True)\naxes[0].legend(['Not job change', 'Job change'])\n\n\n# Test\naxes[1].title.set_text('Frequency of the cities (represented >10)- Test set')\naux = test['city'].value_counts().to_frame()\naux[aux.city>10].plot.bar(ax = axes[1])\n\nplt.show()","cffbc6fb":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.45)\n\ntrain_not_looking = train[train['target']==0].city_development_index\ntrain_looking = train[train['target']==1].city_development_index\ntrain_total = train.city_development_index\n\n# train\nsns.distplot(train_not_looking, ax=axes[0] )\nsns.distplot(train_looking, ax = axes[0])\nsns.distplot(train_total, ax = axes[0] ).set_title('city_development_index histrogram and density function - Train set')\naxes[0].legend(['Not job change', 'Job change','Total'])\n\n\n#test\nsns.distplot(test.city_development_index, ax = axes[1]).set_title('city_development_index histrogram and density function - Test set')\nplt.show()","5e560f81":"df_gender_target = train.groupby('gender')['target'].value_counts().to_frame().unstack()\ndf = pd.DataFrame(df_gender_target.sum(axis=1))\ndf = df.rename(columns={0:'Total'})\ndf['Percen 0 gender'] = np.round((df_gender_target.target[[0.0]][0.0].values\/df.Total.values) * 100,2)\ndf['Percen 1 gender'] = np.round((df_gender_target.target[[1.0]][1.0].values\/df.Total.values) * 100,2)\ndf","e56de677":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,10))\nplt.subplots_adjust(hspace = 0.4)\n# Train\ndf_gender_target = train.groupby('gender')['target'].value_counts().to_frame().unstack()\ndf_gender_target.reindex(['Male', 'Female', 'Other']).plot.bar(ax = axes[0], stacked = True)\naxes[0].title.set_text('Frequency of Gender- Train set')\naxes[0].legend(['Not job change', 'Job change'])\n# Test\naxes[1].title.set_text('Frequency of Gender- Test set')\naux = test['gender'].value_counts().to_frame()\naux.plot.bar(ax = axes[1])\n\nplt.show()","f02deadb":"df_rel_exp_target = train.groupby('relevent_experience')['target'].value_counts().to_frame().unstack()\ndf = pd.DataFrame(df_rel_exp_target.sum(axis=1))\ndf = df.rename(columns={0:'Total'})\ndf['Percen 0 Has expe'] = np.round((df_rel_exp_target.target[[0.0]][0.0].values\/df.Total.values) * 100,2)\ndf['Percen 1 No expe'] = np.round((df_rel_exp_target.target[[1.0]][1.0].values\/df.Total.values) * 100,2)\ndf","3a13a547":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,10))\nplt.subplots_adjust(hspace = 0.7)\n# Train\ndf_rel_exp_target = train.groupby('relevent_experience')['target'].value_counts().to_frame().unstack()\ndf_rel_exp_target.plot.bar(ax = axes[0], stacked = True,)\naxes[0].set_xticklabels(list(df_rel_exp_target.index.values),rotation=25, ha='right')\naxes[0].title.set_text('Frequency of relevent_experience- Train set')\naxes[0].legend(['Not job change', 'Job change'])\n# Test\naxes[1].title.set_text('Frequency of relevent_experience- Test set')\naux = test['relevent_experience'].value_counts().to_frame()\naux.plot.bar(ax = axes[1])\naxes[1].set_xticklabels(list(aux.index.values),rotation=25, ha='right')\nplt.show()","598fe9ab":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.5)\n# Train\ndf_experience_target = train.groupby('experience')['target'].value_counts().to_frame().unstack()\nind = ['<1','1','3', '4', '5', '6', '7', '8', '9','2','10', '11', '12', '13', '14',\n     '15', '16', '17', '18', '19', '20', '>20']\ndf_experience_target = df_experience_target.reindex(index=ind).plot.bar(ax = axes[0], stacked = True)\naxes[0].set_xticklabels(ind,rotation=0, ha='right')\naxes[0].title.set_text('Frequency of experience- Train set')\naxes[0].legend(['Not job change', 'Job change'])\n# Test\naxes[1].title.set_text('Frequency of experience- Test set')\naux = test['experience'].value_counts().to_frame()\naux = aux.reindex(index=ind)\naux.plot.bar(ax = axes[1])\naxes[1].set_xticklabels(ind,rotation=0, ha='right')\nplt.show()","7327442b":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.5)\n# Train\ndf_size_comp_target = train.groupby('company_size')['target'].value_counts().to_frame().unstack()\nind = ['<10','10\/49','50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\ndf_size_comp_target.reindex(index=ind).plot.bar(ax = axes[0], stacked = True)\naxes[0].set_xticklabels(ind,rotation=25, ha='right')\naxes[0].title.set_text('Frequency of company_size - Train set')\naxes[0].legend(['Not job change', 'Job change'])\n# Test\naxes[1].title.set_text('Frequency of company_size - Test set')\naux = test['company_size'].value_counts().to_frame()\naux = aux.reindex(index=ind)\naux.plot.bar(ax = axes[1])\naxes[1].set_xticklabels(ind,rotation=25, ha='right')\nplt.show()","e16c7999":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.5)\n# Train\ndf_type_comp_target = train.groupby('company_type')['target'].value_counts().to_frame().unstack()\nind =['Pvt Ltd','Public Sector','Funded Startup','Early Stage Startup', 'NGO', 'Other']\ndf_type_comp_target.reindex(ind).plot.bar(ax = axes[0], stacked=True)\naxes[0].set_xticklabels(ind,rotation=25, ha='right')\naxes[0].title.set_text('Frequency of company_type - Train set')\naxes[0].legend(['Not job change', 'Job change'])\n# Test\naxes[1].title.set_text('Frequency of company_type - Test set')\naux = test['company_type'].value_counts().to_frame()\naux.plot.bar(ax = axes[1])\naxes[1].set_xticklabels(ind,rotation=25, ha='right')\nplt.show()","96cdb2e7":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,8))\nplt.subplots_adjust(hspace = 0.5)\n# Train\ndf_last_new_job_target = train.groupby('last_new_job')['target'].value_counts().to_frame().unstack()\nind = ['never','1','2','3', '4', '>4']\ndf_last_new_job_target.reindex(index=ind).plot.bar(ax = axes[0], stacked=True)\naxes[0].legend(['Not job change', 'Job change'])\naxes[0].title.set_text('Frequency of last_new_job - Train set')\n# Test\naxes[1].title.set_text('Frequency of last_new_job - Test set')\naux = test['last_new_job'].value_counts().to_frame()\naux = aux.reindex(index=ind)\naux.plot.bar(ax = axes[1])\n\nplt.show()","7155c3cf":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,9))\nplt.subplots_adjust(hspace = 0.6)\n\n# Train\naux = train['training_hours'].value_counts().to_frame()\nn = np.linspace(min(aux.index), max(aux.index), 70, endpoint = True,dtype = int)\ntrain['train_hours_2'] = pd.cut(train.training_hours, n)\ntest['train_hours_2'] = pd.cut(test.training_hours, n)\n\ndf_train_hours_target = train.groupby('train_hours_2')['target'].value_counts().to_frame().unstack()\ndf_train_hours_target.sort_index().plot.bar(ax = axes[0], stacked = True)\naxes[0].title.set_text('Frequency of training_hours - Train set')\naxes[0].legend(['Not job change', 'Job change'])\n\n# Test\n\naux_1 = test['train_hours_2'].value_counts().to_frame()\naux_1.sort_index().plot.bar(ax = axes[1])\naxes[1].title.set_text('Frequency of training_hours - Test set')\naxes[1].legend(['training hours'])\nplt.show()","398fbe09":"df_enro_uni_target = train.groupby('enrolled_university')['target'].value_counts().to_frame().unstack()\ndf = pd.DataFrame(df_enro_uni_target.sum(axis=1))\ndf = df.rename(columns={0:'Total'})\ndf['Percen 0 enrolled_university'] = np.round((df_enro_uni_target.target[[0.0]][0.0].values\/df.Total.values) * 100,2)\ndf['Percen 1 enrolled_university'] = np.round((df_enro_uni_target.target[[1.0]][1.0].values\/df.Total.values) * 100,2)\ndf","14ce4ba8":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,9))\nplt.subplots_adjust(hspace = 0.6)\n\n# Train\ndf_enro_uni_target = train.groupby('enrolled_university')['target'].value_counts().to_frame().unstack()\ndf_enro_uni_target.sort_index().plot.bar(ax = axes[0], stacked = True)\naxes[0].title.set_text('Frequency of enrolled university - Train set')\naxes[0].legend(['Not job change', 'Job change'])\naxes[0].set_xticklabels(df_enro_uni_target.index.values,rotation=25, ha='right')\n\n# Test\n\naux_1 = test['enrolled_university'].value_counts().to_frame()\naux_1.sort_index().plot.bar(ax = axes[1])\naxes[1].title.set_text('Frequency of enrolled university - Test set')\naxes[1].set_xticklabels(aux_1.index.sort_values(),rotation=25, ha='right')\n\nplt.show()","d4a6eb07":"df_edu_level_target = train.groupby('education_level')['target'].value_counts().to_frame().unstack()\ndf = pd.DataFrame(df_edu_level_target.sum(axis=1))\ndf = df.rename(columns={0:'Total'})\ndf['Percen 0 education_level'] = np.round((df_edu_level_target.target[[0.0]][0.0].values\/df.Total.values) * 100,2)\ndf['Percen 1 education_level'] = np.round((df_edu_level_target.target[[1.0]][1.0].values\/df.Total.values) * 100,2)\ndf\n","a9aa2341":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,9))\nplt.subplots_adjust(hspace = 0.6)\n\n# Train\ndf_edu_level_target = train.groupby('education_level')['target'].value_counts().to_frame().unstack()\nind = ['Primary School','High School','Graduate','Masters','Phd'] \ndf_edu_level_target.reindex(ind).plot.bar(ax = axes[0], stacked = True)\naxes[0].title.set_text('Frequency of education level - Train set')\naxes[0].legend(['Not job change', 'Job change'])\naxes[0].set_xticklabels(ind,rotation=25, ha='right')\n\n# Test\n\naux_1 = test['education_level'].value_counts().to_frame()\naux_1.reindex(ind).plot.bar(ax = axes[1])\naxes[1].title.set_text('Frequency of education_level - Test set')\naxes[1].set_xticklabels(ind,rotation=25, ha='right')\n\nplt.show()","7d24ca3d":"df_MajDisci_target = train.groupby('major_discipline')['target'].value_counts().to_frame().unstack()\ndf = pd.DataFrame(df_MajDisci_target.sum(axis=1))\ndf = df.rename(columns={0:'Total'})\ndf['Percen 0 education_level'] = np.round((df_MajDisci_target.target[[0.0]][0.0].values\/df.Total.values) * 100,2)\ndf['Percen 1 education_level'] = np.round((df_MajDisci_target.target[[1.0]][1.0].values\/df.Total.values) * 100,2)\ndf\n","39c3c259":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(16,9))\nplt.subplots_adjust(hspace = 0.6)\n\n# Train\ndf_MajDisci_target = train.groupby('major_discipline')['target'].value_counts().to_frame().unstack()\nind=['STEM','Business Degree', 'Humanities', 'Arts', 'No Major', 'Other']\ndf_MajDisci_target.reindex(ind).plot.bar(ax = axes[0], stacked = True)\naxes[0].title.set_text('Frequency of major discipline - Train set')\naxes[0].legend(['Not job change', 'Job change'])\naxes[0].set_xticklabels(ind,rotation=25, ha='right')\n\n# Test\n\naux_1 = test['major_discipline'].value_counts().to_frame()\naux_1.reindex(ind).plot.bar(ax = axes[1])\naxes[1].title.set_text('Frequency of major discipline - Test set')\naxes[1].set_xticklabels(ind,rotation=25, ha='right')\n\nplt.show()","fecae8c9":"def find_number(text):\n    num = re.findall(r'[0-9]+',text)\n    return \" \".join(num)\n\ntrain['city'] = train['city'].apply(lambda x: find_number(x))\ntrain['city']= train['city'].astype(int)\ntest['city'] = test['city']. apply(lambda x: find_number(x))\ntest['city']= test['city'].astype(int)","570e1730":"train['gender']= train.gender.replace({'Male':1, 'Female':2, 'Other':0})\ntest['gender']= test.gender.replace({'Male':1, 'Female':2, 'Other':0})","c2620ead":"train['relevent_experience']=train.relevent_experience.replace({'Has relevent experience':1, 'No relevent experience':0})\ntest['relevent_experience']=test.relevent_experience.replace({'Has relevent experience':1, 'No relevent experience':0})","f25ebf84":"train['education_level'] = train.education_level.replace({'Primary School': 1,\n                                'High School': 2,\n                                'Graduate': 3,\n                                'Masters': 4,\n                                'Phd': 5})\ntest['education_level'] = test.education_level.replace({'Primary School': 1,\n                                'High School': 2,\n                                'Graduate': 3,\n                                'Masters': 4,\n                                'Phd': 5})","e1e7419e":"train['company_size'] = train.company_size.replace({'<10':0,'10\/49':1,'50-99':2, '100-500': 3,\n                            '500-999':4, '1000-4999':5, '5000-9999':6, '5000-9999': 7,'10000+':8})\ntest['company_size'] = test.company_size.replace({'<10':0,'10\/49':1,'50-99':2, '100-500': 3,\n                            '500-999':4, '1000-4999':5, '5000-9999':6, '5000-9999': 7,'10000+':8})","a17fc31b":"train['experience'] = train.experience.replace({'<1':0,'>20':21})\ntrain['experience'] = train['experience'].astype(str).astype(float)\ntest['experience'] = test.experience.replace({'<1':0,'>20':21})\ntest['experience'] = test['experience'].astype(str).astype(float)","c472aab3":"train['last_new_job'] = train.last_new_job.replace({'never':0, '>4':5})\ntrain['last_new_job'] = train['last_new_job'].astype(str).astype(float)\ntest['last_new_job'] = test.last_new_job.replace({'never':0, '>4':5})\ntest['last_new_job'] = test['last_new_job'].astype(str).astype(float)","52e580e5":"from sklearn.impute import KNNImputer","dd10b284":"col_miss = ['gender', 'education_level','experience','company_size', 'last_new_job']\ntrain_miss_knn = train[['enrollee_id'] + col_miss]\ntrain_no_miss_knn = train.drop(col_miss, axis=1)\n\ntest_miss_knn = test[['enrollee_id'] + col_miss]\ntest_no_miss_knn = test.drop(col_miss, axis=1)","a48c3433":"knn = KNNImputer(n_neighbors=5)\nknn.fit(train_miss_knn)\ntrain_miss_knn = pd.DataFrame(np.round(knn.transform(train_miss_knn)),columns = train_miss_knn.columns )\ntest_miss_knn = pd.DataFrame(np.round(knn.transform(test_miss_knn)),columns = train_miss_knn.columns )","ccbb6d6b":"df_train = pd.merge(train_miss_knn, train_no_miss_knn, on='enrollee_id')\ndf_test = pd.merge(test_miss_knn, test_no_miss_knn, on='enrollee_id')","9d2ffa33":"df_train['enrolled_university'].fillna(df_train['enrolled_university'].mode()[0], inplace=True)\ndf_test['enrolled_university'].fillna(df_test['enrolled_university'].mode()[0], inplace=True)\n\ndf_train['major_discipline'].fillna(df_train['major_discipline'].mode()[0], inplace=True)\ndf_test['major_discipline'].fillna(df_test['major_discipline'].mode()[0], inplace=True)\n\ndf_train['company_type'].fillna(df_train['major_discipline'].mode()[0], inplace=True)\ndf_test['company_type'].fillna(df_test['major_discipline'].mode()[0], inplace=True)","84e3444d":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom keras.models import Sequential\n\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import roc_curve, auc","cc0c4fdd":"features_num = ['gender', 'education_level', 'experience', 'company_size', 'last_new_job', 'city',\n                'city_development_index','relevent_experience', 'training_hours']\nfeatures_cat = ['enrolled_university', 'major_discipline','company_type']","f4b3eb84":"from imblearn.over_sampling import SMOTE\n\ndf_train_X = df_train[features_num + features_cat]\n\npreprocessor = make_column_transformer(\n                (StandardScaler(), features_num),\n                (OneHotEncoder(), features_cat))\n\nX = preprocessor.fit_transform(df_train_X)\nY = df_train[['target']]\nsmote = SMOTE(random_state = 550)\nX_smote, Y_smote = smote.fit_resample(X,Y)\n\nsmote = SMOTE(random_state = 450)\nX_smote1, Y_smote1 = smote.fit_resample(X,Y)\n\n\ndf_train_X = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(X_smote1)], axis = 0).reset_index(drop = True)\ndf_train_y = pd.concat([Y_smote, Y_smote1], axis = 0).reset_index(drop = True)\n","88202727":"X_train, X_valid, y_train, y_valid = train_test_split(df_train_X, df_train_y, test_size=0.3, random_state = 540)","649cbc11":"Input_nodes = [X_valid.shape[1]]","a4a4316d":"model=keras.Sequential([\n        layers.Dense(512, activation = 'relu', input_shape = Input_nodes), \n        layers.Dropout(0.3),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation = 'relu'),\n        layers.Dropout(0.3),\n        layers.BatchNormalization(),\n        layers.Dense(1, activation = 'sigmoid'),\n])","0036152b":"model.compile(\n            loss='binary_crossentropy',\n            optimizer='adam',\n            metrics=[tf.keras.metrics.AUC()],\n)\n","6b2bb1d0":"from tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import roc_curve, auc","ef952fd3":"early_stopping = keras.callbacks.EarlyStopping(\n                                    patience = 10,\n                                    min_delta = 0.001,\n                                    restore_best_weights= True)","16df3d02":"history = model.fit(\n            X_train, y_train,\n            validation_data = (X_valid, y_valid),\n            batch_size = 128,\n            epochs = 70,\n            callbacks = [early_stopping],\n            verbose = 1,\n)","d9fa4aae":"history_df = pd.DataFrame(history.history)","8d7fec01":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(10,8))\nhistory_df.loc[:, ['loss', 'val_loss']].plot(ax = axes[0])\nhistory_df.loc[:, ['auc', 'val_auc']].plot(ax = axes[1])\naxes[0].set_xlabel('epochs')\naxes[1].set_xlabel('epochs')\nplt.show()","b02796c2":"#test\ndf_test = df_test[features_num + features_cat]\nX_test = preprocessor.transform(df_test)\n","99a69c15":"test_preds = model.predict(X_test)\nsample_submission['target'] = [ 1 if i>=0.5 else 0 for i in test_preds]","f45b1875":"sample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","98f3329f":"##### 2.2.5.3. Company_size\n\nBoth plots have similar distribution through compani size, and the proprortion of not looking or looking for a job change seems that it does not differ too much between company_size.","b24da485":"#### 2.2.3. City development index\n\nThis feature gets values between 0 and 1, it is scaled, for that reason we are going to use a histogram and a density function in order to plot the feature. In the train set, we make distinctions between the total, not looking for a job change and looking for a job change. Comparing these three plots and test plot, they have the similar shape, changing a bit the concentration of data in these two peaks around  0.6 and 0.9. ","c7c5dbfc":"#### 3.2.1. Education_level","8e4b125a":"#### 2.2.5. Professional Experience","44fb68f7":"When we draw the target, we realise that there is not balance between the values looking or not looking for job change. Then we will use SMOTE in order to create a more homogeneous sample. ","d7c096d4":"Observing the previous results, obviously the test set contains 1 feature less than the train, which is the target. The 13 features that they have in common, they are 3 numerical and 10 categorical. Both sets have missing values in the same features and similar percentages. ","323c6f19":"### 2.2 Features\n\nLet's study the features from both sets, and how the target is represented in them.","9853f66a":"#### 3.1.2. Gender\n\nTransforming feature, being Other in 0, Male in 1 and Female in 2.","af0e9486":"## Reference \n\nhttps:\/\/www.kaggle.com\/nkitgupta\/who-will-leave-a-job-test-auc-0-93","984cc81d":"#### 2.2.2. City\n\nThe city feature is grouped and we only show the cities which are represented at least 50 times for train set plot and at least 10 times for test set plot. Both have similar shape and even the majority of cities are the same. In addition, the target is added in train plot and values do not have the same proportion in each city.  ","021bf0ba":"The data is composed by different type of features, as you can see below:\n    - Numerical: city_development_index and training_hours.\n    - Categorical: \n        - Nominal: city, gender, relevent_experience, enrolled_university, major_discipline and company_type. \n        - Ordinal: education_level, company size, experience and last_new_job. ","a41b34e2":"- Type of features","f7fec89f":"##### 2.2.6.1. Enrolled_university\n\nBoth barcharts continue to have the same shape. The majoritiy of people are not enrolled in university. However, the proportion between the values which get target differs in the three type of enrolled university. The higher proportion for people who are looking for a job change is in Full time course.","808420ad":"### 3.2. Ordinal Features\n\nWe assign a number depending on level is taking the string value.","145de25b":"#### 3.1.4 Major_discipline and company_type\n\nThese two features, we will apply them One-hot-encode.","e95c5a37":"## 1. Import libraries and dowonload data","daa15ee0":"##### 2.2.6.2. Education_level\n\nBoth barplots have the same shape, where the majority of people from data are Graduated and inside of  this group is where we can find the higher proportion of people who are looking for a job change.","665627fb":"- Null values","92a5d212":"### 3.1. Nominal Features","bd28df2f":"- Shape & dataframe's head","a1581df2":"####\u00a03.1.3. Relevent_experience\nTransforming the feature in binary type: Has relevent experince in 1 and No relevent experience in 0.","1ce9a334":"##### 2.2.5.5. Last_new_job\n\nComparing two plots of last new job, they have similar shape, where the value 1 is the most popular. Looking at the first plot, the proportion of  values from target is different through the last new job feature.","c500ab00":"Note that the target value indicates wether the person in the sample is looking for a job change (=1) or not (=0). Looking at the plot below, the target is binary and there is not balance between values, there is much less people looking for a job change than people not looking for it.","5a9ab407":"### 4.2. Mode\n","5b3f4ac6":"## 5. Modelling","fee18cc2":"##### 2.2.5.2. Experience\n\nPlotting the experience, it has the same shape for train and test data. Moreover, we can observe that the proportion of people who are or not looking for a job change, is changing through different period of experience.","ffa354a3":"##### 2.2.5.6. Training hours \n\nThe training hours feature are grouped by intervals to make it more tidy and see a structure. Then, both plots have nearly the same shape. However, focusing on the proportion of different values of looking or not looking for a job change differs through training hours.","e879b3d3":"#### 3.1.1. City\n\nWe convert the feature in number, taking only the value which is assigned.","df3f204e":"#### 3.2.3. Experience","6dbc0473":"### 4.1. KNN imputer","13c97951":"In this section, we are going to convert categorical data to numerical.","bb4c6216":"#### 2.2.1. Target","a03c2c7d":"##### 2.2.5.4. Company_type \n\nBoth graphs have the same shape. The proportion for people who are or not looking for a job change differs in different through company type.","ce6e7331":"#### 2.1.2. Test","53c0d051":"# HR Analytics job ","b5db4ad1":"##### 2.2.5.1. Relevent_Expereience\n\n\nFor train and test sets, the relevent experience has the same shape. The majority of people from these sets have a relevant experience. However, comparing the percentage of people who are looking for a job change, it is bigger for people who do not have experience.","ca04121e":"#### 2.2.4. Gender\n\nGender's plots have the same shape for train set and test set. Most of the people of both groups are men. Despite of the difference between groups are significantly, the percentage of people who are not looking or looking for a job change are quite homogenous between the gender's type.","7c89b4c0":"## Index\n- [1. Import libraries and download data](#section1)\n- [2. EDA](#section2)\n- [3. Data Engineering](#section3)\n- [4. Cleaning Data](#section4)\n- [5. Modelling](#section5)\n","3416015b":"## 4. Cleaning data\n\nIn this section, we are going to fill the missing values. We will use two process: firstly, for the features that have been converted in numerical, we will use KNN imputer, and secondly, for the features which are still categorical, we will use the mode.","63626048":"##### 2.2.6.3. Major_discipline\n\nThis feature also has the same shape for train and test set. The majority of people have the major discipline in STEM. Looking at the behaviour of the values that target can get, the porportion of them seems similar through the different major discipline.","ed9fff6f":"### 2.1. Structure\n\nLet's see what shape the data has, what type of features there are and whether they contain null values for each data set, train and test.\n\n#### 2.1.1. Train","4fc04891":"- Null values","5adfbd70":"- Shape & dataframe's head","2476201e":"#### 3.2.4. Last_new_job","c6e0a351":"#### 2.2.6. Education","9a762b63":"- Type of features","f5e724f2":"## 2. EDA\n\nWe are going to analyse the data.","d8111693":"## 3. Data Engineering","d6f7090e":"Our problem is based on binary classifcation, and we are going to use a neural network with keras as model and use it to find the predictions.","6ee3f4c3":"#### 3.2.2. Company_size"}}