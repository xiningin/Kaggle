{"cell_type":{"db2e205d":"code","b5717f4b":"code","db703a91":"code","08793c59":"code","8e72cf72":"code","c5a5a6c1":"code","56203f5e":"code","768234aa":"code","fee53be3":"code","000c07b2":"code","547f4f01":"markdown","bd083839":"markdown","ad797ebe":"markdown","a94e8ee1":"markdown","26e1796b":"markdown","7ee09b4d":"markdown","25e204e3":"markdown","6868eb22":"markdown"},"source":{"db2e205d":"import pandas as pd\nfrom time import time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, LSTM, TimeDistributed, BatchNormalization, Lambda, RepeatVector\nfrom keras.layers.core import Dense\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.losses import categorical_crossentropy\nfrom keras.utils import Sequence, plot_model\nfrom keras import backend as K\n\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","b5717f4b":"t = time()\n\nenc_dict = {'Single': '1B','Double': '2B','Triple': '3B','Home Run': 'HR','Walk': 'BB',\n            'Intent Walk': 'IBB','Hit By Pitch': 'HBP','Strikeout': 'K','Sac Fly': 'SF',\n            'Grounded Into DP': 'GIDP','Groundout': 'GO','Lineout': 'LO','Pop Out': 'PO',\n            'Flyout': 'FO','Fielders Choice': 'FC','Sac Bunt': 'SAC','Double Play': 'DP',\n            'Triple Play': 'TP','Batter Interference': 'BI','Fan interference': 'FI',\n            'Catcher Interference': 'CI','Field Error': 'ROE','Bunt Groundout': 'BGO',\n            'Bunt Lineout': 'BLO','Bunt Pop Out': 'BPO','Fielders Choice Out': 'FCO',\n            'Forceout': 'FORCE','Sacrifice Bunt DP': 'SBDP','Strikeout - DP': 'KDP',\n            'Runner Out': 'RO','Sac Fly DP': 'SFDP'\n           }\n\npitches = pd.read_csv('..\/input\/pitches.csv')\npitches = pitches[pitches['ab_id'] > 2018*1e6]\natbats = pd.read_csv('..\/input\/atbats.csv', index_col=0)\natbats = atbats[atbats.index > 2018*1e6]\natbats['year'] = (atbats.index\/\/1e6).astype(int)\natbats['stand'] = atbats['stand'] == 'R'\natbats['p_throws'] = atbats['p_throws'] == 'R'\natbats['event'] = atbats['event'].apply(lambda x: enc_dict[x])\neventcol = atbats['event']\natbats = pd.get_dummies(atbats, columns=['event'], prefix='')\natbats['event'] = eventcol\n\npitches['spin_rate'] = pitches['spin_rate'] \/ 1000\npitches['start_speed'] = pitches['start_speed'] \/ 50\npitches['spin_dir'] = pitches['spin_dir'] \/ 100\n\natbats['_H'] = atbats[['_1B', '_2B', '_3B', '_HR']].sum(axis=1)\natbats['_TB'] = atbats['_1B'] + 2*atbats['_2B'] + 3*atbats['_3B'] + 4*atbats['_HR']\natbats['_K'] = atbats['_K'] + atbats['_KDP']\natbats['_BB'] = atbats['_BB'] + atbats['_IBB']\natbats['_AB'] = 1 - atbats[['_CI', '_SAC', '_BB', \n                            '_HBP', '_RO', '_SF']].sum(axis=1)\natbats['_PA'] = 1 - atbats['_RO']\n\natbats['_outs'] = atbats['o'] - atbats.groupby(['g_id', \n                                                'inning', \n                                                'top'])['o'].shift(1).fillna(0)\n\nstats_to_cum = pd.Index(['H', 'TB', 'PA', 'AB', 'BB', 'HBP', 'IBB', 'K', \n                         '2B', '3B', 'HR', 'GIDP', 'SF'])\n\nbatter_groups = atbats.groupby(['batter_id', 'year'])\npitcher_groups = atbats.groupby(['pitcher_id', 'year'])\n\natbats[stats_to_cum] = (batter_groups['_' + stats_to_cum]\n                                     .transform(pd.Series.cumsum)).astype(int)\n\natbats['opp_' + stats_to_cum] = (pitcher_groups['_' + stats_to_cum]\n                                               .transform(pd.Series.cumsum)).astype(int)\natbats['IP'] = pitcher_groups['_outs'].transform(pd.Series.cumsum) \/ 3\n\natbats['AVG'] = atbats['H']\/atbats['AB']\natbats['SLG'] = atbats['TB']\/atbats['AB']\natbats['OBP'] = atbats[['H', 'BB', 'HBP']].sum(axis=1)\/atbats[['AB', 'BB', 'HBP', \n                                                               'SF']].sum(axis=1)\natbats['OPS'] = atbats['SLG'] + atbats['OBP']\natbats['K%'] = atbats['K'] \/ atbats['AB']\natbats['BB%'] = atbats['BB'] \/ atbats['AB']\natbats['K-BB%'] = atbats['K%'] - atbats['BB%']\natbats['BABIP'] = (atbats['H'] - atbats['HR'])\/(atbats['AB'] + atbats['SF'] \n                                                          - atbats['HR'] - atbats['K'])\natbats['ISO'] = atbats['SLG'] - atbats['AVG']\n\natbats['opp_AVG'] = atbats['opp_H']\/atbats['opp_AB']\natbats['opp_SLG'] = atbats['opp_TB']\/atbats['opp_AB']\natbats['opp_OBP'] = (atbats[['opp_H', 'opp_BB', 'opp_HBP']].sum(axis=1) \/ \n                             atbats[['opp_AB', 'opp_BB', \n                                     'opp_HBP', 'opp_SF']].sum(axis=1))\natbats['opp_OPS'] = atbats['opp_SLG'] + atbats['opp_OBP']\natbats['opp_K%'] = atbats['opp_K'] \/ atbats['opp_AB']\natbats['opp_BB%'] = atbats['opp_BB'] \/ atbats['opp_AB']\natbats['opp_K-BB%'] = atbats['opp_K%'] - atbats['opp_BB%']\natbats['opp_BABIP'] = ((atbats['opp_H'] - atbats['opp_HR'])\/\n                               (atbats['opp_AB'] + atbats['opp_SF'] - \n                                atbats['opp_HR'] - atbats['opp_K']))\n\natbats['FIP'] = (13*atbats['opp_HR'] + 3*(atbats['opp_BB'] + atbats['opp_HBP']) \n                 - 2.0*atbats['opp_K'])\/atbats['IP'] + 3.2\natbats['WHIP'] = atbats[['opp_H', 'opp_BB']].sum(axis=1)\/atbats['IP']\n\natbats.replace(np.inf, 0, inplace=True)\natbats.replace(-np.inf, 0, inplace=True)\n\nbatter_stats_cols = ['stand', 'AB', 'AVG', 'OBP', 'SLG', 'OPS', 'K%', 'BB%', 'BABIP']\npitcher_stats_cols = ['p_throws', 'IP', 'opp_AVG', 'opp_OBP', 'opp_SLG', 'opp_OPS', 'opp_K%',\n                      'opp_BB%', 'opp_BABIP', 'FIP', 'WHIP']\nother_cols = ['batter_id', 'g_id', 'inning', 'o', 'p_score', 'pitcher_id', 'top', 'year', 'event']\n\natbats = atbats[other_cols + batter_stats_cols + pitcher_stats_cols].fillna(0)\n\npitches.loc[pitches['pitch_type'].isna(), 'pitch_type'] = 'UN'\npitches.loc[pitches['pitch_type'].isin(['SC', 'AB', 'EP', 'FA']), 'pitch_type'] = 'UN'\npitches.loc[pitches['pitch_type'] == 'FO', 'pitch_type'] = 'PO'\n\nhome_sp = (atbats[(atbats['inning']==1) & \n                  (atbats['top'])].groupby('g_id')\n                                  .head(n=1)\n                                  .set_index('g_id')['pitcher_id']\n                                  .rename('home_sp'))\naway_sp = (atbats[(atbats['inning']==1) & \n                  ~(atbats['top'])].groupby('g_id')\n                                   .head(n=1)\n                                   .set_index('g_id')['pitcher_id']\n                                   .rename('away_sp'))\n\nstarting_pitchers = home_sp.to_frame().join(away_sp, how='left', on='g_id')\natbats = atbats.join(starting_pitchers, on='g_id', how='left')\n\n\natbats['is_starter'] = ((atbats['pitcher_id'] == atbats['home_sp']) | \n                                (atbats['pitcher_id'] == atbats['away_sp']))\natbats.drop(columns=['home_sp', 'away_sp'], inplace=True)\n\nouting_stats = pitches[['ab_id']].copy()\nouting_stats = outing_stats.join(atbats[['pitcher_id', 'batter_id', \n                                         'g_id', 'inning', 'top']], on='ab_id')\npitches['prev_pitches'] = outing_stats.groupby(['pitcher_id','g_id']).cumcount()\nprev_pitch_ab = (pitches.groupby('ab_id')[['ab_id', 'prev_pitches']]\n                        .head(n=1).set_index('ab_id'))\npitches.drop('prev_pitches', axis=1)\navg_start_len = 90\navg_relief_len = 17\n\navg_outing_len = (outing_stats.groupby(['pitcher_id', 'g_id']).size()\n                              .rolling(window=10, min_periods=1).mean()\n                              .groupby(['pitcher_id'])\n                              .transform(lambda x: x.shift(1))) \n\natbats = atbats.join(avg_outing_len.rename('avg_outing_len'), \n                     on=['pitcher_id', 'g_id'], how='left')\natbats.loc[atbats['avg_outing_len'].isna() & \n            atbats['is_starter'], 'avg_outing_len'] = avg_start_len\natbats.loc[atbats['avg_outing_len'].isna() & \n           ~atbats['is_starter'], 'avg_outing_len'] = avg_relief_len\n\np = pitches.join(atbats[['pitcher_id']], on='ab_id')[['pitcher_id', 'pitch_type', \n                                                      'spin_dir', 'ab_id', 'pitch_num']]\np['spin_dir_shifted'] = ((pitches['spin_dir'] + 1.8) % 3.6) - 1.8\np.loc[p['pitch_type'].isna(), 'pitch_type'] = 'UN'\np_grouped = p.groupby(['pitcher_id', 'pitch_type'])\nshould_shift = (p_grouped['spin_dir'].std() \n                - p_grouped['spin_dir_shifted'].std()) > 0.01 \n                # making this 0.01 instead of 0 b\/c floating point errors\n\np = p.join(should_shift.rename('should_shift'), on=['pitcher_id', 'pitch_type'], how='left')\npitches['spin_dir'] = p['spin_dir'].where(p['should_shift'], p['spin_dir_shifted'])\ndel p, should_shift, p_grouped\n\ntraj_cols = ['start_speed', 'spin_rate', 'spin_dir']\npt = pitches[['ab_id','pitch_type'] + traj_cols]\npt = pt.join(atbats[['pitcher_id', 'batter_id', 'year']], how='left', on='ab_id')\nold_cols = pt.columns\npt = pd.get_dummies(pt, columns=['pitch_type'], prefix='')\npt.drop(['_UN', '_PO'], axis=1, inplace=True)\npt_dummy_cols = pt.columns.drop(old_cols.drop('pitch_type'))\n\npitches_grouped = pt.groupby('ab_id')\npitches_during_ab = pitches_grouped[pt_dummy_cols].sum()\npitches_during_ab['tot_cat'] = pitches_during_ab.sum(axis=1)\npitches_during_ab['tot'] = pitches_grouped.size()\npitches_during_ab = pitches_during_ab.merge(atbats[['pitcher_id', 'batter_id', 'year']], \n                                            left_index=True, right_index=True)\n\npfreq_pitcher = (pitches_during_ab.groupby(['pitcher_id', 'year'])\n                                   [list(pt_dummy_cols) + ['tot', 'tot_cat']]\n                                  .rolling(window=50, min_periods=1).sum())\npfreq_pitcher = pfreq_pitcher.groupby(['pitcher_id', 'year']).shift(1)\npfreq_pitcher = (pfreq_pitcher[pt_dummy_cols].div(pfreq_pitcher['tot_cat'],axis=0)\n                                             .fillna(0).reset_index()\n                                             .set_index('ab_id'))\n\npfreq_batter = (pitches_during_ab.groupby(['batter_id', 'year'])\n                                 [list(pt_dummy_cols) + ['tot', 'tot_cat']]\n                                 .rolling(window=50, min_periods=1).sum())\npfreq_batter = pfreq_batter.groupby(['batter_id', 'year']).shift(1)\npfreq_batter = (pfreq_batter[pt_dummy_cols].div(pfreq_batter['tot_cat'],axis=0)\n                                           .fillna(0).reset_index()\n                                           .set_index('ab_id'))\n\npt = pitches[['pitch_type', 'ab_id'] + traj_cols]\npt = pt.join(atbats[['pitcher_id', 'batter_id', 'year']], on='ab_id')\npt.index.name = 'pID'\ngrouped_rolling = (pt.groupby(['pitcher_id', 'year', 'pitch_type'])[traj_cols]\n                     .rolling(window=20, min_periods=1))\n\ntraj_means = grouped_rolling.mean().fillna(0)\n\ntraj_means = traj_means.unstack(level=2)\n\ntraj_means = (traj_means.set_axis([f\"{y}_{x}_m\" for x, y in traj_means.columns], \n                                  axis=1, inplace=False)\n                        .reset_index()\n                        .set_index('pID'))\n\ntraj_stds = grouped_rolling.std().fillna(0)\ntraj_stds = traj_stds.unstack(level=2)\ntraj_stds = (traj_stds.set_axis([f\"{y}_{x}_std\" for x, y in traj_stds.columns], \n                                axis=1, inplace=False)\n                       .reset_index()\n                       .set_index('pID'))\ntraj_stats = (traj_means.merge(traj_stds, left_index=True, \n                               right_index=True, on=['pitcher_id', 'year'])\n                         .sort_values(by='pID'))\n\ntraj_stats = (traj_stats.groupby(['pitcher_id', 'year'])\n                       .fillna(method='ffill')\n                       .shift(1).fillna(0))\n\ntraj_stats = (traj_stats.merge(pt[['ab_id']], left_index=True, right_index=True)\n                        .groupby('ab_id').head(n=1).set_index('ab_id'))\n\nspindir_mean_cols = [ele for ele in atbats.columns if 'spin_dir_m' in ele]\ntraj_stats[spindir_mean_cols] = atbats[spindir_mean_cols] % 3.6\n\natbats = atbats.merge(pfreq_pitcher, left_index=True, \n                      right_index=True, on=['pitcher_id', 'year'])\natbats = atbats.merge(pfreq_batter, left_index=True, \n                      right_index=True, on=['batter_id', 'year'], suffixes=['_p', '_b'])\n\natbats = atbats.merge(traj_stats, left_index=True, right_index=True)\natbats = atbats.merge(prev_pitch_ab, left_index=True, right_index=True)\natbats.index = atbats.index.astype(int)\n\nnumpitches = pitches.groupby('ab_id').size().rename('num_pitches')\npitches = pitches.join(numpitches, on='ab_id')\natbats = atbats.join(numpitches, on='ab_id')\nprint('Loaded and processed data in {:4.2f} seconds'.format(time()-t))","db703a91":"pitches.loc[pitches['code'].isin(['X', 'D', 'E']), 'code'] = np.nan\ncode_cols = list(pitches['code'].dropna().unique())\npt_cols = list(pitches['pitch_type'].unique())\npitches = pd.get_dummies(pitches, columns=['pitch_type', 'code'], prefix_sep='', prefix='')\n\npitches.fillna({'px': 0, 'pz': 0}, inplace=True)\n\nfreq_traj_stats = (list(pfreq_pitcher.columns.drop(['pitcher_id', 'year']) + '_p') + \n                  list(pfreq_batter.columns.drop(['batter_id', 'year']) + '_b') + \n                  list(traj_stats.columns) + \n                  list(prev_pitch_ab.columns))\n\nstatic_cols = freq_traj_stats + batter_stats_cols + pitcher_stats_cols + ['inning', 'top', 'p_score']\ndynamic_cols = ['b_count','s_count','outs','on_1b','on_2b','on_3b','b_score','pitch_num']\ntarget_cols = pt_cols\noutcome_cols = code_cols + pt_cols + ['px', 'pz']\n\nscaler = MinMaxScaler()\natbats[static_cols] = scaler.fit_transform(atbats[static_cols])\npitches.loc[pitches['type_confidence'].isna(), 'type_confidence'] = 0\npitches['type_confidence'] = pitches['type_confidence']\/ pitches['type_confidence'].mean()","08793c59":"class kerasIter(Sequence):\n    def __init__(self, input_pitch_data, input_ab_data, cols_dyn, cols_static, cols_tar, cols_outcome, \n                 max_batch_size=512, recurrent_mode=True, weights_col=None):\n        #static cols should be in input_ab_data\n        #dyn, tar, outcome should all be in input_pitch_data\n        self.cols_dyn = cols_dyn\n        self.cols_static = cols_static\n        self.cols_tar = cols_tar\n        self.cols_outcome = cols_outcome\n        self.weights_col = weights_col\n        self.recurrent_mode = recurrent_mode\n        \n        all_pitch_cols = self.cols_dyn + self.cols_tar + ['ab_id', 'num_pitches']\n        if self.cols_outcome:\n            all_pitch_cols += self.cols_outcome\n        if weights_col:\n            all_pitch_cols += [weights_col]\n        \n        if (len(set(self.cols_static + self.cols_dyn + self.cols_tar)) \n                        < len(self.cols_static) + len(self.cols_dyn) + len(self.cols_tar)):\n            raise ValueError('Columns repeated in static\/dyn\/tar')\n            \n        all_pitch_cols = list(set(all_pitch_cols))\n        self.pitch_data = input_pitch_data[all_pitch_cols]\n        self.ab_data = input_ab_data[self.cols_static + ['num_pitches']]\n        \n        if (self.pitch_data.isna().sum().sum() > 0) or (self.ab_data.isna().sum().sum() > 0):\n            raise ValueError('input data contains some nan values')\n        \n        self.pitch_data = self.pitch_data.sort_values(by=['num_pitches', 'ab_id', 'pitch_num'])\n        self.ab_data = self.ab_data.sort_values(by=['num_pitches', 'ab_id'])\n        \n        if not np.all(self.pitch_data.groupby('ab_id').head(n=1)['ab_id'] == self.ab_data.index):\n            raise ValueError('pitches and atbats not sorted the same way')\n        \n        #ab_length_counts - number of at-bats of each length (in pitches)\n        #ab_lengths_sorted - index of above (number of pitches in an at-bat), sorted\n        #ab_lengths_counts_sorted - value of above (number of at-bats with that number of pitches), sorted\n        ab_length_counts = self.pitch_data['num_pitches'].loc[self.pitch_data['pitch_num']==1].value_counts()\n        ab_lengths_sorted = ab_length_counts.index[np.argsort(ab_length_counts.index.values)].values\n        ab_lengths_counts_sorted = ab_length_counts.values[np.argsort(ab_length_counts.index.values)]\n        \n        #self.batch_lengths - number of at-bats in each batch\n        #self.ab_lengths - \n        self.batch_lengths = []\n        self.ab_lengths = []\n        num_pitches_in_batch = []\n        for ab_len, num_ab_len in zip(ab_lengths_sorted, ab_lengths_counts_sorted):\n            batch_lens = [max_batch_size for jj in range(num_ab_len\/\/max_batch_size)]\n            leftover = num_ab_len%max_batch_size\n            if leftover > 0:\n                batch_lens.append(leftover)\n            self.batch_lengths += batch_lens\n            self.ab_lengths += [ab_len for jj in batch_lens]\n            num_pitches_in_batch += [ab_len * ele for ele in batch_lens]\n        \n        self.start_indices_ab = np.cumsum([0] + self.batch_lengths[:-1])\n        self.end_indices_ab = [s + n for s, n in zip(self.start_indices_ab, self.batch_lengths)]\n        self.start_indices_p = np.cumsum([0] + num_pitches_in_batch[:-1])\n        self.end_indices_p = [s + n for s, n in zip(self.start_indices_p, num_pitches_in_batch)]\n        \n    def __len__(self):\n        return len(self.start_indices_p)\n    def __getitem__(self, idx):\n        this_group_p = self.pitch_data.iloc[self.start_indices_p[idx]:self.end_indices_p[idx]]\n        this_group_ab = self.ab_data[self.start_indices_ab[idx]:self.end_indices_ab[idx]]\n        \n        dyn_data = this_group_p[self.cols_dyn].values\n        target_data = this_group_p[self.cols_tar].values\n        static_data = this_group_ab[self.cols_static].values\n        if self.cols_outcome:\n            outcome_data = this_group_p[self.cols_outcome].values\n\n        \n        if self.weights_col:\n            weights_data = this_group_p[self.weights_col].values\n                \n        d0 = self.batch_lengths[idx] #number of at-bats\n        d1 = self.ab_lengths[idx] # number of pitches\n        \n        # here we take the outcome data and push it back one timestep, filling in 0 for all columns for first pitch\n        if self.cols_outcome:\n            outcome_data = np.reshape(outcome_data, (d0, d1, len(self.cols_outcome)))\n            outcome_data = np.pad(outcome_data[:, :-1, :], ((0, 0), (1, 0), (0, 0)), 'constant', constant_values=0)\n        \n        if self.recurrent_mode:\n            dyn_data = np.reshape(dyn_data, (d0, d1, len(self.cols_dyn)))\n            if self.cols_outcome:\n                dyn_data = np.concatenate((dyn_data, outcome_data), axis=2)\n            target_data = np.reshape(target_data, (d0, d1, len(self.cols_tar)))\n            if self.weights_col:\n                weights_data = np.reshape(weights_data, (d0, d1))\n                return [dyn_data, static_data], target_data, weights_data\n            else:\n                return [dyn_data, static_data], target_data\n        \n        else:\n            static_data = np.repeat(static_data, d1, axis=0)\n            if self.cols_outcome:\n                outcome_data = np.reshape(outcome_data, (d0*d1, len(self.cols_outcome)))\n                all_cols_data = np.concatenate((static_data, dyn_data, outcome_data), axis=1)\n            else:\n                all_cols_data = np.concatenate((static_data, dyn_data), axis=1)\n            if self.weights_col:\n                return all_cols_data, target_data, weights_data\n            else:\n                return all_cols_data, target_data","8e72cf72":"all_gids = atbats['g_id'].unique()\ntest_gids = np.random.choice(all_gids, 400, replace=False)\ntest_abs = atbats[atbats['g_id'].isin(test_gids)]\ntest_pitches = pitches[pitches['ab_id'].isin(test_abs.index)]\n\ntrain_abs = atbats[~(atbats['g_id'].isin(test_gids))]\ntrain_pitches = pitches[~(pitches['ab_id'].isin(test_abs.index))]","c5a5a6c1":"pt_pred_cols = pd.Index(pt_cols).drop(['UN', 'PO'])\npt_pred_cols_measuredfreqs = '_' + pt_pred_cols + \"_p\"\npt_with_freqs = pitches[list(pt_pred_cols) + ['ab_id', 'type_confidence']].join(atbats[pt_pred_cols_measuredfreqs], on='ab_id')\npt_with_freqs.loc[pt_with_freqs[pt_pred_cols_measuredfreqs].sum(axis=1)<1, '_FF_p'] = 1\n\ncorrect_labels = np.argmax(pt_with_freqs[pt_pred_cols].values, axis=1)\npreds = pt_with_freqs[pt_pred_cols_measuredfreqs]\nweights = pt_with_freqs['type_confidence']\nsimple_loss = log_loss(correct_labels, preds, sample_weight=weights)\n\nprint('Simple model loss: {}'.format(simple_loss))","56203f5e":"def plot_model_hist(mdl_hist, name):\n    plt.figure(figsize=(20,8))\n    \n    plt.subplot(121)\n    plt.plot(mdl_hist.history['acc'])\n    plt.plot(mdl_hist.history['val_acc'])\n    plt.title('Model accuracy ' + name)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.grid()\n\n    plt.subplot(122)\n    plt.plot(mdl_hist.history['loss'])\n    plt.plot(mdl_hist.history['val_loss'])\n    plt.title('Model loss ' + name)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.grid()\n    plt.savefig(name + '_hist.png')\n    plt.show()\n    \ndef train_plot_model(mdl, name, trainGen, testGen):\n    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n    hist = mdl.fit_generator(trainGen, epochs=60, validation_data=testGen, verbose=0, callbacks=callbacks)\n    model.save(name + '.h5')\n    plot_model(mdl, to_file=name + '.png')\n    plot_model_hist(hist, name)\n    \ndef repeat_vector(args):\n    layer_to_repeat = args[0]\n    sequence_layer = args[1]\n    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)","768234aa":"train_iter = kerasIter(train_pitches, train_abs, dynamic_cols, static_cols, target_cols, cols_outcome=None, \n                       max_batch_size=1024, weights_col='type_confidence', recurrent_mode=False)\ntest_iter = kerasIter(test_pitches, test_abs, dynamic_cols, static_cols, target_cols, cols_outcome=None, \n                      max_batch_size=1024, weights_col='type_confidence', recurrent_mode=False)\n\ninputs = Input(shape=(len(static_cols + dynamic_cols),))\nl1 = BatchNormalization() (inputs)\nl1 = Dense(1024, activation='relu') (l1)\n\nl2 = BatchNormalization() (l1)\nl2 = Dense(1024, activation='relu') (l2)\nl2 = Dropout(0.25) (l2)\n\nl3 = BatchNormalization() (l2)\nl3 = Dense(1024, activation='relu') (l3)\nl3 = Dropout(0.25) (l3)\n\nout = Dense(len(target_cols), activation='softmax') (l3)\n\nmodel = Model(inputs=inputs, outputs=out)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\ntrain_plot_model(model, 'dense_no_outcome', train_iter, test_iter)\n","fee53be3":"train_iter = kerasIter(train_pitches, train_abs, dynamic_cols, static_cols, target_cols, outcome_cols, \n                       max_batch_size=1024, weights_col='type_confidence', recurrent_mode=False)\ntest_iter = kerasIter(test_pitches, test_abs, dynamic_cols, static_cols, target_cols, outcome_cols, \n                      max_batch_size=1024, weights_col='type_confidence', recurrent_mode=False)\n\ninputs = Input(shape=(len(static_cols + dynamic_cols + outcome_cols),))\nl1 = BatchNormalization() (inputs)\nl1 = Dense(1024, activation='relu') (l1)\n\nl2 = BatchNormalization() (l1)\nl2 = Dense(1024, activation='relu') (l2)\nl2 = Dropout(0.25) (l2)\n\nl3 = BatchNormalization() (l2)\nl3 = Dense(1024, activation='relu') (l3)\nl3 = Dropout(0.25) (l3)\n\nout = Dense(len(target_cols), activation='softmax') (l3)\n\nmodel = Model(inputs=inputs, outputs=out)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\ntrain_plot_model(model, 'dense_with_outcome', train_iter, test_iter)\n","000c07b2":"train_iter = kerasIter(train_pitches, train_abs, dynamic_cols, static_cols, target_cols, outcome_cols, \n                       max_batch_size=1024, weights_col='type_confidence', recurrent_mode=True)\ntest_iter = kerasIter(test_pitches, test_abs, dynamic_cols, static_cols, target_cols, outcome_cols, \n                      max_batch_size=1024, weights_col='type_confidence', recurrent_mode=True)\n\ndynamic_ins = Input(shape=(None,len(dynamic_cols+outcome_cols)))\nstatic_ins = Input(shape=(len(static_cols), ))\n\nstatic_ins_repeated = Lambda(repeat_vector, output_shape=(None, len(static_cols))) ([static_ins, dynamic_ins])\n\nRL1 = LSTM(100, return_sequences=True, dropout=0.2) (dynamic_ins)\nRL1 = LSTM(100, return_sequences=True, dropout=0.2) (RL1)\nRL1 = LSTM(100, return_sequences=True, dropout=0.2) (RL1)\ndense1_ins = concatenate([RL1, static_ins_repeated])\n#dense1_ins = RL1\ndense1 = TimeDistributed(Dense(1024, activation='relu')) (dense1_ins)\ndense1 = Dropout(0.25) (dense1)\ndense1 = TimeDistributed(Dense(1024, activation='relu')) (dense1)\ndense1 = Dropout(0.25) (dense1)\ndense1 = TimeDistributed(Dense(1024, activation='relu')) (dense1)\ndense1 = Dropout(0.25) (dense1)\n\nout = TimeDistributed(Dense(len(target_cols), activation='softmax')) (dense1)\nmodel = Model(inputs=[dynamic_ins, static_ins], outputs=out)\nadamopt = Adam(lr=2e-3)\nmodel.compile(optimizer=adamopt,\n              loss='categorical_crossentropy',\n              sample_weight_mode='temporal',\n              metrics=['accuracy'])\n# weights_col='type_confidence'\nmodel.summary()\n\ntrain_plot_model(model, 'rnn_pitchtype', train_iter, test_iter)","547f4f01":"# Pitch-Type Prediction, and what we can learn from the results\n\nIn this notebook, I show how we can use pitch-level data to train neural networks to predict which pitch is coming next. This is of course, a difficult task, since pitchers put a lot of effort into making it difficult. Here, I take four approaches to pitch-type prediction, adding more information each time, to see what can help make predictions.\n\nThe first model is simple: model this as truly random, and so just use past pitch frequency as the future pitch probability.\n\nNext, we train a neural network which only knows about the current situation: balls, strikes, who's on first, what this pitcher's stuff is like, and pitch frequencies as before.\n\nThird, we train a similar network, but give it information about the previous pitch. So if pitchers tend to set up a fastball with a slider, this network will pick up on that.\n\nFinally, we train a fully recurrent neural network. This will know everything about the at-bat. If pitchers tend to only throw a curveball once in an at-bat, this network will know.\n\nOne big caveat here: none of these networks have a way to learn about specific pitchers. That's an interesting problem that may be the subject of a future notebook.\n\n## Step 1: Aggregating Data, Feature Engineering, etc.\n\nThis is mostly identical to prior notebooks: \n\n- https:\/\/www.kaggle.com\/pschale\/compiling-basic-batter-pitcher-statistics\n\n- https:\/\/www.kaggle.com\/pschale\/aggregating-pitch-level-trajectory-statistics","bd083839":"## Model 2: Dense Neural Network, no past information","ad797ebe":"## Model 3: Dense Neural Network, with prior pitch info","a94e8ee1":"## Model 1: Just use past pitch frequencies\n\nThis model would be optimal if pitchers perfectly mixed their pitches. Note that we're removing uncategorized pitches and pitchouts, and using the weights from the type_confidence variable. For the first at-bat for each pitcher, we just assume it will be fastballs, since that's the most common pitch.","26e1796b":"## Model 4: Recurrent Neural Network","7ee09b4d":"So there's our first benchmark. Now we can see how the other models perform to deduce how pitchers decide what pitch type to throw. Next we write some functions to train and plot the NN models. It's not very exciting, so I've hidden it by default.","25e204e3":"## Preparing Data for Neural Network\n\nThere are three complications that thwart the usual method of feeding the data into the network:\n\n- Some data goes with an entire at-bat and has not been duplicated for each pitch\n\n- The recurrent network requires entire at-bats at once, which will involve multiple columns from the pitches dataframe\n\n- Sometimes we want to know data from a previous pitch (like what pitch type it was)\n\nTo solve this, we create a class that will be used to pass the data to the fit_generator method of the network as constructed in keras. Note that this is *not* a generator; for proper parallel processing, this object knows where *all* of the chunks are, and can make any of them as requested. A generator only knows how to get the next chunk.","6868eb22":"# Conclusions\n\nModel 1: ~1.4\n\nModel 2: ~1.16\n\nModel 3: ~1.14\n\nModel 4: ~1.11\n\n(these numbers are from a run done in edit mode and may be slightly different than shown above)\n\nLet's start by breaking down what each model believes about how pitches are selected. \n\nModel 1: Pitchers calculate how often each pitch should be thrown. How this is done and what they're maximizing is unknown, but once they have their frequencies they simply draw from that distribution. The distribution can change over time, on a timescale of at least 50 at-bats.\n\nModel 2: Pitchers look around at what's going on before deciding which pitch to throw. They still pick semi-randomly at some level, but it's informed by the game situation. But they don't have memory and they don't plan ahead. Just think about this pitch, then think about next pitch.\n\nModel 3: Same as model 2, plus now we get some planning going, but only one pitch worth. Maybe fastballs set up sliders. Maybe a swinging strike makes a pitcher want to throw the same thing again.\n\nModel 4: Now we think pitchers think about everything that's happened in the at-bat to make a decision. Maybe four pitches ago he swung through a low curveball, so now the pitcher will go to it again.\n\nBy far the largest improvement is at the first stage. This tells us that pitchers definitely tailor pitch selection to in-game situation. It's common baseball knowledge that when the pitcher is behind in the count, they tend to throw fastballs (since that's the easist pitch to throw accurately), and this points to things like that being true. On the other hand, there are clear gains in prediction strength by looking at more information. Looking into which pieces of information are most important may be the subject of a future notebook.\n\nI'd also note that their failure to be perfect RNGs doesn't necessarily mean that pitchers could improve by being more random. For example, in a 3-1 count, narrowing the pitch selection to favor more fastballs should improve accuracy, leading to fewer walks (which presumably more than makes up for the extra hits given up)."}}