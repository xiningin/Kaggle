{"cell_type":{"73f96ab5":"code","5cac2758":"code","961672cb":"code","2640458f":"code","e8399df3":"code","99698335":"code","1cba3b59":"code","cd70e06d":"code","eecc8c77":"code","6167570d":"code","c2f565d3":"code","00828c70":"code","7a84ed29":"code","4f0cf421":"code","3e268667":"code","c30bedc7":"code","b2b7facc":"code","e1d57684":"code","b2d0c2f5":"code","f003f3b8":"code","ba2c270e":"code","cb9cde5a":"code","a1669957":"code","30de5298":"code","dfae8a26":"code","4693efb4":"code","9c06282e":"code","67fe60c9":"code","34f49173":"code","04b92a5f":"code","775e5310":"code","c614d3d4":"code","104c8820":"code","c11dae46":"code","0f7a7284":"code","a728c951":"code","5ee74cfb":"code","893f84d5":"markdown","9797de73":"markdown","e6a8ba2e":"markdown","71f3d092":"markdown","8eb07fa9":"markdown","6ea472f8":"markdown","fdb91781":"markdown","4f85c96e":"markdown","b59134d3":"markdown","ce7dcab6":"markdown","73123253":"markdown","95004a42":"markdown","fe95f989":"markdown","874d04e4":"markdown"},"source":{"73f96ab5":"# installing verta - restart your notebook if prompted (remember by doing this all your local variables will be lost!)\ntry:\n  import verta\nexcept ModuleNotFoundError:\n  !pip install verta==0.12.0a1","5cac2758":"# general imports\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# models\nfrom sklearn import ensemble\nfrom sklearn.linear_model import Ridge, Lasso, LassoLars, LinearRegression, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# metrics\nfrom sklearn import metrics\n\n# misc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold, RFE\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.utils import shuffle\nfrom multiprocessing import Pool\nfrom sklearn import model_selection","961672cb":"# reading the dataset\nloan_df = pd.read_csv(\"..\/input\/loan.csv\")","2640458f":"# looking at missing values\n# percentage of null values in each of the columns \nmissing_val = loan_df.isna().sum()\/len(loan_df)*100\nprint(\"Columns with more than 60% missing values - \")\nprint(missing_val[missing_val > 60].sort_values(ascending=False))\n# since we cannot fill in values for these columns we're going to drop these\nloan_df = loan_df.loc[:, loan_df.isnull().mean() < .35]\nprint(loan_df.shape)\n# got rid of ~50 cols with that threshold","e8399df3":"# dropping arbitrary applicant input\nloan_df.drop(columns = ['emp_title', 'title', 'initial_list_status'], axis = 1, inplace = True)","99698335":"# looking at cols of object type - convert dates to duration, emp_length to int, encode grade and subgrade, \nloan_df.select_dtypes(include='object').columns","1cba3b59":"# all dates like issue_d, earliest_cr_line, last_credit_pull_d will be converted to duration\ndttoday = datetime.now().strftime('%Y-%m-%d')\nloan_df.fillna({'earliest_cr_line':dttoday, 'issue_d':dttoday, 'last_credit_pull_d':dttoday}, inplace=True)\nloan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']] = loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].apply(pd.to_datetime)\n\nloan_df['earliest_cr_line'] = loan_df['earliest_cr_line'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))\/-365)\nloan_df['last_credit_pull_d'] = loan_df['last_credit_pull_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))\/-365)\nloan_df['issue_d'] = loan_df['issue_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))\/-365)\n\nloan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].head(5)","cd70e06d":"# convert zip from string to int\nloan_df['zip_code'] = loan_df['zip_code'].apply(lambda x: str(x)[:3])\nloan_df['zip_code'] = pd.to_numeric(loan_df['zip_code'], errors='coerce')\nloan_df['zip_code'].fillna(0, inplace=True)","eecc8c77":"# converting emp_years to int\nloan_df.fillna({'emp_length':'0 years', 'term':'Unknown', 'home_ownership':'Unknown',\n                'verification_status':'Unknown', 'loan_status':'Unknown', 'purpose':'Unknown',\n                'addr_state':'Unknown', 'application_type':'Unknown', 'disbursement_method':'Unknown'}, inplace=True)\nloan_df['emp_length'].value_counts()","6167570d":"# converting emp_length to float\ndict_emp_length = {'10+ years':10, '6 years':6, '4 years':4, '< 1 year':0.5, '2 years':2,\n       '9 years':9, '0 years':0, '5 years':5, '3 years':3, '7 years':7, '1 year':1,\n       '8 years':8}\nloan_df['emp_length'].replace(dict_emp_length, inplace=True)","c2f565d3":"# dropping arbitrary applicant input, post loan attributes\nto_drop = ['pymnt_plan', 'total_pymnt',\n           'total_pymnt_inv','last_pymnt_d', 'last_pymnt_amnt', 'out_prncp',\n           'out_prncp_inv', 'total_rec_prncp', 'hardship_flag', 'collection_recovery_fee',\n           'collections_12_mths_ex_med', 'debt_settlement_flag','policy_code', 'grade', 'sub_grade']\nloan_df.fillna(0, inplace=True)","00828c70":"loan_dummy_df = pd.get_dummies(loan_df.drop(columns=to_drop, axis=1))\nloan_dummy_df.info()","7a84ed29":"# save new dataset for checkpointing\n# loan_dummy_df.to_csv(\"loan_dummy.csv\", index=False)\nprint(loan_dummy_df.shape)\n\n# splitting into test and train after treating variables\nX_train, X_test, y_train, y_test = train_test_split(loan_dummy_df, loan_df['int_rate'], \n                                                    test_size=0.3, random_state=42)","4f0cf421":"X_train.drop(columns=['int_rate'], axis=1, inplace=True)\nX_test.drop(columns=['int_rate'], axis=1, inplace=True)","3e268667":"# number of columns in the cleaned dataset\nprint(X_train.shape[1])","c30bedc7":"# app settings\nfrom verta import Client\n\n# setting up\nHOST = \"app.verta.ai\"\nEMAIL = \"your-email@gmail.com\"\nDEV_KEY= \"your-dev-key\"\nPROJECT_NAME = \"Feature-Selection\"\n\nclient = Client(host=HOST,\n                email=EMAIL, \n                dev_key=DEV_KEY,\n                ignore_conn_err=True)\nproj = client.set_project(PROJECT_NAME)","b2b7facc":"# Calculating and logging mean absolute error and root mean squared error for each model\ndef get_metrics(X_train, y_train, X_test, y_test, model, run=None):\n    \n    print(\"Getting metrics for model\")\n    # train prediction\n    train_preds = model.predict(X_train)\n    # test prediction\n    test_preds = model.predict(X_test)\n\n    # Calculate the absolute errors\n    train_errors = abs(train_preds - y_train)\n    test_errors = abs(test_preds - y_test)\n    \n    # mean absolute error (mae)\n    train_mae = round(np.mean(train_errors), 4)\n    test_mae = round(np.mean(test_errors), 4)\n    print(\"train mae: \", train_mae, \" test mae: \", test_mae)\n\n    # root mean sq error\n    train_rmse = np.sqrt(metrics.mean_squared_error(y_train, train_preds))\n    test_rmse = np.sqrt(metrics.mean_squared_error(y_test, test_preds))\n    print(\"train rmse: \", train_mae, \" test rmse: \", test_mae)\n\n    \n    # log values only if run is provided    \n    if run:\n        run.log_metric(\"train_mae\", train_mae)\n        run.log_metric(\"train_rmse\", train_rmse)\n        run.log_metric(\"test_mae\", test_mae)\n        run.log_metric(\"test_rmse\", test_rmse)\n    \n    return train_mae, test_mae, train_rmse, test_rmse","e1d57684":"# Elastic net model - this logs all hyperparameters, features and metrics\ndef elastic_net_model(tags, feature_set, drop = True,  **params):\n    # elastic net regression\n    \n    if drop:\n        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n    else:\n        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n    \n    # grid search\n    param_grid = {'l1_ratio': [.1, .5, .7, .85, .98, 1]} \n    en_regr = GridSearchCV(ElasticNet(), param_grid, \n                           scoring='neg_mean_absolute_error',\n                           n_jobs=-1, refit=True, cv=3, verbose=20, return_train_score=True)\n    \n    en_regr.fit(X_train_new, y_train)\n    results = pd.DataFrame(en_regr.cv_results_)\n    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n\n    for _, run_result in results.iterrows():\n        run = client.set_experiment_run()\n        run.log_tags(tags)\n        \n        if drop:\n            run.log_attribute(\"removed_features\", removed_features)\n        else:\n            run.log_attribute(\"feature_set\", feature_set)\n\n        # log hyperparameters\n        run.log_hyperparameters(run_result['params'])\n        \n        # run statistics    \n        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n\n        # log summary stats of validation\n        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n    \n        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n  \n\n    # log best model\n    run_baseline_en = client.set_experiment_run()\n    run_baseline_en.log_hyperparameters(en_regr.best_params_)\n    run_baseline_en.log_model(key=\"elastic_net_model\", model=en_regr)\n    run_baseline_en.log_tags(tags+['best model'])\n    \n    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n                                                             y_train = y_train,\n                                                             X_test = X_test_new,\n                                                             y_test = y_test,\n                                                             model = en_regr,\n                                                             run = run_baseline_en)\n    return en_regr","b2d0c2f5":"# Random forest model - this logs all hyperparameters, features and metrics\ndef random_forest_model(tags, feature_set, drop = True, **params):\n    # Random Forest\n    # creating new train and test sets\n    # logging features     \n    if drop:\n        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n    else:\n        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n    \n    param_grid = {\n    'n_estimators':[50],\n    'max_depth': [4, 9],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    }\n    \n    rf = GridSearchCV(RandomForestRegressor(random_state=42),\n                      param_grid, n_jobs=4, refit=True, verbose=20, cv=3, \n                      scoring='neg_mean_absolute_error',\n                      return_train_score=True)\n    \n    rf.fit(X_train_new, y_train)\n    \n    results = pd.DataFrame(rf.cv_results_)\n    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n\n    for _, run_result in results.iterrows():\n        run = client.set_experiment_run()\n        run.log_tags(tags)\n        \n        if drop:\n            run.log_attribute(\"removed_features\", feature_set)\n        else:\n            run.log_attribute(\"feature_set\", feature_set)\n\n        # log hyperparameters\n        run.log_hyperparameters(run_result['params'])\n        \n        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n\n        # log summary stats of validation\n        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n    \n        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n\n    # best model\n    run_rf = client.set_experiment_run()\n    run_rf.log_hyperparameters(rf.best_params_)\n    run_rf.log_model(key=\"model\", model=rf)\n    run_rf.log_tags(tags+['best model'])\n\n    # metrics for models\n    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n                                                             y_train = y_train,\n                                                             X_test = X_test_new,\n                                                             y_test = y_test,\n                                                             model = rf,\n                                                             run = run_rf)\n    return rf","f003f3b8":"# GBR model - this logs all hyperparameters, features and metrics\ndef gbr_model(tags, feature_set, drop = True, **params):\n    # Gradient Boosting Regressor\n    # creating new train and test sets\n    if drop:\n        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n    else:\n        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n\n    param_grid = {\n        'n_estimators':[250],\n        'learning_rate': [0.1, 0.01,0.05, 0.02],\n        'max_depth': [4, 6, 9],\n        'min_samples_leaf': [3, 7, 15]\n             }\n\n    gbr = GridSearchCV(GradientBoostingRegressor(), param_grid, n_jobs=-1, verbose=20, refit=True,\n                       cv=3, scoring='neg_mean_absolute_error',\n                       return_train_score=True)\n    \n    gbr.fit(X_train_new, y_train)\n    \n    results = pd.DataFrame(gbr.cv_results_)\n    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n\n    for _, run_result in results.iterrows():\n        run = client.set_experiment_run()\n        run.log_tags(tags)\n        \n        if drop:\n            run.log_attribute(\"removed_features\", feature_set)\n        else:\n            run.log_attribute(\"feature_set\", feature_set)\n\n        # log hyperparameters\n        run.log_hyperparameters(run_result['params'])\n        \n        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n\n        # log summary stats of validation\n        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n    \n        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n\n\n    # log best model\n    run_gbr = client.set_experiment_run()\n    run_gbr.log_hyperparameters(gbr.best_params_)\n    run_gbr.log_model(key=\"model\", model=gbr)\n    run_gbr.log_tags(tags+['best model'])\n\n    # metrics for models\n    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n                                                             y_train = y_train,\n                                                             X_test = X_test_new,\n                                                             y_test = y_test,\n                                                             model = gbr,\n                                                             run = run_gbr)\n    return gbr","ba2c270e":"# run_dk = client.set_experiment_run()\n# loan_dummy_dk = pd.get_dummies(loan_df.drop(columns=['grade', 'sub_grade'], axis=1))\n\n\n# # splitting into test and train after treating variables\n# X_train_dk, X_test_dk, y_train_dk, y_test_dk = train_test_split(loan_dummy_dk, loan_dummy_dk['int_rate'], \n#                                                     test_size=0.3, random_state=42)\n\n# # creating new train and test sets\n# X_train_dk.drop(columns=['int_rate'], axis=1, inplace=True)\n# X_test_dk.drop(columns=['int_rate'], axis=1, inplace=True)\n# # MODEL:\n# expt = client.set_experiment(\"domain_knowledge\")\n\n# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n# rf.fit(X_train_dk, y_train)\n# # best model\n# run_rf = client.set_experiment_run()\n# run_rf.log_model(key=\"model\", model=rf)\n# run_rf.log_tags(['domain_knowledge', 'random_forest', 'no_HPP'])\n\n# # metrics for models\n# train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n#                                                          y_train = y_train,\n#                                                          X_test = X_test_new,\n#                                                          y_test = y_test,\n#                                                          model = rf,\n#                                                          run = run_rf)","cb9cde5a":"expt_lv = client.set_experiment(\"low_variance\")\n\n# get list of all the original df columns\nall_columns = X_train.columns\n\n# instantiate VarianceThreshold object\nvt = VarianceThreshold(threshold=0.2)\n\n# fit vt to data\nvt.fit(X_train)\n\n# get the indices of the features that are being kept\nfeature_indices = vt.get_support(indices=True)\n\n# remove low-variance columns from index\nfeature_names = [all_columns[idx]\n                 for idx, _\n                 in enumerate(all_columns)\n                 if idx\n                 in feature_indices]\n\n# get the columns to be removed\nremoved_features = list(np.setdiff1d(all_columns,\n                                     feature_names))\nprint(\"Found {0} low-variance columns.\"\n      .format(len(removed_features)))\nprint(\"Low-variance columns\", removed_features)","a1669957":"# # MODEL:\n# # elatic net regr\n# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=removed_features, drop=True)","30de5298":"# # MODEL:\n# # random forest\n# rf_lv = random_forest_model(tags=['random forest'], feature_set=removed_features, drop=True)","dfae8a26":"# # MODEL:\n# # GBR\n# gbr_lv = gbr_model(tags=['gradient boosting regressor'], feature_set=removed_features, drop=True)","4693efb4":"expt = client.set_experiment(\"Correlated features\")\n\n# corr matrix\ncor = X_train.corr()\ncor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array\ncor_stack = cor.stack()\nprint(\"Columns with corr. greater than 0.7 - \")\nprint(cor_stack[(cor_stack > 0.70) | (cor_stack < -0.70)])","9c06282e":"# all cols whose corr is greater then 0.7; generated dummy columns excluded\ncols_to_drop = ['funded_amnt', 'funded_amnt_inv', 'installment', 'total_acc', 'total_rev_hi_lim','avg_cur_bal',\n                'bc_util', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_tl', 'num_actv_rev_tl', 'num_bc_sats', \n                'num_bc_tl', 'num_il_tl', 'num_op_rev_tl','num_rev_accts','num_rev_tl_bal_gt_0',\n                'num_sats','num_tl_30dpd', 'num_tl_op_past_12m', 'percent_bc_gt_75',\n               'tax_liens', 'tot_hi_cred_lim', 'total_bc_limit', 'total_il_high_credit_limit']","67fe60c9":"# looking at columns highly correlated with the target column\nint_rate_cor = X_train.apply(lambda x: x.corr(y_train))\nprint(\"Columns corr. to target variable -\")\nint_rate_cor[(int_rate_cor > 0.5) | (int_rate_cor < -0.5)]","34f49173":"# # MODEL:\n# # elatic net regr\n# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=cols_to_drop, drop=True)","04b92a5f":"# # MODEL:\n# # random forest\n# rf_corr = random_forest_model(tags=['random forest'], feature_set=cols_to_drop, drop=True)","775e5310":"# # MODEL:\n# # GBR\n# gbr_model(tags=['gradient boosting regressor'], feature_set=cols_to_drop, drop=True)","c614d3d4":"expt = client.set_experiment(\"f_regr\")\n\n# for regression use f_regression\n# k is the number of features selected, here it is set to 20 but can be varied\nselector_f_reg = SelectKBest(f_regression, k=20).fit(X_train, y_train)\n\nselected_cols = [d for d, s in zip(list(X_train.columns), selector_f_reg.get_support()) if s]\nprint(\"K best columns: \", selected_cols)","104c8820":"# # MODEL:\n# # random forest\n# random_forest_model(tags=['random forest', 'k=20'], feature_set=selected_cols, drop=False)\n\n# # elatic net regr\n# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=selected_cols, drop=False)\n\n# # GBR\n# gbr_model(tags=['gradient boosting regressor'], feature_set=selected_cols, drop=False)","c11dae46":"# RFE - Feature ranking with recursive feature elimination.\nexpt = client.set_experiment(\"RFE\")\n\n# run\nrun_rfe = client.set_experiment_run()\n\n# n_features_to_select can be increased or decreased, 80 here is just an example\nhyperparam_rfe = {\"step\":10, \"n_features_to_select\":80}\nhyperparam_rfr = {\"n_estimators\":20, \"max_depth\":4}\n\nestimator = RandomForestRegressor(random_state = 42, n_jobs=-1, **hyperparam_rfr)\nselector = RFE(estimator, **hyperparam_rfe)\n\nselector.fit(X_train, y_train)\n\nprint(\"Support: \", selector.support_)\nrun_rfe.log_artifact(\"support\", selector.support_)\nprint(\"Ranking: \", selector.ranking_)\nrun_rfe.log_artifact(\"ranking\", selector.ranking_)\nselected_cols = [d for d, s in zip(list(X_train.columns), selector.support_) if s]\nprint(\"Selected Features: \", selected_cols)\nrun_rfe.log_artifact(\"feature_set\", selected_cols)\n\nrun_rfe.log_hyperparameters(hyperparam_rfe)\nrun_rfe.log_hyperparameters(hyperparam_rfr)\n\nrun_rfe.log_tags(['random forest'])\n\ntrain_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train,\n                                                         y_train = y_train,\n                                                         X_test = X_test,\n                                                         y_test = y_test,\n                                                         model = selector,\n                                                         run = run_rfe)","0f7a7284":"expt = client.set_experiment(\"ridge\")\n# Ridge\n# increase the num in np.logspace() to search over a larger set of alpha values\nhyperparam_candidates = {\n    'alpha' : np.logspace(-4, -1, 1)\n}\nhyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n                   for values\n                   in itertools.product(*hyperparam_candidates.values())]\n\ndef run_experiment(hyperparams, **params):\n    \n    # create object to track experiment run\n    run = client.set_experiment_run()\n    run.log_tags(['ridge', 'regularization'])\n    \n    # log hyperparameters\n    run.log_hyperparameters(hyperparams)\n    print(hyperparams)\n    \n    # create and train model\n    ridge_model = Ridge(**hyperparams)\n    ridge_model.fit(X_train, y_train)\n    \n    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train, y_train, X_test, y_test, ridge_model, run)\n    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n    \nwith Pool() as pool:\n    pool.map(run_experiment, hyperparam_sets)","a728c951":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_x_train = pd.DataFrame(ss.fit_transform(X_train),columns = X_train.columns)\nprint(scaled_x_train.shape == X_train.shape)\nscaled_x_test = pd.DataFrame(ss.fit_transform(X_test),columns = X_test.columns)\nprint(scaled_x_test.shape == X_test.shape)","5ee74cfb":"expt = client.set_experiment(\"Lasso\")\n# Lasso\n# increase the num in np.logspace() to search over a larger set of alpha values\nhyperparam_candidates = {\n    'alpha' : np.logspace(-4, -1, 1)\n}\n\nhyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n                   for values\n                   in itertools.product(*hyperparam_candidates.values())]\n\ndef run_experiment(hyperparams):\n    \n    # create object to track experiment run\n    run = client.set_experiment_run()\n    run.log_tags(['lasso', 'regularization', 'scaled'])\n    \n    # log hyperparameters\n    run.log_hyperparameters(hyperparams)\n    print(hyperparams)\n    # create and train model\n    model = Lasso(alpha=hyperparams['alpha'], max_iter = 1e5, tol=0.001)\n    model.fit(scaled_x_train, y_train)\n    \n    train_mae, test_mae, train_rmse, test_rmse = get_metrics(scaled_x_train, y_train, scaled_x_test, y_test, model, run)\n    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n    \nwith Pool() as pool:\n    pool.map(run_experiment, hyperparam_sets)","893f84d5":"Feature selection goes hand in hand with the initial steps of cleaning and preparing your data before the training process.\n\n<img src=\"https:\/\/github.com\/aish-anand\/DataScienceExamples\/blob\/master\/featureSelection\/images\/FS_in_ML.png?raw=true\" width=\"500px\"\/>\n\nChecking for missing values is a good first step in preparing the data. We have chosen to remove columns where more than 65% of the values are empty. By working on the dataset slice-by-slice, we have converted the categorical features to one-hot encoding. Next, we will jump into various feature selection methods.","9797de73":"#### Lasso\nLasso Regression can be used to penalize the beta coefficients in a model, and it is very similar to ridge regression. It also adds a penalty term to the cost function of a model, with a *lambda* value that must be tuned. The most important distinction from ridge regression is that Lasso regression can force the Beta coefficient to zero, which will remove that feature from the model. This is why Lasso is preferred at times, especially when you are looking to reduce model complexity. The smaller number of features a model has, the lower the complexity.","e6a8ba2e":"### Keeping track of models\n\nWith a large number of variables, hyperparameters and different types of models, it is easy to lose track of the progress we have made. To help with this problem we are going to be using the [Verta.AI](https:\/\/www.verta.ai\/)'s model management platform to keep track of models and graphs that we build thoughout the process. Setting up Verta is simple and straightforward and more information can be found [here](https:\/\/verta.readme.io\/docs\/getting-started).","71f3d092":"## Cleaning and preparing dataset","8eb07fa9":"### Univariate feature selection\n\nUnivariate feature selection works by selecting the best features based on univariate statistical tests. We can use sklearn\u2019s `SelectKBest` to select a number of features to keep. This method uses statistical tests to select features having the highest correlation to the target.","6ea472f8":"### Models + Logging","fdb91781":"### Removing low variance features\n\nIn sklearn\u2019s `feature_selection` module we find `VarianceThreshold`. It removes all features whose variance doesn\u2019t meet some threshold. By default it removes features with zero variance or features that have the same value for all samples.","4f85c96e":"## Getting out hands dirty\n\nNow that our cleaned datset is in place and we have our model logging set up, we will look at 5 popular methods across these categories in detail:\n\n1. Using domain knowledge\n2. Removing highly correlated features\n3. Univariate feature selection\n4. Recursive feature elimination\n5. Embedded methods\n\nFor each method, we are going to use our smaller feature set which is created to train 3 different types of models -\n1. elastic net model (baseline)\n2. random forests\n3. gradient boosting regressor\n\nWe have chosen these models as they generally work for large datasets, prevent overfitting and are straightforward to understand. However of the three models, the gradient boosting regressor takes the longest to train and requires hyperparameter tuning to be done for each feature set. If compute power is a constraint, skipping this model might be in your interest. \n\n**For each feature selection method, the modeling cells have been commented out to reduce the runtime of this kernel, to run any model, feel free to uncomment those lines**","b59134d3":"### Embedded methods \n\n\nEmbedded methods combine the feature selection and the learning algorithm. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.\n\n#### Ridge\nRidge is another way to do feature selection, where we penalize the coefficients of the model for being too large. This is done by adding a penalty or shrinkage estimator to the regression, which can be controlled with the term *lambda*. Higher the value of lambda, more the shrinkage.","ce7dcab6":"## Wrapping up!\n\nOverall we have seen a number of different ways to do feature selection and we have constructed about 150 different models to see how each of them perform for this dataset. Of these the best methods turned out to be:\n\n* Use Lasso Regression with scaled data:\n    Hyperparameters: `{ alpha=0.0001 ; max_iter=1000; tol=0.0001 }`\n* Gradient Boosting Regressor with SelectKBest:\n    Hyperparameters: `{learning_rate=0.1, n_estimators=100, min_samples_leaf=3, max_depth=9}`\n    \nHere is a boxplot which summarizes how well each feature selection method does - \n![image.png](attachment:image.png)\n![image.png](attachment:image.png)\nFrom the above plots we see that regularization methods work well for this dataset and can be a great starting point. However if you have more compute at your disposal, combining this feature selection method with a gradient boosting would give you much higher accuracies. \n\nHere are some useful links if you would like to learn more -\n* [Feature Selection: A survey](http:\/\/www.realtechsupport.org\/UB\/ML+CT\/papers\/Sahin_FeatureSelectionMethods_2014.pdf)\n* [LendingHome Dataset](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) \n* Verta AI:\n  * Want to use Verta in your experiments? [Start here](https:\/\/verta.readme.io\/docs\/getting-started)\n  * [Why do we need model versioning?](https:\/\/medium.com\/vertaai\/how-to-move-fast-in-ai-without-breaking-things-3ecb74eafd18)","73123253":"### Using domain knowledge to remove features\nGetting to know the dataset and how features can be used for the problem can give one a headstart on building a good model. In our case, the dataset comes with a data dictionary which is a good place to start. We are getting rid of columns which are free text provided by the borrower and columns related to incidents after the loan was granted as these are not of use to us.\nSince we have already removed those columns in the cleaning stage, we can create a model with all the features to see how much worse the model would be.","95004a42":"# Feature Selection with Lending Club Dataset\n\n<!-- introduction - what is FS and why do we need to do it -->\nOne of the most relevant topics in machine learning is identifying which features or attributes to use in your models. In most real world problems, data often has many features but only few are related to the target we are interested in. Feature selection is the process of selecting a subset of features which are then used in model construction.\n\nWe are going to look at data from a popular lending marketplace Lending Club.  Lending club has been transforming the banking system with it's peer-to-peer lending model, where it acts as a bridge between the investor and the borrowers. In this end to end example we're going to be examining different attributes that lending club collects from borrowers and how they influence the interest rates. The dataset contains loan applications from 2007 through 2018 with current loan status and payment information.\n\nOur example dataset has 150 columns containing both categorical and continuous attributes. With feature selection, we are going to create a model with as good or better accuracy while requiring lesser data. Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.","fe95f989":"### Recursive Feature Elimination\nRecursive feature selection works by eliminating the least important features. It continues recursively until the specified number of features is reached. Recursive elimination can be used with any model that assigns weights to features, either through `coef_` or `feature_importances_`. Here we will use random forests to select the best features.","874d04e4":"### Removing highly correlated features\nAfter examining our dataframe's correlation matrix we drop highly correlated\/redundant data to address multicollinearity. When these are not removed it can lead to decreased generalization performance on the test set due to high variance and less model interpretability."}}