{"cell_type":{"d86a319a":"code","961d2e8e":"code","d27f1c50":"code","cd71daf7":"code","3acdfd31":"code","9a1533ce":"code","36c311dc":"code","4cb2b074":"code","09e9beb6":"code","9c22b6d4":"code","ec32fdcc":"code","108b20fb":"code","482c7f65":"code","f4db8848":"code","36632eb8":"code","dca79910":"code","11141d39":"code","e08a3c39":"code","70dba2a6":"code","71134634":"code","3318f785":"code","bbcef7b1":"code","865f9e6a":"code","45f71666":"code","7a228399":"code","c6e0d020":"code","d82c1c15":"code","be9d7ce7":"code","befb7118":"code","0ebe27f9":"code","e127c980":"code","4229ed52":"code","47e9e0f3":"code","169e51d1":"code","f6df15b9":"code","4c578475":"code","c8bde968":"code","2cb73573":"code","ceb5620d":"code","9badd1d5":"code","a61f2cb9":"code","0e9f502e":"code","92692fdb":"code","a10699a9":"code","4e697fdd":"code","f8173222":"code","c22a4450":"code","c78b3e0c":"code","eb79b657":"code","9ba3bfd9":"code","863d375b":"code","54036f7e":"code","43b1f3bd":"code","4590b5c7":"markdown","0e417de8":"markdown","b6cb97ab":"markdown","c3f675ea":"markdown","f6c55e4a":"markdown","4cbac93d":"markdown","e779db49":"markdown","225e6269":"markdown","764c6ab4":"markdown","448bfbe6":"markdown","fbf685bd":"markdown","7e57c70d":"markdown","699dc09d":"markdown","2fe5d4ff":"markdown","45ea0a21":"markdown","94eb6ee2":"markdown","e101114c":"markdown","bb9f919f":"markdown","737cf183":"markdown","b25b5167":"markdown","1ca692ed":"markdown","d6969f2d":"markdown","efe68499":"markdown","de61ea61":"markdown","c31ae097":"markdown","5a448bb4":"markdown","c538ba9d":"markdown","e96abf8a":"markdown","26698640":"markdown","f951295c":"markdown","9d0babc7":"markdown","b097bc8f":"markdown","a5b6eaa7":"markdown","19a122f3":"markdown","af8a1ed6":"markdown","261e1137":"markdown","54bd6726":"markdown","28062980":"markdown","62c23d72":"markdown"},"source":{"d86a319a":"#Uncomment next line to install library from huggingface\n#pip install -qq transformers","961d2e8e":"#importing necessary libraries\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n%matplotlib inline\n%config InlineBackend.figure_format='retina'","d27f1c50":"sns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","cd71daf7":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","3acdfd31":"df.shape","9a1533ce":"df.info()","36c311dc":"df.drop(['keyword', 'location'], axis=1, inplace=True)","4cb2b074":"df.set_index('id', inplace=True)\ndf.head()","09e9beb6":"test.drop(['keyword', 'location'], axis=1, inplace=True)","9c22b6d4":"submission.head()","ec32fdcc":"test['target'] = submission['target']\ntest.head()","108b20fb":"sns.countplot(df.target)\nplt.xlabel('target');","482c7f65":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'","f4db8848":"#Loading a pre-trained BertTokenizer:\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","36632eb8":"sample_txt = 'When was I last outside? I am stuck at home for 20 weeks.'","dca79910":"tokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","11141d39":"tokenizer.sep_token, tokenizer.sep_token_id","e08a3c39":"tokenizer.cls_token, tokenizer.cls_token_id","70dba2a6":"tokenizer.pad_token, tokenizer.pad_token_id","71134634":"tokenizer.unk_token, tokenizer.unk_token_id","3318f785":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","bbcef7b1":"print(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","865f9e6a":"print(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","45f71666":"tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])","7a228399":"token_lens = []\nfor txt in df.text:\n      tokens = tokenizer.encode(txt, max_length=512)\n      token_lens.append(len(tokens))","c6e0d020":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');","d82c1c15":"MAX_LEN = 90","be9d7ce7":"class NLPTweetDataset(Dataset):\n  def __init__(self, texts, targets, tokenizer, max_len):\n    self.texts = texts\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n  def __len__(self):\n    return len(self.texts)\n  def __getitem__(self, item):\n    text = str(self.texts[item])\n    target = self.targets[item]\n    encoding = self.tokenizer.encode_plus(\n      text,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      pad_to_max_length=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n    return {\n      'text': text,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","befb7118":"df_train, df_val = train_test_split(\n  df,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)\ndf_test = test.set_index('id')","0ebe27f9":"df_train.shape, df_val.shape, df_test.shape","e127c980":"def create_data_loader(df, tokenizer, max_len, batch_size):\n  ds = NLPTweetDataset(\n    texts=df.text.to_numpy(),\n    targets=df.target.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n  return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4\n  )","4229ed52":"BATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE) #added the 'target' column to test data for this","47e9e0f3":"data = next(iter(train_data_loader))\ndata.keys()","169e51d1":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","f6df15b9":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","4c578475":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask']\n)","c8bde968":"last_hidden_state.shape","2cb73573":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes):\n    super(SentimentClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  def forward(self, input_ids, attention_mask):\n    _, pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    output = self.drop(pooled_output)\n    return self.out(output)","ceb5620d":"class_names = df.target.unique()\nmodel = SentimentClassifier(len(class_names))\nmodel = model.to(device)","9badd1d5":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","a61f2cb9":"import torch.nn.functional as F\nF.softmax(model(input_ids, attention_mask), dim=1)","0e9f502e":"\nEPOCHS = 10\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","92692fdb":"\ndef train_epoch(\n    model,\n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    n_examples\n):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","a10699a9":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","4e697fdd":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","f8173222":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","c22a4450":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\ntest_acc.item()","c78b3e0c":"def get_predictions(model, data_loader):\n  model = model.eval()\n  texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n  with torch.no_grad():\n    for d in data_loader:\n      texts = d[\"text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      real_values.extend(targets)\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return texts, predictions, prediction_probs, real_values","eb79b657":"y_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","9ba3bfd9":"submission.target = y_pred","863d375b":"submission.head()","54036f7e":"submission.target.describe()","43b1f3bd":"submission.to_csv('submission.csv', index = False)","4590b5c7":"Special token for padding","0e417de8":"# Training","b6cb97ab":"To check the predicted probabilities from our trained model, we\u2019ll apply the softmax function to the outputs","c3f675ea":"Now we have all building blocks required to create a PyTorch dataset. We will now create a class function to create the dataset.","f6c55e4a":"All of the above work can also be done using the encode_plus() method in a single code cell","4cbac93d":"We have the hidden state for each of our 32 tokens (the length of our example sequence)\n768 is the number of hidden units in the feedforward-networks. ","e779db49":"[SEP] - marker for ending of a sentence","225e6269":"The BERT authors have some recommendations for fine-tuning:\n\nBatch size: 16, 32\nLearning rate (Adam): 5e-5, 3e-5, 2e-5\nNumber of epochs: 2, 3, 4\nWe\u2019re going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.","764c6ab4":"Now we\u2019ll use the basic BertModel and build our sentiment classifier on top of it. Let\u2019s begin with loading the model:","448bfbe6":"[CLS] - we must add this token to the start of each sentence, so BERT knows we\u2019re doing classification","fbf685bd":"This does not look like a case of highly imbalanced class, so we can move forward with the original dataset ","7e57c70d":"Looking at an example batch from our training data loader","699dc09d":"The attention mask has the same length","2fe5d4ff":"We need to create a data loader function for the BERT model. We'll use a helper function to do it","45ea0a21":"Now we'll all of this to create a classifier that uses the BERT model","94eb6ee2":"Adding the 'target' column for loading into the data loader function defined later","e101114c":"Calling the BERT 'case' based model as intuitively, it makes sense, since \u201cBAD\u201d might convey more sentiment than \u201cbad\u201d.","bb9f919f":"While the training accuracy keeps increasing with each epoch, we notice that validation results hardly increase and fluctuate around 0.8","737cf183":"Some basic operations convert the text to tokens and tokens to unique integers (ids)","b25b5167":"Beginning the training loop and writing history:","1ca692ed":"The tokenizer is doing most of the work for us. We also return the tweet texts, so it\u2019ll be easier to evaluate the predictions from our model.","d6969f2d":"The classifier delegates most of the work to the BertModel. We include a dropout layer for some regularization and a fully-connected layer for our output. Note that we\u2019re returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.\n\nThis should work like any other PyTorch model. We'll create an instance and move it to the GPU","efe68499":"BERT works with fixed-length sequences. We\u2019ll find the max length and store the token length of each tweet.","de61ea61":"# Including some special toekns in our model:","c31ae097":"We\u2019ll use an example to understand the tokenization process for this model:","5a448bb4":"The token ids are now stored in a Tensor and padded to a length of 32","c538ba9d":"To reproduce the training procedure from the BERT paper, I\u2019ll use the AdamW optimizer provided by Hugging Face. It corrects weight decay, so it\u2019s similar to the original paper. I\u2019ll also use a linear scheduler with no warmup steps.","e96abf8a":"Helper function for training our model for one epoch:","26698640":"Trying it on the encoding of our sample text","f951295c":"Moving the example batch of our training data to the GPU","9d0babc7":"Most of the tweets seem to contain less than 80 tokens, but we\u2019ll be on the safe side and choose a maximum length of 90.","b097bc8f":"# Choosing Sequence Length","a5b6eaa7":"The last_hidden_state is a sequence of hidden states of the last layer of the model. Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state","19a122f3":"The scheduler gets called every time a batch is fed to the model. We\u2019re avoiding exploding gradients by clipping the gradients of the model using clipgrad_norm.\n\nLet\u2019s write another helper function that helps us evaluate the model on a given data loader","af8a1ed6":"# Evaluation","261e1137":"BERT understands tokens that were in the training set. Everything else can be encoded using the [UNK] (unknown) token","54bd6726":"Evaluating against the ground truth provided in sample submission","28062980":"A helper function to obtain the predictions:","62c23d72":"We can inverse the tokenization to look at the special tokens"}}