{"cell_type":{"bc6ef93b":"code","a2f59f42":"code","2871373b":"code","a8b3d745":"code","a910d23e":"code","81443e98":"code","df5c2fd5":"code","99729b6d":"code","06083448":"code","cdbdf0aa":"code","c5a8b003":"code","150a66e6":"code","111efb0d":"code","d5ef6de4":"code","27752252":"code","ff8ab7a0":"code","7032906c":"code","0864c731":"code","93cd2cdf":"code","80662fe2":"code","61313a25":"code","ae700ec3":"code","1b14bc59":"markdown","964cdc5c":"markdown","3053421a":"markdown","7877139e":"markdown","b6f52b0b":"markdown","f17bde92":"markdown","fcd7cc60":"markdown","dbf4698d":"markdown","307f630d":"markdown","21db3cfb":"markdown","4cc88094":"markdown","2cdc157f":"markdown"},"source":{"bc6ef93b":"import numpy as np \nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_palette(sns.cubehelix_palette(10))\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","a2f59f42":"food = pd.read_csv('..\/input\/world-food-facts\/en.openfoodfacts.org.products.tsv', delimiter='\\t', low_memory=False)","2871373b":"fig, ax = plt.subplots(1,1,figsize=(25,10))\nfig= sns.barplot(x='index',y=\"main_category_en\", data=food['main_category_en'].value_counts().reset_index().head(10))\nax.set(ylabel = 'Frequency', xlabel='Category', title='Categories Frequencies')\nplt.show(fig)","a8b3d745":"selected_categories = ['Plant-based foods and beverages', 'Beverages']\nselected_categories = ['Meats']\ntarget_var = 'nutrition-score-uk_100g'\n\npbf = food[food['main_category_en'].isin(selected_categories)] \n\nthreshold = 0.5 #atleast 50% data should not na\ndf = pbf.select_dtypes('number').dropna(axis=1, thresh = int(len(pbf)*threshold)).dropna(how='all').dropna(subset=['nutrition-score-uk_100g']).drop(columns=['nutrition-score-fr_100g'])\ndf.loc[:,df.columns != 'nutrition-score-uk_100g'] = df.loc[:,df.columns != 'nutrition-score-uk_100g'].fillna(0)\ndf","a910d23e":"corrmat = df.corr()\nsns.set(context=\"paper\", font_scale = 1.2)\nf, ax = plt.subplots(figsize=(11, 11))\ncols = corrmat.nlargest(25, 'nutrition-score-uk_100g')['nutrition-score-uk_100g'].index\ncm = corrmat.loc[cols, cols] \nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                 fmt='.2f', annot_kws={'size': 9}, linewidth = 0.1,\n                 yticklabels=cols.values, xticklabels=cols.values, )\nf.text(0.5, 0.93, \"Correlation with nutrition score\", ha='center', fontsize = 24)\nplt.show()","81443e98":"df = df.drop(columns=['salt_100g'])\ndf","df5c2fd5":"from scipy import stats\nfrom scipy.stats import norm, skew \n\nfig, ax = plt.subplots(figsize=(20, 10))\nsns.distplot(df[target_var], fit=norm)\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df[target_var])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n\n#Get also the QQ-plot\nf, ax = plt.subplots(figsize=(20, 10))\nres = stats.probplot(df[target_var], plot=plt)\nplt.show()","99729b6d":"def classify(x):\n    if x <= 9:\n        return ('A')\n    else:\n        return ('B')\n\ndf_plot = df.copy()\ndf_plot['class'] = df_plot[target_var].apply(classify)\ndf_plot['class'].value_counts()","06083448":"fig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.distplot(df_plot[df_plot['class'] == 'A'][target_var], color = \"blue\", ax = ax[0], bins=10)\nsns.distplot(df_plot[df_plot['class'] == 'B'][target_var], color = \"orange\", ax = ax[1], bins=10)\nax[0].set_title(\"Class A\")\nax[1].set_title(\"Class B\")","cdbdf0aa":"from sklearn.manifold import TSNE\npredictor = df.columns[df.columns != target_var]\nX_embedded = TSNE(n_components=2, random_state = 127001).fit_transform(df[predictor])\ndf_embedded = pd.DataFrame(X_embedded)","c5a8b003":"df_combined = df_embedded.copy()\ndf_combined['class'] = df_plot['class'].values","150a66e6":"fig, ax = plt.subplots(figsize=(15,15))\nax = sns.scatterplot(x=0, y=1, data=df_combined, hue='class')\nax.set(ylabel = 'Embedded Feature 2', xlabel='Embedded Feature 1')\nfig.text(0.5, 0.93, \"Meat Data Distribution (After Embedded)\", ha='center', fontsize = 24)\nplt.show()","111efb0d":"fig, ax = plt.subplots(10,1, figsize = (20,30))\nfor i in range(10):\n    sns.violinplot(df[predictor[i]], ax=ax[i])\nfig.tight_layout(pad=5)\nplt.show()","d5ef6de4":"c1 = additives_outlier = df['additives_n'] > 10\nc2 = ing_oil_outlier = df['ingredients_from_palm_oil_n'] > 0.2\nc3 = ing_may_oil_outlier = df['ingredients_that_may_be_from_palm_oil_n'] > 1\nc4 = energy_outlier = df['energy_100g'] > 2500\nc5 = fat_outlier = df['fat_100g'] > 60\nc6 = sat_fat_outlier = df['saturated-fat_100g'] > 25\nc7 = carbo_outlier = df['carbohydrates_100g'] > 20\nc8 = sugar_outlier = df['sugars_100g'] > 5\nc9 = protein_outlier = df['proteins_100g'] > 40\nc10 =sodium_outlier = df['sodium_100g'] > 3\n\n# drop all outlier\ndf_clean = df.drop(df[c1|c2|c3|c4|c5|c6|c7|c8|c9|c10].index)\ndf_clean","27752252":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","ff8ab7a0":"#Validation function\nn_folds = 5\n\nX_train, X_test, y_train, y_test = train_test_split(df_clean[predictor], df_clean[target_var], test_size=0.1, random_state=127001)\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=127001).get_n_splits(df_clean.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(model):\n    rmse = np.sqrt(mean_squared_error(model.predict(X_test), y_test))\n    return rmse","7032906c":"# Base models \nlinear = LinearRegression()\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=127001))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=127001))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =127001)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =127001, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n","0864c731":"# base models score\nrmse_linear = rmsle_cv(linear)\nrmse_lasso = rmsle_cv(lasso)\nrmse_enet = rmsle_cv(ENet)\nrmse_krr = rmsle_cv(KRR)\nrmse_gboost = rmsle_cv(GBoost)\nrmse_xgb = rmsle_cv(model_xgb)\nrmse_lgb = rmsle_cv(model_lgb)","93cd2cdf":"print(f\"\"\"Model RMSE on {n_folds} Folds of Training Data\nlinear model:\\t mean= {rmse_linear.mean()},\\t std= {rmse_linear.std()}\nlasso model:\\t mean= {rmse_lasso.mean()},\\t std= {rmse_lasso.std()}\nenet model:\\t mean= {rmse_enet.mean()},\\t std= {rmse_enet.std()}\nkrr model:\\t mean= {rmse_krr.mean()},\\t std= {rmse_krr.std()}\ngboost model:\\t mean= {rmse_gboost.mean()},\\t std= {rmse_gboost.std()}\nxgboost model:\\t mean= {rmse_xgb.mean()},\\t std= {rmse_xgb.std()}\nlgboost model:\\t mean= {rmse_lgb.mean()},\\t std= {rmse_lgb.std()}\n\"\"\")","80662fe2":"linear.fit(X_train, y_train)\nlasso.fit(X_train, y_train)\nENet.fit(X_train, y_train)\nKRR.fit(X_train, y_train)\nGBoost.fit(X_train, y_train)\nmodel_xgb.fit(X_train, y_train)\nmodel_lgb.fit(X_train, y_train)","61313a25":"rmse_t_linear = rmsle(linear)\nrmse_t_lasso = rmsle(lasso)\nrmse_t_enet = rmsle(ENet)\nrmse_t_krr = rmsle(KRR)\nrmse_t_gboost = rmsle(GBoost)\nrmse_t_xgb = rmsle(model_xgb)\nrmse_t_lgb = rmsle(model_lgb)","ae700ec3":"print(f\"\"\"Model RMSE on Data Test\nlinear model:\\t mean= {rmse_t_linear}\nlasso model:\\t mean= {rmse_t_lasso}\nenet model:\\t mean= {rmse_t_enet}\nkrr model:\\t mean= {rmse_t_krr}\ngboost model:\\t mean= {rmse_t_gboost}\nxgboost model:\\t mean= {rmse_t_xgb}\nlgboost model:\\t mean= {rmse_t_lgb}\n\"\"\")","1b14bc59":"Based on our training data using K-fold cross validation, looks like gradient boosting `gboost` model has lowest average rmse score for the Folds. But looking at `xgboost` standadrt deviation, it's more stable than `gboost`. \n\nNow let's check on out test data, wich model is better","964cdc5c":"Now that we have suspected the data to be distributed into 2 cluster, let's try to check embed predictor and see the distribution using clustering methods. ","3053421a":"### Modeling","7877139e":"Now let's see the target variable distribution","b6f52b0b":"# Food Nutrition Prediction : Meat\nQuick inspection on what really makes nutrition score goods on meat","f17bde92":"### Remove Outlier","fcd7cc60":"Kernel ridge regression is the most robust model on linear regression models. However, ensemble boosting models like `gboost`, `xgboost`, or `lgboost` gives better performance and more stable during training.","dbf4698d":"# Conclusion","307f630d":"## Nutrition Score Prediction","21db3cfb":"From grph above, the distribution seems not normal. It's likely to form 2 normal distributions instead of one. In order to inspect this further, let's try to separate them into **two** classes.","4cc88094":"**How many clusters is there ?**\n\nFrom figure above, TSNE actually can catch several behaviour and separate the data quite well. By combining the pure-embedded data with our self-defined class,  Even if we can't interpret what `x` and `y` axis meaning, we can suspect anomalies on each clusters when very litle poriton of one class are grouped with other. We can also consider the number of classes to match the cluster number instead of dictate it like we already did. \n\n","2cdc157f":"From the correlation plot, we see almost all varibles have positive correlation to the target variable. But there's a redundant variables where the correlation is `1`, `salt_100g` and `sodium_100g` we should remove one of it. "}}