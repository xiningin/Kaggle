{"cell_type":{"28ce6f31":"code","b74dbed7":"code","2be67674":"code","5e6c9ad7":"code","4950469d":"code","fd676bac":"code","4ed43293":"code","1efec6b9":"code","2f60f8d2":"code","6370af65":"code","a485f19b":"code","e814c4af":"code","0ddbcf2d":"code","24ec4eaa":"code","f895e21a":"code","c557473b":"code","a28b5582":"code","5f4462a7":"code","23748aa9":"code","1815cc81":"code","6fce865b":"code","5c8165f5":"code","46b401f1":"code","bc1f3ec7":"code","17dbcdc4":"code","620a10dd":"code","8d65224b":"code","37e7c257":"code","619364f3":"code","d9950ac5":"code","40cc4ead":"code","5017215f":"code","92fbe62d":"code","de220c61":"code","c78ae978":"code","1563fa54":"code","22d61482":"code","a41de0ce":"code","425509ec":"code","97cdb869":"code","6e651769":"code","d076469a":"code","c1d5b106":"code","b78756d0":"code","ec18e9c2":"code","e918c9eb":"code","99af5abb":"code","61a07af2":"code","4001240c":"code","2fb1bf37":"code","56d960c9":"code","952ce77a":"code","71c85980":"code","4f2480ae":"code","037a905e":"code","26045e46":"code","e3a5faed":"code","bd788a7b":"code","f8d00215":"code","4f6eb3c1":"code","b1b12d5f":"code","438db9c3":"code","2872e15a":"code","69a86701":"code","c974e90e":"code","2df116cb":"code","d015c742":"code","d263c01e":"code","2c47c988":"code","bc345f73":"code","03627936":"code","2a22aa11":"code","a0ae02fb":"markdown","3ea6c705":"markdown","33627098":"markdown","c0c36af0":"markdown","0c1967f7":"markdown","9428f6a4":"markdown","d92fc5d0":"markdown","8e7dd9a1":"markdown","821ea913":"markdown","ad21b252":"markdown","1a25478c":"markdown","c56c6d9e":"markdown","2518b75b":"markdown","3d817273":"markdown","68ce04c5":"markdown","1aa8885e":"markdown","4af9bfd5":"markdown","fe922b1d":"markdown","e526856f":"markdown","ae000ed9":"markdown","5eee6b94":"markdown","a8d7c457":"markdown","e2944120":"markdown","a3fe3a0d":"markdown","cac1123d":"markdown","9915f11a":"markdown","f8abc30e":"markdown","742d6bda":"markdown","c1a43eff":"markdown","678091af":"markdown","8575ea1a":"markdown","a6a79251":"markdown","b68b1f9c":"markdown","50693843":"markdown","af21d66c":"markdown","d6b6b7b0":"markdown","315be5a1":"markdown","0000d6f8":"markdown","9755c9fc":"markdown","690b4f31":"markdown","25ae7198":"markdown","e3772ddb":"markdown","b57e1e43":"markdown","65fe2e87":"markdown","defa28b2":"markdown","0d99d8ad":"markdown","a857e231":"markdown","ee3bde61":"markdown","dd30424e":"markdown","fbb1d256":"markdown","5d15d7e5":"markdown","3488052e":"markdown","aba61357":"markdown"},"source":{"28ce6f31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b74dbed7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_rows = None\nsns.set_theme(style='darkgrid', palette='deep', font='sans-serif')","2be67674":"# Load Current Application Data \ncapp=pd.read_csv(\"..\/input\/bank-loans-dataset\/application_data.csv\")","5e6c9ad7":"# Get the info on DataFrame and Size \ncapp.info(verbose=True,null_counts=True)\nprint(\"Shape of Application Data: \",capp.shape)","4950469d":"# Loading Previous Application Data\nprev_app=pd.read_csv(\"..\/input\/bank-loans-dataset\/previous_application.csv\")","fd676bac":"# % of Null Values w.r.t to all the columns of Current Application:\n(capp.isnull().mean()).sort_values(ascending=False)","4ed43293":"capp = capp.loc[:,capp.isnull().mean()<=0.45]\n(capp.isnull().mean()).sort_values(ascending=False)","1efec6b9":"capp.OCCUPATION_TYPE.value_counts().plot.barh()\nplt.show()","2f60f8d2":"capp['OCCUPATION_TYPE'].fillna(method='ffill',inplace=True)","6370af65":"sns.boxplot(x=capp['TARGET'] , y= capp['EXT_SOURCE_3'])\nplt.show()","a485f19b":"capp['EXT_SOURCE_3'] =capp['EXT_SOURCE_3'].fillna(value=capp['EXT_SOURCE_3'].mean())\ncapp['EXT_SOURCE_2'] =capp['EXT_SOURCE_2'].fillna(value=capp['EXT_SOURCE_2'].mean())","e814c4af":"fig, (ax1,ax2,ax3,ax4,ax5,ax6)=plt.subplots(ncols=6,figsize=(32,7))\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_YEAR),ax=ax1)\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_MON),ax=ax2)\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_WEEK),ax=ax3)\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_DAY),ax=ax4)\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_QRT),ax=ax5)\nsns.boxplot((capp.AMT_REQ_CREDIT_BUREAU_HOUR),ax=ax6)\nplt.plot()\n","0ddbcf2d":"## extarct count of missing values in AMT_REQ_CREDIT_BUREAU***** Columns\nclist=[capp[col].isnull().sum() for col in capp if col.startswith('AMT_REQ_CREDIT_BUREAU') ]\nclist","24ec4eaa":"def impute_missing(col):\n    capp[col]=capp[col].fillna(value=capp[col].median())\n    return capp[col].isnull().sum()","f895e21a":"# by calling fuction impute_missing we can get rid of missing values in specified columns\nimpute_missing('AMT_REQ_CREDIT_BUREAU_YEAR')\nimpute_missing('AMT_REQ_CREDIT_BUREAU_MON')\nimpute_missing('AMT_REQ_CREDIT_BUREAU_WEEK')\nimpute_missing('AMT_REQ_CREDIT_BUREAU_DAY')\nimpute_missing('AMT_REQ_CREDIT_BUREAU_HOUR')\nimpute_missing('AMT_REQ_CREDIT_BUREAU_QRT')","c557473b":"fig, (ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(25,7))\nsns.boxplot((capp.OBS_30_CNT_SOCIAL_CIRCLE),ax=ax1)\nsns.boxplot((capp.OBS_60_CNT_SOCIAL_CIRCLE),ax=ax2)\nsns.boxplot((capp.DEF_30_CNT_SOCIAL_CIRCLE),ax=ax3)\nsns.boxplot((capp.DEF_60_CNT_SOCIAL_CIRCLE),ax=ax4)\nplt.plot()","a28b5582":"# Treating Extreme Value with median Value:\ncapp[\"OBS_60_CNT_SOCIAL_CIRCLE\"] = np.where(capp[\"OBS_60_CNT_SOCIAL_CIRCLE\"] > 50, float(capp['OBS_60_CNT_SOCIAL_CIRCLE'].median()), capp[\"OBS_60_CNT_SOCIAL_CIRCLE\"])\ncapp[\"OBS_30_CNT_SOCIAL_CIRCLE\"] = np.where(capp[\"OBS_30_CNT_SOCIAL_CIRCLE\"] > 50, float(capp['OBS_30_CNT_SOCIAL_CIRCLE'].median()), capp[\"OBS_30_CNT_SOCIAL_CIRCLE\"])\n\n# Trating missing Values with Median Value:\nimpute_missing('OBS_60_CNT_SOCIAL_CIRCLE')\nimpute_missing('OBS_30_CNT_SOCIAL_CIRCLE')\nimpute_missing('DEF_30_CNT_SOCIAL_CIRCLE')\nimpute_missing('DEF_60_CNT_SOCIAL_CIRCLE')","5f4462a7":"print(capp['NAME_TYPE_SUITE'].unique())\ncapp['NAME_TYPE_SUITE'].value_counts().plot.barh()\nplt.show()","23748aa9":"capp.NAME_TYPE_SUITE.fillna(capp.NAME_TYPE_SUITE.mode()[0],inplace=True)","1815cc81":"capp.dropna(axis=0,inplace=True)","6fce865b":"for col in capp.columns:\n  k= capp[col].unique()\n  print(col, k)","5c8165f5":"plt.pie(capp['CODE_GENDER'].value_counts(normalize=True)*100,autopct='%1.3f%%')\nplt.title(\"Plot of Gender distribution\")\nplt.show()\nprint(capp['CODE_GENDER'].value_counts())","46b401f1":"capp.drop(capp[capp['CODE_GENDER']=='XNA'].index,inplace=True)","bc1f3ec7":"# Lets extract list of all the days col and having negative values\ntemp_list=[]\nplotnum=1\nplt.figure(figsize=(25,15), facecolor='white')\nfor col in capp.columns:\n  if col.startswith('DAYS'):\n    ax=plt.subplot(5,3,plotnum)\n    sns.boxplot(x=capp['TARGET'],y=capp[col])\n    plt.title(col)\n    plt.plot()\n    temp_list.append(col)\n    plotnum+=1\nprint(temp_list)","17dbcdc4":"# we have list of columns staring with Days stored in temp_list we can convert it to absolute value\ncapp[temp_list] = abs(capp[temp_list])\n","620a10dd":"capp[temp_list].describe()","8d65224b":"print(capp['DAYS_EMPLOYED'].quantile([0,0.1,0.3,0.6,0.8,0.9,0.95,1]))","37e7c257":"df1 = (capp[capp['DAYS_EMPLOYED']==365243.0]).copy()\n\nplt.figure(figsize=(25,7))\nsns.barplot(x=df1['OCCUPATION_TYPE'].value_counts().index,y=df1['OCCUPATION_TYPE'].value_counts())\nplt.xticks(rotation=45)\nplt.title(\"Occupation Type for employed days-365243\")\nplt.plot()\ndf1.shape","619364f3":"df1['ORGANIZATION_TYPE'].value_counts()","d9950ac5":"plt.figure(figsize=(30,10))\nsns.barplot(x=capp['ORGANIZATION_TYPE'].value_counts().index,y=capp['ORGANIZATION_TYPE'].value_counts())\nplt.xticks(rotation=70)\nplt.title(\"Distribution of Organization Type\")\nplt.plot()","40cc4ead":"\ndf0 =capp[capp['TARGET']==1]\nplt.figure(figsize=(30,10))\nsns.barplot(x=df0['ORGANIZATION_TYPE'].value_counts().index,y=df0['ORGANIZATION_TYPE'].value_counts())\nplt.xticks(rotation=70)\nplt.title(\"Distribution of Organization Type\")\nplt.plot()","5017215f":"capp.ORGANIZATION_TYPE.replace(to_replace='XNA',value='Unknown',inplace=True)\ncapp.DAYS_EMPLOYED.replace(to_replace=365243.0,value=0,inplace=True)","92fbe62d":"capp_columns = ['AMT_INCOME_TOTAL','AMT_CREDIT','DAYS_EMPLOYED','AMT_ANNUITY']\nplotnum=1\nplt.figure(figsize=(25,15), facecolor='white')\nfor col in capp_columns:\n  ax=plt.subplot(5,2,plotnum)\n  sns.boxplot(x=capp['TARGET'],y=capp[col])\n  plt.title(col)\n  plt.plot()\n  plotnum+=1","de220c61":"# Creating bins for income amount\nbins = [0,1,2,3,4,5,6,7,8,9,10,15]\nslot = ['0-100K','100K-200K', '200k-300k','300k-400k','400k-500k','500k-600k','600k-700k','700k-800k','800k-900k','900k-1M', '1M Above']\ncapp['AMT_INCOME_TOTAL']=capp['AMT_INCOME_TOTAL']\/100000\ncapp['AMT_INCOME_RANGE']= pd.cut(capp['AMT_INCOME_TOTAL'], bins,labels=slot)\ncapp['AMT_INCOME_RANGE'].value_counts(normalize = True)*100\ncapp['AMT_INCOME_TOTAL']=capp['AMT_INCOME_TOTAL']*100000\ncapp['AMT_INCOME_RANGE'].value_counts(normalize = True)*100","c78ae978":"# Creating bins for Credit amount\nbins = [0,1,2,3,4,5,6,7,8,9,10,15]\nslot = ['0-100K','100K-200K', '200k-300k','300k-400k','400k-500k','500k-600k','600k-700k','700k-800k','800k-900k','900k-1M', '1M Above']\ncapp['AMT_CREDIT']=capp['AMT_CREDIT']\/100000\ncapp['AMT_CREDIT_RANGE']= pd.cut(capp['AMT_CREDIT'], bins,labels=slot)\ncapp['AMT_CREDIT_RANGE'].value_counts(normalize = True)*100\ncapp['AMT_CREDIT']=capp['AMT_CREDIT']*100000\ncapp['AMT_CREDIT_RANGE'].value_counts(normalize = True)*100","1563fa54":"#Creating bins for Employement Time\ncapp['YEARS_EMPLOYED'] = capp['DAYS_EMPLOYED']\/\/365\nbins = [-1,5,10,20,30,40,50,100]\nslots = ['0-5Y','5-10Y','10-20Y','20-30Y','30-40Y','40-50Y','50 above']\ncapp['EMPLOYMENT_YEAR']=pd.cut(capp['YEARS_EMPLOYED'],bins=bins,labels=slots)\ncapp['EMPLOYMENT_YEAR'].value_counts(normalize = True)*100","22d61482":"#Creating bins for amt_annuity\nbins = [0,5000,10000,20000,30000,40000,50000,300000]\nslot = ['Very Low','Low', 'Average','Above Average','High','Extremely High']\ncapp['AMT_ANNUITY_RANGE']=pd.cut(capp['AMT_ANNUITY'],bins=bins,labels=slots)\ncapp['AMT_ANNUITY_RANGE'].value_counts(normalize = True)*100\n","a41de0ce":"#Conversion of Object and Numerical columns to Categorical Columns\ncat_col = ['NAME_CONTRACT_TYPE','CODE_GENDER','NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE',\n                       'NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START',\n                       'ORGANIZATION_TYPE','FLAG_OWN_CAR','FLAG_OWN_REALTY','LIVE_CITY_NOT_WORK_CITY',\n                       'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','REG_REGION_NOT_WORK_REGION',\n                       'LIVE_REGION_NOT_WORK_REGION','REGION_RATING_CLIENT','WEEKDAY_APPR_PROCESS_START',\n                       'REGION_RATING_CLIENT_W_CITY']\nfor col in cat_col:\n    capp[col] =pd.Categorical(capp[col])","425509ec":"# creating a column ratio of Amount Credit and Total Income\ncapp['CREDIT_INCOME_RATIO']=round((capp['AMT_CREDIT']\/capp['AMT_INCOME_TOTAL']))","97cdb869":"plt.figure(figsize=(12,7))\nsns.boxplot(x=capp['TARGET'],y=capp['CREDIT_INCOME_RATIO'])\nplt.title('Distribution of - CREDIT_INCOME_RATIO')\nplt.plot()","6e651769":"list_FinalCol=['SK_ID_CURR','TARGET','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','AMT_ANNUITY_RANGE','AMT_CREDIT','AMT_INCOME_TOTAL',\n'CREDIT_INCOME_RATIO','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','DAYS_EMPLOYED','DAYS_REGISTRATION',\n'FLAG_EMAIL','OCCUPATION_TYPE', 'CNT_FAM_MEMBERS','REGION_RATING_CLIENT_W_CITY','ORGANIZATION_TYPE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',\n'OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','NAME_CONTRACT_TYPE','AMT_ANNUITY',\n'REGION_RATING_CLIENT','AMT_GOODS_PRICE','EMPLOYMENT_YEAR','AMT_INCOME_RANGE']","d076469a":"capp_final =capp[list_FinalCol]","c1d5b106":"plt.pie(capp_final['TARGET'].value_counts(normalize=True),labels=['NON-DEFAULTER','DEFAULTER'],explode=(0,0.05), autopct='%1.1f%%',shadow=True, startangle=90)\nplt.title('Distribution of Defaulters & Non-Defaulters')\nplt.show()","b78756d0":"# Dataset with Target values zeros\nCapp0 = capp_final[capp_final['TARGET']==0]\nCapp0.head()","ec18e9c2":"# Dataset with Target values One's\nCapp1 = capp_final[capp_final['TARGET']==1]\nCapp1.head()","e918c9eb":"cat_col =Capp1.select_dtypes(include='category').columns\nnum_col =Capp1.select_dtypes(exclude='category').columns","99af5abb":"\ndef plotting(column, hue):\n    col = column\n    hue = hue\n    fig = plt.figure(figsize=(13,10))\n\n    ax1 = plt.subplot(221)\n    capp_final[col].value_counts().plot.pie(autopct = \"%1.2f%%\", ax=ax1)\n    plt.legend(loc=\"best\",bbox_to_anchor=(0.2, 0., 0.2, 0.2))\n    plt.title('% Distribution of: '+ column)\n\n    ax2 = plt.subplot(222)\n    df = pd.DataFrame()\n    df['0']= ((Capp0[col].value_counts())\/len(Capp0))\n    df['1']= ((Capp1[col].value_counts())\/len(Capp1))\n    df.plot.bar(ax=ax2)\n    plt.title('Comparision based on Target Value and Total Count')\n\n    ax3 = plt.subplot(223)\n    sns.countplot(x=col, hue=hue, data=Capp0, ax = ax3)\n    plt.xticks(rotation=70)\n    plt.title('% Distribution of(Target=0): '+ column)\n     # Adding the normalized percentage for easier comparision between defaulter and non-defaulter\n    for p in ax3.patches:\n        ax3.annotate('{:.1f}%'.format((p.get_height()\/len(Capp0))*100), (p.get_x()+0.3, p.get_height()+60))\n        \n    ax4 = plt.subplot(224)\n    sns.countplot(x=col, hue=hue, data=Capp1, ax = ax4)\n    plt.xticks(rotation=70)\n    plt.title('% Distribution of(Target=1): '+ column)\n    for p in ax4.patches:\n        ax4.annotate('{:.1f}%'.format((p.get_height()\/len(Capp1))*100), (p.get_x()+0.3, p.get_height()+60))\n\n    fig.tight_layout()\n    plt.show()","61a07af2":"plotcol = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS',\n           'NAME_HOUSING_TYPE','REGION_RATING_CLIENT','AMT_INCOME_RANGE','EMPLOYMENT_YEAR', 'AMT_ANNUITY_RANGE']\n\nfor col in plotcol:\n  print(\"Plotting :\",col)\n  plotting(col, 'TARGET')","4001240c":"num_col0=['CREDIT_INCOME_RATIO','AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE','DAYS_EMPLOYED','DAYS_REGISTRATION','CNT_FAM_MEMBERS']\nfor col in num_col0:\n  fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n  sns.distplot(a=Capp0[col],ax=ax1)\n  ax1.set_title(f'Distribution of {col} for Non-Defaulters',fontsize=12)\n  sns.distplot(a=Capp1[col],ax=ax2)\n  ax2.set_title(f'Distribution of {col} for Defaulters',fontsize=12)\n  plt.show()","2fb1bf37":"corr=Capp0.corr()\ncorr_df = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool)).unstack().reset_index()\ncorr_df.columns=['Column1','Column2','Correlation']\ncorr_df.dropna(subset=['Correlation'],inplace=True)\ncorr_df['Abs_Correlation']=corr_df['Correlation'].abs()\ncorr_df = corr_df.sort_values(by=['Abs_Correlation'], ascending=False)\ncorr_df.head(10)","56d960c9":"corr=Capp1.corr()\ncorr_df = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool)).unstack().reset_index()\ncorr_df.columns=['Column1','Column2','Correlation']\ncorr_df.dropna(subset=['Correlation'],inplace=True)\ncorr_df['Abs_Correlation']=corr_df['Correlation'].abs()\ncorr_df = corr_df.sort_values(by=['Abs_Correlation'], ascending=False)\ncorr_df.head(10)","952ce77a":"def mulvarplot(col1,col2,hue_col):\n  sns.set_theme(style='white', palette='deep', font='sans-serif')\n  fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,8))\n  sns.scatterplot(x=col1,y=col2,data=Capp0,markers=True,hue=hue_col,legend='auto',ax=ax1)\n  ax1.set_xlabel(col1)\n  ax1.set_ylabel(col2)\n  ax1.set_title(f'{col1} vs {col2} for Non-Defaulters',fontsize=12)\n\n  sns.scatterplot(x=col1,y=col2,data=Capp1,markers=True,hue=hue_col,legend='auto',ax=ax2)\n  ax2.set_xlabel(col1)\n  ax2.set_ylabel(col2)\n  ax2.set_title(f'{col1} vs {col2} for Defaulterss ',fontsize=12)\n\n  plt.show()","71c85980":"mulvarplot('AMT_GOODS_PRICE','CREDIT_INCOME_RATIO','AMT_ANNUITY')","4f2480ae":"mulvarplot('AMT_GOODS_PRICE','DAYS_EMPLOYED','CNT_FAM_MEMBERS')","037a905e":"mulvarplot('CREDIT_INCOME_RATIO','CNT_FAM_MEMBERS','TARGET')","26045e46":"prev_app.info(null_counts=True)","e3a5faed":"# droping some of unwanted columns \nprev_app.drop(labels=['WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START','FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY',\n                      'AMT_DOWN_PAYMENT','RATE_DOWN_PAYMENT','RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED','NAME_TYPE_SUITE',\n                      'DAYS_FIRST_DRAWING','DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION'],axis=1,inplace=True)","bd788a7b":"prev_app = prev_app.loc[:,prev_app.isnull().mean()<=0.40]\n\n\n# checking the null value % in Previous Application Dataframe\n(round(prev_app.isnull().sum() \/ prev_app.shape[0] * 100.00,2)).sort_values(ascending=False)","f8d00215":"prev_app['DAYS_DECISION']=abs(prev_app['DAYS_DECISION'])","4f6eb3c1":"## Crating bin of Days Decision:\n\nbins = [0,400,800,1200,1600,2000,2400,2800,3200]\nslots = ['0-400','400-800','800-1200','1200-1600','1600-2000','2000-2400','2400-2800','2800-3200']\nprev_app['DAYS_DECISION_RANGE']=pd.cut(prev_app['DAYS_DECISION'],bins=bins,labels=slots)","b1b12d5f":"prev_app['DAYS_DECISION_RANGE'].value_counts(normalize=True)*100","438db9c3":"# these 3 columns do have some missing values let us obserbe the distribution along with boxplot\nfig, (ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(25,7))\nsns.boxplot((prev_app.AMT_GOODS_PRICE),ax=ax1)\nsns.boxplot((prev_app.CNT_PAYMENT),ax=ax2)\nsns.boxplot((prev_app.AMT_ANNUITY),ax=ax3)\nplt.plot()","2872e15a":"# Let us impute null values:\nn_col = ['AMT_GOODS_PRICE','CNT_PAYMENT','AMT_ANNUITY']\nfor col in n_col:\n    prev_app[col]=prev_app[col].fillna(value=prev_app[col].median())\n    prev_app[col].isnull().sum()\n\nprev_app['PRODUCT_COMBINATION']=prev_app['PRODUCT_COMBINATION'].fillna(value=prev_app['PRODUCT_COMBINATION'].mode())","69a86701":"col_list = ['NAME_CONTRACT_STATUS', 'NAME_CLIENT_TYPE', 'NAME_PORTFOLIO','CHANNEL_TYPE','DAYS_DECISION_RANGE','NAME_CASH_LOAN_PURPOSE',\n            'NAME_YIELD_GROUP','PRODUCT_COMBINATION']\nplt.figure(figsize = (28,16))\nplotnum = 1\nfor col in col_list:\n  plt.subplot(2,4,plotnum)\n  sns.countplot(data= prev_app, x= col)\n  plt.xticks(rotation = 70)\n  plotnum+=1\nplt.show()","c974e90e":"# Let Us combine the Current Application and Previous Apllication\nm_app = pd.merge(capp_final,prev_app,how='left', on=['SK_ID_CURR'])","2df116cb":"m_app.pivot_table(values='TARGET',index='NAME_CONTRACT_STATUS',columns='CODE_GENDER',aggfunc='mean').plot.bar(figsize=(8,5))\nplt.xlabel('Loan Status from Previos Application')\nplt.ylabel('Defaulters')\nplt.title('Gender Vs Previous loan status')\nplt.show()","d015c742":"plt.figure(figsize = (10,8))\nsns.barplot(data=m_app,x='CHANNEL_TYPE', y='CNT_PAYMENT',hue='TARGET' )\nplt.xticks(rotation=70)\nplt.show()","d263c01e":"df0 = pd.pivot_table(data=m_app,values='AMT_ANNUITY_x', index= 'NAME_CLIENT_TYPE',columns='TARGET', aggfunc='count').plot.bar(figsize=(8,5))\nplt.show()","2c47c988":"d = m_app.groupby(\"NAME_FAMILY_STATUS\")[\"TARGET\"]\ndf1 = pd.concat([d.value_counts(),round(d.value_counts(normalize=True).mul(100),2)],axis=1, keys=('Counts','Percentage'))\ndf1.sort_values(by=['TARGET','Percentage'],ascending=False).head(5)","bc345f73":"d = m_app.groupby(\"NAME_CASH_LOAN_PURPOSE\")[\"TARGET\"]\ndf1 = pd.concat([d.value_counts(),round(d.value_counts(normalize=True).mul(100),2)],axis=1, keys=('Counts','Percentage'))\ndf1.sort_values(by=['TARGET','Percentage'],ascending=False).head(10)","03627936":"d = m_app.groupby(\"OCCUPATION_TYPE\")[\"TARGET\"]\ndf1 = pd.concat([d.value_counts(),round(d.value_counts(normalize=True).mul(100),2)],axis=1, keys=('Counts','Percentage'))\ndf1.sort_values(by=['TARGET','Percentage'],ascending=False).head(10)","2a22aa11":"d = m_app.groupby(\"NAME_CONTRACT_STATUS\")[\"TARGET\"]\ndf1 = pd.concat([d.value_counts(),round(d.value_counts(normalize=True).mul(100),2)],axis=1, keys=('Counts','Percentage'))\ndf1.sort_values(by=['TARGET','Percentage'],ascending=False).head(5)","a0ae02fb":"### 2.3 Let us go ahead and plot and understand the corelation of numerical columns :","3ea6c705":"## **Insight**:\n\n**Almost 37% loan applicatants have applied for a new loan within 0-400 days of previous loan decision.**","33627098":"# Insight:\n\n**AMT_CREDIT_RANGE: Around 18% of the Applicant has got Amount Credit of Range 200-300K. Also. around 13% of applicant has got Amount Credit of above 1M.**","c0c36af0":"## 1.3 Null Values","0c1967f7":"## 2. Treating Categorical Variable","9428f6a4":"5.5 Let us plot the Categorical columns from Previous Application Dataset:","d92fc5d0":"5.6 Merging the both the files Current and Previous application :","8e7dd9a1":"5.2 Drouping columns with 40% of null Values","821ea913":"1.4.1 Now that we have around 31% Let us plot and see the distribution of Occupation Type:","ad21b252":"1. EXT_SOURCE_3 & EXT_SOURCE_2 is some kind of score collected from external source, & has around 20% missing value in it.\n2. Let's plot Boxplot around Target variable. and see the distribution around this variable.","1a25478c":"1.Missing Records in - AMT_GOODS_PRICE, AMT_ANNUITY,CNT_FAM_MEMBERS is less than 1%\n2.Loan Annulity or Ammount Goods Price are important in loan application and missing of these records can be considered as Missing not at random & we can drop the rows.","c56c6d9e":"# Insight:\n\n**EMPLOYMENT_YEAR: 67% of the Applicant belongs to Employment Year period 0-5 years**\n","2518b75b":"Now it is clear that applicants whose Organization Type is missing has put their days employed to 1000 years. hence we can assume that missing value of both the columns are corelated\nEither applicant do not want to disclose their Organization Type and Days they are employed.\nWe can impute Days Employed to 0 and Organization Type to Unknown.","3d817273":"Except \"Day Employed\" all the Day column have negative value. We can go ahead with converying them to positive value.","68ce04c5":"## Insight:\n___________________________________________________________________________________\n\n**1. Applicants accuired from Car Dealer & Corporate Sales default rate is higher.**\n\n**2. Civil Marriage & Single applicants tends to Default high**\n\n**3. Applicants not disclosingPurpose of loan has the most default rate.**\n\n**4. Low- Skill Laborers are deafult higher.**\n\n**5. Applicants whoes application was Refused or Canceled earlier, defaulted most.**","1aa8885e":"Days Employed Seems to have oulier value we can see that in details :","4af9bfd5":"5.3 Days Decision columns needs to be converted to positive value","fe922b1d":"1.Days Employed has around 55k records filled with 365243 which comes to around 1000 years and looks like a default value is filled for days employed rather than leaving them blank.\n2. Organization Type has 2nd Most value filled with 'XNA'","e526856f":"## 3.Univariate Ananlysis\n\n3.1 We can split and create two list of Categorical Columns and Numerical Column:","ae000ed9":"##### All the Columns related to \"CNT_SOCIAL_CIRCLE\" seems to have missing values & outlier we will have remove that as well","5eee6b94":"**Above all the columns seems to have outliers values**\n\n2.1 Some of the numerical columns we can covert it to Categorical columns by creating bins, which will help us for further analysis.","a8d7c457":"1. Field OccupationType has missing value of more than 31% and filling those value with most frequent value of the field could make it baised towards one type of Occupation Type.\n\n2. Hence I will go ahead with forward fill which will help us with unbaised filling of missing records.","e2944120":"1. It is clear that our dataset has Unequal Class Distribution (Defaulters with 8.1% & Non- Defaulters with 91.9%) which is technically imbalanced Dataset\n2. We can Split the columns into two parts based on Target Value.","a3fe3a0d":"2.2 Creating List of Columns which are Important and we can consider thise columns for further analysis.","cac1123d":"## 1.4 Missing value Treatment","9915f11a":"The loan providing companies find it hard to give loans to the people due to their insufficient or non-existent credit history. Because of that, some consumers use it to their advantage by becoming a loan defaulter. Suppose you work for a consumer finance company which specializes in lending various types of loans to urban customers. You must use EDA to analyze the patterns present in the data. This will ensure that the applicants who are\u00a0capable of repaying the loan are not rejected.\n\n\u00a0When the company receives a loan application, the company has to decide for loan approval based on the applicant\u2019s profile. Two types of risks are associated with the bank\u2019s decision:\n\nIf the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\nIf the applicant is not likely to repay the loan, i.e. he\/she is likely to default, then approving the loan may lead to a financial loss for the company.\n\n\n**Business Objectives**\n\nThis case study aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicant's using EDA is the aim of this case study.\n","f8abc30e":"## **Insight:**\n\n**1. Applicants with Lower Credit-Income Ration tends to default high**\n**2. Applicants with Lower Period Employed tends to default high**\n**3. Applicants with Family memeber 4-8 has higher default Rate**","742d6bda":"#### From above we can observe that all the columns starting with Days has negative values Also, Gender & ORGANIZATION_TYPE Column has XNA Value which needs to be treated","c1a43eff":"## 1.5 Treating default Values:","678091af":"## Steps Followed on EDA Process!","8575ea1a":"#### 5. Treatment on Previous Application Data\n\n5.1 Let us look at null values in Previous Application","a6a79251":"#### Let us look at distribution of Occupation Type where Days Employed columne is filled with default value 365243","b68b1f9c":"5.4 Imputing Null values","50693843":"##### AMT_REQ_CREDIT_BUREAU_**** Columns seems to have outlier - extreme values, so we can go ahead with Missing value imputation using median.","af21d66c":"# Bank Loan Risk Analytics","d6b6b7b0":"##### Columns AMT_REQ_CREDIT_BUREAU**** has 13% of missing value. Let us plot and see the distribution.","315be5a1":"# Insight:\n\n**AMT_ANNUITY_RANGE: 30% of the Applicant got Annuity Range of 20-30Years & less than 0.5% has got anuuity year of 0-5Years which is very less**","0000d6f8":"## Insight:\n\n\n**1. Name Contact Type: 90% of Applicant belongs to Cash loans & out of all Defaulters 93% of the applicans belongs to Cash Loan itself.**\n\n**2. Gender: Female Applicants are Higher - 66%. Even though Female applicants are higher than male still ratio of defaulters are higher from male.**\n\n**3. Car Owner: Around 66% of Applicants do not own cars & higher defaulter rate too.**\n\n**5. Income Type: 51% of the applicants are belongs to Working Class and Pepole from Working class has the higher default rate. around 61% of applicants from defaulter list are working class & Businessman who never default.**\n\n**6. Education Type: 71% of the applicants are belongs to Secondary and they have higher default rate.**\n\n**7. Family Status: 64% of applicants belongs to married status and follwed by Single & Civil Marraige but Default Rate is higher for Single & Civil Marraige Applicants.**\n\n**8. Region Rating: 73% of applicants belongs to rating 2 status and follwed by 3 & 1 but Default Rate is higher by 15% for Rating 3 .**\n\n**9. Annuity Range: Applicants belongs to Annuity Range 20-40 Years tends to default at higher Rate.**","9755c9fc":"## 1.2 Loading Source Data","690b4f31":"Since \"Name_TYPE_SUITE\" - is nothing but who was accompanying client, In this case we can assume that if client havn't mentioned about Nae_Type_Suite that means clients visited - Unaccompanied.\n##### Hence we can go ahead with mode imputation","25ae7198":"#### Plotting Organization Type from Defaulter DataFrame","e3772ddb":"\n## 1. Import Python Libraries","b57e1e43":"Let us create a variable which shows ratio of Income and Credit of each Applicant which can be used further for Analysis.","65fe2e87":"3.4 Creating Corelation table","defa28b2":"##### Since only 4 records represent Gender as XNA \" Missing\" we can either drop these records","0d99d8ad":"# Insight:\n**1. Income Range: Around 50% of the Applicant has Total Income Range 100-200K. Followed by 200-300k & 0-100K of 21% & 20% respectively.\nIt is Understood that, pepole with very lower income range of 0 to 200K tend to apply for Loan the most.**","a857e231":"## 4. Mulivariate Ananlysis:","ee3bde61":"3.2 Creating a function to plot Categorical Columns for Univariate Analysis:","dd30424e":"1. Data Sourcing \n2. Data Understanding \n4. Checking Data quality\n    * Treatment on Missing Values\n    * Treatment on Extreme Values\n    * Treatment on default Values\n    * Dropping unwanted columns\n4. Data Binning on Numerical Columns\n5. Data Validation \n    * Imbalanced Data Analysis\n    * Univariate Analysis\n    * Multivariate Analysis\n6. Detecting correlation is any\n7. Check and correct Data Quality in Previous Application dataset\n8. Merging Current Application and Previous Application dataset\n9. Data Analysis\n10.Inferences, Recommendations & Risk","fbb1d256":"## Applicant will Not Repay if:-\n____________________________________________________________________________________________________________\n\n> **1. CODE_GENDER: Men tend to default higher than Women**\n\n> **2. NAME_FAMILY_STATUS : Most defaulters are civil marriage or single.**\n\n> **3. NAME_EDUCATION_TYPE: Most defaulters has Lower Secondary & Secondary education**\n\n> **4. REGION_RATING_CLIENT: People who live in Rating 3 Region has highest default Rate.**\n\n> **5. OCCUPATION_TYPE: Avoid Low-skill Laborers, Drivers Security staff default rate is higher.**\n\n> **6. ORGANIZATION_TYPE: Business Type 3 & Self-employed people have relative high defaulting rate.**\n\n> **7. DAYS_EMPLOYED: People who have less than 5 years of employment have high default rate.**\n\n> **8. Higher the Income-Credit Ratio lower the Default Rate.**","5d15d7e5":"## Applicant will Repay if:-\n______________________________________________________________________________________________________________\n\n> **1. NAME_EDUCATION_TYPE: Academic degree default lesser**\n\n> **2. NAME_INCOME_TYPE: Student and Businessmen do not defaults.**\n\n> **3. ORGANIZATION_TYPE: Clients with Trade Type 4 and 5 and Industry type 8 have defaulted less than 3%**\n\n> **4. DAYS_EMPLOYED: Clients with 40+ year experience default lesser.**\n\n> **5. NAME_CASH_LOAN_PURPOSE: Loans bought for Buying a new car & Education are being repayed mostly.**","3488052e":"1. As we can see the median value for positive Target value is 0.4 and for negative Target variable is 0.55.\n2. I don't want my Missing value imputation baised against any of our Target Variable.\n3. Let us impute using mean Imputation.","aba61357":"1.3.1 We have many columns which has null values more than 45%, I will go ahead and remove the columns having more than 45% of null values"}}