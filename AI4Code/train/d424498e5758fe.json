{"cell_type":{"4754638f":"code","e66ba152":"code","86bb2325":"code","f5396c1d":"code","9d5c59fb":"code","d73592e7":"code","3a1aae62":"code","13ae3b2f":"code","4e545228":"code","2ffa710d":"code","2ea93301":"code","10536ff1":"code","5982926c":"code","5d5dc271":"code","4e2ce527":"code","4e987dc3":"code","90123607":"code","b0eb7ee3":"markdown","7835d39a":"markdown","fa36028c":"markdown","6903b219":"markdown","c4f33bc6":"markdown","8c19ba3e":"markdown","1e0dc072":"markdown","04608a9c":"markdown","116cbc0f":"markdown","fd83d96c":"markdown","ca13ba63":"markdown","24b6d906":"markdown","065b26e8":"markdown","811dd567":"markdown","03f75b7b":"markdown","4727c81d":"markdown","fcf3cd84":"markdown"},"source":{"4754638f":"#!pip install transformers #if you are using google colab ","e66ba152":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","86bb2325":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os \nimport torch\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig # model\nfrom transformers import BertTokenizer # tokenizer\nfrom keras.preprocessing.sequence import pad_sequences # add padding\nfrom sklearn.model_selection import train_test_split # split dataset for train and test\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler # create data batches\nfrom transformers import get_linear_schedule_with_warmup # schedule for training BERT (updating weights etc)\nimport time\nimport datetime\nimport random\n\nos.chdir('..\/input\/netflix-shows')\ndf = pd.read_csv('netflix_titles.csv')\n\n# training parameters:\nbatch_size = 32\nepochs = 6\n# optimizer:\nlearning_rate = 2e-5\nepsilon = 1e-8","f5396c1d":"# creating column that tells if movie is for adults or no\n\ndf.insert(2, \"for_adult\", 0) \ndf.loc[(df.rating=='TV-MA') | (df.rating=='R'),'for_adult'] = 1\ndf.head()\n","9d5c59fb":"# taking values\ndescriptions = df.description.values\ndf.drop('description', axis=1, inplace=True)\nlabels = df.for_adult.values\n","d73592e7":"# load tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n# load model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification. \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = True, # Whether the model returns all hidden-states. We will need embeddings later.\n)\n\nmodel.cuda() # run on gpu\n\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","3a1aae62":"# need to format inputs (decriptions)\n#  1.Add additional needed tokens\ninput_ids = []\nfor description in descriptions:\n    encoded_description = tokenizer.encode(\n                        description,                      \n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                   )\n    input_ids.append(encoded_description)\n\n#  2.every input must be the same length, but descriptions are different so me must add padding (adding token id0 to shorter inputs)\n\nMAX_LEN = max([len(desc) for desc in input_ids])\n\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n                          value=0, truncating=\"post\", padding=\"post\")\n\n#  3. Creating attention masks\nattention_masks = []\nfor desc in input_ids:\n    # Create the attention mask.\n    #   - If a token ID is 0, then it's padding, set the mask to 0.\n    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n    att_mask = [int(token_id > 0) for token_id in desc]\n    attention_masks.append(att_mask)\n\n#  4. Split dataset (masks and inputs must match each other)\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=44, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=44, test_size=0.1)\n\n# Converting inputs and outputs into pyTorch tensors (becouse Bert is implemented in pyTorch)\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\n\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\n\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","13ae3b2f":"# optimizer\noptimizer = AdamW(model.parameters(),lr = learning_rate, eps = epsilon )\n# total steps of training is epoch * batch size\ntotal_steps = len(train_dataloader) * epochs\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n","4e545228":"# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","2ffa710d":"import matplotlib.pyplot as plt\n#% matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid')\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\nplt.plot(loss_values, 'b-o')\nplt.title(\"Training loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.show()","2ea93301":"!pip install faiss # k-nn  to calculate semantic similarity (FACEBOOK AI SIMILARITY SEARCH)\n!pip install faiss-gpu\ndf = pd.read_csv('netflix_titles.csv')\nimport faiss # sentence similarity\nimport plotly.express as px #plots","10536ff1":"def time_elapsed(sec):\n  h = int(sec\/3600)\n  m = int(sec\/60)\n  s = sec % 60\n  return \"{}:{:>02}:{:>05.2f}\".format(h,m,s)\n\ndef description_embedding(tokenizer,model,description):\n  MAX_LEN = 128\n  # need to format inputs (decriptions)\n  #  1.Add additional needed tokens\n  input_ids = tokenizer.encode(\n                        description,                      \n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                   )\n  #  2.every input must be the same length, but descriptions are different so me must add padding (adding token id0 to shorter inputs)\n  results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n                            truncating=\"post\", padding=\"post\")\n  input_ids = results[0] #pad sequences requires list of a lists, co to take only one list we make this\n  #  3. Creating attention masks\n  attention_masks = [int(token_id > 0) for token_id in input_ids]\n  # 4. Create torch tensor and add dimension for number of batches (1)\n  input_ids = torch.tensor(input_ids)\n  attention_masks = torch.tensor(attention_masks)\n  input_ids = input_ids.unsqueeze(0)\n  attention_masks = attention_masks.unsqueeze(0)\n  # put model in eval mode\n  model.eval()\n  # copy inputs to GPU\n  input_ids = input_ids.to(device)\n  attention_masks = attention_masks.to(device)\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n  with torch.no_grad():        \n    logits, encoded_layers = model(input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=attention_masks)\n    \n  layer_nr = 12 #last layer\n  batch_nr = 0 # nr of inputs in batch 0 = 1\n  token_nr = 0 # nr of first token\n  emb_vec = encoded_layers[layer_nr][batch_nr][token_nr]\n  # Move to cpu\n  result = emb_vec.detach().cpu().numpy()\n  return result","5982926c":"descriptions = df.description.values\nstart_time = time.time()\nembeddings = []\n# embedding in loop\nfor i, desc in enumerate(descriptions):\n  embedding = description_embedding(tokenizer,model,desc)\n  embeddings.append(embedding)\n  if ((i % 1000 ==0) and (i>0) or (i == len(descriptions)-1)):\n    elapsed_time = time_elapsed(time.time()-start_time)\n    print(f'Progress: {round(i\/len(descriptions)*100,2)}%')\n    print(f'Time elapsed: {elapsed_time}')\nembeddings = np.array(embeddings)\n","5d5dc271":"# k-NN to calculate simillarity\n# initialize of FAISS\ncpu_index = faiss.IndexFlatL2(embeddings.shape[1]) #embeddings.shape[1] is number of features in feature vector\nco = faiss.GpuMultipleClonerOptions()\nco.shard = True\ngpu_index = faiss.index_cpu_to_all_gpus(cpu_index, co=co,ngpu=1)\ngpu_index.add(embeddings) # adding dataset\n","4e2ce527":"# function to calculate recomendations\nindices = pd.Series(df.index, index=df['title']).drop_duplicates()\ndef get_recomendations(title):\n  idx = indices[title]\n  distances, movies = gpu_index.search(embeddings[idx].reshape(1,768),k=11)\n  movie_titles = []\n  for i in range(movies.shape[1]-1):\n    movie_titles.append(df.iloc[movies[0,i+1]].title)\n  likehood = (1-distances[0,1:]\/(max(max(distances)))*0.9)\n  likehood = likehood\/max(likehood)\n  return movie_titles,likehood","4e987dc3":"movie_you_watched = 'Transformers Prime'\nrecomendations, likehood = get_recomendations(movie_you_watched)","90123607":"# creating plot with recomendations\ndf_temp = pd.DataFrame(columns = df.columns)\ntemp_tittle=[]\nfor i,t in enumerate(recomendations):\n  df_temp = df_temp.append(df[(df.title==t)])\n  temp_tittle.append(movie_you_watched)\n\nplot_title = 'Recomendations after watching ' + movie_you_watched \nfig = px.treemap(\n    data_frame = df_temp,\n    names = df_temp['title'],\n    values = likehood,\n    parents = temp_tittle,\n    hover_name = df_temp['title'],\n    hover_data=['director','release_year','rating','country','cast'],\n    title = plot_title\n)\nfig.show()","b0eb7ee3":"# 2.4. Function to get recomendations","7835d39a":"# 1.1. Read Data and import libraries","fa36028c":"# 3.1. Plot with recomendations","6903b219":"# 1.2. Load model and tokenizer","c4f33bc6":"# 1.0 Fine-Tuning","8c19ba3e":"First we Fine-Tune Bert model to gets better embeddings. Then we get embeddings of every descriptions and finally we get similarity of embeddings using FAISS (Facebook AI similarity search).\nThis is simple recomendations engine to help begginers to start using BERT (and BERT is one of the most powerfull tool in NLP at the moment) for some tasks.\n\nCredits to:\nhttps:\/\/wandb.ai\/cayush\/bert-finetuning\/reports\/Sentence-Classification-With-Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA\n","1e0dc072":"# 2.2. Define function to get embeddings","04608a9c":"# 2.3. Initialize FAISS","116cbc0f":"# Using BERT and FAISS for smimilarity search in descriptions as recomendations engine","fd83d96c":"# 2.1. Install and import libraries","ca13ba63":"# 1.3. Format inputs to match BERT expectations.","24b6d906":"# 1.4. Define Optimizer, scheduler and acc function.","065b26e8":"# 1.7. Loss training PLOT","811dd567":"# 2.0 Creating Recomendations engine","03f75b7b":"# 1.6. Training of BERT","4727c81d":"Task for Fine-Tuning is very simple. Basing on descriptions, BERT must decide whether movie is for adult only (1) or no (0).","fcf3cd84":"# 3.0. Choose your movie HERE!"}}