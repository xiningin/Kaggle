{"cell_type":{"23b5b1fe":"code","0e751192":"code","e28dcfc4":"code","ddc6489f":"code","62d846aa":"code","554f0e5e":"code","e3dfd440":"code","379ae056":"code","7fdd8002":"code","2d855f33":"code","c7123117":"code","ac4731ba":"code","d2b0ad62":"code","3ac7b994":"code","f6fbf679":"code","1bdd722a":"code","346e56c8":"code","6342b136":"code","02ac5dda":"code","10146f06":"code","abfa22f0":"code","8da30576":"code","906d2160":"code","40bd6390":"code","02f45822":"code","f2cff898":"code","1a5e0256":"code","2f656a82":"code","2bf178b0":"code","b0894cbc":"code","433ae262":"code","55d889fa":"code","cf24ba74":"code","3b9d5c67":"code","ecfb6c68":"code","ea586f3f":"code","fdf1914c":"code","0cacff8d":"code","b58214cf":"code","feb689a0":"code","e4781d01":"code","cee1a25b":"code","7daff9e8":"code","d302b212":"code","f273bba4":"code","ad9c751e":"code","840d712f":"code","2f6dacab":"code","3b86909b":"code","cf1f1c7e":"code","626b58d0":"code","f55ca183":"code","bbfd7d28":"code","3cb20bf9":"code","a6cdc46a":"code","1d627ddc":"code","eb176ffa":"code","89480439":"code","26d570df":"code","3e7cba6a":"code","4d23db8c":"code","2ba22cab":"code","98fb4f69":"code","3ed4e6b0":"code","7ff59ebf":"code","0416b4ea":"code","e3d32784":"code","83d60fbc":"code","0e4a2c08":"code","629a9646":"code","61ebf04a":"code","737c88bd":"code","beb0082f":"code","518f53fc":"code","d2c1d340":"code","b2778ea3":"code","3531bc47":"code","b5e91ce8":"code","06e25581":"code","b644c279":"code","8252435e":"code","21542b19":"code","08893384":"code","8427808d":"code","6825947d":"code","2beeb1bd":"code","bf37856a":"code","7cc607e5":"code","012c9edb":"code","694eb401":"code","4b921228":"code","30f3663f":"code","a0c2b575":"code","78ec256f":"code","4b5038a0":"code","2a977755":"code","a3f8a388":"code","9ea3a477":"code","96dd062f":"code","96d310d7":"code","19aeab14":"code","f568336f":"code","4ca0a7e6":"code","17bb8c3c":"code","0ada4e41":"code","1bcdf4f7":"code","13adb2b3":"code","6808f635":"code","182b411d":"code","735ef1e2":"code","ccbf6307":"code","4e9fc2ae":"code","61fd32a6":"code","0d0d2334":"code","035ac3a3":"code","62e934b5":"code","b3573c02":"code","2cf04b16":"code","da5a3775":"code","57a48c25":"code","b7abca65":"code","d5af8783":"code","cf3c16dc":"code","6bb40f32":"code","1006a595":"code","d6247eff":"code","f1d151db":"code","8381cd1e":"code","5ec19d78":"code","aa59f0c3":"code","c8bbda3c":"code","217eaa30":"code","a62b8dfc":"code","e10a4858":"code","d27b099c":"code","ee982f57":"code","f7909a84":"code","7a9f4ce8":"code","082cbce7":"code","f5d3c199":"code","b02f5fe2":"code","0d325896":"code","b56fa503":"code","3d803bcc":"code","81e9f002":"code","78384a61":"code","b4e088e2":"code","63fad50e":"code","e59a1980":"code","66bb4470":"code","7ef3a058":"code","4ed8a02f":"code","9503ed38":"code","63ce1eea":"code","067363ac":"code","9dc96ad6":"code","6a2c364c":"code","280ab6ab":"code","09df182e":"code","c3f3848b":"code","6d93c664":"code","f7ca8953":"code","df13ecdb":"code","866492a0":"code","164a7ed8":"code","aa099f19":"code","ee9b8e7e":"code","3f981b19":"code","208d99b8":"code","c474142d":"code","f9b5840b":"code","5dfad9e1":"code","ff0e1c01":"code","cfaf76c1":"code","466e16ef":"code","65c45dc4":"code","2016da23":"code","2681b830":"code","f6e7f925":"code","a0e10d3c":"code","9d7dd28a":"code","21555a08":"code","ea7e90db":"code","e4c7b962":"code","756a5fea":"code","c4d3ca18":"code","2e51f13b":"code","d8c63344":"code","1e8810de":"code","98fedb79":"code","02e7ae48":"code","944b9828":"code","4af75657":"code","7439252a":"code","7e2a169e":"code","ddee242e":"code","0a221a9a":"code","2f9312e5":"markdown","d7d74ec2":"markdown","5e3f5bfd":"markdown","069edee7":"markdown","3f766c33":"markdown","f45d6e3e":"markdown","e34a78f5":"markdown","e12169ac":"markdown","334e7717":"markdown","b8b1be32":"markdown","b319a68e":"markdown","3fe9b090":"markdown","ecd5126c":"markdown","ebd81042":"markdown","8381646d":"markdown","40d7b1b7":"markdown","ba4cf558":"markdown","2214b235":"markdown","b1fc1870":"markdown","b4ed29fd":"markdown","7691ac21":"markdown","f6b11305":"markdown","b98eb428":"markdown","a8292fd3":"markdown","dbc8f1a9":"markdown","d1ad99bb":"markdown","eff80573":"markdown","effc9d35":"markdown","d350584d":"markdown","adea02c1":"markdown","609d9f0c":"markdown","3bee54e9":"markdown","b80a4567":"markdown","ed4189cb":"markdown","b859beb6":"markdown","209cf025":"markdown","bba1adb4":"markdown","43680627":"markdown","b1bfdfd7":"markdown","e0a11b22":"markdown","6dbb185c":"markdown","62e9f930":"markdown","9ea12270":"markdown","c3e3de9d":"markdown","c9fc1613":"markdown","8f9f58bc":"markdown","a7b39557":"markdown","6d0fcdd3":"markdown","399a9aeb":"markdown","9cb36e92":"markdown","41a6fcbf":"markdown","4fc67362":"markdown","7cda7ce6":"markdown","8c20a193":"markdown","1cbbb240":"markdown","3e476985":"markdown","988008e7":"markdown","2372ffa7":"markdown","f55f7f78":"markdown","b6c89e6a":"markdown","56a06753":"markdown","f68e57de":"markdown","582441f4":"markdown","d21aef17":"markdown","10a5793a":"markdown","1e313e3f":"markdown","cb85f3b7":"markdown","cd733124":"markdown","66792199":"markdown","a5de8f29":"markdown","6ac00092":"markdown","b7503946":"markdown","9b463099":"markdown","627e5932":"markdown","f4d705db":"markdown","29d528d9":"markdown","c04787ac":"markdown","76887c21":"markdown","5512e21f":"markdown","7cc5ced9":"markdown","ba32f8a4":"markdown","cb9ea0a8":"markdown","32a3f395":"markdown","823b5359":"markdown","10e23248":"markdown","983a9801":"markdown","a8ca816d":"markdown","a06123b2":"markdown","33ddc60b":"markdown","5aeeea8c":"markdown","239d3ae7":"markdown","65aa9e86":"markdown","c38ddcc3":"markdown","946c4bf4":"markdown","2dd52234":"markdown","00d32d7e":"markdown","c164ba40":"markdown","7bb2f26b":"markdown","22c88c3c":"markdown","19b87877":"markdown","b23724dd":"markdown","5193eb95":"markdown","5f75403e":"markdown","570f9a85":"markdown","fa89fe22":"markdown","3aa6a81f":"markdown","3979766e":"markdown","66dc1742":"markdown","552518af":"markdown","39440c8b":"markdown","50b4c765":"markdown","37235b29":"markdown","9942bb3c":"markdown","7e2d30d7":"markdown","6b1731db":"markdown","9b7319fd":"markdown","0fbedb09":"markdown","4386df0e":"markdown"},"source":{"23b5b1fe":"# Importing computational packages\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", None)\npd.set_option('float_format', '{:f}'.format)","0e751192":"# Importing visualization packages\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","e28dcfc4":"# Importing model building packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import  RobustScaler\nfrom sklearn.preprocessing import PowerTransformer","ddc6489f":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","62d846aa":"from sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import f1_score, classification_report","554f0e5e":"import warnings\nwarnings.filterwarnings(\"ignore\")","e3dfd440":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","379ae056":"df.shape","7fdd8002":"df.describe()","2d855f33":"#observe the different feature type present in the data\ndf.dtypes","c7123117":"df.info()","ac4731ba":"# Checking for the missing value present in each columns\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","d2b0ad62":"classes=df['Class'].value_counts()\nnormal_share=round(classes[0]\/df['Class'].count()*100,2)\nfraud_share=round(classes[1]\/df['Class'].count()*100, 2)\nprint(\"Non-Fraudulent : {} %\".format(normal_share))\nprint(\"    Fraudulent : {} %\".format(fraud_share))","3ac7b994":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\nplt.figure(figsize=(20,6))\n\nplt.subplot(1,2,1)\nax=sns.countplot(df[\"Class\"])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.ylabel(\"Number of transaction\")\nplt.xlabel(\"Class\")\nplt.title(\"Credit Card Fraud Class - data unbalance\")\nplt.grid()\nplt.subplot(1,2,2)\nfraud_percentage = {'Class':['Non-Fraudulent', 'Fraudulent'], 'Percentage':[normal_share, fraud_share]} \ndf_fraud_percentage = pd.DataFrame(fraud_percentage) \nax=sns.barplot(x='Class',y='Percentage', data=df_fraud_percentage)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.title('Percentage of fraudulent vs non-fraudulent transcations')\n\nplt.grid()","f6fbf679":"# Create a scatter plot to observe the distribution of classes with time\nsns.scatterplot( df[\"Class\"],df[\"Time\"],hue=df[\"Class\"])\nplt.title(\"Time vs Class scatter plot\")\nplt.grid()","1bdd722a":"# Create a scatter plot to observe the distribution of classes with Amount\nsns.scatterplot(df[\"Class\"],df[\"Amount\"],hue=df[\"Class\"])\nplt.title(\"Amount vs Class scatter plot\")\nplt.grid()","346e56c8":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,10))\nsns.heatmap(df.corr(),cmap='rainbow')\nplt.show()","6342b136":"# Plotting all the variable in displot to visualise the distribution\nvar = list(df.columns.values)\n# dropping Class columns from the list\nvar.remove(\"Class\")\n\ni = 0\nt0 = df.loc[df['Class'] == 0]\nt1 = df.loc[df['Class'] == 1]\n\nplt.figure()\nfig, ax = plt.subplots(10,3,figsize=(30,45));\n\nfor feature in var:\n    i += 1\n    plt.subplot(10,3,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.grid()\nplt.show()","02ac5dda":"# Drop unnecessary columns\ndf = df.drop(\"Time\", axis = 1)\ndf.head()","10146f06":"y= df[\"Class\"]\nX = df.drop(\"Class\", axis = 1)\ny.shape,X.shape","abfa22f0":"# Spltting the into 80:20 train test size\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42,stratify=y)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","8da30576":"# Checking the split of the class label\nprint(\" Fraudulent Count for Full data : \",np.sum(y))\nprint(\"Fraudulent Count for Train data : \",np.sum(y_train))\nprint(\" Fraudulent Count for Test data : \",np.sum(y_test))","906d2160":"# As PCA is already performed on the dataset from V1 to V28 features, we are scaling only Amount field\nscaler = RobustScaler()\n\n# Scaling the train data\nX_train[[\"Amount\"]] = scaler.fit_transform(X_train[[\"Amount\"]])\n\n# Transforming the test data\nX_test[[\"Amount\"]] = scaler.transform(X_test[[\"Amount\"]])","40bd6390":"X_train.head()","02f45822":"X_test.head()","f2cff898":"# plot the histogram of a variable from the dataset to see the skewness\nvar = X_train.columns\n\nplt.figure(figsize=(30,45))\ni=0\nfor col in var:\n    i += 1\n    plt.subplot(10,3, i)\n    sns.distplot(X_train[col])\n    plt.grid()\n\nplt.show()","1a5e0256":"# Lets check the skewness of the features\nvar = X_train.columns\nskew_list = []\nfor i in var:\n    skew_list.append(X_train[i].skew())\n\ntmp = pd.concat([pd.DataFrame(var, columns=[\"Features\"]), pd.DataFrame(skew_list, columns=[\"Skewness\"])], axis=1)\ntmp.set_index(\"Features\", inplace=True)\ntmp","2f656a82":"# Filtering the features which has skewness less than -1 and greater than +1\nskewed = tmp.loc[(tmp[\"Skewness\"] > 1) | (tmp[\"Skewness\"] <-1 )].index\nskewed.tolist()","2bf178b0":"# preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\npt = PowerTransformer()\n\n# Fitting the power transformer in train data\nX_train[skewed] = pt.fit_transform(X_train[skewed])\n\n\n# Transforming the test data\nX_test[skewed] = pt.transform(X_test[skewed])","b0894cbc":"# plot the histogram of a variable from the dataset again to see the result \nvar = X_train.columns\n\nplt.figure(figsize=(30,45))\ni=0\nfor col in var:\n    i += 1\n    plt.subplot(10,3, i)\n    sns.distplot(X_train[col])\n    plt.grid()\n\nplt.show()","433ae262":"# Class imbalance\ny_train.value_counts()\/y_train.shape","55d889fa":"# Logistic Regression parameters for K-fold cross vaidation\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n#perform cross validation\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        n_jobs=-1,\n                        verbose = 1,\n                        return_train_score=True) \n#perform hyperparameter tuning\nmodel_cv.fit(X_train, y_train)\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)","cf24ba74":"#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)","3b9d5c67":"# cross validation results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","ecfb6c68":"# plot of C versus train and validation scores\nplt.figure(figsize=(16, 8))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('ROC AUC')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')\nplt.grid()","ea586f3f":" model_cv.best_params_","fdf1914c":"# Instantiating the model with best C\nlog_reg_imb_model = model_cv.best_estimator_\n\n# Fitting the model on train dataset\nlog_reg_imb_model.fit(X_train, y_train)","0cacff8d":"# Creating function to display ROC-AUC score, f1 score and classification report\ndef display_scores(y_test, y_pred):\n    '''\n    Display ROC-AUC score, f1 score and classification report of a model.\n    '''\n    print(f\"F1 Score: {round(f1_score(y_test, y_pred)*100,2)}%\") \n    print(\"\\n\\n\")\n    print(f\"Classification Report: \\n {classification_report(y_test, y_pred)}\")","b58214cf":"# Predictions on the train set\ny_train_pred = log_reg_imb_model.predict(X_train)","feb689a0":"display_scores(y_train, y_train_pred)","e4781d01":"# ROC Curve function\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","cee1a25b":"# Predicted probability\ny_train_pred_proba = log_reg_imb_model.predict_proba(X_train)[:,1]","7daff9e8":"# Plot the ROC curve\ndraw_roc(y_train, y_train_pred_proba)","d302b212":"# Making prediction on the test set\ny_test_pred = log_reg_imb_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","f273bba4":"# Predicted probability\ny_test_pred_proba = log_reg_imb_model.predict_proba(X_test)[:,1]","ad9c751e":"# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","840d712f":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(estimator = dtree, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","2f6dacab":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results.head()","3b86909b":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","cf1f1c7e":"print(grid_search.best_estimator_)","626b58d0":"# Model with optimal hyperparameters\ndt_imb_model = grid_search.best_estimator_\n\ndt_imb_model.fit(X_train, y_train)","f55ca183":"y_train_pred = dt_imb_model.predict(X_train)\ndisplay_scores(y_train, y_train_pred)","bbfd7d28":"# Predicted probability\ny_train_pred_proba = dt_imb_model.predict_proba(X_train)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_train, y_train_pred_proba)","3cb20bf9":"y_test_pred = dt_imb_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","a6cdc46a":"# Predicted probability\ny_test_pred_proba = dt_imb_model.predict_proba(X_test)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","1d627ddc":"from sklearn.ensemble import RandomForestClassifier\n# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\nrf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","eb176ffa":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results.head()","89480439":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","26d570df":"print(grid_search.best_estimator_)","3e7cba6a":"# Model with optimal hyperparameters\nrf_imb_model = grid_search.best_estimator_\n\nrf_imb_model.fit(X_train, y_train)","4d23db8c":"y_train_pred = rf_imb_model.predict(X_train)\ndisplay_scores(y_train, y_train_pred)","2ba22cab":"# Predicted probability\ny_train_pred_proba = rf_imb_model.predict_proba(X_train)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_train, y_train_pred_proba)","98fb4f69":"y_test_pred = rf_imb_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","3ed4e6b0":"# Predicted probability\ny_test_pred_proba = rf_imb_model.predict_proba(X_test)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","7ff59ebf":"# creating a KFold object \nfolds = 5\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        n_jobs=-1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)","0416b4ea":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","e3d32784":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", model_cv.best_score_)","83d60fbc":"print(model_cv.best_estimator_)","0e4a2c08":"# Printing best params\nmodel_cv.best_params_","629a9646":"# fit model on training data\nxgb_imb_model = model_cv.best_estimator_\nxgb_imb_model.fit(X_train, y_train)","61ebf04a":"# Predictions on the train set\ny_train_pred = xgb_imb_model.predict(X_train)\n\ndisplay_scores(y_train, y_train_pred)","737c88bd":"# Predicted probability\ny_train_pred_proba_imb_xgb = xgb_imb_model.predict_proba(X_train)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_train, y_train_pred_proba_imb_xgb)","beb0082f":"# Predictions on the test set\ny_test_pred = xgb_imb_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","518f53fc":"# Predicted probability\ny_test_pred_proba = xgb_imb_model.predict_proba(X_test)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","d2c1d340":"var_imp = []\nfor i in xgb_imb_model.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-3])+1)","b2778ea3":"# Variable on Index-17 and Index-14 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 10]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()\nplt.grid()","3531bc47":"print('Train auc =', metrics.roc_auc_score(y_train, y_train_pred_proba_imb_xgb))","b5e91ce8":"fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_pred_proba_imb_xgb)\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint(\"Threshold=\",threshold)","06e25581":"from imblearn.over_sampling import RandomOverSampler\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n#fit and apply the transform\nX_over, y_over = oversample.fit_resample(X_train, y_train)\nX_over.shape, y_over.shape","b644c279":"from collections import Counter\n# Befor sampling class distribution\nprint('Before sampling class distribution:-',Counter(y_train))","8252435e":"# new class distribution \nprint('New class distribution:-',Counter(y_over))","21542b19":"# Logistic Regression parameters for K-fold cross vaidation\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n#perform cross validation\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        n_jobs=-1,\n                        verbose = 1,\n                        return_train_score=True) \n#perform hyperparameter tuning\nmodel_cv.fit(X_over, y_over)\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)","08893384":"#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)","8427808d":"# cross validation results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","6825947d":"# plot of C versus train and validation scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('ROC AUC')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')\nplt.grid()","2beeb1bd":"model_cv.best_params_","bf37856a":"# Instantiating the model\nlogreg_over = LogisticRegression(C=1000)\n\n# Fitting the model with train data\nlogreg_over_model = logreg_over.fit(X_over, y_over)","7cc607e5":"# Predictions on the train set\ny_train_pred = logreg_over_model.predict(X_over)","012c9edb":"# Printing scores\ndisplay_scores(y_over, y_train_pred)","694eb401":"# Predicted probability\ny_train_pred_proba = logreg_over_model.predict_proba(X_over)[:,1]\n# Plot the ROC curve\ndraw_roc(y_over, y_train_pred_proba)","4b921228":"# Evaluating on test data\ny_test_pred = logreg_over_model.predict(X_test)\n\n# Printing the scores\ndisplay_scores(y_test, y_test_pred)","30f3663f":"# Predicted probability\ny_test_pred_proba = logreg_over_model.predict_proba(X_test)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","a0c2b575":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(estimator = dtree, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_over,y_over)","78ec256f":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","4b5038a0":"print(grid_search.best_estimator_)","2a977755":"# Model with optimal hyperparameters\ndt_over_model = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 42,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\n\ndt_over_model.fit(X_over, y_over)","a3f8a388":"# Predictions on the train set\ny_train_pred = dt_over_model.predict(X_over)\ndisplay_scores(y_over, y_train_pred)","9ea3a477":"# Predicted probability\ny_train_pred_proba = dt_over_model.predict_proba(X_over)[:,1]\n# Plot the ROC curve\ndraw_roc(y_over, y_train_pred_proba)","96dd062f":"# Evaluating model on the test data\ny_test_pred = dt_over_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","96d310d7":"# Predicted probability\ny_test_pred_proba = dt_over_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","19aeab14":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\nrf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_over,y_over)","f568336f":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","4ca0a7e6":"# Model with optimal hyperparameters\nrf_over_model = grid_search.best_estimator_\n\nrf_over_model.fit(X_over, y_over)","17bb8c3c":"# Predictions on the train set\ny_train_pred = rf_over_model.predict(X_over)\ndisplay_scores(y_over, y_train_pred)","0ada4e41":"# Predicted probability\ny_train_pred_proba = rf_over_model.predict_proba(X_over)[:,1]\n# Plot the ROC curve\ndraw_roc(y_over, y_train_pred_proba)","1bcdf4f7":"# Evaluating model on the test data\ny_test_pred = rf_over_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","13adb2b3":"# Predicted probability\ny_test_pred_proba = rf_over_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","6808f635":"# creating a KFold object \nfolds = 5\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        n_jobs=-1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_over, y_over) ","182b411d":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","735ef1e2":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", model_cv.best_score_)","ccbf6307":"print(model_cv.best_estimator_)","4e9fc2ae":"model_cv.best_params_","61fd32a6":"# fit model on training data\nxgb_over_model = model_cv.best_estimator_\nxgb_over_model.fit(X_over, y_over)","0d0d2334":"# Predictions on the train set\ny_train_pred = xgb_over_model.predict(X_over)\n\ndisplay_scores(y_over, y_train_pred)","035ac3a3":"# Predicted probability\ny_train_pred_proba = xgb_over_model.predict_proba(X_over)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_over, y_train_pred_proba)","62e934b5":"y_pred = xgb_over_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","b3573c02":"# Predicted probability\ny_test_pred_proba = xgb_over_model.predict_proba(X_test)[:,1]\n\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","2cf04b16":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n# Artificial minority samples and corresponding minority labels from SMOTE are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from SMOTE, we do\nX_train_smote_1 = X_train_smote[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_smote_1.iloc[:X_train_1.shape[0], 0], X_train_smote_1.iloc[:X_train_1.shape[0], 1],\n            label='Artificial SMOTE Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()","da5a3775":"# Creating KFold object with 5 splits\nfolds = KFold(n_splits=5, shuffle=True, random_state=4)\n\n# Specify params\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n\n# Specifing score as roc-auc\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        n_jobs=-1,\n                        return_train_score=True) \n\n# Fit the model\nmodel_cv.fit(X_train_smote, y_train_smote)\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)\n","57a48c25":"#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)","b7abca65":"# cross validation results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","d5af8783":"# plot of C versus train and validation scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('ROC AUC')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')\nplt.grid()","cf3c16dc":"# Printing best params\nmodel_cv.best_params_","6bb40f32":"# Instantiating the model\nlogreg_smote_model = model_cv.best_estimator_\n\n# Fitting the model with balanced data\nlogreg_smote_model.fit(X_train_smote, y_train_smote)","1006a595":"# Evaluating on train data\ny_train_pred = logreg_smote_model.predict(X_train_smote)\ndisplay_scores(y_train_smote, y_train_pred)","d6247eff":"# Predicted probability\ny_train_pred_proba_smote = logreg_smote_model.predict_proba(X_train_smote)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_smote, y_train_pred_proba_smote)","f1d151db":"# Evaluating on test data\ny_test_pred = logreg_smote_model.predict(X_test)\ndisplay_scores(y_test, y_test_pred)","8381cd1e":"# Predicted probability\ny_test_pred_proba_smote = logreg_smote_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba_smote)","5ec19d78":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(estimator = dtree, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train_smote,y_train_smote)","aa59f0c3":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","c8bbda3c":"print(grid_search.best_estimator_)","217eaa30":"grid_search.best_params_","a62b8dfc":"# Model with optimal hyperparameters\ndt_smote_model = grid_search.best_estimator_\n\ndt_smote_model.fit(X_train_smote, y_train_smote)","e10a4858":"# Predictions on the train set\ny_train_pred_smote = dt_smote_model.predict(X_train_smote)\ndisplay_scores(y_train_smote, y_train_pred_smote)","d27b099c":"# Predicted probability\ny_train_pred_proba = dt_smote_model.predict_proba(X_train_smote)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_smote, y_train_pred_proba)","ee982f57":"# Evaluating model on the test data\ny_pred = dt_smote_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","f7909a84":"# Predicted probability\ny_test_pred_smote = dt_smote_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_smote)","7a9f4ce8":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\nmodel = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = model, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train_smote,y_train_smote)","082cbce7":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","f5d3c199":"print(grid_search.best_estimator_)","b02f5fe2":"grid_search.best_params_","0d325896":"# Model with optimal hyperparameters\nrf_smote_model = grid_search.best_estimator_\n\nrf_smote_model.fit(X_train_smote, y_train_smote)","b56fa503":"# Predictions on the train set\ny_train_pred_smote = rf_smote_model.predict(X_train_smote)\ndisplay_scores(y_train_smote, y_train_pred_smote)","3d803bcc":"# Predicted probability\ny_train_pred_proba = rf_smote_model.predict_proba(X_train_smote)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_smote, y_train_pred_proba)","81e9f002":"# Evaluating model on the test data\ny_pred = rf_smote_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","78384a61":"# Predicted probability\ny_test_pred_smote = rf_smote_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_smote)","b4e088e2":"# creating a KFold object \nfolds = 5\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        n_jobs=-1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train_smote, y_train_smote)","63fad50e":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","e59a1980":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", model_cv.best_score_)","66bb4470":"print(model_cv.best_estimator_)","7ef3a058":"model_cv.best_params_","4ed8a02f":"# fit model on training data\nxgb_smote_model = model_cv.best_estimator_\nxgb_smote_model.fit(X_train_smote, y_train_smote)","9503ed38":"y_train_pred = xgb_smote_model.predict(X_train_smote)\ndisplay_scores(y_train_smote, y_train_pred)","63ce1eea":"# Predicted probability\ny_train_pred_proba = xgb_smote_model.predict_proba(X_train_smote)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_smote, y_train_pred_proba)","067363ac":"y_pred = xgb_smote_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","9dc96ad6":"# Predicted probability\ny_test_pred_proba = xgb_smote_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","6a2c364c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn import over_sampling\n\nada = over_sampling.ADASYN(random_state=42)\nX_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n# Artificial minority samples and corresponding minority labels from ADASYN are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from ADASYN, we do\nX_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_adasyn_1.iloc[:X_train_1.shape[0], 0], X_train_adasyn_1.iloc[:X_train_1.shape[0], 1],\n            label='Artificial ADASYN Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()","280ab6ab":"# Creating KFold object with 5 splits\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Specify params\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n\n# Specifing score as roc-auc\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True) \n\n# Fit the model\nmodel_cv.fit(X_train_adasyn, y_train_adasyn)\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)","09df182e":"#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)","c3f3848b":"# cross validation results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","6d93c664":"# plot of C versus train and validation scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('ROC AUC')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')\nplt.grid()","f7ca8953":"model_cv.best_params_","df13ecdb":"# Instantiating the model\nlogreg_adasyn_model = model_cv.best_estimator_\n\n# Fitting the model \nlogreg_adasyn_model.fit(X_train_adasyn, y_train_adasyn)","866492a0":"# Evaluating on test data\ny_train_pred = logreg_adasyn_model.predict(X_train_adasyn)\ndisplay_scores(y_train_adasyn, y_train_pred)","164a7ed8":"# Predicted probability\ny_train_pred_proba = logreg_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_adasyn, y_train_pred_proba)","aa099f19":"# Evaluating on test data\ny_pred = logreg_adasyn_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","ee9b8e7e":"# Predicted probability\ny_test_pred_proba = logreg_adasyn_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","3f981b19":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(estimator = dtree, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train_adasyn,y_train_adasyn)","208d99b8":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","c474142d":"print(grid_search.best_estimator_)","f9b5840b":"# Model with optimal hyperparameters\ndt_adasyn_model =grid_search.best_estimator_\ndt_adasyn_model.fit(X_train_adasyn, y_train_adasyn)","5dfad9e1":"# Evaluating model on the test data\ny_train_pred = dt_adasyn_model.predict(X_train_adasyn)\ndisplay_scores(y_train_adasyn, y_train_pred)","ff0e1c01":"# Predicted probability\ny_train_pred_proba = dt_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_adasyn, y_train_pred_proba)","cfaf76c1":"# Evaluating model on the test data\ny_pred = dt_adasyn_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","466e16ef":"# Predicted probability\ny_test_pred_proba = dt_adasyn_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","65c45dc4":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\nmodel = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = model, \n                           param_grid = param_grid, \n                           scoring= 'roc_auc',\n                           cv = 5, \n                           n_jobs=-1,\n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train_adasyn,y_train_adasyn)","2016da23":"# Printing the optimal roc score and hyperparameters\nprint(\"Best roc auc score : \", grid_search.best_score_)","2681b830":"print(grid_search.best_estimator_)","f6e7f925":"# Model with optimal hyperparameters\nrf_adasyn_model =grid_search.best_estimator_\nrf_adasyn_model.fit(X_train_adasyn, y_train_adasyn)","a0e10d3c":"# Evaluating model on the test data\ny_train_pred = rf_adasyn_model.predict(X_train_adasyn)\ndisplay_scores(y_train_adasyn, y_train_pred)","9d7dd28a":"# Predicted probability\ny_train_pred_proba = rf_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_adasyn, y_train_pred_proba)","21555a08":"# Evaluating model on the test data\ny_pred = rf_adasyn_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","ea7e90db":"# Predicted probability\ny_test_pred_proba = rf_adasyn_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","e4c7b962":"# creating a KFold object \nfolds = 5\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        n_jobs=-1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train_adasyn, y_train_adasyn)","756a5fea":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","c4d3ca18":"# Printing the optimal score and hyperparameters\nprint(\"Best roc auc score : \", model_cv.best_score_)","2e51f13b":"print(model_cv.best_estimator_)","d8c63344":"model_cv.best_params_","1e8810de":"# Model with optimal hyperparameter\nxgb_adasyn_model = model_cv.best_estimator_\nxgb_adasyn_model.fit(X_train_adasyn,y_train_adasyn)","98fedb79":"# Predicting on the train set\ny_train_pred = xgb_adasyn_model.predict(X_train_adasyn)\n# Printing the scores\ndisplay_scores(y_train_adasyn, y_train_pred)","02e7ae48":"# Predicted probability\ny_train_pred_proba = xgb_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n# Plot the ROC curve\ndraw_roc(y_train_adasyn, y_train_pred_proba)","944b9828":"y_pred = xgb_adasyn_model.predict(X_test)\ndisplay_scores(y_test, y_pred)","4af75657":"# Predicted probability\ny_test_pred_proba = xgb_adasyn_model.predict_proba(X_test)[:,1]\n# Plot the ROC curve\ndraw_roc(y_test, y_test_pred_proba)","7439252a":"var_imp = []\nfor i in xgb_adasyn_model.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-3])+1)","7e2a169e":"# Variable on Index-14 and Index-4 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 10]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()\nplt.grid()","ddee242e":"print('Train auc =', metrics.roc_auc_score(y_train_adasyn, y_train_pred_proba))","0a221a9a":"fpr, tpr, thresholds = metrics.roc_curve(y_train_adasyn, y_train_pred_proba )\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint(threshold)","2f9312e5":"# Decision Tree with Random Oversampling","d7d74ec2":"**Observation**\n\nThere is not much insight can be drawn from the distribution of the fraudulent transaction based on time as fraudulent\/non-fraudulent both transaction are distributed over time.","5e3f5bfd":"#### Model Summary\n\n- Train set\n    - ROC Score: 94%\n    - F1 score : 68.32%\n    \n    \n- Test set\n    - ROC Score: 94%\n    - F1 score : 61.73%","069edee7":"#### Model Summary\n- Train set\n    - ROC score : 99%\n    - F1 score: 94.8%\n    \n    \n- Test set\n    - ROC score : 97%\n    - F1 score: 9.67%","3f766c33":"#### Model Summary\n- Train set\n    - ROC score : 100%\n    - F1 score: 98.93%\n- Test set\n    - ROC score : 95%\n    - F1 score: 17.37%","f45d6e3e":"#### Evaluating the model on the test set","e34a78f5":"#### Evaluating the model on the test set","e12169ac":"### Feature Scaling using  RobustScaler Scaler","334e7717":"Dropping `Time` column as this feature is not going to help in the model building.\n#### Understanding from Core Banking Business ","b8b1be32":"#### Model evaluation on train set","b319a68e":"**Observation**\n\nThe dataset has very high class imbalance. Only 492 records are there among 284807 records which are labeld as fradudulent transaction.","3fe9b090":"### Print the class distribution after applying ADASYN","ecd5126c":"#### Plotting the distributions of all the features","ebd81042":"#### XGBoost model with optimal hyperparameter","8381646d":"### Observe the distribution of our classes","40d7b1b7":"#### Predictions on the test set","ba4cf558":"#### Model Summary\n- Train set\n    - ROC score : 100.0%\n    - F1 score: 99.79%\n- Test set\n    - ROC score : 98%\n    - F1 score: 35.22%","2214b235":"#### Model with optimal hyperparameters","b1fc1870":"There are no features which there is high correlatation , corr > .75","b4ed29fd":"#### Evaluating the model on train data","7691ac21":"### Print the important features of the best model to understand the dataset","f6b11305":"# Logistic Regression on balanced data with SMOTE","b98eb428":"### Handling Missing Values","a8292fd3":"# Logistic Regrassion with Random Oversampling","dbc8f1a9":"# XGBoost on balanced data with SMOTE","d1ad99bb":"### Outliers treatment","eff80573":"# Problem Statement\nThe problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n\n \n\nIn this project, we will analyse customer-level data that has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n\n \n\nThe data set is taken from the Kaggle website and has a total of 2,84,807 transactions; out of these, 492 are fraudulent. Since the data set is highly imbalanced, it needs to be handled before model building.\n\n \n\n# Business problem overview\nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\n\nIt has been estimated by Nilson Report that by 2020, banking frauds would account for $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing in new and different ways. \n\n \n\nIn the banking industry, credit card fraud detection using machine learning is not only a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees as well as denials of legitimate transactions.\n\n \n\n# Understanding and defining fraud\nCredit card fraud is any dishonest act or behaviour to obtain information without proper authorisation from the account holder for financial gain. Among different ways of committing frauds, skimming is the most common one, which is a way of duplicating information that is located on the magnetic strip of the card. Apart from this, the other ways are as follows:\n\n- Manipulation\/alteration of genuine cards\n- Creation of counterfeit cards\n- Stealing\/loss of credit cards\n- Fraudulent telemarketing\n \n\n# Data dictionary\nThe data set can be downloaded using this link.\n\n \n\nThe data set includes credit card transactions made by European cardholders over a period of two days in September 2013. Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for 0.172% of the total transactions. The data set has also been modified with principal component analysis (PCA) to maintain confidentiality. Apart from \u2018time\u2019 and \u2018amount\u2019, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. The feature 'amount' is the transaction amount. The feature 'class' represents class labelling, and it takes the value of 1 in cases of fraud and 0 in others.\n\n \n\n# Project pipeline\nThe project pipeline can be briefly summarised in the following four steps:\n\n# Data Understanding: \n\nHere, we need to load the data and understand the features present in it. This would help us choose the features that we will need for our final model.\n\n- **Exploratory data analytics (EDA)**: Normally, in this step, we need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, we do not need to perform Z-scaling. However, we  can check whether there is any skewness in the data and try to mitigate it, as it might cause problems during the model building phase.\n- **Train\/Test split**: Now, we are familiar with the train\/test split that we can perform to check the performance of our models with unseen data. Here, for validation, we can use the k-fold cross-validation method. We need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n- **Model building \/ hyperparameter tuning**: This is the final step at which we can try different models and fine-tune their hyperparameters until we get the desired level of performance on the given data set. We should try and check if we get a better model by various sampling techniques.\n- **Model evaluation**: Evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced, it is is more important to identify the fraudulent transactions accurately than the non-fraudulent ones. Choose an appropriate evaluation metric that reflects this business goal.","effc9d35":"## Exploratory data analysis","d350584d":"#### Logistic Regression with optimal C","adea02c1":"#### Model with optimal hyperparameter","609d9f0c":"#### Model Summary\n\n- Train set\n    - ROC score: 100%\n    - F1 score: 99.75%\n- Test set\n    - ROC score: 98%\n    - F1 score: 82.87%","3bee54e9":"#### Evaluating the model on train data","b80a4567":"#### Model Summary\n- Train set\n    - ROC score : 100%\n    - F1 score: 99.64%\n- Test set\n    - ROC score : 92%\n    - F1 score: 25.31%","ed4189cb":"### Summary to the business\nHere, we have to focus on a high recall in order to detect actual fraudulent transactions in order to save the banks from high-value fraudulent transactions,\n\nAfter performing several models, we have seen that in the balanced dataset with ADASYN technique the XGBoost model has good ROC score(98%) and also high Recall(86%). Hence, we can go with the XGBoost model here.","b859beb6":"#### Prediction and model evalution on the train set","209cf025":"#### Decision Tree with optimal hyperparameters","bba1adb4":"#### Model Summary\n- Train set\n    - ROC score : 99%\n    - F1 score: 98%\n    \n- Test set\n    - ROC score : 95%\n    - F1 score: 7.6%","43680627":"# Decision Tree on balanced data with ADASYN","b1bfdfd7":"#### Evaluating the model on test data","e0a11b22":"#### Evaluating the model on the test set","6dbb185c":"#### Model evaluation on the test set","62e9f930":"#### Model Summary\n- Train set\n    - ROC score : 100.0%\n    - F1 score: 99.99%\n- Test set\n    - ROC score : 98%\n    - F1 score: 81.34%","9ea12270":"#### Evaluating on test data","c3e3de9d":"#### Model evatuation on train data","c9fc1613":"# RandomForest","8f9f58bc":"#### Evaluating the model on train data","a7b39557":"### Print the important features of the best model to understand the dataset\n- This will not give much explanation on the already transformed dataset\n- But it will help us in understanding if the dataset is not PCA transformed","6d0fcdd3":"#### Evaluating on test data","399a9aeb":"# XGBoost with Random Oversampling","9cb36e92":"# Decision Tree on balanced data with SMOTE","41a6fcbf":"# Randomforest on balanced data with SMOTE","4fc67362":"#### Model with optimal hyperparameters","7cda7ce6":"# Random Forest with Random Oversampling","8c20a193":"# Decision Tree","1cbbb240":"We can see that the threshold is 0.46, for which the TPR is the highest and FPR is the lowest and we got the best ROC score.","3e476985":"#### Model with optimal hyperparameter","988008e7":"# Logistic Regression on balanced data with ADASYN","2372ffa7":"#### Prediction on the train set","f55f7f78":"### Select the oversampling method which shows the best result on a model\nWe have used several balancing technique to solve the minority class imbalance. We have used Random Oversampling, SMOTE, and Adasyn technique to balance the dataset and then we performed logistic regression, random forest and XGBoost algorithms to build models on each sampling method.\n\nAfter conducting the experiment on each oversampling method, we have found that XGBoost model is performing well on the  dataset which is balanced with AdaSyn technique. We got ROC score 100% on train data and 98% on the test data and F1 score 100% on train data and 78% in the test data. \n\nHence, we conclude that the `XGBoost model with Adasyn` is the best model.","b6c89e6a":"### Splitting the data into train & test data","56a06753":"We can see very good ROC on the test data set 0.97.","f68e57de":"We have found that 77.97% is the threshold for which TPR is the highest and FPR is the lowest and we get 99.99% ROC score on the train data.","582441f4":"#### Model with optimal hyperparameters","d21aef17":"### Random Oversampling","10a5793a":"#### Print the FPR,TPR & select the best threshold from the roc curve","1e313e3f":"**XGBoost model is giving good performance on the unbalanced data among these 4 models. ROC-AUC score on the train data is 100% and on test data 98%.**","cb85f3b7":"#### Random forest with optimal hyperparameters","cd733124":"#### Evaluating the model on the test set","66792199":"# XGBoost on balanced data with ADASYN","a5de8f29":"#### XGBoost with optimal hyperparameter","6ac00092":"As the whole dataset is transformed with PCA, so assuming that the outliers are already treated. Hence, we are not performing any outliers treatment on the dataframe, though we still see outliers available.","b7503946":"#### Evaluating the model on test data","9b463099":"#### Model Summary\n- Train set\n    - ROC score : 100%\n    - F1 score: 98.92%\n- Test set\n    - ROC score : 98%\n    - F1 score: 52.89%","627e5932":"#### Evaluating the model on the test set","f4d705db":"We need to scale `Amount` column.","29d528d9":"## SMOTE (Synthetic Minority Oversampling Technique)","c04787ac":"#### Evaluating the model on train data","76887c21":"#### Predictions on the test set","5512e21f":"#### Logistic Regression with hyperparameter tuning","7cc5ced9":"#### Evaluating the model on the test set","ba32f8a4":"#### Model Summary\n\n- Train set\n    - ROC Score: 100%\n    - F1 score : 75.32%\n    \n    \n- Test set\n    - ROC Score: 98%\n    - F1 score : 75.56%","cb9ea0a8":"#### Model with optimal hyperparameters","32a3f395":"### Plotting the distribution of a variable to handle skewness","823b5359":"#### Evaluating on test data","10e23248":"#### Model Summary\n- Train set\n    - ROC score : 100%\n    - F1 score: 99.16%\n    \n- Test set\n    - ROC score : 98%\n    - F1 score: 20.6%","983a9801":"#### Evaluating the model on train data","a8ca816d":"#### Random Forest with optimal hyperparameters","a06123b2":"#### Print the FPR,TPR & select the best threshold from the roc curve for the best model","33ddc60b":"Lot of features are highly skewed. So we will check the skewness using skew() and if the skewness is beyond -1 to 1, then we will use power transform to transform the data.","5aeeea8c":"#### Evaluating the model on train data","239d3ae7":"# RandomForest on balanced data with ADASYN","65aa9e86":"#### Evaluating the model with train data","c38ddcc3":"#### Evaluating the model on the test set","946c4bf4":"**Observation**\n\nClearly low amount transactions are more likely to be fraudulent than high amount transaction.","2dd52234":"#### Model Summary\n\n- Train set\n    -     ROC : 98%\n    - F1 Score: 74.47%\n    \n    \n- Test set\n    -     ROC : 97%\n    - F1 score: 72.83%","00d32d7e":"# Credit Card Fraud Detection\n\nIn this project we will predict fraudulent credit card transactions with the help of Machine learning models. \n\nIn order to complete the project, we are going to follow below high level steps to build and select best model.\n- Read the dataset and perform exploratory data analysis\n- Building different classification models on the unbalanced data\n- Building different models on 3 different balancing technique.\n    - Random Oversampling\n    - SMOTE\n    - ADASYN","c164ba40":"#### Evaluating the model on the train data","7bb2f26b":"## Model Building with imbalanced data\nWe are going to build models on below mentioned algorithms and we will compare for the best model. We are not building models on SVM,  and KNN as these algorithms are computationaly expensive and need more computational resources specially for the SVM and KNN.  Skipped models' process is computationally very expensive when we have very large data set. We do not have these resource available so we are skipping these models. Working with below models:\n    - Logistic Regression\n    - Decision Tree\n    - RandomForest\n    - XGBoost\n\n#### Metric selection on imbalance data\nWe are going to use ROC-AUC score as the evaluation metric for the model evaluation purpose. As the data is highly imbalanced and we have only 0.17% fraud cases in the whole data, accuracy will not be the right metric to evaluate the model.","22c88c3c":"We can see most of the features distributions are overlapping for both the fraud and non-fraud transactions.","19b87877":"#### Decision Tree with optimal hyperparameters","b23724dd":"#### Model Summary\n- Train set\n    - ROC score : 97%\n    - F1 score: 90.49%\n- Test set\n    - ROC score : 97%\n    - F1 score: 3.39%","5193eb95":"## ADASYN (Adaptive Synthetic Sampling)","5f75403e":"### There is skewness present in the distribution of the above features:\n- Power Transformer package present in the <b>preprocessing library provided by sklearn<\/b> is used to make the distribution more gaussian","570f9a85":"## Model building with balancing Classes\n\nWe are going to perform below over sampling approaches for handling data imbalance and we will pick the best approach based on model performance.\n- Random Oversampling\n- SMOTE\n- ADASYN","fa89fe22":"#### Model evatuation on train data","3aa6a81f":"#### Model Summary\n- Train set\n    - ROC score : 100.0%\n    - F1 score: 99.89%\n    \n- Test set\n    - ROC score : 97%\n    - F1 score: 52.44%","3979766e":"#  Logistic Regression","66dc1742":"#### Model Summary\n- Train set\n    - ROC score : 100%\n    - F1 score: 99.72%\n- Test set\n    - ROC score : 98%\n    - F1 score: 70.49%","552518af":"#### Logistic Regression with optimal C","39440c8b":"#### Evaluating the model on the train data","50b4c765":"#### Model Summary\n- Train set\n    - ROC score : 99%\n    - F1 score: 94.97%\n- Test set\n    - ROC score : 97%\n    - F1 score: 10.11%","37235b29":"#### Model evatuation on train data","9942bb3c":"#### Logistic Regression with optimal C","7e2d30d7":"# XGBoost","6b1731db":"### Print the class distribution after applying SMOTE ","9b7319fd":"We can see that there is no missing value present in the dataframe.","0fbedb09":"#### Prediction on the train set","4386df0e":"#### Evaluating the model on the test set"}}