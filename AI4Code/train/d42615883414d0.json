{"cell_type":{"f5398072":"code","4ecb6d60":"code","e2cc7ae6":"code","e284681d":"code","e57a9893":"code","2cc607fb":"code","eb00579c":"code","eeca48a6":"code","ffd7289b":"code","1e9d40ef":"code","87cbc71d":"code","43ee84f9":"code","a414550d":"code","b6bd4b10":"code","1e60bbd7":"code","9340ca8c":"code","ef34b32f":"code","c02948fe":"code","5d5fcb7d":"code","14d51ca8":"code","12978b7e":"code","9447f3b1":"code","62bbf066":"code","9fde48e6":"code","3325cb9e":"code","f784fcf5":"code","4bd9bd1b":"code","31c97a02":"code","bb404873":"code","fff73f8c":"code","7217de50":"code","735f51a0":"code","60db1fa4":"code","a97732b2":"code","c6c1d493":"code","def97606":"code","27c61488":"code","2f19c5eb":"code","8b81632b":"code","d823151e":"code","a3259419":"code","fc841f13":"code","b63059aa":"code","5df45624":"code","152c366e":"code","c2c684a5":"code","7e102417":"code","01fbcff4":"code","c68893d0":"code","353a6144":"code","0821b369":"code","fffcc24b":"code","407ad757":"code","4d468203":"code","83874965":"code","32c840ef":"code","7f0dc74d":"code","59d6e602":"code","6b9fc31d":"code","9f02adc9":"code","3e7f3683":"code","2bc80f48":"code","4d2affd7":"code","a8b62b20":"code","1babab26":"code","919162b2":"code","65122227":"code","f9996b20":"code","e26abdcc":"code","b3acdc0d":"code","1069b860":"code","0f4c5929":"code","96fdfd01":"code","19b86688":"code","378ad254":"code","2cae02c0":"markdown","edaed8a0":"markdown","3f4ec64e":"markdown","aa84c5a3":"markdown","c2b02af8":"markdown","8dbdbd2f":"markdown","0bc9c60c":"markdown","992fb718":"markdown","8e0100b9":"markdown","7f452842":"markdown","dc2f3b91":"markdown","6ab63389":"markdown","45875294":"markdown","ad58fd9d":"markdown","ef3c728a":"markdown","ef8ceb66":"markdown","b9259d96":"markdown","d6b1083c":"markdown","b6cbedbb":"markdown","139ffea6":"markdown","e323c6ae":"markdown","a7f05c99":"markdown","f18e621b":"markdown","096e7a1c":"markdown","863e0910":"markdown","568b78da":"markdown","873555dd":"markdown","801efc7b":"markdown","a9868b2b":"markdown","88f154a1":"markdown","6bb55726":"markdown"},"source":{"f5398072":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n# dataframe display settings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# filtering warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4ecb6d60":"df=pd.read_csv(\"\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")\ndf.head()","e2cc7ae6":"df.dropna(inplace=True)","e284681d":"df_new=df.copy(deep=True)","e57a9893":"#administration duration\nq1=df['Administrative_Duration'].quantile(0.25)\nq3=df['Administrative_Duration'].quantile(0.75)\niqr=q3-q1\nu_l1=q3+1.5*iqr","2cc607fb":"df_new['Administrative_Duration']=df_new['Administrative_Duration'].map(lambda x:0 if x==0 else (1 if x>0 and x<u_l1 else 2))\ndf_new['Administrative_Duration'].value_counts()","eb00579c":"df_new['Informational_Duration']=df_new['Informational_Duration'].map(lambda x:0 if x==0 else 1)\ndf_new['Informational_Duration'].value_counts()","eeca48a6":"#ProductRelated_Duration\nq1=df['ProductRelated_Duration'].quantile(0.25)\nq3=df['ProductRelated_Duration'].quantile(0.75)\niqr=q3-q1\nu_l1=q3+1.5*iqr","ffd7289b":"df_new['ProductRelated_Duration']=df_new['ProductRelated_Duration'].map(lambda x:0 if x==0 else (1 if x>0 and x<u_l1 else 2))\ndf_new['ProductRelated_Duration'].value_counts()","1e9d40ef":"df_new.shape","87cbc71d":"df_new['Revenue'].value_counts()","43ee84f9":"ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')\nct","a414550d":"ct.plot.bar()","b6bd4b10":"ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')\nct","1e60bbd7":"ct.plot.bar()","9340ca8c":"ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')\nct","ef34b32f":"ct.plot.bar()","c02948fe":"ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')\nct","5d5fcb7d":"ct.plot.bar()","14d51ca8":"ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')\nct","12978b7e":"ct.plot.bar()","9447f3b1":"ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')\nct","62bbf066":"ct.plot.bar()","9fde48e6":"ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')","3325cb9e":"ct.plot.bar()","f784fcf5":"ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')","4bd9bd1b":"ct.plot.bar()","31c97a02":"ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')","bb404873":"ct.plot.bar()","fff73f8c":"# count of sessions for each duration","7217de50":"sns.countplot(df_new['Administrative_Duration'],hue=df['Revenue'])","735f51a0":"sns.countplot(df_new['Informational_Duration'],hue=df['Revenue'])","60db1fa4":"sns.countplot(df_new['ProductRelated_Duration'],hue=df['Revenue'])","a97732b2":"ct=pd.crosstab(df_new['Region'],df_new['Administrative_Duration'],values=df_new['PageValues'],aggfunc='mean')\nct.plot.bar()","c6c1d493":"ct=pd.crosstab(df_new['Region'],df_new['Informational_Duration'],values=df_new['PageValues'],aggfunc='mean')\nct.plot.bar()","def97606":"ct=pd.crosstab(df_new['Region'],df_new['ProductRelated_Duration'],values=df_new['PageValues'],aggfunc='mean')\nct.plot.bar()","27c61488":"ct=pd.crosstab(df_new['Region'],df_new['Administrative_Duration'],values=df_new['BounceRates'],aggfunc='mean')\nct.plot.bar()","2f19c5eb":"ct=pd.crosstab(df_new['Region'],df_new['Informational_Duration'],values=df_new['BounceRates'],aggfunc='mean')\nct.plot.bar()","8b81632b":"ct=pd.crosstab(df_new['Region'],df_new['ProductRelated_Duration'],values=df_new['BounceRates'],aggfunc='mean')\nct.plot.bar()","d823151e":"df_new.head()","a3259419":"df1=pd.get_dummies(data=df_new,columns=['Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend','Revenue'],drop_first=True)\ndf1.head()","fc841f13":"df1.rename(columns={'Revenue_True':'Revenue'},inplace=True)\ndf1.shape","b63059aa":"df_p=df1.copy(deep=True)","5df45624":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,confusion_matrix\n","152c366e":"X=df_p.drop('Revenue',axis=1)\ny=df_p['Revenue']","c2c684a5":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=5)","7e102417":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","01fbcff4":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","c68893d0":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","353a6144":"cm_reference = pd.DataFrame(np.array([\"TP\",\"FP\",\"FN\",\"TN\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\nprint(cm_reference)","0821b369":"TP=cm[0,0]\nTN=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","fffcc24b":"print(\"True Negatives :\",TN)\nprint(\"True Positives :\",TP)\nprint(\"False Negative :\",FN,\" (Type II error)\")\nprint(\"False Positives :\",FP,\" (Type I error)\")\nprint(\"correctly predicted :\",TP+TN)\nprint(\"miss-classified :\",FN+FP)","407ad757":"print('The acuuracy of the model = TP+TN \/ (TP+TN+FP+FN) = ',(TP+TN)\/float(TP+TN+FP+FN),'\\n\\n',\n\n'The Miss-classification = 1-Accuracy = ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n\\n',\n\n'Sensitivity or True Positive Rate = TP \/ (TP+FN) = ',TP\/float(TP+FN),'\\n\\n',\n\n'Specificity or True Negative Rate = TN \/ (TN+FP) = ',TN\/float(TN+FP),'\\n\\n',\n\n'Positive Predictive value = TP \/ (TP+FP) = ',TP\/float(TP+FP),'\\n\\n',\n\n'Negative predictive Value = TN \/ (TN+FN) = ',TN\/float(TN+FN),'\\n\\n',\n\n'Positive Likelihood Ratio = Sensitivity \/ (1-Specificity) = ',sensitivity\/(1-specificity),'\\n\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity) \/ Specificity = ',(1-sensitivity)\/specificity)","4d468203":"y_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of no purchase (0)','Prob of purchase (1)'])\ny_pred_prob_df.head()","83874965":"roc_auc_score(y_test,y_pred)","32c840ef":"# results matrix\ndf_results = pd.DataFrame(columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])","7f0dc74d":"# itereation results\ndescription = \"Base logit model\"\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(y_test,y_pred),2)\nrecall = round(recall_score(y_test,y_pred),2)\naccuracy = round(accuracy_score(y_test,y_pred),2)\nf1 = round(f1_score(y_test,y_pred),2)\nauc = round(roc_auc_score(y_test,y_pred),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","59d6e602":"print(classification_report(y_test,y_pred))","6b9fc31d":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    ","9f02adc9":"# ROC curve","3e7f3683":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for online shoppers classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","2bc80f48":"## 1)Here,we can see that even decreasing threshold it is causing type 1 error.\n\n## 2)we can use the default threshold 0.5 ","4d2affd7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier","a8b62b20":"DT=DecisionTreeClassifier()\nRF=RandomForestClassifier(criterion='entropy',n_estimators=10)\nBagged=BaggingClassifier(n_estimators=100)\nAB_RF=AdaBoostClassifier(base_estimator=RF,n_estimators=150)\nGBoost=GradientBoostingClassifier(n_estimators=300)\nKNN=KNeighborsClassifier(n_neighbors=9,weights='distance')","1babab26":"models = [] \nmodels.append(('DT',DT))\nmodels.append(('RandomForest',RF))\nmodels.append(('Bagged',Bagged))\nmodels.append(('AdaBoostRF',AB_RF))\nmodels.append(('GradientBoost',GBoost))\nmodels.append(('KNN',KNN))","919162b2":"results = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=3,shuffle=True,random_state=0)\n    cv_results = model_selection.cross_val_score(model,X,y,cv=kfold,scoring='f1_weighted')\n    results.append(np.sqrt(np.abs(cv_results)))\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, 1-np.mean(cv_results),np.std(cv_results,ddof=1)))\n","65122227":"rf=RandomForestClassifier().fit(x_train,y_train)\ny_pred_rf=rf.predict(x_test)\nprint(classification_report(y_test,y_pred_rf))","f9996b20":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred_rf)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\nTP=cm[0,0]\nTN=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","e26abdcc":"# itereation results\ndescription = \"Random Forest Classifier\"  #change the name of models\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(y_test,y_pred_rf),2)\nrecall = round(recall_score(y_test,y_pred_rf),2)\naccuracy = round(accuracy_score(y_test,y_pred_rf),2)\nf1 = round(f1_score(y_test,y_pred_rf),2)\nauc = round(roc_auc_score(y_test,y_pred_rf),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","b3acdc0d":"ada=AdaBoostClassifier().fit(x_train,y_train)\ny_pred_ada=ada.predict(x_test)\nprint(classification_report(y_test,y_pred_ada))","1069b860":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred_ada)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\nTP=cm[0,0]\nTN=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","0f4c5929":"# itereation results\ndescription = \"Ada Boost Classifier\"  #change the name of models\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(y_test,y_pred_ada),2)\nrecall = round(recall_score(y_test,y_pred_ada),2)\naccuracy = round(accuracy_score(y_test,y_pred_ada),2)\nf1 = round(f1_score(y_test,y_pred_ada),2)\nauc = round(roc_auc_score(y_test,y_pred_ada),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","96fdfd01":"gbc=GradientBoostingClassifier().fit(x_train,y_train)\ny_pred_gbc=gbc.predict(x_test)\nprint(classification_report(y_test,y_pred_ada))","19b86688":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred_gbc)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\nTP=cm[0,0]\nTN=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","378ad254":"# itereation results\ndescription = \"Gradient Boost Classifier\"  #change the name of models\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(y_test,y_pred_gbc),2)\nrecall = round(recall_score(y_test,y_pred_gbc),2)\naccuracy = round(accuracy_score(y_test,y_pred_gbc),2)\nf1 = round(f1_score(y_test,y_pred_gbc),2)\nauc = round(roc_auc_score(y_test,y_pred_gbc),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","2cae02c0":"# Administrative_Duration","edaed8a0":"### region n duration vs bounce rate","3f4ec64e":"## with gradient boosting we got very good results and we can also redce our missclassifications .further,we use imbalanced techniques so that bias can be reduced and model can predict more accurately and show disparity between each class .","aa84c5a3":"avg.bounce rate when revenue is true is very less than when revenue is true","c2b02af8":"# modelling","8dbdbd2f":"## DATA SOURCE\n The dataset is obtained from Google Analytics which gathers information about the performance of websites and also all the activities carried out on the website by the customers. The dataset has 12330 unique sessions over a period of 12 months that can be used to predict the behaviour of customers on the website. The target variable is imbalanced with a proportion of majority class (True) being 84.5% and False being 15.5%. The dataset has parameters of google analytics that can be used to predict the revenue generated by customers.\n\n\n### DATA DESCRIPTION\n### CONTINUOS FEATURES\n\n\nAdministrative\t     -Integer\t-Number of different pages visited related \n                                to the administrative concerns of the website\t                    \n\nInformational\t     -Integer\t-Number of different pages visited related to the information                                                                   of the website and other useful contents of the website          \n\nProductRelated\t    - Integer\t-Number of different pages visited related to different \n                                products of the website.\t                                        \n\nBounceRate         \t- Float\t -   Percentage of users who left the website from the landing page\t     \n\nExitRate\t        - Float\t  -  Percentage of users who left from the page the visit\t             \n\nPage Values\t        - Float\t   - Page Value is the average value for a page that a user \n                                visited before making a transaction.\t                             \n\nSpecialDay\t        - Float\t    \n                               \n                               0 \u2013 day of the session is not within 10 days of a special day.\n                               Between 0.1 and 0.5 \u2013 day of the session is between 10 days and 5 days away from a special day.\n                               Between 0.6 and 0.9 \u2013 day of the session is between 4 days and 1 day away from a special day.\n                               1 \u2013 day of the session is a special day.\t\n\nThe features Bounce Rates, Exit Rates and Page Values are terms of Google analytics that are used to determine the performance of a website. The bounce rate is used to show the percentage of single page sessions out of all the sessions. The exit rate shows us the percentage of people who left from a page. Page value gives us the average value of the transaction to all the pages visited which is equally distributed to all pages.\n\n### CATEGORICAL FEATURES\n\n\nAdministrative_Duration\t   -Integer\t    \n                                        \n    Time spent on Administrative pages in seconds\n                                        0 \u2013 No time was spent on the page                                 \n                                        1 - Less time of under 233 was spent on the page\n                                        2 \u2013 More time of  over 233 was spent on the page\t\n\nInformational_duration\t   Integer\t    \n\n    Time spent on Informational pages in seconds                       \n                                          0 \u2013 No time was spent on the page\n                                          1- More time was spent on the page\t\n\nProductRelated_Duration    Integer\t    \n\n    Time spent on pages related to products in seconds                 \n                                          0 \u2013 No time was spent on the page\n                                          1 - Less time of under 3384 was spent on the page\n                                          2 \u2013 More time of  over 3384 was spent on the page\t\n\nBrowse                    -  Integer    \t-ID of bowsers from which the session took place\t                   \n\nRegion\t                   - Integer\t -   ID of Regions from which the session took place\t\n\nTraffic Type\t            -Integer\t-    ID of different types of sources from which the users landed on the website\t\n\nUser Type\t                -String\t   - Whether the user is a returning user or a new user or of any other type\t\n\nRevenue\t                   - Boolean\t-    Whether the user contributed to the revenue by purchasing or not\t\n\nWeekend\t                  -  Boolean  -\tWhether the session was on a weekend or not\t","0bc9c60c":"# ProductRelated_Duration","992fb718":"# Informational_Duration","8e0100b9":"# Region \n\n### region n duration vs page value","7f452842":"# logistic regression\n\nLogistic regression is a type of regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of predictor or independent variables. In logistic regression the dependent variable is always binary. Logistic regression is mainly used to for prediction and also calculating the probability of success. ","dc2f3b91":"we can see when sessions are product related then most of the revenue is generated \n\nmost the time is spent on product related.\n\ninformational pages are mostly not interested ","6ab63389":"page values are very high for product related than other  ","45875294":"we can say product related is mostly preffered over other ","ad58fd9d":"we can see pagevalue is high at duration 0 it is because of other pages","ef3c728a":"we can see avg.bounce rate when revenue true is very less then when it is false\n\neven avg.bounce rate is decreasing as time spent increases.","ef8ceb66":"## we will use kfold cv to decide which model performs better","b9259d96":"# confusion matrix","d6b1083c":"we can see high pagevalue for duration 0 it is because of other pages when this is 0","b6cbedbb":"### Area Under The Curve (AUC)\n\n- The area under the ROC curve quantifies model classification accuracy ; the higher the area, the greater the disparity between true and false positives, and the stronger the model in classifying members of the training dataset.\n- An area of 0.5 corresponds to a model that performs no better than random classification and a good classifier stays as far away from that  as possible. An area of 1 is ideal. \n- The closer the AUC to 1 the better.","139ffea6":"## We can see ensemble models are performing better like bagging and gradientboost and random forest.so lets try building model with them.","e323c6ae":"# evaluation metrics","a7f05c99":"- A common way to visualize the trade-offs of different thresholds is by using an ROC curve, a plot of the true positive rate ( true positives\/ total  positives) versus the false positive rate ( false positives \/total  negatives) for all possible choices of thresholds.\n- A model with good classification accuracy should have significantly more true positives than false positives at all thresholds. \n- The optimum position for roc curve is towards the top left corner where the specificity and sensitivity are at optimum levels","f18e621b":"**From the above statistics it is clear that the model is highly sensitive than specific. The positive values are predicted more accurately than the negative i.e, it is predicting people who will not make purchase more accurately than who make purchase.**","096e7a1c":"we can see avg.exit rate when revenue true is very less then when it is false\n\neven avg.exit rate is decreasing as time spent increases.","863e0910":"### <font color= Blue>Predicted probabilities of  0 (will not make purchase) and 1 ( purchase)  for the test data with a default classification threshold of 0.5 <font>","568b78da":"# ONLINE SHOPPERS INTENTION\n\n### Predicting conversion of revenue based on customer\u2019s activity","873555dd":"we can see avg.Exit rate when revenue true is very less then when it is false\n\neven avg.Exit rate is decreasing as time spent increases.","801efc7b":"avg.exit rate when revenue is true is very less than when revenue is true\n\neven avg.Exit rate is decreasing as time spent increases.","a9868b2b":"we can see avg.bounce rate when revenue true is very less then when it is false\n\neven avg.bounce rate is decreasing as time spent increases.","88f154a1":"# threshold\n- Since the model is predicting people who dont purchase too many type II errors is not advisable. \n- A False Negative ( ignoring the probability of purchase when there actualy is one) is more dangerous than a False Positive in this case.\n- Hence inorder to increase the sensitivity,  threshold can be lowered.","6bb55726":"# splitting the data into train and test"}}