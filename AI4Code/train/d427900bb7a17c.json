{"cell_type":{"79995bad":"code","36e942a8":"code","9384a9e5":"code","6b106cc7":"code","0ab11373":"code","45a469f1":"code","15cfe115":"code","89e2816c":"code","171b9b9d":"code","db85fcad":"code","d623f900":"code","e2613b63":"code","edcfc852":"code","b5270955":"code","7e6870d0":"code","ae696952":"code","a31ed9c9":"code","c64c8a05":"markdown","1e4e94d8":"markdown","e3d9a04f":"markdown","069c1459":"markdown","1821d51b":"markdown"},"source":{"79995bad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","36e942a8":"data = pd.read_json('..\/input\/News_Category_Dataset_v2.json', lines=True)\ndata.head()","9384a9e5":"# 2. Remove punctiation & convert to lowercase:\ndata['headline_processed'] = data.headline.str.replace('[{}]'.format(string.punctuation), '').str.lower()\ndata['short_description_processed'] = data.short_description.str.replace('[{}]'.format(string.punctuation), '').str.lower()","6b106cc7":"data.category.unique()","0ab11373":"def category_cleaner(x):\n    \n    if x == 'THE WORLDPOST':\n        return 'WORLDPOST'\n    elif x == 'PARENTING':\n        return 'PARENTS'\n    elif x == 'ARTS' or x == 'CULTURE & ARTS':\n        return 'ARTS & CULTURE'\n    elif x == 'STYLE':\n        return 'STYLE & BEAUTY'\n    elif x == 'COLLEGE':\n        return 'EDUCATION'\n    elif x == 'TASTE':\n        return 'FOOD & DRINK'\n    else:\n        return x\n    \ndata['category'] = data.category.apply(category_cleaner)\n\n\nle = LabelEncoder()\ndata_labels = le.fit_transform(data.category)\nlist(le.classes_)","45a469f1":"data.authors.unique()","15cfe115":"data['authors'] = data.authors.apply(lambda x: x.split(',')[0])\ndata['authors'] = data.authors.str.replace(' ', '', regex=False)\ndata.authors.unique()","89e2816c":"plt.figure(figsize=(15,15))\nsizes = data.category.value_counts().values\nlabels = data.category.value_counts().index\nplt.pie(sizes, labels=labels, autopct='%.1f%%',\n        shadow=True, pctdistance=0.85, labeldistance=1.05, startangle=20, \n        explode = [0 if i > 0 else 0.2 for i in range(len(sizes))])\nplt.axis('equal')\nplt.show()","171b9b9d":"# Splitting into train sets and test sets\nx_train, x_test, y_train, y_test = train_test_split(data, data_labels, \n                                                    train_size=0.8, test_size=0.2,\n                                                    random_state=555)","db85fcad":"# 3. Text Counters & Classifiers..\n\ncounter = CountVectorizer(stop_words='english')\n\ntrain_headline_counts = counter.fit_transform(x_train.headline_processed)\ntest_headline_counts = counter.transform(x_test.headline_processed)\n\ntrain_short_description_counts = counter.fit_transform(x_train.short_description_processed)\ntest_short_description_counts = counter.transform(x_test.short_description_processed)","d623f900":"# 4. Classify with Naive Bayes..\nclassifier = ComplementNB()\n\nclassifier.fit(train_headline_counts, y_train)\nheadline_soft_predictions = classifier.predict_proba(test_headline_counts)\nheadline_predictions = classifier.predict(test_headline_counts)\n\nclassifier.fit(train_short_description_counts, y_train)\nshort_description_soft_predictions = classifier.predict_proba(test_short_description_counts)\nshort_description_predictions = classifier.predict(test_short_description_counts)","e2613b63":"soft_predictions = (headline_soft_predictions + short_description_soft_predictions) \/ 2\npredictions = np.argmax(soft_predictions, axis = 1)\nprint('Accuracy without tokenization: {}'.format(accuracy_score(predictions, y_test)))","edcfc852":"# Combining both text columns into one\ntrain_counts = counter.fit_transform(x_train.headline_processed.str.cat(x_train.short_description_processed))\ntest_counts = counter.transform(x_test.headline_processed.str.cat(x_test.short_description_processed))\nclassifier.fit(train_counts, y_train)\npredictions_combined =  classifier.predict(test_counts)\ntext_combined_soft_predictions = classifier.predict_proba(test_counts)\nprint('Accuracy with combined text and without tokenization: {}'.format(accuracy_score(predictions, y_test)))","b5270955":"# stemming...\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nporter_stemmer = PorterStemmer()\nstopwords = set(stopwords.words('english'))\n\ndef stemming_tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    words = [porter_stemmer.stem(word) if word not in stopwords else word for word in words]\n    return words\n\n\ndef count_classify_with_tokenizer(tokenizer):\n    counter_words = CountVectorizer(stop_words=stopwords, tokenizer=tokenizer)\n    \n    train_headline_counts = counter_words.fit_transform(x_train.headline_processed)\n    test_headline_counts = counter_words.transform(x_test.headline_processed)\n\n    train_short_description_counts = counter_words.fit_transform(x_train.short_description_processed)\n    test_short_description_counts = counter_words.transform(x_test.short_description_processed)\n\n    classifier.fit(train_headline_counts, y_train) # Train on headlines\n    headline_soft_predictions = classifier.predict_proba(test_headline_counts)\n\n    classifier.fit(train_short_description_counts, y_train) # Train on description\n    short_description_soft_predictions = classifier.predict_proba(test_short_description_counts)\n\n    soft_predictions = (headline_soft_predictions + short_description_soft_predictions) \/ 2\n    predictions = np.argmax(soft_predictions, axis = 1)\n    return accuracy_score(predictions, y_test)\n\nprint('Accuracy with Stemmer: {}'.format(count_classify_with_tokenizer(stemming_tokenizer)))","7e6870d0":"# lemmatizing...\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef lemmatizer_tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    words = [wordnet_lemmatizer.lemmatize(word) if word not in stopwords else word for word in words]\n    return words\n\nprint('Accuracy with Lemmatizer: {}'.format(count_classify_with_tokenizer(lemmatizer_tokenizer)))","ae696952":"train_authors_counts = counter.fit_transform(x_train.authors)\ntest_authors_counts = counter.transform(x_test.authors)\n\nclassifier.fit(train_authors_counts, y_train)\nauthors_prediction = classifier.predict(test_authors_counts)\nauthors_soft_prediction = classifier.predict_proba(test_authors_counts)\naccuracy_score(authors_prediction, y_test)","a31ed9c9":"# Add both probabilities together..\nfinal_prediction = np.argmax((text_combined_soft_predictions + authors_soft_prediction) \/ 2, axis=1)\naccuracy_score(final_prediction, y_test)","c64c8a05":"Remember that we still have a valuable piece of information, the author of the article which should be included in our decision as well.","1e4e94d8":"Exactly the same..","e3d9a04f":"A question that came into my mind, would there be any difference in accuracy if we merged both headlines and short description into one attribute?\nI will try it out and see.","069c1459":"That is it for now, best accuracy I could achieve using Naive Bayes is around 70%","1821d51b":"As you might have realized, some categories are pretty much the same:\n* 'WORLDPOST' || 'THE WORLDPOST'\n* 'PARENTS' || 'PARENTING'\n* 'ARTS' || 'ARTS & CULTURE' || 'CULTURE & ARTS'\n* 'STYLE & BEAUTY' || 'STYLE'\n* 'EDUCATION' || 'COLLEGE'\n* 'TASTE' || 'FOOD & DRINK'"}}