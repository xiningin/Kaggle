{"cell_type":{"b6a64469":"code","018b4ef2":"code","9ae244d0":"code","efa3de88":"code","32a5f6be":"code","605a68af":"code","abe86007":"code","c126e705":"code","fbb1bbc0":"code","870237a3":"code","17921660":"code","a7748399":"code","a0789aec":"code","46d11ec0":"code","db2213d3":"code","0b91a9ba":"code","97f34127":"code","82cd3a7d":"code","4ec22cee":"code","911a21f2":"code","9e4e99de":"code","bb3c4216":"code","194ec1b8":"code","e1058753":"code","044e736f":"code","519a576e":"code","065af259":"code","5a42abc9":"code","036d171a":"code","98179ef0":"code","4093fb15":"code","061d7d9b":"code","32a72558":"code","08a14d79":"code","ce6a453a":"code","58906f9c":"code","63032f19":"code","f46e80cf":"code","149de8dd":"code","ea35207d":"code","f48dc2fa":"code","16615e20":"code","87b0210c":"code","28b8e27c":"code","b312fec1":"code","c2d5c27e":"code","ff845862":"code","28905f46":"code","d6035dc4":"code","d8fd8878":"code","a8282537":"markdown","ab6b15a5":"markdown","b7c13ed7":"markdown","b12dfdaf":"markdown","55887a56":"markdown","c6a3e550":"markdown","7e909336":"markdown","2d0fcbca":"markdown","07380e3d":"markdown","7da324c4":"markdown","b81ea904":"markdown","d684d0d6":"markdown","d6b2a155":"markdown","3fb4db6d":"markdown","6bf468c0":"markdown","fceb31d5":"markdown","21f37f53":"markdown","b6c516b6":"markdown","ff965019":"markdown","00e2e25b":"markdown","19653eb8":"markdown","db5abe54":"markdown","76f4f2bd":"markdown","d5b42bec":"markdown","1004cc39":"markdown","e493630d":"markdown","9cf35ac6":"markdown","6ac8315b":"markdown","b6177da4":"markdown","4ec9a004":"markdown","f46872b5":"markdown","474f2c5e":"markdown","da00468a":"markdown","8f2bca41":"markdown","d067b7f8":"markdown","66a5e6e1":"markdown","7b36e543":"markdown","4fb0e7dd":"markdown","48a33d20":"markdown","eaf950c1":"markdown","ddd0c83e":"markdown","77597604":"markdown","43beefef":"markdown","5d4b1a84":"markdown","5c606094":"markdown","700e3311":"markdown"},"source":{"b6a64469":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","018b4ef2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","9ae244d0":"# Loading the data\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain.shape, test.shape","efa3de88":"# having a look\ntrain.head()","32a5f6be":"# Checking % of NAN values in the columns\n\nfeatures_with_na = [features for features in train.columns if train[features].isnull().sum()>0]\n\nprint(\"Features and its % of missing values: \")\nprint()\nfor feature in features_with_na:\n    print(feature, np.round(train[feature].isnull().mean(), 4))","605a68af":"for feature in features_with_na:\n    data = train.copy()\n    \n    # making a variable that indicates 1 if the observation was missing and 0 if not\n    # in simple, converting missing values to 1 and others to 0\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # calculating the mean SalePrice where the info is missing or present\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","abe86007":"# List of numerical features\nnum_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\n# 'O' means object, i.e. if not Object than obviously it's numerical\nprint(\"No. of numerical features: \", len(num_features))\n\ntrain[num_features].head()","c126e705":"# List of varables that contain Year variables\nyear_features = [feature for feature in num_features if 'Yr' in feature or 'Year' in feature]\n# we used the above logic because all the year variables have either 'Yr' or 'Year' in its name\n\nprint(\"Year Variables: \", year_features)","fbb1bbc0":"for i in year_features:\n    print(i, train[i].unique())","870237a3":"# Analyzing the Temporal datetime variable\n# Checking for relation between house sold year and sales price\n\ntrain.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel(\"Year sold\")\nplt.ylabel('Median House Price')","17921660":"for feature in year_features:\n    if feature != 'YrSold':\n        data = train.copy()\n        data[feature] = data['YrSold'] - data[feature]\n        \n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","a7748399":"discrete_feature=[feature for feature in num_features if len(train[feature].unique())<25 \n                  and feature not in year_features+['Id']]\n\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\ndiscrete_feature","a0789aec":"train[discrete_feature].head()","46d11ec0":"for feature in discrete_feature:\n    data = train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.title(feature)\n    plt.show()","db2213d3":"contin_feature=[feature for feature in num_features if feature not in discrete_feature+ year_features+['Id']]\nprint(\"No. of Continuous Features: {}\".format(len(contin_feature)))","0b91a9ba":"# Creating histograms because we are trying to find out the distribution of the continuous variables\nfor feature in contin_feature:\n    data = train.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","97f34127":"for feature in contin_feature:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data['SalePrice']= np.log(data['SalePrice'])\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel(\"SalePrice\")\n        plt.title(feature)\n        plt.show()","82cd3a7d":"# Finding the outliers\n\nfor feature in contin_feature:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","4ec22cee":"categorical_feature=[feature for feature in train.columns if train[feature].dtype=='O']\n\n#len(categorical_feature)\ncategorical_feature","911a21f2":"train[categorical_feature].head()","9e4e99de":"for feature in categorical_feature:\n    print('The feature {} has {} No. of categories'.format(feature, len(train[feature].unique())))","bb3c4216":"for feature in categorical_feature:\n    data = train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","194ec1b8":"target = train['SalePrice']\ntest_id = test['Id']\n\ntrain_for_con = train.drop(['Id', 'SalePrice'], axis=1)\ntest_for_con = test.drop(['Id'], axis=1)","e1058753":"train_for_con.shape, test_for_con.shape","044e736f":"combined = train_for_con.append(test_for_con)\ncombined.shape","519a576e":"# checking for missing values\ncol_with_na = [x for x in combined.columns if combined[x].isnull().sum()>0]\n\nprint(\"Features and its % of missing values: \")\nprint()\nfor y in col_with_na:\n    print(y, np.round(combined[y].isnull().mean(), 6))","065af259":"# Missing categorical features\n\ncol_nan = [x for x in combined.columns if combined[x].isnull().sum()>0 \n           and combined[x].dtypes=='O']\nprint(\"Categorical Features and its % of missing values: \")\nprint()\nfor y in col_nan:\n    print(y, np.round(combined[y].isnull().mean(), 4))","5a42abc9":"def replace_cat(combined, col_nan):\n    data = combined.copy()\n    data[col_nan]=data[col_nan].fillna('Missing')\n    return data\ncombined = replace_cat(combined, col_nan)\ncombined[col_nan].isnull().any().sum()","036d171a":"num_nan = [x for x in combined.columns if combined[x].isnull().sum()>0\n          and combined[x].dtypes !='O']\n\nprint(\"Numerical Features and its % of missing values: \")\nprint()\nfor y in num_nan:\n    print(y, np.round(combined[y].isnull().mean(), 4))","98179ef0":"for i in num_nan:\n    median_value = combined[i].median()\n    \n    # create a new feature to capture the NAN values\n    # we are replacing the nan values by 1 and others by 0 in the newly created feature\n    combined[i + '_nan'] = np.where(combined[i].isnull(), 1, 0)\n    combined[i].fillna(median_value, inplace=True)\n\ncombined[num_nan].isnull().any().sum()","4093fb15":"for x in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    combined[x] = combined['YrSold'] - combined[x]\n\ncombined.head()","061d7d9b":"# We can see the outcome of the above code as follows\ncombined[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].head()","32a72558":"# Skewed Features\nnum_features = ['LotFrontage','LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    combined[feature] = np.log(combined[feature])\n    \ncombined.head()","08a14d79":"# Also our target, 'SalePrice' is skewed, so...\ntarget = np.log(target)\ntarget.head()","ce6a453a":"## Handling rare categorical features\n# all the categorical features in our dataset\ncategorical_features = [x for x in combined.columns if combined[x].dtype=='O']\n\nfor y in categorical_features:\n    \n    # finding the present % of the features\n    temp = combined.groupby(y)['MSSubClass'].count()\/len(combined)\n    \n    # taking the index of the features with more than 1% presence\n    temp_df = temp[temp>0.01].index\n    \n    # converting the rare features to a new label 'rare_var'\n    combined[y] = np.where(combined[y].isin(temp_df), combined[y], 'Rare_var')","58906f9c":"combined.shape","63032f19":"combined = pd.get_dummies(combined, drop_first=True)\ncombined.shape","f46e80cf":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncombined = pd.DataFrame(scaler.fit_transform(combined), \n                        columns=combined.columns)\ncombined.head()","149de8dd":"X_train = combined[:1460]\nX_test = combined[1460:]\ny_train = target\n\nX_train.shape, X_test.shape, y_train.shape","ea35207d":"from sklearn.ensemble import GradientBoostingRegressor","f48dc2fa":"# Fitting the model\ngradient_boost = GradientBoostingRegressor(learning_rate=0.1,\n                                           n_estimators=50).fit(X_train, y_train)\n# Predicting for the test data\ngrad_boost_predictions = gradient_boost.predict(X_test)\ngrad_boost_predictions = np.expm1(grad_boost_predictions)","16615e20":"from sklearn.model_selection import RandomizedSearchCV","87b0210c":"gb_regressor = GradientBoostingRegressor()\n\n# Hyperparameter optimization\n\nn_estimators = [100, 500, 1000, 1200, 1400]\nmax_depth = [2, 3, 5, 10, 15]\nlearning_rate = [0.05, 0.1, 0.15, 0.20]\nmin_samples_split = [1, 2, 3, 4]\nmin_samples_leaf =[1, 2, 3]\nmin_weight_fraction_leaf = [0, 1, 2]\nmin_impurity_decrease = [0, 1, 2]\n\n# Define the grid of parameters to search\nparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth': max_depth,\n    'learning_rate': learning_rate,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf,\n    'min_weight_fraction_leaf': min_weight_fraction_leaf,\n    'min_impurity_decrease': min_impurity_decrease\n}","28b8e27c":"%%time\nrandom_cv = RandomizedSearchCV(estimator=gb_regressor,\n                              param_distributions=parameter_grid,\n                              cv =5, n_iter=50,\n                              scoring= 'neg_mean_absolute_error',\n                              n_jobs=4,\n                              verbose=5,\n                              return_train_score=True,\n                              random_state=0)","b312fec1":"%%time\nrandom_cv.fit(X_train, y_train)","c2d5c27e":"random_cv.best_estimator_","ff845862":"regressor = GradientBoostingRegressor(max_depth=2,\n                                      min_impurity_decrease=0,\n                                      min_samples_split=3, \n                                      min_weight_fraction_leaf=0,\n                                      n_estimators=1000)","28905f46":"regressor.fit(X_train, y_train)","d6035dc4":"gd_predictions = regressor.predict(X_test)\ngd_predictions = np.expm1(gd_predictions)","d8fd8878":"gd_predictions = pd.DataFrame(gd_predictions)\ngd_predictions = pd.concat([test['Id'], gd_predictions], axis = 1)\ngd_predictions.columns = ['Id', 'SalePrice']\ngd_predictions.to_csv(\"gd_predictions.csv\", index = False)","a8282537":"#### Temporal Variables (eg: datetime variables)\nIn the dataset, we have 4 Year variables. We need to extract information from these features such as No. of years or No. of days. One example in this specific scenario can be the difference in years between the house was build and the year it was sold. This will be considerd in the feature engineering part.","ab6b15a5":"Now it makes sense...\u00b6\n1. The SalePrice for recently built house is high while SalePrice for house build 140 years ago is low.\n2. The SalePrice of house that are remodified recently is high and low for houses that were modified very long ago.\n3. Simarly it is true for the feature GarageYrBlt","b7c13ed7":"After applying the log normal transformation and plotting the distribution we can see that there's a **Monotonic Relation** between the continuous features and the dependent variables","b12dfdaf":"Finding out how many categories each of the categoricl features are having:\n","55887a56":"Fitting the model:\n","c6a3e550":"We can see that the Sale Price is decreasing as the advancement in time which is not quite acceptable as we know that in real life, the opposite only happens.\n\nSo, we will try to compare thr difference between all year feature with the SalePrice.","7e909336":"Defining traget and test_id and preparing train and test set to concatinate to one set\n","2d0fcbca":"Combining the train and test data to do the feature engineering part together","07380e3d":"Creating the regressor object with the optimal parameters:\n","7da324c4":"There's a lot of outliers in our continuous variables. We need to take care of these in the future steps.","b81ea904":"# Hello Everyone!\n\nThis is my first ever kaggle notebook.\n\nHere in this notebook, I have tried to show the complete pipeline of a machine learning problem. \nI've followed the following steps:\n    1. Importng data and have the first look\n    2. Exploratory Data Analysis\n    3. Preparing the data for Feature Engineering\n    4. Feature Engineering\n    5. Model Building\n    6. Parameter Tuning\n    7. Predicting for the Test data\n    8. Creating the submission file","d684d0d6":"Replacing the missing numerical values by median of that column:","d6b2a155":"Scaling the data","3fb4db6d":"**Observation**\nHere, we can see that the missing values are impacting the SalePrice to a great extend.\n\n   1. In case of some feature, with high number of missing values, the median SalePrice is also high.\n   2. In other cases, with less number of missing values, less is the median SalePrice.","6bf468c0":"Now the shape of our combined data...","fceb31d5":"### 2. Checking for Numerical Variables","21f37f53":"### 3. Outliers","b6c516b6":"## This submission file gave me a score of 0.13234","ff965019":"Observations:\n\n1. With the increase in overall quality, the SalePrice is exponentially increasing (a monotonic relationship).\n2. There are some relationship between discrete and dependent variable","00e2e25b":"#### Finding the relationship between the missing values and Sales Price","19653eb8":"We are creating a new feature with the missing categorical variables replacing NAN values by a label 'missing'","db5abe54":"## Our first model\n\n### Gradient Boosting","76f4f2bd":"Having a look at the year variables:","d5b42bec":"### Creating the submission file:\n","1004cc39":"Missing Numerical features:","e493630d":"### 4. Categorical Features","9cf35ac6":"## **Exploratory Data Analysis**","6ac8315b":"Creating dummy variables for the categorical variables","b6177da4":"#### Continuous variables","4ec9a004":"Relationship between categorical features and dependent variable:\n","f46872b5":"## Preprocessing","474f2c5e":"So, we have created our first model and predicted for the test data. But before making the submission file, let's tune the parameters of our model for better performance.","da00468a":"# Feature Engineering","8f2bca41":"Predicting for the test data:\n","d067b7f8":"### We are done with the preprocessing... Let's split the data back into train and test!","66a5e6e1":"For the Temporal Variables (Date Time Variables):\nwe are replacing the years with the difference in years from the Year of Sold\n","7b36e543":"Most of the continuous distribution are not in normal (Gaussian) distribution form but skewed.\n\nWe need to keep in mind that, while dealing a regression problem with non-normal data, we need to convert them into normal (by log normal dist) for better working of the model.","4fb0e7dd":"WE will using Logarithmic Transformation to convert the skewed continous var to normal in the feature engineering part\n","48a33d20":"Finding the relationship between discrete and dependent variables:\n","eaf950c1":"Again,Numerical variables are of two types\n1. Continuous and\n2. Discrete\n\n\nWe are considering a feature to be dicrete if it has less than 25 unique values and are not part of Year Features and Id","ddd0c83e":"Combining The test and train data:","77597604":"### 1. Checking for Missing Values","43beefef":"On EDA, we found that some of our features are skewed\nSo, we'll perform Log Normal Transformation to reduce the skewness.\n","5d4b1a84":"Checking the COntinuous Variables after using Log Normal Transformation:","5c606094":"As I'm a complete begineer, I've used the methods and EDA techniques nicely explained by Krish Naik in his youtube channel. You can check out the videos here:\n","700e3311":"For tuning the parameters, we will be using RandomizedSearchCV"}}