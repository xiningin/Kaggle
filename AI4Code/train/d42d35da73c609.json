{"cell_type":{"53754c3b":"code","492556bb":"code","7e3be7d1":"code","0199378c":"code","b8f48508":"code","005df575":"code","22857bcf":"code","0f40c624":"code","1ecc96b1":"code","d2bde994":"code","deb445e7":"code","e775a497":"code","3863cc4f":"code","e966089b":"code","75f3f7a2":"code","6201e5de":"code","a17be742":"code","3d9d86e4":"code","b9b813ae":"code","f576866d":"code","e35698b5":"code","5cad3c33":"code","9a97b7aa":"code","618e2b73":"code","dce0cdc4":"code","02235b68":"code","89a4a53c":"code","0512b564":"code","d01d2456":"code","6cb1e2ea":"code","57aa14a3":"code","2bc7e41b":"code","5d8fab0d":"code","f572f02a":"code","9fa9ce29":"code","426cd04a":"code","ade45e3e":"code","0bb86d2a":"code","5ac7e588":"code","eee908a9":"code","12dbc402":"code","cf4b5f93":"code","bbfb43b8":"code","4171ceb6":"code","f4fb9d4b":"code","648c349d":"code","ced0d577":"code","b1cdcf42":"code","49cccf16":"code","41f79557":"code","03992ac0":"markdown","403658d2":"markdown","708d6f99":"markdown","67753222":"markdown","557e084a":"markdown","0e4367af":"markdown","cf898016":"markdown","aaad1bc7":"markdown","9ec93956":"markdown","b9cb2f35":"markdown","f12b66f4":"markdown","2c6ae2e2":"markdown","6984b180":"markdown","c2f33663":"markdown","a16bc8d9":"markdown","91ea4c96":"markdown","dfa03bbc":"markdown","6168acfa":"markdown","e3747366":"markdown","9ca16137":"markdown","51911484":"markdown","fc09a771":"markdown","a6e84074":"markdown","136a30b2":"markdown","94a99966":"markdown"},"source":{"53754c3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","492556bb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import r2_score","7e3be7d1":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","0199378c":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","b8f48508":"train.head()","005df575":"# Save the Id \ntrain_id = train['Id']\ntest_id = test['Id']","22857bcf":"sns.jointplot(x='LotFrontage', y='SalePrice', data=train, kind='reg', )\nplt.grid(True)","0f40c624":"train = train.drop(train[(train['LotFrontage']>250) & (train['SalePrice']<400000)].index)\nsns.jointplot(x='LotFrontage', y='SalePrice', data=train, kind='reg', )\nplt.grid(True)","1ecc96b1":"sns.jointplot(x='LotArea', y='SalePrice', data=train, kind='reg' )","d2bde994":"train = train.drop(train[(train['LotArea']>150000) & (train['SalePrice']<300000)].index)\nsns.jointplot(x='LotArea', y='SalePrice', data=train, kind='reg' )\nplt.grid(True)","deb445e7":"sns.jointplot(x='BsmtFinSF1', y='SalePrice', data=train, kind='reg', )\nplt.grid(True)","e775a497":"train = train.drop(train[(train['BsmtFinSF1']>1000) & (train['SalePrice']>600000)].index)\nsns.jointplot(x='BsmtFinSF1', y='SalePrice', data=train, kind='reg')\nplt.grid(True)","3863cc4f":"sns.jointplot(x='WoodDeckSF', y='SalePrice', data=train, kind='reg')\nplt.grid(True)","e966089b":"train = train.drop(train[(train['WoodDeckSF']>0) & (train['SalePrice']>500000)].index)\nsns.jointplot(x='WoodDeckSF', y='SalePrice', data=train, kind='reg')\nplt.grid(True)","75f3f7a2":"sns.jointplot(x='OpenPorchSF', y='SalePrice', data=train, kind='reg')\nplt.grid(True)","6201e5de":"train = train.drop(train[((train['OpenPorchSF']>500) & (train['SalePrice']<300000)) | ((train['OpenPorchSF']<100) \n              & (train['SalePrice']>500000))].index)\nsns.jointplot(x='OpenPorchSF', y='SalePrice', data=train, kind='reg')\nplt.grid(True)","a17be742":"sns.distplot(train['SalePrice'], fit=norm)\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\nprint('\\n mean is {:.2f} and sigma is {:.2f} \\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","3d9d86e4":"train['SalePrice'] = np.log1p(train['SalePrice'])\n\nsns.distplot(train['SalePrice'], fit=norm)\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\nprint('\\n mean is {:.2f} and sigma is {:.2f} \\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","b9b813ae":"# Set 'Id' as Index for the Sake if Simplicity\ntrain = train.set_index('Id')\ntest = test.set_index('Id')","f576866d":"train_num = train.shape[0]\ntest_num = test.shape[0]","e35698b5":"df = pd.concat([train, test], axis=0)\ndf.head()","5cad3c33":"df['MSSubClass'] = df['MSSubClass'].astype('str')","9a97b7aa":"plt.figure(figsize=(10,6))\nsns.heatmap(df.isnull())","618e2b73":"df.isnull().sum()","dce0cdc4":"df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)","02235b68":"num_feat = df.select_dtypes(exclude='object').columns\ncat_feat = df.select_dtypes(include='object').columns\nnum_feat = num_feat[:-1]","89a4a53c":"num_feat","0512b564":"# Input mean value in Numerical Features\nfor col in num_feat:\n    df[col] = df[col].fillna(df[col].mean())\n    \n# Input mode value in Categorical Features\nfor col in cat_feat:\n    df[col] = df[col].fillna(df[col].mode()[0])","d01d2456":"# Handle Features contains year values\n\ndf['YrSold_YearBuilt'] = df['YrSold'] - df['YearBuilt']\ndf['GarageYrBlt'] = df['GarageYrBlt'].astype('str')\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype('str')\ndf.drop(['YrSold', 'YearBuilt'], axis=1, inplace=True)","6cb1e2ea":"train = df.iloc[:train_num, :]\ntest = df.iloc[train_num:, :]","57aa14a3":"# Plotting Heatmap \ncorrmat = train.corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(corrmat, vmin=0, vmax=1, cmap='coolwarm')","2bc7e41b":"#saleprice correlation matrix\nk = 15 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncf = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(16,10))\nhm = sns.heatmap(cf, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values,\n                 cmap='coolwarm', xticklabels=cols.values)","5d8fab0d":"train.drop(['GrLivArea', '1stFlrSF', 'OverallQual', 'GarageCars'], axis=1, inplace=True)\ntest.drop(['GrLivArea', '1stFlrSF', 'OverallQual', 'GarageCars'], axis=1, inplace=True)","f572f02a":"df = pd.concat([train, test], axis=0)\ndf.head()","9fa9ce29":"cat_feat = df.select_dtypes(include='object').columns\ncat_feat","426cd04a":"df_dummy = pd.get_dummies(df, columns=cat_feat, drop_first=True)","ade45e3e":"# Separate train and test set\ntrain = df_dummy.iloc[:train_num, :]\ntest = df_dummy.iloc[train_num:, :]\ntest.drop('SalePrice', axis=1, inplace=True)","0bb86d2a":"X = train.drop('SalePrice', axis=1)\ny = train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=51)","5ac7e588":"model1 = xgb.XGBRegressor()\nmodel1.fit(X_train, y_train)","eee908a9":"xgb_pred = model1.predict(X_test)\nr2_score(y_test, xgb_pred)","12dbc402":"# Input Parameter values: \n\nparam = {'max_depth': [3,5,6,8],\n        'learning_rate': [0.05, 0.1, 0.15, 0.25, 0.3],\n        'n_estimators': [100,200,300,500],\n        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        'min_child_weight': [1,3,5,7]}","cf4b5f93":"# Instantiate model\nregressor = xgb.XGBRegressor()\nrandom_search = RandomizedSearchCV(regressor, param_distributions=param, n_iter =5, scoring = 'neg_mean_squared_error',\n                                   n_jobs=-1, cv=5, verbose=2)","bbfb43b8":"random_search.fit(X_train, y_train)","4171ceb6":"random_search.best_estimator_","f4fb9d4b":"model2 = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=3,\n             min_child_weight=3, monotone_constraints='()',\n             n_estimators=300, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","648c349d":"model2.fit(X, y)","ced0d577":"test_pred = model2.predict(test)\ntest_pred = np.expm1(test_pred)","b1cdcf42":"test_pred_df = pd.DataFrame(test_pred, columns=['SalePrice'])\ntest_id_df = pd.DataFrame(test_id, columns=['Id'])","49cccf16":"submission = pd.concat([test_id_df, test_pred_df], axis=1)\nsubmission.head()","41f79557":"# Save the predictions\nsubmission.to_csv(r'submission.csv', index=False)","03992ac0":"\n## Model Parameters tuning with RandomizedSearchCV","403658d2":"### Predict SalePrices","708d6f99":"# Data Processing\n### Analyse the Data and remove Outliers","67753222":"### Let's check the missing values ","557e084a":"## Upvote this notebook if you like my work.","0e4367af":"### Inputing Missing Values","cf898016":"### Let's check features with high correlation only","aaad1bc7":"### Import Libraries","9ec93956":"### Fit these best estimators into the model","b9cb2f35":"# Problem Statement:\nWe have given several parameter which may affect the Price of house. A regression model has to build which can predict the price of house with a least error. ","f12b66f4":"### Convert Categorical features into Dummy Variables","2c6ae2e2":"### Drop features with with correlation value (more than 0.65)","6984b180":"### Make the Distribution Normal","c2f33663":"### Import Train and Test data","a16bc8d9":"### Transforming some Numerical Features into Categorical Featured","91ea4c96":"## Train Model 1","dfa03bbc":"### Find out the best parameter values:","6168acfa":"### Feature Correlation","e3747366":"### Convert into Dataframe for final submission","9ca16137":"# Steps to build the model.\nThese are the steps that you will see in this particular notebook required to build a regression model:\n1. Import the required libraries.\n2. Analyse the data and remove Outliers.\n3. Check the distribution of the target variable (make the distribution normal if it is not normal).\n4. Handle missing values.\n5. Handle features which contains year values.\n6. Drop features with high correlation (Use Heatmap).\n7. Convert categorical features into Dummy Variables.\n8. Train the model.\n9. Make Prediction of Prices of House","51911484":"### Check the distribusion of Prices","fc09a771":"# Now it's your turn\n# Give it a try :)","a6e84074":"### Dropping features with more than 50% Missing Values","136a30b2":"### Separate Numerical and Categorical Features","94a99966":"# Feature Engineering"}}