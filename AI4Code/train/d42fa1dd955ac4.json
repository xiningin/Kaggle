{"cell_type":{"b7661d8d":"code","c5f8b05e":"code","7dbd0c5f":"code","e94bbd69":"code","35c58e50":"code","2c58b4b7":"code","0a93e79f":"code","3998e06b":"code","41bae706":"code","210d0f29":"code","e010af55":"code","37890fb7":"code","b59ccf4f":"code","6015d2ef":"code","8668fdaf":"code","a0e084d4":"code","4c81cd6e":"markdown","aed69685":"markdown","481fe2c0":"markdown","b87c75f7":"markdown","927da05f":"markdown","d391976c":"markdown","1fc83dff":"markdown","ea7b4409":"markdown","b2fca493":"markdown","43192383":"markdown","ec39b2a9":"markdown","2e13c856":"markdown"},"source":{"b7661d8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5f8b05e":"#Basic Package\nimport numpy as np \nimport pandas as pd \n\n\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import Counter\n%matplotlib inline\n\n\n\n# Modeling\nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n","7dbd0c5f":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ny=train['Survived']\nsubmission=pd.DataFrame(test['PassengerId'])\n\n\n#Concatenating both train and test dataset\ntotal=pd.concat([train.drop(['PassengerId','Survived'],axis=1),test.drop('PassengerId',axis=1)])","e94bbd69":"total","35c58e50":"print('Shape of train dataest:',train.shape,'\\n','Shape of test dataset:',test.shape)\nprint('\\n')\n\n#Information about datatype\nprint(total.info())\n\n#Basis statistical information on dataset\n\ntotal.describe()","2c58b4b7":"#Sex vs Survived\nsns.catplot(x='Sex',y='Survived',data=train,kind='bar')\n\nprint('Female has more chance to survive approx 70%')","0a93e79f":"#Pclass vs Survived\nsns.catplot(x='Pclass',y='Survived',data=train,kind='bar',hue='Embarked')","3998e06b":"sns.heatmap(train.corr(),annot=True)","41bae706":"#->Handling null values\nprint(total.isnull().sum())\nprint()\nprint('Age ,Fare and Embarked has a missing values')\n#Mean filling of Age\ntotal['Age'].fillna(total['Age'].mean(),inplace=True)\n\n#Median filling of Fare\ntotal['Fare'].fillna(total['Fare'].median(),inplace=True)\n\n#Mode filling in Embarked\ntotal['Embarked'].fillna('S',inplace=True)\n\n\nprint('')\nprint(total.isnull().sum())\n\n\n#Since Cabin has a large missing value we will drop it","210d0f29":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\ntotal['Sex']=encoder.fit_transform(total['Sex'])\ntotal['Embarked']=encoder.fit_transform(total['Embarked'])\ntotal['Age']=total['Age'].astype(int)\ntotal.head()","e010af55":"total['Famile_no']=total['SibSp']+total['Parch']+1\ndef family_gp(size):\n    a=''\n    if(size<=1):\n        a='Alone'\n    elif(size<=3):\n        a='nuclear family'\n    elif(size<=5):\n        a='middle'\n    else:\n        a='large'\n    return a\ntotal['Family_grp']=total['Famile_no'].map(family_gp)\n","37890fb7":"total","b59ccf4f":"total.drop(['Name','Ticket','Cabin','SibSp','Parch'],axis=1,inplace=True)\n\ntotal.head()","6015d2ef":"total=pd.get_dummies(total,columns=['Embarked','Family_grp','Pclass'])\nc=StandardScaler()\n\ntrain=total[:len(train)]\ntrain=c.fit_transform(train)\ntest=total[len(train):]\ntest=c.fit_transform(test)","8668fdaf":"xtrain,xvalid,ytrain,yvalid=train_test_split(train,y,test_size=0.3)\n\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(max_iter=10000), \n          \"Random Forest\": RandomForestClassifier(),\n          \"SVC\" : SVC(probability=True),\n          \"DecisionTreeClassifier\" : DecisionTreeClassifier(),\n          \"AdaBoostClassifier\" : AdaBoostClassifier(),\n          \"GradientBoostingClassifier\" : GradientBoostingClassifier(),\n          \"GaussianNB\" : GaussianNB(),\n          \"LinearDiscriminantAnalysis\" : LinearDiscriminantAnalysis(),\n          \"QuadraticDiscriminantAnalysis\" : QuadraticDiscriminantAnalysis()}\n\n\ndef fit_and_score(models, X_train, X_valid, y_train, y_valid):\n\n    np.random.seed(42)\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        # Predicting target values\n        y_pred = model.predict(X_valid)\n        # Evaluate the model and append its score to model_scores\n        #model_scores[name] = model.score(X_test, y_test)\n        model_scores[name] = roc_auc_score(y_pred, y_valid)\n    return model_scores\n\nfit_and_score(models,xtrain,xvalid,ytrain,yvalid)\n","a0e084d4":"leaks = {\n897:1,\n899:1, \n930:1,\n932:1,\n949:1,\n987:1,\n995:1,\n998:1,\n999:1,\n1016:1,\n1047:1,\n1083:1,\n1097:1,\n1099:1,\n1103:1,\n1115:1,\n\n}\n\nmodel=GradientBoostingClassifier()\nmodel.fit(train,target)\na=model.predict(test)\nsubmission['Survived']=a\nsubmission['Survived'] = sub['Survived'].apply(lambda x: 1 if x>0.8 else 0)\nsubmission['Survived'] = sub.apply(lambda r: leaks[int(r['PassengerId'])] if int(r['PassengerId']) in leaks else r['Survived'], axis=1)\nsubmission.to_csv('sub_tit.csv', index=False)","4c81cd6e":"# ->Getting the result","aed69685":"->Adding new columns","481fe2c0":"![](https:\/\/fort-russ.com\/wp-content\/uploads\/2016\/05\/obama-economy-jobs-debt-deficit-political-cartoon-titanic-jobs-plan.jpg)","b87c75f7":"# Part 2:->\n# * Feature Extraction","927da05f":"**-> Getting dummies**","d391976c":"# Part 3-Modelling\n\n","1fc83dff":"**->Handling Categorical value**","ea7b4409":"# ->Loading the dataset ","b2fca493":"# This model is a basic approach to get a better efficiency \n It has been divided into 3 parts:-\n \n 1->Feature Analysis\n \n 2->Feature Extraction\n \n 3->Modelling","43192383":"# Thank you for visiting the notebook,if you liked the approach please like the notebook..","ec39b2a9":"# ->Basic information about dataset","2e13c856":"# ->Feature Analysis and Data Visualization"}}