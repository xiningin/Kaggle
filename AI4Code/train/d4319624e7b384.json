{"cell_type":{"5f257b0c":"code","a7eb9a9b":"code","1682d9af":"code","735a222a":"code","d98c745b":"code","6309bbc1":"code","796519f7":"code","1d5b1b23":"code","614f2360":"code","96a9350c":"code","72560966":"code","3d789fcc":"code","158c877b":"code","d9666e34":"code","49209035":"code","51a85be4":"code","5120e5ff":"code","852030ea":"code","a6539b11":"code","ff12053c":"code","d909524c":"code","6e47061e":"code","dcf16b05":"code","c93e4453":"code","6115b5c1":"code","762b8c16":"code","355b506f":"code","a2f5df69":"code","b5e9654d":"code","0c0da346":"code","5f066bae":"code","fa80f5b1":"code","99f93aad":"code","3ca20b4e":"code","b16ebb8f":"code","6a3972a1":"code","0f4fd925":"code","8975990b":"code","1ad7f3a6":"code","707a05c3":"code","4dfa0e44":"code","893d59a5":"code","1f074fa6":"code","85a14729":"code","e697db17":"code","522848ac":"code","fe00f120":"code","ab71c813":"code","a4c3c385":"code","b8b23743":"code","36d154b8":"code","040407e2":"code","60e32113":"code","a20106db":"code","000771dd":"code","ddb4b65f":"code","48c97c58":"code","0ba7a2d2":"code","e851cf41":"code","27aed8d7":"code","858703a9":"code","e615e5cf":"code","c3c402e5":"code","42853a5f":"code","111cae88":"code","62bd264e":"code","11425884":"code","bba27b8a":"code","ade74aef":"code","b37682e1":"code","a2cc6f29":"code","7452738b":"markdown","e05a6923":"markdown","90947d57":"markdown","ebd7d341":"markdown","669e9be4":"markdown","f12701bc":"markdown","2df4c655":"markdown","06978fae":"markdown","c714fe09":"markdown","74fd8f42":"markdown","c6b380a1":"markdown","0bd5a828":"markdown","e91ee244":"markdown","a7390095":"markdown","f274b042":"markdown","bf3426fe":"markdown","831bde4f":"markdown","09be26ed":"markdown","06eecdb1":"markdown","2f5019e0":"markdown","9e2f4753":"markdown","f957aa64":"markdown","042c163b":"markdown","04b655af":"markdown","5de8325f":"markdown","22d6886f":"markdown"},"source":{"5f257b0c":"import os\nprint(os.listdir(\"..\/input\"))","a7eb9a9b":"import pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Embedding, concatenate, Flatten, Input\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","1682d9af":"df = pd.read_csv(\"..\/input\/bank-full-1540270465813.csv\",na_values=[\"NA\",\" \",\"\",\"?\"],delimiter = ';' )","735a222a":"df.shape","d98c745b":"df.head()","6309bbc1":"df.poutcome.value_counts()","796519f7":"df.y[df.poutcome == 'success'].value_counts()","1d5b1b23":"df.y[df.duration > 319].value_counts().value_counts()","614f2360":"df.pdays[df.pdays== -1].sum()","96a9350c":"df.describe(include='all')","72560966":"df.dtypes","3d789fcc":"df.isna().sum()","158c877b":"cat_cols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\",\"y\"]","d9666e34":"num_attr = ['age', 'balance', 'day', 'duration', 'campaign','pdays','previous']","49209035":"for i in cat_cols:\n    df[i] = df[i].astype(\"category\")","51a85be4":"df.dtypes","5120e5ff":"df.describe(include = \"all\")","852030ea":"df.isnull().sum()","a6539b11":"names = df.columns","ff12053c":"df.head()","d909524c":"df.dtypes","6e47061e":"from sklearn.preprocessing import LabelEncoder","dcf16b05":"LE = LabelEncoder()\nfor i in cat_cols:\n    df[i] = LE.fit_transform(df[i])  ","c93e4453":"for i in num_attr:\n    df[i] = df[i].astype(\"int64\")","6115b5c1":"df.dtypes","762b8c16":"num_ind_attr = df[num_attr]","355b506f":"num_ind_attr.head()","a2f5df69":"scaled_num_ind_attr = num_ind_attr.values","b5e9654d":"scaled_num_ind_attr","0c0da346":"cat_attr_names = cat_cols","5f066bae":"cat_ind_attr_names = cat_attr_names\n\nprint(cat_ind_attr_names)\n\ncat_tar_attr_names = ['y']","fa80f5b1":"df.y.shape","99f93aad":"cat_ind_attr = pd.get_dummies(df[cat_ind_attr_names]).values","3ca20b4e":"cat_ind_attr","b16ebb8f":"cat_ind_attr.shape","6a3972a1":"df[cat_tar_attr_names] = df[cat_tar_attr_names].astype('category')","0f4fd925":"cat_tar_attr = pd.get_dummies(df[cat_tar_attr_names]).values","8975990b":"cat_tar_attr","1ad7f3a6":"job_attr = df.job.values\nmarital_attr = df.marital.values\neducation_attr = df.education.values\ndefault_attr = df.default.values \nhousing_attr = df.housing.values \nloan_attr = df.loan.values \ncontact_attr = df.contact.values \nmonth_attr = df.month.values \npoutcome_attr = df.poutcome.values \n","707a05c3":"job_levels = np.size(np.unique(job_attr, return_counts=True)[0])\nmarital_levels = np.size(np.unique(marital_attr, return_counts=True)[0])\neducation_levels = np.size(np.unique(education_attr, return_counts=True)[0])\ndefault_levels = np.size(np.unique(default_attr, return_counts=True)[0])\nhousing_levels = np.size(np.unique(housing_attr, return_counts=True)[0])\nloan_levels = np.size(np.unique(loan_attr, return_counts=True)[0])\ncontact_levels = np.size(np.unique(contact_attr, return_counts=True)[0])\nmonth_levels = np.size(np.unique(month_attr, return_counts=True)[0])\npoutcome_levels = np.size(np.unique(poutcome_attr, return_counts=True)[0])","4dfa0e44":"scaled_num_ind_attr_train, scaled_num_ind_attr_test, \\\ncat_ind_attr_train, cat_ind_attr_test, \\\nY_train, Y_test, \\\nY_trainS, Y_testS              = train_test_split(scaled_num_ind_attr,\n                                                         cat_ind_attr, \n                                                         cat_tar_attr,\n                                                         df.y,\n                                                         test_size=0.3, random_state=123) \n","893d59a5":"Y_train.shape","1f074fa6":"X_train = np.hstack((scaled_num_ind_attr_train, cat_ind_attr_train))\nX_test = np.hstack((scaled_num_ind_attr_test, cat_ind_attr_test))","85a14729":"from sklearn.svm import SVC","e697db17":"clf = SVC()\nclf.fit(X_train,Y_trainS)","522848ac":"clf.score(X_train,Y_trainS)","fe00f120":"clf.score(X_test,Y_testS)","ab71c813":"model = Sequential()\n##model.add(Dense(250, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(125, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(50, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","a4c3c385":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b8b23743":"model.summary()","36d154b8":"model.fit(X_train, Y_train, epochs=20, verbose=1,batch_size=80)","040407e2":"model.evaluate(X_test, Y_test, )","60e32113":"model.metrics_names","a20106db":"p = model.predict(X_test)\np[:50]","000771dd":"encoding_dim = 9 \nactual_dim = X_train.shape[1]","ddb4b65f":"input_img = Input(shape = (actual_dim,))\n\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim,activation='relu')(input_img)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(actual_dim,activation='sigmoid')(encoded)","48c97c58":"autoencoder = Model(input_img,decoded)\nprint(autoencoder.summary())","0ba7a2d2":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')","e851cf41":"autoencoder.fit(X_train, X_train, epochs=100, batch_size=80)","27aed8d7":"encoder = Model(input_img,encoded)\nprint(encoder.summary())","858703a9":"X_train_nonLinear_features = encoder.predict(X_train)\nX_test_nonLinear_features = encoder.predict(X_test)","e615e5cf":"X_train1=np.concatenate((X_train, X_train_nonLinear_features), axis=1)\nX_test1=np.concatenate((X_test, X_test_nonLinear_features), axis=1)","c3c402e5":"clf = SVC()\nclf.fit(X_train1,Y_trainS)","42853a5f":"clf.score(X_train1,Y_trainS)","111cae88":"clf.score(X_test1,Y_testS)","62bd264e":"model = Sequential()\n##model.add(Dense(250, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(8, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(4, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","11425884":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","bba27b8a":"model.summary()","ade74aef":"model.fit(X_train1, Y_train, epochs=20, verbose=1,batch_size=50)","b37682e1":"model.evaluate(X_test1, Y_test, )","a2cc6f29":"import matplotlib.pyplot as plt\n\nhistory = model.fit(X_train1, Y_train, validation_split=0.25, epochs=30, batch_size=80, verbose=1)\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","7452738b":"## Building a Perceptron with normal data","e05a6923":"Look at first few records","90947d57":"#### Dummification \n    \n    For first model, convert independetn categorical attributes to numeric using dummificcation\n    \n    Target categorical attribut has more than two level. For both the models, target categorical attribute is convert to numeric use dummification.","ebd7d341":"#### Select categorical attributes","669e9be4":"#### Understand the data","f12701bc":"#### Read the data","2df4c655":"#### Select only numeric independent attributes","06978fae":"# Categorical Variable Embeddings","c714fe09":"## Building a Autoencoder","74fd8f42":"Summary Statistics","c6b380a1":"Convert targeet categorical attribute to numeric using dummification","0bd5a828":"##### Following two cells are for explination purpose","e91ee244":"### Transforming all the catagorical attributes into numerical using LabelEncoder ","a7390095":"In this activity we will build two different models\n\n    For first model, categorical attributes are convered in to dummy numeric variables\n    For second model, categorical attributes not convered in to numberic. Each level is given unique number starting with zero and categorical embedding is used ","f274b042":"## Building a MLP over Encoded features","bf3426fe":"Convert independent categorical attribute to numeric using dummification ","831bde4f":"## Building a SVM on Encoded features","09be26ed":"Convert the attributes to appropriate type","06eecdb1":"## Now we can see the power of embaddings in Neural Network.","2f5019e0":"Summary statistics ","9e2f4753":"## Extracting encoded features","f957aa64":"## Checking for NULL or NA values","042c163b":"#### Load the requied libraries","04b655af":"## Type of all the attributes","5de8325f":"#### Missing value imputation","22d6886f":"### Build First Model"}}