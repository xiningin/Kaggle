{"cell_type":{"a92e483c":"code","6d9c9e33":"code","81ebb1b8":"code","41e1cad2":"code","158615a3":"code","07ef90d8":"code","a18c2f1b":"code","d6fce143":"code","00aaac21":"code","acbe673c":"code","cc9b5535":"code","b5c93b7c":"code","36818b54":"code","2898e8ed":"code","52462b38":"code","6e7de0f1":"code","0a0abb46":"code","a2d297ff":"code","65b18e80":"code","7821f8dc":"code","81bbd04f":"code","40e68d28":"code","28ee5733":"code","52a3a6ed":"code","4c0d9d70":"code","a03bbd6b":"code","63964111":"code","48d2db96":"markdown","12a52b43":"markdown","f71c02ee":"markdown","3fd9a978":"markdown","6e27c035":"markdown","4d091496":"markdown","68a347ad":"markdown","481b645c":"markdown","4d32e4f9":"markdown","154f2d3a":"markdown","6053d3ff":"markdown"},"source":{"a92e483c":"from PIL import Image\nimport glob\nimport cv2 \nimport numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torchvision.utils as vutils\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport random\nimport os\nfrom os import listdir\nfrom os.path import isfile, join","6d9c9e33":"USE_CUDA = torch.cuda.is_available()\n\nprint(\"device : {0}\".format(\"cuda\" if USE_CUDA else \"cpu\"))\n\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nPATH = '..\/input\/rmxpcp-rpg-maker-xp-character-dataset'\nPATH_b = '..\/input\/rmxpcptypeb'\n\nbatch_size = 3\nlrG = 0.0002\nlrR = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 45\nL2lambda = 5\n\nrecognizer_epochs = 1\nall_epochs = 4","81ebb1b8":"transform = albumentations.Compose([\n    albumentations.Resize(256, 256), \n    albumentations.pytorch.transforms.ToTensor(),\n])","41e1cad2":"onlyfiles = [file for file in glob.glob(PATH+\"\/train\/*\") if file.endswith('.png')][:15000]\nr_train_images = np.empty(len(onlyfiles), dtype=object)\nfor n, path in enumerate(onlyfiles):\n  r_train_images[n] = path\nprint(\"The number of Recognizer Train Dataset : {0}\".format(len(r_train_images)))\n\nonlyfiles = [file for file in glob.glob(PATH+\"\/train\/*\") if file.endswith('.png')][15000:]\ntrain_images = np.empty(len(onlyfiles), dtype=object)\nfor n, path in enumerate(onlyfiles):\n  train_images[n] = path\nprint(\"The number of Train Dataset : {0}\".format(len(train_images)))\n\nonlyfiles = [file for file in glob.glob(PATH+\"\/test\/*\") if file.endswith('.png')]\ntest_images = np.empty(len(onlyfiles), dtype=object)\nfor n, path in enumerate(onlyfiles):\n  test_images[n] = path\nprint(\"The number of TypeA Test Dataset : {0}\".format(len(test_images)))\n\nonlyfiles = [file for file in glob.glob(PATH+\"\/val\/*\") if file.endswith('.png')]\nval_images = np.empty(len(onlyfiles), dtype=object)\nfor n, path in enumerate(onlyfiles):\n  val_images[n] = path\nprint(\"The number of Val Dataset : {0}\".format(len(val_images)))\n\nonlyfiles = [file for file in glob.glob(PATH_b+\"\/*\") if file.endswith('.png')]\ntypeb_images = np.empty(len(onlyfiles), dtype=object)\nfor n, path in enumerate(onlyfiles):\n  typeb_images[n] = path\nprint(\"The number of TypeB Test Dataset : {0}\".format(len(typeb_images)))","158615a3":"class CustomDataset(Dataset):\n\n    def __init__(self, file, transform=None):\n        self.file = file\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.file)\n    \n\n    def removeBackgroundTensor(self, img, pixpos=(0, 0)):\n        pix = img[:,pixpos[0],pixpos[1]]\n        img[:, :, :][(img[0,:,:] == pix[0]) & (img[1,:,:] == pix[1]) & (img[2,:,:] == pix[2])] = torch.tensor([0, 0, 0, -1.0])\n        return img\n        \n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.file[idx])\n        sketch_image = cv2.cvtColor(img[:,:256,:], cv2.COLOR_BGR2RGBA)\n        real_image = cv2.cvtColor(img[:,256:,:], cv2.COLOR_BGR2RGBA)\n        \n        if self.transform:\n            augmented1 = self.transform(image=sketch_image) \n            image1 = augmented1['image']\n            augmented2 = self.transform(image= real_image) \n            image2 = augmented2['image']\n            \n            \n        image1 = self.removeBackgroundTensor(image1)\n        image2 = self.removeBackgroundTensor(image2)\n        \n        return image1, image2","07ef90d8":"r_train_dataset = CustomDataset(r_train_images, transform)\ntrain_dataset = CustomDataset(train_images, transform)\ntest_dataset = CustomDataset(test_images, transform)\nval_dataset = CustomDataset(val_images, transform)\ntypeb_dataset = CustomDataset(typeb_images, transform)","a18c2f1b":"r_train_dataloader = DataLoader(r_train_dataset, batch_size=5, shuffle=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=True)\ntypeb_dataloader = DataLoader(typeb_dataset, batch_size=5, shuffle=True)","d6fce143":"def save_tensor(tensor, path):\n    if len(tensor.shape) == 3 :\n        transforms.ToPILImage()(tensor.to(torch.device(\"cpu\"))).save(path)\n    else :\n        raise Exception(\"Tensor must be the structure of [channels, height, width]\")","00aaac21":"def check_image(img_tensor, wsplit_size, hsplit_size):\n    width = list(img_tensor.shape)[-1]\n    height = list(img_tensor.shape)[-2]\n    if not isinstance(wsplit_size, int):\n        return False\n    if not isinstance(hsplit_size, int):\n        return False\n    if width<wsplit_size or height<hsplit_size:\n        return False\n    if wsplit_size == 0 or hsplit_size == 0:\n        return False\n    if width%wsplit_size == 0 and height%hsplit_size == 0:\n        return True\n    else :\n        return False","acbe673c":"def splitImage(img_tensor, wsplit_size=64, hsplit_size=64):\n    if not check_image(img_tensor, wsplit_size, hsplit_size):\n        return None\n    else :\n        nn = list(img_tensor.shape)[-4]\n        cn = list(img_tensor.shape)[-3]\n        wn = list(img_tensor.shape)[-1]\/\/wsplit_size\n        hn = list(img_tensor.shape)[-2]\/\/hsplit_size\n\n        himgs = []\n        imgs = []\n        for wi in range(1, wn+1):\n            himgs.append(img_tensor[:,:,:,(wi-1)*wsplit_size:(wi)*wsplit_size].unsqueeze(0))\n        for himg in himgs:\n            for hi in range(1, hn+1):\n                imgs.append(himg[:,:,:,(hi-1)*hsplit_size:(hi)*hsplit_size,:])\n            \n        tarlist = []\n        for ind in range(16):\n            tarlist += [ind]*nn\n            \n        return torch.cat(imgs, dim=1).squeeze(), torch.tensor(tarlist)","cc9b5535":"for x, y in train_dataloader:\n    pic = y\n    \n    #save_tensor(pic[0], '.\/test.png')\n\n    pic = np.transpose(pic.detach().numpy()[0], (1, 2, 0))\n    plt.imshow(pic)\n    plt.show()\n    \n    print(y.shape)\n    img, tar = splitImage(y)\n    \n    i = random.randrange(1,15)\n    plt.imshow(np.transpose(img[i].unsqueeze(0).detach().numpy()[0], (1, 2, 0)))\n    plt.show()\n    print(tar[i])\n    break\n    \n    ","b5c93b7c":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.relu = nn.ReLU(False)\n\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out","36818b54":"class Generator(nn.Module):    \n    def __init__(self, ngf=64):\n        super(Generator, self).__init__()\n        self.in_planes=ngf\n\n        self.conv1 = nn.Conv2d(4, ngf, kernel_size=4, stride=2, padding=1) \n\n        self.conv2 = nn.Conv2d(ngf, ngf*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(ngf*2) \n        self.conv3 = nn.Conv2d(ngf*2, ngf*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(ngf*4) \n\n        self.conv4 = nn.Conv2d(ngf*4, ngf*6, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(ngf*6) \n        self.conv5 = nn.Conv2d(ngf*6, ngf*6, 3, 1, 1)\n        self.conv5_bn = nn.BatchNorm2d(ngf*6) \n        \n        self.conv6 = nn.Conv2d(ngf*6, ngf*8, 4, 2, 1) \n        self.conv6_bn = nn.BatchNorm2d(ngf*8) \n        self.conv7 = nn.Conv2d(ngf*8, ngf*8, 3, 1, 1)\n        self.conv7_bn = nn.BatchNorm2d(ngf*8) \n\n        self.conv8 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1)\n        self.conv8_bn = nn.BatchNorm2d(ngf*8) \n        self.conv9 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1)\n        self.conv9_bn = nn.BatchNorm2d(ngf*8) \n\n        self.conv10 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1) \n        self.conv10_bn = nn.BatchNorm2d(ngf*8) \n\n        self.deconv0 = nn.ConvTranspose2d(ngf*8, ngf*8, 4, 2, 1) \n        self.deconv0_bn = nn.BatchNorm2d(ngf*8) \n        self.deconv1 = nn.ConvTranspose2d(ngf*8*2, ngf*8, 4, 2, 1) \n        self.deconv1_bn = nn.BatchNorm2d(ngf*8) \n        \n        self.deconv2 = nn.ConvTranspose2d(ngf*8*2, ngf*4*2, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(ngf*8) \n        self.deconv3 = nn.ConvTranspose2d(ngf*8*2, ngf*8, 3, 1, 1)\n        self.deconv3_bn = nn.BatchNorm2d(ngf*8) \n        \n        self.deconv4 = nn.ConvTranspose2d(ngf*8*2, ngf*6, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(ngf*6) \n        self.deconv5 = nn.ConvTranspose2d(ngf*6*2, ngf*6, 3, 1, 1) \n        self.deconv5_bn = nn.BatchNorm2d(ngf*6) \n        \n        self.deconv6 = nn.ConvTranspose2d(ngf*6*2, ngf*4, 4, 2, 1) \n        self.deconv6_bn = nn.BatchNorm2d(ngf*4) \n        self.deconv7 = nn.ConvTranspose2d(ngf*4*2, ngf*2, 4, 2, 1) \n        self.deconv7_bn = nn.BatchNorm2d(ngf*2) \n        \n        self.deconv8 = nn.ConvTranspose2d(ngf*2*2, ngf, 4, 2, 1)\n        self.deconv8_bn = nn.BatchNorm2d(ngf) \n        \n        self.deconv9 = nn.ConvTranspose2d(ngf*2, 4, 4, 2, 1) \n\n        self.leaky = nn.LeakyReLU(0.2)\n        self.relu = nn.ReLU(True)\n        self.drop = nn.Dropout(0.5)\n    \n    def conv2d_make_layer(self, planes, num_blocks, kernel_size, stride, padding):\n        strides = [stride] + [2]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(BasicBlock(self.in_planes, planes, kernel_size, stride, padding))\n            self.in_planes = planes\n        return tuple(layers)\n    \n    def tconv2d_make_layer(self, planes, num_blocks, kernel_size, stride, padding):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(BasicBlock(self.in_planes, planes, kernel_size, stride, padding))\n            self.in_planes = planes\n        return tuple(layers)\n    \n    \n\n    def forward(self, input):\n        x1 = self.conv1(input) \n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)    \n        x6 = self.conv6(x5)    \n        x7 = self.conv7(x6)\n        x8 = self.conv8(x7)\n        x9 = self.conv9(x8)\n        \n        x10 = self.leaky(x9)\n        x10 = self.conv10(x10)\n        \n        y1 = self.relu(x10)\n        y1 = self.deconv0(y1)\n        y1 = self.deconv0_bn(y1)\n        y1 = self.drop(y1)\n        y1 = torch.cat([y1,x9], dim=1)\n\n        y2 = self.relu(y1)\n        y2 = self.deconv1(y2)\n        y2 = self.deconv1_bn(y2)\n        y2 = self.drop(y2)\n        y2 = torch.cat([y2,x8], dim=1)\n\n        y3 = self.relu(y2)\n        y3 = self.deconv2(y3)\n        y3 = self.deconv2_bn(y3)\n        y3 = self.drop(y3)\n        y3 = torch.cat([y3,x7], dim=1)\n\n        y4 = self.relu(y3)\n        y4 = self.deconv3(y4)\n        y4 = self.deconv3_bn(y4)\n        y4 = torch.cat([y4,x6], dim=1)\n\n        y5 = self.relu(y4)\n        y5 = self.deconv4(y5)\n        y5 = self.deconv4_bn(y5)\n        y5 = torch.cat([y5,x5], dim=1)\n\n        y6 = self.relu(y5)\n        y6 = self.deconv5(y6)\n        y6 = self.deconv5_bn(y6)\n        y6 = torch.cat([y6,x4], dim=1)\n        \n        y7 = self.relu(y6)\n        y7 = self.deconv6(y7)\n        y7 = self.deconv6_bn(y7)\n        y7 = torch.cat([y7,x3], dim=1)\n\n        y8 = self.relu(y7)\n        y8 = self.deconv7(y8)\n        y8 = self.deconv7_bn(y8)\n        y8 = torch.cat([y8,x2], dim=1)\n\n        y9 = self.relu(y8)\n        y9 = self.deconv8(y9)\n        y9 = self.deconv8_bn(y9)\n        y9 = torch.cat([y9,x1], dim=1)\n        \n        y10 = self.relu(y9)\n        y10 = self.deconv9(y10)\n\n        output = nn.Tanh()(y10)\n        \n        return output","2898e8ed":"class ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=16):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(4, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n        self.relu = nn.ReLU(False)\n        self.avg_pool2d = nn.AvgPool2d(8, 8)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool2d(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out","52462b38":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","6e7de0f1":"def testpic(dataset, amount=1, size=(256, 256), save=False):\n    netG.eval()    \n    fig = plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n\n    num = 0\n    for i, data in enumerate(DataLoader(dataset, batch_size=1, shuffle=False)):\n        if(num>=amount) :\n            break\n        num+=1\n        \n        x = data[0].to(device)\n        y = data[1].to(device)\n\n        pic = netG(x).to(cpu_device)\n        \n        if save == True:\n            save_tensor(pic[0], '.\/test-{}.png'.format(i))\n        \n        pic = np.transpose(pic.detach().numpy()[0], (1, 2, 0))\n        \n        \n        pic = cv2.resize(pic, dsize=size, interpolation=cv2.INTER_CUBIC)\n        pic = cv2.cvtColor(pic, cv2.COLOR_BGR2RGB)\n        \n        y = y.to(cpu_device)\n        y = np.transpose(y.detach().numpy()[0], (1, 2, 0))\n        y = cv2.resize(y, dsize=size, interpolation=cv2.INTER_CUBIC)\n        y = cv2.cvtColor(y, cv2.COLOR_BGR2RGB)\n\n        \n        fig = plt.figure()\n        rows = 1\n        cols = 2\n        \n        ax1 = fig.add_subplot(rows, cols, 1)\n        ax1.imshow(cv2.cvtColor(pic, cv2.COLOR_BGR2RGB))\n        ax1.set_title('G(X)')\n        ax1.axis(\"off\")\n\n        ax2 = fig.add_subplot(rows, cols, 2)\n        ax2.imshow(cv2.cvtColor(y, cv2.COLOR_BGR2RGB))\n        ax2.set_title('Y')\n        ax2.axis(\"off\")\n\n        plt.show()\n        \n        \n\n        \n    netG.eval()","0a0abb46":"def accurate_classification(net, testloader, prefix=\"\"):\n    correct = 0\n    total = 0\n    net = net.to(device)\n    net.eval()\n    with torch.no_grad():\n        for data in testloader:\n            data = splitImage(data[1])\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    if prefix != None and prefix != \"\" :        \n        print('%s : %f %%' % (prefix, 100 * correct \/ total))\n    \n    return 100 * correct \/ total","a2d297ff":"netG = Generator().to(device)\nnetG.apply(weights_init)\n\nnetR = ResNet(BasicBlock, [3, 4, 6, 3]).to(device)\nnetR.apply(weights_init)","65b18e80":"img_list = []\nG_loss = []\nR_loss = []\n\noptimizerR = Adam(netR.parameters(), lr=lrR, betas=(beta1, beta2))\noptimizerG = Adam(netG.parameters(), lr=lrG, betas=(beta1, beta2))","7821f8dc":"def train_r(epoch, max_epoch, loader, net, optimizer):   \n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    criterion = nn.CrossEntropyLoss()\n\n    print('\\nEpoch: %d' % epoch)\n    for batch_idx, (inputs, targets) in enumerate(loader):\n        inputs, targets = splitImage(targets)\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n\n        print('[%d\/%d][%d\/%d]\\tLoss: %.4f'% (epoch, max_epoch, batch_idx, len(loader), train_loss\/(batch_idx+1)))","81bbd04f":"def fit(num_epochs=1000):\n  print(\"Starting Training Loop...\")\n  iters = 0\n  for epoch in range(num_epochs):\n    print(f\"EPOCH{epoch+1}:\")\n    train_one_epoch(train_dataloader, netG, netR, optimizerG, optimizerR, epoch, num_epochs)\n\n\ndef train_one_epoch(dataloader, netG, netR, optimizerG, optimizerR, epoch, num_epochs, iters=0):\n    with torch.autograd.set_detect_anomaly(True):\n      for i, data in enumerate(dataloader):        \n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            #Train Recognizer\n            netG.eval()\n            netR.train()\n            \n            netR.zero_grad()\n            \n            inputs, targets = splitImage(real)\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = netR(inputs)\n            errR = nn.CrossEntropyLoss()(outputs, targets)\n            errR.backward()\n            optimizerR.step()\n        \n            #Train Generator\n            netG.train()\n            netR.eval()\n            \n            netG.zero_grad()\n            G_output = netG(sketch)\n            G_output[:, :, :, :][G_output[:,3,:,:] == -1.0] = torch.tensor([0, 0, 0, -1.0])\n            \n            inputs, targets = splitImage(G_output)\n            inputs, targets = inputs.to(device), targets.to(device)\n            R_output = netR(inputs)\n            errG = nn.L1Loss()(G_output, real)*L1lambda + nn.CrossEntropyLoss()(R_output, targets)*L2lambda\n            errG.backward()\n            optimizerG.step()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_R: %.4f\\tLoss_G: %.4f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errR.item(), errG.item()))\n\n            G_loss.append(errG.item())\n            R_loss.append(errR.item())\n\n            if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n                with torch.no_grad():\n                    fake = netG(sketch).detach().cpu()\n                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n            iters += 1\n","40e68d28":"netR.train()\nfor ind in range(1, recognizer_epochs+1):\n    train_r(ind, recognizer_epochs, r_train_dataloader, netR, optimizerR)\nnetR.eval()","28ee5733":"accurate_classification(netR, test_dataloader, prefix='PreTrained Recognizer Accurate : ')","52a3a6ed":"netG.train()\nnetR.train()\n\nfit(num_epochs=all_epochs)\n\nnetG.eval()\nnetR.eval()","4c0d9d70":"testpic(test_dataset, amount=20)","a03bbd6b":"testpic(typeb_dataset)","63964111":"torch.save(netG.state_dict(), 'Generator.pth')\ntorch.save(netR.state_dict(), 'Recognizer.pth')","48d2db96":"## Define Net","12a52b43":"## PreTrain R","f71c02ee":"## Introduction\n---------------------\nStepNet is an artifical network used in generating sequence image datas like character walking sprites, motion videos or animations.\n\nGenerator **generates images**. And Recognizer **identifies indexs of images** which is splited from output of generator.","3fd9a978":"## Train G & R","6e27c035":"## Define functions used for training\n---------------------------------\nx is a input of dataset.\n\ny is a target of dataset.\n\nN is the number of step.\n\n### (1) Generator Loss\n**label_i** and **G(x)_i** is label and input which is splited based on step from the image generated by Generator.\n\n$${\\mathcal{L}}_{\\boldsymbol{G}}\\boldsymbol{(}\\boldsymbol{G}\\boldsymbol{,\\ }\\boldsymbol{R}\\boldsymbol{)=\\ }{\\boldsymbol{\\lambda }}_{\\boldsymbol{1}}{\\boldsymbol{E}}_{\\boldsymbol{x,y}}\\left[{\\boldsymbol{||y-G(}\\boldsymbol{x}\\boldsymbol{)}\\boldsymbol{||}}_{\\boldsymbol{1}}\\right] + \\sum_{i = 1}^{N}{\\mathbb{E}}_{{\\boldsymbol{label}}_{\\boldsymbol{i}}}\\boldsymbol{[}\\boldsymbol{\\mathrm{ln}}\\boldsymbol{\\mathrm{}}\\boldsymbol{(}\\boldsymbol{R}\\boldsymbol{(}\\boldsymbol{G}\\boldsymbol{(}{\\boldsymbol{x}}\\boldsymbol{)}_{\\boldsymbol{i}}\\boldsymbol{))]}$$\n\n\n\n### (2) Recognizer Loss\n**label_i** and **y_i** is label and input which is splited based on step from the image that is target of dataset.\n\n$${\\mathcal{L}}_{\\boldsymbol{R}}\\boldsymbol{(}\\boldsymbol{G}\\boldsymbol{,\\ }\\boldsymbol{R}\\boldsymbol{)=\\ }\n\\sum_{i = 1}^{N}{\\mathbb{E}}_{{\\boldsymbol{label}}_{\\boldsymbol{i}}}\\boldsymbol{[}\\boldsymbol{\\mathrm{ln}}\\boldsymbol{\\mathrm{}}\\boldsymbol{(}\\boldsymbol{R}\\boldsymbol{(}{\\boldsymbol{y}}_{\\boldsymbol{i}}\\boldsymbol{))]}$$\n\n","4d091496":"### Generator(UNet)","68a347ad":"## Import Library","481b645c":"### Recognizer(ResNet)","4d32e4f9":"## Prepare Data","154f2d3a":"### BasicBlock","6053d3ff":"# StepNet\n"}}