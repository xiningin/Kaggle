{"cell_type":{"fe6eec5d":"code","ffe127bd":"code","f42906f5":"code","56d807d0":"code","28e27109":"code","c722938d":"code","41f0299e":"code","98b264d5":"code","86250ea8":"code","980606c1":"code","825f1f04":"code","cd46ba82":"code","97914f94":"code","a818ae57":"code","c54f6f47":"code","2b3f873d":"code","4952cacf":"code","ca88425f":"code","ac2c7185":"code","209b4c24":"code","2d12ea9d":"code","da97d708":"code","88900395":"code","75843273":"code","76bdb8d2":"code","41206521":"code","2d386d65":"code","a48b09fe":"code","889c8db0":"code","4e1cd73e":"code","9c4d6dfa":"code","3ab912a5":"code","c06d792e":"code","ac6820b9":"code","3c9acfc8":"code","bcbf3bf3":"code","c547ea8c":"code","c3b8f97f":"code","6b935825":"code","f71d3e5a":"code","49eb094c":"code","cc96b501":"markdown","d12c71ba":"markdown","e8cc0968":"markdown","ad280657":"markdown","9dddbf17":"markdown","ca82d5ed":"markdown","bcd9ecf8":"markdown","c5cdfb3d":"markdown","84861bf0":"markdown","56c78e7d":"markdown","0103f5d6":"markdown"},"source":{"fe6eec5d":"#  Importing Important Packages","ffe127bd":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk import ngrams\n# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report","f42906f5":"data=pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')","56d807d0":"## Getting the number of words by splitting them by a space\nwords_per_review = data.Review.apply(lambda x: len(x.split(\" \")))\nwords_per_review.hist(bins = 100)\nplt.xlabel('Review Length (words)')\nplt.ylabel('Frequency')\nplt.show()","28e27109":"data.shape\n","c722938d":"data.head(5)","41f0299e":"percent_val = 100 * data['Rating'].value_counts()\/len(data)\npercent_val","98b264d5":"percent_val.plot.bar()\nplt.show()","86250ea8":"data['sentiment'] = np.where(data.Rating >= 3,1,0)    \n# Mapping the ratings\ndata['sentiment'] = np.where(data.Rating > 3,1,0)\n\n## Removing neutral reviews \ndata = data[data.Rating != 3]\n","980606c1":"#Edits After Removing Stopwords\nEdited_Review = data['Review'].copy()","825f1f04":"data.shape","cd46ba82":"data.head(10)","97914f94":"data['reviews_text_new'] = data['Review'].str.lower()\n","a818ae57":"# For reviews converted to lower case\ntoken_lists_lower = [word_tokenize(each) for each in data['reviews_text_new']]\ntokens_lower = [item for sublist in token_lists_lower for item in sublist]\nprint(\"Number of unique tokens now: \",len(set(tokens_lower)))","c54f6f47":"### Selecting non alpha numeric charactes that are not spaces\nspl_chars = data['reviews_text_new'].apply(lambda review: \n                                                     [char for char in list(review) if not char.isalnum() and char != ' '])\n\n## Getting list of list into a single list\nflat_list = [item for sublist in spl_chars for item in sublist]","2b3f873d":"review_backup = data['reviews_text_new'].copy()\ndata['reviews_text_new'] = data['reviews_text_new'].str.replace(r'[^A-Za-z0-9]+', ' ')","4952cacf":"noise_words = []\neng_stop_words = stopwords.words('english')","ca88425f":"stop_words = set(eng_stop_words)\nwithout_stop_words = []\nstopword = []\nsentence = data['reviews_text_new'][3]\nwords = nltk.word_tokenize(sentence)\n\nfor word in words:\n    if word in stop_words:\n        stopword.append(word)\n    else:\n        without_stop_words.append(word)\n\nprint('-- Original Sentence --\\n', sentence)\nprint('\\n-- Stopwords in the sentence --\\n', stopword)\nprint('\\n-- Non-stopwords in the sentence --\\n', without_stop_words)","ac2c7185":"def stopwords_removal(stop_words, sentence):\n    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]\n\ndata['reviews_text_nonstop'] = data['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))\ndata[['reviews_text_new','reviews_text_nonstop']]","209b4c24":"def make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","2d12ea9d":"# Converting all the texts back to sentences\nmake_sentences(data,'reviews_text_nonstop')","da97d708":"data .head(5)","88900395":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndata[\"After_lemmatization\"] = data['reviews_text_nonstop'].apply(lambda text: lemmatize_words(text))","75843273":"data.head(6)","76bdb8d2":"print(\"- Old Review -\")\nprint(data['Review'][3])\nprint(\"\\n- New Review -\")\nprint(data['reviews_text_nonstop'][3])\nprint(\"\\n- Last Edit Review -\")\nprint(data['After_lemmatization'][3])","41206521":"data['reviews_text_final'] = data['After_lemmatization'].copy()","2d386d65":"data[['reviews_text_final','sentiment']].head(5)","a48b09fe":"### Changes with respect to the previous code\n### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)\n### 2. Including the stopwords in the bag of words features\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize,\n                             lowercase=True,\n                             ngram_range=(1,1))\n\nbow_data = bow_counts.fit_transform(data.reviews_text_new)\n","889c8db0":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n                                                                    data['sentiment'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0)","4e1cd73e":"# Defining and training the model\nlr_model_all_new = LogisticRegression(max_iter = 200)\nlr_model_all_new.fit(X_train_bow, y_train_bow)\n\n# Predicting the results\ntest_pred_lr_all = lr_model_all_new.predict(X_test_bow)\n\n\n## Calculate key performance metrics\n\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","9c4d6dfa":"lr_model_all_new.feature_names=bow_counts.get_feature_names()","3ab912a5":"from joblib import dump, load \n\n# save model to file \ndump(lr_model_all_new, filename=\"Sentiment_Analysis_unigram2.joblib\")","c06d792e":"# import a saved joblib model \nloaded_joblib_model = load(filename=\"Sentiment_Analysis_unigram2.joblib\")","ac6820b9":"len(loaded_joblib_model.feature_names)","3c9acfc8":"import string\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfeats = bow_counts.get_feature_names()\nfeats_len = len(feats)\nsent ='My stay was extremely comfortable. A beautiful hotel surrounded by wonderful staff in a great location.'\nsent =sent.lower()\nsent = sent.translate(str.maketrans('', '', string.punctuation))\nfiltered_sentence = [] \nstop_words = set(stopwords.words('english')) \nword_tokens =word_tokenize(sent)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words ]\nlistToStr = ' '.join(map(str, filtered_sentence))\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(word_tokenize(text))\n    return ([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\nlemmatized_output =[]\nlemmatized_output = lemmatize_words(listToStr)\n\nsent_features=[]\nsent_dict = {}\nfor word in lemmatized_output:\n    if not word in sent_dict:\n        sent_dict[word] = 0\n    sent_dict[word] = sent_dict[word] + 1\nfor i in range(feats_len):\n    if not feats[i] in sent_dict:\n        sent_features.append(0)\n    else:\n        sent_features.append(sent_dict[feats[i]])","bcbf3bf3":"sent ='The condition of the rooms were very bad. Bed sheets, linens were dirty.'\nsent =sent.lower()\nsent = sent.translate(str.maketrans('', '', string.punctuation))\nfiltered_sentence = [] \nstop_words = set(stopwords.words('english')) \nword_tokens =word_tokenize(sent)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words ]\nlistToStr = ' '.join(map(str, filtered_sentence))\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(word_tokenize(text))\n    return ([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\nlemmatized_output =[]\nlemmatized_output = lemmatize_words(listToStr)\nprint(lemmatized_output)","c547ea8c":"len(feats)","c3b8f97f":"\ndf = pd.DataFrame(feats, columns=[\"features\"])\ndf.to_csv('unigram.csv', index=False)","6b935825":"len(sent_features)","f71d3e5a":"joblib_y_preds = loaded_joblib_model.predict([sent_features])","49eb094c":"print(joblib_y_preds)","cc96b501":"# Having a look at 1st ten reviews in the data","d12c71ba":"# Building a machine learning model","e8cc0968":"# Applying logistic regression","ad280657":"# Divide into training and test sets:","9dddbf17":"# Results of Preprocessing data (Removing stopwords & Lemmatization)","ca82d5ed":"# Reading Data","bcd9ecf8":"# Preprocessing Function","c5cdfb3d":"# Making two copies of Reviews to edit","84861bf0":"# Bag-of-words and n-grams","56c78e7d":"# Lemmatization Function","0103f5d6":"1. Converting words to lower\/upper case\n2. Removing special characters\n3. Removing stopwords and high\/low-frequency words\n4. lemmatization"}}