{"cell_type":{"245dfa5a":"code","73968bb8":"code","24624dc0":"code","7f80a2c5":"code","f4fde9cc":"code","5da755e5":"code","2a4d5433":"code","01b0ad41":"code","7fb31687":"code","4a22a834":"code","ce72376f":"code","f77802cf":"code","a4a17af5":"code","4f27d70a":"code","15267f5c":"code","e12e12de":"code","bde366dd":"code","02113493":"code","6d9422b8":"code","8518d477":"code","8cbc068b":"code","ee11d25c":"code","0170af08":"code","1a4e96d6":"code","cf81df74":"code","ae593dca":"code","639a93bc":"code","dc5e69f0":"code","272fabcf":"code","61f6cbe8":"code","da15e8c8":"code","1c7e5029":"code","137ac496":"code","1589c329":"code","af88237e":"code","f980a951":"code","052260e7":"code","ab78b0e0":"code","032857ae":"code","f7b05b6d":"code","03d66089":"code","c5c9c669":"code","a03bb6b2":"code","fd7254ca":"code","043327cc":"code","27981d58":"code","6dd48c2c":"code","2c77247b":"code","90a801ea":"code","34ef4736":"code","a6b9d583":"code","b2fc8ba5":"code","4f9d06d6":"code","f0e782ff":"code","97c49e6f":"code","1b18f272":"code","e573d1ee":"code","2dbe0c2e":"code","c454f4ca":"code","bfda1a07":"code","6802ca8d":"code","4ca673d1":"code","cdf0ad7f":"code","05c1dec9":"code","8a60f445":"code","fbe03b00":"code","597f930b":"code","309d7844":"markdown","7c0da532":"markdown","9ab0cd15":"markdown","da6a80c6":"markdown","e1ad300f":"markdown","82cbd93d":"markdown","d303d0cf":"markdown","9048291a":"markdown","ccdfbbbe":"markdown","3b208d12":"markdown","7f645d29":"markdown","a0f9710b":"markdown","9c3d8603":"markdown","3b723f80":"markdown","8115050f":"markdown","14a9ebd2":"markdown","a1113f8d":"markdown","c0e40e11":"markdown","9ead77a1":"markdown","31f3bbb7":"markdown","e4e2ae02":"markdown","3411ccb8":"markdown","3fc8283e":"markdown","5fed1abc":"markdown","e6268bbc":"markdown","034792c1":"markdown","3aace80e":"markdown","d224b2f8":"markdown","39c60838":"markdown","f16fba7e":"markdown","1a4b4729":"markdown","755e807c":"markdown","4075aa56":"markdown","c852284a":"markdown","214c1bbe":"markdown","f2c00b18":"markdown","ab53dae1":"markdown","d6ae8a6d":"markdown","414599c8":"markdown","845afbfb":"markdown","8f7dbebe":"markdown","2ea88458":"markdown","f915c440":"markdown","97688ea9":"markdown","97dab653":"markdown","64fea499":"markdown","b1ada9ae":"markdown","66b0dc48":"markdown","846c2eeb":"markdown"},"source":{"245dfa5a":"import numpy as np\nimport pandas as pd\n#visualisation \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nfrom datetime import date\nfrom datetime import datetime\nimport datetime as dt\nimport gc\nimport json\n\nfrom mpl_toolkits.basemap import Basemap\nfrom geopy.geocoders import Nominatim\nimport math\n\nfrom scipy import stats\nfrom ipywidgets import interact","73968bb8":"#for warnings\nimport warnings\nwarnings.filterwarnings('ignore')","24624dc0":"#download the files and become in dataframe(pandas)\npath = '..\/input\/salesdb-grocery\/salesdb_grocery_market\/'\n\ncountries = pd.read_csv(path + 'countries.csv', sep=\";\")\ncities = pd.read_csv(path + \"cities.csv\", sep=\";\")\nemployes = pd.read_csv(path + \"employes.csv\", sep=\";\")\nproducts = pd.read_csv(path + \"products.csv\", sep=\";\")\ncategories = pd.read_csv(path + \"categories.csv\", sep=\";\")\ncustomers = pd.read_csv(path + \"customers.csv\", sep=\";\")\nsales = pd.read_csv(path + \"sales.csv\", sep=\";\")\n\n#download file created by myself with cities and location \n#latitude and longitude\ncities_loc = pd.read_csv('..\/input\/cities-loc-sales-grocery-market\/Cities_location')\ncities_loc = cities_loc.drop(columns='Unnamed: 0')","7f80a2c5":"print(\"  -Countries: \\nShape:\", countries.shape)\nprint(\"Head:\")\nprint(countries.head(),\"\\n\")","f4fde9cc":"print(\"  -Cities: \\nShape:\", cities.shape)\nprint(\"Head:\")\nprint(cities.head(),\"\\n\")","5da755e5":"cities_countries=pd.merge(cities, countries, on=['CountryID','CountryID'])\ncities_countries.head()","2a4d5433":"print (\"countries:\", cities_countries['CountryName'].drop_duplicates().values[0])\n#cities_countries.hist(column='CountryName')","01b0ad41":"cities_countries=cities_countries.drop(columns=['CountryID','CountryCode'])\n#cities_countries.head()","7fb31687":"print(\"  -Employes: \\nShape:\", employes.shape)\nprint(\"Head:\")\nprint(employes.head(),\"\\n\")","4a22a834":"Gender=employes.groupby('Gender')['Gender'].count()\nfig, ax=plt.subplots(figsize=(10,4))\nGender.plot.bar(ax=ax)\nplt.title('Employees\\' Gender distribution')\nplt.show()\n","ce72376f":"# We create new features Age\ntoday=date.today()\nAg=[]\nfor i in range(0,(employes.shape[0])):\n    aux = employes['BirthDate'][i]\n    aux = aux[0:10]\n    aux = datetime.strptime(aux, '%Y-%m-%d')\n    aux = int(((today-aux.date()).days)\/365.25)\n    Ag.append(aux)\n    \nemployes['Age']=Ag","f77802cf":"#we create the histogram\ndef group_Age(i):\n    if i<20: return ('<20')\n    elif i<25: return ('20-24')\n    elif i<30: return ('25-29')\n    elif i<35: return ('30-34')\n    elif i<40: return ('35-39')\n    elif i<45: return ('40-44')\n    elif i<50: return ('45-49')\n    elif i<55: return ('50-54')\n    elif i<60: return ('55-59')\n    elif i<65: return ('60-64')\n    else: return ('>65')","a4a17af5":"employes['Age_aux']=list(map(group_Age, list(employes['Age'])))\nfig,ax=plt.subplots(figsize=(10,4))\npd.DataFrame(employes.groupby('Age_aux')['Age_aux'].count()).plot.bar(ax=ax)\nplt.title('Employe\\'s Ages distribution')\nplt.show()","4f27d70a":"Y=[]\nfor i in range(0,(employes.shape[0])):\n    aux = employes['HireDate'][i]\n    aux = aux[0:10]\n    aux = datetime.strptime(aux, '%Y-%m-%d')\n    aux = int(((today-aux.date()).days)\/365.25)\n    Y.append(aux)\n    \nemployes['YearsHired']=Y","15267f5c":"YearHired=employes.groupby('YearsHired')['YearsHired'].count()\nYearHired=pd.DataFrame(YearHired)\nYearHired=YearHired.rename(columns={'YearsHired':'#employes'})\nprint(YearHired.T)\nemployes['Age_aux']=list(map(group_Age, list(employes['Age'])))\nfig,ax=plt.subplots(figsize=(10,4))\nYearHired.plot.bar(ax=ax)\nplt.title('Number of year that the employees are working in the company distribuion')\nplt.show()","e12e12de":"employes=employes.drop(columns=['HireDate','BirthDate','FirstName','MiddleInitial','LastName'])","bde366dd":"print(\"  -Products: \\nShape:\", products.shape)\nprint(\"Head:\")\nprint(products.head(),\"\\n\")","02113493":"print(\"  -Categories: \\nShape:\", categories.shape)\nprint(\"Head:\")\nprint(categories.head(),\"\\n\")","6d9422b8":"# Merge these two tables in order to know \n# what type of categories we have.\nproducts =pd.merge(products, categories, on=['CategoryID','CategoryID'])","8518d477":"products = products.drop(columns=['CategoryID','ModifyDate','IsAllergic', 'VitalityDays','Class', 'Resistant'])","8cbc068b":"products.head()\n","ee11d25c":"products[products['CategoryName']=='Shell fish'].head()","0170af08":"#products with price equal 0\nprint('number of products with price 0:',len(products[products['Price']==0]))","1a4e96d6":"#try to do hitogram\nprice=pd.DataFrame(products.groupby('Price')['Price'].count())\n\nlen(price)","cf81df74":"# 5 of the products less expensive and the price\nproducts=products.sort_values(by=['Price'])\nproducts.head(5)","ae593dca":"# 5 of the products more expensive and the price\nproducts.tail(5)","639a93bc":"#convert Price string to float\nproducts['Price']=products.Price.str.replace(',', '.')\nproducts['Price']=pd.to_numeric(products['Price'])","dc5e69f0":"fig,axes=plt.subplots(nrows=2,figsize=(10,8))\naxes[0].set_title('Products\\'s Prices Histogram')\nproducts['Price'].plot.hist(bins=20,rwidth=0.9, ax=axes[0])\n\nproducts['Price'].plot.density(ax=axes[1])\naxes[1].set_title('Products\\'s Prices Density Function')\nplt.show()","272fabcf":"distribution='uniform'\nmin_price=min(products['Price'])\nmax_price=max(products['Price'])\nstats.kstest(list(products['Price'].sort_values()), distribution, args=(0,100))","61f6cbe8":"#Shape of countries\nprint(\"- CUSTOMERS:\")\nprint(customers.shape)\nprint(customers.head())","da15e8c8":"#merge Customers and cities \ncustomers=pd.merge(customers, cities, on=['CityID', 'CityID'])","1c7e5029":"city_customers = customers.groupby('CityName')['CustomerID'].count()\ncity_customers = pd.DataFrame(city_customers)\ncity_customers = city_customers.rename(columns={\"CustomerID\":\"# customers\"})\ncity_customers=city_customers.sort_values(by=['# customers'])","137ac496":"print(\"the 5 cities with the least customers:\")\ncity_customers.head(5)","1589c329":"print(\"the 5 cities with the most customers:\")\ncity_customers.tail(5)","af88237e":"#histogram with the cities and number of customeres\nx=list(np.arange(1,96,6))\ncity_customers=city_customers.sort_values(by=['# customers'])\ncity_customers['Range']=range(1,len(city_customers)+1)\naux=city_customers[city_customers['Range'].isin(x)]\nlabels=list(aux.index)\ncity_customers=city_customers.drop(columns=['Range'])\n\nfig,ax=plt.subplots(figsize=(10,4))\ncity_customers.plot.bar(ax=ax)\nplt.xticks(x, labels, rotation=100)\nplt.ylim(900,1200)\nplt.title('# customers per city distribution')\nplt.show()","f980a951":"city_cust_mean=city_customers.mean()\ncity_cust_std=city_customers.std()\nprint(\"mean: %f\" %city_cust_mean)\nprint(\"std: %f\" %city_cust_std)\n#Crea Z=(x-E(x))\/std\ncity_cust_norm=(city_customers-city_cust_mean)\/city_cust_std\nfig,axes=plt.subplots(nrows=2,figsize=(10,8))\ncity_cust_norm.plot.hist(bins=10, ax=axes[0])\naxes[0].set_title('Normalised Distribution')\ncity_cust_norm.plot.density(ax=axes[1])\naxes[1].set_title('Density Function')\nplt.show","052260e7":"distribution='norm'\nstats.kstest(list(city_cust_norm['# customers']), distribution, args=(0,1))","ab78b0e0":"city_customers = customers.groupby('CityName')['CityName'].count()\ncities=[[cit, val] for cit,val in city_customers.iteritems()]","032857ae":"scale=0.25","f7b05b6d":"mymap = Basemap(width=10000000,height=6000000,projection='lcc',\n            resolution=None,lat_1=45.,lat_2=55,lat_0=50,lon_0=-107.)\n\n","03d66089":"def cleanloc(string):\n    longitude = float(string.split(',')[1][:-1])\n    latitude = float(string.split(',')[0][1:])\n    return (longitude, latitude)","c5c9c669":"d = {cities_loc['cities'][i]: cleanloc(cities_loc['loc'][i]) for i in cities_loc.index}","a03bb6b2":"plt.figure(figsize=(19,20))\nmymap.bluemarble()\nfor (city,count) in cities:\n    x, y = mymap(*d[city])\n    mymap.plot(x,y,marker='o',color='Red',markersize=int(math.sqrt(count))*scale)\nplt.show()","fd7254ca":"#zipcode\nprint('number of zipcode:',len(customers['Zipcode'].drop_duplicates()))\nprint('number of cities:', len(customers['CityName'].drop_duplicates()))\n","043327cc":"# clean data customers\ncustomers=customers.drop(columns=['FirstName','MiddleInitial','LastName','Address',\n                       'CityID','Address', 'CountryID', 'Zipcode'])","27981d58":"customers.head()","6dd48c2c":"print(\"  -Sales: \\nShape:\", sales.shape)\nprint(\"Head:\")\nprint(sales.head(),\"\\n\")","2c77247b":"# sales with customer\nsales_customer=pd.merge(sales, customers, on=['CustomerID', 'CustomerID'])\nsales_customer_prod=pd.merge(sales_customer, products, on=['ProductID', 'ProductID'])\nsales_customer_prod=sales_customer_prod.drop(columns=['TotalPrice', 'TransactionNumber',\n                                         'CategoryName'])\n                          ","90a801ea":"#PriceTotal=Quantity*Price\nsales_customer_prod['PriceTotal']=sales_customer_prod['Price']*sales_customer_prod['Quantity']\n#DiscountTotal=Discount*Price\nsales_customer_prod['Discount']=sales_customer_prod['Discount'].fillna(0)\nsales_customer_prod['DiscountTotal']=sales_customer_prod['Discount']*sales_customer_prod['Price']\n#Total=PriceTotal-TotalDiscount\nsales_customer_prod['Total']=sales_customer_prod['PriceTotal']-sales_customer_prod['DiscountTotal']\nsales_customer_prod=sales_customer_prod.drop(columns=['Discount'])","34ef4736":"sales_customer_prod.head()","a6b9d583":"a=pd.DataFrame(sales_customer_prod.groupby('ProductName')['Quantity'].sum().sort_values())\nx=list(np.arange(5,len(a)+1, 40))\na['Range']=np.arange(1,len(a)+1)\nlabels=a[a['Range'].isin(x)].index\n\nprint('the worst seller: %s' %a[a['Quantity']==min(a.Quantity)].index[0] )\nprint('the best seller: %s' %a[a['Quantity']==max(a.Quantity)].index[0] )\nfig,ax=plt.subplots(figsize=(10,4))\npd.DataFrame(a['Quantity']).plot.bar(ax=ax)\n\nplt.xticks(x, labels, rotation=100)\nplt.title('number of products are sold')\nplt.ylim(180000,200000)\nplt.show()","b2fc8ba5":"# working with SalesDate. SalesDate contains NaN, so we are going \n# to work with sales_aux dateframe.\nsales_aux=sales_customer_prod[sales_customer_prod['SalesDate'].notnull()]\nsales_aux['SalesDate'] = pd.to_datetime(sales_aux['SalesDate'], format='%Y-%m-%d %H:%M:%S.%f')\nsales_aux['S_Date']=sales_aux['SalesDate'].dt.date","4f9d06d6":"a_1=sales_aux.groupby(['S_Date', 'SalesPersonID'])['SalesID','CustomerID','CityName'].count()\na_2=sales_aux.groupby(['S_Date', 'SalesPersonID'])['CustomerID','CityName'].nunique()\na_2=a_2.rename({'CustomerID':'CustomerUnique','CityName':'CityUnique'}, axis=1)\na_3=sales_aux.groupby(['S_Date', 'SalesPersonID'])['Quantity','Total'].sum()\na=pd.concat([a_1, a_2,a_3], axis=1)","f0e782ff":"a_original=a","97c49e6f":"def set_num_employees(num_employees):\n    global a\n    listemployees=list(np.arange(1,num_employees+1)) \n    a=a_original\n    a=a[np.in1d(a.index.get_level_values(1), listemployees)]","1b18f272":"def represent_data_1(features):\n    if features=='CustomerID':\n        fig, axes=plt.subplots(nrows=3, figsize=(10,12))\n        print(\"mean(sales by SalesPersonID and Day): %f\" %a[features].mean())\n        print(\"std(sales by SalesPersonID and Day): %f\" %a[features].std())\n        print(\"mean(sales by SalesPersonID and Day): %f\" %a['CustomerUnique'].mean())\n        print(\"std(sales by SalesPersonID and Day): %f\" %a['CustomerUnique'].std())\n        a_aux=pd.DataFrame(a[features].unstack())\n        plt.suptitle('%s per seller and day'%features, size=20)\n        # Plot 1.\n        x=list(np.arange(1,len(a_aux)+1, 10))\n        a_aux['Range']=np.arange(1,len(a_aux)+1)\n        labels_aux=list(a_aux[a_aux['Range'].isin(x)].index.astype(str))\n        a_aux=a_aux.drop(columns=['Range'])\n        a_aux.plot.line(ax=axes[0])\n        labels=axes[0].set_xticklabels(labels_aux, rotation=30)\n        # Plot 2.\n        if len(a[features].drop_duplicates())>1:\n            a[features].plot.density(ax=axes[1])\n        # Plot 3.\n        a['CustomerUnique'].unstack().plot.line(ax=axes[2])\n        axes[2].set_title('customer unique')\n        labels=axes[2].set_xticklabels(labels_aux, rotation=30)\n        plt.show()\n    elif features=='CityName':\n        fig, axes=plt.subplots(nrows=3, figsize=(10,12))\n        print(\"mean(sales by SalesPersonID and Day): %f\" %a[features].mean())\n        print(\"std(sales by SalesPersonID and Day): %f\" %a[features].std())\n        print(\"mean(sales by SalesPersonID and Day): %f\" %a['CityUnique'].mean())\n        print(\"std(sales by SalesPersonID and Day): %f\" %a['CityUnique'].std())\n        a_aux=pd.DataFrame(a[features].unstack())\n        plt.suptitle('%s per seller and day'%features, size=20)\n        # Plot 1.\n        x=list(np.arange(1,len(a_aux)+1, 10))\n        a_aux['Range']=np.arange(1,len(a_aux)+1)\n        labels_aux=list(a_aux[a_aux['Range'].isin(x)].index.astype(str))\n        a_aux=a_aux.drop(columns=['Range'])\n        a_aux.plot.line(ax=axes[0])\n        labels=axes[0].set_xticklabels(labels_aux, rotation=30)\n        # Plot 2.\n        if len(a[features].drop_duplicates())>1:\n            a[features].plot.density(ax=axes[1])\n        # Plot 3.\n        a['CityUnique'].unstack().plot.line(ax=axes[2])\n        axes[2].set_title('city unique')\n        labels=axes[2].set_xticklabels(labels_aux, rotation=30)\n        plt.show()\n    else:\n        fig, axes=plt.subplots(nrows=2, figsize=(10,8))\n        print(\"mean(sales by SalesPersonID and Day): %f\" %a[features].mean())\n        print(\"std(sales by SalesPersonID and Day): %f\" %a[features].std())\n        plt.suptitle('%s per seller and day'%features, size=20)\n        a_aux=pd.DataFrame(a[features].unstack())\n        # Plot 1.\n        x=list(np.arange(1,len(a_aux)+1, 10))\n        a_aux['Range']=np.arange(1,len(a_aux)+1)\n        labels_aux=list(a_aux[a_aux['Range'].isin(x)].index.astype(str))\n        a_aux=a_aux.drop(columns=['Range'])\n        a_aux.plot.line(ax=axes[0])\n        labels=axes[0].set_xticklabels(labels_aux, rotation=30)\n        # Plot 2.\n        if len(a[features].drop_duplicates())>1:\n            a[features].plot.density(ax=axes[1])\n        plt.show()\n        ","e573d1ee":"interact(set_num_employees, num_employees=list(np.arange(1,24)))","2dbe0c2e":"interact(represent_data_1, features=['SalesID','CustomerID', 'CityName','Quantity', 'Total'])","c454f4ca":"# Checking the NaN which are in SalesDate\nlen(sales_customer_prod[sales_customer_prod['SalesDate'].isna()])\/len(sales_customer_prod)*100","bfda1a07":"sales_customer_prod_aux=sales_customer_prod.dropna(subset=['SalesDate'])","6802ca8d":"sales_customer_prod_aux['SalesDate']=pd.to_datetime(sales_customer_prod_aux['SalesDate'],format='%Y-%m-%d %H:%M:%S.%f')","4ca673d1":"#Features\nsales_customer_prod_aux['s_weekday']=sales_customer_prod_aux['SalesDate'].dt.weekday\nsales_customer_prod_aux['s_hour']=sales_customer_prod_aux['SalesDate'].dt.hour\nsales_customer_prod_aux['s_day']=sales_customer_prod_aux['SalesDate'].dt.day\nsales_customer_prod_aux['s_month']=sales_customer_prod_aux['SalesDate'].dt.month\nsales_customer_prod_aux['s_year']=sales_customer_prod_aux['SalesDate'].dt.year","cdf0ad7f":"def freq_feature(data, column):\n    aux = data.groupby(column)['Quantity'].sum()\n    aux = pd.DataFrame(aux)\n    return (aux)","05c1dec9":"names_freq = ['s_hour', 's_day', 's_weekday', 's_month','s_year']\nl = []#this is a list of dataframes in order to collect for all frequencies\nfor i in names_freq:\n    aux=freq_feature(sales_customer_prod_aux,i)\n    l.append(aux)","8a60f445":"#aux.plot.bar()\ndef represent_data_4(freq):\n     for i in l:\n            if i.index.name==freq:\n                i.plot.line()\n                plt.title(freq)\n                plt.show()\n","fbe03b00":"interact(represent_data_4, freq=['s_hour', 's_day', 's_weekday', 's_month','s_year'])","597f930b":"print (sales_customer_prod_aux['SalesDate'].min())\nprint (sales_customer_prod_aux['SalesDate'].max())","309d7844":"Secondly, we plot the **number of customers per city distribution**. We **sort the values** and we do **not print all cities**, this makes the graph look more tidy.","7c0da532":"In order to see **how many countries are used**, we are going to merge *cities* and *countries* dataframes.","9ab0cd15":"We are going to analyse a series of data sets from KAGGLE which describe grocery market sales.\n\n","da6a80c6":"<a id='section2'><\/a>\n### 2.3.Products and categories\n\nWe study *products* and *categories* dataframes.\n","e1ad300f":"Observing the plot, we notice that the quantity sold per product is in the range (190000,200000), then **the quantity sold seems surprisingly similar across products**.","82cbd93d":"<a id='section12'><\/a>\n## 1. Import libraries and download data ","d303d0cf":"The histogram show us that there are **around 1000 customers per city**.","9048291a":"#### Analysing some features filtering per SalesDate, SalesPersonID (employees)<a id='section10'><\/a>\n\nIn this section, we **group the data by *SalesDate* and *SalesPersonID**. We want to see **how many sales are done per day and employee**, the **amount of customers are attended per day and employee**, the **number of cities that is served by day and employee**, the **quantity of products is sold by day and employee**, and finally the **money is gathered by day and employee**.","ccdfbbbe":"Looking at the plots above, we observe the following:\n\n- hours: looking a the amount of product sold per hour, we can say that has a range between 3620000 to 3640000, and although there are some peaks it seems that the number of sales per hour is almost constant. We found this strange.\n\n- days_months: This is the amount of the products that are sold each day number of the month. The graph is quite constant during two big blocks: day number 1-9  and day number 10-27. The last days (after 28th day), behave differently because not all months have the same number of days.\n\n- weekday: This graph considers 0 as Monday and 6 as Sunday. We can observe that there is a big difference of amount of sales between the first days of the week an the last one, and seeing the start dates and finishing dates, they are whole weeks except the last one which finished on Wednesday that it can makes the difference for the plot.\n\n- months: The plot is quite constant, except the 5th month, this is because that month only contains 9 days. We find a little less sales in February and this is because it has less days.\n","3b208d12":"#### Employees' Age Distribution","7f645d29":"There is a 0.99% of NaN with respectect the whole SalesDate's data.","a0f9710b":"There are **23 employees in the company**.  \n\nBelow, we plot the *Gender* distribution, *Age* distribution and \nthe *Number of year the employees are working in the company* distribution.\n\n\n#### Employees' Gender Distribution","9c3d8603":"### 2.5. Sales<a id='section15'><\/a>\n\nWe focus now on studying the *sales* dataframe, but complementing it with the *customer* and *products* datasets.","3b723f80":"<a id='section3'><\/a>\n\n#### CategoryName analysis \n\nWe study the *CategoryName* feature because we find this **dataset looks wrong**, for example we have a beer in the Cereals category.\n\nBelow, we **filter *CategoryName* by *Shell fish* and we show 5 values**.","8115050f":"#### Number of year that the employees are working in the company distribution","14a9ebd2":"The graphs above suggest that **number of customer normalised might follow a normal distribution ($N(0,1)$)**. We perform a **Kolmogorov hypothesis test**.\n\nH0: Z~$N(0,1)$\n\nH1: Alternative","a1113f8d":"Observing the plot, we can say that there is almost **the double male than female**.","c0e40e11":"In order to do a better analysis we **merge these three dataframes: *sales*, *customers* and *products,* ** and we also **drop some features** that they do not give any information: *TotalPrice*, *TransactionNumber* and *CategoryName*. \n","9ead77a1":"# SALES GROCERY MARKET","31f3bbb7":"We observe that **United States is the only country that appears**, then we can ignore the countries dataframe. Moreover, we will eliminate *CountryID, CountryCode* features from *cities_countries* dataframe.","e4e2ae02":"The graphs above suggest that **the price might follow a uniform distribution [0,100]**. We perform a **Kolmogorov hypothesis test**\n\nH0: Price ~ Uniform\n\nH1: Alternative","3411ccb8":"We observe in the map that customers are more concentrated in the part East of USA. And also, we cannot appreciate any difference between size of the red dots, this means that more or less all city have the same number of customers, as we have seen in the graphs before.","3fc8283e":"We **join** the *customers* and *cities* dataframe in order to identify what **cities are present**.","5fed1abc":"#### Analysis of the category variable\n\nWe merge these two tables because we would like to have the *CategoryName* in the *products* table. And also, we decide to **drop some columns**: *CategoryID*, *ModifyDate*, *IsAllergic*, *VitalityDays*, *Class* and *Resistant* because observing the variables, they **do not give us coherent information**.\n","e6268bbc":"#### Number of zipcodes per cities<a id='section7'><\/a>","034792c1":"Looking at the histogram, we see that the **mode is 2 years and the mean is around 4 years hired**.  \n\nTo finish with *employes* dataframe we clean the data, eliminating  *HireDate*, *BirthDate*, *FirstName*, *MiddleInitial*, *LastName* features.","3aace80e":"Finally, we show **the way \"number of customers\" is distributed geographically**. We use *geopy.geocoders* and *basemap* libraries. The map takes into account the number of customer per city, in other words the red dots will be proportional depending on the number of customers.","d224b2f8":"Then, **we cannot reject the H0 at the 5% level because $0.05<p_{value}=0.358$**. Therefore, we can think that the number customers per city is distributed as an normal distribution.","39c60838":"The table seems that **the *CategoryName* is not a reliable feature of this dataset**, since for example we can see that  the Kellogs Special K Cereal product is classify as Shell fish instead of Cereals. \n\n####  Price analysis<a id='section4'><\/a>\n\nFirstly, we check if **all of products have a positive price**.","f16fba7e":"<a id='section6'><\/a>\n#### Number of customers per cities\n\nWe analyse how many customers are distributed around USA (It is the only country).\n\nFirstly, we create a dataframe where **number of customers is grouped  by city**, and then **the data is ascendant sorted by number of cities**. We show two tables: **the 5 cities with the least customers and the 5 cities with the most customers**. ","1a4b4729":"Looking at the output above, we can say that **each single products has a different price**.\n\nFollowing, we are going to show the **five cheapest products** and their price, and also, the **five most expensive products** and their price","755e807c":"The p_value=0.436 > 0.05 indicates that **we cannot reject the H0 at the 5%**. So it is reasonable to think that the price is distributed as a uniform distribution. **This seems quite surprising and it might indicate that the data has been artificially generated**.","4075aa56":"<a id='section5'><\/a>\n### 2.4. Customers (and cities)\n\nWe study *customeres* and *cities* dataframes.","c852284a":"<a id='section14'><\/a>\n\n### 2.2 Employees \n\nWe study the *Employes* dataframe.","214c1bbe":"We plot the **Price histogram** and **density function**.","f2c00b18":"You migh modify the number of employees.","ab53dae1":"## Index\n\n- [1. Import libraries and download data](#section12)\n- [2. Dataframes analysis](#section13)\n    - [2.1. Countries and cities](#section1)\n    - [2.2. Employees](#section14)\n    - [2.3. Products and categories](#section2)\n    - [2.4. Customers (and cities)](#section5)\n    - [2.5. Sales](#section15)\n- [3. Conclusion](#section16)\n","d6ae8a6d":"Observing the graphs before, that we can choose different values for the *Feature* value, we have the following:\n\n- SalesID: the first graph shows number number of different SalesIDs per seller, we can see that they range from 2100 to 2450. Therefore we do not have big difference between employees and days. Everything keeps quite stable. Also, looking at the density function we see that there is not a big dispersion.\n\n- CustomerID: We can see the first and second graphs are equal than the  *SalesID* feature, this is because there are some customers do different sales in the same day. For that reason we decided to add a third graph to plot the unique values for customer, meaning that we only count customer with different IDs. The mean and standard deviation obtained are really similar in both cases.\n\n- CityName: We can observe that the first and second plot are the same as the fist and second for *SalesID* and *CustomerID* variables, this is because in the same city is producing different sales for different customers. The third plot shows the unique value for cities which are buying, and the values we get is 96 all the time, this means that there are always customers from all cities the whole time.  \n\n- Quantity: we see that the range is maintaining between 27000 to 32000, this means that the quantity of the products per assistant has similar behaviour per day and also the standard deviation is not very big proportionally with the range.\n\n- Total: This value is consider the net of the sales, in other words total revenue per employee. It ranges between 1350000 to 1600000 and the deviation is quite small proportionally with the range. This means that the company has incomes quite stable across employees and days.\n","414599c8":"The two tables before give us **a contradictory information**. For example we have three types of bread, and two of them belong to the set of the 5 cheapest products, but there is one that belongs to the the 5 most expensive products set. Therefore this is a bit illogical. And also the **price of zucchini (98,4644)** is a little bit out of scale comparing with **beef (99,3193) and shrimp (99,8755)**.","845afbfb":"You migh modify the feature to be plotted.","8f7dbebe":"Thirdly, we normalise the histogram above with (if $x$ is the number of costumers, we compute $z=\\frac{x-E(X)}{std(X)}$). We plot **the distribution and density function**.","2ea88458":"## 3. Conclusion<a id='section16'><\/a>\n\nWe think that the data analysed above might have been produced artificially, for the following points:\n\n- [Products and categories:](#section2) In this section we found some contradictions. For example, in the *products* dataframe there is a feature called [*CategoryName*](#section3), which does not make much sense, since we can find  wasabi, Kellogs, Jerusalem artichokes, and so on, in the *sell fish* category. Moreover, the [*Price*](#section4) variable seems artificial created because each product has different price and density function seems follow an Uniform distribution between 0 and 100. \n\n- [Customers and cities:](#section5) If [Number of costumers per cities](#section6) is analysed, we get that the values are around 1000 customers per cities, and we can say that it is strange that all cities have such a similar number of customers. Besides, if we normalised the data we can say that data seems to follow a Normal distribution (0,1). Additionally, the amount of zip codes and number of cities are equal, [Number of   per cities](#section7), these values should be different, because in the same city, there a huge number of zip codes.\n\n- [Sales:](#section8) In this section, there is [Products popularity](#section9) which is represented by a histogram of products sorted by popularity (items sold). This analysis shows us that the quantity sold of each product ranges between 190000 to 200000, which seems very narrow to us. Another study that we do with the sales data set is to study different features in there of [*Sales_Date* and *SalesPersonID*](#section10). The results also seem too homogeneous across time and employee to be true. Finally, we analyse the [quantity of products sold per different fractions of time](#section11), we can say that in general the graphs are quite constant, the only difference is because there are some fractions of time is missed.","f915c440":"Then, we study the *CountryName* feature and we will see **in which countries have customer**.","97688ea9":"<a id='section9'><\/a>\n\n####\u00a0Popular Products \n\nIn this section, we study **what product is the most and least popular product**. We group the *Quantity* feature by *ProductName*. After that, we sort ascending the *Quantity* from this dataframe and we print some *ProductName*, in order to have a tidy graph.\n\n","97dab653":"<a id='section13'><\/a>\n## 2. Dataframes analysis \n\nWe are going to **analyse each dataframes** separately first, or combining two or three of them if they do not contain much information.\n\n### 2.1. Countries and Cities <a id='section1'><\/a>\n\nIn this section, we analyse the information which is in these two dataframes (*Countries* and *cities*).\n","64fea499":"We observe that it is strange to have the **same number of zip codes than number or cities, since the zip codes** set should be bigger than cities set. It is another reason that we can think that the data is not real.\n\nTo finish, we eliminate some features from customers: *FirstName*, *MiddleInitial*, *LastName*, *Address*, *CityID*, *Address*, *CountryID* and *Zipcode*.","b1ada9ae":"#### What days a week are more popular por selling and also the hours.","66b0dc48":"We create new features related to *Price*. Given that the *price* in the *sales* set does not contain any information, we get this price from the *products* dataframe.","846c2eeb":"The plot shows **the majority of employees are older than 45 years**."}}