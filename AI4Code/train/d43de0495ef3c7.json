{"cell_type":{"df23edbf":"code","f49a51c1":"code","ce223e45":"code","88245c93":"code","0a87ae73":"code","d48e056c":"code","4b0b1f88":"code","31dfbe79":"code","1aa4866c":"code","4c079903":"code","ba206d06":"code","d2fba019":"code","fce4a0d7":"code","e325648d":"code","ea5f8975":"code","fc4f8deb":"code","e9b1a72f":"code","0d3a65a3":"code","74415df2":"code","d6ab5c32":"code","431e39c9":"code","2d941c15":"markdown","0373b283":"markdown","873bee77":"markdown","b55688a6":"markdown","fdbd79fe":"markdown"},"source":{"df23edbf":"import os\nimport numpy\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nimport statsmodels.api as sm \nfrom scipy.interpolate import interp1d\nimport datetime as dt\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","f49a51c1":"osj = os.path.join\nINPUT_DIR = '..\/input\/m5-forecasting-accuracy\/'\ntrain = pd.read_csv(osj(INPUT_DIR, 'sales_train_validation.csv'))\nprice = pd.read_csv(osj(INPUT_DIR, 'sell_prices.csv'))\ncalender = pd.read_csv(osj(INPUT_DIR, 'calendar.csv'))\n\nsample_submit = pd.read_csv(osj(INPUT_DIR, 'sample_submission.csv'))","ce223e45":"calender['date'] = pd.to_datetime(calender['date'])","88245c93":"d_cols = [c for c in train.columns if 'd_' in c] # d_\u3067\u59cb\u307e\u308b\u65e5\u4ed8\u5217\u306e\u30ea\u30b9\u30c8","0a87ae73":"past_sales = train.set_index('id')[d_cols].T.merge(calender.set_index('d')['date'],\n                                                   left_index=True,\n                                                   right_index=True,\n                                                   validate='1:1').set_index('date')","d48e056c":"def make_total_sales_lowess( p ):\n    total_sales = pd.DataFrame(p.sum(axis=1), columns=['total sales'])\n    \n    # \u30af\u30ea\u30b9\u30de\u30b9\u524a\u9664\n    total_sales_noXmas = total_sales.drop(index=[dt.datetime(2011,12,25), \n                                                 dt.datetime(2012,12,25), \n                                                 dt.datetime(2013,12,25), \n                                                 dt.datetime(2014,12,25), \n                                                 dt.datetime(2015,12,25)])\n\n    df = total_sales_noXmas\n    lowess = sm.nonparametric.lowess(df['total sales'], df.index, frac=.3) \n    lowess_x = list(zip(*lowess))[0] \n    lowess_y = list(zip(*lowess))[1] \n    \n    f = interp1d(lowess_x, lowess_y, bounds_error=False)\n    new_lowess_x = df.index\n    new_lowess_y = f(new_lowess_x)\n    \n    total_sales_lowess = total_sales_noXmas\n    total_sales_lowess['lowess'] = new_lowess_y\n    total_sales_lowess['total sales-lowess'] = total_sales_lowess['total sales'] - total_sales_lowess['lowess']\n    \n    return total_sales_lowess","4b0b1f88":"total_sales_lowess = make_total_sales_lowess( past_sales )","31dfbe79":"y_bt = -5000\ny_up = 10000\nstore_list = price['store_id'].unique()\nfor store in store_list :\n    total_sales_lowess_tmp = make_total_sales_lowess( past_sales.loc[:, past_sales.columns.str.contains(store)] )\n    total_sales_lowess_tmp.plot(figsize=(15, 5), alpha=0.8, ylim=[y_bt, y_up],title=store)","1aa4866c":"########################### Read base submission\n############################################################################\nsubmission = pd.read_csv('..\/input\/m5-three-shades-of-dark-darker-magic\/submission_v1.csv')","4c079903":"########################### Day 1\n############################################################################\n\n# Current number of teams       3,215 (day 2 of May)\n# Number of teams 2 days before 3,186 \nteams_now = 3215\nteams_before = 3186\n#submission['F1'] *= teams_now\/teams_before\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F1'] *= 1.009\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F1'] *= 1.02\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F1'] *= 1.009\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F1'] *= 1.011\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F1'] *= 1.01\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F1'] *= 1.011\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F1'] *= 1.011\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F1'] *= 1.015\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F1'] *= 1.02\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F1'] *= 1.015\nsubmission['F1'].sum()","ba206d06":"########################### Day 2\n############################################################################\n\n# In case you'll get a heart attack \n# because of all the MAGIC in this kernel -\n# here is South Korean emergency number - 119\nmagic = submission['F2'].sum()\njapan_emergency = 119\n#submission['F2'] *= magic \/ (magic + japan_emergency)\n                             \nsubmission.loc[submission['id'].str.contains('CA_1'), 'F2'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F2'] *= 0.998\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F2'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F2'] *= 0.996\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F2'] *= 0.995\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F2'] *= 0.996\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F2'] *= 0.996\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F2'] *= 0.998\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F2'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F2'] *= 0.998\n\nsubmission['F2'].sum()","d2fba019":"########################### Day 3\n############################################################################\n\n# 1913 - The Woman suffrage parade of 1913 takes place in Washington, D.C.\n# 1923 - Ankara replaces Istanbul (Constantinople), as the capital of Turkey.\n#submission['F3'] *= 1913\/1923\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F3'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F3'] *= 0.998\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F3'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F3'] *= 0.996\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F3'] *= 0.995\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F3'] *= 0.996\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F3'] *= 0.996\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F3'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F3'] *= 1.005\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F3'] *= 1\nsubmission['F3'].sum()","fce4a0d7":"########################### Day 4\n############################################################################\n\n# I love statistics.\n# When you dig in data you can \n# find VERY interesting \"facts\".\n# Did you know that the mean \n# number of sexual partners \n# per person is 1.0073\nsex_constant = 1.0073\n#submission['F4'] \/= sex_constant\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F4'] *= 0.98\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F4'] *= 1.02\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F4'] *= 0.98\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F4'] *= 1\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F4'] *= 0.99\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F4'] *= 1\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F4'] *= 1\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F4'] *= 1.00\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F4'] *= 1.02\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F4'] *= 1.00\nsubmission['F4'].sum()","e325648d":"########################### Day 5\n############################################################################\n\n# It's Friday  \n# Friday night is a mix (colors)\n# of girls #ffb0e8 and boys #330000\n# Resulted mix HEX color code is #995874\ncolor_of_the_night = 995874\n\n# There are millions \"Stories\"\n# We need to make it Mean, really mean\ncolor_of_the_night \/= 1000000\n#submission['F5'] *= color_of_the_night\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F5'] *= 0.995\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F5'] *= 1\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F5'] *= 0.995\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F5'] *= 0.998\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F5'] *= 0.996\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F5'] *= 0.998\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F5'] *= 0.998\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F5'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F5'] *= 1.01\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F5'] *= 1\nsubmission['F5'].sum()","ea5f8975":"########################### Day 6\n############################################################################\n\n# Fridays not always pass without \"consequences\"\n# BE CAREFUL - DRINK MODERATELY\n# https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/25701909\n# Traumatic brain injury and cognition.\n# PMID: 25701909 DOI: 10.1016\/B978-0-444-63521-1.00037-6\n\n# 1.00037-6 - it can't be just a coincidence\ninjury = 1.000376\n#submission['F6'] *= injury\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F6'] *= 1.0002\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F6'] *= 1.001\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F6'] *= 1.0002\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F6'] *= 1.001\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F6'] *= 1.000\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F6'] *= 1.001\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F6'] *= 1.001\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F6'] *= 1.001\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F6'] *= 1.01\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F6'] *= 1.001\nsubmission['F6'].sum()","fc4f8deb":"########################### Day 7\n############################################################################\n\n# Let's make it aaaall random\n# We will need NumPy\n\n# We will divide the COMPLETELY random number\n# from range >1000000 by 1000000\n# to have some float number from 0 to 1 \n\nimport numpy as np\nnp.random.seed(198505)\ncorrection = np.random.randint(1000000)\/1000000\n#submission['F7'] *= correction\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F7'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F7'] *= 1\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F7'] *= 0.994\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F7'] *= 0.998\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F7'] *= 0.995635\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F7'] *= 0.998\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F7'] *= 0.998\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F7'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F7'] *= 1.01\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F7'] *= 1\nsubmission['F7'].sum()","e9b1a72f":"########################### Day 8\n############################################################################\n#submission['F8'] *= (100-1)\/100 + (100-12)\/10000\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F8'] *= 0.996\nsubmission.loc[submission['id'].str.contains('CA_2'), 'F8'] *= 1\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F8'] *= 0.996\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F8'] *= 0.998\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F8'] *= 0.9988\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F8'] *= 1\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F8'] *= 1\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F8'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F8'] *= 1.01\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F8'] *= 1\nsubmission['F8'].sum()","0d3a65a3":"########################### Day 11\n############################################################################\n\n# Let's not change anything here\n# I have a feeling that we should \n# keep origical prediction\n#submission['F11'] = submission['F11']\nsubmission.loc[submission['id'].str.contains('CA_1'), 'F11'] *= 1 \nsubmission.loc[submission['id'].str.contains('CA_2'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('CA_3'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('CA_4'), 'F11'] *= 1\n\nsubmission.loc[submission['id'].str.contains('TX_1'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('TX_2'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('TX_3'), 'F11'] *= 1\n\nsubmission.loc[submission['id'].str.contains('WI_1'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_2'), 'F11'] *= 1\nsubmission.loc[submission['id'].str.contains('WI_3'), 'F11'] *= 1\nsubmission['F11'].sum()","74415df2":"########################### Days 9-19\n############################################################################\n\n# Just 1% of his fortune is equivalent to the whole health budget for Ethiopia\nfor i in range(9,20):\n    if i!=11:\n        #submission['F'+str(i)] *= 1.01 \n        submission.loc[submission['id'].str.contains('CA_1'), 'F'+str(i)] *= 1\n        submission.loc[submission['id'].str.contains('CA_2'), 'F'+str(i)] *= 1.015\n        submission.loc[submission['id'].str.contains('CA_3'), 'F'+str(i)] *= 1\n        submission.loc[submission['id'].str.contains('CA_4'), 'F'+str(i)] *= 1.013\n\n        submission.loc[submission['id'].str.contains('TX_1'), 'F'+str(i)] *= 1\n        submission.loc[submission['id'].str.contains('TX_2'), 'F'+str(i)] *= 1.013\n        submission.loc[submission['id'].str.contains('TX_3'), 'F'+str(i)] *= 1.013\n\n        submission.loc[submission['id'].str.contains('WI_1'), 'F'+str(i)] *= 1.015\n        submission.loc[submission['id'].str.contains('WI_2'), 'F'+str(i)] *= 1.02\n        submission.loc[submission['id'].str.contains('WI_3'), 'F'+str(i)] *= 1.015\n        print(submission['F'+str(i)].sum())","d6ab5c32":"########################### Days 20-28\n############################################################################\n\n# Of all the people earning A$100,000 a year under the age of 50, 2% are women. Just 2% are women.\nfor i in range(20,29):\n    #submission['F'+str(i)] *= 1.02\n    submission.loc[submission['id'].str.contains('CA_1'), 'F'+str(i)] *= 1.015\n    submission.loc[submission['id'].str.contains('CA_2'), 'F'+str(i)] *= 1.03\n    submission.loc[submission['id'].str.contains('CA_3'), 'F'+str(i)] *= 1.015\n    submission.loc[submission['id'].str.contains('CA_4'), 'F'+str(i)] *= 1.025\n\n    submission.loc[submission['id'].str.contains('TX_1'), 'F'+str(i)] *= 1.02\n    submission.loc[submission['id'].str.contains('TX_2'), 'F'+str(i)] *= 1.025\n    submission.loc[submission['id'].str.contains('TX_3'), 'F'+str(i)] *= 1.025\n\n    submission.loc[submission['id'].str.contains('WI_1'), 'F'+str(i)] *= 1.03\n    submission.loc[submission['id'].str.contains('WI_2'), 'F'+str(i)] *= 1.04\n    submission.loc[submission['id'].str.contains('WI_3'), 'F'+str(i)] *= 1.03\n    print(submission['F'+str(i)].sum())","431e39c9":"########################### Export\n############################################################################\nsubmission.to_csv('submission.csv', index=False)","2d941c15":"# Charts trends","0373b283":"# Adjust submission","873bee77":"We can found following chart trends in recently:\n\nCA_1 : small -\n\nCA_2 : +\n\nCA_3 : small -\n\nCA_4 : small +\n\nTX_1 : stable\n\nTX_2 : small +\n\nTX_3 : small +\n\nWI_1 : +\n\nWI_2 : ++\n\nWI_3 : +\n\nLet's adjust!","b55688a6":"# Charts by store","fdbd79fe":"The original work is [M5 - Witch Time](https:\/\/www.kaggle.com\/kyakovlev\/m5-witch-time).\n\nThank you, Konstantin-san. :-)\n\nThis is a template for doing \"Witch Time\" by store.\nsee [charts by stores](https:\/\/www.kaggle.com\/marutama\/charts-by-stores)\n\n\u3053\u308c\u306fsutore\u3054\u3068\u306bWitch Time\u3059\u308b\u305f\u3081\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3067\u3059\u3002\n\u3053\u306e\u30c1\u30e3\u30fc\u30c8\u3082\u898b\u3066\u304f\u3060\u3055\u3044\u3002 [charts by stores](https:\/\/www.kaggle.com\/marutama\/charts-by-stores)"}}