{"cell_type":{"71786f9e":"code","e27a2462":"code","5e8b0f72":"code","36a01d70":"code","cb0cdd37":"code","d131f7c2":"code","bd50274e":"code","52d23094":"code","32de4829":"code","fef7d619":"code","babedd7f":"code","68895d89":"code","d2f55c72":"code","1b11ee45":"code","ffbcd3d8":"code","a13c3a9a":"code","e7dab5fa":"code","f448d88b":"code","bc3d7b51":"code","c1846fee":"code","00dc4428":"code","0f15b9cb":"code","baa72f93":"markdown","33e90e7d":"markdown","9678cd27":"markdown","4a3692f5":"markdown"},"source":{"71786f9e":"# imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","e27a2462":"# Using UCI repository data\ndf = pd.read_csv('..\/input\/trains-transformed.data',delim_whitespace=True,header=None)\ndf.head()","5e8b0f72":"df.shape","36a01d70":"print(df.columns)","cb0cdd37":"# count of target variables\nprint(df.iloc[:,32].value_counts())","d131f7c2":"# find nulls in the data\ndf.isnull().sum().sum()","bd50274e":"# find NA in the data\ndf.isna().sum().sum()","52d23094":"print('The sum of all the ? symbols in column 3 is',df.iloc[:,3].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 4 is',df.iloc[:,4].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 6 is',df.iloc[:,6].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 8 is',df.iloc[:,8].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 9 is',df.iloc[:,9].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 11 is',df.iloc[:,11].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 12 is',df.iloc[:,12].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 13 is',df.iloc[:,13].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 14 is',df.iloc[:,14].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 15 is',df.iloc[:,15].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 16 is',df.iloc[:,16].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 17 is',df.iloc[:,17].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 18 is',df.iloc[:,18].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 19 is',df.iloc[:,19].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 20 is',df.iloc[:,20].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 21 is',df.iloc[:,21].str.contains('\\-').sum())\n# Replacing all the '-' with NaN.\ndf.iloc[:,12].replace('-',np.nan,inplace=True)\ndf.iloc[:,13].replace('-',np.nan,inplace=True)\ndf.iloc[:,14].replace('-',np.nan,inplace=True)\ndf.iloc[:,15].replace('-',np.nan,inplace=True)\ndf.iloc[:,16].replace('-',np.nan,inplace=True)\ndf.iloc[:,17].replace('-',np.nan,inplace=True)\ndf.iloc[:,18].replace('-',np.nan,inplace=True)\ndf.iloc[:,19].replace('-',np.nan,inplace=True)\ndf.iloc[:,20].replace('-',np.nan,inplace=True)\ndf.iloc[:,21].replace('-',np.nan,inplace=True)","32de4829":"# checking if all the '-' have been removed.\nprint('The sum of all the ? symbols in column 3 is',df.iloc[:,3].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 4 is',df.iloc[:,4].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 6 is',df.iloc[:,6].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 8 is',df.iloc[:,8].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 9 is',df.iloc[:,9].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 11 is',df.iloc[:,11].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 12 is',df.iloc[:,12].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 13 is',df.iloc[:,13].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 14 is',df.iloc[:,14].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 15 is',df.iloc[:,15].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 16 is',df.iloc[:,16].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 17 is',df.iloc[:,17].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 18 is',df.iloc[:,18].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 19 is',df.iloc[:,19].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 20 is',df.iloc[:,20].str.contains('\\-').sum())\nprint('The sum of all the ? symbols in column 21 is',df.iloc[:,21].str.contains('\\-').sum())","fef7d619":"# In the entire dataset we have 51 NaN values(Missing values)\nprint('count of all the missing values in the data is',df.isna().sum().sum())","babedd7f":"#Replacing all the NaN with forward values.\ndf.fillna(method='ffill',inplace=True) ","68895d89":"#Checking if the values are replacced.\nprint('count of all the missing values after treating the data is',df.isna().sum().sum())","d2f55c72":"# Label encoding all the strings\nenc_3 = LabelEncoder()\ndf.iloc[:,3] = enc_3.fit_transform(df.iloc[:,3])\nenc_4 = LabelEncoder()\ndf.iloc[:,4] = enc_4.fit_transform(df.iloc[:,4])\nenc_6 = LabelEncoder()\ndf.iloc[:,6] = enc_6.fit_transform(df.iloc[:,6])\nenc_8 = LabelEncoder()\ndf.iloc[:,8] = enc_8.fit_transform(df.iloc[:,8])\nenc_9 = LabelEncoder()\ndf.iloc[:,9] = enc_9.fit_transform(df.iloc[:,9])\nenc_11 = LabelEncoder()\ndf.iloc[:,11] = enc_11.fit_transform(df.iloc[:,11])\nenc_12 = LabelEncoder()\ndf.iloc[:,12] = enc_12.fit_transform(df.iloc[:,12])\nenc_13 = LabelEncoder()\ndf.iloc[:,13] = enc_13.fit_transform(df.iloc[:,13])\nenc_14 = LabelEncoder()\ndf.iloc[:,14] = enc_14.fit_transform(df.iloc[:,14])\nenc_15 = LabelEncoder()\ndf.iloc[:,15] = enc_15.fit_transform(df.iloc[:,15])\nenc_16 = LabelEncoder()\ndf.iloc[:,16] = enc_16.fit_transform(df.iloc[:,16])\nenc_17 = LabelEncoder()\ndf.iloc[:,17] = enc_17.fit_transform(df.iloc[:,17])\nenc_18 = LabelEncoder()\ndf.iloc[:,18] = enc_18.fit_transform(df.iloc[:,18])\nenc_20 = LabelEncoder()\ndf.iloc[:,20] = enc_20.fit_transform(df.iloc[:,20])\nenc_19 = LabelEncoder()\ndf.iloc[:,19] = enc_19.fit_transform(df.iloc[:,19])\nenc_21 = LabelEncoder()\ndf.iloc[:,21] = enc_21.fit_transform(df.iloc[:,21])","1b11ee45":"df.info()","ffbcd3d8":"X,y = df.iloc[:,0:32],df.iloc[:,32]\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","a13c3a9a":"# Before applying PCA the variables must be scaled\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e7dab5fa":"# Applying PCA\npca_none = PCA(n_components = None)\nX_train_none = pca_none.fit_transform(X_train)\nX_test_none = pca_none.transform(X_test)\nexplained_variance = pca_none.explained_variance_ratio_","f448d88b":"print(explained_variance*100)","bc3d7b51":"print((explained_variance*100).round(2))","c1846fee":"# Applying PCA to convert the features into 4 dimensions only.\npca = PCA(n_components = 4)\nX_train_4 = pca.fit_transform(X_train)\nX_test_4 = pca.transform(X_test)\n#explained_variance = pca.explained_variance_ratio_","00dc4428":"# We can also use Yellowbrick API for PCA\nfrom yellowbrick.features.pca import PCADecomposition","0f15b9cb":"visualizer = PCADecomposition()\nvisualizer.fit_transform(X_train)\n#Using poof command we can easily visualize the PCA plot\nvisualizer.poof()","baa72f93":"> This X_train and X_test can be used to make predictions by training them to a model.","33e90e7d":"> The first 4 variables explain a variance of 80.06% (25.85+22.61+18.82+12.78).","9678cd27":"**After applying PCA the 32 features have been converted to 8 features. In other words 32 dimensions have been reduced to 8 dimensions.**\n**Now lets see the variance of each feature.**","4a3692f5":"**PCA is a Dimensionality reduction technique.**\n* PCA is a unsupervised learning algorithm, because it doesn't require dependent variables.\n* PCA will identify the pattern in the dat and finds the correlation between the variables.\n* PCA is used for Noise filtering, Visualization and Feature extraction.\n* PCA is highly affected by the outliers in the data.\n"}}