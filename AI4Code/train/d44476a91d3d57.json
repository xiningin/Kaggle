{"cell_type":{"5f5a634c":"code","77c18844":"code","b82e3a91":"code","6f6f1238":"code","85e8ca47":"code","ab12bb5e":"code","237b56d0":"code","5823e894":"code","22d7eb1c":"code","e3f0664f":"code","f6b0e842":"code","fa1e8255":"code","c314604b":"code","c9a33207":"code","61bba6df":"code","a19e69e7":"code","622cff89":"code","2b5bc077":"code","a2527806":"code","237092b8":"code","32bbdcb9":"code","f3ec24f7":"code","262751a7":"code","2e3bf295":"code","b3c57600":"code","a680a61c":"code","14b04b45":"code","ff56c4de":"markdown","223a48cb":"markdown","7bd9bf1a":"markdown","a9599b4e":"markdown","b1a62d43":"markdown","3fe8b69e":"markdown","b39334d3":"markdown","8ab8b746":"markdown","12110c3a":"markdown","87cebfc0":"markdown","8ac000e5":"markdown","126f6f49":"markdown","a98cf8bf":"markdown","892e1c8e":"markdown","9d3e7a5e":"markdown"},"source":{"5f5a634c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nimport seaborn as sns\nfrom time import time\n\n#models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMClassifier, plot_importance\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.mixture import GaussianMixture\n\n#stacking stuff\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#PCA\nfrom sklearn.decomposition import PCA\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","77c18844":"global train\nglobal test\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id')\ntrain.head()","b82e3a91":"train.describe()","6f6f1238":"train.dtypes.unique()","85e8ca47":"def type_otimizator(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    return df\n\ntrain = type_otimizator(train)\ntest = type_otimizator(test)\ngc.collect()","ab12bb5e":"train.columns","237b56d0":"train[train.isna()].count()","5823e894":"[col + ' - ' + str(train[col].unique()) for col in train.filter(like='Soil', axis=1).columns]","22d7eb1c":"# let just atrib\nid_test = test.index\n\ny = train.Cover_Type\nx = train.drop(['Cover_Type'], axis = 1)\n\ntrain = train.drop(['Cover_Type'], axis = 1)","e3f0664f":"def getCorrelation():\n    fig, ax = plt.subplots(figsize=(15,13))\n    im = ax.imshow(train.astype(float).corr(), cmap=plt.cm.RdBu)\n\n    ax.set_xticks(np.arange(len(x.columns)))\n    ax.set_yticks(np.arange(len(x.columns)))\n    ax.set_xticklabels(x.columns)\n    ax.set_yticklabels(x.columns)\n\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    ax.set_title(\"Correlation\")\n    fig.tight_layout()\n    plt.colorbar(im);\n    plt.show()\n\ngetCorrelation()","f6b0e842":"# LightGBM \ndef getLGBC():\n    return LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.001,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 1,\n                     n_jobs=-1)","fa1e8255":"def getImportance():\n    clas_lgbc= getLGBC()\n    clas_lgbc.fit(x, y)\n    plot_importance(clas_lgbc, ignore_zero=False, figsize=(8,40))\n    \ngetImportance()\ngc.collect()\n\n\n","c314604b":"#let see some histogram\nplt.figure(figsize=(15,5))\nsns.distplot(x.Hillshade_3pm)\nplt.show()","c9a33207":"drop_columns = ['Soil_Type7', 'Soil_Type15', 'Soil_Type36', 'Soil_Type28', 'Soil_Type27', 'Soil_Type25', 'Soil_Type21', 'Soil_Type9', 'Soil_Type8']","61bba6df":"x = x.drop(drop_columns, axis =1)\ntest = test.drop(drop_columns, axis =1)\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","a19e69e7":"#see the best correlation for Hillshade_3pm\ncorr = x[x.Hillshade_3pm!=0].corr()\nplt.figure(figsize=(20,16))\nsns.heatmap(corr,annot=True)","622cff89":"    \ndef predictHillshade_3pm():\n    t = time()\n    all_data = x.append(test)\n    num_train = len(x)\n\n    cols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\n    HS_zero = all_data[all_data.Hillshade_3pm==0]\n    HS_zero.shape\n\n    HS_train = all_data[all_data.Hillshade_3pm!=0]\n\n    rf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\n    out = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\n    all_data.loc[HS_zero.index,'Hillshade_3pm'] = out\n\n    x['Hillshade_3pm']= all_data.loc[:num_train,'Hillshade_3pm']\n    test['Hillshade_3pm']= all_data.loc[num_train:,'Hillshade_3pm']\n    print('duration: ' + str(time() - t))\n    \n\npredictHillshade_3pm()\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)\n","2b5bc077":"getCorrelation()","a2527806":"ti = time()\npca = PCA(n_components=0.99)\ndfPCA = x.append(test)\nnum_pca = len(x)\ntrans = pca.fit_transform(dfPCA)\nfor i in range(trans.shape[1]):\n    col = 'pca_' + str(i)\n    x[col] = trans[:num_pca, i]\n    test[col] = trans[num_pca:, i]\nprint('duration: ' + str(time() - ti))","237092b8":"def makePCA(df, columnsOrigin, numElement):\n    pca = PCA(n_components=numElement)\n    dfPCA = df[columnsOrigin].copy()\n    elements = pca.fit_transform(dfPCA)\n    columnPCA = ['Column_'+str(c) for c in range(numElement)]\n    return pd.DataFrame(data = elements, columns = columnPCA)\n\n#pca_columns = ['Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type18', 'Soil_Type22', 'Soil_Type22', 'Soil_Type26', 'Soil_Type30', 'Soil_Type35', 'Soil_Type37', 'Soil_Type38']\npca_columns = ['Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type18', 'Soil_Type19', 'Soil_Type22', 'Soil_Type22', 'Soil_Type26', 'Soil_Type30', 'Soil_Type35', 'Soil_Type37', 'Soil_Type38']\n\n#x = x.drop(pca_columns, axis =1)\n#test = test.drop(pca_columns, axis =1)\nx.join(makePCA(x, pca_columns, 7))\nx = x.drop(pca_columns, axis =1)\n\ntest.join(makePCA(test, pca_columns, 7))\ntest = test.drop(pca_columns, axis =1)\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","32bbdcb9":"def getGausianMixtureClassifier(components):\n    return GaussianMixture(n_components=components)\n\ndef applyGausianMixture():\n    t = time()\n    num_train = len(x)\n    all_data = x.append(test)\n    \n    components = 10\n    gm = getGausianMixtureClassifier(components)\n    \n    pred = gm.fit_predict(StandardScaler().fit_transform(all_data))\n    x['GM'] = pred[:num_train]\n    test['GM'] = pred[num_train:]\n    \n    for i in range(components):\n        x['GM'+str(i)] = pred[:num_train]==i  \n        test['GM'+str(i)] = pred[num_train:]==i\n    print('duration: '+ str(time()-t))\n    \n#this is used in final predict\ndef getGausianMixture(x_gm):\n    t = time()\n    components = 10\n    gm = getGausianMixtureClassifier(components)\n    pred = gm.fit_predict(StandardScaler().fit_transform(x_gm))\n    \n    x_gm['GM'] = pred\n    for i in range(components):\n        x_gm['GM'+str(i)] = pred[:] == i\n    print('duration: '+ str(time()-t))\n    return x_gm\n        \n    \napplyGausianMixture()\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)\ngc.collect()","f3ec24f7":"x.head()","262751a7":"# Fit the classifier\ndef fitClassifier(classifier, classifier_Name, xx_train, yy_train, xx_test, yy_test):\n    classifier.fit(xx_train, yy_train)\n    pred_train = classifier.predict(xx_test)\n    print(classifier_Name + ': ' + str(round(accuracy_score(yy_test, pred_train),5)))\n    \n# Random Forest.\ndef getRandomForest():\n    return RandomForestClassifier(n_jobs =  -1, n_estimators = 500, max_features = 15, max_depth = 40, random_state = 1)    \n\n# XGBoost\ndef getXGB():\n    return xgb.XGBClassifier(n_estimators = 800, max_depth = 40, random_state = 1 )\n    \n    \n\n#Neural network\ndef getNN():\n    return MLPClassifier(verbose = False,\n                         max_iter=1000,\n                         tol = 0.0000000010,\n                         solver = 'adam',\n                          hidden_layer_sizes=(100),\n                          activation='relu')\n\n#Neural network (Dense)\ndef getDenseNN():\n    return MLPClassifier(verbose = True,\n                        max_iter=1000,\n                        tol = 0.00000000010,\n                        solver = 'adam',\n                        hidden_layer_sizes=(120,60),\n                        activation='relu')\n\n#extra tree\ndef getExtraTree():\n    return ExtraTreesClassifier(n_estimators = 1000, max_features = 15, max_depth = 40, random_state = 1)    \n\n\n# logistic regretion from stacking\ndef getLogisticRegression():\n    return LogisticRegression(max_iter=1500,\n                              n_jobs=-1,\n                              solver= 'lbfgs',\n                              multi_class = 'multinomial')\n\n# catBooster\ndef getCatBooster():\n    return CatBoostClassifier(iterations=3000, depth=10, eval_metric = 'Accuracy')\n    \n\n# adaBoost\ndef getAdaBoostClassifier():\n    return AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=15), n_estimators = 1500, random_state = 1, learning_rate =0.00001)\n\n#LigthGBMClassifier\ndef getLGBClassifier():\n    return LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.0001,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 1,\n                     n_jobs=-1)\n# 2Layer classifier\ndef get2LayerClassifier():\n    return ExtraTreesClassifier(n_estimators = 500, max_features = 4, max_depth = 20, random_state = 1)    \n    ","2e3bf295":"def score_LGBC():\n    ti = time()\n    clas_lgbc = getLGBC()\n    fitClassifier(clas_lgbc, 'LGBC', x_train, y_train, x_test, y_test)\n    print('duration of LGBC: ' + str(time() - ti))\n    \ndef score_XGB():\n    ti = time()\n    clas_xgb = getXGB()\n    fitClassifier(clas_xgb, 'XGB', x_train, y_train, x_test, y_test)\n    print('duration of XGB: ' + str(time() - ti))\n    \ndef score_ExtraTree():\n    ti = time()\n    clas_extraTree = getExtraTree()\n    fitClassifier(clas_extraTree, 'ExtraTree', x_train, y_train, x_test, y_test)\n    print('duration of ExtraTree: ' + str(time() - ti))\n    \ndef score_RandomForest():\n    ti = time()\n    clas_RandomForest = getRandomForest()\n    fitClassifier(clas_RandomForest, 'RandomForest', x_train, y_train, x_test, y_test)\n    print('duration of RandomForest: ' + str(time() - ti))\n\ndef score_CatBooser():\n    ti = time()\n    clas_Cat = getCatBooster()\n    fitClassifier(clas_Cat, 'CatBooster', x_train, y_train, x_test, y_test)\n    #clas_Cat.fit(x_train, y_train, eval_set=Pool(x_test, y_test))\n    pred_train = clas_Cat.predict(x_test)\n    print('CatBooster' + ': ' + str(round(accuracy_score(y_test, pred_train),5)))    \n    print('duration of CatBooster: ' + str(time() - ti))\n    \ndef score_AdaBoost():\n    ti = time()\n    clas_Ada = getAdaBoostClassifier()\n    fitClassifier(clas_Ada, 'AdaBooster', x_train, y_train, x_test, y_test)\n    print('duration of AdaBooster: ' + str(time() - ti))\n\n\n \n#score_CatBooser() # ** -> 0.87103 -> 0.87676\n#score_ExtraTree() #0.87235 -> 0.875 -> 0.88536\n#score_LGBC() #0.76698 -> 0.76786 -> 0.77888\n#score_XGB() #0.87037 -> 0.86728 -> 0.87324\n#score_RandomForest() #0.86023 -> 0.85935 -> 0.86905\n#score_AdaBoost() # ** -> ** -> 0.79586\n\ngc.collect()","b3c57600":"# Stacking\nclas_rf = getRandomForest()\nclas_xgb = getXGB()\nclas_extree = getExtraTree()\nclas_cat = getCatBooster()\n\n\nclas_rf.fit(x, y)\nclas_xgb.fit(x, y)\nclas_extree.fit(x, y)\nclas_cat.fit(x, y)\n\npred_rf = clas_rf.predict(x)\npred_xgb = clas_xgb.predict(x)\npred_extree = clas_extree.predict(x)\npred_cat = clas_cat.predict(x)\n\n# test\nx_stacked = pd.DataFrame({'pred_rf' : pred_rf,\n                          'pred_xgb' : pred_xgb,\n                          'pred_extree' : pred_extree,\n                          'pred_cat' : pred_cat[:,0].astype(int)})\n\nx_stacked.append(getGausianMixture(x_stacked))\n\nclas_final = get2LayerClassifier()\nclas_final.fit(x_stacked, y)\n\n","a680a61c":"pred_rf = clas_rf.predict(test)\npred_xgb = clas_xgb.predict(test)\npred_extree = clas_extree.predict(test)\npred_cat = clas_cat.predict(test)\n\n# test\ntest_stacked = pd.DataFrame({'pred_rf' : pred_rf,\n                             'pred_xgb' : pred_xgb,\n                             'pred_extree' : pred_extree,\n                             'pred_cat' : pred_cat[:,0].astype(int)})\n\ntest_stacked.append(getGausianMixture(test_stacked))\n\nprediction = clas_final.predict(test_stacked)","14b04b45":"submission = pd.DataFrame({ 'Id': id_test,\n                            'Cover_Type': prediction })\nsubmission.to_csv(\"submission_example.csv\", index=False)","ff56c4de":"Get this idea from: \nhttps:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover","223a48cb":"I am going to try predict without these columns. Better if i make a list of drop columns","7bd9bf1a":"If all is int64, let make some otimization for memory usage","a9599b4e":"Let see what we get","b1a62d43":"**Exploratory analisys**","3fe8b69e":"**Lets make the predictor's (finally, the fun!)**\n\nI wil make some loop to try to find de best prediction for each one, and use the best result. For this initial version, i make it simple, and do not put the loop, for performance reason.\n\nAny alteration on the split data, wil be in a new variable, just to reuse the same split many times","b39334d3":"All the data is number, and the Soil_Type is 0 and 1, i thing dont need use scaler on that. On the others columns i need test. But i do it later, lets see the data correlation.","8ab8b746":"Lets make more easy to predict adding some Gaussian Mixture.\n\nGet the idea from: https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover","12110c3a":"Look like all data is fill up, and looking at describe(), maybe the columns \"Soil_\" types maybe get only 0 and 1, let see that","87cebfc0":"We get some noise, let's encode then to try make data more clear","8ac000e5":"Look likes soil_type7 and soil_type_15 arent good for prediction.\n\nI will look for another \"poor data\", using de LGBC. Let's see the \"importance\" of some data. And drop the zeros one.","126f6f49":"Lets drop here, and make the test and train split.","a98cf8bf":"The idea of histogram on Hillshade_3pm i get from https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover","892e1c8e":"First competition from a beginner in data science. I am make a initial little analisys and try some predict.\n\nCritices are welcome, i really want to learn.\n\nI cold get a team, because i dont get the email.","9d3e7a5e":"Lets see the correlation again"}}