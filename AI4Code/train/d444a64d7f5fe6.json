{"cell_type":{"2fad0737":"code","7b9fb851":"code","fb8735b7":"code","6120770e":"code","ffd6931f":"code","d8ea6b12":"code","81ea1f37":"code","0b9bd2b9":"code","2be83de3":"code","63de19ef":"markdown","f467b294":"markdown","37cf66cc":"markdown"},"source":{"2fad0737":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.callbacks import *\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.inception_v3 import InceptionV3\nimport skimage\nfrom skimage.color import rgb2gray\nimport sklearn\nfrom sklearn.metrics import *\nfrom imageio import imread\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob","7b9fb851":"test = pd.DataFrame()\ntest['file'] = glob.glob(\"..\/input\/walk_or_run_test\/test\/run\/*\")+glob.glob(\"..\/input\/walk_or_run_test\/test\/walk\/*\")\ntest['label'] = [ 1 for _ in glob.glob(\"..\/input\/walk_or_run_test\/test\/run\/*\")]+[0 for _ in glob.glob(\"..\/input\/walk_or_run_test\/test\/walk\/*\")]\n\ntrain = pd.DataFrame()\ntrain['file'] = glob.glob(\"..\/input\/walk_or_run_train\/train\/run\/*\")+glob.glob(\"..\/input\/walk_or_run_train\/train\/walk\/*\")\ntrain['label'] = [ 1 for _ in glob.glob(\"..\/input\/walk_or_run_train\/train\/run\/*\")]+[0 for _ in glob.glob(\"..\/input\/walk_or_run_train\/train\/walk\/*\")]\ntrain = train.sample(frac=1).reset_index(drop=True)\ntrain.head()","fb8735b7":"def make_model():\n    transfer_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=(None,None,3), pooling='avg', classes=1000)\n    model = Sequential()\n    model.add(InputLayer((None,None,3)))\n    model.add(transfer_model)\n    model.add(Dropout(0.5))\n    model.add(Dense(1,activation='sigmoid'))\n    return transfer_model, model\n\ntransfer_model, model = make_model()\nmodel.summary()","6120770e":"def image_gen(files, labels, batch_size=10, randomized=False, random_seed=1):\n    rng = np.random.RandomState(random_seed)\n    img_batch = []\n    label_batch = []\n    while True:\n        indices = np.arange(len(files))\n        if randomized:\n            rng.shuffle(indices)\n        for index in indices:\n            img = imread(files[index])[:,:,0:3]\/255\n            label = labels[index]\n            img_batch.append(img)\n            label_batch.append(label)\n            if len(img_batch) == batch_size:\n                yield np.array(img_batch), np.array(label_batch)\n                img_batch = []\n                label_batch = []\n        \n        if len(img_batch) > 0:\n                yield np.array(img_batch), np.array(label_batch)\n                img_batch = []\n                label_batch = []","ffd6931f":"transfer_model.trainable=False\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\nbatch_size=100\nepochs=50\nhistory = model.fit_generator(image_gen(train['file'], train['label'], batch_size=batch_size, randomized=True, random_seed=1),\n                    steps_per_epoch=int(np.ceil(len(train)\/batch_size)),\n                    epochs=epochs,\n                    validation_data=image_gen(test['file'], test['label'], batch_size=batch_size, randomized=True),\n                    validation_steps=int(np.ceil(len(test)\/batch_size)),\n                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=0), ModelCheckpoint(filepath='.\/weights.hdf5', monitor='val_loss', verbose=0, save_best_only=True)],\n                    verbose=2,\n                   )\nmodel.load_weights('weights.hdf5')","d8ea6b12":"def plot_history(history):\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'], 'r')\n    plt.plot(history.history['val_loss'], 'b')\n    plt.subplot(1,2,2)\n    plt.plot(history.history['acc'], 'r')\n    plt.plot(history.history['val_acc'], 'b')\n    \nplot_history(history)","81ea1f37":"!rm -f *.hdf5\n\ntransfer_model.trainable=True\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\nbatch_size=100\nepochs=50\nhistory = model.fit_generator(image_gen(train['file'], train['label'], batch_size=batch_size, randomized=True, random_seed=1),\n                    steps_per_epoch=int(np.ceil(len(train)\/batch_size)),\n                    epochs=epochs,\n                    validation_data=image_gen(test['file'], test['label'], batch_size=batch_size, randomized=True),\n                    validation_steps=int(np.ceil(len(test)\/batch_size)),\n                    callbacks=[ModelCheckpoint(filepath='.\/weights.hdf5', monitor='val_loss', verbose=0, save_best_only=True)],\n                    verbose=2,\n                   )\nmodel.load_weights('weights.hdf5')","0b9bd2b9":"plot_history(history)","2be83de3":"model.load_weights('weights.hdf5')\npredicted_prob = model.predict_generator(image_gen(test['file'], test['label'], batch_size=1, randomized=False),\n                    steps=len(test))\npredicted = np.round(predicted_prob)\n\ntruth = test['label'].values\nprint(classification_report(truth, predicted))\nsns.heatmap(confusion_matrix(truth, predicted), annot=True)","63de19ef":"First transfer model is frozen, and train the top layer(s).","f467b294":"# Classifying Running & Walking\n\n## Conclusion\n\nIn Keras, we can define CNN to take variable-shape input, as long as there is no layer that require shape info, such as Flatten. For such CNNs, global pooling is used to transform 2d tensor to 1d. \n\nThis classification task is a hard problem with limited training set. As a result, it is easy to overfit with a very simply CNN. The symptom is that val_acc fluctuates wildy, and is way less than training accuracy. Using a pretrained ConvNet will mitigate overfitting, and the accuracy of the test set can reach 85%.","37cf66cc":"Then the transfer model is unfrozn for fine-tuning."}}