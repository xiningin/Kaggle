{"cell_type":{"3977482e":"code","7b301dd0":"code","31bb7c2d":"code","b47d0100":"code","f9aacfdc":"code","b41e469f":"code","d579d16e":"code","97ddf7b1":"code","dbe77d13":"code","978a302c":"code","d366980d":"code","7ff2ef45":"code","0acd5fb7":"markdown","95cc3d25":"markdown","6487dab1":"markdown","ae6c6976":"markdown","ec970052":"markdown","6d43a620":"markdown","068bbf48":"markdown","325d9595":"markdown","310c80bf":"markdown","ae6b3637":"markdown","cfad1367":"markdown","fe47f5a5":"markdown"},"source":{"3977482e":"import os\nos.chdir('..\/input')\nprint(os.listdir(\"..\/input\"))","7b301dd0":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport tensorflow as tf\nimport zipfile\n\nfrom collections import defaultdict\nfrom io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\nfrom mods_folder import ops as utils_ops\n\nimport json\n\nwith open('mods_folder\/old_oid_labels.json') as f:\n    old_oid = json.load(f)\n ","31bb7c2d":"# This is needed to display the images.\n%matplotlib inline","b47d0100":"#MODEL_NAME = 'r-faster-cnn-inception-v2'\nMODEL_NAME = 'slow-rcnn'\nPATH_TO_CKPT = MODEL_NAME + '\/frozen_inference_graph.pb'\n\nPATH_TO_LABELS = 'mods_folder\/oid_bbox_trainable_label_map.pbtxt'\n\nNUM_CLASSES = 545","f9aacfdc":"from mods_folder import label_map_util\n\nfrom mods_folder import visualization_utils as vis_util","b41e469f":"detection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name='')","d579d16e":"label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)","97ddf7b1":"def load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)","dbe77d13":"os.listdir('google-ai-open-images-object-detection-track\/test\/challenge2018_test')\nPATH_TO_TEST_IMAGES_DIR = 'google-ai-open-images-object-detection-track\/test\/challenge2018_test'\n\nTEST_IMAGE_PATHS = os.listdir('google-ai-open-images-object-detection-track\/test\/challenge2018_test')   \n\nTEST_IMAGE_PATHS = TEST_IMAGE_PATHS[:9]\n\nIMAGE_SIZE = (12, 8)","978a302c":"def run_inference_for_single_image(image, graph):\n  with graph.as_default():\n    with tf.Session() as sess:\n      # Get handles to input and output tensors\n      ops = tf.get_default_graph().get_operations()\n      all_tensor_names = {output.name for op in ops for output in op.outputs}\n      tensor_dict = {}\n      for key in [\n          'num_detections', 'detection_boxes', 'detection_scores',\n          'detection_classes', 'detection_masks'\n      ]:\n        tensor_name = key + ':0'\n        if tensor_name in all_tensor_names:\n          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n              tensor_name)\n      if 'detection_masks' in tensor_dict:\n        # The following processing is only for single image\n        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n        detection_masks_reframed = tf.cast(\n            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n        # Follow the convention by adding back the batch dimension\n        tensor_dict['detection_masks'] = tf.expand_dims(\n            detection_masks_reframed, 0)\n      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n\n      # Run inference\n      output_dict = sess.run(tensor_dict,\n                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n      # all outputs are float32 numpy arrays, so convert types as appropriate\n      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n      output_dict['detection_classes'] = output_dict[\n          'detection_classes'][0].astype(np.uint8)\n      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n      if 'detection_masks' in output_dict:\n        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n  return output_dict","d366980d":"prediction_list = []\nfor image_name in TEST_IMAGE_PATHS:\n  image_path = os.path.join('google-ai-open-images-object-detection-track\/test\/challenge2018_test',image_name)  \n  image = Image.open(image_path)\n  # the array based representation of the image will be used later in order to prepare the\n  # result image with boxes and labels on it.\n  image_np = load_image_into_numpy_array(image)\n  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n  image_np_expanded = np.expand_dims(image_np, axis=0)\n  # Actual detection.\n  output_dict = run_inference_for_single_image(image_np, detection_graph)\n    \n  # Visualization of the results of a detection.\n  detection_boxes = np.around(output_dict['detection_boxes'],decimals=2)\n  detection_classes = output_dict['detection_classes']\n  detection_scores = np.around(output_dict['detection_scores'],decimals=1)  \n  ind = np.where(output_dict['detection_boxes'].any(axis=1))[0]\n  ind = list(ind)\n\n  def get_class(id_num):\n        class_label = ''\n        for i in old_oid:\n            if(i['id'] == id_num):\n                class_label = i['name']\n                break\n        return class_label        \n  def get_class_name(id_num):\n        class_name = ''\n        for i in old_oid:\n            if(i['id'] == id_num):\n                class_name = i['display_name']\n                break\n        return class_name \n        \n                \n  pred_str = ''      \n  pred_name = ''\n  for i in ind: \n      l = output_dict['detection_boxes'][i]\n      id_num = output_dict['detection_classes'][i]\n      cls_label = get_class(id_num)\n      cls_name = get_class_name(id_num)  \n      prob = str('{:.4f}'.format(output_dict[\"detection_scores\"][i]))\n      pred_name = pred_name + cls_name + ' ' + prob + ' | '  \n      bounding_box = ' '.join([str('{:.4f}'.format(w))+' ' for w in l])  \n      pred_str = pred_str + cls_label + ' ' + prob + ' ' + bounding_box + ' '\n    \n  vis_util.visualize_boxes_and_labels_on_image_array(\n      image_np,\n      output_dict['detection_boxes'],\n      output_dict['detection_classes'],\n      output_dict['detection_scores'],\n      category_index,\n      instance_masks=output_dict.get('detection_masks'),\n      use_normalized_coordinates=True,\n      line_thickness=8)\n  plt.figure(figsize=IMAGE_SIZE)  \n  plt.imshow(image_np)\n  plt.show()\n  prediction_object = {'ImageId':image_name[:-4], 'PredictionString':pred_str}  \n  print('Detected Classes =>')\n  print(pred_name[:-1])\n  print()\n  print(prediction_object)\n  prediction_list.append(prediction_object)  \n","7ff2ef45":"df = pd.DataFrame.from_dict(prediction_list, orient='columns')\ndf.to_csv('..\/working\/prediction.csv', index=False)","0acd5fb7":"**Folders in input directory, those contain all the necessary files**","95cc3d25":"**Getting all the challenge test images**\n\n*running prediction on just 9 images, because 9 is my lucky number, kidding*","6487dab1":"This is a baseline kernel, the purpose of this kernel to provide the insight of the competition. It is using [Faster RCNN Inception Resnet v2](https:\/\/www.kaggle.com\/aldrin644\/r-faster-cnn-inception-v2) pretrained model on Old Open Image Dataset that contains 545 classes similar to New Open Image Dataset. I have done some analysis between old and new dataset's classes, checkout this [kernel](https:\/\/www.kaggle.com\/aldrin644\/analysis-between-new-and-old-open-image-dataset). I have used [Model Zoo's](https:\/\/www.kaggle.com\/aldrin644\/mods_folder) utility files for object detection purpose. ","ae6c6976":"**Setting path for model and labels **","ec970052":"**Loading class labels**","6d43a620":"**Saving output into CSV**","068bbf48":"**Converting images into numpy array**","325d9595":"**And the for loop that calls the main function again & again**\n\n*but only 9 times*","310c80bf":"**Here comes the main function**","ae6b3637":"**Loading the frozen graph**","cfad1367":"**Following imports are for visualization of bounding boxes on Image** \n\n*Just ignore the warning* ","fe47f5a5":"**Importing all the necessary files**"}}