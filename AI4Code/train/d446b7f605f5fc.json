{"cell_type":{"0e04c13a":"code","7e3fae96":"code","c267fd13":"code","69ac935c":"code","2502e20c":"code","a6fb6f72":"code","e5779fe5":"code","3d7e064e":"code","99d02b34":"code","fee3d955":"code","1bc9fca9":"code","6b999533":"code","0e9b6899":"code","9ec76f86":"code","394ce524":"code","0c4d01e4":"code","6e35fdac":"code","75df12f0":"code","37dbc6d6":"code","6fe78e01":"code","73627683":"code","6cc9e3e8":"code","b08c2dfd":"code","2334805a":"code","ee27cb60":"code","3485a31d":"code","dca00cc6":"code","595f5ff6":"code","57c93644":"code","5f539bd0":"code","0200115f":"code","973f03b5":"code","28f4ff79":"code","1931ad1b":"code","e71c2e64":"code","50e165d0":"code","ba46b695":"code","4ebfdf79":"code","477f4238":"code","2cae60d3":"code","f546be14":"code","75871493":"code","0a17b99a":"code","bf0e2d00":"code","afbe9a19":"code","60a25069":"markdown","03654576":"markdown","c346aeb3":"markdown","db27ac54":"markdown","fcd8a0e4":"markdown","1e018816":"markdown","e4e20526":"markdown","5b34d26a":"markdown","399c53a8":"markdown","9d8802d7":"markdown","29f75345":"markdown","7552f419":"markdown","690414c7":"markdown","059b3c81":"markdown","33c3c742":"markdown","8f6a2307":"markdown","5e72fc2e":"markdown","fb3f2bc1":"markdown","d45950d0":"markdown","0e8cf14e":"markdown","6a3b5c9f":"markdown","0f38ad03":"markdown","0a4c58b5":"markdown","d51332d7":"markdown","69817f79":"markdown","2a1de2dc":"markdown","f90df32e":"markdown","9441e897":"markdown"},"source":{"0e04c13a":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport re\nimport nltk\nfrom tqdm import tqdm\nimport scipy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","7e3fae96":"#loading the data\ndf=pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')","c267fd13":"print(df.shape)   #check the shape of the data","69ac935c":"print(df.head())","2502e20c":"print(df.info())","a6fb6f72":"df['label'].value_counts()","e5779fe5":"y_value_counts=df['label'].value_counts()\nprint(\"Racist tweets  = \",y_value_counts[1], \"with percentage \", (y_value_counts[1]*100)\/(y_value_counts[0]+y_value_counts[1]),'%')\nprint(\"Not Racist tweets  = \",y_value_counts[0], \"with percentage \", (y_value_counts[0]*100)\/(y_value_counts[0]+y_value_counts[1]),'%')","3d7e064e":"#lets see the classes through bar graph\ndata=dict(racist=y_value_counts[1],not_racist=y_value_counts[0])\ncls=data.keys()\nvalue=data.values()\n\nplt.bar(cls,value,color='maroon',width=0.2)","99d02b34":"df['tweet']=df['tweet'].str.replace(' ','_')\ndf['tweet']=df['tweet'].str.replace('-','_')\ndf['tweet']=df['tweet'].str.lower()","fee3d955":"def expand(sent):\n    \"This function will replace english short notations with full form\"\n    \n    sent=re.sub(r\"can't\", \"can not\",sent)\n    sent=re.sub(r\"won't\", \"will not\",sent)\n    \n    sent=re.sub(r\"n\\'t\", \" not\",sent)\n    sent=re.sub(r\"\\'re\", \" are\",sent)\n    sent=re.sub(r\"\\'m\",\" am\",sent)\n    sent=re.sub(r\"\\'s\",\" is\",sent)\n    sent=re.sub(r\"\\'ll\",\" will\",sent)\n    sent=re.sub(r\"\\'ve\",\" have\",sent)\n    sent=re.sub(r\"\\'d\",\" would\",sent)\n    sent=re.sub(r\"\\'t\", \" not\",sent)\n    \n    return sent\n    ","1bc9fca9":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","6b999533":"def preprocess_tweet(text):\n    \"function for preprocess the text data\"\n    \n    preprocessed_tweet=[]\n    \n    for sentence in tqdm(text):\n        sent=expand(sentence)\n        sent=sent.replace(\"\\\\r\",\" \")\n        sent=sent.replace(\"\\\\n\",\" \")\n        sent=sent.replace('\\\\\"',\" \")\n        sent=re.sub(\"[^A-Za-z0-9]+\",\" \",sent)\n        \n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent=\" \".join(i for i in sent.split() if i.lower() not in stopwords)\n        preprocessed_tweet.append(sent.lower().strip())\n        \n    return preprocessed_tweet\n        ","0e9b6899":"preprocessed_tweets=preprocess_tweet(df['tweet'].values)","9ec76f86":"df['tweet']=preprocessed_tweets","394ce524":"df[\"tweet\"][10]","0c4d01e4":"y=df['label']\nx=df.drop(['label'],axis=1)\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=40)\n","6e35fdac":"vect=TfidfVectorizer(min_df=10)\n\nvect.fit(x_train['tweet'].values)\n\ntrain_tweet=vect.transform(x_train['tweet'].values)\ntest_tweet=vect.transform(x_test['tweet'].values)\n\nprint(train_tweet.shape,y_train.shape)\nprint(test_tweet.shape,y_test.shape)","75df12f0":"#calculating sentiment scores for train data\nx_train_sent=np.ndarray.tolist(x_train[\"tweet\"].values)\n\nsia=SentimentIntensityAnalyzer()\nps=[]\nfor i in range(len(x_train_sent)):\n    ps.append((sia.polarity_scores((x_train_sent[i]))))\n    \nx_train_polarity=np.array(ps)\nx_train_polarity=x_train_polarity.reshape(-1,1)\nx_train_polarity.shape\n","37dbc6d6":"#storing only scores of sentiment\nx_t=[]\nfor i in range(len(x_train)):\n    for j in x_train_polarity[0][0]:\n        x_t.append(x_train_polarity[i][0][j])\nx_t=np.array(x_t)\nx_t=x_t.reshape(-1,4)\nx_t.shape","6fe78e01":"#calculating sentiment scores for test data\nx_test_sent=np.ndarray.tolist(x_test[\"tweet\"].values)\n\nsia=SentimentIntensityAnalyzer()\nps=[]\nfor i in range(len(x_test_sent)):\n    ps.append((sia.polarity_scores((x_test_sent[i]))))\n    \nx_test_polarity=np.array(ps)\nx_test_polarity=x_test_polarity.reshape(-1,1)\nx_test_polarity.shape\n","73627683":"#storing only scores of sentiment\nx_tests=[]\nfor i in range(len(x_test)):\n    for j in x_test_polarity[0][0]:\n        x_tests.append(x_test_polarity[i][0][j])\nx_tests=np.array(x_tests)\nx_tests=x_tests.reshape(-1,4)\nx_tests.shape","6cc9e3e8":"from scipy.sparse import hstack","b08c2dfd":"x_tr=hstack((train_tweet,x_t))\nx_te=hstack((test_tweet,x_tests))\n\nprint(x_tr.shape)\nprint(x_te.shape)","2334805a":"wt={0:1,1:5}            #since the data is imbalanced , we assign some more weight to class 1\n\nclf=DecisionTreeClassifier(class_weight=wt)\n\nparameters=dict(max_depth=[1,5,10,50],min_samples_split=[5,10,100,500])\n\nsearch=RandomizedSearchCV(clf,parameters,random_state=10)\nresult=search.fit(x_tr,y_train)\nresult.cv_results_","ee27cb60":"search.best_params_","3485a31d":"cls = DecisionTreeClassifier(max_depth=50,min_samples_split=5,random_state=10,class_weight=wt)\ncls.fit(x_tr,y_train)","dca00cc6":"y_pred_train=cls.predict(x_tr)\ny_pred_test=cls.predict(x_te)","595f5ff6":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,y_pred_train)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,y_pred_test)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","57c93644":"def find_best_threshold(threshold, fpr, tpr):\n    \"\"\"it will give best threshold value that will give the least fpr\"\"\"\n    t = threshold[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    \n    return t\n\ndef predict_with_best_t(proba, threshold):\n    \"\"\"this will give predictions based on best threshold value\"\"\"\n    predictions = []\n    for i in proba:\n        if i>=threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","5f539bd0":"#computing confusion matrix for set_1\n\nfrom sklearn.metrics import confusion_matrix\nbest_t = find_best_threshold(tr_treshold, train_fpr, train_tpr)\nprint(\"Train confusion matrix\")\nm_tr=(confusion_matrix(y_train, predict_with_best_t(y_pred_train, best_t)))\nprint(m_tr)\nprint(\"Test confusion matrix\")\nm_te=(confusion_matrix(y_test, predict_with_best_t(y_pred_test, best_t)))\nprint(m_te)","0200115f":"print(classification_report(y_test, y_pred_test))","973f03b5":"vec=CountVectorizer(min_df=10)\nvec.fit(x_train['tweet'].values)\n\nx_tr_count=vec.transform(x_train['tweet'].values)\nx_te_count=vec.transform(x_test['tweet'].values)\nx_tr_count.shape","28f4ff79":"x_tr_data=hstack((x_tr_count,x_t))\nx_te_data=hstack((x_te_count,x_tests))\n\nx_trn=scipy.sparse.csr_matrix(x_tr_count)\nx_tst=scipy.sparse.csr_matrix(x_te_count)","1931ad1b":"from sklearn.naive_bayes import MultinomialNB","e71c2e64":"mod = MultinomialNB()\nmod.fit(x_trn,y_train)","50e165d0":"train_pred=mod.predict(x_trn)\ntest_pred=mod.predict(x_tst)","ba46b695":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,train_pred)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,test_pred)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","4ebfdf79":"#get the summary of this model\n\nprint(classification_report(y_test, test_pred))","477f4238":"from xgboost import XGBClassifier","2cae60d3":"xg=XGBClassifier()\nparam=dict(max_depth=[4,6,8,10],n_estimators=[100,500,1000,1500])\nsearch=RandomizedSearchCV(xg,param,random_state=10)\nsrch=search.fit(x_tr,y_train)\nsrch.cv_results_","f546be14":"srch.best_estimator_","75871493":"xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None).fit(x_tr, y_train)\n\nprediction = xgb.predict(x_te) \n\nf1_score(y_test, prediction)","0a17b99a":"train_prediction=xgb.predict(x_tr)","bf0e2d00":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,train_prediction)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,prediction)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","afbe9a19":"print(classification_report(y_test, prediction))","60a25069":"#### TFIDF for text data","03654576":"                   precision    recall  f1-score   support\n\n              0       0.97      0.97      0.97      5945\n              1       0.63      0.66      0.64       448\n\n    accuracy                               0.95      6393\n    macro avg          0.80      0.82      0.81      6393\n    weighted avg       0.95      0.95      0.95      6393\n","c346aeb3":"### Splitting data into train and test","db27ac54":"### CountVectorizer()","fcd8a0e4":"### DecisionTreeClassifier()","1e018816":"***We got auc score= 0.7625***","e4e20526":"                     precision    recall  f1-score   support\n\n               0       0.96      0.99      0.98      5945\n               1       0.77      0.47      0.58       448\n\n    accuracy                               0.95      6393\n    macro avg          0.87      0.73      0.78      6393\n    weighted avg       0.95      0.95      0.95      6393\n\n\u200b","5b34d26a":"##### Now the text data is cleaned","399c53a8":"Since the data is in text format, we have to preprocess the data and clean the data to vectorize the data.","9d8802d7":"|MODEL|TEST AUC|\n|----|----|\n|DECISION TREE|0.7625|\n|NAIVE BAYES|0.8157|\n|XGBOOST|0.7280|","29f75345":"## NAIVE BAYES","7552f419":"### XGBOOST","690414c7":"***From the bar graph we can clearly see that there are more not racist tweets than the racist tweets.***","059b3c81":"We can observe that there are more reviews with 0 label i.e. tweet is not racist\/sexist.<br>\nSo our dataset is imbalanced","33c3c742":"#### Hyperparameter Tuning","8f6a2307":"                   precision    recall  f1-score   support\n\n              0       0.97      0.95      0.96      5945\n              1       0.48      0.57      0.52       448\n\n    accuracy                               0.93      6393\n    macro avg          0.72      0.76      0.74      6393\n    weighted avg       0.93      0.93      0.93      6393\n","5e72fc2e":"### NAIVE BAYES","fb3f2bc1":"***Now we are ready with the data.***","d45950d0":"***TEST AUC = 0.7280***","0e8cf14e":"## SUMMARY","6a3b5c9f":"### DECISION TREE","0f38ad03":"## Data Preprocessing","0a4c58b5":"## XGBOOST","d51332d7":"First we will replace the all blank spaces, - with underscore and convert all the letters to lower case.","69817f79":"### Vectorization...","2a1de2dc":"##### Convert the vectors into scipy.sparse matrix","f90df32e":"## IMPORT LIBRARIES...","9441e897":"***TEST AUC = 0.8157***"}}