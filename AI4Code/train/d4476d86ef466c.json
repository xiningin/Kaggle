{"cell_type":{"59b86464":"code","f4bc37a6":"code","e59cb078":"code","027ee6e5":"code","ce735225":"code","2e7f6e0f":"code","29055c8f":"code","afe59a11":"code","7e62d589":"code","f4bfbb0f":"code","0b50b631":"code","705fc20c":"code","2990d802":"code","8e8350de":"code","f01cf672":"code","1aa8b9b0":"code","31c541ee":"code","5544c4f3":"code","a4e9f6bf":"code","26cdc24f":"code","bb2ca56f":"code","b511cb3e":"code","50a212a9":"code","68177eb8":"code","5c15ac7f":"code","2baf6f41":"code","0ffcea54":"code","a9973b4c":"code","2f3b7942":"markdown","7093148d":"markdown","0a21d725":"markdown","306158d6":"markdown","f84302f0":"markdown","0a896c3e":"markdown","8668d203":"markdown","13e28e85":"markdown","11721599":"markdown","3e5f60d4":"markdown","2ecc8983":"markdown","48fd2931":"markdown","429e42f0":"markdown","4c398433":"markdown","e267d204":"markdown","982cbb11":"markdown","02e6755f":"markdown","851e6eb4":"markdown","67568ea6":"markdown","5fa86718":"markdown"},"source":{"59b86464":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set(font_scale=1.4)\n\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport gc\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#def import transformations\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom scipy.stats import normaltest as nt\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import log_loss\nfrom sklearn.mixture import GaussianMixture\n\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import skew\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift","f4bc37a6":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback, LearningRateScheduler, History\n\nimport tensorflow_addons as tfa\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) ","e59cb078":"def add_pca(train_df, test_df, cols, n_comp=20, fit_test = True, prefix='pca_', fit_test_first=False):\n    \n    pca = PCA(n_components=n_comp, random_state=42)\n    pca_titles = [prefix+'_pca_'+str(x) for x in range(n_comp)]\n    \n    #create copies to fill nas as needed\n    temp_train = train_df.copy()\n    temp_test = test_df.copy()\n    \n    for c in cols:\n        fv = temp_train[c].mean()\n        temp_train[c] = temp_train[c].fillna(value=fv)\n        \n        fv = temp_test[c].mean()\n        temp_test[c] = temp_test[c].fillna(value=fv)\n    \n    for p in pca_titles:\n        #we update the actual original dsf\n        train_df[p] = 0.0\n        test_df[p] = 0.0\n    \n    if fit_test==True:    \n        pca_data = pd.concat([temp_train[cols], temp_test[cols]], axis=0)\n        \n        if fit_test_first==True:\n            pca.fit(pca_data[len(train_df):])\n            pca_data = pca.transform(pca_data)\n        else:        \n            #fit and transform on the cleaned data with no NANs\n            pca_data = pca.fit_transform(pca_data)\n        \n        train_df.loc[:, pca_titles] = pca_data[0:len(train_df)]\n        test_df.loc[:, pca_titles] = pca_data[len(train_df):]\n        \n    return train_df, test_df, pca_titles\n\n\n\n\ndef split_distributions(train_df, test_df, cols, target, n_comp=20, prefix='gm', add_labels=False):    \n    \n    gm = GaussianMixture(n_components=n_comp,covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, \n                     init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, \n                     random_state=42, warm_start=False, verbose=0, verbose_interval=10)\n\n    #create copies to fill nas as needed\n    temp_train = train_df.copy()\n    temp_test = test_df.copy()       \n    \n    gm_data = pd.concat([temp_train[[cols]], temp_test[[cols]]], axis=0).reset_index(drop=True)\n    \n    gm.fit(gm_data)\n    \n    gm_data['labels'] = gm.predict(gm_data)\n    \n    if add_labels:\n        gm_titles = [prefix+str(x) for x in range(n_comp)]\n        train_df[gm_titles]=0\n        test_df[gm_titles]=0\n        dummies = pd.get_dummies(gm_data['labels'], prefix=prefix)    \n        train_df.loc[:, gm_titles] = dummies[0:len(train_df)].values\n        test_df.loc[:, gm_titles] = dummies[len(train_df):].values      \n       \n        \n    else:\n        gm_titles = []\n        \n        train_df[prefix+'_label'] = gm.labels_[0:len(train_df)]\n        test_df[prefix+'_label'] = gm.labels_[len(train_df):]\n\n        means = train_df.groupby([prefix+'_label'])[target].mean()\n    \n        train_df['menc_'+prefix+'_label'] = train_df[prefix+'_label'].map(means)\n        test_df['menc_'+prefix+'_label'] = test_df[prefix+'_label'].map(means)\n    \n        gm_titles+=[prefix+'_label', 'menc_'+prefix+'_label']\n    \n    return train_df, test_df, gm_titles\n\n\ndef assign_fold(df, label_column,fold_column, NFOLDS=5):\n\n    from sklearn.model_selection import StratifiedKFold\n    skf = StratifiedKFold(n_splits=NFOLDS)\n\n    df[fold_column]=0\n    \n    f=0\n    for trn_idx, val_idx in skf.split(df, df[label_column]):\n        df.loc[val_idx, 'fold']=f\n        f+=1\n    df[fold_column].value_counts()\n    \n    return df\n\ndef st_scale(train_df, test_df, cols):\n    \n    StSc = StandardScaler()\n    \n    train_df[cols] = StSc.fit_transform(train_df[cols])\n    test_df[cols] = StSc.transform(test_df[cols])\n    \n    return train_df, test_df \n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","027ee6e5":"PATH = '\/kaggle\/input\/tabular-playground-series-jan-2021\/'\ntrain = pd.read_csv(PATH+'train.csv')\ntest = pd.read_csv(PATH+'test.csv')\nsubmission = pd.read_csv(PATH+'sample_submission.csv')\n\nFT_COLS = [x for x in train.columns if 'cont' in x]\nTARGET='target'\n\nprint(train.shape)\ntrain.head(10)","ce735225":"print('describe Train data')\ntrain[FT_COLS+[TARGET]].describe().T","2e7f6e0f":"fig,axes=plt.subplots(figsize=(16,5))\nsns.distplot(train[TARGET], color='Red')\nprint('We can see there are some outliers in the target at low values')\naxes.set_title('Target Distribution')","29055c8f":"print('plot Feature Distributions, KSTest values (Train vs Test)')\n\nnc=3\nnr=int(len(FT_COLS)\/nc+1)\n\nfig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(20,5*nr))\n\nfor count,ft in enumerate(FT_COLS):\n    ks_score = ks_2samp(train[ft], test[ft])[0]\n    \n    sns.kdeplot(train[ft],ax=axes[count\/\/nc, count%nc],color='Green')\n    sns.kdeplot(test[ft],ax=axes[count\/\/nc, count%nc],color='Red')\n    \n    axes[count\/\/nc, count%nc].legend(['Train', 'Test'], facecolor='White')\n    axes[count\/\/nc, count%nc].set_title(ft +' ks stat :' +str(np.round(ks_score, 3)))\n    \nplt.tight_layout()","afe59a11":"corrs = train[FT_COLS+[TARGET]].corr()\nsns.set(font_scale=1.2)\nfig,axes=plt.subplots(figsize=(12,12))\nsns.heatmap(corrs, cmap='seismic_r', annot=True, fmt='0.1f', vmax=1, vmin=-1)\naxes.set_title('Correlation Matrix')","7e62d589":"PCA_COMP=2\ntrain, test, pca_titles = add_pca(train, test, FT_COLS, n_comp=PCA_COMP, fit_test = True,prefix='f' )\n\nfig,axes=plt.subplots(nrows=1, ncols=3, figsize=(20,7))\naxes[0].scatter(x=train[pca_titles[0]], y=train[pca_titles[1]], c=train[TARGET], s=1, alpha=0.8, cmap='seismic', vmax=9,vmin=6)\naxes[1].scatter(x=train[pca_titles[0]], y=train[TARGET], s=1, alpha=0.8, color='Green')\naxes[2].scatter(x=train[pca_titles[1]], y=train[TARGET], s=1, alpha=0.8, color='Green')\n\naxes[0].set_title('PCA1 vs PCA2 (color=Target)')\naxes[1].set_title('PCA1 vs Target')\naxes[2].set_title('PCA2 vs Target')\n\nplt.tight_layout()","f4bfbb0f":"print('Adding some further PCA')\n\nPCA_COMP=6\ntrain, test, pca_titles2 = add_pca(train, test, FT_COLS, n_comp=PCA_COMP, fit_test = True,prefix='extra' )\n\nnc=3\nnr=2\n\nfig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(20,5*nr))\n\nfor count,ft in enumerate(pca_titles2):\n    ks_score = ks_2samp(train[ft], test[ft])[0]\n    \n    sns.kdeplot(train[ft],ax=axes[count\/\/nc, count%nc],color='Green')\n    sns.kdeplot(test[ft],ax=axes[count\/\/nc, count%nc],color='Red')\n    \n    axes[count\/\/nc, count%nc].legend(['Train', 'Test'], facecolor='White')\n    axes[count\/\/nc, count%nc].set_title(ft +' ks stat :' +str(np.round(ks_score, 3)))\n    \nplt.tight_layout()","0b50b631":"#Below are the original number of gaussian mixtures I aimed to split the main features into, based on looking at the distributions\n#(see earlier graphs)\n\noriginal_feature_dict = {'cont1': 5,\n 'cont2': 10,\n 'cont3': 8,\n 'cont4': 8,\n 'cont5': 5,\n 'cont6': 6,\n 'cont7': 4,\n 'cont8': 3,\n 'cont9': 7,\n 'cont10':3,\n 'cont11': 4,\n 'cont12': 2,\n 'cont13': 3,\n 'cont14': 5,}\n\npca_dict = {\n       'extra_pca_0': 4,\n 'extra_pca_1': 1,\n 'extra_pca_2': 1,\n 'extra_pca_3': 2,\n 'extra_pca_4': 1,\n 'extra_pca_5': 1   }\n\n#i found original numbers were too low - easiest approach was to add a multiplier while testing\n\nMULTIPLIER = 3.0\n\noriginal_feature_dict = {a:int(b*MULTIPLIER) for (a,b) in original_feature_dict.items()}\nft_dict = {**original_feature_dict, **pca_dict}\nft_dict","705fc20c":"mixture_title_cols=[]\n\n#split the original feature values into new labels, depending on which sub mixture they are in\nfor f in FT_COLS+pca_titles2:          \n    train, test, titles = split_distributions(train, test,f,TARGET, n_comp=ft_dict[f], prefix=f+'_dim', add_labels=True)    \n    mixture_title_cols+=titles    \n\n#split the original feature values into new columns using the labels created in previous lines\nmixture_value_cols = []\n\nfor count,f in enumerate(FT_COLS+pca_titles2):\n    \n    for d in range(ft_dict[f]): #this goes through each subdistribution for each feature\n        \n        #create a column with the original values, recentred\n        t_median = train[f][train[f+'_dim'+str(d)]==1].median()\n        t_std = train[f][train[f+'_dim'+str(d)]==1].std()\n        \n        #set to NA\n        train[f+'_dim'+str(d)+'_dist'] = np.where(train[f+'_dim'+str(d)]==1,\n                                                 (train[f] - t_median)\/t_std, np.nan)\n        \n        test[f+'_dim'+str(d)+'_dist'] = np.where(test[f+'_dim'+str(d)]==1,\n                                                 (test[f] - t_median)\/t_std, np.nan)\n        \n        mixture_value_cols+=[f+'_dim'+str(d)+'_dist']","2990d802":"nt_data = pd.Series(index=mixture_value_cols,\n                   data=0.0)\n\nfor nd in mixture_value_cols:\n    nt_data[nd] = nt(train[nd][~train[nd].isna()].values.flatten())[0]\n\nnt_data=nt_data.sort_values()\n\n#plot the most 'normal' and 3 of the least\nfig,axes=plt.subplots(figsize=(10,5))\nsns.kdeplot(train[nt_data.index[0]], color='Green')\nsns.kdeplot(train[nt_data.index[-1]], color='Red')\nsns.kdeplot(train[nt_data.index[-2]], color='Red')\nsns.kdeplot(train[nt_data.index[-3]], color='Red')\naxes.legend([nt_data.index[0], nt_data.index[-1], nt_data.index[-2], nt_data.index[-3]], facecolor='white')\naxes.set_xlabel('Value')","8e8350de":"print('Total Original Features', len(FT_COLS))\nprint('Total Submixtures Labels', len(mixture_title_cols))\nprint('Total Submixtures Values', len(mixture_value_cols))\nprint('Total Feature Columns', len(FT_COLS)+len(mixture_title_cols)+len(mixture_value_cols))","f01cf672":"view_feature = 'cont8'\n\nsubdists = [x for x in train.columns if view_feature+'_dim' in x and 'dist' in x]\n\nnc=3\nnr=len(subdists)\/\/nc+1\n\nfig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(20,5*nr), sharex=False)\n\nsns.kdeplot(train[view_feature],ax=axes[0,0],color='Blue')\naxes[0,0].set_title('Original Feature - Train')\n\nfor count1,ft in enumerate(subdists):\n    count=count1+1\n    ks_score = ks_2samp(train[ft], test[ft])[0]\n    \n    sns.kdeplot(train[ft],ax=axes[count\/\/nc, count%nc],color='Green')\n    sns.kdeplot(test[ft],ax=axes[count\/\/nc, count%nc],color='Red')\n    \n    axes[count\/\/nc, count%nc].legend(['Train', 'Test'], facecolor='White')\n    axes[count\/\/nc, count%nc].set_title(ft +' ks stat :' +str(np.round(ks_score, 3)))\n    axes[count\/\/nc, count%nc].set_xlabel(None)\n    axes[count\/\/nc, count%nc].set_ylabel(None)\n    \nplt.tight_layout()","1aa8b9b0":"#fill nas in mixture values\nNAN_VALUE = 0.0\n\nfor d in mixture_value_cols:\n    train[d] = train[d].fillna(value=NAN_VALUE)\n    test[d] = test[d].fillna(value=NAN_VALUE)","31c541ee":"#scale original feature columns\n\n#if true, scale original feature columns and clip\nSCALE = True\n\nif SCALE:\n    train, test = st_scale(train, test, FT_COLS)\n\n    #clip to -2 to +2\n    for f in FT_COLS:\n        train[f] = np.clip(train[f], -2, 2)\n        test[f] = np.clip(test[f], -2, 2)\n        \n\n        \n#if true, scale new features to same range\nSCALE_DISTS = True\n\nif SCALE_DISTS:\n    for d in mixture_value_cols:\n        TEMP_MAX = np.abs(train[d]).max()\n        train[d] = train[d] \/ TEMP_MAX\n        test[d] = test[d] \/ TEMP_MAX","5544c4f3":"#highlight outliers and create filter column\n#i dont think ive seen anything to suggest that the outliers help training\n\ntrain['outlier_filter'] = np.where(train[TARGET]<4, True, False)\nprint('# outliers', sum(train['outlier_filter']))","a4e9f6bf":"#centre the target around zero - i think this helped to converge faster\nTARGET_MEAN = train[TARGET].mean()\n\ntrain[TARGET] = train[TARGET] - TARGET_MEAN\nsns.distplot(train[TARGET])","26cdc24f":"print('Distribution of Original Features by Fold (for CV)')\n\nGROUP_LABEL = 'target_group'\nNQUANTS=10000\ntrain['target_group'] = pd.qcut(train[TARGET], NQUANTS, labels=False)\n\nNFOLDS=10\nFOLD_COL='fold'\n\ntrain = assign_fold(train, GROUP_LABEL, FOLD_COL, NFOLDS=NFOLDS)\n\nnc=3\nnr=int(len(FT_COLS)\/nc+1)\n\nfig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(20,5*nr))\n\nfor count,ft in enumerate(FT_COLS+[TARGET]):\n    \n    for count2, qbin in enumerate(range(NFOLDS)):\n        filt = train[FOLD_COL]==qbin\n        sns.kdeplot(train[ft][filt],ax=axes[count\/\/nc, count%nc])\n    \n    \n    axes[count\/\/nc, count%nc].legend(['fold_'+str(qbin) for qbin in range(NFOLDS)], facecolor='White', fontsize=10)\n    axes[count\/\/nc, count%nc].set_title(ft)\nplt.tight_layout()","bb2ca56f":"print('CV Benchmark - Random Guess of Median')\ncv_bm = np.sqrt(mse(train[TARGET], np.full((train[TARGET].shape), train[TARGET].median())))\ncv_bm","b511cb3e":"def run_training(model, train_df, test_df,sample_submission, fold_col, #input data and folds\n             orig_features, mixture_val_cols,mixture_label_cols, target_col, #\n             benchmark,\n            outlier_col=None, nn=False, epochs=10,batch_size=32, verbose=False,\n            dense=70, dout=0.15, dense_reg = 0.000001,act='elu'):\n\n    FOLD_VALUES = sorted([x for x in train_df[fold_col].unique()]) \n    \n    #set up templates to fill up with predictions\n    oof = np.zeros((len(train_df),))\n    test_predictions = np.zeros((len(sample_submission),))\n    \n    #set up axes to plot training\n    fig,axes=plt.subplots(figsize=(15,6))    \n    axes.set_ylim(0.69,0.75)\n    \n    #random seeds to run - selected at random\n    RANDOM_SEEDS = [0,42,100,1000]\n    #RANDOM_SEEDS = [0,]\n\n    for rs in RANDOM_SEEDS:\n        seed_everything(seed=rs)\n        for fold in FOLD_VALUES:\n            \n            print(' ---  ')\n            print('running random seed', rs, 'fold', fold)\n            if outlier_col:\n                trn_idx = (train_df[fold_col]!=fold) & (~train_df[outlier_col]) #filter outlying target rows       \n            else:\n                trn_idx = train_df[fold_col]!=fold\n            \n            val_idx = train_df[fold_col]==fold            \n            \n            #original features, new features, feature labels (yn), target\n            X_train_orig = train_df.loc[trn_idx,orig_features].values\n            X_train = train_df.loc[trn_idx,mixture_val_cols].values\n            X_train_mask = train_df.loc[trn_idx,mixture_label_cols].values\n            y_train = train_df.loc[trn_idx, target_col].values #- MED\n            \n            #same for validation data\n            X_val_orig = train_df.loc[val_idx,orig_features].values\n            X_val = train_df.loc[val_idx,mixture_val_cols].values\n            X_val_mask = train_df.loc[val_idx,mixture_label_cols].values\n            y_val = train_df.loc[val_idx, target_col].values #- MED\n            \n            #load keras model\n            model = keras_model(orig_features, mixture_val_cols,mixture_label_cols,dout=dout,\n               dense=dense,act=act, dense_reg = dense_reg, descend_fraction = 0.9)\n            \n            K.clear_session()\n            \n            #set up the list of epochs to use for prediction, after training\n            #i have not really had time to test this properly but it seemed to improve CV\n            back_count = 5\n            epoch_list=list(range(epochs-back_count, epochs))\n            \n            #save last few epochs\n            class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n                def on_epoch_end(self, epoch, logs=None):                    \n                    if epoch in epoch_list:\n                        #print('save this epoch')\n                        self.model.save_weights('model_fold_'+str(fold)+'_epoch_'+str(epoch)+'.h5', overwrite=True)\n                    else:\n                        pass\n            cbk = CustomModelCheckpoint()\n            \n            \n            #fit keras model           \n            history = model.fit([X_train_orig, X_train,X_train_mask ], y_train, epochs=epochs, batch_size=batch_size,shuffle=True,\n                         validation_data=([X_val_orig, X_val,X_val_mask], y_val), verbose=verbose,\n                                   callbacks=[cbk])\n            \n            #print outcomes\n            print('Fold Last Epoch Train Error', history.history['root_mean_squared_error'][-1])\n            print('Fold Last Epoch Valid Error', history.history['val_root_mean_squared_error'][-1])\n            \n            #save final weights\n            model.save_weights('model_final_'+str(fold)+'.h5') \n            \n            #plot history\n            sns.lineplot(x=range(epochs), y=history.history['loss'], color='Blue')\n            sns.lineplot(x=range(epochs), y=history.history['val_loss'], color='Red')\n            \n            \n            #predicting validation data using weights from last few epochs\n            val_preds = np.zeros((len(X_val),))\n            for e in epoch_list:\n                model = keras_model(orig_features, mixture_val_cols,mixture_label_cols,dout=dout,\n                           dense=dense,act=act, dense_reg = dense_reg, descend_fraction = 0.9)\n                \n                model.load_weights('model_fold_'+str(fold)+'_epoch_'+str(e)+'.h5')\n                \n                val_preds += model.predict([X_val_orig, X_val,X_val_mask]).mean(axis=1)                \n                \n                #also predict the test data\n                test_predictions += model.predict([test_df[orig_features].values,test_df[mixture_val_cols].values,\n                                                   test_df[mixture_label_cols].values]).mean(axis=1)                \n                       \n            #average validation preds for fold error\n            val_preds = val_preds \/ len(epoch_list)             \n            oof[val_idx]+=val_preds\n            val_error = np.sqrt(mse(y_val, val_preds))\n            print('Fold multi epoch weight prediction error', val_error)\n\n    \n    #divide oof (validation) by number of random seeds  \n    oof = oof \/ len(RANDOM_SEEDS)\n    \n    #total validation error\n    total_val_error = np.sqrt(mse(train_df[target_col], oof))\n    print('final OOF MSE', total_val_error)\n        \n    #list of fold errors\n    fold_errors = []\n    for fold in train_df[fold_col].unique():\n        val_idx = train_df[fold_col]==fold\n        fold_errors+=[np.sqrt(mse(train_df.loc[val_idx, target_col].values, oof[val_idx]))]\n\n    #divide test predictions by # folds and random seed list and number of epochs' weights used\n    test_predictions = test_predictions\/(NFOLDS * len(RANDOM_SEEDS) * len(epoch_list)) \n  \n    return oof, test_predictions, fold_errors    ","50a212a9":"def keras_model(ft_orig, mixture_values, mixture_labels, n_layer=3,bnorm=True,dout=0.2,\n               dense=20,act='elu', dense_reg = 0.000001, descend_fraction = 0.9):   \n    \n    \n    ## original features\n    input1 = L.Input(shape=(len(ft_orig)), name='input_orig')  \n    input1_do = L.Dropout(0.1)(input1)\n    \n    #dense layer linked to original features\n    XA = L.Dense(dense, activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg), name='dense_orig')(input1_do)\n    if bnorm:\n        XA = L.BatchNormalization(name='bn_1')(XA)\n    XA1 = L.Dropout(dout, name='do_orig')(XA)\n    \n    \n    \n    ## new features - sub mixtures\n    input2 = L.Input(shape=(len(mixture_values)), name='input_new')  \n    input3 = L.Input(shape=(len(mixture_labels)), name='input_new2')  \n    #note - i split out Labels vs Values originally to allow more testing. \n    #In practice I think no need for separte inputs as they are concatenated now anyway\n    all_input_combo = tf.keras.layers.Concatenate(axis=1, name='concat_i2')([input2, input3])\n    input2_do = L.Dropout(0.2)(all_input_combo)\n    \n    #dense layer linked to new features\n    XB = L.Dense(dense, activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg), name='dense_new')(input2_do)\n    if bnorm:\n        XB = L.BatchNormalization(name='bn_2')(XB)\n    XB1 = L.Dropout(dout, name='do_new')(XB)\n    \n    \n    \n    \n    \n    ##combine original with normal dists    \n    all_input_combo = tf.keras.layers.Concatenate(axis=1, name='concat_new')([XA1, XB1])\n        \n        \n    ##rest of model\n    \n    #layer 1\n    X = L.Dense(int(dense), activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg))(all_input_combo)\n    if bnorm:\n        X = L.BatchNormalization(name='bn_3')(X)\n    X2 = L.Dropout(dout, name='do_3')(X)\n    \n    X2 = tf.keras.layers.concatenate([all_input_combo, X2],  axis=1)\n    \n    #layer 2\n    X = L.Dense(int((descend_fraction**2)*dense), activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg))(X2)\n    if bnorm:\n        X = L.BatchNormalization()(X)\n    X3 = L.Dropout(dout)(X)\n    \n    X3 = tf.keras.layers.concatenate([X2, X3], axis=1)\n    \n    #layer 3\n    X = L.Dense(int((descend_fraction**3)*dense), activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg *2))(X3)\n    if bnorm:\n        X = L.BatchNormalization()(X)\n    X4 = L.Dropout(dout)(X)    \n    \n    X4 = tf.keras.layers.concatenate([X3, X4],axis=1)\n    \n    #layer 4\n    X = L.Dense(int((descend_fraction**4)*dense), activation=act, activity_regularizer=tf.keras.regularizers.L2(dense_reg*4))(X4)\n    if bnorm:\n        X = L.BatchNormalization()(X)\n    X5 = L.Dropout(dout)(X)        \n       \n    X5 = tf.keras.layers.concatenate([X4, X5], axis=1)\n    \n    #final layers \/ output\n    X = L.Dense(5, activation=act, activity_regularizer=tf.keras.regularizers.L2(0.0001))(X5)\n    output1 = L.Dense(1, activation='linear')(X)\n    \n    #Note, the learning rate & weight decay was optimised at an early stage. I had no time to re run optuna.\n    model = tf.keras.Model(inputs=[input1, input2, input3], outputs=output1)\n    model.compile(loss=root_mean_squared_error,\n                  optimizer=tfa.optimizers.AdamW(learning_rate=0.000809028893821181, weight_decay=9.83479875802558E-06),\n                  metrics=tf.keras.metrics.RootMeanSquaredError())\n    \n    return model\n    \n\nK.clear_session()    \nmodel = keras_model(FT_COLS, mixture_title_cols, mixture_value_cols,\n                    dense=70, \n                    dout=0.15, \n                    dense_reg = 0.000001,\n                    act='elu',\n                    descend_fraction = 0.9)\nmodel.summary()","68177eb8":"oof, test_predictions, fold_errors = run_training(model, train, test,submission, 'fold', FT_COLS,\n                                              mixture_value_cols, mixture_title_cols, TARGET, cv_bm,\n                                            outlier_col='outlier_filter', \n                                            epochs=25,\n                                            batch_size=256, #lower batch sizes looked steadier \n                                            #but i did not get much better end result from limited testing\n                                            \n                                            verbose=False,\n                                            dense=70, \n                                            dout=0.15, \n                                            dense_reg = 0.000001,\n                                            act='elu',)","5c15ac7f":"print('Save Out of Fold Predictions')\noof = pd.DataFrame(columns=['oof_prediction'], index=train['id'], data=oof + TARGET_MEAN)\noof.to_csv('oof_predictions.csv', index=True)\noof.head(10)","2baf6f41":"print('fold errors', fold_errors)\nprint('fold error std', np.array(fold_errors).std())","0ffcea54":"sns.set(font_scale=1.4)\nfig,axes=plt.subplots(figsize=(16,5))\nsns.distplot(train[TARGET] + TARGET_MEAN, color='Green')\nsns.distplot(test_predictions + TARGET_MEAN, color='Red')\naxes.set_title('Train Target Distribution & Test Predictions')\naxes.set_xlim(4,10)\naxes.legend(['Train', 'Test Predicted'])","a9973b4c":"submission['target'] = test_predictions + TARGET_MEAN\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(5)","2f3b7942":"Below are the original number of gaussian mixtures I aimed to split the main features into, based on looking at the distributions (see earlier graphs).\n\nIn practice I found that multiplying this out by a much larger number than I'd really anticipated lead to results improving.","7093148d":"Gaussian Mixture splits each original feature into a number of sub distributions\n\nEach subdistribution has 2 columns, one for a label (Y\/N) and one for the original values, recentred.","0a21d725":"I am not sure if all the steps here are the best possible approach. I believe they improved my CV but did not have time to test more extensively.","306158d6":"# Apply Folds to Data","f84302f0":"# Save out of fold, test predictions","0a896c3e":"PCA does not really help us to see any obvious separation","8668d203":"# Keras Model","13e28e85":"We can see that train and test data follow very similar distributions","11721599":"Folds just selected based on distribution of target.\n\nIt seems like the original feature values are pretty close between the selected folds.\n\nI did try Multilabel Strat with the new sub-mixture labels, but this did not seem to immediately improve results and had little time to explore further.","3e5f60d4":"# Examine split of one feature","2ecc8983":"Note: at the end of every fold, the target is re-predicted with the weights from last few epochs (see CV loop)","48fd2931":"Note: as the target was centred at zero, need to make sure to re-add original mean","429e42f0":"# Run Training & Predictions","4c398433":"# Adding Features","e267d204":"Can look at 'normality' of new distributions with sklearn normality test","982cbb11":"There are some correlations between some of the features","02e6755f":"# Cross Validation Setup","851e6eb4":"It seemed like breaking out existing features into sub-distributions within additional feature columns improved the NN outcome\n\nI added some further PCA, then split out the feature columns using SKLearn Gaussian Mixture.\n\nThe large number of additional features feels clumsy, but the outcome of testing seemed better with more features","67568ea6":"# Tidying of Features & Target","5fa86718":"# Import Train and Test"}}