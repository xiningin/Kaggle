{"cell_type":{"b6ae0276":"code","6c4d3477":"code","52027f32":"code","7effd22c":"code","92e0cff7":"code","0712e493":"code","72a18408":"code","0e34153d":"code","f66d3599":"code","1a81b1b7":"code","881860f5":"code","88c7ec02":"code","d7d8c517":"code","da3ffc3e":"code","5100ae6c":"code","5ed8f168":"code","db9d16f7":"code","f3f6a1ad":"code","f1f36c13":"code","f2b11a7c":"code","3db21db8":"code","65088940":"code","3772f7ab":"code","27543c3f":"code","248275cc":"code","e86aef44":"code","2866e70a":"code","0301a1f3":"markdown","94ab25b3":"markdown","17fa1a60":"markdown","8aa3d59c":"markdown","604e0260":"markdown","55e04dc4":"markdown","1b59150a":"markdown","9f5bf953":"markdown"},"source":{"b6ae0276":"import numpy as np \nimport pandas as pd \nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport math as math\nimport seaborn as sns\n\nfrom datetime import datetime\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","6c4d3477":"# Load the data\ntrain =pd.read_csv(\"\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\")","52027f32":"train.head()","7effd22c":"# Check the data\nprint(\"Number of data: \", train.shape[0], \"\\n\")\n\nMissing = train[train.isnull().any(axis=1)]\nprint(\"Number of records contain 1+ null: \", Missing.shape[0], \"\\n\")","92e0cff7":"# get the number of missing data points per column\nmissing_values_count =train.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","0712e493":"Missing.iloc[np.r_[0:10, len(Missing)-10:len(Missing)]]","72a18408":"train=train.interpolate()\ntrain.isnull().sum()","0e34153d":"def plot_time_series(df, row_num, start_col =1, ax=None):\n    if ax is None:\n            fig = plt.figure(facecolor='w', figsize=(10, 6))\n            ax = fig.add_subplot(111)\n    else:\n        fig = ax.get_figure()\n        \n    series_title = df.iloc[row_num, 0]\n    sample_series = df.iloc[row_num, start_col:]\n    sample_series.plot(style=\".\", ax=ax)\n    ax.set_title(\"Series: %s\" % series_title)\n\nfig, axs  = plt.subplots(4,1,figsize=(12,12))\nplot_time_series(train, 1, ax=axs[0])\nplot_time_series(train, 10, ax=axs[1])\nplot_time_series(train, 100, ax=axs[2])\nplot_time_series(train, 1005, ax=axs[3])\n\nplt.tight_layout()","f66d3599":"train_flattened = pd.melt(train[list(train.columns[-50:])+['Page']], id_vars='Page', var_name='date', value_name='Visits')\ntrain_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\ntrain_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) \/\/ 5 == 1).astype(float)\n\n# Median by page\ndf_median = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].median())\ndf_median.columns = ['median']\n\n# Average by page\ndf_mean = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].mean())\ndf_mean.columns = ['mean']\n\n# Max by page\ndf_mean = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].max())\ndf_mean.columns = ['max']\n\n# Merging data\ntrain_flattened = train_flattened.set_index('Page').join(df_mean).join(df_median)","1a81b1b7":"train_flattened.reset_index(drop=False,inplace=True)\ntrain_flattened['weekday'] = train_flattened['date'].apply(lambda x: x.weekday())","881860f5":"# Feature engineering with the date\ntrain_flattened['year']=train_flattened.date.dt.year \ntrain_flattened['month']=train_flattened.date.dt.month \ntrain_flattened['day']=train_flattened.date.dt.day\n\ntrain_flattened.head()","88c7ec02":"train_flattened.index","d7d8c517":"page_name=train_flattened.columns","da3ffc3e":"# Extracting language of the page from it's name and adding it to a set so that we only have \n# unique entry and can easily find out the total number of languages in dataset\nlang=set()\nfor k in page_name:\n  index=k.find('.wikipedia')\n  lang.add(k[index-1:index-3:-1][::-1])\nprint(lang)","5100ae6c":"train_flattened.dtypes","5ed8f168":"train_flattened['date'] = pd.to_datetime(train_flattened['date'])\ntrain_flattened = train_flattened.set_index('date') ","db9d16f7":"sns.pairplot(train_flattened.dropna(),\n            \n             x_vars=['weekday','year',\n                     'month','day'],\n             y_vars='Visits',\n             height=5,\n             plot_kws={'alpha':0.15, 'linewidth':0}\n            )\nplt.suptitle('Visit by weekday, year of Month , day')\nplt.show()","f3f6a1ad":"y=train_flattened['Visits'].resample('W').mean()","f1f36c13":"y.sort_index(inplace=True)\ny","f2b11a7c":"from statsmodels.tsa.stattools import adfuller\n\n\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n\n    rolmean = pd.Series(timeseries).rolling(window=12).mean()\n    rolstd = pd.Series(timeseries).rolling(window=12).std()\n    \n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","3db21db8":"test_stationarity(y)","65088940":"import itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","3772f7ab":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","27543c3f":"import statsmodels.api as sm\nmod = sm.tsa.statespace.SARIMAX(y,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","248275cc":"pred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)\n\ny_forecasted = pred.predicted_mean\ny_truth = y['2015-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","e86aef44":"print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","2866e70a":"#visualizing forecasts\npred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Visit')\nplt.legend()\nplt.show()","0301a1f3":"## Stationarity For Sample","94ab25b3":"##  Data visualization","17fa1a60":"# Import libraries and data files","8aa3d59c":"### Fitting the ARIMA model","604e0260":" \n## Article names ","55e04dc4":"#  Time Series Forecasting \n","1b59150a":"##  Missing values","9f5bf953":"## ARIMA Model"}}