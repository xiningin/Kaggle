{"cell_type":{"193b67e4":"code","90d82f2c":"code","7f99f703":"code","60f79399":"code","5e921d54":"code","1c430e2d":"code","c450b619":"code","9d55f556":"code","7589c3d1":"code","4faf8f72":"code","6014a7ca":"code","80702e53":"code","378c0bb0":"code","86635279":"code","b5f2e19f":"code","e84e07da":"code","aa212f49":"code","c762180b":"code","f7de8534":"code","d4e60766":"markdown","785c6f29":"markdown","395ff694":"markdown","7b33eb5a":"markdown","9429331d":"markdown","a973435c":"markdown","7d0f98e0":"markdown","b21eb7dc":"markdown","80a561bf":"markdown","24b00b68":"markdown","265986cf":"markdown","e7bb77c4":"markdown","b659d91d":"markdown"},"source":{"193b67e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","90d82f2c":"original_df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\noriginal_df.head()","7f99f703":"print(\"Count of classes available\")\nprint(pd.Index(original_df['Class']).value_counts())\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Class\", data=original_df)","60f79399":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = original_df['Amount'].values\ntime_val = original_df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='b')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='g')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\nplt.show()","5e921d54":"from sklearn.preprocessing import RobustScaler\nrob_scaler = RobustScaler()\noriginal_df['scaled_amount'] = rob_scaler.fit_transform(original_df['Amount'].values.reshape(-1,1))\noriginal_df['scaled_time'] = rob_scaler.fit_transform(original_df['Time'].values.reshape(-1,1))\noriginal_df.drop(['Time','Amount'], axis=1, inplace=True)","1c430e2d":"# Shuffling our data\noriginal_df.sample(frac=1).head()","c450b619":"from sklearn.manifold import TSNE\ndef plot_graph_tsne(X,y):\n    pca_2d = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n    colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y]\n    kwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\n    fig = plt.Figure(figsize=(12,6))\n    plt.scatter(pca_2d[:, 0],pca_2d[:, 1], c=colors, **kwarg_params, label=\"Fraud\")\n    plt.legend()\n    sns.despine()","9d55f556":"from collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nX = original_df.drop('Class', axis=1)\ny = original_df['Class']\nsampler = RandomUnderSampler(sampling_strategy='auto')\nX_rs, y_rs = sampler.fit_sample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_rs))\nplot_graph_tsne(X_rs, y_rs)\n","7589c3d1":"df_under = pd.concat([X_rs, y_rs], axis=1)\ndf_under.head()","4faf8f72":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(50,30))      \nsns.heatmap(df_under.corr(), annot=True, cmap = 'coolwarm', ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix of UnderSampled Data\", fontsize=14)\n\n       \nsns.heatmap(original_df.corr(), cmap = 'coolwarm', ax=ax2)\nax2.set_title(\"Imbalanced Correlation Matrix of Original Data\", fontsize=14)","6014a7ca":"f, axes = plt.subplots(ncols=7, figsize=(60,8))\nsns.boxplot(x=\"Class\", y=\"V3\", data=df_under,  ax=axes[0])\naxes[0].set_title('V3 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V9\", data=df_under,  ax=axes[1])\naxes[1].set_title('V9 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=df_under,  ax=axes[2])\naxes[2].set_title('V10 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=df_under,  ax=axes[3])\naxes[3].set_title('V12 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=df_under,  ax=axes[4])\naxes[4].set_title('V14 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V16\", data=df_under,  ax=axes[5])\naxes[5].set_title('V16 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V17\", data=df_under,  ax=axes[6])\naxes[6].set_title('V17 vs Class Negative Correlation')\n\nplt.show()","80702e53":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x=\"Class\", y=\"V2\", data=df_under,  ax=axes[0])\naxes[0].set_title('V2 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=df_under,  ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=df_under,  ax=axes[2])\naxes[2].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=df_under,  ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","378c0bb0":"from scipy.stats import norm\nfig, ax = plt.subplots(1, 4, figsize=(50,8))\n\nV2 = df_under['V2'].values\nV4 = df_under['V4'].values\nV11 = df_under['V11'].values\nV19 = df_under['V19'].values\n\nsns.distplot(V2, ax=ax[0], fit=norm, color='b')\nax[0].set_title('Distribution of V2', fontsize=8)\nax[0].set_xlim([min(V2), max(V2)])\n\nsns.distplot(V4, ax=ax[1], fit=norm, color='g')\nax[1].set_title('Distribution of V4', fontsize=8)\nax[1].set_xlim([min(V4), max(V4)])\n\nsns.distplot(V11, ax=ax[2], fit=norm, color='g')\nax[2].set_title('Distribution of V11', fontsize=8)\nax[2].set_xlim([min(V11), max(V11)])\n\nsns.distplot(V19, ax=ax[3], fit=norm, color='g')\nax[3].set_title('Distribution of V19', fontsize=8)\nax[3].set_xlim([min(V19), max(V19)])\n\nplt.show()","86635279":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(df_under[[\"V3\",\"V9\",\"V10\",\"V12\",\"V14\",\"V2\",\"V4\",\"V11\",\"V19\"]]))\ndf_under_filtered = df_under[(z < 3).all(axis=1)]\nprint(\"Before outlier removal\",df_under.shape)\nprint(\"Remaining after outlier removal\",df_under_filtered.shape)\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Class\", data=df_under_filtered)","b5f2e19f":"Q1 = df_under[[\"V3\",\"V9\",\"V10\",\"V12\",\"V14\",\"V2\",\"V4\",\"V11\",\"V19\"]].quantile(0.25)\nQ3 = df_under[[\"V3\",\"V9\",\"V10\",\"V12\",\"V14\",\"V2\",\"V4\",\"V11\",\"V19\"]].quantile(0.75)\nIQR = Q3 - Q1\ndf_under_out = df_under[~((df_under < (Q1 - 1.5 * IQR)) |(df_under > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(\"Remaining after outlier removal\",df_under_out.shape)\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Class\", data=df_under_out)","e84e07da":"f, axes = plt.subplots(ncols=9, figsize=(60,8))\nsns.boxplot(x=\"Class\", y=\"V3\", data=df_under_filtered,  ax=axes[0])\naxes[0].set_title('V3 Reduced Outlier')\naxes[0].annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\nsns.boxplot(x=\"Class\", y=\"V9\", data=df_under_filtered,  ax=axes[1])\naxes[1].set_title('V9 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=df_under_filtered,  ax=axes[2])\naxes[2].set_title('V10 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=df_under_filtered,  ax=axes[3])\naxes[3].set_title('V12 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=df_under_filtered,  ax=axes[4])\naxes[4].set_title('V14 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=df_under_filtered,  ax=axes[5])\naxes[5].set_title('V2 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=df_under_filtered,  ax=axes[6])\naxes[6].set_title('V4 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=df_under_filtered,  ax=axes[7])\naxes[7].set_title('V11 Reduced Outlier')\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=df_under_filtered,  ax=axes[8])\naxes[8].set_title('V19 Reduced Outlier')\n\n\nplt.show()","aa212f49":"X = df_under_filtered.drop('Class', axis=1)\ny = df_under_filtered['Class']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)","c762180b":"X_test.shape","f7de8534":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nimport xgboost\nfrom sklearn import svm, tree\nfrom sklearn import metrics\n\nclassifiers = []\nnb_model = GaussianNB()\nclassifiers.append((\"Gausian Naive Bayes Classifier\",nb_model))\nlr_model= LogisticRegression()\nclassifiers.append((\"Logistic Regression Classifier\",lr_model))\n# sv_model = svm.SVC()\n# classifiers.append(sv_model)\ndt_model = tree.DecisionTreeClassifier()\nclassifiers.append((\"Decision Tree Classifier\",dt_model))\nrf_model = RandomForestClassifier()\nclassifiers.append((\"Random Forest Classifier\",rf_model))\nxgb_model = xgboost.XGBClassifier()\nclassifiers.append((\"XG Boost Classifier\",xgb_model))\nlda_model = LinearDiscriminantAnalysis()\nclassifiers.append((\"Linear Discriminant Analysis\", lda_model))\ngp_model =  GaussianProcessClassifier()\nclassifiers.append((\"Gaussian Process Classifier\", gp_model))\nab_model =  AdaBoostClassifier()\nclassifiers.append((\"AdaBoost Classifier\", ab_model))\n\ncv_scores = []\nnames = []\nfor name, clf in classifiers:\n    print(name)\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n    y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n    print(\"Model Score : \",clf.score(X_test, y_pred))\n    print(\"Number of mislabeled points from %d points : %d\"% (X_test.shape[0],(y_test!= y_pred).sum()))\n    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n    cv_scores.append(scores)\n    names.append(name)\n    print(\"Cross validation scores : \",scores.mean())\n    confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n    print(\"Confusion Matrix \\n\",confusion_matrix)\n    classification_report = metrics.classification_report(y_test,y_pred)\n    print(\"Classification Report \\n\",classification_report)","d4e60766":"Now we will count the total values present in each class and plotted a count plot.\n\n**Analysis**\n\n1. It can be observed that, Non Fraud transactions in too much quantity compared to fraud transaction.\n2. If we try to apply machine learning to train a classification algorithm, then it will be biased towards non fraud transactions.\n3. Hence we will explore different oversampling and undersampling techniques.","785c6f29":"Once plotted the boxplot for the above negatively and positively feature, we observed great deal of available outliers (it is also called as one of the anomaly detected techniques). ","395ff694":"In order to view our dataset we need to reduce the dimensionality to 2. We have 3 options.\n\n1. Princple Component Analysis (PCA)\n2. Singular Value Decomposition (SVD)\n3. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nWe observed that, tSNE shows accurate which can depicts clearly separated Fraud and Non Fraud cases.","7b33eb5a":"We will have the distribution plot for the \"Time\" and \"Amount\" to see, how distributed are they in the gaussian curve. We can see that, the graphs are highly skewed. Hence it becomes very important to scale the data.","9429331d":"First we will perform the undersampling and further check, how our model preforms on undersampled data. There is great library which can be explored further for different [undersampling](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html#module-imblearn.under_sampling) techniques.\n\nOur aim to create equally sampled data from each class, in order to remove bias from the dataset. As we understood before, we have only 492 classes for non fraud samples, then we must take only 492 fraud samples. Hence we will go with RandomUnderSampler for imblearn library.","a973435c":"Lets plot the correlation matrix for different features. We plotted correlation matrix for undersampled data and for original data. We had following observation:\n\n1.  Negative Correlation for anomaly check - v3, v9, v10, v12, v14, v16, v17 are highly negatively correlated. \n2.  Positive Correlation for anomaly chcek - v2, v4, v11, v19 are highly positively correlated. \n\nWe will go ahead and create box for above feature to look for available outliers.","7d0f98e0":"After removing those outlier, we created boxplot for those feature. We observed that, outliers has been reduced upto great extent.","b21eb7dc":"**Outlier removal**\n\nTo remove sufficient amount outlier we have tried to method Z-score method and IQR method.\n\n1. Z-Score method : The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, \n\n2. IQR Method : IQR is somewhat similar to Z-score in terms of finding the distribution of data and then keeping some threshold to identify the outlier. Then we multiply with the threshold (1.5) and try to include only those values.\n\nWe observed that, IQR method tends to remove too many outlier and hence we will proceed with Z-score method.","80a561bf":"Reading the csv file through pandas and view the dataset.\n\n**Analysis**\n1. To protect the identity of the user and to make the anonymoused data, the team has already performed the PCA and given the component variable which need to be used for the further analysis\n2. After looking at the data, column \"Time\" and \"Amount\" has not been scaled. Hence we will go through certain scaling algorithm which can be used to scale the data as similar as other variable.\n3. Class has two values as \"0\" --> Non Fraud Transactions and \"1\" --> Fraud Transactions.\n4. There are no null values present in the dataset","24b00b68":"# Dealing with Imbalanced Dataset\n**Our goal**\n1. To create undersample and oversampled data\n2. To remove detect and remove the outliers\n3. Train on multiple algorithms for the classification \n","265986cf":"We train different model with the above data and cross validated it. We observed XGBoost Classifier did a great job. It could be because of the of random forest based boosting technique which it follow within.","e7bb77c4":"**Scaling**\n\nThere are various types of scaling techniques available under sklearn package. It will be interesting to check which scaling method could scale our variable best, which will help to center our data for our gaussian curve. We tested with various scaling techniques like StandardScaler, MinMaxScaler, and RobustScaler and have decided to go with RobustScaler. Robust Scaler is less prone to outlier. Visit below page for more details\n\n[Comparision between other scaling method.](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html)","b659d91d":"We also also plotted the distribution graph for the features to look, how deviated are they from the normal distribution"}}