{"cell_type":{"2ff2b1b2":"code","934460dd":"code","14c1a125":"code","66ae9ccc":"code","e7cceef0":"code","a72e0ab5":"code","424ec3bf":"code","1867d882":"code","8069e873":"code","6717261d":"code","a25ff3c2":"code","9148bcbc":"code","f1cdabe2":"code","5e7979e0":"code","d9b1f77a":"code","c830d712":"code","efa0479f":"code","9d40d8bd":"code","aefde134":"code","1332c00e":"code","e6c348a3":"code","e2f9be60":"code","52eef39a":"markdown","377001d8":"markdown","7d620db1":"markdown","15f63b43":"markdown","a921a99f":"markdown","7e833d55":"markdown","91685d54":"markdown","40985992":"markdown","a7c4779f":"markdown","7b8d8dd3":"markdown","849d2021":"markdown","cf611dff":"markdown"},"source":{"2ff2b1b2":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport eli5","934460dd":"train = pd.read_csv('..\/input\/train.csv').fillna(' ')\nvalid = pd.read_csv('..\/input\/valid.csv').fillna(' ')\ntest = pd.read_csv('..\/input\/test.csv').fillna(' ')","14c1a125":"train.head()","66ae9ccc":"train_val = pd.concat([train, valid])","e7cceef0":"sns.countplot(train_val['label']);\nplt.title('Train+val: Target distribution');","a72e0ab5":"plt.subplots(1, 2)\nplt.subplot(1, 2, 1)\ntrain_val['text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Text length in words');\nplt.subplot(1, 2, 2)\ntrain_val['title'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Title length in words');","424ec3bf":"plt.subplots(1, 2)\nplt.subplot(1, 2, 1)\ntest['text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Text length in words');\nplt.subplot(1, 2, 2)\ntest['title'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Title length in words');","1867d882":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_val[\"title\"], title=\"Word Cloud of Titles\")","8069e873":"title_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), lowercase=True, max_features=50000)\ntext_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)","6717261d":"%%time\nX_train_text = text_transformer.fit_transform(train_val['text'])\nX_test_text = text_transformer.transform(test['text'])","a25ff3c2":"%%time\nX_train_title = title_transformer.fit_transform(train_val['title'])\nX_test_title = title_transformer.transform(test['title'])","9148bcbc":"X_train = hstack([X_train_text, X_train_title])\nX_test = hstack([X_test_text, X_test_title])","f1cdabe2":"X_train.shape, X_test.shape","5e7979e0":"logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial',\n                          random_state=17, n_jobs=4)","d9b1f77a":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)","c830d712":"%%time\ncv_results = cross_val_score(logit, X_train, train_val['label'], cv=skf, scoring='f1_macro')","efa0479f":"cv_results, cv_results.mean()","9d40d8bd":"%%time\nlogit.fit(X_train, train_val['label'])","aefde134":"eli5.show_weights(estimator=logit, \n                  feature_names= list(text_transformer.get_feature_names()) + \n                                 list(title_transformer.get_feature_names()),\n                 top=(50, 5))","1332c00e":"test_preds = logit.predict(X_test)","e6c348a3":"pd.DataFrame(test_preds, columns=['label']).head()","e2f9be60":"pd.DataFrame(test_preds, columns=['label']).to_csv('logit_tf_idf_starter_submission.csv',\n                                                  index_label='id')","52eef39a":"**Cross-validation**","377001d8":"As for model, let's simply pick logistic regression.****","7d620db1":"We'll be validating with train + validation files.","15f63b43":"After concatenation, we get 200k sparse features to represent news titles and texts.","a921a99f":"**Trying to interpret model weights with ELI5 - look reasonable.**","7e833d55":"We can see that test titles are a bit shorter.","91685d54":"As an entertainment, we can build a wordcloud for news titles. However, no useful insights from such a picture.","40985992":"It's nice to see that cross-validation is more or less stable across folds. Let's train the model on train + val.","a7c4779f":"**What you can try next**\n\n - tune hyperparams, those of `TfIdfVectorizers` as well\n - add Word2Vec\/GloVE\/Fasttext embeddings, at least for titles\n - switch to ULMFiT and other heavy stuff\n ","7b8d8dd3":"We'll be using 2 Tf-Idf vectorizers - for titles and texts separetely.","849d2021":"**Preparing submission.**","cf611dff":"Tf-Idf + Logistic regression is a very nice baseline for many tasks. Here we'll briefly explore a dataset with news classified as \"news\" (normal), \"clickbait\", and others. Then we'll build a simple baseline based on Tf-Idf representations of news titles and texts."}}