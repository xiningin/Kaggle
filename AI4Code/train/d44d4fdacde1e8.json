{"cell_type":{"4f867f9c":"code","02892db5":"code","3e0f164c":"code","8f7e7a19":"code","4fdd1af6":"code","45b7da9e":"code","5f3cddae":"code","c7f13021":"code","11b0ee01":"code","39b5c819":"code","5a7160dd":"code","6c7e937b":"code","65b1346c":"code","f4544568":"code","0a516da2":"code","1e65b250":"code","048729d4":"code","4e0d5460":"markdown","bdad9186":"markdown","42c1cb14":"markdown","6316a1da":"markdown","7015291a":"markdown","14d3e114":"markdown","2c52901c":"markdown","e21313ef":"markdown","ffed77b9":"markdown","25120ee9":"markdown"},"source":{"4f867f9c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","02892db5":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')\ntrain_tn = pd.read_parquet('..\/input\/pog-youtube-like-thumbnail-feature-embeddings\/train_thumbnail_feats.parquet')\ntest_tn = pd.read_parquet('..\/input\/pog-youtube-like-thumbnail-feature-embeddings\/test_thumbnail_feats.parquet')","3e0f164c":"train.shape, train_tn.shape, test.shape, test_tn.shape","8f7e7a19":"th_feat_cols = [f'f{c}' for c in range(1000)]\n\npca = PCA(n_components=5, random_state=529)\ntrain_tn_pca = pca.fit_transform(train_tn[th_feat_cols].values)\ntrain_tn_pca_df = pd.DataFrame(train_tn_pca, columns=[f'pca{x}' for x in range(5)])\ntrain_tn_pca_df['id'] = train_tn['id']\n\ntest_tn_pca = pca.transform(test_tn[th_feat_cols].values)\ntest_tn_pca_df = pd.DataFrame(test_tn_pca, columns=[f'pca{x}' for x in range(5)])\ntest_tn_pca_df['id'] = test_tn['id']","4fdd1af6":"train = train.merge(train_tn_pca_df,\n            how='left',\n            on=['id'],\n           )\n\ntest = test.merge(test_tn_pca_df,\n            how='left',\n            on=['id'],\n           )","45b7da9e":"cfg = {\n    'TARGET' : 'target',\n    'N_FOLDS' : 5,\n    'RANDOM_STATE': 529,\n    'N_ESTIMATORS' : 50_000,\n    'LEARNING_RATE': 0.1\n}\n\ntrain_vids = train['video_id'].unique()","5f3cddae":"kf = KFold(n_splits=cfg['N_FOLDS'],\n           shuffle=True,\n           random_state=cfg['RANDOM_STATE'])\n\n# Create Folds\nfold = 1\nfor tr_idx, val_idx in kf.split(train_vids):\n    fold_vids = train_vids[val_idx]\n    train.loc[train['video_id'].isin(fold_vids), 'fold'] = fold\n    fold += 1\ntrain['fold'] = train['fold'].astype('int')","c7f13021":"def create_features(df, train=True):\n    \"\"\"\n    Adds features to training or test set.\n    \"\"\"\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    # Feature 1 - Age of video\n    df['video_age_seconds'] = (df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    df['categoryId'] = df['categoryId'].astype('category')\n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    \n    return df","11b0ee01":"train['isTrain'] = True\ntest['isTrain'] = False\ntt = pd.concat([train, test]).reset_index(drop=True).copy()\ntt = create_features(tt)\ntrain_feats = tt.query('isTrain').reset_index(drop=True).copy()\ntest_feats = tt.query('isTrain == False').reset_index(drop=True).copy()","39b5c819":"train.columns","5a7160dd":"FEATURES = ['video_age_seconds',\n            'trending_dow',\n            'published_dow',\n            'duration_seconds',\n            'categoryId',\n            'comments_disabled',\n            'ratings_disabled',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',\n            'pca0', 'pca1', 'pca2', 'pca3', 'pca4'\n]\nTARGET = ['target']","6c7e937b":"X_test = test_feats[FEATURES]\noof = train_feats[['id','target','fold']].reset_index(drop=True).copy()\nsubmission_df = test[['id']].copy()","65b1346c":"regs = []\nfis = []\n# Example Fold 1\nfor fold in range(1, 6):\n    print(f'===== Running for fold {fold} =====')\n    # Split train \/ val\n    X_tr = train_feats.query('fold != @fold')[FEATURES]\n    y_tr = train_feats.query('fold != @fold')[TARGET]\n    X_val = train_feats.query('fold == @fold')[FEATURES]\n    y_val = train_feats.query('fold == @fold')[TARGET]\n    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n\n    # Create our model\n    reg = lgb.LGBMRegressor(n_estimators=cfg['N_ESTIMATORS'],\n                            learning_rate=cfg['LEARNING_RATE'],\n                            objective='mae',\n                            metric=['mae'],\n                            importance_type='gain'\n                           )\n    # Fit our model\n    reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            early_stopping_rounds=500,\n            verbose=200,\n           )\n\n    # Predicting on validation set\n    fold_preds = reg.predict(X_val,\n                             num_iteration=reg.best_iteration_)\n    oof.loc[oof['fold'] == fold, 'preds'] = fold_preds\n    # Score validation set\n    fold_score = mean_absolute_error(\n        oof.query('fold == 1')['target'],\n            oof.query('fold == 1')['preds']\n    )\n\n    # Creating a feature importance dataframe\n    fi = pd.DataFrame(index=reg.feature_name_,\n                 data=reg.feature_importances_,\n                 columns=[f'{fold}_importance'])\n\n    # Predicting on test\n    fold_test_pred = reg.predict(X_test,\n                num_iteration=reg.best_iteration_)\n    submission_df[f'pred_{fold}'] = fold_test_pred\n    print(f'Score of this fold is {fold_score:0.6f}')\n    regs.append(reg)\n    fis.append(fi)","f4544568":"oof_score = mean_absolute_error(oof['target'], oof['preds'])\nprint(f'Out of fold score {oof_score:0.6f}')","0a516da2":"fis_df = pd.concat(fis, axis=1)\nfis_df.sort_values('1_importance').plot(kind='barh', figsize=(12, 8),\n                                       title='Feature Importance Across Folds')\nplt.show()","1e65b250":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\n\nsubmission_df['target'] = submission_df[pred_cols].mean(axis=1)\n# Visually check correlation between fold predictions\nsns.heatmap(submission_df[pred_cols].corr(), annot=True)","048729d4":"submission_df[['id','target']] \\\n    .to_csv('submission.csv', index=False)","4e0d5460":"# Feature Engineering","bdad9186":"# Setup KFold","42c1cb14":"# Run PCA on Thumbnail Features\n- Reduce the dimensionality of the data\n- From 1000 features -> 5","6316a1da":"# Create Submission","7015291a":"# Set Target and Features","14d3e114":"# Pog Competition Baseline\n- LightGBM + Embedding Features\n- Using PCA to reduce the dimensionality of the features from 1000 -> 5\n- We find the results are that the model is WORSE with these features.","2c52901c":"## SQL Example\n\nThe below merge can be thought about as a join in sql\n\n- VIDEO_TABLE (id)\n- THUMBNAIL_TABLE (id)\n\n```{sql}\nSELECT *\nFROM VIDEO_TABLE v\n    LEFT JOIN THUMBNAIL_TABLE t\nON\n v.id = t.id\n```","e21313ef":"# Look at Fold Feature Importances","ffed77b9":"# Create Folds\n- This is how we will later split when validating our models","25120ee9":"# Train LGBM Model"}}