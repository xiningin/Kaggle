{"cell_type":{"6dbc3e1a":"code","c9212fb8":"code","3ae0ee5b":"code","659c571d":"markdown","ba02174c":"markdown","e23788e7":"markdown"},"source":{"6dbc3e1a":"!pip install simpletransformers \n!pip uninstall fsspec -y\n!pip install fsspec==2021.5.0\n!pip install seqeval==0.0.12 simpletransformers==0.45.3 tokenizers==0.8.1rc1 transformers==3.0.2","c9212fb8":"import pandas as pd, torch, warnings; warnings.simplefilter('ignore'); from simpletransformers.classification.classification_model import ClassificationModel\ntrain_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning2.csv')[['text', 'target']]\ntest_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/test_data_cleaning2.csv')[['id', 'text']]\nmodel = ClassificationModel('distilbert', 'distilbert-base-uncased', args={'fp16': False,'train_batch_size': 4, 'gradient_accumulation_steps': 2, 'model_args.silent' : True,\n        'learning_rate': 2e-05, 'do_lower_case': True, 'overwrite_output_dir': True, 'manual_seed': 42, 'num_train_epochs': 1}, weight = [0.44, 0.56])\nmodel.train_model(train_data)\ntest_data[\"target\"], _ = model.predict(test_data['text'])\ntest_data.drop(columns=['text']).to_csv(\"submission.csv\", index=False, header=True)","3ae0ee5b":"# In addition - accuracy evaluation\n# Accuracy\nimport sklearn\nresult, model_outputs, wrong_predictions = model.eval_model(train_data, acc=sklearn.metrics.accuracy_score)\nprint('Accuracy = ',round(result['acc'],2),'%', sep = \"\")\n\n# Confusion_matrix, Accuracy_score, Classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\npredictions, _ = model.predict(train_data['text'])\nmatrix = confusion_matrix(train_data[\"target\"],predictions)\nprint(matrix)\n\nscore = accuracy_score(train_data[\"target\"],predictions)\nprint(score)\n\nreport = classification_report(train_data['target'],predictions)\nprint(report)","659c571d":"## This notebook is based on:\n* notebook [NLP with DT cleaning: Simple Transformers predict](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-dt-cleaning-simple-transformers-predict)\n* notebook [NLP with DT: Simple Transformers Research](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-dt-simple-transformers-research)\n* notebook [SimpleTransformers + Hyperparam Tuning + k-fold CV](https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv)\n* libraries [simpletransformers](https:\/\/github.com\/ThilinaRajapakse\/simpletransformers), [transformers](https:\/\/huggingface.co\/transformers)\n* dataset [NLP with Disaster Tweets - cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data)\n\nMore detailed solution see: [NLP with DT: Simple Transformers Research](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-dt-simple-transformers-research)\n\nList of models see: https:\/\/huggingface.co\/transformers\/pretrained_models.html","ba02174c":"### The notebook is universal and can be used in other Kaggle competitions or for forecasting data from one dataset after a little adaptation to another data structure.","e23788e7":"# Supershort NLP classification notebook for prize competition \"[Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\""}}