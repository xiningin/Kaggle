{"cell_type":{"6ab765db":"code","b89aa3a8":"code","57ae6d7b":"code","e78f2503":"code","f3765f6d":"code","d0a4753d":"code","156b7012":"code","7da0b726":"code","eeeb3d57":"code","e0bd1bd3":"code","25343ccc":"code","56491cdd":"code","7995ba62":"code","efe6da5b":"code","e9eb31c6":"markdown"},"source":{"6ab765db":"# Imports\nimport numpy as np\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nfrom keras import backend\nfrom keras.models import Model\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg16 import VGG16\n\nfrom scipy.optimize import fmin_l_bfgs_b","b89aa3a8":"# Hyperparams\nITERATIONS = 50\nCHANNELS = 3\nIMAGE_SIZE = 500\nIMAGE_WIDTH = IMAGE_SIZE\nIMAGE_HEIGHT = IMAGE_SIZE\nIMAGENET_MEAN_RGB_VALUES = [123.68, 116.779, 103.939]\nCONTENT_WEIGHT = 0.02\nSTYLE_WEIGHT = 4.5\nTOTAL_VARIATION_WEIGHT = 0.995\nTOTAL_VARIATION_LOSS_FACTOR = 1.25","57ae6d7b":"# Paths\ninput_image_path = \"cai.jpg\"\nstyle_image_path = \"wu3.png\"\noutput_image_path = \"output.png\"\ncombined_image_path = \"combined.png\"\n\n# San Francisco\n#san_francisco_image_path = \"http:\/\/science.china.com.cn\/images\/attachement\/png\/site555\/20151126\/e89a8ffb139317c116544b.png\"\n\n# Warsaw by Tytus Brzozowski, http:\/\/t-b.pl\n#tytus_image_path = \"http:\/\/src.leju.com\/imp\/imp\/deal\/df\/fe\/5\/bd92ef65cb531d8f2f2c26acae1_p24_mk24_s600X0.png\"","e78f2503":"#Input visualization \ninput_image = Image.open(\"..\/input\/input.jpg\")\ninput_image = input_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\ninput_image.save(input_image_path)\ninput_image","f3765f6d":"# Style visualization \nstyle_image = Image.open(\"..\/input\/style.jpg\")\nstyle_image = style_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\nstyle_image.save(style_image_path)\nstyle_image","d0a4753d":"# Data normalization and reshaping from RGB to BGR\ninput_image_array = np.asarray(input_image, dtype=\"float32\")\ninput_image_array = np.expand_dims(input_image_array, axis=0)\ninput_image_array[:, :, :, 0] -= IMAGENET_MEAN_RGB_VALUES[2]\ninput_image_array[:, :, :, 1] -= IMAGENET_MEAN_RGB_VALUES[1]\ninput_image_array[:, :, :, 2] -= IMAGENET_MEAN_RGB_VALUES[0]\ninput_image_array = input_image_array[:, :, :, ::-1]\n\nstyle_image_array = np.asarray(style_image, dtype=\"float32\")\nstyle_image_array = np.expand_dims(style_image_array, axis=0)\nstyle_image_array[:, :, :, 0] -= IMAGENET_MEAN_RGB_VALUES[2]\nstyle_image_array[:, :, :, 1] -= IMAGENET_MEAN_RGB_VALUES[1]\nstyle_image_array[:, :, :, 2] -= IMAGENET_MEAN_RGB_VALUES[0]\nstyle_image_array = style_image_array[:, :, :, ::-1]\n","156b7012":"# Model\ninput_image = backend.variable(input_image_array)\nstyle_image = backend.variable(style_image_array)\ncombination_image = backend.placeholder((1, IMAGE_HEIGHT, IMAGE_SIZE, 3))\n\ninput_tensor = backend.concatenate([input_image,style_image,combination_image], axis=0)\nmodel = VGG19(input_tensor=input_tensor, include_top=False)","7da0b726":"def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\n\nlayers = dict([(layer.name, layer.output) for layer in model.layers])\n\ncontent_layer = 'block2_conv2'\nlayer_features = layers[content_layer]\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\n\nloss = backend.variable(0.)\nloss += CONTENT_WEIGHT * content_loss(content_image_features,\n                                      combination_features)","eeeb3d57":"def gram_matrix(x):\n    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n    gram = backend.dot(features, backend.transpose(features))\n    return gram","e0bd1bd3":"def compute_style_loss(style, combination):\n    style = gram_matrix(style)\n    combination = gram_matrix(combination)\n    size = IMAGE_HEIGHT * IMAGE_WIDTH\n    return backend.sum(backend.square(style - combination)) \/ (4. * (CHANNELS ** 2) * (size ** 2))\n\nstyle_layers = [\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\", \"block5_conv3\"]\nfor layer_name in style_layers:\n    layer_features = layers[layer_name]\n    style_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    style_loss = compute_style_loss(style_features, combination_features)\n    loss += (STYLE_WEIGHT \/ len(style_layers)) * style_loss","25343ccc":"def total_variation_loss(x):\n    a = backend.square(x[:, :IMAGE_HEIGHT-1, :IMAGE_WIDTH-1, :] - x[:, 1:, :IMAGE_WIDTH-1, :])\n    b = backend.square(x[:, :IMAGE_HEIGHT-1, :IMAGE_WIDTH-1, :] - x[:, :IMAGE_HEIGHT-1, 1:, :])\n    return backend.sum(backend.pow(a + b, TOTAL_VARIATION_LOSS_FACTOR))\n\nloss += TOTAL_VARIATION_WEIGHT * total_variation_loss(combination_image)","56491cdd":"outputs = [loss]\noutputs += backend.gradients(loss, combination_image)\n\ndef evaluate_loss_and_gradients(x):\n    x = x.reshape((1, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n    outs = backend.function([combination_image], outputs)([x])\n    loss = outs[0]\n    gradients = outs[1].flatten().astype(\"float64\")\n    return loss, gradients\n\nclass Evaluator:\n\n    def loss(self, x):\n        loss, gradients = evaluate_loss_and_gradients(x)\n        self._gradients = gradients\n        return loss\n\n    def gradients(self, x):\n        return self._gradients\n\nevaluator = Evaluator()","7995ba62":"x = np.random.uniform(0, 255, (1, IMAGE_HEIGHT, IMAGE_WIDTH, 3)) - 128.\n\nfor i in range(ITERATIONS):\n    x, loss, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.gradients, maxfun=20)\n    print(\"Iteration %d completed with loss %d\" % (i, loss))\n    \nx = x.reshape((IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\nx = x[:, :, ::-1]\nx[:, :, 0] += IMAGENET_MEAN_RGB_VALUES[2]\nx[:, :, 1] += IMAGENET_MEAN_RGB_VALUES[1]\nx[:, :, 2] += IMAGENET_MEAN_RGB_VALUES[0]\nx = np.clip(x, 0, 255).astype(\"uint8\")\noutput_image = Image.fromarray(x)\noutput_image.save(output_image_path)\noutput_image","efe6da5b":"# Visualizing combined results\ncombined = Image.new(\"RGB\", (IMAGE_WIDTH*3, IMAGE_HEIGHT))\nx_offset = 0\nfor image in map(Image.open, [input_image_path, style_image_path, output_image_path]):\n    combined.paste(image, (x_offset, 0))\n    x_offset += IMAGE_WIDTH\ncombined.save(combined_image_path)\ncombined","e9eb31c6":"Check out corresponding Medium article:\n\n[Style Transfer - Styling Images with Convolutional Neural Networks](https:\/\/towardsdatascience.com\/style-transfer-styling-images-with-convolutional-neural-networks-7d215b58f461)"}}