{"cell_type":{"8d0e8e35":"code","4724079a":"code","5a044177":"code","91264d79":"code","7c8ad351":"code","e970c188":"code","6ff707b2":"code","936365ee":"code","cf485af0":"code","ac83fc70":"code","7aad10f0":"code","93b1a89f":"code","d16e2571":"code","bcbdf338":"code","bea1d946":"code","987c787c":"code","400292c1":"code","26b3ac72":"code","1b074ec7":"code","0e64e04f":"code","743eade0":"code","076373dc":"code","5e8eb11a":"code","051757d4":"code","fbaf1e75":"code","bf573ac5":"code","ced8b1f7":"code","8d8a5c9f":"code","e8823774":"code","4eed7d74":"code","99e64ee1":"code","4ce4d47f":"code","3f0b05e9":"code","090e412d":"code","8d7d8bbc":"code","affc7410":"code","e53f26e1":"code","d98c9b3b":"code","56848a85":"code","f95bf8b9":"code","733df865":"code","a93c778f":"code","83e19765":"code","de4ec65a":"code","bea85237":"code","276ba564":"code","bcc2cca8":"markdown","c442486d":"markdown","c98c5b5b":"markdown","2ea7c00d":"markdown","f5cf8842":"markdown","5cf7a4c1":"markdown","db7e1dea":"markdown","e2d44dbf":"markdown","cd3eff9d":"markdown","f16f6649":"markdown","a1879e45":"markdown","cff82bde":"markdown","f39ed8ce":"markdown"},"source":{"8d0e8e35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom wordcloud import WordCloud\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4724079a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Activation, Flatten\nfrom keras.models import Model\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","5a044177":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nimport nltk \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\n# Use English stemmer.\nword_stemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()","91264d79":"from gensim.models.keyedvectors import KeyedVectors","7c8ad351":"real_news_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake_news_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","e970c188":"real_news_df['text'] = real_news_df['title'] + \" \" + real_news_df['text'] + \" \" + real_news_df['subject']\nfake_news_df['text'] = fake_news_df['title'] + \" \" + fake_news_df['text'] + \" \" + real_news_df['subject']","6ff707b2":"print(real_news_df.shape)\nprint(fake_news_df.shape)\nreal_news_df = real_news_df[real_news_df['text'].str.len() >= 3]\nfake_news_df = fake_news_df[fake_news_df['text'].str.len() >=3]\nreal_news_df['real_fact'] = 1\nfake_news_df['real_fact'] = 0\nprint(real_news_df.shape)\nprint(fake_news_df.shape)","936365ee":"print(real_news_df.head(3))","cf485af0":"print(fake_news_df.head(3))","ac83fc70":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","7aad10f0":"def get_cleaned_data(input_data, mode='df'):\n    stop = stopwords.words('english')\n    \n    input_df = ''\n    \n    if mode != 'df':\n        input_df = pd.DataFrame([input_data], columns=['text'])\n    else:\n        input_df = input_data\n        \n    #lowercase the text\n    input_df['text'] = input_df['text'].str.lower()\n    \n    input_df['text'] = input_df['text'].apply(lambda elem: decontracted(elem))\n    \n    #remove special characters\n    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))\n    \n    # remove numbers\n    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    #remove stopwords\n    input_df['text'] = input_df['text'].apply(lambda x: ' '.join([word.strip() for word in x.split() if word not in (stop)]))\n    \n    #stemming, changes the word to root form\n#     input_df['text'] = input_df['text'].apply(lambda words: [word_stemmer.stem(word) for word in words])\n    \n    #lemmatization, same as stemmer, but language corpus is used to fetch the root form, so resulting words make sense\n#     more description @ https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python\n    input_df['text'] = input_df['text'].apply(lambda words: (wordnet_lemmatizer.lemmatize(words)))\n#     print(input_df.head(3))\n    \n    return input_df","93b1a89f":"fake_news_df = get_cleaned_data(fake_news_df)","d16e2571":"real_news_df = get_cleaned_data(real_news_df)","bcbdf338":"news_data_df = pd.concat([real_news_df, fake_news_df], ignore_index = True)\nprint(news_data_df.shape)","bea1d946":"news_data_df.head(2)","987c787c":"MAX_SEQUENCE_LENGTH = 500\nMAX_NUM_WORDS = 10000\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.3","400292c1":"x_train,x_test,y_train,y_test = train_test_split(news_data_df.text,news_data_df.real_fact,random_state = 42, test_size=VALIDATION_SPLIT, shuffle=True)","26b3ac72":"tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n\n# Updates internal vocabulary based on a list of texts. \n# This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" \n# It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. \n# So lower integer means more frequent word (often the first few are stop words because they appear a lot).\ntokenizer.fit_on_texts(x_train)\n\n# Transforms each text in texts to a sequence of integers. \n# So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n# sequences = tokenizer.texts_to_sequences(news_data_df.text)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nX_train = pad_sequences(tokenized_train, maxlen=MAX_SEQUENCE_LENGTH)\n\nword_index = tokenizer.word_index\nprint('Found {} unique tokens. and {} lines '.format(len(word_index), len(X_train)))","1b074ec7":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = pad_sequences(tokenized_test, maxlen=MAX_SEQUENCE_LENGTH)","0e64e04f":"def get_embeddings(path):\n  # model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300', binary=True, limit=500000)\n  wv_from_bin = KeyedVectors.load_word2vec_format(path, binary=True, limit=500000) \n  #extracting word vectors from google news vector\n  embeddings_index = {}\n  for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n      coefs = np.asarray(vector, dtype='float32')\n      embeddings_index[word] = coefs\n  \n  return embeddings_index","743eade0":"embeddings_index = {}\nembeddings_index = get_embeddings('\/kaggle\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin')\nprint('Found %s word vectors.' % len(embeddings_index))","076373dc":"# # prepare embedding matrix format - 1\n# num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n# embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n# for word, i in word_index.items():\n#     if i >= MAX_NUM_WORDS:\n#         continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None:\n#         # words not found in embedding index will be all-zeros.\n#         embedding_matrix[i] = embedding_vector","5e8eb11a":"vocab_size = len(tokenizer.word_index) + 1\n\nembedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    try:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)","051757d4":"del embeddings_index","fbaf1e75":"news_data_df[news_data_df['real_fact'] == 0]","bf573ac5":"news_data_df[news_data_df['real_fact'] == 1]","ced8b1f7":"def cnn_net1():\n    model = Sequential()\n\n    #Non-trainable embeddidng layer\n    model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n    \n    model.add(Dropout(0.2))\n    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.2))\n    model.add(Dense(units = 250 , activation = 'relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","8d8a5c9f":"def cnn_net2():\n    model = Sequential()\n\n    #Non-trainable embeddidng layer\n    model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n    \n    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n    model.add(MaxPooling1D(4))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n    model.add(MaxPooling1D(4))\n    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n    model.add(MaxPooling1D(4))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(units = 128 , activation = 'relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","e8823774":"def lstm_net1():\n    model = Sequential()\n\n    #Non-trainable embeddidng layer\n    model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n    \n    model.add(LSTM(units=128 , return_sequences = True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(units=64))\n    model.add(Dropout(0.1))\n    model.add(Dense(units = 32 , activation = 'relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","4eed7d74":"unseen_data_fake = \"\"\"\nAmericans to fund killing babies in abortion that she has been caught trying to add taxpayer financing of abortions to the bill to combat the Coronavirus and provide economic stimulus to the nation as it deals with the COVD-19 outbreak.\nNancy Pelosi has a long history of promoting abortion and her first act after becoming Speaker in 2019 was pushing legislation to use tax money for abortions. So it\u2019s no surprise she is trying to exploit the Coronavirus pandemic to push abortion funding again.\nAs The Daily Caller reports: House Speaker Nancy Pelosi sought to include a potential way to guarantee federal funding for abortion into the coronavirus economic stimulus plan, according to multiple senior White House officials.\nSpeaking to the Daily Caller, those officials alleged that while negotiating the stimulus with U.S. Treasury Secretary Steve Mnuchin, Pelosi tried to lobby for \u201cseveral\u201d provisions that stalled bipartisan commitment to the effort. One was a mandate for up to $1 billion to reimburse laboratory claims, which White House officials say would set a precedent of health spending without protections outlined in the Hyde Amendment.\nLifeNews depends on the support of readers like you to combat the pro-abortion media. Please donate now.\n\u201cA New mandatory funding stream that does not have Hyde protections would be unprecedented,\u201d one White House official explained. \u201cUnder the guise of protecting people, Speaker Pelosi is working to make sure taxpayer dollars are spent covering abortion\u2014which is not only backwards, but goes against historical norms.\u201d\nA second White House official referred to the provision as a \u201cslush fund\u201d and yet another questioned \u201cwhat the Hyde Amendment and abortion have to do with protecting Americans from coronavirus?\u201d\nAmericans should insist to their members of Congress that we need a clean bill that provides aggressive action to help patients and spur the economy. Killing babies with our tax dollars is not the answer to the coronavirus and the situation should not be exploited for political gain.\n\"\"\"\n\nunseen_data_real = \"\"\"\nPrice spikes, however, would cause demand to wither and some expensive avocados might be leftover, and stores might try to ration avocados, he added.\n\"Exactly what the retail strategy would be in this case, I\u2019m not sure. But we would have vastly fewer avocados,\" Sumner said.\nJust how fast avocados would disappear, if at all, would depend on whether the Trump administration enacts a full or partial border closure. White House economic adviser Larry Kudlow told CNBC he\u2019s looking for ways to keep some commerce flowing.\n\"We are looking at different options, particularly if you can keep those freight lanes, the truck lanes, open,\" he said this week.  \nBen Holtz owns Rocky H Ranch, a 70-acre family-run avocado farm in northern San Diego County. He agreed avocados would run out within weeks.\n\"Mexico is the big player today. California is not. You shut down the border and California can\u2019t produce to meet the demand,\" Holtz said. \"There will be people without their guacamole.\"\nWhile Mexico\u2019s avocado harvest is year-round, California\u2019s is limited to April through July. Growers in the state have picked only about 3 percent of what\u2019s expected to be a much smaller crop of about 175 million pounds this year, Holtz said. A heat wave last summer reduced the crop size.\nCalifornia\u2019s avocado harvest has averaged approximately 300 million pounds in recent years, according to data from the California Avocado Commission. By contrast, the U.S. has imported more than 1.5 billion pounds of avocados from Mexico annually. Representatives from the commission did not respond to requests for this article.\nAltogether, the U.S. received 43 percent of its fruit and vegetable imports from Mexico in 2016, according to the U.S. Department of Agriculture.\nAlso affecting this year\u2019s avocado supply, a California avocado company in March recalled shipments to six states last month after fears the fruit might be contaminated with a bacterium that can cause health risks.\nUntil the early 2000s, California was the nation\u2019s leading supplier of avocados, Holtz said. Mexico gradually overtook the state and now dominates sales in the U.S.\n\"It\u2019s a very big possibility,\" Holtz said of avocado shortages. \"Three weeks would dry up the Mexican inventory. California alone consumes more avocados than are grown in our state. Cold storage supply chain is basically three weeks or less of inventory. Most of the time it\u2019s seven days.\"\nA spokeswoman for the California Restaurant Association said \"we haven\u2019t heard concerns from restaurants, it doesn\u2019t mean they aren\u2019t worried.\" A national grocers association said it will \"continue to closely monitor any developments\" at the border, but did not have information about the potential impact on avocados.\n\"\"\"","99e64ee1":"def get_pred_output(text_to_check):\n    sequences = tokenizer.texts_to_sequences([text_to_check])\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    predicted_val = model.predict_classes(data)\n#     predicted_val = model.predict(data)    \n#     if predicted_val.max() > 0.7:\n#         output = 1\n#     else:\n#         output = 0\n    return predicted_val\n    ","4ce4d47f":"# train a 1D convnet with global maxpooling\nmodel = cnn_net1()\n\nbatch_size = 256\nepochs = 8\n\nmodel.summary()","3f0b05e9":"history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)","090e412d":"accr_train = model.evaluate(X_train,y_train)\nprint('Accuracy Train: {}'.format(accr_train[1]*100))\naccr_test = model.evaluate(X_test,y_test)\nprint('Accuracy Test: {}'.format(accr_test[1]*100))","8d7d8bbc":"pred = model.predict_classes(X_test)\ncf_matrix = confusion_matrix(y_test,pred)\nsns.heatmap(cf_matrix, annot=True, fmt='g', xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])","affc7410":"text_to_check = unseen_data_real\npred = get_pred_output(text_to_check)\nprint('Unseen real data prediction {} '.format(pred[0]))\n\ntext_to_check = unseen_data_fake\npred = get_pred_output(text_to_check)\nprint('Unseen fake data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[1000]\npred = get_pred_output(text_to_check)\nprint('Seen real data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[31000]\npred = get_pred_output(text_to_check)\nprint('Seen fake data prediction {} '.format(pred[0]))","e53f26e1":"# train a 1D convnet with a deep network\nmodel = cnn_net2()\n\nbatch_size = 256\nepochs = 8\n\nmodel.summary()","d98c9b3b":"history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)","56848a85":"accr_train = model.evaluate(X_train,y_train)\nprint('Accuracy Train: {}'.format(accr_train[1]*100))\naccr_test = model.evaluate(X_test,y_test)\nprint('Accuracy Test: {}'.format(accr_test[1]*100))","f95bf8b9":"pred = model.predict_classes(X_test)\ncf_matrix = confusion_matrix(y_test,pred)\nsns.heatmap(cf_matrix, annot=True, fmt='g', xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])","733df865":"text_to_check = unseen_data_real\npred = get_pred_output(text_to_check)\nprint('Unseen real data prediction {} '.format(pred[0]))\n\ntext_to_check = unseen_data_fake\npred = get_pred_output(text_to_check)\nprint('Unseen fake data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[1000]\npred = get_pred_output(text_to_check)\nprint('Seen real data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[31000]\npred = get_pred_output(text_to_check)\nprint('Seen fake data prediction {} '.format(pred[0]))\n","a93c778f":"#training an LSTM network\nmodel = lstm_net1()\n\nbatch_size = 256\nepochs = 8\n\nmodel.summary()","83e19765":"history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)","de4ec65a":"pred = model.predict_classes(X_test)\ncf_matrix = confusion_matrix(y_test,pred)\nsns.heatmap(cf_matrix, annot=True, fmt='g', xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])","bea85237":"accr_train = model.evaluate(X_train,y_train)\nprint('Accuracy Train: {}'.format(accr_train[1]*100))\naccr_test = model.evaluate(X_test,y_test)\nprint('Accuracy Test: {}'.format(accr_test[1]*100))","276ba564":"text_to_check = unseen_data_real\npred = get_pred_output(text_to_check)\nprint('Unseen real data prediction {} '.format(pred[0]))\n\ntext_to_check = unseen_data_fake\npred = get_pred_output(text_to_check)\nprint('Unseen fake data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[1000]\npred = get_pred_output(text_to_check)\nprint('Seen real data prediction {} '.format(pred[0]))\n\ntext_to_check = news_data_df.text[31000]\npred = get_pred_output(text_to_check)\nprint('Seen fake data prediction {} '.format(pred[0]))\n","bcc2cca8":"**Remove missing text rows from the data whose length is less than 3 characters (cells with only white spaces for ex.) and also rows with NAN**","c442486d":"Please upvote if you like the notebook, Thanks.","c98c5b5b":"The LSTM model + Pre-trained Word2Vec, was able to predit the **unseen** real & fake data better compared with CNN + Pre-trained Word2Vec. This clearly shows the underlying benefits of RNN in sentiment analysis.\n\nWe need to increase the training iterations and test more on unseen dataset to conclude.","2ea7c00d":"Verify if the dataframe has real vs fact values, real news has real_fact = 1 and fake news has real_fact = 0","f5cf8842":"Preparing embedding matrix.","5cf7a4c1":"Data Cleaning ","db7e1dea":"LSTM network for learning the embedding and classification","e2d44dbf":"**Prepare the CNN model with GlobalMaxPooling for classification**","cd3eff9d":"Free up the memory","f16f6649":"Fetch pre-trained embedding index from GoogleNews-vectors-negative300:\n* GoogleNews vectors are in most to least frequent order, so the first N are usually the N-sized subset. \n* So use limit=500000 to get the most-frequent 500,000 words' vectors \u2013 saving 5\/6ths of the memory\/load-time.","a1879e45":"**Prepare the CNN model with Deep network for classification**","cff82bde":"All models with pre-trained Word2Vec from GoogleNewsVectors.\n\nAccuracies achieved with validation data are: \n* CNN with GlobalMaxpool: 99%\n* CNN with deep network: 95%\n* LSTM: 99%","f39ed8ce":"Vectorize the text samples into a 2D integer tensor"}}