{"cell_type":{"562c0571":"code","8a49e34f":"code","510b93f2":"code","86fb3e02":"code","227ed338":"code","d70dd132":"code","76051db5":"code","b8733977":"code","5a04b242":"code","3914f79c":"code","08fdf69a":"code","1a975855":"code","b3bd7443":"code","b2deeecc":"code","59926f8f":"markdown","be8f1850":"markdown","9c07b850":"markdown","87186e83":"markdown","39bdc0ac":"markdown","72056869":"markdown","6cde52dd":"markdown","2d0b64d7":"markdown","678f54b8":"markdown","eacb9b81":"markdown","b9669e52":"markdown","9c92bbda":"markdown","9f7200de":"markdown"},"source":{"562c0571":"import warnings\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib as mat\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport random\nimport os\n\nfrom numpy.random import seed","8a49e34f":"\n\nBATCH = 64\nNEW_SIZE = 150\n\nrandom.seed(42)\nos.environ['PYTHONHASHSEED'] = str(42)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\n\n\ntf.random.set_seed(42)\n\n\nIMG_SIZE = 224\nSEED = 42\n\nwarnings.filterwarnings('ignore')\n\npath = \"..\/input\/labeled-chest-xray-images\/chest_xray\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n\ntrain_path = os.path.join(path, \"train\")\ntest_path = os.path.join(path, \"test\")\n\ntrain_normal = glob.glob(train_path+\"\/NORMAL\/*.jpeg\")\ntrain_pneumonia = glob.glob(train_path+\"\/PNEUMONIA\/*.jpeg\")\n\ntest_normal = glob.glob(test_path+\"\/NORMAL\/*.jpeg\")\ntest_pneumonia = glob.glob(test_path+\"\/PNEUMONIA\/*.jpeg\")\ntrain_list = [x for x in train_normal]\ntrain_list.extend([x for x in train_pneumonia])\n\ndf_train = pd.DataFrame(np.concatenate(\n    [['Normal']*len(train_normal), ['Pneumonia']*len(train_pneumonia)]), columns=['class'])\ndf_train['image'] = [x for x in train_list]\n\ntest_list = [x for x in test_normal]\ntest_list.extend([x for x in test_pneumonia])\n\ndf_test = pd.DataFrame(np.concatenate(\n    [['Normal']*len(test_normal), ['Pneumonia']*len(test_pneumonia)]), columns=['class'])\ndf_test['image'] = [x for x in test_list]","510b93f2":"\ntrain_df, val_df = train_test_split(\n    df_train, test_size=0.20, random_state=SEED, stratify=df_train['class'])\n","86fb3e02":"\n# loading images\ntrain_datagen = ImageDataGenerator(rescale=1\/255.,\n                                   zoom_range=0.1,\n                                   # rotation_range = 0.1,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1)\n\nval_datagen = ImageDataGenerator(rescale=1\/255.)\n\nds_train = train_datagen.flow_from_dataframe(train_df,\n                                             x_col='image',\n                                             y_col='class',\n                                             target_size=(\n                                                 IMG_SIZE, IMG_SIZE),\n                                             class_mode='binary',\n                                             batch_size=BATCH,\n                                             seed=SEED)\n\nds_val = val_datagen.flow_from_dataframe(val_df,\n                                         x_col='image',\n                                         y_col='class',\n                                         target_size=(IMG_SIZE, IMG_SIZE),\n                                         class_mode='binary',\n                                         batch_size=BATCH,\n                                         seed=SEED)\n\nds_test = val_datagen.flow_from_dataframe(df_test,\n                                          x_col='image',\n                                          y_col='class',\n                                          target_size=(IMG_SIZE, IMG_SIZE),\n                                          class_mode='binary',\n                                          batch_size=1,\n                                          shuffle=False)\n# Setting callbakcs:\n\n\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    min_delta=0.0000001,\n    restore_best_weights=True,\n)\n\nplateau = callbacks.ReduceLROnPlateau(\n    monitor='acc',\n    factor=0.2,\n    patience=2,\n    min_delt=0.0000001,\n    cooldown=0,\n    verbose=1\n)\n","227ed338":"\n\ndef get_model():\n     # PREPROCESS INPUT TO 150X150\n    # The input of the network is adjusted to a 150 \u00d7 150 image block:\n    x = layers.Input(shape=(NEW_SIZE, NEW_SIZE, 3))\n    inputs = x\n    i = 0\n    # block from 1-49:\n    # and is convoluted by a series of 49 convolutional layers:\n    # loop through blocks:\n\n    while i < 49:\n        # setting (b) from paper:  - filter:2^kernel size, kernel: 3x3, dilation rate: 2\n        x = layers.Conv2D(filters=8, kernel_size=(3, 3),\n                          dilation_rate=1, activation='relu', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n       # x = layers.MaxPool2D()(x)\n        \n        i = i + 1\n    x = layers.Dropout(0.5)(x)\n\n        \n# dropout to stop half of the neurons followed by global mean\n    x = layers.GlobalAvgPool2D()(x)\n    # Head 2 dense layers\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dense(2, activation='relu')(x)\n# output into classification using sigmoid\n    output = layers.Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inputs=[inputs], outputs=output)\n\n    return model\n\n","d70dd132":"keras.backend.clear_session()\n\nmodel = get_model()\nmodel.compile(loss='binary_crossentropy'\n              , optimizer = keras.optimizers.Adam(), metrics='binary_accuracy')\n\nmodel.summary()","76051db5":"\nhistory = model.fit(ds_train,\n                    batch_size=BATCH, epochs=55,\n                    validation_data=ds_val,\n                    callbacks=[early_stopping, plateau],\n                    )","b8733977":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.set_ylim(0, 0.5)\nax.legend(['train', 'val'], loc='best')\nplt.show()","5a04b242":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['binary_accuracy'])\nsns.lineplot(x = history.epoch, y = history.history['val_binary_accuracy'])\nax.set_title('Learning Curve (Accuracy)')\nax.set_ylabel('Accuracy')\nax.set_xlabel('Epoch')\nax.set_ylim(0.80, 1.0)\nax.legend(['train', 'val'], loc='best')\nplt.show()","3914f79c":"score = model.evaluate(ds_val, steps = len(val_df)\/BATCH, verbose = 0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","08fdf69a":"score = model.evaluate(ds_test, steps = len(df_test), verbose = 0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","1a975855":"num_label = {'Normal': 0, 'Pneumonia' : 1}\nY_test = df_test['class'].copy().map(num_label).astype('int')\nds_test.reset()\npredictions = model.predict(ds_test, steps=len(ds_test), verbose=0)\npred_labels= np.where(predictions>0.5, 1, 0)\nprint(\"Test Accuracy: \", accuracy_score(Y_test, pred_labels))\nconfusion_matrix = metrics.confusion_matrix(Y_test, pred_labels)\n\n\n\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n\nplt.xlabel(\"Predicted Label\", fontsize= 12)\nplt.ylabel(\"True Label\", fontsize= 12)\n\nplt.show()","b3bd7443":"print(metrics.classification_report(Y_test, pred_labels, labels = [0, 1]))","b2deeecc":"roc_auc = metrics.roc_auc_score(Y_test, predictions)\nprint('ROC_AUC: ', roc_auc)\n\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, predictions)\n\nplt.plot(fpr, tpr, label = 'ROC_AUC = %0.3f' % roc_auc)\n\nplt.xlabel(\"False Positive Rate\", fontsize= 12)\nplt.ylabel(\"True Positive Rate\", fontsize= 12)\nplt.legend(loc=\"lower right\")\n\nplt.show()","59926f8f":"**DECLARANDO VARI\u00c1VEIS PARA O TREINAMENTO**\n\n\nAqui \u00e9 valido dizer que ja s\u00e3o feitas algumas altera\u00e7\u00f5es em rela\u00e7\u00e3o ao *framework* de origem devido ao algoritmo, nesse caso o BATCH_SIZE \u00e9 ajustado para 64 e \u00e9 declarado outra vari\u00e1vel para o novo tamanho de imagem que ser\u00e1 processada previamente,como descrito no artigo:\n\n\"In addition, we perform weight update with small batch data, and the batch size is set to 64...\"\n\n\n\"The input of the network is adjusted to a 150 \u00d7 150 image block after preprocessing,...\"","be8f1850":"**C\u00d3DIGO E ALGORITMO**\n\nO projeto consiste na cria\u00e7\u00e3o de um modelo de CNN descrito por Ziang e Leng, e com aux\u00edlio de um framework feito por Jonas Paluci Barbosa para implementa\u00e7\u00e3o de tal.\n\nO c\u00f3digo do framework pode ser encontrado em: https:\/\/www.kaggle.com\/jonaspalucibarbosa\/chest-x-ray-pneumonia-cnn-transfer-learning\n\nEnquanto o artigo de refer\u00eancia para o algoritmo: https:\/\/pubmed.ncbi.nlm.nih.gov\/31262537\/\n\n","9c07b850":"**Curva de aprendizagem:**","87186e83":"**Criando o set de Valida\u00e7\u00e3o Para Treinamento**\n\n\u00c9 utilizado os mesmos parametros do framework, com 20% sendo reservado para teste.","39bdc0ac":"**Criando o Modelo Descrito**\n\nAgora estamos prontos para declarar o modelo descrito por Liang e Zheng, que consiste em uma s\u00e9rie de 49 layers de convolu\u00e7\u00f5es e maxpooling com normaliza\u00e7\u00f5es de *Batch*, seguido de um GlobalAveragePooling, que \u00e9 seguido por mais duas camadas completamente conectadas(Dense), com 128 e 2 n\u00f3s em cada respectivamente.\n\n\nO tamanho de kernel \u00e9 (3x3) com uma taxa de dilata\u00e7\u00e3o de 1, como descrito no artigo, tamb\u00e9m \u00e9 sempre utilizado a ativa\u00e7\u00e3o Relu, apenas na camada de classifica\u00e7\u00e3o para output que usa-se a ativa\u00e7\u00e3o Sigmoid. O Dropout de 0.5 antes da m\u00e9dia geral tamb\u00e9m \u00e9 mencionado.\n\n\nNa conclus\u00e3o do artigo os autores citam que usam 49 camadas de convolu\u00e7\u00e3o e apenas uma vari\u00e1vel global para m\u00e9dia da camada de Pool, que tamb\u00e9m \u00e9 implementado.","72056869":"O modelo \u00e9 compilado utilizando o *Adam Optmizer* em par\u00e2metros *default*, como descrito no artigo","6cde52dd":"Os autores do artigo tamb\u00e9m especificam que o treinamento \u00e9 realizado em 100 \u00e9pocas. Entretanto n\u00e3o foi poss\u00edvel realizar devido ao poder de processamento, logo apenas 55 \u00e9pocas foram usadas.","2d0b64d7":"**IMPORTANDO BIBLIOTECAS E PACKAGES:**","678f54b8":"**M\u00e9tricas de Performance**","eacb9b81":"**Curva de Treinamento pelas \u00e9pocas**","b9669e52":"**Preparando os dados**\n\nAgora s\u00e3o carregados os conjuntos de imagens do banco de dados, por enquanto vamos utilizar o tamanho original (224x224) e a *reescaling* ser\u00e1 feito dentro do modelo. Al\u00e9m disso tamb\u00e9m s\u00e3o definidos os *callbacks*.","9c92bbda":"\u00c9 poss\u00edvel perceber que apesar de seguir diversos aspectos citados no artigo, n\u00e3o fopi poss\u00edvel alcan\u00e7ar o m\u00e1ximo potencial como descrito nos resultados, entretanto, alguns vaores como a acur\u00e1cia e precis\u00e3o foram relativamente parecidos. \u00c9 v\u00e1lido citar que foram realizados apenas 15 \u00e9pocas por conta do *kernel* reservado para o notebook.","9f7200de":" \n**Projeto 3 de Introdu\u00e7\u00e3o a Intelig\u00eancia Artificial**\n\n\n\n\n  Aluno: Felipe Nascimento Rocha - 17050084\n  \n  \n  Professor: D\u00edbio Borges"}}