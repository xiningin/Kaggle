{"cell_type":{"70b01089":"code","f66cd26f":"code","37a57f62":"code","2c44f958":"code","09f0635a":"code","0220bfd3":"code","4b672451":"code","12e0b2fc":"code","95847101":"code","76048ebe":"code","02c01e10":"code","0991fd76":"code","1dd992da":"code","123c8f80":"code","0d46accd":"code","a3587b88":"code","c550deed":"markdown","5a392a4b":"markdown","5173e09a":"markdown","1649a74e":"markdown","964d0295":"markdown","b068740a":"markdown","e4d33a51":"markdown","7e1d6188":"markdown","df7c3a6a":"markdown"},"source":{"70b01089":"# Import numpy, pandas, and matplotlib using the standard aliases. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import mpimg from matplotlib.image\nimport matplotlib.image as mpimg\n\n# Import train_test_split from sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Import pickle. \nimport pickle\n\n# Import tensorflow and all needed tools from tensorflow.keras. \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","f66cd26f":"# Load the training data into a DataFrame named 'train'. \n# Print the shape of the resulting DataFrame. \n# You do not need the test data in this notebook. \ntrain = pd.read_csv('..\/input\/cifar10-mu\/train.csv')\n\nprint('Number of rows:', train.shape[0])\nprint('Number of columns:', train.shape[1])","37a57f62":"# Display the head of the train DataFrame. \ntrain.head()","2c44f958":"train['label'] = train.label.apply(str)","09f0635a":"# Display a DataFrame showing the proportion of observations with each \n# possible of the target variable (which is label). \ntrain.label.value_counts().sort_index().to_frame().T","0220bfd3":"import os\n\ntrain_path = \"..\/input\/cifar10-mu\/train_images\/\"\nprint('Training Images: ', len(os.listdir(train_path)))","4b672451":"# Sample 16 images from the training set and display these along with their labels.\nimage = pd.concat([train.groupby(['label'], as_index=False)['filename'].min(), \n                   train.groupby(['label'], as_index=False)['filename'].max()], ignore_index=True)\n\n\n# The images should be arranged in a 4x4 grid of subplots. \n# Please set the figure sizeto (6,6)\n\nf, axs = plt.subplots(4, 4, figsize=(6, 6))\naxs = axs.flatten()\ni = 0\nfor ax in axs:\n    ax.imshow(mpimg.imread('..\/input\/cifar10-mu\/train_images\/'+image.filename[i]))\n    ax.text(0, -5, f'Label {image.label[i]}', color='k')\n    ax.axis('off')\n    i = i + 1\n\nplt.tight_layout()    \nplt.show()","12e0b2fc":"# Split the dataframe train into two DataFrames named train_df and valid_df. \n# Use 20% of the data for the validation set. \n# Use stratified sampling so that the label proportions are preserved.\n# Set a random seed for the split. \ntrain_df, valid_df = train_test_split(train, test_size=0.2, random_state=42, stratify=train.label)\n\nprint(train_df.shape)\nprint(valid_df.shape)","95847101":"# Create image data generators for both the training set and the validation set. \n# Use the data generators to scale the pixel values by a factor of 1\/255. \ntrain_datagen = ImageDataGenerator(rescale=1\/255)\nvalid_datagen = ImageDataGenerator(rescale=1\/255)","76048ebe":"# Complete the code for the data loaders below. \n\nBATCH_SIZE = 64\n\ntrain_loader = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = train_path,\n    x_col = 'filename',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)\n\nvalid_loader = valid_datagen.flow_from_dataframe(\n    dataframe = valid_df,\n    directory = train_path,\n    x_col = 'filename',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)","02c01e10":"# Run this cell to determine the number of training and validation batches. \n\nTR_STEPS = len(train_loader)\nVA_STEPS = len(valid_loader)\n\nprint(TR_STEPS)\nprint(VA_STEPS)","0991fd76":"# Use this cell to construct a convolutional neural network model. \n# Your model should make use of each of the following layer types:\n#    Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten, Dense\n# You can start by mimicking the architecture used in the \n# Aerial Cactus competetition, but you should explore different architectures\n# by adding more layers and\/or adding more nodes in individual layers\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape=(32,32,3)),\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.25),\n    BatchNormalization(),\n\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.75),\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(10, activation='softmax')\n])\n\ncnn.summary()","1dd992da":"opt = tf.keras.optimizers.Adam(0.001)\ncnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC()])","123c8f80":"%%time \n\nh1 = cnn.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 20,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1\n)","0d46accd":"# Display training curves after each run. \nhistory = h1.history\nn_epochs = len(history['loss'])\n\nplt.figure(figsize=[10,4])\nplt.subplot(1,2,1)\nplt.plot(range(1, n_epochs+1), history['loss'], label='Training')\nplt.plot(range(1, n_epochs+1), history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(range(1, n_epochs+1), history['accuracy'], label='Training')\nplt.plot(range(1, n_epochs+1), history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.show()","a3587b88":"# When you are satisfied with the model you have found, \n# save the model and the combined history dictionary to files.\n# Download these filesto your local device and then upload them \n# as a Kaggle dataset. \n# Save your pipeline to a file. \ncnn.save('CIFAR_10_CNN_Model.h1')\npickle.dump(history, open(f'CIFAR_10_history_v11.pkl', 'wb'))","c550deed":"# CIFAR 10 Image Classification\n\nMost of the code cells below include comments explaining the task to be performed in those cells. Please delete the comments and add code to perform those tasks. There are a few code cells in which code has already been provided for you. In some cases, you will need to complete this code.\n\n\u26a0 **NOTE:** You should make use of GPU acceleration in this notebook. \n\n","5a392a4b":"# Train Network","5173e09a":"# View Sample of Images","1649a74e":"# Data Generators","964d0295":"# Load Training DataFrame","b068740a":"# Save Model and History","e4d33a51":"# Import Packages","7e1d6188":"# Label Distribution","df7c3a6a":"# Build Network"}}