{"cell_type":{"6d0e9fee":"code","f51733d1":"code","2ab5a022":"code","67fe701a":"code","2bc1d704":"code","74a24c70":"code","32f0e9fd":"code","d185593e":"code","ee93ff22":"code","e75e27b1":"code","31f4172f":"code","e09fce7a":"code","f024d5b6":"code","65894a60":"code","0e960d69":"code","708a507c":"code","c7caeab3":"code","4f30f5d4":"code","ddc3b953":"code","c49b53eb":"code","f69d3259":"code","6acf313b":"code","bd4274c4":"code","f0c51181":"code","b91491e5":"code","039ba403":"code","92e153d9":"code","17a71ed9":"code","7fe59c39":"code","fdd934d5":"code","2d001ab1":"code","6fca0eef":"code","7b74c543":"code","88b83797":"code","83932afa":"code","915f2180":"code","0ac4623f":"code","c7d5499d":"code","29252fa0":"code","2df69d60":"code","fde472d8":"code","ece68554":"code","fe318867":"code","594ce68c":"code","ad442056":"code","763e95db":"code","af369e76":"code","6148e797":"code","4e662bcb":"code","edd6bc57":"code","85abcdef":"code","0202f978":"code","7722bf10":"code","5e06992a":"code","f668cfea":"code","5d7c9a7c":"code","ba218302":"code","a94bcb95":"code","474bc485":"code","71fa7be8":"code","281ac2ad":"code","f07c1ed5":"code","3f376a67":"code","abc6083a":"code","097ca990":"code","cea93992":"code","7bd73f5a":"code","d2cfd8fc":"code","120331cb":"code","f06b56dc":"code","38578062":"code","ae6a40d6":"code","366229ad":"code","6bd045cc":"code","7628d0e2":"code","fa23eb94":"code","e98b30ae":"code","66997f5b":"code","f6302faa":"code","d3c82624":"code","71473919":"code","7d0fcfa9":"code","fe59dc30":"code","abbd0b6e":"code","03964386":"code","a38fd4e2":"code","1f451209":"code","042db101":"code","58dfae78":"code","4d9e10f8":"code","0882af0c":"code","64ec64ed":"code","ca3c3e0f":"code","e6aaf467":"code","4c7cf51d":"code","0bcf8a8e":"code","386c0242":"code","0b4e78dc":"code","ac4f4af6":"code","65863121":"code","01d71b09":"code","da1c61d3":"code","9b51d15b":"code","024f93d3":"code","1ead40a0":"code","87ac246a":"code","d8a72397":"code","ce891e03":"code","490ec46a":"code","b690018c":"code","1992ac4d":"code","888ff3a8":"code","c9ec91f5":"code","f5f65218":"markdown","853dd3a6":"markdown","56e9f3a5":"markdown","4cdf2266":"markdown","1271c493":"markdown","7b741dea":"markdown","d727e529":"markdown","798248ce":"markdown","0bcf2e03":"markdown","761f6fd1":"markdown","b487c294":"markdown","8729e1f0":"markdown","404972f4":"markdown","a33a6d57":"markdown","aadb95f0":"markdown","3b39104d":"markdown","e7088a12":"markdown","d2bb950d":"markdown","2a53c8c5":"markdown","09e4ef3f":"markdown","a4d911b1":"markdown","083b1b88":"markdown","f7d85aea":"markdown","5b2ac56c":"markdown","47ccb48c":"markdown","26ec5bcd":"markdown","e800dc1c":"markdown","93b18cd7":"markdown"},"source":{"6d0e9fee":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix","f51733d1":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf","2ab5a022":"\n# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/titanic\/train.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5 \n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            ","67fe701a":"df.head()","2bc1d704":"df.tail()","74a24c70":"df.dtypes","32f0e9fd":"df.columns","d185593e":"df.shape","ee93ff22":"df.size","e75e27b1":"df.info()","31f4172f":"df.describe()","e09fce7a":"df.duplicated().sum()","f024d5b6":"df.isnull().sum()","65894a60":"df.drop(['Name','Ticket','Cabin'],axis = 1,inplace = True)\ndf","0e960d69":"df.skew()","708a507c":"df.corr()","c7caeab3":"! pip install Autoviz","4f30f5d4":"! pip install xlrd","ddc3b953":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz('..\/input\/titanic\/train.csv')","c49b53eb":"df['Sex'].value_counts()","f69d3259":"sns.countplot(x = 'Sex',data = df)\nplt.show()","6acf313b":"df['Embarked'].value_counts()","bd4274c4":"sns.countplot(x = 'Embarked',data = df)\nplt.show()","f0c51181":"df['Survived'].value_counts()","b91491e5":"sns.countplot(x = 'Survived',data = df)\nplt.show()\n# more died than survived","039ba403":"fig = px.histogram(df, 'Age',             \n                   color=\"Sex\",\n                   title=\"<b>Average Age Gender wise<\/b>\")\n\nfig.add_vline(x=df['Age'].mean(), line_width=2, line_dash=\"dash\", line_color=\"black\")\n\nfig.show()","92e153d9":"fig = px.histogram(data_frame = df,\n             x = \"Sex\",\n             color=\"Survived\", title=\"<b>Counts of surivied or not<\/b>\",\n             pattern_shape_sequence=['x'],\n             template='plotly_dark')\n\nfig.show()\n# females survived morethan males even males are nearly 2 times of females","17a71ed9":"fig = px.histogram(data_frame = df,\n             x = \"Embarked\",\n             color=\"Survived\", title=\"<b>Counts of Embarked surivied or not<\/b>\",\n             pattern_shape_sequence=['x'],\n             template='plotly_dark')\n\nfig.show()\n# Compared to other embarkes only in embarked C survived\/death ration is more and greatherthan 1","7fe59c39":"fig = px.histogram(data_frame = df,\n             x = \"Age\",\n             color=\"Survived\", title=\"<b>Counts of Embarked surivied or not<\/b>\",\n             pattern_shape_sequence=['x'],\n             template='plotly_dark')\nfig.show()\n\n# suprisingly one with age 81 survived","fdd934d5":"fig = px.histogram(data_frame = df,\n             x = \"Fare\",\n             color=\"Survived\", title=\"<b>Counts of surivied or not<\/b>\",\n             pattern_shape_sequence=['x'],\n             template='plotly_dark')\n\nfig.show()","2d001ab1":"sns.barplot(x = 'Survived',y = 'Fare',data = df)\nplt.show()\n# if ticket price is more surivial rate is also more","6fca0eef":"fig = px.histogram(df, 'Fare',             \n                   color = 'Survived')\nfig.show()\n\n# only 7 persons whos ticket fare is morethan 100 died","7b74c543":"sns.barplot(x = 'Survived',y = 'Fare',hue = 'Sex',data = df)\nplt.show()","88b83797":"sns.barplot(x = 'Survived',y = 'Parch',data = df)\nplt.show()\n\n#higher the parch higher survival rate","83932afa":"sns.barplot(x = 'Survived',y = 'Parch',hue = 'Sex',data = df)\nplt.show()","915f2180":"sns.barplot(x = 'Survived',y = 'Pclass',data = df)\nplt.show()\n\n# lesser the pclass more survival","0ac4623f":"sns.barplot(x = 'Survived',y = 'Pclass',hue = 'Sex',data = df)\nplt.show()","c7d5499d":"plt.figure(figsize=(6,8))\nx = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.histplot(x[i],kde = True)\n    plt.show()","29252fa0":"plt.figure(figsize=(16,9))\nax = sns.heatmap(df.corr(),annot = True, cmap = 'viridis')\nplt.show()","2df69d60":"plt.figure(figsize=(6,8))\nx = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.scatterplot(x = 'Survived',y = i,data = x,color = 'Red')\n    plt.show()","fde472d8":"plt.figure(figsize=(6,8))\nx = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.scatterplot(x = 'Age',y = i,data = x,color = 'Red')\n    plt.show()","ece68554":"plt.figure(figsize=(6,8))\nx = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.scatterplot(x = 'Fare',y = i,data = x,color = 'Red')\n    plt.show()","fe318867":"sns.pairplot(df)","594ce68c":"x = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.boxplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","ad442056":"x = df.drop(['Sex','Embarked'],axis = 1)\nfor i in x.columns:\n    sns.violinplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","763e95db":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>')","af369e76":"df_nunique = {var: pd.DataFrame(df[var].value_counts()) \n              for var in {'Survived', 'Pclass', 'Sex', \n                          'SibSp', 'Parch', 'Embarked'}}\nmulti_table([df_nunique['Survived'], df_nunique['Pclass'], \n             df_nunique['Sex'], df_nunique['SibSp'], \n             df_nunique['Parch'], df_nunique['Embarked']])\n\n# Now we can see how mny unique values in each column","6148e797":"train_no_NA = df.dropna()\n\ntrain_cat_visual_0 = train_no_NA[['Sex', 'SibSp', 'Parch', 'Embarked', \n                                  'Pclass', 'Survived']].columns.tolist()","4e662bcb":"sns.set_theme(rc = {'figure.dpi': 250, 'axes.labelsize': 7, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.55)\n\nfig, ax = plt.subplots(3, 2, figsize = (6.5, 7.5))\n\nfor indx, (column, axes) in list(enumerate(list(zip(train_cat_visual_0, ax.flatten())))):\n    \n    sns.violinplot(ax = axes, x = train_no_NA[column], \n                   y = train_no_NA['Age'],\n                   scale = 'width', linewidth = 0.5, \n                   palette = 'crest', inner = None)\n    \n    plt.setp(axes.collections, alpha = 0.3)\n    \n    sns.stripplot(ax = axes, x = train_no_NA[column], \n                  y = train_no_NA['Age'],\n                  palette = 'crest', alpha = 0.9, \n                  s = 1.5, jitter = 0.07)\n    sns.pointplot(ax = axes, x = train_no_NA[column],\n                  y = train_no_NA['Age'],\n                  color = '#ff5736', scale = 0.25,\n                  estimator = np.mean, ci = 'sd',\n                  errwidth = 0.5, capsize = 0.15, join = True)\n    \n    plt.setp(axes.lines, zorder = 100)\n    plt.setp(axes.collections, zorder = 100)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \nplt.tight_layout()\nplt.show()","edd6bc57":"sns.set_theme(rc = {'figure.dpi': 100, 'axes.labelsize': 10, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.8)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nsns.barplot(data = df, x = 'Sex', y = 'Age', hue = 'Pclass', \n            palette = 'crest', alpha = 0.8);\n\nplt.show()\n","85abcdef":"train_cat_visual_1 = df.select_dtypes(\n                     include = ['object', 'category']).columns.tolist()\n\ntrain_cat_visual_1.append('Pclass')","0202f978":"sns.set_theme(rc = {'figure.dpi': 250, 'axes.labelsize': 7, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.55)\n\nfig, ax = plt.subplots(3, 2, figsize = (6.5, 7.5))\n\nfor indx, (column, axes) in list(enumerate(list(zip(train_cat_visual_1, \n                                                    ax.flatten())))):\n    \n    sns.countplot(ax = axes, x = df[column], hue = df['Survived'], \n                  palette = 'crest', alpha = 0.8)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \naxes_legend = ax.flatten()\n\naxes_legend[1].legend(title = 'Survived', loc = 'upper right')\naxes_legend[2].legend(title = 'Survived', loc = 'upper right')\n\nplt.tight_layout()\nplt.show()","7722bf10":"df_groupby = {var: pd.DataFrame(df.groupby([var, 'Survived']).size()) \n              for var in {'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'}}\nmulti_table([df_groupby['Pclass'], df_groupby['Sex'], df_groupby['SibSp'], \n             df_groupby['Parch'], df_groupby['Embarked']])","5e06992a":"train_num_visual_0 = df.select_dtypes(include = ['float64']).columns.tolist()","f668cfea":"sns.set_theme(rc = {'figure.dpi': 120, 'axes.labelsize': 8, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.65)\n\nfig, ax = plt.subplots(2, 1, figsize = (7, 6))\n\nfor indx, (column, axes) in list(enumerate(list(zip(train_num_visual_0, ax.flatten())))):\n    \n    sns.scatterplot(ax = axes, y = df[column].index, x = df[column], \n                    hue = df['Survived'], palette = 'crest', alpha = 0.8)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \nplt.tight_layout()\nplt.show()","5d7c9a7c":"sns.set_theme(rc = {'figure.dpi': 120, 'axes.labelsize': 8, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.65)\n\nfig, ax = plt.subplots(2, 1, figsize = (6, 6))\n\nfor indx, (column, axes) in list(enumerate(list(zip(train_num_visual_0, ax.flatten())))):\n    \n    sns.histplot(ax = axes, x = df[column], hue = df['Survived'], \n                 palette = 'crest', alpha = 0.8, multiple = 'stack')\n    \n    legend = axes.get_legend() # sns.hisplot has some issues with legend\n    handles = legend.legendHandles\n    legend.remove()\n    axes.legend(handles, ['0', '1'], title = 'Survived', loc = 'upper right')\n    Quantiles = np.quantile(df[column], [0, 0.25, 0.50, 0.75, 1])\n    \n    for q in Quantiles: axes.axvline(x = q, linewidth = 0.5, color = 'r')\n        \nplt.tight_layout()\nplt.show()","ba218302":"# Column with null values and their count\nc = 0\nis_null = []\nfor i in df.columns:\n    if df[i].isnull().sum()>0:\n        is_null.append(i)\n        print(i,df[i].isnull().sum())\n        c = c+1\nprint('Number of columns containing null values are:',c)","a94bcb95":"x = df.drop(['Sex','Embarked'],axis = 1)\ndef count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","474bc485":"# If your null value column contain outliers then fill them with medain or else mean\n# If your null value column is categorical then fill them with mode\nfor i in is_null:\n    if df[i].dtype == 'int64' or df[i].dtype == 'float64':\n        if i in a:\n                df[i].fillna(df[i].median(),inplace=True)\n        else:\n            df[i].fillna(df[i].mean(),inplace=True)\n    elif df[i].dtype == 'object':\n        df[i].fillna(df[i].mode()[0],inplace=True)","71fa7be8":"df.isnull().sum()# no null values","281ac2ad":"df1=pd.get_dummies(data=df,columns=['Sex','Embarked'],drop_first=True)\ndf1","f07c1ed5":"df2 = pd.read_csv('..\/input\/titanic\/test.csv')\ndf2.drop(['Name','Ticket','Cabin'],axis = 1,inplace = True)\ndf2","3f376a67":"df2.isnull().sum()","abc6083a":"# Column with null values and their count\nd = 0\nis_null1 = []\nfor i in df2.columns:\n    if df2[i].isnull().sum()>0:\n        is_null1.append(i)\n        print(i,df2[i].isnull().sum())\n        d = d+1\nprint('Number of columns containing null values are:',d)","097ca990":"y = df2.drop(['Sex','Embarked'],axis = 1)\ndef count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            m.append(i)\n            print('Count of outliers are:',x+y)\nglobal m\nm = []\nfor i in y.columns:\n    count_outliers(df2,i)","cea93992":"for i in is_null1:\n    if df2[i].dtype == 'int64' or df2[i].dtype == 'float64':\n        if i in m:\n                df2[i].fillna(df2[i].median(),inplace=True)\n        else:\n            df2[i].fillna(df2[i].mean(),inplace=True)\n    elif df2[i].dtype == 'object':\n        df2[i].fillna(df2[i].mode()[0],inplace=True)","7bd73f5a":"df2.isnull().sum()","d2cfd8fc":"df2","120331cb":"df3=pd.get_dummies(data=df2,columns=['Sex','Embarked'],drop_first=True)\ndf3","f06b56dc":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix\nscaler = StandardScaler()\nscaler.fit(df1.drop(['Survived'],axis = 1))\nscaled_features = scaler.transform(df1.drop('Survived',axis = 1))","38578062":"X = scaled_features\nY = df1['Survived']\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=44)","ae6a40d6":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train,Y_train)","366229ad":"pred = knn.predict(X_test)\npred","6bd045cc":"print(confusion_matrix(Y_test,pred))","7628d0e2":"print(classification_report(Y_test,pred))","fa23eb94":"error_rate= []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,Y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != Y_test))","e98b30ae":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","66997f5b":"print(metrics.accuracy_score(Y_test, pred))\nknn.score(X_train,Y_train)","f6302faa":"pred1 = knn.predict(df3)\npred1","d3c82624":"f = []\nfor i in df3.PassengerId:\n    f.append(i)\n# list of passengerId","71473919":"p = pd.DataFrame(data = [pred1])\nd = p.transpose()\nd.insert(loc = 0, column = 'PassengerId', value=f, allow_duplicates = False)","7d0fcfa9":"d.rename(columns = ({ 0: 'test_prediction'}))","fe59dc30":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,Y_train)","abbd0b6e":"predR = logmodel.predict(X_test)","03964386":"print(classification_report(Y_test,pred))\nprint(confusion_matrix(Y_test,pred))","a38fd4e2":"#logmodel.predict_proba(X_test,pred)\nlogmodel.score(X_test,Y_test)","1f451209":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, Y_train)\ny_pred = dtc.predict(X_test)","042db101":"confusion_matrix(Y_test, y_pred)","58dfae78":"print(classification_report(Y_test, y_pred))","4d9e10f8":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(Y_test, y_pred))","0882af0c":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, Y_train)\ny_pred = svc.predict(X_test)","64ec64ed":"confusion_matrix(Y_test, y_pred)","ca3c3e0f":"print(classification_report(Y_test, y_pred))","e6aaf467":"print(accuracy_score(Y_test, y_pred))","4c7cf51d":"from sklearn.model_selection import GridSearchCV\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, Y_train)","0bcf8a8e":"dtc = grid_search.best_estimator_\ny_pred = dtc.predict(X_test)","386c0242":"print(accuracy_score(Y_test, y_pred))","0b4e78dc":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","ac4f4af6":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 10, verbose = 1)\ngrid_search.fit(X_train, Y_train)","65863121":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","01d71b09":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME.R', learning_rate = 0.001, n_estimators = 200)\nada.fit(X_train, Y_train)","da1c61d3":"confusion_matrix(Y_test, y_pred)","9b51d15b":"print(accuracy_score(Y_test, y_pred))","024f93d3":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search = GridSearchCV(gb, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, Y_train)","1ead40a0":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","87ac246a":"gb = GradientBoostingClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 180)\ngb.fit(X_train,Y_train)\ny_pred = gb.predict(X_test) ","d8a72397":"print(accuracy_score(Y_test, y_pred))","ce891e03":"sgbc = GradientBoostingClassifier(learning_rate = 0.1, subsample = 0.9, max_features = 0.75, loss = 'deviance',\n                                  n_estimators = 100)\nsgbc.fit(X_train, Y_train)\ny_pred = sgbc.predict(X_test)","490ec46a":"print(accuracy_score(Y_test, y_pred))","b690018c":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(iterations = 180, learning_rate = 0.1)\ncat.fit(X_train, Y_train)\ny_pred = cat.predict(X_test)","1992ac4d":"print(accuracy_score(Y_test, y_pred))","888ff3a8":"from xgboost import XGBClassifier\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, n_estimators = 10)\nxgb.fit(X_train, Y_train)\ny_pred = xgb.predict(X_test)","c9ec91f5":"print(accuracy_score(Y_test, y_pred))","f5f65218":"## Encoding","853dd3a6":"# Total EDA of train dataset in one code","56e9f3a5":"## Replacing null values with mean median or mode","4cdf2266":"# Prediction on test data using KNN","1271c493":"# Hyper parameter Tuning","7b741dea":"### name,ticket columns are useless \n### cabin contains many null values","d727e529":"# Importing Libraries","798248ce":"# Advanced Data Visualisation","0bcf2e03":"# Prediction using support vector classifier","761f6fd1":"# Cat boost Classifier","b487c294":"# Loading Train DataSet","8729e1f0":"## grouped tavles for categorical values","404972f4":"# EDA of train dataset","a33a6d57":"# Gradient Boosting Classifier","aadb95f0":"# Prediction using Logistic Regression","3b39104d":"# Data visualisation","e7088a12":"# Boosting: ADA boost classifier","d2bb950d":"## Getting unique values of each category","2a53c8c5":"# Prediction using Knn","09e4ef3f":"# Stochastic Gradient Boosting (SGB)","a4d911b1":"# Feature Selection","083b1b88":"# Cleaning and Preprocessing of Test Data","f7d85aea":"# Prediction using Decisiontree","5b2ac56c":"# Data Preprocessing","47ccb48c":"# XGB Classifier","26ec5bcd":"# Data visualisation using Autoviz","e800dc1c":"## Exploring numerical variables","93b18cd7":"# Feature Scaling"}}