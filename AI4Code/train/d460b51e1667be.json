{"cell_type":{"3e14c622":"code","ad8c03e5":"code","e0b02e47":"code","12deec62":"code","6c7a235d":"code","148333c8":"code","3ab727a7":"code","79e8ba70":"code","ce770ea1":"code","e52e0958":"code","98823cfb":"code","8cfb3f71":"code","2ab67e74":"code","c0ca8db2":"code","92b2d35f":"code","3f6bb59c":"code","76ab9e7e":"code","6e55fca8":"code","f785b550":"code","d9c022cf":"code","f09248c6":"code","f6b9d658":"code","dbabb7a1":"code","45a1df94":"markdown","21973a90":"markdown","4c07c812":"markdown","4d9d919e":"markdown","ffa77894":"markdown","cc8edaa4":"markdown","bc756cb1":"markdown","db77ecbd":"markdown","79c771ba":"markdown","6d471047":"markdown","ee460eef":"markdown","81f81d9a":"markdown","c716c8a0":"markdown"},"source":{"3e14c622":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom hyperopt import tpe, hp, Trials\nfrom hyperopt.fmin import fmin\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load pretrained spacy model for lemmatization\nsp = spacy.load('en_core_web_sm')","ad8c03e5":"# Import train and test\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","e0b02e47":"train.info()","12deec62":"# Check stats on target column\nprint(train.target.mean())\nprint(train.target.std())\n\ntrain.hist(['target'], bins=20)","6c7a235d":"fig = go.Figure(data=[go.Histogram(x=train.target.values,\n                                   marker_line_width=1, \n                                   marker_line_color=\"midnightblue\", \n                                   xbins_size = 0.2)])\n\nfig.update_layout(title_text='Distribution of reading ease score')\nfig.show()","148333c8":"fig = go.Figure(data = [go.Box(y=train.target.values, name = \"Reading ease score\")])\n\nfig.update_layout(title_text='Boxplot of reading ease score')\nfig.show()","3ab727a7":"# Check missing values\ntrain.isnull().sum()","79e8ba70":"train.url_legal.value_counts()","ce770ea1":"train.license.value_counts()","e52e0958":"train.excerpt.head()","98823cfb":"# url_legal_seen = train.url_legal.values\n# for idx in range(len(test)):\n#     if test.loc[idx, \"url_legal\"] not in url_legal_seen:\n#         test.loc[idx, \"url_legal\"] = \"unknown\"\n\n# le = LabelEncoder()\n# train[\"url_legal_encoded\"] = le.fit_transform(train.url_legal.values)\n# test[\"url_legal_encoded\"] = le.transform(test.url_legal.values)","8cfb3f71":"le2 = LabelEncoder()\ntrain[\"license_encoded\"] = le2.fit_transform(train.license.values)\ntest[\"license_encoded\"] = le2.transform(test.license.values)","2ab67e74":"train.corr()","c0ca8db2":"stopwords = stopwords.words('english')\n\ndef preprocessing_excerpt(text):\n    text = text.lower()\n    text = word_tokenize(text)\n    text = [x for x in text if x not in stopwords]\n    text = \" \".join(text)\n    return str(sp(text))","92b2d35f":"train['excerpt'] = train['excerpt'].apply(preprocessing_excerpt)\ntest['excerpt'] = test['excerpt'].apply(preprocessing_excerpt)","3f6bb59c":"vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(train.excerpt.values)\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\n\ntest_vectors = vectorizer.transform(test.excerpt.values)\ntest_feature_names = vectorizer.get_feature_names()\ntest_dense = test_vectors.todense()\ntest_denselist = test_dense.tolist()","76ab9e7e":"X = pd.DataFrame(denselist)\ny = train.target.values\n\nX_test = test_denselist","6e55fca8":"# hyperopt\ndef optimize(params, x, y):\n        \n    regressor = xgb.XGBRegressor(**params,\n                                 n_estimators = 100,\n                                 tree_method='gpu_hist', gpu_id=0)\n    \n    kf = KFold(n_splits = 5)\n    \n    rmses = []\n    \n    for train_idx, test_idx in kf.split(X=x):\n\n        xtrain = x.loc[train_idx, :]\n        ytrain = y[train_idx]       \n        xtest = x.loc[test_idx, :]\n        ytest = y[test_idx]\n        \n        regressor.fit(xtrain, ytrain)\n        \n        preds = regressor.predict(xtest)\n        \n        mse = mean_squared_error(ytest, preds)\n        fold_rmse = math.sqrt(mse)\n        rmses.append(fold_rmse)\n        \n    return np.mean(rmses)","f785b550":"# seed = 42\n\n# param_space ={'eta': hp.choice('eta', np.arange(0.05, 0.31, 0.05)),\n#               'max_depth': hp.choice('max_depth', np.arange(5, 16, 1, dtype=int)),\n#               'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n#               'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n#               'subsample': hp.uniform('subsample', 0.8, 1)\n#               }\n\n# opt_f = partial(optimize,\n#                 x = X,\n#                 y = y)\n    \n# trials = Trials()\n\n    \n# hopt = fmin(fn = opt_f,\n#             space = param_space,\n#             algo = tpe.suggest,\n#             max_evals = 10,\n#             trials = trials,\n#             return_argmin=False,\n#             rstate = np.random.RandomState(seed))\n\n# print(hopt)","d9c022cf":"# f, ax = plt.subplots(1)\n# xs = [t['tid'] for t in trials.trials]\n# ys = [t['misc']['vals']['eta'] for t in trials.trials]\n# ax.set_xlim(xs[0]-10, xs[-1]+10)\n# ax.scatter(xs, ys, s=20, linewidth=0.01, alpha=0.75)\n# ax.set_title('$x$ $vs$ $t$ ', fontsize=18)\n# ax.set_xlabel('$t$', fontsize=16)\n# ax.set_ylabel('$x$', fontsize=16)","f09248c6":"hopt = {'colsample_bytree': 0.4, 'eta': 0.2, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.8820207917706627}","f6b9d658":"eta = hopt['eta']\nmd = hopt['max_depth']\ncbt = hopt['colsample_bytree']\nmcw = hopt['min_child_weight']\nss = hopt['subsample']\n\nregressor = xgb.XGBRegressor(eta = eta,\n                             max_depth = md,\n                             colsample_bytree = cbt,\n                             min_child_weight = mcw,\n                             subsample = ss,\n                             tree_method='gpu_hist', gpu_id=0)\n\nregressor.fit(pd.DataFrame(X), y)","dbabb7a1":"y_pred = regressor.predict(pd.DataFrame(X_test))\ntest_ids = test['id'].values\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'target': y_pred\n})\n\nsubmission.to_csv('submission.csv', index=False)","45a1df94":"**<font color=\"blue\" size=\"4\">Context<\/font>**\n\n> Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\n**<font color=\"blue\" size=\"4\">What is CommonLit ?<\/font>**\n\n> CommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\n**<font color=\"blue\" size=\"4\">What does this competition consists in ?<\/font>**\n\n> The purpose of this competition is to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\n**<font color=\"blue\" size=\"4\">What if it works well ?<\/font>**\n\n> you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.","21973a90":"<div align='center'><font size=\"5\" color='#353B47'>CommonLit readibility prize<\/font><\/div>\n<div align='center'><font size=\"4\" color=\"#353B47\">A first approach using XGBoost with hyperopt<\/font><\/div>\n<br>\n<hr>","4c07c812":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n----","4d9d919e":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n----","ffa77894":"# References\n\n* https:\/\/medium.com\/district-data-labs\/parameter-tuning-with-hyperopt-faa86acdfdce\n\n* Approach (almost) any Machine Learning Problem - Abishek Thakur","cc8edaa4":"<hr>\n<br>\n<div align='justify'><font color=\"#353B47\" size=\"4\">Thank you for taking the time to read this notebook. I hope that I was able to answer your questions or your curiosity and that it was quite understandable. <u>any constructive comments are welcome<\/u>. They help me progress and motivate me to share better quality content. I am above all a passionate person who tries to advance my knowledge but also that of others. If you liked it, feel free to <u>upvote and share my work.<\/u> <\/font><\/div>\n<br>\n<div align='center'><font color=\"#353B47\" size=\"3\">Thank you and may passion guide you.<\/font><\/div>","bc756cb1":"# <div id=\"chap3\">3. Preprocessing","db77ecbd":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n----","79c771ba":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n----","6d471047":"# <div id=\"summary\">Summary<\/div>\n\n**<font size=\"2\"><a href=\"#chap1\">1. Import libraries<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap2\">2. EDA<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. Preprocessing<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap4\">4. Training<\/a><\/font>**","ee460eef":"# <div id=\"chap1\">1. Import libraries","81f81d9a":"# <div id=\"chap4\">4. Training","c716c8a0":"# <div id=\"chap2\">2. EDA"}}