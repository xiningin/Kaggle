{"cell_type":{"e0322ebf":"code","5e71d538":"code","d5150baa":"code","322cbfc1":"code","33ae2273":"code","f5cfda98":"code","d75c7048":"code","e82d6937":"code","fec96c35":"code","636381a7":"code","c4330d0f":"code","9afd6d25":"code","1ce136ef":"code","7dcc656d":"code","ce826060":"code","e8d858db":"code","5441423c":"code","fd663fda":"code","df2de678":"code","4ff5cde1":"code","b8375f1e":"code","dc6708e8":"code","2758950a":"code","0fa6d9bd":"code","a8c5603f":"code","caf136fd":"code","6d9f33d5":"code","aff146d1":"code","f6b22ff9":"code","d37f3fda":"code","c2570d63":"code","51d681aa":"code","ff8ddb6b":"code","32368749":"code","11381107":"code","d29a0bb9":"code","5aafce20":"code","043825bf":"code","50adfccc":"code","0fb75a3e":"code","538e1ecf":"code","ebf6ce6c":"code","c25498ff":"markdown","de7b561f":"markdown","4834028a":"markdown","a2ad9de5":"markdown","59afd108":"markdown","b83f0c5c":"markdown","980c4bdf":"markdown","dbe98de5":"markdown","8deb08bb":"markdown"},"source":{"e0322ebf":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport copy\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\nfrom logging import getLogger, StreamHandler, FileHandler, Formatter\nfrom contextlib import contextmanager\nfrom time import time\n\ndef get_logger(name, log_level=\"DEBUG\", output_file=None, handler_level=\"INFO\", format_str=\"[%(asctime)s] %(message)s\"):\n    logger = getLogger(name)\n\n    formatter = Formatter(format_str)\n\n    # add new handler if no handlers are added\n    if not len(logger.handlers):\n        handler = StreamHandler()\n        logger.setLevel(log_level)\n        handler.setLevel(handler_level)\n\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    if output_file:\n        file_handler = FileHandler(output_file)\n        file_handler.setFormatter(formatter)\n        file_handler.setLevel(handler_level)\n        logger.addHandler(file_handler)\n\n    return logger\n\nlogger = get_logger(__name__)\n\n@contextmanager\ndef timer(name):\n    start = time()\n    yield\n    duration = time() - start\n    logger.info(f'[{name}] done in {duration:.2f} s')","5e71d538":"raw_data = fetch_california_housing()\n\nX, y = raw_data.get('data', None), raw_data.get('target', None)\nfeat_names = raw_data.get('feature_names')","d5150baa":"X.shape","322cbfc1":"all_df = pd.DataFrame(X, columns=feat_names)","33ae2273":"all_df.head()","f5cfda98":"def make_value_count_df(df_input, c):\n    \"\"\"make count encoding feature\n    \"\"\"\n    _df = all_df.groupby(c).size().reset_index()\n    _df = _df.rename(columns={ 0: f'count_{c}' })\n\n    _df = pd.merge(df_input[[c]], _df, on=c, how='left').drop(columns=[c])\n    return _df\n\n\ndef value_count_feature(df_input):\n    \"\"\"Count Encoding \u3092\u884c\u3046\u7279\u5fb4\u91cf\n    \u4e00\u65e6\u5168\u90e8\u306e\u30ab\u30e9\u30e0\u3092\u5bfe\u8c61\u306b\u3059\u308b\n    \"\"\"\n    df_out = pd.DataFrame()\n    for c in all_df.columns:\n        _df = make_value_count_df(df_input, c)\n        df_out = pd.concat([df_out, _df], axis=1)\n    df_out = df_out.fillna(0)\n    return df_out","d75c7048":"processers = [\n    value_count_feature,\n    lambda x: x\n]\n\nfeat_df = pd.concat([f(all_df) for f in processers], axis=1)","e82d6937":"y_cut = pd.qcut(y, q=10)\ny_cut.value_counts()","fec96c35":"from sklearn.model_selection import StratifiedKFold\n\ndef get_fold(n_folds=10):\n    y_cat = pd.qcut(y, q=10).codes\n    fold = StratifiedKFold(n_splits=n_folds, random_state=71)\n    return fold.split(X, y_cat)","636381a7":"from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, f1_score, mean_absolute_error, mean_squared_error, \\\n    r2_score, mean_squared_log_error, median_absolute_error, explained_variance_score\nfrom collections import OrderedDict\n\ndef regression_metrics(y_true, predict):\n    name_func_map = OrderedDict({\n        'rmse': lambda *x: mean_squared_error(*x) ** .5\n    })\n\n    for m in [mean_squared_log_error, median_absolute_error, median_absolute_error, mean_squared_error, r2_score,\n              explained_variance_score, mean_absolute_error]:\n        name_func_map[m.__name__] = m\n\n    data, idx = [], []\n\n    for name in name_func_map.keys():\n        idx.append(name)\n\n        try:\n            f = name_func_map[name]\n            p = np.copy(predict)\n\n            if f == mean_squared_log_error:\n                p[p < .0] = 0.\n\n            m = name_func_map[name](y_true, p)\n        except Exception:\n            m = None\n        data.append(m)\n\n    df_metrics = pd.DataFrame(data, index=idx, columns=['score'])\n    return df_metrics","c4330d0f":"def train_model(clf_class, params=None, n_folds=10):\n    if params is None:\n        params = {}\n    params = copy.deepcopy(params)\n    oof = np.zeros_like(y)\n    for idx_train, idx_valid in get_fold(n_folds):\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        clf = clf_class(**params)\n        clf.fit(x_train, y_train)\n        oof[idx_valid] = clf.predict(x_valid)\n        \n    return oof","9afd6d25":"from sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgbm","1ce136ef":"# Define Models. (Not set parameters)\nmodels = {\n    'Ridge': [Ridge, {}],\n    'Random Forest': [RandomForestRegressor, { 'n_jobs': -1 }],\n    'Decision Tree': [DecisionTreeRegressor, {}],\n    'lightGBM': [lgbm.LGBMRegressor, { 'n_jobs': -1 }],\n    'XGBoost': [xgb.XGBRegressor, {'n_jobs': -1 }],\n    'XGBoost GPU': [xgb.XGBRegressor, { 'device': 'gpu', 'n_jobs': -1, 'updater': 'grow_gpu_hist' }]\n}\n\npred_df = pd.DataFrame()\nresult_df = pd.DataFrame()","7dcc656d":"for name, (cls, params) in models.items():\n    start = time()\n    with timer(name):\n        oof = train_model(cls, params=params)\n    d = time() - start\n    _df = regression_metrics(y, oof)\n    _df = _df.append(pd.Series(data={ 'score': d }, name='time'))\n    result_df[name] = _df['score']\n\n    pred_df[name] = oof","ce826060":"result_df","e8d858db":"fig, ax = plt.subplots()\nax.scatter(result_df.T['time'], result_df.T['rmse'], s=100)\n\nfor i, row in result_df.T.iterrows():\n    ax.text(row['time'] + .3, row['rmse'] - .005, row.name)\n\nax.set_xlabel('Time')\nax.set_ylabel('RMSE')","5441423c":"# gut feeling and try 3 ~ 5 times param. ;)\nmy_params = {\n    'n_estimators': 10000,\n    'reg_lambda': .1,\n    'reg_alpha': .1,\n    'objective': 'poisson',\n    'colsample_bytree': .8,\n    'min_child_samples': 20,\n    'subsample': .8,\n    'learning_rate': .1,\n    'random_state': 0,\n    'max_depth': 5,\n    'max_leaves': 15\n}\n\ndef fit_lgbm(X, y, params, verbose=400):\n    \"\"\"\n    run lightGBM training and return Out of Fold Prediction\n    \"\"\"\n    models = []\n    oof_pred = np.zeros_like(y)\n    for idx_train, idx_valid in get_fold():\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgbm.LGBMRegressor(**params)\n\n        clf.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], \n                verbose=verbose, \n                eval_metric='rmse',\n                early_stopping_rounds=200)\n        pred_i = clf.predict(x_valid)\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n    return oof_pred, models","fd663fda":"with timer('lgbm_my_params'):\n    oof, _ = fit_lgbm(X, y, my_params)\n_df = regression_metrics(y, oof)\nresult_df['lgbm_me'] = _df['score']\npred_df['lgbm_me'] = oof\nprint(mean_squared_error(y, oof) ** .5)","df2de678":"result_df","4ff5cde1":"preds = []\nfor i in range(5):\n    p = copy.deepcopy(my_params)\n    p['random_state'] = i\n    oof_i, _ = fit_lgbm(X, y, p)\n    preds.append(oof_i)","b8375f1e":"preds = np.array(preds)\noof = np.mean(preds, axis=0)\n_df = regression_metrics(y, oof)\n\nresult_df['lgbm_me-SA'] = _df['score']\npred_df['lgbm_me-SA'] = oof","dc6708e8":"result_df","2758950a":"from optuna import create_study, samplers","0fa6d9bd":"def get_default_parameter_suggestions(trial):\n    \"\"\"\n    Get parameter sample for Boosting (like XGBoost, LightGBM)\n\n    Args:\n        trial(trial.Trial):\n\n    Returns:\n        dict: parameter sample generated by trial object\n    \"\"\"\n    return {\n        'n_estimators': 10000,\n        'objective': trial.suggest_categorical('objective', ['rmse', 'gamma', 'poisson', 'tweedie']),\n        # L2 \u6b63\u5247\u5316\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e3),\n        # L1 \u6b63\u5247\u5316\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e3),\n        # \u5f31\u5b66\u7fd2\u6728\u3054\u3068\u306b\u4f7f\u3046\u7279\u5fb4\u91cf\u306e\u5272\u5408\n        # 0.5 \u3060\u3068\u5168\u4f53\u306e\u3046\u3061\u534a\u5206\u306e\u7279\u5fb4\u91cf\u3092\u6700\u521d\u306b\u9078\u3093\u3067, \u305d\u306e\u7bc4\u56f2\u5185\u3067\u6728\u3092\u6210\u9577\u3055\u305b\u308b\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        # \u5b66\u7fd2\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u3046\u3061\u4f7f\u7528\u3059\u308b\u5272\u5408\n        # colsample \u3068\u306f\u53cd\u5bfe\u306b row \u65b9\u5411\u306b\u30b5\u30f3\u30d7\u30eb\u3059\u308b\n        'subsample': trial.suggest_uniform('subsample', .5, 1.),\n        # \u6728\u306e\u6700\u5927\u306e\u6df1\u3055\n        # \u305f\u3068\u3048\u3070 5 \u306e\u6642\u5404\u5f31\u5b66\u7fd2\u6728\u306e\u5404\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u30eb\u30fc\u30eb\u306f\u3001\u6700\u5927\u3067\u30825\u306b\u5236\u9650\u3055\u308c\u308b.\n        'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7, 8]),\n        # \u672b\u7aef\u30ce\u30fc\u30c9\u306b\u542b\u307e\u308c\u308b\u6700\u5c0f\u306e\u30b5\u30f3\u30d7\u30eb\u6570\n        # \u3053\u308c\u3092\u4e0b\u56de\u308b\u3088\u3046\u306a\u5206\u5272\u306f\u4f5c\u308c\u306a\u304f\u306a\u308b\u305f\u3081, \u5927\u304d\u304f\u8a2d\u5b9a\u3059\u308b\u3068\u3088\u308a\u5168\u4f53\u306e\u50be\u5411\u3067\u3057\u304b\u5206\u5272\u304c\u3067\u304d\u306a\u304f\u306a\u308b\n        # [NOTE]: \u6570\u3067\u3042\u308b\u306e\u3067\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5927\u304d\u3055\u4f9d\u5b58\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 40)\n    }","a8c5603f":"class Objective:\n    def __init__(self):\n        pass\n    \n    def __call__(self, trial):\n        params = get_default_parameter_suggestions(trial)\n        oof, _ = fit_lgbm(X, y, params)\n        return mean_squared_error(y, oof) ** .5","caf136fd":"study = create_study(sampler=samplers.RandomSampler(seed=71))\nobjective_func = Objective()","6d9f33d5":"start = time()\nwith timer('optuna LightGBM'):\n    study.optimize(objective_func, n_trials=100, n_jobs=-1)\nduration = time() - start","aff146d1":"study.trials_dataframe().sort_values('value').head()","f6b22ff9":"params = study.best_params\nparams['n_estimators'] = 20000\noof, _ = fit_lgbm(X, y, params)\n_df = regression_metrics(y, oof)\n_df = _df.append(pd.Series(data={ 'score': duration }, name='time'))\nresult_df['lgbm_optuna'] = _df\npred_df['lgbm_optuna'] = oof","d37f3fda":"result_df","c2570d63":"n_plots = 1000\nidx = np.random.permutation(range(len(y)))[:n_plots]\nfor name, row in pred_df.T.iterrows():\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.scatter(row[idx], y[idx], alpha=.4)\n    ax.set_xlim(ax.set_ylim(0, 5.1))\n    ax.set_title(name)\n    ax.set_xlabel('Predict')\n    ax.set_ylabel('Target')\n    fig.tight_layout()\n    fig.savefig(f'{name}_vs_target.png', dpi=150)","51d681aa":"result_df.T.sort_values('rmse')[['rmse']]","ff8ddb6b":"_df = result_df.T.rename_axis('name', axis=0)\n_df = _df.reset_index()\n_df = _df.sort_values('rmse', ascending=False).reset_index(drop=True)","32368749":"_df.to_csv('result.csv', index=False)","11381107":"_df.round(4)","d29a0bb9":"fig, ax = plt.subplots(figsize=(6, 4))\nsns.barplot(data=_df, x='rmse', y='name', orient='h', palette='viridis', ax=ax)\nax.set_xlim(.5, .8)\nfig.tight_layout()\nfig.savefig('optuna_vs_mine.png', dpi=120)","5aafce20":"def rf_objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n        'criterion': trial.suggest_categorical('criterion', ['mae', 'mse']),\n        'max_depth': trial.suggest_int('max_depth', 4, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 20),\n        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n        'random_state': 0\n    }\n    \n    oof = train_model(RandomForestRegressor, params, n_folds=4)\n    return mean_squared_error(y, oof) ** .5","043825bf":"start = time()\nwith timer('optimize random forest'):\n    rf_study = create_study()\n    rf_study.optimize(rf_objective, n_trials=100, n_jobs=-1)\nduration = time() - start","50adfccc":"rf_study.trials_dataframe().sort_values('value').head()","0fb75a3e":"rf_study.best_params","538e1ecf":"p = rf_study.best_params\noof = train_model(RandomForestRegressor, params=p)\n\n_df = regression_metrics(y, oof)\n_df = _df.append(pd.Series(data={ 'score': duration }, name='time'))\nresult_df['Random Forest Optuna'] = _df\npred_df['Random Forest Optuna'] = oof","ebf6ce6c":"result_df.to_csv('result.csv')\npred_df.to_csv('predictions.csv', index=False)","c25498ff":"### Seed Averaging\n\ntrain some models only change seed number and average these outputs.  \nUsualy, it improve model peformance (but more computational complexity, more time).\n\nTry it.","de7b561f":"## Appendix\n\nRandom Forest \u3067\u3082 Tuning \u3059\u308b\u3068\u3069\u3046\u306a\u306e\u304b","4834028a":"### Evaluation\n\nShow result model scores!!","a2ad9de5":"### Parameter Search\n\nTo find good hyper parameters, use optuna!","59afd108":"## Dataset\n\nuse `California Housing` dataset@scikit-learn.\n\n### Property\n\n* n_features: `8`\n* target: house price. normalized between 0 and 5.","b83f0c5c":"## Visualization\n\nvisualize predict vs target distribution by scatter plot.","980c4bdf":"### Feature Engineering\n\na little feature engineering (Count Encoding)\n\nn_fetures: 8 -> 16","dbe98de5":"### Validation Set\n\nTo evaluate models, use K-Fold.  \nWhen make folds, for balance the target distribution, we use Stratified-K-Fold by quontile-cutted target categries.","8deb08bb":"# Gradient Boosting VS Other Methods\n\nThe porpose of the notebook is to evaluate the performance between evaluate Gradient Boosted Decision Tree and the other methods like Ridge\/Random Forest.\n\n## Use Models\n\n* Decision Tree\n* Ridge Regression\n* Random Forest\n* Gradient Boosted Decision Tree\n  * XGBoost\n  * LightGBM"}}