{"cell_type":{"3fae36d1":"code","ddcc70c7":"code","21183582":"code","cf0d035b":"code","5aaa14e5":"code","45bc07b9":"code","686cc609":"code","18f793e4":"code","c3cc14e6":"code","5b3df590":"code","04e2d2b3":"code","7659d0ce":"code","bab5fa9b":"code","d7999555":"code","3ef86cdb":"code","194f93e2":"code","08b61610":"code","03594e5d":"code","d04042f2":"code","7ac92e64":"code","c9222c8c":"code","0825c47a":"code","cb030107":"code","fc542576":"code","1ac8685a":"code","f75b3f85":"code","64734f40":"code","831502c1":"code","e2a79c59":"code","eeda2f55":"code","e87fef8b":"code","fedb4a3c":"code","41e7debf":"code","793349eb":"code","f71f726e":"code","4de42dde":"code","01b8c749":"code","666fdc69":"code","77d6cc81":"code","629570e0":"code","2d22e39b":"code","8adc21f3":"code","d677f1d0":"code","8a89ec89":"code","c970d03d":"code","d9dcdfe4":"code","28853e98":"code","e32ffa34":"code","c9642fa2":"code","9aa8a70e":"code","5f09bd6a":"code","47c852a3":"code","9d530744":"code","52946436":"code","d3d359ff":"code","01072e1c":"code","0b2bec7d":"code","c1e5ba68":"code","d3cfc7d9":"code","6bb66d0a":"code","317f068e":"code","c9d749d0":"code","4f6d0bf2":"code","c189e7f4":"code","3c083458":"code","887d6d82":"code","8dd0400c":"code","113fa1a8":"code","a053224a":"code","a03c22ef":"code","9dd6a9d8":"code","1b7bb29f":"code","c2edc090":"code","8f3dbd80":"code","af196933":"code","15f397e6":"code","be838c70":"code","4f06102d":"code","fbf02710":"code","63467b99":"code","828a5797":"code","2064178d":"code","b097c2a3":"code","db822881":"code","6594f551":"code","5f17addb":"code","77a525af":"code","76fa2ae3":"code","1191eac8":"code","b29aa418":"code","1a44b09a":"code","270b6968":"code","34d9d3bc":"code","edcf18c8":"code","4e3135ff":"markdown","56010e9e":"markdown","2fc25b5e":"markdown","38a28238":"markdown","486046f7":"markdown","5757276d":"markdown","500d3f34":"markdown","486266ac":"markdown","9fcb398a":"markdown","8607100b":"markdown","251a1b19":"markdown","b2ec952d":"markdown","f8e83a13":"markdown","aad7dcf8":"markdown","c153c826":"markdown","a6748c96":"markdown","21a2c529":"markdown","9b6109b7":"markdown","fc585290":"markdown","fb60f762":"markdown","6db0730a":"markdown","d71fe744":"markdown","b22aa8d1":"markdown","3be73ea3":"markdown","1cee4635":"markdown","21afb293":"markdown","b7f537ce":"markdown","10e13e63":"markdown","b58eceb2":"markdown","61c4024a":"markdown","f0b17a3b":"markdown","0452bd57":"markdown","43e7eae0":"markdown","43679939":"markdown"},"source":{"3fae36d1":"import re\nimport nltk\nimport yaml\nfrom collections import Counter \nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nfrom typing import List\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, plot_confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom numpy import hstack\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import model_selection\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as xp\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom catboost import CatBoostClassifier","ddcc70c7":"#DEFINE CONSTANTS\nSHOW_DATA = 15                # how many data will be show\nPERCISION = 5                # how many digits will be show: 2.123352135\nSEED = 42\n# def safe_drop_column(df:pd.DataFrame, column_name: str):\n#     try:\n#         df = df.drop([column_name], axis=1, inplace=True)\n#     except KeyError:\n#         pass\n#     return df\n\ndef cm_to_inch(value):\n    return value\/2.54\n\ndef nan_elements(df:pd.DataFrame):\n    \"info about NaN elements, in every column (in percentage)\"\n    return df.isna().sum()\/df.shape[0]*100\n\nSTOP_WORDS = nltk.corpus.stopwords.words()\nplt.rcParams[\"figure.figsize\"]=(cm_to_inch(15),cm_to_inch(10))\nDROP = False\npalette = [\"#ff0000\",\"#853939\",\"#4f0202\"]\ncolor_palette = sns.color_palette(palette)\ncolor_palette.as_hex()","21183582":"#load train and test data\n# csv_train = \".\/train.csv\"\n# csv_test = \".\/test.csv\"\n# #load train and test data\ncsv_train = \"..\/input\/mymusicalprefrences\/train.csv\"\ncsv_test = \"..\/input\/mymusicalprefrences\/test.csv\"\ntrain_data = pd.read_csv(csv_train)\ntest_data = pd.read_csv(csv_test)\n# df = pd.concat([train_data,test_data],sort = False, axis = 0\ndescription = yaml.load(open(f\"..\/input\/mymusicalprefrences\/Description.yaml\",'r'),Loader=yaml.FullLoader)\ndf = pd.concat([train_data,test_data]).reset_index(drop=True)\ndf.head()","cf0d035b":"display(df.info())\ndisplay(df.shape)","5aaa14e5":"msno.matrix(df,color=color_palette[-1])","45bc07b9":"#DEFINE CONSTANTS 2\nCOL_ID = \"Id\"                 \nCOL_CATEGORY = \"Category\"          \nCOL_ARTIST = \"Artists\"         # cat  \nCOL_TRACK = \"Track\"               \nCOL_VERSION = \"Version\"            \nCOL_DURATION = \"Duration\"            \nCOL_ARTISTS_GENRES = \"Artists_Genres\"     \nCOL_ALBUM = \"Album\"              \nCOL_RELEASE_YEAR = \"Release_year\"       \nCOL_ALBUM_TYPE = \"Album_type\"        \nCOL_LABELS = \"Labels\"             \nCOL_KEY = \"Key\"                 \nCOL_BPM = \"BPM\"                \nCOL_VOCAL = \"Vocal \"              \nCOL_COUNTRY = \"Country\"            \nCOL_ENERGY = \"Energy\"             \nCOL_DANCEBILITY = \"Dancebility\"        \nCOL_HAPPINESS = \"Happiness\"          ","686cc609":"BLACK_LIST = []","18f793e4":"df[COL_CATEGORY] = df[COL_CATEGORY].fillna(\"None\").replace({0:\"Dislike\",1:\"Like\"})\n","c3cc14e6":"categorical_features = {COL_ARTIST,COL_TRACK,COL_VERSION,COL_ARTISTS_GENRES,COL_ALBUM,COL_ALBUM_TYPE,COL_LABELS,COL_VOCAL,COL_COUNTRY,COL_KEY}\nnumerical_features = {COL_DURATION,COL_RELEASE_YEAR,COL_BPM,COL_ENERGY,COL_DANCEBILITY,COL_HAPPINESS}\ndisplay(df[categorical_features].head())\ndisplay(df[numerical_features].head())\ndisplay(df[numerical_features].describe())","5b3df590":"def split_words(df: pd.DataFrame,col:str):\n    data = []\n    for i in df.index:\n        data.extend(df.loc[i,col].split(\"|\"))\n    if \"\" in data:\n        data.remove(\"\")\n    return data\n\n\ndef split_to_onehot(df, col):\n    \"\"\"\n    This method converts features separated by '|' into one-hot vectors.\n    Additionally it drops unnecessary values, which present only in \n    test set \/ train set or have only one value.\n    \"\"\"\n    # Getting all unique ganres values.\n    unique = list(set(split_words(df, col)))\n    \n    \n    # Putting values into binary form \n    onehot = df.loc[:,[\"Category\"]]\n    onehot[unique] = np.zeros((len(unique),), dtype = np.int8)\n    for i in df.index:\n        g = set(df.loc[i,col].split(\"|\"))\n        for j in g:\n            if j!=\"\":\n                onehot.loc[i,j] = 1\n                \n    # Dropping unnecessary values            \n    _a = onehot.groupby(\"Category\").sum()\n    only_one = list(_a.sum()[_a.sum()==1].index)\n    only_train = list(_a.loc[\"None\"][_a.loc[\"None\"]==0].index)\n    only_test = list(_a.loc[[\"Like\",'Dislike']].sum()[_a.loc[[\"Like\",'Dislike']].sum()==0].index)\n    _a = set(only_one + only_train + only_test)\n    onehot = onehot.drop(_a, axis=1)\n    \n    return onehot\n\ndef onehot_to_tsne2(df, title):\n    \"\"\"\n    This method converts one-hot representation into two tsne values.\n    Such operation is needed to shrink the dimentionality of the dataset\n    \"\"\"\n    onehot = df.drop(\"Category\",axis=1)\n    embedding = TSNE(n_components=2, init=\"pca\")\n    embedded = embedding.fit_transform(onehot)\n    embedded = pd.DataFrame(embedded,columns=[f\"{title}_tsne_1\",f\"{title}_tsne_2\"])\n    return embedded\n\ndef plot_commulative_onehot(onehot):\n    \"\"\"\n    Method of plotting commulative values of the one hot feature representation\n    \"\"\"\n    _df = onehot.groupby(\"Category\").sum()\n    fig = go.Figure()\n    for i in range(len(_df.index)):\n        k = _df.index[i]\n        x,y=[],[]\n        for g in _df.columns:\n            if _df.loc[k,g]!=0:\n                x.append(g)\n                y.append(_df.loc[k,g])\n        fig.add_trace(go.Bar(x=x, y=y,name=k,marker=dict(color=color_palette[i])))\n    fig.show()","04e2d2b3":"sns.pairplot(df[list(numerical_features)+[COL_CATEGORY]],palette=color_palette[-3:], hue=COL_CATEGORY)","7659d0ce":"display(description[COL_ARTIST])\ndf[COL_ARTIST] = df[COL_ARTIST].fillna(value = \"NoName\")\ndf[COL_ARTIST] = df[COL_ARTIST].map(str)\ndf[COL_ARTIST].value_counts()\n","bab5fa9b":"artist_onehot = split_to_onehot(df, COL_ARTIST)\ncounter_artist = Counter(split_words(df, COL_ARTIST))\n#     most_common = [word for word,_ in counter_words.most_common(100)]\n#     most_common\n# counter_words.most_common(50)[:5]\n# print(\"Summary: \", sum([y for x,y in counter_artist.items()]))","d7999555":"artist_embedded = onehot_to_tsne2(artist_onehot, \"Artist\")\n_df = artist_embedded.copy(deep=True)\n_df[[COL_CATEGORY,COL_ARTIST]] = df[[COL_CATEGORY,COL_ARTIST]]\nxp.scatter(_df,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\",color=COL_CATEGORY, hover_data=[COL_ARTIST], color_discrete_sequence = palette, height=500)","3ef86cdb":"le = preprocessing.LabelEncoder()\nle.fit(df[COL_ARTIST])\ndf[COL_ARTIST+\"numeric\"] = le.transform(df[COL_ARTIST])","194f93e2":"df = pd.concat([df,artist_embedded[[\"Artist_tsne_1\",\"Artist_tsne_2\"]]], axis=1)\n# if DROP:\n#     df = df.drop(COL_ARTIST, axis=1)\nBLACK_LIST.append(COL_ARTIST)","08b61610":"display(description[COL_TRACK])\ndf[COL_TRACK] = df[COL_TRACK].fillna(value = \"NoLabel\")\ndf[COL_TRACK] = df[COL_TRACK].map(str)\ndf[COL_TRACK].value_counts()","03594e5d":"track_onehot = split_to_onehot(df, COL_TRACK)\nplot_commulative_onehot(track_onehot)","d04042f2":"track_embedded = onehot_to_tsne2(track_onehot, \"Track\")\n_df = track_embedded.copy(deep=True)\n_df[[COL_CATEGORY,COL_TRACK]] = df[[COL_CATEGORY,COL_TRACK]]\nxp.scatter(_df,x=\"Track_tsne_1\",y=\"Track_tsne_2\",color=COL_CATEGORY,color_discrete_sequence = palette, hover_data=[COL_TRACK], height=500)","7ac92e64":"df[COL_TRACK+\"s\"] = df[COL_TRACK].map(lambda words: [ word.lower() for word in re.findall(r'[^\\W\\d]+', words)] )\ncounter_words = Counter(df[COL_TRACK+\"s\"].sum())\nmost_common = [word for word,_ in counter_words.most_common(100)]\ndf[COL_TRACK+\"s\"] = df[COL_TRACK+\"s\"].map(lambda words: int(any([word in most_common for word in words])))\ndf[COL_TRACK+\"s\"].value_counts()","c9222c8c":"le = preprocessing.LabelEncoder()\nle.fit(df[COL_TRACK])\ndf[COL_TRACK+\"numeric\"] = le.transform(df[COL_TRACK])\n","0825c47a":"df = pd.concat([df,track_embedded], axis=1)\nBLACK_LIST.append(COL_TRACK)\nif DROP:\n    df = df.drop(COL_TRACK, axis=1)","cb030107":"display(description[COL_VERSION])\ndf[COL_VERSION] = df[COL_VERSION].fillna(value = \"Soundtrack\")\ndf[COL_VERSION] = df[COL_VERSION].map(str)\ndf[COL_VERSION] = df[COL_VERSION].map(lambda name: \"Other\" if name not in [\"Soundtrack\"] else name)\ndf[COL_VERSION].value_counts()\nxp.bar(df, x=COL_VERSION, y=COL_TRACK, color=COL_CATEGORY, height=500, color_discrete_sequence=palette)\ndf[COL_VERSION] = df[COL_VERSION].map(lambda name: 0 if name not in [\"Soundtrack\"] else 1)\nBLACK_LIST.append(COL_VERSION)","fc542576":"display(description[\"Artists Genres\"])\ndf[COL_ARTISTS_GENRES] = df[COL_ARTISTS_GENRES].fillna(value = \"NoGenre\")\ndf[COL_ARTISTS_GENRES] = df[COL_ARTISTS_GENRES].map(str)\ndf[COL_ARTISTS_GENRES].value_counts()","1ac8685a":"genre_onehot = split_to_onehot(df, COL_ARTISTS_GENRES)\nplot_commulative_onehot(genre_onehot)","f75b3f85":"genres_embedded = onehot_to_tsne2(genre_onehot, \"Genres\")\n_df = genres_embedded.copy(deep=True)\n_df[[COL_CATEGORY,COL_ARTISTS_GENRES]] = df[[COL_CATEGORY,COL_ARTISTS_GENRES]]\nxp.scatter(_df,x=\"Genres_tsne_1\",y=\"Genres_tsne_2\",color=COL_CATEGORY, hover_data=[COL_ARTISTS_GENRES], height=500)\n","64734f40":"xp.scatter(df, x=COL_KEY, y=COL_ARTISTS_GENRES,color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","831502c1":"xp.scatter_3d(df, z=COL_KEY,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\" ,color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","e2a79c59":"xp.scatter_3d(df, z=COL_BPM,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\" ,color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","eeda2f55":"xp.scatter_3d(df, z=COL_RELEASE_YEAR,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\",color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","e87fef8b":"xp.scatter_3d(df, z=COL_DANCEBILITY,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\" ,color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","fedb4a3c":"xp.scatter_3d(df, z=COL_HAPPINESS,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\",color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","41e7debf":"xp.scatter_3d(df, z=COL_ENERGY,x=\"Artist_tsne_1\",y=\"Artist_tsne_2\" ,color=COL_CATEGORY, height=500, color_discrete_sequence=palette)","793349eb":"# Col_Genre_mostly = COL_ARTISTS_GENRES+\"Mostly\"\n# count_genre = 20\n# counter_genres = Counter(df[COL_ARTISTS_GENRES].sum())\n# most_common_genres = [genre for genre,_ in counter_genres.most_common(count_genre)]\n# df[Col_Genre_mostly] = df[COL_ARTISTS_GENRES].map(lambda genres: any([genre in most_common_genres for genre in genres]))\n# df[Col_Genre_mostly] = df[COL_ARTISTS_GENRES].astype(np.int64)","f71f726e":"df = pd.concat([df,genres_embedded], axis=1)\n\nif DROP:\n    df = df.drop(COL_ARTISTS_GENRES, axis=1)\n# BLACK_LIST.append(COL_ARTISTS_GENRES)","4de42dde":"display(description[COL_ALBUM])\ndf[COL_ALBUM] = df[COL_ALBUM].fillna(value = \"Single\")\ndf[COL_ALBUM] = df[COL_ALBUM].map(str)\ndf[COL_ALBUM].value_counts()","01b8c749":"album_onehot = split_to_onehot(df, COL_ALBUM)\nplot_commulative_onehot(album_onehot)","666fdc69":"album_embedded = onehot_to_tsne2(album_onehot, \"Album\")\n_df = album_embedded.copy(deep=True)\n_df[[COL_CATEGORY,COL_ALBUM]] = df[[COL_CATEGORY,COL_ALBUM]]\nxp.scatter(_df,x=\"Album_tsne_1\",y=\"Album_tsne_2\",color=COL_CATEGORY, hover_data=[COL_ALBUM], height=500)\n# ","77d6cc81":"le = preprocessing.LabelEncoder()\nle.fit(df[COL_ALBUM])\ndf[COL_ALBUM+\"numeric\"] = le.transform(df[COL_ALBUM])\ndf = pd.concat([df,album_embedded], axis=1)\nBLACK_LIST.append(COL_ALBUM)\n","629570e0":"display(description[COL_ALBUM_TYPE])\ndf[COL_ALBUM_TYPE] = df[COL_ALBUM_TYPE].fillna(value = \"unknown\")\ndf[COL_ALBUM_TYPE] = df[COL_ALBUM_TYPE].map(str)\ndf[COL_ALBUM_TYPE].value_counts()\nBLACK_LIST.append(COL_ALBUM_TYPE)","2d22e39b":"display(description[COL_LABELS])\ndf[COL_LABELS] = df[COL_LABELS].fillna(value = \"No Name\")\ndf[COL_LABELS] = df[COL_LABELS].map(str)\ndf[COL_LABELS] = df[COL_LABELS].map(lambda x : x.replace('\/', '|'))\ndf[COL_LABELS].value_counts()","8adc21f3":"labels_onehot = split_to_onehot(df, COL_LABELS)\nplot_commulative_onehot(labels_onehot)","d677f1d0":"labels_embedded = onehot_to_tsne2(labels_onehot, \"Labels\")\n_df = labels_embedded.copy(deep=True)\n_df[[COL_CATEGORY,COL_TRACK]] = df[[COL_CATEGORY,COL_TRACK]]\nxp.scatter(_df,x=\"Labels_tsne_1\",y=\"Labels_tsne_2\",color=COL_CATEGORY, hover_data=[COL_TRACK], height=500)","8a89ec89":"count = 40\ncounter_labels = Counter(df[COL_LABELS].sum())\nmost_common_labels = [label for label,_ in counter_labels.most_common(count)]\ndf[COL_LABELS] = df[COL_LABELS].map(lambda labels: any([label in most_common_labels for label in labels]))\ndf[COL_LABELS] = df[COL_LABELS].astype(np.int64)","c970d03d":"df = pd.concat([df,labels_embedded[[\"Labels_tsne_1\",\"Labels_tsne_2\"]]], axis=1)\nBLACK_LIST.append(COL_LABELS)\nif DROP:\n    df = df.drop(COL_LABELS, axis=1)","d9dcdfe4":"display(description[COL_KEY])\ndf[COL_KEY] = df[COL_KEY].map(str)\ndf[COL_KEY].value_counts()\nxp.bar(df,x=COL_KEY, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","28853e98":"COL_MOOD = \"Key_Mood\"\ndf[COL_MOOD] = df[COL_KEY].map(lambda key: 0 if key[-5:] == \"Minor\" else 1)\ndf[COL_MOOD].value_counts()","e32ffa34":"def simplify_key(key:str):\n    assert type(key) == str\n    equal_key = {\n        \"C#\":\"C#\",\"C\":\"C\",\"C\u266d\":\"B\",\n        \"D#\":\"D#\",\"D\":\"D\",\"D\u266d\":\"C#\",\n        \"E#\":\"F\" ,\"E\":\"E\",\"E\u266d\":\"D#\",\n        \"F#\":\"F#\",\"F\":\"F\",\"F\u266d\":\"E\",\n        \"G#\":\"G#\",\"G\":\"G\",\"G\u266d\":\"F#\",\n        \"A#\":\"A#\",\"A\":\"A\",\"A\u266d\":\"G#\",\n        \"B#\":\"C\" ,\"B\":\"B\",\"B\u266d\":\"A#\"\n    }\n    return equal_key[key]\n    \ndf[COL_KEY] = df[COL_KEY].map(lambda key: key.split(\" \")[0])\ndf[COL_KEY] = df[COL_KEY].map(simplify_key)\ndf[COL_KEY].value_counts()","c9642fa2":"COL_KEY_LETTER = \"Key_Letter\"\ndf[COL_KEY_LETTER] = df[COL_KEY].map(lambda key: key[:1])\ndf[COL_KEY_LETTER].value_counts()\nxp.bar(df,x=COL_KEY_LETTER, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","9aa8a70e":"# key_onehot = split_to_onehot(df, COL_KEY)\n# key_artist = Counter(split_words(df, COL_KEY))","5f09bd6a":"# key_onehot = split_to_onehot(df, COL_KEY)\n# key_artist = Counter(split_words(df, COL_KEY))\n\n# key_embedded = onehot_to_tsne2(key_onehot, \"Key\")\n# _df = key_embedded.copy(deep=True)\n# _df[[COL_CATEGORY,COL_KEY]] = df[[COL_CATEGORY,COL_KEY]]\n# xp.scatter(_df,x=\"Keys_tsne_1\",y=\"Keys_tsne_2\",color=COL_CATEGORY, hover_data=[COL_TRACK], height=500)\n\n# df = pd.concat([df,key_embedded[[\"Key_tsne_1\",\"Key_tsne_2\"]]], axis=1)\n# if DROP:\n#     df = df.drop(COL_KEY, axis=1)","47c852a3":"# df = pd.concat([df,key_embedded[[\"Key_tsne_1\",\"Key_tsne_2\"]]], axis=1)\nif DROP:\n    df = df.drop(COL_KEY, axis=1)","9d530744":"xp.scatter(df, x=COL_BPM,y=COL_ARTISTS_GENRES ,color=\"Category\", height=500, color_discrete_sequence=palette)","52946436":"display(description[COL_KEY])\ndf[COL_VOCAL] = df[COL_VOCAL].fillna(value = \"NA\")\ndf[COL_VOCAL] = df[COL_VOCAL].map(str)\ndf[COL_VOCAL].value_counts()\nxp.bar(df,x=COL_VOCAL, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","d3d359ff":"df[COL_VOCAL] = df[COL_VOCAL].map(lambda voice: \"F\" if voice not in  [\"M\"] else \"M\")\ndf[COL_VOCAL].value_counts()\nxp.bar(df,x=COL_VOCAL, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","01072e1c":"display(description[COL_COUNTRY])\ndf[COL_COUNTRY] = df[COL_COUNTRY].fillna(value = \"NA\")\ndf[COL_COUNTRY] = df[COL_COUNTRY].map(str)\ndf[COL_COUNTRY].value_counts()\nxp.bar(df,x=COL_COUNTRY, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","0b2bec7d":"COL_COUNTRY_2 = COL_COUNTRY+\"_2\"\ndf[COL_COUNTRY_2] = df[COL_COUNTRY]\ndf[COL_COUNTRY_2] = df[COL_COUNTRY_2].replace({\"GB\":\"USA\",\"UK\":\"RUS\",\"KZ\":\"RUS\"})\ndf[COL_COUNTRY_2] = df[COL_COUNTRY_2].map(lambda country: country if country in {\"USA\",\"RUS\"} else \"OTHER\")\nxp.bar(df,x=COL_COUNTRY_2, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","c1e5ba68":"# country_onehot = split_to_onehot(df, COL_COUNTRY)\n# country_artist = Counter(split_words(df, COL_COUNTRY))\n# ","d3cfc7d9":"\n# country_embedded = onehot_to_tsne2(country_onehot, \"Country\")\n# _df = country_embedded.copy(deep=True)\n# _df[[COL_CATEGORY,COL_KEY]] = df[[COL_CATEGORY,COL_KEY]]\n# xp.scatter(_df,x=\"Country_tsne_1\",y=\"Country_tsne_2\",color=COL_CATEGORY, hover_data=[COL_TRACK], height=500)\n\n","6bb66d0a":"# df = pd.concat([df,country_embedded[[\"Country_tsne_1\",\"Country_tsne_2\"]]], axis=1)\nif DROP:\n    df = df.drop(COL_KEY, axis=1)\n# BLACK_LIST.append(COL_COUNTRY)","317f068e":"# countries_data = {\n# \"AT\":  [\"Austria\" ,1],\n# \"AU\":  [\"Australia\" ,0],\n# \"BEL\": [\"Belarus\" ,1],\n# \"BAR\": [\"Barbados\" ,1],\n# \"CA\":  [\"Canada\" ,0],\n# \"COL\": [\"Colombia\" ,1],\n# \"DE\":  [\"Germany\" ,1],\n# \"DK\":  [\"Denmark\" ,1],\n# \"FIN\": [\"Finland\" ,1],\n# \"FR\":  [\"France\" ,1],\n# \"IRL\": [\"Ireland\" ,1],\n# \"ITL\": [\"Italy\" ,1],\n# \"JM\":  [\"Jamaica\" ,1],\n# \"KZ\":  [\"Kazakhstan\" ,1],\n# \"KR\":  [\"South Korea\" ,1],\n# \"NO\":  [\"Norway\" ,1],\n# \"NED\": [\"Netherlands\" ,1],\n# \"POR\": [\"Portugal\" ,1],\n# \"RUS\": [\"Russia\" ,1],\n# \"SWE\": [\"Sweden\" ,1],\n# \"UA\":  [\"Ukraine\" ,1],\n# \"JP\":  [\"Japan\"  ,1],\n# \"TR\":  [\"Trinidad\" ,1],\n# \"AZE\": [\"Azerbaijan\" ,1],\n# \"GB\":  [\"Great Britain\" ,0],\n# \"NA\":  [\"Not available\" ,1],\n# \"USA\": [\"USA\" ,0]\n# }\n\ncountries_data = {\n\"AT\":  [\"Austria\" ,2],\n\"AU\":  [\"Australia\" ,0],\n\"BEL\": [\"Belarus\" ,1],\n\"BAR\": [\"Barbados\" ,2],\n\"CA\":  [\"Canada\" ,0],\n\"COL\": [\"Colombia\" ,2],\n\"DE\":  [\"Germany\" ,2],\n\"DK\":  [\"Denmark\" ,2],\n\"FIN\": [\"Finland\" ,2],\n\"FR\":  [\"France\" ,2],\n\"IRL\": [\"Ireland\" ,2],\n\"ITL\": [\"Italy\" ,2],\n\"JM\":  [\"Jamaica\" ,2],\n\"KZ\":  [\"Kazakhstan\" ,1],\n\"KR\":  [\"South Korea\" ,2],\n\"NO\":  [\"Norway\" ,2],\n\"NED\": [\"Netherlands\" ,2],\n\"POR\": [\"Portugal\" ,2],\n\"RUS\": [\"Russia\" ,1],\n\"SWE\": [\"Sweden\" ,2],\n\"UA\":  [\"Ukraine\" ,1],\n\"JP\":  [\"Japan\"  ,2],\n\"TR\":  [\"Trinidad\" ,2],\n\"AZE\": [\"Azerbaijan\" ,1],\n\"GB\":  [\"Great Britain\" ,0],\n\"NA\":  [\"Not available\" ,2],\n\"USA\": [\"USA\" ,0]\n}\n\ncountries_language = {counties:str(num[1]) for counties,num in countries_data.items()}\ndf[COL_COUNTRY] = df[COL_COUNTRY].map(lambda x : countries_language[x.split(\"|\")[0]])\ndf[COL_COUNTRY].value_counts()\nle = preprocessing.LabelEncoder()\nle.fit(df[COL_COUNTRY])\ndf[COL_COUNTRY] = le.transform(df[COL_COUNTRY])\nxp.bar(df,x=COL_COUNTRY, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","c9d749d0":"display(description[COL_BPM])\ndisplay(df[COL_BPM].head())\nxp.bar(df,x=COL_BPM, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)","4f6d0bf2":"COL_BPM_GROUP = COL_BPM + \"_GROUP\"\ndf[COL_BPM_GROUP] = (df.loc[:,COL_BPM]\/\/20)\ndf[COL_BPM_GROUP]","c189e7f4":"for col in [COL_BPM,COL_ENERGY,COL_DANCEBILITY,COL_HAPPINESS]:\n    display(description[col])\n    df[col] = df[col].fillna(method = \"ffill\")\n\ndf[[COL_BPM,COL_ENERGY,COL_DANCEBILITY,COL_HAPPINESS]].describe()\nsns.pairplot(df[[COL_BPM,COL_ENERGY,COL_DANCEBILITY,COL_HAPPINESS,COL_CATEGORY]],palette=color_palette[-3:], hue=COL_CATEGORY)\nBLACK_LIST.append(COL_BPM)","3c083458":"df[\"BPM\"] = df[\"BPM\"].apply(lambda x: str(x)[1:] if str(x)[0]=='`' else x)\ndf[['Energy', 'Happiness', 'Dancebility','BPM']] = df[['Energy', 'Happiness', 'Dancebility','BPM']].fillna(0)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy', 'Happiness', 'Dancebility']].apply(lambda x: x\/sum(x), axis=1)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy%', 'Happiness%', 'Dancebility%']].fillna(0)\n\n# sns.pairplot(df[['Energy%', 'Happiness%', 'Dancebility%']+[COL_CATEGORY]],palette=color_palette[-3:], hue=COL_CATEGORY)","887d6d82":"display(description[\"Release year\"])\ndf[COL_RELEASE_YEAR] = df[COL_RELEASE_YEAR].fillna(method = \"ffill\")\nxp.bar(df,x=COL_RELEASE_YEAR, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)\n","8dd0400c":"COL_RELEASE_YEAR_GROUP = COL_RELEASE_YEAR + \"_GROUP\"\ndef process_group( df: pd.DataFrame)-> pd.core.series.Series:\n    def cat_fun(year):\n        if year < 2000:\n            return \"0\"\n        elif year < 2005:\n            return \"1\"\n        elif year < 2010:\n            return \"2\"\n        elif year < 2015:\n            return \"3\"\n        else:\n            return \"4\"\n#     def cat_fun(year):\n#         if year < 2005:\n#             return 0\n#         elif year < 2010:\n#             return 1\n#         elif year < 2015:\n#             return 2\n#         else:\n#             return 3\n        \n    df[COL_RELEASE_YEAR] = df[COL_RELEASE_YEAR].fillna(method = \"ffill\")\n    return df[COL_RELEASE_YEAR].map(cat_fun)\n\n\ndef COL_RELEASE_GROUP_process(year_delta:int, df: pd.DataFrame) -> pd.core.series.Series:\n    df[COL_RELEASE_YEAR] = df[COL_RELEASE_YEAR].fillna(method = \"ffill\")\n    return df[COL_RELEASE_YEAR].map(lambda age: int(age\/\/year_delta))\n\n\n\n# train_data[COL_RELEASE_YEAR_GROUP] = COL_RELE\n# n ASE_GROUP_process(10,train_data)\n\ndf[COL_RELEASE_YEAR_GROUP] = process_group(df)\ndf[COL_RELEASE_YEAR_GROUP][:SHOW_DATA]\n# train_data[COL_RELEASE_YEAR_GROUP].plot(kind='hist')\ndf[COL_RELEASE_YEAR_GROUP].value_counts()\nxp.bar(df,x=COL_RELEASE_YEAR_GROUP, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)\nBLACK_LIST.append(COL_RELEASE_YEAR)","113fa1a8":"display(description[COL_DURATION])\ndf[COL_DURATION] = df[COL_DURATION].fillna(method = \"ffill\")\ndf[COL_DURATION].value_counts()\nsns.distplot(df[COL_DURATION], hist=False, color=palette[0])","a053224a":"COL_DURATION_GROUP = COL_DURATION + \"_GROUP\"\n# def process_duration_group( df: pd.DataFrame)-> pd.core.series.Series:\n#     def cut_fun(duration):\n#         duration_delta = 60000\n#         duration = int(duration\/\/duration_delta)\n#         if duration < 3:\n#             return \"0\"\n#         elif duration < 4:\n#             return \"1\"\n#         elif duration < 5:\n#             return \"2\"\n#         else:\n#             return \"3\"\n#     df[COL_DURATION] = df[COL_DURATION].fillna(method = \"ffill\")\n#     return df[COL_DURATION].map(cut_fun)\n\n\n# def COL_DURATION_GROUP_process( df: pd.DataFrame,duration_delta:int = 60000) -> pd.core.series.Series:\n#     df[COL_DURATION] = df[COL_DURATION].fillna(method = \"ffill\")\n#     return df[COL_DURATION].map(lambda duration: str(int(duration\/\/duration_delta)))\n\n\n# df[COL_DURATION_GROUP] = COL_DURATION_GROUP_process(duration_delta = 60000,df =train_data)\n# df[COL_DURATION_GROUP] = process_duration_group(df)\n\ndf[COL_DURATION_GROUP] = df[COL_DURATION]\/\/60000\ndf[COL_DURATION_GROUP] = df[COL_DURATION_GROUP].map(lambda x : str(x))\ndf[COL_DURATION_GROUP][:SHOW_DATA]\n# df[COL_DURATION_GROUP].plot.pie()\ndf[COL_DURATION_GROUP].value_counts()\nxp.bar(df,x=COL_RELEASE_YEAR_GROUP, y=COL_TRACK,color=COL_CATEGORY,height=500, color_discrete_sequence=palette)\n","a03c22ef":"COL_DURATION_P = COL_DURATION+\"%\"\n\ndf[[COL_DURATION_P]] = df[[COL_DURATION]].apply(lambda x: x\/sum(x), axis=1)\ndf[[COL_DURATION_P]] = df[[COL_DURATION]].fillna(0)\n# df = df.drop(COL_DURATION, axis=1)\n# BLACK_LIST.append(COL_DURATION)\n# BLACK_LIST.append(COL_DURATION_GROUP)","9dd6a9d8":"xp.scatter(df, x = COL_RELEASE_YEAR, y=COL_TRACK,color=COL_CATEGORY, height=500,color_discrete_sequence=palette)","1b7bb29f":"display(df.info())\ndisplay(df.describe())","c2edc090":"df[COL_CATEGORY] = df[COL_CATEGORY].replace({\"Dislike\":0,\"Like\":1,\"None\": np.nan})\ndisplay(df.isna().sum()\/df.shape[0]*100)\ndisplay(df.info())\ndisplay(df.head())\n","8f3dbd80":"\n\nf_list = df.columns.tolist()\n\nprint(BLACK_LIST)\nfor feature in BLACK_LIST:\n    display(feature)\n    f_list.remove(feature)\n    \ndf_process_dummies = pd.get_dummies(df[f_list])\nfeatures_list = df_process_dummies.columns.tolist()\nfeatures = features_list[:]\nfeatures.remove(COL_CATEGORY)","af196933":"df_train = df_process_dummies.loc[~df_process_dummies[COL_CATEGORY].isnull()]\ndf_test =  df_process_dummies.loc[df_process_dummies[COL_CATEGORY].isnull()]\ndisplay(df_train.head())\ndisplay(df_test.head())\ndisplay(df_train.describe())\ndisplay(df_test.describe())","15f397e6":"# display(df_test.head())","be838c70":"# train = df_train.columns.tolist()\n# test = df_test.columns.tolist()\n# list(set(train) - set(test))\n# # train","4f06102d":"# TPN,FPN = CM[0][0]+CM[1][1],CM[0][1]+CM[1][0]\n# (TPN,FPN)","fbf02710":"dft = df_train.copy(deep=True)\ntd = df_test.copy(deep=True)\n\ndf_y = dft[COL_CATEGORY]\n\ndf_x = dft[features_list]","63467b99":"X_train, X_test, y_train, y_test = train_test_split(df_x,df_y,test_size=0.4, random_state=0)\n[X_train.shape,\\\nX_test.shape,\\\ny_test.shape,\\\ny_train.shape]\n\n","828a5797":"# cat_df = set(df.columns.tolist())\n# cat_dummies = set(df_process_dummies.columns.tolist())\n\n# col_cat = list(cat_dummies-cat_df)\n# cat_f = []\n# for feature in features:\n#     cols = set(feature.split(\"_\"))\n#     if cat_cols & cols:\n#         cat_f.append(feature) \n\n# cat_features = [df_x.columns.get_loc(col) for col in cat_f]\n# print(cat_features)","2064178d":"from sklearn.model_selection import StratifiedKFold\n\nn_fold = 4 # amount of data folds\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=SEED)\n\nparams_cbc = {'verbose': 200,\n              'random_seed': SEED,\n#               'cat_features': cat_features,\n#           'task_type': 'GPU',\n              'iterations':10\n         }\n\nparam_rfc = {'criterion': 'entropy',\n 'max_depth': 15,\n 'max_features': 'auto',\n 'n_estimators': 200,\n#              \"oob_score\" : True,\n             \"n_jobs\":-1\n            }\n\n# get a list of base models\ndef get_models():\n    models = list()\n#     models.append(('gbc',GradientBoostingClassifier()))\n    models.append(('rfc_2', RandomForestClassifier(**param_rfc)))\n#     models.append(('lr', LogisticRegression(n_jobs=-1,solver='liblinear')))\n#     models.append(('knn', KNeighborsClassifier(n_neighbors=30)))\n    models.append(('cbc', CatBoostClassifier(**params_cbc)))\n#     models.append(('cart', DecisionTreeClassifier()))\n#     models.append(('svm', SVC(probability=True)))\n#     models.append(('tre', AdaBoostClassifier(n_estimators = 100)))\n#     models.append(('bayes', GaussianNB()))\n\n#     models.append(('rfc', RandomForestClassifier(n_estimators=75, random_state=42)))\n\n#     models.append(('dtc', DecisionTreeClassifier()))\n\n\n#     models.append(('svc', SVC()))\n    \n    return models","b097c2a3":"y = y_train\nX = X_train[features]\nXtest = X_test[features]\n\ntrain = X.columns.tolist()\ntest = Xtest.columns.tolist()\n\n# X_train = X\n# X_val = Xtest\n# y_train = y\n# y_val = y_test\n\n# create the base models\nmodels = get_models()\n\nmodels","db822881":"# y = y_train\n# X = X_train[features]\n# Xtest = X_test[features]\n\n# models = get_models()\n# e_model = VotingClassifier(models,voting='soft')\n\n# kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n# results = model_selection.cross_val_score(e_model, X, y, cv=kfold)\n\n# e_model.fit(X, y)\n# e_model = RandomForestClassifier(n_estimators = 300, max_depth=10, criterion='entropy')\n# e_model.get_params().keys()\n","6594f551":"# '''\ny = y_train\nX = X_train[features]\nXtest = X_test[features]\n\nmodels = get_models()\n# e_model = RandomForestClassifier(random_state=SEED)\ne_model = VotingClassifier(models,voting='soft',n_jobs=-1)\n\n# kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n# results = model_selection.cross_val_score(e_model, X, y, cv=kfold)\n\nparam_rfc = {\n    'criterion': 'entropy',\n 'max_depth': 20,\n#  'max_features': 'auto',\n 'n_estimators': 300,\n#              \"oob_score\" : True,\n             \"n_jobs\":-1\n            }\n# estimator = SVR(kernel=\"linear\")\n# e_model = RFE(e_model, n_features_to_select = 12)\n# e_model = RFE(e_model, n_features_to_select = 20)\n# e_model = Ridge(alpha = 1)\n# param_grid = { \n#     'n_estimators': [20,50,100,200,500,1000],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'max_depth' : [1,3,5,10,15],\n#     'criterion' :['gini', 'entropy']\n# }\n# e_model = RandomForestClassifier(n_estimators = 100,oob_score = True, random_state=SEED,max_depth=10)\n\n\n# rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True, random_state=SEED) \n# rfc =  RandomForestClassifier(**param_rfc)\n\n# param_grid = { \n#     'n_estimators': [50,100,200,300,500,1000],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'max_depth' : [5,10,15,20],\n#     'criterion' :['gini', 'entropy']\n# }\n# e_model = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n\nrfc = RandomForestClassifier(**param_rfc)\n\n# rfc = RandomForestClassifier(n_jobs=-1, max_features= 'auto',n_estimators=200, oob_score = True, random_state=SEED,criterion='entropy',max_depth= 15) \n# rfc = RandomForestClassifier(n_estimators = 1000,oob_score = True,max_depth=6, criterion='entropy', random_state=SEED)\n\n# cbc = CatBoostClassifier(**params_cbc)\n# cbc = CatBoostClassifier()\n# \ne_model = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\",dual = False))),\n  ('classification', rfc)\n])\n\n\n\n# e_model = Pipeline([\n#   ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\",dual = False))),\n#   ('classification', RandomForestClassifier(n_estimators = 500,\n#                                             oob_score = True,\n#                                             max_depth=15, criterion='entropy', random_state=SEED))\n# ])\n\n\n\n# e_model = cbc\n# e_model = rfc\n\n\ne_model.fit(X, y)\n\n# '''","5f17addb":"# e_model.best_params_\n# {'criterion': 'entropy',\n#  'max_depth': 15,\n#  'max_features': 'auto',\n#  'n_estimators': 200}\n# kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n# results = model_selection.cross_val_score(e_model, X, y, cv=kfold)","77a525af":"# '''\npredictions = e_model.predict(Xtest)\ny_pred_proba = e_model.predict_proba(Xtest)[::,1]\nfpr_e, tpr_e, _ = roc_curve(y_test,  y_pred_proba)\nroc_auc_e = roc_auc_score(y_test, y_pred_proba)\nmd_e = str(e_model)\nmd_e = md_e[:md_e.find('(')]\nplt.plot(fpr_e, tpr_e, label='ROC fold %s (auc = %0.2f)' % (md_e, roc_auc_e))\noutput = pd.DataFrame({COL_ID: X_test.Id, COL_CATEGORY: predictions})\n\nlabel = \"ENSAMBLE\"\nprint(label,classification_report(X_test[COL_CATEGORY], output[COL_CATEGORY]))\n\n\n    \nlw = 2\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic example\")\nplt.legend(loc=\"lower right\")\n# '''","76fa2ae3":"# '''\nplot_confusion_matrix(e_model,X_test[features], y_test)  \nCM = confusion_matrix(y_test,predictions)\n\n# cv = model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=SEED)\n# #find out CV\n# # scores = model_selection.cross_val_score(e_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# print(scores.mean())\n# # '''","1191eac8":"# '''\ndef cm_to_inch(value):\n    return value\/2.54\nplt.rcParams[\"figure.figsize\"]=(cm_to_inch(15*3),cm_to_inch(10*3))\n\ny = y_train\nX = X_train[features]\nXtest = X_test[features]\n\nfor label, model in models:\n    \n    model.fit(X, y)\n    predictions = model.predict(Xtest)\n    y_pred_proba = model.predict_proba(Xtest)[::,1]\n    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\n    md = str(model)\n    md = md[:md.find('(')]\n    plt.plot(fpr, tpr, label='ROC fold %s (auc = %0.2f)' % (md, roc_auc))\n    output = pd.DataFrame({COL_ID: X_test.Id, COL_CATEGORY: predictions})\n\n    \nprint(label,classification_report(X_test[COL_CATEGORY], output[COL_CATEGORY]))\n\n\nplt.plot(fpr_e, tpr_e, label='ROC fold %s (auc = %0.2f)' % (md_e, roc_auc_e))   \nlw = 2\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic example\")\nplt.legend(loc=\"lower right\")\n# '''","b29aa418":"# # test_X = pd.get_dummies(X_test[features])\n# # for model in models:\n# #     y_pred_proba = model.predict_proba(test_X)[::,1]\n# #     fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n# #     auc = roc_auc_score(y_test, y_pred_proba)\n# #     plt.plot(fpr,tpr,label=f\"f, auc=\"+str(round(auc,PERCISION)))\n\n# lw = 2\n# plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.0])\n# plt.xlabel(\"False Positive Rate\")\n# plt.ylabel(\"True Positive Rate\")\n# plt.title(\"Receiver operating characteristic example\")\n# plt.legend(loc=\"lower right\")\n\n# # plt.savefig('ROC.png')\n# plt.show()","1a44b09a":"# Y = df_y\n# X = pd.get_dummies(df_x[features])\n# Xtest = pd.get_dummies(td[features])\n# print(Y.shape, X.shape,Xtest.shape)\n# model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=SEED)\n\n# model.fit(X, Y)\n# predictions = model.predict(Xtest)\n# output = pd.DataFrame({\"Id\": td.Id, COL_CATEGORY: predictions})\n# df_output = output.groupby(\"Id\").agg({COL_CATEGORY:'mean'}).reset_index()\n# df_output[COL_CATEGORY] = df_output[COL_CATEGORY].astype(np.int64)\n# df_output.to_csv('submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")\n# #","270b6968":"# df_output[COL_CATEGORY]","34d9d3bc":"# '''\nY = df_y\nX = df_x[features]\nXtest = td[features]\n\nmodels = get_models()\ne_model = VotingClassifier(models,voting='soft',n_jobs=-1)\n\n# # kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n# # results = model_selection.cross_val_score(e_model, X, Y, cv=kfold)\n\n# e_model.fit(X, Y)\n\n\n\n# rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True, random_state=SEED) \n# param_grid = { \n#     'n_estimators': [50,100,200],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'max_depth' : [5,10,15,20],\n#     'criterion' :['gini', 'entropy']\n# }\n# e_model = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n\n# e_model = RandomForestClassifier(n_estimators = 1000, max_depth=10, criterion='entropy')\n\n\n# rfc = RandomForestClassifier(n_jobs=-1, max_features= 'auto',n_estimators=200, oob_score = True, random_state=SEED, criterion='entropy',max_depth= 15)  \n# rfc = RandomForestClassifier(n_estimators = 1000,oob_score = True,max_depth=6, criterion='entropy', random_state=SEED)\n# rfc = RandomForestClassifier(**param_rfc)\n\ne_model = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\",dual = False))),\n  ('classification', e_model)\n])\n# kfold = model_selection.KFold(n_splits=5, random_state=SEED, shuffle=True)\n# results = model_selection.cross_val_score(e_model, X, y, cv=kfold)\n\n# estimator = SVR(kernel=\"linear\")\n# e_model = RFE(e_model, n_features_to_select = 25)\n# e_model = rfc\n\ne_model.fit(X, Y)\n\n\npredictions = e_model.predict(Xtest)\noutput = pd.DataFrame({\"Id\": td.Id, COL_CATEGORY: predictions})\nprint(output.shape)\noutput[COL_CATEGORY] = output[COL_CATEGORY].astype(np.int64)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n# '''","edcf18c8":"output.head()","4e3135ff":"Prepared some functions for [t-distributed stochastic neighbor embedding (t-SNE)](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding), which will reduce the dimension of the data. ","56010e9e":"Intro","2fc25b5e":"### 1.10 Numeric features [Energy, Dancebility,Happines]","38a28238":"### Feature [Release year]","486046f7":"Now convert track data into columns if it's the most common. ","5757276d":"### 1.3 Feature [Version]","500d3f34":"After process of compressing of dimentional I encode all data to numeric code, maybe it will be usefull","486266ac":"The data presented above look extremely holistic with respect to genres","9fcb398a":"### 1.7 Feature [Labels]","8607100b":"Load data","251a1b19":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","b2ec952d":"Info about NaN elements, in every column","f8e83a13":"### 1.9 Feature [Country]","aad7dcf8":"Notice, that mostly track born in (USA,GB),(UK,RUS,KZ) and other countries, lets modify data","c153c826":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","a6748c96":"### Feature [Duration]","21a2c529":"### 1.5 Feature [Album]","9b6109b7":"### 1.2 Feature [Track]","fc585290":"Before convert to numeric let's observe distribution on chart","fb60f762":"### Feature [Vocal]","6db0730a":"### 1.4 Feature [Artists Genres]","d71fe744":"### 1.8 Feature [Key]","b22aa8d1":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","3be73ea3":"Create Black List which will be stored unnecessary columns","1cee4635":"# Time to FIT model","21afb293":"Let's observe some distributions on charts","b7f537ce":"### 1.1 Explore feature [Artist]","10e13e63":"# 1. Explore data","b58eceb2":"### 1.6 Feature [Album type]","61c4024a":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","f0b17a3b":"### 1.9 Numeric features [BPM] ","0452bd57":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","43e7eae0":"Using tsne, I reduce the high-dimensional space of vocabulary word feature vector to a two-dimensional. and reduced this dimentional by compresses them down to 2-dimensional x,y coordinate pairs.","43679939":"Let's observe numerical features how them will be distribute between categories."}}