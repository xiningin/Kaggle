{"cell_type":{"69ec80c9":"code","607cdc79":"code","934f0a7b":"code","13c5a704":"code","c9d87f4e":"code","1f4f73ed":"code","3a570662":"code","6e3e56ca":"code","f2537fc0":"code","b9592c66":"code","836aa604":"code","d6ceccf9":"code","79d5fade":"code","4b55e9a8":"code","61fd9e38":"code","cba54e95":"code","786f2cb9":"code","3f071f0a":"code","a439e0b3":"code","aae3f5bb":"code","d5630cb3":"code","220cf00f":"code","e3f6b71e":"code","62cbf5d4":"code","773cbadc":"code","696afe57":"code","6de1a895":"code","946aa42c":"code","b668518a":"code","26ce781e":"code","298b09b0":"code","a700ad27":"code","8be38d3d":"code","fd759555":"code","1303db89":"code","6bc0790c":"code","a8d82bef":"code","a9b6eb18":"code","4c217c8b":"code","8d50bfda":"code","129f0227":"code","384145b5":"code","b1172114":"code","555f324d":"code","2a7e4d5f":"code","5ee0f7be":"code","b7958355":"code","d74c1929":"code","7a67483e":"code","b062ddc3":"code","30e6152a":"code","ad455bac":"code","545a15d6":"code","a26ad1de":"code","8fa54195":"code","f2e89629":"code","a5a84044":"code","9c7d1c99":"code","63e535b9":"code","c83c9659":"code","a9e2d2df":"code","bba9c1fc":"code","f318bf91":"code","851d5009":"code","fd5409f8":"code","1c0045ec":"code","4b69c2bb":"code","8d1b3da1":"code","9264e9f7":"code","48fb3134":"code","03ac15a9":"code","c1f91307":"code","7bce36e3":"code","f843d74d":"code","1e71ee52":"code","86d4e899":"code","63633019":"code","f1f8620e":"code","572ad7dd":"code","74efed06":"code","bc7d8843":"markdown","7378fbc0":"markdown","a590ef74":"markdown","bec43817":"markdown","acad8507":"markdown","4edce2fc":"markdown","4efbe06c":"markdown","9de04a3d":"markdown","180e0ba9":"markdown","1c99c879":"markdown","ae036bd0":"markdown","bd72635c":"markdown","3669ee98":"markdown","440742c5":"markdown","bc62a1bb":"markdown","43e597c8":"markdown","fbf1c28b":"markdown","8b1fc033":"markdown","2392acc9":"markdown","9df97af4":"markdown","01cc9bc8":"markdown","a7b1434c":"markdown","70d7fbf9":"markdown","2cfb6d8b":"markdown","b79c752e":"markdown","5c1b5b24":"markdown","6727e3c6":"markdown","0ae25b52":"markdown","e2889617":"markdown","a17ed129":"markdown","e4590d59":"markdown","e7fa3928":"markdown","a5b20884":"markdown","9b627e54":"markdown","6333d993":"markdown","96ebfefe":"markdown","6dd9ea47":"markdown","186176f9":"markdown","d180ac17":"markdown","8dbda91b":"markdown","1f4d10b3":"markdown","aafd4ec5":"markdown","d2926ce1":"markdown","95cb32ef":"markdown","f88abf80":"markdown","426bba04":"markdown","41902ea0":"markdown","24899f84":"markdown","9a0f25a9":"markdown"},"source":{"69ec80c9":"import pandas as pd\nimport numpy as np\nimport numpy.random as nr\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nfrom sklearn import feature_selection as fs\nimport sklearn.decomposition as skde\nimport sklearn.metrics as sklm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport scipy.stats as ss\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.linear_model import ElasticNet, Lasso,  Ridge, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nimport xgboost as xgb\n\n%matplotlib inline","607cdc79":"df_features = pd.read_csv('..\/input\/train_values_wJZrCmI.csv')\ndf_labels = pd.read_csv('..\/input\/train_labels.csv')\ndf = df_features.merge(df_labels, on='row_id')\n\ndf.shape","934f0a7b":"df = df.drop_duplicates(keep = 'last')\ndf = df.drop_duplicates(subset = 'row_id', keep = 'last')\n\ndf.shape","13c5a704":"print(df.isnull().sum())\n\ndf.drop('bank_interest_rate', axis = 1, inplace = True)\ndf.drop('mm_interest_rate', axis = 1, inplace = True)\ndf.drop('mfi_interest_rate', axis = 1, inplace = True)\ndf.drop('other_fsp_interest_rate', axis = 1, inplace = True)\n\ndf.isnull().sum()","c9d87f4e":"# df['education_level'].fillna(df['education_level'].median(), inplace=True)\n# df['share_hh_income_provided'].fillna(df['share_hh_income_provided'].median(), inplace=True)\n\n# df.dropna(subset=['education_level'], inplace=True)\n# df.dropna(subset=['share_hh_income_provided'], inplace=True)\n\ndf['education_level'].fillna(4, inplace=True)\ndf['share_hh_income_provided'].fillna(0, inplace=True)\n\ndf.isnull().sum()\n# print(df.loc[df['education_level'] == 'Unknown'])","1f4f73ed":"df.dtypes","3a570662":"# def replace_boolean(data):\n#     for col in data:\n#         data[col].replace(True, 1, inplace=True)\n#         data[col].replace(False, 0, inplace=True)\n        \n# replace_boolean(df)\n# df.dtypes","6e3e56ca":"df.describe()","f2537fc0":"# df = df[df.poverty_probability > 0]\n# df = df[df.poverty_probability < 1]\n# df = df[df.num_financial_activities_last_year < 10]\ndf.shape","b9592c66":"# df['poverty_probability'].describe().to_csv('D:\\\\Users\\\\user\\\\Desktop\\\\MPP DS Cert\\DAT102x - Microsoft Professional Capstone Data Science\\\\label_summary.csv')","836aa604":"def hist_plot(vals, lab):\n    ## Distribution plot of values\n    sns.set(style=\"whitegrid\", palette='Blues_r')\n    sns.distplot(vals)\n    plt.title('Histogram of ' + lab)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n    \nsns.set_style(\"whitegrid\")\nhist_plot(df['poverty_probability'], 'PPI')","d6ceccf9":"#FUNCTION FOR returning 0 for ln(0)\nfrom numpy import errstate,isneginf\na = df['poverty_probability']\nwith errstate(divide='ignore'):\n    res = np.log(a)\nres[isneginf(res)]=0\nhist_plot(res, 'ln PPI')","79d5fade":"#CUBE ROOT LABELS\ndf['cbrt_poverty_probability'] = np.cbrt(df['poverty_probability'])\nhist_plot(df['cbrt_poverty_probability'], 'cube root PPI')","4b55e9a8":"#SQUARE LABELS\ndf['squared_poverty_probability'] = np.square(df['poverty_probability'])\nhist_plot(df['squared_poverty_probability'], 'squared PPI')","61fd9e38":"list(df.columns.values)","cba54e95":"def create_age_group(data):\n    age_conditions = [\n    (data['age'] < 30 ),\n    (data['age'] >= 30) & (data['age'] < 45),\n    (data['age'] >= 45) & (data['age'] < 60),\n    (data['age'] >= 60)\n    ]\n    age_choices = ['Under 30', '30 to 44', '45 to 59', '60 or Over']\n    data['age_group'] = np.select(age_conditions, age_choices)\n\ncreate_age_group(df)","786f2cb9":"def count_unique(df, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(df[col].value_counts())\n\ncat_cols = ['age_group','country','is_urban','female','married','religion','relationship_to_hh_head',\n 'education_level','literacy','can_add','can_divide','can_calc_percents','can_calc_compounding',\n 'employed_last_year','employment_category_last_year','employment_type_last_year',\n 'income_ag_livestock_last_year','income_friends_family_last_year','income_government_last_year',\n 'income_own_business_last_year','income_private_sector_last_year','income_public_sector_last_year',\n 'borrowing_recency','formal_savings','informal_savings','cash_property_savings',\n 'has_insurance','has_investment','borrowed_for_emergency_last_year','borrowed_for_daily_expenses_last_year',\n 'borrowed_for_home_or_biz_last_year','phone_technology','can_call','can_text','can_use_internet',\n 'can_make_transaction','phone_ownership','advanced_phone_use','reg_bank_acct',\n 'reg_mm_acct','reg_formal_nbfi_account','financially_included','active_bank_user',\n 'active_mm_user','active_formal_nbfi_user','active_informal_nbfi_user','nonreg_active_mm_user', 'share_hh_income_provided', \n'num_times_borrowed_last_year','num_shocks_last_year','num_formal_institutions_last_year',\n            'num_informal_institutions_last_year']\n\ncount_unique(df, cat_cols)\ndf.shape","3f071f0a":"def plot_violin(df, cols, col_y, title):\n    for col in cols:\n        sns.set(style=\"whitegrid\")\n        sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n        sns.violinplot(col, col_y, data=df)\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel(col_y)# Set text for y axis\n        plt.title(title + ' by ' + col)\n        plt.show()\n        \nplot_violin(df, cat_cols, 'poverty_probability', 'PPI')    ","a439e0b3":"num_cols = ['age', 'avg_shock_strength_last_year', \n            'num_financial_activities_last_year', \n            'poverty_probability', 'poverty_probability'] \n\ndef plot_density_hist(df, cols, bins = 10, hist = False):\n    for col in cols:\n        sns.set(style=\"whitegrid\", palette='Blues_r')\n        sns.distplot(df[col], bins = bins, rug=True, hist = hist)\n        plt.title('Histogram of ' + col) # Give the plot a main title\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel('Frequency')# Set text for y axis\n        plt.show()\n        \nplot_density_hist(df, num_cols, bins = 20, hist = True)","aae3f5bb":"num_cols = ['age', 'avg_shock_strength_last_year', \n            'num_financial_activities_last_year', 'poverty_probability'] \n\nsns.set(style=\"whitegrid\", palette='Blues_r')\nsns.pairplot(df[num_cols])","d5630cb3":"## Compute the correlation matrix\ncorrs = df[num_cols].corr()\n\n## Create the hierarchical clustering model\ndist = sch.distance.pdist(corrs)   # vector of pairwise distances using correlations\nlinkage = sch.linkage(dist, method='complete') # Compute the linkages for the clusters\nind = sch.fcluster(linkage, 0.5*dist.max(), 'distance')  # Apply the clustering algorithm\n\n## Order the columns of the correlaton matrix according to the hierarchy\ncolumns = [corrs.columns.tolist()[i] for i in list((np.argsort(ind)))]  # Order the names for the result\ncorrs_clustered = corrs.reindex(columns) ## Reindex the columns following the heirarchy \n\n## Correlation Plot\ncorrs_clustered.style.background_gradient().set_precision(2)","220cf00f":"num_corrs = df[num_cols].corr()\nf,ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(num_corrs, annot=True, square=True, linewidths=.1, fmt= '.2f',ax=ax, \n           cmap=\"RdBu\")\nplt.show()","e3f6b71e":"def plot_box_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    sns.boxplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n    \ndef plot_violin_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n#     fig, ax = plt.subplots(figsize=(11,8))\n    sns.violinplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n\nplot_violin_1(df, 'religion', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_shocks_last_year', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_formal_institutions_last_year', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_informal_institutions_last_year', 'poverty_probability', 'PPI')","62cbf5d4":"    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    fig, ax = plt.subplots(figsize=(11,8))\n    sns.violinplot('relationship_to_hh_head', 'poverty_probability', data=df) #order=['Under 30', '30 to 44', '45 to 59', '60 or Over']\n    plt.xlabel('relationship_to_hh_head') # Set text for the x axis\n    plt.ylabel('poverty_probability')# Set text for y axis\n    plt.title('PPI' + ' by ' + 'relationship_to_hh_head')\n    plt.show()\n","773cbadc":"religion_categories = {'N':'N_Q', 'O':'O_P',\n                       'P':'O_P', 'Q':'N_Q','X':'X'}\ndf['religion'] = [religion_categories[x] for x in df['religion']]\nprint(df['religion'].value_counts())\n\n#num_shocks_last_year 4_5\nnum_shocks_last_year_categories = {0:'0', 1:'1', 2:'2',\n                       3:'3', 4:'4_5', 5:'4_5'}\ndf['num_shocks_last_year'] = [num_shocks_last_year_categories[x] for x in df['num_shocks_last_year']]\nprint(df['num_shocks_last_year'].value_counts())\n\n#num_formal_institutions_last_year 3_or_over\nnum_formal_institutions_last_year_categories = {0:'0', 1:'1', 2:'2',\n                       3:'3_4_5_6', 4:'3_4_5_6', 5:'3_4_5_6', 6:'3_4_5_6'}\ndf['num_formal_institutions_last_year'] = [num_formal_institutions_last_year_categories[x] for x in df['num_formal_institutions_last_year']]\nprint(df['num_formal_institutions_last_year'].value_counts())\n\n#num_informal_institutions_last_year 2_or_over\nnum_informal_institutions_last_year_categories = {0:'0', 1:'1', 2:'2_3_4',\n                       3:'2_3_4', 4:'2_3_4'}\ndf['num_informal_institutions_last_year'] = [num_informal_institutions_last_year_categories[x] for x in df['num_informal_institutions_last_year']]\nprint(df['num_informal_institutions_last_year'].value_counts())\n\nrelationship_to_hh_head_categories = {'Other':'Other', 'Spouse':'Spouse',\n                                      'Head':'Head',\n                                      'Son\/Daughter':'Son\/Daughter',\n                                      'Sister\/Brother':'Sister\/Brother',\n                                      'Father\/Mother': 'Father\/Mother',\n                                      'Unknown':'Other'}\ndf['relationship_to_hh_head'] = [relationship_to_hh_head_categories[x] for x in df['relationship_to_hh_head']]\nprint(df['relationship_to_hh_head'].value_counts())","696afe57":"plot_violin_1(df, 'religion', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_shocks_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_formal_institutions_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_informal_institutions_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'relationship_to_hh_head', 'poverty_probability', 'PPI') ","6de1a895":"# df = df[df.relationship_to_hh_head != 'Unknown']\n# df.shape","946aa42c":"# df['ln_age'] = np.log(df['age'])\n# hist_plot(df['ln_age'], 'natural log age')","b668518a":"# df['cbrt_num_times_borrowed_last_year'] = np.cbrt(df['num_times_borrowed_last_year'])\n# hist_plot(df['cbrt_num_times_borrowed_last_year'], 'cube root num_times_borrowed_last_year')","26ce781e":"Labels = np.array(df['poverty_probability'])\n# Labels = np.array(df['poverty_probability'])","298b09b0":"def encode_string(cat_features):\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder(categories='auto')\n    encoded = ohe.fit_transform(cat_features.values.reshape(-1,1)).toarray()\n    pdfn = ohe.get_feature_names()\n    print(pdfn)\n    return encoded\n\nfeatures_cat_cols = ['country','is_urban','female','married','religion','relationship_to_hh_head',\n 'education_level','literacy','can_add','can_divide','can_calc_percents','can_calc_compounding',\n 'employed_last_year','employment_category_last_year','employment_type_last_year',\n 'income_ag_livestock_last_year','income_friends_family_last_year','income_government_last_year',\n 'income_own_business_last_year','income_private_sector_last_year','income_public_sector_last_year',\n 'borrowing_recency','formal_savings','informal_savings','cash_property_savings',\n 'has_insurance','has_investment','borrowed_for_emergency_last_year','borrowed_for_daily_expenses_last_year',\n 'borrowed_for_home_or_biz_last_year','phone_technology','can_call','can_text','can_use_internet',\n 'can_make_transaction','phone_ownership','advanced_phone_use','reg_bank_acct',\n 'reg_mm_acct','reg_formal_nbfi_account','financially_included','active_bank_user',\n 'active_mm_user','active_formal_nbfi_user','active_informal_nbfi_user','nonreg_active_mm_user', 'share_hh_income_provided', \n'num_times_borrowed_last_year','num_shocks_last_year','num_formal_institutions_last_year',\n            'num_informal_institutions_last_year']\n\nFeatures = encode_string(df['age_group'])\nfor col in features_cat_cols:\n    temp = encode_string(df[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n    \nprint(Features.shape)\nprint(Features[:2, :])","a700ad27":"features_num_cols = ['avg_shock_strength_last_year',\n                     'num_financial_activities_last_year']\nFeatures = np.concatenate([Features, np.array(df[features_num_cols])], axis = 1)\nprint(Features.shape)\nprint(Features[:2, :])","8be38d3d":"# Features selection\nprint(Features.shape)\n\n## Define the variance threhold and fit the threshold to the feature array.\nsel = fs.VarianceThreshold(threshold=(.95 * (1 - .95)))\nFeatures_reduced = sel.fit_transform(Features)\nprint(sel.get_support())\n\n## Print the support and shape for the transformed features\nprint(Features_reduced.shape)","fd759555":"#Select k best features with Recursive Feature Eliminiation\n## Reshape the Label array\nLabels = Labels.reshape(Labels.shape[0],)\n## Set folds for nested cross validation\nnr.seed(562)\nfeature_folds = ms.KFold(n_splits=10, shuffle = True)\n## Define the model\nlin_mod_l2 = linear_model.Ridge()\n## Perform feature selection by CV with high variance features only\nnr.seed(265)\nselector = fs.RFECV(estimator = lin_mod_l2, cv = feature_folds,\n                      scoring = 'r2')\nselector = selector.fit(Features_reduced, Labels)\nprint(selector.support_)\nprint(selector.ranking_)\n\nFeatures_reduced = selector.transform(Features_reduced)\nprint(Features_reduced.shape)","1303db89":"nr.seed(265)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 0.2)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = Features_reduced[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])","6bc0790c":"scaler = preprocessing.StandardScaler().fit(x_train[:,104:])\nx_train[:,104:] = scaler.transform(x_train[:,104:])\nx_test[:,104:] = scaler.transform(x_test[:,104:])\nprint(x_train[:2,])","a8d82bef":"GBoost = GradientBoostingRegressor()\nmodel_xgb = xgb.XGBRegressor()\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01)","a9b6eb18":"nr.seed(265)\ninside = ms.KFold(n_splits=5, shuffle = True)\nnr.seed(562)\noutside = ms.KFold(n_splits=5, shuffle = True)\n\nnr.seed(2652)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {'n_estimators': [2000, 3000]}\n\n## Perform the grid search over the parameters\ngsearch = ms.GridSearchCV(estimator = model_lgb, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'r2',\n                      return_train_score = True)","4c217c8b":"gsearch.fit(Features_reduced, Labels)\ngsearch.best_params_, gsearch.best_score_","8d50bfda":"GBoost = GradientBoostingRegressor(n_estimators= 2000, learning_rate=0.01,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=7, min_samples_split=15, \n                                   loss='ls', random_state = 1)\n\nmodel_xgb = xgb.XGBRegressor(max_depth = 5, min_child_weight = 0, gamma = 0, \n                           subsample = 0.8, colsample_bytree = 0.8, \n                           scale_pos_weight = 1, reg_lambda = 1,\n                           learning_rate =0.01, n_estimators=2000, \n                           objective = 'reg:squarederror', seed = 14)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01, n_estimators=2100, \n                              bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.4,\n                              min_data_in_leaf = 5,  \n                              feature_fraction_seed=3, bagging_seed=2)","129f0227":"#Validation function\nn_folds = 5\n\ndef r2_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\n    r2 = cross_val_score(model, x_train, y_train, scoring=\"r2\", cv = kf)\n    return(r2)","384145b5":"score = r2_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b1172114":"score = r2_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","555f324d":"score = r2_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","2a7e4d5f":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","5ee0f7be":"averaged_models = AveragingModels(models = (model_xgb, model_lgb))\n\n# score = r2_cv(averaged_models)\n# print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b7958355":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d74c1929":"stacked_averaged_models = StackingAveragedModels(base_models = (model_lgb, model_xgb),\n                                                 meta_model = GBoost)\n\nscore = r2_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","7a67483e":"def rsquare(y, y_pred):\n    return r2_score(y, y_pred)","b062ddc3":"averaged_models.fit(x_train, y_train)\navg_pred = averaged_models.predict(x_test)\nprint(rsquare(y_test, avg_pred))","30e6152a":"stacked_averaged_models.fit(x_train, y_train)\nstacked_pred = stacked_averaged_models.predict(x_test)\nprint(rsquare(y_test, stacked_pred))","ad455bac":"GBoost.fit(x_train, y_train)\ngb_pred = GBoost.predict(x_test)\nprint(rsquare(y_test, gb_pred))","545a15d6":"model_xgb.fit(x_train, y_train)\nxgb_pred = model_xgb.predict(x_test)\nprint(rsquare(y_test, xgb_pred))","a26ad1de":"model_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(x_test)\nprint(rsquare(y_test, lgb_pred))","8fa54195":"avg_pred[avg_pred < 0] = 0\navg_pred[avg_pred > 1] = 1","f2e89629":"stacked_pred[stacked_pred < 0] = 0\nstacked_pred[stacked_pred > 1] = 1","a5a84044":"gb_pred[gb_pred < 0] = 0\ngb_pred[gb_pred > 1] = 1","9c7d1c99":"xgb_pred[xgb_pred < 0] = 0\nxgb_pred[xgb_pred > 1] = 1","63e535b9":"lgb_pred[lgb_pred < 0] = 0\nlgb_pred[lgb_pred > 1] = 1","c83c9659":"ensemble = avg_pred*0.55 + lgb_pred*0.45\nprint('R2 score on test data:')\nprint(rsquare(y_test, ensemble))","a9e2d2df":"def print_metrics(y_true, y_predicted, n_parameters):\n    ## First compute R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)\/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    ## Print the usual metrics and the R^2 values\n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \nprint_metrics(y_test, lgb_pred, 105)    ","bba9c1fc":"fimp = xgb.plot_importance(model_xgb)\nfig = fimp.figure\nfig.set_size_inches(20, 20)","f318bf91":"# hist_plot(y_score, 'Predicted PPI')","851d5009":"def hist_resids(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.set(style = 'whitegrid', palette='Blues_r')\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(y_test, lgb_pred)    ","fd5409f8":"def resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ss.probplot(resids.flatten(), plot = plt)\n    ax.get_lines()[0].set_marker('+')\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n    \nresid_qq(y_test, lgb_pred)   ","1c0045ec":"def resid_plot(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.regplot(y_score, resids, fit_reg=False, marker=\"+\", scatter_kws={'alpha':0.5})\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n\nresid_plot(y_test, lgb_pred) ","4b69c2bb":"test_df = pd.read_csv('..\/input\/test_values.csv')","8d1b3da1":"test_df = test_df.drop_duplicates(keep = 'last')\ntest_df = test_df.drop_duplicates(subset = 'row_id', keep = 'last')\n\ntest_df.shape\ntest_df.drop('bank_interest_rate', axis = 1, inplace = True)\ntest_df.drop('mm_interest_rate', axis = 1, inplace = True)\ntest_df.drop('mfi_interest_rate', axis = 1, inplace = True)\ntest_df.drop('other_fsp_interest_rate', axis = 1, inplace = True)\n\n\ntest_df['education_level'].fillna(4, inplace=True)\ntest_df['share_hh_income_provided'].fillna(0, inplace=True)\n# test_df.dropna(subset=['education_level'], inplace=True)\n# test_df.dropna(subset=['share_hh_income_provided'], inplace=True)\n\ntest_df.isnull().sum()","9264e9f7":"test_df.shape","48fb3134":"# replace_boolean(test_df)\n# test_df.dtypes","03ac15a9":"# test_df = test_df[test_df.relationship_to_hh_head != 'Unknown']\n# test_df.shape","c1f91307":"create_age_group(test_df)\ntest_df['religion'] = [religion_categories[x] for x in test_df['religion']]\ntest_df['num_shocks_last_year'] = [num_shocks_last_year_categories[x] for x in test_df['num_shocks_last_year']]\ntest_df['num_formal_institutions_last_year'] = [num_formal_institutions_last_year_categories[x] for x in test_df['num_formal_institutions_last_year']]\ntest_df['num_informal_institutions_last_year'] = [num_informal_institutions_last_year_categories[x] for x in test_df['num_informal_institutions_last_year']]\ntest_df['relationship_to_hh_head'] = [relationship_to_hh_head_categories[x] for x in test_df['relationship_to_hh_head']]","7bce36e3":"print(test_df['relationship_to_hh_head'].value_counts())","f843d74d":"# test_df.loc[test_df.education_level.isnull(), :]","1e71ee52":"test_Features = encode_string(test_df['age_group'])\nfor col in features_cat_cols:\n    test_temp = encode_string(test_df[col])\n    test_Features = np.concatenate([test_Features, test_temp], axis = 1)\n\nprint(test_Features.shape)\nprint(test_Features[:2, :])\n","86d4e899":"test_Features = np.concatenate([test_Features, np.array(test_df[features_num_cols])], axis = 1)\nprint(test_Features.shape)\nprint(test_Features[:2, :])","63633019":"print(test_Features.shape)\ntest_Features_reduced = sel.fit_transform(test_Features)\nprint(test_Features_reduced.shape)\n\ntest_Features_reduced = selector.transform(test_Features_reduced)\nprint(test_Features_reduced.shape)","f1f8620e":"nr.seed(265)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = None)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = test_Features_reduced\n# y_test = np.ravel(Labels[indx[1]])\n\nscaler = preprocessing.StandardScaler().fit(x_train[:,104:])\nx_train[:,104:] = scaler.transform(x_train[:,104:])\nx_test[:,104:] = scaler.transform(x_test[:,104:])","572ad7dd":"averaged_models.fit(x_train, y_train)\navg_pred = averaged_models.predict(x_test)\n# model_xgb.fit(x_train, y_train)\n# xgb_pred = model_xgb.predict(x_test)\nmodel_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(x_test)\nensemble = avg_pred*0.55 + lgb_pred*0.45","74efed06":"test_id = test_df['row_id']\nsub = pd.DataFrame({ 'row_id': test_id,\n                            'poverty_probability': lgb_pred })\nsub.to_csv('prediction_result_lgb_3.csv', index=False)","bc7d8843":"### Hyperparameter tuning\n\nHyperparameters are manually and recursively selected with a grid search, here is an example of finding the optimized value for XGB Regressor's `n_estimators`:","7378fbc0":"# Exploratory data analysis\n### Exploring the label\n\nSquare transformation on the label produces a less skewed distribution, but decreases our R-squared. Other transformations did not make the distribution more normal.","a590ef74":"### Recursive feature elimination\n\nWe used the scikit-learn `RFECV` function to determine which features to retain using a cross validation method, with the ridge regression model:","bec43817":"# Construct the Regression Models\n\nDifferent regression models are tested with optimized hyperparameters. We first fit the models with pre-selected parameters to our train data, and fine-tune them recursively. Here we will test Gradient Boosting Regressor, XGboost Regressor and LGB Regressor, the current best-performing algorithms for data science competitions.","acad8507":"Joining numeric features:","4edce2fc":"#### Explore numeric features\nCreating combined kernel density estimation and histogram plots for all numeric features:","4efbe06c":"#### Replace missing values   \nAssigning missing values to a new category for both `education_level` and `share_hh_income_provided` gives with best R-squared for our regression model.","9de04a3d":"Creating a residual histogram plot, a residual q-q plot, and a residuals vs. predicted values plot:","180e0ba9":"Averaging XBG and LGB and print out their cross-validation r-square score:","1c99c879":"### Averaging base models\nWe build a new class to extend scikit-learn with our model and leverage inheritance:","ae036bd0":"Drop duplicates, uninformative features, and impute missing values:","bd72635c":"Stacking GBR on top of the averaged XBG and LGB and print out their cross-validation r-square score:","3669ee98":"### Rescale numeric features\n\nWe use the `StandardScaler` to z-score scale the numeric features.","440742c5":"Replacing true\/false with 1\/0 for categorical features isn't necessary for the new OneHotEncoder, as it takes strings as arguments.\n\n","bc62a1bb":"In this case, it looks like simply averaging XGB and LGB gives a better score.","43e597c8":"### Adding a meta-model\nLet's create a class for stacking average models:","fbf1c28b":"Optimized parameters are plugged into the models:","8b1fc033":"We then create a function to evaluate the models' performances on the test data after fitting to the training data.","2392acc9":"#### Exploring categorical features\nCounting unique values for categorical features - we see that some features, like religion, have values with very little data. Feature engineering will be performed to solve the problem.\n\nNote that some discrete numerical features are treated as categorical features.","9df97af4":"Create violin plots for the new features:","01cc9bc8":"### Exploring features","a7b1434c":"Creating violin plots for all categorical features against our label PPI - only some features have values that are distinctively different in PPI distributions.","70d7fbf9":"# Model Matrix Preparation\n\nWe are predicting the label `poverty_probability` with both categorical and numeric features. Categorical features are one-hot encoded, and joint back with the numeric features.","2cfb6d8b":"# Split the Dataset\n\nWith the model matrix constructed, we now create randomly sampled training and test data sets in a 8:2 ratio:","b79c752e":"### Drop duplicates\nNo duplicates detected","5c1b5b24":"Feature engineering:","6727e3c6":"Fit the cross validated grid search over the data and print the best parameter value:","0ae25b52":"However, on average, the amount of financial activities seems to decrease with age.","e2889617":"#### Transforming numeric features\n\nTransformed numeric features worsened our model.","a17ed129":"The whole training dataset was used to train the model, and the model is used to predict the separate test dataset:","e4590d59":"Create the final submission file:","e7fa3928":"# Poverty Prediction: From Visualization to Model Stacking (My First Kernel!)\n**By Johnny Yiu**\n<a href=\"https:\/\/www.worldbank.org\/en\/news\/video\/2018\/10\/17\/new-ways-of-looking-at-poverty\"><img src=\"https:\/\/i.imgur.com\/jIVQ94c.png\" title=\"Source: World Bank\" \/><\/a>\n\n## Overview\nThe motivation of creating this notebook is a data science competition hosted by **Microsoft** and **DrivenData**, which is a part of the Capstone project of the **Microsoft Professional Certificate in Data Science** and also my very first competition related to machine learning. By the end of the competition, I was at the **top 4%** on the leaderboard, and I am happy to share with you my approach, especially to those who are just beginning their journey on data science.\n\n<a href=\"https:\/\/datasciencecapstone.org\/competitions\/15\/predicting-poverty\/leaderboard\/\"><img src=\"https:\/\/i.imgur.com\/Yo4Oeya.png\" title=\"Leaderboard\" \/><\/a>\n\n<a href=\"https:\/\/datasciencecapstone.org\/competitions\/15\/predicting-poverty\/leaderboard\/\"><img src=\"https:\/\/i.imgur.com\/ItktCZr.png\" title=\"Best Score\" \/><\/a>\n\n## Goal\nParticipants are to predict the probability that individuals across 7 different countries live below the poverty line at the $2.50\/day threshold, given other socioeconomic indicators. The probability of being in poverty was calculated using the **Poverty Probability Index (PPI)**, which estimates an individual's poverty status using 10 questions about a household\u2019s characteristics and asset ownership. The remaining data comes from the **Financial Inclusion Insights** household surveys conducted by InterMedia.\n\n<a href=\"https:\/\/www.povertyindex.org\"><img src=\"https:\/\/www.povertyindex.org\/sites\/default\/files\/PPI-logo-RGB-header-image.png\" title=\"PPI\" \/><\/a>\n<a href=\"http:\/\/finclusion.org\"><img src=\"https:\/\/i.imgur.com\/jjPjT06.png\" title=\"Fii\" \/><\/a>\n\n## Data\nThe dataset was retrieved from datasciencecapstone.org and contains the PPI along with 58 features of 12,600 individuals across 7 different countries.\n\n**Information on the competition and the data:**\nhttps:\/\/datasciencecapstone.org\/competitions\/15\/predicting-poverty\/page\/47\/\n\n## Model\nI have hand-picked 3 regression-based models (Gboost, XGBoost and LightGBM) and used a 5-fold cross validation to evaluation their performance. A stacked model of the 3 is then tested for prediction results. The final r2 score was 0.4213, resulting a top 4% on the leaderboard. \n\n**The model stacking approach here is inspired by Serigne, make sure to check it out at:**\n[Serigne's Stacked Regressions Notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard).","a5b20884":"### Evaluate models with cross validation\nWe define a cross validation strategy to evaluate the r-square of our models:","9b627e54":"A pairplot and a correlation matrix of numeric features is created - apparent relationships are neither found within numeric features, and between the label and these features.","6333d993":"### Model selection\n\n#### Eliminate low variance features\n\nWe eliminate features with low variance using the scikit-learn `VarianceThreshold` based function. A p = 0.95 was found to optimize the model.","96ebfefe":"# Data Pre-processing\nWe start by first preparing the data into suitable formats for analysis.\n\n\nThe required packages and data sets are loaded. I have merged the features and labels data sets into one dataset.","6dd9ea47":"### Feature engineering\n#### Aggregate categorical features\n\nThe aggregation of categorical features was performed to reduce the number of categories. For discrete variables, rare values are combined to form a range of values. For categorical variables, rare values are combined with common values that share a more similar distribution in PPI.\n\n- Religion O is combined with religion P to form religion O_P, and religion N is combined with religion Q to form religion \u2018N_Q\u2019\n- The number of shocks of 4 or above are combined to form \u20184_5\u2019\n- The number of formal financial institutions used of 3 or above are combined to form \u20183_4_5_6\u2019\n- The number of informal financial institutions used of 2 or above are combined to form \u20182_3_4\u2019\n- The number of shocks of 4 or above are combined to form \u20184_5\u2019 \n- The category \u2018Unknown\u2019 for relationship to the head of the household is combined with \u2018Other\u2019","186176f9":"Removing outliers decreased our R-squared.","d180ac17":"For meaningful interpretation, predicted scores with negative values are converted to 0, and scores larger than 1 are converted to 1:","8dbda91b":"We first fit the selected models to the training data and fit them to the test data, then ensemeble the models with the assigned weights:","1f4d10b3":"# External Test Data Preparation\n\nA seperated file containing test data was provided by DrivenData for testing and submission. We prepare these test features the same way the train data was prepared in order to fit the model.","aafd4ec5":"### Treat missing values\nDropping columns with mostly missing values:","d2926ce1":"Removing values named 'Unknown' for relationship_to_hh_head was attempted, but it worsened our model.","95cb32ef":"Onehotencode categorical features:","f88abf80":"A feature importance plot is created find out what are the top features, here a sample plot for XGB regressor is printed:","426bba04":"The feautre age_group is created from age:","41902ea0":"It turned out that I achieved my highest score (0.4213) by submitting predictions using the LGB model, instead of the ensembled \/ averaged model. The discrepancy between the r2 score (0.4690) for the local test data and the external test data might be due to overfitting.","24899f84":"# Model Stacking\n\nHere, 2 model stacking approaches are applied - \n1. averaging base models, and \n2. stacking averaged models and adding a meta-model\n\nFor more information, read [Serigne's Stacked Regressions Notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard).","9a0f25a9":"# Print metrics and evaluations\n\nThe model's performance metrics on the locally splitted test data is printed:"}}