{"cell_type":{"7fafe7d6":"code","5ac43b7a":"code","cf55746a":"code","f9175ee0":"code","ebc75ed3":"code","fd958501":"code","44d821e5":"code","acb640da":"code","c6a27ca6":"code","604d8915":"code","b62e0546":"code","9b916bde":"code","41a729c5":"code","e9cc7567":"code","a1680f01":"code","6e6beb0a":"code","d563cfb5":"code","ea996824":"code","0ebb4ceb":"code","5afce21a":"code","39c367ae":"code","294fed58":"code","2af76d1a":"code","648f242f":"code","c75ce8ba":"code","b166b7c4":"code","d0f859aa":"code","96420e81":"code","b13ab37b":"code","ef8e95d3":"code","624da180":"code","d904bd75":"code","9bd2d83b":"code","01d394f8":"code","119154fa":"code","5e389225":"code","15a884c3":"code","6b4a9c44":"code","a6eb28d3":"code","56c05b56":"markdown","5a60b409":"markdown","d1a5f032":"markdown","d5ac0a05":"markdown","b3fbac9b":"markdown","5e6f20ab":"markdown","6055f984":"markdown","9bed859b":"markdown","a461e0a1":"markdown","2f97a497":"markdown","2317291d":"markdown","58c5b50a":"markdown","d9bf5379":"markdown","53afea90":"markdown","85618e1f":"markdown","636cf2d6":"markdown","ffa7aa70":"markdown","b7e7afcf":"markdown","6dca7703":"markdown","2dca228a":"markdown","11fc17c2":"markdown","bf347808":"markdown","8895a644":"markdown","a6a00023":"markdown","8cd70f8a":"markdown","8dbcf053":"markdown","e281c428":"markdown","65fe420d":"markdown","6600664f":"markdown","239dbf4a":"markdown","66ee04ec":"markdown","b6d6192e":"markdown"},"source":{"7fafe7d6":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","5ac43b7a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntrain.head()","cf55746a":"test = pd.read_csv(\"..\/input\/test.csv\")","f9175ee0":"train.shape[0] \/ test.shape[0]","ebc75ed3":"train.info()","fd958501":"test.info()","44d821e5":"column_types = train.dtypes\ncolumn_types[column_types==np.int64]","acb640da":"train.isnull().sum().sum()","c6a27ca6":"test.isnull().sum().sum()","604d8915":"sns.countplot(train.target, palette=\"Set2\");","b62e0546":"magic = \"wheezy-copper-turtle-magic\"\ntrain_corr = train.drop([\"target\", magic], axis=1).corr()\ntest_corr = test.drop(magic, axis=1).corr()","9b916bde":"train_corr_flat = train_corr.values.flatten()\ntrain_corr_flat = train_corr_flat[train_corr_flat != 1]\n\ntest_corr_flat = test_corr.values.flatten()\ntest_corr_flat = test_corr_flat[test_corr_flat != 1]\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(train_corr_flat, ax=ax[0], color=\"tomato\")\nsns.distplot(test_corr_flat, ax=ax[1], color=\"limegreen\");\nax[0].set_title(\"Off-diagonal train corr \\n distribution\")\nax[1].set_title(\"Off-diagonal test corr \\n distribution\");\nax[0].set_xlabel(\"feature correlation value\")\nax[1].set_xlabel(\"feature correlation value\");","41a729c5":"plt.figure(figsize=(25,25))\nsns.heatmap(train_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","e9cc7567":"plt.figure(figsize=(25,25))\nsns.heatmap(test_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","a1680f01":"target_medians = train.groupby(\"target\").median()\nsorted_target_distance = np.abs(target_medians.iloc[0]-target_medians.iloc[1]).sort_values(ascending=False)","6e6beb0a":"sorted_target_distance.head()","d563cfb5":"sorted_target_distance.tail()","ea996824":"fig, ax = plt.subplots(2,2,figsize=(20,10))\nsns.distplot(train.loc[train.target==0, \"wheezy-myrtle-mandrill-entropy\"], color=\"Blue\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==1, \"wheezy-myrtle-mandrill-entropy\"], color=\"Red\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==0, \"wheezy-copper-turtle-magic\"], color=\"Blue\", ax=ax[0,1])\nsns.distplot(train.loc[train.target==1, \"wheezy-copper-turtle-magic\"], color=\"Red\", ax=ax[0,1])\nax[1,0].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"skanky-carmine-rabbit-contributor\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,0].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,0].set_ylabel(\"skanky-carmine-rabbit-contributor\")\nax[1,1].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"wheezy-copper-turtle-magic\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,1].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,1].set_ylabel(\"wheezy-copper-turtle-magic\");","0ebb4ceb":"n_splits=3\nn_repeats=3\n\nX=train.drop([\"target\", \"id\"], axis=1).values\ny=train.target.values\nXTest = test.drop(\"id\", axis=1).values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nXTest = scaler.transform(XTest)","5afce21a":"X, x_val, y, y_val = train_test_split(X,y,test_size=0.2, stratify=y, random_state=2019)","39c367ae":"skf = RepeatedStratifiedKFold(n_repeats=n_repeats,\n                              n_splits=n_splits,\n                              random_state=2019)\n\np_val = np.zeros(y_val.shape)\npTest = np.zeros(XTest.shape[0])\nfor train_idx, test_idx in skf.split(X,y):\n    \n    x_train, x_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    lr=LogisticRegression(penalty=\"l1\", C=1, solver=\"saga\")\n    lr.fit(x_train, y_train)\n    p_test = lr.predict_proba(x_test)[:,1]\n    p_val += lr.predict_proba(x_val)[:,1]\n    pTest += lr.predict_proba(XTest)[:,1]\n    print(roc_auc_score(y_test, p_test))\n\np_val \/= (n_splits*n_repeats)\npTest \/= (n_splits*n_repeats)\n","294fed58":"print(roc_auc_score(y_val, p_val))","2af76d1a":"feat1 = \"wheezy-myrtle-mandrill-entropy\"\nfeat2 = \"skanky-carmine-rabbit-contributor\"\nfeat3 = \"wheezy-copper-turtle-magic\"","648f242f":"N = 10000\n\ntrace1 = go.Scatter3d(\n    x=train[feat1].values[0:N], \n    y=train[feat2].values[0:N],\n    z=train[feat3].values[0:N],\n    mode='markers',\n    marker=dict(\n        color=train.target.values[0:N],\n        colorscale = \"Jet\",\n        opacity=0.3,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = 'The turtle place',\n    scene = dict(\n        xaxis = dict(title=feat1),\n        yaxis = dict(title=feat2),\n        zaxis = dict(title=feat3),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","c75ce8ba":"fig, ax = plt.subplots(5,5,figsize=(20,25))\nfor turtle1 in range(5):\n    for turtle2 in range(5):\n        my_turtle=turtle2+turtle1*5\n        ax[turtle1, turtle2].scatter(train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat1].values,\n                                     train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat2].values,\n                                     c=train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, \"target\"].values, cmap=\"coolwarm\", s=5, alpha=0.5)\n        ax[turtle1, turtle2].set_xlim([-15,15])\n        ax[turtle1, turtle2].set_ylim([-15,15])","b166b7c4":"names = list(train.drop([\"id\", \"target\"], axis=1).columns.values)","d0f859aa":"first_names = []\nsecond_names = []\nthird_names = []\nfourth_names = []\n\nfor name in names:\n    words = name.split(\"-\")\n    first_names.append(words[0])\n    second_names.append(words[1])\n    third_names.append(words[2])\n    fourth_names.append(words[3])","96420e81":"print(len(first_names), len(np.unique(first_names)))\nprint(len(second_names), len(np.unique(second_names)))\nprint(len(third_names), len(np.unique(third_names)))\nprint(len(fourth_names), len(np.unique(fourth_names)))","b13ab37b":"feature_names = pd.DataFrame(index=train.drop([\"target\", \"id\"], axis=1).columns.values, data=first_names, columns=[\"kind\"])\nfeature_names[\"color\"] = second_names\nfeature_names[\"animal\"] = third_names\nfeature_names[\"goal\"] = fourth_names\nfeature_names.head()","ef8e95d3":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"kind\", data=feature_names, order=feature_names.kind.value_counts().index, palette=\"Greens_r\")\nplt.xticks(rotation=90);","624da180":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"animal\", data=feature_names, order=feature_names.animal.value_counts().index, palette=\"Oranges_r\")\nplt.xticks(rotation=90);","d904bd75":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"color\", data=feature_names, order=feature_names.color.value_counts().index, palette=\"Purples_r\")\nplt.xticks(rotation=90);","9bd2d83b":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"goal\", data=feature_names, order=feature_names.goal.value_counts().index, palette=\"Reds_r\")\nplt.xticks(rotation=90);","01d394f8":"feature_names[feature_names.goal==\"learn\"]","119154fa":"combined = train.drop([\"id\", \"target\"], axis=1).append(test.drop(\"id\", axis=1))\ncombined[combined.duplicated()]","5e389225":"n_subsamples_test = test.groupby(magic).size() \nn_subsamples_train = train.groupby(magic).size() \n\nplt.figure(figsize=(20,5))\nplt.plot(n_subsamples_test.values, '.-', label=\"test\")\nplt.plot(n_subsamples_train.values, '.-', label=\"train\")\nplt.plot(n_subsamples_test.values + n_subsamples_train.values, '.-', label=\"total\")\nplt.legend();\nplt.xlabel(magic)\nplt.ylabel(\"sample count\");","15a884c3":"my_magic=0\n\ntrain_subset = train.loc[train[magic]==my_magic].copy()\ntest_subset = test.loc[test[magic]==my_magic].copy()","6b4a9c44":"n_splits=20\nn_repeats=5\n\nX=train_subset.drop([\"target\", \"id\"], axis=1).values\ny=train_subset.target.values\nXTest = test_subset.drop(\"id\", axis=1).values\n\n#scaler = StandardScaler()\n#X = scaler.fit_transform(X)\n#XTest = scaler.transform(XTest)\n\nX, x_val, y, y_val = train_test_split(X,y,test_size=0.2, stratify=y, random_state=2019)","a6eb28d3":"skf = RepeatedStratifiedKFold(n_repeats=n_repeats,\n                              n_splits=n_splits,\n                              random_state=2019)\n\nimportances = np.zeros(shape=(n_splits*n_repeats, XTest.shape[1]))\np_val = np.zeros(y_val.shape)\npTest = np.zeros(XTest.shape[0])\n\nm=0\nfor train_idx, test_idx in skf.split(X,y):\n    \n    x_train, x_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    lr=LogisticRegression(penalty=\"l1\", C=0.1, solver=\"liblinear\", max_iter=300)\n    lr.fit(x_train, y_train)\n    p_test = lr.predict_proba(x_test)[:,1]\n    p_val += lr.predict_proba(x_val)[:,1]\n    pTest += lr.predict_proba(XTest)[:,1]\n    importances[m,:] += lr.coef_[0]\n    print(roc_auc_score(y_test, p_test))\n    m+=1\n\np_val \/= (n_splits*n_repeats)\npTest \/= (n_splits*n_repeats)","56c05b56":"### Insights\n\n* If you compare correlations carefully, you can observe that some of the nearby features have some slight correlations in train and test. This is especially interesting for some subsequent features. Can we find some sense in the way of column order?\n* This is not always true. Take a look at the lower right corner. The correlation between the last two features in train looks strong enough to \"be\" something, but in test this correlation has gone! As we have already seen by the correlation distributions we can expect some differences in the feature distributions of train and test. ","5a60b409":"### What about the column names?\n\nHave you asked yourself as well who had this nice phantasy to build up the column names? Is there a logic behind it? Can we find connections given the column names? Well I don't know it and we all have to work on it but perhaps a sketch helps...","d1a5f032":"### Ideas and insights\n\nPuhhh.... breath in and out. That looks like a further puzzle to solve... perhaps... probably... hopefully :-) \n\n1. The one turtle magic, is one magic.\n2. There a lot of further goal descriptions that could be interpretable or at least leave a guess on what to try out with them.\n3. Color has a natural order. Do we need this kind of order somewhere?\n4. A lot of animals only have one or two occurences. But perhaps it's not about a single animal name and we can introduce a higher level of abstraction like \"bird\" or \"dog\" instead of a specific species of dogs, birds etc..\n5. The kind is a mysterium for me... perhaps the length of these adjectives is important or the first character or we can find a grouping given the other feature names?!\n\nWould be great to create a connection map... who is connected with who? Can we visualize a network? Could be a good timepoint to learn how to realize that. ","d5ac0a05":"### Duplicates?","b3fbac9b":"## Sneak a peek","5e6f20ab":"As Chris already pointed out, we won't get far by ignoring the pattern found with our magic turtle. ","6055f984":"### Insights\n\nCrazy turtle! :-D\n\n* By taking the feature mean per target and computing distances between the classes I wanted to find features that show clear separating forces. But... even the feature with highest difference in classes medians looks like \"no big deal\" at all. \n* Our crazy magic turtle feature has no difference at all! And it's distribution is almost uniform even though the small bin peaks look interesting (just an artifact of plotting?). \n* By plotting two features with highest distances in class medians I wanted to see if we can see something like in don't overfit (overlapping gaussians)... but this time it doesn't look like that. Let's see what logistic regression tells us with its weights in the next step.\n* The scatter plot of wheezy-copper-turtle-magic and our highest class median distance feature, \"wheezy_myrtle-mandrill-entropy\", looks indeed crazy! It's not that the turtle magic spreads smoothly with lower density over the whole space of the mandrills tails. No! It has some subsequent \"peak\" tails depending on the value of magic turtle. This again leads to the question: What role does the magic turtle play in this game?","9bed859b":"## Prepare to start","a461e0a1":"### Take a look","2f97a497":"## Diving into single datasets\n\nHow many single rows do we have given a single magic turtle value? Do all subsets in the data have the same amount of samples?","2317291d":"## Table of contents\n\n1. Prepare to start\n2. Sneak a peek\n    * Take a look\n    * Size of the data\n    * Dtypes\n    * A magic feature?\n    * Missing values\n3. Basic Exploratory Analysis\n    * Feature correlations\n    * Distances between class medians\n    * What logistic regression wants to tell us\n    * The magic turtle again \n    * What about the column names?","58c5b50a":":-o What's that? Wheezy-copper-turtle-magic... again!","d9bf5379":"### Choosing a subset","53afea90":"Hmm... two int columns?","85618e1f":"### A magic feature?","636cf2d6":"### Class balance","ffa7aa70":"### Missing values\n\nLet's start with obvious once:","b7e7afcf":"### Insights\n\n* Looks almost decorrelated. But don't be too fast. Maybe this slight values are still something fruitful. \n* Do you see what is missing in train? We have +- 0.015 in test on both sides but only -0.01 and +0.015 in train. There must be some differences in feature distributions between train and test.","6dca7703":"Obviously not! The zick-zack nature above seemed to be a plotting artifact. ","2dca228a":"Ohhh! :-D Cool! The wheezy-copper-turtle-magic oooohhh magic! What does that mean for the comp?","11fc17c2":"### What logistic regression wants to tell us...","bf347808":"### Insights\n\n* Hey, we are given crazy column names! Can we find a meaning behind them? Hmm... ;-)\n* Very interesting id-column, isn't it?","8895a644":"### Feature correlation","a6a00023":"### The magic turtle again\n\nCan't get enough! :-) Let's look at a 3D-Scatterplot with magic turtle:","8cd70f8a":"### Dtypes","8dbcf053":"Very balanced in train!","e281c428":"## Basic exploratory analysis\n\nOk, now the colorful part starts. We have already found an interesting feature by peeking at the data. Perhaps we can find some more during basic EDA. Let's stay curious and critical! Do we know if test and train behave the same? No! For this reason, I don't like to combine train and test right now... :-)","65fe420d":"### Insights\n\n* Very cool and beautiful again! :-)\n* Choose some other features for feat1 and feat2. By doing so you will definitely find some spread that separates target classes! ","6600664f":"Roughly we have twice as much train data than test data.","239dbf4a":"No obvious missing values in train and test. Perhaps there are some non-obvious once but let's move this topic to exploratory data analysis.","66ee04ec":"Let's follow the idea that these names indeed have some meaning... in this case: what are features to discard? Is there one more magic feature and what makes it magically?","b6d6192e":"### Size of the data"}}