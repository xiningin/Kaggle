{"cell_type":{"59980265":"code","54ae104b":"code","8257ef90":"code","e010171f":"code","b6e51b17":"code","ecea9f09":"markdown","db57d463":"markdown","566837dc":"markdown","e8376886":"markdown","0cd887e5":"markdown","fa0449c1":"markdown"},"source":{"59980265":"#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport numpy   as np\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col=0)\n\n#===========================================================================\n# here, for this simple demonstration we shall only use the numerical columns \n# and ingnore the categorical features\n#===========================================================================\nX_train = train_data.select_dtypes(include=['number']).copy()\nX_train = X_train.drop(['SalePrice'], axis=1)\ny_train = train_data[\"SalePrice\"]\n# fill in any missing data with the mean value\nX_train = X_train.fillna(X_train.mean())","54ae104b":"print(X_train.shape[1])","8257ef90":"std      = np.std(X_train, axis=0)\nX_train \/= std","e010171f":"from sklearn import linear_model\nregressor = linear_model.Lasso(alpha=100,\n                               positive=True,\n                               fit_intercept=False, \n                               max_iter=1000,\n                               tol=0.0001)\nregressor.fit(X_train, y_train)","b6e51b17":"import eli5\neli5.show_weights(regressor, top=-1, feature_names = X_train.columns.tolist())","ecea9f09":"# Related reading\n* ESL:=  [\"*The Elements of Statistical Learning*\" by Trevor Hastie, Robert Tibshirani  and Jerome Friedman (2nd Ed.) Springer (2009)](https:\/\/web.stanford.edu\/~hastie\/ElemStatLearn\/)\n* [Arthur E. Hoerl and Robert W. Kennard \"*Ridge Regression: Biased Estimation for Nonorthogonal Problems*\", Technometrics **vol 12** pp. 55-67 (1970)](https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00401706.1970.10488634)\n* [Arthur E. Hoerl and Robert W. Kennard \"*Ridge Regression: Applications to Nonorthogonal Problems*\", Technometrics **vol 12** pp. 69-82 (1970)](https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00401706.1970.10488635)\n* [Robert Tibshirani \"*Regression Shrinkage and Selection Via the Lasso*\", Journal of the Royal Statistical Society: Series B (Methodological) **vol 58** pp. 267-288 (1996)](https:\/\/doi.org\/10.1111\/j.2517-6161.1996.tb02080.x)\n* [*Introduction to shrinkage and Ridge*](https:\/\/youtu.be\/I8bPQ272Pbs) YouTube video (for ESL \u00a7 3.4)\n* [*LASSO regression*](https:\/\/youtu.be\/FlSQgXv7Dvw) YouTube video (for ESL \u00a7 3.4.3)\n* [*Selecting the tuning parameter for Ridge regression and LASSO*](https:\/\/youtu.be\/8oEZkHqf_Rk) YouTube video","db57d463":"note that scikit-learn represents $\\lambda$ by `alpha`","566837dc":"with the following number of features","e8376886":"Finally we shall use the [ELI5](https:\/\/eli5.readthedocs.io\/en\/latest\/autodocs\/eli5.html) library to show the results, the greater the weight the more important the feature","0cd887e5":"# Feature importance using the LASSO\n\nWhen creating a model not all of the features in our training data are of equal importance. If we have sufficient computational resources at our disposal then we could indeed include all of the available features in our model, but this has (at least) two drawbacks; this can lead to [overfitting](https:\/\/www.kaggle.com\/carlmcbrideellis\/overfitting-and-underfitting-the-titanic), and also reduces the interpretability of our model. It is much more informative to create our model on a subset of the most influential features, in other words create a *sparse* model. In this short notebook we shall be ranking the features in a given dataset using the LASSO. Other notable feature importance techniques are \n\n* [Boruta-SHAP](https:\/\/www.kaggle.com\/carlmcbrideellis\/feature-selection-using-the-boruta-shap-package)\n* [Recursive Feature Elimination (RFE)](https:\/\/www.kaggle.com\/carlmcbrideellis\/recursive-feature-elimination-rfe-example)\n* [Permutation Importance](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-permutation-importance-example)\n\n## Regularization, or 'shrinkage'\nLet us start by looking at our old friend the linear regression. When finding the best fit it is usual to minimise a *cost function* (also known as a *loss function* or *objective function*), in this case the *residual sum of squares* (RSS), given by (ESL Eq. 3.2):\n\n$$ \\mathrm{RSS}(\\beta) = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 $$\n\nwhere $\\beta_j$ are the regression coefficients associated with each feature.\n\n### Ridge regression\nWe can shrink these regression coefficients by additionally imposing a *penalty term* (or *regularization term*) in our cost function (ESL Eq. 3.41):\n\n$$ \\mathrm{PRSS} = \\mathrm{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n\nwhere $\\lambda \\gt 0 $ is the '*tuning parameter*'. If $\\lambda = 0$ we revert to the original residual sum of squares cost function. The optimal value for the tuning parameter is found via cross-validation. Note the similarity between the penalty term and the  $\\ell^2$ vector norm. With Ridge regression we may well find that many of the coefficients become very small, however they are generally non-zero.\n\n### LASSO regression\nWith LASSO (Least Absolute Shrinkage and Selection Operator) regression we now have the following penalty term (ESL Eq. 3.53):\n\n$$ \\mathrm{PRSS} = \\mathrm{RSS} + \\lambda \\sum_{j=1}^p |\\beta_j| $$\n\nnote now the use of the $\\ell^1$ norm, which is sometimes known as the [Manhattan distance](https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry). This change has the effect of actually forcing some of the coefficients to become zero, in other words this is actually removing features from the model, and is effectively performing feature selection.\n\n## Example\nWe shall now apply LASSO to the [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) dataset","fa0449c1":"We shall use the [sklearn.linear_model.Lasso](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html) whose details are given [here](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#lasso).\n\nFirstly we should standardize the features, *i.e.* divide each feature by the standard deviation of that feature, thus each feature that is passed to the LASSO now has a standard deviation of one."}}