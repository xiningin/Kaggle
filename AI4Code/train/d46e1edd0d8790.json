{"cell_type":{"0a425c25":"code","63e8fae9":"code","853f00c4":"code","b3515a4c":"code","109062a8":"code","603e11af":"code","cc99b8ae":"code","e78276ef":"code","ff10f485":"code","54de06a3":"code","a2eaf8ab":"code","afa3c66b":"code","231ddf30":"code","00588d84":"code","150451e7":"code","4a6359b0":"code","da8e22e9":"code","b79683c8":"code","ce51eece":"code","963a14a8":"code","78d85f3d":"code","8409dd44":"code","6abbe2dd":"code","ad4c3f1a":"code","92db4b81":"code","f85ef90f":"code","fb0c46ad":"code","aea84728":"code","90927eeb":"code","e4c1a323":"code","5e1dd060":"code","d46f625c":"code","afd7885e":"code","ae230b76":"code","043874be":"code","2967e547":"code","f5ad01eb":"code","5cd101ce":"code","a414367f":"code","2bb0fdc9":"code","ccc0a6d6":"code","65452e6b":"code","8177ea47":"code","04f8967a":"code","e98f20a6":"code","abe7aa8f":"code","5799b142":"code","f2362754":"code","3979dcce":"code","3e698ba8":"code","c9b56f75":"code","1aceb785":"code","e90bf30c":"code","90b6c824":"code","ab772b89":"code","d30128e7":"code","75ad6c55":"code","f1a07cdd":"code","6dba0e4c":"code","9dce0baf":"code","a163ea55":"code","4c6902f5":"markdown","a65147e3":"markdown","c9a7fa3f":"markdown","999ee23b":"markdown","d5b1adee":"markdown","e9f7de97":"markdown","a0abadcf":"markdown","2599a784":"markdown","b33c230a":"markdown","a0f2b7a8":"markdown","847cc30a":"markdown","69ce1562":"markdown","bd9a6a48":"markdown","27cea446":"markdown","ba898b0b":"markdown","9aef6293":"markdown","5f0f9fed":"markdown","14b97215":"markdown","19944f95":"markdown","ae1ccfb7":"markdown","2d39e064":"markdown","90a6cb28":"markdown","cb50ea58":"markdown"},"source":{"0a425c25":"!pip3 install seaborn==0.11.0\n!pip install seaborn==0.11.0","63e8fae9":"import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfrom colorama import Fore, Back, Style\nred = Fore.RED\ngrn = Fore.GREEN\nblu = Fore.BLUE\nylw = Fore.YELLOW\nwht = Fore.WHITE\n\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff","853f00c4":"print(sns.__version__) \n'''We use seaborn version 0.11.0 to use histplot in PairPlot'''","b3515a4c":"path = '..\/input\/tabular-playground-series-jan-2021\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample = pd.read_csv(path + 'sample_submission.csv')","109062a8":"print(f'{train.shape}\\n{test.shape}\\n{sample.shape}')","603e11af":"train.head()","cc99b8ae":"test.head()","e78276ef":"sample.head()","ff10f485":"print('number of null columns in train set :- ',np.sum(train.isnull().sum() > 0))\nprint('number of null columns in test set :-',np.sum(test.isnull().sum() > 0 ))","54de06a3":"features = [f'cont{x}'for x in range(1,15)]\ntarget = ['target']","a2eaf8ab":"all_data = train[features+['id']].append(test)","afa3c66b":"all_data.head()","231ddf30":"def plot_distribution(ds,feature,color):\n    plt.figure(dpi=125)\n    sns.distplot(ds[feature],color=color);\n    print(\"{}Max value of {} is {}\\n{}Min value of {} is {}\\n{}Mean value of {} is {}\\n{}Std value of {} is {}\\n{}Median value of {} is {}\".format(red,feature,ds[feature].max(),blu,feature,ds[feature].min(),grn,feature,ds[feature].mean(),ylw,feature,ds[feature].std(),wht,feature,ds[feature].median()));","00588d84":"def plot_grid(ds,c1,c2,c3):\n    f = sns.PairGrid(ds[:100]);\n    plt.figure(figsize=(10,10));\n    f.map_upper(plt.scatter,color = c1);\n    f.map_lower(sns.kdeplot,color = c2);\n    #f.map_diag(sns.histplot,color = c3 );\n    f.map_diag(sns.kdeplot, lw=3, legend=False,color = c3);","150451e7":"plot_grid(all_data[features],'lightgreen','magenta','red');","4a6359b0":"plot_grid(test,'limegreen','darkmagenta','seagreen');","da8e22e9":"corr1,corr2,corr3 = train[features].corr(),train[features].corr(method='pearson'),train[features].corr(method='spearman');\nplt.figure(figsize=(12,9));\nfig = ex.imshow(corr1);\nfig.show();","b79683c8":"plt.figure(figsize=(12,9))\nfig2 = sns.heatmap(corr2, annot=True, cmap=\"mako\")\nplt.show()","ce51eece":"plt.figure(figsize=(12,9))\nfig2 = sns.heatmap(corr3, annot=True, cmap=\"rocket_r\")\nplt.show()","963a14a8":"plt.figure(figsize=(30,15))\nplt.subplot(3,5,1)\nsns.kdeplot(train['cont1'],color='#4285F4',shade=True,alpha=0.8);\nplt.subplot(3,5,2)\nsns.kdeplot(train['cont2'],color='#7D0552',shade=True,alpha=0.8);\nplt.subplot(3,5,3)\nsns.kdeplot(train['cont3'],color='#7FFFD4',shade=True,alpha=0.8);\nplt.subplot(3,5,4)\nsns.kdeplot(train['cont4'],color='#C04000',shade=True,alpha=0.8);\nplt.subplot(3,5,5)\nsns.kdeplot(train['cont5'],color='#4285F4',shade=True,alpha=0.8);\nplt.subplot(3,5,6)\nsns.kdeplot(train['cont6'],color='#F88017',shade=True,alpha=0.8);\nplt.subplot(3,5,7)\nsns.kdeplot(train['cont7'],color='#4285F4',shade=True,alpha=0.8);\nplt.subplot(3,5,8)\nsns.kdeplot(train['cont8'],color='#78C7C7',shade=True,alpha=0.8);\nplt.subplot(3,5,9)\nsns.kdeplot(train['cont9'],color='#728C00',shade=True,alpha=0.8);\nplt.subplot(3,5,10)\nsns.kdeplot(train['cont10'],color='#254117',shade=True,alpha=0.8);\nplt.subplot(3,5,11)\nsns.kdeplot(train['cont11'],color='#6CC417',shade=True,alpha=0.8);\nplt.subplot(3,5,12)\nsns.kdeplot(train['cont12'],color='#CCFB5D',shade=True,alpha=0.8);\nplt.subplot(3,5,13)\nsns.kdeplot(train['cont13'],color='#6A287E',shade=True,alpha=0.8);\nplt.subplot(3,5,14)\nsns.kdeplot(train['cont14'],color='#E3319D',shade=True,alpha=0.8);\nplt.subplot(3,5,15)\nsns.kdeplot(train['target'],color='#E3319D',shade=True,alpha=0.8);","78d85f3d":"plot_distribution(train,'target','blue')","8409dd44":"lesser,greater = train[train['target'] <= train['target'].median()],train[train['target'] >= train['target'].median()] ","6abbe2dd":"plt.figure(figsize=(30.8,15))\nplt.subplot(3,5,1)\nsns.kdeplot(lesser['cont1'],color='#4285F4',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont1'],color='#4F2083',shade=True,alpha=0.8);\nplt.subplot(3,5,2)\nsns.kdeplot(lesser['cont2'],color='#7D0852',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont2'],color='#43A45F',shade=True,alpha=0.8);\nplt.subplot(3,5,3)\nsns.kdeplot(lesser['cont3'],color='#350836',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont3'],color='#7FFFD4',shade=True,alpha=0.8);\nplt.subplot(3,5,4)\nsns.kdeplot(lesser['cont4'],color='#C08408',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont4'],color='#DA6DBD',shade=True,alpha=0.8);\nplt.subplot(3,5,5)\nsns.kdeplot(lesser['cont5'],color='#4285F4',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont5'],color='#FD376A',shade=True,alpha=0.8);\nplt.subplot(3,5,6)\nsns.kdeplot(lesser['cont6'],color='#F88017',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont6'],color='#508352',shade=True,alpha=0.8);\nplt.subplot(3,5,7)\nsns.kdeplot(lesser['cont7'],color='#4285F4',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont7'],color='#1FCDCB',shade=True,alpha=0.8);\nplt.subplot(3,5,8)\nsns.kdeplot(lesser['cont8'],color='#A3FC08',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont8'],color='#78C7C7',shade=True,alpha=0.8);\nplt.subplot(3,5,9)\nsns.kdeplot(lesser['cont9'],color='#EDEE2C',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont9'],color='#728C08',shade=True,alpha=0.8);\nplt.subplot(3,5,10.8)\nsns.kdeplot(lesser['cont10'],color='#254117',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont10'],color='#9717A9',shade=True,alpha=0.8);\nplt.subplot(3,5,11)\nsns.kdeplot(lesser['cont11'],color='#6CC417',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont11'],color='#872B93',shade=True,alpha=0.8);\nplt.subplot(3,5,12)\nsns.kdeplot(lesser['cont12'],color='#C13408',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont12'],color='#CCFB5D',shade=True,alpha=0.8);\nplt.subplot(3,5,13)\nsns.kdeplot(lesser['cont13'],color='#6A287E',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont13'],color='#CCFB5D',shade=True,alpha=0.8);\nplt.subplot(3,5,14)\nsns.kdeplot(lesser['cont14'],color='#E3319D',shade=True,alpha=0.2);\nsns.kdeplot(greater['cont14'],color='#203E8B',shade=True,alpha=0.8);\nplt.subplot(3,5,15)\nsns.kdeplot(lesser['target'],color='#D8AB59',shade=True,alpha=0.2);\nsns.kdeplot(greater['target'],color='#C61923',shade=True,alpha=0.8);","ad4c3f1a":"train = train[train['target'] > 0]","92db4b81":"plot_distribution(train,'target','purple')","f85ef90f":"from torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import MinMaxScaler\ntorch.manual_seed(7)","fb0c46ad":"def rmse_score(yhat, yreal):\n    return (sqrt(mean_squared_error(yreal, yhat.detach().numpy())))","aea84728":"from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom math import sqrt","90927eeb":"class TPSnn(nn.Module):\n    def __init__(self,in_size,hidden_size1,hidden_size2,hidden_size3,hidden_size4,num_classes):\n        super().__init__()\n        self.linear1 = nn.Linear(in_size, hidden_size1)\n        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n        self.linear3 = nn.Linear(hidden_size2,hidden_size3)\n        self.linear4 = nn.Linear(hidden_size3,hidden_size4)\n        self.linear5 = nn.Linear(hidden_size4,num_classes)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self,xb):\n        xb = xb.view(-1,xb.size(1)).float()\n        #xb = xb.float()\n        out = self.linear1(xb)\n        #out = self.dropout(out)\n#         out = F.relu(out)\n        out = self.linear2(out)\n        #out = self.dropout(out)\n        #out = F.relu(out)\n        out = self.linear3(out)\n        #out = self.dropout(out)\n        out = F.relu(out)\n#         out = self.linear4(out)\n#         out = self.dropout(out)\n#         #out = F.relu(out)\n#         out = self.linear5(out)\n#         out = F.relu(out)\n        return out\n    \n    def training_step(self,batch):\n        features, labels = batch\n        out = self(features)\n#         loss_x = F.mse_loss\n#         loss = loss_x(out,labels)\n        loss_x = nn.MSELoss()\n        loss = torch.sqrt(loss_x(out,labels))\n        return loss\n    \n    def validation_step(self,batch):\n        features,labels = batch\n        out = self(features)\n#         loss_x = F.mse_loss\n#         loss = (loss_x(out,labels))\n        loss_x = nn.MSELoss()\n        loss = torch.sqrt(loss_x(out,labels))\n        return {'RMSE': loss}\n\n    def validation_epoch_end(self,outputs):\n        batch_losses = [x['RMSE'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n#         batch_accs = [x['valid_acc'] for x in outputs]\n#         epoch_acc = torch.stack((batch_accs).t).mean()\n        return {'RMSE': epoch_loss.item()}\n\n    def epoch_end(self,epoch,result):\n        print(\"Epoch [{}], RMSE: {:.4f}\".format(epoch, result['RMSE']))#, result['valid_acc']","e4c1a323":"def convert_to_tensors(ds, valid_size = 0.30,train_set=True):\n    \n    batch_size = 60000#int(ds.shape[0]\/\/3.5)\n    scaler = MinMaxScaler()\n    \n    if(train_set == True):\n        \n        targets_t = ds.target.values\n        features_t = ds.drop(labels = ['target'],axis = 1).values\n        \n        features_t = scaler.fit_transform(features_t)\n        \n        targetsTrain = torch.from_numpy(targets_t).type(torch.FloatTensor)\n        featuresTrain = torch.from_numpy(features_t)\n        \n        train_tensor = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n        \n        valid_size = round(len(train_tensor) * (valid_size))\n        train_size = len(train_tensor) - valid_size\n\n        ttrain_ds, tvalid_ds = random_split(train_tensor, [train_size, valid_size])\n        \n        print(\"train_size:- \",len(ttrain_ds),\"test_size:- \", len(tvalid_ds))\n              \n        train_loader = DataLoader(ttrain_ds, batch_size = batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        valid_loader = DataLoader(tvalid_ds, batch_size = batch_size, num_workers=4, pin_memory=True)\n              \n        return train_loader,valid_loader\n    \n    else:\n        \n        featuresTest = ds.values\n        featuresTest = scaler.fit_transform(featuresTest)\n        featuresTensor = torch.from_numpy(featuresTest)\n        print(len(featuresTensor))\n        test_loader = DataLoader(featuresTensor, batch_size*2, num_workers=4, pin_memory = True)\n        \n        return test_loader","5e1dd060":"train.shape","d46f625c":"train_loader,valid_loader = convert_to_tensors(train)","afd7885e":"input_size = train.shape[1] -1\nhidden_size1 = 128\nhidden_size2 = 1024\nhidden_size3 = 64\nhidden_size4 = 8\nnum_classes = 1","ae230b76":"model = TPSnn(input_size, hidden_size1=hidden_size1, hidden_size2= hidden_size2,hidden_size3 = hidden_size3,hidden_size4 = hidden_size4,num_classes = num_classes)\nmodel","043874be":"for t in model.parameters():\n    print(t.shape)","2967e547":"for xb,yb in train_loader:\n    print(xb,'yb',yb)\n    break","f5ad01eb":"for data, labels in train_loader:\n    outputs = model(data)\n    print(labels.shape)\n    loss_x = nn.MSELoss()\n    loss = torch.sqrt(loss_x(outputs,labels))\n    #acc = rmse_score(outputs,labels)\n    print('Loss:', loss.item())\n    #print('Initial Acc:',float(acc),\"%\")\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:10].data)\nprint('Sample labels :\\n', labels[:10].data)","5cd101ce":"def evaluate(model, valid_loader):\n    outputs = [model.validation_step(batch) for batch in valid_loader]\n    return model.validation_epoch_end(outputs)","a414367f":"def fit(epochs, lr, model, train_loader, valid_loader, opt_func = torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(),lr)\n    for epoch in tqdm(range(epochs)):\n        # Training ==>\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        # Validation ==>\n        result = evaluate(model,valid_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","2bb0fdc9":"model = TPSnn(input_size,hidden_size1=hidden_size1, hidden_size2= hidden_size2, hidden_size3= hidden_size3,hidden_size4 = hidden_size4,num_classes = num_classes)\nmodel","ccc0a6d6":"history = [evaluate(model,valid_loader)]\nhistory","65452e6b":"history += fit(12, 0.02, model, train_loader, valid_loader) #Learning Rate = 0.5","8177ea47":"history += fit(20, 0.0001, model, train_loader, valid_loader)","04f8967a":"history += fit(60, 0.00001, model, train_loader, valid_loader)","e98f20a6":"#history += fit(60, 0.00003, model, train_loader, valid_loader)","abe7aa8f":"for xb,yb in valid_loader:\n    print(xb,'yb',yb)","5799b142":"def predict(model,preds,train=False):\n    y = torch.FloatTensor()\n    if train == True:\n        for pred,lbl in preds:\n            y_ = model(pred)\n            y = torch.cat((y,y_),dim=0)\n        return y\n    if train != True:\n        for pred in preds:\n            y_ = model(pred)\n            y = torch.cat((y,y_),dim=0)\n        return y","f2362754":"test_loader = convert_to_tensors(test,train_set = False)","3979dcce":"train_preds = predict(model,train_loader,train=True)","3e698ba8":"train_preds2 = predict(model,valid_loader,train=True)","c9b56f75":"f'{train_preds.size()},{train_preds2.size()}'","1aceb785":"train_preds = torch.cat((train_preds,train_preds2),dim=0)","e90bf30c":"train_preds = train_preds.detach().cpu().numpy()","90b6c824":"train['t_preds'] = train_preds","ab772b89":"sns.jointplot(x = 'target', y = 't_preds',data = train,joint_kws={'alpha' : 0.3},color = 'goldenrod');\nplt.show()","d30128e7":"preds = predict(model,test_loader)","75ad6c55":"preds = preds.detach().cpu().numpy()","f1a07cdd":"preds = preds.reshape(-1)","6dba0e4c":"sub = pd.DataFrame({'id':test.id, 'target':preds})\nsub.to_csv('submission.csv',index=False)","9dce0baf":"sub.head()","a163ea55":"x = pd.read_csv('.\/submission.csv')\nx.head()","4c6902f5":"# EDA \ud83d\udcca\ud83d\udcca","a65147e3":"# Importing libraries\ud83d\udcda","c9a7fa3f":"#### As we can see it was an outlier","999ee23b":"### Train Data","d5b1adee":"## Model","e9f7de97":"# PyTorch Neural Network\u2764\ufe0f\u200d\ud83d\udd25","a0abadcf":"### Test Data","2599a784":"We see that there are no null values in train and test set","b33c230a":"# Trying some things on target","a0f2b7a8":"![rmse.JPG](attachment:rmse.JPG)","847cc30a":"We can see some similarities between some of the features\ud83e\udd14\ud83e\udd14","69ce1562":"cont 6 to cont 13 are somewhat related.. not much though","bd9a6a48":"# My Submission \ud83d\ude4b\u200d\u2642\ufe0f","27cea446":"# Getting data \ud83d\udcbd","ba898b0b":"##  Correlation Matrix \ud83c\udfb5\ud83c\udfb5","9aef6293":"## Metrics \ud83d\udcd0","5f0f9fed":"## Feature Distibutions \ud83d\udcc8\ud83d\udcc8","14b97215":"## A lot of work is remaining to be done here \ud83d\ude05\ud83d\ude05...i have lots of things to try here\ud83d\ude0b\ud83d\ude0b.. lots of improvements to make\ud83d\udd27\ud83d\udd27.. i'll get to it as soon as i can\u23f1\ufe0f\u23f1\ufe0f.. ","19944f95":"We see 2 spikes from the median of the distribution. We can split the dataset from the medain. Suggested by [maunish](https:\/\/www.kaggle.com\/maunish).","ae1ccfb7":"#### I think 0.0 value for target is an outlier..","2d39e064":"## preds","90a6cb28":"## Pair Grid","cb50ea58":"# Work in Progress\ud83d\udea7\ud83d\udea7"}}