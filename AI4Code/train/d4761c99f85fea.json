{"cell_type":{"9849c27e":"code","930016ae":"code","726afb53":"code","7d484093":"code","2ffe0fe8":"code","863bea10":"code","c5a83821":"code","26edf3c0":"code","61046d23":"code","4d7d8155":"code","7e362cbe":"code","532e1050":"code","40ecdbdf":"code","83296338":"code","716ba2af":"code","450e2855":"code","8eaf1c47":"code","2bf38594":"code","4ac92520":"code","ede02193":"code","f3a48643":"code","5b87a746":"code","ca7bc4f5":"code","e95e0a05":"code","67ae7955":"code","5bd4f568":"code","4ca067e9":"code","e1f71470":"code","d4504d7f":"code","f437089a":"code","6f64c79d":"code","78f3140e":"code","3456fb9d":"code","5a8d9c37":"code","29c71ceb":"code","eb100b8c":"code","43fca602":"code","d5416996":"code","6d7f7f48":"code","35f90d19":"code","449d2873":"code","356a725c":"markdown","cfe26e64":"markdown","c0e83b7c":"markdown","83f00f37":"markdown","fcf8e674":"markdown","2d7a168f":"markdown","556bcae6":"markdown","29061167":"markdown","97cd45ef":"markdown","2d6306be":"markdown","5db331d4":"markdown","7359c227":"markdown","f8b36ed6":"markdown","5ca2625a":"markdown"},"source":{"9849c27e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","930016ae":"import matplotlib.pyplot as plt\nimport seaborn as sns","726afb53":"df=pd.read_csv(\"..\/input\/cancer-classification\/cancer_classification.csv\")\ndf.head()","7d484093":"df.info()\n#There is not any null values in our dataset","2ffe0fe8":"df.isnull().sum()\n#Here I check again whether there are null values or not","863bea10":"df.describe()\n#Here we can fin statistical information about the data","c5a83821":"plt.figure(figsize=(12,8))\nsns.set_style(\"whitegrid\")\nsns.countplot(\"benign_0__mal_1\",data=df)\n#This column is our target to predict and there are around 360 malignant and 210 benign tumors in the target column","26edf3c0":"df.corr()\n#Here we can see the mutual correlation values between columns","61046d23":"plt.figure(figsize=(15,10))\nmask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(),cmap=\"jet\", annot=True, mask = mask,linewidths=0.2)\n#we can easliy see that there is a negative correlation between the target column and the mojority of the feature columns","4d7d8155":"df.corr()[\"benign_0__mal_1\"].sort_values(ascending=False)\n#Here we can see the correlation values between the target column and the features\n#This means that we will get good predictions with little errors","7e362cbe":"df.corr()[\"benign_0__mal_1\"][:-1].sort_values(ascending=False).plot(kind=\"bar\",figsize=(15,10),color=\"green\")\n#we can see better the correlations with this bar plot","532e1050":"X=df.drop(\"benign_0__mal_1\", axis=1) #I assign every column as features apart from benign_0__mal_1 column\nX_deep=df.drop(\"benign_0__mal_1\", axis=1).values # I make a separate X set for deep learning because it requires numpy array\ny=df[\"benign_0__mal_1\"]\ny_deep=df[\"benign_0__mal_1\"].values","40ecdbdf":"X # This is for normal machine algorithms","83296338":"X_deep#  this is for machine learning algorithm","716ba2af":"from sklearn.model_selection import train_test_split","450e2855":"X_deep_train, X_deep_test, y_deep_train,y_deep_test=train_test_split(X_deep, y_deep, test_size=0.25)","8eaf1c47":"from sklearn.preprocessing import MinMaxScaler","2bf38594":"scaler=MinMaxScaler()","4ac92520":"X_deep_train=scaler.fit_transform(X_deep_train) # here we fit and transform our X dataset with the scaler\nX_deep_test=scaler.fit_transform(X_deep_test)","ede02193":"X_deep_train #Here is the scaled version of the dataset","f3a48643":"X_deep_train.shape","5b87a746":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n#Here we import necessary units to create a basic deep learning algorithm","ca7bc4f5":"model1=Sequential()\nmodel1.add(Dense(units=30, activation=\"relu\")) #Here we select units as 30 because we have 30 features to train\nmodel1.add(Dense(units=15, activation=\"relu\"))\nmodel1.add(Dense(units=1, activation=\"sigmoid\"))# this is the last layer and we select sigmoid function as the function\n#because we will predict a binary class\nmodel1.compile(loss=\"binary_crossentropy\",optimizer=\"adam\") #This configures model for training\n","e95e0a05":"X_deep_train.shape","67ae7955":"X_deep_test.shape","5bd4f568":"y_deep_train.shape","4ca067e9":"y_deep_test.shape","e1f71470":"model1.fit(x=X_deep_train,y=y_deep_train,epochs=300, validation_data=(X_deep_test, y_deep_test))","d4504d7f":"pd.DataFrame(model1.history.history)\n#Here we can see the comparison of our loss both in the training data and the validation data","f437089a":"pd.DataFrame(model1.history.history).plot(figsize=(15,10))\n#Here we plot both our training data and validation data to detect overfitting\n#There seem a overfiting in our model, it means too many epochs and we need to decrease it","6f64c79d":"from tensorflow.keras.callbacks import EarlyStopping\n# We will use EarlyStopping when the algorithm began to detrail from the validation data","78f3140e":"early_stop=EarlyStopping(monitor=\"val_loss\", patience=24, verbose=1, mode=\"min\")\n# Here we select mode as minimum because we want to minimize the validation loss,it is higher as we can see in the plot above\n#Patience means even if the algorithm encounters a divergence, it will wait 24 apochs more to be sure about stoppimg training","3456fb9d":"#we have redefine our model from the beginning\nmodel2=Sequential()\nmodel2.add(Dense(units=30, activation=\"relu\")) \nmodel2.add(Dense(units=15, activation=\"relu\"))\nmodel2.add(Dense(units=1, activation=\"sigmoid\")) \nmodel2.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")","5a8d9c37":"# here we will add our early stop into callbacks parameter\nmodel2.fit(x=X_deep_train,y=y_deep_train,epochs=300, validation_data=(X_deep_test, y_deep_test), callbacks=[early_stop])","29c71ceb":"pd.DataFrame(model2.history.history).plot(figsize=(15,10))\n# in the plot below we can see that early stop worked good before our model began to diverge too much ","eb100b8c":"from tensorflow.keras.layers import Dropout","43fca602":"model3=Sequential()\nmodel3.add(Dense(units=30, activation=\"relu\"))\nmodel3.add(Dropout(0.5)) # we can choose the precentage between 0 and 100, 0.5 represents %50 of the layer\nmodel3.add(Dense(units=15, activation=\"relu\"))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(units=1, activation=\"sigmoid\")) \nmodel3.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")\nmodel3.fit(x=X_deep_train,y=y_deep_train,epochs=300, validation_data=(X_deep_test, y_deep_test), callbacks=[early_stop])\n# we will also use early stop as we did in the previous model","d5416996":"pd.DataFrame(model3.history.history).plot(figsize=(15,10))\n#Our model performs better than previous models thanks to dropout","6d7f7f48":"predictions1=model1.predict_classes(X_deep_test) \npredictions2=model2.predict_classes(X_deep_test)#this model is the our model's performance with early stop\npredictions3=model3.predict_classes(X_deep_test)# This model represents our model with early stop and dropout layer","35f90d19":"from sklearn.metrics import classification_report, confusion_matrix","449d2873":"print(\"The Report of Model 1: \", classification_report(y_deep_test, predictions1))\nprint(\"The Confusion Table of Model 1:\", confusion_matrix(y_deep_test,predictions1))\nprint(\"*******************************************************************\")\nprint(\"The Report of Model 2: \", classification_report(y_deep_test, predictions2))\nprint(\"The Confusion Table of Model 2:\", confusion_matrix(y_deep_test,predictions2))\nprint(\"*******************************************************************\")\nprint(\"The Report of Model 3: \", classification_report(y_deep_test, predictions3))\nprint(\"The Confusion Table of Model 3:\", confusion_matrix(y_deep_test,predictions3))\nprint(\"*******************************************************************\")\n","356a725c":"Here we will compare the performances of the models we have used above","cfe26e64":"# 2. Splitting and Training the Machine Learning Algorithms:","c0e83b7c":"For a binary classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","83f00f37":"We have to recreate out model in order to see the difference","fcf8e674":"As we can see above, the early stopp happened in the 73 epoch, so we do not need to worry about how many epochs we should assign when we use early stop","2d7a168f":"There other tecniques to prevent overfitting, one of them is adding dropout layers into our model that will drop percentage of neurons randomly ","556bcae6":"Thanks to dropout, our model could train until 131. epoch","29061167":"In the reports above we can easily see that the last model with dropout layers and early stop outperform better than the others with %97 accuracy and only 5 FalsePositives compared to %96 accuracy and 1 False neagtive and 5 False Positive of the 2. Model and %92 Accuracy and 7 False Negatives and 4 False Positives of the 1. Model. In predicting cancer tumors as benign and malignant it is very important to have 0 False Negative because we do not want to say a pation that the person has not the malignant cancer tumor althoug he has, it is very dangerous. Therefore, the model3 is very good because it has 0 False Negative prediction","97cd45ef":"# 3. Evaluation of Performances of Our Models","2d6306be":"Now we will scale our X train set before applying the algorithm ","5db331d4":"Firstly we will train our deep learning algorithm and later we will also other machine learning algorithms for classification","7359c227":"# 1. Exploratory Data Analysis:","f8b36ed6":"\n |  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n |  at each step during training time, which helps prevent overfitting.\n |  Inputs not set to 0 are scaled up by 1\/(1 - rate) such that the sum over\n |  all inputs is unchanged.\n \n Fraction of the input units to drop.","5ca2625a":"EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n |  \n |  Stop training when a monitored metric has stopped improving."}}