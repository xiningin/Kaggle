{"cell_type":{"c0bef8fc":"code","daa03653":"code","b3b8917d":"code","eef42036":"code","817907eb":"code","724d9959":"code","ec91959f":"code","c4301981":"code","aef2a5cb":"code","3d3f1e6e":"code","bc388781":"code","d19d5bd9":"code","05a804ce":"code","62914148":"code","202e1210":"code","d2db8d95":"code","17913a11":"code","0c0d7ece":"code","0efa4483":"code","c50cf630":"code","bfd38370":"code","9f6ba31f":"code","ab39b30f":"code","c668d5d1":"code","1c24e9ab":"code","9247deb4":"code","63c4376b":"code","9b915682":"code","c754434f":"code","2f551615":"code","b5564d7d":"code","6becedae":"code","ff17b114":"code","011d0b01":"code","c404ed53":"code","2dfa3595":"code","a8341924":"code","c2349f4a":"code","c5e8e480":"code","8ea90d86":"code","ff60f42b":"code","5652bf9b":"code","f18aef2b":"markdown","321e6e22":"markdown","38db1d01":"markdown","80cbcffd":"markdown","d307aa9a":"markdown","7fc8cf64":"markdown","bab389bc":"markdown","af9dc7b1":"markdown"},"source":{"c0bef8fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","daa03653":"%config Completer.use_jedi = False #AutoCompletion","b3b8917d":"data = pd.read_csv(\"\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv\", sep='\\t')\ndata.head()","eef42036":"print(len(data.columns))\ndata.columns","817907eb":"data.info()","724d9959":"data['Dt_Customer'] =pd.to_datetime(data['Dt_Customer'])","ec91959f":"data.describe().T","c4301981":"print(data.shape)\nprint(\"After performing some transformations\")\ntran_data= data.copy()\ntran_data = tran_data[tran_data['Income'].notnull()]\n\ntran_data['Year'] = tran_data['Dt_Customer'].apply(lambda row:row.year)\n\n\ntran_data['Children'] = tran_data['Kidhome']+ tran_data['Teenhome']\ntran_data['Age'] = pd.Timestamp('now').year- tran_data['Year_Birth']\n\ntran_data = tran_data.drop(['ID','Z_CostContact', 'Z_Revenue', 'Kidhome', 'Teenhome', 'Dt_Customer', 'Year', 'Year_Birth'], axis=1)\n\nprint(tran_data.shape)","aef2a5cb":"tran_data['Education'].value_counts()","3d3f1e6e":"#regrouping EDucaation as Fully Graduated and under_graduated\ntran_data.Education = tran_data.Education.replace(['PhD','Graduation', 'Master'], 'fully_Graduated')\n\ntran_data.Education = tran_data.Education.replace(['Basic', '2n Cycle'], 'under_Graduated')\ntran_data['Education'].value_counts()","bc388781":"tran_data['Marital_Status'].value_counts()","d19d5bd9":"tran_data.Marital_Status = tran_data.Marital_Status.replace(['Married','Together'], 'Partner')\n\ntran_data.Marital_Status = tran_data.Marital_Status.replace(['Single','Divorced', 'Widow', 'Alone','Absurd', 'YOLO'], 'Single')\ntran_data['Marital_Status'].value_counts()","05a804ce":"sns.boxplot(tran_data['Income'])\n","62914148":"#removing Income >200000\ntran_data=tran_data[tran_data['Income']<200000]\ntran_data.shape","202e1210":"#Calling all the amount on commmodities as expense\ncols = [i for i in tran_data.columns if str(i).startswith('Mnt')]\nprint(cols)\ntran_data['Expense'] = tran_data[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum(axis=1)\ntran_data=tran_data.drop(cols, axis=1)\ntran_data['Expense']","d2db8d95":"#response to campaign in column\nxgb_data = tran_data.copy()\ncols_response= [i for i in tran_data.columns if str(i).startswith('Accept')]\ncols_response= cols_response +['Response']\ntran_data['Responses'] = tran_data[cols_response].sum(axis = 1)\n\n# if there is any responses it would be 1 otherwise it would be 0\ndef mapp(num):\n    if num >=1:\n        result = 1\n    else:\n        result = 0\n    return result\n\n\ntran_data['Responses'] = tran_data['Responses'].apply(mapp)\ntran_data= tran_data.drop(cols_response, axis=1)\ntran_data['Responses'].value_counts()","17913a11":"tran_data.head().T","0c0d7ece":"# Replacing the age with the age groups\n\ndef age_category(age):\n    if  25<=age <= 35:\n        age = 0\n    elif 35 < age <= 45:\n        age = 1\n    elif 45 < age <= 55:\n        age = 2\n    elif 55 < age <= 65:\n        age = 3\n    elif 65 < age <= 75:\n        age = 4\n    elif age > 75:  \n        age = 5\n    return age  \n\ntran_data.Age = tran_data.Age.apply(age_category)\ntran_data.Age.unique()\n\nxgb_data.Age = xgb_data.Age.apply(age_category)\nxgb_data.Age.unique()","0efa4483":"#Dropping unwanted Columns 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth'\ndrop_cols= ['NumDealsPurchases', 'NumWebPurchases',\n       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\ntran_data.drop(drop_cols, axis = 1,inplace = True)\nxgb_data.drop(drop_cols, axis = 1,inplace = True)","c50cf630":"#scaling numerical and categorical data\ncat_cols= tran_data.select_dtypes('object').columns \nprint(cat_cols)\ntran_data =pd.get_dummies(columns=cat_cols,data=tran_data)\nxgb_data =pd.get_dummies(columns=cat_cols,data=xgb_data)\n\n","bfd38370":"plt.figure(figsize= (15,15))\nsns.heatmap(tran_data.corr(), annot=True)","9f6ba31f":"tran_data.nunique()","ab39b30f":"from sklearn.preprocessing import StandardScaler\nto_scale= ['Income', 'Recency', 'Expense']\nX_Scaled =StandardScaler().fit_transform(tran_data[to_scale])\nX_Scaled[:5,:]","c668d5d1":"X_Scaled =pd.DataFrame(X_Scaled, columns =to_scale)\nfinal_data = pd.concat((tran_data.drop(to_scale, axis=1), X_Scaled), axis=1)\nfinal_data.head()\n","1c24e9ab":"tran_data.info()","9247deb4":"final_data.dropna(inplace=True)","63c4376b":"features = ['Income', 'Age', 'Expense']","9b915682":"from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\n\nmodel = KMeans(init = 'k-means++')\nvisualizer = KElbowVisualizer(model, k = 10, random_state = 42)\n\nvisualizer.fit(final_data[['Income',  'Expense']])\nvisualizer.show()","c754434f":"model = KMeans(n_clusters=4, init='k-means++', random_state=42).fit(final_data[['Income', 'Expense']])\npreds = model.predict(final_data[['Income', 'Expense']])\ndt_Kmeans = final_data[['Income','Expense']]\ndt_Kmeans['Cluster'] = preds","2f551615":"plt.figure(figsize=(10,7))\nsns.boxplot(data=dt_Kmeans, x='Cluster', y = 'Income');\nplt.xlabel('Cluster', fontsize=20, labelpad=20)\nplt.ylabel('Income', fontsize=20, labelpad=20)\nplt.title(\"PLot Showing Income distribution for different clusters\")","b5564d7d":"plt.figure(figsize=(10,7))\nsns.boxplot(data=dt_Kmeans, x='Cluster', y = 'Expense');\nplt.xlabel('Cluster', fontsize=20, labelpad=20)\nplt.ylabel('Expense', fontsize=20, labelpad=20)\nplt.title(\"PLot Showing Expense distribution for different clusters\")","6becedae":"plt.figure(figsize=(10,7))\nsns.scatterplot(data=dt_Kmeans, x='Income', y='Expense', hue='Cluster');\nplt.xlabel('Income', fontsize=20, labelpad=20)\nplt.ylabel('Total Expense', fontsize=20, labelpad=20);","ff17b114":"import xgboost as xgb","011d0b01":"xgb_data.head()","c404ed53":"cat_cols= xgb_data.select_dtypes('object').columns \nxgb_data =pd.get_dummies(columns=cat_cols,data=xgb_data)\n\nto_scale= ['Income', 'Recency', 'Expense']\nX_Scaled =StandardScaler().fit_transform(xgb_data[to_scale])\nX_Scaled[:5,:]\n\nX_Scaled =pd.DataFrame(X_Scaled, columns =to_scale)\nfinal_xgb_data = pd.concat((xgb_data.drop(to_scale, axis=1), X_Scaled), axis=1)\nfinal_xgb_data.head()","2dfa3595":"final_xgb_data.dropna(inplace=True)","a8341924":"xi = final_xgb_data.drop(['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response'], axis=1)\nyi = final_xgb_data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']]","c2349f4a":"from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import f1_score","c5e8e480":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(xi, yi.iloc[:,0], test_size=0.2, random_state=7)\nxgb_train = xgb.DMatrix(x_train, label=y_train)\nxgb_test = xgb.DMatrix(x_test, label=y_test)","8ea90d86":"y1= yi.iloc[:,0]\ny2= yi.iloc[:,1]\ny3= yi.iloc[:,2]\ny4= yi.iloc[:,3]\ny5= yi.iloc[:,4]\ny6= yi.iloc[:,5]\nlist_prom=[y1,y2,y3,y4,y5, y6]","ff60f42b":"import shap\n\nshap_values_lis=[]\nfor i in range (0, len(list_prom)):\n    params = {\n    \"eta\": 0.002,\n    \"max_depth\": 3,\n    \"objective\": \"survival:cox\",\n    \"subsample\": 0.5\n    }\n    X_train, X_test, y_train, y_test = train_test_split(xi, list_prom[i], test_size=0.2, random_state=7)\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_test = xgb.DMatrix(X_test, label=y_test)\n    model_train = xgb.train(params, xgb_train, 10000, evals = [(xgb_test, \"test\")], verbose_eval=1000)\n    shap_values = shap.TreeExplainer(model_train).shap_values(xi)\n    shap_values_lis.append(shap_values)","5652bf9b":"for i in range (0, len(shap_values_lis)):\n    print(\"Shap values for AcceptedCmp{} \".format(i))\n    shap.summary_plot(shap_values,shap_values_lis[i], feature_names=xi.columns)","f18aef2b":"* Income Column contains 24 null valued rows, we can remove these rows\n* KidHome and TeenHome could be combined\n* Z_CostContact and Z_revenue column doesn't contains any variability, hence we can remove these cols. \n* Age feature can be added by calculating difference between Year- YearBirth","321e6e22":"Build a classification model (binomial or multinomial - recommended: xgboost python implementation) on the promotion done by the company. Run SHAP analysis on the model results, and write a short text of what would be your recommendation to business for the next round of campaigns.","38db1d01":"# 2 Build a classification model (binomial or multinomial - recommended: xgboost python implementation) on the promotion done by the company.","80cbcffd":"# Build unsupervised Algorithm to cluster customer data","d307aa9a":"In the above plot we have developed multiple cluster mostly based on the income and Expense of the customer , like high income and high Expense customer , high income and low spending customer , low income low spending customer and a very few points of customers\nwho are low income and high spending customers .\nThis gives us various segments of Customers based on their income and Spending.","7fc8cf64":"We can see that with the SHAP values for each label with respect to the features modelled:\n1. Education, Complain, Marital_status_Single have low impact on the model.\n2. Income, Recency and Expense have huge positive impact on Model.","bab389bc":"The number of optimum clusters are 4.","af9dc7b1":"Looking into the heat map it is clear that Columns with the nature of expences purchases a higher correlation with the income"}}