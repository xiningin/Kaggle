{"cell_type":{"9c91faf0":"code","b2f2275e":"code","a07658f8":"code","5ad0960a":"code","11203db9":"code","e7c4760d":"code","c8123541":"code","0663f909":"code","9e18df6d":"code","4dd7c858":"code","a1c1c6d9":"code","f7fc89ad":"code","9b48aef6":"code","8ebd08ac":"code","92d0d33e":"code","a3f2de65":"code","8b341c72":"code","272b7282":"code","eb03ed82":"code","a13c3231":"code","8b28eaed":"code","738071e3":"code","33e12580":"code","017c0b86":"markdown","46df0c31":"markdown"},"source":{"9c91faf0":"# Importing libraries\nimport os\nimport random\nimport gc\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport catboost as cb\n\nimport tensorflow as tf\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate, BatchNormalization\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\n\nwarnings.filterwarnings('ignore')","b2f2275e":"# Importing the training data\ntrain_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain_df.head()","a07658f8":"# Importing the test data\ntest_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_id = test_df.Id.copy()\ntest_df.head()","5ad0960a":"# Setting seeds\nnp.random.seed(123)\nrandom.seed(123)\ntf.random.set_seed(123)\n\n# Setting constants\nbatch_size = 32\nimage_size = 224\nchannels = 3\nshuffle_size = 1024 \n\n# Setting auto tune\nAUTOTUNE = tf.data.experimental.AUTOTUNE  ","11203db9":"# Splittin the training data into training and validation sets\n# Train_df, Val_df = train_test_split(train_df, test_size=0.09, random_state=123)\nTrain_df = train_df.iloc[:9000,:]\nVal_df = train_df.iloc[9000:,:]","e7c4760d":"# Mapping the images ID to the image paths\nTrain_df.Id = Train_df.Id.map(lambda x: '..\/input\/petfinder-pawpularity-score\/train\/' + x + '.jpg')\nVal_df.Id = Val_df.Id.map(lambda x: '..\/input\/petfinder-pawpularity-score\/train\/' + x + '.jpg')\ntest_df.Id = test_df.Id.map(lambda x: '..\/input\/petfinder-pawpularity-score\/test\/' + x + '.jpg')","c8123541":"# Creating training and validation datasets on tensorflow format\ntrain_ds = Dataset.from_tensor_slices((Train_df['Id'].values,Train_df.iloc[:,1:-1],Train_df['Pawpularity'].values))\nval_ds = Dataset.from_tensor_slices((Val_df['Id'].values,Val_df.iloc[:,1:-1],Val_df['Pawpularity'].values))","0663f909":"# Defining functions to decode image paths and preprocess images \ndef read_img(labeled):\n    def img_to_array(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.cast(image, tf.float32)\n        image = tf.image.resize(image, (image_size, image_size))\n        image = tf.keras.applications.efficientnet.preprocess_input(image)\n        return image\n    def mapping_train(path, struct_data, score):\n        return (img_to_array(path),struct_data), score\n    def mapping_test(path, struct_data):\n        return (img_to_array(path),struct_data)\n    return mapping_train if labeled else mapping_test\n\ndef augment(data, score):\n    image, struct_data = data\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.95, 1.05)\n    image = tf.image.random_contrast(image, 0.95, 1.05)\n    image = tf.image.random_brightness(image, 0.1)\n    return (image, struct_data), score\n\ndef preprocess(ds, batch_size, ds_type, labeled):\n    labeled_read_img = read_img(labeled)\n    ds = ds.map(labeled_read_img, num_parallel_calls=AUTOTUNE)\n    if ds_type=='train':\n        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n        ds = ds.shuffle(shuffle_size, reshuffle_each_iteration=True)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(AUTOTUNE)\n    return ds","9e18df6d":"# Preprocessing data\nTrain = preprocess(train_ds, batch_size, 'train', True)\nValidation = preprocess(val_ds, batch_size, 'train', True)","4dd7c858":"# Importing EfficientNetB0 pretrained model\nEffNetB0_path = \"..\/input\/efficientnetb0-pretrained\/EfficientNetB0.h5\"\nEffB0 = tf.keras.models.load_model(EffNetB0_path)\nEffB0.trainable=False\n#EffB0.get_layer('top_conv').trainable=True","a1c1c6d9":"# Defining the neural network model\n# The named layers are going to be used to extract features for the catboost model training\n\nInp1 = Input(shape=(image_size,image_size,channels))\nout1 = EffB0(Inp1)\nout1 = BatchNormalization()(out1)\nout1 = Dropout(0.2)(out1)\nout1 = Dense(16, activation='relu', kernel_initializer='he_normal')(out1)\nout1 = Dense(16, activation='relu', kernel_initializer='he_normal', name=\"Last_layer_Eff\")(out1)\n\nInp2 = Input(shape=(12,))\nout2 = Dense(16, activation='relu', kernel_initializer='he_normal', name=\"Last_layer_FFN\")(Inp2)\n\nout = concatenate([out1,out2], axis=1)\nout = Dense(16, activation='relu', kernel_initializer='he_normal')(out)\nout = Dense(1, activation='relu')(out)\n\nPawModel = Model(inputs=[Inp1,Inp2], outputs=out)","f7fc89ad":"# Plotting the defined model architecture\nplot_model(PawModel, show_shapes=True)","9b48aef6":"# Setting callbacks\nearly_stopping = EarlyStopping(patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=1)\n\n# Compiling model\nPawModel.compile(loss=\"mse\", \n              optimizer = tf.keras.optimizers.Adam(learning_rate=8e-5), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n# Training model\nresults = PawModel.fit(Train,\n                      epochs=25,\n                      validation_data = Validation,\n                      callbacks=[early_stopping, reduce_lr], verbose=2)","8ebd08ac":"# PLotting training results\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(results.history['root_mean_squared_error'])\nplt.plot(results.history['val_root_mean_squared_error'])\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.legend(['Train','Val'])\nplt.title('RMSE evolution')\n\nplt.subplot(122)\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'])\nplt.title('Loss evolution');","92d0d33e":"# Preparing the test dataset\ntest_ds = Dataset.from_tensor_slices((test_df['Id'].values,test_df.iloc[:,1:], np.multiply(test_df.iloc[:,1].values,0)))\nTest = preprocess(test_ds, batch_size, 'test', True)\n\n# Predicting on the test set\ntest_NN_pred = PawModel.predict(Test)","a3f2de65":"# Creating the feature extraction models\nEffB0_features = Model(PawModel.inputs, PawModel.get_layer(\"Last_layer_Eff\").output)\nFFN_features = Model(PawModel.inputs, PawModel.get_layer(\"Last_layer_FFN\").output)\n\n# Extracting the EfficientNetB0 and the feed forward features on the training set\nFFN_train_features = FFN_features.predict(Train)\nEffB0_train_features = EffB0_features.predict(Train)\n\n# Extracting the EfficientNetB0 and the feed forward features on the validation set\nFFN_val_features = FFN_features.predict(Validation)\nEffB0_val_features = EffB0_features.predict(Validation)\n\n# Extracting the EfficientNetB0 and the feed forward features on the test set\nFFN_test_features = FFN_features.predict(Test)\nEffB0_test_features = EffB0_features.predict(Test)","8b341c72":"# Extracting the training labels\ni=1\nfor batch_train in Train:\n    if i==1:\n        train_labels = batch_train[1]\n    else:\n        train_labels = np.concatenate((train_labels,batch_train[1].numpy()), axis=0)\n    i+=1\n\n# Extracting the validation labels\ni=1\nfor batch_val in Validation:\n    if i==1:\n        val_labels = batch_val[1]\n    else:\n        val_labels = np.concatenate((val_labels,batch_val[1].numpy()), axis=0)\n    i+=1","272b7282":"# Creating the Traing, and Test sets for the CatBoost model\nTrain_cb = np.concatenate((EffB0_train_features,FFN_train_features), axis=1)\nTrain_label_cb = train_labels\n\nVal_cb = np.concatenate((EffB0_val_features,FFN_val_features), axis=1)\nVal_label_cb = val_labels\n\nTest_cb = np.concatenate((EffB0_test_features,FFN_test_features), axis=1)\n\n# Both the training and validation sets used to training the PawModel are combined as the Training set for the CatBoost to be trained using cross validation.\nTrain_cb = np.concatenate((Train_cb,Val_cb),axis=0)\nTrain_label_cb = np.concatenate((train_labels,val_labels),axis=0)","eb03ed82":"# Constants \ncounter=0\noof_score=0\ntest_cb_pred = np.zeros((Test_cb.shape[0],))\n\n# CatBoost parameters\ncb_params = {\n    'loss_function' : 'RMSE',\n    'eval_metric' : 'RMSE',\n    'iterations' : 10000,\n    'grow_policy' : 'SymmetricTree',\n    'depth' : 5,\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.07,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0,\n    'random_state': 123\n}","a13c3231":"# Stratified cross-validation training process\nkfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=123)\n\nfor idx, (train, val) in enumerate(kfold.split(Train_cb, Train_label_cb)):\n    counter += 1\n    print(f'\\n Fold {counter}:\\n')\n\n    train_x, train_y = Train_cb[train], Train_label_cb[train]\n    val_x, val_y = Train_cb[val], Train_label_cb[val]\n    \n    # Training model\n    model = cb.CatBoostRegressor(**cb_params)\n    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], early_stopping_rounds=100, verbose=500)\n    \n    # Calculating the Out-Of-Fold score\n    y_pred = model.predict(val_x)\n    oof_score += np.sqrt(mean_squared_error(val_y, y_pred))\n    \n    test_cb_pred += model.predict(Test_cb)\n    \n    print(f'Fold:{idx}: OOF score: {oof_score\/counter}\\n')\n    print('='*25)\n    \n    # Freeing up memory\n    del model, y_pred\n    del train_x, train_y\n    del val_x, val_y\n    gc.collect()\n    \nprint(f'\\n Average OOF score: {oof_score\/counter}')\ntest_cb_pred = test_cb_pred\/counter","8b28eaed":"# Calculating the final prediction as the average of both predictions\nFinal_pred = (np.squeeze(test_NN_pred)+test_cb_pred)\/2","738071e3":"# Creating the Submission file\nSubmission_df=pd.DataFrame()\nSubmission_df['Id']=test_id\nSubmission_df['Pawpularity']=Final_pred\nSubmission_df.to_csv('submission.csv',index=False)","33e12580":"Submission_df","017c0b86":"# CatBoosted EfficientNet approach for Pawpularity prediction\n\nEven though, the most performing solution that are publicly available in this competition uses Swin Transformer models and RAPIDS SVR boost models. There are many other \"simpler\" solution that achieved good results such as the CatBoost- EffNet B2 by [ROBIN SMITS](https:\/\/www.kaggle.com\/rsmits\/effnet-b2-feature-models-catboost), and the Pretrained EfficientNet by [DEV KHANT](http:\/\/https:\/\/www.kaggle.com\/devkhant24\/pretrained-model-efficientnet).\n\nIn this notebook, I present a CatBoosted EfficientNet model which is a quite simple structured model and it achieves acceptable results.\nThe model architeture used in this notebook is presented in the following graph:\n\n![NN architeture.png](attachment:623f2337-996e-4dfc-b189-c3e02753612d.png)\n\nThe pocess of training this model is conducted over two steps:\n1. Training a model with a frozen EfficientNetB0 backbone for image feature extraction, a shallow dense network for the meta data input, and a fully connected head.\n2. Using the trained model in the first step, and replacing the head with a catboost regression model.\n\nThe final predication is calculated as the average prediction of both models.\n","46df0c31":"**If you find this notebook useful, don't forget to upvote it!**"}}