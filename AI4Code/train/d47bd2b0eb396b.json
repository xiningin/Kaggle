{"cell_type":{"6c088a78":"code","9bbc6817":"code","beeb7d9e":"code","96370ccf":"code","53694f70":"code","6f4b584c":"code","68416113":"code","7d6156e7":"code","faefefdd":"code","951d3c17":"code","ce0858f7":"markdown","652152bc":"markdown","f3e9bc0b":"markdown","e2191f59":"markdown"},"source":{"6c088a78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9bbc6817":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom seaborn import heatmap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport plotly.express as px\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\n%matplotlib inline","beeb7d9e":"df_2020=pd.read_csv('..\/input\/world-happiness-report\/2020.csv')\ndf_2019=pd.read_csv('..\/input\/world-happiness-report\/2019.csv')\ndf_2018=pd.read_csv('..\/input\/world-happiness-report\/2018.csv')\ndf_2017=pd.read_csv('..\/input\/world-happiness-report\/2017.csv')\ndf_2016=pd.read_csv('..\/input\/world-happiness-report\/2016.csv')\ndf_2015=pd.read_csv('..\/input\/world-happiness-report\/2015.csv')\n# pick up common features and rename\n\n# print(df_2015.columns)\n# print(df_2016.columns)\n# print(df_2017.columns)\n# print(df_2018.columns)\n# print(df_2019.columns)\n# print(df_2020.columns)\n\ndf_2017 = df_2017.rename(columns = {'Happiness.Score' : 'Happiness Score', 'Happiness.Rank' : 'Happiness Rank', 'Economy..GDP.per.Capita.' : 'Economy (GDP per Capita)', 'Health..Life.Expectancy.' : 'Health (Life Expectancy)','Trust..Government.Corruption.' : 'Trust (Government Corruption)', 'Dystopia.Residual' : 'Dystopia Residual'})\ndf_2018 = df_2018.rename(columns = {'Healthy life expectancy' : 'Health (Life Expectancy)', 'Social support' : 'Family', 'Score' : 'Happiness Score', 'Country or region':'Country', 'Overall rank' : 'Happiness Rank', 'GDP per capita' : 'Economy (GDP per Capita)', 'Perceptions of corruption' : 'Trust (Government Corruption)'})\ndf_2019 = df_2019.rename(columns = {'Freedom to make life choices' : 'Freedom', 'Score' : 'Happiness Score', 'Country or region':'Country', 'Overall rank' : 'Happiness Rank', 'GDP per capita' : 'Economy (GDP per Capita)', 'Social support' : 'Family','Healthy life expectancy' : 'Health (Life Expectancy)','Freedom to make life choices' : 'Freedom', 'Perceptions of corruption' : 'Trust (Government Corruption)'})\ndf_2020 = df_2020.rename(columns = {'Ladder score' : 'Happiness Score', 'Country name':'Country', 'Regional indicator':'Region', 'Explained by: Log GDP per capita' : 'Economy (GDP per Capita)', 'Social support' : 'Family', 'Healthy life expectancy' : 'Health (Life Expectancy)','Freedom to make life choices' : 'Freedom', 'Perceptions of corruption' : 'Trust (Government Corruption)'})\ndf_2015.drop([\"Happiness Rank\", \"Region\", \"Standard Error\", \"Dystopia Residual\"], inplace = True, axis = 1)\ndf_2016.drop([\"Dystopia Residual\", \"Happiness Rank\", \"Region\", \"Lower Confidence Interval\", \"Upper Confidence Interval\"], inplace = True, axis = 1)\ndf_2017.drop([\"Happiness Rank\", \"Dystopia Residual\", \"Whisker.low\", \"Whisker.high\"], inplace = True, axis = 1)\ndf_2018.drop([\"Freedom to make life choices\", \"Happiness Rank\"], inplace = True, axis = 1)\ndf_2019.drop(\"Happiness Rank\", inplace = True, axis = 1)\ndf_2020.drop([\"Logged GDP per capita\", \"Region\", \"Standard error of ladder score\", \"upperwhisker\", \"lowerwhisker\", \"Ladder score in Dystopia\", \"Explained by: Social support\", \"Explained by: Healthy life expectancy\", \"Explained by: Freedom to make life choices\", \"Explained by: Generosity\", \"Explained by: Perceptions of corruption\", \"Dystopia + residual\"], inplace = True, axis = 1)\ndf_2015['year'] = 2015\ndf_2016['year'] = 2016\ndf_2017['year'] = 2017\ndf_2018['year'] = 2018\ndf_2019['year'] = 2019\ndf_2020['year'] = 2020\n\n\n\n\ndf_all = pd.concat([df_2019,df_2018,df_2017,df_2016,df_2015])\n# fill null values with mean\ndf_all.fillna((df_all.mean()), inplace = True)\nprint(df_all.info())\n# print(df_all)\n\n","96370ccf":"plt.plot(df_all.groupby(\"year\").mean()[\"Happiness Score\"])\nplt.ylabel(\"Happiness Score\")\nplt.ylim(5, 5.6)\n\n\n","53694f70":"df_top = df_all.head(5).sort_values('Happiness Score', ascending = True)\npx.bar(df_top, x='Happiness Score', y='Country', title = \"top 10\")","6f4b584c":"y = df_all['Happiness Score']\nX = df_all[['Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Generosity', \n          'Trust (Government Corruption)']] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\nconstant_mse = mean_squared_error(np.ones(y_test.shape) * np.mean(y_train), y_test)\nprint(\"Constant MSE: %.2f\" % constant_mse)","68416113":"from sklearn.neighbors import KNeighborsRegressor\n\nmse_list = []\nk_range = range(1, 51)\nfor k in k_range:\n    model = KNeighborsRegressor(k)\n    model.fit(X_train, y_train)\n    mse_list.append(mean_squared_error(model.predict(X_test), y_test))\nk_list = [k for k in k_range]\nplt.plot(k_list, mse_list)\nplt.plot(k_list, [constant_mse for i in range(len(k_list))])\nplt.show()\n\n# best parameter for knn\nknnReg = KNeighborsRegressor (n_neighbors = 4)\nknnReg.fit(X_train,y_train)\ny_pred = knnReg.predict(X_test).reshape(-1, 1)\n\nscore = r2_score(y_test,y_pred)\n# print(r2_sc)\n\n","7d6156e7":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nlm = LinearRegression()\nlm.fit(X_train,y_train)\ny_pred = lm.predict(X_test)\n# score = lm.score(X_test, y_test)\nscore = r2_score(y_test, y_pred)\n# score = r2_score(y_pred, y_test)\nmse = mean_squared_error(y_test, y_pred)\nscore, mse\n\n\n\n","faefefdd":"dtr= DecisionTreeRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)\ntest_mse = mean_squared_error(y_test, y_pred)\ny_pred_train = dtr.predict(X_train)\ntrain_mse = mean_squared_error(y_train, y_pred_train)\n# d=dtr.score(X_test, y_test)\nscore = r2_score(y_test, y_pred)\nscore, test_mse ","951d3c17":"rf = RandomForestRegressor(n_estimators = 13579)\nrf.fit(X_train, y_train)\ny_hat = rf.predict(X_test)\nerrors = abs(y_hat - y_test)\nacc = 1 - errors\n# c=rf.score(X_test, y_test)\nscore = r2_score(y_test, y_pred)\nscore, np.mean(acc)","ce0858f7":"**Data Overview**","652152bc":"**Data processing:**","f3e9bc0b":"KNN","e2191f59":"**linear regression**"}}