{"cell_type":{"0fb90144":"code","89741e14":"code","ad4b1f41":"code","4b717271":"code","86bb528e":"code","c1a960b0":"code","e416f97a":"code","a75cd168":"code","16f0efa4":"code","7d8f1d4f":"code","1c730921":"code","dece4711":"code","fc998c10":"code","80ac71e3":"code","1c167eba":"code","de7b6227":"code","2ad1df1a":"code","2c4c2908":"code","af1a0ad9":"code","ef4de3f1":"code","dbea5669":"code","5d9e778b":"code","39552a93":"code","e856416d":"code","dbfa919b":"code","4a0b5781":"code","ad08880f":"code","5ccc37d9":"code","2faa0a56":"code","9aed4181":"code","01fa147f":"code","9ee3cb03":"code","7e6f7d9e":"code","83be9428":"code","2aad37f5":"code","0ed424e3":"code","1b7d8fba":"code","e77c6c56":"code","5c9e6ad1":"code","8fd902bd":"code","ed9f3ed1":"code","eda50297":"code","b50cbfb9":"code","1a7f9310":"code","dfa9b155":"code","a7bb50b5":"code","8e116b51":"code","248b5d0f":"code","bb066ce3":"code","c4b80ded":"code","e5bc7a0d":"code","35718b9e":"code","e27acddf":"code","c7b49848":"code","c3fdbe52":"code","210889b8":"code","0f93f502":"code","65a103b5":"code","81da9c16":"code","10df9e19":"code","a1f959c8":"code","f53dbaf9":"code","d64ff363":"markdown","b7aad8db":"markdown","d56b308f":"markdown","ec76601f":"markdown","71a55f7f":"markdown","113d6353":"markdown","f26b35ec":"markdown","cded080e":"markdown","19f95fe4":"markdown","67b086a6":"markdown","fdded832":"markdown","fcd8f608":"markdown","500c4985":"markdown","bbe2cc99":"markdown","42aefefd":"markdown","41c41c7f":"markdown","60cb251d":"markdown","61f4c0e8":"markdown","0f028d9c":"markdown","33ea54c8":"markdown","ca1f294e":"markdown","25f8a1c7":"markdown","a42f40c7":"markdown","0634d22f":"markdown","729066a8":"markdown"},"source":{"0fb90144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ndf=pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'latin1', header = None)","89741e14":"df = df[[5, 0]]","ad4b1f41":"df.columns = ['Tweets', 'Sentiments']","4b717271":"set_map = {0: 'Negative', 4: 'Positive'}","86bb528e":"df['Word_counts'] = df['Tweets'].apply(lambda x: len(str(x).split()))","c1a960b0":"df['Character_counts'] = df['Tweets'].apply(lambda x: len(x))","e416f97a":"def get_avg_word_len(x):\n    words = x.split()\n    word_len = 0\n    for word in words:\n        word_len = word_len + len(word)\n    return word_len\/len(words)","a75cd168":"df['Avg_word_len'] = df['Tweets'].apply(lambda x: get_avg_word_len(x))","16f0efa4":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","7d8f1d4f":"df['stop_words'] = df['Tweets'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))","1c730921":"df['hashtags_count'] = df['Tweets'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\ndf['mention_count'] = df['Tweets'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))","dece4711":"df['numerics_count'] = df['Tweets'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))","fc998c10":"df['UPPER_CASE_COUNT'] = df['Tweets'].apply(lambda x: len([t for t in  x.split() if t.isupper() and len(x)>3]))","80ac71e3":"contractions = {\n\"aight\": \"alright\",\n\"ain't\": \"am not\",\n\"amn't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"can not\",\n\"cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"daren't\": \"dare not\",\n\"daresn't\": \"dare not\",\n\"dasn't\": \"dare not\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"d'ye\": \"do you\",\n\"e'er\": \"ever\",\n\"everybody's\": \"everybody is\",\n\"everyone's\": \"everyone is\",\n\"finna\": \"fixing to\",\n\"g'day\": \"good day\",\n\"gimme\": \"give me\",\n\"giv'n\": \"given\",\n\"gonna\": \"going to\",\n\"gon't\": \"go not\",\n\"gotta\": \"got to\",\n\"hadn't\": \"had not\",\n\"had've\": \"had have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'dn't've'd\": \"he would not have had\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"he've\": \"he have\",\n\"how'd\": \"how would\",\n\"howdy\": \"how do you do\",\n\"how'll\": \"how will\",\n\"how're\": \"how are\",\n\"I'll\": \"I will\",\n\"I'm\": \"I am\",\n\"I'm'a\": \"I am about to\",\n\"I'm'o\": \"I am going to\",\n\"innit\": \"is it not\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"may've\": \"may have\",\n\"methinks\": \"me thinks\",\n\"mightn't\": \"might not\",\n\"might've\": \"might have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"must've\": \"must have\",\n\"needn't\": \"need not\",\n\"ne'er\": \"never\",\n\"o'clock\": \"of the clock\",\n\"o'er\": \"over\",\n\"ol'\": \"old\",\n\"oughtn't\": \"ought not\",\n\"'s\": \"is\",\n\"shalln't\": \"shall not\",\n\"shan't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she shall\",\n\"she'll\": \"she will\",\n\"she's\": \"she has\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"somebody's\": \"somebody has\",\n\"somebody's\": \"somebody is\",\n\"someone's\": \"someone has\",\n\"someone's\": \"someone is\",\n\"something's\": \"something has\",\n\"something's\": \"something is\",\n\"so're\": \"so are\",\n\"that'll\": \"that shall\",\n\"that'll\": \"that will\",\n\"that're\": \"that are\",\n\"that's\": \"that has\",\n\"that's\": \"that is\",\n\"that'd\": \"that would\",\n\"that'd\": \"that had\",\n\"there'd\": \"there had\",\n\"there'd\": \"there would\",\n\"there'll\": \"there shall\",\n\"there'll\": \"there will\",\n\"there're\": \"there are\",\n\"there's\": \"there has\",\n\"there's\": \"there is\",\n\"these're\": \"these are\",\n\"these've\": \"these have\",\n\"they'd\": \"they had\",\n\"they'd\": \"they would\",\n\"they'll\": \"they shall\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they're\": \"they were\",\n\"they've\": \"they have\",\n\"this's\": \"this has\",\n\"this's\": \"this is\",\n\"those're\": \"those are\",\n\"those've\": \"those have\",\n\"'tis\": \"it is\",\n\"to've\": \"to have\",\n\"'twas\": \"it was\",\n\"wanna\": \"want to\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd\": \"we would\",\n\"we'd\": \"we did\",\n\"we'll\": \"we shall\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'd\": \"what did\",\n\"what'll\": \"what shall\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what're\": \"what were\",\n\"what's\": \"what has\",\n\"what's\": \"what is\",\n\"what's\": \"what does\",\n\"what've\": \"what have\",\n\"when's\": \"when has\",\n\"when's\": \"when is\",\n\"where'd\": \"where did\",\n\"where'll\": \"where shall\",\n\"where'll\": \"where will\",\n\"where're\": \"where are\",\n\"where's\": \"where has\",\n\"where's\": \"where is\",\n\"where's\": \"where does\",\n\"where've\": \"where have\",\n\"which'd\": \"which had\",\n\"which'd\": \"which would\",\n\"which'll\": \"which shall\",\n\"which'll\": \"which will\",\n\"which're\": \"which are\",\n\"which's\": \"which has\",\n\"which's\": \"which is\",\n\"which've\": \"which have\",\n\"who'd\": \"who would\",\n\"who'd\": \"who had\",\n\"who'd\": \"who did\",\n\"who'd've\": \"who would have\",\n\"who'll\": \"who shall\",\n\"who'll\": \"who will\",\n\"who're\": \"who are\",\n\"who's\": \"who has\",\n\"who's\": \"who is\",\n\"who's\": \"who does\",\n\"who've\": \"who have\",\n\"why'd\": \"why did\",\n\"why're\": \"why are\",\n\"why's\": \"why has\",\n\"why's\": \"why is\",\n\"why's\": \"why does\",\n\"won't\": \"will not\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd've\": \"you all would have\",\n\"y'all'dn't've'd\": \"you all would not have had\",\n\"y'all're\": \"you all are\",\n\"you'd\": \"you had\",\n\"you'd\": \"you would\",\n\"you'll\": \"you shall\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\" u \": \"you\",\n\" ur \": \"your\",\n\" n \": \"and\"\n}","1c167eba":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x","de7b6227":"df['Tweets'] = df['Tweets'].apply(lambda x: cont_to_exp(x))","2ad1df1a":"import re","2c4c2908":"df['Emails'] = df['Tweets'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)',x))","af1a0ad9":"df['Emails_count'] = df['Emails'].apply(lambda x: len(x))","ef4de3f1":"df['Tweets'] = df['Tweets'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '',x))","dbea5669":"df['URL_Flags'] = df['Tweets'].apply(lambda x: len(re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)))","5d9e778b":"df['Tweets'] = df['Tweets'].apply(lambda x: re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '', x))","39552a93":"df['Tweets'] = df['Tweets'].apply(lambda x: re.sub('RT', '', x))","e856416d":"df['Tweets'] = df['Tweets'].apply(lambda x: re.sub('[^a-z A-Z 0-9-]+', '', x))","dbfa919b":"df['Tweets'] = df['Tweets'].apply(lambda x: ' '.join(x.split()))","4a0b5781":"from bs4 import BeautifulSoup","ad08880f":"df['Tweets'] = df['Tweets'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())","5ccc37d9":"import spacy","2faa0a56":"df['Tweets'] = df['Tweets'].apply(lambda x: ' '.join([t for t in x.split() if t not in STOP_WORDS]))","9aed4181":"text = ' '.join(df['Tweets'])","01fa147f":"text = text.split()","9ee3cb03":"freq_comm = pd.Series(text).value_counts()","7e6f7d9e":"f_20 = freq_comm[:20]","83be9428":"df['Tweets'] = df['Tweets'].apply(lambda x: \" \".join([t for t in x.split() if t not in f_20]))","2aad37f5":"rare_20 = freq_comm[-20:]","0ed424e3":"df['Tweets'] = df['Tweets'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare_20]))","1b7d8fba":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline","e77c6c56":"x = ' '.join(text[:20000])","5c9e6ad1":"wc = WordCloud(width = 800, height = 400).generate(x)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","8fd902bd":"df_0 = df[df['Sentiments'] == 0].sample(2000)\ndf_4 = df[df['Sentiments'] == 4].sample(2000)","ed9f3ed1":"dfr = df_0.append(df_4)","eda50297":"dfr_feat = dfr.drop(labels = ['Tweets', 'Sentiments', 'Emails'], axis = 1).reset_index(drop = True)","b50cbfb9":"y = dfr['Sentiments']","1a7f9310":"from sklearn.feature_extraction.text import CountVectorizer","dfa9b155":"cv = CountVectorizer()\ntext_counts = cv.fit_transform(dfr['Tweets'])","a7bb50b5":"dfr_bow = pd.DataFrame(text_counts.toarray(), columns = cv.get_feature_names())","8e116b51":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","248b5d0f":"sgd = SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\nlgr = LogisticRegression(random_state=42, max_iter=200)\nlgr_cv = LogisticRegressionCV(cv=2, random_state=42, max_iter=1000)\nsvm = LinearSVC(random_state=42, max_iter=200)\nrfc = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)","bb066ce3":"clf = {'SGD': sgd, 'LGR': lgr, 'LGR_CV': lgr_cv, 'SVM': svm, 'RFC': rfc}","c4b80ded":"def classify(X,y):\n    scaler = MinMaxScaler(feature_range=(0,1))\n    X = scaler.fit_transform(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n    \n    for key in clf.keys():\n        clf[key].fit(X_train, y_train)\n        y_pred = clf[key].predict(X_test)\n        ac = accuracy_score(y_test, y_pred)\n        print(key, ' ---> ' , ac)","e5bc7a0d":"classify(dfr_bow, y)","35718b9e":"classify(dfr_feat, y)","e27acddf":"X = dfr_feat.join(dfr_bow)","c7b49848":"classify(X, y)","c3fdbe52":"from sklearn.feature_extraction.text import TfidfVectorizer","210889b8":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(dfr['Tweets'])","0f93f502":"classify(pd.DataFrame(X.toarray()), y)","65a103b5":"nlp = spacy.load('en_core_web_lg')","81da9c16":"def get_vec(x):\n    doc = nlp(x)\n    return doc.vector.reshape(1, -1)","10df9e19":"dfr['vec'] = dfr['Tweets'].apply(lambda x: get_vec(x))","a1f959c8":"X= np.concatenate(dfr['vec'].to_numpy(), axis=0)","f53dbaf9":"classify(pd.DataFrame(X), y)","d64ff363":"#### Count and Remove Emails","b7aad8db":"#### Common words removal","d56b308f":"#### Count URLs and remove them","ec76601f":"#### If numeric digits are present in tweets","71a55f7f":"#### Machine Learning Models for Text Classification\n#### BoW","113d6353":"#### Charcter_counts","f26b35ec":"#### Removing STOP_WORDS","cded080e":"#### Removing RETWEETS","19f95fe4":"#### Preprocessing and cleaning","67b086a6":"#### Word2Vec","fdded832":"#### UPPER_case_words_count","fcd8f608":"#### Rare Words Removal","500c4985":"#### Removing multiple spaces","bbe2cc99":"#### Mapping Sentiments","42aefefd":"#### TFIDF","41c41c7f":"#### Count hashtags(#) and @ mentions","60cb251d":"#### Manual Features","61f4c0e8":"#### Removal of special chars and punctuation","0f028d9c":"### ML Algorithms","33ea54c8":"#### Word Cloud Visualization","ca1f294e":"#### Removing HTML tags","25f8a1c7":"#### Stop_Words_Count","a42f40c7":"#### Average Word Length","0634d22f":"#### Manual + BoW","729066a8":"#### Word_Counts"}}