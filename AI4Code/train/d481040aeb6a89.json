{"cell_type":{"3db358ec":"code","c9e36c26":"code","516f81f6":"code","77b875e8":"code","0452266d":"code","1bff6608":"code","fc8e1785":"code","1488c9c4":"code","b120b091":"code","b730e86d":"code","04cc8d30":"code","4e2cb1fa":"code","9276fea7":"code","bab0f957":"code","f9755835":"code","bb23c669":"code","d3467853":"code","a8e88f01":"code","e5e5e699":"code","64b2bb23":"code","6175d1c9":"code","a0fb5c50":"code","31622315":"code","3e7df05a":"code","04ce31d1":"code","cf187be7":"code","bfdcd7a4":"code","1b2794e4":"code","f54885ec":"code","4801b6e2":"code","75d78f96":"code","57a75783":"code","d92bd128":"code","03343e6d":"code","d7c94049":"code","a4513b60":"code","8767aa4c":"code","eda44a40":"code","a7781d5f":"code","34896cc5":"code","0f7eeee5":"code","fcc625c5":"code","a8cd0cff":"code","e9bab23d":"code","3949ca43":"code","456b476a":"code","43ea371a":"code","17042517":"code","e96c8fea":"code","a5715314":"code","d0b6a293":"code","9a3c3ff1":"code","80200593":"markdown","8e8fcbfa":"markdown","ba7628d5":"markdown","bcddefda":"markdown","24d1ac87":"markdown","295da01b":"markdown","a24c6888":"markdown","d867cf48":"markdown","2dac8055":"markdown","29d0f1e0":"markdown","86bf2541":"markdown","249bf2df":"markdown","ab99040d":"markdown","8013762b":"markdown","77427d5b":"markdown","bd78c866":"markdown","3c3fd021":"markdown","fa6341fd":"markdown","6963b028":"markdown","467476f7":"markdown","b31317aa":"markdown","830e2d61":"markdown","0882270f":"markdown","636405f4":"markdown","80c3a002":"markdown","bbd20aca":"markdown","a573c9f7":"markdown","d83e7f17":"markdown","ae9b5942":"markdown","50db20bf":"markdown","e39c4f2e":"markdown","f11b01e7":"markdown","5e9a4e5d":"markdown","ca5a8239":"markdown","d28e259e":"markdown","0639faf5":"markdown","199e8a39":"markdown","432cb7af":"markdown","1977e619":"markdown","fc97729a":"markdown","f56dd306":"markdown","01a21b1a":"markdown","1b9a62d9":"markdown","03715060":"markdown"},"source":{"3db358ec":"!pip install seaborn==0.11.1\n!pip install keract","c9e36c26":"import io\nimport time\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_validate, GridSearchCV, cross_val_predict, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix, accuracy_score\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.data.experimental import AUTOTUNE\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D, ReLU, ELU\nfrom keras.models import Model\nfrom keras.optimizers import Adam, Nadam\nfrom keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint\nfrom keras.utils import to_categorical\n\nimport keract","516f81f6":"import tensorflow as tf\n\ntry:\n    tpu_cluster = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu_cluster.master())\n    \n    tf.config.experimental_connect_to_cluster(tpu_cluster)\n    tf.tpu.experimental.initialize_tpu_system(tpu_cluster)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster)\nexcept:\n    tpu_strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', tpu_strategy.num_replicas_in_sync)","77b875e8":"RANDOM_SEED = 20210102\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","0452266d":"train_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nsubmission_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","1bff6608":"train_df.info()","fc8e1785":"submission_df.info()","1488c9c4":"train_image_df = train_df.copy()\ntrain_image_df.drop(columns=['label'], inplace=True)\n\ntrain_label_df = train_df['label']","b120b091":"plt.figure(figsize=(10, 5))\n\nfor index, (image, label) in enumerate(zip(train_image_df[0:30], train_label_df[0:30])):\n    ax = plt.subplot(3, 10, index + 1)\n    ax.axis('off')\n\n    plt.imshow(np.reshape(train_image_df.values[index], (28, 28)), cmap='binary')\n\n    plt.title('Label: %i\\n' % label, fontsize=10);","b730e86d":"train_df.describe()","04cc8d30":"sns.histplot(x=train_label_df, bins=10);","4e2cb1fa":"train_label_df.value_counts(), train_label_df.value_counts(normalize=True)","9276fea7":"train_image_df = train_image_df.astype('float32') \/ 255.0\nsubmission_df = submission_df.astype('float32') \/ 255.0","bab0f957":"X_train, X_val, y_train, y_val = train_test_split(train_image_df, train_label_df, test_size=0.2, random_state=RANDOM_SEED)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=RANDOM_SEED)","f9755835":"sns.histplot(data=y_val, bins=10)\nsns.histplot(data=y_test, bins=10);","bb23c669":"from sklearn.decomposition import PCA\n\npc_analyser = PCA(random_state=RANDOM_SEED)\n\npc_analyser.fit(train_image_df)\n\naccumulated_variance_ratio = np.cumsum(pc_analyser.explained_variance_ratio_)\n\nplt.plot(accumulated_variance_ratio)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\nprint('95%% of variance can be covered by %d components' % (np.argmax(accumulated_variance_ratio >= 0.95) + 1))","d3467853":"pc_analyser = PCA(n_components=154, random_state=RANDOM_SEED)\n\nreduced_train_image_df = pc_analyser.fit_transform(train_image_df)","a8e88f01":"pca_analyzer = PCA(\n    n_components=2,\n    random_state=RANDOM_SEED\n)\n\nx2d = pca_analyzer.fit_transform(train_image_df)\n\npca_df = pd.DataFrame(data=x2d, columns=['pc1', 'pc2'])\npca_df = pd.concat([pca_df, train_label_df], axis=1)\n\nplt.figure(figsize=(10, 10))\nsns.scatterplot(data=pca_df, x='pc1', y='pc2', hue='label', alpha=0.3, palette='muted');","e5e5e699":"X_tsne = TSNE(\n    n_components=2,\n    init='pca',\n    perplexity=50,\n    early_exaggeration=12,\n    random_state=RANDOM_SEED,\n    n_jobs=-1,\n).fit_transform(reduced_train_image_df)\n\npca_df = pd.DataFrame(data=X_tsne, columns=['comp1', 'comp2'])\npca_df = pd.concat([pca_df, train_label_df], axis=1)","64b2bb23":"plt.figure(figsize=(10, 10))\nsns.scatterplot(data=pca_df, x='comp1', y='comp2', hue='label', alpha=0.3, palette='muted');","6175d1c9":"def plot_confusion_matrix_by_predictions(y_true, y_predicted, *, labels=None,\n                          sample_weight=None, normalize=None,\n                          display_labels=None, include_values=True,\n                          xticks_rotation='horizontal',\n                          values_format=None, colorbar=False,\n                          cmap='rocket_r', ax=None):\n    \n    cm = confusion_matrix(y_true, y_predicted, sample_weight=sample_weight,\n                          labels=labels, normalize=normalize)\n\n    if display_labels is None:\n        if labels is None:\n            display_labels = unique_labels(y_true, y_predicted)\n        else:\n            display_labels = labels\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=display_labels)\n\n    return disp.plot(include_values=include_values,\n                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,\n                     values_format=values_format)\n","a0fb5c50":"def score_classification_model(model, X_train, y_train):\n    \n    cv_scores = cross_validate(\n        model, X_train, y_train, \n        scoring=['accuracy'],\n        cv=5,\n        n_jobs=-1, verbose=0\n    )\n\n    cv_y_predicted = cross_val_predict(\n        model, X_train, y_train,\n        cv=5,\n        n_jobs=-1\n    )\n\n    cv_accuracy, accuracy_std = cv_scores['test_accuracy'].mean(), cv_scores['test_accuracy'].std()\n\n    model.fit(X_train, y_train)\n\n    y_train_predicted = model.predict(X_train)\n\n    train_accuracy = accuracy_score(y_train, y_train_predicted)\n\n    print('[Train] Accuracy: %.4f' % (train_accuracy))\n    print('Train Set Report:')\n    print(classification_report(y_train, y_train_predicted, digits=3))\n\n    print('[CV] Accuracy: %.4f (%.4f)' % (cv_accuracy, accuracy_std))\n    print('CV Report:')\n    print(classification_report(y_train, cv_y_predicted, digits=3))\n    \n    # display confusion matrixes\n\n    _, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 10))\n\n    ax0.set_title('Train Confusion Matrix')\n    plot_confusion_matrix(\n        model, X_train, y_train,\n        cmap=plt.cm.Blues,\n        ax=ax0,\n    )\n\n    ax1.set_title('CV Confusion Matrix')\n    plot_confusion_matrix_by_predictions(\n        y_train, cv_y_predicted,\n        cmap=plt.cm.Blues,\n        ax=ax1,\n    )\n\n    return y_train_predicted, cv_y_predicted","31622315":"ksvm_pipeline = Pipeline([\n    ('pca', PCA(n_components=154, random_state=RANDOM_SEED)),\n    ('classifier', SVC(kernel='rbf', gamma=0.06)),\n])","3e7df05a":"# score_classification_model(ksvm_pipeline, X_train, y_train);","04ce31d1":"# y_test_pred = ksvm_pipeline.predict(X_test)\n\nprint('Test Report')\nprint(classification_report(y_test, y_test_pred, digits=4))\nplot_confusion_matrix_by_predictions(y_test, y_test_pred)","cf187be7":"# y_submission = ksvm_pipeline.predict(submission_df)\n\nsubmission_label_df = pd.DataFrame({\n    'ImageId': list(range(1, len(y_submission) + 1)), \n    'Label': y_submission\n})\n\nsubmission_label_df.to_csv('.\/ksvm_submission.csv', index=False, header=True)","bfdcd7a4":"def plot_training_history(training_history, metrics=['loss', 'accuracy'], best_epoch_metric='val_accuracy', figsize=(15, 5)):\n    \"\"\"\n    Plot Keras training history in two plots: Loss Plot and Metric Plot\n    \"\"\"\n    training_history_df = pd.DataFrame(training_history.history)\n\n    best_epoch = np.argmax(training_history_df[best_epoch_metric])\n\n    _, axes = plt.subplots(1, len(metrics), figsize=figsize)\n\n    for idx, metric in enumerate(metrics):\n        training_history_df[[metric, 'val_' + metric]].plot(\n            title=metric, \n            grid=True, \n            ax=axes[idx]\n        )\n        plt.gca().set_ylim(0, 1)\n        \n        axes[idx].axvline(\n            best_epoch,\n            ls=\"--\",\n            c=\"k\",\n            lw=1,\n        )\n        \n    digits = 4\n    headers = ['Train', 'Validation']\n    width = max(len(headers[1]), digits)\n    rows = []\n    \n    for metric in metrics:\n        rows.append([metric, training_history_df[metric][best_epoch], training_history_df['val_' + metric][best_epoch]])\n    \n    head_fmt = '{:>{width}s}' + ' {:>9} ' * len(headers)\n    report = head_fmt.format('', *headers, width=width)\n    report += '\\n\\n'\n    row_fmt = ' {:>9}' + ' {:>9.{digits}f}' * 2 + '\\n' \n    \n    for row in rows:\n        report += row_fmt.format(*row, width=width, digits=digits)\n    \n    return report","1b2794e4":"def plot_misclassified_samples(y_test, y_test_pred, true_label, pred_label):\n    missclass_label_indices = [\n        idx for idx, (label_true, label_pred) in enumerate(zip(y_test, y_test_pred)) \n        if label_true == true_label and label_pred == pred_label\n    ]\n\n    fig = plt.figure(figsize=(20, 10))\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n    for i, idx in enumerate(missclass_label_indices[:30]):    \n        ax = fig.add_subplot(3, 10, i + 1)\n        ax.axis('off')\n\n        ax.text(0.5, -0.35, 'ID = ' + str(idx), fontsize=10, ha='center', transform=ax.transAxes) \n        ax.text(0.5, -0.6, 'pred = ' + str(y_test_pred[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n        ax.text(0.5, -0.8, 'act = ' + str(y_test[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n        ax.imshow(np.reshape(X_test.values[idx], (28, 28)), cmap='binary')","f54885ec":"NUM_CLASSES = 10\n\ny_train_onehot = to_categorical(y_train, NUM_CLASSES)\ny_val_onehot = to_categorical(y_val, NUM_CLASSES)\ny_test_onehot = to_categorical(y_test, NUM_CLASSES)","4801b6e2":"with tpu_strategy.scope():\n    input_layer = Input((784))\n\n    mlp = Dense(200)(input_layer)\n    mlp = BatchNormalization()(mlp)\n    mlp = ReLU()(mlp)\n    mlp = Dropout(0.2)(mlp)\n\n    mlp = Dense(200)(input_layer)\n    mlp = BatchNormalization()(mlp)\n    mlp = ReLU()(mlp)\n    mlp = Dropout(0.2)(mlp)\n\n    output_layer = Dense(NUM_CLASSES, activation='softmax')(mlp)\n\n    mlp_model = Model(input_layer, output_layer, name='MLP')\n\n    mlp_model.compile(\n        loss='categorical_crossentropy', \n        optimizer=Adam(lr=0.0005), \n        metrics=['accuracy']\n    )\n\nmlp_model.summary()","75d78f96":"early_stopping = EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nmlp_train_history = mlp_model.fit(\n    X_train, y_train_onehot, \n    validation_data=(X_val, y_val_onehot),\n    batch_size=32, \n    epochs=50, \n    shuffle=True,\n    callbacks=[early_stopping],\n)","57a75783":"print(plot_training_history(mlp_train_history))","d92bd128":"mlp_model.evaluate(X_test, y_test_onehot)","03343e6d":"y_test_true = np.argmax(y_test_onehot, axis=-1)\ny_test_pred = np.argmax(mlp_model.predict(X_test), axis=-1)","d7c94049":"print('Test Report')\nprint(classification_report(y_test_true, y_test_pred, digits=4))\nplot_confusion_matrix_by_predictions(y_test_true, y_test_pred)","a4513b60":"plot_misclassified_samples(y_test_true, y_test_pred, 9, 7)","8767aa4c":"y_submission = np.argmax(mlp_model.predict(submission_df), axis=-1)\n\nsubmission_label_df = pd.DataFrame({\n    'ImageId': list(range(1, len(y_submission) + 1)), \n    'Label': y_submission\n})\n\nsubmission_label_df.to_csv('.\/mlp_submission.csv', index=False, header=True)","eda44a40":"# will be feed to predict method\nX_test2d = X_test.to_numpy().reshape((-1, 28, 28))\nX_submission2d = submission_df.to_numpy().reshape((-1, 28, 28))","a7781d5f":"IMAGE_SHAPE = (28, 28, 1)\nbatch_size = 16 * tpu_strategy.num_replicas_in_sync\n\nprint('Num of Replicas: {}'.format(tpu_strategy.num_replicas_in_sync))\nprint('Batch Size: {}'.format(batch_size))","34896cc5":"def get_transofmation_matrix(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transform matrix which transforms indicies\n    # https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\n    \n    # degree to radians\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # rotation \n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    \n    rotation_matrix = tf.reshape(\n        tf.concat([\n            c1,   s1,   zero, \n            -s1,  c1,   zero, \n            zero, zero, one\n        ], \n        axis=0), \n        (3, 3)\n    )\n        \n    # shear\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = tf.reshape(\n        tf.concat([\n            one,  s2,   zero, \n            zero, c2,   zero, \n            zero, zero, one\n        ],\n        axis=0),\n        (3, 3)\n    )    \n    \n    # zoom\n    zoom_matrix = tf.reshape(\n        tf.concat([\n            one \/ height_zoom, zero, zero, \n            zero, one \/ width_zoom, zero, \n            zero, zero, one\n        ], \n        axis=0), \n        (3, 3)\n    )\n    \n    # shift\n    shift_matrix = tf.reshape(\n        tf.concat([\n            one, zero, height_shift, \n            zero, one, width_shift, \n            zero, zero, one\n        ], \n        axis=0), \n        (3, 3)\n    )\n    \n    return K.dot(\n        K.dot(rotation_matrix, shear_matrix), \n        K.dot(zoom_matrix, shift_matrix)\n    )\n\ndef augment_image(\n    image_shape, \n    rotation_range, \n    shear_range, \n    height_zoom_range, \n    width_zoom_range, \n    height_shift_range, \n    width_shift_range\n):\n    def transform_image(image, label):\n        # input image - is one image of size [dim,dim,3] not a batch of image_shape\n        # output - image randomly rotated, sheared, zoomed, and shifted\n\n        image_width = image_shape[0]\n        xdim = image_width % 2 #fix for size 331\n\n        rotation = rotation_range * tf.random.normal([1], dtype='float32')\n        shear = shear_range * tf.random.normal([1], dtype='float32') \n        h_zoom = height_zoom_range + tf.random.normal([1], dtype='float32') \/ 10.\n        w_zoom = width_zoom_range + tf.random.normal([1], dtype='float32') \/ 10.\n        h_shift = height_shift_range * tf.random.normal([1], dtype='float32') \n        w_shift = width_shift_range * tf.random.normal([1], dtype='float32') \n\n        transformation_matrix = get_transofmation_matrix(rotation, shear, h_zoom, w_zoom, h_shift, w_shift) \n\n        # LIST DESTINATION PIXEL INDICES\n        x = tf.repeat(tf.range(image_width \/\/ 2, -image_width \/\/ 2, -1), image_width)\n        y = tf.tile(tf.range(-image_width \/\/ 2, image_width \/\/ 2), [image_width])\n        z = tf.ones([image_width * image_width], dtype='int32')\n        idx = tf.stack([x, y, z])\n\n        # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n        idx2 = K.dot(transformation_matrix, tf.cast(idx, dtype='float32'))\n        idx2 = K.cast(idx2, dtype='int32')\n        idx2 = K.clip(idx2, -image_width \/\/ 2 + xdim + 1, image_width \/\/ 2)\n\n        # FIND ORIGIN PIXEL VALUES           \n        idx3 = tf.stack([image_width \/\/ 2 - idx2[0, ], image_width \/\/ 2 - 1 + idx2[1, ]])\n        d = tf.gather_nd(image, tf.transpose(idx3))\n\n        return tf.reshape(d, image_shape), label\n    \n    return transform_image","0f7eeee5":"image_augmentor = augment_image(\n    IMAGE_SHAPE,\n    rotation_range=5,\n    shear_range=5,\n    height_zoom_range=1.1, \n    width_zoom_range=1.1, \n    height_shift_range=1., \n    width_shift_range=1.,\n)","fcc625c5":"# will be feed to ImageDataGenerator\n# X_train3d = X_train.to_numpy().reshape(-1, 28, 28, 1)\n# X_val3d = X_val.to_numpy().reshape(-1, 28, 28, 1)\n\n\n# data_augmentator = ImageDataGenerator(\n#     rotation_range = 10,  \n#     zoom_range = 0.1, \n#     width_shift_range = 0.1, \n#     height_shift_range = 0.1\n# )\n\n# X_augmentation = X_train3d[:100, ]\n# y_augmentation = y_train_onehot[:100, ]\n\n# plt.figure(figsize=(15, 4.5))\n\n# for i in range(30):  \n#     plt.subplot(3, 10, i + 1)\n    \n#     augmented_image, _ = data_augmentator.flow(X_augmentation, y_augmentation).next()\n    \n#     plt.imshow(augmented_image[0].reshape((28, 28)), cmap='binary')\n#     plt.axis('off')\n        \n# plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n# plt.show()","a8cd0cff":"X_train3d = X_train.to_numpy().reshape(-1, 28, 28, 1)\n\naugmented_image_batch = (\n    tf.data.Dataset\n        .from_tensor_slices((X_train3d[11:12, :], y_train_onehot[11:12, :]))\n        .map(image_augmentor, num_parallel_calls=AUTOTUNE)\n        .repeat()\n        .shuffle(4048)\n        .take(50)\n)\n\nplt.figure(figsize=(15, 4.5))\n\nfor i, (augmented_image, label) in enumerate(augmented_image_batch):  \n    plt.subplot(5, 10, i + 1)\n    \n    plt.imshow(tf.reshape(augmented_image, (28, 28)), cmap='binary')\n    plt.axis('off')\n        \nplt.subplots_adjust(wspace=-0.1, hspace=-0.1)\nplt.show()","e9bab23d":"def build_cnn_model():\n    cnn_input_layer = Input((28, 28, 1))\n\n    # layer 1\n    cnn = Conv2D(32, 3, activation='relu', padding='same')(cnn_input_layer)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(32, 3, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(32, 5, strides=2, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Dropout(0.4)(cnn)\n\n    # layer 2\n    cnn = Conv2D(64, 3, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(64, 3, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(64, 5, strides=2, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Dropout(0.4)(cnn)\n\n    # layer 3\n    cnn = Conv2D(128, 3, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(128, 3, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = Conv2D(128, 5, strides=2, activation='relu', padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Dropout(0.4)(cnn)\n\n    cnn = Flatten()(cnn)\n\n    # layer 4\n    cnn = Dense(128, kernel_initializer='he_normal')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = ReLU()(cnn)\n    cnn = Dropout(0.4)(cnn)\n\n    cnn_output_layer = Dense(NUM_CLASSES, activation='softmax')(cnn)\n\n    cnn_model = Model(cnn_input_layer, cnn_output_layer, name='CNN')\n    cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])\n    \n    return cnn_model","3949ca43":"def split_training_set(X, y, test_size=0.1):\n    X_train_ensamble, X_val_ensamble, y_train_ensamble, y_val_ensamble = train_test_split(\n            X, y,\n            test_size=test_size, \n            random_state=RANDOM_SEED\n        )\n\n    X_train3d = X_train_ensamble.to_numpy().reshape((-1, 28, 28, 1))\n    X_val3d = X_val_ensamble.to_numpy().reshape((-1, 28, 28, 1))\n    y_train_onehot = to_categorical(y_train_ensamble, NUM_CLASSES)\n    y_val_onehot = to_categorical(y_val_ensamble, NUM_CLASSES)\n    \n    # convert in-memory train and validation sets into TF Dataset to take advantages of TPU acceleration\n    \n    X_train_dataset = (\n        tf.data.Dataset\n            .from_tensor_slices((X_train3d, y_train_onehot))\n            .map(image_augmentor, num_parallel_calls=AUTOTUNE)\n            .repeat()\n            .shuffle(4048)\n            .batch(batch_size)\n            .prefetch(AUTOTUNE)\n    )\n\n    X_val_dataset = (\n        tf.data.Dataset\n            .from_tensor_slices((X_val3d, y_val_onehot))\n            .batch(batch_size)\n            .cache()\n            .prefetch(AUTOTUNE)\n    )\n    \n    return X_train_dataset, X_val_dataset","456b476a":"early_stopping = EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlearning_rate_scheduler = LearningRateScheduler(lambda epoch: 1e-3 * 0.95 ** epoch)\n\nn_estimators = 10\nstep_per_epoch = train_image_df.shape[0] \/\/ batch_size\n\ncnn_ensamble = []\ntraining_histories = []\n\n\nfor i in range(n_estimators):\n    \n    with tpu_strategy.scope():\n        model = build_cnn_model()\n        \n    model_saver = ModelCheckpoint(filepath='model_best_MNIST_{}.h5'.format(i), \n                                   save_best_only=True,\n                                   verbose=2)\n    \n    X_train_dataset, X_val_dataset = split_training_set(train_image_df, \n                                                        train_label_df,\n                                                        test_size=0.1)\n    \n    print(\"CNN {}: Training...\".format(i))\n\n    cnn_train_history = model.fit(\n        X_train_dataset,\n        validation_data=X_val_dataset,\n        steps_per_epoch=step_per_epoch,\n        epochs=200, \n        shuffle=True,\n        callbacks=[early_stopping, learning_rate_scheduler, model_saver],\n        verbose=1,\n    )\n\n    cnn_ensamble.append(model)\n    training_histories.append(cnn_train_history)\n\n    train_history_df = pd.DataFrame(cnn_train_history.history)\n    best_epoch = np.argmax(train_history_df['val_accuracy'])\n    print(\"CNN {}: Train Accuracy: {:0.4f} Validation Accuracy: {:0.4f}\".format(\n        i, \n        train_history_df['accuracy'][best_epoch], \n        train_history_df['val_accuracy'][best_epoch]\n    ))","43ea371a":"def evaluate_ensamble(ensamble_list, X_test):\n    vote_list = np.zeros((X_test.shape[0], NUM_CLASSES))\n    \n    for model in ensamble_list:\n        vote_list = vote_list + model.predict(X_test)\n        \n    return np.argmax(vote_list, axis=1)","17042517":"for training_history in training_histories:\n    print(plot_training_history(training_history))","e96c8fea":"y_test_true = np.argmax(y_test_onehot, axis=-1)\ny_test_pred = evaluate_ensamble(cnn_ensamble, X_test2d)\n\nprint('Test Report')\nprint(classification_report(y_test_true, y_test_pred, digits=4))\n\nmatrix = plot_confusion_matrix_by_predictions(y_test_true, y_test_pred)\nmatrix.figure_.set_figheight(10)\nmatrix.figure_.set_figwidth(10)","a5715314":"plot_misclassified_samples(y_test_true, y_test_pred, 0, 8)","d0b6a293":"from keract import get_activations, display_activations, display_heatmaps\n\nsample_image = X_test2d[3201].reshape((28, 28, 1))\n\nactivations = get_activations(cnn_ensamble[0], np.array([sample_image]))\n\ndisplay_activations(activations, cmap='binary');","9a3c3ff1":"y_submission = evaluate_ensamble(cnn_ensamble, X_submission2d)\n\nsubmission_label_df = pd.DataFrame({\n    'ImageId': list(range(1, len(y_submission) + 1)), \n    'Label': y_submission\n})\n\nsubmission_label_df.to_csv('.\/cnn_ensamble_submission.csv', index=False, header=True)","80200593":"In our LeNet-like architecture, the first layer is Conv2d layer that's why we need to reshape our flat input to 28x28 matrix to be able to feed it to our CNNs:","8e8fcbfa":"### Submission","ba7628d5":"# Summary\n\nA fundamental computer vision problem such as MNIST can be pretty challenging, especially if you want to understand what is the most precise architecture you can build from the existing blocks.\n\nWe tried 3 models and ended up using CNN ensamble to climb to Top %6 (0.999657%). Thanks to awesome Kaggle community that shared a lot of useful information and knowledge.","bcddefda":"Dataset distribution is nearly uniform (~1% of each kind of digits). Therefore, we can assume that dataset is **balanced**.","24d1ac87":"Even 2-components PCA shows that there are regins in the space with higher densities of particular digits:\n- blue 0s region\n- orange 1s region\n- green 2s region\n- red 3s region\n- violet 4s region\n- magenta 6s region\n\n5s, 7s, 8s, 9s regions are harder to find on the plot.\n","295da01b":"Let's have a quick preview of augmented images:","a24c6888":"Let's take a look at the datset at hand:","d867cf48":"## SVM","2dac8055":"<img style=\"margin-left:0\" src=\"https:\/\/i.dlpng.com\/static\/png\/1280814-best-25-number-writing-practice-ideas-on-pinterest-writing-writing-numbers-png-1280_752_preview.png\" width=\"600px\">\n\n**Digit Recognizer** is a Kaggle competion on the digit MNIST dataset. The goal is to perform OCR on the bunch of handwritten digits and come up with the solution that provides the best accuracy.\n\n- **Github**: https:\/\/github.com\/roma-glushko\/kaggle-digit-recognizer\n- **Experiment Notes**: https:\/\/github.com\/roma-glushko\/kaggle-digit-recognizer\/blob\/master\/experiments.md","29d0f1e0":"## Label Distribution","86bf2541":"Kaggle MNIST is differently distributed than <a href=\"http:\/\/yann.lecun.com\/exdb\/mnist\/\">the original MNIST<\/a>. We have\n- 48,000 examples in the training set (60,000 in the original training set)\n- 28,000 in the test set (10,000 in the original test set)","249bf2df":"**T-SNE disciminates our digits well**. This mean we should try to **use non-linear classification algorightms** to draw decision boundries between points.\n\nAlso, we cannot use T-SN embeddings without hacky approaches as it was designed to visualize relationship and not for **feature representation**.\n\nMore Info:\n- https:\/\/datascience.stackexchange.com\/a\/25928\/57207\n- https:\/\/stackoverflow.com\/a\/54266231\/4371397","ab99040d":"# Classification \ud83e\uddea","8013762b":"## References:\n\n- https:\/\/www.kaggle.com\/jedrzejdudzicz\/mnist-dataset-100-top-6\n- https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n- https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\n- https:\/\/www.kaggle.com\/c\/digit-recognizer\/discussion\/61480\n- https:\/\/www.kaggle.com\/dingli\/digits-recognition-with-cnn-keras","77427d5b":"### Submission","bd78c866":"## Digit Pixel Distribution","3c3fd021":"As we expected, we need **only 154 components** to **cover 95% of digit variance** which is 5 times less. Great \ud83d\ude4c","fa6341fd":"### Submission","6963b028":"### Architecture","467476f7":"### Data Augmentation","b31317aa":"Had not play with MLP a  lot as well. I have tried up to ten different architectures and stopped on the current one. It gives:\n- **~0.9787** on the validation set\n- **0.97614** on Kaggle sumission\n\nThis brings me invisible improvment to what was achived with SVM model. I belive we could do better with MLP model.","830e2d61":"Let's visualize a couple of samples:","0882270f":"PCA is linear approach and we aslo want to try some non-linear visualization approach. \n\nT-SNE is a good candidate to reveal non-linear dependencies in data. T-SNE tries to save distances between a point and its neighbors in the original space when embbeds them in the reduced space. However, this is not necessarily means saving position of points, but rather their structure in the space. As a result, similar\/close points get mapped closer to each other and dissimilar points stay apart.","636405f4":"Eventually we get **validation and test set distributed in the same way** (as well as the original dataset). Thanks to **stratified splitting** that Sklearn performs under the hood.","80c3a002":"### Evaluation","bbd20aca":"## PCA and Manifold Learning","a573c9f7":"We are going to perform just a slight changes to the original images. It's going to be random zooming, rotation and height\/width shifts.\n\nData augmentation can hurt in this task. We can imagine that huge rotation can turn 9s into 6s and visa versa.","d83e7f17":"Digit pictures consists of **28x28=784** pixels, each pixel can take value from 0 to 255 range (gray-scale one channel).","ae9b5942":"I sumbitted only one prediction from the SVM model and got **0.97610**. In general this is pretty decent result. I had not spend a lot of time playing with SVM as this task is mainly for NN models where they can achieve even higher accuracy.","50db20bf":"# Dataset Overview","e39c4f2e":"## Dataset Processing","f11b01e7":"I was little bit in hurry to start playing with **Convolutional Neural Networks** \ud83d\ude4c\n\nCNN is a proven leader in Computer Vision tasks. Convolutional layers are something that makes difference. This is a learnable NN blocks that are capable of extracting feeatures from the images and capture low and high level details and patterns from spartial dimentions. \n\nThis is a section of the notebook where I spent most of the time and burnt most of GPU resources \ud83d\udd25\n\nI have tried 40 different CNN architectures, tens of learning rates and batch sizes, a few optimizers. See full list of my experiments: https:\/\/github.com\/roma-glushko\/kaggle-digit-recognizer\/blob\/master\/experiments.md\n\nAs a result, I kept only the final solution that brought me to **Top 7%** with a accuracy score **0.99657**.\n\nThe solution is an **ensamble of 10x 3-double-convolutional-layers NNs + 128Dense layer with BatchNorms after each layer and 0.4 Droupouts in the end of each layer**. On top of that I used data augmentation with slight degree of random zooming, rotation to make models generalize better. \n\nAlso, using **Adam(learning_rate=1e-3)** boosted to model accuracy (I tried Nadam as well).\n\nEnsamble of NN models helps a lot with measuring a real impact of changes during experiment as NN experiments stay stochastic even when the random seed is fixed.\n\nAn idea of this architecture comes from the following experiments:\n- https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n- https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist","5e9a4e5d":"SVM is pretty **slow** on this amout of data.","ca5a8239":"Let's perform a pixel normalization and then split our full training set into train, validation and test sets:","d28e259e":"Neural Networks are great models to tackle complex non-linear problems where a bunch of data is available for learning from.\nWe will be using **Keras** as a NN framework.\n\nBefore we start building any models, we need to make sure our labels are **onehot encoded** so we can use cross entropy loss function during network training:","0639faf5":"### Evaluation","199e8a39":"### Training","432cb7af":"## CNN","1977e619":"## Multilayer Perceptron","fc97729a":"The very first model we are going to try is **RBF-kerneled SVM classifier on PCA processed dataset**:","f56dd306":"Since our dataset is **balanced** and we need to assign digit class to image representation, this is a **multiclass classification problem**.\nWe can simply use **accuracy** as metric to maximize during experiments.","01a21b1a":"We can immediately recognize that **some of the pixels always contain zero value** accross all samples (pixel0, pixel1, pixel2, pixel3, pixel4, etc). From the image previews, we can see that these pixels are border of the canvas. **Each digit** is centred on the canvas. So most of the images would have **empty borders** which doesn't bring **any new information** for the classication model.\n\nAnother thing to notice: all of the digits are **formed by lines\/curves** and they do take special regions of the canvas (comparing to landscape picture which takes the whole canvas). Also, **pixels in the neighborhood** correlate (digit parts formed by more than 1 pixel lines). So there should be less **degree of freedom** because of that and we most likely don't need the whole 28x28 dimention to classify images. \n\nThis is the reason why we would like to perform **principal component analysis**.","1b9a62d9":"### Architecture","03715060":"For final submission, we need to train our ensamble on the whole training dataset. Let's prepare it here:"}}