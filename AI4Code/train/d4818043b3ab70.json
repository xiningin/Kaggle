{"cell_type":{"7207381f":"code","69faa583":"code","96243f11":"code","d49501e4":"code","57b3399a":"code","4413b367":"code","1798afa8":"code","432c819d":"code","5abb80a8":"code","9a347145":"code","4cdd11fc":"code","28260c37":"code","df9ed85e":"code","a5d44782":"code","11e5c7d1":"code","1ba6886f":"code","d07aff9a":"code","dff58164":"code","be6daa93":"code","173c0c7e":"code","db763b76":"code","3ca90e69":"code","d39e6c60":"code","a9391c3c":"markdown","0f023364":"markdown","eeaf1558":"markdown","ae0be442":"markdown","ee0a45e5":"markdown","044e6e64":"markdown","9d97d89b":"markdown","d3f6bfa3":"markdown","06369143":"markdown","f90a55db":"markdown","42f82df3":"markdown","1069f31d":"markdown","6e100b37":"markdown","4ec7526a":"markdown","5f6690ed":"markdown"},"source":{"7207381f":"# basic packages\nimport os, argparse\nimport json\nimport shutil\nimport warnings\nimport time\nimport psutil\nfrom pathlib import Path\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport re, gc\nfrom typing import Dict\nfrom collections import OrderedDict, defaultdict\n\n# torch related\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR, StepLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport torch.nn.functional as F\n# transformers & tokenizers\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\nimport tokenizers\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom shutil import copyfile\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\",category=UserWarning)","69faa583":"copyfile(src = \"..\/input\/utils-v10\/utilsv10.py\", dst = \"..\/working\/utilsv10.py\")\ncopyfile(src = \"..\/input\/utils-v10\/dataset10.py\", dst = \"..\/working\/dataset10.py\")\ncopyfile(src = \"..\/input\/utils-v10\/dataset11.py\", dst = \"..\/working\/dataset11.py\")\n\nfrom utilsv10 import (binary_focal_loss, get_learning_rate, jaccard_list, get_best_pred, ensemble, ensemble_words,get_char_prob,\n                   load_model, save_model, set_seed, write_event, evaluate, get_predicts_from_token_logits)\n\nfrom dataset10 import TrainDataset, MyCollator\nfrom dataset11 import TrainDataset as TrainDataset11","96243f11":"df_pred = pd.read_csv('..\/input\/tweet-sentiment-fast\/test_all_post_finetune_0608.csv') #lb724\ndf_pred1 = pd.read_csv('..\/input\/tweet-sentiment-fast\/test_all_post_finetune_large.csv') #lb717\ndf_train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n\ndf_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n#df_test = pd.read_csv('..\/input\/tweet-sentiment-fast\/test_hidden.csv')\ndf_test.loc[:, 'selected_text'] = df_test.text.values\ndf_test['text_clean'] = df_test['text'].apply(lambda x: \" \".join(x.split()))\n\ndf_full = pd.read_csv('\/kaggle\/input\/complete-tweet-sentiment-extraction-data\/tweet_dataset.csv')\ndf_full = df_full[df_full.text.notnull()].copy()\ndf_full['text_clean'] = df_full['text'].apply(lambda x: \" \".join(x.split()))\ndf_full = df_full.drop_duplicates(subset='text')\ndf_full = df_full[~df_full.aux_id.isin(df_train.textID)]\ndf_full.rename(columns={'sentiment': 'raw_sentiment'}, inplace=True)","d49501e4":"def find_sentiment(textID, text, sentiment):\n    text_clean = \" \".join(text.split())\n    if textID in df_full.aux_id.values:\n        return df_full['raw_sentiment'].loc[df_full.aux_id==textID].values[0]\n    elif text in df_full.text.values:\n        return df_full['raw_sentiment'].loc[df_full.text==text].values[0]\n    elif text_clean in df_full.text_clean.values:\n        return df_full['raw_sentiment'].loc[df_full.text_clean==text_clean].values[0]\n    else:\n        return sentiment","57b3399a":"#%%time\n# find raw sentiment\ndf_test['raw_sentiment'] = df_test.apply(lambda x: find_sentiment(x['textID'], x['text'], x['sentiment']), axis=1)\nna_mask = df_test.raw_sentiment.isnull()\nprint(na_mask.sum())","4413b367":"#test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntest = df_test.copy()\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base\/', do_lower_case=False)\n\nclass Args:\n    post = True\n    tokenizer = tokenizer\n    offset = 4\n    batch_size = 4\n    workers = 0\nargs = Args()\n\n# v11\nclass Args11:\n    post = True\n    tokenizer = tokenizer\n    offset = 7\n    batch_size = 4\n    workers = 0\nargs11 = Args11()","1798afa8":"class TweetModel(nn.Module):\n\n    def __init__(self, pretrain_path=None, dropout=0.2, config=None):\n        super(TweetModel, self).__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrain_path, output_hidden_states=True)\n            self.bert = AutoModel.from_pretrained(\n                pretrain_path, cache_dir=None, config=config)\n        \n        self.cnn =  nn.Conv1d(self.bert.config.hidden_size*3, self.bert.config.hidden_size, 3, padding=1)\n\n        # self.rnn = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size\/\/2, num_layers=2,\n        #                     batch_first=True, bidirectional=True)\n        self.gelu = nn.GELU()\n\n        self.whole_head = nn.Sequential(OrderedDict([\n            ('dropout', nn.Dropout(0.1)),\n            ('l1', nn.Linear(self.bert.config.hidden_size*3, 256)),\n            ('act1', nn.GELU()),\n            ('dropout', nn.Dropout(0.1)),\n            ('l2', nn.Linear(256, 2))\n        ]))\n        self.se_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.inst_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, inputs, masks, token_type_ids=None, input_emb=None):\n        _, pooled_output, hs = self.bert(\n            inputs, masks, token_type_ids=token_type_ids, inputs_embeds=input_emb)\n\n        seq_output = torch.cat([hs[-1],hs[-2],hs[-3]], dim=-1)\n\n        # seq_output = hs[-1]\n\n        avg_output = torch.sum(seq_output*masks.unsqueeze(-1), dim=1, keepdim=False)\n        avg_output = avg_output\/torch.sum(masks, dim=-1, keepdim=True)\n        # +max_output\n        whole_out = self.whole_head(avg_output)\n\n        seq_output = self.gelu(self.cnn(seq_output.permute(0,2,1)).permute(0,2,1))\n        \n        se_out = self.se_head(self.dropout(seq_output))  #()\n        inst_out = self.inst_head(self.dropout(seq_output))\n        return whole_out, se_out[:, :, 0], se_out[:, :, 1], inst_out","432c819d":"def predict_wu(model: nn.Module, valid_df, valid_loader, args, progress=False) -> Dict[str, float]:\n    # run_root = Path('..\/experiments\/' + args.run_root)\n    model.eval()\n    all_end_pred, all_whole_pred, all_start_pred, all_inst_out = [], [], [], []\n    if progress:\n        tq = tqdm.tqdm(total=len(valid_df))\n    with torch.no_grad():\n        for tokens, types, masks, _, _, _, _, _, _, _ in valid_loader:\n            if progress:\n                batch_size = tokens.size(0)\n                tq.update(batch_size)\n            masks = masks.cuda()\n            tokens = tokens.cuda()\n            types = types.cuda()\n            whole_out, start_out, end_out, inst_out = model(tokens, masks, types)\n            \n            all_whole_pred.append(torch.softmax(whole_out, dim=-1)[:,1].cpu().numpy())\n            inst_out = torch.softmax(inst_out, dim=-1)\n            for idx in range(len(start_out)):\n                length = torch.sum(masks[idx,:]).item()-1 # -1 for last token\n                all_start_pred.append(torch.softmax(start_out[idx, args.offset:length], axis=-1).cpu())\n                all_end_pred.append(torch.softmax(end_out[idx, args.offset:length], axis=-1).cpu())\n                all_inst_out.append(inst_out[idx,:,1].cpu())\n            assert all_start_pred[-1].dim()==1\n\n    all_whole_pred = np.concatenate(all_whole_pred)\n    \n    if progress:\n        tq.close()\n    return all_whole_pred, all_start_pred, all_end_pred, all_inst_out","5abb80a8":"# convrt part I char-level probability to clean text version (an array of length 160)\ndef _convrt_prob_partI(text, prob, max_char_len=160):\n    clean_text = \" \".join(text.split())\n    new_prob = []\n    p1, p2 = 0, 0\n    while p1 < len(text):\n        if text[p1] not in [\" \", \"\\t\", \"\\xa0\"]:\n            if text[p1] == clean_text[p2]:\n                new_prob.append(prob[p1])\n                p1 += 1\n                p2 += 1\n        else:\n            if p1 + 1 < len(text) and text[p1+1] not in [\" \", \"\\t\", \"\\xa0\"]:\n                if clean_text[p2] == \" \":\n                    new_prob.append(prob[p1])\n                    p1 += 1\n                    p2 += 1\n                else:\n                    p1 += 1                   \n            else:\n                p1 += 1\n    if len(new_prob) < max_char_len:\n        new_prob = new_prob + (max_char_len - len(new_prob))*[0]\n    return new_prob","9a347145":"def get_prediction_partI(weights_path, test, test_loader, args, output_name):\n    # load model\n    config = RobertaConfig.from_pretrained('..\/input\/roberta-base', output_hidden_states=True)\n    model = TweetModel(config=config)\n\n    # 10-fold predict\n    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = [], [], [], []  \n    for fold in range(10):\n        load_model(model, f'{weights_path}\/best-model-%d.pt' % fold)\n        model.cuda()\n        fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds = predict_wu(model, test, test_loader, args, progress=True)\n\n        all_whole_preds.append(fold_whole_preds)\n        all_start_preds.append(fold_start_preds)\n        all_end_preds.append(fold_end_preds)\n        all_inst_preds.append(fold_inst_preds)\n\n\n    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = ensemble(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test)\n    word_preds, inst_word_preds, scores = get_predicts_from_token_logits(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test, args)\n    # word_preds, inst_word_preds, scores = get_predicts_from_token_logits(fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds, test, args)\n    start_char_prob, end_char_prob = get_char_prob(all_start_preds, all_end_preds, test, args)\n    \n    test['start_char_prob'] = start_char_prob\n    test['end_char_prob'] = end_char_prob\n    test['selected_text'] = word_preds\n\n    test['prob_start'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['start_char_prob']), axis=1)\n    test['prob_end'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['end_char_prob']), axis=1)\n\n    test.to_pickle(f'{output_name}.pkl')\n    np.save(f\"start_{output_name}.npy\", np.array(test['prob_start'].tolist()))\n    np.save(f\"end_{output_name}.npy\", np.array(test['prob_end'].tolist()))","4cdd11fc":"collator = MyCollator()\ntest_set = TrainDataset(test, None, tokenizer=tokenizer, mode='test', offset=args.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                         num_workers=args.workers)\nget_prediction_partI(weights_path='..\/input\/roberta-v10-10',\n                     test=test, \n                     test_loader=test_loader, \n                     args=args, \n                     output_name='output_v10',\n                    )\n\nmem = psutil.virtual_memory()\nprint(f'{mem.percent:5} - {mem.free\/1024**3:10.2f} - {mem.available\/1024**3:10.2f} - {mem.used\/1024**3:10.2f}')","28260c37":"collator = MyCollator()\ntest_set = TrainDataset11(test, None, tokenizer=tokenizer, mode='test', offset=args11.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                         num_workers=args.workers)\nget_prediction_partI(weights_path='..\/input\/roberta-v11-10',\n                     test=test, \n                     test_loader=test_loader, \n                     args=args11, \n                     output_name='output_v11',\n                    )\n\nmem = psutil.virtual_memory()\nprint(f'{mem.percent:5} - {mem.free\/1024**3:10.2f} - {mem.available\/1024**3:10.2f} - {mem.used\/1024**3:10.2f}')","df9ed85e":"# import helper scripts\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/tweet-sentiment\/common.py\", dst = \"..\/working\/common.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/dataset.py\", dst = \"..\/working\/dataset.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/models.py\", dst = \"..\/working\/models.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/metrics.py\", dst = \"..\/working\/metrics.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/utils.py\", dst = \"..\/working\/utils.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/predict_fn.py\", dst = \"..\/working\/predict_fn.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/nlp_albumentations.py\", dst = \"..\/working\/nlp_albumentations.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/transform.py\", dst = \"..\/working\/transform.py\")\ncopyfile(src = \"..\/input\/tweet-sentiment\/run_inference_kaggle.py\", dst = \"..\/working\/run_inference_kaggle.py\")\n\nfrom dataset import process_data\nfrom dataset import TweetDataset_kaggle as TweetDataset\nfrom models import TweetModel, TweetModel_v2\nfrom common import *\nfrom metrics import *\nfrom utils import *\nfrom utils import _convrt_back\n\nset_seed(42)\n\n# # %% [code]\n# !python run_inference_kaggle.py --model_name='roberta_base' \\\n#                                 --model_path='..\/input\/roberta-base\/' \\\n#                                 --raw_sentiment=1\n\n# mem = psutil.virtual_memory()\n# print(f'{mem.percent:5} - {mem.free\/1024**3:10.2f} - {mem.available\/1024**3:10.2f} - {mem.used\/1024**3:10.2f}')\n\n# # %% [code]\n# !python run_inference_kaggle.py --model_name='roberta_base_noRawSenti' \\\n#                                 --model_path='..\/input\/roberta-base-bs32-v2-0608' \\\n#                                 --raw_sentiment=0\n\n# mem = psutil.virtual_memory()\n# print(f'{mem.percent:5} - {mem.free\/1024**3:10.2f} - {mem.available\/1024**3:10.2f} - {mem.used\/1024**3:10.2f}')","a5d44782":"def get_model(model_name):\n    # return model and tokenizer\n    if model_name.startswith('roberta_base'):\n        model_info = {\n            'name': 'roberta-base',\n            'model_path': '..\/input\/roberta-base' if ON_KAGGLE else 'roberta-base',\n            'from_pretrained': False if ON_KAGGLE else True,\n            'vocab_file': '..\/input\/roberta-base\/vocab.json',\n            'merges_file': '..\/input\/roberta-base\/merges.txt',\n        }\n    elif model_name.startswith('roberta_large'):\n        model_info = {\n            'name': 'roberta-large',\n            'model_path': '..\/input\/roberta-large' if ON_KAGGLE else 'roberta-large',\n            'from_pretrained': False if ON_KAGGLE else True,\n            'vocab_file': '..\/input\/roberta-large\/vocab.json',\n            'merges_file': '..\/input\/roberta-large\/merges.txt',\n        }\n    else:\n        raise RuntimeError('%s is not implemented.' % model_name)\n\n    model = TweetModel_v2(model_info)\n    tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=model_info['vocab_file'],\n        merges_file=model_info['merges_file'],\n        lowercase=False,\n        add_prefix_space=True\n    )\n    return model, tokenizer","11e5c7d1":"N_FOLD = 10\nPOST_PROCESS = True\nparams = {\n    'models': [\n        #'roberta_base', \n        'roberta_base_noRawSenti', \n        'roberta_large',\n    ],\n    'batch_size': 4,\n    'workers': 1 if ON_KAGGLE else 8,\n    'max_len': 192, \n    'folds': list(x for x in range(N_FOLD)),\n    'limit': 0,\n}\n\npath_lib = {\n    'roberta_base': '..\/input\/roberta-base\/',\n    'roberta_base_noRawSenti': '..\/input\/roberta-base-bs32-v2-0608',\n    'roberta_large': '..\/input\/roberta-large-bs32-v2-0608',\n}","1ba6886f":"def predict_II(model, data_loader, tokenizer):\n    start_probs, end_probs = [], []\n    with torch.no_grad():\n        for i, d in enumerate(tqdm.tqdm(data_loader, ascii=True)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            decode_selected = d[\"decode_selected\"]\n            raw_tweets = d[\"raw_tweet\"]\n            raw_selecteds = d[\"raw_selected_text\"]\n            text_span = d[\"text_span\"]\n            \n            ids = ids.cuda()\n            token_type_ids = token_type_ids.cuda()\n            mask = mask.cuda()\n            \n            start_idxs, end_idsx = [], []\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            if len(outputs) == 2:\n                outputs_start, outputs_end = outputs\n                outputs_mask = None\n            elif len(outputs) == 3:\n                outputs_start, outputs_end, outputs_mask = outputs   \n            # probability\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy() \n            \n            for px, tweet in enumerate(orig_tweet):\n                # char level probs\n                raw_tweet = raw_tweets[px]\n                span_start, span_end = text_span[0][px], text_span[1][px]\n                span_start, span_end = int(span_start), int(span_end)\n                token_ids = d[\"ids\"][px][span_start: span_end]\n                clean_text = \" \".join(raw_tweet.split())\n                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_start[px, span_start: span_end], tokenizer)\n                if len(prob_char) < 160: # padding\n                    prob_char += [0] * (160 - len(prob_char))\n                start_probs.append(prob_char)\n                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_end[px, span_start: span_end], tokenizer)\n                if len(prob_char) < 160: # padding\n                    prob_char += [0] * (160 - len(prob_char))\n                end_probs.append(prob_char)\n    start_probs = np.array(start_probs)\n    end_probs = np.array(end_probs)\n    return start_probs, end_probs","d07aff9a":"# predict\nfor model_name in params['models']:\n    model, tokenizer = get_model(model_name)\n    if model_name.endswith(\"noRawSenti\"):\n        df_test_tmp = df_test.copy()\n        df_test_tmp['raw_sentiment'] = \"\"\n        tweet_dataset = TweetDataset(\n            df=df_test_tmp,\n            sentiment_weights=[1,1,1],\n            tokenizer=tokenizer,\n            mode='test',\n            lower_case=0,\n            max_len=params['max_len'],\n        )\n    else:\n        tweet_dataset = TweetDataset(\n            df=df_test,\n            sentiment_weights=[1,1,1],\n            tokenizer=tokenizer,\n            mode='test',\n            lower_case=0,\n            max_len=params['max_len'],\n        )      \n        \n    data_loader = DataLoader(\n        tweet_dataset,\n        batch_size=params['batch_size'],\n        num_workers=0,\n    )\n        \n    for i, fold in enumerate(params['folds']):\n        if model_name == \"roberta_base_noRawSenti\":\n            if fold < 5:\n                path = path_lib[model_name] + '-part2'\n            else:\n                path = path_lib[model_name]\n        elif model_name == \"roberta_large\":\n            if fold <= 2:\n                path = path_lib[model_name] + '-part1'\n            elif fold <= 5:\n                path = path_lib[model_name] + '-part2'\n            elif fold == 6:\n                path = path_lib[model_name] + '-part3-2'\n            elif fold == 7:\n                path = path_lib[model_name] + '-part3'\n            else:\n                path = path_lib[model_name] + '-part4'\n        else:\n            path = path_lib[model_name]\n        load_model(model, f\"{path}\/best_jac_{fold}.pt\", multi2single=False)\n        model.cuda()\n        model.eval()\n        probs_start_pred, probs_end_pred = predict_II(model, data_loader, tokenizer)\n        if i == 0:\n            probs_start = probs_start_pred\n            probs_end = probs_end_pred\n        else:\n            probs_start += probs_start_pred\n            probs_end += probs_end_pred \n            \n    probs_start \/= len(params['folds'])\n    probs_end \/= len(params['folds'])\n\n    df_test['prob_start'] = probs_start.tolist()\n    df_test['prob_end'] = probs_end.tolist()\n    df_test.to_pickle(f'{model_name}.pkl')\n    np.save(f\"start_{model_name}.npy\", probs_start)\n    np.save(f\"end_{model_name}.npy\", probs_end)","dff58164":"# load .npy file from disk and ensemble char level probability\n\nmodel_names = [\n               #'roberta_base', \n               'roberta_base_noRawSenti',\n               'roberta_large',\n               'output_v10',\n               'output_v11',\n              ]\nfor i, model_name in enumerate(model_names):\n    prob_start_tmp = np.load(f'start_{model_name}.npy')\n    prob_end_tmp = np.load(f'end_{model_name}.npy')\n    if i == 0:\n        prob_start = prob_start_tmp\n        prob_end = prob_end_tmp\n    else:\n        prob_start += prob_start_tmp\n        prob_end += prob_end_tmp\n\nprob_start \/= len(model_names)\nprob_end \/= len(model_names)\n        \ndf_test['prob_start'] = prob_start.tolist()\ndf_test['prob_end'] = prob_end.tolist()","be6daa93":"def _get_pred_char(df, probs_start, probs_end):\n    df['start_idx'] = np.argmax(probs_start, axis=1)\n    df['end_idx'] = probs_end.shape[1] - np.argmax(probs_end[:, ::-1], axis=1) - 1\n    df['prob_start'] = probs_start.tolist()\n    df['prob_end'] = probs_end.tolist()\n    idxs = np.where(df.start_idx > df.end_idx)\n    \n    for idx in idxs[0]:\n        prob_start = df.prob_start.values[idx]\n        prob_end = df.prob_end.values[idx]\n        start_idx = df.start_idx.values[idx]\n        end_idx = df.end_idx.values[idx]\n        if prob_start[start_idx] > prob_end[end_idx] or end_idx == 0:\n            end_idx = len(prob_start) - np.argmax(prob_end[start_idx:][::-1]) - 1\n        else:\n            start_idx = np.argmax(prob_start[:end_idx])\n        df['start_idx'].iloc[idx] = start_idx     \n        df['end_idx'].iloc[idx] = end_idx    \n    #df.rename(columns={'selected_text': 'pred'}, inplace=True)\n    df['pred_char'] = df.apply(lambda x: x['text_clean'][x['start_idx']: x['end_idx']+1], axis=1)\n    return df","173c0c7e":"def post_neutral(df):\n    df['select_pt'] = df.apply(lambda x: len(x['pred_char'].strip())\/len(x['text_clean']), axis=1).values\n\n    raw_sents = ['neutral', 'sadness', 'worry', 'happiness', 'love', 'enthusiasm']\n    mm = (df['sentiment'] == 'neutral') & (df['raw_sentiment'].isin(raw_sents))\n    mm = (mm |\\\n          ((df.select_pt > 0.85) & (df.sentiment.isin(['positive']))) |\\\n          ((df.select_pt > 0.85) & (df.sentiment.isin(['negative']))) |\\\n          ((df.select_pt < 0.2) & (df.sentiment.isin(['neutral']))))\n    \n    df['pred_exp'] = df['pred_char'].values\n    df['pred_exp'].loc[mm] = df['text_clean'].loc[mm].values\n    print(f\"# of modified samples: {np.sum(df['pred_exp'] != df['pred_char'])}\")\n    return df\n\ndef _post_shift_new(text, pred):\n    clean_text = \" \".join(text.split())\n    start_clean = clean_text.find(\" \".join(pred.split()))\n    \n    raw_pred = _convrt_back(text, pred, \"neutral\")\n    raw_pred = raw_pred.strip()\n    start = text.find(raw_pred)\n    end = start + len(raw_pred)\n    \n    extra_space = start - start_clean \n    \n    if start>extra_space and extra_space>0:\n        if extra_space==1:\n            if text[start-1] in [',','.','?','!'] and text[start-2]!=' ':\n                start -= 1\n        elif extra_space==2:\n            start -= extra_space\n            if text[end-1] in [',','.','!','?','*']:\n                end -= 1\n        else:\n            end -= (extra_space-2)\n            start -= extra_space\n    \n    pred = text[start:end]\n    if pred.count(\"'\") == 1:\n        if pred[0] == \"'\":\n            if text.find(pred) + len(pred) < len(text) and text[text.find(pred) + len(pred)] == \"'\":\n                pred += \"'\"\n        else:\n            if text.find(pred) - 1 >= 0 and text[text.find(pred) - 1] == \"'\":\n                pred = \"'\" + pred   \n                \n    return pred\n\ndef post_shift(df):\n    df['pred_final'] = df['pred_exp'].copy()\n    df['jac_text'] = df.apply(lambda x: jaccard(x['text'], x['pred_exp']), axis=1)\n    mask = (df.sentiment != 'neutral') & (df.start_idx != 0) & (df.jac_text != 1)\n    df['pred_final'].loc[mask] = df.apply(lambda x: _post_shift_new(x['text'], x['pred_exp']), axis=1).loc[mask].values\n    jac = df.apply(lambda x: jaccard(x['pred_exp'], x['pred_final']), axis=1)\n    print(f\"# of modified samples: {np.sum(jac != 1)}\")\n    return df","db763b76":"# _get_pred_char bug ==> lb 717 (pred1)\n# POST_PROCESS bug ==> lb 0\nPOST_PROCESS=True\ntry:\n    df_test = _get_pred_char(df_test, np.array(df_test.prob_start.tolist()), np.array(df_test.prob_end.tolist()))\n    try:\n        if POST_PROCESS:\n            df_test = post_neutral(df_test)\n            df_test = post_shift(df_test)\n            df_test['selected_text'] = df_test['pred_final'].values\n        else:\n            df_test['selected_text'] = df_test['pred_char'].values\n    except:\n        df_test['selected_text'] = \" \"\nexcept:\n    df_test = pd.merge(df_test[['textID']], df_pred1, how='left', on='textID')","3ca90e69":"df_test.to_csv(\"raw_prediction.csv\", index=False)\ndf_sub = df_test[['textID', 'selected_text']].copy()\ndf_sub.to_csv(\"submission.csv\", index=False)","d39e6c60":"df_sub.head()","a9391c3c":"## Predict","0f023364":"### V10 prediction","eeaf1558":"## Preapare data","ae0be442":"## Prediction","ee0a45e5":"### V11 prediction","044e6e64":"## Helper functions & params","9d97d89b":"# Ensemble of different models","d3f6bfa3":"## Model","06369143":"# Model Inference Part I","f90a55db":"# Post processing","42f82df3":"# Load packages and scripts","1069f31d":"# Save prediction","6e100b37":"## Dataloader, model","4ec7526a":"## Parse data","5f6690ed":"# Model Inference Part II"}}