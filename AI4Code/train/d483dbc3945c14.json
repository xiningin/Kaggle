{"cell_type":{"6caeae04":"code","e6ff5b1f":"code","7a526fef":"code","8516c94a":"code","b8997188":"code","147fdff2":"code","f96263d2":"code","11e1f56a":"code","75f1d2ca":"code","7f6c8c6b":"code","2bf9b4ff":"code","3611768a":"code","0d8c41a7":"code","6f6f9275":"code","f2e7fee1":"code","171eb71e":"code","b985b62c":"code","3d683b73":"code","1d9f2d26":"code","a8ef0a51":"code","8591728d":"code","a3a2e639":"code","abd4646c":"code","880025bd":"code","8df98d14":"code","b6d02b11":"code","c8f11dfe":"code","220ae485":"code","6c5dea32":"code","90f3590c":"code","a74f2a01":"code","9ee473be":"code","527c4110":"code","257494de":"markdown","2a78101d":"markdown","e121b783":"markdown","1f20bc7d":"markdown","eea3b2d1":"markdown","6d8e5939":"markdown","1b16d840":"markdown","7690b521":"markdown","e08ceeeb":"markdown","b2fb9c54":"markdown","42efe43f":"markdown","922e4a5e":"markdown","e6902b4b":"markdown","fdfa7388":"markdown","52619b17":"markdown","97e5b3bf":"markdown","ff647753":"markdown","1e542d44":"markdown","dcb20dbc":"markdown","3429531e":"markdown","79e1079e":"markdown","d08af768":"markdown","f6d89bdc":"markdown","34a66e03":"markdown","1698c708":"markdown"},"source":{"6caeae04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ni = 10\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        if i > 10:\n            break\n        i += 1\n\n# Any results you write to the current directory are saved as output.","e6ff5b1f":"data = pd.read_csv('\/kaggle\/input\/mytitanic\/train.csv')\nnumeric_columns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nnumeric_data = data[numeric_columns]\nlabels = data['Survived']","7a526fef":"from sklearn.preprocessing import LabelEncoder\n\nnumeric_data['Age'] = data['Age'].fillna(numeric_data['Age'].median())\nnumeric_data['Sex'] = LabelEncoder().fit_transform(data['Sex'])","8516c94a":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(numeric_data, labels, test_size=0.2, random_state=42)\n\nrfc = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5, min_samples_leaf=1, \n                                    min_weight_fraction_leaf=0.0, random_state=20)\nrfc.fit(X_train, y_train)\nX_train_preds_rfc = rfc.predict(X_train)\nX_test_preds_rfc = rfc.predict(X_test)\n\n# End\nprint(accuracy_score(y_train, X_train_preds_rfc))\nprint(accuracy_score(y_test, X_test_preds_rfc))","b8997188":"# Your code here\n\nX_train = \nX_test = \n# End","147fdff2":"# Your code here\n\nmodel = \n\n# End\n\nmodel.summary()","f96263d2":"# Your code here\n\n# End\n","11e1f56a":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\ndef plot_learning_history(model, epochs_num):\n    x = np.arange(epochs_num)\n    history = model.history.history\n\n    hist = [\n        go.Scatter(x=x, y=history[\"accuracy\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n        go.Scatter(x=x, y=history[\"val_accuracy\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n        go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n        go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n    ]\n    layout = go.Layout(\n        title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n        yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n    )\n    py.iplot(go.Figure(data=hist, layout=layout), show_link=False)\n","75f1d2ca":"# Your code here\n\n# End","7f6c8c6b":"# Your code here\n\nX_train_preds_nn = \nX_test_preds_nn = \n\n# End\n\nprint(accuracy_score(y_train, X_train_preds_nn))\nprint(accuracy_score(y_test, X_test_preds_nn))","2bf9b4ff":"path = \"\/kaggle\/input\/imdb-movie-reviews-dataset\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\ntestFiles = [x for x in os.listdir(path+\"test\/\") if x.endswith(\".txt\")]\npositiveReviews, negativeReviews, testReviews = [], [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\nfor tfile in testFiles:\n    with open(path+\"test\/\"+tfile, encoding=\"latin1\") as f:\n        testReviews.append(f.read())\nreviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n    pd.DataFrame({\"review\":testReviews, \"label\":-1, \"file\":testFiles})\n], ignore_index=True).sample(frac=1, random_state=1)\n","3611768a":"reviews.head()","0d8c41a7":"print(reviews.iloc[0]['review'])","6f6f9275":"# Your code here\n\n# End","f2e7fee1":"X.shape","171eb71e":"# Your code here\n\n# End","b985b62c":"X.shape","3d683b73":"# Your code here\n\n# End","1d9f2d26":"X_train, X_test, y_train, y_test = train_test_split(X, reviews['label'].values, test_size=0.2, random_state=42)","a8ef0a51":"print(X_train.shape)\nprint(X_test.shape)","8591728d":"# Your code here\n\nmodel = \n\n# End\n\nmodel.summary()","a3a2e639":"# Your code here\n\n# End","abd4646c":"# Your code here\n\n# End","880025bd":"unseen_sentence = [\"this movie very good\"]\n\n# Your code here\n\nprobability = \n\n# End\n\nprint(probability)\nprint(\"Positive!\") if probability > 0.5 else print(\"Negative!\")","8df98d14":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models,layers\n\nMAX_LENGTH = 220\nMAX_WORDS = 10000\nEMBEDDING_DIM = 300\n\n# Extract tokens from reviews\ntokenizer = Tokenizer(num_words=MAX_WORDS)\ntokenizer.fit_on_texts(reviews['review'])\n\n# Transform words to indices\nsequences = tokenizer.texts_to_sequences(reviews['review'])\nword_index = tokenizer.word_index\n\nprint(f'Found {len(word_index)} unique tokens.' )\n\n# Pad shorter sentences with 0 and cut longer sentences\ndata = pad_sequences(sequences, maxlen=MAX_LENGTH)\n\n# Get labels\nlabels = np.array(reviews['label'])","b6d02b11":"X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)","c8f11dfe":"print(f'Shape of Data tensor is {data.shape}')\nprint(f'Shape of Labels tensor is {labels.shape}')","220ae485":"from gensim.models import KeyedVectors\n\ndef build_matrix(word_index, path):\n    embedding_index = KeyedVectors.load(path, mmap='r')\n    embedding_matrix = np.zeros((MAX_WORDS + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        for candidate in [word, word.lower()]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                break\n        if i == MAX_WORDS:\n            break\n    return embedding_matrix","6c5dea32":"embeddings_path = \"\/kaggle\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim\"\n\nembedding_matrix = build_matrix(word_index, embeddings_path)","90f3590c":"model = models.Sequential()\nmodel.add(layers.Embedding(MAX_WORDS + 1, EMBEDDING_DIM, input_length=MAX_LENGTH))\nmodel.add(layers.SpatialDropout1D(0.4))\nmodel.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.summary()","a74f2a01":"model.layers[0].set_weights([embedding_matrix])","9ee473be":"from keras import optimizers\n\nEPOCHS=3\n\nmodel.compile(optimizer='rmsprop', \n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(X_train, y_train,\n                    epochs=EPOCHS,\n                    batch_size=256,\n                    validation_data=(X_test, y_test))","527c4110":"plot_learning_history(model, EPOCHS)","257494de":"4 c)","2a78101d":"Now we train a model that is able to process sequential data. It is called LSTM and is a type of RNN (Recurrent neural networks). It's beyond the scope of the laboratory class, but you'll learn about it during the lecture.\n\nWe define the Embedding layer that is a lookup table for word embeddings. We initialize the embedding layer (weights) with trained word embeddings.","e121b783":"We now prepare splits:","1f20bc7d":"We prevent embeddings from training, so we can use it as is. ","eea3b2d1":"**6: Define and compile the neural network for sentiment analysis. **\n","6d8e5939":"## Word embeddings\n\n[Word embeddings](https:\/\/en.wikipedia.org\/wiki\/Word_embedding) are dense high-dimensional (but low dimensional compared to tf-idf) vectors that place words with similar semantics in the same area of the hyperspace. \n\nNote that pre-trained word-embeddings are attached in the Notebook. \n\n**9: Read and run the following code carefully **\n \nWe're going to train two models - one with randomly initialized embeddings, one with pre-trained embeddings. ","1b16d840":"Plot learning history using the function below:","7690b521":"Why history shows different accuracy than `accuracy_score`?","e08ceeeb":"** 5: Optional: Print vocabulary of your `TfidfVectorizer` to understand this representation **\n\nYou can see that a given element in vector represent different word. ","b2fb9c54":"** 3: Think about what to do to maximize the training score. Create a model that will try to overfit training data as strong as possible. Get at least 85% training score.**\n\nYou don't have to write the code again, you can use the code above if you want to.","42efe43f":"Plot learning history using the `plot_learning_history()` function","922e4a5e":"4 d)","e6902b4b":"## Class 3 - deep learning","fdfa7388":"**2: Train the network to maximize test score. **\n\na) Try to get at least the score of Random Forest\n\nb) For `fit()` method, use argument `validation_data`, pass `(X_test, y_test)` tuple\n\n**Watchout! When you want to run `fit()` again, you have create `model` variable again. Otherwise it's continuing the next epochs of fitting","52619b17":"## Training first neural network\n\n**1: Plese use `keras` library to define and compile Neural Network.**\n\nWhat is the neural network build with?\n\n1. Layers with hidden units\n2. Activation functions\n3. Loss function\n4. Way of upgrading the weights (so-called optimizer), f.e. 'sgd' or 'adam'\n5. Learning rate for the optimizer\n6. Batch size\n7. Number of epochs\n\nYou may need some regularization also. You can choose dropout, L-2 penalty or batch-normalization too.\n\nNote that a casual feed-forward layer in `keras` is called `Dense`\n\n**Please name the variable with model `model`**\n\n**Please add argument `metrics=['accuracy']` to `compile` method**","97e5b3bf":"Verify the splits:","ff647753":"Example review:","1e542d44":"## Data preparation\n\nDo neural networks need standardized data? Why? Does RandomForest need it? Why?\n\n**0: Optional: Standardize data**\n\nYou can use `StandardScaler` from `sklearn.preprocessing`","dcb20dbc":"## TFIDF features\n\nTFIDF features ([here](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf) or [here](http:\/\/datameetsmedia.com\/bag-of-words-tf-idf-explained\/)) were explained during the lecture. We can use it to represent the text and input it to the neural network.\n\nNote that Bag-of-words methods are high-dimensional [sparse](https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix) vectors. Therefore please perform the following steps:\n\n**4: Implement steps:**\n\na) Use `TfidfVectorizer` from sklearn to extract features. Extract features from reviews to `X` variable\n\nb) Check the dimension (number of features) of the resulting vectors\n\nc) Reduce the number of features using `SelectKBest` end some algorithm f.e. `f_classif`. Leave 10k features.\n\nd) Verify whether your new dimensions is equal to 10k\n\nPackages: `sklearn.feature_extraction.text` and `sklearn.feature_selection`\n\n4 a)\n","3429531e":"\n4 b)","79e1079e":"Use `predict_classes` method to predict classes for train and test data.","d08af768":"** 8: Test your model on your sentence. **\n\nTo get the probability for input vector, you should use `model.predict()`. Note that you have to use your trained vectorizer, features selector, and model.","f6d89bdc":"**7: Train the neural network for sentiment analysis **\n\na) You should get over 90% accuracy\n\nb) For `fit()` method, use argument `validation_data`, pass `(X_test, y_test)` tuple","34a66e03":"## Preparing the experiment\n\nLet's load and derive data from previous classes and train a model.","1698c708":"## Real task introduction: sentiment analysis\n\nSentiment analysis is the automated process that uses AI to identify positive and negative opinions from the text. Sentiment analysis may be used for getting insights from social media comments, responses from surveys or product reviews, and making data-driven decisions.\n\nWe can load the data from the attached files"}}