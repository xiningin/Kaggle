{"cell_type":{"a6610404":"code","3de08238":"code","fb01039a":"code","ca72a7a2":"code","fc346a38":"code","2163c4dc":"code","998d0a8a":"code","b7c65136":"code","245c0c87":"code","2ffdf0cb":"code","06285c95":"code","562e6ae0":"code","e3611493":"code","39ceb2b0":"code","826fdaf2":"code","1f37a9db":"code","9192d3c2":"code","c86efd92":"code","fe1e6ce2":"code","1d0f12ea":"markdown","ad75993b":"markdown"},"source":{"a6610404":"#Bibliotecas\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","3de08238":"#Pegando arquivos .csv \ndf = pd.read_csv('..\/input\/dataset_treino.csv',sep=',', encoding='ISO-8859-1')\ndfTest = pd.read_csv('..\/input\/dataset_teste.csv',sep=',', encoding='ISO-8859-1')\ndf.head(10)","fb01039a":"#Shape Treino e Teste\nprint(\"Shape df Treino: \", df.shape)\nprint (\"Shape df Teste: \", dfTest.shape)","ca72a7a2":"#Data Types\nprint (\"==== Treino ==== \\n\", df.dtypes)\nprint (\"\\n ==== Teste ==== \\n\", dfTest.dtypes)","fc346a38":"#Colunas Com valores nulos\nprint(\"==== Treino ====\\n\", df.isnull().any())\nprint(\"\\n ==== Teste ==== \\n\", dfTest.isnull().any())","2163c4dc":"#Tendo uma no\u00e7\u00e3o da distribui\u00e7\u00e3o das classes\ndf['classe'].value_counts()","998d0a8a":"#Bibliotecas de visualiza\u00e7\u00e3o\nimport matplotlib.pyplot as plt\n%matplotlib inline","b7c65136":"#Histograma\ndf.hist()\nplt.show()","245c0c87":"#Visualizando as correla\u00e7\u00f5es entre as vari\u00e1veis numericamente\ncorrelacoes = df.corr()\nprint (correlacoes)","2ffdf0cb":"#Visualizando as correla\u00e7\u00f5es entre as vari\u00e1veis graficamente\ncols = ['id', 'num_gest', 'glic', 'pres_sang','gros_pele', 'insul', 'bmi', 'ind_hist', 'idade','classe']\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\ncax = ax.imshow(correlacoes, interpolation='nearest', vmin = -1, vmax = 1)\nfig.colorbar(cax)\nticks = np.arange(0, 10, 1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(cols)\nax.set_yticklabels(cols)\nplt.show()","06285c95":"#Colocando os Dados na mesma escala\nfrom sklearn.preprocessing import MinMaxScaler\n\n#colsDFSel = ['num_gestacoes', 'glicose', 'pressao_sanguinea','insulina', 'bmi', 'indice_historico', 'grossura_pele', 'idade','classe']\ncolsDFSel = ['num_gestacoes', 'glicose', 'insulina', 'bmi', 'indice_historico', 'idade', 'classe']\n\n\n#colsDFTesteSel = ['num_gestacoes', 'glicose', 'pressao_sanguinea','insulina', 'bmi', 'indice_historico', 'grossura_pele', 'idade']\ncolsDFTesteSel = ['num_gestacoes', 'glicose', 'insulina', 'bmi', 'indice_historico','idade']\n\ncolTrain = colsDFSel\ndfMLTrain = df[colTrain]\narrayTrain = dfMLTrain.values\n\ncolTest =  colsDFTesteSel\ndfMLTest = dfTest[colTest]\narrayTest = dfMLTest.values\n\n#Fazendo split nos dados de treino\nXTrain = arrayTrain[:,0:6]\nYTrain = arrayTrain[:,6]\nXTest = arrayTest[:,0:6]\n\n#Criando escala\nscaler = MinMaxScaler(feature_range = (0, 1))\nrescaledXTrain = scaler.fit_transform(XTrain)\nrescaledXTest = scaler.fit_transform(XTest)\n\n#Dados na escala\nprint(rescaledXTrain[0:5,:])","562e6ae0":"#Fazendo Feature Selection usando chi2 test\n\n#Bibliotecas\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#Selecionando as 5 melhores features para usar no modelo\ntest = SelectKBest(score_func = chi2, k = 5) \nfit = test.fit(rescaledXTrain, YTrain)\n\nprint(\"====== Features =======\\n\", colsDFSel)\n\n#Score\nprint(fit.scores_)\nprint(\"========================\\n\")\nfeatures = fit.transform(rescaledXTrain)\n\n#Sumarizando Features selecionadas\nprint(features[0:5,:])","e3611493":"#Compara\u00e7\u00e3o de modelos\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n#Definindo numero de folds\nnum_folds = 10\nseed = 7\n\n\n#Preparando modelos\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('LDA', LinearDiscriminantAnalysis()))\nmodelos.append(('NB', GaussianNB()))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('DTC', DecisionTreeClassifier()))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('ETC', ExtraTreesClassifier()))\nmodelos.append(('ABC', AdaBoostClassifier()))\nmodelos.append(('BGC', BaggingClassifier()))\nmodelos.append(('RFC', RandomForestClassifier()))\nmodelos.append(('GPC', GaussianProcessClassifier())) #===========> n\u00e3o deve aceitar valores negativos\nmodelos.append(('MLPC', MLPClassifier()))\n\n\n#Avalia\u00e7\u00e3o dos modelos\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = model_selection.KFold(n_splits = num_folds, random_state = seed)\n    cv_results = model_selection.cross_val_score(modelo, rescaledXTrain, YTrain, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot pra comparar os modelos\nfig = plt.figure()\nfig.suptitle('Compara\u00e7\u00e3o dos Modelos')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","39ceb2b0":"#KNN\nmodelKNN = KNeighborsClassifier()\n\nmodelKNN.fit(rescaledXTrain, YTrain)\n\nparam_grid_knn = { \n    'n_neighbors': [3, 4, 5, 6, 7],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'weights': ['uniform','distance'],\n    'leaf_size':[1, 2, 3],\n    'p': [2, 1]  \n}\nCV_knn = GridSearchCV(estimator=modelKNN, param_grid=param_grid_knn, cv=num_folds)\nCV_knn.fit(rescaledXTrain, YTrain)\nprint (CV_knn.best_params_)\nprint (round(CV_knn.score(rescaledXTrain, YTrain) * 100, 2))\n\n\n# Predictions\nYPredKNN = CV_knn.predict(rescaledXTest)","826fdaf2":"#DTC\nmodelDTC = DecisionTreeClassifier()\n\n\nparam_grid_dtc = { \n    'min_samples_split': [0.1, 0.15, 0.2, 0.5],\n    'min_samples_leaf': [0.1, 0.5, 1, 2, 3],\n    'max_depth': [None, 1, 2],\n    'criterion': ['gini','entropy'],\n    'presort':[True,False]\n}\n\nCV_dtc = GridSearchCV(estimator=modelDTC, param_grid=param_grid_dtc, cv=num_folds)\nCV_dtc.fit(rescaledXTrain, YTrain)\nprint (CV_dtc.best_params_)\nprint (round(CV_dtc.score(rescaledXTrain, YTrain) * 100, 2))\n\n#Predictions\nYPredDTC = CV_dtc.predict(rescaledXTest)","1f37a9db":"#RFC\nmodelRFC = RandomForestClassifier()\n\nparam_grid_rfc = { \n    'n_estimators': [1, 2, 3],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'criterion': ['gini','entropy'],\n    'bootstrap':[True,False],\n    'verbose':[0, 1],\n    'warm_start':[True,False]\n}\n\nCV_rfc = GridSearchCV(estimator=modelRFC, param_grid=param_grid_rfc, cv=num_folds)\nCV_rfc.fit(rescaledXTrain, YTrain)\nprint (CV_rfc.best_params_)\nprint (round(CV_rfc.score(rescaledXTrain, YTrain) * 100, 2))\n\n\n# Predictions\nYPredRFC = CV_rfc.predict(rescaledXTest)","9192d3c2":"#ABC\nmodelABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2))\nparam_grid_abc = {\n    'n_estimators': [200],\n    'algorithm': ['SAMME', 'SAMME.R'],\n    'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.5, 1]\n}\nCV_abc = GridSearchCV(estimator=modelABC, param_grid=param_grid_abc, cv=num_folds)\nCV_abc.fit(rescaledXTrain, YTrain)\nprint (CV_abc.best_params_)\n\nprint (round(CV_abc.score(rescaledXTrain, YTrain) * 100, 2))\n\n# Predictions\nYPredABC = CV_abc.predict(rescaledXTest)","c86efd92":"#Gerando o resultado para o Submission File\nresultado = YPredABC.astype(int)\n#resultado = YPredRFC.astype(int)\n#resultado = YPredDTC.astype(int)\n#resultado = YPredKNN.astype(int)","fe1e6ce2":"#Criando Submission file\nsubmission = pd.DataFrame({\n        \"id\": dfTest['id'],\n        \"classe\": resultado\n    })\n\nsubmission.to_csv('submission.csv', index=False)","1d0f12ea":"## Machine Learning","ad75993b":"## An\u00e1lise dos Dados "}}