{"cell_type":{"a88d362f":"code","d28e8001":"code","d5880a9d":"code","dc0032e7":"code","8195c93f":"code","e1303dc9":"code","90e9fd17":"code","97420435":"code","a92952af":"code","6ad1653c":"code","76a5b74c":"code","fe0d8803":"code","a684fe1b":"code","4c92b905":"code","2d991833":"code","6daeb2b3":"code","f8ecb1b3":"code","d5a1f494":"code","49e04d1c":"code","0541edbb":"code","9b2665bc":"code","1d7a9a5d":"code","8a74f525":"code","2850acd2":"code","c39bb82a":"code","1c807647":"code","4e4752a1":"code","c570fbd0":"code","3a484234":"code","8c1b599c":"code","8f0536ab":"code","ccae4cb1":"code","c01eb7a0":"code","64076cc5":"code","b2c2ee5c":"code","ede05575":"markdown","d1c0d689":"markdown","a2917ada":"markdown","9097e693":"markdown","789968de":"markdown","b2b7cf2a":"markdown","bb8cbcc7":"markdown"},"source":{"a88d362f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nimport torchvision\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms,models\nfrom torchvision.utils import make_grid\nfrom PIL import Image\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d28e8001":"path= '..\/input\/flowers-recognition\/flowers\/'","d5880a9d":"img_names = []\nfor folder,subfolders,filenames in os.walk(path):\n    for img in filenames:\n        img_names.append(folder+'\/'+img)\nprint(\"images len:\",len(img_names))","dc0032e7":"img_sizes = []\nrejected = []\nfor item in img_names:\n    try:\n        with Image.open(item) as img:\n            img_sizes.append(img.size)\n    except:\n        rejected.append(item)\nprint(f'images:{len(img_sizes)}')\nprint(f'rejected:{len(rejected)}')","8195c93f":"df = pd.DataFrame(img_sizes)\ndf.describe()","e1303dc9":"df.head()","90e9fd17":"daisy_flw = Image.open(\"..\/input\/flowers-recognition\/flowers\/daisy\/10466558316_a7198b87e2.jpg\")\ndisplay(daisy_flw)","97420435":"train_transform =transforms.Compose([\n    transforms.RandomRotation(10),\n    transforms.RandomHorizontalFlip(),\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean =[0.485,0.456,0.406], std = [0.229,0.224,0.225])\n])","a92952af":"test_transform =transforms.Compose([\n    transforms.RandomRotation(10),\n    transforms.RandomHorizontalFlip(),\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean =[0.485,0.456,0.406], std = [0.229,0.224,0.225])\n])","6ad1653c":"## The full_dataset is splitted into train and test datsets\n\nroot= '..\/input\/flowers-recognition\/'\n# we are defining the root where all train-test datas will be loaded\n\nfull_dataset = datasets.ImageFolder(os.path.join(root,'flowers'),transform=train_transform)\nprint('full dataset:',full_dataset)\n\ntrain_size = int(0.8 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\n\nprint('train size:',train_size)\nprint('test size:',test_size)\ntrain_data, test_data = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","76a5b74c":"#train_data = datasets.ImageFolder(os.path.join(root,'flowers'),transform=train_transform)\n#test_data = datasets.ImageFolder(os.path.join(root,'flowers'),transform=train_transform)\n\n# we are loading datesets as a train loader and test loader with train batch size =20 and test batch size =10\n\ntrain_btch_size =20\ntest_btch_size = 10\n\ntrain_loader = DataLoader(train_data,batch_size=train_btch_size, shuffle = True,num_workers = 8)\ntest_loader =  DataLoader(test_data,batch_size =test_btch_size ,shuffle = False,num_workers = 8)\n\nclass_names = full_dataset.classes\nprint(class_names)\n","fe0d8803":"print(\"Train size:\",len(train_data))\nprint(\"Test size:\",len(test_data))\n","a684fe1b":"for images,labels in train_loader:\n    break\n\nprint('labels:',labels.numpy())\nprint('images:',*np.array([class_names[i] for i in labels]))\n\nim = make_grid(images,nrow=train_btch_size)\n\nim_inv = transforms.Normalize( \n    mean=[-0.485\/0.229, -0.456\/0.224, -0.406\/0.225],\n    std=[1\/0.229, 1\/0.224, 1\/0.225])\nim = im_inv(im)\nplt.figure(figsize =(20,4))\nplt.imshow(np.transpose(im.numpy(),(1,2,0)))\n\n","4c92b905":"\nprint(\"images.shape:\",images.shape)\nprint(\"labels.shape:\",labels.shape)\nprint(\"len(labels)\",len(labels))","2d991833":"class ConvolutionalNetworks(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3,32,3,1)\n        self.conv2 = nn.Conv2d(32,64,3,1)\n        self.conv3 = nn.Conv2d(64,128,3,1)\n        self.fc1   = nn.Linear(128*26*26,16)\n        self.fc2   = nn.Linear(16,8)\n        self.fc3   = nn.Linear(8,6)\n        \n    def forward(self,X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X,2,2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X,2,2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X,2,2)\n        \n        X = X.view(-1,128*26*26)\n        X = F.relu(self.fc1(X))\n        X = F.relu(self.fc2(X))\n        X = self.fc3(X)\n        return F.log_softmax(X,dim =1)\n    ","6daeb2b3":"# We are going to use pytorch GPU therefore we need to convert datatypes into .cuda() format\nfrom torch.autograd import Variable\n\nuse_gpu = torch.cuda.is_available()\nuse_gpu","f8ecb1b3":"CNN_model=ConvolutionalNetworks()\nif torch.cuda.is_available():\n    CNN_model.cuda() \ncriterion= nn.CrossEntropyLoss()\nif use_gpu:\n     criterion = criterion.cuda()\noptimizer = torch.optim.Adam(CNN_model.parameters(),lr=0.001)\nCNN_model","d5a1f494":"total_param = []\nfor param in CNN_model.parameters():\n    total_param.append(param.numel())\n\nprint(total_param)\nprint(\"Number of parameters:\",sum(total_param))","49e04d1c":"epochs = 3\ntrain_losses =[]\ntest_losses = []\ntrain_correct =[]\ntest_correct=[]\nfor epoch in range(epochs):\n    \n    trn_corr =0\n    tst_corr =0\n    for batch,(X_train,y_train) in enumerate(train_loader):\n        batch+=1\n        if torch.cuda.is_available():\n            X_train = Variable(X_train).cuda()\n            y_train = Variable(y_train).cuda()\n        \n        y_pred = CNN_model(X_train)\n        loss   = criterion(y_pred,y_train)\n        predicted =torch.max(y_pred.data,1)[1]\n        trn_corr+=(predicted == y_train).sum()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch%100 ==0:\n            print(f'epoch:{epoch} batch:{batch} loss:{loss.item()} accuracy:{trn_corr.item()*100\/(batch*train_btch_size)}')\n            \n        \n    train_losses.append(loss)\n    train_correct.append(trn_corr.item()*100\/(batch*train_btch_size))\n    \n    with torch.no_grad():\n        for X_test,y_test in test_loader:\n            \n            if torch.cuda.is_available():\n                X_test = Variable(X_test).cuda()\n                y_test = Variable(y_test).cuda()\n\n            y_val= CNN_model(X_test)\n            predicted_test = torch.max(y_val.data,1)[1]\n            tst_corr +=(predicted_test==y_test).sum()\n\n        loss =criterion(y_val,y_test)\n        test_losses.append(loss)\n        test_correct.append(tst_corr.item()*100\/(batch*test_btch_size))  \n    \n   \n    ","0541edbb":"#  Plot train loss and test loss \nplt.figure(figsize=(12,8))\nplt.plot(train_losses,label = 'train loss')\nplt.plot(test_losses,label  = 'test loss')\nplt.legend()\n\n#  Plot train and test accuracy\nplt.figure(figsize=(12,8))\nplt.plot(train_correct,label = 'train correct')\nplt.plot(test_correct, label = 'test correct')\nplt.legend()\n","9b2665bc":"train_correct[-1]","1d7a9a5d":"AlexNetModel = models.alexnet(pretrained = True)\nAlexNetModel","8a74f525":"for param in AlexNetModel.parameters():\n    param.requires_grad = True # it will train all weights .it will take long time to train it\n    #param.requies_grad = False # it will only train last couple of layers. therefor it will take short time to update the parameters","2850acd2":"# Modifying AlexNet architecture\ntorch.manual_seed(42)\nAlexNetModel.classifier = nn.Sequential(nn.Linear(9216,1024),\n                                             nn.ReLU(),\n                                             nn.Dropout(0.4),\n                                             nn.Linear(1024,6),\n                                             nn.LogSoftmax(dim=1),)\nAlexNetModel.eval()","c39bb82a":"if torch.cuda.is_available():\n    AlexNetModel.cuda()","1c807647":"epochs = 3\ntrain_losses =[]\ntest_losses = []\ntrain_correct =[]\ntest_correct=[]\nfor epoch in range(epochs):\n    \n    trn_corr =0\n    tst_corr =0\n    for batch,(X_train,y_train) in enumerate(train_loader):\n        batch+=1\n        if torch.cuda.is_available():\n            X_train = Variable(X_train).cuda()\n            y_train = Variable(y_train).cuda()\n        \n        y_pred = AlexNetModel(X_train)\n        loss   = criterion(y_pred,y_train)\n        predicted =torch.max(y_pred.data,1)[1]\n        trn_corr+=(predicted == y_train).sum()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch%100 ==0:\n            print(f'epoch:{epoch} batch:{batch} loss:{loss.item()} accuracy:{trn_corr.item()*100\/(batch*20)}')\n            \n        \n    train_losses.append(loss)\n    train_correct.append(trn_corr)","4e4752a1":"ResnetModel = models.resnet18(pretrained= True)\nResnetModel","c570fbd0":"for param in ResnetModel.parameters():\n    param.requires_grad = True","3a484234":"ResnetModel.fc = nn.Linear(512,6)\nResnetModel","8c1b599c":"if torch.cuda.is_available():\n    ResnetModel.cuda()","8f0536ab":"epochs = 3\ntrain_losses =[]\ntest_losses = []\ntrain_correct =[]\ntest_correct=[]\nfor epoch in range(epochs):\n    \n    trn_corr =0\n    tst_corr =0\n    for batch,(X_train,y_train) in enumerate(train_loader):\n        batch+=1\n        if torch.cuda.is_available():\n            X_train = Variable(X_train).cuda()\n            y_train = Variable(y_train).cuda()\n        \n        y_pred = ResnetModel(X_train)\n        loss   = criterion(y_pred,y_train)\n        predicted =torch.max(y_pred.data,1)[1]\n        trn_corr+=(predicted == y_train).sum()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch%100 ==0:\n            print(f'epoch:{epoch} batch:{batch} loss:{loss.item()} accuracy:{trn_corr.item()*100\/(batch*10)}')\n            \n        \n    train_losses.append(loss)\n    train_correct.append(trn_corr)","ccae4cb1":"plt.plot(train_losses,label = 'train loss')\nplt.legend()\n","c01eb7a0":"## my CNN test\nCNN_model.eval()\nCNN_model.cuda()\nwith torch.no_grad():\n    y_result =CNN_model(Variable(train_data[25][0].view(1,3,224,224)).cuda()).argmax()\nprint('predicted:{}'.format(y_result.item()))\nprint('label:',train_data[25][1])","64076cc5":"## Testing Alexnet model\nAlexNetModel.eval()\nAlexNetModel.cuda()\nwith torch.no_grad():\n    y_result =AlexNetModel(Variable(train_data[25][0].view(1,3,224,224)).cuda()).argmax()\nprint('predicted:{}'.format(y_result.item()))\nprint('label:',train_data[25][1])","b2c2ee5c":"plt.imshow(np.transpose(train_data[25][0],(1,2,0)))\nprint('label:',train_data[25][1])\n","ede05575":"### Test your Model","d1c0d689":"> ## Using Pretrained Model","a2917ada":"# 2) Resnet18 Model","9097e693":"## Modifying Resnet Acrhitecture","789968de":"In this section we are implementing trasferlearning in order to tune parameters in the fully connected layer ","b2b7cf2a":"# 1) AlexNet Model","bb8cbcc7":"### Use train test split"}}