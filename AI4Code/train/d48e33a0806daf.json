{"cell_type":{"f6c42dd9":"code","555afe16":"code","fba5d3db":"code","3ba7f607":"code","f1cff4ea":"code","68db592f":"code","21d9a66e":"code","1ac21450":"code","f7192479":"code","19042037":"code","1ca492ce":"code","8af337a1":"code","65e022be":"code","450a7295":"code","7f002652":"markdown","69c80af9":"markdown","e006ab73":"markdown","21fa534f":"markdown","01abf60c":"markdown"},"source":{"f6c42dd9":"import random\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nimport glob\nimport os\nimport gc\nfrom joblib import Parallel, delayed","555afe16":"path_submissions = '\/'\ntarget_name = 'target'\nscores_folds = {}","fba5d3db":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","3ba7f607":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')","f1cff4ea":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\n\nstock_id = '0'\ntime_id = book_example.time_id.unique()\n\nbook_example = book_example[book_example['time_id'].isin(time_id)]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id'].isin(time_id)]\ntrade_example.loc[:,'stock_id'] = stock_id\n\nbook_example['wap'] = calc_wap(book_example)\n\n#book_example.groupby('time_id', as_index=False).apply(lambda x: x.reset_index())['wap'].unstack(level=0).plot(legend=None)\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]\n\nbook_example = book_example.merge(trade_example, on=['seconds_in_bucket','time_id'],how='left', suffixes=('', '_y'))\nbook_example = book_example.loc[:, ~book_example.columns.str.endswith('_y')]","68db592f":"rv = pd.DataFrame(book_example[['log_return','time_id']].groupby(['time_id']).agg(realized_volatility)).reset_index()\n\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv', dtype = {'stock_id': np.int32, 'time_id': np.int32, 'target': np.float64})\ntrain.head()\n\ntrain_0 = train[train['stock_id']==0]\ndf_rv_train = train_0.merge(rv, on = ['time_id'], how = 'right')\n\ndf_rv_train['error'] = (df_rv_train['target'] - df_rv_train['log_return'])\ndf_rv_train['percentage_error'] = (df_rv_train['target'] - df_rv_train['log_return'])\/df_rv_train['target']","21d9a66e":"df = book_example","1ac21450":"def _roll(a, shift):\n    \"\"\" Roll 1D array elements. Improves the performance of numpy.roll()\"\"\"\n\n\n    if not isinstance(a, np.ndarray):\n        a = np.asarray(a)\n    idx = shift % len(a)\n    return np.concatenate([a[-idx:], a[:-idx]])\n\n\ndef _get_length_sequences_where(x):\n    \"\"\" This method calculates the length of all sub-sequences where the array x is either True or 1. \"\"\"\n    if len(x) == 0:\n        return [0]\n    else:\n        res = [len(list(group)) for value, group in itertools.groupby(x) if value == 1]\n        return res if len(res) > 0 else [0]\n\ndef _aggregate_on_chunks(x, f_agg, chunk_len):\n    \"\"\"Takes the time series x and constructs a lower sampled version of it by applying the aggregation function f_agg on\n    consecutive chunks of length chunk_len\"\"\"\n    \n    return [\n        getattr(x[i * chunk_len : (i + 1) * chunk_len], f_agg)()\n        for i in range(int(np.ceil(len(x) \/ chunk_len)))\n    ]\n\ndef _into_subchunks(x, subchunk_length, every_n=1):\n    \"\"\"Split the time series x into subwindows of length \"subchunk_length\", starting every \"every_n\".\"\"\"\n    len_x = len(x)\n\n    assert subchunk_length > 1\n    assert every_n > 0\n\n    # how often can we shift a window of size subchunk_length over the input?\n    num_shifts = (len_x - subchunk_length) \/\/ every_n + 1\n    shift_starts = every_n * np.arange(num_shifts)\n    indices = np.arange(subchunk_length)\n\n    indexer = np.expand_dims(indices, axis=0) + np.expand_dims(shift_starts, axis=1)\n    return np.asarray(x)[indexer]\n\n\ndef set_property(key, value):\n    \"\"\"\n    This method returns a decorator that sets the property key of the function to value\n    \"\"\"\n\n    def decorate_func(func):\n        setattr(func, key, value)\n        if func.__doc__ and key == \"fctype\":\n            func.__doc__ = (\n                func.__doc__ + \"\\n\\n    *This function is of type: \" + value + \"*\\n\"\n            )\n        return func\n\n    return decorate_func","f7192479":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef large_standard_deviation(x):\n    if (np.max(x)-np.min(x)) == 0:\n        return np.nan\n    else:\n        return np.std(x)\/(np.max(x)-np.min(x))\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) \/ mean\n    else:\n        return np.nan\n\ndef variance_std_ratio(x):\n    y = np.var(x)\n    if y != 0:\n        return y\/np.sqrt(y)\n    else:\n        return np.nan\n\ndef ratio_beyond_r_sigma(x, r):\n    if x.size == 0:\n        return np.nan\n    else:\n        return np.sum(np.abs(x - np.mean(x)) > r * np.asarray(np.std(x))) \/ x.size\n\ndef range_ratio(x):\n    mean_median_difference = np.abs(np.mean(x) - np.median(x))\n    max_min_difference = np.max(x) - np.min(x)\n    if max_min_difference == 0:\n        return np.nan\n    else:\n        return mean_median_difference \/ max_min_difference\n    \ndef has_duplicate_max(x):\n    return np.sum(x == np.max(x)) >= 2\n\ndef has_duplicate_min(x):\n    return np.sum(x == np.min(x)) >= 2\n\ndef has_duplicate(x):\n    return x.size != np.unique(x).size\n\ndef count_duplicate_max(x):\n    return np.sum(x == np.max(x))\n\ndef count_duplicate_min(x):\n    return np.sum(x == np.min(x))\n\ndef count_duplicate(x):\n    return x.size - np.unique(x).size\n\ndef sum_values(x):\n    if len(x) == 0:\n        return 0\n    return np.sum(x)\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef realized_abs_skew(series):\n    return np.power(np.abs(np.sum(series**3)),1\/3)\n\ndef realized_skew(series):\n    return np.sign(np.sum(series**3))*np.power(np.abs(np.sum(series**3)),1\/3)\n\ndef realized_vol_skew(series):\n    return np.power(np.abs(np.sum(series**6)),1\/6)\n\ndef realized_quarticity(series):\n    return np.power(np.sum(series**4),1\/4)\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef count(series):\n    return series.size\n\n#drawdons functions are mine\ndef maximum_drawdown(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef maximum_drawup(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    \n\n    series = - series\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef drawdown_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef drawup_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    series=-series\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef max_over_min(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.max(series)\/np.min(series)\n\ndef max_over_min_sq(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.square(np.max(series)\/np.min(series))\n\ndef mean_n_absolute_max(x, number_of_maxima = 1):\n    \"\"\" Calculates the arithmetic mean of the n absolute maximum values of the time series.\"\"\"\n    assert (\n        number_of_maxima > 0\n    ), f\" number_of_maxima={number_of_maxima} which is not greater than 1\"\n\n    n_absolute_maximum_values = np.sort(np.absolute(x))[-number_of_maxima:]\n\n    return np.mean(n_absolute_maximum_values) if len(x) > number_of_maxima else np.NaN\n\n\ndef count_above(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x >= t) \/ len(x)\n\ndef count_below(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x <= t) \/ len(x)\n\n#number of valleys = number_peaks(-x, n)\ndef number_peaks(x, n):\n    \"\"\"\n    Calculates the number of peaks of at least support n in the time series x. A peak of support n is defined as a\n    subsequence of x where a value occurs, which is bigger than its n neighbours to the left and to the right.\n    \"\"\"\n    x_reduced = x[n:-n]\n\n    res = None\n    for i in range(1, n + 1):\n        result_first = x_reduced > _roll(x, i)[n:-n]\n\n        if res is None:\n            res = result_first\n        else:\n            res &= result_first\n\n        res &= x_reduced > _roll(x, -i)[n:-n]\n    return np.sum(res)\n\ndef mean_abs_change(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef mean_change(x):\n    x = np.asarray(x)\n    return (x[-1] - x[0]) \/ (len(x) - 1) if len(x) > 1 else np.NaN\n\ndef mean_second_derivative_central(x):\n    x = np.asarray(x)\n    return (x[-1] - x[-2] - x[1] + x[0]) \/ (2 * (len(x) - 2)) if len(x) > 2 else np.NaN\n\n\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) \/ mean\n    else:\n        return np.nan\n\ndef variance(x):\n    return np.var(x)\n\ndef skewness(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.skew(x)\n\ndef kurtosis(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.kurtosis(x)\n\ndef root_mean_square(x):\n    return np.sqrt(np.mean(np.square(x))) if len(x) > 0 else np.NaN\n\ndef absolute_sum_of_changes(x):\n    return np.sum(np.abs(np.diff(x)))\n\ndef longest_strike_below_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x < np.mean(x))) if x.size > 0 else 0\n\ndef longest_strike_above_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x > np.mean(x))) if x.size > 0 else 0\n\ndef count_above_mean(x):\n    m = np.mean(x)\n    return np.where(x > m)[0].size\n\ndef count_below_mean(x):\n    m = np.mean(x)\n    return np.where(x < m)[0].size\n\ndef last_location_of_maximum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmax(x[::-1]) \/ len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_maximum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmax(x) \/ len(x) if len(x) > 0 else np.NaN\n\ndef last_location_of_minimum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmin(x[::-1]) \/ len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_minimum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmin(x) \/ len(x) if len(x) > 0 else np.NaN\n\n# Test non-consecutive non-reoccuring values ?\ndef percentage_of_reoccurring_values_to_all_values(x):\n    if len(x) == 0:\n        return np.nan\n    unique, counts = np.unique(x, return_counts=True)\n    if counts.shape[0] == 0:\n        return 0\n    return np.sum(counts > 1) \/ float(counts.shape[0])\n\ndef percentage_of_reoccurring_datapoints_to_all_datapoints(x):\n    if len(x) == 0:\n        return np.nan\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    value_counts = x.value_counts()\n    reoccuring_values = value_counts[value_counts > 1].sum()\n    if np.isnan(reoccuring_values):\n        return 0\n\n    return reoccuring_values \/ x.size\n\n\ndef sum_of_reoccurring_values(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    counts[counts > 1] = 1\n    return np.sum(counts * unique)\n\ndef sum_of_reoccurring_data_points(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    return np.sum(counts * unique)\n\ndef ratio_value_number_to_time_series_length(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if x.size == 0:\n        return np.nan\n\n    return np.unique(x).size \/ x.size\n\ndef abs_energy(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.dot(x, x)\n\ndef quantile(x, q):\n    if len(x) == 0:\n        return np.NaN\n    return np.quantile(x, q)\n\n# crossing the mean ? other levels ? \ndef number_crossing_m(x, m):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    # From https:\/\/stackoverflow.com\/questions\/3843017\/efficiently-detect-sign-changes-in-python\n    positive = x > m\n    return np.where(np.diff(positive))[0].size\n\ndef maximum(x):\n    return np.max(x)\n\ndef absolute_maximum(x):\n    return np.max(np.absolute(x)) if len(x) > 0 else np.NaN\n\ndef minimum(x):\n    return np.min(x)\n\ndef value_count(x, value):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if np.isnan(value):\n        return np.isnan(x).sum()\n    else:\n        return x[x == value].size\n\ndef range_count(x, min, max):\n    return np.sum((x >= min) & (x < max))","19042037":"count_above_0 = lambda x: count_above(x,0)\ncount_above_0.__name__ = 'count_above_0'\n\ncount_below_0 = lambda x: count_below(x,0)\ncount_below_0.__name__ = 'count_below_0'\n\nvalue_count_0 = lambda x: value_count(x,0)\nvalue_count_0.__name__ = 'value_count_0'\n\ncount_near_0 = lambda x: range_count(x,-0.00001,0.00001)\ncount_near_0.__name__ = 'count_near_0_0'\n\nratio_beyond_01_sigma = lambda x: ratio_beyond_r_sigma(x,0.1)\nratio_beyond_01_sigma.__name__ = 'ratio_beyond_01_sigma'\n\nratio_beyond_02_sigma = lambda x: ratio_beyond_r_sigma(x,0.2)\nratio_beyond_02_sigma.__name__ = 'ratio_beyond_02_sigma'\n\nratio_beyond_03_sigma = lambda x: ratio_beyond_r_sigma(x,0.3)\nratio_beyond_03_sigma.__name__ = 'ratio_beyond_03_sigma'\n\nnumber_crossing_0 = lambda x: number_crossing_m(x,0)\nnumber_crossing_0.__name__ = 'number_crossing_0'\n\nquantile_01 = lambda x: quantile(x,0.1)\nquantile_01.__name__ = 'quantile_01'\n\nquantile_025 = lambda x: quantile(x,0.25)\nquantile_025.__name__ = 'quantile_025'\n\nquantile_075 = lambda x: quantile(x,0.75)\nquantile_075.__name__ = 'quantile_075'\n\nquantile_09 = lambda x: quantile(x,0.9)\nquantile_09.__name__ = 'quantile_09'\n\nnumber_peaks_2 = lambda x: number_peaks(x,2)\nnumber_peaks_2.__name__ = 'number_peaks_2'\n\nmean_n_absolute_max_2 = lambda x: mean_n_absolute_max(x,2)\nmean_n_absolute_max_2.__name__ = 'mean_n_absolute_max_2'\n\nnumber_peaks_5 = lambda x: number_peaks(x,5)\nnumber_peaks_5.__name__ = 'number_peaks_5'\n\nmean_n_absolute_max_5 = lambda x: mean_n_absolute_max(x,5)\nmean_n_absolute_max_5.__name__ = 'mean_n_absolute_max_5'\n\nnumber_peaks_10 = lambda x: number_peaks(x,10)\nnumber_peaks_10.__name__ = 'number_peaks_10'\n\nmean_n_absolute_max_10 = lambda x: mean_n_absolute_max(x,10)\nmean_n_absolute_max_10.__name__ = 'mean_n_absolute_max_10'\n\n#How to treat the first step ?\n#to test immediately\nget_first_ret = lambda x: np.log(x.iloc[0])\nget_first_ret .__name__ = 'get_first_ret'\n\nget_first_vol = lambda x: np.square(np.log(x.iloc[0]))\nget_first_vol .__name__ = 'get_first_vol'","1ca492ce":"base_stats = [mean,sum,length,standard_deviation,variation_coefficient,variance,skewness,kurtosis]\nhigher_order_stats = [abs_energy,root_mean_square,sum_values,realized_volatility,realized_abs_skew,realized_skew,realized_vol_skew,realized_quarticity]\nmin_median_max = [minimum,median,maximum]\nadditional_quantiles = [quantile_01,quantile_025,quantile_075,quantile_09]\nother_min_max = [absolute_maximum,max_over_min,max_over_min_sq]\nmin_max_positions = [last_location_of_maximum,first_location_of_maximum,last_location_of_minimum,first_location_of_minimum]\npeaks = [number_peaks_2, mean_n_absolute_max_2, number_peaks_5, mean_n_absolute_max_5, number_peaks_10, mean_n_absolute_max_10]\ncounts = [count_unique,count,count_above_0,count_below_0,value_count_0,count_near_0]\nreoccuring_values = [count_above_mean,count_below_mean,percentage_of_reoccurring_values_to_all_values,percentage_of_reoccurring_datapoints_to_all_datapoints,sum_of_reoccurring_values,sum_of_reoccurring_data_points,ratio_value_number_to_time_series_length]\ncount_duplicate = [count_duplicate,count_duplicate_min,count_duplicate_max]\nvariations = [mean_abs_change,mean_change,mean_second_derivative_central,absolute_sum_of_changes,number_crossing_0]\nranges = [variance_std_ratio,ratio_beyond_01_sigma,ratio_beyond_02_sigma,ratio_beyond_03_sigma,large_standard_deviation,range_ratio]\nget_first = [get_first_ret, get_first_vol]\n\nall_functions = base_stats + higher_order_stats + min_median_max + additional_quantiles + other_min_max + min_max_positions + peaks + counts + variations + ranges \n\n# not usefull\n#+ get_first\n\n# too slow\n#+ reoccuring_values + count_duplicate \n","8af337a1":"df['bid_ask_spread'] = df['bid_price1'] - df['ask_price1']\ndf['volume_imbalance'] = df['bid_size1'] - df['ask_size1']\ndf['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n\ncreate_feature_dict = {\n        'wap': all_functions,\n        'log_return': all_functions,\n        'bid_ask_spread': all_functions,\n        'volume_imbalance': all_functions,\n    }","65e022be":"%%time\n\ndf_train_stock_0_feat = df.groupby('time_id').agg(create_feature_dict)\ndf_train_stock_0_feat.columns = ['_'.join(col) for col in df_train_stock_0_feat.columns]\ndf_train_stock_0_feat = df_train_stock_0_feat.fillna(0)","450a7295":"def test_constant(df):\n    filter = ~(df!= df.iloc[0]).any()\n    return filter[filter]\n\nconstants_col = test_constant(df_train_stock_0_feat).index\n\nerror_cols = []\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.set_style(style='white')\n\nfor col in df_train_stock_0_feat.columns:\n    if col not in constants_col:\n        \n        try:\n            \n            sns.regplot(x=df_train_stock_0_feat[col],y=df_rv_train['target'],color=(random.random(), random.random(), random.random()), order = 2, line_kws={\"color\": 'black'})\n            plt.ylim(0, 0.05)\n            plt.title(col+' v.s. target',size=20)\n            plt.show()\n\n        except:\n            error_cols.append(col)\n            pass\n\nerror_cols","7f002652":"# Statistical Agregation","69c80af9":"# Tools","e006ab73":"# Feature Engineering: Aggregation Functions\n\nSome statistical aggregation functions. The functions that currently do not appears in other notebooks mostly come from the time series manipulation package tsfresh (https:\/\/tsfresh.readthedocs.io\/en\/latest\/) and personnal market finance education \/ experience (drawdowns, max over min). \n\n# Other Feature Engineering Notebooks: \n\nThis notebook is part of a serie on basic Feature Engineering \/ visual variable selection notebooks:\n\n1) Base Features: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-1-base-features\n\n2) Aggregation Functions: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-2-aggregation-functions\n\n3) RV aggregation: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-3-rv-aggregation\/","21fa534f":"# Lambda functions to facilitate application","01abf60c":"# Base Features"}}