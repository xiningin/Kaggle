{"cell_type":{"7159b126":"code","0499fd30":"code","b14ae5bc":"code","735d3512":"code","6ae9d01b":"code","7b3c8ad3":"code","dbba090c":"code","e623a174":"code","cb14bf83":"code","1ecb14aa":"code","31b4010c":"code","310f0491":"code","c63267bd":"code","19e83e37":"code","6a854584":"code","71fc9843":"code","0e92068f":"code","8a53076a":"code","ebdc957b":"code","a284ca0f":"code","e34dca41":"code","f2d44b25":"code","97af9f35":"code","e8597d7a":"code","266acd44":"code","f6a0dfaf":"code","0330eaef":"code","6c73ab27":"code","2efd42da":"code","2c500a2d":"markdown","9d88b205":"markdown"},"source":{"7159b126":"import pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import datasets, ensemble, metrics  \nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport datetime as dt","0499fd30":"# Read Train\/Test files \ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/housetrain.csv')\ntest= pd.read_csv('..\/input\/house-prices-data\/test.csv')","b14ae5bc":"# Check sample of data\n\n#test.head()\ntrain.head()","735d3512":"#Check columns\n\n\n#test.columns\ntrain.columns","6ae9d01b":"# Check Shape\ntrain.shape\ntest.shape","7b3c8ad3":"# Check the missing values in Train\/Test\n\ntrain.isnull().sum().sort_values().tail(25)\ntest.isnull().sum().sort_values().tail(25)","dbba090c":"# Let's check missing values on heatmap\nsns.heatmap(train.isnull(), yticklabels = False, cbar= False)\n#sns.heatmap(test.isnull(), yticklabels = False, cbar= False)","e623a174":"test['GarageArea'].value_counts()","cb14bf83":"# DIFFERENTIATING Columns\n\n# All categorical columns\ncat_cols = [col for col in train.columns if train[col].dtype == \"object\"]\n\n# All numerical columns\nnum_cols = [col for col in train.columns if train[col].dtype != \"object\"]\n\nprint('Numeric columns:', num_cols)\n\n\nprint('Categorical columns:', cat_cols)","1ecb14aa":"final_df= pd.concat([train,test],axis=0)\nfinal_df.shape","31b4010c":"# Differentiate Categorical variables\ncat_final=final_df[cat_cols]\n\n\ncat_final.shape","310f0491":"# Drop categorical columns\n\nfinal_df.drop(cat_cols , axis = 1, inplace = True)\n\n\nfinal_df.shape\n","c63267bd":"# Impute Numerical missing values with Mean\n\nimputerr= SimpleImputer(strategy = 'mean')\n\nnum_final = pd.DataFrame(imputerr.fit_transform(final_df))\n\n\n# imputation removed column names, put them back\nnum_final.columns = final_df.columns\n\n\nnum_final.shape\n","19e83e37":"# Apply LabelEncoder to each column with categorical data\n\n#cat_final=cat_final.astype(str)\n\n\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_cat = pd.DataFrame(my_imputer.fit_transform(cat_final))\n\n\nimputed_cat.columns = cat_final.columns\n\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_final = pd.DataFrame(OH_encoder.fit_transform(imputed_cat))\n\n\n\nOH_cols_final.index = imputed_cat.index\n\n\nOH_cols_final= OH_cols_final.reset_index(drop=True)\n\n\nOH_cols_final.shape\n","6a854584":"# Combine Numeric & Categorical Datasets\n\nfinal = pd.concat([num_final, OH_cols_final], axis= 1)\n\nfinal.shape","71fc9843":"# Null confirmation\nfinal.isnull().sum().sort_values().tail(25)","0e92068f":"# Split back to train\/ test\ntrain=final.iloc[:1460,:]\ntest=final.iloc[1460:,:]","8a53076a":"train.shape\ntest.shape","ebdc957b":"#Separate Target Variable \n\ny= train['SalePrice']\ntrain.drop(['SalePrice'], axis=1, inplace = True)\ntest.drop(['SalePrice'], axis=1, inplace = True)","a284ca0f":"y","e34dca41":"#(OPTIONAL)For Interactive purposes only!!\n\ndef timer(start_time = None):\n    d= dt.datetime.now()\n    if not start_time:\n        start_time= d\n        return start_time\n    elif start_time:\n        thour, temp_sec= divmod((d-start_time).total_seconds(),3600)\n        tmin,tsec= divmod(temp_sec, 60)\n        \n        print('\\n Time Taken :  %i Hours %i Minutes %i Seconds.' %(thour, tmin, round(tsec,2)))","f2d44b25":"# Define model\nmy_model= XGBRegressor()","97af9f35":"train.columns","e8597d7a":"# Define Param set\n\nparams={\n    'learning_rate'    :  [0.01,0.1,0.2],\n    'max_depth'        :  [2,8,16],\n    'min_child_weight' :  [1,5,8],\n    'subsample'        :  [0.7,0.85,1.0],\n    'gamma'            :  [0,0.1,0.2],\n    'colsample_bytree' :  [0.1,0.5,0.9],\n    'n_estimators'     :  [100,500,900,1400],\n    'booster'          :  ['gbtree'],\n    'criterion'        :  ['gini','entropy']\n}","266acd44":"# I used GridSearch. RandomizedSearch also gives good result\nparams={\n    'learning_rate'    :  [0.1],\n    'n_estimators'     :  [300],\n    'max_depth':          [5],\n    'min_child_weight':   [3],\n    'gamma':              [0.05],\n    'subsample':          [0.6],\n    'colsample_bytree':   [0.55],\n    'reg_alpha':[0,0.001,0.08,1,1.25,1.5.2.0],\n    'reg_lambda':[0,0.001,0.08,1,1.25,1.5.2.0]\n    \n    \n    \n}\n\ngrid_srch = GridSearchCV(estimator=my_model, param_grid=params,\n                              cv=10, \n                              scoring='neg_root_mean_squared_error',\n                              n_jobs=-1, verbose=5, return_train_score= True)","f6a0dfaf":"# Fit the Search algo\nstart_time= timer(None)\n\ngrid_srch.fit(train,y)\n\ntimer(start_time)","0330eaef":"#Find Best Hyperparameers\n# cv=10, Gridsearch\ngrid_srch.best_estimator_","6c73ab27":"#Define model with best Hyperparameters\nmy_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, criterion='gini',\n             gamma=0.15, gpu_id=-1, importance_type='gain',\n             interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n             max_depth=3, min_child_weight=1, missing=None,\n             monotone_constraints='()', n_estimators=200, n_jobs=0,\n             num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, subsample=1.0, tree_method='exact',\n             validate_parameters=1, verbosity=None)\n\n# Fit the model\n\nmy_model.fit(train, y)\n\n# Get predictions\npredictions = my_model.predict(test)","2efd42da":"# SUBMISSION\ntest['Id']=test['Id'].astype(str)\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions})\n\nmy_submission.to_csv('submission.csv', index=False)","2c500a2d":"predictors= train.columns\nxgb1 = XGBRegressor(\n objective='reg:squarederror',\n learning_rate =0.01,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n#  tree_method='gpu_hist',\n    \n booster='gbtree', \n nthread=4,\n scale_pos_weight=1,\n seed=20)\nmodelfit(xgb1, train, predictors, y)","9d88b205":"def modelfit(alg, dtrain, predictors,target, useTrainCV=True, cv_folds=5, early=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain, label=target)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, early_stopping_rounds= 50, metrics={'rmse'})\n        print(cvresult)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], target,eval_metric='rmse')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"MSE : {}\".format(metrics.mean_squared_error(target, dtrain_predictions)))\n    #print (\"RMSE Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')"}}