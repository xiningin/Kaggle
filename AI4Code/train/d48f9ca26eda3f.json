{"cell_type":{"8a1efac0":"code","0725b4a7":"code","7031a27f":"code","6d65d4bb":"code","c2e60506":"code","4db701f5":"code","91daa4c8":"code","14432bba":"code","e712a827":"code","9cfabeea":"code","b58bf108":"code","232544c0":"code","1d17ae61":"code","c5c64f41":"code","0cc17ce6":"code","6d03604b":"code","2d254759":"code","cc791952":"code","9d32898c":"code","687ae7f4":"code","607daa79":"code","9eebb28d":"code","059875d8":"code","a51489c6":"code","c003c339":"code","01a932cb":"code","76b2238b":"code","e2052887":"code","06205c23":"code","0c9a28d7":"code","5b3619e9":"code","42afbe02":"code","2dcf7a99":"code","2e1dffbf":"code","f6a60b4a":"code","cbeab32a":"code","4c2ab607":"code","706ebfa0":"code","7d73a1e9":"code","3924284d":"code","d8ff2f2c":"code","381fec7d":"code","786423da":"code","fa16996b":"code","2fdcf414":"code","dfae1fa7":"code","cabdf331":"code","bfa971c1":"code","64d850b3":"code","694fb8b4":"code","850dfe88":"code","23371baa":"code","4fcba702":"markdown","8897dbf5":"markdown","e918e092":"markdown","bcb2922a":"markdown","c97f39fd":"markdown","613f67b5":"markdown","4fba21b0":"markdown","25d892a9":"markdown","5e23f2a4":"markdown","8eea06eb":"markdown","70c1e7d9":"markdown","22648e6b":"markdown","1dcef945":"markdown","e95a0834":"markdown","024b4f47":"markdown","0adf9875":"markdown","4ba2f75b":"markdown","56fb72c7":"markdown","706a206b":"markdown","499900b0":"markdown","d5bd03dc":"markdown","ad74cd72":"markdown","36888c29":"markdown","54933ebd":"markdown","897edbf1":"markdown","3c37c75b":"markdown"},"source":{"8a1efac0":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import tree\nfrom sklearn.tree import export_text, export_graphviz\nimport graphviz\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0725b4a7":"import sklearn\nsklearn.__version__, np.__version__, pd.__version__","7031a27f":"columns = [\n    'neighbourhood_group', 'room_type', 'latitude', 'longitude',\n    'minimum_nights', 'number_of_reviews','reviews_per_month',\n    'calculated_host_listings_count', 'availability_365',\n    'price'\n]\n\ndf = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv', usecols=columns)\ndf.reviews_per_month = df.reviews_per_month.fillna(0)","6d65d4bb":"df.info()","c2e60506":"df.describe()","4db701f5":"df['room_type'].value_counts()","91daa4c8":"df.price = np.where(df.price>0, np.log(df.price), 0)","14432bba":"df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.price\ny_val = df_val.price\ny_test = df_test.price\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']","e712a827":"dv = DictVectorizer(sparse=False)\n\ntrain_dicts = df_train.fillna(0).to_dict(orient='records')\nval_dicts = df_val.fillna(0).to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dicts)\nX_val = dv.transform(val_dicts)","9cfabeea":"dtr = DecisionTreeRegressor(max_depth=1)\ndtr.fit(X_train, y_train)\n\ny_pred_val = dtr.predict(X_val)\n\nmse_val = MSE(y_val, y_pred_val)\nrmse_val = np.sqrt(mse_val)\n\nprint('Number of nodes = {0}'.format(dtr.tree_.node_count))\nprint('Parameters used: {}'.format(dtr.get_params()))\nprint('Decision Tree Regressor feature = {0}'.format(dtr.tree_.feature))\nprint('Decision Tree Regressor threshold = {0}'.format(dtr.tree_.threshold))\nprint('Validation RMSE = {0}'.format(rmse_val))\nprint('Feature importances: {0}'.format(dtr.feature_importances_))\n\nprint(export_text(dtr, feature_names=dv.get_feature_names()))","b58bf108":"# Setting dpi = 300 to make image clearer than default\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=300)\n\ntree.plot_tree(dtr, feature_names = dv.get_feature_names())\nplt.show()","232544c0":"dot_data = export_graphviz(dtr, feature_names=dv.get_feature_names(), filled=True, rounded=True)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"tree\")","1d17ae61":"rf = RandomForestRegressor(n_estimators=10, random_state=1, n_jobs=-1)\nrf.fit(X_train, y_train)\n\ny_pred_val = rf.predict(X_val)\n\nprint('Root Mean Squared Error = {0}'.format(np.sqrt(MSE(y_val, y_pred_val))))","c5c64f41":"scores = []\n\nfor n in range(10, 201, 10):\n    rf = RandomForestRegressor(n_estimators=n, random_state=1)\n    rf.fit(X_train, y_train)\n\n    y_pred_val = rf.predict(X_val)\n    rmse = np.sqrt(MSE(y_val, y_pred_val))\n    \n    scores.append((n, rmse))","0cc17ce6":"df_scores = pd.DataFrame(scores, columns=['n_estimators', 'rmse'])    ","6d03604b":"plt.plot(df_scores.n_estimators, df_scores.rmse)\nplt.show()","2d254759":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10, 201, 10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                   max_depth=d,\n                                   random_state=1)\n        rf.fit(X_train, y_train)\n\n        y_pred_val = rf.predict(X_val)\n        rmse = np.sqrt(MSE(y_val, y_pred_val))\n\n        scores.append((d, n, rmse))","cc791952":"df_scores = pd.DataFrame(scores, columns=['max_depth','n_estimators','rmse'])","9d32898c":"for d in [0, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend();","687ae7f4":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10, 201, 10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                   max_depth=d,\n                                   random_state=42)\n        rf.fit(X_train, y_train)\n\n        y_pred_val = rf.predict(X_val)\n        rmse = np.sqrt(MSE(y_val, y_pred_val))\n\n        scores.append((d, n, rmse))","607daa79":"df_scores = pd.DataFrame(scores, columns=['max_depth','n_estimators','rmse'])","9eebb28d":"for d in [0, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend();","059875d8":"rf = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=1, n_jobs=-1)\nrf.fit(X_train, y_train)\n\ny_pred_val = rf.predict(X_val)\n\nprint('Root Mean Squared Error = {0}'.format(np.sqrt(MSE(y_val, y_pred_val))))\n\nprint('Feature importances: {0}'.format(rf.feature_importances_))","a51489c6":"importances = rf.feature_importances_\nforest_importances = pd.Series(importances, index=dv.feature_names_)\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\nfig, ax = plt.subplots(figsize = (14,8))\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","c003c339":"!pip install xgboost","01a932cb":"import xgboost as xgb","76b2238b":"features = dv.get_feature_names()\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=features)","e2052887":"watchlist = [(dtrain, 'train'), (dval, 'val')]","06205c23":"%%capture output\n\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel3 = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                   verbose_eval=5,\n                   evals=watchlist)","0c9a28d7":"s3 = output.stdout","5b3619e9":"print(s3[:100])","42afbe02":"y_pred_val3 = model3.predict(dval)","2dcf7a99":"def parse_xgb_output(output):\n    results = []\n\n    for line in output.stdout.strip().split('\\n'):\n        it_line, train_line, val_line = line.split('\\t')\n\n        it = int(it_line.strip('[]'))\n        train = float(train_line.split(':')[1])\n        val = float(val_line.split(':')[1])\n\n        results.append((it, train, val))\n    \n    columns = ['num_iter', 'train_auc', 'val_auc']\n    df_results = pd.DataFrame(results, columns=columns)\n    return df_results","2e1dffbf":"df_score3 = parse_xgb_output(output)","f6a60b4a":"plt.plot(df_score3.num_iter, df_score3.train_auc, label='train')\nplt.plot(df_score3.num_iter, df_score3.val_auc, label='val')\nplt.legend();","cbeab32a":"plt.plot(df_score3.num_iter, df_score3.val_auc, label='val')\nplt.legend();","4c2ab607":"%%capture output\n\nxgb_params = {\n    'eta': 0.1, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel1 = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                   verbose_eval=5,\n                   evals=watchlist)","706ebfa0":"s1 = output.stdout","7d73a1e9":"print(s1[:100])","3924284d":"y_pred_val1 = model1.predict(dval)","d8ff2f2c":"df_score1 = parse_xgb_output(output)","381fec7d":"plt.plot(df_score1.num_iter, df_score1.train_auc, label='train')\nplt.plot(df_score1.num_iter, df_score1.val_auc, label='val')\nplt.legend();","786423da":"plt.plot(df_score1.num_iter, df_score1.val_auc, label='val')\nplt.legend();","fa16996b":"%%capture output\n\nxgb_params = {\n    'eta': 0.01, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel01 = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                    verbose_eval=5,\n                    evals=watchlist)","2fdcf414":"s01 = output.stdout","dfae1fa7":"print(s01[:100])","cabdf331":"y_pred_val01 = model01.predict(dval)","bfa971c1":"df_score01 = parse_xgb_output(output)","64d850b3":"plt.plot(df_score01.num_iter, df_score01.train_auc, label='train')\nplt.plot(df_score01.num_iter, df_score01.val_auc, label='val')\nplt.legend();","694fb8b4":"plt.plot(df_score01.num_iter, df_score01.val_auc, label='val')\nplt.legend();","850dfe88":"plt.plot(df_score01.num_iter, df_score3.val_auc, label='val squared error eta = 0.3')\nplt.plot(df_score01.num_iter, df_score1.val_auc, label='val squared error eta = 0.1')\nplt.plot(df_score01.num_iter, df_score01.val_auc, label='val squared error eta = 0.01')\nplt.legend();","23371baa":"scores = {}","4fcba702":"## 6.10 Homework\n\nThe goal of this homework is to create a tree-based regression model for prediction apartment prices (column `'price'`).\n\nIn this homework we'll again use the New York City Airbnb Open Data dataset - the same one we used in homework 2 and 3.\n\nYou can take it from [Kaggle](https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv)\nor download from [here](https:\/\/raw.githubusercontent.com\/alexeygrigorev\/datasets\/master\/AB_NYC_2019.csv)\nif you don't want to sign up to Kaggle.\n\nLet's load the data:","8897dbf5":"What's the most important feature? \n\n* `neighbourhood_group=Manhattan`\n* `room_type=Entire home\/apt`\t\n* `longitude`\n* `latitude`","e918e092":"Now change `eta` first to `0.1` and then to `0.01`","bcb2922a":"## Answer 4. The best max_depth is 15.","c97f39fd":"## A6. The best eta seems to be 0.3 with gives us the lowest squared error.","613f67b5":"After which value of `n_estimators` does RMSE stop improving?\n\n- 10\n- 50\n- 70\n- 120","4fba21b0":"Now, use `DictVectorizer` to turn train and validation into matrices:","25d892a9":"## Answer 5.  The most important feature is room_type=Entire home\/apt.","5e23f2a4":"**When we changed the seed for the model, we did not get a different answer.**","8eea06eb":"## Question 4\n\nLet's select the best `max_depth`:\n\n* Try different values of `max_depth`: `[10, 15, 20, 25]`\n* For each of these values, try different values of `n_estimators` from 10 till 200 (with step 10)\n* Fix the random seed: `random_state=1`","70c1e7d9":"Which feature is used for splitting the data?\n\n* `room_type`\n* `neighbourhood_group`\n* `number_of_reviews`\n* `reviews_per_month`","22648e6b":"## Question 2\n\nTrain a random forest model with these parameters:\n\n* `n_estimators=10`\n* `random_state=1`\n* `n_jobs=-1`  (optional - to make training faster)","1dcef945":"## Answer 2.  The RMSE on the validation set is closest to 0.459.","e95a0834":"What's the RMSE of this model on validation?\n\n* 0.059\n* 0.259\n* 0.459\n* 0.659","024b4f47":"Now let's train an XGBoost model! For this question, we'll tune the `eta` parameter\n\n* Install XGBoost\n* Create DMatrix for train and validation\n* Create a watchlist\n* Train a model with these parameters for 100 rounds:\n\n```\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    \n    'seed': 1,\n    'verbosity': 1,\n}\n```","0adf9875":"## Question 5\n\nWe can extract feature importance information from tree-based models. \n\nAt each step of the decision tree learning algorithm, it finds the best split. \nWhen doing it, we can calculate \"gain\" - the reduction in impurity before and after the split. \nThis gain is quite useful in understanding what are the imporatant features \nfor tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the `feature_importances_` field. \n\nFor this homework question, we'll find the most important feature:\n\n* Train the model with these parametes:\n    * `n_estimators=10`,\n    * `max_depth=20`,\n    * `random_state=1`,\n    * `n_jobs=-1` (optional)\n* Get the feature importance information from this model","4ba2f75b":"## Answer 1. room_type is used for splitting the data.","56fb72c7":"What's the best eta?\n\n* 0.3\n* 0.1\n* 0.01","706a206b":"What's the best `max_depth`:\n\n* 10\n* 15\n* 20\n* 25\n\n","499900b0":"## Question 1\n\nLet's train a decision tree regressor to predict the price variable. \n\n* Train a model with `max_depth=1`","d5bd03dc":"* Apply the log tranform to `price`\n* Do train\/validation\/test split with 60%\/20%\/20% distribution. \n* Use the `train_test_split` function and set the `random_state` parameter to 1","ad74cd72":"## Answer 3. The RMSE stops improving after n_estimators = 120.","36888c29":"## Question 6","54933ebd":"## Submit the results\n\n\nSubmit your results here: https:\/\/forms.gle\/wQgFkYE6CtdDed4w8\n\nIt's possible that your answers won't match exactly. If it's the case, select the closest one.\n\n\n## Deadline\n\n\nThe deadline for submitting is 20 October 2021, 17:00 CET (Wednesday). After that, the form will be closed.","897edbf1":"Bonus question (not graded):\n\nWill the answer be different if we change the seed for the model?","3c37c75b":"## Question 3\n\nNow let's experiment with the `n_estimators` parameter\n\n* Try different values of this parameter from 10 to 200 with step 10\n* Set `random_state` to `1`\n* Evaluate the model on the validation dataset"}}