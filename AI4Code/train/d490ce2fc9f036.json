{"cell_type":{"eeb57bed":"code","ff1507f7":"code","d4edbff1":"code","cbe0a235":"code","e964a7b4":"code","867c7717":"code","13053c66":"code","bbe79cae":"code","ec8a1508":"code","4c137c00":"code","ecf59d82":"code","1a273ee9":"code","84b6a3ec":"markdown"},"source":{"eeb57bed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff1507f7":"training_folder_name = '..\/input\/tiny-imagenet\/tiny-imagenet-200\/train'\n\n# All images are 128x128 pixels\nimg_size = (128,128)\n\n# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)","d4edbff1":"# Import PyTorch libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)","cbe0a235":"from PIL import Image\n\n# function to resize image\ndef resize_image(src_image, size=(128,128), bg_color=\"white\"): \n    from PIL import Image, ImageOps \n    \n    # resize the image so the longest dimension matches our target size\n    src_image.thumbnail(size, Image.ANTIALIAS)\n    \n    # Create a new square background image\n    new_image = Image.new(\"RGB\", size, bg_color)\n    \n    # Paste the resized image into the center of the square background\n    new_image.paste(src_image, (int((size[0] - src_image.size[0]) \/ 2), int((size[1] - src_image.size[1]) \/ 2)))\n  \n    # return the resized image\n    return new_image","e964a7b4":"def load_dataset(data_path):\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    # Load all the images\n    transformation = transforms.Compose([\n        # Randomly augment the image data\n            # Random horizontal flip\n        transforms.RandomHorizontalFlip(0.5),\n            # Random vertical flip\n        transforms.RandomVerticalFlip(0.3),\n        # transform to tensors\n        transforms.ToTensor(),\n        # Normalize the pixel values (in R, G, and B channels)\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Load all of the images, transforming them\n    full_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=transformation\n    )\n    \n    \n    # Split into training (70% and testing (30%) datasets)\n    train_size = int(0.7 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    \n    # use torch.utils.data.random_split for training\/test split\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n    \n    # define a loader for the training data we can iterate through in 50-image batches\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # define a loader for the testing data we can iterate through in 50-image batches\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n        \n    return train_loader, test_loader","867c7717":"train_folder = '..\/input\/tiny-imagenet\/tiny-imagenet-200\/train'\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = load_dataset(train_folder)\nbatch_size = train_loader.batch_size\nprint(\"Data loaders ready to read\", train_folder)","13053c66":"# Create a neural net class\nclass Net(nn.Module):\n    \n    \n    # Defining the Constructor\n    def __init__(self, num_classes=3):\n        super(Net, self).__init__()\n        \n        # In the init function, we define each layer we will use in our model\n        \n        # Our images are RGB, so we have input channels = 3. \n        # We will apply 12 filters in the first convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        \n        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        \n        # We in the end apply max pooling with a kernel size of 2\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        \n        # A drop layer deletes 20% of the features to help prevent overfitting\n        self.drop = nn.Dropout2d(p=0.2)\n        \n        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128\/2\/2 is 32.\n        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n        \n        # We need to flatten these in order to feed them to a fully-connected layer\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        # In the forward function, pass the data through the layers we defined in the init function\n        \n        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n        x = F.relu(self.pool(self.conv1(x))) \n        \n        # Use a ReLU activation function after layer 2\n        x = F.relu(self.pool(self.conv2(x)))  \n        \n        # Select some features to drop to prevent overfitting (only drop during training)\n        x = F.dropout(self.drop(x), training=self.training)\n        \n        # Flatten\n        x = x.view(-1, 32 * 32 * 24)\n        # Feed to fully-connected layer to predict class\n        x = self.fc(x)\n        # Return class probabilities via a log_softmax function \n        return torch.log_softmax(x, dim=1)\n    \ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n    device = \"cuda\"\n\n# Create an instance of the model class and allocate it to the device\nmodel = Net(num_classes=len(classes)).to(device)\n\nprint(model)","bbe79cae":"def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Use the CPU or GPU as appropriate\n        # Recall that GPU is optimized for the operations we are dealing with\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n\n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics so we see some progress\n        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n            \n    # return average loss for the epoch\n    avg_loss = train_loss \/ (batch_idx+1)\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss","ec8a1508":"def test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            \n            # Get the predicted classes for this batch\n            output = model(data)\n            \n            # Calculate the loss for this batch\n            test_loss += loss_criteria(output, target).item()\n            \n            # Calculate the accuracy for this batch\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target==predicted).item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    avg_loss = test_loss \/ batch_count\n    print('Validation set: Average loss: {:.6f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        avg_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return avg_loss","4c137c00":"# Use an \"Adam\" optimizer to adjust weights\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\n# Train over 10 epochs (We restrict to 10 for time issues)\nepochs = 10\nprint('Training on', device)\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, train_loader, optimizer, epoch)\n        test_loss = test(model, device, test_loader)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n        validation_loss.append(test_loss)","ecf59d82":"plt.figure(figsize=(15,15))\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","1a273ee9":"# Defining Labels and Predictions\ntruelabels = []\npredictions = []\nmodel.eval()\nprint(\"Getting predictions from test set...\")\nfor data, target in test_loader:\n    for label in target.data.numpy():\n        truelabels.append(label)\n    for prediction in model(data).data.numpy().argmax(1):\n        predictions.append(prediction) \n\n# Plot the confusion matrix\ncm = confusion_matrix(truelabels, predictions)\ntick_marks = np.arange(len(classes))\n\ndf_cm = pd.DataFrame(cm, index = classes, columns = classes)\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Shape\", fontsize = 20)\nplt.ylabel(\"True Shape\", fontsize = 20)\nplt.show()","84b6a3ec":"## Tiny ImageNet instead of ImageNet\nImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. This of course requres computational cost that I can't provide. Thus, I chose to work with the tiny ImageNet dataset.<br> The same model can be used  on the ImageNet dataset."}}