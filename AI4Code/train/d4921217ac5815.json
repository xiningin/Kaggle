{"cell_type":{"b75d9ab2":"code","066f85b1":"code","e3d6666f":"code","bf72c376":"code","495bc13e":"code","038bed1a":"code","2da8c170":"code","c30511da":"code","fc15bdbd":"code","c93e0d9d":"code","44d3134a":"code","4cf1d800":"code","b7562dda":"code","9f1d458c":"code","2873c5e4":"code","4c5572ca":"code","775c9dd3":"code","873704aa":"code","a85bd92e":"code","ac7ff122":"code","35147717":"code","50d2335f":"code","3a45a7cc":"code","e7b0bb29":"code","f6c2995e":"code","5be3eb1f":"code","e06104ab":"code","59fbf4a0":"code","ae2b3e07":"code","692bba01":"code","7f1e787f":"code","c418b14d":"code","1a6fbade":"code","f95c22bf":"code","624fd928":"code","67b1e1f7":"code","ff74f56d":"code","0680ee7a":"code","e2c7dffd":"code","cbe83ce5":"code","d898bb22":"code","ebb6a558":"code","98e33e38":"code","60b3f9c7":"code","8ec3c377":"code","3dbf71f0":"code","949e23e2":"code","5ab4e877":"code","e5585f31":"code","efef397a":"code","923e4021":"code","34ba02f4":"code","5deaa07c":"code","40e5103b":"code","80ef264e":"code","905c27a8":"code","2733bf63":"code","1ce225f9":"code","81062c96":"code","cf9c5b58":"code","995f12d0":"code","7febdbab":"code","d50a6db7":"code","3b3ba480":"code","22491d25":"code","ee56b6de":"code","a6de2db0":"code","a7c3beee":"code","254e08f7":"code","fcff6200":"code","54d24daf":"markdown","be80b8d3":"markdown","5af85ac8":"markdown","0b81d88d":"markdown","f577a81e":"markdown","284272e6":"markdown","033efa20":"markdown","4093df69":"markdown","ebbf41f1":"markdown","6678c5cf":"markdown","0a6c6c7d":"markdown","8e28d360":"markdown","3b160c17":"markdown","392930f2":"markdown","a6778d69":"markdown","560fbf8d":"markdown","d980be9f":"markdown","0fc3dc26":"markdown","a22c82ff":"markdown","c2f1691d":"markdown","4b7c527f":"markdown","0f589db0":"markdown","684af370":"markdown","0b96a956":"markdown","0e5fa2d9":"markdown","61c35ef0":"markdown","fba1b392":"markdown","fe7eed92":"markdown","8515fbaf":"markdown","001cde28":"markdown","d1b727a8":"markdown","df2d8403":"markdown","b61c34d4":"markdown","fef09f4d":"markdown","e53277d4":"markdown","4dc88c7a":"markdown","b6d73d81":"markdown","718bd27b":"markdown","550b473a":"markdown","06a04cb3":"markdown","556e291c":"markdown","c97a1861":"markdown","3c278e3d":"markdown","89229105":"markdown","849ded81":"markdown","64f2e39e":"markdown","985e4bb3":"markdown","3a79d3f9":"markdown","b9798b6b":"markdown","842423c1":"markdown","68cbd59a":"markdown","8026d933":"markdown","87949902":"markdown","ef739480":"markdown","74b4789d":"markdown","9b7eca35":"markdown","9b138646":"markdown","c6d9f41d":"markdown","8bf46e44":"markdown","aef196aa":"markdown","4ecd584b":"markdown","0cfbccb3":"markdown","28e38de3":"markdown","4bfd78fb":"markdown","4abfec05":"markdown","e5396aed":"markdown","279743be":"markdown","f0f058e2":"markdown","2753ebc8":"markdown","0044811b":"markdown"},"source":{"b75d9ab2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nplt.style.use('fivethirtyeight')\n# sns.set(font_scale=2.5)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","066f85b1":"data=pd.read_csv('..\/input\/titanic\/train.csv')","e3d6666f":"data.head()","bf72c376":"data.isnull().sum()","495bc13e":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\ndata['Survived'].value_counts().plot.pie(explode=[0, 0.1],autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')","038bed1a":"data.groupby(['Sex', 'Survived'])['Survived'].count()","2da8c170":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex: Survived vs Dead')\nplt.show()","c30511da":"pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","fc15bdbd":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\ndata['Pclass'].value_counts().plot.bar(ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass: Survived vs Dead')\nplt.show()","c93e0d9d":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='summer_r')","44d3134a":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","4cf1d800":"print('Oldest Passenger was of: ', data['Age'].max(),'Years')\nprint('Youngest Passenger was of: ', data['Age'].min(), 'Years')\nprint('Average Age on the ship: ', data['Age'].mean(), 'Years')","b7562dda":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 20))\nsns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 20))\nplt.show()","9f1d458c":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')","2873c5e4":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r')","4c5572ca":"data['Initial'].replace(['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don']\n                       ,['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'], inplace=True)","775c9dd3":"data.groupby('Initial')['Age'].mean()","873704aa":"data.loc[(data.Age.isnull()) &(data.Initial == 'Mr'), 'Age'] = 33\ndata.loc[(data.Age.isnull()) &(data.Initial == 'Miss'), 'Age'] = 22\ndata.loc[(data.Age.isnull()) &(data.Initial == 'Mrs'), 'Age'] = 36\ndata.loc[(data.Age.isnull()) &(data.Initial == 'Master'), 'Age'] = 5\ndata.loc[(data.Age.isnull()) &(data.Initial == 'Other'), 'Age'] = 46","a85bd92e":"data.Age.isnull().any()","ac7ff122":"f, ax = plt.subplots(1, 2, figsize=(20, 10))\ndata[data['Survived'] == 0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Dead')\nx1=list(range(0, 85, 5))\nax[0].set_xticks(x1)\ndata[data['Survived'] == 1].Age.plot.hist(ax=ax[1], bins=20, edgecolor='black', color = 'green')\nax[1].set_title('Survived')\nx2=list(range(0, 85, 5))\nax[1].set_xticks(x2)\nplt.show()","35147717":"sns.factorplot('Pclass', 'Survived', col='Initial', data=data)\nplt.show()","50d2335f":"pd.crosstab([data.Embarked, data.Pclass],[data.Sex, data.Survived], margins=True).style.background_gradient(cmap='summer_r')","3a45a7cc":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\ndata['Embarked'].value_counts().plot.bar(ax=ax[0])\nax[0].set_title('Number Of Passengers By Embarked')\nax[0].set_ylabel('Count')\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Embarked: Survived vs Dead')\nplt.show()","e7b0bb29":"sns.factorplot('Embarked', 'Survived', data=data)\nfig = plt.gcf()\nfig.set_size_inches(5, 3)\nplt.show()","f6c2995e":"f, ax = plt.subplots(2, 2, figsize=(20, 15))\nsns.countplot('Embarked', data=data, ax=ax[0, 0])\nax[0, 0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0, 1])\nax[0, 1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1, 0])\nax[1, 0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=data, ax=ax[1, 1])\nax[1, 1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","5be3eb1f":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data)\nplt.show()","e06104ab":"data['Embarked'].fillna('S', inplace=True)","59fbf4a0":"data.Embarked.isnull().any()","ae2b3e07":"pd.crosstab([data.SibSp], data.Survived).style.background_gradient(cmap='summer_r')","692bba01":"f, ax = plt.subplots(1, 2, figsize=(20, 10))\nsns.barplot('SibSp', 'Survived', data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp', 'Survived', data=data, ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.show()","7f1e787f":"pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r')","c418b14d":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","1a6fbade":"f, ax = plt.subplots(1, 2, figsize=(20, 8))\nsns.barplot('Parch', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(3)\nplt.show()","f95c22bf":"print('Highest Fare was: ', data['Fare'].max())\nprint('Lowest Fare was: ', data['Fare'].min())\nprint('Average Fare was: ', data['Fare'].mean())","624fd928":"f, ax = plt.subplots(1, 3, figsize=(20, 8))\nsns.distplot(data[data['Pclass'] == 1].Fare, ax=ax[0])\nax[0].set_title('Fare in Pclass 1')\nsns.distplot(data[data['Pclass'] == 2].Fare, ax=ax[1])\nax[1].set_title('Fare in Pclass 2')\nsns.distplot(data[data['Pclass'] == 3].Fare, ax=ax[2])\nax[2].set_title('Fare in Pclass 3')\nplt.show()","67b1e1f7":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.1)\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","ff74f56d":"data['Age_band']=0\ndata.loc[data['Age']<=16, 'Age_band']=0\ndata.loc[(data['Age']>16) & (data['Age']<=32), 'Age_band']=1\ndata.loc[(data['Age']>32) & (data['Age']<=48), 'Age_band']=2\ndata.loc[(data['Age']>48) & (data['Age']<=64), 'Age_band']=3\ndata.loc[data['Age']>64, 'Age_band']=4\ndata.head()","0680ee7a":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer_r')","e2c7dffd":"sns.factorplot('Age_band', 'Survived', data=data, col='Pclass')\nplt.show()","cbe83ce5":"# data['Family_Size']=0\n# data['Family_Size']= data['Parch'] + data['SibSp']\n# data['Alone'] = 0\n# data.loc[data.Family_Size==0, 'Alone']=1\n\n# f, ax = plt.subplots(1, 2, figsize=(18, 6))\n# sns.factorplot('Family_Size', 'Survived', data=data, ax=ax[0])\n# ax[0].set_title('Family_Size vs Survived')\n# sns.factorplot('Alone', 'Survived', data=data, ax=ax[1])\n# ax[1].set_title('Alone vs Survived')\n# plt.close(2)\n# plt.close(3)\n# plt.show()\n\ndata['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.barplot('Family_Size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.barplot('Alone','Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.show()","d898bb22":"sns.factorplot('Alone', 'Survived', data=data, hue='Sex', col='Pclass')\nplt.show()","ebb6a558":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","98e33e38":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91, 'Fare_cat']=0\ndata.loc[(data['Fare']>7.91) & (data['Fare']<14.454), 'Fare_cat']=1\ndata.loc[(data['Fare']>14.454) & (data['Fare']<31), 'Fare_cat']=2\ndata.loc[(data['Fare']>31) & (data['Fare']<513), 'Fare_cat']=3\n","60b3f9c7":"sns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","8ec3c377":"data['Sex'].replace(['male', 'female'],[0,1], inplace=True)\ndata['Embarked'].replace(['S', 'C', 'Q'],[0,1,2], inplace=True)\ndata['Initial'].replace(['Mr', 'Mrs', 'Miss', 'Master' ,'Other'],[0,1,2,3,4], inplace=True)","3dbf71f0":"# sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\n# fig=plt.gcf()\n# fig.set_size_inches(30, 25)\n# plt.xticks(fontsize=14)\n# plt.yticks(fontsize=14)\n# plt.show()\n\ndata.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\n# sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.1, vmax=1.0, annot_kws={\"size\": 10}, cbar=True)\n# fig=plt.gcf()\n# fig.set_size_inches(20,18)\n# plt.xticks(fontsize=14)\n# plt.yticks(fontsize=14)\n# plt.show()\n\nheatmap_data = data.corr()\n\ncolormap = plt.cm.PuBu\nplt.figure(figsize=(20, 16))\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n\ndel heatmap_data","949e23e2":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","5ab4e877":"train,test=train_test_split(data,test_size=0.3,random_state=45,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","e5585f31":"model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction1 = model.predict(test_X)\nprint('Accuracy for rbf SVM is ', metrics.accuracy_score(prediction1, test_Y))","efef397a":"model=svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is', metrics.accuracy_score(prediction2, test_Y))","923e4021":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3 = model.predict(test_X)\nprint('The accuracy of the Logistic Regression is', metrics.accuracy_score(prediction3, test_Y))","34ba02f4":"model=DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is', metrics.accuracy_score(prediction4, test_Y))","5deaa07c":"model=KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is', metrics.accuracy_score(prediction5, test_Y))","40e5103b":"a_index=list(range(1, 11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1, 11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12, 6)\nplt.show()\nprint('Accuracies for different values of n are: ',a.values,'with the max value as ',a.values.max())","80ef264e":"model=GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is', metrics.accuracy_score(prediction6, test_Y))","905c27a8":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is', metrics.accuracy_score(prediction7, test_Y))","2733bf63":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22, shuffle=True) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","1ce225f9":"plt.subplots(figsize=(12, 6))\nbox=pd.DataFrame(accuracy, index=[classifiers])\nbox.T.boxplot()","81062c96":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8, 5)\nplt.show()","cf9c5b58":"f, ax=plt.subplots(3, 3, figsize=(12, 10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0, 0], annot=True, fmt='2.0f')\nax[0, 0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0, 1], annot=True, fmt='2.0f')\nax[0, 1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0, 2], annot=True, fmt='2.0f')\nax[0, 2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1, 0], annot=True, fmt='2.0f')\nax[1, 0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1, 1], annot=True, fmt='2.0f')\nax[1, 1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1, 2], annot=True, fmt='2.0f')\nax[1, 2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[2, 0], annot=True, fmt='2.0f')\nax[2, 0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2, wspace=0.2)\nplt.show()","995f12d0":"from sklearn.model_selection import GridSearchCV\nC = [0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nkernel=['rbf', 'linear']\nhyper={'kernel':kernel, 'C':C, 'gamma': gamma}\ngd=GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","7febdbab":"n_estimators = range(100, 1000, 100)\nhyper={'n_estimators': n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=22), param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","d50a6db7":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF', svm.SVC(probability=True, kernel='rbf',C=0.5, gamma=0.1)),\n                                              ('RFor', RandomForestClassifier(n_estimators=500, random_state=22)),\n                                              ('LR', LogisticRegression(C=0.6)),\n                                              ('DT', DecisionTreeClassifier(random_state=22)),\n                                              ('NB', GaussianNB()),\n                                              ('svm', svm.SVC(kernel='linear', probability=True))\n                                             ],\n                        voting='soft').fit(train_X, train_Y)\nprint('The accuracy for ensembled model is:', ensemble_lin_rbf.score(test_X, test_Y))\ncross=cross_val_score(ensemble_lin_rbf, X, Y, cv = 10, scoring = \"accuracy\")\nprint('The cross validated score is', cross.mean())","3b3ba480":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=22, n_estimators=700)\nmodel.fit(train_X, train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for baged KNN is:', metrics.accuracy_score(prediction, test_Y))\nresult=cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for bagged KNN is:', result.mean())","22491d25":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200, random_state=22, learning_rate=0.1)\nresult=cross_val_score(ada, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for Adaboost is:', result.mean())","ee56b6de":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=800, random_state=22, learning_rate=0.1)\nresult=cross_val_score(grad, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:', result.mean())","a6de2db0":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=700, learning_rate=0.1)\nresult=cross_val_score(xgboost, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for XGBoost is:', result.mean())","a7c3beee":"n_estimaters=list(range(100, 1100, 100))\nlearn_rate=[0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nhyper={'n_estimators':n_estimators, 'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","254e08f7":"ada=AdaBoostClassifier(n_estimators=200, random_state=22, learning_rate=0.05)\nresult=cross_val_predict(ada, X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, result), cmap='summer', annot=True, fmt='2.0f')\nplt.show()","fcff6200":"f, ax = plt.subplots(2, 2, figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500, random_state=22)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 0])\nax[0, 0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200, random_state=22, learning_rate=0.05)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 1], color='#ddff11')\nax[0, 1].set_title('Feature Importance in Adaboost')\nmodel=GradientBoostingClassifier(n_estimators=500, random_state=22, learning_rate=0.1)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 0], cmap='RdYlGn_r')\nax[1, 0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900, random_state=22, learning_rate=0.1)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 1], color='#FD0F00')\nax[1, 1].set_title('Feature Importance in XgBoost')\nplt.show()","54d24daf":"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.","be80b8d3":"#### Inprepreting Confusion Matrix\n\nThe ledft diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong predictions made. Lets consider the first plot for rbf-SVM:\n\n1) The no. of correct predictions are 491(for dead) + 247(for survived) with the mean CV accuracy being(491+247)\/891 = 82.8% which we did get earlier.\n\n2) Errors -> wrongly classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by prdicting dead as survived.\n\nBy looking at all the metrics, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predictiong passengers who survived.\n\nHyper-Parameters Tuning\n\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\n\nWe will tune the hyper-parameters for the 2 best classifier i.2 the SVM and RandomForest.","5af85ac8":"The sinking of the Titanic is the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. this is a very unforgetable disaster tha no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling probelm. How do we check features, how do we add new features and som Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that newbies can understand evey phase of it.\n\nIf You Like the notebook and think that it helped you..","0b81d88d":"Clearly, as the Fare_cat increases, the survival chances increases. This feature may become may an important feature during modeling along with Sex.\n\n## Converting String Values into Numeric\n\nSince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.","f577a81e":"We got the highest accuracy for AdaBoost. We will try to increase it wiht Hyper-Parameter Tuning\n\n**Hyper-parameter Tuning for AdaBoost**","284272e6":"Okay so there are some misspelled Initials like Mlle and Mme that stand for Miss. I will replace them with Miss and same thing for other values.","033efa20":"#### Observations:\n\nHere too the results are quit similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n\nThe chances or survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.","4093df69":"## Part1: Exploratory Data Analysis(EDA)","ebbf41f1":"This feature represents whether a person is alone or with his family members.\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife","6678c5cf":"#### Droping UnNeeded Features\nName -> We don't need name feature as it cannot be converted into any categorical value.\n\nAge -> We have the Age_band feature, so no need of this.\n\nTicket -> It is any random string that cannot be categorised.\n\nFare -> We have the Fare_cat feature, so unneeded\n\nCabin -> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\nFare_Range -> We have the fare_cat feature\n\nPassengerId -> Cannot br categorised.\n","0a6c6c7d":"#### Linear Support Vector Machine(linear - SVM)","8e28d360":"This looks interesting. The number of men on the ship is a lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survial rates for a women on the ship is around 75% while that for men in around 18-19%.\n\nThis looks to be a very important feature for modeling. But is it the best?? Let's check other features.","3b160c17":"## Family_Size and Alone\nAt this point, we can create a new feature called 'Family_size' and 'Alone' and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.","392930f2":"## Correlation Between The Features","a6778d69":"## Boosting\n\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a week model. Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continuous, and new classifier are added to the model until the limit is reached on the accuracy.\n\nAdaboost(Adaptive Boost)\nThe weak learner or estimator in this case is a Decision Tree. But we can change the default base_estimator to ant argorithm of our choice.","560fbf8d":"#### Random Forests","d980be9f":"The chances for Port C is highest around 0.55 while it is lowest for S","0fc3dc26":"## Types Of Features\n\n#### Categories Features:\nA categorical variable is one that hs two or more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are alseo known as **Normal Variables**.\n\n**Categorical Features in the dataset: Sex, Embarked.**\n\n#### Ordinal Features:\nAn ordinal variables is similar to categorical values, but the difference between them is that we can have relative ordering or sortinf between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\n#### Continuous Feature:\nA feature is said to be continuous if it can take values between any two points or between the minimum or maximum values in the features column.\n\n**Continuous Features in the dataset: Age\n\n#### Analysis The Features\n\n#### Sex -> Categorical Feature","a22c82ff":"The Women and Child first policy thus holds true irrespective of the class","c2f1691d":"#### K-Nearest Neighbours(KNN)","4b7c527f":"**Stochastic Gradient Boosting**\n\nHere too the weak learner is a Decision Tree.","0f589db0":"#### Gaussian Naive Bayes","684af370":"**Filling NaN Ages**","0b96a956":"Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default is 5. Lets check the accuracies over various of n_neighbours.","0e5fa2d9":"#### Radial Support Vector Machines(rbf-SVM)","61c35ef0":"## SibSp -> Discrete Feature","fba1b392":"## EDA To Prediction (DieTanic)\n\n* Sometimes life has a cruel sense of humor, giving you the thing you always wanted at the worst time possible\n","fe7eed92":"## Embarked -> Categorical Value","8515fbaf":"## Age -> countinuous Feature","001cde28":"#### Logistic Regression","d1b727a8":"## Fare -> continuous Feature","df2d8403":"## Part2: Feature Engineering and Data Cleaning\nNow what is Feature Engineering?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There matbe be many redundant features which should be eliminated. Also we can get or add new features bt observing or extracting information from other features. \n\nAn example would be getting the initials feature_using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will transform the existing relevant features to suitable form for predictive Modeling.\n\n## Age_band\n\nProblem With Age Feature:\n\nAs I have mentioned earlier that Age is a continuos feature, ther is a peoblem with Continuous Variabels in Machine Learning Models.\n\nEg: If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.\n\nNow if I say to groupp them by their Age, then how would you do it? If ther are 30 persons, there may be 30 age value. Now this is problematic.\n\nWe need to convert these continuous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n\nOkay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80\/5 = 16. So bins of size 16.\n\n","b61c34d4":"**Observations:**\n1) The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n\n2) The oldest Passenger was saved(80 years)\n\n3) Maximum number of deaths were in the age group of 30-40.","fef09f4d":"Many a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n\n1) The K-Fold Cross Vaildation works by first dividing the dataset into k-subsets.\n\n2) Let's say we divide the dataset into (k-5) parts. We reserve 1 part for training and train the algorithms over the 4 parts.\n\n3) We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\nThis is called K-Fold Cross Vaildation.\n\n4) An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","e53277d4":"#### Decision Tree","4dc88c7a":"As we had seen earlier, the Age feature has 177 null values. To replce these NaN values, we can assign them the mean age of the dataset.\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that s 29 years. Is ther any way to find out what age-band does the passenger lie?\n\n**Bingo!!!!**, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups.\n\n**What's In A Name?? -> Feature**","b6d73d81":"#### Observaitons:\n1) The number of children increases with Pclass and the survival rate for passengers below Age 10(i.e children) looks to be good irrescpective of the Pclass.\n\n2) Survival chances for Passengers aged 20-50 from Pcalss is high and is even better for Women.\n\n3) For males, the survival chances decreases with an increase in age.","718bd27b":"Family_Size=0 means that the passenger is alone. Clearly, if you are alone or family_size = 0, then chances for survivals is very low. For family size > 4, the chances decrease too. This also looks to be an important feature the model. Lets examine this further.","550b473a":"We use FactorPlot in this case, beacause they make the seperation of categorical values easy.\nLooking at the CrossTab and the FactorPlot, We can easily infer that survival for Women from Pclass 1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nLooks like Pclass is also an important feature. Lets analyse other features.","06a04cb3":"#### How many Survived?","556e291c":"The Age, Cabin, Embarked have null values. I will try to fix them.","c97a1861":"As discussed above, we can clearly see that as the fare_range increases, the chances of survival increases.\n\nNow we cannot pass the Fare_Range values as it is. We should convert it into singleton vlaues same as we did in Age_Band","3c278e3d":"Okay so here we are using the Regex: [A-Za-z]-+). So what it does is, it looks for strings which lie between A-Z or a-z and followed by a. So we successfully extract the initials from the Name.","89229105":"## SVM","849ded81":"It isevident that not many passengers survived the accident.\n\nOut of 891 passengers in training set, only around 350 Survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights for the data and see which categories of the passengers did survive and who didn't.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age, etc.\n\nFirst let us understand the different types of features.","64f2e39e":"People say **Money Can't Buy Everything.** But we can clearly see that Passengers Of Pclass 1 were given very high priority while rescue. Even though the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%\n\nFor Pclass 1% survived is around 63% while for Pcalss 2 is around 48%. So money and status matters. Such a materialistic world.\n\nLets Dive in little bit more and check for other interesting observations. Lets check survival rate with **Sex and Pclass** Together.","985e4bb3":"There looks to be a large distribution in the fares of Passengers in Pclass2 and this distribution goes on decreasing as the standards reduces. As this is also continuous, we can convert into discrete values by using binning.","3a79d3f9":"#### Confusion Matrix for the Best Model","b9798b6b":"## Pclass -> Ordinal Feature","842423c1":"#### Filling Embarked NaN\nAs we saw that maximum passengers boarded from Port S, we replace NaN with S.","68cbd59a":"## Concepts of the Notebook\n#### Part1: Exploratory Data Analysis(EDA)\n1) Analysis of the features\n\n2) Finding any relations or trends considering multiple features\n\n#### Part2: Feature Engineering and Data Cleaning:\n1) Adding any few features\n\n2) Removing redundant features\n\n3) Converting features into suitable form for modeling\n\n#### Part3: Predictive Modeling\n1) Running Basic Algorithms\n\n2) Cross Validation\n\n3) Ensembling\n\n4) Inportant Features Extraction","8026d933":"#### Observations:\n1) Maximum passengers boarded from S. Majority of them being Pclass3.\n\n2) The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3) The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n\n4) Port Q had almost 95% of the Passengers were from Pclass3.\n","87949902":"#### Bagging \n\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averagif, there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n\n**Bagged KNN**\n\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbors, as small value of n_neighbors.","ef739480":"#### Random Forests","74b4789d":"## Fare_Range\nSince fare is also a continuous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.\n\nSo what qcut does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally sapced into 5 seperate bins or value ranges.","9b7eca35":"#### Chances for Survival by Port Of Embarkation","9b138646":"True that... the survival rate decreases as the age increases irrespctive of the Pclass.","c6d9f41d":"## Ensembling\n\nEnsembling is a good way to increase the accuracy or performance of a model. In simple word, it is the combination of various simle models to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stabilityof the model. Ensembling can be done in ways like:\n\n1) Voting Classifier\n\n2) Bagging\n\n3) Boosting\n\n## Voting Classifier\n\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediciton of all the submodels. The submodels or the basemodels are all of different types.","8bf46e44":"#### Interpreting The Heatmap\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot between alphabets or strings. Before understanding the plot, let us see exactly correlation is.\n\n**Positive Correlation:** If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\n**Negative Correlation:** If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect positive correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is Known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should us both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap, we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features.","aef196aa":"## Feature Importance","4ecd584b":"#### Observations:\n1) The survival chances are almost 1 for women for Pclass1 and Pcalss2 irrespective of the Pclass.\n\n2) Port S looks to be very unlucky for Pclass3 Passengers as the survival rate for both men and women is very low.(Money Matters)\n\n3) Port Q looks to be unlukiest for Men, as almost all were from Pclass 3.","0cfbccb3":"## Cross Validation","28e38de3":"## Parch","4bfd78fb":"#### Observations:\n\nThe barplot and factor shows that is a passenger is alone onboard with no siblings, he save 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is 0%. The reason may be Pclass?\n\nThe reason is Pclass. The crosstab shows tha Person with SibSp>3 were all in Pcalss3. It is imminent that all the large families in Pclass3 (>3) died.","4abfec05":"Now the above correlation plot, we can see some positively related features. Some of them being SibSp and FamilySize and Parch and FamilySize and some negative ones like Alone and Family_Size","e5396aed":"**XGBoost**","279743be":"The crosstab again shows that larger families were in Pclass3.","f0f058e2":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nNow this this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over?? The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data cahnges, the accuracy will also change. It may increase or decrease. This is known as model Validation.","2753ebc8":"#### Observations in a Nutshell for all features:\n\n**Sex:** The chance of survival for women is high as compared to men.\n\n**Pclass:** There is a visivle trend that being a 1st class Passenger gives you better chances of survival. The survival rate for Pclass3 is very low. For women, the chance of survival from Pclass1 is almost 1 and is high too tor those from PCalss2. Money wins!!\n\n**Age:** Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n\n**Embarked:** This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass3.\n\n**Parch+SibSp:** Having 1-2 SibSp on board or 1-3 Parents show a greater chance of probability rather than being alone or having a large family travelling with you.","0044811b":"## Part3: Predictive Modeling\n\nWe have gained some isights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms. Following are the argorithms I will use to make the models:\n\n1) Logistic Regression\n\n2) Support Vector Machines(Linear and radial)\n\n3) Random Forest\n\n4) K-Nearest Neighbors\n\n5) Naive Bayes\n\n6) Decision Tree\n\n7) Logistic Regression"}}