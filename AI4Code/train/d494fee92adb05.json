{"cell_type":{"2302745b":"code","89cf520a":"code","112bd06d":"code","f9cb46cb":"code","886ebdec":"code","a1eb8b46":"code","e07066bd":"code","53b4e2a0":"code","ae38b0ee":"code","bb20800b":"code","928280eb":"code","7873bf64":"code","fab7762b":"code","d5010582":"code","8d5928f3":"code","9dc95e55":"code","0684327f":"code","7073b52c":"code","3110af00":"code","ce034498":"code","0ff3f57d":"code","e4ee69e6":"code","792a857c":"markdown","5c35f017":"markdown","c7e7360d":"markdown","c6e5f896":"markdown","79a42c9c":"markdown","5ba90cbb":"markdown","5683f6dd":"markdown","e731f985":"markdown","8cecec0a":"markdown"},"source":{"2302745b":"default_path = '..\/input\/'","89cf520a":"from keras.datasets import mnist","112bd06d":"import pandas as pd\ntrain_csv = pd.read_csv(default_path+'train.csv')\ntest_csv = pd.read_csv(default_path+'test.csv')","f9cb46cb":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nkaggle_training_data = np.array(train_csv)\nkaggle_testing_data = np.array(test_csv)\nprint(kaggle_training_data.shape, kaggle_testing_data.shape)\n\nkaggle_feature = kaggle_training_data[:, 1:]\nkaggle_label = kaggle_training_data[:, :1]","886ebdec":"# keras dataset\n(keras_train_X, keras_train_y), (keras_test_X, keras_test_y) = mnist.load_data()\nkeras_train_X = keras_train_X.reshape(-1, 28*28)\nkeras_test_X = keras_test_X.reshape(-1, 28*28)\nprint(keras_train_X.shape)","a1eb8b46":"# concat\nall_feature = np.r_[kaggle_feature, keras_train_X, keras_test_X]\nall_label = np.r_[kaggle_label.ravel(), keras_train_y, keras_test_y]\nprint(all_feature.shape, all_label.shape)","e07066bd":"X_train, X_test, y_train, y_test = train_test_split(all_feature, all_label, test_size = 0.1, shuffle=True)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","53b4e2a0":"import seaborn as sns\nsns.set(style='white')\ng = sns.countplot(train_csv['label'])","ae38b0ee":"# check missing value\ntrain_csv.isnull().any().any()","bb20800b":"# Normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","928280eb":"# Reshape\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28,1)","7873bf64":"# One-hot encoding\nfrom keras.utils import to_categorical\ny_train_oneHot = to_categorical(y_train, num_classes= 10)\ny_test_oneHot = to_categorical(y_test, num_classes=10)","fab7762b":"import matplotlib.pyplot as plt\nplt.imshow(X_train[0][:, :, 0])","d5010582":"from keras.layers import Convolution2D, Dense, MaxPooling2D, Dropout, Flatten\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same', input_shape = (28, 28, 1), activation='relu'))\nmodel.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu'))       \nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Convolution2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu'))       \nmodel.add(Convolution2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu'))       \nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=256, activation='relu',))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.summary()","8d5928f3":"# optimizer = 'adam'\n#model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n# optimizer = 'RMSprop'\nmodel.compile(optimizer='RMSprop', loss = 'categorical_crossentropy', metrics=['accuracy'])","9dc95e55":"from keras.callbacks import ReduceLROnPlateau\n# To keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n# With the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs.\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)","0684327f":"from keras.preprocessing.image import ImageDataGenerator\n# I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(X_train)\n\n","7073b52c":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train, y_train_oneHot, batch_size=512),\n                              epochs = 30, validation_data=(X_test, y_test_oneHot),\n                              verbose = 1, steps_per_epoch=X_train.shape[0]\/512, \n                              callbacks=[learning_rate_reduction])","3110af00":"fig, [ax, ax1] = plt.subplots(1, 2)\nfig.set_size_inches(12, 4)\nax.plot(history.history['acc'], label='Train acc')\nax.plot(history.history['val_acc'], label='Val acc')\nax.legend(loc='best')\n\nax1.plot(history.history['loss'], label='Train loss')\nax1.plot(history.history['val_loss'], label='Val loss')\nax1.legend(loc='best')","ce034498":"model.evaluate(X_test, y_test_oneHot)","0ff3f57d":"testing_data = scaler.transform(kaggle_testing_data).reshape(-1, 28, 28, 1)\nCNN_prediction = model.predict_classes(testing_data)","e4ee69e6":"result = pd.DataFrame(CNN_prediction)\nresult.index += 1\nresult.index.name = 'ImageId'\nresult.columns = ['Label']\nresult.to_csv('results_CNN_.csv',header=True)\nresult.head()","792a857c":"(2018.8.23\u66f4\u65b0)\u4e0d\u7ba1\u600e\u9ebc\u505a\u7e3d\u662f\u5361\u572899.95%\u9644\u8fd1...\u76f4\u5230\u770b\u5230LB\u524d\u9762\u7684\u4eba\u7684Kernel\u5f8c\u624d\u767c\u73fe...\n\n\u6709\u4eba\u898f\u5b9a\u4e0d\u80fd\u5f9e\u5916\u90e8\u5f15\u5165data\u55ce...\u6211\u5011\u81ea\u5df1\u5f9ekeras\u5f15\u9032\u66f4\u591a\u7684\u8cc7\u6599\u4f86Train\u5427QQ\n\n\u9019\u6280\u5de7\u771f\u662f\u592a\u725bB\u5566\uff0c\u6c92\u6709data\u4f55\u4e0d\u53bb\u5176\u4ed6\u5730\u65b9\u627edata\u5462\n\n---\n\n\u7b2c\u4e00\u6b21\u53c3\u52a0Kaggle\u7684\u5be6\u4f5c\uff0c\u81ea\u5df1\u7df4\u7fd2\u4e86Sklearn\u7684\u5e7e\u500b\u6a21\u578b\uff0c\u6700\u5f8c\u53c3\u8003\u5225\u4eba\u7684kernel\u5b8c\u6210\u4e86\u9019\u7bc7(Top3%)\u3002\n\nresult: 0.99928%(Top12%)\n\nreference: \n- [Introduction to CNN Keras - 0.997 (top 6%)\n](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6)\n- [Simple Keras+Tensorflow CNN on MNIST](https:\/\/www.kaggle.com\/nandvard\/simple-keras-tensorflow-cnn-on-mnist)\n","5c35f017":"# \u4e0d\u540cClassifier Model\u7684\u7df4\u7fd2\n\u53ea\u662f\u7df4\u7fd2Code\u600e\u9ebc\u5beb\uff0c\u6c92\u6709\u8abf\u53c3\u7684\u90e8\u4efd\uff0c\u6700\u5f8c\u4e5f\u6c92\u6709\u4f7f\u7528\u9019\u4e9bModel\u3002","c7e7360d":"## 3.AdaBoost\n```\nfrom sklearn import ensemble\nAdaBoost = ensemble.AdaBoostClassifier(n_estimators=200, learning_rate=0.1).fit(X_train[:5000], y_train[:5000])\nAdaBoost.score(X_test[:1000], y_test[:1000])\n```","c6e5f896":"## 2.SVC\n```\nfrom sklearn.svm import SVC\nSVC = SVC(kernel = 'poly').fit(X_train, y_train.ravel())\nSVC.score(X_test[:1000], y_test[:1000])\n\nresult = pd.DataFrame(SVC.predict(testing_data))\nresult.index += 1\nresult.index.name = 'ImageId'\nresult.columns = ['Label']\nresult.to_csv('results_SVC.csv',header=True)\nresult\n```","79a42c9c":"## 6.Naive Bayes\n```\nfrom sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB().fit(X_train, y_train)\nNB.score(X_test[:1000], y_test[:1000])\n```","5ba90cbb":"# CNN","5683f6dd":"## 1.KNNClassifier\n```\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=10)\nKNN.fit(X_train, y_train)\nprint(KNN.predict(X_test[:10]).shape,  y_test[:10].flatten())\n\nprint(X_test[:10].shape)\n\n#KNN.score(KNN.predict(X_test[:10]), y_test[:10].flatten())\n\nprediction = KNN.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, prediction))\n\nprediction_test = KNN.predict(testing_data)\nprediction_test\n```","e731f985":"## 5.Random Forst\n```\nfrom sklearn import ensemble\nRandomForst = ensemble.RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\nRandomForst.score(X_test, y_test)\n```","8cecec0a":"## 4.Bagging\n```\nBagging = ensemble.BaggingClassifier(n_estimators=200).fit(X_train[:5000], y_train[:5000])\nBagging.score(X_test[:1000], y_test[:1000])\n```"}}