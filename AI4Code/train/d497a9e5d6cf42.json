{"cell_type":{"39d4726c":"code","c74d18b5":"code","76a2201d":"code","4c7ae909":"code","d5296e37":"code","9c5afbd6":"code","4eb740a4":"code","8b3a6202":"code","aac6a2a9":"code","77dbe780":"code","ae790d7b":"code","2d5f0966":"code","cdd096d0":"code","ce986950":"code","503f2385":"code","92d94c3b":"code","b02dd898":"code","7636a323":"code","db44808d":"code","99c923cb":"code","f2eb8ab3":"code","daf37cd5":"code","de0073f0":"code","cf782781":"code","363c158a":"code","fb68d3f7":"code","c4a98a36":"code","64e49f07":"code","dd562944":"code","681e9bb7":"code","765d8da5":"code","09b2ebc6":"code","fceda7a5":"code","5ba1d8eb":"markdown","0842ed70":"markdown","12852ccd":"markdown","1be23c9d":"markdown","8eaf85c7":"markdown","b5521e22":"markdown","2ac3df7a":"markdown"},"source":{"39d4726c":"#installing contractions library\n!pip install contractions","c74d18b5":"#Generic Data Processing & Visualization Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re,string,unicodedata\nimport contractions #import contractions_dict\n%matplotlib inline","76a2201d":"#Importing text processing libraries\nimport spacy\nimport spacy.cli\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n#downloading wordnet\/punkt dictionary\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')","4c7ae909":"#Installing & Importing Sentiment Analysis Library  - AFINN\n!pip install afinn\nfrom afinn import Afinn","d5296e37":"data = pd.read_csv(\"..\/input\/nlp-dataset-collected-from-youtube-comments\/iran.csv\")","9c5afbd6":"data.shape","4eb740a4":"#checking for null\/missing values\ndata.isna().sum()","8b3a6202":"#dropping the index with missing comments\ndata = data.dropna()\ndata.shape","aac6a2a9":"#creating a new column in the dataset for word count\ndata ['word_count'] = data['Comments'].apply(lambda x:len(str(x).split(\" \")))","77dbe780":"data.head()","ae790d7b":"#taking a copy of the clean dataset\ndata_clean = data.copy()","2d5f0966":"#lowering cases\ndata_clean['Comments'] = data_clean['Comments'].str.lower()","cdd096d0":"#stripping leading spaces (if any)\ndata_clean['Comments'] = data_clean['Comments'].str.strip()","ce986950":"#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n  for punctuations in punctuation:\n    text = text.replace(punctuations, '')\n  return text\n\n#apply to the dataset\ndata_clean['Comments'] = data_clean['Comments'].apply(remove_punct)","503f2385":"#function to remove special characters\ndef remove_special_chars(text, remove_digits=True):\n  pattern = r'[^a-zA-z0-9\\s]'\n  text = re.sub(pattern, '', text)\n  return text\n\n#applying the function on the clean dataset\ndata_clean['Comments'] = data_clean['Comments'].apply(remove_special_chars)","92d94c3b":"#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata_clean['Comments'] = data_clean['Comments'].apply(remove_accented_chars)  ","b02dd898":"#Function to expand contractions\ndef expand_contractions(con_text):\n  con_text = contractions.fix(con_text)\n  return con_text\n\n#applying the function on the clean dataset\ndata_clean['Comments'] = data_clean['Comments'].apply(expand_contractions)  ","7636a323":"data_clean.head()","db44808d":"#dropping 'label' column as it is does not serve any purpose\ndata_clean = data_clean.drop(columns='label',axis=1)","99c923cb":"#back up of the prepared data\ndata_clean_bckup = data_clean.copy()","f2eb8ab3":"stopword_list = set(stopwords.words('english'))","daf37cd5":"tokenizer = ToktokTokenizer()","de0073f0":"#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata_clean['Comments_Clean'] = data_clean['Comments'].apply(remove_stopwords)      ","cf782781":"#Function for stemming\ndef simple_stemmer(text):\n  ps = nltk.porter.PorterStemmer()\n  text = ' '.join([ps.stem(word) for word in text.split()])\n  return text\n\n#applying the function\ndata_clean['Normalized_Comments'] = data_clean['Comments_Clean'].apply(simple_stemmer)\n","363c158a":"#dropping unwanted columns\ndata_clean = data_clean.drop(columns=data_clean[['Comments_Clean']],axis=1)\ndata_clean.head()","fb68d3f7":"#rearranging columns\ndata_clean = data_clean[['Comments','Normalized_Comments','word_count']]\n\n#taking backup \ndata_clean_bckup_norm = data_clean.copy()\n\ndata_clean.head()","c4a98a36":"#Instantiating Afinn Library\naf = Afinn()","64e49f07":"#function to perform Afinn Sentiment Analyis\ndef afinn_sent_analysis(text):\n  score = af.score(text)\n  return score\n\n#applying the function to Normalized Comments\ndata_clean['afinn_score'] = [afinn_sent_analysis(comm) for comm in data_clean['Normalized_Comments']]","dd562944":"#function to categorize the afinn sentiment score\ndef afinn_sent_category(score):\n  categories = ['positive','negative','neutral']\n  if score > 0:\n    return categories[0]\n  elif score < 0:\n    return categories[1]\n  else:\n    return categories[2]  \n\ndata_clean['afinn_sent_category'] = [afinn_sent_category(scr) for scr in data_clean['afinn_score']]","681e9bb7":"#taking backup \ndata_clean_bckup_afinn = data_clean.copy()","765d8da5":"data_clean.head()","09b2ebc6":"sns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(figsize=(8,8))\nax = sns.countplot(x=\"afinn_sent_category\", data=data_clean)\nplt.title('Sentiment Category Count Plot')\nplt.ylabel('Count')\nplt.xlabel('Sentiment Category')\n\n#ax.set_xticklabels(ax.get_xticklabels(),rotation=0)\n#i=0\n#for p in ax.patches:\n#    height = p.get_height()\n#    ax.text(p.get_x()+p.get_width()\/2., height + 1,\n#        data_clean['afinn_sent_category'].value_counts()[i],ha=\"center\")\n#    i += 1","fceda7a5":"sns.set(style=\"whitegrid\")\nf, ax = plt.subplots(figsize=(15, 10))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(x=\"afinn_score\", y=\"word_count\", \n                hue=\"afinn_sent_category\", \n                palette=\"ch:r=-.2,d=.3_r\", \n                sizes=(1,8), \n                data=data_clean, ax=ax)","5ba1d8eb":"**Text Processing\/Normalization - Stemming**\n\napplying the most simplest stemmer i.e. PorterStemmer","0842ed70":"**Sentiment Analysis - Using Afinn Library**","12852ccd":"**Data Exploration**","1be23c9d":"**Data Preperation**","8eaf85c7":"**Visualisation**","b5521e22":"**Youtube Comments Sentiment Analysis**\n\nFirst of all thank you @gsc ankith for upload this dataset. Here I have tried to perform a very simple sentiment analysis using AFINN library","2ac3df7a":"**Text Processing\/Normalization - Removing Stop Words**"}}