{"cell_type":{"33dc2ba2":"code","bb0dde60":"code","79092c4d":"code","16cb6c43":"code","54de0582":"code","01da7149":"code","57d07239":"code","0d9e1f68":"code","dbe0f997":"code","78e5f412":"code","67e405a8":"code","a8ff165c":"code","deab3f8a":"code","4191fb9d":"code","883b481b":"code","a8d24201":"code","97cdf00b":"code","34f03af7":"code","cc2da7c5":"code","7641a51a":"code","4dd016f2":"code","1a004c49":"code","2031f6c9":"code","0a177866":"code","5cc24c03":"code","714bf3c4":"code","9ae59136":"code","583e819d":"code","a3ceb188":"code","5230ebe4":"code","73d087e7":"code","c12ca5b5":"code","6ca97333":"code","ba7d05bb":"code","a9097d48":"code","b162449d":"code","49d82f42":"code","a1f0cc5f":"code","5fe41286":"code","869d61d2":"code","1080aad1":"code","16c2f180":"code","cf39ddf3":"code","2decc8ca":"markdown","9a8db6e8":"markdown","34edccca":"markdown","f0fdff8f":"markdown"},"source":{"33dc2ba2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","bb0dde60":"dataset = pd.read_csv('..\/input\/the-spotify-hit-predictor-dataset\/dataset-of-90s.csv')#importing CSV file\ndataset.head()","79092c4d":"features_with_na = [ feature for feature in dataset.columns if dataset[feature].isnull().sum() >1]#extracting feature with nan values\nfeatures_with_na","16cb6c43":"dataset.isnull().sum() #no null values","54de0582":"numerical_feature = [ feature for feature in dataset.columns if dataset[feature].dtypes != \"O\"] #extracting numerical feature\nnumerical_feature","01da7149":"discrete_feature = [feature for feature in numerical_feature if len(dataset[feature].unique()) <15] #extracting discrete features\ndiscrete_feature","57d07239":"for feature in discrete_feature: #understanding impact of discrete values on taget\n    data = dataset.copy()\n    data.groupby(feature)[\"target\"].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel(\"target\")\n    plt.show()","0d9e1f68":"continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]#extracting continuous features\ncontinuous_feature","dbe0f997":"for feature in continuous_feature: #plotting continuous features\n    data = dataset.copy()\n    data[feature].hist(bins=20)\n    plt.xlabel(feature)\n    plt.ylabel(\"count\")\n    plt.show()","78e5f412":"for feature in continuous_feature: #transforming skew features to log transform\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data[feature].hist(bins=20)\n        plt.xlabel(feature)\n        plt.ylabel(\"counts\")\n        plt.show()","67e405a8":"for feature in continuous_feature: #checking outliers\n    data = dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","a8ff165c":"categorical_features = [feature for feature in dataset.columns if dataset[feature].dtypes == \"O\"] #extracting categorical features\ncategorical_features","deab3f8a":"for feature in categorical_features:\n    print(\"The Feature is {} and the no of categories are: {}\".format(feature,len(dataset[feature].unique())))","4191fb9d":"dataset.corr()","883b481b":"sns.heatmap(dataset.corr()) #correlation","a8d24201":"def correlation(dataset, threshold):#extracting highly correlated features \n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n    return col_corr","97cdf00b":"correlation_features = correlation(dataset,0.7)\ncorrelation_features","34f03af7":"for feature in continuous_feature:#handling skew data with log transform\n    if 0 in dataset[feature].unique():\n        pass\n    else:\n        dataset[feature] = np.log(dataset[feature])\n        dataset[feature].hist(bins=20)\n        plt.xlabel(feature)\n        plt.ylabel(\"counts\")\n        plt.show()","cc2da7c5":"dataset = dataset.drop([\"loudness\",\"sections\"],axis = 1) #dropping highly correlated features","7641a51a":"dataset.describe()","4dd016f2":"feature = [feature for feature in dataset.columns if feature not in [\"target\",\"artist\",\"uri\",\"track\"]]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(dataset[feature])","1a004c49":"x = scaler.transform(dataset[feature]) #scaling data","2031f6c9":"dataset.head()","0a177866":"data = pd.concat([dataset[[\"target\",\"artist\",\"uri\",\"track\"]].reset_index(drop= True),pd.DataFrame(scaler.transform(dataset[feature]),columns = feature)],axis=1)","5cc24c03":"data.head()","714bf3c4":"data.drop([\"artist\",\"uri\",\"track\"],axis=1,inplace=True) #dropping useless features","9ae59136":"data.head()","583e819d":"X = data.iloc[:,1:]\nY = data.iloc[:,[0]]\nY","a3ceb188":"from sklearn.model_selection import train_test_split #train-test split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=30)","5230ebe4":"from sklearn.datasets import make_classification\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_auc_score","73d087e7":"from sklearn.linear_model import LogisticRegression \nlr = LogisticRegression(solver='liblinear')\nlr.fit(x_train,y_train)\nytrain_pred = lr.predict_proba(x_train)\nytest_pred = lr.predict_proba(x_test)\nprint(\"roc train score :\",roc_auc_score(y_train,ytrain_pred[:,1]))\nprint(\"roc test score :\",roc_auc_score(y_test,ytest_pred[:,1]))","c12ca5b5":"from sklearn.ensemble import RandomForestClassifier\nrandom_model = RandomForestClassifier()\nrandom_model.fit(x_train,y_train)\nytrain_pred = random_model.predict_proba(x_train)\nytest_pred = random_model.predict_proba(x_test)\nprint(\"roc train score :\",roc_auc_score(y_train,ytrain_pred[:,1]))\nprint(\"roc test score :\",roc_auc_score(y_test,ytest_pred[:,1]))","6ca97333":"from sklearn.ensemble import AdaBoostClassifier\nada_model = AdaBoostClassifier()\nada_model.fit(x_train,y_train)\nytrain_pred = ada_model.predict_proba(x_train)\nytest_pred  = ada_model.predict_proba(x_test)\nprint(\"roc train score :\",roc_auc_score(y_train,ytrain_pred[:,1]))\nprint(\"roc test score :\",roc_auc_score(y_test,ytest_pred[:,1]))","ba7d05bb":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = AdaBoostClassifier()\nknn_model.fit(x_train,y_train)\nytrain_pred = knn_model.predict_proba(x_train)\nytest_pred  = knn_model.predict_proba(x_test)\nprint(\"roc train score :\",roc_auc_score(y_train,ytrain_pred[:,1]))\nprint(\"roc test score :\",roc_auc_score(y_test,ytest_pred[:,1]))","a9097d48":"pred = []#predicting probablities of all the models test data\nfor model in [lr,random_model,ada_model,knn_model]:\n    pred.append(pd.Series(model.predict_proba(x_test)[:,1]))\nfinal_prediction = pd.concat(pred,axis=1).mean(axis=1)\nprint(\"test ROC-AUC\",roc_auc_score(y_test,final_prediction))","b162449d":"pd.concat(pred,axis=1)","49d82f42":"from sklearn.metrics import roc_curve, auc #finding threshold,fpr,tpr\nfpr,tpr,threshold = roc_curve(y_test,final_prediction)\nthreshold","a1f0cc5f":"from sklearn.metrics import accuracy_score #finding accuracy with different threshold\naccuracy_ls = []\nfor thres in threshold:\n    y_pred = np.where(final_prediction>thres,1,0)\n    accuracy_ls.append(accuracy_score(y_test,y_pred,normalize=True))\n    \naccuracy_ls = pd.concat([pd.Series(threshold),pd.Series(accuracy_ls)],axis=1)\naccuracy_ls.columns = ['threshold','accuracy']\naccuracy_ls.sort_values(by='accuracy',ascending=False,inplace =True)\naccuracy_ls.head()\n","5fe41286":"accuracy_ls #accuracy vs threshold","869d61d2":"from sklearn.ensemble import RandomForestClassifier #choose random forest as final model\nrandom_model = RandomForestClassifier( criterion='gini')\nrandom_model.fit(x_train,y_train)\ny_pred = random_model.predict(x_test)\nprint(\"roc test score :\",roc_auc_score(y_test,y_pred))","1080aad1":"threshold = 0.493468 #best threshold for the model\n\npredicted_proba = random_model.predict_proba(x_test)\npredicted = (predicted_proba [:,1] >= threshold).astype('int')\n\n\naccuracy = accuracy_score(y_test, predicted)\nprint(accuracy)","16c2f180":"from sklearn.metrics import precision_score, recall_score, f1_score #measuring perfomance of model\np_score = precision_score(y_test, y_pred)\nprint('p_score',p_score)\nr_score = recall_score(y_test, y_pred)\nprint('r_score',r_score)\nf1 = f1_score(y_test, y_pred)\nprint('f1 score',f1)","cf39ddf3":"from sklearn.metrics import confusion_matrix # confusion matrix\nconfusion_matrix(y_test,y_pred)","2decc8ca":"## Final Model and Results","9a8db6e8":"## Feature Engineering","34edccca":"## Exploratory Data Analysis","f0fdff8f":"## Selecting Threshold for Classification"}}