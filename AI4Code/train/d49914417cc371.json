{"cell_type":{"88aac593":"code","97fcc644":"code","5d68c1eb":"code","5cf1fbd0":"code","f2ea8d91":"code","f179fccc":"code","18d9cecb":"code","da109d4f":"code","527f16f0":"code","2920541a":"code","825f6fda":"code","2a0a03a5":"code","59149395":"code","91f18d8b":"code","4971dd69":"code","3dd72dbe":"code","d2aea99f":"code","74395378":"code","db0ec7ed":"markdown","e4258161":"markdown","377301d9":"markdown","3841be71":"markdown","181ea4a8":"markdown","b83cbc62":"markdown","e075cf35":"markdown","e53a784c":"markdown","4344b735":"markdown","53d07cbb":"markdown","54dc5351":"markdown","bcda5fbd":"markdown"},"source":{"88aac593":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","97fcc644":"tf.__version__","5d68c1eb":"dataset = pd.read_csv('..\/input\/titanic\/train.csv')\ndataset['Age'].fillna(dataset['Age'].dropna().median(), inplace = True)\ndataset['Embarked'].fillna(dataset['Embarked'].dropna().mode()[0], inplace = True)\nX = dataset.drop(['Survived', 'Name', 'Ticket', 'Fare', 'Cabin', 'PassengerId'], axis = 1)\ny = dataset.iloc[:, 1].values","5cf1fbd0":"X","f2ea8d91":"y","f179fccc":"X['Family'] = X['SibSp'] + X['Parch'] + 1\nX.drop(['Parch', 'SibSp'], axis = 1, inplace = True)\nX","18d9cecb":"X.isnull().sum()","da109d4f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n#X['Sex'] = le.fit_transform(X['Sex'])\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), ['Embarked', 'Sex'])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","527f16f0":"X","2920541a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","825f6fda":"ann = tf.keras.models.Sequential()","2a0a03a5":"ann.add(tf.keras.layers.Dense(units=10, activation='relu'))","59149395":"ann.add(tf.keras.layers.Dense(units=10, activation='relu'))","91f18d8b":"ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","4971dd69":"ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","3dd72dbe":"ann.fit(X_train, y_train, batch_size = 32, epochs = 120)","d2aea99f":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","74395378":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","db0ec7ed":"### Predicting the results of the Test set","e4258161":"### Initializing the ANN","377301d9":"### Training the ANN model on the Training set","3841be71":"### Compiling the ANN","181ea4a8":"### Adding the output layer","b83cbc62":"### Adding the second hidden layer","e075cf35":"### Splitting the dataset into the Training set and Test set","e53a784c":"### Adding the input layer and the first hidden layer","4344b735":"### Importing the dataset","53d07cbb":"Thanks for seeing this and I would appreciate any feedbacks or methods to increase the accuracy :)","54dc5351":"### Importing the libraries","bcda5fbd":"Ps: i have only used the train.csv file over here as i wanted to try out ann"}}