{"cell_type":{"bc70eaf6":"code","53aba3fa":"code","5125f313":"code","fc08f5aa":"code","24a7be50":"code","8e363cac":"code","91002634":"code","f4e747ab":"code","33d9a5cc":"code","80904b23":"code","8802eda5":"code","2ba9ec4b":"markdown","80ede529":"markdown","0661b419":"markdown","83ed9038":"markdown","80cec24a":"markdown","a43aab3e":"markdown","fa98b4a9":"markdown","23652f33":"markdown","81ff731a":"markdown","0ed6bc43":"markdown","b38c0a1b":"markdown"},"source":{"bc70eaf6":"import seaborn as sns\nimport sys, pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","53aba3fa":"df_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\ntrain_y = list(df_train[\"label\"])\ntrain_X = np.array( df_train.drop(\"label\",axis = 1))\n\nsns.countplot(train_y)\nplt.show()\nplt.imshow(train_X[0].reshape((28,28)),cmap = \"gray_r\")","5125f313":"test_X = np.array(pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\"))","fc08f5aa":"# Normalizing is important because it prevents gradient explosions\n\ntrain_X = train_X \/ 255.0\n\ntest_X = test_X \/ 255.0","24a7be50":"def create_batches(X,y,batch_size = 16):\n    output_X = []\n    output_y = []\n    \n    \n    num_batches = int(len(X) \/ batch_size)\n    num_mod = np.mod(len(X),batch_size)\n    x = 0\n    for i in range(num_batches):\n        output_X.append(X[x:x+batch_size])\n        output_y.append(y[x:x+batch_size])\n        x = x + batch_size\n    if num_mod != 0:\n        if num_mod == 1:\n            output_X.append(X[-1])\n            output_y.append(y[-1])\n        else:\n            output_X.append([X[-(num_mod):-1][0],X[-1]])\n            output_y.append([y[-(num_mod):-1][0],y[-1]])\n        \n    return output_X,output_y\n\ntrain_X,train_y = create_batches(train_X,train_y)","8e363cac":"# Layer\nclass layer_dense:\n    def __init__(self,n_inputs,n_neurons):\n        #randomly initialize the weights an biases\n        self.weights = np.random.randn(n_inputs,n_neurons) * 0.1 # initialize it with the random numbers between 0 and 1\n        self.biases = np.zeros((1,n_neurons)) # initialize it with 0\n        self.name = \"layer_dense\"\n    def forward(self,inputs):\n        self.input = inputs # Save the input for the backpropagation algo\n        self.output = np.dot(inputs,self.weights) + self.biases # a * w + b\n        return self.output\n# Activations\nclass ReLU:\n    def __init__(self):\n        self.name = \"relu\"\n    def forward(self,inputs):\n        self.input = inputs\n        self.output = np.maximum(0,inputs) # Just looking if the number is bigger than zero\n        return self.output","91002634":"# seed it\nnp.random.seed(10)\n\n# You can simply just add things to the network by adding objects to this list\nnet = [\n    layer_dense(784,200),\n    ReLU(),\n    layer_dense(200,10),\n]","f4e747ab":"# Hyperparameters\nreg = 1e-3\nstep_size = 1e-3 # learning rate\n\nnum_samples = len(train_X[0])\n\n# Runs the network\ndef forward(inputs,net):\n    out = inputs\n    for layer in net:\n        out = layer.forward(out)\n    return out\n\ndef softmax(output):\n    # Get the probabilities with softmax\n    exp = np.exp(output)\n    probs = exp \/ np.sum(exp, axis=1, keepdims=True)\n    return probs\n\ndef backpropagation(net,dscores):\n    n = 1\n    for layer in reversed(net): # The loop starts at the end of the network\n        if layer.name == \"relu\":\n            # backpropagate the ReLU non-linearity\n            dscores[layer.output <= 0] = 0\n        else:\n            dW = 0\n            dB = 0\n            \n            # backpropate the gradient to the parameters\n            dW = np.dot(layer.input.T, dscores)\n            db = np.sum(dscores, axis=0, keepdims=True)\n            \n            # Check if it is the last layer \n            # if it isn't it will backpropagate into the next layer\n            if n != len(net):\n                dscores = np.dot(dscores, layer.weights.T)\n                \n            #Apply regularization\n            dW += reg*layer.weights\n            \n            #Apply gradients\n            layer.weights += -step_size * dW\n            layer.biases += -step_size * db\n        n += 1\n        \ndef train(epochs):\n    for i in range(epochs):\n        for i in range(len(train_X)-1):\n            \n            # Get the current batch\n            feat = train_X[i]\n            target = train_y[i]\n            \n            # Run batch trough the network\n            output = forward(feat,net)\n            \n            # Get the probabilities with softmax\n            probs = softmax(output)\n            \n            # Compute the gradient of the output\n            dscores = probs\n            dscores[range(num_samples),target] -= 1\n            dscores \/= num_samples\n            dscores[output <= 0] = 0\n\n            backpropagation(net,dscores)\n            \n            sys.stdout.write(f\"\\rTraining - {round(i \/ len(train_X) * 100)}%\")\n            sys.stdout.flush()","33d9a5cc":"# You can train for more epochs if you want\nEPOCHS = 3000\n\ntrain(EPOCHS)","80904b23":"# Predict on test data\nout = forward(test_X,net)\n# Apply sotmax and argmax\nsub = softmax(out).argmax(axis = 1)\nsns.countplot(sub)\nplt.show()\n# Show the distribution of the prediction","8802eda5":"sub_df = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\n\nsub_df[\"Label\"] = sub \n\nsub_df.to_csv(\"submission.csv\",index = False)\n\nsub_df.head()","2ba9ec4b":"### 4.2 Make a submission","80ede529":"### 1.4 Create batches","0661b419":"### 2.2 Create the nertwork","83ed9038":"### 1.2 Test data","80cec24a":"### 2.1 Create layer objects","a43aab3e":"### 1.3 Normalize the data","fa98b4a9":"# Loading the data\n### 1.1 Training data","23652f33":"# Simple NN from Scratch MNIST\n\n* **1. Loading the data**\n    * 1.1 Training data\n    * 1.2 Test data\n    * 1.3 Normalize the data\n    * 1.4 Create batches\n* **2. Neural Network**\n    * 2.1 Create layer and activation objects\n    * 2.2 Create the network\n    * 2.3 Define backpropagation function and the training loop\n* **3. Training**\n    * 3.1 Start training\n* **4. Evaluation**\n    * 4.1 Test the model    \n    * 4.2 Make a submission    \n\nThis is a 3 layer neural network with a single dense layer. It uses backpropagation and gradient descent to fit to the data. **Without NN frameworks!** ","81ff731a":"### 2.3 Define backpropagation function and the training loop","0ed6bc43":"### 3.1 Start training","b38c0a1b":"### 4.1 Test the model"}}