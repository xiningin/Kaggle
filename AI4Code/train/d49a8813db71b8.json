{"cell_type":{"e2943348":"code","836c454d":"code","79b7a778":"code","ad73d515":"code","c47d7174":"code","d5d5f8c1":"code","55a83c4f":"code","f0f9a332":"markdown"},"source":{"e2943348":"# First of all let's import basic libraries and data to get started\nimport numpy as np\nfrom numpy  import array\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris = pd.read_csv(\"..\/input\/Iris.csv\") # load data\niris.drop('Id',axis=1,inplace=True) # Id column is redundant as df already assigns index","836c454d":"\niris.info()\n\n","79b7a778":"# Next I'm using dataframe.describe function. Works for both object and numeric\niris.describe()\n\n# Means are in the same order of magnitude for all features so scaling might not be beneficial.\n# If mean values were of different orders of magnitude, scaling could significantly \n# improve accuracy of a classifier.","ad73d515":"# Although not essential for this dataset let's see if scaling provides additional insights.\n\n# First we need to split X, Y into separate sets to feed X to the scaler.\nX = iris.drop('Species',1)\nY = iris.Species\n\n# Scaling of X\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nprint('X_scaled type is',type(X_scaled))\n\n# As output of scaler is np array I'll transform back to df for easier exploration and plotting\nX_scaled_df = pd.DataFrame(X_scaled,columns=['s_SepalLength','s_SepalWidth',\n                                             's_PetalLength','s_PetalWidth'])\ndf = pd.concat([X_scaled_df,Y],axis=1)\n\n# Notice x-axis on subplots are all the same for all features (0 to 1) after scaling.\nfig = plt.figure(figsize=(12,7))\nfig.suptitle('Frequency Distribution of Features by Species ',fontsize=20)\n\nax1 = fig.add_subplot(221)\ndf.groupby(\"Species\").s_PetalLength.plot(kind='hist',alpha=0.8,legend=True,title='s_PetalLength')\nax2 = fig.add_subplot(222,sharey=ax1)\ndf.groupby(\"Species\").s_PetalWidth.plot(kind='hist',alpha=0.8,legend=True,title='s_PetalWidth')\nax3 = fig.add_subplot(223,sharey=ax1)\ndf.groupby(\"Species\").s_SepalLength.plot(kind='hist',alpha=0.8,legend=True,title='s_SepalLength')\nax4 = fig.add_subplot(224,sharey=ax1)\ndf.groupby(\"Species\").s_SepalWidth.plot(kind='hist',alpha=0.8,legend=True,title='s_SepalWidth')\n\nplt.tight_layout(pad=4, w_pad=1, h_pad=1.5)\nplt.show()\nX_scaled_df.describe()","c47d7174":"# Another useful visualization I like to use during exploration is a 2D PCA plot.\n# It can quickly indicate how easy or difficult the classification problem is.\n# This is particularly relevant for high-dimensional datasets.\n\nfrom sklearn.decomposition import PCA\n# spliting input and target\nX = iris.drop('Species',1)\nY = iris.Species\n\npca = PCA(n_components=2).fit_transform(X) # pca output is an array\npca_df = pd.DataFrame(pca,columns=['pca_1','pca_2']) # transforming back to df\npca_Y = pd.concat([pca_df, Y],axis=1)\n\n# The 3 species cluster nicely which is a good indication a classifier can be trained \n# at high accuracy. \n# It is also expected that accuracy of identifying setosa will be higher.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.FacetGrid(pca_Y, hue=\"Species\", palette=\"Set1\", size=6).map(plt.scatter, \"pca_1\", \"pca_2\").add_legend()\nplt.show()","d5d5f8c1":"# Let's now build a classifier and evaluate accuracy\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 0)\nprint(\"train sample size\",x_train.shape,type(x_train))\nprint(\"test sample size\",x_test.shape,type(x_test))","55a83c4f":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\nclf = SVC(kernel = 'linear').fit(x_train,y_train)\nclf.predict(x_train)\ny_pred = clf.predict(x_test)\n\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['setosa','versicolor','virginica'], \n                     columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n","f0f9a332":"In this example the model overall accuracy on the test set was 0.978 (Accurate predictions\/All or True Positives\/All).\n \nHowever, the confusion matrix gives additional insight into accuracy by class and intuition for precision and recall efficiency.\n \nAn insight we can get from the matrix is that the model was very accurate at classifying setosa and versicolor (True Positive\/All = 1.0). However, accuracy for virginica was lower (11\/12 = 0.917).\n\nIf for any reason, successful classification of virginica was particularly desired to the use case (e.g. it's a poisonous plant), then the confusion matrix will help highlight differences between the classes. \n\nsklearn.metrics module has many functions that calculate evaluation scores like precision and recall. In this example I wanted to show the intuition for Machine Learning model evaluation beyond a simple assessment of overall test\/pred accuracy.\n\n**Please leave comments and upvote if you liked this Notebook.**\n\nDiego Schapira"}}