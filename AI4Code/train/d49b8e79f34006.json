{"cell_type":{"e6824e1e":"code","fc69312c":"code","05500e50":"code","e0d3477b":"code","65463c22":"code","191aae0c":"code","14ff5491":"code","9df46971":"code","7aa80dd9":"code","5c842605":"code","9ae8cd0c":"code","7a93606a":"code","ff201a46":"code","01b75a06":"code","6a486800":"code","b4f33aa1":"markdown","d613ee4e":"markdown","72ea0584":"markdown","a15766f2":"markdown","30f4e83e":"markdown","71ba15fe":"markdown","8198f0c4":"markdown","f4556474":"markdown","e4994178":"markdown"},"source":{"e6824e1e":"# clone from the official repo\n!git clone 'https:\/\/github.com\/hadarishav\/Ruddit.git'","fc69312c":"# what files are available?\n!ls .\/Ruddit\/Dataset\/","05500e50":"# install Python Reddit API Wrapper\n!pip install praw","e0d3477b":"# import necessary modules and APIs\nimport numpy as np \nimport pandas as pd \nfrom time import time\nimport praw\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom kaggle_secrets import UserSecretsClient","65463c22":"# read the ruddit data file\nruddit = pd.read_csv('.\/Ruddit\/Dataset\/Ruddit.csv')\nprint(ruddit.shape)\nruddit.head()","191aae0c":"# we need a list of post_id to extract comments\nposts = ruddit.post_id.unique()\n# number of unique post_id\nlen(posts)","14ff5491":"# create a dictionary with POST_ID as key and an array of COMMENT_IDs as values\n# an optimized method, utilizes pandas groupby groups attribute\nr = ruddit.groupby('post_id')[['post_id', 'comment_id']]\npairs = dict()\nfor j in r.groups:\n    pairs[j] = ruddit['comment_id'].iloc[r.groups[j]].to_numpy()","9df46971":"# do a random check\n# what comment_ids are there for a given post_id?\npairs['3vdy9k']","7aa80dd9":"# generate separate columns for extracted texts and their URLs\nruddit['txt'] = np.nan\nruddit['url'] = np.nan\nruddit.head()","5c842605":"# credentials, keep them secret\n# to develop your own, please follow PRAW docs and Reddit API\nuser_secrets = UserSecretsClient()\n# save and retrieve secrets using kaggle_secrets\nsecret_value_0 = user_secrets.get_secret(\"CLIENT_AGENT\")\nsecret_value_1 = user_secrets.get_secret(\"CLIENT_ID\")\nsecret_value_2 = user_secrets.get_secret(\"CLIENT_SECRET\")","9ae8cd0c":"# create a reddit crawler\nreddit = praw.Reddit(\n    user_agent= secret_value_0,\n    client_id=secret_value_1,\n    client_secret=secret_value_2\n)","7a93606a":"# collect post ids which lead to errors, like forbidden 403\nissue_posts = []\n# iterate over all post ids\nfor p in tqdm(posts[:10], desc='overall progress'): \n    # process 10 posts for demo\n    now = time()\n    try:\n        # create a submission to Reddit API\n        submission = reddit.submission(id=p)\n        # read the URL\n        URL = submission.url\n        # flatten the comment tree\n        submission.comments.replace_more(limit=None)\n    except Exception as e:\n        # if there is an error making submission\n        issue_posts.append((p, e))\n        continue\n    delta = int(time()-now)\n    # let's know the time taken for each submission\n    desc = str(p)+' '+str(delta)+' sec'\n    # iterate over actual comment ids \n    for c in tqdm(submission.comments.list(), desc=desc):\n        # iff our data contains that id\n        if c in pairs[p]:\n            # locate in our data\n            index = ruddit[ruddit['comment_id']== str(c)].index\n            # replace our data\n            ruddit.loc[index,['txt','url']] = [c.body,URL+'\/'+str(c)+'\/']","ff201a46":"# drop rows where we don't have texts\n# reorder columns for elegance\nruddit_1 = ruddit.dropna(axis=0,inplace=False)\\\n[['post_id','comment_id','txt','url','offensiveness_score']]\nruddit_1.head()","01b75a06":"# if there are problematic post_ids, \n# publish them into a CSV file\nif len(issue_posts):\n    print(len(issue_posts))\n    issue_1 = pd.DataFrame(data=issue_posts, columns=['post_id', 'error_msg'])\n    issue_1.to_csv('post_with_issue_1.csv', index=False)\n    print(issue_1.head())","6a486800":"# publish our extracted text data\nruddit_1.to_csv('ruddit_with_text_1.csv',index=False) ","b4f33aa1":"# Install PRAW library and create the environment","d613ee4e":"# Publish CSV file","72ea0584":"# Read data","a15766f2":"`comment_id` is the Ruddit Comment ID, `post_id` is the Ruddit parent Post ID, `offensiveness_score` is the score value calculated by authors of the original paper cited above (target value, in case of model fine-tuning) \n\nWhile reading the `Thread_structure.txt` file (available in the Dataset directory), it can be understood that for a given `post_id`, there are one or more `comment_id`. Hence we need to know the unique post ids and their corresponding comment ids.","30f4e83e":"### Find the complete Dataset **[here](https:\/\/www.kaggle.com\/rajkumarl\/ruddit-jigsaw-dataset)**\n\n#### Thank you for your time!","71ba15fe":"# Clone the Repo","8198f0c4":"# Ruddit  - A Supporting Dataset for Jigsaw Comments Severity Rating\n\nThis dataset contains offensive comments from Reddit and offensiveness score corresponding to each comment. \nUnfortunately, the original dataset does not contain the 'text' that we need for modelling. Rather it provides the post ids and comment ids so that one can extract those comments. This notebook uses **[PRAW](https:\/\/praw.readthedocs.io\/en\/stable\/)**, a python library to extract comments from Reddit via **[Reddit API](https:\/\/www.reddit.com\/wiki\/api)**.\n\nPlease find the Original Paper **[Ruddit: Norms of Offensiveness for English Reddit Comments](https:\/\/aclanthology.org\/2021.acl-long.210\/)**\n\nPlease find the official repo [here](https:\/\/github.com\/hadarishav\/Ruddit)\n\nAcknowledgement:\n\n` @inproceedings{hada-etal-2021-ruddit,\n    title = \"Ruddit: {N}orms of Offensiveness for {E}nglish {R}eddit Comments\",\n    author = \"Hada, Rishav  and\n      Sudhir, Sohi  and\n      Mishra, Pushkar  and\n      Yannakoudakis, Helen  and\n      Mohammad, Saif M.  and\n      Shutova, Ekaterina\",\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https:\/\/aclanthology.org\/2021.acl-long.210\",\n    doi = \"10.18653\/v1\/2021.acl-long.210\",\n    pages = \"2700--2717\",\n    abstract = \"On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best{--}Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.\",\n}\n`\n","f4556474":"# Extract texts","e4994178":"![Python Reddit Scraping](https:\/\/d1y2qj23ol72q6.cloudfront.net\/2021\/01\/Python-Reddit-Banner-2.jpg)\n> *[Image Source](https:\/\/d1y2qj23ol72q6.cloudfront.net\/2021\/01\/Python-Reddit-Banner-2.jpg)*"}}