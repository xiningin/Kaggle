{"cell_type":{"a030a19d":"code","23deda57":"code","976b3e8c":"code","8597d0bf":"code","6d4225c9":"code","098ad2ef":"code","4d1cb2f5":"code","130b122e":"code","1b1cca64":"code","7ee57a2f":"code","1bdfe9a4":"code","9e07829b":"code","8cd94220":"code","4095a164":"code","4d8c4153":"code","ce6191c8":"code","e4635efb":"code","0d6f204f":"code","d4fee09d":"code","b21a71ca":"code","7aa1a855":"code","d520d2e6":"code","55bd6292":"code","42f7ac52":"code","7d683a7d":"code","d98be37d":"code","3256c744":"code","2078f514":"code","d6582091":"markdown","df87ed88":"markdown","5b66372d":"markdown","658be2a8":"markdown","4d812420":"markdown","ddf2f704":"markdown","e0143bb5":"markdown","3c5782fd":"markdown","96b20915":"markdown","1be91f3d":"markdown","e23b06d6":"markdown","12e29309":"markdown","7a9d7ccc":"markdown","38d18e40":"markdown","539b79ce":"markdown","06b5f9c0":"markdown","cf5a834d":"markdown","68a0191b":"markdown","9bfcb9aa":"markdown","31a09111":"markdown","0983e9b1":"markdown","f8960a2f":"markdown","75290155":"markdown","33ac7157":"markdown","4db8ac67":"markdown","ad9a9a08":"markdown","a8a1f3df":"markdown","e95586c3":"markdown","b5d79c12":"markdown","2429ad18":"markdown","ede93048":"markdown"},"source":{"a030a19d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom datetime import datetime\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","23deda57":"dataRead=pd.read_csv(r'..\/input\/walmart-dataretail-analysis\/Walmart_Store_sales.csv')","976b3e8c":"print(\"The shape of this dataset in (rows,cols) : \",dataRead.shape)","8597d0bf":"for i,name in enumerate(dataRead):\n    print(\"Column \",i,\"'s name: \",name)","6d4225c9":"print(\"Overview of the data: \",dataRead.head(5))","098ad2ef":"UniqueStoreNum=dataRead[\"Store\"].unique()\nprint(\"The unique store numbers are: \\n\",UniqueStoreNum)\n","4d1cb2f5":"maxSalesAggregation=dataRead.groupby(\"Store\").sum()[\"Weekly_Sales\"] \nmaxStoreSales=[[\"Store Number with maximum sales\",maxSalesAggregation.idxmax()],[\"Max Sales                      \",maxSalesAggregation.max()]]\nfor i in maxStoreSales:\n    print(i)","130b122e":"orderedSales=maxSalesAggregation.astype(int).sort_values().reset_index()\nprint(\"Store number with the sales ordered from least to most: \\n\",orderedSales)","1b1cca64":"plt.xlabel(\"Non-store specific numbers\")\nplt.ylabel(\"Increasing store sales\")\nplt.title(\"Increasing store sales\")\nplt.plot(orderedSales[\"Weekly_Sales\"])\n","7ee57a2f":"def computeStdDeviation(df,ddofVal,storeNum):\n    df_stdDev=df.std(ddof=1)\n    maxStdDev=[[\"Store\",storeNum],[\"Std Deviation\",df_stdDev[\"Weekly_Sales\"]]]\n    return maxStdDev\ndef computeMean(df,storeNum):\n    df_mean=df.mean(numeric_only=True)\n    average=[[\"Store\",storeNum],[\"Mean\/Average\",df_mean[\"Weekly_Sales\"]]]\n    return average\n","1bdfe9a4":"#Dictionaries to hold standard deviation and the mean\nStdDeviationsDict={}\nmeanDict={}\n\nfor storeNum in range(1,46):\n    #Select Store\n    df=dataRead[(dataRead[\"Store\"]==(storeNum))]   \n    \n    #Calculate store specific std. deviation\n    ddofVal=1\n    maxStdDev=computeStdDeviation(df,ddofVal,storeNum)\n    StdDeviationsDict[maxStdDev[0][1]]=maxStdDev[1][1]\n    \n    #Calculate store specific mean\/average\n    meanVal=computeMean(df,storeNum)\n    meanDict[meanVal[0][1]]=meanVal[1][1]\n\nmax_key = max(StdDeviationsDict, key=StdDeviationsDict.get)\nprint(\"Maximum standard deviation is: \")\nprint(\"Store number                  : \",max_key)\nprint(\"Has a sample std. deviation of: \",StdDeviationsDict[max_key])\n","9e07829b":"for storeNum in range(1,46):\n    orderedSales.loc[orderedSales.Store==storeNum,\"Standard Deviation\"]=StdDeviationsDict[storeNum]\n    orderedSales.loc[orderedSales.Store==storeNum,\"Mean\"]=int(meanDict[storeNum])\norderedSales=orderedSales.set_index(\"Store\")\nprint(orderedSales)\nplt.xlabel(\"Total Store Sales\")\nplt.ylabel(\"Standard Deviation\")\nplt.title(\"Standard Deviation of Sales against Total Sales for 2010-2012\")\nplt.plot(orderedSales[\"Weekly_Sales\"],orderedSales[\"Standard Deviation\"])\n","8cd94220":"for storeNum in range(1,46):\n    orderedSales[\"Coefficient of Variation (%)\"]=(orderedSales[\"Standard Deviation\"]\/orderedSales[\"Mean\"])*100\nprint(orderedSales)\nplt.xlabel(\"Total Store Sales\")\nplt.ylabel(\"Coefficient of Variation (%)\")\nplt.title(\"Coefficient of Variation (%) against Total Sales for 2010-2012\")\nplt.plot(orderedSales[\"Weekly_Sales\"],orderedSales[\"Coefficient of Variation (%)\"])\n","4095a164":"\ndataRead['Date']=pd.to_datetime(dataRead['Date'])\ndef yearQuartersSalesPerStore(dataRead,year):    \n    quarter_2_start = datetime.strptime('01\/04\/'+str(year-2000), '%d\/%m\/%y')\n    quarter_2_end = datetime.strptime('30\/06\/'+str(year-2000), '%d\/%m\/%y')\n    quarter_3_start = datetime.strptime('01\/07\/'+str(year-2000), '%d\/%m\/%y')\n    quarter_3_end =  datetime.strptime('30\/09\/'+str(year-2000), '%d\/%m\/%y')        \n    df_q2 = dataRead.loc[(dataRead['Date'] >= quarter_2_start)\n                         & (dataRead['Date'] < quarter_2_end)]\n    df_q3 = dataRead.loc[(dataRead['Date'] >= quarter_3_start)\n                         & (dataRead['Date'] < quarter_3_end)]\n    q2_StoreSales=pd.DataFrame(df_q2.groupby('Store')['Weekly_Sales'].sum())\n    q3_StoreSales=pd.DataFrame(df_q3.groupby('Store')['Weekly_Sales'].sum())\n    q2_StoreSales.reset_index(inplace=True)\n    q3_StoreSales.reset_index(inplace=True)\n    q2_StoreSales.rename(columns={'Weekly_Sales': 'Q2_Sales_'+str(year)},inplace=True)\n    q3_StoreSales.rename(columns={'Weekly_Sales': 'Q3_Sales_'+str(year)},inplace=True)\n    df_yearComparison=q2_StoreSales.merge(q3_StoreSales,how='inner',on='Store')\n    return df_yearComparison","4d8c4153":"salesByYear={}\nfor year in range(dataRead['Date'].min().year,dataRead['Date'].max().year+1):\n    df_yearComparison=yearQuartersSalesPerStore(dataRead,year)\n    salesByYear[year]=df_yearComparison\n    \ndf_salesComparison=salesByYear[2010].merge(salesByYear[2011],how='inner',on='Store')   \ndf_salesComparison=df_salesComparison.merge(salesByYear[2012],how='inner',on='Store')\n","ce6191c8":"for year in range(dataRead['Date'].min().year,dataRead['Date'].max().year+1):\n    df_salesComparison['Quarterly_Growth_Rate_'+str(year)]=((df_salesComparison['Q3_Sales_'+str(year)]-df_salesComparison['Q2_Sales_'+str(year)])\/df_salesComparison['Q2_Sales_'+str(year)])*100\n#df_salesComparison[['Store','Quarterly_Growth_Rate_2012']]\ndf_past=((df_salesComparison['Quarterly_Growth_Rate_2010']+df_salesComparison['Quarterly_Growth_Rate_2011'])\/2)\ndf_present=df_salesComparison['Quarterly_Growth_Rate_2012']\ndf_salesComparison['Difference_in_Growth_Rate']=((df_present-df_past))","e4635efb":"\nstores_GrowthRate_Max_to_Least=df_salesComparison.sort_values('Difference_in_Growth_Rate',ascending=False)['Store'].tolist()\nprint(\"Store's ordered from the best to the worst growth rate:\\n\",stores_GrowthRate_Max_to_Least)","0d6f204f":"plt.style.use('classic')\nstoreEnd=9\nfor index,storeNum in enumerate(stores_GrowthRate_Max_to_Least[:storeEnd]):\n    df=df_salesComparison[df_salesComparison['Store']==storeNum][['Q2_Sales_2010','Q3_Sales_2010','Q2_Sales_2011','Q3_Sales_2011','Q2_Sales_2012','Q3_Sales_2012']]\n    plt.subplot(3,3,index+1)\n    plt.title(\"Store \"+str(storeNum))\n    plt.bar(\"10Q2\",df['Q2_Sales_2010'],color='blue')\n    plt.bar(\"10Q3\",df['Q3_Sales_2010'],color='blue')\n    plt.bar(\"11Q2\",df['Q2_Sales_2011'],color='blue')\n    plt.bar(\"11Q3\",df['Q3_Sales_2011'],color='blue')\n    plt.bar(\"12Q2\",df['Q2_Sales_2012'],color='blue')\n    plt.bar(\"12Q3\",df['Q3_Sales_2012'],color='blue')\n    plt.xticks(fontsize=7)\nplt.tight_layout()","d4fee09d":"plt.style.use('classic')\nstoreStart=36\nstoreEnd=45\nfor index,storeNum in enumerate(stores_GrowthRate_Max_to_Least[storeStart:storeEnd]):\n    df=df_salesComparison[df_salesComparison['Store']==storeNum][['Q2_Sales_2010','Q3_Sales_2010','Q2_Sales_2011','Q3_Sales_2011','Q2_Sales_2012','Q3_Sales_2012']]\n    plt.subplot(3,3,index+1)\n    plt.title(\"Store \"+str(storeNum))\n    plt.bar(\"10Q2\",df['Q2_Sales_2010'],color='blue')\n    plt.bar(\"10Q3\",df['Q3_Sales_2010'],color='blue')\n    plt.bar(\"11Q2\",df['Q2_Sales_2011'],color='blue')\n    plt.bar(\"11Q3\",df['Q3_Sales_2011'],color='blue')\n    plt.bar(\"12Q2\",df['Q2_Sales_2012'],color='blue')\n    plt.bar(\"12Q3\",df['Q3_Sales_2012'],color='blue')\n    plt.xticks(fontsize=7)\nplt.tight_layout()","b21a71ca":"BestImprovement['Difference_in_Growth_Rate'].iloc[0]","7aa1a855":"print(\"Best improvement in growth rate of Q2\/Q3 from 2010 and 2011 compared to Q2\/Q3 in 2012:\")\nBestImprovement=df_salesComparison[df_salesComparison['Difference_in_Growth_Rate']==df_salesComparison['Difference_in_Growth_Rate'].max()]\nprint(\"Store Number: \",BestImprovement['Store'].iloc[0])\nprint(\"Difference in Growth rate: \",BestImprovement['Difference_in_Growth_Rate'].iloc[0])\n\nprint(\"Worst improvement in growth rate:\")\nWorstImprovement=df_salesComparison[df_salesComparison['Difference_in_Growth_Rate']==df_salesComparison['Difference_in_Growth_Rate'].min()]\nprint(\"Store Number: \",WorstImprovement['Store'].iloc[0])\nprint(\"Difference in Growth rate: \",WorstImprovement['Difference_in_Growth_Rate'].iloc[0])","d520d2e6":"df_holiday=dataRead[dataRead['Holiday_Flag']==1]\ndf_notHoliday=dataRead[dataRead['Holiday_Flag']!=1]\nnumberOfStores=45\nnumberOfYears=3\nprint(\"The number of non-holiday weeks over the three year period:          \",df_notHoliday.shape[0])\nprint(\"The number of weeks of holiday over the three year period:           \",df_holiday.shape[0])\nprint(\"The number of weeks of holiday per year:                             \",int(df_holiday.shape[0]\/numberOfYears))\nprint(\"\")\nprint(\"The number of weeks per store over the three year period                \",int(dataRead.shape[0]\/numberOfStores))\nprint(\"The number of weeks of non-holiday per store over the three year period \",int(df_notHoliday.shape[0]\/numberOfStores))\nprint(\"The number of weeks of holiday per store over the three year period:    \",int(df_holiday.shape[0]\/numberOfStores))\n","55bd6292":"import datetime as dt\ndf_holiday2=df_holiday['Date'].sort_values().unique()\ndf_holiday3 = pd.DataFrame(df_holiday2,columns=['Date'])\ndf_holiday3['Year']=df_holiday3['Date'].dt.year\ndf_holiday3['Quarters']=df_holiday3['Date'].dt.quarter\nprint(df_holiday3)","42f7ac52":"totalSales=df_holiday.groupby('Date')['Weekly_Sales'].sum().reset_index()\ntotalSales=totalSales.rename(columns={\"Weekly_Sales\":\"Holiday_WeeklySales\"})\ndf_finalHoliday=pd.merge(totalSales,df_holiday3,how='inner',on='Date')\ndf_finalHoliday.rename(columns={'Date':'Holi_Date'})\ndf_finalHoliday=df_finalHoliday[['Date','Year','Quarters','Holiday_WeeklySales']]\nprint(df_finalHoliday)","7d683a7d":"df_notHoliday2=df_notHoliday.groupby('Date')['Weekly_Sales'].sum()\ndf_notHoliday3=df_notHoliday2.reset_index()\ndf_notHoliday3['Quarters']=df_notHoliday3['Date'].dt.quarter\ndf_notHoliday3['Year']=df_notHoliday3['Date'].dt.year\ndf_notHoliday4=df_notHoliday3.groupby(['Year','Quarters']).mean()\ndf_notHoliday4=df_notHoliday4.rename(columns={'Weekly_Sales':'NonHoliday_WeeklySales'})\nprint(df_notHoliday4['NonHoliday_WeeklySales'].astype(int))\ndf_notHoliday4=df_notHoliday4.reset_index()\n# df_notHoliday4['NonHoliday_WeeklySales'].astype(int)\n","d98be37d":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\ndf_finalHoliday=pd.merge(df_finalHoliday,df_notHoliday4,how='inner',on=['Year','Quarters'])\ndf_finalHoliday=df_finalHoliday[['Date','Year','Quarters','Holiday_WeeklySales','NonHoliday_WeeklySales']]\nprint(df_finalHoliday)","3256c744":"df_finalHoliday['Holiday_Sales_Difference']=df_finalHoliday['Holiday_WeeklySales']-df_finalHoliday['NonHoliday_WeeklySales']\nprint(\"Remembering that these are dates relating to weeks (i.e. the dates represent Fridays)\")\nprint(df_finalHoliday)","2078f514":"\nprint(\"The next step is in sorting the results from those holidays that have the best sales to the holidays that do not.\")\ndf_finalHoliday.sort_values('Holiday_Sales_Difference',ascending=False).reset_index().drop(['index'],axis=1)","d6582091":"### **Solution 4.1 - Holidays with higher sales than the mean sales during non-holiday season - considering all stores together**\nTo make a more accurate analysis of the impact of holidays on sales, a comparison will be undertaken on a quarterly basis. As an example, since there are four holidays during the 2010 year, and all four of these days fall in the fourth quarter of the year, the comparison of sales will be done with only Q4 of 2010 in consideration. ","df87ed88":"## Step (1) **Import libraries**","5b66372d":"To ensure we use the correct ddof (either 0 or 1), we need to consider if we are working with a sample or the population dataset. Since we are looking at 45 stores over a two year period from 2010 to 2012, we are generalising to a two year period. Since we are finding the standard deviation of a specific period of time and not an exhaustive time series view, we will use the sample standard deviation to compute the result.","658be2a8":"The reason we use standard deviation is in how it tells us the variability of the distribution from the mean.\n\nYou can see this by the way in which the numerator sums up the squared difference between each point and the mean. In the case of the population standard deviation, we then divide by the divisor which is the population size:  \n* N\n\nIn the case of the sample standard deviation, we divide by one less than the sample size:\n* n-1 \n\nThe reason that we compute the sample standard deviation with one less than the sample size lies in the fact that to get a **true unbiased estimator**, the variance is a lot closer to what the variance would be when using the population mean. Sal from Khan Academy explains this with an example quite well in this video on [Why we divide by n - 1 in variance](https:\/\/www.khanacademy.org\/math\/ap-statistics\/summarizing-quantitative-data-ap\/more-standard-deviation\/v\/another-simulation-giving-evidence-that-n-1-gives-us-an-unbiased-estimate-of-variance) \n\nThe **final step** in calculating the standard deviation is in computing the square root of the result. This is done as a means of cancelling the affects of the earlier step of squaring the distribution. ","4d812420":"We can also define the **sample standard deviation** as:","ddf2f704":"### Question 2.1","e0143bb5":"The **orderedSales** dataframe was used in question 1 and holds the total sales for each store over the duration of the dataset (i.e. 2010-2012). <br\/>\nThis dataframe was already sorted from smallest to greatest sales in **question 1**, thus it will not be necessary to sort the values from smallest to largest when plotting the result. <br\/>\nConsequently, we will create a new column in the **orderedSales** dataframe and populate it with the standard deviation results for each store number.","3c5782fd":"**The store with the maximum standard deviation is therefore:**","96b20915":"It would seem that the lower the store's sales, the more fluctuation we see in the the variability between the mean and the standard deviation.  ","1be91f3d":"### **Solution 2.2 - finding the coefficient of variation**\nNow we will compute the **coefficient of mean to standard deviation** for each store:","e23b06d6":"We now create a dataframe which informs us of which yearly quarters the **holidays** fall in. This is done by finding all unique holiday dates and then sorting this in ascending order.","12e29309":"The two main holiday events that sit on opposite extremes are that of:\n1. Christmas\n2. Thanksgiving <br\/>\nFor Christmas we can see that it consistently ranks lowest (at index=7 and index=9)<br\/>\nWhilst for Thanksgiving we can see sales consistently ranking highest of all holidays (at index=0 and index=1)\nThe reason Thanksgiving week ranks so highly is most likely due to Black Friday always being held the day after Thanksgiving (on a Thursday).","7a9d7ccc":"The nine stores with the **best growth rate** performance are shown next. <br\/>\nBest to worst are located as such:\n* Best (top left)\n* Worst (bottom right)","38d18e40":"**Final step will be to:**\n1. Sum the sales for all stores on non-holidays\n2. Filter to the quarter in which the holiday occurs\n3. Average across number of weeks of sales data in that quarter\n4. Merge for the final comparison into the dataframe \"df_finalHoliday\"","539b79ce":"Finding the sales by holiday and summing across all stores. ","06b5f9c0":"Since we are trying to find holidays with higher sales than non-holidays, we will:\n* **subtract** the **NonHoliday_WeeklySales** column from the **Holiday_WeeklySales** column.","cf5a834d":"# Walmart Data-Retail Analysis\n## Basic statistic tasks from this [homepage](https:\/\/www.kaggle.com\/vik2012kvs\/walmart-dataretail-analysis)\n","68a0191b":"## Question 1: \n**Which store has maximum sales**\n\nThe investigation looks at: \n* Finding the store with the maximum sales\n* Viewing store number against increasing sales for all stores\n* Plotting increasing sales","9bfcb9aa":"**Reading in the Walmart dataset and getting an overview of its contents:**\n* Row and column number\n* Column names\n* first 5 rows","31a09111":"See this article for some examples of when to use the sample or population in computing the standard deviation ([here](https:\/\/statistics.laerd.com\/statistical-guides\/measures-of-spread-standard-deviation.php))","0983e9b1":"### Definition: Standard Deviation \n\nThe standard deviation shows us the **spread of the distribution**.<br \/>\n\n\n\nMathematically, we define the **population standard deviation** as:","f8960a2f":"## Question 2: \n**Which store has maximum standard deviation i.e., the sales vary a lot. Also, find out the coefficient of mean to standard deviation**\n\nThis question has been split into two parts:\n1. Finding the standard deviation \n2. Finding the coefficient of mean to standard deviation","75290155":"We will be using the Python library numpy to find the standard deviation. We have the ability to modify the function parameter called **ddof**. This is the **delta degree of freedom**. For documenation on what this means, here is a link to the Python documentation for [numpy.std (take a look at the Notes section on this page)](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.std.html).\n","33ac7157":"### **Solution 2.1 - finding the standard deviation**\nNow we will compute the standard deviation of each store's sales for 2010-2012.","4db8ac67":"![savemehere](http:\/\/1.bp.blogspot.com\/-Thl3QpEYySM\/U8vr2tTrNXI\/AAAAAAAD8a0\/gTybvoxm0jQ\/s1600\/Sample+std+dev.png)\n\nWhere:\n* Standard deviation: $\\sigma$\n* Sample mean: <span style=\"text-decoration: overline\">x<\/span>\n* Sample point: x<sub>i<\/sub>\n* Sample size: n","ad9a9a08":"\n![](https:\/\/www.gstatic.com\/education\/formulas2\/355397047\/en\/population_standard_deviation.svg)\n\nWhere:\n* Standard deviation: $\\sigma$\n* Population mean: $\\mu$\n* Sample point: x<sub>i<\/sub>\n* Population size: N\n\nAs a Side note: when we calculate everything within the square root, we are actually calculating the\n* Variance: $\\sigma^2$.","a8a1f3df":"An interesting insight would be in considering the spread of the distribution of store sales against total store sales. <br\/>\n**(i.e. standard deviation against store sales)**","e95586c3":"Best improvements in growth rate of Q2\/Q3 from 2010 and 2011 compared to Q2\/Q3 in 2012:","b5d79c12":"We can see that the **standard deviation** or spread of the distribution **increases** as the total **store's sales increase**.","2429ad18":"### **Solution 3.1 - Which store has good quarterly growth during 2012's quarter 3**\nAn important consideration when looking at quarterly growth is not just of the present year, but in making a comparison on previous years. \n\nThis analysis will look at which store has made the biggest quarterly growth rate improvement with only 2012 in consideration, but also in considering the biggest growth rate improvement relative to the years 2010 and 2011. ","ede93048":"The nine stores with the **worst growth rate** performance are now shown:"}}