{"cell_type":{"a6ca39c0":"code","c6e3a0a3":"code","9d908702":"code","5ce421a5":"code","d5fdec74":"code","1b0c1b87":"code","3d6d75ce":"code","c0c6ccda":"code","e0a60b5f":"code","1a6fca2a":"code","28971003":"code","e15f1781":"code","b9b01885":"code","c878d481":"code","b321e7f1":"code","cfda26f8":"code","881324e1":"code","2b0280a0":"code","40fa5a5a":"code","0ba584ea":"code","d7f0d161":"code","b9fab8e6":"code","3c9f5e22":"code","2e12a711":"code","1c3e26df":"code","e2b6e89f":"code","33298b98":"code","717a4ee5":"code","0c6a5a24":"code","106b515d":"code","2d60ae94":"code","5a14acf9":"code","12e24be6":"code","21f961a7":"code","97ad43be":"code","aebaaf14":"code","c5a5b611":"code","36e99942":"code","cd90bf9e":"code","6de62128":"code","9ee0a70c":"code","012f6980":"code","302f6155":"code","ef2cbe7f":"code","1929b1ed":"code","2f4f0bfc":"code","e8b40c10":"code","1dea1c94":"code","ec5a71d8":"code","14ab4bf2":"code","9608933c":"code","adc10bed":"code","2b09da70":"code","aab4d847":"code","9f4aa809":"code","e287f214":"code","d0e684e9":"code","129cb599":"markdown","0ea008b3":"markdown","abf976b2":"markdown","27f8ed41":"markdown","d239745b":"markdown","0af2a387":"markdown","4c26074c":"markdown","8e77111c":"markdown","722e44c6":"markdown","02f300d3":"markdown","088ea0ea":"markdown","81f79958":"markdown","0ec59b82":"markdown","4a6209ef":"markdown","c2b33eab":"markdown","a3aa17c4":"markdown","f79004df":"markdown","74589d88":"markdown","2a1baa4d":"markdown","07ab49a6":"markdown","dcefff89":"markdown","a77571fe":"markdown","33d385d6":"markdown","05d4b448":"markdown","209648f2":"markdown","243b3d5f":"markdown","d03af0b3":"markdown","760ecd62":"markdown","b46c41a0":"markdown","c03dd872":"markdown","4a738973":"markdown","8fe1ed13":"markdown","12159d7d":"markdown","722ebc44":"markdown","1d135040":"markdown","a5db599f":"markdown","c620476b":"markdown","585bc25d":"markdown","6e2c4e29":"markdown"},"source":{"a6ca39c0":"!pip install ..\/input\/kerasapplications\/keras-team-keras-applications-3b180cb -f .\/ --no-index\n!pip install ..\/input\/efficientnet\/efficientnet-1.1.0\/ -f .\/ --no-index\n\nimport efficientnet.tfkeras as efn","c6e3a0a3":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, train_test_split \nimport tensorflow as tf\nfrom tensorflow.keras import Model, backend\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.utils import Sequence\nfrom keras.utils.vis_utils import plot_model\n\nimport pydicom\nimport cv2\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(42)\n\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\n\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\/\"","9d908702":"train = pd.read_csv(ROOT + \"train.csv\")\ntest = pd.read_csv(ROOT + \"test.csv\")\nsub = pd.read_csv(ROOT + \"sample_submission.csv\")\n\nprint(\"Training data shape: \", train.shape)\nprint(\"Test data shape: \", test.shape)\n\ntrain.head(10)","5ce421a5":"train.isnull().sum()","d5fdec74":"test.isnull().sum()","1b0c1b87":"dupRows_train = train[train.duplicated(subset=['Patient', 'Weeks'], keep=False)]\n\nprint(\"There are {} duplicate rows here ({:.2f} percent of the total).\".format(len(dupRows_train), len(dupRows_train)\/len(train)*100))\ndupRows_train","3d6d75ce":"train.drop_duplicates(subset=['Patient', 'Weeks'], keep=False, inplace=True)","c0c6ccda":"stats = []\nfor col in train.columns:\n    stats.append((col,\n                  train[col].nunique(),\n                  train[col].value_counts().index[0],\n                  train[col].value_counts().values[0],\n                  train[col].isnull().sum() * 100 \/ train.shape[0],\n                  train[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n                  train[col].dtype))\nstats_df = pd.DataFrame(stats, columns=['Feature', 'Unique values', 'Most frequent item', 'Freuquence of most frequent item', 'Percentage of missing values', 'Percentage of values in the biggest category', 'Type'])\nstats_df.sort_values('Percentage of missing values', ascending=False)","e0a60b5f":"data = train.groupby(\"Patient\").first().reset_index(drop=True)\ndata.head()","1a6fca2a":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(16, 12))\n\nsns.distplot(data[\"Age\"], ax=ax1, bins=data[\"Age\"].max()-data[\"Age\"].min()+1, color=palette_ro[1])\nax1.annotate(\"Min: {:,}\".format(data[\"Age\"].min()), xy=(data[\"Age\"].min(), 0.005), \n             xytext=(data[\"Age\"].min()-8, 0.02),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(data[\"Age\"].max()), xy=(data[\"Age\"].max(), 0.005), \n             xytext=(data[\"Age\"].max()-2, 0.02),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.axvline(x=data[\"Age\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Med: {:.0f}\".format(data[\"Age\"].median()), xy=(data[\"Age\"].median(), 0.056), \n             xytext=(data[\"Age\"].median()-15, 0.065),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nsns.countplot(x=\"Sex\", ax=ax2, data=data, palette=palette_ro[-2::-4])\nsns.countplot(x=\"SmokingStatus\", ax=ax3, data=data,\n              order=[\"Never smoked\", \"Ex-smoker\", \"Currently smokes\"], palette=palette_ro[-3::-2])\n\nsns.distplot(data[data[\"Sex\"]==\"Male\"].Age, label=\"Male\", ax=ax4, hist=False, color=palette_ro[5])\nsns.distplot(data[data[\"Sex\"]==\"Female\"].Age, label=\"Female\", ax=ax4, hist=False, color=palette_ro[1])\n\nsns.distplot(data[data[\"SmokingStatus\"]==\"Never smoked\"].Age, label=\"Never smoked\", ax=ax5, hist=False, color=palette_ro[4])\nsns.distplot(data[data[\"SmokingStatus\"]==\"Ex-smoker\"].Age, label=\"Ex-smoker\", ax=ax5, hist=False, color=palette_ro[2])\nsns.distplot(data[data[\"SmokingStatus\"]==\"Currently smokes\"].Age, label=\"Currently smokes\", ax=ax5, hist=False, color=palette_ro[0])\n\nsns.countplot(x=\"SmokingStatus\", ax=ax6, data=data, hue=\"Sex\",\n              order=[\"Never smoked\", \"Ex-smoker\", \"Currently smokes\"], palette=palette_ro[-2::-4])\n\nfig.suptitle(\"Distribution of unique patients data\", fontsize=18);","28971003":"fig, ax = plt.subplots(figsize=(16, 6))\n\nsns.distplot(train[\"Weeks\"], ax=ax, color=palette_ro[1], bins=train[\"Weeks\"].max()-train[\"Weeks\"].min()+1)\nax.annotate(\"Min: {:,}\".format(train[\"Weeks\"].min()), xy=(train[\"Weeks\"].min(), 0.005), \n            xytext=(train[\"Weeks\"].min()-8, 0.008),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax.annotate(\"Max: {:,}\".format(train[\"Weeks\"].max()), xy=(train[\"Weeks\"].max(), 0.005), \n            xytext=(train[\"Weeks\"].max()-2, 0.008),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=-0.2\"))\nax.axvline(x=0, color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax.annotate(\"CT Scan\", xy=(0, 0.013), \n            xytext=(-12, 0.016),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax.axvline(x=train[\"Weeks\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax.annotate(\"Med: {:.0f}\".format(train[\"Weeks\"].median()), xy=(train[\"Weeks\"].median(), 0.020), \n            xytext=(train[\"Weeks\"].median()+2, 0.024),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\n\nax.set_title(\"Weeks Distribution\", fontsize=18);","e15f1781":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train[\"FVC\"], ax=ax1, color=palette_ro[5], hist=False)\nax1.annotate(\"Min: {:,}\".format(train[\"FVC\"].min()), xy=(train[\"FVC\"].min(), 0.00005), \n             xytext=(train[\"FVC\"].min()-300, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(train[\"FVC\"].max()), xy=(train[\"FVC\"].max(), 0.00005), \n             xytext=(train[\"FVC\"].max()-200, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.axvline(x=train[\"FVC\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Med: {:,.0f}\".format(train[\"FVC\"].median()), xy=(train[\"FVC\"].median(), 0.00005), \n             xytext=(train[\"FVC\"].median()-750, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"FVC Distribution\", fontsize=16);\n\nsns.distplot(train[\"Percent\"], ax=ax2, color=palette_ro[3], hist=False)\nax2.annotate(\"Min: {:.2f}\".format(train[\"Percent\"].min()), xy=(train[\"Percent\"].min(), 0.0015), \n             xytext=(train[\"Percent\"].min()-8, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax2.annotate(\"Max: {:.2f}\".format(train[\"Percent\"].max()), xy=(train[\"Percent\"].max(), 0.0015), \n             xytext=(train[\"Percent\"].max()-4, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax2.axvline(x=train[\"Percent\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Med: {:.2f}\".format(train[\"Percent\"].median()), xy=(train[\"Percent\"].median(), 0.0015), \n             xytext=(train[\"Percent\"].median()-17, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax2.set_title(\"Percent Distribution\", fontsize=16);","b9b01885":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train[train[\"Sex\"]==\"Male\"].FVC, label=\"Male\", ax=ax1, hist=False, color=palette_ro[5])\nax1.axvline(x=train[train[\"Sex\"]==\"Male\"].FVC.median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Male\\nMed: {:,.0f}\".format(train[train[\"Sex\"]==\"Male\"].FVC.median()), xy=(train[train[\"Sex\"]==\"Male\"].FVC.median(), 0.0006), \n             xytext=(train[train[\"Sex\"]==\"Male\"].FVC.median()+100, 0.00065),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train[train[\"Sex\"]==\"Female\"].FVC, label=\"Female\", ax=ax1, hist=False, color=palette_ro[1])\nax1.axvline(x=train[train[\"Sex\"]==\"Female\"].FVC.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Female\\nMed: {:,.0f}\".format(train[train[\"Sex\"]==\"Female\"].FVC.median()), xy=(train[train[\"Sex\"]==\"Female\"].FVC.median(), 0.0008), \n             xytext=(train[train[\"Sex\"]==\"Female\"].FVC.median()+100, 0.00085),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\n\nsns.distplot(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax2, hist=False, color=palette_ro[4])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Never smoked\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.0005), \n             xytext=(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-1000, 0.00055),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\nsns.distplot(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax2, hist=False, color=palette_ro[2])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax2.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.00058), \n             xytext=(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()-1200, 0.0007),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.25\"))\nsns.distplot(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax2, hist=False, color=palette_ro[0])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.0009), \n             xytext=(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+400, 0.00095),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"Relationship between FVC and Sex\", fontsize=16)\nax2.set_title(\"Relationship between FVC and SmokingStatus\", fontsize=16);","c878d481":"train_m = train[train[\"Sex\"]==\"Male\"].reset_index(drop=True)\ntrain_f = train[train[\"Sex\"]==\"Female\"].reset_index(drop=True)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax1, hist=False, color=palette_ro[4])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Never smoked\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.0005), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-1400, 0.0006),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax1, hist=False, color=palette_ro[2])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax1.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.00063), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()-1400, 0.00045),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax1, hist=False, color=palette_ro[0])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.00066), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+400, 0.00055),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax2, hist=False, color=palette_ro[4])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Never smoked\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.001), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-600, 0.0015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax2, hist=False, color=palette_ro[2])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax2.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.0013), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()+100, 0.0018),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.25\"))\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax2, hist=False, color=palette_ro[0])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.0035), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+200, 0.004),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"Relationship between FVC and SmokingStatus in Male\", fontsize=16)\nax2.set_title(\"Relationship between FVC and SmokingStatus in Female\", fontsize=16);","b321e7f1":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Age (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Age\"])), fontsize=16);","cfda26f8":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Weeks\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Weeks\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Weeks (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Weeks\"])), fontsize=16);","881324e1":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Age (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Age\"])), fontsize=16);","2b0280a0":"def get_tab(df):\n    vector = [(df.Age.values[0] - 30) \/ 30]\n    \n    if df.Sex.values[0] == 'male':\n       vector.append(0)\n    else:\n       vector.append(1)\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        vector.extend([0,0])\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        vector.extend([1,1])\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        vector.extend([0,1])\n    else:\n        vector.extend([1,0])\n    return np.array(vector)","40fa5a5a":"A = {} \nTAB = {} \nP = [] \nfor i, p in tqdm(enumerate(train.Patient.unique())):\n    sub = train.loc[train.Patient == p, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.vstack([weeks, np.ones(len(weeks))]).T\n    a, b = np.linalg.lstsq(c, fvc)[0]\n    \n    A[p] = a\n    TAB[p] = get_tab(sub)\n    P.append(p)\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.distplot(list(A.values()), ax=ax, color=palette_ro[1]);","0ba584ea":"# def get_img(path):\n#     d = pydicom.dcmread(path)\n#     return cv2.resize(d.pixel_array \/ 2**11, (528, 528))    # changed from 512\n\n# https:\/\/www.kaggle.com\/allunia\/pulmonary-dicom-preprocessing\ndef get_img(path, new_shape=(528, 528)):\n    d = pydicom.dcmread(path)\n    scan = d.pixel_array \/ 2**11\n    \n    left = int((scan.shape[0]-512)\/2)\n    right = int((scan.shape[0]+512)\/2)\n    top = int((scan.shape[1]-512)\/2)\n    bottom = int((scan.shape[1]+512)\/2)\n    \n    img = scan[top:bottom, left:right]\n    cropped_resized_scan = cv2.resize(img, new_shape, interpolation=cv2.INTER_LANCZOS4)\n    return cropped_resized_scan\n\n# get_img(\"..\/input\/osic-pulmonary-fibrosis-progression\/train\/ID00007637202177411956430\/1.dcm\")","d7f0d161":"class IGenerator(Sequence):\n    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n    def __init__(self, keys, a, tab, batch_size=32):\n        self.keys = [k for k in keys if k not in self.BAD_ID]\n        self.a = a\n        self.tab = tab\n        self.batch_size = batch_size\n        \n        self.train_data = {}\n        for p in train.Patient.values:\n            self.train_data[p] = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n    \n    def __len__(self):\n        return 1000\n    \n    def __getitem__(self, idx):\n        x = []\n        a, tab = [], [] \n        keys = np.random.choice(self.keys, size = self.batch_size)\n        for k in keys:\n            try:\n                i = np.random.choice(self.train_data[k], size=1)[0]\n                img = get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{k}\/{i}')\n                x.append(img)\n                a.append(self.a[k])\n                tab.append(self.tab[k])\n            except:\n                print(k, i)\n       \n        x,a,tab = np.array(x), np.array(a), np.array(tab)\n        x = np.expand_dims(x, axis=-1)\n        return [x, tab] , a","b9fab8e6":"%%time\ndef get_efficientnet(model, shape):\n    models_dict = {\n        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n    }\n    return models_dict[model]\n\ndef build_model(shape=(528, 528, 1), model_class=None):    # changed from 512\n    inp = L.Input(shape=shape)\n    base = get_efficientnet(model_class, shape)\n    x = base(inp)\n    x = L.GlobalAveragePooling2D()(x)\n    inp2 = L.Input(shape=(4,))\n    x2 = L.GaussianNoise(0.2)(inp2)\n    x = L.Concatenate()([x, x2]) \n    x = L.Dropout(0.32)(x)    # changed from 0.4\n    x = L.Dense(1)(x)\n    model = Model([inp, inp2] , x)\n    \n    weights = [w for w in os.listdir('..\/input\/osic-model-weights') if model_class in w][0]\n    model.load_weights('..\/input\/osic-model-weights\/' + weights)\n    return model\n\nmodel_classes = [\"b6\"] #['b0','b1','b2','b3',b4','b5','b6','b7']    # changed from b5\nmodels = [build_model(model_class=m, shape=(528, 528, 1)) for m in model_classes]    # changed from 512\nprint('Number of models: ' + str(len(models)))","3c9f5e22":"plot_model(models[0])","2e12a711":"def score(fvc_true, fvc_pred, sigma):\n    sigma_clip = np.maximum(sigma, 70)\n    delta = np.abs(fvc_true - fvc_pred)\n    delta = np.minimum(delta, 1000)\n    sq2 = np.sqrt(2)\n    metric = (delta \/ sigma_clip)*sq2 + np.log(sigma_clip * sq2)\n    return np.mean(metric)","1c3e26df":"tr_p, vl_p = train_test_split(P, \n                              shuffle=True, \n                              train_size=0.8)\n\nsubs = []\nfor model in models:\n    metric = []\n    for q in tqdm(range(1, 10)):\n        m = []\n        for p in vl_p:\n            x = [] \n            tab = [] \n\n            if p in ['ID00011637202177653955184', 'ID00052637202186188008618']:\n                continue\n\n            ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n            for i in ldir:\n                if int(i[:-4]) \/ len(ldir) < 0.8 and int(i[:-4]) \/ len(ldir) > 0.15:\n                    x.append(get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/{i}')) \n                    tab.append(get_tab(train.loc[train.Patient == p, :])) \n            if len(x) < 1:\n                continue\n            tab = np.array(tab) \n\n            x = np.expand_dims(x, axis=-1) \n            _a = model.predict([x, tab]) \n            a = np.quantile(_a, q \/ 10)\n\n            percent_true = train.Percent.values[train.Patient == p]\n            fvc_true = train.FVC.values[train.Patient == p]\n            weeks_true = train.Weeks.values[train.Patient == p]\n\n            fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n            percent = percent_true[0] - a * abs(weeks_true - weeks_true[0])\n            m.append(score(fvc_true, fvc, percent))\n        print(np.mean(m))\n        metric.append(np.mean(m))\n\n    q = (np.argmin(metric) + 1)\/ 10\n\n    sub = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv') \n    test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv') \n    A_test, B_test, P_test, W, FVC = {}, {}, {}, {}, {} \n    STD, WEEK = {}, {} \n    for p in test.Patient.unique():\n        x = [] \n        tab = [] \n        ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/test\/{p}\/')\n        for i in ldir:\n            if int(i[:-4]) \/ len(ldir) < 0.8 and int(i[:-4]) \/ len(ldir) > 0.15:\n                x.append(get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/test\/{p}\/{i}')) \n                tab.append(get_tab(test.loc[test.Patient == p, :])) \n        if len(x) <= 1:\n            continue\n        tab = np.array(tab) \n\n        x = np.expand_dims(x, axis=-1) \n        _a = model.predict([x, tab]) \n        a = np.quantile(_a, q)\n        A_test[p] = a\n        B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p]\n        P_test[p] = test.Percent.values[test.Patient == p] \n        WEEK[p] = test.Weeks.values[test.Patient == p]\n\n    for k in sub.Patient_Week.values:\n        p, w = k.split('_')\n        w = int(w) \n\n        fvc = A_test[p] * w + B_test[p]\n        sub.loc[sub.Patient_Week == k, \"FVC\"] = fvc\n        sub.loc[sub.Patient_Week == k, \"Confidence\"] = (\n            P_test[p] - A_test[p] * abs(WEEK[p] - w) \n    ) \n\n    _sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()\n    subs.append(_sub)","e2b6e89f":"N = len(subs)\nsub = subs[0].copy() # ref\nsub[\"FVC\"] = 0\nsub[\"Confidence\"] = 0\nfor i in range(N):\n    sub[\"FVC\"] += subs[0][\"FVC\"] * (1\/N)\n    sub[\"Confidence\"] += subs[0][\"Confidence\"] * (1\/N)","33298b98":"sub.head()","717a4ee5":"sub[[\"Patient_Week\", \"FVC\", \"Confidence\"]].to_csv(\"submission_img.csv\", index=False)","0c6a5a24":"img_sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","106b515d":"sub = pd.read_csv(ROOT + \"sample_submission.csv\")\nsub.head()","2d60ae94":"sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\nsub = sub.merge(test.drop('Weeks', axis=1), on=\"Patient\")\nsub.head()","5a14acf9":"train['WHERE'] = 'train'\ntest['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = train.append([test, sub])\n\nprint(train.shape, test.shape, sub.shape, data.shape)\nprint(train.Patient.nunique(), test.Patient.nunique(), sub.Patient.nunique(), data.Patient.nunique())\n\ndata.head(10)","12e24be6":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test', 'min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n\ndata.head(10)","21f961a7":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient', 'FVC']].copy()\nbase.columns = ['Patient', 'base_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)\n\nbase.head()","97ad43be":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base\n\ndata.head(10)","aebaaf14":"categorical_features = ['Sex', 'SmokingStatus']\nfeatures_nn = []\nfor col in categorical_features:\n    for mod in data[col].unique():\n        features_nn.append(mod)\n        data[mod] = (data[col] == mod).astype(int)\n\ndata.head(10)","c5a5b611":"data['Percent_n'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\ndata['Age_n'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\ndata['base_FVC_n'] = (data['base_FVC'] - data['base_FVC'].min() ) \/ ( data['base_FVC'].max() - data['base_FVC'].min() )\ndata['base_week_n'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\nfeatures_nn += ['Age_n', 'Percent_n', 'base_week_n', 'base_FVC_n']\n\nprint(features_nn)\ndata.head(10)","36e99942":"train = data.loc[data.WHERE=='train']\ntest = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data\n\ntrain.shape, test.shape, sub.shape","cd90bf9e":"C1, C2 = tf.constant(70, dtype=\"float32\"), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n    return backend.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.5, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return backend.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss","6de62128":"def make_model():\n    inp = L.Input(len(features_nn), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(inp)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(inp, preds, name=\"NeuralNet\")\n    model.compile(loss=mloss(0.64),    # changed from 0.8\n                  optimizer=tf.keras.optimizers.Adam(lr=0.1, decay=0.01),\n                  metrics=[score])\n    return model\n\nmodel = make_model()\nmodel.summary()","9ee0a70c":"plot_model(model)","012f6980":"X_train = train[features_nn].values\nX_test = sub[features_nn].values\n\ny_train = train['FVC'].values\n\noof_train = np.zeros((X_train.shape[0], 3))\ny_preds = np.zeros((X_test.shape[0], 3))","302f6155":"BATCH_SIZE = 128\nEPOCHS = 804    # changed from 800\nNFOLD = 5\n\nkf = KFold(n_splits=NFOLD)","ef2cbe7f":"%%time\nfor fold_id, (tr_idx, va_idx) in enumerate(kf.split(X_train)):\n    print(f\"FOLD {fold_id+1}\")\n    model = make_model()\n    model.fit(X_train[tr_idx], y_train[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n              validation_data=(X_train[va_idx], y_train[va_idx]), verbose=0)\n    print(\"train\", model.evaluate(X_train[tr_idx], y_train[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", model.evaluate(X_train[va_idx], y_train[va_idx], verbose=0, batch_size=BATCH_SIZE))\n    oof_train[va_idx] = model.predict(X_train[va_idx], batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD","1929b1ed":"fig, ax = plt.subplots(figsize=(12, 12))\n\nidxs = np.random.randint(0, y_train.shape[0], 100)\nax.plot(y_train[idxs], label=\"ground truth\", color=palette_ro[0])\nax.plot(oof_train[idxs, 0], label=\"q20\", color=palette_ro[3], ls=':', alpha=0.5)\nax.plot(oof_train[idxs, 1], label=\"q50\", color=palette_ro[4], ls=':', alpha=0.5)\nax.plot(oof_train[idxs, 2], label=\"q80\", color=palette_ro[5], ls=':', alpha=0.5)\nax.legend(loc=\"best\");","2f4f0bfc":"sigma_opt = mean_absolute_error(y_train, oof_train[:, 1])\nsigma_unc = oof_train[:, 2] - oof_train[:, 0]\nsigma_mean = np.mean(sigma_unc)\nprint(sigma_opt, sigma_mean)","e8b40c10":"print(sigma_unc.min(), sigma_unc.mean(), sigma_unc.max(), (sigma_unc>=0).mean())","1dea1c94":"print(np.mean(y_train \/ oof_train[:, 1]))","ec5a71d8":"fig, ax = plt.subplots(figsize=(16, 6))\n\nsns.distplot(sigma_unc, ax=ax, color=palette_ro[1])\nax.set_title(\"uncertainty in prediction\", fontsize=18);","14ab4bf2":"sub.head(10)","9608933c":"sub['FVC1'] = y_preds[:, 1]\nsub['Confidence1'] = y_preds[:, 2] - y_preds[:, 0]\n\nsub.head(10)","adc10bed":"subm = sub[['Patient_Week', 'FVC', 'Confidence', 'FVC1', 'Confidence1']].copy()\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']","2b09da70":"if sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']\n\nsubm.head(10)","aab4d847":"subm.describe().T","9f4aa809":"org_test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(org_test)):\n    subm.loc[subm['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    subm.loc[subm['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70\n\nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)\nreg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","e287f214":"df1 = img_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\ndf2 = reg_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\n\ndf = df1[['Patient_Week']].copy()\ndf['FVC'] = 0.2*df1['FVC'] + 0.8*df2['FVC']    # changed from 0.25, 0.75\ndf['Confidence'] = 0.0*df1['Confidence'] + 1.0*df2['Confidence']    # changed from 0.26, 0.74\ndf.head()","d0e684e9":"df.to_csv('submission.csv', index=False)","129cb599":"<a id=\"submit\"><\/a>\n# Ensemble & Submit \ud83d\udcdd","0ea008b3":"You can see that `FVC` of `Female` tends to be much lower than of `Male`, and there is also a difference in FVC by `SmokingStatus`, but this may be because there are more `Female` in `Never smoked`. Let's check it out.<br>\n<font color=\"RoyalBlue\">\u7537\u6027\u3068\u6bd4\u3079\u308b\u3068\u5973\u6027\u306e FVC \u306f\u304b\u306a\u308a\u4f4e\u304f\u306a\u308b\u50be\u5411\u306b\u3042\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002\u55ab\u7159\u72b6\u614b\u306b\u3088\u3063\u3066\u3082 FVC \u306b\u5dee\u304c\u51fa\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u55ab\u7159\u672a\u7d4c\u9a13\u8005\u306b\u5973\u6027\u304c\u591a\u3044\u305f\u3081\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u5b9f\u969b\u306b\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002<\/font>","abf976b2":"Add `Where` column to all the dataframes.<br>\n<font color=\"RoyalBlue\">\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b Where \u5217\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002<\/font><br>\nThen, in order to process the `train`, `test` and `sub` at the same time, these three are concatenated vertically into `data`.<br>\n<font color=\"RoyalBlue\">\u305d\u3057\u3066\u3001train, test, sub \u3092\u540c\u6642\u306b\u30c7\u30fc\u30bf\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u3001\u7e26\u65b9\u5411\u306b\u7d50\u5408\u3057\u3066 data \u3068\u3057\u3066\u304a\u304d\u307e\u3059\u3002<\/font>","27f8ed41":"We will create a modified version of the Laplace Log Likelihood function, which is the evaluation function for this competition.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30b3\u30f3\u30da\u3067\u306e\u8a55\u4fa1\u95a2\u6570\u3067\u3042\u308b\u4fee\u6b63\u7248\u30e9\u30d7\u30e9\u30b9\u5bfe\u6570\u5c24\u5ea6\u306e\u95a2\u6570\u3092\u4f5c\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002<\/font>","d239745b":"We could create `submission.csv`. Thank you so much for reading!<br>\n<font color=\"RoyalBlue\">submission.csv \u3092\u4f5c\u6210\u3067\u304d\u307e\u3057\u305f\u3002\u8aad\u3093\u3067\u304f\u3060\u3055\u308a\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01<\/font>","0af2a387":"<a id=\"percent\"><\/a>\n## FVC & Percent distribution \ud83d\udca8","4c26074c":"There was almost no correlation between `FVC` and `Age`.<br>\n<font color=\"RoyalBlue\">FVC \u3068\u5e74\u9f62\u306b\u306f\u76f8\u95a2\u306f\u307b\u307c\u898b\u3089\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002<\/font>","8e77111c":"<a id=\"quantile_m\"><\/a>\n## Build the model \ud83e\udde0\n\nThis competition is evaluated on a modified version of the Laplace Log Likelihood. For each true FVC measurement, you will predict both an FVC and a confidence measure (standard deviation \u03c3). The metric is computed as:<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30b3\u30f3\u30da\u3067\u306f\u3001\u30e9\u30d7\u30e9\u30b9\u5bfe\u6570\u5c24\u5ea6\u306e\u4fee\u6b63\u7248\u3067\u8a55\u4fa1\u3055\u308c\u307e\u3059\u3002\u771f\u306e\u5404 FVC \u6e2c\u5b9a\u306b\u3064\u3044\u3066\u3001FVC \u3068\u4fe1\u983c\u5ea6\uff08\u6a19\u6e96\u504f\u5dee \u03c3\uff09\u306e\u4e21\u65b9\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\u30e1\u30c8\u30ea\u30c3\u30af\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002<\/font><br><br>\n\n$\\large \\sigma_{clipped} = max(\\sigma, 70),$<br>\n$\\large \\Delta = min ( |FVC_{true} - FVC_{predicted}|, 1000 ),$<br>\n$\\Large metric = -   \\frac{\\sqrt{2} \\Delta}{\\sigma_{clipped}} - \\ln ( \\sqrt{2} \\sigma_{clipped} ).$<br>\n\nIn the following code, C1 is the value of confidence clipping in the modified Laplace Log Likelihood, an evaluation metric, and C2 is the error threshold.<br>\n<font color=\"RoyalBlue\">\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u306b\u304a\u3044\u3066\u3001C1 \u306f\u8a55\u4fa1\u6307\u6a19\u3067\u3042\u308b\u4fee\u6b63\u7248\u30e9\u30d7\u30e9\u30b9\u5bfe\u6570\u5c24\u5ea6\u306b\u304a\u3051\u308b\u4fe1\u983c\u5ea6\u306e\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u306e\u5024\u3001C2 \u306f\u8aa4\u5dee\u306e\u95be\u5024\u3067\u3059\u3002<\/font><br>\n\nThe `score` function takes the true and predicted values of the target variable and returns a score based on the modified Laplace Log Likelihood.\nThe `qloss` function is a pinball loss function, which is the loss function used when a multiple-quantile regression prediction is trained.\nThe `mloss` function takes a percentage and returns a function that sums the return values of the `score` function and the `qloss` function according to the percentage.<br>\n<font color=\"RoyalBlue\">score \u95a2\u6570\u306f\u3053\u306e\u8a55\u4fa1\u30e1\u30c8\u30ea\u30c3\u30af\u3067\u3059\u3002\u30b3\u30f3\u30da\u306e\u76ee\u7684\u5909\u6570\u306e\u771f\u306e\u5024\u3068\u4e88\u6e2c\u5024\u3092\u53d7\u3051\u53d6\u308a\u3001\u4fee\u6b63\u7248\u30e9\u30d7\u30e9\u30b9\u5bfe\u6570\u5c24\u5ea6\u306b\u57fa\u3065\u3044\u305f\u30b9\u30b3\u30a2\u3092\u8fd4\u3057\u307e\u3059\u3002<br>\nqloss \u95a2\u6570\u306f\u3001\u91cd\u5206\u4f4d\u70b9\u56de\u5e30\u4e88\u6e2c\u304c\u5b66\u7fd2\u3059\u308b\u3068\u304d\u306b\u4f7f\u7528\u3059\u308b\u640d\u5931\u95a2\u6570\u3067\u3042\u308b\u30d4\u30f3\u30dc\u30fc\u30eb\u30ed\u30b9\u95a2\u6570\u3067\u3059\u3002<br>\nmloss \u95a2\u6570\u306f\u5272\u5408 _lambda \u3092\u53d7\u3051\u53d6\u308a\u3001\u305d\u306e\u5272\u5408\u306b\u5fdc\u3058\u3066 score \u95a2\u6570\u3068 qloss \u95a2\u6570\u306e\u623b\u308a\u5024\u3092\u5408\u8a08\u3059\u308b\u95a2\u6570\u3092\u8fd4\u3057\u307e\u3059\u3002<\/font><br><br>\nHere, we define confidence as the difference between the predicted values at 0.2 and 0.8 quartiles.<br>\n<font color=\"RoyalBlue\">\u3053\u3053\u3067\u3001\u4fe1\u983c\u5ea6\u30920.2\u5206\u4f4d\u70b9\u30680.8\u5206\u4f4d\u70b9\u306b\u304a\u3051\u308b\u4e88\u6e2c\u5024\u306e\u5dee\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002<\/font>","722e44c6":"The following table from [Is this Malware? [EDA, FE and lgb][updated]](https:\/\/www.kaggle.com\/artgor\/is-this-malware-eda-fe-and-lgb-updated)<br>\n\n<font color=\"RoyalBlue\">\u30ab\u30e9\u30e0\u540d \/ \u30ab\u30e9\u30e0\u3054\u3068\u306e\u30e6\u30cb\u30fc\u30af\u5024\u6570 \/ \u6700\u3082\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u5024 \/ \u6700\u3082\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u5024\u306e\u51fa\u73fe\u56de\u6570 \/ \u6b20\u640d\u640d\u5024\u306e\u5272\u5408 \/ \u6700\u3082\u591a\u3044\u30ab\u30c6\u30b4\u30ea\u306e\u5272\u5408 \/ dtypes \u3092\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002<br>\ntrain \u306b\u304a\u3051\u308b Patient \u306e\u56fa\u6709 ID \u6570\u306f176\u306e\u3088\u3046\u3067\u3059\u3002<\/font>","02f300d3":"Next, we organize the data by Patient's unique ID.<br>\nIn the for statement, we extract `FVC` from `fvc` and `Weeks` from `weeks` in a NumPy array format, respectively.\nWe join weeks and the array with all 1's in the vertical direction and transpose them, and then assign them to `c`.\nThen find the least-squares of `c` and `fvc`, and add the slope to `A`.\nWe then add the value obtained from the above function `get_tab` to `TAB` and the unique ID of Patient to `p`.<br>\n<font color=\"RoyalBlue\">\u6b21\u306b\u3001Patient \u306e\u56fa\u6709 ID \u3054\u3068\u306b\u30c7\u30fc\u30bf\u3092\u6574\u7406\u3057\u307e\u3059\u3002<br>\nfor \u6587\u306e\u4e2d\u3067 fvc \u306b FVC, weeks \u306b Weeks \u3092\u305d\u308c\u305e\u308c NumPy array \u5f62\u5f0f\u3067\u53d6\u308a\u51fa\u3057\u307e\u3059\u3002\u3002<br>\nweeks \u3068\u8981\u7d20\u304c\u5168\u30661\u306e\u914d\u5217\u3092\u7e26\u65b9\u5411\u306b\u7d50\u5408\u3057\u3001\u8ee2\u7f6e\u3057\u305f\u3082\u306e\u3092 c \u3068\u3057\u307e\u3059\u3002<br>\n\u305d\u3057\u3066 c \u3068 fvc \u306e\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u6c42\u3081\u3001\u305d\u306e\u50be\u304d\u3092 A \u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002<br>\n\u3055\u3089\u306b\u4e0a\u8a18\u306e\u4e0a\u8a18\u306e\u95a2\u6570 get_tab \u3067\u53d6\u5f97\u3057\u305f\u5024\u3092 TAB \u306b\u3001Patient \u306e\u56fa\u6709 ID \u3092 p \u306b\u8ffd\u52a0\u3057\u3066\u3044\u304d\u307e\u3059\u3002<\/font>","088ea0ea":"* `Patient` - \u5404\u60a3\u8005\u56fa\u6709\u306e ID\uff08\u60a3\u8005\u306e DICOM \u30d5\u30a9\u30eb\u30c0\u306e\u540d\u524d\u3067\u3082\u3042\u308a\u307e\u3059\uff09\n* `Weeks` - \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3 CT \u306e\u524d\u5f8c\u306e\u76f8\u5bfe\u7684\u306a\u9031\u6570\uff08\u8ca0\u6570\u306e\u5834\u5408\u3082\uff09\n* `FVC` - \u8a18\u9332\u3055\u308c\u305f\u52aa\u529b\u80ba\u6d3b\u91cf\uff08mL\uff09\n* `Percent` - %FVC, \u30d1\u30fc\u30bb\u30f3\u30c8\u80ba\u6d3b\u91cf\u3002\u5e74\u9f62\u30fb\u8eab\u9577\u30fb\u6027\u5225\u304b\u3089\u8a08\u7b97\u3057\u305f\u4e88\u6e2c FVC \u306b\u5bfe\u3059\u308b\u5b9f\u969b\u306e FVC \u306e\u5272\u5408\n* `Age` - \u5e74\u9f62\n* `Sex` - \u6027\u5225\uff08`Male` \/ `Female`\uff09\n* `SmokingStatus` - \u55ab\u7159\u72b6\u614b\uff08`Never smoked` \/ `Ex-smoker` \/ `Currently smokes`\uff09\n\n<a id=\"explore\"><\/a>\n# Explore CSV data \ud83d\udcca","81f79958":"Create a function to read a DICOM file from the given path.\nFor ease of use in deep learning we will divide the value by 2048 and then crop and resize the image.<br>\n<font color=\"RoyalBlue\">\u6e21\u3055\u308c\u305f\u30d1\u30b9\u304b\u3089 DICOM \u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u95a2\u6570\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002<br>\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u6271\u3044\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u5024\u3092 2**11 \u3067\u5272\u308a\u3001\u3055\u3089\u306b\u753b\u50cf\u3092\u30af\u30ed\u30c3\u30d7\u30fb\u30ea\u30b5\u30a4\u30ba\u3057\u307e\u3059\u3002<\/font>","0ec59b82":"<a id=\"fvc\"><\/a>\n## Relationships between FVC and other variables \ud83e\udd1d\n\nLet's look at the relationships between the objective variable, `FVC`, and other variables.<br>\n<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u3067\u3042\u308b FVC \u3068\u4ed6\u306e\u5909\u6570\u3068\u306e\u95a2\u4fc2\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002<\/font>","4a6209ef":"Next, we associate `base` with `Patient` in `data`.\nWe will create `base_week` column in `data`, which will be `Weeks` minus `min_week`.\nThis will add the `base_FVC` and `base_week` columns to `data`.\nWe should remove the `base` column.<br>\n<font color=\"RoyalBlue\">\u6b21\u306b\u3001data \u306b base \u3092 Patient \u306b\u7d10\u3065\u3051\u3066\u7d50\u5408\u3057\u307e\u3059\u3002<br>\ndata \u306b base_week \u5217\u3092\u4f5c\u6210\u3057\u3001Weeks \u304b\u3089 min_week \u3092\u5f15\u3044\u305f\u5024\u3068\u3057\u307e\u3059\u3002<br>\n\u3053\u308c\u3067\u3001data \u306b base_FVC \u5217\u3068 base_week \u5217\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002<br>\nbase \u306f\u524a\u9664\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002<\/font>","c2b33eab":"From here, calculate `base_FVC` (= the `FVC` of `Patient` at `min_week`) and the `base_week` (= how many weeks have passed since `min_week`).<br>\n<font color=\"RoyalBlue\">\u3053\u3053\u304b\u3089\u3001base_FVC\uff08\uff1dPatient \u306e min_week \u6642\u306e FVC\uff09\u3068 base_week\uff08\uff1dmin_week \u304b\u3089\u4f55\u9031\u7d4c\u3063\u305f\u3068\u304d\u306e\u30c7\u30fc\u30bf\u304b\uff09\u3092\u7b97\u51fa\u3057\u3066\u3044\u304d\u307e\u3059\u3002<\/font><br>\n\n\nFirst, extract the rows in the data where `Weeks` is `min_week` and set them to `base`.\nExtract only the `Patient` and `FVC` columns from the `base`, and change the column name from `FVC` to `base_FVC`.\nThen create a new `nb` column and set all the values to 1.\nGroup the `base` with `Patient` and compute the cumulative sum with the `nb` column.\nExtract only the rows from `base` that have `nb` columns of 1, and replace `base`.\nThis allows us to eliminate duplicate `Patient` rows from the `base` dataframe with `base_FVC` in it.\nLet's remove the `nb` column.<br>\n<font color=\"RoyalBlue\">\u307e\u305a\u3001data \u306e Weeks \u304c min_week \u3067\u3042\u308b\u884c\u3092\u62bd\u51fa\u3057\u3001base \u3068\u3057\u307e\u3059\u3002<br>\nbase \u304b\u3089 Patient, FVC \u5217\u3060\u3051\u3092\u629c\u304d\u51fa\u3057\u307e\u3059\u3002<br>\n\u5217\u540d\u3092 FVC \u304b\u3089 base_FVC \u306b\u5909\u66f4\u3057\u307e\u3059\u3002<br>\n\u305d\u3057\u3066\u65b0\u305f\u306b nb \u5217\u3092\u4f5c\u308a\u3001\u5024\u3092\u3059\u3079\u30661\u3068\u3057\u307e\u3059\u3002<br>\nbase \u3092 Patient \u3067\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3001nb \u5217\u3092\u6307\u5b9a\u3057\u3066\u7d2f\u7a4d\u548c\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002<br>\nbase \u304b\u3089 nb \u5217\u304c1\u306e\u884c\u306e\u307f\u3092\u62bd\u51fa\u3057\u3001base \u3092\u7f6e\u304d\u63db\u3048\u307e\u3059\u3002<br>\n\u3053\u308c\u306b\u3088\u308a\u3001base_FVC \u304c\u8f09\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0 base \u304b\u3089 Patient \u306e\u91cd\u8907\u3092\u7121\u304f\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002<br>\nnb \u5217\u306f\u524a\u9664\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002<\/font>","a3aa17c4":"Read the original `test.csv` and overwrite the `FVC` and `Confidence` in the predicted data to be submitted if they are known in the `test.csv`.<br>\n<font color=\"RoyalBlue\">\u30aa\u30ea\u30b8\u30ca\u30eb\u306e test.csv \u3092\u8aad\u307f\u8fbc\u307f\u3001\u6295\u7a3f\u4e88\u5b9a\u306e\u4e88\u6e2c\u30c7\u30fc\u30bf\u306e\u4e2d\u306b test.csv \u3067\u65e2\u77e5\u306e\u30c7\u30fc\u30bf\u304c\u3042\u308c\u3070 FVC \u3068 Confidence \u3092\u4e0a\u66f8\u304d\u3057\u307e\u3059\u3002<\/font>","f79004df":"Let's create a model.\nWe take image data and run the tensor through `EfficientNetB6` and `GlobalAveragePooling2D`, and we take the pre-processed CSV data and add Gaussian noise to the tensor and concatenate them together to output the model.\nThe model weights are trained.<br>\n<font color=\"RoyalBlue\">\u3067\u306f\u3001\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002<br>\n\u753b\u50cf\u30c7\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u3063\u3066 EfficientNetB6 \u3068 GlobalAveragePooling2D \u306b\u901a\u3057\u305f\u30c6\u30f3\u30bd\u30eb\u3068\u3001\u524d\u51e6\u7406\u6e08\u307f\u306e CSV \u30c7\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u3063\u3066\u30ac\u30a6\u30b7\u30a2\u30f3\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u30c6\u30f3\u30bd\u30eb\u3092\u9023\u7d50\u3057\u3066\u51fa\u529b\u3057\u307e\u3059\u3002<br>\n\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u306f\u8a13\u7df4\u6e08\u307f\u306e\u3082\u306e\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002<\/font>","74589d88":"<a id=\"unique\"><\/a>\n## Distribution of unique patients data \ud83d\ude37 (Age, Sex, SmokingStatus)\n<font color=\"RoyalBlue\">\u3067\u306f\u3001train \u306b\u304a\u3051\u308b Patient \u306e\u56fa\u6709 ID \u3054\u3068\u306e\u5e74\u9f62\u3001\u6027\u5225\u3001\u55ab\u7159\u72b6\u6cc1\u306e\u5206\u5e03\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002<\/font>","2a1baa4d":"Add a `min_week` column for the minimum number of weeks per Patient.<br>\n<font color=\"RoyalBlue\">Patient \u3054\u3068\u306e\u6700\u5c0f\u306e\u9031\u6570\u3092\u793a\u3059 min_week \u5217\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002<\/font>","07ab49a6":"There was almost no correlation between `FVC` and `Weeks` either.<br>\n<font color=\"RoyalBlue\">FVC \u3068\u9031\u6570\u306b\u3082\u76f8\u95a2\u306f\u307b\u307c\u898b\u3089\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002<\/font>","dcefff89":"We calculate the optimized \ud835\udf0e (standard deviation) from the `oof_train`. `sigma_opt` is the mean absolute error between the correct value of each fold and the prediction (median), `sigma_unc` is the difference between the prediction (0.2 quantile) and the prediction (0.8 quantile), and `sigma_mean` is the mean value of the difference.<br>\n<font color=\"RoyalBlue\">\u3067\u306f\u3001oof_train \u304b\u3089\u6700\u9069\u5316\u3055\u308c\u305f \ud835\udf0e\uff08\u6a19\u6e96\u504f\u5dee\uff09\u3092\u8a08\u7b97\u3057\u307e\u3057\u3087\u3046\u3002\u5404\u30d5\u30a9\u30fc\u30eb\u30c9\u306e\u6b63\u89e3\u5024\u3068\u4e88\u6e2c\u5024\uff08\u4e2d\u592e\u5024\uff09\u3068\u306e\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee\u3092 sigma_opt, \u4e88\u6e2c\u5024\uff080.2\u5206\u4f4d\u6570\uff09\u3068\u4e88\u6e2c\u5024\uff080.8\u5206\u4f4d\u6570\uff09\u3068\u306e\u5dee\u3092 sigma_unc, \u305d\u306e\u5e73\u5747\u5024\u3092 sigma_mean \u3068\u3057\u307e\u3059\u3002<\/font>","a77571fe":"`subm` is defined by extracting the required columns from the `sub` and leaving only the rows with non-null data in `FVC1`.<br>\n<font color=\"RoyalBlue\">`sub` \u304b\u3089\u5fc5\u8981\u306a\u5217\u3092\u629c\u304d\u51fa\u3057\u3001`FVC1` \u306e\u30c7\u30fc\u30bf\u304c `null` \u3067\u306a\u3044\u884c\u3060\u3051\u306b\u3057\u305f\u3082\u306e\u3092 `subm` \u3068\u3057\u307e\u3059\u3002<\/font>","33d385d6":"Perform one-hot-encoding of `Sex` and `SmokingStatus`.<br>\n<font color=\"RoyalBlue\">Sex \u3068 SmokingStatus \u306e\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u884c\u3044\u307e\u3059\u3002<\/font>","05d4b448":"Prepare a submission file from the predictions of the neural network.<br>\n<font color=\"RoyalBlue\">\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u7d50\u679c\u304b\u3089\u63d0\u51fa\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b\u6e96\u5099\u3092\u3057\u307e\u3059\u3002<\/font>","209648f2":"Compared to `male`, `female` seems to be of a wider age range and is less likely to smoke. And, `Never smoked` tend to be younger than `Ex-smoker`.<br>\n<font color=\"RoyalBlue\">\u7537\u6027\u306b\u6bd4\u3079\u3066\u5973\u6027\u306f\u5e74\u9f62\u5c64\u304c\u5e45\u5e83\u304f\u3001\u55ab\u7159\u8005\u304c\u5c11\u306a\u3044\u3088\u3046\u3067\u3059\u3002\u307e\u305f\u3001\u55ab\u7159\u672a\u7d4c\u9a13\u8005\u306f\u5143\u55ab\u7159\u8005\u3088\u308a\u3082\u82e5\u3044\u50be\u5411\u306b\u3042\u308a\u307e\u3059\u3002<\/font>\n\n<a id=\"weeks\"><\/a>\n## Weeks distribution \ud83d\udcc5","243b3d5f":"When limited to `Male`, `FVC` does not seem to change much with `SmokingStatus`. In the case of `Female`, there is a difference, but this is probably due to the small sample size (especially for `Currently smokes`). It may also be important to consider that patients who are `Currently smokes` are likely to be less severely affected.<br>\n<font color=\"RoyalBlue\">\u7537\u6027\u306b\u9650\u5b9a\u3057\u3066\u307f\u308b\u3068\u3001FVC \u306f\u55ab\u7159\u72b6\u614b\u306b\u3088\u3063\u3066\u306f\u3042\u307e\u308a\u5909\u5316\u3057\u306a\u3044\u3088\u3046\u3067\u3059\u3002\u5973\u6027\u306e\u5834\u5408\u306f\u5dee\u304c\u51fa\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u30b5\u30f3\u30d7\u30eb\u6570\u304c\u5c11\u306a\u3044\u305f\u3081\u3067\u3057\u3087\u3046\uff08\u7279\u306b\u73fe\u55ab\u7159\u8005\uff09\u3002\u73fe\u55ab\u7159\u8005\u306e\u60a3\u8005\u306b\u306f\u91cd\u75c7\u8005\u304c\u5c11\u306a\u3044\u3067\u3042\u308d\u3046\u3053\u3068\u3082\u8003\u616e\u3059\u3079\u304d\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002<\/font>","d03af0b3":"Linear Decay predictions have been made!<br>\n<font color=\"RoyalBlue\">Linear Decay \u3067\u306e\u4e88\u6e2c\u304c\u4f5c\u6210\u3067\u304d\u307e\u3057\u305f\uff01<\/font>\n\n<a id=\"quantile\"><\/a>\n# Multiple Quantile Regression \ud83c\udf12\n<a id=\"quantile_d\"><\/a>\n## Data preprocessing for Multiple Quantile Regression \ud83e\uddf9\n\nNext, let's pre-process the data for multiple quantile regression. First, check the format of the `sub` (sample_submission.csv).<br>\n<font color=\"RoyalBlue\">\u6b21\u306b\u3001\u91cd\u5206\u4f4d\u70b9\u56de\u5e30\u306e\u305f\u3081\u306e\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u3001sub (sample_submission.csv) \u306e\u5f62\u5f0f\u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002<\/font>","760ecd62":"Let's illustrate the correct and predicted values.<br>\n<font color=\"RoyalBlue\">\u6b63\u89e3\u5024\u3068\u4e88\u6e2c\u5024\u3092\u56f3\u793a\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002<\/font>","b46c41a0":"Split `Patient_Week` in `sub` into `Patient` and `Weeks`, according to the `train` and `test` formats. Then, attach the `Patient` to `Patient` in `sub` and merge it with the `Patient`. This makes it easier to handle the prediction.<br>\n<font color=\"RoyalBlue\">sub \u306e Patient_Week \u3092 Patient \u3068 Weeks \u306b\u5206\u5272\u3057\u3001train \u3084 test \u306e\u5f62\u5f0f\u306b\u5408\u308f\u305b\u307e\u3059\u3002\u305d\u3057\u3066\u3001sub \u306b test \u3092 Patient \u306b\u7d10\u3065\u3051\u3066\u7d50\u5408\u3057\u307e\u3059\u3002\u3053\u3046\u3059\u308c\u3070\u3001\u4e88\u6e2c\u6642\u306b\u7c21\u5358\u306b\u51e6\u7406\u3092\u884c\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002<\/font>","c03dd872":"There don't seem to be too many of them. These duplicate rows should be removed.<br>\n<font color=\"RoyalBlue\">\u6570\u306f\u3042\u307e\u308a\u591a\u304f\u306a\u3044\u3088\u3046\u3067\u3059\u3002\u3053\u308c\u3089\u306e\u91cd\u8907\u3057\u305f\u884c\u306f\u524a\u9664\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002<\/font>","4a738973":"Now, we will use the model to make predictions.<br>\n<font color=\"RoyalBlue\">\u305d\u308c\u3067\u306f\u3001\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\u3092\u884c\u3044\u307e\u3059\u3002<\/font>","8fe1ed13":"There are no missing values in both `train` and `test`.<br>\n<font color=\"RoyalBlue\">train \u3068 test \u306e\u4e21\u65b9\u306b\u6b20\u640d\u5024\u306f\u3042\u308a\u307e\u305b\u3093\u3002<\/font><br>\nBefore EDA, we will try to find duplicate rows in the `train` where the `Patient` and `Weeks` elements match.<br>\n<font color=\"RoyalBlue\">EDA \u306e\u524d\u306b\u3001train \u306e\u4e2d\u306b\u3042\u308b Patient \u3068 Weeks \u306e\u8981\u7d20\u304c\u4e00\u81f4\u3057\u91cd\u8907\u3057\u3066\u3044\u308b\u884c\u3092\u63a2\u3057\u3066\u307f\u307e\u3059\u3002<\/font>","12159d7d":"There was a positive correlation between `FVC` and `Percent`, which is not surprising since `Percent` is a value calculated from `FVC` and other data.<br>\n<font color=\"RoyalBlue\">FVC \u3068 Percent \u306b\u306f\u6b63\u306e\u76f8\u95a2\u304c\u898b\u3089\u308c\u307e\u3057\u305f\u3002Percent \u306f FVC \u306a\u3069\u304b\u3089\u7b97\u51fa\u3059\u308b\u5024\u306a\u306e\u3067\u5f53\u7136\u3068\u3044\u3048\u3070\u5f53\u7136\u3067\u3059\u3002<\/font>\n\n<a id=\"efficient\"><\/a>\n# Linear Decay (based on EfficientNets) \ud83d\udcf7\nFirst of all, let's try to make predictions from DICOM and other data. We make a function to put `Age`, `Sex`, and `SmokingStatus` into NumPy array.<br>\n<font color=\"RoyalBlue\">\u305d\u308c\u3067\u306f\u3001\u307e\u305a\u306f DICOM \u3092\u542b\u3081\u305f\u30c7\u30fc\u30bf\u304b\u3089\u4e88\u6e2c\u3092\u51fa\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002Age, Sex, SmokingStatus \u3092\u5909\u63db\u3057\u3064\u3064 NumPy array \u306b\u307e\u3068\u3081\u308b\u95a2\u6570\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002<\/font>","722ebc44":"<a id=\"overview\"><\/a>\n# Overview \ud83e\uddd0\n[Pulmonary fibrosis is a lung disease that occurs when lung tissue becomes damaged and scarred. This thickened, stiff tissue makes it more difficult for your lungs to work properly. As pulmonary fibrosis worsens, you become progressively more short of breath.](https:\/\/www.mayoclinic.org\/diseases-conditions\/pulmonary-fibrosis\/symptoms-causes\/syc-20353690)<br>\n<font color=\"RoyalBlue\">\u80ba\u7dda\u7dad\u75c7\u3068\u306f\u3001\u80ba\u306e\u7d44\u7e54\u304c\u50b7\u3064\u304d\u3001\u50b7\u8de1\u304c\u6b8b\u308b\u3053\u3068\u3067\u8d77\u3053\u308b\u80ba\u306e\u75c5\u6c17\u3067\u3059\u3002\u3053\u306e\u80a5\u539a\u3057\u305f\u786c\u3044\u7d44\u7e54\u306f\u3001\u80ba\u306e\u6b63\u5e38\u306a\u52d5\u4f5c\u3092\u56f0\u96e3\u306b\u3057\u307e\u3059\u3002\u80ba\u7dda\u7dad\u75c7\u304c\u60aa\u5316\u3059\u308b\u3068\u3001\u5f90\u3005\u306b\u606f\u5207\u308c\u304c\u3072\u3069\u304f\u306a\u308a\u307e\u3059\u3002<\/font>\n<img src='https:\/\/i.imgur.com\/edKPRik.png' width=\"600\">\nIn \"OSIC Pulmonary Fibrosis Progression\", we needs to predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs by using AI machine learning. In detail, we must predict both a Forced vital capacity (FVC) and a confidence measure for each patient.<br>\n<font color=\"RoyalBlue\">\u300cOSIC \u80ba\u7dda\u7dad\u75c7\u306e\u9032\u884c\u300d\u3067\u306f\u3001\u80ba\u306eCT\u30b9\u30ad\u30e3\u30f3\u306b\u57fa\u3065\u3044\u3066\u60a3\u8005\u306e\u80ba\u6a5f\u80fd\u306e\u4f4e\u4e0b\u306e\u91cd\u75c7\u5ea6\u3092 AI \u6a5f\u68b0\u5b66\u7fd2\u3092\u7528\u3044\u3066\u4e88\u6e2c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002<\/font><br>\n<font color=\"RoyalBlue\">\u8a73\u3057\u304f\u8a00\u3046\u3068\u3001\u5404\u60a3\u8005\u306e\u52aa\u529b\u80ba\u6d3b\u91cf\uff08FVC\uff09\u3068\u4fe1\u983c\u5ea6\u306e\u4e21\u65b9\u3092\u4e88\u6e2c\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002<\/font>\n\n# Table of contents \ud83d\udcd6\n* [Overview \ud83e\uddd0](#overview)\n* [Acknowledgements \ud83d\ude47](#acknowledgements)\n* [Setup \ud83d\udcbb](#setup)\n* [Load the data \ud83d\udcc3](#load)\n* [Explore CSV data \ud83d\udcca](#explore)\n    * [Distribution of unique patients data \ud83d\ude37 (Age, Sex, SmokingStatus)](#unique)\n    * [Weeks distribution \ud83d\udcc5](#weeks)\n    * [FVC & Percent distribution \ud83d\udca8](#percent)\n    * [Relationships between FVC and other variables \ud83e\udd1d](#fvc)\n* [Linear Decay (based on EfficientNets) \ud83d\udcf7](#efficient)\n* [Multiple Quantile Regression \ud83c\udf12](#quantile)\n    * [Data preprocessing for Multiple Quantile Regression \ud83e\uddf9](#quantile_d)\n    * [Build the model \ud83e\udde0](#quantile_m)\n    * [Cross validation \ud83d\udcad](#quantile_c)\n* [Ensemble & Submit \ud83d\udcdd](#submit)\n\n<a id=\"acknowledgements\"><\/a>\n# Acknowledgements \ud83d\ude47\n\n- Ulrich GOUE's [Osic-Multiple-Quantile-Regression-Starter](https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter)\n- Michael Kazachok's [Linear Decay (based on ResNet CNN)](https:\/\/www.kaggle.com\/miklgr500\/linear-decay-based-on-resnet-cnn)\n- Wei Hao Khoong's [](http:\/\/)[EfficientNets + Quantile Regression (Inference)](https:\/\/www.kaggle.com\/khoongweihao\/efficientnets-quantile-regression-inference)\n\n<a id=\"setup\"><\/a>\n# Setup \ud83d\udcbb\nAll seed values are fixed at 42.<br>\n<font color=\"RoyalBlue\">\u30b7\u30fc\u30c9\u5024\u306f\u5168\u306642\u3067\u56fa\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002<\/font><br>","1d135040":"<a id=\"load\"><\/a>\n# Load the data \ud83d\udcc3","a5db599f":"Now that the process is done, let's split data into `train`, `test` and `sub` using the `WHERE` column, and remove `data`.<br>\n<font color=\"RoyalBlue\">\u51e6\u7406\u304c\u7d42\u308f\u3063\u305f\u306e\u3067\u3001WHERE \u5217\u3092\u4f7f\u3063\u3066 data \u3092 train, test, sub \u306b\u5206\u5272\u3057\u76f4\u3057\u307e\u3057\u3087\u3046\u3002data \u306f\u524a\u9664\u3057\u3066\u304a\u304d\u307e\u3059\u3002<\/font>","c620476b":"<a id=\"quantile_c\"><\/a>\n## Cross validation \ud83d\udcad\n\nUse the model we created to cross-validate.<br>\n<font color=\"RoyalBlue\">\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3044\u307e\u3057\u3087\u3046\u3002<\/font>","585bc25d":"Normalize `Percent`, `Age`, `base_FVC`, and `base_week`.<br>\n<font color=\"RoyalBlue\">Percent, Age, base_FVC, and base_week \u306e\u6b63\u898f\u5316\u3092\u884c\u3044\u307e\u3059\u3002<\/font>","6e2c4e29":"Ensemble two models.<br>\n<font color=\"RoyalBlue\">\uff12\u3064\u306e\u30e2\u30c7\u30eb\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u307e\u3059\u3002<\/font>"}}