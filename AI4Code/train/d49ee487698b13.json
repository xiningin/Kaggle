{"cell_type":{"fb59024e":"code","fef52872":"code","e3659373":"code","55e30839":"code","030f2078":"code","d0aef919":"code","0b51304f":"code","7f05ffa4":"code","8f538487":"code","ac9d3c13":"code","d59b5cca":"code","d8552678":"code","15becd78":"code","c14edd1b":"code","74db8722":"code","490f1bd5":"code","674c26ca":"code","8ea636eb":"code","fd896574":"code","3e152d53":"code","0b98cefb":"code","e2e2d8de":"code","4bc11f5d":"code","6951e662":"code","f1499460":"code","dba81400":"code","56d6ab96":"code","16ae686e":"code","aac9270b":"code","468e50a0":"code","35a5dc22":"code","d17b517b":"code","b6c49fbf":"code","af84cc2d":"code","b8255a5d":"code","aa4f6d44":"code","d13de7b8":"code","b2d6cedc":"code","95216eae":"code","f4b52f48":"code","79712c4c":"code","07497e19":"code","d3038db6":"code","a673a153":"code","fa2d05f7":"code","9e137e85":"markdown","ed0ab560":"markdown","de5936c8":"markdown","01736383":"markdown","a9a59eea":"markdown","59de0428":"markdown","a98c50ce":"markdown","8989531b":"markdown","9458f694":"markdown","27866bb2":"markdown","37cf3042":"markdown","f42cc2ca":"markdown","e1c28eb7":"markdown","202f70ed":"markdown","c242fcde":"markdown","f0069aa4":"markdown","167b89a6":"markdown","7bd42266":"markdown","a2d3b931":"markdown","9ec85bda":"markdown","0d716677":"markdown","e71c2edc":"markdown","3dbf852f":"markdown","98a50713":"markdown","4467d660":"markdown","6e7d426b":"markdown","4dde3cfc":"markdown","335f7a97":"markdown","dbe9956c":"markdown","063f712c":"markdown","6d2a92a3":"markdown","5dc4e10c":"markdown","c411930f":"markdown","dc8341c5":"markdown","e4a9e1ff":"markdown","865a987b":"markdown","b39d7025":"markdown","f9d72164":"markdown","4c122c67":"markdown","839a6481":"markdown"},"source":{"fb59024e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import StandardScaler\nplt.style.use('seaborn')\nsns.set(font_scale=2)\npd.set_option('display.max_columns', 500)","fef52872":"def plot_kde_hist_for_numeric(df, col, target):\n    fig, ax = plt.subplots(1,4 , figsize=(16, 8))\n    \n    sns.kdeplot(df.loc[df[target] == 0, col], ax=ax[0], label='no heart disease')\n    sns.kdeplot(df.loc[df[target] == 1, col], ax=ax[0], label='has heart disease')\n    \n    df.loc[df[target] == 0, col].hist(ax=ax[1], bins=10)\n    df.loc[df[target] == 1, col].hist(ax=ax[1], bins=10)\n    \n    df.loc[df[target] == 0, col].plot.box(ax=ax[2])\n    df.loc[df[target] == 1, col].plot.box(ax=ax[3])\n    \n    \n    plt.suptitle(col, fontsize=30)\n    ax[0].set_title('KDE plot')\n    ax[1].set_title('Histogram')\n    ax[1].legend(['no heart disease', 'has heart disease'])\n    \n    ax[2].set_title('no heart disease')\n    ax[3].set_title('has heart disease')\n    plt.show()\n    \ndef corr_heatmap(df):\n    correlations = df.corr()\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    fig, ax = plt.subplots(figsize=(20, 20))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show()\n\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y\n","e3659373":"%time df = pd.read_csv('..\/input\/heart.csv')","55e30839":"print(df.shape)","030f2078":"df.head()","d0aef919":"df.describe()","0b51304f":"df.dtypes","7f05ffa4":"df['target'].value_counts().plot.bar()\nplt.title('Target')\nplt.show()","8f538487":"%%time\ndf.isnull().values.any()","ac9d3c13":"sns.countplot(x='sex', hue='target',data=df)\nplt.show()","d59b5cca":"sns.countplot(x='cp', hue='target',data=df)\nplt.show()","d8552678":"sns.countplot(x='fbs', hue='target',data=df)\nplt.show()","15becd78":"sns.countplot(x='restecg', hue='target',data=df)\nplt.show()","c14edd1b":"sns.countplot(x='exang', hue='target',data=df)\nplt.show()","74db8722":"sns.countplot(x='slope', hue='target',data=df)\nplt.show()","490f1bd5":"sns.countplot(x='ca', hue='target',data=df)\nplt.show()","674c26ca":"sns.countplot(x='thal', hue='target', data=df)\nplt.show()","8ea636eb":"plot_kde_hist_for_numeric(df, 'age', target='target')","fd896574":"male_df = df[df['sex'] == 1]\nplot_kde_hist_for_numeric(male_df, 'age', target='target')","3e152d53":"female_df = df[df['sex'] == 0]\nplot_kde_hist_for_numeric(female_df, 'age', target='target')","0b98cefb":"plot_kde_hist_for_numeric(df, 'trestbps', target='target')","e2e2d8de":"plot_kde_hist_for_numeric(df, 'chol', target='target')","4bc11f5d":"plot_kde_hist_for_numeric(df, 'thalach', target='target')\n","6951e662":"plot_kde_hist_for_numeric(df, 'oldpeak', target='target')\n","f1499460":"corr_heatmap(df)","dba81400":"array = df.values\nX = array[:,0:12].astype(float)\nY = array[:,13]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n    test_size=validation_size, random_state=seed)","56d6ab96":"num_folds = 10\nseed = 7\nscoring =  'accuracy'","16ae686e":"models = []\nmodels.append(( 'LR' , LogisticRegression()))\nmodels.append(( 'LDA' , LinearDiscriminantAnalysis()))\nmodels.append(( 'KNN' , KNeighborsClassifier()))\nmodels.append(( 'CART' , DecisionTreeClassifier()))\nmodels.append(( 'NB' , GaussianNB()))\nmodels.append(( 'SVM' , SVC()))","aac9270b":"results = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","468e50a0":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","35a5dc22":"pipelines = []\npipelines.append(( 'ScaledLR' , Pipeline([( 'Scaler' , StandardScaler()),( 'LR' ,\n    LogisticRegression())])))\npipelines.append(( 'ScaledLDA' , Pipeline([( 'Scaler' , StandardScaler()),( 'LDA' ,\n    LinearDiscriminantAnalysis())])))\n\npipelines.append(( 'ScaledKNN' , Pipeline([( 'Scaler' , StandardScaler()),( 'KNN' ,\n    KNeighborsClassifier())])))\npipelines.append(( 'ScaledCART' , Pipeline([( 'Scaler' , StandardScaler()),( 'CART' ,\n    DecisionTreeClassifier())])))\npipelines.append(( 'ScaledNB' , Pipeline([( 'Scaler' , StandardScaler()),( 'NB' ,\n    GaussianNB())])))\npipelines.append(( 'ScaledSVM' , Pipeline([( 'Scaler' , StandardScaler()),( 'SVM' , SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","d17b517b":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Scaled Algorithm Comparison' )\nax = fig.add_subplot(111 )\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","b6c49fbf":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' ,  'poly' ,  'rbf' ,  'sigmoid' ]\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[ 'mean_test_score' ]\nstds = grid_result.cv_results_[ 'std_test_score' ]\nparams = grid_result.cv_results_[ 'params' ]\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","af84cc2d":"# ensembles\nensembles = []\nensembles.append(( 'AB' , AdaBoostClassifier()))\nensembles.append(( 'GBM' , GradientBoostingClassifier()))\nensembles.append(( 'RF' , RandomForestClassifier()))\nensembles.append(( 'ET' , ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)\n  \n  \n  \n  ","b8255a5d":"fig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Ensembles models' )\nax = fig.add_subplot(111 )\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","aa4f6d44":"X_t, y_t = augment(X, Y, t=20)","d13de7b8":"X_train, X_validation, Y_train, Y_validation = train_test_split(X_t, y_t,\n    test_size=validation_size, random_state=seed)","b2d6cedc":"models = []\nmodels.append(( 'LR' , LogisticRegression()))\nmodels.append(( 'LDA' , LinearDiscriminantAnalysis()))\nmodels.append(( 'KNN' , KNeighborsClassifier()))\nmodels.append(( 'CART' , DecisionTreeClassifier()))\nmodels.append(( 'NB' , GaussianNB()))\nmodels.append(( 'SVM' , SVC()))","95216eae":"results = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","f4b52f48":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","79712c4c":"pipelines = []\npipelines.append(( 'ScaledLR' , Pipeline([( 'Scaler' , StandardScaler()),( 'LR' ,\n    LogisticRegression())])))\npipelines.append(( 'ScaledLDA' , Pipeline([( 'Scaler' , StandardScaler()),( 'LDA' ,\n    LinearDiscriminantAnalysis())])))\n\npipelines.append(( 'ScaledKNN' , Pipeline([( 'Scaler' , StandardScaler()),( 'KNN' ,\n    KNeighborsClassifier())])))\npipelines.append(( 'ScaledCART' , Pipeline([( 'Scaler' , StandardScaler()),( 'CART' ,\n    DecisionTreeClassifier())])))\npipelines.append(( 'ScaledNB' , Pipeline([( 'Scaler' , StandardScaler()),( 'NB' ,\n    GaussianNB())])))\npipelines.append(( 'ScaledSVM' , Pipeline([( 'Scaler' , StandardScaler()),( 'SVM' , SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","07497e19":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Scaled Algorithm Comparison' )\nax = fig.add_subplot(111 )\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","d3038db6":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' ,  'poly' ,  'rbf' ,  'sigmoid' ]\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[ 'mean_test_score' ]\nstds = grid_result.cv_results_[ 'std_test_score' ]\nparams = grid_result.cv_results_[ 'params' ]\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","a673a153":"# ensembles\nensembles = []\nensembles.append(( 'AB' , AdaBoostClassifier()))\nensembles.append(( 'GBM' , GradientBoostingClassifier()))\nensembles.append(( 'RF' , RandomForestClassifier()))\nensembles.append(( 'ET' , ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n  kfold = KFold(n_splits=num_folds, random_state=seed)\n  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)\n  \n  ","fa2d05f7":"fig = plt.figure(figsize=(16, 8))\nfig.suptitle( 'Ensembles models' )\nax = fig.add_subplot(111 )\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","9e137e85":"GBM wins with more data","ed0ab560":"   ** 1. age**: The person's age in years.<br>\n   ** 2. sex**: The person's sex (1 = male, 0 = female)<br>\n   ** 3. cp**: The chest pain experienced (Value 0: typical angina, Value 1: atypical angina, Value 2: non-anginal pain, Value 3: asymptomatic)<br>\n   ** 4. trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital). <br>\n   ** 5. chol**: The person's cholesterol measurement in mg\/dl. <br>\n   ** 6. fbs**: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)<br>\n   ** 7. restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)<br>\n   ** 8. thalach**: The person's maximum heart rate achieved.<br>\n   ** 9. exang**: Exercise induced angina (1 = yes; 0 = no)<br>\n   ** 10. oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here).<br>\n   ** 11. slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)<br>\n   ** 12. ca**: The number of major vessels (0-3). <br>\n   ** 13. thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)<br>\n   ** 14. target**: Heart disease (0 = no, 1 = yes)<br>","de5936c8":"### restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)","01736383":"### slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)","a9a59eea":"Chest pain such as atypical angina and non-anginal pain tend to be present more in people with heart disease","59de0428":"## Male","a98c50ce":"## Variables definition","8989531b":"### Standardize the data","9458f694":"### chol: The person's cholesterol measurement in mg\/dl.","27866bb2":"### Correlations","37cf3042":"#### Sex variable","f42cc2ca":"### Age","e1c28eb7":"## Check the target","202f70ed":"People with heart disease tend to have a fasting blood sugar less than 120 mg\/dl","c242fcde":"### thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)","f0069aa4":"#### exang: Exercise induced angina (1 = yes; 0 = no)","167b89a6":"#### The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)","7bd42266":"Data augmentation has increased the accuracy of our model","a2d3b931":"### Standardize data","9ec85bda":"People with heart disease tend to have a ST-T wave abnormality","0d716677":"### Import libraries","e71c2edc":"## Train","3dbf852f":"### Read the dataset","98a50713":"### ca: The number of major vessels (0-3).","4467d660":"### Tuning SVM","6e7d426b":"### Tune SVM","4dde3cfc":"We have relatively more female with heart disease in the dataset. Does that mean that more women tend to have heart disease than men? ","335f7a97":"### Data augmentation","dbe9956c":"### thalach: The person's maximum heart rate achieved.","063f712c":"### oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot.","6d2a92a3":"Logistic regression still win","5dc4e10c":"## Female","c411930f":"Logistic regression seems to perform better than SVM. Let's try ensemble models","dc8341c5":"### Checking missing","e4a9e1ff":"SVM wins with more standardized data","865a987b":"### Exploratory data analysis","b39d7025":"### Utility functions","f9d72164":"#### cp variable: The chest pain experienced (Value 0: typical angina, Value 1: atypical angina, Value 2: non-anginal pain, Value 3: asymptomatic)","4c122c67":"### trestbps: The person's resting blood pressure (mm Hg on admission to the hospital). ","839a6481":"## Heart Disease analysis and model"}}