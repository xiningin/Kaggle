{"cell_type":{"ffb3cff3":"code","ec20593b":"code","fe67252c":"code","d9a21a6c":"code","f7f60795":"code","f2427a02":"code","c6138f42":"code","e7300724":"code","0a4aaa50":"code","21c2e065":"code","0fa088d9":"code","3b54a2df":"code","1e8afd3e":"code","2d17c9be":"code","5903d5cd":"code","8527b425":"code","98781b2c":"code","955a7dc9":"code","03551d4a":"code","5a713d2a":"code","3f4dac8e":"code","3f1102a5":"code","88a4f1fc":"code","d6a5523f":"code","2189ff9b":"code","4076286e":"code","e742b8c5":"code","1f137cd9":"code","4a7c0d76":"code","ef5942e2":"code","37fc4b0f":"code","b8340d5e":"code","48aa2677":"code","57db3688":"code","36727a5e":"code","f2cf86a4":"code","2e64eb3e":"code","d5393ba8":"code","0af81063":"code","356389ad":"code","a5030b2d":"code","de8fa5d2":"code","20b075f7":"code","ffa2650a":"code","e5511008":"code","f5d7d5d5":"code","2c793044":"code","cb0f83e8":"code","db73790f":"code","01b29b7d":"code","0dbf261b":"code","cca4eefc":"code","d7304061":"code","048f1473":"code","49ced02e":"code","b97f4350":"markdown","4d6fa9e2":"markdown","947113c3":"markdown","d8d1841f":"markdown","5f2766ce":"markdown","809f304d":"markdown","9fa19d68":"markdown","7d88eebb":"markdown","c1c68754":"markdown"},"source":{"ffb3cff3":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport glob","ec20593b":"import string\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom gensim import utils\nimport gensim.parsing.preprocessing as gsp\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score","fe67252c":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","d9a21a6c":"train2.toxic = train2.toxic.round().astype(int)\ntrain = pd.concat([train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000)\n    ])\n#rate=10\n#train = train[::rate]\ntrain.head()","f7f60795":"valid.head()","f2427a02":"test.head()","c6138f42":"print(\"Validation data set size:\",valid.shape)\nprint(\"Test data set size:\",test.shape)\nprint(\"Training data set size:\",train.shape)","e7300724":"def get_language(text):\n    return Detector(\"\".join(x for x in text if x.isprintable()),quiet=True).languages[0].name","0a4aaa50":"pip install pyicu","21c2e065":"pip install pycld2","0fa088d9":"from polyglot.detect import Detector\nfrom polyglot.utils import pretty_list","3b54a2df":"train['language'] = train[\"comment_text\"].apply(get_language)","1e8afd3e":"train.head()","2d17c9be":"train['toxic'].value_counts()","5903d5cd":"re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()\n \nfilters = [\n           gsp.strip_tags, #remove tags \n           gsp.strip_punctuation, #remove punctuation\n           gsp.strip_multiple_whitespaces, #standarized the spaces \n           gsp.strip_numeric,\n           gsp.remove_stopwords, #stop words  \n           gsp.strip_short, \n           gsp.stem_text #stemming \n          ]\n\ndef clean_text(s):\n    s = str(s).lower() \n    s = utils.to_unicode(s)\n    for f in filters:\n        s = f(s)\n    return s","8527b425":"train[train.language=='xx']","98781b2c":"#clean the text first \ntrain['comment_text'].fillna(\"unknown\", inplace=True)\ntrain[\"comment_text\"] = train[\"comment_text\"].apply(clean_text)","955a7dc9":"#vectorization of the model \nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize, strip_accents='unicode', use_idf=1,smooth_idf=1, sublinear_tf=1)\n\npipeline = Pipeline([\n    ('tfidf', vec),\n    ('logreg', LogisticRegression(penalty='elasticnet')),\n])\n\nparameters = {\n    'tfidf__max_features': [None, 1000, 5000, 50000],\n    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or unigrams + bigrams\n    'logreg__penalty' : ['l1', 'l2'],\n    'logreg__C' : np.logspace(-4, 4, 20),\n    'logreg__solver' : ['liblinear'],\n}","03551d4a":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nrandom_search = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, verbose=1,scoring=['roc_auc'],cv=cv, n_iter=10, refit='roc_auc')","5a713d2a":"%%time \n\nrandom_search.fit(train[\"comment_text\"], train['toxic']);","3f4dac8e":"pd.DataFrame(random_search.cv_results_).sort_values('mean_test_roc_auc', ascending=False)","3f1102a5":"#valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nvalid['comment_text'].fillna(\"unknown\", inplace=True)\nvalid[\"comment_text\"] = valid[\"comment_text\"].apply(clean_text)\n\n#score the validation dataset\ny_valid = valid['toxic']\ny_pred_valid = random_search.best_estimator_.predict_proba(valid[\"comment_text\"])\n# print('Testing accuracy %s' % accuracy_score(y_valid, y_pred_valid))\n# print('Testing F1 score: {}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\nprint('Validation AUC score %s' % roc_auc_score(y_valid, y_pred_valid[:, 1]))","88a4f1fc":"test[\"comment_text\"] = test[\"content\"].apply(clean_text)\n\n#score the submission file \ny_pred = random_search.best_estimator_.predict_proba(test[\"comment_text\"])\n\n#load the sample submission file \nsample_sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\n\nsubmid = pd.DataFrame({'id': sample_sub[\"id\"]})\nsubmission = pd.concat([submid,test.comment_text, pd.DataFrame(y_pred[:, 1],columns=['toxic'])], axis=1)\nsubmission.head()","d6a5523f":"submission.toxic = submission.toxic.round().astype(int)","2189ff9b":"submission.toxic.value_counts()","4076286e":"submission.head(10)","e742b8c5":"!pip install -U transformers","1f137cd9":"import seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nimport tokenizers\nfrom tokenizers import BertWordPieceTokenizer","4a7c0d76":"def clean_text(text):\n    text = str(text)\n    text = re.sub(r'[0-9\"]', '', text) # number\n    text = re.sub(r'#[\\S]+\\b', '', text) # hash\n    text = re.sub(r'@[\\S]+\\b', '', text) # mention\n    text = re.sub(r'https?\\S+', '', text) # link\n    text = re.sub(r'\\s+', ' ', text) # multiple white spaces\n#     text = re.sub(r'\\W+', ' ', text) # non-alphanumeric\n    return text.strip()\n\ndef text_process(text):\n    ws = text.split(' ')\n    if(len(ws)>160):\n        text = ' '.join(ws[:160]) + ' ' + ' '.join(ws[-32:])\n    return text\n\ndef fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    print('encoding with', tokenizer)\n    \n    # for transformers 3.5\n    if isinstance(tokenizer, transformers.DistilBertTokenizer) or \\\n        isinstance(tokenizer, transformers.DistilBertTokenizerFast):\n    #     tokenizer.enable_truncation(max_length=maxlen)\n    #     tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i:i+chunk_size].tolist()\n    #         encs = tokenizer.encode_batch(text_chunk)\n            encs = tokenizer(text_chunk, padding='max_length', truncation=True, max_length=maxlen)\n    #         all_ids.extend([enc.ids for enc in encs])\n            all_ids.extend(encs['input_ids']) \n    elif isinstance(fast_tokenizer, tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer): \n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i:i+chunk_size].tolist()\n            encs = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([enc.ids for enc in encs])\n\n    \n    return np.array(all_ids)","ef5942e2":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n\nsave_path = '\/kaggle\/working\/distilbert_base_cased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\nfast_tokenizer = tokenizer\n\n# \"faster as the tokenizers from transformers because they are implemented in Rust.\"\n# fast_tokenizer = BertWordPieceTokenizer('distilbert_base_cased\/vocab.txt', lowercase=False)","37fc4b0f":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b8340d5e":"# Configuration\nAUTO = tf.data.experimental.AUTOTUNE\nSHUFFLE = 2048\nEPOCHS1 = 20\nEPOCHS2 = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nVERBOSE = 2","48aa2677":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","57db3688":"train2.toxic = train2.toxic.round().astype(int)\ntrain = pd.concat([train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000)\n    ])\n#rate=10\n#train = train[::rate]\ntrain.head()","36727a5e":"def len_sent(data):\n    return len(data.split())\ntrain[\"num_words_comment_text\"] = train[\"comment_text\"].apply(lambda x : len_sent(x))\n#sns.kdeplot(train[train[\"toxic\"] == 0][\"num_words_comment_text\"].values, shade = True, color = \"red\", label='non_toxity')\n#sns.kdeplot(train[train[\"toxic\"] == 1][\"num_words_comment_text\"].values, shade = True, color = \"blue\", label='toxity')\n\ny_train = train['toxic'].values\n\n","f2cf86a4":"train.head()","2e64eb3e":"del train['toxic']; gc.collect()","d5393ba8":"train['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))\ntrain['comment_text'] = train['comment_text'].apply(lambda x: text_process(x))\nx_train = fast_encode(train['comment_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)","0af81063":"\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(SHUFFLE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ndel x_train; gc.collect()\n\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nvalid['comment_text'] = valid.apply(lambda x: clean_text(x['comment_text']), axis=1)\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: text_process(x))\nx_valid = fast_encode(valid['comment_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\ny_valid = valid['toxic'].values\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ndel x_valid; gc.collect()","356389ad":"lrs = ReduceLROnPlateau(monitor='val_auc', mode ='max', factor = 0.7, min_lr= 1e-7, verbose = 1, patience = 2)\nes1 = EarlyStopping(monitor='val_auc', mode='max', verbose = 1, patience = 5, restore_best_weights=True)\nes2 = EarlyStopping(monitor='auc', mode='max', verbose = 1, patience = 1, restore_best_weights=True)\ncallbacks_list1 = [lrs,es1]\ncallbacks_list2 = [lrs,es2]","a5030b2d":"def build_model(transformer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = tf.keras.layers.Dropout(0.4)(cls_token)\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy'])\n    \n    return model","de8fa5d2":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","20b075f7":"# not train on order to save memory\nn_steps = len(y_train) \/\/ (BATCH_SIZE*8)\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS1,\n    callbacks=callbacks_list1,\n    verbose=VERBOSE\n)\n\ndel train_dataset; gc.collect()","ffa2650a":"train_history_df = pd.DataFrame.from_dict(train_history.history)\ntrain_history_df","e5511008":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10, 5))\nplt.plot(train_history_df['val_auc'], label='val_auc')\nplt.plot(train_history_df['auc'], label='auc')\nplt.legend(fontsize=15)","f5d7d5d5":"plt.figure(figsize=(10, 5))\nplt.plot(train_history_df['accuracy'], label='accuracy')\nplt.plot(train_history_df['val_accuracy'], label='val_accuracy')\nplt.legend(fontsize=15)","2c793044":"n_steps = len(y_valid) \/\/ (BATCH_SIZE)\n\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS2,\n    callbacks=callbacks_list2,\n    verbose=VERBOSE\n)","cb0f83e8":"train_history2_df = pd.DataFrame.from_dict(train_history_2.history)\ntrain_history2_df","db73790f":"plt.figure(figsize=(10, 5))\nplt.plot(train_history2_df['loss'], label='loss')\nplt.plot(train_history2_df['auc'], label='auc')\nplt.plot(train_history2_df['accuracy'], label='accuracy')\nplt.legend(fontsize=15)","01b29b7d":"x_test = fast_encode(test['content'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","0dbf261b":"sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsub['toxic'] = model.predict(test_dataset, verbose=1)","cca4eefc":"test[\"comment_text\"] = test[\"content\"].apply(clean_text)","d7304061":"sub[\"comment_text\"] = test.comment_text\nsub.toxic = sub.toxic.round().astype(int)","048f1473":"sub.head(10)","49ced02e":"sub.toxic.value_counts()","b97f4350":"## TPU Config","4d6fa9e2":"## Callbacks","947113c3":"## Build Model","d8d1841f":"# DistilBERT","5f2766ce":"## Run Model","809f304d":"### Validation dataset fitting\n- With AUC model","9fa19d68":"## ROC AUC","7d88eebb":"## Load model in TPU","c1c68754":"# Logistic Regression"}}