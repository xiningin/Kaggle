{"cell_type":{"d50947eb":"code","914311d9":"code","0b0bf3db":"code","29137632":"code","1e114484":"code","3e09f160":"code","3d6e6483":"code","d4a51456":"code","e917bc0e":"code","2fab113c":"code","bfcb99bf":"code","211e4be1":"code","d054cabc":"code","c6842f61":"code","9bdd8fb4":"code","6cfc16c3":"code","371145cc":"code","d672d337":"code","adf43905":"code","fc5ee527":"code","09bb2a8f":"code","e586c95c":"code","bb360e00":"code","d83d5907":"code","be5accfd":"code","eb42c9f9":"code","822ed525":"code","34583cc0":"code","20511c77":"code","c8413bc2":"code","eaad210e":"code","bf7cc086":"code","48f6c008":"code","39b89a94":"code","8a036816":"code","147f2ebc":"code","90a9f26f":"code","1c737c08":"code","c6bc0d83":"code","b661dc33":"code","600bc53f":"code","8ed28a13":"code","53dbf4cd":"code","6e163dc3":"code","89c8f8ff":"code","a393956b":"code","601f96ba":"code","3174d872":"code","58f7cec7":"code","a6b72ce3":"code","abe60942":"code","19eb7042":"code","2c6a5d6b":"code","49a25a2e":"markdown","9c032193":"markdown","e07a2d4f":"markdown","26a4322e":"markdown","1bf6a226":"markdown","a6829c2c":"markdown","aec39ba1":"markdown","b82a5547":"markdown","a61f39e1":"markdown","6feb7f18":"markdown","00c1dcf4":"markdown","00becd51":"markdown","86c04cbc":"markdown","58aae0a2":"markdown","5efa933b":"markdown","8d418df1":"markdown","d29a39f6":"markdown","1a33bdd7":"markdown","7d08466a":"markdown","f74cf601":"markdown","208ae4de":"markdown","2d7a860b":"markdown","51e501ff":"markdown","98f49a9d":"markdown","f6827a13":"markdown","4e214e6b":"markdown","8dff8d68":"markdown","3494256b":"markdown","52bb7d3e":"markdown","8dfc2e96":"markdown","aa90451f":"markdown","108d3550":"markdown","e0c44267":"markdown","5db2c304":"markdown","93f9246d":"markdown","fd889dd9":"markdown","d03881ad":"markdown","ac136a90":"markdown","524e6d48":"markdown","8bc62fb9":"markdown","76224cd2":"markdown","4fc54840":"markdown","4856aec7":"markdown","93dbe00f":"markdown","949a1eba":"markdown"},"source":{"d50947eb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","914311d9":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head(n = 20)","0b0bf3db":"train.shape","29137632":"train.isnull().sum()","1e114484":"sns.heatmap(train.isnull(),  yticklabels = False, cmap = 'viridis')","3e09f160":"x = train['Survived'].value_counts()\nlabel = ['Not Survived.', 'Survived']\nfig, axarr = plt.subplots()\nplt.pie(x, startangle = 90, explode = [0, 0.1], labels = label, colors = ['lightblue', 'gold'], autopct = '%0.01f')    \naxarr.set_title('balanced or imbalaced?')","3d6e6483":"sns.countplot(x = 'Survived',hue = 'Sex', data = train, palette  = 'RdBu_r')","d4a51456":"sns.countplot(x = 'Survived', hue = 'Pclass', data = train, palette = 'rainbow')","e917bc0e":"figure = plt.figure(figsize=(7.6,6))\nplt.hist([train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], stacked=True, color = ['g','r'],\n         bins = 30,label = ['Survived','Dead'])\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.legend()","2fab113c":"plt.figure(figsize = (12,6))\nsns.countplot(x = \"SibSp\", data = train, hue= 'Survived')\nplt.legend(loc = 'upper right')","bfcb99bf":"plt.figure(figsize = (14,6))\nsns.countplot(x = 'Parch', data = train, hue = 'Survived')\nplt.legend(loc = 'upper right')","211e4be1":"plt.figure(figsize = (14,6))\nsns.distplot(train['Fare'], color = 'red')","d054cabc":"train.columns","c6842f61":"X = train.drop([\"Survived\", 'PassengerId', 'Name', 'Ticket','Cabin',  'Embarked'], axis = 1)\ny = train.iloc[:,1].values","9bdd8fb4":"X","6cfc16c3":"y","371145cc":" X = X.values # converting dataframe into numpy array cause imputer don't support Panda DataFrame","d672d337":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values= np.nan, strategy='mean')\nX[:,2:3] = imputer.fit_transform(X[:,2:3])\nprint(X)","adf43905":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:,1] = le.fit_transform(X[:,1])\n","fc5ee527":"print(X)","09bb2a8f":"\nfrom sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)","e586c95c":"\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","bb360e00":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\n# predict the test set\ny_pred = lr.predict(X_test)\n\n#  Evaluate the performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred))","d83d5907":"from sklearn.model_selection import cross_val_score\ncrs = cross_val_score(estimator = lr, X = X_train, y = y_train, cv = 10)\nprint('accuracy cross_val_score: {}'.format(crs.mean()))\nprint('train accuracy: {}'.format(lr.score(X_train,y_train)))","be5accfd":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ndt.fit(X_train,y_train)\n\n# predict the test set\ny_pred = dt.predict(X_test)\n\n#  Evaluate the performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred))","eb42c9f9":"from sklearn.model_selection import cross_val_score\ncrs = cross_val_score(estimator = dt, X = X_train, y = y_train, cv = 5)\nprint('accuracy cross_val_score: {}'.format(crs.mean()))\nprint('train accuracy: {}'.format(lr.score(X_train,y_train)))","822ed525":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n\nrf.fit(X_train,y_train)\n\n# predict the test set\ny_pred = rf.predict(X_test)\n\n # Evaluate the performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred))","34583cc0":"from sklearn.model_selection import cross_val_score\ncrs = cross_val_score(estimator = rf, X = X_train, y = y_train, cv = 5)\nprint('accuracy cross_val_score: {}'.format(crs.mean()))\nprint('train accuracy: {}'.format(lr.score(X_train,y_train)))","20511c77":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    score = knn.predict(X_test)\n    error_rate.append(1-score.mean())","c8413bc2":"plt.figure(figsize = (20,6))\nplt.plot(range(1,40), error_rate, color = 'blue', marker = 'o', linestyle = 'dashed',markerfacecolor = 'red', linewidth  =2, markersize = 10)","eaad210e":"knn = KNeighborsClassifier(n_neighbors = 7,metric='minkowski', p =2)\nknn.fit(X_train, y_train)\n\n# predict the test set\ny_pred = knn.predict(X_test)\n\n#  Evaluate the performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred))","bf7cc086":"from sklearn.model_selection import cross_val_score\ncrs = cross_val_score(estimator = dt, X = X_train, y = y_train, cv = 5)\nprint('accuracy cross_val_score: {}'.format(crs.mean()))\nprint('train accuracy: {}'.format(lr.score(X_train,y_train)))","48f6c008":"from sklearn.svm import SVC\nclassifier = SVC()\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","39b89a94":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf', C = 1, gamma= 0.1)\nsvc.fit(X_train,y_train)\n\n# predict the test set\ny_pred = svc.predict(X_test)\n\n# Evaluate the performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred))","8a036816":"from sklearn.model_selection import cross_val_score\ncrs = cross_val_score(estimator = svc, X = X_train, y = y_train, cv = 5)\nprint('accuracy cross_val_score: {}'.format(crs.mean()))\nprint('train accuracy: {}'.format(lr.score(X_train,y_train)))","147f2ebc":"testdf  = pd.read_csv('..\/input\/titanic\/test.csv')","90a9f26f":"testdf.head()","1c737c08":"X1  = testdf.drop(['PassengerId', 'Name','Ticket','Cabin', 'Embarked'],axis = 1)","c6bc0d83":"X1.isnull().sum()","b661dc33":"X1","600bc53f":" X1 = X1.values  #converting dataframe into numpy array cause imputer don't support Panda DataFrame.","8ed28a13":"imputer  = SimpleImputer()\nX1[:,2:3]  = imputer.fit_transform(X1[:,2:3])\n\nX1[:,5:6] = imputer.fit_transform(X1[:,5:6])","53dbf4cd":"X1","6e163dc3":"X1[:,1] = le.fit_transform(X1[:,1])\n","89c8f8ff":"X1","a393956b":"pd.DataFrame(X1).isnull().sum()","601f96ba":"X1  = sc.fit_transform(X1)","3174d872":"X1","58f7cec7":"predicted_survived = svc.predict(X1)","a6b72ce3":"predicted_survived","abe60942":"submission = pd.DataFrame({\n    'PassengerId': testdf['PassengerId'],\n    'Survived': predicted_survived\n})","19eb7042":"submission","2c6a5d6b":"submission.to_csv('titanic_submission.csv',index = False)","49a25a2e":"## Applying best model  on test.csv dataset","9c032193":"## Modeling","e07a2d4f":"#### importing libraries","26a4322e":"- Great! now we can submit this result to the kaggle __Submit Predictions__.","1bf6a226":"### Saving dataset ","a6829c2c":"- Here above the figure we can see that person they didn't have  parents or childs their survival or death frequency higher compare to person they had childs or parents.","aec39ba1":"- how many Number of Parents\/Children Survived or not","b82a5547":"## knn","a61f39e1":"### Svm ","6feb7f18":"#### Handling the categorical values on test.csv","00c1dcf4":"#### dropping the unnecessary column for test dataframe","00becd51":"- Here in above heatmap yellow values are the nan or missing values in '__Age__' & '__Cabin__' column.\n- Cabin have more missing values.\n- Age have some missing values.","86c04cbc":"#### Survived vs Pclass Column","58aae0a2":"### Decision Tree","5efa933b":"#### Feature Scaling on test.csv","8d418df1":"### RandomForest","d29a39f6":"- 0  maximun no.  of people didn't  have any  sibling or spouse.\n- 1 sibling or spouse\n- 2 sibling or spouse.\n- 3 sibling or spouse\n- similary 4, 5, 6 sibling or spouse.","1a33bdd7":"##### distribution of Age","7d08466a":"#### balanced or imbalaced?","f74cf601":"- Now this save csv file we can submit to kaggle __Submit Predictions__. and we can check  our performance and ranking.","208ae4de":"#### Loading Dataset","2d7a860b":"#### Checking the missing values","51e501ff":"- Here above the result we can see that in '__Age__' column there are __177__ missing values, '__cabin__' column have __687__ missing values\nand '__Embarked__' have __2__ missing values.","98f49a9d":"#### Handling the missing values on train dataframe","f6827a13":"##  EDA","4e214e6b":"### Logistic Regression","8dff8d68":"#### handling the categorical data on train  dataset","3494256b":"####  Survived vs Parch(parent\/child)","52bb7d3e":"#### checking missing values using heat map","8dfc2e96":"- Here in fare Column we have 1 missing values \n- And Age Column have 86 missing values","aa90451f":"#### Applynig SVM dataset","108d3550":"#### splitting  train.csv  into train and test set","e0c44267":"#### Applying support vector machine on test.csv and predicting the dataset","5db2c304":"#### Handling the missing values  on test.csv set","93f9246d":"#### faeture scaling  on train.csv dataset","fd889dd9":"# Kaggle's Titanic Competition: Machine Learning from Disaster","d03881ad":"##### predicted values","ac136a90":"#### Handling the missing values on train set","524e6d48":"#### Applying grid search to find  out the best parameters for svm","8bc62fb9":"## Data preparing","76224cd2":"##### shape of train dataset","4fc54840":"- We can create simple heatmap & visualize that how many missing value in our dataset.","4856aec7":"#### Distirbution of Fare","93dbe00f":"#### Survived vs Sibsp Column","949a1eba":"#### Survived vs Sex Column"}}