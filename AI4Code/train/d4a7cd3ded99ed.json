{"cell_type":{"6e7f62c9":"code","1c5650c4":"code","0835f331":"code","3d8deb2f":"code","e5d59d90":"code","3cec7d76":"code","f3e0dd3f":"code","cd06faa3":"code","1bc07d57":"code","d6884f86":"code","62fe133b":"markdown","5667fdff":"markdown","089e4a14":"markdown","76078278":"markdown","e0fef61d":"markdown","fac8d02c":"markdown","137f3d68":"markdown","393a6192":"markdown","fb0cc5a7":"markdown","032eb841":"markdown","b41b907c":"markdown","5848be74":"markdown","895e367d":"markdown","58a674b5":"markdown","bfe7e8d1":"markdown","e28cb6d2":"markdown","046f6ed7":"markdown","b328e5ea":"markdown","e7aef7df":"markdown"},"source":{"6e7f62c9":"#pip install mlxtend","1c5650c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.metrics import accuracy_score\nfrom mlxtend.plotting import plot_decision_regions\nimport random\nimport scipy.stats\n\n#Classifiers\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Dataset\nfrom mlxtend.data import iris_data","0835f331":"#Load Data, only using petal and sepal length\ndf = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\n\ny = df['Species'].to_numpy()\nX = df[['SepalLengthCm','PetalLengthCm']].to_numpy()\n\n# Treatment for encoding from nominal to ordinal\nseries = pd.Series(y)\n\nmapper = {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}\nordvar = series.replace(mapper)\n\ny = pd.factorize(ordvar)[0]\n\n\n#Divide data into Train and Validation\nc = list(zip(X, y))\nrandom.shuffle(c)\nX, y = zip(*c)\nX, y = np.array(list(X)), np.array(list(y))\n\nsplit_amount = int(len(X)*0.2)\n\nX_test = X[:split_amount]\nX_train = X[split_amount:]\n\ny_test = y[:split_amount]\ny_train = y[split_amount:]","3d8deb2f":"# Initializing Classifiers\nclf1 = SVC(kernel='linear')\nclf2 = SVC(kernel='rbf')\nclf3 = LinearDiscriminantAnalysis()\nclf4 = KNeighborsClassifier(n_neighbors=3)\nclf5 = Perceptron()\n\nlabels = ['SVM linear',\n          'SVM RBF',\n          'LinearDiscriminantAnalysis',\n          'KNN',\n          'Perceptron']","e5d59d90":"# Plotting Decision Regions\ngs = gridspec.GridSpec(2, 3)\nfig = plt.figure(figsize=(10, 8))\npreds = []\n                       \nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5],\n                         labels,\n                         [(0,0), (0,1), (1,0), (1,1), (0,2)]):\n                       \n    clf.fit(X_train, y_train) #FIT model\n    preds.append(np.array(clf.predict(X_test))) #MakePredictions\n\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_train, y=y_train,\n                                clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()","3cec7d76":"def f_test(y_true, *y_models):\n    n = len(y_true)\n    m = len(y_models)\n\n    avgACC = sum([accuracy_score(y_true, y) for y in y_models]) \/ m\n\n    gjSum = sum([((y_true == ypred).sum() \/ n)**2 for ypred in y_models])\n    ssa = n * gjSum - n * m * avgACC**2\n\n    mjSum = 0\n    for i in range(n):\n        y = y_true[i]\n        count = 0\n        for ypred in y_models:\n            if y == ypred[i]:\n                count += 1\n        mjSum += count**2\n    ssb = 1\/m * mjSum - m * n * avgACC**2\n\n    sst = m * n * avgACC*(1-avgACC)\n\n    ssab = sst - ssa - ssb\n\n    msa = ssa \/ (m-1)\n\n    msab = ssab \/ ((m-1) * (n-1))\n\n    f = msa \/ msab\n\n    degrees_of_freedom_1 = m - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * n\n\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n\n    return f, p_value","f3e0dd3f":"def nullHyp(p_value):\n    if p_value < 0.05:   #significance level \u03b1=0.05\n        print(\"Rejected the null hypothesys\") #there is a difference between the classification accuracies\n    else:\n        print(\"Accepted the null hypothesys\") #There is no difference between the classification accuracies","cd06faa3":"#TRY WITHOUT PECEPTRON\n\nf, p_value = f_test(y_test, preds[0],preds[1],preds[2],preds[3])\nprint('F: %.3f' % f)\nprint('p-value: %.3f' % p_value)\nnullHyp(p_value)","1bc07d57":"#TRY WITH PECEPTRON\n\nf, p_value = f_test(y_test, preds[0],preds[1],preds[2],preds[3], preds[4])\nprint('F: %.3f' % f)\nprint('p-value: %.3f' % p_value)\nnullHyp(p_value)","d6884f86":"from mlxtend.evaluate import combined_ftest_5x2cv\n\nscore1 = clf1.fit(X, y).score(X_test, y_test)\nprint('Linear SVM accuracy: %.2f%%' % (score1*100))\n\nscore2 = clf5.fit(X, y).score(X_test, y_test)\nprint('Perceptron accuracy: %.2f%%\\n' % (score2*100))\n\n# Here we get the values for the whole X and Y, pre-split\nf, p = combined_ftest_5x2cv(estimator1=clf1,\n                            estimator2=clf5,\n                            X=X, y=y,\n                            random_seed=1)\n\nprint('F statistic: %.3f' % f)\nprint('p value: %.3f' % p)\nnullHyp(p)","62fe133b":"### Evaluation analysis","5667fdff":"The 5x2cv combined F test is a procedure for comparing the performance of two models (classifiers or regressors) in which we repeat the splitting of the train\/test data 5 times.\n\nIn each of the 5 iterations, we fit A and B to the training split and evaluate their performance (pA and pB) on the test split. Then, we rotate the training and test sets (the training set becomes the test set and vice versa) compute the performance again, which results in 2 performance difference measures. Then it is paired with the F-test we discussed above and gives results for the F statistic and P value, which allows us to accept or reject the null hypothesis","089e4a14":"### Including the Perceptron","76078278":"### Import dependencies and classifiers","e0fef61d":"## F-Test\n\nAn F-test is a statistical test in which the test statistic has an F-distribution under the null hypothesis.  \nIt is most often used when comparing statistical models that have been fitted to a data set.  \n\nProcedure:  \n{C1, . . . , CM} is a set of classifiers which have all been tested on the same dataset.   \n- m: number of classifiers  \n- n: number of examples in the test set\n\nF statistic is distributed according to an F distribution\nwith (M \u2212 1) and (M \u2212 1) \u00d7 n degrees of freedom.\n\nF-test described [here](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/evaluate\/ftest\/)\n\nWith the F-value, we can then look up the p-value from an F-distribution table for the\ncorresponding degrees of freedom or obtain it computationally from a cumulative F-distribution\nfunction.","fac8d02c":"As we can observe from the plots, apart from the perceptron, these models have **high accuracy** and can accurately predict the regions of said classes, even though the dataset isn't very large.\n\nThe **perceptron** fails to classify the majority of the examples correctly.","137f3d68":"# The F-test for Comparing Multiple Classifiers","393a6192":"As we can see, the perceptron gives off the worst results, since its accuracy to the testing data is only around 40%, while the others are in the range of 92-97% accuracy","fb0cc5a7":"The method of using the F-test for comparing two classifiers is somewhat loosely based on Looney's work.\n\nThus method can be used to compare **two** or **more** classifiers. And, in the context of the F-test, our null hypothesis is that there that there is no difference between the classification accuracies\n\nA null **hypothesis** is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population.\nIn this case, we are trying to prove that there is no difference between the classification accuracies of the multiple classifiers, therefore, proving the null hypothesis.","032eb841":"### Excluding the Perceptron","b41b907c":"## Plotting the data\n\nTo have a better understanding on the similarity of the models we included a plot on the several trained models","5848be74":"### Load data\n\nFor this work, we use the Iris Dataset with **150 examples** and **3 classes** for classification. This is a very common dataset to be used in small testing.\nWe only kept **2 features** so we could predict from them.\n\nSince these scores are based off a normal test\/train split of the data, we shuffle our data, and then **split 80% for training and 20% for testing**.\n\nWe used k-fold in previous iterations but the recommended methods for comparing model and algorithm is one of the two:  \n - **Combined 5x2cv F test**\n - **Nested cross validation**\n    \nAnd therefore, we use the combined 5x2cv F test in a future cell, ahead.","895e367d":"## Requirements","58a674b5":"## Initialization of the classifiers\n\nTo test the different classifiers, we assign each one of the following to a variable and then compare them visually, to better visualize the regions they classify. Since the data is simple to observe and only classifies into one of 3 classes, the visualization of the result is easy to assess.  \n\nThese are the five classifiers used:\n - **SVM Linear**\n - **SVM RBF**\n - **Linear Discriminant**\n - **KNN**\n - **Perceptron**","bfe7e8d1":"## Data Preprocessing","e28cb6d2":"Assuming a significance level \u03b1=0.05, we can test the null hypothesis:  \n**There is no difference between the classification accuracies of the models**.\n\nIf the p-value is smaller than \u03b1, we can reject the null hypothesis and conclude that there is a difference between the classification accuracies.","046f6ed7":"### Evaluate two models with 5x2cv combined F-test\n\nAfter computing the F-value, we can then look up the p-value from a F-distribution table for the corresponding degrees of freedom or obtain it computationally from a cumulative F-distribution function. In practice, if we successfully rejected the null hypothesis at a previously chosen significance threshold (0.05), we could perform multiple post hoc pair-wise tests.\n\nSo, as we rejected the models evaluation when the **Perceptron** was included, we could evaluate them pair-wise and discover what pair or pairs was the \"outlier\". Since we already know it is the **Perceptron**, we also performed the 5x2cv combined F-test with the **Perceptron** and the **Linear SVM**","b328e5ea":"## Models evaluation","e7aef7df":"As we can see (most of the times), in the comparison between the first 4 classifiers, we accepted the null hypothesys, meaning that there is no important difference between the classification accuracys of the models.\n\nHowever, when we compare Peceptron as well, which didnt fit the data and has a bad accuracy, we can reject the null hypothesys, meaning that the models have significant difference in classification accuracys."}}