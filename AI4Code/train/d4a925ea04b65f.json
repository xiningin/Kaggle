{"cell_type":{"c355edb9":"code","5a14dcf3":"code","f67042fd":"code","936961ec":"code","81b89a04":"code","54e5c497":"code","6dbe8feb":"code","125cf513":"code","bf65bb05":"code","fbf23924":"code","c0bec43d":"code","961c2fa7":"code","c0ad5885":"code","e4f3953f":"code","0cd424e1":"code","cf3dec58":"code","70712de9":"code","8e6fa06f":"code","2349327e":"code","9e7884bc":"code","e00b8d62":"code","1aba7bb1":"code","b999c905":"code","f5ab7cf7":"code","146457ec":"code","0a624ffa":"code","97c71dcc":"code","2d6db18f":"code","dc393096":"code","024c020a":"code","288f14c4":"code","685e800b":"code","b6a1d6f6":"code","6fbdcea5":"code","c85794ca":"code","2fac788a":"code","9aa4731b":"code","64080b46":"code","5ccc6b3d":"code","88bce7ce":"code","d3a70ec2":"code","80104f5f":"code","22df603b":"code","215d8f83":"code","b51b8666":"code","aee9eac1":"code","5cf802b5":"code","ce93d772":"code","9a5e7993":"markdown","a572b5ec":"markdown","17000100":"markdown","913a2a1c":"markdown","8f4df837":"markdown","e225110b":"markdown","5d7109b8":"markdown","f04fdb59":"markdown","d5c81e0d":"markdown","25a85614":"markdown"},"source":{"c355edb9":"#we'll try to solve the problem of house prices\n#first we'll clean the data\n#then we'll use Machine Learning regression algorithmes to predict the selling price of a house based on 79 characteristics\n\n#Let's load the required libraries\n\nimport numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5a14dcf3":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","f67042fd":"train=train.drop(index=[523,1298],axis=0)","936961ec":"test=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","81b89a04":"print('th train data has {} rows and {} features'.format(train.shape[0],train.shape[1]))\nprint('the test data has {} rows and {} features'.format(test.shape[0],test.shape[1]))","54e5c497":"data=pd.concat([train.iloc[:,:-1],test],axis=0)\nprint('tha data has {} rows and {} features'.format(data.shape[0],data.shape[1]))","6dbe8feb":"data.columns","125cf513":"data.info()","bf65bb05":"num_features=data.select_dtypes(include=['int64','float64'])\ncategorical_features=data.select_dtypes(include='object')","fbf23924":"num_features.describe()","c0bec43d":"categorical_features.describe()","961c2fa7":"data.isnull().sum().sort_values(ascending=False)[:34]","c0ad5885":"print(categorical_features.isnull().sum().sort_values(ascending=False)[:23])","e4f3953f":"print(num_features.isnull().sum().sort_values(ascending=False)[:11])","0cd424e1":"f = open(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\", \"r\")\nprint(f.read())","cf3dec58":"data = data.drop(columns=['Id','Street','PoolQC','Utilities'],axis=1)","70712de9":"data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","8e6fa06f":"data['LotFrontage'].isnull().sum()","2349327e":"#create a new class 'other'\nfeatures=['Electrical','KitchenQual','SaleType','Exterior2nd','Exterior1st','Alley','Fence', 'MiscFeature','FireplaceQu','GarageCond','GarageQual','GarageFinish','GarageType','BsmtCond','BsmtExposure','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']\nfor name in features:\n    data[name].fillna('Other',inplace=True)","9e7884bc":"data[features].isnull().sum()","e00b8d62":"data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","1aba7bb1":"data['Functional']=data['Functional'].fillna('typ')","b999c905":"\"\"\"mode=['Electrical','KitchenQual','SaleType','Exterior2nd','Exterior1st']\nfor name in mode:\n    data[name].fillna(data[name].mode()[0],inplace=True)\"\"\"","f5ab7cf7":"zero=['GarageArea','GarageYrBlt','MasVnrArea','BsmtHalfBath','BsmtHalfBath','BsmtFullBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageCars']\nfor name in zero:\n    data[name].fillna(0,inplace=True)","146457ec":"data.isnull().sum().sum()","0a624ffa":"data.loc[data['MSSubClass']==60, 'MSSubClass']=0\ndata.loc[(data['MSSubClass']==20)|(data['MSSubClass']==120), 'MSSubClass']=1\ndata.loc[data['MSSubClass']==75, 'MSSubClass']=2\ndata.loc[(data['MSSubClass']==40)|(data['MSSubClass']==70)|(data['MSSubClass']==80), 'MSSubClass']=3\ndata.loc[(data['MSSubClass']==50)|(data['MSSubClass']==85)|(data['MSSubClass']==90)|(data['MSSubClass']==160)|(data['MSSubClass']==190), 'MSSubClass']=4\ndata.loc[(data['MSSubClass']==30)|(data['MSSubClass']==45)|(data['MSSubClass']==180), 'MSSubClass']=5\ndata.loc[(data['MSSubClass']==150), 'MSSubClass']=6","97c71dcc":"object_features = data.select_dtypes(include='object').columns\nobject_features","2d6db18f":"def dummies(d):\n    dummies_df=pd.DataFrame()\n    object_features = d.select_dtypes(include='object').columns\n    for name in object_features:\n        dummies = pd.get_dummies(d[name], drop_first=False)\n        dummies = dummies.add_prefix(\"{}_\".format(name))\n        dummies_df=pd.concat([dummies_df,dummies],axis=1)\n    return dummies_df","dc393096":"dummies_data=dummies(data)\ndummies_data.shape","024c020a":"data=data.drop(columns=object_features,axis=1)\ndata.columns","288f14c4":"final_data=pd.concat([data,dummies_data],axis=1)\nfinal_data.shape","685e800b":"#Re-spliting the data into train and test datasets\ntrain_data=final_data.iloc[:1458,:]\ntest_data=final_data.iloc[1458:,:]\nprint(train_data.shape)\ntest_data.shape","b6a1d6f6":"# X: independent variables and y: target variable\nX=train_data\ny=train.loc[:,'SalePrice']","6fbdcea5":"from sklearn.linear_model import Ridge, RidgeCV, LassoCV, ElasticNet","c85794ca":"model_las_cv = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nmodel_las_cv.fit(X,y)\nlas_cv_preds=model_las_cv.predict(test_data)","2fac788a":"model_ridge_cv = RidgeCV(alphas=(0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nmodel_ridge_cv.fit(X, y)\nridge_cv_preds=model_ridge_cv.predict(test_data)","9aa4731b":"model_ridge = Ridge(alpha=10, solver='auto')\nmodel_ridge.fit(X, y)\nridge_preds=model_ridge.predict(test_data)","64080b46":"model_en = ElasticNet(random_state=1, alpha=0.00065, max_iter=3000)\nmodel_en.fit(X, y)\nen_preds=model_en.predict(test_data)","5ccc6b3d":"import xgboost as xgb","88bce7ce":"model_xgb = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\nmodel_xgb.fit(X, y)\nxgb_preds=model_xgb.predict(test_data)","d3a70ec2":"from sklearn.ensemble import GradientBoostingRegressor","80104f5f":"model_gbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber', \n                                random_state =42)\nmodel_gbr.fit(X, y)\ngbr_preds=model_gbr.predict(test_data)","22df603b":"from lightgbm import LGBMRegressor","215d8f83":"model_lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\nmodel_lgbm.fit(X, y)\nlgbm_preds=model_lgbm.predict(test_data)","b51b8666":"final_predictions = 0.3 * lgbm_preds + 0.3 * gbr_preds + 0.1 * xgb_preds + 0.3 * ridge_cv_preds","aee9eac1":"#display the first 5 predictions of sale price\nfinal_predictions[:5]","5cf802b5":"#make the submission data frame\nsubmission = {\n    'Id': test.Id.values,\n    'SalePrice': final_predictions + 0.007 * final_predictions\n}\nsolution = pd.DataFrame(submission)\nsolution.head()","ce93d772":"#make the submission file\nsolution.to_csv('submission.csv',index=False)","9a5e7993":"We'll combine these two data sets using the concat attribute","a572b5ec":"After reading the description, there is the way we will impute and cleaning the missing values:\n\n1-impute the mean value to certain numerical features\n\n2-create a new class 'other' for certain categorical features\n\n3-use the mode value for the other categotical features\n\n4-drop the ID and certain unnecessary features","17000100":"display the columns with the number of missing values","913a2a1c":"Let's have fun now with Machine Learning and the Regression algorithms","8f4df837":"In our data set we have 80 features, check their names and data types using the dataframe attributes: columns and info","e225110b":"It seems that we have 34 features with NaN values\n\nbefore processing these values\n\nit is important to know exactly what each feature means by reading the data description file","5d7109b8":"Import train and test data sets, display the first 5 lines of the train data set and explore their shapes","f04fdb59":"It seems we have some features with null values, we'll deal with these values after\n\nWe'll divide the data into numerical and categorical data and verify their descriptive statistics","d5c81e0d":"That's great, our dataset no longer has any missing values.\n\nAnother pre-processing step is to transform categorical features into numerical features","25a85614":"remove outliers"}}