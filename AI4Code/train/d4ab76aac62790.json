{"cell_type":{"3fad802a":"code","8fea4144":"code","9d2f33f1":"code","1a252173":"code","e7c0a7ff":"code","d1666481":"code","de3f9b3e":"code","5986bb13":"code","a9a22004":"code","f6c31d32":"code","6d2eeb35":"code","84decdef":"code","362f5c83":"code","b55004db":"code","5b444f01":"code","d6ee6054":"code","e61c2899":"code","106dc907":"code","dd68fe06":"code","aea9699b":"markdown","05e69552":"markdown","b4e67779":"markdown","7dbe77d7":"markdown","45c1cec4":"markdown","58637f6d":"markdown","e0f05783":"markdown","028f7e51":"markdown","376fffec":"markdown","09bad31a":"markdown","3943b619":"markdown","3a4682f8":"markdown","09193ba9":"markdown","fc6116d2":"markdown","449401ec":"markdown","15512f71":"markdown","aaaca861":"markdown"},"source":{"3fad802a":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nimport string\nfrom unidecode import unidecode\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score,recall_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split","8fea4144":"train_data = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")","9d2f33f1":"train_data","1a252173":"train_data.info()","e7c0a7ff":"# L\u1ea5y ra c\u00e2u c\u00f3 nh\u00e3n 0\nnon_toxic_data = train_data[train_data.target == 0]\n# L\u1ea5y ra c\u00e2u c\u00f3 nh\u00e3n 1\ntoxic_data = train_data[train_data.target == 1]","d1666481":"# L\u1ea5y m\u1eabu 5 c\u00e2u h\u1ecfi nh\u00e3n 0\nfor sentence in non_toxic_data.question_text.sample(5):\n    print(sentence)","de3f9b3e":"# L\u1ea5y m\u1eabu 5 c\u00e2u h\u1ecfi nh\u00e3n 1\nfor sentence in toxic_data.question_text.sample(5):\n    print(sentence)","5986bb13":"# Pie chart bi\u1ec3u di\u1ec7n t\u1ec9 l\u1ec7 c\u00e2u h\u1ecfi\nlabels = 'Non-toxic', 'Toxic'\nsizes = [\n    (non_toxic_data.shape[0]\/train_data.shape[0])*100,\n    (toxic_data.shape[0]\/train_data.shape[0])*100\n]\nplt.pie(sizes, labels=labels, autopct=\"%.2f%%\", startangle=180)\nplt.axis('equal')\nplt.show()","a9a22004":"test_data","f6c31d32":"test_data.info()","6d2eeb35":"# T\u1eeb \u0111i\u1ec3n chu\u1ea9n h\u00f3a t\u1eeb vi\u1ebft t\u1eaft\ncontraction_dict = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n# Chu\u1ea9n b\u1ecb b\u1ed9 x\u1eed l\u00fd d\u1eef li\u1ec7u nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nsw = stopwords.words('english')\n\nsw.remove('not')\n\nlemma = WordNetLemmatizer()","84decdef":"def normalize_text(text):\n    # Chuy\u1ec3n v\u1ec1 d\u1ea1ng unicode \n    text = unidecode(text).encode(\"ascii\")\n    text = str(text, \"ascii\")\n\n    # Chuy\u1ec3n v\u1ec1 d\u1ea1ng vi\u1ebft th\u01b0\u1eddng\n    text = text.lower()\n    \n    # B\u1ecf d\u1ea5u, ch\u1eef s\u1ed1, k\u00fd hi\u1ec7u \u0111\u1eb7c bi\u1ec7t\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  \n    text = re.sub('\\n', '', text)\n    text = re.sub('[\u2019\u201c\u201d\u2026]', ' ', text)  \n    text = ''.join(i for i in text if not i.isdigit())\n    \n    # T\u00e1ch t\u1eeb\n    tokens = word_tokenize(text)\n\n    # Chu\u1ea9n h\u00f3a c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\n    tokens = [contraction_dict.get(word) if (contraction_dict.get(word) != None) else word for word in tokens]\n\n    # Lo\u1ea1i b\u1ecf stopwords\n    tokens = [word for word in tokens if not word in sw]\n\n    # R\u00fat g\u1ecdn t\u1eeb\n    tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n    tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n    text = ' '.join(tokens)\n\n    return text","362f5c83":"train_data['normalize'] = train_data['question_text'].apply(normalize_text)\ntrain_data","b55004db":"X = train_data['normalize']\ny = train_data['target']\n\n# Chia train_data th\u00e0nh 2 t\u1eadp d\u1eef li\u1ec7u train v\u00e0 test (t\u1ec9 l\u1ec7 4:1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","5b444f01":"# Khai b\u00e1o h\u00e0m th\u1ef1c hi\u1ec7n\ncount_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,3))\n\n# Ti\u1ebfn h\u00e0nh t\u00ednh to\u00e1n tr\u1ecdng s\u1ed1 c\u1ee7a c\u00e1c t\u1eeb trong t\u1eadp hu\u1ea5n luy\u1ec7n\ncount_vectorizer.fit(X_train)\n\n# Bi\u1ebfn \u0111\u1ed5i c\u00e1c c\u00e2u trong t\u1eadp train th\u00e0nh ma tr\u1eadn tr\u1ecdng s\u1ed1 c\u1ee7a vector h\u00f3a\ncv_X_train = count_vectorizer.fit_transform(X_train)\nprint(cv_X_train.shape)\ncv_X_test = count_vectorizer.transform(X_test)\nprint(cv_X_test.shape)","d6ee6054":"# Khai b\u00e1o m\u00f4 h\u00ecnh\nmodel_LR = LogisticRegression(solver='liblinear', class_weight=\"balanced\")\n\n# Ti\u1ebfn h\u00e0nh hu\u1ea5n luy\u1ec7n\nmodel_LR.fit(cv_X_train, y_train)\n\n# K\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n\ny_pred_LR = model_LR.predict(cv_X_test)","e61c2899":"print(\"Logistic Regression\\n\")\nprint('Recall: ', recall_score(y_pred_LR, y_test))\nprint('F1 score :', f1_score(y_pred_LR, y_test), '\\n')\nprint(classification_report(y_test, y_pred_LR))","106dc907":"# normalize d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o\ntest_data['normalize'] = test_data['question_text'].apply(normalize_text)\n\n# vector h\u00f3a d\u1eef li\u1ec7u\nX_sub = count_vectorizer.transform(test_data['normalize'])\n\n# K\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n\ntest_data['prediction'] = model_LR.predict(X_sub)\n\ntest_data","dd68fe06":"result = test_data[['qid', 'prediction']]\nresult.to_csv('submission.csv', index=False)\nresult","aea9699b":"# 2. T\u1eadp train\nD\u1eef li\u1ec7u g\u1ed3m 3 gi\u00e1 tr\u1ecb: qid, question_text, target\n* qid: unique id c\u1ee7a c\u00e2u h\u1ecfi\n* question_text: c\u00e2u h\u1ecfi c\u1ea7n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c cho d\u01b0\u1edbi d\u1ea1ng text\n* target: label 0\/1 c\u1ee7a c\u00e2u h\u1ecfi (non-toxic\/toxic)\n\n\nKhi ph\u00e2n lo\u1ea1i, ta d\u00f9ng question_text l\u00e0 \u0111\u1ea7u v\u00e0o X, target l\u00e0 label y","05e69552":"# Th\u00eam c\u00e1c th\u01b0 vi\u1ec7n","b4e67779":"Quan s\u00e1t m\u1ed9t s\u1ed1 c\u00e2u h\u1ecfi c\u00f3 nh\u00e3n 0 (non-toxic)","7dbe77d7":"# Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u\nS\u1eed d\u1ee5ng c\u00f4ng c\u1ee5 nltk \u0111\u1ec3 ti\u1ec1n x\u1eed l\u00fd\n* Chuy\u1ec3n v\u1ec1 d\u1ea1ng unicode\n* Chuy\u1ec3n v\u1ec1 d\u1ea1ng vi\u1ebft th\u01b0\u1eddng\n* B\u1ecf d\u1ea5u, ch\u1eef s\u1ed1 v\u00e0 k\u00fd hi\u1ec7u \u0111\u1eb7c bi\u1ec7t\n* T\u00e1ch t\u1eeb\n* Chu\u1ea9n h\u00f3a c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\n* Lo\u1ea1i b\u1ecf stopwords\n* R\u00fat g\u1ecdn t\u1eeb\n* Vector h\u00f3a t\u1eeb","45c1cec4":"# Submission","58637f6d":"Ki\u1ec3m tra t\u1eadp train","e0f05783":"D\u1eef li\u1ec7u sau khi normalize","028f7e51":"Quan s\u00e1t m\u1ed9t s\u1ed1 c\u00e2u h\u1ecfi c\u00f3 nh\u00e3n 1 (toxic)","376fffec":"Ki\u1ec3m tra ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u","09bad31a":"# Th\u1ef1c nghi\u1ec7m m\u00f4 h\u00ecnh","3943b619":"# 3. T\u1eadp test\nD\u1eef li\u1ec7u g\u1ed3m 2 gi\u00e1 tr\u1ecb: qid, question_text\n* qid: unique id c\u1ee7a c\u00e2u h\u1ecfi\n* question_text: c\u00e2u h\u1ecfi c\u1ea7n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c cho d\u01b0\u1edbi d\u1ea1ng text","3a4682f8":"# M\u00f4 t\u1ea3 m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c ch\u1ecdn\n**Logistic Regression**\n* M\u1ed9t m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n, ph\u1ed5 bi\u1ebfn cho b\u00e0i to\u00e1n ph\u00e2n l\u1edbp c\u00f3 ph\u00e2n b\u1ed1 Bernoulli:\n\\begin{align*}\n    Y|X = \\mathbf x \\sim Ber(y|\\sigma(f(\\mathbf x)))\n\\end{align*}\n* S\u1eed d\u1ee5ng \u0111\u1ea7u ra:\n\\begin{align*}\n    f(\\mathbf x) = \\mathbf w^T\\mathbf x + w_0\n\\end{align*}\n* S\u1eed d\u1ee5ng h\u00e0m logistic l\u00e0 sigmoid:\n\\begin{align*}\n    \\sigma(z) = \\frac 1 {1+e^{-z}}\n\\end{align*}\n* Hu\u1ea5n luy\u1ec7n b\u1eb1ng h\u00e0m l\u1ed7i:\n\\begin{align*}\n    L(\\mathbf w, w_0) &= P(D) = \\prod_{i=1}^n P(y_i|\\mathbf x_i) = \\prod_{i=1}^n \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}\\\\\n    \\ell(\\mathbf w, w_0) &= -\\log L(\\mathbf w, w_0) = \\sum_{i=1}^n -y_i\\log\\mu_i - (1-y_i)\\log(1-\\mu_i)\\\\\n    \\nabla_{\\mathbf w} \\ell(\\mathbf w, w_0) &=\\sum_{i=1}^n (\\mu_i - y_i)\\mathbf x_i\\\\\n    \\nabla_{w_0} \\ell(\\mathbf w, w_0) &= \\sum_{i=1}^n (\\mu_i - y_i)\n\\end{align*}\nTrong \u0111\u00f3: $\\mu_i = \\sigma(f(\\mathbf x_i)) = \\sigma(\\mathbf w^T\\mathbf x + w_0)$","09193ba9":"**Vector h\u00f3a t\u1eeb**\n\n\nS\u1eed d\u1ee5ng CountVectorizer \u0111\u1ec3 tr\u00edch xu\u1ea5t c\u00e1c t\u1eeb, bi\u1ebfn words th\u00e0nh d\u1ea1ng vectors tr\u00ean c\u01a1 s\u1edf t\u1ea7n su\u1ea5t (s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n) c\u1ee7a c\u00e1c t\u1eeb trong b\u1ed9 d\u1eef li\u1ec7u.","fc6116d2":"Ki\u1ec3m tra t\u1eadp test","449401ec":"# Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n# 1. L\u1ea5y d\u1eef li\u1ec7u c\u1ee7a Quora","15512f71":"# M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n* **M\u1ee5c ti\u00eau:** Ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u h\u1ecfi tr\u00ean Quora l\u00e0 toxic hay non-toxic\n* **Input:** C\u00e2u h\u1ecfi ti\u1ebfng anh \u0111\u01b0\u1ee3c cho d\u01b0\u1edbi d\u1ea1ng text\n* **Output:** 0\/1 (non-toxic\/toxic)","aaaca861":"# 4. Nh\u1eadn x\u00e9t\n* D\u1eef li\u1ec7u kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null\n* T\u1eadp train: 1306122 d\u00f2ng x 3 c\u1ed9t (qid, question_text, target)\n* T\u1eadp test: 375806 d\u00f2ng x 2 c\u1ed9t (qid, question_text)\n* Trong d\u1eef li\u1ec7u train c\u00f3 t\u1edbi 93.81% c\u00e2u h\u1ecfi \u0111\u00e1nh nh\u00e3n 0 m\u00e0 ch\u1ec9 c\u00f3 6.19% c\u00e2u h\u1ecfi \u0111\u00e1nh nh\u00e3n 1 => M\u1ea5t c\u00e2n b\u1eb1ng v\u1ec1 d\u1eef li\u1ec7u. \u0110\u00f3 l\u00e0 l\u00fd do m\u00e0 \u0111\u1ec1 y\u00eau c\u1ea7u \u0111\u00e1nh gi\u00e1 b\u1eb1ng F1 Score\n\n\n![image.png](attachment:2de04f62-a0a5-4db3-9146-eb5e628d1e4a.png)"}}