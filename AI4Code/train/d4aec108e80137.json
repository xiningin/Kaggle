{"cell_type":{"e0ce9939":"code","fa112350":"code","dec3d66e":"code","3d98c686":"code","cf106383":"code","6071f28b":"code","57185d41":"code","93f5d04a":"code","b7af9d5c":"code","0a14451c":"code","7c3baee1":"code","037cecd9":"code","5e220306":"code","4b1caefc":"code","a6b185f5":"code","85a1799b":"code","ca1bfd04":"code","94e9b41b":"code","cd72bbbe":"code","7d623edc":"code","4e2a3d10":"code","f8d44c62":"code","d4a6e299":"code","58446076":"code","b33cb19f":"code","bb6c90ed":"code","e00d279e":"code","40a20204":"code","a4eff2af":"code","f717ef9c":"markdown","bc3e4831":"markdown","8a85c70c":"markdown","a45f8b58":"markdown","e3ee050a":"markdown"},"source":{"e0ce9939":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nwarnings.filterwarnings('ignore')\nimport graphviz\nfrom IPython.display import Image  \nfrom subprocess import call\nfrom sklearn.metrics import accuracy_score\n!pip install pydotplus\nimport pydotplus","fa112350":"train_data = pd.read_csv('..\/input\/titanic\/train.csv', sep=',')\nX_test_data = pd.read_csv('..\/input\/titanic\/test.csv', sep=',')\ny_test_data = pd.read_csv('..\/input\/titanic\/gender_submission.csv', sep=',')","dec3d66e":"train_data.head()","3d98c686":"train_data.isnull().sum()","cf106383":"X_test_data.isnull().sum()","6071f28b":"# filling NaN in \"Embarked\" and \"Fare\"\n\ntrain_data['Embarked'].fillna(value='S',inplace=True) # S is most popular value \nmean_Fare = train_data[\"Fare\"].mean()\ntrain_data['Fare'].fillna(value=mean_Fare,inplace=True)\n    \nX_test_data['Embarked'].fillna(value='S',inplace=True) # S is most popular value \nmean_Fare = X_test_data['Fare'].mean()\nX_test_data['Fare'].fillna(value=mean_Fare,inplace=True)","57185d41":"from random import choices\n# filling NaN in \"Age\" \nx = train_data['Age'].dropna()\nhist, bins = np.histogram( x,bins=15)\n\nbin_centers = 0.5*(bins[:len(bins)-1]+bins[1:])\nprobabilities = hist\/hist.sum()\n\n#dictionary with random numbers from existing age distribution\ntrain_data['Age_rand'] = train_data['Age'].apply(lambda v: np.random.choice(bin_centers, p=probabilities))\nAge_null_list = train_data[train_data['Age'].isnull()].index\ntrain_data.loc[Age_null_list,'Age'] = train_data.loc[Age_null_list,'Age_rand']\n    \n# filling NaN in \"Age\" \nx = X_test_data['Age'].dropna()\nhist, bins = np.histogram( x,bins=15)\n\nbin_centers = 0.5*(bins[:len(bins)-1]+bins[1:])\nprobabilities = hist\/hist.sum()\n\n#dictionary with random numbers from existing age distribution\nX_test_data['Age_rand'] = X_test_data['Age'].apply(lambda v: np.random.choice(bin_centers, p=probabilities))\nAge_null_list = X_test_data[X_test_data['Age'].isnull()].index\nX_test_data.loc[Age_null_list,'Age'] = X_test_data.loc[Age_null_list,'Age_rand']","93f5d04a":"# Gender\ngenders = {'male': 1, 'female': 0}\ntrain_data['Sex'] = train_data['Sex'].apply(lambda s: genders.get(s))\nX_test_data['Sex'] = X_test_data['Sex'].apply(lambda s: genders.get(s))","b7af9d5c":"# Embarkment\nembarkments = {'U': 0, 'S': 1, 'C': 2, 'Q': 3}\ntrain_data['Embarked'] = train_data['Embarked'].apply(lambda e: embarkments.get(e))\nX_test_data['Embarked'] = X_test_data['Embarked'].apply(lambda e: embarkments.get(e))","0a14451c":"plt.subplots(figsize = (10,10))\ndata = train_data.loc[:,['Survived','Pclass', 'SibSp', 'Parch', 'Embarked', 'Fare', 'Age', 'Sex']]\nsns.heatmap(data.corr(),\n            annot=True,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20)\nplt.show()","7c3baee1":"train_data, valid_data = train_test_split(train_data, test_size=0.2)","037cecd9":"X_train = train_data.loc[:,['Pclass', 'SibSp', 'Parch', 'Embarked', 'Fare', 'Age', 'Sex']]\nX_valid = valid_data.loc[:,['Pclass', 'SibSp', 'Parch', 'Embarked', 'Fare', 'Age', 'Sex']]\nX_test = X_test_data.loc[:,['Pclass', 'SibSp', 'Parch', 'Embarked', 'Fare', 'Age', 'Sex']]\n\ny_train = train_data.loc[:,['Survived']]\ny_valid = valid_data.loc[:,['Survived']]\ny_test = y_test_data.loc[:,['Survived']]","5e220306":"X_train.describe()","4b1caefc":"X_valid.describe()","a6b185f5":"X_test.describe()","85a1799b":"X_train_matrix = X_train.to_numpy()\nX_valid_matrix = X_valid.to_numpy()\nX_test_matrix = X_test.to_numpy()\ny_train_matrix = y_train.to_numpy()\ny_valid_matrix = y_valid.to_numpy()\ny_test_matrix = y_test.to_numpy()","ca1bfd04":"acc_train_array = []\nacc_valid_array = []\nfor depth in range(1,11):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = depth)\n    decision_tree = decision_tree.fit(X_train_matrix, y_train_matrix)\n    y_pred = decision_tree.predict(X_train_matrix)\n    acc_train = accuracy_score(y_train_matrix, y_pred)\n    acc_train_array.append(acc_train)\n    y_pred = decision_tree.predict(X_valid_matrix)\n    acc_valid = accuracy_score(y_valid_matrix, y_pred)\n    acc_valid_array.append(acc_valid)\n    dot_data = tree.export_graphviz(decision_tree, \n                                out_file=None,\n                                max_depth = depth,\n                                filled=True, \n                                rounded=True,                                \n                                special_characters=True,\n                                class_names = ['Died', 'Survived'],\n                                feature_names = X_train.columns.values)\n    pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n    pydot_graph.write_png('..\/working\/tree_depth_' + str(depth) + '.png')\nfig, ax = plt.subplots(figsize = (12,6))\nprint('For train set:')\nprint(acc_train_array)\nprint('For valid set:')\nprint(acc_valid_array)\nax.set_xlabel('depth')\nax.set_ylabel('accuracy')\nax.plot(range(1,11), acc_train_array)\nax.plot(range(1,11), acc_valid_array)\nplt.show()","94e9b41b":"print('{:^10}{:^20}{:^20}'.format('depth','train accuracy','valid accuracy'))\nfor i in range(10):\n    print('{:^10}{:^20.5}{:^20.5}'.format(str(i+1), str(acc_train_array[i]), str(acc_valid_array[i])))","cd72bbbe":"Image('..\/working\/tree_depth_1.png')","7d623edc":"Image('..\/working\/tree_depth_2.png')","4e2a3d10":"Image('..\/working\/tree_depth_3.png')","f8d44c62":"Image('..\/working\/tree_depth_4.png')","d4a6e299":"Image('..\/working\/tree_depth_5.png')","58446076":"Image('..\/working\/tree_depth_6.png')","b33cb19f":"Image('..\/working\/tree_depth_7.png')","bb6c90ed":"Image('..\/working\/tree_depth_8.png')","e00d279e":"Image('..\/working\/tree_depth_9.png')","40a20204":"Image('..\/working\/tree_depth_10.png')","a4eff2af":"decision_tree = tree.DecisionTreeClassifier(max_depth = 5)\ndecision_tree = decision_tree.fit(X_train_matrix, y_train_matrix)\n# Predicting results for test dataset\ny_pred = decision_tree.predict(X_test_matrix)\nsubmission = pd.DataFrame({\n        'PassengerId': [ int(x) for x in X_test_data.loc[:,['PassengerId']].to_numpy()],\n        'Survived': y_pred\n    })\nsubmission.to_csv('..\/working\/submission.csv', index=False)","f717ef9c":"> ## EDA and data preparation\n\n### Load data\n","bc3e4831":"### As we see, starting from depth 6 the model is overfited and model the depth 5 shows the best accuracy on the validation dataset.","8a85c70c":"### Missing data filling\n(Reference: https:\/\/www.kaggle.com\/mknorps\/titanic-with-decision-trees)","a45f8b58":"## This kernel demonstrates the impact of decision tree depth on the classification accuracy\n","e3ee050a":"## Lets build 10 models with different depth and compare the results"}}