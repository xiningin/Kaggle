{"cell_type":{"93d53b52":"code","42532f8d":"code","c3da920a":"code","353b7976":"code","bca901f1":"code","b6787dc5":"code","4edb0380":"code","67038178":"code","23271421":"code","d9bb04cf":"code","34669ce5":"code","1a7e4c8e":"code","f4c0beb3":"code","12e7d6b1":"code","51dee086":"code","16642d55":"code","da25a213":"code","61c047a0":"code","ed46a6d1":"code","8554fdc3":"markdown","76255fca":"markdown","522cbe63":"markdown","41056dd5":"markdown","e30f6b3c":"markdown","d84641f5":"markdown","524f189a":"markdown","4439f5c7":"markdown","44bf392f":"markdown"},"source":{"93d53b52":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nimport plotly.express as px","42532f8d":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","c3da920a":"### Keras self attention example code (not formatted for this target shape!! - won't work without that  \n### https:\/\/github.com\/keras-team\/keras-io\/blob\/master\/examples\/nlp\/text_classification_with_transformer.py\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\n## Implement multi head self attention as a Keras layer\n\"\"\"\n\n\nclass MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim \/\/ num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score \/ tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n\n\n\"\"\"\n## Implement a Transformer block as a layer\n\"\"\"\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\n\"\"\"\n## Implement embedding layer\nTwo seperate embedding layers, one for tokens, one for token index (positions).\n\"\"\"\n\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n","353b7976":"from tensorflow.keras.layers import  Layer\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras import backend as K\n\n### https:\/\/stackoverflow.com\/questions\/62948332\/how-to-add-attention-layer-to-a-bi-lstm\/62949137#62949137\n#### Built to receive 3D tensors and output 3D tensors (return_sequences=True) or 2D tensors (return_sequences=False)\nclass attention(Layer):\n    \n    def __init__(self, return_sequences=True):\n        self.return_sequences = return_sequences\n        super(attention,self).__init__()\n        \n    def build(self, input_shape):\n        \n        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n                               initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n                               initializer=\"zeros\")\n        \n        super(attention,self).build(input_shape)\n        \n    def call(self, x):\n        \n        e = K.tanh(K.dot(x,self.W)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        if self.return_sequences:\n            return output\n        \n        return K.sum(output, axis=1)\n    \n    ### https:\/\/stackoverflow.com\/questions\/58678836\/notimplementederror-layers-with-arguments-in-init-must-override-get-conf\n    ### I'm not doing this right... \n    ### https:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n#             'vocab_size': self.vocab_size,\n            'num_layers': self.num_layers,\n            'units': self.units,\n            'd_model': self.d_model,\n            'num_heads': self.num_heads,\n            'dropout': self.dropout,\n        })\n        return config\n","bca901f1":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n\n## consider adding convlstm layer(s) - https:\/\/keras.io\/api\/layers\/recurrent_layers\/conv_lstm2d\/\n\n## example of adding attention to an lstm\n# https:\/\/stackoverflow.com\/questions\/62948332\/how-to-add-attention-layer-to-a-bi-lstm\/62949137#62949137\n# https:\/\/stackoverflow.com\/questions\/56946995\/how-to-build-a-attention-model-with-keras\ndef build_model(seq_len=107, pred_len=68, dropout=0.35, embed_dim=100, hidden_dim=128):\n    inputs = L.Input(shape=(seq_len, 3))\n\n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n\n### maybe use stackedRNN cells layer? https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/StackedRNNCells\n    hidden = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True))(reshaped)\n    hidden = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True))(hidden)\n\n    \n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it\n    truncated = hidden[:, :pred_len]\n    \n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss='mse')\n    \n    return model\n\n\n## adding self attention - currently I have bugs, doesn' work. TODO. \n## consider adding convlstm layer(s) - https:\/\/keras.io\/api\/layers\/recurrent_layers\/conv_lstm2d\/\n\n## example of adding attention to an lstm - BUT! Tensorflow's layers are NOT self attention!\n# https:\/\/stackoverflow.com\/questions\/62948332\/how-to-add-attention-layer-to-a-bi-lstm\/62949137#62949137\n# https:\/\/stackoverflow.com\/questions\/56946995\/how-to-build-a-attention-model-with-keras\n## another way of attention with lstm (doesn't use TF layers) - https:\/\/levelup.gitconnected.com\/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb\n\n## Keras self attention example code (not formatted for this target)  -https:\/\/github.com\/keras-team\/keras-io\/blob\/master\/examples\/nlp\/text_classification_with_transformer.py\ndef build_model_att(seq_len=107, pred_len=68, dropout=0.45, embed_dim=100, hidden_dim=256):\n    inputs = L.Input(shape=(seq_len, 3))\n    \n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    \n    embed = tf.keras.layers.SpatialDropout2D(rate=dropout\/3)(embed) # add spatial dropout\n    \n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n### maybe use stackedRNN cells layer? https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/StackedRNNCells\n    hidden = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout,recurrent_dropout=dropout\/3, return_sequences=True))(reshaped)\n    hidden = attention(return_sequences=True)(hidden)\n    hidden = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout,recurrent_dropout=dropout\/3, return_sequences=True))(hidden)\n\n    \n#     hidden_att = attention(return_sequences=False)(hidden) ## https:\/\/stackoverflow.com\/questions\/62948332\/how-to-add-attention-layer-to-a-bi-lstm\/62949137#62949137\n#     hidden_att = tf.keras.layers.GlobalAveragePooling1D()(hidden_att)\n\n\n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it \n    ### ??? - Dan - how is this truncation ok, vs leaving the cols in ? \n    truncated = hidden[:, :pred_len]\n    \n# # #     # add self attention ? \n#     truncated = attention(return_sequences=True)(truncated)\n    \n    \"\"\"\n    ### Try adding TF attention - Dan: \n    #### https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/AdditiveAttention\n    # Query-value attention of shape [batch_size, Tq, filters].\n#     query_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n#     [query_seq_encoding, value_seq_encoding])\n    query_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n    [hidden, embed])\n    # Reduce over the sequence axis to produce encodings of shape # [batch_size, filters].\n    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n    query_value_attention_seq)\n    \n    ### concat - Dan: \n    # Concatenate query and document encodings to produce a DNN input layer.\n    joint = tf.keras.layers.Concatenate()(\n    [truncated, query_value_attention])\n    \"\"\"\n    \n#     ### concat - Dan: \n#     # Concatenate query and document encodings to produce a DNN input layer.\n#     joint = tf.keras.layers.Concatenate()(\n#     [truncated, hidden_att])\n\n    \n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss='mse')\n    \n    return model\n\n\nmodel = build_model_att()\nmodel.summary()","b6787dc5":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","4edb0380":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_df = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","67038178":"train_inputs = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))","23271421":"# model = build_model()\nmodel = build_model_att()\nmodel.summary()","d9bb04cf":"history = model.fit(\n    train_inputs, train_labels, \n    batch_size=64,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n#         tf.keras.callbacks.ModelCheckpoint('model.h5'), ## error with model saving\/get confiug\n        tf.keras.callbacks.EarlyStopping(patience=4, mode='auto',restore_best_weights=True)\n    ],\n    validation_split=0.3 # 0.3 ## note - validation (randomly) is unstable, and val_loss is lower when using larger validation split\n)","34669ce5":"N_EPOCHS = len(history.history['loss'])\nprint(\"number of training epochs picked:\",N_EPOCHS)","1a7e4c8e":"fig = px.line(\n    history.history, y=['loss', 'val_loss'], \n    labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n    title='Training History')\nfig.show()","f4c0beb3":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","12e7d6b1":"# Caveat: The prediction format requires the output to be the same length as the input,\n# although it's not the case for the training data.\nmodel_short = build_model(seq_len=107, pred_len=107)\nmodel_long = build_model(seq_len=130, pred_len=130)\n","51dee086":"\n\n#### add workaround instead of saving model? \nmodel_short.fit(\n    train_inputs, train_labels, \n    batch_size=64,\n    epochs=N_EPOCHS,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()]\n)\n\nmodel_long.fit(\n    train_inputs, train_labels, \n    batch_size=64,\n    epochs=N_EPOCHS,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()]\n)","16642d55":"# model_short.load_weights('model.h5')\n# model_long.load_weights('model.h5')\n\npublic_preds = model_short.predict(public_inputs)\nprivate_preds = model_long.predict(private_inputs)","da25a213":"print(public_preds.shape, private_preds.shape)","61c047a0":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","ed46a6d1":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","8554fdc3":"## Load and preprocess data","76255fca":"## Post-processing and submit","522cbe63":"Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes.","41056dd5":"* without attention:\nEpoch 1\/8 113ms\/step - loss: 0.9491 - val_loss: 0.8325\nEpoch 8\/8 - 54ms\/step - loss: 0.8296 - val_loss: 0.7613\n\n\n* with self attention, including the truncated output layer:\nEpoch 25\/25 - 2s 77ms\/step - loss: 0.8938 - val_loss: 0.8295\n\n* with self attention, but without it on the output layer: \nEpoch 25\/25 - 2s 75ms\/step - loss: 0.8217 - val_loss: 0.7622\n","e30f6b3c":"## Define helper functions and useful vars","d84641f5":"For each sample, we take the predicted tensors of shape (107, 5) or (130, 5), and convert them to the long format (i.e. $629 \\times 107, 5$ or $3005 \\times 130, 5$):","524f189a":"## Predict on test set","4439f5c7":"## Build and train model","44bf392f":"* Notebook forked and model modified.\n* Instead of GRU - LSTM\n* Added self attention attention. \n* Note - need to add proper evaluation loop.., and proper attention (NOTE: TF attention layers are not \"designed\" for self attention out of the box)"}}