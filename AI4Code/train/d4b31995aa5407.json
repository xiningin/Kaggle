{"cell_type":{"9413a366":"code","92643556":"code","f252a873":"code","de466311":"code","ce3a7b2f":"code","89834ebb":"code","f3a4f666":"code","dd649b29":"code","ff8935c1":"code","8699d56e":"code","c6f914c5":"code","55d12b24":"code","89ba689d":"code","e980912b":"code","ad8d176a":"code","e8db79bd":"code","98b24d58":"code","0bbccb86":"code","1ad1aea8":"code","66af8764":"code","ff303fea":"code","2ff390be":"code","c9b9b711":"code","702f8fdf":"code","42fdded0":"code","fcf97a18":"code","6aca5372":"code","8244ad83":"code","a9a409ee":"code","f3a076d5":"code","1526736a":"code","ec9ed7d6":"code","116e71bf":"code","dda62095":"code","e22280c3":"code","c3519d62":"code","388eab00":"code","d228b95e":"code","63820cce":"code","608e02eb":"code","fb123cec":"code","73e9095a":"code","824a7792":"code","77542d61":"code","c9fda569":"code","5dc95746":"code","654e2b2f":"code","a5efafa5":"code","40e036ab":"code","4ddf432f":"code","e120de6f":"code","ff565953":"code","c283df8b":"code","1e159f90":"code","1fdb3d9a":"code","4ef71363":"markdown","d82e5e0a":"markdown","7cd6a7f5":"markdown","39033350":"markdown","bf128580":"markdown","5b7f9cbc":"markdown","39d9af47":"markdown","97e048ee":"markdown","5b12790c":"markdown","172519bd":"markdown","f14aa6f6":"markdown","e27e4a53":"markdown","0b5f9b4a":"markdown","40560f6b":"markdown","825138f8":"markdown","efff9f88":"markdown"},"source":{"9413a366":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler #column standardization\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom scipy.stats import chi2_contingency\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","92643556":"train = pd.read_csv('\/kaggle\/input\/network-intrusion-detection\/Train_data.csv')","f252a873":"test = pd.read_csv('\/kaggle\/input\/network-intrusion-detection\/Test_data.csv')","de466311":"# checking number of columns and type of each columnb\ntrain.info()","ce3a7b2f":"# check the first 10 records of train dataset\ntrain.head(10)","89834ebb":"train['protocol_type'].value_counts()","f3a4f666":"train['flag'].value_counts()","dd649b29":"pd.set_option('display.max_row', None)\ntrain['service'].value_counts()","ff8935c1":"train['class'].value_counts()","8699d56e":"pro_flg_serv = train.groupby(['protocol_type','service','class'])['class'].count()\npro_flg_serv","c6f914c5":"train.describe()","55d12b24":"# https:\/\/stackoverflow.com\/questions\/28576540\/how-can-i-normalize-the-data-in-a-range-of-columns-in-my-pandas-dataframe\n# Normalize the data across numeric columns in dataset. Therefore, removing 4 object columns from list\ncols_to_norm = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', \n                'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n                'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', \n                'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', \n                'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', \n                'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', \n                'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\ntrain[cols_to_norm] = StandardScaler().fit_transform(train[cols_to_norm])\ntest[cols_to_norm] = StandardScaler().fit_transform(test[cols_to_norm])","89ba689d":"train.head()","e980912b":"# https:\/\/stackoverflow.com\/questions\/38913965\/make-the-size-of-a-heatmap-bigger-with-seaborn\ncorr_df = train[cols_to_norm].corr(method='pearson')\nfig, ax = plt.subplots(figsize=(12,12)) \nsns.heatmap(corr_df)","ad8d176a":"pd.set_option('display.max_column',None)\ncorr_df","e8db79bd":"# Since num_outbound_cmds and is_host_login is NAN value, we are dropping it from column and row.\ncorr_df.drop(index=['is_host_login','num_outbound_cmds'], columns=['is_host_login','num_outbound_cmds'], inplace=True)","98b24d58":"corr_df.shape","0bbccb86":"# https:\/\/thispointer.com\/python-pandas-how-to-add-rows-in-a-dataframe-using-dataframe-append-loc-iloc\/\ncorr_col_df = pd.DataFrame(columns=[\"Column1\",\"Column2\",\"Corr_value\"])\nfor i in corr_df.columns:\n    for j in corr_df.index:\n        if (i != j) and (corr_df[i][j] > 0.7):\n            corr_col_df = corr_col_df.append({ \"Column1\" : i, \"Column2\" : j, \"Corr_value\" : corr_df[i][j] }, ignore_index=True)\n            #print(i, \"\\t\", j, \"\\t\", corr_df[i][j])\n            \ncorr_col_df","1ad1aea8":"# records are repeating while is creating unnecessary complexity in analysis. Will try to remove duplicate records\n\nind_list = []\nfor i in range(len(corr_col_df)):\n    for j in range(len(corr_col_df)):\n        #print(\"j\", j)\n        #print(\"corr\", uni_corr_col_df['Corr_value'][j])\n        #print(\"columns\", uni_corr_col_df['Column1'][i], uni_corr_col_df['Column2'][j])\n        if ((i!=j) and (corr_col_df['Corr_value'][i] == corr_col_df['Corr_value'][j]) \n            and (corr_col_df['Column1'][i] == corr_col_df['Column2'][j]) \n            and (corr_col_df['Column2'][i] == corr_col_df['Column1'][j])):\n            ind_list.append([i,j])\n\n# Unique pair value from list - \n# https:\/\/www.geeksforgeeks.org\/python-remove-duplicates-from-nested-list\/\n# https:\/\/stackoverflow.com\/questions\/47051854\/remove-duplicates-based-on-the-content-of-two-columns-not-the-order\nfor i in ind_list:\n    i.sort()\nuni_ind_list = list(set(tuple(i) for i in ind_list)) \n\n# store unique records into dataframe\nuni_corr_col_df = pd.DataFrame(columns=[\"Column1\",\"Column2\",\"Corr_value\"])\nfor i in uni_ind_list:\n    uni_corr_col_df = uni_corr_col_df.append(corr_col_df.iloc[i[0]], ignore_index=True)\n    \nuni_corr_col_df","66af8764":"# identifying columns to delete from dataframe\ncol_corr = set() # Set of all the names of deleted columns\nfor i in range(len(uni_corr_col_df)):\n    if (uni_corr_col_df['Column1'][i] not in col_corr):\n        colname = uni_corr_col_df['Column2'][i] # getting the name of column\n        col_corr.add(colname)\n\ncol_corr = list(col_corr)\nprint(col_corr)\n\n# dropping identified columns from train and test dataset\ntrain.drop(col_corr, axis=1, inplace=True)\ntest.drop(col_corr, axis=1, inplace=True)","ff303fea":"train.shape","2ff390be":"test.shape","c9b9b711":"col_corr = set() # Set of all the names of deleted columns\nfor i in range(len(corr_df.columns)):\n    for j in range(i):\n        if (corr_df.iloc[i, j] >= 0.7) and (corr_df.columns[j] not in col_corr):\n            colname = corr_df.columns[i] # getting the name of column\n            col_corr.add(colname)\ncol_corr = list(col_corr)\nprint(col_corr)\n\n# dropping identified columns from train and test dataset\n\n#Uncomment below code if you are using Method 2.\n#train.drop(col_corr, axis=1, inplace=True)\n#test.drop(col_corr, axis=1, inplace=True)","702f8fdf":"train.shape","42fdded0":"test.shape","fcf97a18":"#https:\/\/towardsdatascience.com\/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\n# we need to pass data in cross tabular format to chi2_contingency. Therefore, using pd.crosstab\n# we are assuming that significant value is 0.05\nalpha = 0.05\nstat, p, dof, expected = chi2_contingency(pd.crosstab(train['protocol_type'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nprotocol_type and class columns are dependent\")\nelse:\n    print(\"\\nprotocol_type and class columns are independent\")","6aca5372":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['service'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nservice and class columns are dependent\")\nelse:\n    print(\"\\nservice and class columns are independent\")","8244ad83":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['flag'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nflag and class columns are dependent\")\nelse:\n    print(\"\\nflag and class columns are independent\")","a9a409ee":"# convert dependent variable to number.\nlabel_encoder = LabelEncoder()\ntrain['class'] = label_encoder.fit_transform(train['class'])","f3a076d5":"y = train['class']\ny.shape","1526736a":"X = train.drop('class', axis=1)\nX.shape","ec9ed7d6":"\nX = pd.get_dummies(X)","116e71bf":"X.head()","dda62095":"X.shape","e22280c3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","c3519d62":"model = LogisticRegression(random_state=45).fit(X_train, y_train)","388eab00":"y_pred = model.predict(X_test)","d228b95e":"metrics.confusion_matrix(y_test,y_pred)","63820cce":"metrics.f1_score(y_test, y_pred)","608e02eb":"\nX['protocol_type'] = label_encoder.fit_transform(X['protocol_type'])\nX['service'] = label_encoder.fit_transform(X['service'])\nX['flag'] = label_encoder.fit_transform(X['flag'])\n\nX['protocol_type'] = X['protocol_type'].astype('category')\nX['service'] = X['service'].astype('category')\nX['flag'] = X['flag'].astype('category')","fb123cec":"X.head()","73e9095a":"X.shape","824a7792":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","77542d61":"model = LogisticRegression(random_state=45).fit(X_train, y_train)","c9fda569":"y_pred = model.predict(X_test)","5dc95746":"metrics.confusion_matrix(y_test,y_pred)","654e2b2f":"metrics.f1_score(y_test, y_pred)","a5efafa5":"test.shape","40e036ab":"test = pd.get_dummies(test)\ntest.shape","4ddf432f":"# Here we are checking if there is any column which is present in test dataset but not in train dataset. \n# If yes, then we will delete it from test dataset because model is not trained on those columns.\nfor i in list(test.columns):\n    if i not in list(X.columns):\n        print(i)\n        test.drop(i, axis=1, inplace=True)","e120de6f":"test.shape","ff565953":"# in order to fetch particular column index, we can use df.columns.get_loc()\n# https:\/\/stackoverflow.com\/questions\/13021654\/get-column-index-from-column-name-in-python-pandas\n# to add a column on particular index loc in dataframe, we can use df.insert()\n# https:\/\/stackoverflow.com\/questions\/18674064\/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas\n\n# Here, we are identifying missing columns from test dataset.\nfor i in list(X.columns):\n    if i not in list(test.columns):\n        print(i)\n        ind = X.columns.get_loc(i)\n        test.insert(loc=ind, column=i,value=0)","c283df8b":"test.shape","1e159f90":"test.head()","1fdb3d9a":"model.predict(test)","4ef71363":"### End of the notebook!","d82e5e0a":"### Approach 1 - using get_dummies to encode categorical values into numeric.","7cd6a7f5":"#### Observation - 4 columns are object, rest all are numeric","39033350":"### **Identify collinearity between columns**","bf128580":"### Loading data into train and test dataframes","5b7f9cbc":"#### ***When to use Chi-Squared Test:***\nWhen the data type of our feature to be tested and the target variable are both categorical (i.e. we have a classification problem) we can use Chi-Squared test. Now we will identify correlation between categorical data","39d9af47":"### **Exploratory Data Analysis**","97e048ee":"#### **Method 2 - To identify correlated columns and drop them from train and test dataframe**","5b12790c":"#### **There are 2 methods which i have used to drop either of the correlated column.**\n\n#### ***Method 1***","172519bd":"### Approach 2 - using labelEncoder to encode categorical values to numerical and then convert it to type as category. To verify the results with get_dummies function","f14aa6f6":"### Predicting test dataset values\n\nBefore passing test dataset values to the generated model, we need to perform same preprocessing and operations which we did on train dataset.\n1. Drop correlated columns from test dataset --> We have already drop correlated columns from test dataset along with train dataset.\n2. Normalized the columns --> We have already completed this activity with train dataset.\n3. get_dummies on categorical columns of test dataset --> We need to perform this activity now.","e27e4a53":"#### **Observation** -\n\n1. All records are anomaly when\n\n    - protocol_type = \"tcp\" and service as \"Z39_50\", \"bgp\", \"courier\", \"csnet_ns\", \"ctf\", \"daytime\", \"discard\", \"echo\", \"efs\",\"exec\", \"gopher\", \"hostnames\", \"http_443\", \"http_8001\", \"imap4\", \"iso_tsap\", \"klogin\", \"kshell\", \"ldap\", \"link\", \"login\", \"mtp\", \"name\", \"netbios_dgm\", \"netbios_ns\", \"netbios_ssn\", \"netstat\", \"nnsp\", \"nntp\", \"pm_dump\", \"pop_2\", \"printer\", \"private\", \"remote_job\", \"rje\", \"sql_net\", \"ssh\", \"sunrpc\", \"supdup\", \"systat\", \"uucp\", \"uucp_path\", \"vmnet\", \"whois\"\n    - protocol_type = \"icmp\" and service = \"tim_i\"\n\n2. All records are normal when\n\n    * protocol_type = \"tcp\" and with service = \"IRC\"\n    * protocol_type = \"icmp\" and service as \"red_i\", \"urh_i\"\n    * protocol_type = \"udp\" and service = \"ntp_u\"","0b5f9b4a":"### Observation:\n\nWe don't have same number of columns as train dataset. Therefore we need to identify missing columns from test dataset.","40560f6b":"### **Encoding the categorical data and splitting it into 75-25**","825138f8":"### Observations -\n1. With get dummies on all columns of type object, F1 score is 0.9736\n2. With converting object type columns to ascategory and then doing label encoding, F1 score is 0.9576\n\n#### From the F1 score, we can say that Approach 1 is better than Approach 2 in this case. Therefore, we are goinf ahead with Approach 1.","efff9f88":"#### **Observation** - We have approx. 53-47% of class label records. Therefore, we can pass this data to ML algorithms."}}