{"cell_type":{"0fbd03a7":"code","64fb62a2":"code","f9b87056":"code","02f40333":"code","83ee703c":"code","619c0e62":"code","53a8e7c1":"code","5ba28cdf":"code","bac8ccc5":"code","9fe1fdc4":"code","483fe38f":"code","56c365ab":"code","c173cca5":"code","9dd1457e":"code","7140d2b5":"code","3d03d962":"code","e225555d":"code","c7eb627d":"code","1f062166":"code","48d31f49":"code","30bc87e7":"code","70a6bcd3":"code","492fbbc7":"code","972b339f":"code","e6ee6762":"code","86a44b48":"markdown","edb506a7":"markdown","db7ad0b4":"markdown","14e4d14c":"markdown","c9dc2abd":"markdown","f3b197a8":"markdown","1b1577c0":"markdown","2d08f6db":"markdown","9225eb65":"markdown","5d595e83":"markdown","1c152965":"markdown","eb3c405b":"markdown","f5eb738a":"markdown","690fe2f2":"markdown","5d0b3899":"markdown","405b56ab":"markdown","57d81b40":"markdown","933a9bfb":"markdown","bfeb48cb":"markdown","0a16d542":"markdown","ecace7f3":"markdown","974835ac":"markdown"},"source":{"0fbd03a7":"!pip install xlrd\n!pip install openpyxl\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom statistics import *\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score,accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nsns.set(context= \"notebook\", color_codes=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","64fb62a2":"############################ Read in Train data ##############################\n\nTrainData = pd.read_excel('..\/input\/predict-a-doctors-consultation-fee\/train.xlsx', sheet_name='Sheet1')","f9b87056":"print('The Train Data has ', TrainData.shape[0], 'rows and ', TrainData.shape[1], 'columns\\n')\nTrainData.info()","02f40333":"print('Null Values Count :\\n')\nTrainData.isna().sum()","83ee703c":"print('\\nThis is how the data looks like\\n')\nTrainData.head()","619c0e62":"print('\\n Number of unique elements in --')\nprint('\\n Qualification :', TrainData['Qualification'].nunique())\nprint('\\n Place :', TrainData['Place'].nunique())\nprint('\\n Profile :', TrainData['Profile'].nunique())","53a8e7c1":"print('The unique elements of Profile are :\\n', TrainData['Profile'].unique())","5ba28cdf":"####################################################################################\n\ndef PreProcess(Data):\n    ################## Starting with 'Experience' column ########################\n    Data['Experience'] = Data['Experience'].astype(str).str.split(' ').map(lambda x: x[0])\n    Data['Experience'] = Data['Experience'].astype(np.int64)\n    \n    ########################## 'Place' column #################################\n    ########### We drop the entire row for null values of 'Place' #############\n    Data = Data.dropna(axis=0, subset=['Place'])\n        \n    ########### Keeping only the city in 'Place' ##############################\n    City = Data['Place'].str.split(', ', n = 1, expand = True)\n    Data['Place'] = City[1]\n    Data.drop(Data[Data['Place'] == 'Sector 5, Delhi'].index, inplace = True)\n    Data.drop(Data[Data['Place'].isna()].index, inplace = True)\n    \n    ################# Resetting index after row deletion ######################\n    Data = Data.reset_index(drop=True)\n    \n    ################## 'Qualification' column #################################\n    idx = Data.index[Data['Qualification'].str.contains('|'.join(['Fellowship','MRCS','FCGP', 'FRCS', 'FWFO', 'FPFA', 'FICD']))]\n    Data['Qualification'][idx] = 'Fellowship'\n    idx = Data.index[Data['Qualification'].str.contains('|'.join(['MD', 'M.D','MS','MDS', 'PhD', 'DNB', 'DLO','DDV',\n                                                              'DDVL', 'Diploma in Dermatology']))]\n    Data['Qualification'][idx] = 'MD'\n    idx = Data.index[Data['Qualification'].str.contains('|'.join(['MBBS', '39 years experience']))]\n    Data['Qualification'][idx] = 'MBBS'\n    idx = Data.index[Data['Qualification'].str.contains('BDS')]\n    Data['Qualification'][idx] = 'BDS'\n    idx = Data.index[Data['Qualification'].str.contains('|'.join(['BHMS','LCEH', 'GCEH']))]\n    Data['Qualification'][idx] = 'BHMS'\n    idx = Data.index[Data['Qualification'].str.contains('|'.join(['BAMS', 'BSAM', 'G.A.M.S', 'DAM']))]\n    Data['Qualification'][idx] = 'BAMS'\n    \n    ####################### Working on Rating #################################\n    ## Extracting the integer from the string\n    Data['Rating'] = Data.Rating.str.extract('(\\d+)')\n    \n    ## Fill Nan with -1 and then convert to str and integer\n    Data['Rating'] = Data['Rating'].fillna(-1).astype(str).astype(np.float64)\n    Data['Rating'] = Data['Rating'].replace(-1, np.nan)\n    \n    ####### Imputing mean values depending upon 'Qualification' and 'Experience' #############\n    for qual in list(Data.Qualification.unique()):\n        Mean = Data[(Data['Qualification'] == qual) & (Data['Experience'] >= 30)]['Rating'].mean()\n        Data.loc[(Data['Qualification'] == qual) & (Data['Experience'] >= 30), 'Rating'] = Mean\n        Mean = Data[(Data['Qualification']== qual) & (Data['Experience']>=20) & (Data['Experience']<30)]['Rating'].mean()\n        Data.loc[(Data['Qualification']==qual) & (Data['Experience']>=20) & (Data['Experience']<30),'Rating'] = Mean\n        Mean = Data[(Data['Qualification'] == qual) & (Data['Experience'] >= 10) & (Data['Experience'] < 20)]['Rating'].mean()\n        Data.loc[(Data['Qualification'] == qual) & (Data['Experience'] >= 10) & (Data['Experience'] < 20),'Rating'] = Mean\n        Mean = Data[(Data['Qualification'] == qual) & (Data['Experience'] <10)]['Rating'].mean()\n        Data.loc[(Data['Qualification'] == qual) & (Data['Experience'] <10),'Rating'] = Mean\n    ########## Fill remaining null values, if any, with 0 ################################\n    Data['Rating'] = Data['Rating'].fillna(0)\n    \n    ########### Dropping 'Miscellaneous_Info' column ####################\n    Data.drop('Miscellaneous_Info', axis=1, inplace=True)\n\n    \n    return Data\n","bac8ccc5":"df = PreProcess(TrainData)","9fe1fdc4":"df.head()","483fe38f":"df.describe()","56c365ab":"sns.set(context= \"notebook\", color_codes=True)\nfig, ax = plt.subplots(nrows=3, ncols=2, dpi=100, figsize=(20,16))\n                                                          \nsns.distplot(df['Fees'], ax=ax[0,0])\nax[0,0].set_ylabel('Fees', fontsize =24)\nax[0,0].set_xlabel(' ')\nax[0,0].tick_params(axis='both', labelsize=14)\n\nsns.boxplot(df['Fees'], ax=ax[0,1])\nax[0,1].set_xlabel(' ')\nax[0,1].tick_params(axis='both', labelsize=14)\n\nsns.distplot(df['Experience'], ax=ax[1,0], color = 'indianred')\nax[1,0].set_ylabel('Experience', fontsize =24)\nax[1,0].set_xlabel(' ')\nax[1,0].tick_params(axis='both', labelsize=14)\n\nsns.boxplot(df['Experience'], ax=ax[1,1], color = 'indianred')\nax[1,1].set_xlabel(' ')\nax[1,1].tick_params(axis='both', labelsize=14)\n\nsns.distplot(df['Rating'], ax=ax[2,0], color = 'mediumseagreen')\nax[2,0].set_ylabel('Rating', fontsize =24)\nax[2,0].set_xlabel(' ')\nax[2,0].tick_params(axis='both', labelsize=14)\n\nsns.boxplot(df['Rating'], ax=ax[2,1], color = 'mediumseagreen')\nax[2,1].set_xlabel(' ')\nax[2,1].tick_params(axis='both', labelsize=14)\n\nfig.suptitle('Distribution  of  Continuous  Variables', fontsize=30)\n\n\nplt.show()","c173cca5":"############################# Plotting categorical Variables #######################\n\nsns.set(context= \"notebook\", color_codes=True)\nfig, ax=plt.subplots(nrows=3, ncols=2, dpi=100, figsize=(20,16))\n\ndf['Place'].value_counts().plot(kind='bar', color=['mediumseagreen', 'red', 'orange', 'dodgerblue',\n                                                   'cyan', 'indianred','purple','gold'], ax=ax[0,0], rot=30)\nax[0,0].tick_params(axis='both', labelsize=14)\n\ndf.boxplot(by='Place', column= ['Fees'], ax=ax[0,1], grid = False, rot=30)\nax[0,1].tick_params(axis='both', labelsize=14)\nax[0,0].set_ylabel('Places', fontsize=24)\nax[0,1].set_title(' ')\n\n\ndf['Qualification'].value_counts().plot(kind='bar', color=['mediumseagreen', 'red', 'orange', 'dodgerblue',\n                                                   'cyan', 'indianred','purple','gold'], ax=ax[1,0], rot=30)\nax[1,0].tick_params(axis='both', labelsize=14)\n\ndf.boxplot(by='Qualification', column= ['Fees'], ax=ax[1,1], grid = False, rot=30)\nax[1,1].tick_params(axis='both', labelsize=14)\nax[1,0].set_ylabel('Qualification', fontsize=24)\nax[1,1].set_title(' ')\n\ndf['Profile'].value_counts().plot(kind='bar', color=['mediumseagreen', 'red', 'orange', 'dodgerblue',\n                                                   'cyan', 'indianred','purple','gold'], ax=ax[2,0], rot=30)\nax[2,0].tick_params(axis='both', labelsize=14)\n\ndf.boxplot(by='Profile', column= ['Fees'], ax=ax[2,1], grid = False, rot=30)\nax[2,1].tick_params(axis='both', labelsize=14)\nax[2,0].set_ylabel('Profile', fontsize=24)\nax[2,1].set_title(' ')\n\nfig.suptitle('Distribution  of  Fees  By', fontsize=30)\n\nplt.show()\n","9dd1457e":"############################ Preparing Data for Modelling ####################################\n\nX = df[['Qualification', 'Experience', 'Rating','Place', 'Profile']].copy()\ny = df['Fees']\n\n### Get dummies\nX = pd.get_dummies(X,columns = ['Qualification', 'Place', 'Profile'],prefix=['Qualification', 'Place', 'Profile'])\n\n### Standardise 'Experience' and 'Rating' column\nmms = MinMaxScaler()\nX[['Experience', 'Rating']] = mms.fit_transform(X[['Experience', 'Rating']])\n\n### Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\nX_train.shape\n\n\n### Task Evaluation Score\n\ndef TaskScore(y_pred,y):\n    val = np.sqrt(np.square(np.log10(y_pred+1) - np.log10(y+1)).mean())\n    return (1 - val) ","7140d2b5":"######################################### LinearRegression() ###############################\n\n\n### Model Fitting\nLReg = LinearRegression()\nLReg.fit(X_train, y_train)\n\n### Prediction and Evaluation\n\nprint('LinearRegression\\n')\nprint('On Training Data: ')\ny_pred = LReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = LReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Linear Regression', fontsize=16)\n\nplt.show()","3d03d962":"######################################### ElasticNetRegression() ###############################\n\n\n### Model Fitting\nENetReg = ElasticNet(alpha=1.0, l1_ratio=0.5)\nENetReg.fit(X_train, y_train)\n\n### Prediction and Evaluation\n\nprint('ElasticNet Regression\\n')\nprint('On Training Data: ')\ny_pred = ENetReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = ENetReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Elastic Net Regression (default parameters)', fontsize=16)\n\nplt.show()","e225555d":"### Model Fitting\nENetReg = ElasticNet(alpha=0.01, l1_ratio=0.97)\nENetReg.fit(X_train, y_train)\n\n### Prediction and Evaluation\n\nprint('ElasticNet Regression\\n')\nprint('On Training Data: ')\ny_pred = ENetReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = ENetReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Elastic Net Regression (parameter tuning)', fontsize=16)\n\nplt.show()","c7eb627d":"##################################### RandomForestRegressor() ##########################\n\n\n### Model Fitting\nRFReg = RandomForestRegressor(n_estimators=100, random_state=0)\nRFReg.fit(X_train,y_train)\n\n### Prediction and Evaluation\n\nprint('RandomForest Regression\\n')\nprint('On Training Data: ')\ny_pred = RFReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = RFReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Random Forest Regression', fontsize=16)\n\nplt.show()","1f062166":"################################## SupportVectorRegressor() #########################################\n\n### Model Fitting\nSVReg = SVR()\nSVReg.fit(X_train,y_train)\n\n### Prediction and Evaluation\n\nprint('SupportVector Regression\\n')\nprint('On Training Data: ')\ny_pred = SVReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = SVReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Support Vector Regression (default parameters)', fontsize=16)\n\nplt.show()","48d31f49":"### Model Fitting\nSVReg = SVR(gamma=0.2, C=10, kernel = 'rbf')\nSVReg.fit(X_train,y_train)\n\n### Prediction and Evaluation\n\nprint('SupportVector Regression\\n')\nprint('On Training Data: ')\ny_pred = SVReg.predict(X_train)\nprint('Task Score :', TaskScore(y_pred,y_train))\nprint(\"Mean Squared Error : \", mean_squared_error(y_train,y_pred))\n\nprint('\\nOn Testing Data')\ny_pred = SVReg.predict(X_test)\nprint('Task Score :',TaskScore(y_pred,y_test))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\n\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize = (14,6))\n\nax[0].scatter(y_pred, (y_pred - y_test), c='b')\nax[0].set_xlabel('Predicted', fontsize=14)\nax[0].set_ylabel('Residuals', fontsize=14)\nsns.distplot(y_pred - y_test, ax=ax[1])\nax[1].set_xlabel('Residuals', fontsize=14)\nax[1].set_ylabel(' ')\nfig.suptitle('Residual Plot : Support Vector Regression (parameter tuning)', fontsize=16)\nplt.show()","30bc87e7":"TestData = pd.read_excel('..\/input\/predict-a-doctors-consultation-fee\/test.xlsx', sheet_name='Sheet1')","70a6bcd3":"dftest = PreProcess(TestData)\ndftest.head()","492fbbc7":"print('The test data has ', dftest.shape[0], 'rows and ', dftest.shape[1], 'columns')\n","972b339f":"############################ Preparing Data for Modelling ####################################\n\n### Get dummies\ndftest = pd.get_dummies(dftest,columns = ['Qualification', 'Place', 'Profile'],prefix=['Qualification', 'Place', 'Profile'])\n\n### Standardise 'Experience' and 'Rating' column\nmms = MinMaxScaler()\ndftest[['Experience', 'Rating']] = mms.fit_transform(dftest[['Experience', 'Rating']])\n\n######################################### Support Vector Regression() ###############################\n\n### Prediction and Evaluation\n\nFees = np.round(SVReg.predict(dftest), 2)\nFees = pd.DataFrame(Fees)\nFees.columns = ['Fees']\n","e6ee6762":"######################### Write the output to a csv file #########################\n\nimport csv\n\n\nFees.to_csv('Doctor Fees.csv',index=None, columns=['Fees'])\n         ","86a44b48":"1. **VISUALIZING NUMERICAL VARIABLES**","edb506a7":"The dataset contains data mostly of cities Bangalore, Mumbai and Delhi. The 'Qualification' of doctors in the datatset is dominated by 'MD' across various 'Profiles' of Dentist, General Medicine, Dermatologits, Homeopath, Ayurveda and ENT Specialists.","db7ad0b4":"3.  ELASTIC NET REGRESSION WITH FITTED PARAMETERS","14e4d14c":"There are 1420, 877 and 6 unique levels within 'Qualification', 'Place' and 'Profile' variable columns. Need to do some data cleaning with 'Qualification' and 'Place'","c9dc2abd":"4. RANDOM FOREST REGRESSOR","f3b197a8":"Inferences from Residual plots:\n\nFor a perfect model, the scatter plot of residuals should not show any patterns and should have a normal distribution.\nIn Linear Regression, the distribution of residuals(diff(predicted, actual)) is a bit left skewed.\nElastic Net Regression with default parameters shows errorneous dstribution of residuals. With particular values of alpha and l1_ratio, residuals of Elastic Net Regression shows near normal distribution.\nRandom Forest with default parameters shows normal distribution of residuals and hence the model can be considered. But the difference in MSE score between train and test data does not make the model preferable. There might be a problem of overfitting the train data by the model.\nSupport Vector Regression with tuned parameters gives a better distribution of residuals rather than the default parameters.\nHence the models to be reckoned are :\nLinear Regression\nElastic Net Regression with fitted parameters\nSupport Vector Regression with fitted parameters.\n\nFor this problem I choose Support Vector Regression Model with fitted parameters.","1b1577c0":"# **BUILDING A MODEL TO PREDICT DOCTOR'S CONSULTATION FEES**","2d08f6db":"# **PREPARING DATA FOR MODELLING**","9225eb65":"# DATA MODELLING","5d595e83":"5. SUPPORT VECTOR REGRESSOR WITH DEFAULT PARAMETERS","1c152965":"1. Predictor Variables : Qualification, Experience, Rating, Place, Profile, Miscellaneous_Info. \n2. Target Variable : Fees","eb3c405b":"**IMPORTING REQUIRED LIBRARIES**","f5eb738a":"6. SUPPORT VECTOR REGRESSOR WITH DEFAULT PARAMETERS","690fe2f2":"There are null values in the TrainData. Rating has the most missing data, followed by Miscellaneous_Info and Place. ","5d0b3899":"1. LINEAR REGRESSION","405b56ab":"2.  ELASTIC NET REGRESSION WITH DEFAULT PARAMETERS","57d81b40":"# ****PREDICTION ON TEST SET ","933a9bfb":"# **BASIC DATA EXPLORATION**","bfeb48cb":"2. **VISUALIZING CATEGORICAL VARIABLES**","0a16d542":"Numerical Variable columns : 'Experience', 'Rating' and 'Fees' . \nCategorical variable columns : 'Qualification', 'Place', and 'Profile'.\n\nMiscellaneous_Info is an alphanumeric column. \nExtracting any logical data from Micellaneous_Info column is beyond my scope (and that might alter my final result).\nNevertheless I shall drop the column - Miscellanous_Info.\n","ecace7f3":"#  **DATA VISUALIZATION**","974835ac":"The feature variable 'Experience' is right skewed and has some outliers. \n'Rating' data is concentrated between 90 and 100.\nThere are no outliers in the target variable 'Fees'."}}