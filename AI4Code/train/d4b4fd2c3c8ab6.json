{"cell_type":{"335d8557":"code","c554c2bc":"code","c007274e":"code","2ffcb9c0":"code","caf712e8":"code","66fde0f1":"code","1cc7e5aa":"code","cdb34270":"code","cec6129e":"code","c98b3d18":"code","18d60704":"code","214aaf24":"code","19577ce3":"code","50e6cf3e":"code","207416aa":"code","83ee23e2":"code","dc4cd19f":"code","fa213b17":"code","47e26520":"code","c47ca967":"code","1be4c32a":"code","e718248c":"code","143a3141":"markdown","cd2b7a94":"markdown","c2bdf263":"markdown","e4d063cf":"markdown","ef84d7bd":"markdown","013b38e2":"markdown","b39b4191":"markdown","8da00cea":"markdown","0a8615e6":"markdown","28dd8184":"markdown","8d3442a6":"markdown","63805596":"markdown","d7c16104":"markdown","8ad1533d":"markdown","d4e8d753":"markdown"},"source":{"335d8557":"# Import some basic libraries in python for data preprocessing\n\nimport json\nimport numpy as np\nimport pandas as pd","c554c2bc":"# Read movies.json file and load into moviesData\n\nmoviesData = []\n\nwith open('..\/input\/indian-regional-movie\/movies.json') as moviesFile:\n    for line in moviesFile:\n        moviesData.append(json.loads(line))\n\n# Now take the moviesData and create a DataFrame\n\nmoviesDF = pd.DataFrame.from_records(moviesData)\n\n# View the head of the moviesDF DataFrame\n\nmoviesDF.head()","c007274e":"# Let's create a dataframe with name and description\nplotDF = moviesDF.loc[:, ['name', 'description']]\n\n# Convert both columns to lowercase\nplotDF.loc[:, 'name'] = plotDF.loc[:, 'name'].apply(lambda x: x.lower())\nplotDF.loc[:, 'description'] = plotDF.loc[:, 'description'].apply(lambda x: x.lower())\n\n# Drop all the rows which are empty\nplotDF = plotDF[plotDF['description'] != '']\n\n# Now drop duplicates in the plotDF\nplotDF = plotDF.drop_duplicates()\n\nplotDF.head()","2ffcb9c0":"# Lets quickly view the info of plotDF\nplotDF.info()\n\n# There are 2850 unique movies, we can see that there are no null description","caf712e8":"# Import the TfIdfVectorizer from scikit-learn library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define a TF IDF Vectorizer Object\n# with the removal of english stopwords turned on\ntfidf = TfidfVectorizer(stop_words = 'english')\n\n# Now costruct the TF-IDF Matrix by applying the fit_transform method on the description feature\ntfidf_matrix = tfidf.fit_transform(plotDF['description'])\n\n# View the shape of the TfIdf_matrix\ntfidf_matrix.shape","66fde0f1":"# Import linear_kernel from scikit-learn to compute the dot product\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix by using linear_kernel\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# Let's view the shape of the cosine similarity matrix\nprint(\"Shape of cosine similarity matrix: \", cosine_sim.shape)\n\n# Let's quickly view how the matrix looks like in first few column and rows\ncosine_sim[0:5, 0:5]","1cc7e5aa":"# Construct a pandas series of movie name as index and index as value\nindices = pd.Series(plotDF.index, index = plotDF['name'])\nindices.head()","cdb34270":"# Function for returning top 10 movies for a movie title as input\ndef plot_based_recommender(title, df = plotDF, cosine_sim = cosine_sim, indices = indices):\n  # Convert title to lower-case\n  title = title.lower()\n\n  # Obtain the index of the movie that matched the title \n  try:\n    idx = indices[title]\n  except KeyError:\n    print('Movie does not exist :(')\n    return False\n\n  # Get the pairwise similarity score of all the movies with that movie\n  # and convert it into a list of tuples (position, similarity score)\n  sim_scores = list(enumerate(cosine_sim[idx]))\n\n  # Sort the movies based on the cosine similarity scores\n  sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n\n  # Get the scores of the top 10 most similar movies. Ignore the first movie.\n  sim_scores = sim_scores[1:11]\n\n  # get the movie indices\n  movie_indices = [sim_score[0] for sim_score in sim_scores]\n\n  # Return the top 10 similar movies\n  return df['name'].iloc[movie_indices]","cec6129e":"plot_based_recommender('Pyaar Ka Punchnama')","c98b3d18":"# Prepare the data\nmetaDF = moviesDF[['name', 'genre', 'language', 'director', 'cast', 'description']]\nmetaDF.head()","18d60704":"# We want to keep only the first 3 genre and cast (actors) in the list format\n# We want to keep only the first director\n\nmetaDF.loc[:, 'genre'] = metaDF.loc[:, 'genre'].apply(lambda x: x[:3] if len(x) > 3 else x)\nmetaDF.loc[:, 'cast'] = metaDF.loc[:, 'cast'].apply(lambda x: x[:3] if len(x) > 3 else x)\nmetaDF.loc[:, 'director'] = metaDF.loc[:, 'director'].apply(lambda x: x[:1])\nmetaDF.head()","214aaf24":"def sanitize(x):\n    if isinstance(x, list):\n        # Strip spaces\n        return [i.replace(\" \", \"\") for i in x]\n    else:\n        # if it is empty, return an empty string\n        if isinstance(x, str):\n            return x.replace(\" \", \"\")\n        else: \n            return ''","19577ce3":"#Apply the generate_list function to cast, keywords, director and genres \nfor feature in ['director', 'cast']:\n    metaDF[feature] = metaDF[feature].apply(sanitize)","50e6cf3e":"metaDF.head()","207416aa":"# Function that creates a soup out of the desired metadata\ndef create_soup(x):\n    return ' '.join(x['genre']) + ' ' + ' '.join(x['director']) + ' ' + ' '.join(x['cast'] + [x['name']] + [x['description']])\n\n# Create the new soup feature \nmetaDF['soup'] = metaDF.apply(create_soup, axis=1)\n\n#Display the soup of the first movie \nmetaDF.iloc[0]['soup']","83ee23e2":"# Let's create a dataframe with name and soup\nsoupDF = metaDF.loc[:, ['name', 'soup']]\n\n# Convert both columns to lowercase\nsoupDF.loc[:, 'name'] = soupDF.loc[:, 'name'].apply(lambda x: x.lower())\nsoupDF.loc[:, 'soup'] = soupDF.loc[:, 'soup'].apply(lambda x: x.lower())\n\n# Drop all the rows which are empty\nsoupDF = soupDF[soupDF['soup'] != '']\n\n# Now drop duplicates in the soupDF\nsoupDF = soupDF.drop_duplicates()\n\nsoupDF.head()\nprint(soupDF.shape)","dc4cd19f":"# Import the TfIdfVectorizer from scikit-learn library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define a TF IDF Vectorizer Object\n# with the removal of english stopwords turned on\ntfidf = TfidfVectorizer(stop_words = 'english')\n\n# Now costruct the TF-IDF Matrix by applying the fit_transform method on the description feature\ntfidf_matrix = tfidf.fit_transform(soupDF['soup'])\n\n# View the shape of the TfIdf_matrix\ntfidf_matrix.shape","fa213b17":"# Import linear_kernel from scikit-learn to compute the dot product\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix by using linear_kernel\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# Let's view the shape of the cosine similarity matrix\nprint(\"Shape of cosine similarity matrix: \", cosine_sim.shape)\n\n# Let's quickly view how the matrix looks like in first few column and rows\ncosine_sim[0:5, 0:5]","47e26520":"# Construct a pandas series of movie name as index and index as value\nindices = pd.Series(soupDF.index, index = soupDF['name'])\nindices.head()","c47ca967":"# Function for returning top 10 movies for a movie title as input\ndef plot_based_recommender(title, df = soupDF, cosine_sim = cosine_sim, indices = indices):\n  # Convert title to lower-case\n  title = title.lower()\n\n  # Obtain the index of the movie that matched the title \n  try:\n    idx = indices[title]\n  except KeyError:\n    print('Movie does not exist :(')\n    return False\n\n  # Get the pairwise similarity score of all the movies with that movie\n  # and convert it into a list of tuples (position, similarity score)\n  sim_scores = list(enumerate(cosine_sim[idx]))\n\n  # Sort the movies based on the cosine similarity scores\n  sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n\n  # Get the scores of the top 10 most similar movies. Ignore the first movie.\n  sim_scores = sim_scores[1:11]\n\n  # get the movie indices\n  movie_indices = [sim_score[0] for sim_score in sim_scores]\n\n  # Return the top 10 similar movies\n  return df['name'].iloc[movie_indices]","1be4c32a":"plot_based_recommender('Pyaar Ka Punchnama')","e718248c":"# database \ndb = {} \ndb['cosine_sim'] = cosine_sim\ndb['indices'] = indices\n\nimport pickle\n\ndbfile = open('examplePickle', 'ab') \npickle.dump(db, dbfile)                      \ndbfile.close() ","143a3141":"# Indian Regional Recommendation Systems\n\nThe goal for a recommendation system is to extract information from data about the relationship existing between users and products. One of the common usage is to take the products that the user already likes and and answer the question \"What other products can be recommended to the user?\"\n\nThere are multiple ways of answering the above question. These are the three main types algorithms being used for recommendation systems, to try answering different aspects of recommendations:\n\n- **Content Based Filtering**\n  - Used to find products with \"similar\" attributes. Example: if a person likes movies from action genere then recommend other books from action genere.\n  - The movie will have different attributes, such as, direction, cast, cinematography, story etc. A field specialist will rate all these attributes on a fixed scale.\n  - Based on similarity between these attributes, we can recommend movies to a new user once they have picked up a few movies that they liked.\n  - One major drawback of this method is getting a field expert to rate movies on their attributes. It is time conuming and inefficient.\n  \n- **Collaberative Filtering**\n  - This is somewhat indirect way of recommending, but is the most used method in the industry. Here we find products liked by \"similar\" users (they have the same interest as the active user) and recommend them to the active user. \n  - For an active user, who has rated some movies highly in the system; we find other users who have rated these movies in a similar manner (they might have rated other movies as well). These users have commen interests, so we can use other user's ratings as a guide for the likeliness of movie recommendation to the active user.\n  - Here the assumption is that the movies that \"similar\" users like are similar to each other. In this method also we are measuring similarity of products, however here the similarity is measured indirectly through similarity of users.\n- **Association Rules Learning**\n  - Here we recommend \"complimentary\" products, for example, if someone is buying a smartphone then recommend a back cover or a tempered glass etc.\n  - In movie recommendation, the products are substitutable to each other. However, in the above example of smartphone, the products are complimentary.\n  - This algorithm tries to answer the question, \"If a person likes to buy smartphones, along with it, what else would they like to buy?\" i.e \"Which products are *associated* with each other?\"","cd2b7a94":"### 2. Create the TF-IDF matrix\n\nEach row of this matrix represents the TF_IDF vector of the description feature of the corresponding movie in the plotDF data frame.","c2bdf263":"## Building a content based recommender system\n\nHere we will attempt to built two types of content based recommendation systems\n\n* **Plot Based Recommendation System**: This model takes movie descriptions and taglines into consideration and provides recommendation with similar plot descriptions.\n\n* **Metadata Based Recommendation System**: This model takes different features such as, genres, keywords, cast, and crew etc into consideration and provides recommendations that are most similar.","e4d063cf":"### Prep the Data","ef84d7bd":"#### TF-IDF  Vectorizer\n\nNot all words in a document carry equal weight, (for example we saw that stop words have no weightage at all).\n\n**TF-IDF: Term Frequency - Inverse Document Frequency**\n\nIt assigns the weights to each word according to the formula (for every word i in document j):\n\n$$ w_{i,j} = tf_{i,j} \\times log\\bigg(\\frac{N}{df_i} \\bigg) $$\n\nWhere,\n\n  $w_{i,j}$ is the weight of word i in document j\n\n  $tf_{i,j}$ is the term frequency for word i in document j\n\n  $N$ is the total number of documents\n\n  $df_i$ is the number of documents that contain the term i\n  \n<br>\n\n**pro**: Speeds up the computation of cossine similarity score","013b38e2":"## I. Plot Based Recommendation System\n\n**Goal:** Compute the similarity matrix of all the movies with each other, based on their plot text (using pairwise cosine similarity method)\n\n**Approach:** Represent plot text (or documents) as vectors i.e a series of numbers, where each number\/dimension represents occurance of a specific word in the vocabulary. The size of the vocabulary vector is the number of unique words present when all the documents are put together.\n\n**Methods:** To create these vectors for all the movies, we use vectorizers;\n  - Count Vectorizer\n  - TF-IDF Vectorizer","b39b4191":"### Steps for building a Plot based Recommender\n\nIt takes a movie title as an argument and outputs a list of movies that are most similar based on their plots.\n\n**Steps**\n\n1. Clean the data to the format required to build the model\n2. Create TF-IDF vectors for the plot description of every movie\n3. Compute the pairwise cosine similarity between every movie\n4. Write a recommender function that takes in a movie title as an argument and outputs movies most similar to it based on the plot.","8da00cea":"#### 4. Build the recommender function\n\n**Steps**\n\n1. Input the title of the movie as an argument\n2. Obtain the index of the movie from the *indeces* series\n3. Get the list of cosine similarity scores for that particular movie with all movies using cosine_sim matrix.And, convert this in to a list of tuples where the first element is the position and second element is the similarity score.\n4. Sort this list of tuples on the basis of the cosine similarity scores.\n5. Get the top 10 elements of the list, while ignoring the first element as it refers to the similarity score with itself.\n6. Return the titles corresponding to the indices of the top 10 elements.","0a8615e6":"### Cosine Similarity:\n\nCosine similarity between two documents, x and y,\n\n$$ cosine(x, y) = \\frac{x.y^T}{||x||.||y||} $$\n\nIt takes a value between -1 and 1, higher the score, more similar the documents are to each other.","28dd8184":"## II. Metadata Based Recommender System\n\nTo build this model, we will be using the following meta-data:\n\n- genre\n- language\n- director\n- cast\n- smovie description\n\nApart from the difference in data being used, we will largely follow the same steps as the plot based recommender system.","8d3442a6":"#### Count Vectorizer\n\nThis is the simplest type of vectorizer. \n\nConsider an example, where we have three documents (texts)\n\nA: The sun is a star\n\nB: My love is like a red, red rose\n\nC: Mary had a little lamb\n\nNow our goal is to convert these documents into vectors.\n\n**Step-1**\n\nWe first compute the vocabulary; i.e the vector comprising of all the unique words present across all the documents.\n\nV = (the, sun, is, a, star, my, love, like, red, rose, mary, had, little, lamb)\n\nThe size of this vocabulary is 14.\n\n__Special__\n\nA common practice is to remove common words e.g. a, the, is, had, my etc.. These are also called *'Stop Words'*.\n\nAfter removing these stop words;\n\nV: (like, little, lamb, love, mary, red, rose, sun, star)\n\n**Step-2**\n\nNow each document is interpreted as a vector of size 9, where each dimension represents the number of times each word occurs.\n\nSo, using the CountVectorizer approach, A, B and C will be represented as:\n\nA: (0, 0, 0, 0, 0, 0, 0, 1, 1)\n\nB: (1, 0, 0, 1, 0, 2, 1, 0, 0)\n\nC: (0, 1, 1, 0, 1, 0, 0, 0, 0)","63805596":"### Get movies data from file into DataFrame","d7c16104":"### 1. Prep the data","8ad1533d":"The spaces between the names of actors and directors can create problems as they can be considered as separate words. We do not want that, so our next step is to strip the spaces between the names.","d4e8d753":"### 3. Computing the cosine similarity score\n\nHere we are going to create a matrix of size 1751 x 1751, where i-th row and j-th column column represents the similarity score between movies i and j.\n\nThis matrix will be symmetric in nature and all the elements along the diagonal will be 1, since it the similarity score of the matrix with itself.\n\nAlso, as we have represented the movie plots as TF-IDF vectors, they have the magnitude of 1. So, we need not calculate the magnitude of the dot product (denominator of the cosine similarity function will always be 1).\n\nSo out cosine similarity function reduces to:\n\n$$cosine(x, y) = x.y^T$$\n\n"}}