{"cell_type":{"f104be1c":"code","b99e4de4":"code","78614af3":"code","18877497":"code","e83fa3da":"code","32f11110":"code","44bdbe29":"code","5c7db40b":"code","1610786a":"code","aa6d6f5c":"code","f2b6c1b9":"code","fd9ae119":"code","7509ca45":"code","18a6f41d":"code","62ff1d87":"code","d944c477":"code","b70cf073":"code","eaac6f13":"code","e361826c":"markdown","6a546212":"markdown","806e0adc":"markdown","abfae678":"markdown","bd5d27bc":"markdown","4b47c3a1":"markdown","edb9476a":"markdown"},"source":{"f104be1c":"import numpy as np\nimport pandas as pd\nimport datatable as dt\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import precision_recall_curve\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport gc\nimport janestreet\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline","b99e4de4":"def print_score(pred, y_test):\n    print('accuracy: {:.4}'.format(accuracy_score(pred, y_test)))\n    print('precision: {:.4}'.format(precision_score(pred, y_test)))\n    print('recall: {:.4}'.format(recall_score(pred, y_test)))\n    print('f1: {:.4}'.format(f1_score(pred, y_test)))\n    print('auc: {:.4}'.format(roc_auc_score(pred, y_test)))\n\ndef reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n#                 reducing float16 for calculating numpy.nanmean\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df\n\ndef precision_recall_curve_plot(y_test, pred_proba_c1):\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n    \n    plt.figure(figsize=(8,6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n    \n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n    \n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","78614af3":"path = '\/kaggle\/input\/jane-street-market-prediction\/'\ntrain = dt.fread(path+'train.csv')\ntrain = train.to_pandas()","18877497":"train.info()","e83fa3da":"train.head()","32f11110":"train = reduce_memory_usage(train)\ntrain.info()","44bdbe29":"len(train), len(train[train['weight']>0])","5c7db40b":"train['action']=0\ntrain.loc[train['resp']>0.0,'action']=1\nfeatures = ['feature_{}'.format(i) for i in range(0,130)]","1610786a":"train","aa6d6f5c":"train_df = train[train['date']>85]\ndel train","f2b6c1b9":"train_data = train_df[features]\ntrain_target = train_df['action']","fd9ae119":"print(train_data.shape, train_target.shape)","7509ca45":"train_data.head()","18a6f41d":"train_target.value_counts()","62ff1d87":"lgb_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'num_leaves': 300,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n#     'reg_lambda': 0.3,\n#     'reg_alpha': 0.3\n}","d944c477":"lgb_clf = LGBMClassifier(n_jobs=-1)\nlgb_clf.fit(train_data, train_target)","b70cf073":"ftr_importances_values = lgb_clf.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index=train_data.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:30]\n\nplt.figure(figsize=(8,6))\nplt.title('Feature importances top 30')\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","eaac6f13":"env = janestreet.make_env() # initialize the environment \niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test: \n    sample_prediction_df.action = lgb_clf.predict(test_df[features])\n    env.predict(sample_prediction_df)","e361826c":"# Defining functions","6a546212":"# packages","806e0adc":"# Preprocessing data","abfae678":"# Modeling","bd5d27bc":"# Loading files","4b47c3a1":"# Submission","edb9476a":"### model selection"}}