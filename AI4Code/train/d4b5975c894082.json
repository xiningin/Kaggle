{"cell_type":{"6bff73e7":"code","32ec408b":"code","44a77f86":"code","1440f649":"code","a5766f8b":"code","41dcf4a9":"code","96718f4e":"code","d113f17f":"code","bf75a817":"code","05ef803f":"code","38db12b8":"code","12e9c929":"code","e59716da":"code","7116c6aa":"code","942ba027":"code","afeb7806":"code","7a769847":"code","a99b0549":"code","9793a18c":"code","7bf016ae":"code","31c06119":"code","b9ca52c4":"code","d3a28dd9":"code","9617802a":"code","d1bceb9a":"code","1efe2801":"code","86db8cd8":"code","977bb768":"code","a7bd6ae0":"code","872e0a4d":"code","c11e78f2":"code","3de3126c":"code","1dc447b1":"code","edabc7c0":"code","c2a930cd":"code","a46d5da8":"code","dfd05541":"code","4b84a3e3":"code","cf90fe83":"code","cfb52c3d":"code","b499bf72":"code","b5e62fbe":"code","4a7ff4d3":"code","d666f279":"code","3f24ffe9":"code","a80583e2":"code","81434f03":"code","53e9d318":"code","8d975ae3":"code","533c66d1":"code","1c3e4f7f":"code","2b214844":"code","dbef24fd":"code","b05b8875":"code","f6737f59":"code","c2d6e92e":"code","151547fd":"code","7730158b":"code","8710d6be":"code","5c653aca":"code","a2b9b0f2":"markdown","b9c2588e":"markdown","75fa742a":"markdown","fd9421e6":"markdown","f33028d7":"markdown","6b90c0e3":"markdown","3d2bbb3c":"markdown","da4bebde":"markdown","76e66048":"markdown","5f171bea":"markdown"},"source":{"6bff73e7":"!pip install git+https:\/\/github.com\/tensorflow\/examples.git","32ec408b":"# GENERAL\n\nimport os\nimport os.path\nfrom pathlib import Path\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport math\nimport glob\nimport cv2\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# I PACKAGES\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU, ReLU, Conv2DTranspose\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\n# WARNINGS\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)\n\n\nAUTO_MODEL_TUNE = tf.data.AUTOTUNE","44a77f86":"FACE_PATH = Path(\"..\/input\/pretty-face\/face\/face\")\nSEG_PATH = Path(\"..\/input\/pretty-face\/layout\/layout\")","1440f649":"FACE_LIST_RED = list(FACE_PATH.glob(r\"*.png\"))\nSEG_LIST_RED = list(SEG_PATH.glob(r\"*.png\"))","a5766f8b":"print(len(FACE_LIST_RED))\nprint(len(SEG_LIST_RED))","41dcf4a9":"FACE_LIST = FACE_LIST_RED[:70]\nSEG_LIST = SEG_LIST_RED[:70]","96718f4e":"print(len(FACE_LIST))\nprint(len(SEG_LIST))","d113f17f":"FACE_SERIES = pd.Series(FACE_LIST,name=\"FACE\").astype(str)\nSEG_SERIES = pd.Series(SEG_LIST,name=\"SEG\").astype(str)\n\nMAIN_PROCESS_DATA = pd.concat([FACE_SERIES,SEG_SERIES],axis=1)","bf75a817":"MAIN_PROCESS_DATA","05ef803f":"for index_random in range(5):\n    \n    \n    EXAMPLE_FACE = cv2.cvtColor(cv2.imread(MAIN_PROCESS_DATA[\"FACE\"][index_random]),cv2.COLOR_BGR2RGB)\n    EXAMPLE_SEG = cv2.cvtColor(cv2.imread(MAIN_PROCESS_DATA[\"SEG\"][index_random]),cv2.COLOR_BGR2RGB)\n\n\n    figure,axis = plt.subplots(1,2,figsize=(10,10))\n\n    axis[0].set_title(\"FACE\")\n    axis[0].set_xlabel(EXAMPLE_FACE.shape)\n    axis[0].imshow(EXAMPLE_FACE)\n\n    axis[1].set_title(\"SEG\")\n    axis[1].set_xlabel(EXAMPLE_SEG.shape)\n    axis[1].imshow(EXAMPLE_SEG)\n\n    plt.tight_layout()\n    plt.show()","38db12b8":"BUFFER_SIZE = 400\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nOUTPUT_CHANNELS = 3\nLAMBDA = 100\nEPOCHS = 70\nTYPE_ARRAY = \"float32\"","12e9c929":"FACE_MAIN_LIST = []\nSEG_MAIN_LIST = []\n\nfor x_face,x_seg in zip(MAIN_PROCESS_DATA.FACE,MAIN_PROCESS_DATA.SEG):\n    \n    FACE_IMG = cv2.cvtColor(cv2.imread(x_face),cv2.COLOR_BGR2RGB)\n    SEG_IMG = cv2.cvtColor(cv2.imread(x_seg),cv2.COLOR_BGR2RGB)\n    \n    RESIZE_FACE = cv2.resize(FACE_IMG,(IMG_WIDTH,IMG_HEIGHT))\n    RESIZE_SEG = cv2.resize(SEG_IMG,(IMG_WIDTH,IMG_HEIGHT))\n    \n    REDUCE_FACE = RESIZE_FACE \/ 255.\n    REDUCE_SEG = RESIZE_SEG \/ 255.\n    \n    FACE_MAIN_LIST.append(REDUCE_FACE)\n    SEG_MAIN_LIST.append(REDUCE_SEG)","e59716da":"print(np.shape(np.array(FACE_MAIN_LIST)))\nprint(np.shape(np.array(SEG_MAIN_LIST)))","7116c6aa":"for index_random in range(5):\n    \n    \n    EXAMPLE_FACE = FACE_MAIN_LIST[index_random]\n    EXAMPLE_SEG = SEG_MAIN_LIST[index_random]\n\n\n    figure,axis = plt.subplots(1,2,figsize=(10,10))\n\n    axis[0].set_title(\"FACE\")\n    axis[0].set_xlabel(EXAMPLE_FACE.shape)\n    axis[0].imshow(EXAMPLE_FACE)\n\n    axis[1].set_title(\"SEG\")\n    axis[1].set_xlabel(EXAMPLE_SEG.shape)\n    axis[1].imshow(EXAMPLE_SEG)\n\n    plt.tight_layout()\n    plt.show()","942ba027":"ARRAY_FACE = np.array(FACE_MAIN_LIST,dtype=TYPE_ARRAY)\nARRAY_SEG = np.array(SEG_MAIN_LIST,dtype=TYPE_ARRAY)","afeb7806":"print(ARRAY_FACE.shape)\nprint(ARRAY_SEG.shape)","7a769847":"TARGET_TENSOR = tf.data.Dataset.from_tensor_slices(ARRAY_SEG).batch(BATCH_SIZE)\nOUTPUT_TENSOR = tf.data.Dataset.from_tensor_slices(ARRAY_FACE).batch(BATCH_SIZE)","a99b0549":"print(TARGET_TENSOR.element_spec)\nprint(OUTPUT_TENSOR.element_spec)","9793a18c":"def DOWN_SAMPLE(filters,size,apply_batchnorm=True):\n    \n    Ini_Kernel = tf.random_normal_initializer(0.,0.03)\n    \n    Model = Sequential()\n    \n    Model.add(Conv2D(filters,\n                     size,\n                     strides=2,\n                     padding=\"same\",\n                     use_bias=False,\n                     kernel_initializer=Ini_Kernel))\n    \n    \n    if apply_batchnorm:\n        Model.add(BatchNormalization())\n        \n        \n    return Model","7bf016ae":"Model_DOWN = DOWN_SAMPLE(3,4)","31c06119":"def UP_SAMPLE(filters,size,apply_dropout=False):\n    \n    Ini_Kernel = tf.random_normal_initializer(0.,0.03)\n    \n    Model = Sequential()\n    \n    Model.add(Conv2DTranspose(filters,\n                              size,\n                              strides=2,\n                              padding=\"same\",\n                              use_bias=False,\n                              kernel_initializer=Ini_Kernel))\n    \n    \n    Model.add(BatchNormalization())\n    \n    if apply_dropout:\n        \n        Model.add(Dropout(0.2))\n        \n    Model.add(ReLU())\n    \n    return Model","b9ca52c4":"Model_UP = UP_SAMPLE(3,4)","d3a28dd9":"def Generator_Model(width_i,height_i,dimension_i):\n    \n    INPUT_LAYER = tf.keras.layers.Input(shape=[width_i,height_i,dimension_i])\n    \n    DOWN_LIST = [\n        \n        DOWN_SAMPLE(64,4,apply_batchnorm=False),\n        DOWN_SAMPLE(128,4),\n        DOWN_SAMPLE(256,4),\n        DOWN_SAMPLE(512,4),\n        DOWN_SAMPLE(512,4),\n        DOWN_SAMPLE(512,4),\n        DOWN_SAMPLE(512,4),\n        DOWN_SAMPLE(512,4)\n    ]\n    \n    \n    UP_LIST = [\n        \n        UP_SAMPLE(512,4,apply_dropout=True),\n        UP_SAMPLE(512,4,apply_dropout=True),\n        UP_SAMPLE(512,4),\n        UP_SAMPLE(512,4),\n        UP_SAMPLE(256,4),\n        UP_SAMPLE(128,4),\n        UP_SAMPLE(64,4)\n    ]\n    \n    \n    Ini_Kernel = tf.random_normal_initializer(0.,0.03)\n    \n    TRANS_LAYER = Conv2DTranspose(OUTPUT_CHANNELS,4,strides=2,\n                                 padding=\"same\",\n                                 kernel_initializer=Ini_Kernel,\n                                 activation=\"tanh\")\n    \n    LAST_X = INPUT_LAYER\n    \n    SKIPPING_PRE_LAYER = []\n    \n    for x_down in DOWN_LIST:\n        \n        LAST_X = x_down(LAST_X)\n        SKIPPING_PRE_LAYER.append(LAST_X)\n        \n    SKIPPING_PRE_LAYER = reversed(SKIPPING_PRE_LAYER[:-1])\n    \n    for x_up,x_skip in zip(UP_LIST,SKIPPING_PRE_LAYER):\n        \n        LAST_X = x_up(LAST_X)\n        LAST_X = tf.keras.layers.Concatenate()([LAST_X,x_skip])\n        \n    LAST_X = TRANS_LAYER(LAST_X)\n    \n    return tf.keras.Model(inputs=INPUT_LAYER,outputs=LAST_X)","9617802a":"Generation_Model = Generator_Model(IMG_WIDTH,IMG_HEIGHT,OUTPUT_CHANNELS) ","d1bceb9a":"tf.keras.utils.plot_model(Generation_Model, show_shapes=True)","1efe2801":"def Discriminator_Model(width_i,height_i,dimension_i):\n    \n    Ini_Kernel = tf.random_normal_initializer(0.,0.03)\n    \n    INPUT_LAYER = tf.keras.layers.Input(shape=[width_i,height_i,dimension_i],name=\"INPUT_LAYER\")\n    TARGET_LAYER = tf.keras.layers.Input(shape=[width_i,height_i,dimension_i],name=\"TARGET_LAYER\")\n    \n    LAYER_X = tf.keras.layers.concatenate([INPUT_LAYER,TARGET_LAYER])\n    \n    DOWN_SAMPLE_1 = DOWN_SAMPLE(64,4,False)(LAYER_X)\n    DOWN_SAMPLE_2 = DOWN_SAMPLE(64,4)(DOWN_SAMPLE_1)\n    DOWN_SAMPLE_3 = DOWN_SAMPLE(64,4)(DOWN_SAMPLE_2)\n    \n    ZERO_PADD_LAY_1 = tf.keras.layers.ZeroPadding2D()(DOWN_SAMPLE_3)\n    CONV2D_LAY_1 = Conv2D(512,\n                        4,\n                        strides=1,\n                        kernel_initializer=Ini_Kernel,\n                        use_bias=False)(ZERO_PADD_LAY_1)\n    \n    BATCH_LAY_1 = BatchNormalization()(CONV2D_LAY_1)\n    ACT_LAY_1 = LeakyReLU()(BATCH_LAY_1)\n    \n    ZERO_PADD_LAY_2 = tf.keras.layers.ZeroPadding2D()(ACT_LAY_1)\n    \n    LAST_LAY = Conv2D(1,\n                     4,\n                     strides=1,\n                     kernel_initializer=Ini_Kernel)(ZERO_PADD_LAY_2)\n    \n    return tf.keras.Model(inputs=[INPUT_LAYER,TARGET_LAYER],outputs=LAST_LAY)","86db8cd8":"Discrimination_Model = Discriminator_Model(IMG_WIDTH,IMG_HEIGHT,OUTPUT_CHANNELS)","977bb768":"tf.keras.utils.plot_model(Discrimination_Model, show_shapes=True)","a7bd6ae0":"LOSS_FUNCTION = tf.keras.losses.BinaryCrossentropy(from_logits=True)","872e0a4d":"def Generation_Loss(discriminator_out,generator_out,target_in):\n    \n    GAN_LOSS = LOSS_FUNCTION(tf.ones_like(discriminator_out),discriminator_out)\n    \n    MEAN_ABS_ERROR_OUT = tf.reduce_mean(tf.abs(target_in - generator_out))\n    \n    TOTAL_LOSS_OUT = GAN_LOSS + (LAMBDA * MEAN_ABS_ERROR_OUT)\n    \n    return TOTAL_LOSS_OUT,GAN_LOSS,MEAN_ABS_ERROR_OUT","c11e78f2":"def Discrimination_Loss(real_out,generated_out):\n    \n    REAL_LOSS = LOSS_FUNCTION(tf.ones_like(real_out),real_out)\n    GEN_LOSS = LOSS_FUNCTION(tf.zeros_like(generated_out),generated_out)\n    \n    TOTAL_LOSS = REAL_LOSS + GEN_LOSS\n    \n    return TOTAL_LOSS","3de3126c":"GENERATOR_OPT = tf.keras.optimizers.RMSprop(lr=0.0003,clipvalue=1.0,decay=1e-8)\nDISCRIMINATOR_OPT = tf.keras.optimizers.RMSprop(lr=0.0003,clipvalue=1.0,decay=1e-8)","1dc447b1":"os.mkdir(\".\/OUT_CLASS\")\n\ndef generate_images_single(model, test_input, tar, number_i):\n    \n    prediction = model(test_input, training=True)\n    \n    plt.figure(figsize=(15, 15))\n    \n    plt.imshow(prediction[0])\n    plt.savefig('.\/OUT_CLASS\/out_res_{:04d}.png'.format(number_i))\n    plt.axis('off')\n    \n    plt.tight_layout()    \n    plt.show()","edabc7c0":"os.mkdir(\".\/MULTI_CLASS\")\n\ndef generate_images_for_example(model, test_input, tar, number_i):\n    \n    prediction = model(test_input, training=True)\n    \n    RESULT_IN_OUT_LIST = [test_input[0], tar[0], prediction[0]]\n    \n    \n    figure,axis = plt.subplots(1, 3,figsize=(14,14))\n\n    axis[0].imshow(RESULT_IN_OUT_LIST[0])\n    axis[0].axis('off')\n\n    axis[1].imshow(RESULT_IN_OUT_LIST[1])\n    axis[1].axis('off')\n\n    axis[2].imshow(RESULT_IN_OUT_LIST[2])\n    axis[2].axis('off')\n    \n    plt.savefig('.\/MULTI_CLASS\/multi_res_{:04d}.png'.format(number_i))\n    \n    plt.tight_layout()\n    plt.show()","c2a930cd":"@tf.function\n\ndef Train_Process(INPUT_TENSOR,TARGET_TENSOR,EPOCH):\n    \n    with tf.GradientTape() as GEN_TAPE, tf.GradientTape() as DISC_TAPE:\n        \n        GENERATION_OUT = Generation_Model(INPUT_TENSOR,training=True)\n        \n        DISCRIMINATION_REAL = Discrimination_Model([INPUT_TENSOR,TARGET_TENSOR],training=True)\n        DISCRIMINATION_FAKE = Discrimination_Model([INPUT_TENSOR,GENERATION_OUT],training=True)\n        \n        GEN_TOTAL_LOSS,GENERATOR_LOSS,GEN_MEAN_ABS_ERROR = Generation_Loss(DISCRIMINATION_FAKE,\n                                                                          GENERATION_OUT,\n                                                                          TARGET_TENSOR)\n        \n        \n        DISC_LOSS = Discrimination_Loss(DISCRIMINATION_REAL,DISCRIMINATION_FAKE)\n        \n        \n        \n    Generator_Gradient = GEN_TAPE.gradient(GEN_TOTAL_LOSS,Generation_Model.trainable_variables)\n    Discriminator_Gradient = DISC_TAPE.gradient(DISC_LOSS,Discrimination_Model.trainable_variables)\n    \n    GENERATOR_OPT.apply_gradients(zip(Generator_Gradient,Generation_Model.trainable_variables))\n    DISCRIMINATOR_OPT.apply_gradients(zip(Discriminator_Gradient,Discrimination_Model.trainable_variables))","a46d5da8":"EXAMPLE_INP,EXAMPLE_TAR = next(iter(TARGET_TENSOR.take(1))),next(iter(OUTPUT_TENSOR.take(1)))\n\ncounting_img = 0\n\nfor epoch in range(EPOCHS):\n\n    n_count = 0\n    \n    for image_x, image_y in tf.data.Dataset.zip((TARGET_TENSOR.take(epoch),OUTPUT_TENSOR.take(epoch))):\n        \n        Train_Process(image_x, image_y, epoch)\n        \n        if n_count % 10 == 0:\n            \n            print ('.', end='')\n            \n        n_count += 1\n        \n    \n        clear_output(wait=True)\n        \n        generate_images_for_example(Generation_Model,\n                                    image_x,\n                                    image_y,\n                                    counting_img)\n    \n    \n    \n    \n        counting_img = counting_img + 1","dfd05541":"TEST_SEG_LIST_NEW = SEG_LIST_RED[600:605]\nTEST_SERIES = pd.Series(TEST_SEG_LIST_NEW,name=\"TEST\").astype(str)","4b84a3e3":"TESTING_IMG_LIST = []\n\nfor x_seg in TEST_SERIES:\n\n    SEG_IMG = cv2.cvtColor(cv2.imread(x_seg),cv2.COLOR_BGR2RGB)\n    \n    RESIZE_SEG = cv2.resize(SEG_IMG,(IMG_WIDTH,IMG_HEIGHT))\n    \n    REDUCE_SEG = RESIZE_SEG \/ 255.\n\n    TESTING_IMG_LIST.append(REDUCE_SEG)","cf90fe83":"TEST_ARRAY = np.array(TESTING_IMG_LIST,dtype=\"float32\")","cfb52c3d":"TEST_TENSOR = tf.data.Dataset.from_tensor_slices(TEST_ARRAY).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","b499bf72":"def generate_images_predict(model, test_input):\n    \n    prediction = model(test_input, training=True)\n    \n    plt.figure(figsize=(15, 15))\n    \n    plt.imshow(prediction[0])\n    plt.savefig('.\/OUT_CLASS\/out_res_{:04d}.png'.format(2))\n    plt.axis('off')\n    \n    plt.tight_layout()    \n    plt.show()","b5e62fbe":"for x_counting_tensor in TEST_TENSOR.take(3):\n    \n    generate_images_predict(Generation_Model,x_counting_tensor)","4a7ff4d3":"TEST_SEG_LIST_NEW_2 = SEG_LIST_RED[610:655]\nTEST_SERIES_2 = pd.Series(TEST_SEG_LIST_NEW_2,name=\"TEST_'\").astype(str)","d666f279":"TESTING_IMG_LIST_2 = []\n\nfor x_seg in TEST_SERIES_2:\n\n    SEG_IMG = cv2.cvtColor(cv2.imread(x_seg),cv2.COLOR_BGR2RGB)\n    \n    RESIZE_SEG = cv2.resize(SEG_IMG,(IMG_WIDTH,IMG_HEIGHT))\n    \n    REDUCE_SEG = RESIZE_SEG \/ 255.\n\n    TESTING_IMG_LIST_2.append(REDUCE_SEG)","3f24ffe9":"TEST_ARRAY_2 = np.array(TESTING_IMG_LIST_2,dtype=\"float32\")","a80583e2":"TEST_TENSOR_2 = tf.data.Dataset.from_tensor_slices(TEST_ARRAY_2).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","81434f03":"LIST_PREDICT_MULTI = []","53e9d318":"os.mkdir(\".\/MULTI_CLASS_PREDICT\")","8d975ae3":"def generate_images_for_multi_predict(model, test_input):\n    \n    prediction = model(test_input, training=True)\n    \n    RESULT_IN_OUT_LIST = [test_input[0], prediction[0]]\n    \n    \n    figure,axis = plt.subplots(1, 2,figsize=(15,10))\n\n    axis[0].imshow(RESULT_IN_OUT_LIST[0])\n    axis[0].axis('off')\n\n    axis[1].imshow(RESULT_IN_OUT_LIST[1])\n    axis[1].axis('off')\n\n    SAVE_NUM = random.randint(0,120)\n    plt.savefig('.\/MULTI_CLASS_PREDICT\/multi_res_predict{:04d}.png'.format(SAVE_NUM))\n    \n    PRE_READ_IMG = cv2.cvtColor(cv2.imread('.\/MULTI_CLASS_PREDICT\/multi_res_predict{:04d}.png'.format(SAVE_NUM)),cv2.COLOR_BGR2RGB)\n    LIST_PREDICT_MULTI.append(PRE_READ_IMG)\n    \n    plt.tight_layout()\n    plt.show()","533c66d1":"for x_counting_tensor in TEST_TENSOR_2.take(20):\n    \n    generate_images_for_multi_predict(Generation_Model,x_counting_tensor)","1c3e4f7f":"os.mkdir(\".\/SINGLE_CLASS_PREDICT\")","2b214844":"SINGLE_READ_LIST = []\n\ndef generate_images_single_pre(model, test_input):\n    \n    plt.style.use(\"dark_background\")\n    \n    prediction = model(test_input, training=True)\n    \n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(prediction[0])\n    \n    SAVE_NUM = random.randint(0,120)\n    plt.savefig('.\/SINGLE_CLASS_PREDICT\/sin_res_{:04d}.png'.format(SAVE_NUM))\n    \n    \n    \n    PRE_READ_IMG = cv2.cvtColor(cv2.imread('.\/SINGLE_CLASS_PREDICT\/sin_res_{:04d}.png'.format(SAVE_NUM)),cv2.COLOR_BGR2RGB)\n    SINGLE_READ_LIST.append(PRE_READ_IMG)\n    \n    \n    plt.tight_layout()    \n    plt.show()","dbef24fd":"for x_counting_tensor in TEST_TENSOR_2.take(32):\n    \n    generate_images_single_pre(Generation_Model,x_counting_tensor)","b05b8875":"def displaying_plot(source):\n    \n    figure = plt.figure(figsize=(16,11))\n\n    Image_List = []\n    plt.style.use(\"dark_background\")\n    for indexing in source:\n        \n        Read_IMG = plt.imshow(indexing, animated=True,cmap=\"hot\")\n        plt.axis('off')\n        Image_List.append([Read_IMG])\n\n    Animation_Func = animation.ArtistAnimation(figure, Image_List, interval=350, repeat_delay=10200)\n    \n    plt.close()\n    \n    return Animation_Func","f6737f59":"HTML(displaying_plot(SINGLE_READ_LIST).to_html5_video())","c2d6e92e":"HTML(displaying_plot(LIST_PREDICT_MULTI).to_html5_video())","151547fd":"OUTPUT_RES_PATH = Path(\".\/MULTI_CLASS\")\nOUTPUT_RES_LIST = list(OUTPUT_RES_PATH.glob(r\"*.png\"))","7730158b":"print(len(OUTPUT_RES_LIST))\n\nOUT_RES_SERIES = pd.Series(OUTPUT_RES_LIST,name=\"RES\").astype(str)","8710d6be":"OUT_READ_RES = []\n\nfor x_seg in OUT_RES_SERIES:\n\n    SEG_IMG = cv2.cvtColor(cv2.imread(x_seg),cv2.COLOR_BGR2RGB)\n    \n    RESIZE_SEG = cv2.resize(SEG_IMG,(IMG_WIDTH,IMG_HEIGHT))\n    \n    REDUCE_SEG = RESIZE_SEG \/ 255.\n\n    OUT_READ_RES.append(REDUCE_SEG)","5c653aca":"HTML(displaying_plot(OUT_READ_RES).to_html5_video())","a2b9b0f2":"### OPTIMIZATION AND GENERATION IMAGES CREATION","b9c2588e":"### DISCRIMINATOR CREATION","75fa742a":"# DATA PROCESS","fd9421e6":"# BEFORE TRAINING","f33028d7":"### LAYER CREATION","6b90c0e3":"### GENERATOR CREATION","3d2bbb3c":"### TRAINING SECTION","da4bebde":"### LOSS FUNCTION CREATION","76e66048":"### TRAINING STEP CREATION - LAST STEP","5f171bea":"# MODEL PROCESS"}}