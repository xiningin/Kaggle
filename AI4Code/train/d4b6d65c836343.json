{"cell_type":{"f27d1de4":"code","79b59e86":"code","12f40813":"code","2ca6a6e8":"code","45df43ea":"code","014d094f":"code","bd5a5742":"code","75272b32":"code","a74e61f3":"code","c79a47b1":"code","b6639d0b":"code","3ed9361a":"code","c7739bb6":"code","28c18df1":"code","6b2e76d0":"code","233987b5":"code","cefb2963":"code","31d61101":"code","e3458d9f":"code","c85f051a":"code","cb91850a":"code","437e75b1":"code","ba14e298":"code","f6e12344":"code","ed628c78":"code","164691eb":"code","bc3f58d3":"code","69dc3e5f":"code","c23dbb2f":"code","16708225":"code","c471701f":"code","b4af275e":"code","9a1c7ab2":"code","e975bb44":"code","276b4f94":"code","2b264623":"code","10c6f339":"code","95b4b960":"code","b04ad6dc":"code","0d62c11d":"code","6c2c0b1b":"code","bfaa3aef":"code","c1b780fc":"code","d337512c":"code","6af0dd23":"code","a60d21d3":"code","7c6f176c":"code","b4641447":"code","e9477d22":"code","f3cf5370":"code","0e5e1520":"code","a17a1734":"code","131b46c1":"markdown","86a622ab":"markdown","7f66a08a":"markdown","fd68f7c7":"markdown","c66bc9b2":"markdown","5705776f":"markdown","0a9b4705":"markdown","f70dcfb6":"markdown","6b2058a9":"markdown","0cfee530":"markdown"},"source":{"f27d1de4":"#Libraries to import\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import Imputer , Normalizer , scale, LabelEncoder, OneHotEncoder, MinMaxScaler, RobustScaler, PolynomialFeatures, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import mean_absolute_error, average_precision_score, precision_recall_curve, roc_curve, precision_score, make_scorer, accuracy_score, classification_report, confusion_matrix, mean_squared_error, recall_score, f1_score, roc_auc_score, r2_score\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pydot\nimport matplotlib.patches as mpatches\nfrom pandas import get_dummies\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport scipy\nimport math\nimport json\nimport sys\nimport csv\nimport os\nimport tqdm\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Flatten, Embedding, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import SGD\nfrom tqdm import tqdm_notebook\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import Counter\nfrom string import punctuation\nfrom numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom gensim.models import Word2Vec\nfrom scipy.stats import norm\nfrom keras.callbacks import ModelCheckpoint\nimport time\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom keras.utils.vis_utils import plot_model","79b59e86":"df = pd.read_csv('..\/input\/StudentsPerformance.csv')","12f40813":"df.head()","2ca6a6e8":"df.describe()","45df43ea":"def plot_gaussian_distribution(df, var):\n    f, (ax1) = plt.subplots(1,1, figsize=(20, 6))\n    dist = df[var].values\n    sns.distplot(dist,ax=ax1, fit=norm, color='#FB8861')\n    ax1.set_title('Distribution for ' + str(var), fontsize=14)\n    plt.show()\n\ndef plot_boxplot(df, var, target):\n    \"\"\"\n    var has to be a categorical variable\n    target has to be discrete\n    \"\"\"\n    f, axes = plt.subplots(ncols=2, figsize=(20,4))\n    sns.boxplot(x=var, y=target, data=df, palette=\"Blues\", ax=axes[0])\n    axes[0].set_title('Boxplot for ' + str(var) + ' and ' + str(target))\n    plt.show()\n    \ndef plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + var_name )\n    fig.tight_layout()\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 24 , 20 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n                    corr,\n                    cmap = cmap,\n                    square=True,\n                    cbar_kws={ 'shrink' : .9 },\n                    ax=ax,\n                    annot = True,\n                    annot_kws = { 'fontsize' : 12 }\n                    )\n\ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels","014d094f":"def num_missing(x):\n    return sum(x.isnull())\n\nprint(\"Missing values per column:\")\nprint(df.apply(num_missing, axis=0))\n\nprint(\"\\nMissing values per row:\")\nprint(df.apply(num_missing, axis=1).head())","bd5a5742":"plot_gaussian_distribution(df, 'math score')","75272b32":"plot_gaussian_distribution(df, 'reading score')","a74e61f3":"plot_gaussian_distribution(df, 'writing score')","c79a47b1":"new_df = df[['math score', 'reading score', 'writing score']]","b6639d0b":"plot_correlation_map(new_df)","3ed9361a":"plt.boxplot(df['math score'])","c7739bb6":"plt.boxplot(df['reading score'])","28c18df1":"plt.boxplot(df['writing score'])","6b2e76d0":"df = df[df['reading score'] > 30]\ndf = df[df['math score'] > 30]\ndf = df[df['writing score'] > 30]","233987b5":"plt.boxplot(df['math score'])","cefb2963":"plt.boxplot(df['reading score'])","31d61101":"plt.boxplot(df['writing score'])","e3458d9f":"plot_gaussian_distribution(df, 'math score')","c85f051a":"plot_gaussian_distribution(df, 'reading score')","cb91850a":"plot_gaussian_distribution(df, 'writing score')","437e75b1":"df.head()","ba14e298":"plot_distribution(df, 'math score', 'gender', row = 'race\/ethnicity', col = 'lunch')","f6e12344":"plot_distribution(df, 'math score', 'parental level of education', row = 'test preparation course', col = 'gender')","ed628c78":"new_df = df","164691eb":"class MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","bc3f58d3":"new_df = MultiColumnLabelEncoder(columns = ['gender','race\/ethnicity', 'lunch', 'test preparation course']).fit_transform(new_df)","69dc3e5f":"new_df.head()","c23dbb2f":"plot_distribution(new_df, 'math score', 'parental level of education')","16708225":"dict = {\"master's degree\" : 6, \"associate's degree\": 5, \"bachelor's degree\": 4, \"some college\": 3, \"some high school\": 2, \"high school\": 1}\nnew_df['parental level of education'] = new_df['parental level of education'].map(dict).astype(int)","c471701f":"new_df.head()","b4af275e":"X = new_df.iloc[:, [0,1,2,3,4,6,7]].values\ny = new_df.iloc[:, 5].values","9a1c7ab2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)","e975bb44":"decision_tree = DecisionTreeRegressor()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nprint(acc_decision_tree)","276b4f94":"random_forest = RandomForestRegressor(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(acc_random_forest)","2b264623":"sc = StandardScaler()\nnew_df[['math score', 'reading score', 'writing score']] = sc.fit_transform(new_df[['math score', 'reading score', 'writing score']])","10c6f339":"new_df = pd.get_dummies(new_df, columns=['race\/ethnicity', 'parental level of education'], drop_first=True)","95b4b960":"new_df.head()","b04ad6dc":"y = new_df['math score'].values","0d62c11d":"new_df.drop(['math score'], axis=1, inplace=True)","6c2c0b1b":"X = new_df.values","bfaa3aef":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c1b780fc":"def create_polynomial_regression_model(X_train, X_test, y_train, y_test, degree):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_train_poly = poly_features.fit_transform(X_train)\n    poly_model = LinearRegression()\n    poly_model.fit(X_train_poly, y_train)\n    y_train_predicted = poly_model.predict(X_train_poly)\n    y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n    r2_train = r2_score(y_train, y_train_predicted)\n    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n    r2_test = r2_score(y_test, y_test_predict)\n  \n    print(\"The model performance for the training set\")\n    print(\"-------------------------------------------\")\n    print(\"RMSE of training set is {}\".format(rmse_train))\n    print(\"R2 score of training set is {}\".format(r2_train))\n    \n    print(\"\\n\")\n    \n    print(\"The model performance for the test set\")\n    print(\"-------------------------------------------\")\n    print(\"RMSE of test set is {}\".format(rmse_test))\n    print(\"R2 score of test set is {}\".format(r2_test))","d337512c":"create_polynomial_regression_model(X_train, X_test, y_train, y_test, 2)","6af0dd23":"X_train.shape","a60d21d3":"model = Sequential()\nmodel.add(Dense(128, input_dim=14))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1))\nmodel.add(Activation('linear'))","7c6f176c":"model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])","b4641447":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","e9477d22":"model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","f3cf5370":"predictions = model.predict(X_test)","0e5e1520":"res = 0\nfor i in range(len(y_test)):\n    val = (y_test[i] - predictions[i])*(y_test[i] - predictions[i])\n    res += val\nres \/= len(y_test)\nprint(\"Mean squared error on the test set: \", res)","a17a1734":"XGBModel = XGBRegressor()\nXGBModel.fit(X_train, y_train, verbose=False)\n\nXGBpredictions = XGBModel.predict(X_test)\nMAE = mean_absolute_error(y_test, XGBpredictions)\nprint('XGBoost test MAE = ',MAE)","131b46c1":"# One Hot encode for non tree based models","86a622ab":"# Relationship between the different scores","7f66a08a":"# XGBoost","fd68f7c7":"# Random Forest","c66bc9b2":"# Predicting math score","5705776f":"# Decision Tree","0a9b4705":"# ANN","f70dcfb6":"1. All the three scores have a Gaussian distribution\n\n2. All three scores have a high correlation with eachother. Hence, we will have to use all of them for training of each one of them.\n\n3. In all of the three scores, we can remove the students having values less than 30 as they act as outliers.","6b2058a9":"# Test the ANN model","0cfee530":"# Polynomial Regression"}}