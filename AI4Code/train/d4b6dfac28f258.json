{"cell_type":{"00fe10ba":"code","93790c00":"code","9f98422b":"code","97212e48":"code","1732a009":"code","af95621f":"code","797dd66f":"code","6639491c":"code","1385cb8d":"code","c4e5b8f6":"code","c1c8e228":"code","cd39e06b":"code","bd0107ab":"code","d6f1f46b":"code","79b37254":"code","14120d3e":"code","69927865":"code","bca8af08":"code","77c157c4":"code","4edc7559":"code","1d3b5f4a":"code","d83d21cc":"code","aa1810ac":"code","dce9b45a":"code","6a8d02ae":"code","53c8484a":"code","4190d0d4":"code","fc67ca4e":"code","431a7d8d":"code","92b3c67d":"code","786f8654":"code","21111633":"code","8fe54aff":"code","79658d53":"code","07d3c5c0":"code","eb0284fc":"code","af0743ab":"code","77742397":"code","a04e2245":"code","87d481f5":"code","6cae86b1":"code","2d2c58d3":"code","9a915323":"code","6d6d5da6":"code","abb03b63":"code","b9d96a52":"code","5e9a6142":"code","9c73043d":"code","55bc5308":"code","a4019ddc":"code","bfd2722e":"code","7d0df8e2":"code","7d36c038":"code","337e8f31":"code","251b9c96":"code","dbe078da":"code","ab236b95":"code","4eef9bee":"code","87282ec7":"code","859777de":"code","0d11a7e3":"code","39ff065e":"code","ca0829ef":"code","8b730ada":"code","5aed6c1e":"code","ca6249b3":"code","6ec305c3":"code","c7e1ddef":"code","641a6771":"code","8e2eb31e":"code","d2f51ccf":"code","1794d65a":"code","222a54a4":"code","36a9aa7f":"code","e9916c7d":"code","6ced6f6e":"code","a3826a51":"code","5273de71":"code","e6dd825a":"code","962bebbf":"code","d8ab3169":"code","54feb360":"code","7aff6804":"code","4d344bb1":"code","76c1d531":"code","f6d6e594":"code","32ae7885":"code","ae0d5b31":"code","71583b0e":"code","5a386ee4":"code","9273ce2c":"markdown","ba872eca":"markdown","e6c72b09":"markdown","5a6337c6":"markdown","4024a88a":"markdown","798b76dc":"markdown","35b55f4d":"markdown","40b42d00":"markdown","baa6b221":"markdown","b4a639c0":"markdown","660789d0":"markdown","0b355a61":"markdown","dedd8607":"markdown"},"source":{"00fe10ba":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, confusion_matrix, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier","93790c00":"import warnings\nwarnings.filterwarnings('ignore')","9f98422b":"# Reading Credit_Risk dataset\ndf = pd.read_csv('..\/input\/credit-risk-analysis\/Credit_Risk_Analysis\/Credit_Risk.csv')\ndf.head()","97212e48":"# Shape of our dataset\ndf.shape","1732a009":"df.info()","af95621f":"# Some statistics of dataset\ndf.describe().T","797dd66f":"# Count of duplicated values\ndf.duplicated().sum()","6639491c":"# Count of missing values\ndf.isnull().sum()","1385cb8d":"# Filling missing values for each coulumns \n# I filled missing values of categoric features with their mode (with the largest number of class) \n# I filled missing values of numerical features with their median, because all of them dont distribute normally\ndf['Gender'].fillna(df['Gender'].value_counts().index[0], inplace = True)\ndf['Married'].fillna(df['Married'].value_counts().index[0], inplace = True)\ndf['Dependents'].fillna(df['Dependents'].value_counts().index[0], inplace = True)\ndf['Self_Employed'].fillna(df['Self_Employed'].value_counts().index[0], inplace = True)\ndf['LoanAmount'].fillna(df['LoanAmount'].median(), inplace = True)\ndf['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median(), inplace = True)\ndf['Credit_History'].fillna(df['Credit_History'].value_counts().index[0], inplace = True)","c4e5b8f6":"df.isnull().sum()","c1c8e228":"#On the figures below we can see the number of classes of categorical variables with considering Loan_Status\nfig, ax =plt.subplots(3,3, figsize = (15,15))\nsns.countplot(df['Gender'], hue = df['Loan_Status'], ax=ax[0,0])\nsns.countplot(df['Married'], hue = df['Loan_Status'], ax=ax[0,1])\nsns.countplot(df['Dependents'], hue = df['Loan_Status'], ax=ax[0,2])\nsns.countplot(df['Education'], hue = df['Loan_Status'], ax=ax[1,0])\nsns.countplot(df['Self_Employed'], hue = df['Loan_Status'], ax=ax[1,1])\nsns.countplot(df['Credit_History'], hue = df['Loan_Status'], ax=ax[1,2])\nsns.countplot(df['Property_Area'], hue = df['Loan_Status'], ax=ax[2,0])\nfig.show()","cd39e06b":"# Counts of classes of Loan_Status. It is clear that Y (who can get loan) is almost two times larger than N (who can not get loan)\nsns.countplot(df['Loan_Status']);","bd0107ab":"# On the figures below, we can see distributions of ApplicantIncome, CoapplicantIncome, LoanAmount and Loan_Amount_Term respectively\n# It is clear that distributions of these features are not normal.\nsns.displot(data = df, x = 'ApplicantIncome', kde = True, color = 'skyblue');","d6f1f46b":"sns.displot(data = df, x = 'CoapplicantIncome', kde = True, color = 'olive');","79b37254":"sns.displot(data = df, x = 'LoanAmount', kde = True, color = 'gold');","14120d3e":"sns.displot(data = df, x = 'Loan_Amount_Term', kde = True, color = 'teal');","69927865":"# For ApplicantIncome\nsns.boxplot(df['ApplicantIncome']);","bca8af08":"Q1 = df['ApplicantIncome'].quantile(0.25)\nQ3 = df['ApplicantIncome'].quantile(0.75)\nIQR = Q3-Q1\nprint(\"Q1 \", Q1)\nprint(\"Q3 \", Q3)\nprint(\"IQR \", IQR)","77c157c4":"down = Q1 - 1.5*IQR\nup = Q3 + 1.5*IQR\nprint(\"Down: \", down)\nprint(\"Up: \", up)","4edc7559":"outliers = df['ApplicantIncome']>up\noutliers","1d3b5f4a":"df[outliers].index","d83d21cc":"df.loc[df[outliers].index, 'ApplicantIncome'] = up","aa1810ac":"# For CoapplicantIncome\nsns.boxplot(df['CoapplicantIncome']);","dce9b45a":"Q1_CI = df['CoapplicantIncome'].quantile(0.25)\nQ3_CI = df['CoapplicantIncome'].quantile(0.75)\nIQR_CI = Q3_CI-Q1_CI\nprint(\"Q1_CI \", Q1_CI)\nprint(\"Q3_CI \", Q3_CI)\nprint(\"IQR \", IQR_CI)","6a8d02ae":"down_CI = Q1_CI - 1.5*IQR_CI\nup_CI = Q3_CI + 1.5*IQR_CI\nprint(\"Down: \", down_CI)\nprint(\"Up: \", up_CI)","53c8484a":"outliers_CI = df['CoapplicantIncome']>up_CI\noutliers_CI","4190d0d4":"df[outliers_CI].index","fc67ca4e":"df.loc[df[outliers_CI].index, 'CoapplicantIncome'] = up_CI","431a7d8d":"# For LoanAmount\nsns.boxplot(df['LoanAmount']);","92b3c67d":"Q1_LA = df['LoanAmount'].quantile(0.25)\nQ3_LA = df['LoanAmount'].quantile(0.75)\nIQR_LA = Q3_LA-Q1_LA\nprint(\"Q1_LA \", Q1_LA)\nprint(\"Q3_LA \", Q3_LA)\nprint(\"IQR_LA \", IQR_LA)","786f8654":"down_LA = Q1_LA - 1.5*IQR_LA\nup_LA = Q3_LA + 1.5*IQR_LA\nprint(\"Down: \", down_LA)\nprint(\"Up: \", up_LA)","21111633":"outliers_LA = df['LoanAmount']>up_LA\noutliers_LA","8fe54aff":"df[outliers_LA].index","79658d53":"df.loc[df[outliers_LA].index, 'LoanAmount'] = up_LA","07d3c5c0":"# For Loan_Amount_Term\nsns.boxplot(df['Loan_Amount_Term']);","eb0284fc":"Q1_LAT = df['Loan_Amount_Term'].quantile(0.25)\nQ3_LAT = df['Loan_Amount_Term'].quantile(0.75)\nIQR_LAT = Q3_LAT-Q1_LAT\nprint(\"Q1_LAT \", Q1_LAT)\nprint(\"Q3_LAT \", Q3_LAT)\nprint(\"IQR_LAT \", IQR_LAT)","af0743ab":"down_LAT = Q1_LAT - 1.5*IQR_LAT\nup_LAT = Q3_LAT + 1.5*IQR_LAT\nprint(\"Down: \", down_LAT)\nprint(\"Up: \", up_LAT)","77742397":"outliers_LAT = (df['Loan_Amount_Term'] < down_LAT) | (df['Loan_Amount_Term'] > up_LAT) \noutliers_LAT","a04e2245":"df[outliers_LAT].index","87d481f5":"df.loc[df[outliers_LAT].index, 'Loan_Amount_Term'] = up_LAT","6cae86b1":"df = pd.get_dummies(df, columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'])","2d2c58d3":"df.head()","9a915323":"# After encoding delete some columns beacues they include the same information with their pair\ndf.drop(['Gender_Female', 'Married_No', 'Education_Not Graduate', 'Self_Employed_No'], axis = 1, inplace = True)","6d6d5da6":"# Creidy_History column is in float type and we must change it\ndf['Credit_History'] = df['Credit_History'].replace({1.0: 'Yes', 0.0: 'No'})","abb03b63":"df = pd.get_dummies(df, columns = ['Credit_History'])\ndf.drop('Credit_History_No', axis = 1, inplace = True)","b9d96a52":"# Preparation of our tagret 'Loan_Status'. I replaced Y with 1 and N with 0\ndf['Loan_Status'] = df['Loan_Status'].replace({'Y': 1, 'N': 0})","5e9a6142":"df.reset_index(inplace = True)\ndf.head()","9c73043d":"# I deleted index and Loan_ID columns because Loand_ID is just number which identify loans and it is not informative for models\ndf.drop(['index', 'Loan_ID'], axis = 1, inplace = True)\ndf.head()","55bc5308":"df.head()","a4019ddc":"# In this heatmap, we can see correlation between features. Loan_Amount dont have correlation with other features\n#because its all values were equal to 360 after filling missing values and outlier treatment steps\n\nplt.figure(figsize = (15,15))\nsns.heatmap(df.corr(), annot = True, cbar = True, vmin = -1, vmax= 1);","bfd2722e":"df.head()","7d0df8e2":"# Definition our independed features (X) and depended feature (Target - y)\nX = df.drop('Loan_Status', axis = 1)\ny = df[['Loan_Status']]\nprint(\"X shape: \", X.shape)\nprint(\"y shape: \", y.shape)","7d36c038":"# This method rescales the data so that the mean is 0 and the standard deviation is 1.\nsc = StandardScaler()\nscaled_X = sc.fit_transform(X)\nscaled_X","337e8f31":"# Train Test Split. Mostly test size is selected as 0.30 in most problems but in our problem, the dataset is not big enough. \n# For this reason I have used 20% of the dataset for testing\nX_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = 0.20, random_state = 42)\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of y_test: \", y_test.shape)","251b9c96":"models = [ LogisticRegression,\n          GradientBoostingClassifier,\n          RandomForestClassifier,\n          DecisionTreeClassifier,\n          KNeighborsClassifier,\n          SVC,\n          LGBMClassifier]","dbe078da":"def MakePrediction(algorithm):\n    model = algorithm().fit(X_train, y_train)\n    print(algorithm.__name__, \" Train Score: \", model.score(X_train, y_train))\n    print(algorithm.__name__, \" Test Score: \", model.score(X_test, y_test))\n    print(\"_______________________________________________________________\")","ab236b95":"\nfor i in models:\n    MakePrediction(i)","4eef9bee":"lr_model = LogisticRegression().fit(X_train, y_train)","87282ec7":"cross_val_score(lr_model, X_train, y_train, cv = 5).mean()","859777de":"val_lr = cross_val_score(lr_model, X_test, y_test, cv = 5).mean()\nval_lr","0d11a7e3":"gbm_model = GradientBoostingClassifier()\ngbm_params = {\"learning_rate\": [0.1, 0.01, 0.001, 0.05],\n              \"n_estimators\": [100,500,1000],\n              \"max_depth\": [3,5,10],\n              \"min_samples_split\": [2, 5, 10]}\n\ngmb_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","39ff065e":"gmb_cv_model.best_params_","ca0829ef":"tuned_gbm = GradientBoostingClassifier(learning_rate = 0.001,\n                                       n_estimators = 500,\n                                       max_depth = 3,\n                                       min_samples_split = 2).fit(X_train, y_train)","8b730ada":"print(\"Train Score: \", tuned_gbm.score(X_train, y_train))\nprint(\"Test Score: \", tuned_gbm.score(X_test, y_test))","5aed6c1e":"rf_model = RandomForestClassifier()\nrf_params = {\"max_depth\": [2, 5, 8, 10],\n             \"n_estimators\": [100, 500, 1000],\n             \"max_features\": [2,5,8],\n             \"min_samples_split\": [2,5,10]}\n\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","ca6249b3":"rf_cv_model.best_params_","6ec305c3":"rf_tuned = RandomForestClassifier(max_depth = 2,\n                                  n_estimators = 500,\n                                  max_features = 5,\n                                  min_samples_split = 2).fit(X_train, y_train)","c7e1ddef":"print(\"Train Score: \", rf_tuned.score(X_train, y_train))\nprint(\"Test Score: \", rf_tuned.score(X_test, y_test))","641a6771":"dt_model = DecisionTreeClassifier()\ndt_params = {\"max_depth\": list(range(1,10)),\n             \"min_samples_split\": list(range(2,50)),\n             \"max_features\": [2, 5, 7, 10]}\n\ndt_cv_model = GridSearchCV(dt_model, dt_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","8e2eb31e":"dt_cv_model.best_params_","d2f51ccf":"tuned_dt = DecisionTreeClassifier(max_depth = 4, max_features = 10, min_samples_split = 42).fit(X_train, y_train)","1794d65a":"print(\"Train Score: \", tuned_dt.score(X_train, y_train))\nprint(\"Test Score: \", tuned_dt.score(X_test, y_test))","222a54a4":"knn_model = KNeighborsClassifier()\nknn_params = {\"n_neighbors\": np.arange(1,50)}\n\nknn_cv = GridSearchCV(knn_model, knn_params, cv = 5).fit(X_train, y_train)","36a9aa7f":"knn_cv.best_params_","e9916c7d":"tuned_knn = KNeighborsClassifier(n_neighbors = 10).fit(X_train, y_train)","6ced6f6e":"print(\"Train Score: \", tuned_knn.score(X_train, y_train))\nprint(\"Test Score: \", tuned_knn.score(X_test, y_test))","a3826a51":"svm_model = SVC()\nsvm_params = {\"C\": np.arange(1,10), \"kernel\": [\"linear\", \"rbf\"]}\nsvm_cv_model = GridSearchCV(svm_model, svm_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","5273de71":"svm_cv_model.best_params_","e6dd825a":"tuned_svm = SVC(C = 1, kernel = 'linear').fit(X_train, y_train)","962bebbf":"print(\"Train Score: \", tuned_svm.score(X_train, y_train))\nprint(\"Test Score: \", tuned_svm.score(X_test, y_test))","d8ab3169":"lgbm_model =  LGBMClassifier()\nlgbm_params = {\"n_estimators\": [100,300, 500, 1000, 2000],\n              \"subsample\": [0.6, 0.8, 1],\n              \"max_depth\": [3, 4, 5, 6],\n              \"learning_rate\": [0.1,0.001, 0.01, 0.02, 0.05],\n              \"min_child_samples\": [5, 10, 20]}\n\nlgbm_cv = GridSearchCV(lgbm_model, lgbm_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","54feb360":"lgbm_cv.best_params_","7aff6804":"tuned_lgbm = LGBMClassifier(n_estimators = 1000,\n                            subsample = 0.6,\n                            max_depth = 3,\n                            learning_rate = 0.001,\n                            min_child_samples = 10).fit(X_train, y_train) ","4d344bb1":"print(\"Train Score: \", tuned_lgbm.score(X_train, y_train))\nprint(\"Test Score: \", tuned_lgbm.score(X_test, y_test))","76c1d531":"tuned_models = [\"Validated Logistic Regression\",\n                \"Tuned Gradient Boosting\",\n                \"Tuned Random Forest\",\n                \"Tuned Decision Tree\",\n                \"Tuned KNN\", \n                \"Tuned SVM\", \n                \"Tuned LightGBM\"] \n\ntest_scores = [val_lr,\n               tuned_gbm.score(X_test, y_test),\n               rf_tuned.score(X_test, y_test),\n               tuned_dt.score(X_test, y_test),\n               tuned_knn.score(X_test, y_test),\n               tuned_svm.score(X_test, y_test),\n               tuned_lgbm.score(X_test, y_test)]","f6d6e594":"# The best accurate model is Decision Tree for this dataset\ntuned_results = pd.DataFrame(test_scores, columns = [\"Test Score\"], index = tuned_models)\ntuned_results.sort_values(by = 'Test Score', ascending = False)","32ae7885":"sns.barplot(x = tuned_results[\"Test Score\"], y = tuned_results.index);\n\nplt.xlabel(\"Test Scores of Each Models\")\nplt.ylabel(\"Model Names\")\nplt.title(\"Performance of Tuned Models \")\nplt.show()","ae0d5b31":"# As we can see the best result belongs to Decision Tree with 0.796748 accuracy score  after hyperparameter tunning.\n# Also we can improve our results expanding dataset.\n# In the Loan_Status, the number of 'No' values is very little, for this reason our models can not learn characteristics of this values very well.\n# I think in this way our models can not classify 'No' values with high accuracy\n\ncm = confusion_matrix(y_test, tuned_dt.predict(X_test))\nax = sns.heatmap(cm, annot = True, cbar = False, fmt = 'g');\nax.set_xlabel('Predicted Labels',fontsize = 15)\nax.set_ylabel('True Labels',fontsize = 15)\nax.set_title('Decision Tree Classifier')\nplt.show()","71583b0e":"print(classification_report(y_test, tuned_dt.predict(X_test)))","5a386ee4":"# Also we can see the most informative feature is Credit_History_Yes for Decision Tree Classifier\nimportances = pd.Series(tuned_dt.feature_importances_,\n                            index = X.columns).sort_values(ascending = False)\n\nsns.barplot(x = importances, y = importances.index)\nplt.xlabel(\"Feature Importance Values\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importances\")\nplt.show()","9273ce2c":"### Gradien Boosting ","ba872eca":"## Prediction Step","e6c72b09":"### KNeighbors","5a6337c6":"### DecisionTree","4024a88a":"### Outlier detection and treatment with BoxPLot method","798b76dc":"### Comparision of Tuned Models","35b55f4d":"### LogisticRegression","40b42d00":"\n### Visualization","baa6b221":"### One-Hot Encoding","b4a639c0":"### Tuning of Models (Hyperparameter tuning)","660789d0":"### SVC ","0b355a61":"###  LGBMClassifier","dedd8607":"### Random Forest "}}