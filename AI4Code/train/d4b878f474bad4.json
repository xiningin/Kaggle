{"cell_type":{"e74cfef5":"code","95bbc903":"code","16b9914a":"code","fd8f02e8":"code","bd4af08e":"code","3fe7d846":"code","ef128460":"code","a38769e2":"code","569a7765":"code","433491ba":"code","3c9a9357":"code","3181966a":"code","da14592a":"code","53b9a6fa":"code","9dcca5dc":"code","645f546e":"code","9290f728":"code","1ca3231d":"code","275de677":"code","973ff009":"code","88754ff2":"code","655282f3":"code","cd09ec50":"code","9c1ace7c":"code","cfc81593":"code","ba12e139":"code","088bdba4":"code","ecb2d7b4":"code","8c43bb62":"code","d6b0dd16":"code","75186c12":"code","034ff0bb":"code","d6ca7e0e":"code","9520cdda":"code","cfb41b82":"code","004b205b":"code","3372be4c":"markdown","2feae0c7":"markdown","09720487":"markdown","60eb6851":"markdown","33d90eb9":"markdown","eeb549f3":"markdown","7fd4b290":"markdown","1b79e5d6":"markdown","bc150581":"markdown","7efe1707":"markdown","f0158a7e":"markdown","72123854":"markdown","b55d348f":"markdown","91a37eea":"markdown","76506aaa":"markdown","2c47a634":"markdown","b0a9c482":"markdown","fd95c119":"markdown","1065993f":"markdown","b078bee6":"markdown","9fd66d1d":"markdown"},"source":{"e74cfef5":"from sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nimport pandas as pd, numpy as np, os, seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nimport warnings\nfrom sklearn import linear_model\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import Lasso\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.svm import SVR\nfrom sklearn import svm\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.model_selection import KFold \nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor ","95bbc903":"#Importing data for train and test respectively:\n\ndef import_data(data):\n    df = pd.read_csv('..\/input\/{}.csv'.format(data))\n    df = df.set_index('Id')\n    return df\n\ntrain = import_data('train')\ntest = import_data('test')","16b9914a":"print('Train dataset:\\nRows: {}\\nColumns: {}'.format(train.shape[0], train.shape[1]))\ntrain.head()","fd8f02e8":"print('Test dataset:\\nRows: {}\\nColumns: {}'.format(test.shape[0], test.shape[1]))\ntest.head()","bd4af08e":"print(train[['SalePrice']].describe())","3fe7d846":"corrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.6]\nplt.figure(figsize=(6,6))\ng = sns.heatmap(\n    train[top_corr_features].corr(), \n    annot = True, cmap = \"Blues\", \n    cbar = False, vmin = .5, \n    vmax = .7, square=True\n    )","ef128460":"#Then using the most correlated features in a pairplit to identify possible outlier filters\n#sns.pairplot(train[top_corr_features])","a38769e2":"#The following filter is chosen\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<450000)].index)","569a7765":"#z-score on top-correlating features\n\n'''\nfor kol in top_corr_features:\n    z = np.abs(stats.zscore(train[kol]))\n    train['z'] = z\n    train = train[train.z < 3]\n    train = train.drop('z', axis = 1)\ntrain.shape\n'''","433491ba":"print('The sales prices are skewed quite a bit')\nplt.hist(train.SalePrice, bins = 20)\nplt.suptitle('Prices before normalizing')\nplt.show()\n\n#let's also just defined our target as y:\ny = train['SalePrice'].values\n\nprint('We therefore apply log1p to normalize the sales prices for prediction')\n#Normalizing sales price\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nplt.hist(train.SalePrice, bins = 20)\nplt.suptitle('Normalized prices')\nplt.show()","3c9a9357":"#we don't need sales price\n","3181966a":"#Interpretations of null values\n#Inspired by: https:\/\/www.kaggle.com\/iamprateek\/my-submission-to-predict-sale-price\/data\n\n#Fill out values with most common value\ncommonNa = [\n    'MSZoning','Electrical','KitchenQual',\n    'Exterior1st','Exterior2nd','SaleType',\n    'LotFrontage','Functional'\n    ]\n\n#Fill with zero value\ntoZero = [\n    'MasVnrArea','GarageYrBlt','BsmtHalfBath',\n    'BsmtFullBath','GarageArea','GarageCars',\n    'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n    'TotalBsmtSF'\n    ]\n\n#Fill with No data\ntoNoData = [\n    'PoolQC','MiscFeature','Alley',\n    'Fence','FireplaceQu','GarageType', \n    'GarageFinish','GarageQual', \n    'GarageCond','BsmtQual','BsmtCond', \n    'BsmtExposure','BsmtFinType1','BsmtFinType2',\n    'MasVnrType'\n    ]\n\n#Function fill missing values\ndef fillNa_fe(df):\n    df['Functional'] = df['Functional'].fillna('Typ')\n    for i in commonNa:\n        df[i] = df[i].fillna(df[i].mode()[0])\n    for i in toNoData:\n        df[i] = df[i].fillna('None')\n    for i in toZero:\n        df[i] = df[i].fillna(0)\n    #Removing utilies. No predictive value\n    df.drop(\n        ['Utilities'], \n        axis=1, \n        inplace=True\n        )\n    #A little bit of festure engineering\n    df['totalSF'] = df['TotalBsmtSF'] \\\n                   + df['1stFlrSF'] \\\n                   + df['2ndFlrSF'] \\\n                   + df['GrLivArea']\n    return df\n\ntrain, test = fillNa_fe(train), fillNa_fe(test)","da14592a":"for df in [train,test]:\n    print(df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = False).any())","53b9a6fa":"'''\ndef skew_(df):\n    #columns which are skew-candidates\n    colls = [col for col in df.columns if df[col].dtype in ['int64','float']]\n    skews_df = [col for col in df[colls].columns if df[col].skew() > .7]\n\n    #function to correct skew\n    def skewfix(data, data2):\n        for i in data2:\n            data[i] = np.log1p(data[i])\n            return data\n    return skewfix(df, skews_df)\n\ntrain, test = skew_(train), skew_(test)\n'''","9dcca5dc":"#We need to encode variables with categorical data:\nencoder = LabelEncoder()\nsc = StandardScaler()\n\ndef encode(df):\n    cat_df = [col for col in df.columns if df[col].dtype not in ['int','float']]\n    for col in cat_df:\n        df[col] = encoder.fit_transform(df[col])\n    df_ = sc.fit_transform(df)\n    df = pd.DataFrame(data=df_, columns = df.columns)\n    return df\n\ntest,train=encode(test),encode(train)","645f546e":"#y, the target, has alredy been defined as train['SalePrice'].value\nX = train.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=16)","9290f728":"#XGBoost\n#Best para,eters chosen by grid search\nparams = {\n    'colsample_bytree': [0.4],\n    'gamma': [0.0],\n    'learning_rate': [0.01],\n    'max_depth': [4],\n    'min_child_weight': [2],\n    'n_estimators': [4000],\n    'seed': [36],\n    'subsample': [0.5],\n    'verbosity':[0]\n    }\ngbm = XGBRegressor()\nreg_cv = GridSearchCV(gbm, params, verbose = 0)\nreg_cv.fit(X_train,y_train)\nreg_cv.best_params_\nXGBoost = XGBRegressor(**reg_cv.best_params_)\nXGBoost.fit(X_train,y_train,verbose = False)\nxgboost_predictions = XGBoost.predict(X_test)\nprint('XGBoost \\tRMSE: \\t{}$'.format(int(np.sqrt(metrics.mean_squared_error(y_test, xgboost_predictions)))))","1ca3231d":"reg_cv.best_params_","275de677":"#Support Vector Regression\nparameters = {\n    'kernel':('linear','poly'), \n    'C':[120],\n    'gamma': [.0003],\n    'epsilon':[0.1]\n    }\nsvr = svm.SVR()\nclf = GridSearchCV(svr, parameters)\nclf.fit(X_train,y_train)\nclf.best_params_\nsupv = XGBRegressor(**clf.best_params_)\nsupv.fit(X_train,y_train,verbose = False)\nsupv_predictions = supv.predict(X_test)\nprint(int(np.sqrt(metrics.mean_squared_error(y_test,supv_predictions))))","973ff009":"clf.best_params_","88754ff2":"from sklearn.pipeline import make_pipeline\n\n'''\nTesting pipelines\nThanks goes to: https:\/\/www.kaggle.com\/shaygu\/house-prices-begginer-top-7\/output\n'''\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=16)\n\nalphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alphas_lasso, max_iter=1e7, random_state=16, cv=kfolds))","655282f3":"lasso.fit(X_train,y_train)\nlasso_predictions = lasso.predict(X_test)\n\nprint('Lasso \\t \\tRMSE: \\t{}$'.format(int(np.sqrt(metrics.mean_squared_error(y_test, lasso_predictions)))))","cd09ec50":"%%time\nparameters = {\n    'objective':['regression'],\n    'num_leaves':[4],\n    'learning_rate':[0.01],\n    'n_estimators':[9000],\n    'max_bin':[500],\n    'bagging_fraction':[.5,.3],\n    'bagging_freq':[5],\n    'bagging_seed':[7],\n    'feature_fraction':[.2],\n    'feature_fraction_seed':[7],\n    'verbose':[-1]\n    }\n\nlight = LGBMRegressor()\nclf = GridSearchCV(light, parameters)\nclf.fit(X_train, y_train)\nclf.best_params_\nlightgbm = LGBMRegressor(**clf.best_params_)\nlightgbm.fit(X_train,y_train)\nlightgbm_predictions = lightgbm.predict(X_test)","9c1ace7c":"clf.best_params_","cfc81593":"%%time\n\nalphas_ela = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nl1 = [0.70, 0.80, 0.85, 0.90, 0.95, 0.99]\n\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=5000, cv=kfolds, alphas=alphas_ela, l1_ratio=l1)\n\nelasticnet.fit(X_train,y_train)\nelasticnet_predictions = np.expm1(elasticnet.predict(X_test))\nprint('Elasticnet \\tRMSE: \\t{}$'.format(int(np.sqrt(metrics.mean_squared_error(y_test,elasticnet_predictions)))))","ba12e139":"parameters = {\n    'n_estimators':[7000],\n    'learning_rate':[0.01], \n    'max_depth':[4], \n    'max_features':['sqrt'], \n    'min_samples_leaf':[10], \n    'min_samples_split':[10], \n    'loss':['huber'], \n    'random_state' :[16]\n    }\ngbr = GradientBoostingRegressor()\nclf = GridSearchCV(gbr, parameters)\nclf.fit(X_train, y_train)\nclf.best_params_\ngbr = GradientBoostingRegressor(**clf.best_params_)\ngbr.fit(X_train,y_train)\ngbr_predictions = np.expm1(gbr.predict(X_test))","088bdba4":"clf.best_params_","ecb2d7b4":"alphas_alt = [\n    14.5, 14.6, 14.7, \n    14.8, 14.9, 15.0, \n    15.1, 15.2, 15.3, \n    15.4, 15.5, 15.6\n    ]\n\nridge = RidgeCV(\n    alphas=alphas_alt, \n    cv=kfolds\n    )\n\nridge.fit(X_train,y_train)\nridge_predictions = np.expm1(ridge.predict(X_test))","8c43bb62":"%%time\nstack_gen = StackingCVRegressor(\n    regressors=(\n        lightgbm, \n        supv, \n        XGBoost, \n        lightgbm, \n        ridge, \n        gbr, \n        elasticnet, \n        lasso\n        ), \n    meta_regressor=XGBoost,\n    use_features_in_secondary=True\n    )\nstack_gen_model = stack_gen.fit(np.array(X_test), np.array(y_test))\nstack_predictions = stack_gen_model.predict(np.array(X_test))","d6b0dd16":"def blend(data):\n    return (\n        (0.10 * supv.predict(data)) +\n        (0.20 * XGBoost.predict(data)) +\n        (0.15 * lightgbm.predict(data)) +\n        (0.15 * gbr.predict(data)) +\n        (0.10 * ridge.predict(data)) +\n        (0.10 * elasticnet.predict(data)) +\n        (0.10 * lasso.predict(data)) +\n        (0.10 * stack_gen_model.predict(np.array(data)))\n        )","75186c12":"%%time\nblended_predictions = blend(test)","034ff0bb":"def calc(model): return int(np.sqrt(metrics.mean_squared_error(y_test, model)))\n\nprint('XGBoost \\tRMSE: \\t{}$'.format(calc(xgboost_predictions)))\nprint('SVR \\t\\tRMSE: \\t{}$'.format(calc(supv_predictions)))\nprint('lightGBM \\tRMSE: \\t{}$'.format(calc(lightgbm_predictions)))\nprint('GBR  \\t\\tRMSE: \\t{}$'.format(calc(gbr_predictions)))\nprint('Ridge \\t \\tRMSE: \\t{}$'.format(calc(ridge_predictions)))\nprint('Elasticnet \\tRMSE: \\t{}$'.format(calc(elasticnet_predictions)))\nprint('Lasso \\t \\tRMSE: \\t{}$'.format(calc(lasso_predictions)))\nprint('Stack_gen \\tRMSE: \\t{}$'.format(calc(stack_predictions)))\nprint('Blended \\tRMSE: \\t{}$'.format(int(np.sqrt(metrics.mean_squared_error(y_test, blend(X_test))))))","d6ca7e0e":"res = pd.DataFrame(\n    blended_predictions, \n    index=import_data('test').reset_index()['Id']\n    )\n#p=stack_gen_model.predict(np.array(test))\n#res = pd.DataFrame(p, index=import_data('test').reset_index()['Id'])\nres.rename(columns={0:'SalePrice','index':'Id'},inplace=True)\nres.reset_index(inplace=True)\nres.head()","9520cdda":"res.to_csv('subm.csv',index = 0)","cfb41b82":"\ndef createFrame(data):\n    '''\n    Compare results\n    '''\n    index = index=import_data('test').reset_index()['Id']\n    df_ = {\n        'lightgbm':lightgbm.predict(data),\n        'XGBoost':XGBoost.predict(data),\n        'GBR':gbr.predict(data),\n        'ridge':ridge.predict(data),\n        'elastic':elasticnet.predict(data),\n        'lasso':lasso.predict(data),\n        'stakg_gen':stack_gen_model.predict(np.array(data)),\n        'blended':blend(data)\n        }\n    df = pd.DataFrame(df_, index=index)\n    return df\n\npreds = createFrame(test)\npreds.round(1).head(20)","004b205b":"for kol in preds.columns:\n    res = preds[kol].to_frame()\n    res.set_index(import_data('test').reset_index()['Id'])\n    res.rename(columns={kol:'SalePrice','index':'Id'},inplace=True)\n    #q1, q2 = res['SalePrice'].quantile(0.01), res['SalePrice'].quantile(0.9)\n    #res['SalePrice'] = res['SalePrice'].apply(\n    #lambda x: x if x > q1 else x*0.95)\n    #res['SalePrice'] = res['SalePrice'].apply(\n    #lambda x: x if x < q2 else x*1.05)\n    res.to_csv('{}.csv'.format(kol))","3372be4c":"**Writing the prediction file**","2feae0c7":"**We want to predict the price; let's look at the price data!**","09720487":"**Outliers with z-score and IQR**","60eb6851":"# Predicting house prices\n\nMy purpose with this kernel is solely to try different ML algorithms and to learn about stacking and ensambling.\n\nI am therefore not including much EDA in this kernel and I am not describing the data cleaning- and feature engineering (FE) in details. I will refer to other kernels instead, as many others have already done an outstanding job doing in these regards!  \n\nMy kernel includes:  \n>Light introduction (1)    \n>Data cleaning: Outliers (2) and Skew, null  and FE (5)  \n>Data preparation: Encode and scale (4)  \n>Model fit (5)  \n>Ensambling for final model (6)\n\n","33d90eb9":"**Let's just have a look at the first 5 rows of each data set and the shape of the dataset**","eeb549f3":"> The extra column in Train is the target, i.e. the house price","7fd4b290":"# 2 Outliers","1b79e5d6":"**Correcting skew**","bc150581":"Inspired by: [https:\/\/www.kaggle.com\/niteshx2\/top-50-beginners-stacking-lgb-xgb\/](https:\/\/www.kaggle.com\/niteshx2\/top-50-beginners-stacking-lgb-xgb\/)","7efe1707":"I will not go through the dataset itself in details.  \n> [I instead recommend to read about the data set here!](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)","f0158a7e":"# 5 Fit prediction models","72123854":"**Evaluating prediction errors**","b55d348f":"**Null values**","91a37eea":"# 4 Encode and scale data","76506aaa":"# 6 Emsambling to make final predictions","2c47a634":"# 3 Handling skew and null values","b0a9c482":"**Creating file with final predictions**","fd95c119":"**Checking if threre still are null values in the two dataframes**","1065993f":"Finding features correlating highly, >60%, with sales price in the training data set","b078bee6":"# 1 Data overview","9fd66d1d":"*Pairplot can be used to identify outliers as well:*"}}