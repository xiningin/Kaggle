{"cell_type":{"2b8cb1e8":"code","aefc5cb6":"code","30711308":"code","d5f57061":"code","ed423856":"code","410f6fb9":"code","1e95b14e":"code","ade0add0":"code","51cd8e85":"code","78e96d04":"code","d87deb05":"code","415a36b9":"code","03359bd4":"code","e067c9a3":"code","5ecf3e9c":"code","a7e99f91":"code","8af136a3":"code","eb7842fb":"code","3b8d90e9":"code","b98adcf2":"code","6bc38c91":"code","d63216da":"code","2e3685c5":"code","5bcf4326":"code","2ad8b42c":"code","d3c6a11a":"markdown"},"source":{"2b8cb1e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom fastai.basics import *\nfrom fastai.callbacks import * ","aefc5cb6":"PATH='..\/input\/'\n!ls '..\/input\/'","30711308":"#float_data = pd.read_csv(\"..\/input\/train.csv\",  nrows=10000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\nfloat_data = pd.read_csv(PATH+'train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}).values","d5f57061":"# Data is a 629M long time series. \n# The data is recorded in bins of 4096 samples. Withing those bins seismic data is recorded at 4MHz, \n# but there is a 12 microseconds (??? or is it 1.2 millisecond) gap between each bin, an artifact of the recording device.\n# Test data consists of 150,000 long time series. We don't know where the gaps are in the test data.\n\n# Our dataset will create items that are each based on 150,000 long chunks of data\n\nclass EQdataset (Dataset):\n    def __init__(self, data, min_index =0, max_index = None, bin_density = 0.1, is_test = False):\n        \n        self.LENGTH_SAMPLE =150000\n        self.BIN_SIZE = 4096   #4096\n        self.BIN_CHUNK_SIZE = 64    #needs to be a divisor of 4096 (bin size)  #16\n        self.BIN_CHUNK_COUNT = int(self.BIN_SIZE\/self.BIN_CHUNK_SIZE) \n        self.min_index = min_index\n        self.max_index = max_index\n        if max_index is None:\n            self.max_index = len(data) - 1\n        \n        \n        self.data = data[self.min_index:self.max_index+1]\n        \n        self.is_test = is_test\n        \n        if is_test:\n            self.dataset_size = 1\n            self.ending_positions = [150000]\n        else:\n            # Pick indices of ending positions\n            # dataset_size = number of bins * bin_density\n            self.dataset_size = int(np.round(len(self.data)\/ self.BIN_SIZE * bin_density) )\n            # remember self.data index starts at 0 and not at min_index\n            self.ending_positions = np.random.randint(self.LENGTH_SAMPLE, self.max_index - self.min_index , size=self.dataset_size)\n\n        \n        \n            \n    \n    def __len__(self):\n        return self.dataset_size\n                 \n    def __getitem__(self, idx):\n        \n        ending_position = self.ending_positions[idx]\n        starting_position = ending_position - self.LENGTH_SAMPLE\n        idx_data = self.data[starting_position: ending_position,:]\n        \n        \n        # for each data point in the last bin, we form a 35 data point time series using data points separated by 4096 rows\n        # This way we have a constant time gap between each data point in the time series\n        rnn_length = int(np.floor (self.LENGTH_SAMPLE\/self.BIN_SIZE)) -1   # =36\n        rnn_data_indexes = np.array([self.LENGTH_SAMPLE -1 - (rnn_length -1 - n) * self.BIN_SIZE for  n in range(rnn_length)])\n        \n        # 256 rnn inputs of length 36 and depth 16\n        \n        #X = np.array([[idx_data[index-self.BIN_CHUNK_SIZE+1 -bin_chunk_num*self.BIN_CHUNK_SIZE : index+1-bin_chunk_num*self.BIN_CHUNK_SIZE][:,0] for index in rnn_data_indexes] \n                     #for bin_chunk_num in range(self.BIN_CHUNK_COUNT)])\n        \n        X = idx_data[self.LENGTH_SAMPLE-self.BIN_CHUNK_COUNT*rnn_length*self.BIN_CHUNK_SIZE:,0].reshape((\n            rnn_length,self.BIN_CHUNK_COUNT,self.BIN_CHUNK_SIZE)).transpose(1,0,2)\n        # 256 rnn output of length 36\n        #y = np.array([idx_data[rnn_data_indexes - bin_chunk_num*self.BIN_CHUNK_SIZE][:,1] for bin_chunk_num in range(self.BIN_CHUNK_COUNT)])\n        \n        #y = idx_data[150000-self.BIN_CHUNK_COUNT*rnn_length*self.BIN_CHUNK_SIZE:,1].reshape((rnn_length,self.BIN_CHUNK_COUNT,self.BIN_CHUNK_SIZE)).transpose(1,0,2)[:,:,15] \n        \n        if self.is_test:\n            y =0\n            \n        else: \n            y = idx_data[self.LENGTH_SAMPLE -1,1]\n            \n        # 'meta' data\n        #inspired by https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples\n        meta = []\n        idx_data_series = pd.Series(idx_data[:,0])\n        for windows in [10, 100, 1000]:\n            \n            x_roll_std = idx_data_series.rolling(windows).std().dropna().values\n            meta.append(np.quantile(x_roll_std, 0.05))    \n        \n        \n        return [torch.from_numpy(X), torch.from_numpy(np.array(meta,dtype='float32'))] , y\n        \n        ","ed423856":"\n\nclass EQRNN(nn.Module):\n    def __init__(self, input_size = 64,meta_size=0, fc1_size = 40, hidden_size=80,num_layers=1,bidirectional=True, dropout=0.5):\n        \n        super().__init__()\n        self.input_size = input_size\n        \n        self.meta_size = meta_size\n        \n        self.fc1_size = fc1_size\n        self.hidden_size = hidden_size\n        \n        self.fc1 = nn.Linear(input_size, fc1_size)\n        \n        self.bidirectional,self.num_layers = bidirectional,num_layers\n        if bidirectional: self.num_directions = 2\n        else: self.num_directions = 1\n            \n        self.dropout = dropout\n            \n        self.rnn = nn.GRU(fc1_size, hidden_size,bidirectional=self.bidirectional,batch_first=True)\n        \n       \n        self.layers2 = nn.Sequential(\n            \n            nn.Linear(self.num_directions * hidden_size +meta_size,128),    #32\n            nn.ReLU(),\n            nn.Dropout(self.dropout),   ##added\n            nn.Linear(128,128),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n            nn.Linear(128,1),\n            \n            \n        )\n        \n        \n        #self.final_layers = nn.Sequential(\n            \n           # nn.Linear(256,128),\n           # nn.ReLU(),\n            #nn.Dropout(self.dropout),   ##added\n           # nn.Linear(128,128),\n           # nn.ReLU(),\n           # nn.Dropout(self.dropout),\n           # nn.Linear(128,1),\n            \n            \n        #)\n        self.final_layers = nn.Sequential(\n            \n            nn.Linear(4,16),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),   ##added\n            nn.Linear(16,16),\n            nn.ReLU(),\n            nn.Dropout(self.dropout),\n            nn.Linear(16,1)\n        )\n            \n        \n        \n    def forward(self,input_seq, meta):\n    \n        \n        #cont_input_seq, cat_input_seq, meta_input_seq = input_seqs[0]\n        # Note: dataloader provides vector in batch first form, whereas recurent layers need to have batch_first=True specified\n        # hidden layers still have batch size in second position even with batch_first=True\n        batch_size = input_seq.size(0)\n        rnn_count = input_seq.size(1)\n        seq_len = input_seq.size(2)\n        \n        \n        \n        \n        input_seq = F.relu(self.fc1(input_seq))\n        \n        \n        # combine first two dimensions (batchsize and rnncount) so that our input is 3D\n        \n        input_seq = input_seq.view(batch_size*rnn_count,seq_len, -1)\n        \n        \n        #output of shape ( batch_size*rnn_count, seq_len, num_directions * hidden_size)\n        #h_n (not needed)\n        output, h_n = self.rnn(input_seq)#,h_0)\n        \n        \n        \n        \n        #SpatialDropout1D in TensorFlow\n        #https:\/\/discuss.pytorch.org\/t\/spatial-dropout-in-pytorch\/21400\/2\n        \n        # drop whole channels across all timesteps\n        \n        output = output.permute(0, 2, 1)    # convert to [batch,rnn_count, channels, time]\n        output = F.dropout2d(output, self.dropout, training=self.training)\n        output = output.permute(0,2, 1)   # back to original\n        \n        \n        #same as GlobalMaxPool1D in TensorFlow?\n        # take max over seq_len dimension\n        # could also try concatenation of Average and Max Pooling \n        \n        output = F.adaptive_max_pool1d(output.permute(0, 2, 1),output_size=1).permute(0, 2, 1)\n        output = F.dropout(output,self.dropout,training=self.training)\n        \n        #output2 = F.adaptive_avg_pool1d(output.permute(0,2,1),output_size=1).permute(0, 2, 1).view(batch_size,-1)\n        #output2 = F.dropout(output2,self.dropout,training=self.training)\n        #output = torch.cat((output1,output2),1)\n        \n        # now shape (bs, rnn_count, num_directions * hidden_size)\n        \n        # reseparate the first two dims\n        \n        output = output.view(batch_size,rnn_count, -1)\n        \n        \n        \n        #output = torch.cat((output,meta.view(batch_size,1,self.meta_size).repeat(1,rnn_count,1)), 2)\n        \n        #add meta data to output\n        #output = torch.cat((meta_input_seq.view(batch_size,-1),output),1)\n        \n        #shared layers for all rnn_count outputs\n        output = self.layers2(output)\n        \n        # now shape (bs, rnn_count, num_directions * hidden_size)\n        \n        output = output.view(batch_size,-1)\n        #output = self.final_layers(output)\n        \n        output = torch.mean(output,dim =1)\n        \n        \n        #output = torch.cat((output.view(batch_size,-1),meta), 1)\n        \n        #output = self.final_layers(output)\n        \n        return output\n        \n        ","410f6fb9":"val_max_index = int(len(float_data) * 0.2 )    #last index for validation\n\n\nval_ds = EQdataset(float_data,min_index =0, max_index = val_max_index)\ntrn_ds = EQdataset(float_data,min_index =val_max_index +1)","1e95b14e":"trn_ds[0][0][0].shape","ade0add0":"bs = 4096\nbs2 = 4096\nnum_workers = 4\n\n#train_dl = DataLoader(trn_ds, batch_size=bs,shuffle=True, num_workers=num_workers)\n#val_dl = DataLoader(val_ds, batch_size=bs2,shuffle=False, num_workers=num_workers)\n","51cd8e85":"net = EQRNN(dropout=0.5)\nnet","78e96d04":"\n# CUDA for PyTorch\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\n#criterion = nn.NLLLoss(weight=torch.from_numpy(weights).to(device))\n\ncriterion =  nn.L1Loss()\n#criterion = mywloss\n\ndatabunch = DataBunch.create(trn_ds,val_ds, device=device, bs =32)","d87deb05":"learn = Learner(databunch,net,callback_fns=[ShowGraph], loss_func = criterion)\nlearn.opt_func=AdamW\n# save the best model\nlearn.callbacks = [SaveModelCallback(learn,monitor='val_loss',mode='min')]","415a36b9":"#lr_find(learn)\n#learn.recorder.plot()","03359bd4":"lr=1e-2\nwd = 1e-3\nlearn.fit(2,lr,wd=wd)","e067c9a3":"trn_ds = EQdataset(float_data,min_index =val_max_index +1)\ndatabunch = DataBunch.create(trn_ds,val_ds, device=device, bs=32)\nlearn.data = databunch","5ecf3e9c":"learn.fit(2,lr,wd=wd)","a7e99f91":"lr=1e-3\nwd = 1e-3\nlearn.fit(2,lr,wd=wd)","8af136a3":"learn.fit(4,lr,wd=wd)","eb7842fb":"learn.save('EQ205')","3b8d90e9":"#preds, target = learn.get_preds(ds_type=DatasetType.Valid)\n","b98adcf2":"# Load submission file\nsubmission = pd.read_csv(PATH+'sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n\n# Load each test data\n\ntest= []\nfor i, seg_id in enumerate(tqdm(submission.index)):\n    \n    seg = pd.read_csv(PATH+ seg_id + '.csv',dtype={'acoustic_data': np.float32})\n    test_ds  = EQdataset(np.array(seg),is_test = True)\n    learn.data = DataBunch.create(trn_ds,val_ds,test_ds, device=device)  \n    preds, _ = learn.get_preds(ds_type=DatasetType.Test)\n    submission['time_to_failure'][i] =preds\n   ","6bc38c91":"test_ds  = EQdataset(np.array(seg),is_test = True)\nlearn.data = DataBunch.create(trn_ds,val_ds,test_ds, device=device)  \npreds, _ = learn.get_preds(ds_type=DatasetType.Test)","d63216da":"submission['time_to_failure'].min()","2e3685c5":"submission.to_csv('EQsubmission2.csv')","5bcf4326":"float_data[:,1].min()","2ad8b42c":"float_data[150000][1] - float_data[0][1]","d3c6a11a":"## Submission"}}