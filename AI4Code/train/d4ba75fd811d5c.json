{"cell_type":{"b3215386":"code","a78bdcfd":"code","b1abda05":"code","ae34c531":"code","5981f37b":"code","ab04069f":"code","06f94ac8":"code","63e8c1c9":"code","46524907":"code","f63d6000":"code","a40979b2":"code","ea0c0102":"code","7fdd5f69":"code","cca5343f":"code","b7867911":"code","8c1cda3b":"code","b53d0638":"code","4d82a4c0":"code","d6e71e26":"code","c121bf14":"code","26e9f9cf":"code","9a1fc675":"code","cf019580":"code","dc5f98d2":"code","eee776ba":"code","447d5a54":"code","4877d5ad":"code","b5e57e8c":"code","de6fb9fa":"code","2dfbccc8":"code","0c74464b":"code","6c1a698f":"code","6a383ca7":"code","8db729b3":"code","9a2f9f1e":"code","7edfaaad":"code","88888ce4":"code","65f55464":"code","832a9cac":"code","a2325d58":"code","8b2ccb5b":"code","cd8a526e":"code","fbd34538":"code","5d856032":"code","31c7b591":"code","913c020a":"code","5675499b":"code","81c25cf8":"code","82bcc732":"code","61a0a021":"code","f29eef76":"code","4ea747c0":"code","251cc9b2":"code","33c85912":"code","94f7811c":"code","4b02d289":"code","a1a12ee1":"code","3f5f50aa":"code","1135d317":"code","7c1825a3":"markdown","998418ed":"markdown","e0c797cd":"markdown","b2150c0b":"markdown","bb5855ef":"markdown"},"source":{"b3215386":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a78bdcfd":"#Importing required libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# warnings triggered during the process of importing a module (ignored by default)","b1abda05":"# Loading of Dataset\ndf=pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf.head()\n","ae34c531":"df.info()","5981f37b":"df.shape\n# gives rows and column","ab04069f":"# date column is in object datatype, we need to convert it after taking only year that is required for our analysis.\ndfnew=[ ]\nfor i in df['date'].values:\n    dfnew.append(i[0:4])\ndf['date']=dfnew\n# to extract year from date column\n\ndf['date']=df['date'].astype(int)  # the column was in object datatype","06f94ac8":"df['age']=df['date']-df['yr_built']  \n# calculating age of house ","63e8c1c9":"df['yr_renovated']=df['yr_renovated'].apply(lambda x: 1 if x!=0 else 0)\n# treating 0 in this column as house is renovated or not","46524907":"df.drop(['id','date','yr_built'],axis=1,inplace=True)\n\n# dropping not required columns in the dataset","f63d6000":"df.isnull().sum()\n# isnull() is used to find the null values present in the system \n# sum() gives the total of (null values==True)","a40979b2":"df.duplicated().sum()\n# checking for duplicate values","ea0c0102":"df.drop_duplicates(inplace=True)\n# dropping duplicated record and also saving the changes by passing (inplace=True) parameter","7fdd5f69":"df.duplicated().sum()\n# cross-checking for duplicated values","cca5343f":"corr_features =[]\n\nfor i , r in df.corr().iterrows():\n    k=0\n    for j in range(len(r)):\n        if i!= r.index[k]:\n            if r.values[k] >=0.5:\n                corr_features.append([i, r.index[k], r.values[k]])\n        k += 1\ncorr_features\n\n# correlaion between the columns","b7867911":"feat =[]\nfor i in corr_features:\n    if i[2] >= 0.8:\n        feat.append(i[0])\n        feat.append(i[1])\n        print(feat)\n        \n# highly correlated features","8c1cda3b":"corr = df.corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, annot=True, fmt= '.2f',annot_kws={'size': 15}, cmap= 'coolwarm')\nplt.show()\nprint(corr)\n\n# heatmap ","b53d0638":"for i in df.iloc[0:,1:].columns:\n    sns.boxplot(df[i],data=df)\n    print(i)\n    plt.show()","4d82a4c0":"L =[1,2,3,4,9,10,14]\ndef outlier(df):\n    for x in df.iloc[:,L].columns :        \n        Q1=df[x].quantile(0.25)\n        Q3=df[x].quantile(0.75)\n        IQR=Q3-Q1\n        Lower = Q1-(1.5*IQR)\n        Upper = Q3+(1.5*IQR)\n        df.loc[:,x]= np.where(df[x].values > Upper, Upper-1, df[x].values)\n        df.loc[:,x]= np.where(df[x].values < Lower, Lower+1, df[x].values)\n\noutlier(df)","d6e71e26":"for i in df.iloc[:,1:].columns:\n    sns.boxplot(df[i],data=df)\n    print(i)\n    plt.show()","c121bf14":"#Data Visualisation","26e9f9cf":"y=df['price']","9a1fc675":"X=df.copy()","cf019580":"from sklearn.model_selection import train_test_split","dc5f98d2":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.30,random_state=4)","eee776ba":"display(X_train.head(),y_train.head(),X_test.head(),y_test.head())","447d5a54":"# from sklearn.preprocessing import StandardScaler,MinMaxScaler\n# sc=StandardScaler()\n\n# X_train_transform= sc.fit_transform(X_train)\n# X_test_transform= sc.transform(X_test)\n\n# Scaling the dataframe using StandardScaler and can also use Minmax scaler also","4877d5ad":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\n\n# using LinearRegression Model","b5e57e8c":"lr.fit(X_train,y_train)\n#fitting the model (training the model)","de6fb9fa":"pred_lr_train=lr.predict(X_train)\n# prediction on training data in order to evaluate model is not overfit or underfit","2dfbccc8":"pred_lr=lr.predict(X_test)","0c74464b":"from sklearn.metrics import mean_squared_error\nprint('mean_squared_error',mean_squared_error(y_test,pred_lr))","6c1a698f":"from sklearn.metrics import mean_absolute_error\nprint('mean_absolute_error',mean_absolute_error(y_test,pred_lr))","6a383ca7":"from sklearn.metrics import mean_squared_error\nRMS=mean_squared_error(y_test,pred_lr,squared=False)\nprint('RMS: ',RMS)","8db729b3":"from sklearn.metrics import r2_score\nr2_score(y_test,pred_lr)","9a2f9f1e":"from sklearn.metrics import r2_score\nr2_score(y_train,pred_lr_train)\n# checking R2 score for training data","7edfaaad":"#our model is neither underfit nor overfit\n# lets see it in another model as R2 score is 1","88888ce4":"from sklearn.neighbors import KNeighborsRegressor\nkn=KNeighborsRegressor(n_neighbors=3,metric='euclidean')","65f55464":"# finding optimal value for k neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\n\nerror_rate = []\nfor i in range(1,20):\n    knn1 = KNeighborsRegressor(n_neighbors=i)\n    knn1.fit(X_train,y_train)\n    pred_i = knn1.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=5)   ## plot() used to plot the points on the canvas\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')  ## X axis ki labelling hogi\nplt.ylabel('Error Rate')  ## Y-axis ki labelling hogi\ny_pred = knn1.predict(X_test) ## predict() use \n\n\n# method to find value of k","832a9cac":"kn.fit(X_train,y_train)","a2325d58":"pred_knn=kn.predict(X_test)","8b2ccb5b":"pred_knn_train=kn.predict(X_train)","cd8a526e":"from sklearn.metrics import r2_score\nr2_score(y_test,pred_knn)","fbd34538":"from sklearn.metrics import r2_score\nr2_score(y_train,pred_knn_train)\n# checking R2 score for training data","5d856032":"# our model is neither underfit nor overfit","31c7b591":" #1. Visualizing the differences between actual prices and predicted ","913c020a":"plt.scatter(y_test,\n            pred_lr , \n            color = 'red')\n\n\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","5675499b":"#  2.  Checking residuals - Heteroskedasticity - The presence\u00a0of non-constant variance in \n#     the error terms results n\u00a0heteroskedasticity\n# # The  variance in the error terms should be costant. ","81c25cf8":"plt.scatter(pred_lr,\n            y_test-pred_lr)\n\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","82bcc732":"# 3. Checking Normality of errors\n","61a0a021":"sns.distplot(y_test-pred_lr)\n\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","f29eef76":"# 4. Check for Multicollinearity\n\n# \"multicollinearity\" refers to predictors that are correlated with other predictors\n\n# * If the VIF is equal to 1 there is no multicollinearity among factors,\n#   but if the VIF is greater than 1, the predictors may be moderately correlated. \n  \n# *  A VIF between 5 and 10 indicates high multicollinearity that may be problematic","4ea747c0":"from statsmodels.stats.outliers_influence import variance_inflation_factor","251cc9b2":"X.head()","33c85912":"X.shape","94f7811c":"[i for i in range(X.shape[1])] # Coz we want VIF score for all columns , by this command we \n# are genrating VIF score for each column","4b02d289":"# VIF(array , index)\nh = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(h)\n\npd.DataFrame(h, index=X.columns, columns = ['VIF Score'])\n\n# VIF < 4 - no multicollinearity\n# VIF > 4 - 10 - multicollinearity","a1a12ee1":"#5   Autocorrelation(Durbin- Watson Test)\nDW Statistic Test\nif DW<=2  (no auto correlation)\n   2< DW<10 (auto correlation)","3f5f50aa":"from statsmodels.stats.stattools import durbin_watson\ny_test - pred_lr # SKLEARN LINEAR REGRESSION PREDICTED VALUE","1135d317":"durbin_watson(y_test-pred_lr)\n","7c1825a3":"KNN Regressor","998418ed":"# **ASSUMPTIONS IN LINEAR REGRESSION MODEL**\n\nThere are basically 5 assumptions in Linear Regression\n1) Linearity of Model \n2) Heteroskedasticity\n3) Normal distribution of Error terms\n4) Multicollinearity\n5) AutoCorrelation Normal distribution of Error terms","e0c797cd":"**Evaluation Metrics for Regression Problem**\n#MSE(Mean Squared Error)\n#RMSE(Root MeanSquared Error)\n#MAE(Mean Absolute Error)\n#MAPE(Mean Absolute Percentage Error)\n#R2 Score\n#adjusted R2 score","b2150c0b":"Dataset contains:**\n\nId: a notation for a house\n\nDate: Date house was sold\n\nPrice: Price is prediction target\n\nBedrooms: Number of Bedrooms\/House\n\nBathrooms: Number of bathrooms\/House\n\nSqft_Living: square footage of the home\n\nSqft_Lot: square footage of the lot\n\nFloors: Total floors (levels) in house\n\nWaterfront: House which has a view to a waterfront\n\nView: Has been viewed\n\nCondition: How good the condition is ( Overall )\n\nGrade: overall grade given to the housing unit, based on King County grading system\n\nSqft_Above: square footage of house apart from basement\n\nSqft_Basement: square footage of the basement\n\nYr_Built: Built Year\n\nYr_Renovated: Year when house was renovated\n\nZipcode: Zip\n\nLat: Latitude coordinate\n    \nLong: Longitude coordinate\n\nSqft_Living15: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n\nSqft_Lot15: lotSize area in 2015(implies-- some renovations)","bb5855ef":"#SCALING "}}