{"cell_type":{"0bd0c958":"code","73745207":"code","10d7cf74":"code","5acafbd5":"code","ad925a1f":"code","3bec91f9":"code","843d5240":"code","0a4faa2d":"code","8c7ccfac":"code","78f2c5cc":"code","4a7dc777":"code","9f39742f":"code","fad412bf":"code","ef537554":"code","b4c24d0c":"code","22702971":"code","7a335a1d":"code","a5417b24":"code","3e9d4d51":"code","02fde3f4":"code","0608e6b9":"code","c35fc888":"code","118e0522":"code","5ee953b5":"code","b7ef1cb2":"code","48659df8":"code","0409fe78":"code","1bb8ba14":"code","28e82ca7":"code","6a36d4a6":"code","f6ef6789":"code","85894728":"markdown","30f907c8":"markdown","3e2c5583":"markdown","bad42511":"markdown","9c7a6a1b":"markdown","41901e8e":"markdown","811abc44":"markdown","b2f44fa9":"markdown","53118aeb":"markdown","33975edc":"markdown","da5cf6c1":"markdown","95134361":"markdown","a75e1e21":"markdown","944767bc":"markdown","84c9dfdc":"markdown","78e6d963":"markdown"},"source":{"0bd0c958":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Mute the sklearn warning\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","73745207":"data = pd.read_csv(\"..\/input\/Orange_Telecom_Churn_Data.csv\")","10d7cf74":"data.head().T","5acafbd5":"# Check columns data type \ndata.dtypes.value_counts()","ad925a1f":"data.state.value_counts().plot(kind = 'bar', figsize = [14,6], color = 'indianred')\nplt.title('Checking State Level')\nplt.xlabel('State')\nplt.ylabel('Count');","3bec91f9":"print(\"State Level: {}\".format(len(data.state.value_counts())))\nprint(\"Area Code Level: {}\".format(len(data.area_code.value_counts())))\nprint(\"Phone Number Level: {}\".format(len(data.phone_number.value_counts())))","843d5240":"# Remove extraneous columns\ndata.drop(['state', 'area_code', 'phone_number'], axis=1, inplace=True)","0a4faa2d":"data.columns","8c7ccfac":"# Checking shape \ndata.shape","78f2c5cc":"plt.figure(figsize=(10,6))\nax = sns.countplot(x = 'churned', data = data)\nax.set_ylim(top=5000)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/len(data.churned)), (p.get_x()+ 0.3, p.get_height()+200))\n\nplt.title('Distribution of Churned (Target)')\nplt.xlabel('Churned')\nplt.ylabel('Count');","4a7dc777":"plt.figure(figsize=(10,6))\ndata.total_day_calls.plot(kind = 'hist', bins = 50, color = 'indianred')\nplt.title('Total Calls in Day', fontsize = 16)\nplt.xlabel('Number of calls');","9f39742f":"# Subplot \nplt.figure(figsize=(12,8))\n\nplt.subplot(221)\nsns.scatterplot(x='total_day_charge', y= 'total_eve_charge', data = data, hue = 'churned' )\n\nplt.subplot(222)\nsns.scatterplot(x='total_night_calls', y= 'total_day_calls', data = data, hue = 'churned' )\n\nplt.subplot(223)\nsns.scatterplot(x='total_intl_calls', y= 'total_intl_charge', data = data, hue = 'churned' )\n\nplt.subplot(224)\nsns.scatterplot(x='account_length', y= 'total_day_calls', data = data, hue = 'churned' )\n\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35);\n","fad412bf":"sns.set(style='white')\n\n# Compute the correlation matrix\ncorr = data.loc[:,[i for i in list(data.columns) if i not in ['churned']]].corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","ef537554":"lb = LabelBinarizer()\nfor col in ['intl_plan', 'voice_mail_plan', 'churned']:\n    data[col] = lb.fit_transform(data[col])","b4c24d0c":"msc = MinMaxScaler()\ndata = pd.DataFrame(msc.fit_transform(data),  \n                    columns=data.columns)","22702971":"# Get a list of all the columns that don't contain the label\nx_cols = [x for x in data.columns if x != 'churned']\n\n# Split the data into two dataframes\nX_data = data[x_cols]\ny_data = data['churned']","7a335a1d":"# Train the KNN model and fit on training set\nknn = KNeighborsClassifier(n_neighbors=3)\nknn = knn.fit(X_data, y_data)\ny_pred = knn.predict(X_data)","a5417b24":"# Function to calculate the % of values that were correctly predicted\ndef accuracy(real, predict):\n    return sum(y_data == y_pred) \/ float(real.shape[0])","3e9d4d51":"print(accuracy(y_data, y_pred))","02fde3f4":"print(roc_auc_score(y_data, knn.predict_proba(X_data)[::,1]))","0608e6b9":"# Create trainset and testset \nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30, random_state=42)","c35fc888":"# Run the model on 5 StratifiedKFold and check ROC AUC score\nkf = StratifiedKFold(n_splits=5,shuffle=False,random_state=121)\npred_test_full =0\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X_data,y_data):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_data.loc[train_index],X_data.loc[test_index]\n    ytr,yvl = y_data.loc[train_index],y_data.loc[test_index]\n    \n    #model\n    knn = KNeighborsClassifier()\n    knn.fit(xtr,ytr)\n    score = roc_auc_score(yvl,knn.predict_proba(xvl)[::,1])\n    print('ROC AUC score:',score)\n    cv_score.append(score)    \n    ","118e0522":"print(\"Average CV ROC AUC score: {}\".format(np.mean(cv_score)))","5ee953b5":"# Check model performance with different number of n_neighbours\nfor p in [1,2]:\n    if p == 1:\n        accuracies_1 = [] \n        number = []\n        for i in range(1,21):\n            knn = KNeighborsClassifier(n_neighbors=i, weights='uniform', p = p)\n            knn = knn.fit(X_train, y_train)\n            y_pred = knn.predict(X_test)\n            acc = roc_auc_score(y_test, y_pred)\n            accuracies_1.append(acc)\n            number.append(i)\n    else:\n        accuracies_2 = [] \n        for i in range(1,21):\n            knn = KNeighborsClassifier(n_neighbors=i, weights='uniform', p = p)\n            knn = knn.fit(X_data, y_data)\n            y_pred = knn.predict(X_data)\n            acc = accuracy(y_data, y_pred)\n            accuracies_2.append(acc)\nresult1 = pd.DataFrame({'NumberOfk':number,'Accuracy_1':accuracies_1})\nresult2 = pd.DataFrame({'NumberOfk':number,'Accuracy_2':accuracies_2})\n\nfinal = pd.merge(result1, result2)\n\nplt.figure(figsize=(12,6))\nfinal.Accuracy_1.plot(label='1')\nfinal.Accuracy_2.plot(label='2')\nplt.legend()\nplt.xlabel('n_neighbours')\nplt.ylabel('Accuracy');","b7ef1cb2":"def plot_learning_curve(model, x, y ):\n    # Learning curve \n    plt.figure(figsize=(12,6))\n    train_sizes, train_scores, valid_scores = learning_curve(model, x, y, scoring = 'accuracy', n_jobs = -1, train_sizes=np.linspace(0.01, 1.0, 20))\n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(valid_scores, axis=1)\n    test_std = np.std(valid_scores, axis=1)\n    # Draw lines\n    plt.plot(train_sizes, train_mean, '--', color=\"green\",  label=\"Training score\")\n    plt.plot(train_sizes, test_mean, color=\"brown\", label=\"Cross-validation score\")\n    # Draw bands\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n    # Create plot\n    plt.title(\"Learning Curve\", fontsize = 16)\n    plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.show()","48659df8":"knn = KNeighborsClassifier()\nplot_learning_curve(knn, X_train,y_train)","0409fe78":"# Grid search for KNNclassifier tuning \nclassifier = KNeighborsClassifier()\nparameters = [{'metric':['minkowski','euclidean','manhattan'],\n               'weights': ['uniform','distance'], \n               'n_neighbors':  np.arange(1,10)}]\n\ngrid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","1bb8ba14":"# Check hyperparameters\nprint(best_parameters )","28e82ca7":"# Run the model on 5 StratifiedKFold and check ROC AUC score\nkf = StratifiedKFold(n_splits=5,shuffle=False,random_state=121)\npred_test_full =0\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X_data,y_data):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_data.loc[train_index],X_data.loc[test_index]\n    ytr,yvl = y_data.loc[train_index],y_data.loc[test_index]\n    \n    #model\n    knn = KNeighborsClassifier(n_neighbors=8, weights='distance', metric = 'manhattan')\n    knn.fit(xtr,ytr)\n    score = roc_auc_score(yvl,knn.predict_proba(xvl)[::,1])\n    print('ROC AUC score:',score)\n    cv_score.append(score)    \n","6a36d4a6":"print(\"Average CV ROC AUC score: {}\".format(np.mean(cv_score)))","f6ef6789":"knn = KNeighborsClassifier(n_neighbors=8, weights='distance', metric = 'manhattan')\nplot_learning_curve(knn, X_train,y_train)","85894728":"## Introduction\n\nNearest neighbour is type of instance-based classifier, which trains itself, based on the training\ndata and predicts the class label of the unknown or target data points. Considering a point which\nneeds to be classified, the kNN (Nearest neighbour classifier) tries to find k nearest neighbour to\nthe target data point. It takes the class labels of the nearest neighbour and assigns the label of\ntarget data point accordingly.\n\nIn this notebook we are using customer churn data from the telecom industry. ","30f907c8":"### Data Exploration","3e2c5583":"As can be seen 85.86% entries are with chruned as False and only 14.14%  are churned. This is called a **Unbalanced Class** problem. In classification when the target unequal difference for different classes. This problem could be hadled using different procedures such as cross validation, class balancing using over sampling, under sampling, mixed sampling.\n\nLet us plot few other features and see how is distribution, correlation.","bad42511":"Without any cross validation the KNN classifier gives an accuracy of 94% and roc auc score of .97 Since we passed all the data to the model the model is now overfit. To overcome this cross validtion is applied.","9c7a6a1b":"Scatter plots are the best way to check relation between different features and here as `chruned` as target below we are checking features having hue as churned or not. ","41901e8e":"### Learning Curve \n\nA learning curve is the plot of the training\/cross-validation error versus the sample size. The learning curve can be used to detect whether the model has the high bias or high variance. If the model suffers from high bias problem, as the sample size increases, training error will increase and the cross validation error will decrease and at last they will be very close to each other but still at a high error rate for both training and classification error. And increasing the sample size will not help much for high bias problem.\n\nIf the model suffers from high variance, as the keep increasing the sample size, the training error will keep increasing and cross-validation error will keep decreasing and they will end up at a low training and cross-validation error rate. So more samples will help to improve the model prediction performance if the model suffer from high variance.","811abc44":"Target feature here is churned below we will explore the target, and some of the other features using which we will predict the churned.","b2f44fa9":"To find the nearest neighbours the distance between centroid to other data points is calculated. The Minkowski Distance is a generalized form\n\n$\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1\/p}$\n\nWhere n is the number of attributes, $\ud835\udc65i$ and $\ud835\udc66i$ are component of data objects x and y. where p\nis degree of distance. \n\nBased on value of p, the distance measures metric gets changed. For `p = 1`,\nthe distance is known as City-block metric. Whereas `p = 2`, Euclidean distance. For `\ud835\udc5f = \u221e`, supermum\ndistance corresponds to maximum difference between any dimensions","53118aeb":"### Hyperparameter optimization\n\nTuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process.","33975edc":"By Tuning the model and using 5 Stratified fold the average ROC AUC score was increased from 0.855 to 0.877 on the cross validation set. ","da5cf6c1":"As this is an example of unballanced class problem, `Stratified K fold` is used. Stratified K-Folds cross-validator\nProvides train\/test indices to split data in train\/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","95134361":"* Notice that the data contains a state, area code, and phone number.\n\nThere are 51 distinct value in state, 3 in area code and 5000 in phone number. Either we can transform these features into dummy variable or we could drop these features. Tranforming phone number does not make sense as it has 5000 unique values. State and area code could be transformed into dummy variable but it will lead to increase the number of column in dataset. Hence, we will not be using them, so they can be dropped from the data.","a75e1e21":"Notice that some of the columns are categorical data and some are floats. These features will need to be numerically encoded.\n\nFurther, We need to scale the data as K-nearest neighbors requires scaled data. ","944767bc":"There are 4 features with data type object Let us verify if we should keep these features for further modeling or we should drop them out.","84c9dfdc":"### Model and Evaluation","78e6d963":"We should seprate the target `churned` and other features. All the features except `chruned` will be used to predict the target hence will store them in `X_data` and `churned` to `y_data`\n\nAfter that will just train the knn with 3 number of neighbors and no other parameters. "}}