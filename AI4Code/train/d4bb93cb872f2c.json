{"cell_type":{"2dcc33e2":"code","f9114062":"code","809b4237":"code","5ddffc0b":"code","12c775fa":"code","e1ff0036":"code","99c665f3":"code","27ba6b17":"code","8cdfdea1":"code","894e08ce":"code","0f5f1096":"code","3170a701":"code","9017bdd6":"code","6c484139":"code","7dcd7e4c":"code","af51b370":"code","1b3ae2db":"code","4b12b417":"code","df1c7f5d":"code","cb0f0baf":"code","ad925da8":"code","fd387492":"code","96edf0f3":"code","bd547eb1":"code","67f8ac95":"code","adbd3b08":"code","2efd6870":"code","e093dcb5":"code","2725b9b7":"code","d0e7784d":"code","b55447c2":"code","6890e8b5":"code","75353db7":"code","24224786":"code","74732617":"code","72b6aaf5":"code","3fad9b72":"code","c21a691c":"code","c78e1765":"code","7dfaa1e3":"code","1476b17e":"code","331a5a8b":"code","98f7bb4d":"code","d8e8011d":"code","9c95352b":"code","2d79e4bc":"code","f8bf072e":"code","bc6aed1e":"code","71e7be41":"code","ca5864b7":"code","60ab699d":"markdown","1523c85d":"markdown","abc84439":"markdown","9880677f":"markdown","3df4e00f":"markdown","ef16093e":"markdown","ccfe759e":"markdown","d6bf0a02":"markdown","7074f8a9":"markdown","ee8c9130":"markdown","2929c21e":"markdown","90c5c166":"markdown","0fd49576":"markdown","6a79f80d":"markdown","0e7b89ca":"markdown","a7df6ddb":"markdown","06382a47":"markdown","86aefc62":"markdown","1e21585a":"markdown","d8ba44f6":"markdown","0dbf07b0":"markdown","6731b88b":"markdown","b213058c":"markdown","86e8160e":"markdown","040738e9":"markdown","86ade75c":"markdown","1cdeea75":"markdown","dba185a4":"markdown","93eb0d5a":"markdown","fbe1f443":"markdown","ca4efb6a":"markdown","3290a550":"markdown","3fafdeb1":"markdown","50198048":"markdown","a3aeb11d":"markdown","61c746be":"markdown","b8768b46":"markdown","e9768dfd":"markdown","82fe7d33":"markdown","8352dffb":"markdown","24195c23":"markdown"},"source":{"2dcc33e2":"# # Downgraded to the previous version due to the critical bug in 0.16\n# !pip install catboost==0.15","f9114062":"import pandas as pd\nimport numpy as np\n\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom catboost import Pool, CatBoostClassifier\n\nfrom scipy.stats import pearsonr, chi2_contingency\nfrom itertools import combinations\nfrom statsmodels.stats.proportion import proportion_confint","809b4237":"data = pd.read_csv(\n    '..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True,\n#     nrows=123456,\n    low_memory=False\n    )\n# data = data[(data.issue_d >= '2018-01-01 00:00:00') & (data.issue_d < '2019-01-01 00:00:00')]\ndata = data[data.issue_d >= '2008-01-01 00:00:00']\ndata.dropna(how=\"all\",axis=1,inplace=True)\ndata = data.reset_index(drop=True)\ndata.head()","5ddffc0b":"data['loan_status'].value_counts()","12c775fa":"data = data.loc[data['loan_status'].isin(['Fully Paid','Charged Off',\"Default\"])]#.drop_duplicates() #,\"Default\". ## ,\"Current\" - excluded\n# data = data.loc[~data['loan_status'].isin(['Current','In Grace Period',\"Late (16-30 days)\"])]\n\nprint(data.shape)\ndata.issue_d = pd.to_datetime(data.issue_d,infer_datetime_format=True)\ndata.zip_code = data.zip_code.str.replace(\"xx\",\"\").astype(\"float\") ## nullable int type","e1ff0036":"browse_notes = pd.read_excel('..\/input\/lending-club-loan-data\/LCDataDictionary.xlsx',\n                             sheet_name=1)\nbrowse_notes.head()","99c665f3":"browse_feat = browse_notes['BrowseNotesFile'].dropna().values\nbrowse_feat = [re.sub('(?<![0-9_])(?=[A-Z0-9])', '_', x).lower().strip() for x in browse_feat]","27ba6b17":"data_feat = data.columns.values\nnp.setdiff1d(browse_feat, data_feat)","8cdfdea1":"np.setdiff1d(data_feat, browse_feat)","894e08ce":"wrong = ['is_inc_v', 'mths_since_most_recent_inq', 'mths_since_oldest_il_open',\n         'mths_since_recent_loan_delinq', 'verified_status_joint']\ncorrect = ['verification_status', 'mths_since_recent_inq', 'mo_sin_old_il_acct',\n           'mths_since_recent_bc_dlq', 'verification_status_joint']\n\nbrowse_feat = np.setdiff1d(browse_feat, wrong)\nbrowse_feat = np.append(browse_feat, correct)","0f5f1096":"avail_feat = np.intersect1d(browse_feat, data_feat)\nX = data[avail_feat].copy()\nX.info()","3170a701":"X.select_dtypes('object').head()","9017bdd6":"X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)\nX['sec_app_earliest_cr_line'] = pd.to_datetime(X['sec_app_earliest_cr_line'], infer_datetime_format=True)","6c484139":"X['emp_length'] = X['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\nX['emp_length'] = X['emp_length'].str.extract('(\\d+)').astype('float')\n# X['id'] = X['id'].astype('float') # errrors in early rows","7dcd7e4c":"X.drop(['id',\"url\"],axis=1,inplace=True,errors=\"ignore\")","af51b370":"X.head()","1b3ae2db":"print(X.columns)","4b12b417":"nan_mean = 100*X.isna().mean()\nnan_mean = nan_mean[nan_mean != 0].sort_values()\nnan_mean","df1c7f5d":"X = X.drop(['member_id'], axis=1, errors='ignore') # keep desc despite being spare","cb0f0baf":"fill_empty = ['emp_title', 'verification_status_joint']\n# fill_max = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n#             'mths_since_last_major_derog', 'mths_since_last_record',\n#             'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n#             'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n#             'pct_tl_nvr_dlq','sec_app_mths_since_last_major_derog']\n# fill_min = np.setdiff1d(X.columns.values, np.append(fill_empty, fill_max))\n\nX[fill_empty] = X[fill_empty].fillna('')\n# X[fill_max] = X[fill_max].fillna(X[fill_max].max())\n# X[fill_min] = X[fill_min].fillna(X[fill_min].min())","ad925da8":"num_feat = X.select_dtypes('number').columns.values\nX[num_feat].nunique().sort_values()","fd387492":"X = X.drop(['num_tl_120dpd_2m', 'id'], axis=1, errors='ignore')","96edf0f3":"X_sample = X.sample(n=200000)","bd547eb1":"num_feat = X.select_dtypes('number').columns.values\ncomb_num_feat = np.array(list(combinations(num_feat, 2)))\ncorr_num_feat = np.array([])\nfor comb in comb_num_feat:\n    corr = pearsonr(X_sample[comb[0]], X_sample[comb[1]])[0]\n    corr_num_feat = np.append(corr_num_feat, corr)","67f8ac95":"high_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.97]\nhigh_corr_num","adbd3b08":"X = X.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')","2efd6870":"cat_feat = X.select_dtypes('object').columns.values\nX_sample[cat_feat].nunique().sort_values()","e093dcb5":"X = X.drop(['url'], axis=1, errors='ignore') ## keep , 'emp_title'","2725b9b7":"## exclude zipcode, state.. \ncat_feat = X.select_dtypes('object').columns.values\ncat_feat = [c for c in cat_feat if (c not in([\"zipcode\",'addr_state',\"state\",'emp_title',\"desc\",\"purpose\",\"title\"]))]\ncat_feat","d0e7784d":"comb_cat_feat = np.array(list(combinations(cat_feat, 2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(X_sample, values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0] \/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)","b55447c2":"high_corr_cat = comb_cat_feat[corr_cat_feat >= 0.95]\nhigh_corr_cat","6890e8b5":"X = X.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')","75353db7":"data['loan_status'].value_counts()","24224786":"y = data['loan_status'].copy()\n# y = y.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int') # ORIG\n## 1 if \"good\" - consider dropping \"current\"\n# y = y.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int')\ny = y.isin(['Fully Paid']).astype('int')\n\nX['loan_status_fullyPaid'] = y\ny.value_counts()","74732617":"X['loan_status'].describe()","72b6aaf5":"print(X.shape)\nprint(X.columns)\nX.head()","3fad9b72":"X.drop_duplicates().sample(frac=1).to_csv(\"LC_all.csv.gz\",index=False,compression=\"gzip\")","c21a691c":"# X_mod = X[X.grade == 'E'].copy()\n# X_mod = X_mod.drop(['grade', 'int_rate'], axis=1, errors='ignore')\n# y_mod = y[X_mod.index]\n\n# X_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, stratify=y_mod, random_state=0)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=0)","c78e1765":"# cat_feat_ind = (X_train.dtypes == 'object').nonzero()[0]\n# pool_train = Pool(X_train, y_train, cat_features=cat_feat_ind)\n# pool_val = Pool(X_val, y_val, cat_features=cat_feat_ind)\n# pool_test = Pool(X_test, y_test, cat_features=cat_feat_ind)\n\n# n = y_train.value_counts()\n# model = CatBoostClassifier(#learning_rate=0.03,\n#                            iterations=600,\n#                            early_stopping_rounds=50,\n#                            class_weights=[1, n[0] \/ n[1]],\n#                            verbose=False,\n#                            random_state=0)\n# model.fit(pool_train, eval_set=pool_val, plot=True);","7dfaa1e3":"# y_pred_test = model.predict(pool_test)\n\n# acc_test = accuracy_score(y_test, y_pred_test)\n# prec_test = precision_score(y_test, y_pred_test)\n# rec_test = recall_score(y_test, y_pred_test)\n# print(f'''Accuracy (test): {acc_test:.3f}\n# Precision (test): {prec_test:.3f}\n# Recall (test): {rec_test:.3f}''')\n\n# cm = confusion_matrix(y_test, y_pred_test)\n# ax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\n# ax.set_xlabel('Predicted')\n# ax.set_ylabel('True');","1476b17e":"# y_pred_const = np.ones(y_test.size)\n\n# acc = accuracy_score(y_test, y_pred_const)\n# prec = precision_score(y_test, y_pred_const)\n# rec = recall_score(y_test, y_pred_const)\n# print(f'''Accuracy (constant prediction): {acc:.3f}\n# Precision (constant prediction): {prec:.3f}\n# Recall (constant prediction): {rec:.3f}''')\n\n# cm = confusion_matrix(y_test, y_pred_const)\n# ax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\n# ax.set_xlabel('Predicted')\n# ax.set_ylabel('True');","331a5a8b":"# feat = model.feature_names_\n# imp = model.feature_importances_\n# df = pd.DataFrame({'Feature': feat, 'Importance': imp})\n# df = df.sort_values('Importance', ascending=False)[:10]\n# sns.barplot(x='Importance', y='Feature', data=df);","98f7bb4d":"# corr = X_mod[df['Feature'].values].corr()\n# mask = np.zeros_like(corr)\n# mask[np.triu_indices_from(mask)] = True\n# sns.heatmap(corr, mask=mask, square=True, cmap='RdBu_r', vmin=-1, vmax=1, annot=True, fmt='.2f');","d8e8011d":"# good = X_mod.loc[y_mod == 1, 'loan_amnt']\n# bad = X_mod.loc[y_mod == 0, 'loan_amnt']\n\n# bins = 20\n# sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n# ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n# ax.set_ylabel('Density')\n# ax.legend();","9c95352b":"# good = X_mod.loc[y_mod == 1, 'mths_since_recent_inq']\n# bad = X_mod.loc[y_mod == 0, 'mths_since_recent_inq']\n\n# bins = 20\n# sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n# ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n# ax.set_ylabel('Density')\n# ax.legend();","2d79e4bc":"# good = X_mod.loc[y_mod == 1, 'revol_util']\n# bad = X_mod.loc[y_mod == 0, 'revol_util']\n\n# bins = 20\n# sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n# ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n# ax.set_ylabel('Density')\n# ax.legend();","f8bf072e":"# y_proba_val = model.predict_proba(pool_val)[:, 1]\n# p_val, r_val, t_val = precision_recall_curve(y_val, y_proba_val)\n# plt.plot(r_val, p_val)\n# plt.xlabel('Recall')\n# plt.ylabel('Precision');","bc6aed1e":"# p_max = p_val[p_val != 1].max()\n# t_all = np.insert(t_val, 0, 0)\n# t_adj_val = t_all[p_val == p_max]\n# y_adj_val = (y_proba_val > t_adj_val).astype(int)\n# p_adj_val = precision_score(y_val, y_adj_val)\n# print(f'Adjusted precision (validation): {p_adj_val:.3f}')","71e7be41":"# n = y_adj_val.sum()\n# ci = proportion_confint(p_adj_val * n, n, alpha=0.05, method='wilson')\n# print(f'95% confidence interval for adjusted precision: [{ci[0]:.3f}, {ci[1]:.3f}]')","ca5864b7":"# y_proba_test = model.predict_proba(pool_test)[:, 1]\n# y_adj_test = (y_proba_test > t_adj_val).astype(int)\n# p_adj_test = precision_score(y_test, y_adj_test)\n# r_adj_test = recall_score(y_test, y_adj_test)\n# print(f'''Adjusted precision (test): {p_adj_test:.3f}\n# Adjusted recall (test): {r_adj_test:.3f}''')\n\n# cm_test = confusion_matrix(y_test, y_adj_test)\n# ax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)\n# ax.set_xlabel('Predicted')\n# ax.set_ylabel('True');","60ab699d":"## 2.3. Multicollinearity\n\nAlthough highly correlated features (*multicollinearity*) aren't a problem for the machine learning models based on decision trees (as used here), these features decrease importances of each other and can make feature analysis more difficult. Therefore, I calculate feature correlations and remove the features with very high correlation coefficients before applying machine learning.\n\nI start with numeric features and before calculating their correlations, it's a good practice to look at the number of their unique values.","1523c85d":"Note that for pairs of a numeric and a categorical feature correlation coefficients can't be interpreted in a meaningful way and therefore shouldn't be used.","abc84439":"Still not all the features from \"Browse Notes\" `browse_feat` could be matched with the original features `data_feat` so I print out the unmatched features from both lists to see if some of them could be matched manually.","9880677f":"The features `desc` and `member_id` are completely empty in 2018","3df4e00f":"Then I print out the number of unique values for categorical features.","ef16093e":"# 3. Modelling approach\n\nAs mentioned in the beginning, the goal of this project is to predict good loans among the high risk \/ high interest loans. The Lending Club has two features: 1) `grade` that assigns risk levels to loans (\"A\" for the lowest risk, \"E\" for the highest risk); 2) `int_rate` that assigns the interest rate according to the risk level (lowest rates for the grade \"A\", highest rates for the grade \"E\"). Therefore, for the modelling dataset `X_mod` I choose only the loans with the grade \"E\" and remove the features `grade` and `int_rate` since the latter is correlated with `grade` by design.\n\nThe modelling dataset is then split into training, validation and testing parts. The validation dataset will be used to adjust some of the hyperparameters such as number of iterations, precision\/recall. Both splits are stratified to ensure similar distribution of classes and to avoid one of the classes being left out in the resulting splits. The latter could happen especially in the case of highly imbalanced datasets. The parameter `random_state=0` is added for the reproducibility of results.","ccfe759e":"For modelling I use [CatBoost](https:\/\/catboost.ai\/) - a gradient boosting library based on decision trees. CatBoost is very efficient for datasets that contain categorical features with many categories. Instead of traditional one-hot encoding that generates a lot of features and makes gradient boosting of shallow decision tress difficult, CatBoost uses mean encoding that replaces each categorical feature with only one numerical feature. More about mean encoding can be found in [this video](https:\/\/www.coursera.org\/lecture\/competitive-data-science\/concept-of-mean-encoding-b5Gxv).\n\nBefore fitting the model I transform the datasets and their targets into the CatBoost objects `Pool()`. The model is defined using the object `CatBoostClassifier()` with several parameters. In `CatBoostClassifier()` the optimal value for the `learning_rate` is calculated automatically for the chosen `iterations` but it's not always the best value. Therefore, I set it to the quite small value 0.03 to ensure good convergence. The parameter `iterations` is set to 1000 so that the model would converge even with such a small learning rate. Another parameter `early_stopping_rounds` is set to 100 (0.1 of the parameter `iterations`) to stop the training and to save time if overfitting is observed. Since the dataset is highly imbalanced, the ratio of two classes `n[0] \/ n[1]` is passed to the parameter `class_weights`. Using class weights here is equivalent to random oversampling. Note that the final model will be defined not by the last iteration but by the best error score on the evaluation dataset. One can see this optimal point on the CatBoost plot (only visible in the Edit mode of the notebook).","d6bf0a02":"From the histogram for the feature `revol_util` (top 3) the loan is less likely to be returned (bad loans) if the revolving utilization is lower. This actually doesn't make much sense because revolving utilization is the percentage of the used credit on your credit card so higher revolving utilization indicates worse financial stability. Nevertheless, this dataset shows otherwise and it could be an interesting topic for discussion.","7074f8a9":"Finally all matching features are saved in the list `avail_feat` a new DataFrame `X` that only contains these features, is created. It is a good practice to set every newly created DataFrame as a copy in order to avoid hidden chained assignments and `SettingWithCopyWarning` further down the code.","ee8c9130":"* correlations selection stuff & all descriptive text + modelling forked from : https:\/\/www.kaggle.com\/danofer\/minimizing-risks-for-loan-investments\n    * notably better selection of features not present at time of prediction\n    \n* Additionally based on - https:\/\/www.kaggle.com\/danofer\/lendingclub-data-defaults","2929c21e":"The highly correlated pairs with the absolute value of their correlation coefficient \u22650.97 are printed below.","90c5c166":"# 1. Introduction\n\nNowadays one can invest in the loans of other people using online peer-to-peer lending platforms like, for example, the [Lending Club](https:\/\/www.lendingclub.com\/). On the Lending Club borrowers with higher credit scores (more trustworthy and less risky) get lower interest rates for their loans while borrowers with lower credit scores (less trustworthy and more risky) get higher rates. From the point of view of the investor the loans with higher interest rates are more attractive because they provide higher return on investment (ROI) but on the other hand they pose risks of being not returned at all (defaulted). Therefore, the machine learning model that could predict which of the high interest loans are more likely to be returned, would bring added value by minimizing the associated risks.\n\n<img src=\"https:\/\/i.imgur.com\/T4Chhxw.jpg\" width=\"400\"\/>","0fd49576":"The feature `url` has a unique value for each entry and should be removed to avoid overfitting. The feature `emp_title` has very large number of unique values and leads to the memory error when creating a contingency table, therefore I remove it as well.","6a79f80d":"The features `earliest_cr_line` and `sec_app_earliest_cr_line` are dates and their type should be changed to `datetime`. Later they will be transformed to ordinal numeric features by the machine learning model.","0e7b89ca":"From the histogram for the feature `mths_since_recent_inq` (top 2) the loan is less likely to be returned (bad loans) if the borrower had an inquiry recently. This also makes sense because inquiries are usually done when someone applies for a loan, a credit card, etc. so recent inquiries could indicate bad financial stability of the borrower.","a7df6ddb":"It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.","06382a47":"# 4. Feature importances\n\nDuring the early draft of this project the analysis of feature importances helped me to realize that the [Lending Club dataset provided by Wendy Kan from Kaggle](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) was actually including features that aren't available for investors. So I decided to include this analysis here in case someone finds it useful.\n\nAmong all features I selected 10 with the largest importance values (see below). The top 3 features are `loan_amnt`, `mths_since_recent_inq` and `revol_util`. The importances of `mths_since_recent_inq` and `revol_util`, however, are quite close to each other and the rest of the features so this ranking might slightly change for a different train-test split.","86aefc62":"This dataset contains more than 150 features but some of them are only relevant after the loan is issued and therefore, not available at the moment of investing. To get the list of features that are visible to investors, I use the [Lending Club Data Dictionary provided by Wendy Kan](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) (the sheet called \"Browse Notes\")\n* https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data","1e21585a":"This time I remove the second feature from each highly correlated feature pair in order to keep the feature `grade`. It will be used later to select high risk \/ high interest loans.","d8ba44f6":"From the confusion matrix it is clear that the model tries to predict both classes and doesn't prefer one over the other due to their imbalance. The latter is a common mistake and if you see accuracy scores without the confusion matrix, be very skeptical about those results. If over-\/undersampling isn't applied for imbalanced classes, the classifier will opt for the constant prediction in favor of the majority class. In this case the accuracy for this testing dataset will be quite high 0.909 (see below), although this classifier doesn't have any predictive power! This skewness in class predictions is very visible on the confusion matrix.","0dbf07b0":"From the precision-recall curve the best precision is 1 but then the recall would be extremely low so in the end the model might not predict good loans at all. Therefore, I exclude 1 from the precision array and find its maximum. The threshold array `t` returned by `precision_recall_curve()` is missing the threshold 0 in the beginning so I add it to match the dimension of the precision array `p`. Then I find the threshold that correspond to the maximum precision and recalculate the predicted labels. The obtained precision score for the adjusted labels is indeed the maximum (excluding 1) as can be seen from the precision-recall curve.","6731b88b":"The first feature (chosen arbitrarily) from each highly correlated feature pair is then removed.","b213058c":"The highly correlated pairs with the absolute value of their correlation coefficient \u22650.95 are printed below.","86e8160e":"The missing lines due to Excel formatting are removed and the feature names are saved in `browse_feat`. Some of them, however, have a different spelling format from the one used in the loan dataset. This format uses capital letters instead of underscores so I identify them using using regular expressions and then correct them. For some features there are whitespaces in front of their names which I remove as well.","040738e9":"The loans with the statuses `Current` and `Fully Paid` are definitely considered good. The loans with the statuses `In Grace Period` can be considered good or not depending on strictness of the investor. In this project I consider them as good loans. All the other statuses are considered as bad loans. Note that this dataset is highly imbalanced with the minority class being 0.037 of the majority class.","86ade75c":"For all pairs of the categorical features `comb_cat_feat` I calculate the Cramer's V correlation coefficient that is expressed through the chi-square statistic $\\chi^2$ of the contingency table:\n\n$$ V = \\sqrt{ \\frac{ \\chi^2 }{ n (\\text{min}(K_1, K_2) - 1) } } $$\n\nwhere $n$ is the sum of all elements in the contingency table, $K_1$ and $K_2$ are the dimensions of the contingency table. Note that Pearson's R correlation coefficient isn't applicable to categorical features and shouldn't be used.","1cdeea75":"For all pairs of the numeric features `comb_num_feat` I calculate their Pearson's R correlation coefficient and store it in `corr_num_feat`.\n\n* due to memory errors -we will do this on a sample of the data","dba185a4":"Finally by using the adjusted threshold on the testing dataset, the adjusted precision 0.939 is indeed within the above 95% confidence interval. Note, however, that the recall is significantly decreased from 0.632 to 0.068 but the precision only increased from 0.931 to 0.939. Of course the gain in precision depends on the train-test split and for a different testing dataset can be closer to the right boundary of the confidence interval. Getting higher values than that, however, is unlikely.","93eb0d5a":"In gradient boosting the importances of highly correlated features usually split between them. From the correlation heatmap (see below) the feature `revol_util` (top 3) is quite highly correlated with `bc_util` (top 5) which leads to the decreased importance of `revol_util`.","fbe1f443":"Note that the above precision-recall curve is only valid for this particular validation dataset. So the precision that corresponds to the adjusted threshold on this dataset will be different from the precision for the same threshold on a different dataset. If these two datasets, however, are sampled from the same population, the precision values will have a certain spread that can be estimated using confidence intervals. Since precision is a proportion, it has the binomial distribution and its confidence interval can be conveniently calculated using the Statsmodels method `proportion_confint()`. Typically 95% confidence interval is calculated which corresponds to the parameter `alpha=0.05`. This means that the precision will be within this interval in 95% of cases. Since the obtained precision values are close to 1 (edge case), it's better to use the Wilson interval by setting the parameter `method='wilson'`. Also one should carefully calculate the denominator for adjusted precision `n` by taking the total amount of predicted good loans (class 1) after adjustment and not before.","ca4efb6a":"## export","3290a550":"For categorical features `emp_title`, `verification_status_joint` the missing values should be filled with an empty string so these features are placed in the list `fill_empty`.\n\nFor some of the numeric features the missing values should be filled using the maximum value of the respective columns so these features are placed in the list `fill_max`. For example, the feature `mths_since_last_record` indicates the number of months since the last record (like bankruptcy, foreclosure, tax liens, etc.) so if missing, one should assume that no records were made and the number of months since the \"last\" record should be a maximum.\n\nFor the rest of the numeric features the missing values should be filled using the minimum value of the respective columns so these features are placed in the list `fill_min`. For example, the feature `emp_length` indicates the number of working years so if missing, one should assume that the borrower never worked and the number of working years should be a minimum.","3fafdeb1":"Indeed some of the features are spelled differently but mean the same thing, for example `verified_status_joint` and `verification_status_joint`. So I remove wrong and add correct ones to the list `browse_feat`.","50198048":"## 2.3. Missing values\n\nAlthough the machine learning model used here can deal with missing values, it is a good practice to do it yourself or ideally with a domain expert. Below is the table of columns with missing values and their ratio to the total number of rows.","a3aeb11d":"## 2.2. Feature types\n\nLet's check the categorical features and see if any of them could be transformed to other types.","61c746be":"Since the validation dataset was used to tune hyperparameters (the number of iterations), I predict targets for the testing dataset which the model hasn't seen yet. The metrics reported here are accuracy, precision and recall. They all have sensible values which is also confirmed by the confusion matrix shown below.","b8768b46":"# 2. Data preprocessing\n\n## 2.1. Available features\n\nThe [Lending Club dataset provided by Wendy Kan from Kaggle](https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data) doesn't contain some of the features that are available for investors like, for example, FICO scores. Therefore, I use the [Lending Club dataset provided by Nathan George](https:\/\/www.kaggle.com\/wordsforthewise\/lending-club) that has all Lending Club loan features. This dataset contains more than 2 million rows so to reduce the processing times, I only selected the loans issued in 2018 (\u22480.5 million rows).","e9768dfd":"The feature `num_tl_120dpd_2m` has only one value (a constant) and can be removed. The feature `id` has a unique value for each row and should also be removed, otherwise the model will overfit.","82fe7d33":"## 2.4. Target feature\n\nThe target feature for this dataset is an indicator if the loan is good (1) or bad (0). To identify good loans, I use their loan statuses and print their counts below. The description for each status is provided by the Lending Club:\n\n- Current: Loan is up to date on all outstanding payments.\n- In Grace Period: Loan is past due but within the 15-day grace period.\n- Late (16-30): Loan has not been current for 16 to 30 days.\n- Late (31-120): Loan has not been current for 31 to 120 days.\n- Fully paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.\n- Default: Loan has not been current for an extended period of time.\n- Charged Off: Loan for which there is no longer a reasonable expectation of further payments.","8352dffb":"# 5. Model adjustment\n\nThe previously reported model was obtained by minimizing both false positive and false negative errors that contribute to precision and recall respectively. In reality, however, one of these errors might have a larger impact so it would be better to optimize for it instead. In case of loan investing, the false positive errors are the number of bad loans that were identified as good so the investor will loose money by investing in them. This is a direct loss and should be avoided. The false negative errors are the number of good loans that were identified as bad so the investor will not earn extra money by not investing in them. This is a missed opportunity and is less critical compared to the direct loss. Therefore, the false positive errors should be decreased (higher precision) even if the false negative errors will be increased (lower recall). The connection between precision and recall can be visualized using the precision-recall curve (see below). To calculate it, one requires probabilities of belonging to class 1 rather than the predicted labels. This precision-recall curve is calculated for the validation dataset because adjusting precision or recall is similar to adjusting hyperparameters. For each precision-recall pair the function `precision_recall_curve()` also returns the corresponding probability threshold. This threshold is the actual hyperparameter that will be used to obtain the best precision.","24195c23":"The features `emp_length` and `id` are numeric and their type should be changed to `float`. In case of `emp_length` I replace the extreme cases of \"< 1 year\" and \"10+ years\" with \"0 years\" and \"11 years\" respectively to separate these groups from the rest."}}