{"cell_type":{"91c5e5b7":"code","a7686c1b":"code","d9a165af":"code","6b0841e2":"code","f6e08250":"code","61cad929":"code","9b415f77":"code","5f4d99d9":"code","9043f127":"code","0b7b0450":"code","63471a5d":"code","e04ec341":"code","68e9a221":"code","28e41dfb":"code","eced967f":"code","8b3b18fa":"code","3a51fb96":"code","590eb436":"code","615e84fe":"code","b0bd485e":"markdown","8e869edc":"markdown","57ed4d8f":"markdown","65a7af53":"markdown","9884428c":"markdown","a1826006":"markdown","f5796b28":"markdown"},"source":{"91c5e5b7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('darkgrid')\nsns.set(rc={'figure.figsize':(15, 10)})\n\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom statistics import mean\n\nRS = 69420\n\n# 9.5K to beat","a7686c1b":"df = pd.read_csv(\"..\/input\/superconductor-dataset\/train.csv\")\ndf","d9a165af":"# from matplotlib.backends.backend_pdf import PdfPages\n# import matplotlib\n# matplotlib.use('pdf')\n\n# pp = PdfPages('C:\/Users\/Elite_PC\/OneDrive\/Documents\/Website\/ML Projects\/SuperConductor\/Distribution_plot.pdf')\n\n# for i in range(len(df.columns)):\n#     plt.figure()\n#     plot = sns.distplot(df.iloc[:, i])\n#     plot.set_title((df.columns[i] + '_distribution'), fontsize=20)\n#     plot = plot.get_figure()\n#     pp.savefig(plot)\n    \n# pp.close()","6b0841e2":"X = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nX.shape, y.shape","f6e08250":"import optuna \nfrom optuna.visualization import plot_optimization_history\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\ndef objective(trial: Trial, X, y) -> float:\n    \n    # Split into Train-Test-Validation, 60, 20, 20\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RS)\n    \n    evals = [(X_val, y_val)]\n    \n    # Assign Parameter Dict\n    param = {\n                \"n_estimators\":trial.suggest_int('n_estimators', 0, 1000),\n                'max_depth':trial.suggest_int('max_depth', 2, 25),\n                'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n                'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n                'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n                'subsample':trial.suggest_float('subsample', 0, 1),\n                'gamma':trial.suggest_int('gamma', 0, 5),\n                'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n                'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01)\n            }\n    \n    # Build Model\n    model = XGBRegressor(**param,\n                         predictor = 'gpu_predictor',\n                         tree_method = 'gpu_hist',\n                         eval_metric = 'rmse',\n                         verbosity=1)\n    \n    # Fit Model\n    model.fit(X_train, y_train, eval_set = evals, eval_metric = 'rmse', early_stopping_rounds = 5)\n    \n    # Predict\n    y_pred = model.predict(X_test)\n    y_valpred = model.predict(X_val)\n    \n    # Compute Metrics\n    test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n    val_rmse = mean_squared_error(y_val, y_valpred, squared=False)\n    \n    return mean((val_rmse, test_rmse))","61cad929":"%%time\n# You can vary number of trials to change the result\nstudy = optuna.create_study(study_name= 'SuperConductor',\n                            direction= 'minimize',\n                            sampler= TPESampler())\n\nstudy.optimize(lambda trial : objective(trial, X, y), n_trials= 250)","9b415f77":"hist = study.trials_dataframe()\n\nplot_optimization_history(study)","5f4d99d9":"XGB_Params = study.best_trial.params\nprint('Best Trial RMSE: {},\\nparams {}'.format(study.best_trial.value, study.best_trial.params))","9043f127":"import optuna \nfrom optuna.visualization import plot_optimization_history\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\ndef objective(trial: Trial, X, y) -> float:\n    \n    # Split into Train-Test-Validation, 60, 20, 20\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RS)\n    \n    evals = [(X_val, y_val)]\n    \n    # Assign Parameter Dict\n    param = {\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            }\n    \n    # Build Model\n    model = LGBMRegressor(**param,\n                          device = 'gpu',\n                          verbosity=1)\n    \n    # Fit Model\n    model.fit(X_train, y_train, eval_set = evals, eval_metric = 'rmse', early_stopping_rounds = 5)\n    \n    # Predict\n    y_pred = model.predict(X_test)\n    y_valpred = model.predict(X_val)\n    \n    # Compute Metrics\n    test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n    val_rmse = mean_squared_error(y_val, y_valpred, squared=False)\n    \n    return mean((val_rmse, test_rmse))","0b7b0450":"%%time\nstudy = optuna.create_study(study_name= 'SuperConductor_LGBM',\n                            direction= 'minimize',\n                            sampler= TPESampler())\n\nstudy.optimize(lambda trial : objective(trial, X, y), n_trials= 1000)","63471a5d":"plot_optimization_history(study)","e04ec341":"LGBM_Params = study.best_trial.params\nprint('Best Trial RMSE: {},\\nparams {}'.format(study.best_trial.value, study.best_trial.params))","68e9a221":"lgbm = LGBMRegressor(**LGBM_Params,\n                     device='gpu')\n\nxgb = XGBRegressor(**XGB_Params,\n                   predictor = 'gpu_predictor',\n                   tree_method = 'gpu_hist',\n                   verbosity=1)","28e41dfb":"from vecstack import StackingTransformer\n\nmodels = [('xgb', xgb)]\n\nstack = StackingTransformer(estimators= models,\n                            regression= True,\n                            metric= mean_squared_error,\n                            n_folds= 10, \n                            shuffle= True,  \n                            random_state= RS,    \n                            verbose= 2)","eced967f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)","8b3b18fa":"%%time\nstack = stack.fit(X_train, y_train)","3a51fb96":"# Create Stacked Train-Test\nS_train = stack.transform(X_train)\nS_test = stack.transform(X_test)","590eb436":"# Train Final Layer\nlgbm1 = lgbm\nlgbm1 = lgbm1.fit(S_train, y_train)\ny_pred = lgbm1.predict(S_test)\nprint('Final RMSE: [%.6f]' % mean_squared_error(y_test, y_pred, squared= False))","615e84fe":"dump(stack, \"D:\/SuperConductor\/Stack.joblib\")\ndump(lgbm1, \"D:\/SuperConductor\/Stacked_Lgbm.joblib\")","b0bd485e":"# Import Libraries","8e869edc":"# XGB Hyper-Param Tuning","57ed4d8f":"# Stacking Models","65a7af53":"# Data Import and Exploration","9884428c":"This notebook serves as a benchmark for the current state-of-the-art approach to modelling superconductor critical temperatures as developed by me","a1826006":"# LightGBM Hyper-Param Tuning","f5796b28":"The following code will produce a pdf of distribution plots for each column"}}