{"cell_type":{"b12356fe":"code","cbe77596":"code","65308bb3":"code","e60e13f4":"code","5782335b":"code","1ee67f46":"code","164f84a1":"code","a25136de":"code","7a4e774f":"code","df8e8732":"code","621ab1b8":"markdown","4bba3b86":"markdown","3060ae5d":"markdown","be134eac":"markdown","d0ed268d":"markdown","f831ce7a":"markdown","47c9d344":"markdown"},"source":{"b12356fe":"EPOCHS = 3\nBATCH_SIZE = 32\nLABEL_SMOOTHING = 0.1","cbe77596":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport math\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor","65308bb3":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nPAD_ID = 1\nSEED = 777\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')","e60e13f4":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","5782335b":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","1ee67f46":"import pickle\n\ndef scheduler(epoch, lr):\n    if (epoch < 2):\n        new_lr = lr\n    else:\n        new_lr = lr * 0.25\n    print (\"Setting learning rate to {:10.3E}\".format(new_lr))\n    return new_lr\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    model  = TFRobertaForQuestionAnswering.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    \n    x = model(ids_,attention_mask=att_,token_type_ids=tok_)\n        \n    def loss_fn(y_true, y_pred):\n        ll = tf.shape(y_pred)[1]\n        y_true = y_true[:, :ll]\n        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n            from_logits=False, label_smoothing=LABEL_SMOOTHING)\n        loss = tf.reduce_mean(loss)\n        return loss\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x[0], x[1]])\n    optimizer = tf.keras.optimizers.Nadam(learning_rate=3e-5)\n    model.compile(loss=loss_fn, optimizer=optimizer)\n\n    x1_padded = tf.pad(x[0], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x[1], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n\n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model\n\nmodel, padded_model = build_model()\nmodel.summary()\n","164f84a1":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","a25136de":"jac = []; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\n\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n    \n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    \n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = 'sb-model-fold_%i.h5'%(fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        history = model.fit(\n            inpT, targetT, \n            epochs=epoch,\n            initial_epoch=epoch - 1,\n            batch_size=BATCH_SIZE,\n            verbose=DISPLAY,\n            callbacks=[reduce_lr],\n            validation_data=(inpV, targetV),\n            shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n                            \n    print('Loading model...')\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n            \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        if (train.loc[k,'sentiment'] == 'neutral'):\n            st = train.loc[k,'text']\n            all.append(jaccard(st,train.loc[k,'selected_text']))\n        else:\n            a = np.argmax(oof_start[k,])\n            b = np.argmax(oof_end[k,])\n            if a>b:\n                st = train.loc[k,'text']\n            else:\n                text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n                enc = tokenizer.encode(text1)\n                st = tokenizer.decode(enc.ids[a-1:b])\n            all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","7a4e774f":"all = []\nfor k in range(input_ids_t.shape[0]):\n    if (test.loc[k,'sentiment'] == 'neutral'):\n        st = test.loc[k,'text']\n    else:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b:\n            st = train.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","df8e8732":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","621ab1b8":"# Loading data","4bba3b86":"# Motivation\n\nOn [TensorFlow roBERTa Explained](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/143281), Chris explained how the roBERTa model worked and how we could add custom Q&A heads to it, as the Huggingface implementation of TFRobertaForQuestionAnswering wasn't available at that time.\n\nAt [v2.9.0](https:\/\/github.com\/huggingface\/transformers\/releases\/tag\/v2.9.0), released on May 7, the [TFRobertaForQuestionAnswering](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html#tfrobertaforquestionanswering) model was released.\n\nIn this notebook, I'll use the [Faster (2x) TF roBERTa](https:\/\/www.kaggle.com\/seesee\/faster-2x-tf-roberta) as base for running this model provided by [Hugging Face](https:\/\/huggingface.co\/).\n\nThe intention here is to show a way of adding it to your ensemble and that's it.\n\nIf you want to turn this into an inference notebook, just comment the whole loop at `for epoch in range(1, EPOCHS + 1):` and add your offline trained models into a dataset of your own.","3060ae5d":"# Training","be134eac":"# Metrics","d0ed268d":"# Modeling","f831ce7a":"# Generating submission","47c9d344":"# Configuration"}}