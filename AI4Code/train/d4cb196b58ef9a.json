{"cell_type":{"146f02d3":"code","f1135309":"code","748ef7ea":"code","441bf7d6":"code","1a33d3a3":"code","c672858f":"code","4c28762b":"code","0059c2ff":"code","2849fcbf":"code","4b669ccc":"code","63c4338c":"code","9c210f7e":"code","c47e6545":"code","0b7ae60e":"code","36f8e408":"code","2341f84b":"code","83bd6dba":"code","cb40e23b":"code","0d5afabf":"code","12cffe69":"code","6b4dac2d":"code","e06a9a46":"code","3d530450":"code","75c1f551":"code","7c7f5128":"code","2cf2be48":"code","02c216ce":"code","18db80b7":"code","d37a5caf":"code","5cc67de4":"code","e83fefdc":"code","cc4db6d8":"code","476a6e44":"code","a22aa0dc":"code","71bb3cc7":"code","f1d8e608":"code","45a5842c":"markdown","ebfd7fe4":"markdown","e05bdf5f":"markdown","d2d1d1dc":"markdown","66d4520e":"markdown","a6b5a98f":"markdown"},"source":{"146f02d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1135309":"!pip install imutils > \/dev\/null\n!pip install cython > \/dev\/null\n# Install pycocotools from this GitHub rep\n!pip install git+\"https:\/\/github.com\/philferriere\/cocoapi.git#subdirectory=PythonAPI\" > \/dev\/null ","748ef7ea":"TOP_PATH = ('\/kaggle\/input\/coco-2017-dataset\/coco2017')\nTRAIN_PATH = ('\/kaggle\/input\/coco-2017-dataset\/coco2017\/train2017')\nVAL_PATH = ('\/kaggle\/input\/coco-2017-dataset\/coco2017\/val2017')\nANNOTATIONS_PATH = ('\/kaggle\/input\/coco-2017-dataset\/coco2017\/annotations')\nTEST_PATH = ('\/kaggle\/input\/coco-2017-dataset\/coco2017\/test2017')\nWORKING_DIR = ('\/kaggle\/working')\nMASK_TRAIN_DIR = ('\/kaggle\/working\/mask_train')\nMASK_VAL_DIR =('\/kaggle\/working\/mask_val')\nMASK_TRAIN_SMOOTH_DIR = ('\/kaggle\/working\/mask_train_smooth')","441bf7d6":"os.chdir(WORKING_DIR)","1a33d3a3":"# Import all the libraries\nimport numpy as np\nimport cv2\nimport requests\nimport os\nimport imutils\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n########3\nfrom pycocotools import coco, cocoeval, _mask\nfrom pycocotools import mask as maskUtils\nfrom pycocotools.coco import COCO\nimport skimage.io as io\nimport random\n#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n### For visualizing the outputs ###\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nfrom random import shuffle\n\nfrom PIL import Image \nimport sys\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.metrics import *\n\n\n\n# Load the TensorBoard notebook extension.\n\n%load_ext tensorboard\n\n\nfrom datetime import datetime\n\nseed = 2019\n\nrandom.seed = seed\nnp.random.seed = seed","c672858f":"os.listdir(ANNOTATIONS_PATH)","4c28762b":"ANNOTATION_FILE_VAL = (ANNOTATIONS_PATH + '\/instances_val2017.json')\nANNOTATION_FILE_TRAIN = (ANNOTATIONS_PATH + '\/instances_train2017.json')","0059c2ff":"coco_train = COCO(ANNOTATION_FILE_TRAIN)\ncatIds_train = coco_train.getCatIds() # Get all Categories ('horse','human' etc...)\nimgIds_train = coco_train.getImgIds() # Get all image ID's (dict with path and annotations)\nimgDict_train = coco_train.loadImgs(imgIds_train) # Func to load images from path\nprint(len(imgIds_train) , len(catIds_train))","2849fcbf":"coco_val = COCO(ANNOTATION_FILE_VAL)\ncatIds_val = coco_val.getCatIds()\nimgIds_val = coco_val.getImgIds()\nimgDict_val = coco_val.loadImgs(imgIds_val)\nprint(len(imgIds_val) , len(catIds_val))","4b669ccc":"shuffle(imgIds_train) # randoms shuffling\nshuffle(imgIds_val) # random shuffling\n\n# take less samples so that training period is less\nimgIds_train = imgIds_train[0:60000]\nimgIds_val = imgIds_val[0:5000]\n# imgIds_train = imgIds_train[0:3000]\n# imgIds_val = imgIds_val[0:500]\n","63c4338c":"# used to format string according to naming convention of the files\nprint(\"{0:012d}.jpg\".format(12345678))\nprint(\"{0:012d}.jpg\".format(4))","9c210f7e":"train_images = [\"COCO_train2014_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n# keep aside train and val image names\nval_images = [\"COCO_val2014_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\nprint(len(train_images) , len(val_images))","c47e6545":"os.chdir(WORKING_DIR)","0b7ae60e":"!mkdir mask_train","36f8e408":"count = 0 \nunfitted_image_dict = []\nfor ID in tqdm(imgIds_train):\n    file_path = MASK_TRAIN_DIR+\"\/{0:012d}.jpg\".format(ID)\n    # basic functionality is to take all masks, and create a unified mask\n    sampleImgIds = coco_train.getImgIds(imgIds = [ID])\n    sampleImgDict = coco_train.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n    # take all annotations for all categories\n    annIds = coco_train.getAnnIds(imgIds=sampleImgDict['id'], \n                                catIds=catIds_train, \n                                iscrowd=0)\n    anns = coco_train.loadAnns(annIds)\n    \n    # separating images with background as the only mask\n    if len(anns)==0:\n        unfitted_image_dict.append(sampleImgDict)\n    else:\n        mask = coco_train.annToMask(anns[0])\n        # unify all the masks provided\n        for i in (range(len(anns))):\n            mask = mask | coco_train.annToMask(anns[i])\n        mask = Image.fromarray(mask * 255 , mode = \"L\")\n        # save the mask\n        mask.save(file_path)\n        count = count + 1\nprint(count)","2341f84b":"os.chdir(WORKING_DIR)","83bd6dba":"!mkdir mask_val","cb40e23b":"count = 0 \nunfitted_image_dict = []\nfor ID in tqdm(imgIds_val):\n    file_path = MASK_VAL_DIR+\"\/{0:012d}.jpg\".format(ID)\n\n    sampleImgIds = coco_val.getImgIds(imgIds = [ID])\n    sampleImgDict = coco_val.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n\n    annIds = coco_val.getAnnIds(imgIds=sampleImgDict['id'], \n                                catIds=catIds_val, \n                                iscrowd=0)\n    anns = coco_val.loadAnns(annIds)\n\n    if len(anns)==0:\n        unfitted_image_dict.append(sampleImgDict)\n    else:\n        mask = coco_val.annToMask(anns[0])\n\n        for i in (range(len(anns))):\n            mask = mask | coco_val.annToMask(anns[i])\n        mask = Image.fromarray(mask * 255 , mode = \"L\")\n        mask.save(file_path)\n        count = count + 1\nprint(count)","0d5afabf":"os.chdir(WORKING_DIR)","12cffe69":"r1 = random.randint(0, 1)\nr1","6b4dac2d":"class DataGen(keras.utils.Sequence):\n    def __init__(self , path_input , path_mask , batch_size = 8 , image_size = 128):\n        self.ids = os.listdir(path_mask) # only take those images that we have segregated masks of\n        self.path_input = path_input\n        self.path_mask = path_mask\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.on_epoch_end()\n        \n    def __load__(self , id_name):\n        image_path = os.path.join(self.path_input , id_name)\n        mask_path = os.path.join(self.path_mask , id_name)\n        image = cv2.imread(image_path , 1) # 1 specifies RGB format\n        image = cv2.resize(image , (self.image_size , self.image_size)) # resizing before inserting to the network\n        mask = cv2.imread(mask_path , -1) # loads image with all channels \n        mask = cv2.resize(mask , (self.image_size , self.image_size))\n        mask = mask.reshape((self.image_size , self.image_size , 1))\n        # normalize image\n#         image = image \/ 255.0\n        mask = mask \/ 255.0\n        return image , mask\n    \n    def __getitem__(self , index):\n        # used to create the last \n        if (index + 1)*self.batch_size > len(self.ids):\n            self.batch_size = len(self.ids) - index * self.batch_size\n        file_batch = self.ids[index * self.batch_size : (index + 1) * self.batch_size]\n        images = []\n        masks = []\n        \n        # Function to normalize image\n        def normalize_image(image, mean, std):\n            for channel in range(3):\n                image[:,:,channel] = (image[:,:,channel] - mean[channel]) \/ std[channel]\n            return image\n        \n        # Function to add horizontal flips\n        def horizontal_flip(image,mask):\n            r = random.randint(0,1)\n            image = image\n            mask = mask\n            if (r==1):\n                image = np.fliplr(image)\n                mask = np.fliplr(mask)\n            return image,mask\n        \n        # We can add more augmentations for example\n            \n\n        for id_name in file_batch : \n            # protected variables \n            _img , _mask = self.__load__(id_name)\n            # normalize the image using weights per ImageNet precalculated weights\n            _img = normalize_image(np.array(_img) \/ 255.0, \n                                      mean=[0.485, 0.456, 0.406], \n                                      std=[0.229, 0.224, 0.225])\n            # randomly horizontal flip image \n            _img,_mask = horizontal_flip(_img,_mask)\n\n            images.append(_img)\n            masks.append(_mask)\n        \n        images = np.array(images)\n        masks = np.array(masks)\n        return images , masks\n    \n    def on_epoch_end(self):\n        pass\n    def __len__(self):\n        return int(np.ceil(len(self.ids) \/ float(self.batch_size)))","e06a9a46":"# use image size 128 for better results \nimage_size = 128 \nepochs = 10\nbatch_size = 10","3d530450":"train_gen = DataGen(path_input = TRAIN_PATH , path_mask = MASK_TRAIN_DIR , batch_size = batch_size , image_size = image_size)\nval_gen = DataGen(path_input =  VAL_PATH, path_mask =  MASK_VAL_DIR, batch_size = batch_size , image_size = image_size)","75c1f551":"x, y = val_gen.__getitem__(12)","7c7f5128":"plt.imshow(x[0]) # just an example to show that the dataloader is working fine","2cf2be48":"plt.imshow(y[0])","02c216ce":"# def crop_img(tensor,target_tensor):\n#     target_size = target_tensor.shape[2]\n#     tensor_size = tensor.shape[2]\n#     delta = tensor_size-target_size\n#     delta = delta\/\/2 \n#     return tensor[:,delta:tensor_size-delta,delta:tensor_size-delta,:]","18db80b7":"# U-Net Modified Architecture\n\n\nimage_size = 128 # keep image size in the pattern of 64,128,256,512,1024\nh = image_size\nw = image_size\nchannels = 3\n\n# input area\nmodel_in = Input(shape=(h, w, channels))\n\n\n# down convolutions\nconv1 = BatchNormalization(scale=True)(model_in)\nconv1 = Conv2D(64, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv1)\nconv1 = Conv2D(64, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv1)\ndown1 = MaxPooling2D((2, 2), strides=2)(conv1)\n\nconv2 = BatchNormalization(scale=True)(down1)\nconv2 = Conv2D(128, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv2)\nconv2 = Conv2D(128, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv2)\ndown2 = MaxPooling2D((2, 2), strides=2)(conv2)\n\nconv3 = BatchNormalization(scale=True)(down2)\nconv3 = Conv2D(256, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv3)\nconv3 = Conv2D(256, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv3)\ndown3 = MaxPooling2D((2, 2), strides=2)(conv3)\n\nconv4 = BatchNormalization(scale=True)(down3)\nconv4 = Conv2D(512, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv4)\nconv4 = Conv2D(512, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv4)\ndown4 = MaxPooling2D((2, 2), strides=2)(conv4)\n\n# bottleneck region \nconv5 = BatchNormalization(scale=True)(down4)\nconv5 = Conv2D(1024, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv5)\nconv5 = Conv2D(1024, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv5)\n\n\n\n# up convolutions\nconv6 = BatchNormalization(scale=True)(conv5)\nconv6 = Conv2DTranspose(512, (2, 2), activation='relu',strides=(2,2),kernel_initializer=\"he_normal\", padding=\"same\")(conv6)\nconcat1 = concatenate([conv4, conv6], axis=3)\nconv6 = Conv2D(512, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(concat1)\nconv6 = Conv2D(512, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv6)\n\nconv7 = BatchNormalization(scale=True)(conv6)\nconv7 = Conv2DTranspose(256, (2, 2), activation='relu',strides=2,kernel_initializer=\"he_normal\", padding=\"same\")(conv7)\nconcat2 = concatenate([conv3, conv7], axis=3)\nconv7 = Conv2D(256, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(concat2)\nconv7 = Conv2D(256, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv7)\n\nconv8 = BatchNormalization(scale=True)(conv7)\nconv8 = Conv2DTranspose(128, (2, 2), activation='relu',strides=2,kernel_initializer=\"he_normal\", padding=\"same\")(conv8)\nconcat3 = concatenate([conv2, conv8], axis=3)\nconv8 = Conv2D(128, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(concat3)\nconv8 = Conv2D(128, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv8)\n\nconv9 = BatchNormalization(scale=True)(conv8)\nconv9 = Conv2DTranspose(128, (2, 2), activation='relu',strides=2,kernel_initializer=\"he_normal\", padding=\"same\")(conv9)\nconcat4 = concatenate([conv1, conv9], axis=3)\nconv9 = Conv2D(64, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(concat4)\nconv9 = Conv2D(64, (3, 3), activation='relu',kernel_initializer=\"he_normal\", padding=\"same\")(conv9)\n\n# final 1x1 convolution \nconv10 = Conv2D(1,kernel_size=(1,1),activation=\"sigmoid\",kernel_initializer=\"he_normal\", padding=\"same\")(conv9)\n\nmodel = Model(model_in, conv10)","d37a5caf":"def iou_coeff(y_true, y_pred):\n    intersection = 0 \n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection =  (2.*(y_true_f * y_pred_f) + tf.keras.backend.epsilon())\n    union = (y_true_f + y_pred_f + tf.keras.backend.epsilon())\n    return (intersection\/union)","5cc67de4":"model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou_coeff])\n# model.compile(optimizer = SGD(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',MeanIoU(num_classes=2)])","e83fefdc":"model.summary()","cc4db6d8":"train_steps =  len(os.listdir(MASK_TRAIN_DIR))\/batch_size\nlogdir = WORKING_DIR+\"\/logs\/scalars\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir) \nhistory = model.fit_generator(train_gen , validation_data = val_gen , steps_per_epoch = train_steps , epochs=epochs, callbacks=[tensorboard_callback])","476a6e44":"print(history.history.keys())","a22aa0dc":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for IOU\nplt.plot(history.history['iou_coeff'])\nplt.plot(history.history['val_iou_coeff'])\nplt.title('model iou')\nplt.ylabel('iou')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","71bb3cc7":"!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip \n! yes |unzip ngrok-stable-linux-amd64.zip\n\n# LOG_DIR = 'tb_folder\/log\/' \nget_ipython().system_raw(\n    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n    .format(logdir)\n)\nget_ipython().system_raw('.\/ngrok http 6006 &')\n! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","f1d8e608":"x, y = val_gen.__getitem__(1)\nprint(x.shape)\nresult = model.predict(x)\nprint(result.shape)\nprint(y[0].shape)\n\nresult = result > 0.5\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")","45a5842c":"# Dataset Processing","ebfd7fe4":"## Modificiations from the original paper are \n- Image size changed from 572 to 128, for better performance.\n- Same Padding is added, so that the we don't have to crop and concat images.\n- Without same padding, a custom loss function has to be made which would include cropping.\n- Weights are initialized using 'he_normal' to avoid vanishing and gradint explosion problem as existing in the original architecture.\n- Provided batch_normalization while the original architecture did not have it.","e05bdf5f":"# Install libs","d2d1d1dc":"# Data Loader","66d4520e":"# Define U-Net and Train the model","a6b5a98f":"# Import Libraries"}}