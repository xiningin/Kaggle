{"cell_type":{"f0d8ef46":"code","72a8b635":"code","c1dd53a8":"code","f40fde03":"code","6efc5b62":"code","177474e2":"code","29d53833":"code","2188b4db":"code","725fd332":"code","8187220b":"code","748cddf6":"code","ef71a514":"code","a9d28e0b":"code","021c663a":"code","fab5df5f":"code","fd0ebfe2":"code","7c5000e4":"code","b2b9744f":"markdown","e7bdd55b":"markdown","49e665a4":"markdown","1c6def38":"markdown"},"source":{"f0d8ef46":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72a8b635":"#Ce projet porte sur le machine learning afin d'estimer le prix d'une maison en fonction de ses caract\u00e9ristiques\n#Beaucoup de caract\u00e9ristiques sont pr\u00e9sentes dans la base de donn\u00e9es. Il faut donc dans un premier temps garder seulement\n#les donn\u00e9es int\u00e9ressantes.\n\n\n# On importe les diff\u00e9rentes librairies utiles \u00e0 ce projet\n\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.metrics import r2_score, mean_squared_error, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')","c1dd53a8":"# On charge toutes les donn\u00e9es du projet \n\ntrain_data_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_data_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\ntrain_data = pd.read_csv(train_data_path, index_col='Id') #donn\u00e9es d'entrainement\ntest_data = pd.read_csv(test_data_path, index_col='Id') #donn\u00e9es de test","f40fde03":"#On retire les colonnes o\u00f9 il manque plus de 50% des valeurs\n\ncolumn_with_nan = train_data.columns[train_data.isnull().any()]\ntrain_data_shape = train_data.shape\n\nfor column in column_with_nan:\n    if train_data[column].isnull().sum()*100.0\/train_data_shape[0] > 50:\n        train_data.drop(column,1, inplace=True)","6efc5b62":"#On fait corr\u00e9ler les diff\u00e9rents crit\u00e8res pour voir l'importance de ceux-ci et on trace la matrice de corr\u00e9lation \n\ncorrmat = train_data.corr()\nplt.subplots(figsize=(16,8))\nsns.heatmap(corrmat,linewidths=.5,annot=True)","177474e2":"#On supprime les diff\u00e9rents crit\u00e8res sans utilit\u00e9\n\ndel train_data['OpenPorchSF']\ndel train_data['2ndFlrSF']\ndel train_data['EnclosedPorch']\ndel train_data['3SsnPorch']\ndel train_data['ScreenPorch']\ndel train_data['PoolArea']\ndel train_data['YrSold']\ndel train_data['MoSold']\ndel train_data['MiscVal']\n\ndel test_data['OpenPorchSF']\ndel test_data['2ndFlrSF']\ndel test_data['EnclosedPorch']\ndel test_data['3SsnPorch']\ndel test_data['ScreenPorch']\ndel test_data['PoolArea']\ndel test_data['YrSold']\ndel test_data['MoSold']\ndel test_data['MiscVal']","29d53833":"#On fait corr\u00e9ler les diff\u00e9rents crit\u00e8res pour voir l'importance de ceux-ci et on trace la matrice de corr\u00e9lation \n\ncorrmat = train_data.corr()\nplt.subplots(figsize=(16,8))\nsns.heatmap(corrmat,linewidths=.5,annot=True)","2188b4db":"#On trace les diff\u00e9rentes distributions en prenant comme indicateur principale le prix \n\nCorr_Column = list(train_data.corr()[\"SalePrice\"][(train_data.corr()[\"SalePrice\"]>0.50) | (train_data.corr()[\"SalePrice\"]<-0.50)].index)\nprint(Corr_Column)\nsns.pairplot(train_data[Corr_Column], hue='SalePrice', palette=\"rocket\")","725fd332":"#On trace la courbe de distribution du prix de vente des maisons\n\nsns.distplot(train_data['SalePrice'], color='r')\nplt.show()","8187220b":"#Dans cette partie nous cherchons \u00e0 ce que nos donn\u00e9es aient toutes le m\u00eame format\n\nfull_data_set = pd.concat([train_data, test_data])\n\n#On cr\u00e9e un format nombre et un format objet\ncolonnes_numeriques = full_data_set.select_dtypes('number').columns\ncolonnes_criteres = full_data_set.select_dtypes('object').columns\n\n#On remplie les cases num\u00e9riques manquantes par la moyenne de la colonne\nfull_data_set[colonnes_numeriques] = full_data_set[colonnes_numeriques].fillna(full_data_set[colonnes_numeriques].mean())\n#On remplie les cases cat\u00e9goris\u00e9es par None\nfull_data_set [colonnes_criteres] = full_data_set[colonnes_criteres].fillna(np.nan).replace([np.nan], ['None'])\n\n#On convertit toutes les donn\u00e9es sous le m\u00eame format\nfull_data_set = pd.get_dummies(full_data_set)\n\ntrain_data = full_data_set.copy()[:len(train_data)]\ntest_data = full_data_set.copy()[len(train_data):]\ntest_data.drop(['SalePrice'], axis=1, inplace=True)","748cddf6":"#On s\u00e9pare maintenant les diff\u00e9rentes donn\u00e9es en deux groupes\n#Comme vu pendant les cours, les donn\u00e9es de test repr\u00e9sente 20% des donn\u00e9es totales \nX = train_data.drop('SalePrice', axis=1)\ny = train_data.dropna(subset=['SalePrice'], axis=0).SalePrice\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)","ef71a514":"linear_regressor = LinearRegression()\nlinear_regressor.fit(X_train, y_train)\ny_pred = linear_regressor.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","a9d28e0b":"svr = LinearSVR(random_state=0)\nsvr.fit(X_train, y_train)\ny_pred = svr.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","021c663a":"dtr = DecisionTreeRegressor(random_state=0, max_depth=5)\ndtr.fit(X_train, y_train)\ny_pred = dtr.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","fab5df5f":"knn = KNeighborsRegressor()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","fd0ebfe2":"rfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\ny_pred = rfr.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","7c5000e4":"xgb = XGBRegressor()\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_valid)\nscore = r2_score(y_valid, y_pred)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\ndisplay(score, rmse)","b2b9744f":"**Traitement des donn\u00e9es - Data processing**","e7bdd55b":"Gr\u00e2ce aux diff\u00e9rents tests on obtient des r\u00e9sultats assez vari\u00e9s. La m\u00e9thode de la r\u00e9gression lin\u00e9aire et SVR n'ont pas des bons r\u00e9sultats. Cela peut venir de la quantit\u00e9 d'informations (pas forc\u00e9ment utiles) qui cat\u00e9gorisent les maisons. Avec moins de crit\u00e8res on aurait certainement des meilleurs r\u00e9sultats. Ensuite la m\u00e9thode des KNN et DTR obtiennent des r\u00e9sultats assez \"correctes\". Finalement les m\u00e9thodes XGB et RGR nous permettent d'avoir des estimations sur les prix de maisons avec plus de 80% de chance d'\u00eatre conforme \u00e0 la r\u00e9alit\u00e9.","49e665a4":"**Exploitation - Tests**","1c6def38":"On remarque qu'il y a plusieurs caract\u00e9ristique qui n'ont aucun int\u00e9r\u00eat\nOn supprime donc ces caract\u00e9risques \"secondaires\" afin de ne garder que les informations importantes pour l'estimation d'une maison\n"}}