{"cell_type":{"280805c5":"code","6fb4c0fd":"code","9a11a332":"code","8f09a444":"code","4acdf6a4":"code","c464ed9d":"code","b2ca2b48":"code","3d2a2fb4":"code","b1f8e182":"code","2a67a88c":"code","394fe1cc":"code","1169ed7d":"code","037e83b3":"code","7c4561c4":"code","c8ccd6ec":"code","946479c8":"code","20cb5462":"code","6c8846df":"markdown","8725419a":"markdown","77eab216":"markdown","7225b1cf":"markdown","b2f89125":"markdown","e72499c9":"markdown","1f21d939":"markdown","2428afcd":"markdown","082e54b5":"markdown","2a642269":"markdown","e16d4f48":"markdown","c19e2faf":"markdown"},"source":{"280805c5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras","6fb4c0fd":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","9a11a332":"train.head()","8f09a444":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/train\/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/test\/\" + identifier + \".jpg\")","4acdf6a4":"train.head()","c464ed9d":"train[\"Pawpularity\"].hist()","b2ca2b48":"tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\nimage_size = 260\nbatch_size = 128\nepochs = 10\nAUTO = tf.data.experimental.AUTOTUNE\noutput_dataset_path = \"\/kaggle\/input\/pawpularity-with-efficientnetb2-finetuning-output\/\"\nmodes = [\"training\", \"inference\"]\nmode = modes[1]\nis_trainging_mode = mode == modes[0]\nfolds = [0, 1, 2, 3, 4]","3d2a2fb4":"def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n    h = tf.shape(img)[0]\n    w = tf.shape(img)[1]\n    c = tf.shape(img)[2]\n    origin_area = tf.cast(h*w, tf.float32)\n\n    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh \/ rl)), tf.int32)\n\n    e_height_h = tf.minimum(e_size_h, h)\n    e_width_h = tf.minimum(e_size_h, w)\n\n    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n    erase_area = tf.cast(erase_area, tf.uint8)\n\n    pad_h = h - erase_height\n    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n    pad_bottom = pad_h - pad_top\n\n    pad_w = w - erase_width\n    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n    pad_right = pad_w - pad_left\n\n    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    erase_mask = tf.squeeze(erase_mask, axis=0)\n    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))","b1f8e182":"def data_augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = random_erasing(image)\n    return image","2a67a88c":"def preprocess_training(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = data_augment(image)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)\n\ndef preprocess_validation(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)","394fe1cc":"def base_model():\n    efficient_net = tf.keras.applications.EfficientNetB2(\n        include_top = False, \n        input_shape = (image_size, image_size, 3)\n    )    \n    efficient_net.trainable = False\n    return efficient_net","1169ed7d":"def get_tabular_model(inputs):\n    width = 32\n    depth = 3\n    activation = \"relu\"\n    kernel_regularizer = keras.regularizers.l2()\n    x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(inputs)\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.Concatenate()([x, inputs])\n    return x","037e83b3":"def get_model():\n    image_inputs = tf.keras.Input((image_size, image_size , 3))\n    tabular_inputs = tf.keras.Input(len(tabular_columns))\n    efficient_net = base_model()\n    image_x = efficient_net(image_inputs)\n    image_x = tf.keras.layers.GlobalAveragePooling2D()(image_x)\n    image_x = tf.keras.layers.Dropout(0.5)(image_x)\n    tabular_x = get_tabular_model(tabular_inputs)\n    x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[image_inputs, tabular_inputs], outputs=[output])\n    return model","7c4561c4":"tf.keras.backend.clear_session()\nmodels = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if index not in folds: \n        continue\n    if not is_trainging_mode:\n        break\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-5\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    \n    optimizer = tf.keras.optimizers.Adam(3e-4)\n    \n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess_training).shuffle(512).batch(batch_size).cache().prefetch(AUTO)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess_validation).batch(batch_size).cache().prefetch(AUTO)\n    model = keras.models.load_model(output_dataset_path +\"model_%d.h5\"%(index))\n    efficientnetb2 = model.get_layer(\"efficientnetb2\")\n    efficientnetb2.trainable = True\n    for i, layer in enumerate(efficientnetb2.layers):\n        if i < len(efficientnetb2.layers) - 20:\n            layer.trainable = False\n        else:\n            if type(layer) != keras.layers.BatchNormalization:\n                layer.trainable = True\n            else:\n                layer.trainable = False\n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\")\n    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\", \"mape\", rmse])\n    history = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    model.load_weights(checkpoint_path)\n    models.append(model)","c8ccd6ec":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular), 0","946479c8":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[tabular_columns])).map(preprocess_test_data).batch(batch_size).cache().prefetch(AUTO)","20cb5462":"total_results = []\nif is_trainging_mode:\n    for model in models:\n        total_results.append(model.predict(test_ds).reshape(-1))\nelse:\n    for i in folds:\n        model = keras.models.load_model(output_dataset_path +\"model_%d.h5\"%(i))\n        total_results.append(model.predict(test_ds).reshape(-1))\nresults = np.mean(total_results, axis=0).reshape(-1)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","6c8846df":"## Import datasets","8725419a":"I wrote a notebook to find optimal Tabualr Prediciton Model [here](https:\/\/www.kaggle.com\/lonnieqin\/pawpularity-model-with-dnn-on-meta-data?scriptVersionId=79014727).","77eab216":"## The Base Model (Efficient Net)","7225b1cf":"## Submission","b2f89125":"### Random Blockout Augmentation","e72499c9":"## Model Development","1f21d939":"# Pawpularity with EfficientNetB2 FineTuning\n\n## Table of Contents\n- Summary\n- Set up\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n\n\n## Summary\nIn this Notebook, I will:\n* Use EfficientNetB2 as Image Model to fit on Image Data.\n* Use DNN as Tabular Model to fit on Tabular Data.\n* Combine the total result of Image Model and Tabular Model.\n* Use Data Augmentation and Regularization method to prevent overfiting.\n* Use K-Fold training to improve final score.\n\n## Set up","2428afcd":"### Distribution of Pawpularities","082e54b5":"## Data Preprocessing","2a642269":"### Preprocess funciton\nI am doing data augmentation on training preprocess function, so that when make evaluation on validation dataset and prediction on test dataset, data augmentation won't be triggered.","e16d4f48":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","c19e2faf":"### The Tabular Prediction Model"}}