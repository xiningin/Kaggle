{"cell_type":{"68336e77":"code","0ad69981":"code","7106b02f":"code","4bfe4eac":"code","25b2c52b":"code","9f0353f0":"code","1a123707":"code","5ffd8915":"code","c18e1167":"code","90aee522":"code","e9d7d77b":"code","97b801fe":"code","bf86047a":"code","bd01930a":"code","a0622954":"code","6c084db3":"code","c828732b":"code","1c4dddcd":"code","2f9fc614":"code","45f08b29":"code","1130c2d6":"code","d23b4cc0":"code","65c6ddf5":"code","a4d11ebc":"code","c3bcd91c":"code","b6a2446d":"code","7cb26309":"code","869dd912":"code","443205f4":"markdown"},"source":{"68336e77":"# eventually install EfficientNets\n# or other needed packages\n!pip install -q efficientnet >> \/dev\/null","0ad69981":"# this Notebook is the second to work on my CXR TFREC Dataset\n# the dataset contains, in TFREC format, all images taken from NIH CXR dataset\n# image have been transformed from initial format (png, 1024x1024)\n# to JPEG 256x256 and packed in TFREC files\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport os\nimport re, math\nimport time","7106b02f":"# to remove some warnings\n\n# TF2 way to reduce logging\n# this remove also INFO, verify if needed\nimport logging\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)","4bfe4eac":"DEVICE = \"TPU\" # or \"TPU\"\n\n# TFRecords file for training\/validation\nGCS_PATH = KaggleDatasets().get_gcs_path('cxr-tfrec256-may2020')\n\nIMG_SIZES = 256\n\nIMAGE_SIZE = [IMG_SIZES, IMG_SIZES]\n\n# tune it, dependes on Image, size, TPU or GPU\n#BATCH_SIZE = 64\nBATCH_SIZE = 32\n\nEPOCHS = 50\n\n# for K-fold CV\nFOLDS = 5\n\n# WHICH EFFICIENTNET TO USE (B?, B0 from B7)\nEFF_NETS = 0","25b2c52b":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE == \"GPU\":\n    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n    print(\"Num GPUs Available: \", n_gpu)\n    \n    if n_gpu > 1:\n        print(\"Using strategy for multiple GPU\")\n        strategy = tf.distribute.MirroredStrategy()\n    else:\n        print('Standard strategy for GPU...')\n        strategy = tf.distribute.get_strategy()\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","9f0353f0":"# changed\nROT_ = 20.0\nSHR_ = 2.0\n# changed\nHZOOM_ = 4.0\nWZOOM_ = 4.0\n# changed\nHSHIFT_ = 4.0\nWSHIFT_ = 4.0\n\n# image augmentation\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\ndef transform(image, DIM=256):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM\/\/2, -DIM\/\/2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM\/\/2, DIM\/\/2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])","1a123707":"# not using metadata (only image, for now)\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n        'label' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    \n    # do image augmentation\n    image = transform(image, DIM=IMG_SIZES)\n    # hearth on the right is not labeled as anomaly, therefore it is ok to flip\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.7, 1.3)\n    image = tf.image.random_contrast(image, 0.8, 1.2)\n    image = tf.image.random_brightness(image, 0.1)\n        \n    image_idx = example['image_idx']\n    label = example['label']\n        \n    return image, label \n\ndef read_labeled_tfrecord_for_test(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n        'label' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    \n    # DON't do image augmentation\n    label = example['label']\n        \n    return image, label \n\ndef read_unlabeled_tfrecord(example, return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_idx'                   : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['image_idx'] if return_image_name else 0\n\ndef decode_image(image_data):\n    # qua dovrebbe fare la conversione in RGB come richiesto da EfficientNet\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    \n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\n# count # of images in files.. (embedded in file name)\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","5ffd8915":"def load_dataset(filenames, labeled=True, ordered=False, isTest=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    \n    if isTest == False:\n        dataset = dataset.map(read_labeled_tfrecord)\n    else:\n        dataset = dataset.map(read_labeled_tfrecord_for_test)\n    \n    # returns a dataset of (image, labels) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef get_training_dataset(filenames):\n    dataset = load_dataset(filenames, labeled=True, isTest = False)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_valid_dataset(filenames):\n    dataset = load_dataset(filenames, labeled=True, isTest = True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(filenames):\n    # for predictions it is crucial the ordering\n    dataset = load_dataset(filenames, labeled=True, isTest = True, ordered=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","c18e1167":"# here we define the DNN Model\n\nEFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n\n# as default it used B0\n\ndef build_model(dim = IMG_SIZES, ef = 0):\n    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n    \n    base = EFNS[ef](input_shape=(*IMAGE_SIZE, 3), weights='imagenet', include_top = False)\n    \n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs = inp,outputs = x)\n    \n    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n    \n    # removed label smoothing\n    fn_loss = tf.keras.losses.BinaryCrossentropy() \n    \n    # loss = [focal_loss]\n    model.compile(optimizer = opt, loss = [fn_loss], metrics=['AUC', 'accuracy'])\n    \n    return model","90aee522":"try_model = build_model(dim=IMG_SIZES, ef=EFF_NETS)\n\ntry_model.summary()","e9d7d77b":"# definisce la variazione temporale del learning rate\n\ndef get_lr_callback(batch_size=8):\n    lr_start   = 0.00001\n    lr_max     = 0.000015 * 10\n    # lr_max     = 0.00001 * 20\n    lr_min     = 0.00001\n    lr_ramp_ep = 20\n    # lr_ramp_ep = 20\n    lr_sus_ep  = 10\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    \n    return lr_callback","97b801fe":"class LRFinder(tf.keras.callbacks.Callback):\n    \n    '''\n    A simple callback for finding the optimal learning rate range for your model + dataset. \n    \n    # Usage\n        ```python\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(epoch_size\/batch_size), \n                                 epochs=3)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size\/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        \n    # References\n        Blog post: jeremyjordan.me\/nn-learning-rate\n        Original paper: https:\/\/arxiv.org\/abs\/1506.01186\n\n    '''\n    \n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        x = self.iteration \/ self.total_iterations \n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        K.set_value(self.model.optimizer.lr, self.clr())\n \n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('Learning rate')\n        plt.grid()\n        plt.show()\n        \n        \n    def plot_loss(self):\n        '''Helper function to quickly observe the learning rate experiment results.'''\n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')\n        plt.grid()\n        plt.show()","bf86047a":"all_files = tf.io.gfile.glob(GCS_PATH + '\/train_valid*.tfrec')\nnum_total_files = len(all_files)\n\nn_images = count_data_items(all_files)\n\nprint('Total number of image for train-validation:', n_images)","bd01930a":"def train_one_fold(fold, files_train, files_valid):\n    # train_files = list of train_files names\n    # valid_files = list of valid files\n    \n    # constant to customize output\n    VERBOSE = 1\n    tStart = time.time()\n    \n    # BUILD MODEL\n    if DEVICE=='TPU':\n        # to avoid OOM\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    K.clear_session()\n    with strategy.scope():\n        print('Building model...')\n        model = build_model(dim=IMG_SIZES, ef=EFF_NETS)\n    \n    # callback to save best model for each fold\n    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=1, save_best_only=True,\n                                            save_weights_only=True, mode='min', save_freq='epoch')\n    \n    # TRAIN\n    history = model.fit(get_training_dataset(files_train), \n                        epochs=EPOCHS, \n                        callbacks = [sv, get_lr_callback(BATCH_SIZE)], \n                        steps_per_epoch = count_data_items(files_train)\/BATCH_SIZE\/\/REPLICAS,\n                        validation_data = get_valid_dataset(files_valid), \n                        validation_steps = count_data_items(files_valid)\/BATCH_SIZE\/\/REPLICAS,\n                        verbose=VERBOSE)\n    \n    tElapsed = round(time.time() - tStart, 1)\n    \n    print(' ')\n    print('Time (sec) elapsed: ', tElapsed)\n    print('...')\n    print('...')\n    \n    return history","a0622954":"# code to manage K-fold CV\nSHOW_FILES = True\n\nskf = KFold(n_splits = FOLDS, shuffle = True, random_state=42)\n\noof_pred = []; oof_tar = []; oof_val = []; oof_names = [] \n# preds = np.zeros((count_data_items(files_test),1))\n\n# for others investigations\n# we store all the history\nhistories = []\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(num_total_files))):\n    # display fold info\n    print('')\n    print('#'*60) \n    print('#### FOLD', fold+1)\n    \n    print('#### Image Size %i, EfficientNet B%i, batch_size %i'%\n          (IMG_SIZES, EFF_NETS, BATCH_SIZE*REPLICAS))\n    print('#### Epochs: %i' %(EPOCHS))\n    print('#'*60)\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    train_files = tf.io.gfile.glob([GCS_PATH + '\/train_valid%.2i*.tfrec'%x for x in idxT])\n    valid_files = tf.io.gfile.glob([GCS_PATH + '\/train_valid%.2i*.tfrec'%x for x in idxV])\n    \n    if SHOW_FILES:\n        print('Number of training images', count_data_items(train_files))\n        print('Number of validation images', count_data_items(valid_files))\n    \n    # in files h5 fol are numbered starting from 1\n    \n    # here launch the training for the fold\n    history = train_one_fold(fold+1, train_files, valid_files)\n    \n    histories.append(history)","6c084db3":"# lr_finder.plot_loss()","c828732b":"# lr_finder.plot_lr()","1c4dddcd":"def plot_auc(hist):\n    plt.figure(figsize=(14,6))\n    \n    plt.plot(hist.history['auc'], label='Training auc')\n    plt.plot(hist.history['val_auc'], label='Validation auc')\n    plt.title('AUC')\n    plt.legend(loc='lower right')\n    plt.ylabel('AUC')\n    plt.xlabel('epoch')\n    plt.grid()\n    plt.show();","2f9fc614":"for fold in range(0, FOLDS):\n    plot_auc(histories[fold])","45f08b29":"def plot_loss(hist):\n    plt.figure(figsize=(14,6))\n    \n    plt.plot(hist.history['loss'], label='Training loss')\n    plt.plot(hist.history['val_loss'], label='Validation loss')\n    plt.title('Loss')\n    plt.legend(loc='lower right')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.grid()\n    plt.show();","1130c2d6":"for fold in range(0, FOLDS):\n    plot_loss(histories[fold])","d23b4cc0":"# calcolo su test set. Su test set no augmentation\n\nfiles_test = tf.io.gfile.glob(GCS_PATH + '\/test*.tfrec')\n\nnum_total_test_files = len(files_test)\n\nlist_auc = []\nlist_acc = []\n\nfor fold in range(1, FOLDS + 1):\n    \n    model = build_model(dim=IMG_SIZES, ef=EFF_NETS)\n    print('Evaluating on fold:', fold)\n    \n    model.load_weights('fold-%i.h5'%fold)\n    \n    test_loss, test_auc, test_acc = model.evaluate(get_test_dataset(files_test), verbose = 1, batch_size = BATCH_SIZE,\n                                        steps = count_data_items(files_test)\/BATCH_SIZE)\n\n    print('Test AUC: ', round(test_auc, 3))\n    print('Test ACC: ', round(test_acc, 3))\n    print('')\n    \n    list_auc.append(test_auc)\n    list_acc.append(test_acc)\n\n# compute avg, std\nnp_auc = np.array(list_auc)\nnp_acc = np.array(list_acc)\n\navg_auc = np.mean(np_auc)\nstd_auc = np.std(np_auc)\navg_acc = np.mean(np_acc)\nstd_acc = np.std(np_acc)\n\nprint('AUC: %.3f , std: %.3f' %(avg_auc, std_auc))\nprint('ACC: %.3f , std: %.3f' %(avg_acc, std_acc))","65c6ddf5":"# all the label from the test dataset\ny_true = np.concatenate([y for x, y in get_test_dataset(files_test)], axis=0)","a4d11ebc":"avg_preds = np.zeros((count_data_items(files_test), 1))\n\nfor fold in range(1, FOLDS + 1):\n    \n    model = build_model(dim=IMG_SIZES, ef=EFF_NETS)\n    print('Predicting  on fold:', fold)\n    \n    model.load_weights('fold-%i.h5'%fold)\n    preds = model.predict(get_test_dataset(files_test), verbose = True,\n                          steps = count_data_items(files_test)\/BATCH_SIZE)\n    \n    avg_preds += preds * 1.\/FOLDS","c3bcd91c":"# convert probabilities in labels\nTHRESHOLD = 0.50\n\ny_pred = np.concatenate([y for y in avg_preds], axis=0)\ny_pred = y_pred > THRESHOLD\ny_pred = y_pred.astype(int)\n\nacc = accuracy_score(y_true, y_pred)\n\nprint('The accuracy on TEST set is:', round(acc, 3))","b6a2446d":"tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\nprint('The Confusion Matrix')\nprint('TP:', tp)\nprint('TN:', tn)\nprint('FP:', fp)\nprint('FN:', fn)","7cb26309":"SENS = tp\/float(fn + tp)\nSPEC = tn\/float(tn + fp)\n\nprint('SENS:', round(SENS, 3))\nprint('SPEC:', round(SPEC, 3))","869dd912":"# check accuracy, FP, FN for different thresholds\n\nTHRESHOLDS = [0.5, 0.55, 0.6, 0.65, 0.7]\n\nlist_acc = []\n\nfor THRESHOLD in THRESHOLDS:\n    y_pred = np.concatenate([y for y in avg_preds], axis=0)\n    y_pred = y_pred > THRESHOLD\n    y_pred = y_pred.astype(int)\n    acc = accuracy_score(y_true, y_pred)\n\n    print('THR: %f: accuracy on TEST set is: %.3f' %(THRESHOLD, round(acc, 3)))\n    list_acc.append(acc)\n    \n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    print('FP:', fp)\n    print('FN:', fn)\n    SENS = tp\/float(fn + tp)\n    SPEC = tn\/float(tn + fp)\n\n    print('SENS:', round(SENS, 3))\n    print('SPEC:', round(SPEC, 3))","443205f4":"### For preparing Udacity exam 1.\nchanges according wp from NPJ\n\nNotebook 2 is the Notebook supporting K-fold CV"}}