{"cell_type":{"26900dc3":"code","e00d25c8":"code","6369a2b7":"code","b10ce27b":"code","21c0c78a":"code","3ca42431":"code","96514b6e":"code","891bdbbc":"code","4e88e165":"code","3609492d":"code","7019688d":"code","504846ab":"code","c0b6f2d7":"code","8ad75503":"code","124ff86b":"code","40bb70d0":"code","efc9615c":"code","76cc048e":"code","be440725":"code","2a83b03e":"code","670e43cb":"code","aef32146":"code","868c38f6":"code","0765a427":"code","5128fad7":"code","dc78bd5a":"code","16547197":"code","fb84c874":"code","33e0df61":"code","4af33d07":"code","3088c857":"code","e9f4e94c":"code","849b1f03":"code","c59f1aeb":"code","5b222934":"code","e7b35284":"markdown","3e78db18":"markdown","5f577090":"markdown","91c3f0aa":"markdown","761cfbb6":"markdown","9859f37b":"markdown","c7502a7c":"markdown","17907519":"markdown","4c9e4a5c":"markdown","3bc0779d":"markdown","fdf4c325":"markdown","b1a1a012":"markdown","22726065":"markdown","41e8222c":"markdown","eb26437a":"markdown","1c7fcbee":"markdown","7fdce069":"markdown"},"source":{"26900dc3":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n","e00d25c8":"def timer(func):\n    def wrapper(*args, **kws):\n        st = time.time()\n        res = func(*args, **kws)\n        et = time.time()\n        tt = (et-st)\/60\n        print(f'Time taken is {tt:.2f} mins')\n        return res\n    return wrapper\n","6369a2b7":"# validation data\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf_val.shape","b10ce27b":"# Test data\n\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf_sub.shape","21c0c78a":"n_folds = 7","3ca42431":"def clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    \n    return data","96514b6e":"@timer\ndef train_pipeline(pipeline, data_path_name, n_folds, clean_prm = False):\n    val_preds_arr1_tmp = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2_tmp = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr_tmp = np.zeros((df_sub.shape[0], n_folds))\n\n    for fld in range(n_folds):\n        print(\"\\n\\n\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'..\/input\/jigsaw-folded-csv-files\/{data_path_name}_fld{fld}.csv')\n        print(df.shape)\n\n        print(\"\\nTrain:\")\n        # Train the pipeline\n        pipeline.fit(df['text'], df['y'])\n\n        # What are the important features for toxicity\n\n        print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n        feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                      np.round(pipeline['clf'].coef_,2) )), \n                             key = lambda x:x[1], \n                             reverse=True)\n\n        display(pd.DataFrame(feature_wts[:50], columns = ['feat','val']).T)\n        #.plot('feat','val',kind='barh',figsize = (8,8) )\n        #plt.show()\n\n        if clean_prm:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(clean(df_val,'less_toxic')['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(clean(df_val,'more_toxic')['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(clean(df_sub,'text')['text'])\n        else:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(df_val['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(df_sub['text'])\n    return val_preds_arr1_tmp, val_preds_arr2_tmp, test_preds_arr_tmp","891bdbbc":"features = FeatureUnion([\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1, val_preds_arr2, test_preds_arr = train_pipeline(pipeline, \n                                                                \"df\", \n                                                                n_folds,\n                                                                clean_prm=False)\n","4e88e165":"features = FeatureUnion([\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1c, val_preds_arr2c, test_preds_arrc = train_pipeline(pipeline, \n                                                                   \"df_clean\", \n                                                                   n_folds,\n                                                                   clean_prm=True)\n","3609492d":"features = FeatureUnion([\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1_, val_preds_arr2_, test_preds_arr_ = train_pipeline(pipeline, \n                                                                   \"dfr\", \n                                                                   n_folds,\n                                                                   clean_prm=False)\n","7019688d":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n","504846ab":"print(\"Find right weight\")\n\nwts_acc = []\nfor i in range(30,70,1):\n    for j in range(0,20,1):\n        w1 = i\/100\n        w2 = (100 - i - j)\/100\n        w3 = (1 - w1 - w2 )\n        p1_wt = w1*p1 + w2*p3 + w3*p5\n        p2_wt = w1*p2 + w2*p4 + w3*p6\n        wts_acc.append( (w1,w2,w3, \n                         np.round((p1_wt < p2_wt).mean() * 100,2))\n                      )\nsorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]\nprint(wts_acc[0])","c0b6f2d7":"w1, w2, w3, _ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n\nprint(f'w1 : {w1}, w2: {w2}, w3: {w3}')\n\np1_wt = w1*p1 + w2*p3 + w3*p5\np2_wt = w1*p2 + w2*p4 + w3*p6","8ad75503":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n","124ff86b":"\n### Incorrect predictions with similar scores\n\ndf_val[(df_val.correct == 0) & (df_val.p1 < 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","40bb70d0":"df_val[(df_val.correct == 0) & (df_val.p1 > 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","efc9615c":"### Incorrect predictions with dis-similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","76cc048e":"df_val[(df_val.correct == 0) & (df_val['diff'] < 0.4*df_val['diff'].max())].sort_values('diff', ascending=False).head(20)\n","be440725":"# Predict using pipeline\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + \\\n                  w2*test_preds_arr_.mean(axis=1) + \\\n                  w3*test_preds_arrc.mean(axis=1)","2a83b03e":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","670e43cb":"same_score = df_sub['score'].value_counts().reset_index()[:10]\nsame_score","aef32146":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","868c38f6":"# Same comments have same score - which is ok ","0765a427":"# # Rank the predictions \n\n# df_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\n# print(df_sub['score'].rank().nunique())","5128fad7":"import os\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom tqdm import tqdm","dc78bd5a":"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nCONFIG = dict(\n        seed=42,\n        test_batch_size=64,\n        max_length=128,\n        num_classes=1,\n        device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    )","16547197":"def set_model_name(model_code):\n    \n    if model_code == 'roberta':\n        model_name = '..\/input\/roberta-base'\n        \n    elif model_code == 'hatebert':\n        model_name = '..\/input\/d\/withmaster\/hatebert\/GroNLP\/model'\n\n    CONFIG['model_name'] = model_name\n    CONFIG['tokenizer'] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ndef set_model_path_name(model_path_name):\n    MODEL_PATHS = [\n        f'..\/input\/{model_path_name}\/Loss-Fold-0.bin',\n        f'..\/input\/{model_path_name}\/Loss-Fold-1.bin',\n        f'..\/input\/{model_path_name}\/Loss-Fold-2.bin',\n        f'..\/input\/{model_path_name}\/Loss-Fold-3.bin',\n        f'..\/input\/{model_path_name}\/Loss-Fold-4.bin'\n    ]\n    \n    return MODEL_PATHS","fb84c874":"def set_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                    text,\n                    truncation=True,\n                    add_special_tokens=True,\n                    max_length=self.max_len,\n                    padding='max_length'\n                )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n            'ids' : torch.tensor(ids, dtype=torch.long),\n            'mask' : torch.tensor(mask, dtype=torch.long)\n        }\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids, attention_mask=mask,\n                        output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype=torch.long)\n        mask = data['mask'].to(device, dtype=torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f'Getting predictions for model {i+1}')\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n        \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","33e0df61":"df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ndf.head()","4af33d07":"set_model_name('roberta')\nMODEL_PATHS = set_model_path_name('..\/input\/pytorch-w-b-jigsaw-starter')\nset_seed(CONFIG['seed'])\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                        num_workers=2, shuffle=False, pin_memory=True)\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","3088c857":"set_model_name('roberta')\nMODEL_PATHS = set_model_path_name('..\/input\/jigsaw-starter-roberta-rowruddit-mrl')\nset_seed(CONFIG['seed'])\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                        num_workers=2, shuffle=False, pin_memory=True)\npreds2 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","e9f4e94c":"set_model_name('roberta')\nMODEL_PATHS = set_model_path_name('..\/input\/jigsaw-starter-roberta-1st-mrl')\n\nset_seed(CONFIG['seed'])\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                        num_workers=2, shuffle=False, pin_memory=True)\npreds3 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","849b1f03":"preds = ((preds1 - preds1.min()) \/ (preds1.max() - preds1.min()) + (preds2 - preds2.min()) \/ (preds2.max() - preds2.min()) \n        + (preds3 - preds3.min()) \/ (preds3.max() - preds3.min()))\n\npreds = preds\/3","c59f1aeb":"df_sub['score'] = df_sub['score']*0.85+preds*0.15","5b222934":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","e7b35284":"# Validate the pipeline ","3e78db18":"# Training function","5f577090":"# Imports","91c3f0aa":"## Optimize the model weights for ensemble","761cfbb6":"# Create Sklearn Pipeline with \n-  TFIDF - Take 'char_wb' as analyzer to capture subwords well\n-  Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","9859f37b":"# Toxic Training","c7502a7c":"## Ruddit data Training","17907519":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","4c9e4a5c":"# Toxic __clean__ Training","3bc0779d":"#### Some of these just look incorrectly tagged \n","fdf4c325":"## 0.84+ score by ensemble of simple TF-Idf and Ridge regression\n\n### Ensemble of TfIdf - Ridge models using data from \n- Toxic competition\n- Toxic CLEANED competition\n- Ruddit toxic data\n- Toxic multilingual competition\n- Toxic tweet data\n\n### Analysis of bad predictions\n","b1a1a012":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data","22726065":"## Correct the rank ordering","41e8222c":"# Predict on test data ","eb26437a":"## Load Test, Validation data  \n","1c7fcbee":"#### Some cool starters notebooks : \n- https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n- https:\/\/www.kaggle.com\/steubk\/jrsotc-ridgeregression-ensemble-of-3\n- https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-85\/notebook","7fdce069":"# Ensemble DLM"}}