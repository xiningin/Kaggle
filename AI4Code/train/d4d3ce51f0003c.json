{"cell_type":{"84b1f46a":"code","832d1d0d":"code","aca670ff":"code","87d35966":"code","13b46613":"code","ea689740":"code","a8c6315c":"code","f39eb6f7":"code","5b636507":"code","9d99c5a4":"code","29904599":"code","8af2d81f":"code","e6f77621":"code","77fc6028":"code","cdd4c1c5":"code","e44760f9":"code","8e36735c":"code","c0ff7f46":"code","2295d260":"code","6593846b":"code","067845f7":"code","031b3fbc":"code","04d05f13":"code","3c3f5aa5":"code","aebd913a":"code","f1008064":"code","3602ff45":"code","8679285b":"code","62ec2cf2":"code","139cdb9a":"code","c8c82e13":"code","70b56ba7":"code","67dbeee8":"code","edf93d11":"code","ff37388c":"code","d144fc28":"code","a9924546":"code","3571f6e0":"code","59eb3e3e":"code","ae4578d3":"code","c8f9d0f2":"code","326590e9":"markdown","c12eeb92":"markdown","7fa97012":"markdown","4a805625":"markdown","9ae26c02":"markdown","e1153ec6":"markdown","20f62fd9":"markdown","37250b47":"markdown","0f55b2e7":"markdown","3c403269":"markdown"},"source":{"84b1f46a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport folium\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\n\n#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')","832d1d0d":"df = pd.read_csv('\/kaggle\/input\/sf-parks\/SF_Park_Scores.csv')\ndf.head()","aca670ff":"df.info()","87d35966":"df.isnull().sum(axis = 0)","13b46613":"df = df.rename(columns={'Facility Type':'FacilityType','Square Feet':'SquareFeet','Perimeter Length':'PerimeterLength'})","ea689740":"df.tail()","a8c6315c":"park_data = df","f39eb6f7":"all_data_na = (park_data.isnull().sum() \/ len(park_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","5b636507":"#Data Cleaning Process\n# I will drop the Floor Count    \npark_data = park_data.drop([\"Floor Count\"], axis = 1)\npark_data = park_data.dropna()\n#I will put 0 for numeric data\npark_data[\"Latitude\"] = park_data[\"Latitude\"].fillna(0)\npark_data[\"Longitude\"] = park_data[\"Longitude\"].fillna(0)\npark_data[\"Acres\"] = park_data[\"Acres\"].fillna(0)\npark_data[\"Perimeter Length\"] = park_data[\"PerimeterLength\"].fillna(0)\npark_data[\"Square Feet\"] = park_data[\"SquareFeet\"].fillna(0)\npark_data[\"Zipcode\"] = park_data[\"Zipcode\"].fillna(0)\n#I will put None for strings\npark_data[\"State\"] = park_data[\"State\"].fillna(\"None\")\npark_data[\"Address\"] = park_data[\"Address\"].fillna(\"None\")\npark_data[\"Facility Name\"] = park_data[\"Facility Name\"].fillna(\"None\")\npark_data[\"FacilityType\"] = park_data[\"FacilityType\"].fillna(\"None\")","9d99c5a4":"#info about the dataset\npark_data.info()","29904599":"park_data.Zipcode.unique()","8af2d81f":"\n#the distrubution of parks in san francisco in terms of longtitude and latitude\nlongitude = list(park_data.Longitude) \nlatitude = list(park_data.Latitude)\nplt.figure(figsize = (10,10))\nplt.plot(longitude,latitude,'.', alpha = 0.4, markersize = 30)\nplt.show()","e6f77621":"from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\n\n#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')","77fc6028":"address = 'san francisco'\n\ngeolocator = Nominatim(user_agent=\"san francisco\")\nlocation = geolocator.geocode(address)\nlatitude_toronto = location.latitude\nlongitude_toronto = location.longitude\nprint('The geograpical coordinate of san francisco are {}, {}.'.format(latitude_toronto, longitude_toronto))","cdd4c1c5":"map_toronto = folium.Map(location=[latitude_toronto, longitude_toronto], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, Neighbourhood in zip(park_data['Latitude'], park_data['Longitude'], park_data['Park'], park_data['Score']):\n    label = '{}, {}'.format(Neighbourhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_toronto)  \n    \nmap_toronto","e44760f9":"plt.subplots(figsize=(22,12))\nsns.countplot(y=park_data['Zipcode'],order=park_data['Zipcode'].value_counts().index)\nplt.show()","8e36735c":"import matplotlib as mpl\nfrom wordcloud import WordCloud, STOPWORDS\n\nmpl.rcParams['font.size']=12                \nmpl.rcParams['savefig.dpi']=100             \nmpl.rcParams['figure.subplot.bottom']=.1 \nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(park_data['Park']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=1000)","c0ff7f46":"#SF Parks Facility Score points\npark_data['Score'].value_counts().sort_index().plot.line(figsize=(12, 6),color='mediumvioletred',fontsize=16,title='SF Parks Score')","2295d260":"# Which Public Administation has the highest parks \nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True) # this is important\n\nz = {'PSA1': 'PSA1', 'PSA2': 'PSA2', 'PSA3': 'PSA3','PSA4': 'PSA4','PSA5': 'PSA5','PSA6': 'PSA6','GGP': 'GGP'}\ndata = [go.Bar(\n            x = park_data.PSA.map(z).unique(),\n            y = park_data.PSA.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = park_data.PSA.value_counts().values\n                        ),\n           \n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","6593846b":"def getNearbyVenues(names, latitudes, longitudes, radius=500):\n    \n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https:\/\/api.foursquare.com\/v2\/venues\/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)","067845f7":"CLIENT_ID = 'MIF2SPKPEZP0YIUKZMPBLCA3R4ESWCFJDYPFTPHV4PTFVXIO' # your Foursquare ID\nCLIENT_SECRET = '5UOPEX5O43CRW0TZWYYOXST2VJBTPKIXAJWJ2TJVH3S23ZEK' # your Foursquare Secret\nVERSION = '20180604'\nLIMIT = 30\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)","031b3fbc":"manhattan_venues = getNearbyVenues(names=park_data['Park'].head(20),\n                                   latitudes=park_data['Latitude'],\n                                   longitudes=park_data['Longitude']\n                                  )","04d05f13":"print(manhattan_venues.shape)\nmanhattan_venues.head()","3c3f5aa5":"manhattan_venues.groupby('Neighborhood').count()","aebd913a":"print('There are {} uniques categories.'.format(len(manhattan_venues['Venue Category'].unique())))","f1008064":"# one hot encoding\nmanhattan_onehot = pd.get_dummies(manhattan_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\nmanhattan_onehot['Neighborhood'] = manhattan_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nfixed_columns = [manhattan_onehot.columns[-1]] + list(manhattan_onehot.columns[:-1])\nmanhattan_onehot = manhattan_onehot[fixed_columns]\n\nmanhattan_onehot.head()","3602ff45":"manhattan_onehot.shape","8679285b":"manhattan_grouped = manhattan_onehot.groupby('Neighborhood').mean().reset_index()\nmanhattan_grouped","62ec2cf2":"manhattan_grouped.shape","139cdb9a":"num_top_venues = 5\n\nfor hood in manhattan_grouped['Neighborhood']:\n    print(\"----\"+hood+\"----\")\n    temp = manhattan_grouped[manhattan_grouped['Neighborhood'] == hood].T.reset_index()\n    temp.columns = ['venue','freq']\n    temp = temp.iloc[1:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n    print('\\n')","c8c82e13":"def return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]","70b56ba7":"num_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n\n# create a new dataframe\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = manhattan_grouped['Neighborhood']\n\nfor ind in np.arange(manhattan_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(manhattan_grouped.iloc[ind, :], num_top_venues)\n\nneighborhoods_venues_sorted.head()","67dbeee8":"# set number of clusters\nkclusters = 5\n\nmanhattan_grouped_clustering = manhattan_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(manhattan_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10] ","edf93d11":"# add clustering labels\nneighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n#park_data.drop(['Cluster Labels'], axis=1)\nmanhattan_merged = park_data\n\n# merge toronto_grouped with toronto_data to add latitude\/longitude for each neighborhood\nmanhattan_merged = manhattan_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Park')\n\nmanhattan_merged.head() # check the last columns!","ff37388c":"address = 'san francisco'\n\ngeolocator = Nominatim(user_agent=\"san francisco\")\nlocation = geolocator.geocode(address)\nlatitude_toronto = location.latitude\nlongitude_toronto = location.longitude\nprint('The geograpical coordinate of san francisco are {}, {}.'.format(latitude_toronto, longitude_toronto))","d144fc28":"map_clusters = folium.Map(location=[latitude_toronto, longitude_toronto], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(manhattan_merged['Latitude'], manhattan_merged['Longitude'], manhattan_merged['Park'], manhattan_merged['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        #color=rainbow[kcluster-1],\n        fill=True,\n        #fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters","a9924546":"manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 0, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]","3571f6e0":"manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 1, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]","59eb3e3e":"manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 2, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]","ae4578d3":"manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 3, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]","c8f9d0f2":"manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 4, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]","326590e9":"## Examine Clusters","c12eeb92":"# San Francisco Data Analysis ","7fa97012":"#### Cluster 3","4a805625":"#### Cluster 1","9ae26c02":"## By the Observations and Analysis of Data. Following are the findings\n* Public Administation PSA4 has most parks\n* Park Play ground are the most used workds in it\n* Clusters are formed so that easily group can be classified","e1153ec6":"#### Cluster 2","20f62fd9":"### Based on Location Data. ","37250b47":"#### Cluster 5","0f55b2e7":"#### Cluster 4","3c403269":"## Cluster Neighborhoods"}}