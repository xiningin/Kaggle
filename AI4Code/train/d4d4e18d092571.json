{"cell_type":{"3c4957d7":"code","e1a02d30":"code","33e5a133":"code","e46aacd2":"code","2dc43dcf":"code","c19cf6c5":"code","6fcbcff7":"code","16aa3fc1":"code","a4101eb3":"code","a27592de":"code","708452f7":"code","0baec4be":"code","af0edfc1":"code","b83242f6":"code","7caabcfc":"code","f2251f5f":"code","171775a7":"code","176f3f50":"code","5b31af6d":"code","1318e4f1":"code","f845bc9f":"code","67eb159b":"code","89d8561e":"code","6f20c9d0":"code","c6efcade":"code","20ea80bb":"code","427dfcb5":"code","038b49f3":"code","4d4df821":"code","3e6bb198":"code","d083345b":"code","c6705793":"code","c770ba93":"code","08026114":"code","cc73f643":"code","3dd663fd":"code","40b38850":"code","16a90cef":"markdown","7646ac3d":"markdown","87ff5eba":"markdown","62be3c2f":"markdown","ef9406fb":"markdown"},"source":{"3c4957d7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","e1a02d30":"import cv2\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","33e5a133":"print(os.listdir(\"..\/input\/grocery-dataset\/grocerydataset\/\"))","e46aacd2":"shelf_images = \"..\/input\/grocery-dataset\/grocerydataset\/ShelfImages\/\"\nproduct_images = \"..\/input\/grocery-dataset\/grocerydataset\/ProductImagesFromShelves\/\"","2dc43dcf":"# lets get shelves photo data from shelf_images\n\njpg_files = [ f for f in os.listdir(f'{shelf_images}') if f.endswith('JPG') ]\nphotos_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in jpg_files], columns=['file', 'shelf_id', 'planogram_id'])","c19cf6c5":"print(len(jpg_files))\nprint(jpg_files[:5])\nphotos_df.head(5)","6fcbcff7":"# let's get products on shelves photo from ProductImagesFromShelves\n\nproducts_df = pd.DataFrame([[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))]\n                           for i in range(11)\n                           for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n                          columns = ['file', 'shelf_id', 'planogram_id', 'category', 'xmin', 'ymin', 'w', 'h'])\n\n# convert from width, height to xmax, ymax\n\nproducts_df['xmax'] = products_df['xmin'] + products_df['w']\nproducts_df['ymax'] = products_df['ymin'] + products_df['h']","16aa3fc1":"print(products_df.shape)\n(products_df.head(5))","a4101eb3":"# our data contains many photos of each shelf. In order not to full ourselves, \n# we need to split not by products nor planograms, but by shelves.\n\n# get distinct shelves\nshelves = list(set(photos_df['shelf_id'].values))\nprint(len(shelves))\nprint(shelves)","a27592de":"# train\/test split \nshelves_train, shelves_validation, _, _ = train_test_split(shelves, shelves, test_size=0.3, random_state=42)","708452f7":"# mark all records in dataframes with is_train flag\n\ndef is_train(shelf_id):\n    return shelf_id in shelves_train\n\nphotos_df['is_train'] = photos_df['shelf_id'].apply(is_train)\nproducts_df['is_train'] = products_df['shelf_id'].apply(is_train)","0baec4be":"photos_df.head(5)","af0edfc1":"products_df.head(5)","b83242f6":"df = products_df[products_df.category != 0].groupby(['category', 'is_train'])['category'].count().unstack('is_train').fillna(0)\n\ndf.plot(kind='barh', stacked=True)\nplt.show()","7caabcfc":"# fuction to display shelf photos with rectangle product\n\ndef draw_shelf_photo(file):\n    file_products_df = products_df[products_df.file == file]\n    coordinates = file_products_df[['xmin','ymin','xmax','ymax']].values\n    im = cv2.imread(f'{shelf_images}{file}')\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    for xmin,ymin,xmax,ymax in coordinates:\n        cv2.rectangle(im, (xmin,ymin), (xmax,ymax), (0,255,0), 5)\n    plt.imshow(im)","f2251f5f":"# draw one photo to check our data\n\nfig = plt.gcf()\nfig.set_size_inches(12, 6)\ndraw_shelf_photo('C1_P03_N1_S2_1.JPG')","171775a7":"from IPython.display import Image\nImage('..\/input\/grocery-dataset-extra-images\/brands.png', width=500)","176f3f50":"# They proposed approach to brands recognition as a combination of following algorithms.\nImage('..\/input\/grocery-dataset-extra-images\/brand_recognition.png', width=500)","5b31af6d":"# this approach accuracy is below\nImage('..\/input\/grocery-dataset-extra-images\/brand_recognition_accuracy.png', width=500)","1318e4f1":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport itertools\n\nfrom sklearn.metrics import confusion_matrix\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten, Input, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import BatchNormalization\n\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import backend as K\n\n%matplotlib inline","f845bc9f":"# we already have photos_df and products_df ready\n\n# neural networks work with input of fixed size, so we need to resize our\n# packs images to the chosen size. The size is some kind of metaparameter and \n# you should try different variants. Logically, the bigger size you select,\n# the better performace you'll have. Unfortunatelly it is not true, because \n# of over fitting. The more parameters your neural network have, the easier it\n# became over fitted\n\nnum_classes = 10\nSHAPE_WIDTH = 80\nSHAPE_HEIGHT = 120","67eb159b":"# resize pack to fixed size SHAPE_WIDTH*SHAPE_HEIGHT\n\ndef resize_pack(pack):\n    fx_ratio = SHAPE_WIDTH \/ pack.shape[1]\n    fy_ratio = SHAPE_HEIGHT \/ pack.shape[0]\n    pack = cv2.resize(pack, (0,0), fx=fx_ratio, fy=fy_ratio)\n    return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]","89d8561e":"print(photos_df.columns)\nprint(products_df.columns)","6f20c9d0":"# x - image\n# y - class\n# f - is_train flag\nx, y, f = [], [], []\n\nfor file, is_train in photos_df[['file','is_train']].values:\n    photos_rects = products_df[products_df['file'] == file]\n    rects_data = photos_rects[['category','xmin','ymin','xmax','ymax']]\n    im = cv2.imread(f'{shelf_images}{file}')\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    for category, xmin, ymin, xmax, ymax in rects_data.values:\n        if category == 0:\n            continue\n        pack = resize_pack(np.array(im[ymin:ymax, xmin:xmax]))\n        x.append(pack)\n        f.append(is_train)\n        y.append(category - 1)","c6efcade":"# display one SHAPE_WIDTH x SHAPE_HEIGHT resized pack image, \n# it is hard to recognize category with our eyes, let's see\n# how neural network will do the job\nplt.imshow(x[60])\nplt.show()","20ea80bb":"# lets split the data into train\/validation dataset based on is_train flag\nx,y,f = np.array(x), np.array(y), np.array(f)\nx_train, x_validation, y_train, y_validation = x[f], x[~f], y[f], y[~f]\n\n# save validation images\nx_validation_images = x_validation","427dfcb5":"x_train.shape, x_validation.shape, y_train.shape, y_validation.shape","038b49f3":"# convert y_train and y_validation into one hot arrays\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_validation = keras.utils.to_categorical(y_validation, num_classes)","4d4df821":"y_train.shape, y_validation.shape","3e6bb198":"# normalize x_train and x_validation\nx_train, x_validation = x_train.astype('float32'), x_validation.astype('float32')\nx_train \/= 255\nx_validation \/= 255","d083345b":"# let's see what do we have\nprint('x_train shape:', x_train.shape)\nprint('y_train shape:', y_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_validation.shape[0], 'validation samples')","c6705793":"def lr_schedule(epoch):\n    lr = 1e-3\n    if epoch > 5:\n        lr *= 1e-1\n    print(\"learning rate: \", lr)\n    return lr","c770ba93":"def resnet_layer(inputs,\n                 num_filters=16,\n                 kernel_size=3,\n                 strides=1,\n                 activation='relu',\n                 batch_normalization=True,\n                 conv_first=True):\n    conv = Conv2D(num_filters,\n                  kernel_size=kernel_size,\n                  strides=strides,\n                  padding='same',\n                  kernel_initializer='he_normal',\n                  kernel_regularizer=l2(1e-4))\n\n    x = inputs\n    if conv_first:\n        x = conv(x)\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n    else:\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n        x = conv(x)\n    return x","08026114":"def resnet_v1(input_shape, depth, num_classes=10):\n    if (depth - 2) % 6 != 0:\n        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n    # Start model definition.\n    num_filters = 16\n    num_res_blocks = int((depth - 2) \/ 6)\n\n    inputs = Input(shape=x_train.shape[1:])\n    x = resnet_layer(inputs=inputs)\n    # Instantiate the stack of residual units\n    for stack in range(3):\n        for res_block in range(num_res_blocks):\n            strides = 1\n            if stack > 0 and res_block == 0:  # first layer but not first stack\n                strides = 2  # downsample\n            y = resnet_layer(inputs=x,\n                             num_filters=num_filters,\n                             strides=strides)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters,\n                             activation=None)\n            if stack > 0 and res_block == 0:  # first layer but not first stack\n                # linear projection residual shortcut connection to match\n                # changed dims\n                x = resnet_layer(inputs=x,\n                                 num_filters=num_filters,\n                                 kernel_size=1,\n                                 strides=strides,\n                                 activation=None,\n                                 batch_normalization=False)\n            x = keras.layers.add([x, y])\n            x = Activation('relu')(x)\n        num_filters *= 2\n\n    # Add classifier on top.\n    # v1 does not use BN after last shortcut connection-ReLU\n    x = AveragePooling2D(pool_size=8)(x)\n    y = Flatten()(x)\n    outputs = Dense(num_classes,\n                    activation='softmax',\n                    kernel_initializer='he_normal')(y)\n\n    # Instantiate model.\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\nn = 3\nversion = 1\nif version == 1:\n    depth = n * 6 + 2\nelif version == 2:\n    depth = n * 9 + 2\nmodel_type = 'ResNet%dv%d' % (depth, version)\n\nmodel = resnet_v1(input_shape=x_train.shape[1:], depth=depth, num_classes=num_classes)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])","cc73f643":"# lets see our model architecture and how many trainable parammeters it has\n#model.summary()","3dd663fd":"# this will do preprocessing and reall time data augmentation\n\ndatagen = ImageDataGenerator(featurewise_center = False, # set input mean to 0 over the dataset\n                             samplewise_center = False, # set each sample mean to 0\n                             featurewise_std_normalization = False, # divide inputs by the std of the dataset\n                             samplewise_std_normalization = False, # divide each input by its std\n                             zca_whitening = False, # apply zca whitening\n                             rotation_range = 5, # randomly rotate images in the range (degrees, 0 to 180)\n                             width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)\n                             height_shift_range = 0.1, # randomly shift images vertically (fraction of total height)\n                             horizontal_flip = False, # randomly flip images\n                             vertical_flip = False) # randomly flip images      \ndatagen.fit(x_train)","40b38850":"# let's run training process, 20 epochs is enough\nbatch_size = 50\nepochs = 15\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                    validation_data=(x_validation, y_validation),\n                    epochs=epochs, verbose=1, workers=4, \n                    callbacks=[LearningRateScheduler(lr_schedule)])","16a90cef":"**Implementation with CNN**","7646ac3d":"The dataset contains 11 classes. Class 0 is \"garbage\" (unclassified data). Class 1 is Marlboro, 2 - Kent, 3 - Camel etc. It's very important that our split contains enough data for training for each class and also enugh data for validation. So, let's visualize our split. Yellow is for training, blue is for testing. If the split is not OK, please, select another random_state and repeat previous step!","87ff5eba":"Reference and complete code from : https:\/\/github.com\/empathy87\/nn-grocery-shelves\n\nRecognition of Product Positions on Shelf Images with Deep Learning","62be3c2f":"lets build ResNet CNN","ef9406fb":"We will use data from two folders.\n\n**ShelfImages**\n\nDirectory contains JPG files named the same way as C3_P06_N3_S3_1.JPG file:\n\nC3_P06 - shelf id\n\nN3_S3_1 - planogram id\n\n**ProductImagesFromShelves**\n\nDirectory contains png files grouped by category named the same way as C1_P01_N1_S2_1.JPG_1008_1552_252_376.png file:\n\nC1_P01_N1_S2_1.JPG - shelf photo file\n\n1008 - x\n\n1552 - y\n\n252 - w\n\n376 - h"}}