{"cell_type":{"5a184a73":"code","d0b1e7cf":"code","0320a133":"code","f9cefeff":"code","67a84cda":"code","7d0ba2fb":"code","7af06441":"code","afb462d9":"code","403d91be":"code","c7acadd8":"code","8db39931":"code","75fc3ed1":"code","036505ce":"code","b0e13aa5":"code","9df3be7e":"code","01b98f49":"code","3435ab77":"code","6a08af45":"code","9ebd06e2":"code","78f98521":"code","c555a3ea":"code","69dbf9cf":"code","0cebcb77":"code","0c0046ed":"code","07073c6e":"code","7c12ef43":"code","9fc9a129":"code","42f09ca8":"code","16a3c502":"code","06f6ccb9":"code","4f563447":"code","40e68e57":"code","72d5fda3":"code","511c8a59":"code","82da96de":"code","99c98f0a":"code","ded610d2":"code","f9aadf33":"code","2b58e19b":"code","819db2d0":"code","259c8ca6":"code","20a311e3":"code","8e6d351d":"code","bc07834a":"code","e5f262c1":"code","e6c5a9b2":"code","42af6ce7":"code","4d0aacd1":"code","e571592d":"code","9ead13a7":"code","b8e92ab4":"code","e6405f6a":"code","9dd37a48":"code","0b39d66c":"code","c519c05c":"code","8ce336b2":"code","efccffe5":"code","ac742686":"code","9bd9c144":"code","d3cc6d26":"code","954d6aaa":"markdown","0064ed67":"markdown","aff67817":"markdown","ba42a287":"markdown","1345f1c9":"markdown","04d802fa":"markdown","71c66bdf":"markdown","bfbe08ff":"markdown","2a7fe498":"markdown","e6a4a8f6":"markdown","1fc026dd":"markdown","00c0537b":"markdown","7d79f6f2":"markdown","3355bc0a":"markdown","494199ad":"markdown","c33de0f0":"markdown","bbb4dbfc":"markdown","68b99639":"markdown","67d35ca3":"markdown","5de7608c":"markdown","f0ed0641":"markdown","4a713dc6":"markdown","11765adf":"markdown","fd05e3ad":"markdown","7864f236":"markdown","09aa581d":"markdown","e1312fe2":"markdown","967415c2":"markdown","7615ba14":"markdown","d100a2c0":"markdown","c2775640":"markdown","4038ab46":"markdown","0881db75":"markdown","c07d9301":"markdown","f5750e1c":"markdown","8f1b1a0b":"markdown","02c91fe0":"markdown","b38a481c":"markdown","182b60e7":"markdown","477057ad":"markdown","55bd455f":"markdown"},"source":{"5a184a73":"pip install textfeatures","d0b1e7cf":"pip install nltk","0320a133":"import nltk","f9cefeff":"import plotly.express as px","67a84cda":"!pip install hvplot\nimport hvplot.pandas  # custom install\n\nfrom glob import glob\n\nfrom bq_helper import BigQueryHelper\nfrom dask import bag, diagnostics \nfrom urllib import request\n\nimport missingno as msno","7d0ba2fb":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport pydicom\nimport matplotlib.pyplot as plt\n\n# import useful tools\nfrom glob import glob\nfrom PIL import Image\nimport cv2\nimport pydicom as dcm\nimport random\nimport matplotlib.patches as patches\nfrom sklearn.model_selection import KFold\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nfrom sklearn.model_selection import StratifiedKFold\nimport warnings\n\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Work with phash\nimport imagehash\n\n# import data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nimport matplotlib\nimport pydicom as dicom\n\nfrom bokeh.plotting import figure\nfrom bokeh.io import output_notebook, show, output_file\nfrom bokeh.models import ColumnDataSource, HoverTool, Panel\nfrom bokeh.models.widgets import Tabs\nimport skimage.io as io\n\n# import data augmentation\nimport albumentations as albu\n\n# import math module\nimport math\n\n# Libraries\nimport pandas_profiling\nimport xgboost as xgb\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\n# One-hot encoding\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Other\nfrom random import randint\nimport warnings\nimport csv\nwarnings.filterwarnings(\"ignore\")","7af06441":"# Color scheme\ncolors = [\"#EDAC54\", \"#F4C5B7\", \"#DD7555\", \"#B95F18\", \"#000080\"]","afb462d9":"def plot_images(images_number):\n    \n    plot_list = train_df['image'].sample(n=images_number).tolist()\n    size = np.sqrt(images_number)\n    if int(size)*int(size) < images_number:\n        size = int(size) + 1\n        \n    plt.figure(figsize=(10, 10))\n    \n    ind=0\n    for image_id in plot_list:\n        plt.subplot(size, size, ind + 1)\n        image = cv2.imread(os.path.join('..\/input\/shopee-product-matching\/train_images\/', image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        plt.title(image_id, fontsize=12)\n        plt.axis(\"off\")\n        ind+=1\n    plt.show()","403d91be":"# Setup the paths to train and test images\n\nTEST_DIR = \"..\/input\/shopee-product-matching\/test_images\/\"\nTRAIN_DIR = \"..\/input\/shopee-product-matching\/train_images\/\"\ndataset_dir = \"..\/input\/shopee-product-matching\/\"","c7acadd8":"# Glob the directories and get the lists of train and test images\ntrain_fns = glob(TRAIN_DIR + '*')\ntest_fns = glob(TEST_DIR + '*')\n\n# Loading training data and test data\ntrain_df = pd.read_csv(dataset_dir+'train.csv')\ntest = pd.read_csv(dataset_dir+'test.csv')\nsample = pd.read_csv(dataset_dir+'sample_submission.csv')","8db39931":"train_images = dataset_dir + \"\/train_images\/\" + train_df['image']\ntrain_df['path'] = train_images\n\ntest_images = dataset_dir + \"\/test_images\/\" + test['image']\ntest['path'] = test_images","75fc3ed1":"# Let's find out how many images are under the directory\nprint('Train images: %d' %len(os.listdir(os.path.join(dataset_dir, \"train_images\"))))\nprint('Test images: %d' %len(os.listdir(os.path.join(dataset_dir, \"test_images\"))))","036505ce":"train_df.head(5)","b0e13aa5":"train_df.info()","9df3be7e":"train_df.nunique().to_frame().rename(columns={0:\"Unique Values\"}).style.background_gradient(cmap=\"plasma\")","01b98f49":"# Display of training data\nprint(train_df)","3435ab77":"# Check for missing values in the training data\ntrain_df.isnull().sum()","6a08af45":"labels = train_df.groupby(\"label_group\")[\"image\"].count().reset_index()\nlabels.columns=[\"label_group\",\"image_num\"]\nlabels","9ebd06e2":"sortlabels = labels.sort_values(\"image_num\")\nsortlabels","78f98521":"BASE = '..\/input\/shopee-product-matching\/train_images\/'\n\ndef displayDF(train_df, random=False, COLS=6, ROWS=4, path=BASE):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train_df))\n            else: row = COLS*k + j\n            name = train_df.iloc[row,1]\n            title = train_df.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndisplayDF(train_df,random=True)","c555a3ea":"groups = train_df.label_group.value_counts()\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Index of Unique Item',size=14)\nplt.title('Duplicate Count vs. Unique Item Count',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:50].astype('str'),groups.values[:50])\nplt.xticks(rotation = 45)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 50 Duplicated Items',size=16)\nplt.show()","69dbf9cf":"for k in range(5):\n    print('#'*40)\n    print('### TOP %i DUPLICATED ITEM:'%(k+1),groups.index[k])\n    print('#'*40)\n    top = train_df.loc[train_df.label_group==groups.index[k]]\n    displayDF(top, random=False, ROWS=2, COLS=4)","0cebcb77":"# Display of test data\nprint(test)","0c0046ed":"# Display of sample data\nprint(sample)","07073c6e":"plt.figure(figsize=(8,8))\n\nfor num,a in enumerate(test[\"path\"]):\n    \n    plt.subplot(1,3,num+1)\n    img = cv2.imread(a)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    plt.imshow(img)\n    plt.axis(\"off\")","7c12ef43":"plot_images(8)","9fc9a129":"top10_names = train_df['label_group'].value_counts().index.tolist()[:15]\ntop10_values = train_df['label_group'].value_counts().tolist()[:15]\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=top10_names, y=top10_values)\nplt.xticks(rotation=45)\nplt.xlabel(\"Label Group\")\nplt.ylabel(\"Image Count\")\nplt.title(\"Top-15 Label Groups by Image Count\")\nplt.show()","42f09ca8":"stopwords = set(STOPWORDS) \nwordcloud = WordCloud(width = 800, \n                      height = 800,\n                      background_color ='white',\n                      min_font_size = 10,\n                      stopwords = stopwords,).generate(' '.join(train_df['title'])) \n\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() ","16a3c502":"def preprocess_text(text, flg_stemm=False, flg_lemm=True):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","06f6ccb9":"#Clean Address\ntrain_df[\"clean_title\"] = train_df[\"title\"].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, ))","4f563447":"#Length of Title\ntrain_df['clean_title_len'] = train_df['clean_title'].apply(lambda x: len(x))","40e68e57":"#Word Count\ntrain_df['clean_title_word_count'] =train_df[\"clean_title\"].apply(lambda x: len(str(x).split(\" \")))","72d5fda3":"#Character Count\ntrain_df['clean_title_char_count'] = train_df[\"clean_title\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))","511c8a59":"#Average Word Length\ntrain_df['clean_title_avg_word_length'] = train_df['clean_title_char_count'] \/ train_df['clean_title_word_count']","82da96de":"def plot_distribution(x, title):\n\n    fig = px.histogram(\n    train_df, \n    x = x,\n    width = 800,\n    height = 500,\n    title = title\n    )\n    \n    fig.show()","99c98f0a":"plot_distribution(x = 'clean_title_len', title = 'Title Length Distribution')","ded610d2":"plot_distribution(x = 'clean_title_word_count', title = 'Word Count Distribution')","f9aadf33":"plot_distribution(x = 'clean_title_char_count', title = 'Character Count Distribution')","2b58e19b":"plot_distribution(x = 'clean_title_avg_word_length', title = 'Average Word Length Distribution')","819db2d0":"phashgroup = train_df.groupby(\"image_phash\")[\"path\"].count().reset_index()\nphashgroup.columns=[\"image_phash\",\"counts\"]\nphashgroup","259c8ca6":"sortphash = phashgroup.sort_values(\"counts\")\nsortphash","20a311e3":"tmpdf = train_df[train_df[\"image_phash\"]==sortphash[\"image_phash\"].iloc[-1]]\ntmpdf","8e6d351d":"# Shape columns\ntrain_df['img_shape'] = train_df['path'].progress_apply(lambda x: np.shape(io.imread(x)))","bc07834a":"shapes = pd.DataFrame().from_records(train_df['img_shape'])\nshapes.columns = ['Width', 'Height', 'Colors']\n\nsns.set_style(\"white\")\nsns.jointplot(x = shapes.iloc[:, 0].astype('float32'), \n              y = shapes.iloc[:, 1].astype('float32'),\n              height = 8, color = '#000080')\nplt.show()","e5f262c1":"# Get the count of apparitions per image\nimage_count = train_df[\"image\"].value_counts().reset_index()\nimage_count.columns = [\"image\", \"count\"]\nimage_count_duplicates = image_count[image_count[\"count\"] > 1]\nprint(\"Total no. of images with duplicates: {:,}\".format(len(image_count_duplicates)))\n\n#Plot\nfig, ax = plt.subplots(figsize=(16, 7))\nplt.bar(x=image_count_duplicates.iloc[::16][\"image\"],\n        height=image_count_duplicates.iloc[::16][\"count\"],\n        color=colors[4])\nplt.title(\"Duplicated Images: How many apparitions?\", fontsize=20)\nplt.xticks([])\nplt.xlabel(\"Image ID\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16);","e6c5a9b2":"def image_viz(image_path):\n    \"\"\"\n    Function for visualization.\n    Takes path to image as input.\n    \"\"\"\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \n    plt.imshow(img)\n    plt.axis('off')","42af6ce7":"def match_matrix(phash_array):\n    \"\"\"\n    A function that checks for matches by phash value.\n    Takes phash values as input.\n    Output - phash diff matrix (pandas data frame)\n    \"\"\"\n    phashs = phash_array.apply(lambda x: imagehash.hex_to_hash(x))\n    phash_matrix = pd.DataFrame()\n    pbar = tqdm.tqdm(total = len(phash_array), desc = 'Progress:', \n                     position = 0, leave = True)\n    for idx, i in enumerate(phash_array):\n        pbar.update(1)\n        phash_matrix = pd.concat([phash_matrix, phashs - imagehash.hex_to_hash(i)], \n                                 axis = 1)\n    pbar.close()\n    phash_matrix.columns = range(len(phash_array))\n    return phash_matrix","4d0aacd1":"train_part = train_df.iloc[:1000, :]\nmatches = match_matrix(train_part['image_phash'])\nmatches","e571592d":"test_match = match_matrix(test['image_phash'][:3])\ntest_match","9ead13a7":"train_part.loc[[11,12],['title']]","b8e92ab4":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[11, 'path'], \n                         train_part.loc[12, 'path']]):\n    plt.subplot(1, 2, idx + 1)\n    image_viz(i)\nplt.show()","e6405f6a":"train_part.loc[[889,890,891],['posting_id','image_phash','title','label_group']]","9dd37a48":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[889, 'path'], \n                         train_part.loc[890, 'path'], \n                         train_part.loc[891, 'path']]):\n    plt.subplot(1, 3, idx + 1)\n    image_viz(i)\nplt.show()","0b39d66c":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[997, 'path'], \n                         train_part.loc[520, 'path']]):\n    plt.subplot(1, 2, idx + 1)\n    image_viz(i)\nplt.show()","c519c05c":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[55, 'path'], \n                         train_part.loc[312, 'path']]):\n    plt.subplot(1, 2, idx + 1)\n    image_viz(i)\nplt.show()","8ce336b2":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[128, 'path'], \n                         train_part.loc[515, 'path']]):\n    plt.subplot(1, 2, idx + 1)\n    image_viz(i)\nplt.show()","efccffe5":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate([train_part.loc[216, 'path'], \n                         train_part.loc[567, 'path']]):\n    plt.subplot(1, 2, idx + 1)\n    image_viz(i)\nplt.show()","ac742686":"plt.figure(figsize = (15, 10))\nfor idx, i in enumerate(test['path']):\n    plt.subplot(1, 3, idx + 1)\n    image_viz(i)\nplt.show()","9bd9c144":"test","d3cc6d26":"# coding: utf-8\nfrom tqdm import tqdm\nimport time\n\n# Set the total value \nbar = tqdm(total = 1000)\n# Add description\nbar.set_description('Progress rate')\nfor i in range(100):\n    # Set the progress\n    bar.update(25)\n    time.sleep(1)","954d6aaa":"# Your upvote is my motivation","0064ed67":"# Display Duplicated Items from Train Data\nUsing the column label_group which is the ground truth, we can display examples of duplicated items.","aff67817":"# sample","ba42a287":"* The number of uniqueness is the same for image and path. image_phash and label_group are fewer in number than image, so there is a possibility to aggregate common images with these.","1345f1c9":"<h2 style='color:white; background:#000080; border:0'><center>Images analysis<\/center><\/h2>","04d802fa":"# The Goal of this Competition\nFinding near-duplicates in large datasets is an important problem for many online businesses. Our task is to identify which products have been posted repeatedly. ","71c66bdf":"* minimum images are 2, max images are 51","bfbe08ff":"* Take a look at the distribution of the number of images in Label Groups. There are some groups with a small number of images, but there are neither groups with a large number of images nor groups with a small number of images.","2a7fe498":"* Draw the height, width, and color of the image as a scatter plot with histogram.","e6a4a8f6":"* Prepare a library to draw a histogram.","1fc026dd":"* Posting_id is different, but title, image_phash and label_group are the same image.","00c0537b":"* Understanding same label_groups\n* Counting the number of images in each label group","7d79f6f2":"<h2 style='color:white; background:#000080; border:0'><center>Acknowledgements<\/center><\/h2>","3355bc0a":"# test","494199ad":"# Evaluation Criteria\nSubmissions will be evaluated based on their mean F1 score. The mean is calculated in a sample-wise fashion, meaning that an [F1 score](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/f-score) is calculated for every predicted row, then averaged.","c33de0f0":"What is Shopee ?\n[Wikipedia page](https:\/\/en.wikipedia.org\/wiki\/Shopee) says that Shopee Pte Ltd (\/\u0283\u0252pi\u02d0\/ SHO-pee) is a Singaporean multinational technology company which focuses mainly on e-commerce. Headquartered under Sea Group (previously known as Garena),[2] Shopee was first launched in Singapore in 2015, and later expanded its reach to Malaysia, Thailand, Taiwan, Indonesia, Vietnam, the Philippines and Brazil.\n\n[The competition page](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/data) states that Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.","bbb4dbfc":"Basic NLP","68b99639":"<h2 style='color:white; background:#000080; border:0'><center>Test Image<\/center><\/h2>","67d35ca3":"Let's figure out the unique number for each columns.","5de7608c":"* Check the fit by Phash value.","f0ed0641":"* [Shopee - Data understanding and analysis](https:\/\/www.kaggle.com\/isaienkov\/shopee-data-understanding-and-analysis)\n* [Shopee: Before we start (EDA, PHASH, Baseline)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/shopee-before-we-start-eda-phash-baseline)\n* [EDA of Shopee for starter](https:\/\/www.kaggle.com\/chumajin\/eda-of-shopee-for-starter)\n* [Shopee: text prep | FE | image augmentation](https:\/\/www.kaggle.com\/andradaolteanu\/shopee-text-prep-fe-image-augmentation)\n* [[V5]Shopee InDepth EDA:One stop for all your needs](https:\/\/www.kaggle.com\/ishandutta\/v5-shopee-indepth-eda-one-stop-for-all-your-needs)\n* [[V7]Shopee InDepth EDA:One stop for all your needs](https:\/\/www.kaggle.com\/ishandutta\/v7-shopee-indepth-eda-one-stop-for-all-your-needs)\n* [Shopee: Before we start (EDA, PHASH, Baseline)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/shopee-before-we-start-eda-phash-baseline)\n* [RAPIDS cuML TfidfVectorizer and KNN](https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-tfidfvectorizer-and-knn)","4a713dc6":"Duplicated Images","11765adf":"<h2 style='color:white; background:#000080; border:0'><center>Loading data<\/center><\/h2>","fd05e3ad":"* An example of a similar, but not identical, image is shown below.","7864f236":"* An example of a similar, but not identical, image is shown below.","09aa581d":"* An example of a similar, but not identical, image is shown below.","e1312fe2":"* Posting_id and title are different, but image_phash and label_group are the same image.","967415c2":"[](http:\/\/)<h2 style='color:white; background:#000080; border:0'><center>Titles analysis<\/center><\/h2>","7615ba14":"<h2 style='color:white; background:#000080; border:0'><center>Usefull functuions<\/center><\/h2>","d100a2c0":"* There are 1,246 images that have 2 or more apparitions\n* the title differs for most of them\n* the label_group differs for most of them as well","c2775640":"[The competition page](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/data) states the following about the IDs of the submitted file.\n\n* sample_submission.csv - a sample submission file in the correct format.\n\n* posting_id - the ID code for the posting.\n\nmatches - Space delimited list of all posting IDs that match this posting. Posts always self-match. Group sizes were capped at 50, so there's no need to predict more than 50 matches.**","4038ab46":"[The competition page](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/data) states the following about each ID.\nFiles\n[train\/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n\n* posting_id - the ID code for the posting.\n\n* image - the image id\/md5sum.\n\n* image_phash - a perceptual hash of the image.\n\n* title - the product description for the posting.\n\n* label_group - ID code for all postings that map to the same product. Not provided for the test set.\n\n* [train\/test]images - the images associated with the postings.\n\n* sample_submission.csv - a sample submission file in the correct format.\n\n* posting_id - the ID code for the posting.\n\n* matches - Space delimited list of all posting IDs that match this posting. Posts always self-match. Group sizes were capped at 50, so there's no need to predict more than 50 matches.","0881db75":"* NLTK (Natural Language Toolkit) is useful for natural language processing. The official documentation is [here](http:\/\/www.nltk.org\/).","c07d9301":"<h2 style='color:white; background:#000080; border:0'><center>Statistics<\/center><\/h2>","f5750e1c":"Image Label Groups by No. of Images","8f1b1a0b":"# Work in progress\u2026","02c91fe0":"* Fortunately, it turns out there are no missing values.","b38a481c":"<center><img src = \"https:\/\/klgadgetguy.com\/wp-content\/uploads\/2018\/10\/6ce1f4f6d79353c5f24ee047a5132d77.jpg\" width = \"750\" height = \"500\"\/><\/center>  ","182b60e7":"<h2 style='color:white; background:#000080; border:0'><center>Overview<\/center><\/h2>","477057ad":"* maximum counts are 26.","55bd455f":"<h2 style='color:white; background:#000080; border:0'><center>Data Visualization<\/center><\/h2>"}}