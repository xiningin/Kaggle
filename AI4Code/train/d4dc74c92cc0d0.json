{"cell_type":{"d688bbd7":"code","b98b8610":"code","34b5c330":"code","0c0cb74f":"code","4d936d7f":"code","13a648ee":"code","6b6412a4":"code","4ca4144a":"code","4ee0811d":"code","7929aca7":"code","2594ec17":"code","c8a65179":"code","30ae2823":"code","75f0c6bb":"code","2309213e":"code","9cb1c279":"markdown","6a9c13a6":"markdown","8ad38282":"markdown","38021ebb":"markdown","caa0de12":"markdown","d084e560":"markdown","b3ea5bed":"markdown","9db60138":"markdown","f2b71116":"markdown","71f3c4a8":"markdown","e8600411":"markdown"},"source":{"d688bbd7":"import numpy as np\nimport pandas as pd\nfrom math import exp\n\nfrom IPython.display import clear_output\nfrom numpy.random import randn\nfrom numpy.random import rand\nfrom tqdm import tqdm\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings('ignore')","b98b8610":"path = '..\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndf = pd.read_csv(path)\ndf = shuffle(df)\ndf.head()","34b5c330":"features = df[df.columns[:-1]]\nfor column in list(features.columns):\n    features[column] = (features[column] - features[column].mean()) \/ features[column].std()\nlabels = df['Outcome']","0c0cb74f":"def sigmoid(x):\n    return 1.0 \/ (1.0 + exp(-x))\n\ndef forward(row, weights):\n    activation = weights[-1]\n    for i in range(len(row)):\n        activation += weights[i] * row[i]\n    return activation","4d936d7f":"def predict(row, network):\n    inputs = row\n    for layer in network:\n        new_inputs = list()\n        for node in layer:\n            out = forward(inputs, node)\n            out = sigmoid(out)\n            new_inputs.append(out)\n        inputs = new_inputs\n    return inputs[0]\n\ndef predict_dataset(X, network):\n    yhats = list()\n    for row in X:\n        yhat = predict(row, network)\n        yhats.append(yhat)\n    return yhats","13a648ee":"def objective(X, y, network):\n    yhat = predict_dataset(X, network)\n    yhat = [round(y) for y in yhat]\n    score = accuracy_score(y, yhat)\n    return score\n\ndef step(network, step_size):\n    new_net = list()\n    for layer in network:\n        new_layer = list()\n        for node in layer:\n            new_node = node.copy() + randn(len(node)) * step_size\n            new_layer.append(new_node)\n        new_net.append(new_layer)\n    return new_net\n\ndef hill_climbing(X, y, objective, solution, n_iter, step_size):\n    solution_eval = objective(X, y, solution)\n    for i in tqdm(range(n_iter)):\n        candidate = step(solution, step_size)\n        candidte_eval = objective(X, y, candidate)\n        # store only if the score is better\n        if candidte_eval >= solution_eval:\n            solution, solution_eval = candidate, candidte_eval\n    return [solution, solution_eval]","6b6412a4":"def get_model(n_inputs, n_hidden=10):\n    hidden1 = [rand(n_inputs + 1) for _ in range(n_hidden)]\n    output1 = [rand(n_hidden + 1)]\n    network = [hidden1, output1]\n    return network","4ca4144a":"n_iter = 1000\nstep_size = 0.1\nn_inputs = features.shape[1]\nnetworks = [get_model(n_inputs) for a in range(3)]\nkf = KFold(n_splits=3)","4ee0811d":"scores = []\nfor i, (train_index, test_index) in enumerate(kf.split(features)):\n    clear_output(wait=True)\n    \n    features, labels = shuffle(features, labels)\n    features_ = features.values\n    labels_ = labels.values\n\n    X_train, X_test, y_train, y_test = train_test_split(features_, labels_, test_size=0.2)\n\n    network, score = hill_climbing(X_train, y_train, objective, networks[i], n_iter, step_size)\n    networks[i] = network\n    print('Best Accuracy: %f' % (score))\n\n    yhat = predict_dataset(X_test, networks[i])\n    yhat = [round(y) for y in yhat]\n\n    score = accuracy_score(y_test, yhat)\n    print('Test Accuracy: %.5f' % (score * 100))\n    scores.append(score)","7929aca7":"features, labels = shuffle(features, labels)\nfeatures_ = features.values\nlabels_ = labels.values\n\nX_train, X_test, y_train, y_test = train_test_split(features_, labels_, test_size=0.2)","2594ec17":"def get_ensemble(X, models):\n    outs = list()\n    for i in range(3):\n        out = predict_dataset(X, networks[i])\n        outs.append(out)\n    return np.moveaxis(np.array(outs), 1, 0)","c8a65179":"meta_train = get_ensemble(X_train, networks)\nmeta_test = get_ensemble(X_test, networks)","30ae2823":"reg = LinearRegression().fit(meta_train, y_train)\nprint(\"Linear Regression Accuracy:{:1.3f}\".format(reg.score(meta_test, y_test)))","75f0c6bb":"model = LogisticRegression(solver='liblinear')\nmodel.fit(meta_train, y_train)","2309213e":"yhat = model.predict(meta_test)\nacc = accuracy_score(y_test, yhat)\nprint(\"Logistic Regression Accuracy:{:1.3f}\".format(acc))","9cb1c279":"## Hill Climbing","6a9c13a6":"<pre>\n\n                  ___      .-\"\"-.      ___\n    Diabetes      \\  \"-.  \/      \\  .-\"  \/       By\n      Hill         > -=.\\\/        \\\/.=- <      Alin\n    Climbing       > -='\/\\        \/\\'=- <        Cijov\n                  \/__.-'  \\      \/  '-.__\\\n                           '-..-'\n<\/pre>","8ad38282":"# Blending","38021ebb":"## Prediction functions","caa0de12":"## Parameters","d084e560":"## Linear Regression","b3ea5bed":"# Model","9db60138":"# Training","f2b71116":"## Forward function","71f3c4a8":"# Prepare Data","e8600411":"## Logistic Regression"}}