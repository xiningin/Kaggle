{"cell_type":{"9366f9c4":"code","78e58f19":"code","cbb4b180":"code","bad75d95":"code","96ebc976":"code","b272ba14":"code","cde25b75":"code","43e92991":"code","da088c6d":"code","9ac6d5b6":"code","8646b3a9":"code","6c4a4d80":"code","58331a8f":"code","5469664c":"code","540e3b0b":"code","aa2e20e5":"code","03ec4803":"code","f3768758":"code","cacb6a04":"code","0c0bebcc":"code","44abf235":"code","94abb947":"code","3b04b983":"code","6c6dec9d":"code","1e92378d":"code","4099c36e":"code","0a9e82e4":"code","605e22b2":"code","ebc29b90":"code","59a1e1f1":"code","fe4e1638":"code","383e01d0":"code","ac5ed128":"code","1b6511e0":"code","273699c7":"code","b33bbf46":"code","415b394e":"code","5e60e62c":"code","815546b0":"code","2b7c9a03":"code","15d12c03":"code","e9779336":"code","cd327737":"code","9027de3f":"code","d2b5af62":"code","abfc3fce":"markdown","4fad426b":"markdown","f09c5983":"markdown","dd423325":"markdown","77fdae13":"markdown","b1d55d60":"markdown","644a1f48":"markdown","e49e688f":"markdown","2fc82c77":"markdown","e4dd797a":"markdown","f6f84d05":"markdown","8851f92f":"markdown","615d196a":"markdown","4a3a7cb4":"markdown","51a924f5":"markdown","2eab69d4":"markdown","ae075b94":"markdown","bd896f6b":"markdown","667b1ce4":"markdown","b1ba5c03":"markdown","69cb55d4":"markdown","ba15e852":"markdown","2c199b06":"markdown","c2ac3301":"markdown","c5d88b16":"markdown","3651fa46":"markdown","218b5aae":"markdown","1cffc298":"markdown","8b7a8a37":"markdown","d295a8cb":"markdown"},"source":{"9366f9c4":"#Data management\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\nfrom pandas_profiling import ProfileReport\n\n#TextBlob Features\nfrom textblob import TextBlob\n\n#Plotting\nimport matplotlib.pyplot as plt\n\n#SciKit-Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n#nltk\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\n\n#Tensorflow \/ Keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n#Test\nfrom collections import Counter","78e58f19":"#Training Data\npath = \"..\/input\/twitter-entity-sentiment-analysis\/twitter_training.csv\"\ntrain_df = pd.read_csv(path, names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Tweet_Content\"])\n\n#Test Data (Not to be used until the full model has been trained)\ntest_path = \"..\/input\/twitter-entity-sentiment-analysis\/twitter_validation.csv\"\ntest_df = pd.read_csv(test_path, names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Tweet_Content\"])","cbb4b180":"train_df = train_df.dropna()\ntest_df = test_df.dropna()","bad75d95":"train_df.shape","96ebc976":"df = train_df.sample(frac=0.1)\ndf.reset_index(drop=True, inplace=True)","b272ba14":"#See overall information about the data frame\ndf.info()","cde25b75":"#Display a few entries\ndf.head()","43e92991":"profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\nprofile","da088c6d":"#Checking balance of target classes\nsentiments = list(df[\"Sentiment\"].unique())\n\nsentiment_nums = [len(df[df[\"Sentiment\"] == sentiment]) \/ len(df) for sentiment in sentiments]\n\nplt.bar(sentiments, sentiment_nums)","9ac6d5b6":"#View all possible entities\nprint(df[\"Entity\"].unique())\nprint(len(df[\"Entity\"].unique()))","8646b3a9":"#One-hot encode using Pandas' get_dummies()\nonehot = pd.get_dummies(df[\"Entity\"], prefix=\"Entity\")\n\n#Join these new columns back into the DataFrame\ndf = df.join(onehot)\n\n#Display a sample of the data with our new columns\ndf.head()","6c4a4d80":"#Remove the Entity column, as we have transformed this data into a more usable format\n\ndf = df.drop(\"Entity\", axis=1)","58331a8f":"#Adding dimensions with textblob\ndef tb_enrich(ls):\n    #Enriches a column of text with TextBlob Sentiment Analysis outputs\n    tb_polarity = []\n    tb_subject = []\n\n    for tweet in ls:\n        tb_polarity.append(TextBlob(tweet).sentiment[0])\n        tb_subject.append(TextBlob(tweet).sentiment[1])\n    \n\n    return tb_polarity, tb_subject\n    \ndf[\"Polarity\"], df[\"Subjectivity\"] = tb_enrich(list(df[\"Tweet_Content\"]))","5469664c":"df.head()","540e3b0b":"#Define the indexing for each possible label in a dictionary\nclass_to_index = {\"Neutral\":0, \"Irrelevant\":1, \"Negative\":2, \"Positive\": 3}\n\n#Creates a reverse dictionary\nindex_to_class = dict((v,k) for k, v in class_to_index.items())\n\n#Creates lambda functions, applying the appropriate dictionary\nnames_to_ids = lambda n: np.array([class_to_index.get(x) for x in n])\nids_to_names = lambda n: np.array([index_to_class.get(x) for x in n])","aa2e20e5":"#Test each function\nprint(names_to_ids([\"Positive\", \"Negative\", \"Irrelevant\", \"Neutral\"]))\nprint(ids_to_names([0,1,2,3]))","03ec4803":"#Convert the \"Sentiment\" column into indexes\ndf[\"Sentiment\"] = names_to_ids(df[\"Sentiment\"])","f3768758":"df.head()","cacb6a04":"def remove_stopwords(ls):\n    #Lemmatises, then removes stop words\n    ls = [lemmatiser.lemmatize(word) for word in ls if word not in (stop_english) and (word.isalpha())]\n    \n    #Joins the words back into a single string\n    ls = \" \".join(ls)\n    return ls\n\n#Splits each string into a list of words\ndf[\"Tweet_Content_Split\"] = df[\"Tweet_Content\"].apply(word_tokenize)\n\n#Applies the above function to each entry in the DataFrame\nlemmatiser = WordNetLemmatizer()\nstop_english = Counter(stopwords.words()) #Here we use a Counter dictionary on the cached\n                                          # list of stop words for a huge speed-up\ndf[\"Tweet_Content_Split\"] = df[\"Tweet_Content_Split\"].apply(remove_stopwords)","0c0bebcc":"df.head()","44abf235":"#Define the Tokeniser\ntokeniser = Tokenizer(num_words=1000, lower=True)\n\n#Create the corpus by finding the most common words\ntokeniser.fit_on_texts(df[\"Tweet_Content_Split\"])","94abb947":"#Create our bag of words matrix\ntweet_tokens = tokeniser.texts_to_matrix(list(df[\"Tweet_Content_Split\"]))","3b04b983":"tweet_tokens.shape","6c6dec9d":"print(df.shape)\nprint(tweet_tokens.shape)","1e92378d":"#Combining the dataframe with the tokens using pd.concat\nfull_df = pd.concat([df, pd.DataFrame(tweet_tokens)], sort=False, axis=1)\nfull_df.shape","4099c36e":"#Remove dependent variable\ny = full_df[\"Sentiment\"]\n\n#Drop all non-useful columns\nfull_df = full_df.drop([\"Sentiment\", \"Tweet_ID\", \"Tweet_Content\", \"Tweet_Content_Split\"], axis=1)","0a9e82e4":"#Display final shape\nfull_df.shape","605e22b2":"X_train, X_test, y_train, y_test = train_test_split(full_df, y, test_size=0.2, random_state=1)","ebc29b90":"#Test model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(12, input_dim=1034, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(12, activation='relu'),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dense(4, activation='sigmoid')\n])\nmodel.compile(\n     loss='sparse_categorical_crossentropy',\n     optimizer='adam',\n     metrics=['accuracy']\n)","59a1e1f1":"h = model.fit(\n     X_train, y_train,\n     validation_data=(X_test, y_test),\n     epochs=30,\n     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)]\n)","fe4e1638":"#Generate predictions\ny_pred = np.argmax(model.predict(X_test), axis=1)\n\n#Assign labels to predictions and test data\ny_pred_labels = ids_to_names(y_pred)\ny_test_labels = ids_to_names(y_test)","383e01d0":"y_unique = list(set(y_test_labels))\ncm = confusion_matrix(y_test_labels, y_pred_labels, labels = y_unique, normalize='true')\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_unique)\ndisp.plot()","ac5ed128":"#Use the full dataset!\ndf = train_df\n\n# the test dataframe was loaded earlier and is named test_df","1b6511e0":"#One-hot encode using Pandas' get_dummies()\n\n##Train\nonehot = pd.get_dummies(df[\"Entity\"], prefix=\"Entity\")\n\n#Join these new columns back into the DataFrame\ndf = df.join(onehot)\n\n\n##Test\nonehot = pd.get_dummies(test_df[\"Entity\"], prefix=\"Entity\")\n\ntest_df = test_df.join(onehot)","273699c7":"#Enrich using TextBlob's built in sentiment analysis\n\n##Train\ndf[\"Polarity\"], df[\"Subjectivity\"] = tb_enrich(list(df[\"Tweet_Content\"]))\n\n\n##Test\ntest_df[\"Polarity\"], test_df[\"Subjectivity\"] = tb_enrich(list(test_df[\"Tweet_Content\"]))","b33bbf46":"#Convert the \"Sentiment\" column into indexes\n\n##Train\ndf[\"Sentiment\"] = names_to_ids(df[\"Sentiment\"])\ny = df[\"Sentiment\"]\n\n##Test\ntest_df[\"Sentiment\"] = names_to_ids(test_df[\"Sentiment\"])\ny_test = test_df[\"Sentiment\"]","415b394e":"#Removing stopwords and lemmatising\n\n##Train\n#Splits each string into a list of words\ndf[\"Tweet_Content_Split\"] = df[\"Tweet_Content\"].apply(word_tokenize)\n\n#Applies the above function to each entry in the DataFrame\nlemmatiser = WordNetLemmatizer()\nstop_english = Counter(stopwords.words()) #Here we use a Counter dictionary on the cached\n                                          # list of stop words for a huge speed-up\ndf[\"Tweet_Content_Split\"] = df[\"Tweet_Content_Split\"].apply(remove_stopwords)\n\n##Test\ntest_df[\"Tweet_Content_Split\"] = test_df[\"Tweet_Content\"].apply(word_tokenize)\n\ntest_df[\"Tweet_Content_Split\"] = test_df[\"Tweet_Content_Split\"].apply(remove_stopwords)","5e60e62c":"#Bag of Words\n\n#Define the Tokeniser\ntokeniser = Tokenizer(num_words=1000, lower=True)\n\n#Create the corpus by finding the most common words\ntokeniser.fit_on_texts(df[\"Tweet_Content_Split\"])\n\n##Train\n#Create bag of words matrix\ntweet_tokens = tokeniser.texts_to_matrix(list(df[\"Tweet_Content_Split\"]))\n\n##Test\n#Create bag of words matrix\ntweet_tokens_test = tokeniser.texts_to_matrix(list(test_df[\"Tweet_Content_Split\"]))","815546b0":"#Combining the dataframe with the tokens using pd.concat\n\n#Reset axes to avoid overlapping\ndf.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\n##Train\nfull_df = pd.concat([df, pd.DataFrame(tweet_tokens)], sort=False, axis=1)\n\n##Test\nfull_test_df = pd.concat([test_df, pd.DataFrame(tweet_tokens_test)], sort=False, axis=1)","2b7c9a03":"#Final prep\n\n##Train\n#Drop all non-useful columns\nfull_df = full_df.drop([\"Sentiment\", \"Tweet_ID\", \"Tweet_Content\", \"Tweet_Content_Split\", \"Entity\"], axis=1)\n\n\n##Test\nfull_test_df = full_test_df.drop([\"Sentiment\", \"Tweet_ID\", \"Tweet_Content\", \"Tweet_Content_Split\", \"Entity\"], axis=1)","15d12c03":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(12, input_dim=1034, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(12, activation='relu'),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dense(4, activation='sigmoid')\n])\nmodel.compile(\n     loss='sparse_categorical_crossentropy',\n     optimizer='adam',\n     metrics=['accuracy']\n)","e9779336":"h = model.fit(\n     full_df, y,\n     epochs=30,\n     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)]\n)","cd327737":"#Generate predictions\ny_pred = np.argmax(model.predict(full_test_df), axis=1)\n\n#Assign labels to predictions and test data\ny_pred_labels = ids_to_names(y_pred)\ny_test_labels = ids_to_names(y_test)","9027de3f":"y_unique = list(set(y_test_labels))\ncm = confusion_matrix(y_test_labels, y_pred_labels, labels = y_unique, normalize='true')\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_unique)\ndisp.plot()","d2b5af62":"#To see the final accuracy\naccuracy_score(y_test, y_pred)","abfc3fce":"### Adding the Tokenised Strings to the DataFrame\n\nCurrently, the tokens are contained in a matrix titled \"tweet_tokens\". We then want to combine these back into the dataframe containing all of the current data. This is completed below, and then we test to make sure that this has occurred correctly by looking at the number of columns compared to that of the original matrices.","4fad426b":"## Final Data Preparation\n\nThe data is now almost ready for a model to be trained on it, but a few final preparations will need to occur. For example, we need to drop the columns that we don't plan to use, such as the \"Tweet_Content\" column, which has had its useful information extracted already.\n\nWe also split the data into a training and test set, such that we can evaluate our model's performance without touching the held-out data. We do this because if we continually test against this held-out data, it loses its usefulness as unseen \"real-world\" data.\n\nSections under this header include:\n- Dropping Unused Data\n- Test-Train Split","f09c5983":"### One-Hot Encoding\n\nHere, we create a Boolean column for each possible entity, as there are only 32 of these. 32 columns are added, each signifying if the Tweet was related to the given entity. Each Tweet will only be related to one entity in total.","dd423325":"### Importing Data","77fdae13":"## Model Evaluation\n\nNow that we've trained the model, we can view it's accuracy with a confusion matrix. This allows us to see the predictions for Tweets with various true values. From this, we might see that we are better at predicting certain classes than others, such as in this model, where we can predict Negative and Positive sentiment significantly better than Irrelevant or Neutral.","b1d55d60":"### Subsetting Data\n\nAs this dataset is quite large, during the exploration process we begin by subsetting the data during the training process, to speed up any testing steps. We also reset the index of the dataframe, to make adding columns easier later on in the process.","644a1f48":"### Stop Word Removal and Lemmatisation with NLTK\n\nHere, we first split each string into its individual words, before checking if these:\n1. Contain text\n2. Are in the list of stopwords\n\nIf there is no text in the word, meaning there are only numbers or punctuation (or other characters), or the word is a stop word (words such as \"with\", \"a\", \"the\"), then the word is removed from the string.\n\nWe also lemmatise in this step, in which we convert words into their root form, such that tense and other details can be ignored in our final model (a negative statement in the past tense is still negative).","e49e688f":"### Labels to Indexes (and Back Again)\n\nHere, as discussed above, we create a dictionary in order to change the text in the \"Sentiment\" column into an index representing the predicted sentiment.\n\nWe then convert the \"Sentiment\" column in the training data to the labels, which is what we will learn to predict","2fc82c77":"### Dropping NA Values\n\nHere, we drop any rows with null values, as these miss out on key information.","e4dd797a":"## NLP Data Enrichment\n\nIn this section, we work with the text in the \"Tweet_Content\" column, extracting all the insight from this that we can, and converting it into a useable format for the neural network that we will train later. This involves removing words that have little meaning (stop word removal), and grouping words with the same meaning regardless of details such as tense (lemmatisation). Then, we use tokenisation in order to codify the presence of words into a matrix, similar in fashion to one-hot encoding. This is known as a \"Bag of Words\" method.\n\nSections under this header include:\n\n- Stop Word Removal and Lemmatisation with NLTK\n- Tokenisation\n- Adding the Tokenised Strings to the DataFrame","f6f84d05":"### Dropping Unused Data\n\nWe drop non-useful columns from the DataFrame here. These either have no use (Tweet ID), or have already had the useful information extracted (Tweet Content). We also remove the \"y\" or dependent variable here, so we don't accidentally train on it.","8851f92f":"## Training with Full Dataset\n\nNow that we are happy with our model, we can train using the full dataset, and predict the held-out test data. This involves performing all of our transformation steps on both this training dataset and the held-out test data. Luckily, we can reuse the code from above to achieve this, so little further explanation is required.","615d196a":"## Model Construction and Training\n\nFinally, it is time to construct our model. In this case, we use a neural network constructed with Keras. We then train it with our data in the training dataset, and validate using the test datasets.\n\nSections under this header include:\n- Model Construction\n- Training","4a3a7cb4":"## Preparation\n\nThis section is where we prepare for the project, through a variety of initial steps. The steps in this section are as follows:\n\n- Importing Packages\n- Importing Data\n- Dropping NA Values\n- Subsetting Data","51a924f5":"### Basic Visualisation\n\nWe can display basic statistics about the data using pandas, and also view a few entries of the dataset, to see example points with which we'll work.","2eab69d4":"### Model Construction\n\nHere, we define the neural network that we will train to predict the output. This model is constructed with the following layers:\n1. Dense\n2. Dropout\n\nThe Dense layers are fully-connected layers. This means that inbetween each layer, we can transfer data from any neuron to any one in the next layer (or indeed all others), scaled by the weight associated with that transfer. These weights are trained.\n\nThe Dropout layers prevent our overall weights from getting too large, as can happen with larger neural networks. This helps to stop certain areas of the network from overloading the network as a whole.\n","ae075b94":"### Importing Packages","bd896f6b":"## Final Model Evaluation","667b1ce4":"### NLP Data Enrichment","b1ba5c03":"### Test-Train Split\n\nHere, we use SciKit-Learn's inbuilt function to split our data into a test set and a train set, with the appropriate labels. We use a constant random state to make this replicable.","69cb55d4":"### Bag of Words\n\nWe create our bag of words, which results in a matrix, in which each column represents the presence of a given word in each row's associated string of text.","ba15e852":"## Data Exploration\n\nHere, we explore the data, testing if it is balanced, and checking for patterns in missing rows. This can generally be done in an automated fashion with pandas-profiling. The sections under this header include:\n\n- Basic visualisation\n- Automated Data Exploration with pandas-profiling\n- Checking for balance in output categories","2c199b06":"## Basic Data Enrichment\n\nHere, we consider basic features that can enhance the dataset. This will include one-hot encoding of the categorical \"Entity\" variable. We can also use pre-trained NLP systems here to extract features from the text, such as the one from TextBlob, giving us a \"polarity\" and \"subjectivity\" value from any text that we give it. These extra features are added to the dataframe.\n\nWe can also create an indexer to convert sentiments from labels to indexes, and back again. This is useful in understanding our predictions later on.\n\nSections under this header include:\n\n- One-Hot Encoding\n- Enrichment with Pre-Trained NLP Models (TextBlob)\n- Labels to Indexes (and Back Again)","c2ac3301":"### Basic Data Enrichment","c5d88b16":"### Checking Balance of the Data in Output Categories\n\nWe want to check the balance of the output column (Sentiment), such that we don't train a model that always predicts one output. This model might have a high accuracy, but we wouldn't have learned anything about trends in the data, other than the count in the most common sentiment. It might be tempting to think about balancing the test data too, but remember that data in the real world will be unlikely to come nicely balanced, and test data is analagous to real world data\n\n\nThe proportion of sentiments ranges from about 0.15 to 0.3, which is generally good balance, such that we are unlikely to see a scenario in which only one class is predicted. We will however be looking out for if our training accuracy forms a plateau at about 0.3, which could be indicative of this problem.","3651fa46":"We hit just over 80% accuracy which isn't bad! We don't predict Irrelevant sentiment quite as well, but the accuracy on this is still very pleasing!","218b5aae":"### Training\n\nNext, we fit this model with our data, using backpropagation, for 30 epochs. We can view the increase in accuracy of the model through the different epochs, on both the training and test dataset.","1cffc298":"### Enrichment with Pre-Trained Models (TextBlob)\n\nHere, we use the built-in functionality of TextBlob to add dimensionality to the data, by using it to analyse the text of the \"Tweet_Content\" column, and storing the outputs in a new column. This pre-trained model allows us to easily analyse strings of text, without any of the more involved NLP methods. Though we also plan to use more methods for NLP, this step can be a good to see if this model picks up on areas that our created one does not.","8b7a8a37":"### Automated Data Exploration with Pandas-Profiling\n\nPandas-profiling is a library used to automatically explore data. This gives us a good overview of the dataset, which we can use to inform our later work.","d295a8cb":"### Model Definition and Training\n\nThis time, we train with all of the available training data"}}