{"cell_type":{"cf475fed":"code","a36d7148":"code","ba76d777":"code","90e22d44":"code","496fef37":"code","a0eff4ae":"code","c79f0216":"code","0a8c074e":"code","ece2a64a":"code","7028ee8f":"code","c9fa008d":"code","ff67463b":"code","2103b1b3":"code","d10cd674":"code","ed0aa151":"code","e15c768c":"code","6ae79891":"code","254b75aa":"code","f012b5be":"code","dd54c562":"code","4ca74b46":"code","5289f82e":"code","b53dd3a8":"code","f70b1207":"code","f72beab8":"code","11a9af91":"code","ce0a8b01":"code","e2b6538e":"code","72b13032":"code","fc8c88fc":"code","b9cf03b6":"code","646e0c3c":"code","684beb17":"code","eb158db8":"code","9acc37bd":"code","4f9a1f5d":"code","67f37249":"markdown","b0d28aff":"markdown","829bd8c2":"markdown","2138988b":"markdown","552ae791":"markdown","03098c08":"markdown","94e72bea":"markdown","2600081d":"markdown","da7e6031":"markdown","a44117d0":"markdown","9fcac357":"markdown","e2475172":"markdown","0d7976a7":"markdown","c52085a8":"markdown","e3cf7020":"markdown","39caa0df":"markdown","f66fb03e":"markdown","403cd92f":"markdown","6f561f47":"markdown","7c0c570a":"markdown","71a82f47":"markdown"},"source":{"cf475fed":"!pip install -q forgebox","a36d7148":"from forgebox.imports import *\nfrom forgebox.df import PandasDisplay\n\nfrom datetime import datetime\nfrom typing import List, Tuple\nimport random\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom forgebox.html import DOM\nfrom forgebox.images.widgets import image_dom\nfrom itertools import chain\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ba76d777":"DATA = Path(\"\/kaggle\/input\/us-election-2020-tweets\/\")\n\ntrump_df = pd.read_csv(\n    DATA\/\"hashtag_donaldtrump.csv\",\n    lineterminator='\\n', error_bad_lines=False)\nbiden_df = pd.read_csv(DATA\/\"hashtag_joebiden.csv\", error_bad_lines=False)\n# Remove the error lines (last column nan)\ntrump_df = trump_df[~trump_df.collected_at.isna()]\nbiden_df = biden_df[~biden_df.collected_at.isna()]","90e22d44":"trump_df.columns","496fef37":"us_countries = ['United States of America','United States']\n\ndef compare_vc(col, filter_query=\"tweet_id!=''\"):\n    \"\"\"\n    Compare value counts of 2 candidate\n    \"\"\"\n    DOM(f\"Value counts under 2 candidates on column {col.upper()}\", \"h3\")()\n    return biden_df.query(filter_query).vc(col).rename(columns={col: \"biden\"}).join(\n        trump_df.query(filter_query).vc(col).rename(columns={col: \"trump\"})\n    ).head(20)","a0eff4ae":"compare_vc(\"country\")","c79f0216":"compare_vc(\"state\",f\"(country in {us_countries})\")","0a8c074e":"compare_vc(\"source\")","ece2a64a":"all_df = pd.concat([trump_df, biden_df])\n\nall_df = all_df.query(\"retweet_count>2\").reset_index(drop=True)\n\ndef replace_us(x): return \"US\" if x in us_countries else x\n\nall_df[\"country\"]= all_df[\"country\"].apply(replace_us)","7028ee8f":"%%time\nimport re\n\ndef find_at(text):\n    return re.findall(r\"@(\\w+)\",text)\n\ndef find_hashtag(text):\n    return re.findall(r\"#(\\w+)\",text)\n\nall_df[\"at_s\"] = all_df.tweet.apply(find_at)\nall_df[\"hash_tags\"] = all_df.tweet.apply(find_hashtag)","c9fa008d":"p_display=PandasDisplay(max_colwidth = 0,max_rows=120)","ff67463b":"all_hashtags = chain(*all_df[\"hash_tags\"])\nhashtag_stat = pd.DataFrame({\"hashtag\":all_hashtags}).vc(\"hashtag\")\nwith p_display:\n    display(hashtag_stat.head(100))","2103b1b3":"def start_review(hashtag_stat):\n    gen = iter(hashtag_stat.index)\n    def next_ht():\n        \"\"\"\n        Review hashtags one by one\n        \"\"\"\n        ht = next(gen)\n        DOM(f\"#{ht}\",\"h3\")()\n        has_ht = all_df[\"hash_tags\"].apply(lambda x: ht in x)\n        total = has_ht.sum()\n        DOM(f\"Total count: {total}, {int(total*100\/len(all_df))}%\",\"h4\")()\n        frame = DOM(\"\",\"div\",{\"class\":\"row\"})\n        for tweet in all_df[has_ht].sample(12).tweet:\n            frame.append(DOM(tweet.replace(ht, f\"<span class='text-danger'>{ht}<\/span>\"),\n                             \"div\",{\"class\":\"col-sm-4\"}))\n        frame()\n    return next_ht","d10cd674":"PRO_TRUMP = [\"Trump2020\",\"MAGA\",\"HunterBiden\", \"China\", \"MAGA2020\", \"KAG\", \"Trump2020Landslide\", \"Trump2020LandslideVictory\",\n             \"TRUMP2020ToSaveAmerica\",\"maga\", \"HunterBidenLaptop\", \"BidenCrimeFamiily\", \"PresidentTrump\",\n             \"ccp\",\"hunter\", \"HunterBidenEmails\", \"4MoreYears\",\"TrumpPence2020\"\n            ]\nPRO_BIDEN = [ \"TrumpOut\", \"TrumpGenocide\", \"Biden2020\", \"TrumpMeltdown\",\"BlueWave\",\"BidenHarris2020\", \"BidenHaris2020\",\n             \"ByeByeTrump\", \"TrumpIsLosing\",\"Resist\", \"VoteHimOut\", \"PresidentElectJoe\", \"Truth\",\n             \"TrumpVirus\",\"VoteBidenHarris2020\", \"VoteBlue\",\"CountEveryVote\", \"BidenPresident\",\n             \"BidenHarrisToSaveAmerica\",\"TrumpOut\", \"TrumpCrimeFamily\", \"BidenHarrisLandslide2020\",\n             \"TrumpIsANationalDisgrace\",\"TrumpCollapse\",\"PresidentElect\", \"ByeByeTrump\",\"VoteBidenHarrisToSaveAmerica\",\n             \"VoteBlueToSaveAmerica\", \"VoteBiden\", \"DumpTrump\",\"JoeBiden2020\"\n            ]","ed0aa151":"next_ht = start_review(hashtag_stat)","e15c768c":"next_ht()","6ae79891":"with p_display:\n    display(\n        all_df[[\"tweet\", \"user_name\", \"retweet_count\", \"at_s\", \"hash_tags\"]].sample(20))","254b75aa":"class HashTagGraph(Dataset):\n    @classmethod\n    def from_df(cls, df: pd.DataFrame):\n        # count unique hash tag numbers in a row\n        df[\"ht_count\"] =  df.hash_tags.apply(lambda x:len(set(x)))\n        # we use the lower case\n        df[\"hash_tags_lower\"] = df.hash_tags.apply(lambda x:list(i.lower() for i in x))\n        # we use the group with more than 1 hash tag\n        hashtag_groups = list(df.query(\"ht_count>1\")[\"hash_tags_lower\"])\n        return cls(hashtag_groups)\n        \n    def __init__(self, hashtag_groups: List[str]):\n        self.hashtag_groups = hashtag_groups\n        self.all_hashtags = list(chain(*self.hashtag_groups))\n        self.hashtag_stat = pd.DataFrame(\n            pd.DataFrame({\"hashtags\": self.all_hashtags})[\"hashtags\"].value_counts()\n        )\n        self.total_pool = np.arange(len(self.hashtag_stat))\n        self.i2c = np.array(self.hashtag_stat.index)\n        self.c2i = dict((v,k) for k,v in enumerate(self.hashtag_stat.index))\n        \n    def __len__(self): return len(self.hashtag_groups)\n        \n    def __getitem__(self, idx: int) -> Tuple[int]:\n        group = self.hashtag_groups[idx]\n        choices = random.sample(group, 2 )\n        return tuple([\n            self.c2i[choices[0]],\n            self.c2i[choices[1]],\n            random.choice(self.total_pool),\n        ])","f012b5be":"ds = HashTagGraph.from_df(all_df)\nds[5]","dd54c562":"def get_dl(df, batch_size=256, num_workers=4):\n    ds = HashTagGraph.from_df(df)\n    return DataLoader(\n        ds, shuffle=True,\n        batch_size=batch_size,\n        num_workers=num_workers)\n\ndef get_gen(df, **kwargs):\n    return iter(get_dl(df, **kwargs))","4ca74b46":"gen = get_gen(all_df)\n\nfor i in range(30):\n    x1,x2,z = next(gen)\nprint(\",\".join(map(str,map(lambda x:x.shape, [x1, x2, z]))))","5289f82e":"import pytorch_lightning as pl\npl.__version__","b53dd3a8":"class GraphData(pl.LightningDataModule):\n    def __init__(self, df, batch_size=256):\n        super().__init__()\n        self.df=df\n        self.batch_size=batch_size\n        self.ds = HashTagGraph.from_df(self.df)\n        self.c2i=ds.c2i\n        self.i2c=ds.i2c\n        \n    def train_dataloader(self):\n        return DataLoader(self.ds, shuffle=True, batch_size=self.batch_size, num_workers=4)","f70b1207":"graph_data = GraphData(all_df)\nnum_cates = len(graph_data.i2c)\nnum_cates","f72beab8":"class Graph(pl.LightningModule):\n    def __init__(\n        self,\n        num_cates: int=num_cates,\n        margin: float=32.\n    ):\n        super().__init__()\n        self.num_cates = num_cates\n        self.emb = nn.Embedding(num_cates, 32)\n        self.crit = nn.CosineEmbeddingLoss(margin)\n        \n    def forward(\n        self, x1, x2, z):\n        x_1 = torch.cat([x1,x1])\n        x_2 = torch.cat([x2,z])\n        y = torch.cat([\n            torch.ones_like(x2),\n            -torch.zeros_like(z),\n        ])\n        return self.crit(self.emb(x_1), self.emb(x_2), y)\n        \n    def configure_optimizers(self): return torch.optim.Adam(self.parameters())\n        \n    def training_step(self, batch, batch_idx):\n        x1, x2, z = batch\n        loss = self(x1, x2, z)\n        return {\"loss\":loss}\n    \ngraph = Graph()","11a9af91":"trainer = pl.Trainer(max_epochs=40)","ce0a8b01":"trainer.fit(graph, graph_data)","e2b6538e":"vecs = graph.emb.weight.data.numpy()\nvecs.shape","72b13032":"class CosineSim:\n    def __init__(self, names: List[str], vecs: np.array, top_k: int=20):\n        self.names = np.array(names)\n        self.n2i = dict((v,k) for k,v in enumerate(self.names))\n        self.top_k = top_k\n        self.vecs = vecs\n        self.l2 = np.sqrt(np.power(vecs,2).sum(-1))\n        self.vecs_normed = vecs\/(self.l2[:,None])\n        \n    def __repr__(self):return f\"Cosine Similarity Engine:\\n\\t\"+\",\".join(self.names[:10])+\",...\"\n    \n    def __call__(self, name):\n        i = self.n2i.get(name)\n        \n        if i is None: raise ValueError(f\"[ERROR] name '{name}' not found\")\n        \n        sim_map = (self.vecs_normed[i,None]*self.vecs_normed).sum(-1)\n        df = pd.DataFrame({\"names\":self.names,\"sim\":sim_map}).sort_values(by = \"sim\", ascending=False)\n        return df.head(self.top_k)","fc8c88fc":"CosineSim(graph_data.ds.hashtag_stat.index, vecs)(\"trump2020\")","b9cf03b6":"top_hashtags = graph_data.ds.hashtag_stat.head(150)\ntop_ids = np.array(list(graph_data.ds.c2i[i] for i in top_hashtags.index))\n\ntsne=TSNE(2, metric=\"cosine\", )\nemb_coords = tsne.fit_transform(graph.emb.weight.data.numpy()[top_ids])\n\n# pca=PCA(2)\n# emb_coords = pca.fit_transform(graph.emb.weight.data.numpy()[top_ids])","646e0c3c":"pro_trump = list(map(lambda x:x.lower(),PRO_TRUMP))\npro_biden = list(map(lambda x:x.lower(),PRO_BIDEN))\n\ncolor_list =list((\n    \"1 pro_trump\" if i in pro_trump else \n    (\"0 pro_biden\" if i in pro_biden else\n     \"2 no_preference\")) for i in top_hashtags.index)\nfig = px.scatter(\n    emb_coords,\n    x=0, y=1,\n    hover_name=top_hashtags.index,\n    color=color_list,\n    color_discrete_map={\"0 pro_biden\":\"blue\",\"1 pro_trump\":\"red\" }\n)\nfig.update_traces(marker_size=8)\nfig.show()","684beb17":"hashtags = list(chain(*all_df[\"hash_tags_lower\"]))\nhashtag_stat = pd.DataFrame({\"hashtags\": hashtags}).vc(\"hashtags\")\n\nh2count = dict(zip(hashtag_stat.index, hashtag_stat.hashtags))","eb158db8":"def calc_owner(df, col):\n    keys = list(df.vc(col).index)[:20]\n    result = []\n    for k in keys:\n        tags = list(chain(*df.query(f\"{col}=='{k}'\").hash_tags_lower))\n        k_df = pd.DataFrame({\"ht\": tags}).vc(\"ht\").reset_index()\n        k_df[f\"{col}\"] = k\n        k_df = k_df.rename(columns={\"index\": \"hashtag\", \"ht\":\"ct\"})\n        result.append(k_df)\n    return_df = pd.concat(result)[[col, \"hashtag\", \"ct\", ]]\n    return_df[\"total\"] = return_df[\"hashtag\"].apply(lambda x:h2count[x])\n    return_df[\"ownership\"] = return_df[\"ct\"] \/ return_df[\"total\"]\n    return return_df\n\ndef clean_onwership(df, col):\n    col_df = calc_owner(df, col)\n    rt = []\n    wc_painter = WordCloud(\n            background_color=\"white\",\n            height=500, width=800\n            )\n    for k in col_df[col].unique():\n        k_df = col_df.query(f\"{col}=='{k}'\").query(f\"ct>3\")\n        k_df = k_df.sort_values(by = [\"ownership\",\"total\"], ascending=False).reset_index(drop=2).head(50)\n        wc = wc_painter.generate(\" \".join(k_df.hashtag))\n        DOM(f\"{col}:{k}\", \"h3\")()\n        image_dom(wc.to_image())()","9acc37bd":"clean_onwership(all_df, \"country\")","4f9a1f5d":"clean_onwership(all_df.query(\"country=='US'\"), \"state\")","67f37249":"## Find closest hashtag","b0d28aff":"## Display some samples","829bd8c2":"## Loss Function\n> Cosine Embedding Loss\n\n$\\text{loss}(x, y) =\n    \\begin{cases}\n    1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n    \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n    \\end{cases}\n$","2138988b":"But to be honest, the hashtag distribution among countries, states etc demonstrate nothing meaning full, we still don't know what kind of information is pro-trump, what is anti-trump","552ae791":"## All ats and hashtags","03098c08":"# Visualize manifold\n\n> We visualize the the TSNE of the top 150 mentioned hashtags, and marked the blue or red for clear preference of such hashtag\n\n> You can see the clustering effect of the graph isn't that impressive \ud83e\udd76, but some similar words are groupped up together","94e72bea":"# Word cloud hashtag by country","2600081d":"## Words Ownership","da7e6031":"## Frequent Hash Tag","a44117d0":"# How people hashtagging\n### During election 2020\n\n> A hack on [election tweets dataset](https:\/\/www.kaggle.com\/manchunhui\/us-election-2020-tweets), please upvote the dataset for his dedicated effort to collect the data\n\n![data image](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/935914\/1582537\/4fc3f5fa3b371aad3c097e7fb892305b\/dataset-cover.jpg?t=2020-10-23-13-26-22)\n\nThe election is at some extent all over, but what worries me is the ever so separated perception of media, stories, even the basic definition of things that leads to a devided society.  As the problem described so vividly by the Netflix documentary [The Social Delimma](https:\/\/www.imdb.com\/title\/tt11464826\/). \n\nThis notebook tries to look closely at the diversity forms of tweet texts and how online society devides. And I hope the data science community can move toward the goal of bringing down segregation\/news integrity in one day","9fcac357":"I repeated run next_ht to review the tweet message \ud83d\ude2d","e2475172":"## Combine the tweet data into one","0d7976a7":"## Train a graph","c52085a8":"## Find the hash_tag(#) and the at(@)\n\nWith regex ```r\"@(\\w+)\"``` for @ and ```r\"#(\\w+)\"``` for hash tag","e3cf7020":"# Word cloud hashtag by state\n\n> See what's on eash states' mind","39caa0df":"Here are the top mentioned hashtags ","f66fb03e":"# Manual review\n> I mannually review 12 tweets sampled from each hashtag, of the top frequent hashtags. And put the hashtag into \"Pro Trump\" or \"Pro Biden\" list to the best of my judgement.(eg if some hashtag looks like pro-trump, but the sample tweets hashtagged this phrase isn't always pro trump, I won't label it pro trump)\n\n> To be clear I'm a coder\/ Chinese citizen who's hooked to the US 2020 election rallies news\/ tweets\/ posts\/ conspiracy theory non-stop, all of those from both sides, so my judgement might not be very accurate, besides, there are lots of non-english expression. I seriously hoped someone can try and relabel that","403cd92f":"All the tweets are either hash tagging D.J.Trump or J.R.Biden\uff0c saved separately in 2 csv files","6f561f47":"## Learn the graph","7c0c570a":"> Okay let's try words ownership by country, states, some words just appearing a lot, like trump biden, but they are in almost evey tweet, but some words are only owned by certain category of data","71a82f47":"Remove the error lines (last column nan)"}}