{"cell_type":{"1ea64ae8":"code","86abd2b3":"code","73018d81":"code","3e006605":"code","d16c059b":"code","27b7b041":"code","6e24f27e":"code","71c0cfa8":"code","535a96af":"code","1f2c924d":"code","616b61cd":"code","53f40c9d":"code","037ee2e9":"code","c44e30b9":"code","542fb943":"code","8fb30a3c":"code","7344a132":"code","53d89d1e":"code","1cd6af26":"code","b29e559d":"code","236dc17a":"code","535bf0be":"code","cf9ab1f3":"code","e3f6c30d":"code","d3d3861f":"code","0d8937ef":"code","a0014e12":"code","855263b1":"code","e3d611fb":"code","137686d8":"code","48dc5450":"code","167fe699":"code","4a5d1753":"code","924c26c4":"code","1b808c79":"code","cf2803fc":"code","0e4fb3cd":"code","c5ea2600":"markdown","3ba4c77d":"markdown","4107cfce":"markdown","adb24001":"markdown","d96665c8":"markdown","43faccbf":"markdown","b7e4b301":"markdown","8d121005":"markdown","7415a6e2":"markdown","50098bc6":"markdown","37631e26":"markdown","dce78bec":"markdown","5f1ed570":"markdown","b063b9ef":"markdown","709da994":"markdown"},"source":{"1ea64ae8":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom collections import Counter\nfrom random import seed\nfrom math import sqrt\nfrom random import randrange\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","86abd2b3":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","73018d81":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","3e006605":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","d16c059b":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","27b7b041":"df15 = pd.read_csv('\/kaggle\/input\/players_15.csv', delimiter=',', encoding='utf8')\ndf15.dataframeName = 'players_15.csv'\n\ndf16 = pd.read_csv('\/kaggle\/input\/players_16.csv', delimiter=',', encoding='utf8')\ndf16.dataframeName = 'players_16.csv'\n\ndf17 = pd.read_csv('\/kaggle\/input\/players_17.csv', delimiter=',', encoding='utf8')\ndf17.dataframeName = 'players_17.csv'\n\ndf18 = pd.read_csv('\/kaggle\/input\/players_18.csv', delimiter=',', encoding='utf8')\ndf18.dataframeName = 'players_18.csv'\n\ndf19 = pd.read_csv('\/kaggle\/input\/players_19.csv', delimiter=',', encoding='utf8')\ndf19.dataframeName = 'players_19.csv'\n\ndf20 = pd.read_csv('\/kaggle\/input\/players_20.csv', delimiter=',', encoding='utf8')\ndf20.dataframeName = 'players_20.csv'","6e24f27e":"df = pd.concat([df15, df16, df17, df18, df19, df20])","71c0cfa8":"# remove duplicates and take the mean\ndf = df.groupby('short_name').mean().reset_index()\ndf.dataframeName = 'fifa_players'","535a96af":"df.head(5)","1f2c924d":"skills_df = df[['pace', 'shooting', 'passing', 'dribbling', 'overall']]\nskills_df.dataframeName = 'fifa_players'","616b61cd":"skills_df.dropna(inplace=True)","53f40c9d":"plotCorrelationMatrix(skills_df, 5)","037ee2e9":"x_df = df[['shooting', 'passing', 'dribbling', 'overall']]\nx_df = x_df.dropna()","c44e30b9":"high_iloc = x_df[(x_df['overall'] >= 80) & (x_df['overall'] < 100)].index.values\nmedium_iloc = x_df[(x_df['overall'] >= 50) & (x_df['overall'] < 80)].index.values\nlow_iloc = x_df[x_df['overall'] < 50].index.values","542fb943":"x_df.loc[high_iloc, 'overall'] = 2.0\nx_df.loc[medium_iloc, 'overall'] = 1.0\nx_df.loc[low_iloc, 'overall'] = 0.0","8fb30a3c":"shooting_mean = x_df['shooting'].mean()\nshooting_std = x_df['shooting'].std()\n\npassing_mean = x_df['passing'].mean()\npassing_std = x_df['passing'].std()\n\ndribbling_mean = x_df['dribbling'].mean()\ndribbling_std = x_df['dribbling'].std()","7344a132":"x_df['shooting'] = (x_df['shooting'] - shooting_mean) \/ shooting_std\nx_df['passing'] = (x_df['passing'] - passing_mean) \/ passing_std\nx_df['dribbling'] = (x_df['dribbling'] - dribbling_mean) \/ dribbling_std","53d89d1e":"def entropy(y):\n    hist = np.bincount(y)\n    ps = hist \/ len(y)\n    return -np.sum([p * np.log2(p) for p in ps if p > 0])","1cd6af26":"class Node:\n\n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\n    def is_leaf_node(self):\n        return self.value is not None","b29e559d":"class DecisionTree:\n\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n\n    def fit(self, X, y):\n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        self.root = self._grow_tree(X, y)\n\n    def predict(self, X):\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n\n    def _grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        n_labels = len(np.unique(y))\n\n        # stopping criteria\n        if (depth >= self.max_depth\n                or n_labels == 1\n                or n_samples < self.min_samples_split):\n            leaf_value = self._most_common_label(y)\n            return Node(value=leaf_value)\n\n        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n\n        # greedily select the best split according to information gain\n        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n        \n        # grow the children that result from the split\n        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n        return Node(best_feat, best_thresh, left, right)\n\n    def _best_criteria(self, X, y, feat_idxs):\n        best_gain = -1\n        split_idx, split_thresh = None, None\n        for feat_idx in feat_idxs:\n            X_column = X[:, feat_idx]\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                gain = self._information_gain(y, X_column, threshold)\n\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feat_idx\n                    split_thresh = threshold\n\n        return split_idx, split_thresh\n\n    def _information_gain(self, y, X_column, split_thresh):\n        # parent loss\n        parent_entropy = entropy(y)\n\n        # generate split\n        left_idxs, right_idxs = self._split(X_column, split_thresh)\n\n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 0\n\n        # compute the weighted avg. of the loss for the children\n        n = len(y)\n        n_l, n_r = len(left_idxs), len(right_idxs)\n        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n        child_entropy = (n_l \/ n) * e_l + (n_r \/ n) * e_r\n\n        # information gain is difference in loss before vs. after split\n        ig = parent_entropy - child_entropy\n        return ig\n\n    def _split(self, X_column, split_thresh):\n        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n        return left_idxs, right_idxs\n\n    def _traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n\n        if x[node.feature] <= node.threshold:\n            return self._traverse_tree(x, node.left)\n        return self._traverse_tree(x, node.right)\n\n    def _most_common_label(self, y):\n        counter = Counter(y)\n        most_common = counter.most_common(1)\n        if(len(most_common) > 0):\n            most_common = most_common[0][0]\n        else:\n            most_common = 0.0\n        return most_common   ","236dc17a":"def accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) \/ len(y_true)\n    return accuracy","535bf0be":"dataset = x_df.to_numpy()\n\nX = np.array(dataset[:,:-1], dtype=np.float64)\ny = np.array(dataset[:,-1], dtype=np.int64)","cf9ab1f3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)","e3f6c30d":"clf = DecisionTree(max_depth=10)\nclf.fit(X_train, y_train)\n    \ny_pred = clf.predict(X_test)\nacc = accuracy(y_test, y_pred)\n\nprint (\"Accuracy:\", acc)","d3d3861f":"def player_stats_normalized(row):\n    pace = (row[0] - pace_mean) \/ pace_std\n    shooting = (row[1] - shooting_mean) \/ shooting_std\n    passing = (row[2] - passing_mean) \/ passing_std\n    dribbling = (row[3] - dribbling_mean) \/ dribbling_std\n    \n    return [pace, shooting, passing, dribbling]","0d8937ef":"def expected_group(overall):\n    if(overall >= 80):\n        return 2.0\n    if(overall >=50):\n        return 1.0\n    else:\n        return 0.0","a0014e12":"def player_stats_by_name(name):\n    player_stats = df[['shooting', 'passing', 'dribbling', 'overall']].iloc[df[df['short_name'] == name].index[0]].to_numpy()\n    player_stats_norm = player_stats_normalized(player_stats)\n    player_overall = np.array([expected_group(player_stats[-1])])\n    return np.hstack((player_stats_norm, player_overall))","855263b1":"def predict_by_stats(stats):\n    predicted = clf.predict(stats[:-1].reshape(1,4))[0]\n    expected = stats[-1]\n    return float(predicted), expected","e3d611fb":"# use the decision tree to predict Messi group\nmessi_stats = player_stats_by_name('L. Messi')\npredicted, expected = predict_by_stats(messi_stats)\nprint(\"Predicted \", predicted, \" Expected \", expected)","137686d8":"players = np.array(df['short_name'].unique())\nnp.random.shuffle(players)","48dc5450":"players_overall = [player_stats_by_name(p) for p in players[:25]]","167fe699":"def remove_nan(players_name_overall):\n    indexes = []\n    result = None\n    for i, (name, stats) in enumerate(players_name_overall):\n        if(np.isnan(stats).any()):\n            indexes.append(i)\n    result = np.delete(players_name_overall, indexes, axis=0)\n    return result","4a5d1753":"players_name_overall = list(map(list, zip(players, players_overall)))\nplayers_name_overall = remove_nan(players_name_overall)","924c26c4":"players = players_name_overall[:,0]\nplayers_stats = players_name_overall[:,1]","1b808c79":"predicted = []\nexpected = []\nfor stats in players_stats:\n    pred, expec = predict_by_stats(stats)\n    predicted.append(pred)\n    expected.append(expec)","cf2803fc":"players_df_stats = df[df['short_name'].isin(players)][['short_name', 'shooting', 'passing', 'dribbling', 'overall']]\nplayers_df_predicted = pd.DataFrame(list(map(list, zip(players, predicted, expected))), columns=[\"short_name\", \"predicted\", \"expected\"])\ndf_final = pd.merge(players_df_stats, players_df_predicted, how='right', on='short_name')","0e4fb3cd":"pd.set_option('display.max_rows', df_final.shape[0]+1)\ndf_final","c5ea2600":"# List of content\n\n1. [Exploratory Analysis](#exploratory)\n1. [Implementation](#implementation)\n2. [Analyze Results](#results)\n3. [Random Players Test](#test)\n4. [Resources](#resources)","3ba4c77d":"### Let's check 1st file: \/kaggle\/input\/players_15.csv","4107cfce":"<a id=\"resources\"><\/a>\n# Resources\n\nhttps:\/\/www.youtube.com\/watch?v=Oq1cKjR8hNo","adb24001":"Let's take a quick look at what the data looks like:","d96665c8":"<a id=\"results\"><\/a>\n# Analyze results","43faccbf":"Looking at the correlation table,<br>\nFeatures(X_train): shooting, passing, dribbling.\nTarget(y_train): overall","b7e4b301":"Please don't forget to up-vote if you enjoy the reading of this notebook.<br>\nUp-votes are pure motivation into creative notebook creation.","8d121005":"## FIFA Decision Tree from scratch\nI'm going to use the FIFA player dataset in order to build a decision tree.<br>\nI will first analyse the data and than chose the features and targets.","7415a6e2":"<img align=left width='500px' src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/10\/Flag_of_FIFA.svg\/1024px-Flag_of_FIFA.svg.png' \/>","50098bc6":"<a id=\"exploratory\"><\/a>\n# Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","37631e26":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","dce78bec":"Now you're ready to read in the data and use the plotting functions to visualize the data.","5f1ed570":"<a id=\"test\"><\/a>\n# Random Players Test","b063b9ef":"<a id=\"implementation\"><\/a>\n# Implementation","709da994":"There are 7 csv files in the current version of the dataset:\n"}}