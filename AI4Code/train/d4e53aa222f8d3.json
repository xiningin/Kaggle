{"cell_type":{"31c6295a":"code","ab511dfb":"code","ac588e51":"code","ec8c6c38":"code","5f3f991c":"code","bc154ca4":"code","278e8f24":"code","0377a770":"code","18f53036":"code","68e389f4":"code","386c0ddf":"code","db9758d1":"code","10cede43":"code","78941d1a":"code","9c1a01a7":"code","092b9207":"code","c5b5d8dd":"code","234c712e":"code","056942bb":"code","8d8d4465":"code","4512adab":"code","8817052a":"code","d22352f6":"code","9284076a":"code","934b2a43":"code","f55b38a5":"code","7c2940a3":"markdown","2eac6226":"markdown","42807d90":"markdown","02d3b4e1":"markdown","3b02fe97":"markdown","01accf7b":"markdown","0cb8113d":"markdown","762213d0":"markdown","4bd70278":"markdown","86b997ab":"markdown","878915ac":"markdown","85062a96":"markdown","6a7fe7e1":"markdown"},"source":{"31c6295a":"import pandas as pd\nfrom collections import Counter\nimport numpy as np\nimport time\nimport operator\nimport re\nimport gc\n\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nnltk.download('stopwords')","ab511dfb":"TRAIN_DATA_PATH = '\/kaggle\/input\/quora-insincere-questions-classification\/train.csv'\nTEST_DATA_PATH = '\/kaggle\/input\/quora-insincere-questions-classification\/test.csv'","ac588e51":"train_data = pd.read_csv(TRAIN_DATA_PATH)\ntest_data = pd.read_csv(TEST_DATA_PATH)","ec8c6c38":"train_data.head()","5f3f991c":"test_data.head()","bc154ca4":"print(f'Train data shape: {train_data.shape}')\nprint(f'Test data shape: {test_data.shape}')","278e8f24":"value_counts = train_data['target'].value_counts()\nvalue_counts_percentage = train_data['target'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%'\npd.concat([value_counts, value_counts_percentage], axis=1, keys=['Counts', 'Percentage'])","0377a770":"train_data['question_text'][35]","18f53036":"STOPWORDS = nltk.corpus.stopwords.words('english')\n\ntokenizer = WordPunctTokenizer()\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text)\n    return [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n\ndef remove_punctuation(text):\n    return \"\".join([i for i in text if i not in punctuation])\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\n\ndef preprocess_data(data):\n    # Lower Casing\n    data['preprocessed_text'] = data['question_text'].apply(lambda x: x.lower())\n\n    # Remove Punctuation\n    data['preprocessed_text'] = data['preprocessed_text'].apply(lambda x: remove_punctuation(x))\n\n    # Remove Stopwords\n    data['preprocessed_text'] = data['preprocessed_text'].apply(lambda x: remove_stopwords(x))\n\n    # Tokenization\n    # data['preprocessed_text'] = data['preprocessed_text'].apply(lambda x: tokenizer.tokenize(x))\n\n    # Lemmitization\n    # data['preprocessed_text'] = data['preprocessed_text'].apply(lambda x: lemmatize_words(x))","68e389f4":"%%time\npreprocess_data(train_data)\npreprocess_data(test_data)\n\ntrain_data.head()","386c0ddf":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef vocab_to_integer(vocab):\n    return {word: ii for ii, word in enumerate(vocab, 1)}","db9758d1":"%%time\nall_questions = pd.concat([train_data['preprocessed_text'], test_data['preprocessed_text']])\nfinal_vocab = build_vocab(all_questions)\nword_to_idx = vocab_to_integer(final_vocab)","10cede43":"vocab_original = build_vocab(pd.concat([train_data['question_text'], test_data['question_text']]))","78941d1a":"hparam = {}\nhparam['VOCAB_SIZE'] = len(final_vocab) + 1\nhparam['PAD_LENGTH'] = 77\nhparam['MINIBATCH_SIZE'] = 512\nhparam['LEARNING_RATE'] = 1e-3\nhparam['EPOCHS'] = 4\nhparam['LSTM_HIDDEN_SIZE'] = 128\nhparam['WORD_EMB_DIM'] = 0\nhparam['KFOLDS'] = 3","9c1a01a7":"def load_embed(file):\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n    \n    if file.split('\/')[-1] == 'wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o) > 100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","092b9207":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n    print('{} known words, {} unique'.format(nb_known_words, len(known_words)))\n    print('{} unknown words, {} unique'.format(nb_unknown_words, len(unknown_words)))\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    return unknown_words","c5b5d8dd":"def create_emb_matrix(nb_words, embed_size):\n    # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 (\u0441\u043b\u043e\u0432\u0430, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432, \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0432 \u0432\u0438\u0434\u0435 \u043d\u0443\u043b\u0435\u0432\u043e\u0433\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0430)\n    return np.zeros((nb_words, embed_size), dtype=np.float32)\n\ndef fill_emb_matrix(word_idx, emb_matrix, emb_index):\n    for word, i in word_idx:\n        emb_vector = emb_index.get(word)\n        if emb_vector is not None:\n            emb_matrix[i] = emb_vector\n    return emb_matrix\n\ndef add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","234c712e":"import zipfile\n\nz= zipfile.ZipFile('..\/input\/quora-insincere-questions-classification\/embeddings.zip')\nz.extractall()","056942bb":"_glove = '.\/glove.840B.300d\/glove.840B.300d.txt'\n_paragram =  '.\/paragram_300_sl999\/paragram_300_sl999.txt'\n_wiki_news = '.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n_google_news = '.\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\n\nembeddings = [{'name': 'glove', 'path': _glove},\n              {'name': 'paragram', 'path': _paragram},\n              {'name': 'fasttext', 'path': _wiki_news}]","8d8d4465":"%%time\n\nconc_embedding = None\nword_index = word_to_idx\nnb_words = min(hparam['VOCAB_SIZE'], len(word_index) + 1)\nhparam['VOCAB_SIZE'] = nb_words\nprint(hparam['VOCAB_SIZE'], len(word_index) + 1)\nprint(f\"Got a vocab size of {nb_words} number of words\")\n\nfor embedding in embeddings:\n    emb_name = embedding['name']\n    emb_path = embedding['path']\n    print(\"Running procedure on {}\".format(emb_name))\n    \n    # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438\n    print(\"Loading {}\".format(emb_name))\n    emb_index = load_embed(emb_path)\n    \n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u0432 \u043d\u0438\u0436\u043d\u0435\u043c \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0435\n    print(\"Adding lowercase to {}\".format(emb_name))\n    add_lower(emb_index, vocab_original)\n    \n    \n    _ = check_coverage(final_vocab, emb_index)\n    \n    emb_size = 300\n    hparam['WORD_EMB_DIM'] += emb_size\n    \n    # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 \u0444\u043e\u0440\u043c\u0430\u0442 word2vec\n    emb_matrix = create_emb_matrix(nb_words, emb_size)\n    print(emb_matrix.size)\n    print(emb_matrix.shape)\n    emb_matrix = fill_emb_matrix(word_index.items(), emb_matrix, emb_index)\n    \n    # \u041a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043d\u043e\u0432\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c\u0438\n    if conc_embedding is not None:\n        conc_embedding = np.concatenate((conc_embedding, emb_matrix), axis=1)\n        print(\"Concatenated! New shape: {}\".format(conc_embedding.shape))\n    else:\n        conc_embedding = emb_matrix\n    print(\"=================================================\")\n    \n    del emb_matrix, emb_index, emb_name, emb_path, emb_size\n    import gc; gc.collect()","4512adab":"def embed_word_to_int(X, vocab_to_int):\n    embedded_X = []\n    for q in X:\n        tmp_X = []\n        for w in q.split():\n            tmp_X.append(vocab_to_int[w])\n        embedded_X.append(tmp_X)\n    return embedded_X\n\n# \u0421\u0442\u0430\u0432\u0438\u043c \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0441\u043b\u043e\u0432\u0443 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0435 \u0446\u0435\u043b\u043e\u0435 \u0447\u0438\u0441\u043b\u043e\nX_train = embed_word_to_int(train_data['preprocessed_text'].values, word_to_idx)\nX_test = embed_word_to_int(test_data['preprocessed_text'].values, word_to_idx)\n\npad_length = hparam['PAD_LENGTH']\n\n# \u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a \u0435\u0434\u0438\u043d\u043e\u043c\u0443 \u0440\u0430\u0437\u043c\u0435\u0440\u0443\nX_train_pad = pad_sequences(X_train, maxlen=pad_length, padding='pre', truncating='pre')\nX_test_pad = pad_sequences(X_test, maxlen=pad_length, padding='pre', truncating='pre')\n\nprint(train_data['preprocessed_text'][25])\nprint(X_train[25])\nprint(X_train_pad[25])","8817052a":"def train_val_pred(dataset, hparam, embedding_matrix):\n    \n    # \u0414\u043e\u0441\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\n    X_train = dataset['X_train']\n    y_train = dataset['y_train']\n    X_val = dataset['X_val']\n    y_val = dataset['y_val']\n    X_test = dataset['X_test']\n\n    # \u0414\u043e\u0441\u0442\u0430\u0435\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\n    VOCAB_SIZE = hparam['VOCAB_SIZE']\n    PAD_LENGTH = hparam['PAD_LENGTH']\n    MINIBATCH_SIZE = hparam['MINIBATCH_SIZE']\n    LEARNING_RATE = hparam['LEARNING_RATE']\n    EPOCHS = hparam['EPOCHS']\n    LSTM_HIDDEN_SIZE = hparam['LSTM_HIDDEN_SIZE']\n    WORD_EMB_DIM = hparam['WORD_EMB_DIM']\n    \n    # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c (2-x BiLSTM)\n    inp = Input(shape=(PAD_LENGTH,))\n    x = Embedding(VOCAB_SIZE, WORD_EMB_DIM, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\n    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    predictions = Dense(1, activation='sigmoid')(conc)\n    model = Model(inputs=inp, outputs=predictions)\n    adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    model.summary()\n    \n    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=MINIBATCH_SIZE, \n          validation_data = (X_val, y_val))\n    \n    \n    val_preds = model.predict(X_val, batch_size=MINIBATCH_SIZE, verbose=1)\n    best_f1 = -1\n    best_thresh = -1\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        f1 = metrics.f1_score(y_val, (val_preds > thresh).astype(int))\n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    print(\"Best f1 score = {} at thresh {}\".format(best_f1, best_thresh))\n    \n    # \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    test_preds = model.predict(X_test)\n    \n    del embedding_matrix, model, inp, x, adam\n    import gc; gc.collect()\n    \n    return test_preds, val_preds, best_thresh, best_f1","d22352f6":"kfold = StratifiedKFold(n_splits=hparam['KFOLDS'], shuffle=True, random_state=2019)","9284076a":"X = X_train_pad\ny = train_data['target'].values\n\nresults = []\n\nfor fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    print(f\"Training on {len(X_train)} and validating on {len(X_val)} number of words\")\n    \n    dataset = {'X_train': X_train, 'y_train': y_train,\n          'X_val': X_val, 'y_val': y_val,\n          'X_test': X_test_pad}\n    \n    test_preds, val_preds, thresh, f1 = train_val_pred(dataset, hparam, conc_embedding)\n    \n    print(\"len(test_preds) = {}, len(val_preds) = {}, thresh = {} at f1 = {}\".format(len(test_preds), \n                                                                                     len(val_preds), \n                                                                                     thresh, \n                                                                                     f1))\n    new_result = {'name': 'fold-' + str(fold), \n                  'test_preds': test_preds, \n                  'val_preds': val_preds, \n                  'thresh': thresh, \n                  'f1': f1}\n    results.append(new_result)\n    \n    import gc; gc.collect()","934b2a43":"print(\"Got {} number of results!\".format(len(results)))\navg_thresh = 0\nfor result in results:\n    print(\"{} gave f1 score {} with thresh {}\".format(result['name'], result['f1'], result['thresh']))\n    avg_thresh += result['thresh']\n\navg_thresh = avg_thresh \/ len(results)\nprint(\"Got an average threshold at {}\".format(avg_thresh))","f55b38a5":"print(\"Avg treshold {}\".format(avg_thresh))\n\nfactor = 1.0 \/ len(results)\npred_test_y = results[0]['test_preds'] * factor\n\nprint(\"Using factor: \", factor)\n\nfor i in range(1, len(results)):\n    pred_test_y += factor * results[i]['test_preds']\n    \n\npred_test_y_res = (pred_test_y > avg_thresh).astype(int)\n\nresults_dict = {'qid':test_data['qid'].values, 'prediction':[]}\n\nfor prediction in pred_test_y_res:\n    results_dict['prediction'].append(prediction[0])\n    \nprint(results_dict['qid'][:15])\nprint(results_dict['prediction'][:15])\n    \ndf = pd.DataFrame(data=results_dict)\ndf.to_csv('submission.csv', index=False)\nprint(\"Saved\")","7c2940a3":"\u0417\u0430\u0434\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 + \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b","2eac6226":"\u041d\u0438\u0436\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435\u0442 \u0432 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u0445.","42807d90":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0445 \u043a\u0443\u0441\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.","02d3b4e1":"# Loading Data","3b02fe97":"\u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a f-\u043c\u0435\u0440 \u0438 \u0442\u0440\u0435\u0448\u0445\u043e\u043b\u0434\u044b \u0434\u043b\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u0443\u0441\u043a\u043e\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.","01accf7b":"\u0421\u0442\u0440\u043e\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043c\u0430\u043f\u043f\u0438\u043c \u0441\u043b\u043e\u0432\u0430 \u043d\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0442\u0443\u044e\u0449\u0438\u0435 \u0438\u043d\u0434\u0435\u043a\u0441\u044b.","0cb8113d":"\u0414\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u0442\u043e\u0434 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438, \u043c\u0435\u0442\u0440\u0438\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 - f-\u043c\u0435\u0440\u0430","762213d0":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432.","4bd70278":"\u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0438\u0437 \u0430\u0440\u0445\u0438\u0432\u0430","86b997ab":"\u041f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043c \u0442\u0435\u043a\u0441\u0442: \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443, \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e \u0438 \u0441\u0442\u043e\u043f\u0432\u043e\u0440\u0434\u044b (\u0441\u043b\u043e\u0432\u0430, \u043d\u0435 \u043d\u0435\u0441\u0443\u0449\u0438\u0435 \u043e\u0441\u043e\u0431\u043e\u0433\u043e \u0441\u043c\u044b\u0441\u043b\u0430).","878915ac":"\u0421\u0442\u0440\u043e\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432.","85062a96":"# Prerocessing","6a7fe7e1":"\u041a\u043b\u0430\u0441\u0441\u044b \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b. \u0414\u043e\u043b\u044f \u043f\u0440\u043e\u0432\u043e\u043a\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 6.19% \u043e\u0442 \u043e\u0431\u0449\u0435\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432."}}