{"cell_type":{"f1d15086":"code","475505d3":"code","db6232b6":"code","c411e55e":"code","5fff4e37":"code","21c9ce8a":"code","e9407eaf":"code","25983642":"code","1e66facf":"code","29a55e06":"code","d56e6581":"code","24f8ef1f":"code","edc66e87":"code","1668e991":"code","13f26808":"code","d90fbb7b":"code","5379eb23":"code","cdad1d77":"code","5d8716fe":"code","23daa649":"code","f6a5d2a5":"code","907df016":"code","e3849de5":"code","12c203eb":"code","0705de27":"code","0e3a1e05":"code","bd184549":"code","837a4ba3":"code","89211117":"code","c6d87090":"code","5faf01a4":"code","58c51fb3":"code","81fe16b2":"code","4ccce0f6":"code","0dc729d8":"code","28c35d34":"code","6d173fa4":"code","b9110473":"code","5a7975f2":"code","6c51c172":"code","c5fbf8f6":"code","a88eefa3":"code","aade9413":"code","0bbd0343":"code","ef612110":"code","4bce9fd8":"code","e5375929":"code","5f21af3a":"code","ede298bf":"code","1e57c977":"code","5f754dcc":"code","77144596":"code","d19de313":"code","2fdb36ab":"code","b1f81bfa":"code","787f8a95":"code","67a106bd":"code","6ba3661b":"code","3e91e0b2":"code","60ba0ae6":"code","e145bf98":"code","b9522398":"code","653c6cf7":"code","47cc70f1":"code","a983c513":"code","32e9982c":"code","32922856":"code","17e1c560":"code","f8a03760":"code","766a94db":"code","cdb7a499":"code","b693074e":"code","721d0363":"code","18e8482c":"code","9cb75db4":"code","1227843f":"code","145816a1":"code","bdd77b42":"code","94b88f28":"code","aa2e35fa":"code","1f663437":"code","8932ea8a":"code","cdeaf532":"code","7305212f":"code","f426f74c":"code","689f9be9":"code","7057d035":"code","e0c0b86f":"code","0fe80b6c":"code","ca31e8a3":"code","e9ec90b8":"code","38985eca":"code","85cf0f32":"code","fe63ae50":"code","4d279d36":"code","822054dc":"markdown","19afba84":"markdown","64ae2802":"markdown","a6c896e7":"markdown","fd8349cf":"markdown","20125f9a":"markdown","404a157e":"markdown","87d3b1f0":"markdown","24c726c6":"markdown","928003de":"markdown","bd974346":"markdown","a5c83c6b":"markdown","f758b0d8":"markdown","e4013712":"markdown","888a79d6":"markdown","b0d85e8f":"markdown","ca004117":"markdown","c8edbdcc":"markdown","ac03ae9d":"markdown","c513a8cb":"markdown","b05f5808":"markdown","12f1a78e":"markdown","019d8a12":"markdown","78d5004d":"markdown","def7f7d4":"markdown","be890695":"markdown","a07ad481":"markdown","d433ef0f":"markdown","d11081c8":"markdown","b319654a":"markdown","31a516b3":"markdown","a145c593":"markdown","a5d78959":"markdown","48a3e003":"markdown","c6bb995d":"markdown","b667de2e":"markdown","d3fa6aea":"markdown","6b151fdf":"markdown","07ed7aca":"markdown","4e0d7426":"markdown","53f33ced":"markdown","a45fb5fa":"markdown","970cab0e":"markdown","c6c43f2d":"markdown","740807f3":"markdown","e4c85d9e":"markdown","d5946f60":"markdown","8fb3a692":"markdown","eb3a5814":"markdown","df56b5cd":"markdown","6349a410":"markdown"},"source":{"f1d15086":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport shutil\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import median\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\n\nfrom collections import Counter\n\nimport string\nimport re\nfrom nltk.corpus import stopwords\n\nimport scipy\nfrom scipy import hstack\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom math import sqrt\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, f_regression","475505d3":"import tracemalloc\nimport time","db6232b6":"tracemalloc.start()\n\nstart_time = time.time()\nsnapshot1 = tracemalloc.take_snapshot()","c411e55e":"# https:\/\/www.kaggle.com\/peterhurford\/lgb-and-fm-18th-place-0-40604\ndef split_cat(text):\n    try:\n        return text.split(\"\/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")","5fff4e37":"train = pd.read_csv('train.tsv', sep='\\t', \n                      dtype={'item_condition_id': 'category', 'shipping': 'category'}, \n                      converters={'category_name': split_cat})\ntest = pd.read_csv('test.tsv', sep='\\t', \n                     dtype={'item_condition_id': 'category', 'shipping': 'category'}, \n                     converters={'category_name': split_cat})","21c9ce8a":"print('Shape of train data: ', train.shape)\nprint('Shape of test data: ', test.shape)\ntrain.head(5)","e9407eaf":"train.isnull().any()","25983642":"test.isnull().any()","1e66facf":"# Split category_name by '\/' into subcategories and replace nulls with 'missing'\ntrain['gencat_name'] = train['category_name'].str.get(0).replace('', 'missing').astype('category')\ntrain['subcat1_name'] = train['category_name'].str.get(1).fillna('missing').astype('category')\ntrain['subcat2_name'] = train['category_name'].str.get(2).fillna('missing').astype('category')\ntrain.drop('category_name', axis=1, inplace=True)","29a55e06":"# Split category_name by '\/' into subcategories and replace nulls with 'missing'\ntest['gencat_name'] = test['category_name'].str.get(0).replace('', 'missing').astype('category')\ntest['subcat1_name'] = test['category_name'].str.get(1).fillna('missing').astype('category')\ntest['subcat2_name'] = test['category_name'].str.get(2).fillna('missing').astype('category')\ntest.drop('category_name', axis=1, inplace=True)","d56e6581":"train['item_description'].fillna('missing', inplace=True)\ntrain['brand_name'] = train['brand_name'].fillna('missing').astype('category')","24f8ef1f":"test['item_description'].fillna('missing', inplace=True)\ntest['brand_name'] = test['brand_name'].fillna('missing').astype('category')","edc66e87":"train[train.duplicated()]","1668e991":"train.isnull().any()","13f26808":"print('Removed {} rows' .format(len(train[train.price<=0])))\ntrain = train[train.price > 0].reset_index(drop=True)","d90fbb7b":"train.name.describe()","5379eb23":"train.item_condition_id.describe()","cdad1d77":"condition_count = Counter(list(train.item_condition_id))\nx, y = zip(*condition_count.most_common())\nplt.figure(figsize=[8,6])\nplt.bar(x, y, )\nfor i, val in enumerate(y):\n           plt.annotate(val, (x[i], y[i]), color='b')\nplt.xlabel('item condition')\nplt.ylabel('count')\nplt.grid(False, axis='x')\nplt.show()","5d8716fe":"train.brand_name.describe()","23daa649":"brand_count = Counter(list(train.brand_name.values))\nx, y = zip(*brand_count.most_common(15))\n\nplt.figure(figsize=[6,8])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Brand name')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","f6a5d2a5":"brand_missing = train[train.brand_name=='missing'].shape[0]\nprint('Brand name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(brand_missing, 100.0*brand_missing\/train.shape[0]))","907df016":"train.gencat_name.describe()","e3849de5":"gencat_count = Counter(list(train.gencat_name.values))\nx, y = zip(*gencat_count.most_common(15))\nplt.figure(figsize=[6,8])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('General category')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","12c203eb":"gencat_missing = train[train.gencat_name=='missing'].shape[0]\nprint('category name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(gencat_missing, 100.0*gencat_missing\/train.shape[0]))","0705de27":"train.subcat1_name.describe()","0e3a1e05":"subcat1_count = Counter(list(train.subcat1_name.values))\nx, y = zip(*subcat1_count.most_common(15))\nplt.figure(figsize=[6,10])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Sub-category1')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","bd184549":"subcat1_missing = train[train.subcat1_name=='missing'].shape[0]\nprint('subcategory1 name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(subcat1_missing, 100.0*subcat1_missing\/train.shape[0]))","837a4ba3":"train.subcat2_name.describe()","89211117":"subcat2_count = Counter(list(train.subcat2_name.values))\nx, y = zip(*subcat2_count.most_common(15))\nplt.figure(figsize=[6,10])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Sub-category2')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","c6d87090":"subcat2_missing = train[train.subcat2_name=='missing'].shape[0]\nprint('subcategory2 name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(subcat2_missing, 100.0*subcat2_missing\/train.shape[0]))","5faf01a4":"desc_missing = train[train.item_description=='missing'].shape[0]\nprint('item description is missing for {} datapoints, i.e. {:.5f} % of train data.' .format(desc_missing, 100.0*desc_missing\/train.shape[0]))","58c51fb3":"train[train.item_description=='missing']","81fe16b2":"sns.FacetGrid(train,size=6) \\\n    .map(sns.kdeplot,\"price\") \\\n    .add_legend();\nplt.title('price density distribution')\nplt.show();","4ccce0f6":"sns.boxplot(y='price', data=train, showfliers=False)\nplt.show()","0dc729d8":"for i in range(0, 100, 10):\n    var =train[\"price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","28c35d34":"for i in range(90, 100, 1):\n    var =train[\"price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint(\"100 percentile value is \",var[-1])","6d173fa4":"def preprocess_name(text_col):\n    preprocessed_names = []\n    for sentence in tqdm(text_col.values):\n        sent = sentence.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        preprocessed_names.append(sent.lower().strip())\n    return preprocessed_names\n\nstopwords = stopwords.words('english')\ndef preprocess_desc(text_col):\n    preprocessed_descs = []\n    for sentence in tqdm(text_col.values):\n        sent = sentence.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n        preprocessed_descs.append(sent.lower().strip())\n    return preprocessed_descs","b9110473":"train['preprocessed_name'] = preprocess_name(train['name'])\ntest['preprocessed_name'] = preprocess_name(test['name'])\n\ntrain['preprocessed_description'] = preprocess_desc(train['item_description'])\ntest['preprocessed_description'] = preprocess_desc(test['item_description'])","5a7975f2":"def clean_cat(cat_values):\n    '''takes categorical column values as arguments and returns list of cleaned categories'''\n    \n    catogories = list(cat_values)\n\n    cat_list = []\n    for i in tqdm(catogories):\n        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n        i = i.replace(' ','')\n        i = i.replace('&','_')\n        cat_list.append(i.strip())\n    \n    return cat_list ","6c51c172":"train['gencat_name'] = clean_cat(train['gencat_name'].values)\ntest['gencat_name'] = clean_cat(test['gencat_name'].values)\n\ntrain['subcat1_name'] = clean_cat(train['subcat1_name'].values)\ntest['subcat1_name'] = clean_cat(test['subcat1_name'].values)\n\ntrain['subcat2_name'] = clean_cat(train['subcat2_name'].values)\ntest['subcat2_name'] = clean_cat(test['subcat2_name'].values)","c5fbf8f6":"sns.set(style='whitegrid')\nplt.figure(figsize=(12,6))\nsns.boxplot(x='item_condition_id', y='price', data=train, showfliers=False)\nplt.title('item_condition-wise distribution of price')\nplt.show()","a88eefa3":"plt.figure(figsize=(15,6))\nsns.boxplot(y='price', x='gencat_name', data=train, showfliers=False)\nplt.xticks(rotation=45)\nplt.title('category-wise distribution of price')\nplt.show()","aade9413":"sns.barplot(y='gencat_name', x='price', data=train)\nplt.title('mean price of various categories')\nplt.show()","0bbd0343":"plt.figure(figsize=(10,25))\nsns.barplot(y='subcat1_name', x='price', data=train)\nplt.title('mean price of various subcategories')\nplt.show()","ef612110":"plt.figure(figsize=(10,25))\nsns.barplot(y='subcat1_name', x='price', data=train, estimator=median)\nplt.title('median price of various subcategories')\nplt.show()","4bce9fd8":"def get_name_first(name):\n    \n    name =  re.sub('[^A-Za-z0-9]+', ' ', name) .split()\n    if len(name):\n            return name[0].lower()\n    return ''\n        \n        \ntrain['name_first'] = train['name'].apply(get_name_first)\ntest['name_first'] = test['name'].apply(get_name_first)","e5375929":"def transform_test(base_col, feat_col):\n    '''\n    Returns feat_col column of test data by mapping from the values already calculated for the same column in train data\n    \n    Parameters:\n    \n    base_col: column based on which a transform(count, mean, median) has been applied\n    \n    feat_col: desired feature column after applying the transform\n    '''\n    #Create dictionary of feature values from train data\n    di = pd.Series(train[feat_col].values, index=train[base_col].values).to_dict()\n    \n    #Map test data using dictionary and fill NAs with 0\n    \n    if base_col == 'item_condition_id':\n        #No chance of NAs\n        return test[base_col].map(di).astype(float)\n        \n    return test[base_col].map(di).fillna(0)","5f21af3a":"train['name_first_count'] = train.groupby('name_first')['name_first'].transform('count')\ntest['name_first_count'] = transform_test('name_first', 'name_first_count')\n\ntrain['gencat_name_count'] = train.groupby('gencat_name')['gencat_name'].transform('count')\ntest['gencat_name_count'] = transform_test('gencat_name', 'gencat_name_count')\n\ntrain['subcat1_name_count'] = train.groupby('subcat1_name')['subcat1_name'].transform('count')\ntest['subcat1_name_count'] = transform_test('subcat1_name', 'subcat1_name_count')\n\ntrain['subcat2_name_count'] = train.groupby('subcat2_name')['subcat2_name'].transform('count')\ntest['subcat2_name_count'] = transform_test('subcat2_name', 'subcat2_name_count')\n\ntrain['brand_name_count'] = train.groupby('brand_name')['brand_name'].transform('count')\ntest['brand_name_count'] = transform_test('brand_name', 'brand_name_count')","ede298bf":"train['NameLower'] = train.name.str.count('[a-z]')\ntrain['DescriptionLower'] = train.item_description.str.count('[a-z]')\ntrain['NameUpper'] = train.name.str.count('[A-Z]')\ntrain['DescriptionUpper'] = train.item_description.str.count('[A-Z]')\ntrain['name_len'] = train['name'].apply(lambda x: len(x))\ntrain['des_len'] = train['item_description'].apply(lambda x: len(x))\ntrain['name_desc_len_ratio'] = train['name_len']\/train['des_len']\ntrain['desc_word_count'] = train['item_description'].apply(lambda x: len(x.split()))\ntrain['mean_des'] = train['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x)) * 10\ntrain['name_word_count'] = train['name'].apply(lambda x: len(x.split()))\ntrain['mean_name'] = train['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x))  * 10\ntrain['desc_letters_per_word'] = train['des_len'] \/ train['desc_word_count']\ntrain['name_letters_per_word'] = train['name_len'] \/ train['name_word_count']\ntrain['NameLowerRatio'] = train['NameLower'] \/ train['name_len']\ntrain['DescriptionLowerRatio'] = train['DescriptionLower'] \/ train['des_len']\ntrain['NameUpperRatio'] = train['NameUpper'] \/ train['name_len']\ntrain['DescriptionUpperRatio'] = train['DescriptionUpper'] \/ train['des_len']","1e57c977":"test['NameLower'] = test.name.str.count('[a-z]')\ntest['DescriptionLower'] = test.item_description.str.count('[a-z]')\ntest['NameUpper'] = test.name.str.count('[A-Z]')\ntest['DescriptionUpper'] = test.item_description.str.count('[A-Z]')\ntest['name_len'] = test['name'].apply(lambda x: len(x))\ntest['des_len'] = test['item_description'].apply(lambda x: len(x))\ntest['name_desc_len_ratio'] = test['name_len']\/test['des_len']\ntest['desc_word_count'] = test['item_description'].apply(lambda x: len(x.split()))\ntest['mean_des'] = test['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x)) * 10\ntest['name_word_count'] = test['name'].apply(lambda x: len(x.split()))\ntest['mean_name'] = test['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x))  * 10\ntest['desc_letters_per_word'] = test['des_len'] \/ test['desc_word_count']\ntest['name_letters_per_word'] = test['name_len'] \/ test['name_word_count']\ntest['NameLowerRatio'] = test['NameLower'] \/ test['name_len']\ntest['DescriptionLowerRatio'] = test['DescriptionLower'] \/ test['des_len']\ntest['NameUpperRatio'] = test['NameUpper'] \/ test['name_len']\ntest['DescriptionUpperRatio'] = test['DescriptionUpper'] \/ test['des_len']","5f754dcc":"from nltk.corpus import stopwords\n\nRE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\ns_words = {x: 1 for x in stopwords.words('english')} #converting to dictionary for fast look up\nnon_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')","77144596":"#https:\/\/www.kaggle.com\/peterhurford\/lgb-and-fm-18th-place-0-40604\n\ndef to_number(x):\n    try:\n        if not x.isdigit():\n            return 0\n        x = int(x)\n        if x > 100:\n            return 100\n        else:\n            return x\n    except:\n        return 0","d19de313":"train['NamePunctCount'] = train.name.str.count(RE_PUNCTUATION)\ntrain['DescriptionPunctCount'] = train.item_description.str.count(RE_PUNCTUATION)\ntrain['NamePunctCountRatio'] = train['NamePunctCount'] \/ train['name_word_count']\ntrain['DescriptionPunctCountRatio'] = train['DescriptionPunctCount'] \/ train['desc_word_count']\ntrain['NameDigitCount'] = train.name.str.count('[0-9]')\ntrain['DescriptionDigitCount'] = train.item_description.str.count('[0-9]')\ntrain['NameDigitCountRatio'] = train['NameDigitCount'] \/ train['name_word_count']\ntrain['DescriptionDigitCountRatio'] = train['DescriptionDigitCount']\/train['desc_word_count']\ntrain['stopword_ratio_desc'] = train['item_description'].apply(lambda x: len([w for w in x.split() if w in s_words])) \/ train['desc_word_count']\ntrain['num_sum'] = train['item_description'].apply(lambda x: sum([to_number(s) for s in x.split()])) \ntrain['weird_characters_desc'] = train['item_description'].str.count(non_alphanumpunct)\ntrain['weird_characters_name'] = train['name'].str.count(non_alphanumpunct)\ntrain['prices_count'] = train['item_description'].str.count('[rm]')\ntrain['price_in_name'] = train['item_description'].str.contains('[rm]', regex=False).astype('category')","2fdb36ab":"test['NamePunctCount'] = test.name.str.count(RE_PUNCTUATION)\ntest['DescriptionPunctCount'] = test.item_description.str.count(RE_PUNCTUATION)\ntest['NamePunctCountRatio'] = test['NamePunctCount'] \/ test['name_word_count']\ntest['DescriptionPunctCountRatio'] = test['DescriptionPunctCount'] \/ test['desc_word_count']\ntest['NameDigitCount'] = test.name.str.count('[0-9]')\ntest['DescriptionDigitCount'] = test.item_description.str.count('[0-9]')\ntest['NameDigitCountRatio'] = test['NameDigitCount'] \/ test['name_word_count']\ntest['DescriptionDigitCountRatio'] = test['DescriptionDigitCount']\/test['desc_word_count']\ntest['stopword_ratio_desc'] = test['item_description'].apply(lambda x: len([w for w in x.split() if w in s_words])) \/ test['desc_word_count']\ntest['num_sum'] = test['item_description'].apply(lambda x: sum([to_number(s) for s in x.split()])) \ntest['weird_characters_desc'] = test['item_description'].str.count(non_alphanumpunct)\ntest['weird_characters_name'] = test['name'].str.count(non_alphanumpunct)\ntest['prices_count'] = test['item_description'].str.count('[rm]')\ntest['price_in_name'] = test['item_description'].str.contains('[rm]', regex=False).astype('category')","b1f81bfa":"train['brand_mean_price'] = train.groupby('brand_name')['price'].transform('mean')\ntest['brand_mean_price'] = transform_test('brand_name', 'brand_mean_price')\n\ntrain['name_mean_price'] = train.groupby('name_first')['price'].transform('mean')\ntest['name_mean_price'] = transform_test('name_first', 'name_mean_price')\n\ntrain['gencat_mean_price'] = train.groupby('gencat_name')['price'].transform('mean')\ntest['gencat_mean_price'] = transform_test('gencat_name', 'gencat_mean_price')\n\ntrain['subcat1_mean_price'] = train.groupby('subcat1_name')['price'].transform('mean')\ntest['subcat1_mean_price'] = transform_test('subcat1_name', 'subcat1_mean_price')\n\ntrain['subcat2_mean_price'] = train.groupby('subcat2_name')['price'].transform('mean')\ntest['subcat2_mean_price'] = transform_test('subcat2_name', 'subcat2_mean_price')\n\ntrain['condition_mean_price'] = train.groupby('item_condition_id')['price'].transform('mean')\ntest['condition_mean_price'] = transform_test('item_condition_id', 'condition_mean_price')","787f8a95":"train['brand_median_price'] = train.groupby('brand_name')['price'].transform('median')\ntest['brand_median_price'] = transform_test('brand_name', 'brand_median_price')\n\ntrain['name_median_price'] = train.groupby('name_first')['price'].transform('median')\ntest['name_median_price'] = transform_test('name_first', 'name_median_price')\n\ntrain['gencat_median_price'] = train.groupby('gencat_name')['price'].transform('median')\ntest['gencat_median_price'] = transform_test('gencat_name', 'gencat_median_price')\n\ntrain['subcat1_median_price'] = train.groupby('subcat1_name')['price'].transform('median')\ntest['subcat1_median_price'] = transform_test('subcat1_name', 'subcat1_median_price')\n\ntrain['subcat2_median_price'] = train.groupby('subcat2_name')['price'].transform('median')\ntest['subcat2_median_price'] = transform_test('subcat2_name', 'subcat2_median_price')\n\ntrain['condition_median_price'] = train.groupby('item_condition_id')['price'].transform('median')\ntest['condition_median_price'] = transform_test('item_condition_id', 'condition_median_price')","67a106bd":"train.drop(['name', 'item_description'], axis=1, inplace=True)\ntest.drop(['name', 'item_description'], axis=1, inplace=True)","6ba3661b":"print(train.shape, test.shape)","3e91e0b2":"plt.figure(figsize=(18,18))\n\nplt.subplot(3,3,1)\nsns.regplot(x='brand_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('brand_mean_price vs price(target)')\n\nplt.subplot(3,3,2)\nsns.regplot(x='gencat_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('category_mean_price vs price(target)')\n\nplt.subplot(3,3,3)\nsns.regplot(x='subcat1_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_mean_price vs price(target)')\n\nplt.subplot(3,3,4)\nsns.regplot(x='subcat2_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_mean_price vs price(target)')\n\nplt.subplot(3,3,5)\nsns.regplot(x='condition_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('condition_mean_price vs price(target)')\n\nplt.subplot(3,3,6)\nsns.regplot(x='brand_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('brand_median_price vs price(target)')\n\nplt.subplot(3,3,7)\nsns.regplot(x='subcat1_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_median_price vs price(target)')\n\nplt.subplot(3,3,8)\nsns.regplot(x='subcat2_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_median_price vs price(target)')\n\nplt.subplot(3,3,9)\nsns.regplot(x='name_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('name_median_price vs price(target)')\n            \nplt.show()","60ba0ae6":"n_rows = train.shape[0]\ntrain = train[train.preprocessed_name != ''].reset_index(drop=True)\n\nprint('Dropped {} rows'.format(n_rows - train.shape[0]))","e145bf98":"n_rows = train.shape[0]\ntrain = train[train.preprocessed_description != ''].reset_index(drop=True)\n\nprint('Dropped {} rows'.format(n_rows - train.shape[0]))\n\nprint('Shape of train data: ', train.shape)","b9522398":"from sklearn.model_selection import train_test_split\n\ny_tr = np.log1p(train['price'])\ntrain.drop(['price'], axis=1, inplace=True)\n\ntrain_df, cv_df , y_train, y_cv = train_test_split(train, y_tr, test_size=0.1, random_state=42)\n\nprint('Train size: {}, CV size: {}, Test size: {}' .format(train_df.shape, cv_df.shape, test.shape))","653c6cf7":"del train, y_tr\ngc.collect()","47cc70f1":"#Cleaning brand name before using count vectorizer\n# Using same preprocessing as used earlier for categories: 'clean_cat()' function\n\ntrain_df['brand_name'] = clean_cat(train_df['brand_name'].values)\ncv_df['brand_name'] = clean_cat(cv_df['brand_name'].values)\ntest['brand_name'] = clean_cat(test['brand_name'].values)","a983c513":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_brand_oneHot = vectorizer.fit_transform(train_df['brand_name'].values)\n\ncv_brand_oneHot = vectorizer.transform(cv_df['brand_name'].values)\ntest_brand_oneHot = vectorizer.transform(test['brand_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_brand_oneHot.shape, \"\\n\", cv_brand_oneHot.shape, \"\\n\", test_brand_oneHot.shape)","32e9982c":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_gencat_oneHot = vectorizer.fit_transform(train_df['gencat_name'].values)\n\ncv_gencat_oneHot = vectorizer.transform(cv_df['gencat_name'].values)\ntest_gencat_oneHot = vectorizer.transform(test['gencat_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_gencat_oneHot.shape, \"\\n\", cv_gencat_oneHot.shape, \"\\n\", test_gencat_oneHot.shape)","32922856":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_subcat1_oneHot = vectorizer.fit_transform(train_df['subcat1_name'].values)\n\ncv_subcat1_oneHot = vectorizer.transform(cv_df['subcat1_name'].values)\ntest_subcat1_oneHot = vectorizer.transform(test['subcat1_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_subcat1_oneHot.shape, \"\\n\", cv_subcat1_oneHot.shape, \"\\n\", test_subcat1_oneHot.shape)","17e1c560":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_subcat2_oneHot = vectorizer.fit_transform(train_df['subcat2_name'].values)\n\ncv_subcat2_oneHot = vectorizer.transform(cv_df['subcat2_name'].values)\ntest_subcat2_oneHot = vectorizer.transform(test['subcat2_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_subcat2_oneHot.shape, \"\\n\", cv_subcat2_oneHot.shape, \"\\n\", test_subcat2_oneHot.shape)","f8a03760":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_features=250000)\n\ntrain_name_tfidf = vectorizer.fit_transform(train_df['preprocessed_name'].values)\n\ncv_name_tfidf = vectorizer.transform(cv_df['preprocessed_name'].values)\ntest_name_tfidf = vectorizer.transform(test['preprocessed_name'].values)\n\nprint(\"Shape of matrices after vectorization\")\nprint(train_name_tfidf.shape, \"\\n\", cv_name_tfidf.shape, \"\\n\", test_name_tfidf.shape)","766a94db":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_features=500000)\n\ntrain_description_tfidf = vectorizer.fit_transform(train_df['preprocessed_description'].values)\n\ncv_description_tfidf = vectorizer.transform(cv_df['preprocessed_description'].values)\ntest_description_tfidf = vectorizer.transform(test['preprocessed_description'].values)\n\nprint(\"Shape of matrices after vectorization\")\nprint(train_description_tfidf.shape, \"\\n\", cv_description_tfidf.shape, \"\\n\", test_description_tfidf.shape)","cdb7a499":"submission_df = pd.DataFrame(test['test_id'])\nprint(submission_df.shape)\nsubmission_df.head()","b693074e":"cols = set(train_df.columns.values) - {'train_id'}\nskip_cols = {'preprocessed_name', 'item_condition_id', 'brand_name',\n  'shipping', 'preprocessed_description', 'gencat_name',\n  'subcat1_name', 'subcat2_name', 'name_first', 'price_in_name'}\n\ncols_to_normalize = cols - skip_cols\nprint(\"Normalizing following columns: \", cols_to_normalize)\n\ndef normalize(df):\n    result1 = df.copy()\n    for feature_name in df.columns:\n        if (feature_name in cols_to_normalize):\n            max_value = df[feature_name].max()\n            min_value = df[feature_name].min()\n            result1[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result1\n","721d0363":"train_normalized = normalize(train_df)\ncv_normalized = normalize(cv_df)\ntest_normalized = normalize(test)","18e8482c":"del train_df, cv_df, test\ngc.collect()","9cb75db4":"#Separating and storing all numerical features\n\nX_tr = train_normalized[list(cols_to_normalize)]\nX_val = cv_normalized[list(cols_to_normalize)]\nX_te = test_normalized[list(cols_to_normalize)]\n\nX_tr.head(2)","1227843f":"from scipy.sparse import csr_matrix\n\n# Storing categorical features to sparse matrix\n\nX_tr_cat = csr_matrix(pd.get_dummies(train_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nX_cv_cat = csr_matrix(pd.get_dummies(cv_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nX_te_cat = csr_matrix(pd.get_dummies(test_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nprint(X_tr_cat.shape, X_cv_cat.shape, X_te_cat.shape)","145816a1":"del train_normalized, cv_normalized, test_normalized\ngc.collect()","bdd77b42":"from scipy.sparse import hstack\n\n# stack all categorical and text sparse matrices\n\ntrain_sparse = hstack((train_brand_oneHot, train_gencat_oneHot, train_subcat1_oneHot, train_subcat2_oneHot, \\\n               train_name_tfidf, train_description_tfidf, X_tr_cat)).tocsr()\n\ncv_sparse = hstack((cv_brand_oneHot, cv_gencat_oneHot, cv_subcat1_oneHot, cv_subcat2_oneHot, \\\n               cv_name_tfidf, cv_description_tfidf, X_cv_cat)).tocsr()\n\ntest_sparse = hstack((test_brand_oneHot, test_gencat_oneHot, test_subcat1_oneHot, test_subcat2_oneHot, \\\n               test_name_tfidf, test_description_tfidf, X_te_cat)).tocsr()","94b88f28":"print(train_sparse.shape, cv_sparse.shape, test_sparse.shape)","aa2e35fa":"# stack dense feature matrix with categorical and text vectors\n\nX_train = hstack((X_tr.values, train_sparse)).tocsr()\n\nX_cv = hstack((X_val.values, cv_sparse)).tocsr()\n\nX_test = hstack((X_te.values, test_sparse)).tocsr()","1f663437":"print('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_cv.shape, X_test.shape))","8932ea8a":"del vectorizer\ndel train_brand_oneHot, train_gencat_oneHot, train_subcat1_oneHot, train_subcat2_oneHot, \\\n            train_name_tfidf, train_description_tfidf, X_tr_cat\n\ndel cv_brand_oneHot, cv_gencat_oneHot, cv_subcat1_oneHot, cv_subcat2_oneHot, cv_name_tfidf, cv_description_tfidf, X_cv_cat\n\ndel test_brand_oneHot, test_gencat_oneHot, test_subcat1_oneHot, test_subcat2_oneHot, \\\n               test_name_tfidf, test_description_tfidf, X_te_cat\ngc.collect()","cdeaf532":"from sklearn.metrics import mean_squared_error as mse\nfrom math import sqrt\nfrom sklearn.linear_model import Ridge","7305212f":"alpha = [1, 2, 3, 3.5, 4, 4.5, 5, 6, 7] \ncv_rmsle_array=[] \nfor i in tqdm(alpha):\n    model = Ridge(solver=\"sag\", random_state=42, alpha=i)\n    model.fit(X_train, y_train)\n    preds_cv = model.predict(X_cv)\n    cv_rmsle_array.append(sqrt(mse(y_cv, preds_cv)))\n\nfor i in range(len(cv_rmsle_array)):\n    print ('RMSLE for alpha = ',alpha[i],'is',cv_rmsle_array[i])\n    \nbest_alpha = np.argmin(cv_rmsle_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_rmsle_array)\nax.scatter(alpha, cv_rmsle_array)\nfor i, txt in enumerate(np.round(cv_rmsle_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_rmsle_array[i]))\n\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Error\")\nplt.show()","f426f74c":"print(\"Best alpha: \",  alpha[best_alpha])\nmodel = Ridge(solver=\"sag\", random_state=42, alpha=alpha[best_alpha])\nmodel.fit(X_train, y_train)\nridge_preds_tr = model.predict(X_train)\nridge_preds_cv = model.predict(X_cv)\nridge_preds_te = model.predict(X_test)\n\nprint('Train RMSLE:', sqrt(mse(y_train, ridge_preds_tr)))\n\nridge_rmsle = sqrt(mse(y_cv, ridge_preds_cv))\nprint(\"Cross validation RMSLE: \", ridge_rmsle)","689f9be9":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.01)\nmodel.fit(X_train, y_train>= 4)\n\nmnb_preds_tr = model.predict_proba(X_train)[:, 1]\nmnb_preds_cv = model.predict_proba(X_cv)[:, 1]\nmnb_preds_te = model.predict_proba(X_test)[:, 1]","7057d035":"# from sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\n\nfselect = SelectKBest(f_regression, k=48000)\ntrain_features = fselect.fit_transform(train_sparse, y_train)\n\ncv_features = fselect.transform(cv_sparse)\ntest_features = fselect.transform(test_sparse)","e0c0b86f":"print('Shapes after SelectKBest:', train_features.shape, cv_features.shape, test_features.shape)","0fe80b6c":"# stack feature matrix with Ridge, MNB model predictions, engineered features\nX_train = hstack((X_tr.values, ridge_preds_tr.reshape(-1,1), mnb_preds_tr.reshape(-1,1), train_features)).tocsr()\n\nX_cv = hstack((X_val.values, ridge_preds_cv.reshape(-1,1), mnb_preds_cv.reshape(-1,1), cv_features)).tocsr()\n\nX_test = hstack((X_te.values, ridge_preds_te.reshape(-1,1), mnb_preds_te.reshape(-1,1), test_features)).tocsr()\n\nprint('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_cv.shape, X_test.shape))","ca31e8a3":"del train_features, cv_features\ngc.collect()","e9ec90b8":"print('Time taken: ', time.time()-start_time)\nsnapshot2 = tracemalloc.take_snapshot()\ntop_stats = snapshot2.compare_to(snapshot1, 'lineno')\n\nprint(\"[ Top 10 ]\")\nfor stat in top_stats[:10]:\n    print(stat)","38985eca":"submission_df['price'] = np.exp(ridge_preds_te) - 1\n\nsubmission_df.to_csv('ridge_submission.csv', index=False)","85cf0f32":"scipy.sparse.save_npz(\"cv_final.npz\", X_cv)\nnp.save('y_cv', y_cv)\n\ndel X_cv, y_cv\ngc.collect()","fe63ae50":"scipy.sparse.save_npz(\"train_final.npz\", X_train)\nnp.save('y_train', y_train)\n\ndel X_train, y_train\ngc.collect()","4d279d36":"scipy.sparse.save_npz(\"test_final.npz\", X_test)\n\ndel X_test\ngc.collect()","822054dc":"Majority of the items  are in **condition 1**","19afba84":"#### subcat1_name","64ae2802":"### Univariate analysis on the above features","a6c896e7":"**There is slight variation of price based on item condition. Median Price decreases as we go from condition 1 to 4. Items in condition 5 seem to be having higher price, which is a bit strange.**","fd8349cf":"### Check for duplicate rows, NAs","20125f9a":"## 4. Featurization","404a157e":"### 4.3. Categorical features\nOne-hot encoding of brand_name, gencat_name, subcat1_name, subcat2_name.\n","87d3b1f0":"## 1. Data Overview","24c726c6":"#### name","928003de":"#### item_condition_id","bd974346":"### 4.2. Train, Test split for cross validation","a5c83c6b":"### 6.1. Ridge Model\nLinear least squares with l2 regularization\n<a href= \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html\">sklearn.linear_model.Ridge <\/a>","f758b0d8":"### Check for rows with invalid price","e4013712":"## 3. Basic Exploratory Data Analysis","888a79d6":"### 5.1. Normalize numerical features","b0d85e8f":"**Prices of items belonging to various categories and subcategories vary significantly.**<br>\n**This indicates that categories are going to be important features in determining the price of an item.**","ca004117":"### Column-wise overview of data","c8edbdcc":"\n**NameLower**: # lowercase letters in name\n\n**DescriptionLower**: # lowercase letters in description\n\n**NameUpper**: # uppercase letters in name\n\n**DescriptionUpper**: # uppercase letters in description\n\n**name_len**: char length of name\n\n**des_len**: char length of desc\n\n**name_desc_len_ratio**: name_len \/ des_len\n\n**desc_word_count**\n\n**mean_des**: 10 * desc_word_count \/ des_len\n\n**name_word_count**\n\n**mean_name**: 10 * name_word_count \/ name_len\n\n**desc_letters_per_word**: des_len \/ desc_word_count\n\n**name_letters_per_word**: name_len \/ name_word_count\n\n**NameLowerRatio**: NameLower \/ name_len\n\n**DescriptionLowerRatio**: DescriptionLower \/ des_len\n\n**NameUpperRatio**: NameUpper \/ name_len\n\n**DescriptionUpperRatio**: DescriptionUpper \/ des_len","ac03ae9d":"### 6.2. Multinomial Naive Bayes: Regression using Classification\n<a href= \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\">sklearn.naive_bayes.MultinomialNB <\/a>","c513a8cb":"# Mercari Price Suggestion Challenge\n\n## Overview\n\nPredicting the price of a product is a tough challenge since very similar products having minute differences such as different brand name, additional specifications, quality of the product, demand of the product, etc. can have very different prizes. For example the price of a pair of running shoes by a very common brand (say Puma) might be around INR 2,500 whereas a similar pair made by Asics can cost around INR 10,000.<br>\n\nPrice prediction gets even more difficult when there is a huge range of products, which is common with most of the online shopping platforms. While it might be simpler to predict the price of a particular category of products using some simple criteria, it\u2019s highly challenging to predict the price of almost anything that is listed on online platforms. We may have multiple listings  of the same product by a large number of sellers priced differently.<br>\n\n<a href=\"https:\/\/www.mercari.com\">Mercari<\/a> is Japan\u2019s biggest community-powered shopping app. Mercari\u2019s challenge is to build an algorithm that automatically suggests the right product prices.","b05f5808":"### 5.2. Remove  non-features from dataframes","12f1a78e":"**Features such as *brand_mean_price, brand_median price, subcat2_mean_price, subcat2_median_price* show strong linear trends.**<br>\n\n**Therefore they seem to be useful in determinig price of items.**","019d8a12":"#### subcat2_name","78d5004d":"### 5.3. Consolidate all features to a sparse matrix","def7f7d4":"#### Hyper-parameter tuning","be890695":"## Data collection\n\nThe data can be downloaded from <a href=\"https:\/\/www.kaggle.com\/c\/mercari-price-suggestion-challenge\/data\">Kaggle<\/a> competion page.<br>\n\nWe have been provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition","a07ad481":"### 4.4. Tfidf vectorization on text features\n1-3 grams of name<br>\n1-3 grams of item_description","d433ef0f":"### 4.1. Feature Engineering","d11081c8":"#### brand_name","b319654a":"**name_first**: cleaned name first word\n\n**name_first_count**: count of name in data\n\n**gencat_name_count**: count of gencat in data\n\n**subcat1_name_count**: count of subcat1 in data\n\n**subcat2_name_count**: count of subcat2 in data\n\n**brand_name_count**: count of brand_name in data","31a516b3":"### Handling missing values","a145c593":"## ML Problem\n\nUsing the given data, we have to come up with a model that predicts the price of a product listed on Mercari as accurately as possible.\n\nThis is a standard regression problem.","a5d78959":"Majority of the items are from the category **women**","48a3e003":"## 2. Pre-processing","c6bb995d":"Target variable is ln(1+y), therefore  calculating mean square error on ln(1+y) will effectively give us MSLE on y\n\n- MSLE(y^, y) = MSE(ln(1+y^), ln(1+y))","b667de2e":"#### item_description","d3fa6aea":"**brand_mean_price**: mean price of items by a brand\n\n**name_mean_price**: mean price of an item by name\n\n**gencat_mean_price**: mean price of items belonging to gencat\n\n**subcat1_mean_price**: mean price of items belonging to subcat1\n\n**subcat2_mean_price**: mean price of items belonging to subcat2\n\n**condition_mean_price**: mean price of items by condition\n\n**brand_median_price**: median price of items by a brand\n\n**name_median_price**: median price of an item by name\n\n**gencat_median_price**: median price of items belonging to gencat\n\n**subcat1_median_price**: median price of items belonging to subcat1\n\n**subcat2_median_price**: median price of items belonging to subcat2\n\n**condition_median_price**: median price of items by condition","6b151fdf":"#### price","07ed7aca":"Columns **brand_name, item_description** have NAs.\n\nNAs in **category_name** have been replaced by empty lists because of the converter we have used while loading the data.\n\nWe will replace NAs, empty lists with *'missing'*.","4e0d7426":"### SelectKBest:  Selecting  top 48k features from categorical and text features","53f33ced":"## Performance Metric\n\nThe performance of the model is measured by Root Mean Squared Logarithmic Error(RMSLE). Lesser the RMSLE, better is our prediction model.<br>\n\nThe RMSLE is calculated as\n\\begin{align*}\n\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }\n\\end{align*}<br>\n\n- \u03f5 is the RMSLE value (score)\n- n is the total number of observations in the (public\/private) data set,\n- pi is your prediction of price, and\n- ai is the actual sale price for i\n- log(x) is the natural logarithm of x","a45fb5fa":"## 5. Data preparation","970cab0e":"**Dropping rows with blank name and description**","c6c43f2d":"#### gencat_name","740807f3":"This model has been used  as a correction for the tendency of the Ridge model to underestimate.\nThe predicted values are saved for use as features in a later model.\nThe authors of 18th place solution <a href= \"https:\/\/www.kaggle.com\/peterhurford\/lgb-and-fm-18th-place-0-40604\">kernel<\/a> improved their RMSLE by 0.003 using this.\n","e4c85d9e":"- **97% of datapoints have price less than USD 100.**\n- **Very few (only 1%) datapoints have price more than USD 170**","d5946f60":"## 0. Loading data","8fb3a692":"No duplicate rows in train data","eb3a5814":"**NamePunctCount**: # punctuations in name\n\n**DescriptionPunctCount**: # punctuations in desc\n\n**NamePunctCountRatio**: NamePunctCount \/ name_word_count\n\n**DescriptionPunctCountRatio**: DescriptionPunctCount \/ desc_word_count\n\n**NameDigitCount**: # digits in name\n\n**DescriptionDigitCount**: # digits in desc\n\n**NameDigitCountRatio**: NameDigitCount \/ name_word_count\n\n**DescriptionDigitCountRatio**: DescriptionDigitCount \/ desc_word_count\n\n**stopword_ratio_desc**: # stopwords in desc \/ desc_word_count\n\n**num_sum**: Sum of numbers in desc\n\n**weird_characters_desc**: # non-alphanumeric, non-punct in desc\n\n**weird_characters_name**: # non-alphanumeric, non-punct in desc\n\n**prices_count**: # of [rm] (removed price) in desc\n\n**price_in_name**: 1 if desc contains [rm]; 0 otherwise","df56b5cd":"## 6. Modeling","6349a410":"#### Training using best hyper-parameters and testing"}}