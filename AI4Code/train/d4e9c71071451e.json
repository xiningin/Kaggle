{"cell_type":{"4bd785e8":"code","222dc050":"code","55b6d694":"code","ad79b3c5":"code","3be5e3ff":"code","f71c4e54":"code","9b989747":"code","5be6f9b8":"code","07ea1977":"code","0c82303b":"code","f549baf8":"code","bd02764b":"code","ba3d0eb2":"code","ec87fc55":"code","f76bd75f":"code","feeaaa09":"code","7d22466c":"code","7c885736":"code","dbae079e":"code","79ab903f":"code","f8f593d0":"code","09f06314":"code","316295df":"code","7f890aa5":"code","4674a0c1":"code","c2d23105":"code","76e7affe":"code","feb61711":"code","a20ecec4":"code","bed776db":"code","4c4a4478":"code","039410ca":"code","fde2a869":"code","5a0f35aa":"code","82feda0a":"code","2264f90d":"code","4b6617eb":"code","99416e2a":"code","eab96dbd":"code","da4bd678":"code","5a35df48":"code","791e1f16":"code","ec6bdbd5":"code","478d8779":"markdown","d82ea644":"markdown","c47ea0f8":"markdown","b9958147":"markdown","52868268":"markdown","bf286884":"markdown","4caea6a5":"markdown","6fd339e4":"markdown","7dd60ba8":"markdown","fc1aa0ae":"markdown","9dd2bf34":"markdown","209c634c":"markdown","f75c4863":"markdown","82b18717":"markdown","879d5267":"markdown","eb515ac3":"markdown","b330eb31":"markdown","acb7a0af":"markdown","1a7403d3":"markdown","0deaab2b":"markdown","ab2a1bec":"markdown","c5ee1e67":"markdown","6132992d":"markdown","a7a8e6d5":"markdown","9b80d03b":"markdown","a1fa1673":"markdown","4f0a9e6f":"markdown","660f765c":"markdown","631b7782":"markdown","c685805e":"markdown","65df0998":"markdown","3dbc4aa1":"markdown","802bdd32":"markdown","62c1dacc":"markdown","a0a9e2ba":"markdown","ab4072d3":"markdown","c85ea02a":"markdown"},"source":{"4bd785e8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\nplt.style.use(\"ggplot\")\n\nimport sklearn\nfrom sklearn.decomposition import TruncatedSVD\n\n\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","222dc050":"amazon_ratings = pd.read_csv('..\/input\/ratings_Electronics.csv')\namazon_ratings.columns = ['UserId','ProductId','Rating','Timestamp']\namazon_ratings = amazon_ratings.dropna()\namazon_ratings.head()","55b6d694":"print(\"Shape: %s\" % str(amazon_ratings.shape))\nprint(\"Column names: %s\" % str(amazon_ratings.columns))","ad79b3c5":"amazon_ratings.shape","3be5e3ff":"# Unique Users and Products\n\nprint(\"Unique UserID count: %s\" % str(amazon_ratings.UserId.nunique()))\nprint(\"Unique ProductID count: %s\" % str(amazon_ratings.ProductId.nunique()))","f71c4e54":"# Rating frequency\n\nsns.set(rc={'figure.figsize': (11.7, 8.27)})\nsns.set_style('whitegrid')\nax = sns.countplot(x='Rating', data=amazon_ratings, palette=sns.color_palette('Greys'))\nax.set(xlabel='Rating', ylabel='Count')\nplt.show()","9b989747":"# Mean rating for each Product\n\nproduct_rating = amazon_ratings.groupby('ProductId')['Rating'].mean()\nproduct_rating.head()","5be6f9b8":"# Mean rating KDE distribution\n\nax = sns.kdeplot(product_rating, shade=True, color='grey')\nplt.show()","07ea1977":"# Count of the number of ratings per Product\n\nproduct_rating_count = amazon_ratings.groupby('ProductId')['Rating'].count()\nproduct_rating_count.head()","0c82303b":"# Number of ratings per product KDE distribution\n\nax = sns.kdeplot(product_rating_count, shade=True, color='grey')\nplt.show()","f549baf8":"# Un-Reliability factor\n\nunreliability = amazon_ratings.groupby('ProductId')['Rating'].std(ddof = -1)\nunreliability.head()","bd02764b":"# Un-Reliability factor KDE distribution\n\nax = sns.kdeplot(unreliability, shade=True, color='grey')\nplt.show()","ba3d0eb2":"\nunique_products_list = amazon_ratings.ProductId.unique()\ndata_model = pd.DataFrame({'Rating': product_rating[unique_products_list],\\\n                           'Count': product_rating_count[unique_products_list], \\\n                          'Unreliability': unreliability[unique_products_list]})\ndata_model.head()","ec87fc55":"print(\"Data model shape (number of data points): %s\" % str(data_model.shape))","f76bd75f":"# Rating versus count\n\nsns.set_style('ticks')\nplt.figure(num=None, figsize=(11.7, 8.27), dpi=100, facecolor='w', edgecolor='k')\n\nax = data_model.plot(kind='scatter', x='Rating', y='Count', color='grey', alpha=0.1)\nplt.show()","feeaaa09":"# Less than 100 ratings\n\nax = data_model[data_model.Count < 101].plot(kind='scatter', x='Rating', y='Count', color='grey', alpha=0.1)\nplt.show()","7d22466c":"# 100 to 200 ratings\n\nax = data_model[data_model.Count > 100]\\\n[data_model.Count<201].plot(kind='scatter', x='Rating', y='Count', color='grey', alpha=0.4)\nplt.show()","7c885736":"# 200 to 500 ratings\n\nax = data_model[data_model.Count > 200]\\\n[data_model.Count<501].plot(kind='scatter', x='Rating', y='Count', color='grey', alpha=0.4)\nplt.show()","dbae079e":"# Adding unreliability factor to the above plots 100 to 200 ratings\n\nax = data_model[data_model.Count > 100]\\\n[data_model.Count<201].plot(kind='scatter', x='Unreliability', y='Count', c='Rating', cmap='jet', alpha=0.6)\nplt.show()","79ab903f":"# Addding unreliability factor to the above plots 200 to 500 ratings\n\nax = data_model[data_model.Count > 200]\\\n[data_model.Count<501].plot(kind='scatter', x='Unreliability', y='Count', c='Rating', cmap='jet', alpha=0.6)\nplt.show()","f8f593d0":"# Coefficient of corelation between Unreliability and Rating\n\ncoeff_corelation = np.corrcoef(x=data_model.Unreliability, y=data_model.Rating)\nprint(\"Coefficient of corelation: \")\nprint(coeff_corelation)","09f06314":"# Summarise Count\n\nprint(data_model.Count.describe())","316295df":"# Summarise Rating\n\nprint(data_model.Rating.describe())","7f890aa5":"# Summarise Unreliability\n\nprint(data_model.Unreliability.describe())","4674a0c1":"# Removing outliers and improbable data points\n\ndata_model = data_model[data_model.Count > 50][data_model.Count < 1001].copy()\nprint(data_model.shape)","c2d23105":"# Normalization function to range 0 - 10\n\ndef normalize(values):\n    mn = values.min()\n    mx = values.max()\n    return(10.0\/(mx - mn) * (values - mx)+10)\n    ","76e7affe":"data_model_norm = normalize(data_model)\ndata_model_norm.head()","feb61711":"# Setting up the model\n\n# Recommend 20 similar items\nengine = KNeighborsClassifier(n_neighbors=20)\n\n# Training data points\ndata_points = data_model_norm[['Count', 'Rating', 'Unreliability']].values\n\n#Training labels\nlabels = data_model_norm.index.values\n\nprint(\"Data points: \")\nprint(data_points)\nprint(\"Labels: \")\nprint(labels)\n\nengine.fit(data_points, labels)","a20ecec4":"# Enter product ID to get a list of 20 recommended items\n\n# User entered value\nproduct_id = 'B00L3YHF6O'\n\nproduct_data = [data_model_norm.loc[product_id][['Count', 'Rating', 'Unreliability']].values]\n\nrecommended_products = engine.kneighbors(X=product_data, n_neighbors=20, return_distance=False)\n\n# List of product IDs form the indexes\n\nproducts_list = []\n\nfor each in recommended_products:\n    products_list.append(data_model_norm.iloc[each].index)\n\nprint(\"Recommended products: \")\nprint(products_list)\n\n# Showing recommended products\n\nax = data_model_norm.plot(kind='scatter', x='Rating', y='Count', color='grey', alpha=0.20)\ndata_model_norm.iloc[recommended_products[0]].plot(kind='scatter', x='Rating', y='Count',\\\n                                                   color='orange', alpha=0.5, ax=ax)\n\nax2 = data_model_norm.plot(kind='scatter', x='Rating', y='Unreliability', color='grey')\ndata_model_norm.iloc[recommended_products[0]].plot(kind='scatter', x='Rating', y='Unreliability',\\\n                                                   color='orange', alpha=0.5, ax=ax2)\nplt.show()","bed776db":"popular_products = pd.DataFrame(amazon_ratings.groupby('ProductId')['Rating'].count())\nmost_popular = popular_products.sort_values('Rating', ascending=False)\nmost_popular.head(10)","4c4a4478":"most_popular.head(30).plot(kind = \"bar\")","039410ca":"# Subset of Amazon Ratings\n\namazon_ratings1 = amazon_ratings.head(10000)","fde2a869":"ratings_utility_matrix = amazon_ratings1.pivot_table(values='Rating', index='UserId', columns='ProductId', fill_value=0)\nratings_utility_matrix.head()","5a0f35aa":"ratings_utility_matrix.shape","82feda0a":"X = ratings_utility_matrix.T\nX.head()","2264f90d":"X.shape","4b6617eb":"X1 = X","99416e2a":"SVD = TruncatedSVD(n_components=10)\ndecomposed_matrix = SVD.fit_transform(X)\ndecomposed_matrix.shape","eab96dbd":"correlation_matrix = np.corrcoef(decomposed_matrix)\ncorrelation_matrix.shape","da4bd678":"X.index[99]","5a35df48":"i = \"1616833742\"\n\nproduct_names = list(X.index)\nproduct_ID = product_names.index(i)\nproduct_ID","791e1f16":"correlation_product_ID = correlation_matrix[product_ID]\ncorrelation_product_ID.shape","ec6bdbd5":"Recommend = list(X.index[correlation_product_ID > 0.90])\n\n# Removes the item already bought by the customer\nRecommend.remove(i) \n\nRecommend[0:9]","478d8779":"###### This plot fails to provide much information due to the large number of data points leading to clustered data. So let's break it down into a number of ranges","d82ea644":"#### Recommendation System - Part II\n\n###### Model-based collaborative filtering system\n\n* Recommend items to users based on purchase history and similarity of ratings provided by other users who bought items to that of a particular customer.\n\n* A model based collaborative filtering technique is closen here as it helps in making predictinfg products for a particular user by identifying patterns based on preferences from multiple user data.","c47ea0f8":"****Product Id**** # Here are the top 10 products to be displayed by the recommendation system to the above customer based on the purchase history of other customers in the website.","b9958147":"#### Recommendation System - Method 2 - Part I\n\n###### Product popularity based recommendation system targeted at new customers\n\n* Popularity based are a great strategy to target the new customers with the most popular products sold on a business's website and is very useful to cold start a recommendation engine.\n\n* Dataset : Amazon product review dataset","52868268":"###### Here we see a trend. It looks like the which have a high unreliability score, seem to have a lower rating over a significant count range. Let's see if there is an corelation between these factors.","bf286884":"###### It's clear that the count ranges form 1 to 7533 ratings, the Mean rating ranges from 1 to 5 and the Unrelaibility factor ranges form 0 to 1.92. These values cannot be use directly as they have a vastly varying range.","4caea6a5":"#### Decomposing the Matrix","6fd339e4":"#### Unique products in subset of data","7dd60ba8":"* Creating fields and measures form the existing data\n\n* This helps generate more data points and validates the idealogy","fc1aa0ae":"###### As expected, the utility matrix obtaned above is sparce, I have filled up the unknown values wth 0.","9dd2bf34":"#### Index # of product ID purchased by customer","209c634c":"* Creating a final collection of all the various measures and features for each product","f75c4863":"#### Data wrangling","82b18717":"###### Correlation for all items with the item purchased by this customer based on items rated by other customers people who bought the same product","879d5267":"###### Analysis:\n\n* The above graph gives us the most popular products (arranged in descending order) sold by the business.\n\n* For eaxmple, product, ID # B001MA0QY2 has sales of over 7000, the next most popular product, ID # B0009V1YR8 has sales of 3000, etc.","eb515ac3":"#### Data modelling\n\n\nLet's see if we are ready to make prediction. If not we must model the data into an appropriate format.","b330eb31":"###### We can notice a large spike in the mean rating at value 5. This is a valuable indicator that points to the skewness of the data. Hence we need to further analyse this issue.","acb7a0af":"#### Data exploration","1a7403d3":"#### Correlation Matrix","0deaab2b":"#### Recommendation System - Method 1","ab2a1bec":"###### Correlation_matrix","c5ee1e67":"# Project  Description -\n\nThis project on Recommendation Systems is conducted on a data set of amazon reviews data.\n\n#### Data Description:\n\nAmazon Reviews data (data source) The repository has several datasets. For this case study, we are using the Electronics dataset.\n\n\n#### Domain:\n\nE-commerce\n\n#### Context:\n\nOnline E-commerce websites like Amazon, Flipkart uses different recommendation models to provide different suggestions to different users. Amazon currently uses item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in\nreal-time.\n\n#### Attribute Information:\n\n\n    \u25cf userId : Every user identified with a unique id\n    \u25cf productId : Every product identified with a unique id\n    \u25cf Rating : Rating of the corresponding product by the corresponding user\n    \u25cf timestamp : Time of the rating ( ignore this column for this exercise)\n\n   \n\n#### Learning Outcomes:\n\n\n    \u25cf Exploratory Data Analysis\n    \u25cf Creating a Recommendation system using real data\n    \u25cf Collaborative filtering\n   \n\n#### Objective:\n\nBuild a recommendation system to recommend products to customers based on the their previous ratings for other products.\n\n#### Steps and tasks:\n\n1. Read and explore the given dataset. (Rename column\/add headers, plot histograms, find data\ncharacteristics) - (2.5 Marks)\n\n2. Take a subset of the dataset to make it less sparse\/ denser. ( For example, keep the users only who has given 50 or more number of ratings ) - (2.5 Marks)\n\n3. Split the data randomly into train and test dataset. ( For example, split it in 70\/30 ratio) - (2.5 Marks)\n\n4. Build Popularity Recommender model. - (20 Marks)\n\n5. Build Collaborative Filtering model. - (20 Marks)\n\n6. Evaluate both the models. ( Once the model is trained on the training data, it can be used to compute the error (RMSE) on predictions made on the test data.) - (7.5 Marks)\n\n7. Get top - K ( K = 5) recommendations. Since our goal is to recommend new products for each user based on his\/her habits, we will recommend 5 new products. - (7.5 Marks)\n\n8. Summarise your insights. - (7.5 marks)\n    ","6132992d":"###### This graphs confirms the expectation that most items have around 50 - 100 ratings. We do have a bunch of outliers that have only a single rating and few Products have over 2000 ratings.","a7a8e6d5":"Once we have modelled the data, we recomending similar items based on Count of ratings, Mean rating and the Unreliability factor","9b80d03b":"#### Data transforming","a1fa1673":"###### We notice that there is medium-strong negative corelation from the -0.26862181 coefficient. This means that as the unreliability factor increases, there is a medium-strong change that the rating of the product decreases. This is a good indicator as it clarifies any questions regarding unreliability.","4f0a9e6f":"#### Loading the dataset","660f765c":"#### Transposing the matrix","631b7782":"###### Now that the engine is setup and we have initialized it with the required data points and labels, we can use it to recommend a list of 20 similar items","c685805e":"#### Recommending top 10 highly correlated products in sequence","65df0998":"* Isolating Product ID # 1616833742 from the Correlation Matrix\n\n* Assuming the customer buys Product ID # 1616833742 (randomly chosen)","3dbc4aa1":"#### Conclusion\n\n\n* The engine recommends similar products based on feature such as number of ratings, mean ratings and unreliability factor of the Product. \n* As seen from the above output, we can alter the number of items recommended, and using this we can integrate onine sale trends into retails stores by recommending similar products to the store. \n* This also can be used as an added feature as a plus point when discussing item sales and profits with the stores.","802bdd32":"#### Utility Matrix based on products sold and user reviews\n\n****Utility Matrix**** : An utlity matrix is consists of all possible user-item preferences (ratings) details represented as a matrix. The utility matrix is sparce as none of the users would buy all teh items in the list, hence, most of the values are unknown.","62c1dacc":"###### We notice that the density becomes sparse as the number of ratings (count) increases. Let's have a look if unreliability has any corelation with the count of ratings and mean rating of the Product.","a0a9e2ba":"#### Importing libraries","ab4072d3":"###### The plot show that a large portion of the products are highly reliable. For this unreliabilit factor we used standard devaiation. But we noticed above that a large porition of the Products have a single review. These items have varying ratings but high reliability. This issue needs to tbe addressed.","c85ea02a":"**Let's explore the data model more**"}}