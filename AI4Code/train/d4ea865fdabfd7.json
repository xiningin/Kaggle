{"cell_type":{"f4be861d":"code","63b61f2e":"code","160b5817":"code","14a8abbe":"code","c6080c51":"code","c3342e42":"code","a96d07c8":"code","7782d8ca":"code","0eaafc00":"code","081d6ee0":"code","b280d9c1":"code","9ab86227":"code","11dfb54d":"code","ed871706":"code","1bf86d55":"code","c1379d17":"code","0998bf72":"markdown","d7a6d17d":"markdown","e678b21b":"markdown","8a3b82f6":"markdown","d155e364":"markdown","7e8cc541":"markdown","8a8a7c84":"markdown","82c2b35b":"markdown","593a9e06":"markdown","13d699a1":"markdown"},"source":{"f4be861d":"import math, os, re, warnings, random, time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers, Sequential, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.utils import to_categorical\n\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')","63b61f2e":"#Variables\nAUTO = tf.data.experimental.AUTOTUNE\n\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-8\nEPOCHS = 10\nHEIGHT = 28\nWIDTH = 28\nHEIGHT_RS = 28\nWIDTH_RS = 28\nCHANNELS = 1\nN_CLASSES = 9\nN_FOLDS = 5\nFOLDS_USED = 5\nES_PATIENCE = 5","160b5817":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/fashionmnist'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","14a8abbe":"def count_data_items(filenames):\n    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ndata_base_path = \"..\/input\/fashionmnist\/\"\ntrain = pd.read_csv(f'{data_base_path}fashion-mnist_train.csv')\ntest = pd.read_csv(f'{data_base_path}fashion-mnist_test.csv')\n\nprint(f'Train samples: {len(train)}')\n\nprint(\"train missing values:\", train.isnull().any().sum())\nprint(\"test missing values:\", test.isnull().any().sum())\n\n\n\nTRAIN_IMAGES = train.copy()\nTEST_IMAGES = test.copy()\n\nprint(TRAIN_IMAGES.shape)\nprint(TEST_IMAGES.shape)\n# NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n# print(f'GCS: train images: {NUM_TRAINING_IMAGES}')\nprint(type(TRAIN_IMAGES))\n\n\nCLASSES = ['T-shirt\/top', \n           'Trousers', \n           'Pullover', \n           'Dress', 'Coat', \n           'Sandal', \n           'Shirt', \n           'Sneaker', \n           'Bag', \n           'Ankle boot']\n\nX = TRAIN_IMAGES.iloc[:,1:]\nY = TRAIN_IMAGES.iloc[:,0] \nplt.figure(figsize=(10, 10))\nfor i in range(36):\n    plt.subplot(6, 6, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X.loc[i].values.reshape((28,28))) #calling the .values of each row\n    label_index = int(Y[i]) #setting as an int as the number is stored as a string\n    plt.title(CLASSES[label_index])\nplt.show()","c6080c51":"# def converting(image):\n#     img = tf.image.convert_image_dtype(img, dtype=tf.float32) \/ 255.0\n#     return image\n\ninitial_fold = StratifiedKFold(n_splits = 4, random_state =1, shuffle = True)\n\nfor train_index, test_index in initial_fold. split(X, Y):\n    TRAIN_IMAGES, TRAIN_VAL = X.iloc[train_index], X.iloc[test_index]\n    TEST_IMAGES, TEST_VAL = Y[train_index], Y[test_index]\n    \nTRAIN_IMAGES =TRAIN_IMAGES.values.reshape(-1, HEIGHT,WIDTH,1)\nTRAIN_VAL =TRAIN_VAL.values.reshape(-1, HEIGHT,WIDTH,1)\n\n\n\n\nTEST_IMAGES = to_categorical(TEST_IMAGES, num_classes=10)\nTEST_VAL = to_categorical(TEST_VAL, num_classes=10)\n\nprint(\"Train image shape: \", TRAIN_IMAGES.shape)\nprint(\"Train Val shape: \", TRAIN_VAL.shape)\nprint(\"Test image shape: \", TEST_IMAGES.shape)\nprint(\"Test  Val shape: \", TEST_VAL.shape)","c3342e42":"def data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     train_datagen = ImageDataGenerator(rescale=1.\/255)\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_flip_up_down(image)\n    \n#     if p_spatial > .75:\n#         image = tf.image.transpose(image)\n#     image = tf.image.rot90(image, k = 3)\n#     datagen.fit(image)\n    \n    return image","a96d07c8":"def scale_image(image):\n    \"\"\"\n        Cast tensor to float and normalizes (range between 0 and 1).\n    \"\"\"\n#     image = tf.reshape(image, [28, 28, 1])\n    image = tf.cast(image, tf.float32)\n    image \/= 255.0\n#     image = tf.cast(image, tf.float32) \/ 255\n    \n    return image\n# TRAIN_IMAGES = np.asarray(TRAIN_IMAGES).astype('float32')\n\n# dataset = tf.data.Dataset.from_tensor_slices(TRAIN_IMAGES)\n# dataset = dataset.map(scale_image, num_parallel_calls=AUTO)\n    \n\n\ndef get_dataset(FILENAMES, ordered = False, repeated = False, cached = False, augment = False):\n    \n    if augment:\n        FILENAMES = data_augment(FILENAMES)\n        \n#     FILENAMES = scale_image(FILENAMES)\n    return FILENAMES\n\n","7782d8ca":"TRAIN_IMAGES = get_dataset(TRAIN_IMAGES, ordered=True, augment=True)\n\n# print(TRAIN_IMAGES.shape)\n# print(TRAIN_VAL.shape)\n\n# print(TRAIN_IMAGES[0].shape)","0eaafc00":"from keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout,Conv2D,MaxPooling2D, BatchNormalization\nfrom keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\n\n# Merge inputs and targets\ninputs = np.concatenate((TRAIN_IMAGES, TRAIN_VAL), axis=0)\ntargets = np.concatenate((TEST_IMAGES, TEST_VAL), axis=0)\nacc_per_fold = []\nloss_per_fold = []\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=5, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold.split(inputs, targets):\n\n  model = Sequential()\n  model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                  data_format='channels_last', input_shape=(28,28,1)))\n  model.add(BatchNormalization())\n\n  model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                  data_format='channels_last'))\n  model.add(BatchNormalization())\n  model.add(Dropout(0.25))\n\n  model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                  data_format='channels_last'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n      \n      \n  model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                  data_format='channels_last'))\n  model.add(BatchNormalization())\n  model.add(Dropout(0.25))\n\n  model.add(Flatten())\n  model.add(Dense(512, activation='relu'))\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(128, activation='relu'))\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(10, activation='softmax'))\n\n\n  early_stopping_callback = EarlyStopping(monitor='val_loss', \n  #                                         min_delta=1e-2,\n                                          patience=4,\n                                          verbose = 1,\n                                          restore_best_weights=True)\n\n  ckpt = ModelCheckpoint(\n          f'modelcheck{fold_no}.hdf5',\n          verbose=1,\n          save_best_only=True,\n          save_weights_only=True\n      )\n  optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999 )\n\n  model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n\n  print('------------------------------------------------------------------------')\n  print(f'Training for fold {fold_no} ...')\n\n  history = model.fit(TRAIN_IMAGES,TEST_IMAGES,\n                  batch_size=128,\n                  epochs=10,\n                  validation_data=(TRAIN_VAL,TEST_VAL),\n  #                  validation_steps = TRAIN_VAL.shape[0] \/\/ 64,\n                  verbose=1,\n                  callbacks = [early_stopping_callback,ckpt])  \n  scores = model.evaluate(TRAIN_VAL, TEST_VAL)\n#   model.save('model.h5')\n  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n \n  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n  acc_per_fold.append(scores[1] * 100)\n  loss_per_fold.append(scores[0])\n\n  # Increase fold number\n  fold_no = fold_no + 1  ","081d6ee0":"!pip install visualkeras\nimport visualkeras\n\nvisualkeras.layered_view(model)\n","b280d9c1":"print(TRAIN_VAL.shape)\nplt.title('Model Accuracy')\nplt.plot(range(1,11),history.history['accuracy'])\nplt.plot(range(1,11),history.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('accuracy')\nplt.legend(labels=['train','validation'])\nplt.show()\n\nplt.title('Model Loss')\nplt.plot(range(1,11),history.history['loss'])\nplt.plot(range(1,11),history.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.legend(labels=['train','validation'])\nplt.show()\n","9ab86227":"print(TRAIN_VAL.shape)\nplt.title('Model Accuracy')\nplt.plot(range(1,11),history.history['accuracy'])\nplt.plot(range(1,11),history.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('accuracy')\nplt.legend(labels=['train','validation'])\nplt.show()\n\nplt.title('Model Loss')\nplt.plot(range(1,11),history.history['loss'])\nplt.plot(range(1,11),history.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.legend(labels=['train','validation'])\nplt.show()\n\n\nscore = model.evaluate(TRAIN_VAL, TEST_VAL)\n\nprint('Loss: {:.4f}'.format(score[0]))\nprint('Accuracy: {:.4f}'.format(score[1]))\n\n\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","11dfb54d":"from itertools import *\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","ed871706":"Y_pred = model.predict(TRAIN_VAL)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(TEST_VAL,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, \n            classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])","1bf86d55":"correct = []\nfor i in range(len(TEST_VAL)):\n    if(Y_pred_classes[i] == Y_true[i]):\n        correct.append(i)\n    if(len(correct) == 4):\n        break\nTRAIN_VAL = np.array(TRAIN_VAL)\n\n    \nfig, ax = plt.subplots(2,2, figsize=(12,6))\nfig.set_size_inches(10,10)\nax[0,0].imshow(TRAIN_VAL[correct[0]].reshape(28,28), cmap='gray')\nax[0,0].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[correct[0]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[correct[0]]]))\nax[0,1].imshow(TRAIN_VAL[correct[1]].reshape(28,28), cmap='gray')\nax[0,1].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[correct[1]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[correct[1]]]))\nax[1,0].imshow(TRAIN_VAL[correct[2]].reshape(28,28), cmap='gray')\nax[1,0].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[correct[2]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[correct[2]]]))\nax[1,1].imshow(TRAIN_VAL[correct[3]].reshape(28,28), cmap='gray')\nax[1,1].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[correct[3]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[correct[3]]]))","c1379d17":"incorrect = []\nfor i in range(len(TEST_VAL)):\n    if(not Y_pred_classes[i] == Y_true[i]):\n        incorrect.append(i)\n    if(len(incorrect) == 4):\n        break\n\nfig, ax = plt.subplots(2,2, figsize=(12,6))\nfig.set_size_inches(10,10)\nax[0,0].imshow(TRAIN_VAL[incorrect[0]].reshape(28,28), cmap='gray')\nax[0,0].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[incorrect[0]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[incorrect[0]]]))\nax[0,1].imshow(TRAIN_VAL[incorrect[1]].reshape(28,28), cmap='gray')\nax[0,1].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[incorrect[1]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[incorrect[1]]]))\nax[1,0].imshow(TRAIN_VAL[incorrect[2]].reshape(28,28), cmap='gray')\nax[1,0].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[incorrect[2]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[incorrect[2]]]))\nax[1,1].imshow(TRAIN_VAL[incorrect[3]].reshape(28,28), cmap='gray')\nax[1,1].set_title(\"Predicted Label : \" + str(CLASSES[Y_pred_classes[incorrect[3]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(CLASSES[Y_true[incorrect[3]]]))\n","0998bf72":"## Custom Step size + TTA","d7a6d17d":"## Augmentations","e678b21b":"## Get Augmentations","8a3b82f6":"## CNN Fashion Mnist 95% Accuracy\n","d155e364":"## Load data","7e8cc541":"## Model Params","8a8a7c84":"## Auxiliary functions","82c2b35b":"## Evaluation","593a9e06":"## Training","13d699a1":"Hello! This is my very first external dataset model that I created a few months ago - Just starting out in ML\n\n## Basic fashion Mnist model\n- Before I start, I did **NOT** use tensorflow's inbuilt fashion mnist dataset.\n- I created this notebook for a 24 hour competition, and decided to use Fashion Mnist as a dataset.\n- Going to convert this code into pytorch and improve it in pytorch, as I found more affection towards the framework.\n\n\n**Agenda:**\n- Model Params\n- Create Dataset\n- Augmentations\n- Aux functions\n- Training loop and testing"}}