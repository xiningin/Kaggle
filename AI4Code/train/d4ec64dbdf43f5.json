{"cell_type":{"4ee9ec17":"code","bafeb22b":"code","7a7c2e59":"code","e57eb8b8":"code","5740f012":"code","77b56482":"code","389d977d":"code","31103596":"code","181aff8f":"code","2767ec17":"code","c88c29cc":"code","8b9a2389":"code","778ca8a1":"code","dc331d4b":"code","0b37a194":"code","ede9baf9":"code","0b2fa821":"code","a447f416":"code","54803c71":"code","01bfad67":"code","02feff92":"code","e48024f6":"code","cec3e3df":"code","12f176ab":"code","293805d3":"code","626910cb":"code","376e5833":"markdown","fbdb038b":"markdown","336dbe60":"markdown","6efb25b8":"markdown","3f6e5beb":"markdown","ef108f5d":"markdown","f15380cc":"markdown","ecb249ed":"markdown","4e306036":"markdown","5463a416":"markdown"},"source":{"4ee9ec17":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","bafeb22b":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, OneHotEncoder\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')","7a7c2e59":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntrain_drug = pd.read_csv(\"..\/input\/lish-moa\/train_drug.csv\")\n","e57eb8b8":" params = {\"n_genes_pca\": 50,\n          \"n_cells_pca\": 20,\n          \"batch_size\": 256,\n          \"lr\": 1e-3,\n          \"weight_decay\": 1e-5,\n          \"n_folds\": 5,\n          \"early_stopping_steps\": 5,\n          \"hidden_size\": 512,\n          \"boost_rate\": 1.0,  # original: 1.0\n          \"num_nets\": 20,  # Number of weak NNs. original: 40\n          \"epochs_per_stage\": 1,  # Number of epochs to learn the Kth model. original: 1\n          \"correct_epoch\": 1,    #  Number of epochs to correct the whole week models original: 1\n          \"model_order\": \"second\"  # You could put \"first\" according to the original implemention, but error occurs. original: \"second\"\n          }","5740f012":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","77b56482":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","389d977d":"# GENES\nn_comp = params[\"n_genes_pca\"]\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","31103596":"#CELLS\nn_comp = params[\"n_cells_pca\"]\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","181aff8f":"data = pd.concat([pd.DataFrame(train_features), pd.DataFrame(test_features)])\ndata2 = pd.get_dummies(data, columns=[\"cp_time\", \"cp_dose\"])\n\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n# train_features = pd.concat((train_features, train2), axis=1)\n# test_features = pd.concat((test_features, test2), axis=1)\ntrain_features = train2\ntest_features = test2\ntrain_features","2767ec17":"from sklearn.feature_selection import VarianceThreshold\n\n\n# var_thresh = VarianceThreshold(threshold=0.5)\n# data = train_features.append(test_features)\n# data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n# train_features_transformed = data_transformed[ : train_features.shape[0]]\n# test_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\n# train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n#                               columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n# train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n# test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n#                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n# test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nfeature_cols = train_features.columns[4:].tolist()\nparams[\"feat_d\"] = len(feature_cols)\ntrain_features","c88c29cc":"feature_cols[:10]","8b9a2389":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","778ca8a1":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","dc331d4b":"train","0b37a194":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","ede9baf9":"folds_ = train.copy()\n\nfolds = []\n\n# LOAD FILES\ntrain_feats = train_features\nscored = target\ndrug = train_drug\n# scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\ndrug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\ntargets = target_cols\nscored = scored.merge(drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc <= 18].index.sort_values()\nvc2 = vc.loc[vc > 18].index.sort_values()\n\n#         vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n#         vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits = params[\"n_folds\"], shuffle = True, random_state = 0)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits = params[\"n_folds\"], shuffle = True, random_state = 0)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\nfor fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\nscored['fold'] = scored.drug_id.map(dct1)\nscored.loc[scored.fold.isna(),'fold'] =\\\n    scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\nscored.fold = scored.fold.astype('int8')\nfolds.append(scored.fold.values)\n\ndel scored['fold']\n\ns = np.stack(folds)\ntrain[\"kfold\"] = s.reshape(-1, )\n\n# mskf = MultilabelStratifiedKFold(n_splits=5)\n\n# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#     folds.loc[v_idx, 'kfold'] = int(f)\n\ntrain","0b2fa821":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :] , dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","a447f416":"from enum import Enum\nclass ForwardType(Enum):\n    SIMPLE = 0\n    STACKED = 1\n    CASCADE = 2\n    GRADIENT = 3\n\nclass DynamicNet(object):\n    def __init__(self, c0, lr):\n        self.models = []\n        self.c0 = c0\n        self.lr = lr\n        self.boost_rate  = nn.Parameter(torch.tensor(lr, requires_grad=True, device=\"cuda\"))\n\n    def add(self, model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n        for m in self.models:\n            params.extend(m.parameters())\n\n        params.append(self.boost_rate)\n        return params\n\n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_cuda(self):\n        for m in self.models:\n            m.cuda()\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n\n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1,-1), batch, axis=0)\n            return None, torch.Tensor(c0).cuda()\n        middle_feat_cum = None\n        prediction = None\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n                else:\n                    middle_feat_cum, pred = m(x, middle_feat_cum)\n                    prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    def forward_grad(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n            return None, torch.Tensor(c0).cuda()\n        # at least one model\n        middle_feat_cum = None\n        prediction = None\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, prediction = m(x, middle_feat_cum)\n            else:\n                middle_feat_cum, pred = m(x, middle_feat_cum)\n                prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    @classmethod\n    def from_file(cls, path, builder):\n        d = torch.load(path)\n        net = DynamicNet(d['c0'], d['lr'])\n        net.boost_rate = d['boost_rate']\n        for stage, m in enumerate(d['models']):\n            submod = builder(stage)\n            submod.load_state_dict(m)\n            net.add(submod)\n        return net\n\n    def to_file(self, path):\n        models = [m.state_dict() for m in self.models]\n        d = {'models': models, 'c0': self.c0, 'lr': self.lr, 'boost_rate': self.boost_rate}\n        torch.save(d, path)","54803c71":"class MLP_1HL(nn.Module):\n    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n        super(MLP_1HL, self).__init__()\n#         self.in_layer = nn.Linear(dim_in, dim_hidden1)\n#         self.out_layer = nn.Linear(dim_hidden1, 206)\n#         self.lrelu = nn.LeakyReLU(0.1)\n#         self.relu = nn.ReLU()\n        self.layer1 = nn.Sequential(\n                        nn.Dropout(0.2),\n                        nn.Linear(dim_in, dim_hidden1),\n                        )\n        self.layer2 = nn.Sequential(\n                        nn.ReLU(),\n#                         nn.BatchNorm1d(dim_hidden1),\n#                         nn.Dropout(0.4),\n                        nn.Linear(dim_hidden1, 206))\n        if bn:\n            self.bn = nn.BatchNorm1d(dim_hidden1)\n            self.bn2 = nn.BatchNorm1d(dim_in)\n\n    def forward(self, x, lower_f):\n        if lower_f is not None:\n            x = torch.cat([x, lower_f], dim=1)\n            x = self.bn2(x)\n        out = self.layer1(x)\n        return out, self.layer2(out)\n\n    @classmethod\n    def get_model(cls, stage, params):\n        if stage == 0:\n            dim_in = params[\"feat_d\"]\n        else:\n            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n        model = MLP_1HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n        return model\n\n\nclass MLP_2HL(nn.Module):\n    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n        super(MLP_2HL, self).__init__()\n        # self.in_layer = SpLinear(dim_in, dim_hidden1) if sparse else nn.Linear(dim_in, dim_hidden1)\n        # self.dropout_layer = nn.Dropout(0.4)\n        # self.lrelu = nn.LeakyReLU(0.1)\n        # self.relu = nn.ReLU()\n        # self.hidden_layer = nn.Linear(dim_hidden1, dim_hidden2)\n        # self.out_layer = nn.Linear(dim_hidden2, 206)\n        # self.bn = nn.BatchNorm1d(dim_hidden1)\n        self.bn2 = nn.BatchNorm1d(dim_in)\n\n        self.layer1 = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(dim_in, dim_hidden1),\n            nn.ReLU(),\n            nn.BatchNorm1d(dim_hidden1),\n            nn.Dropout(0.4),\n            nn.Linear(dim_hidden1, dim_hidden2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.ReLU(),\n#             nn.Dropout(0.4),\n            nn.Linear(dim_hidden2, 206)\n        )\n\n    def forward(self, x, lower_f):\n        if lower_f is not None:\n            x = torch.cat([x, lower_f], dim=1)\n            x = self.bn2(x)\n        # out = self.lrelu(self.in_layer(x))\n        # out = self.bn(out)\n        # out = self.hidden_layer(out)\n        middle_feat = self.layer1(x)\n        out = self.layer2(middle_feat)\n        return middle_feat, out\n\n    @classmethod\n    def get_model(cls, stage, params):\n        if stage == 0:\n            dim_in = params[\"feat_d\"]\n        else:\n            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n        model = MLP_2HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n        return model","01bfad67":"from torch.nn.modules.loss import _WeightedLoss\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","02feff92":"def get_optim(params, lr, weight_decay):\n    optimizer = optim.Adam(params, lr, weight_decay=weight_decay)\n    #optimizer = SGD(params, lr, weight_decay=weight_decay)\n    return optimizer\n\ndef logloss(net_ensemble, test_loader):\n    loss = 0\n    total = 0\n    loss_f = nn.BCEWithLogitsLoss() # Binary cross entopy loss with logits, reduction=mean by default\n    for data in test_loader:\n        x = data[\"x\"].cuda()\n        y = data[\"y\"].cuda()\n        # y = (y + 1) \/ 2\n        with torch.no_grad():\n            _, out = net_ensemble.forward(x)\n        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n        loss += loss_f(out, y)\n        total += 1\n\n    return loss \/ total\n","e48024f6":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","cec3e3df":"c0_ = np.log(np.mean(train_targets_scored.iloc[:, 1:].values, axis=0))\ndef train_fn(seed=0):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(params[\"n_folds\"]):\n        seed_everything(seed)\n        \n        train_idx = train[train[\"kfold\"] != fold].index\n        val_idx = train[train[\"kfold\"] == fold].index\n        \n        train_df = train[train[\"kfold\"] != fold].reset_index(drop=True)\n        val_df = train[train[\"kfold\"] == fold].reset_index(drop=True)\n        \n        x_train = train_df[feature_cols].values\n        y_train = train_df[target_cols].values  #\n        \n        x_val = val_df[feature_cols].values  #\n        y_val = val_df[target_cols].values  #\n        \n        train_ds = MoADataset(x_train, y_train)\n        val_ds = MoADataset(x_val, y_val)\n        train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=params[\"batch_size\"], shuffle=False)\n        \n        best_score = np.inf\n        val_score = best_score\n        best_stage = params[\"num_nets\"] - 1\n\n        c0 = torch.tensor(c0_, dtype=torch.float).to(device)\n        net_ensemble = DynamicNet(c0, params[\"boost_rate\"])\n        loss_f1 = nn.MSELoss(reduction='none')\n#         loss_f2 = nn.BCEWithLogitsLoss(reduction='none')\n        loss_f2 = SmoothBCEwLogits(smoothing=0.001, reduction=\"none\")\n        loss_models = torch.zeros((params[\"num_nets\"], 3))\n\n        all_ensm_losses = []\n        all_ensm_losses_te = []\n        all_mdl_losses = []\n        dynamic_br = []\n\n        lr = params[\"lr\"]\n        L2 = params[\"weight_decay\"]        \n        \n        early_stop = 0\n        for stage in range(params[\"num_nets\"]):\n            t0 = time.time()\n            #### Higgs 100K, 1M , 10M experiment: Subsampling the data each model training time ############\n            # indices = list(range(len(train)))\n            # split = 1000000\n            # indices = sklearn.utils.shuffle(indices, random_state=41)\n            # train_idx = indices[:split]\n            # train_sampler = SubsetRandomSampler(train_idx)\n            # train_loader = DataLoader(train, opt.batch_size, sampler = train_sampler, drop_last=True, num_workers=2)\n            ################################################################################################\n\n            model = MLP_2HL.get_model(stage, params)  # Initialize the model_k: f_k(x), multilayer perception v2\n            model.to(device)\n\n\n\n            optimizer = get_optim(model.parameters(), lr, L2)\n            net_ensemble.to_train() # Set the models in ensemble net to train mode\n            stage_mdlloss = []\n            for epoch in range(params[\"epochs_per_stage\"]):\n                for i, data in enumerate(train_loader):\n                    x = data[\"x\"].to(device)\n                    y = data[\"y\"].to(device)\n                    middle_feat, out = net_ensemble.forward(x)\n                    # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n                    if params[\"model_order\"] == 'first':\n                        grad_direction = y \/ (1.0 + torch.exp(y * out))\n                    else:\n                        h = 1 \/ ((1 + torch.exp(y * out)) * (1 + torch.exp(-y * out)))\n                        grad_direction = y * (1.0 + torch.exp(-y * out))\n                        # out = torch.as_tensor(out)\n                        nwtn_weights = (torch.exp(out) + torch.exp(-out)).abs()\n                    _, out = model(x, middle_feat)\n                    # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n                    loss = loss_f1(net_ensemble.boost_rate * out, grad_direction)  # T\n                    loss = loss * h\n                    loss = loss.mean()\n                    model.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    stage_mdlloss.append(loss.item()) \n                    \n\n            net_ensemble.add(model)\n            sml = np.mean(stage_mdlloss)\n\n\n            stage_loss = []\n            lr_scaler = 2\n            # fully-corrective step\n            if stage != 0:\n                # Adjusting corrective step learning rate \n                if stage % 3 == 0:\n                    #lr_scaler *= 2\n                    lr \/= 2\n                    # L2 \/= 2\n                optimizer = get_optim(net_ensemble.parameters(), lr \/ lr_scaler, L2)\n                for _ in range(params[\"correct_epoch\"]):\n                    for i, data in enumerate(train_loader):\n                        x = data[\"x\"].to(device)\n                        y = data[\"y\"].to(device)\n\n                        _, out = net_ensemble.forward_grad(x)\n                        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n                        # y = (y + 1.0) \/ 2.0\n                        loss = loss_f2(out, y).mean() \n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n                        stage_loss.append(loss.item())\n                        \n\n            \n            sl_te = logloss(net_ensemble, val_loader)  # -----\n            # Store dynamic boost rate\n            dynamic_br.append(net_ensemble.boost_rate.item())\n            # store model\n            # net_ensemble.to_file(f\".\/{fold}FOLD_{seed}_.pth\")\n            # net_ensemble = DynamicNet.from_file(f\".\/{fold}FOLD_{seed}_.pth\", lambda stage: MLP_2HL.get_model(stage, self.params))\n\n            elapsed_tr = time.time()-t0\n            sl = 0\n            if stage_loss != []:\n                sl = np.mean(stage_loss)\n\n            \n\n            all_ensm_losses.append(sl)\n            all_ensm_losses_te.append(sl_te)\n            all_mdl_losses.append(sml)\n            print(f'Stage - {stage}, training time: {elapsed_tr: .1f} sec, boost rate: {net_ensemble.boost_rate: .4f}, Training Loss: {sl: .5f}, Val Loss: {sl_te: .5f}')\n\n\n            net_ensemble.to_cuda()\n            net_ensemble.to_eval() # Set the models in ensemble net to eval mode\n\n            # Train\n            # print('Acc results from stage := ' + str(stage) + '\\n')\n            # AUC\n#             val_score = auc_score(net_ensemble, val_loader) \n            if sl_te < best_score:\n                best_score = sl_te\n                best_stage = stage\n                net_ensemble.to_file(f\".\/{fold}FOLD_{seed}_.pth\")\n                early_stop = 0\n            else:\n                \n                early_stop += 1\n                \n\n#             test_score = auc_score(net_ensemble, val_loader)\n#             print(f'Stage: {stage}, AUC@Val: {val_score:.4f}, AUC@Test: {test_score:.4f}')\n\n#             loss_models[stage, 1], loss_models[stage, 2] = val_score, test_score\n            \n    \n            if early_stop > params[\"early_stopping_steps\"]:\n                print(\"early stopped!\")\n                break\n\n#         val_auc, te_auc = loss_models[best_stage, 1], loss_models[best_stage, 2]\n        print(f'Best validation stage: {best_stage}')\n\n        net_ensemble = DynamicNet.from_file(f\".\/{fold}FOLD_{seed}_.pth\", lambda stage: MLP_2HL.get_model(stage, params))\n        net_ensemble.to_cuda()\n        net_ensemble.to_eval()\n\n        preds = []\n        with torch.no_grad():\n            for data in val_loader:\n                x = data[\"x\"].to(device)\n                _, pred = net_ensemble.forward(x)\n                preds.append(pred.sigmoid().detach().cpu().numpy())\n        oof[val_idx, :] = np.concatenate(preds)\n\n        x_test = test[feature_cols].values\n        test_ds = TestDataset(x_test)\n        test_loader = DataLoader(test_ds, batch_size=params[\"batch_size\"], shuffle=False)\n\n        preds = []\n        with torch.no_grad():\n            for data in test_loader:\n                x = data[\"x\"].to(device)\n                _, pred = net_ensemble.forward(x)\n                preds.append(pred.sigmoid().detach().cpu().numpy())\n        predictions += np.concatenate(preds) \/ params[\"n_folds\"]\n        \n    oof = np.clip(oof, 1e-3, 1 - 1e-3)\n    predictions = np.clip(predictions, 1e-3, 1 - 1e-3)\n\n\n    train[target_cols] = oof\n    test[target_cols] = predictions\n\n    val_results = train_targets_scored.drop(columns=target_cols).merge(train[[\"sig_id\"] + target_cols], on=\"sig_id\", how=\"left\").fillna(0)\n\n    y_true = train_targets_scored[target_cols].values\n    y_pred = val_results[target_cols].values\n\n    score = 0\n    for i in range(len(target_cols)):\n        score_ = log_loss(y_true[:, i], y_pred[:, i])\n        score += score_ \/ len(target_cols)\n#     score = score\n    print(\"CV log_loss \", score)\n\n    sub = sample_submission\n    sub = sub.drop(columns=target_cols).merge(test[[\"sig_id\"]+target_cols], on=\"sig_id\", how=\"left\").fillna(0)\n\n#         sub = sub.drop(columns=self.target_cols).merge(test_[[\"sig_id\"]+self.target_cols+[\"cp_time_24\", \"cp_dose_D2\"]], on=\"sig_id\", how=\"left\").fillna(0)\n#         sub.loc[:, [\"atp-sensitive_potassium_channel_antagonist\", \"erbb2_inhibitor\"]] = 0.000012\n#         sub = sub.drop([\"cp_time_24\", \"cp_dose_D2\"], axis=1)\n    return sub","12f176ab":"sub = train_fn()","293805d3":"sub","626910cb":"sub.to_csv(\"submission.csv\", index=False)","376e5833":"# Dataset Classes","fbdb038b":"# CV strategy ","336dbe60":"# Weak Models","6efb25b8":"# Import Libraries","3f6e5beb":"# Submission","ef108f5d":"# Preprocessing","f15380cc":"# Parameters","ecb249ed":"# GrowNet (Gradient Boosting Neural Networks)\n\n## What is GrowNet?\n![image.png](attachment:image.png)\n\n*A novel gradient boosting framework is proposed where shallow neural networks\nare employed as \u201cweak learners\u201d. General loss functions are considered under this\nunified framework with specific examples presented for classification, regression\nand learning to rank. A fully corrective step is incorporated to remedy the pitfall\nof greedy function approximation of classic gradient boosting decision tree. The\nproposed model rendered outperforming results against state-of-the-art boosting\nmethods in all three tasks on multiple datasets. An ablation study is performed to\nshed light on the effect of each model components and model hyperparameters.*  \n(from Abstract part of paper)\n\nThe idea of GrowNet is simple.\nIn contrast to Gradient Boosting Decision Tree (GBDT), Grownet employs Neural Networks as weak learners. \nGrowNet uses the outputs of the previous learners as inputs.\nThe paper shows GrowNet performed better than XGBoost.\n\nSee the links for more details.\n\ngithub  \nhttps:\/\/github.com\/sbadirli\/GrowNet\n\narxiv  \nhttps:\/\/arxiv.org\/pdf\/2002.07971\n\n## About this notebook\nThis notebook introduces an implemention of GrowNet. The implemention is based classification task on github. The original implemention is for one label, but this competition has 206 labels. So I fixed some part of the original implemention.  \nI don't do special feature engineering in this notebook because this is just an implemention of GrowNet. What I did is\n* put PCA features\n* apply label smoothing\n* use Chris's new CV strategy  \n* use 2 hidden layers NN as a week model  \n* clip predictions as postprocessing  \n\nFeel free to add more feature engineerings.\n\noriginal implemention(PyTorch)  \nhttps:\/\/github.com\/sbadirli\/GrowNet\/tree\/master\/Classification\n\nIn this competition, most people say NN performs the best on both CV and LB while other algorisms don't perform as well as NN. So mixture of gradient boosting and NN might be suitable for this task. ","4e306036":"# Dynamic Model","5463a416":"# Training"}}