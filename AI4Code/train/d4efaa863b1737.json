{"cell_type":{"ec81d27a":"code","bad6986b":"code","bba29692":"code","bf30fcd9":"code","5296fd4e":"code","08100a5d":"code","da645a34":"code","9f078023":"code","135aff72":"code","62d5d532":"code","7af08eee":"code","3c4700ff":"code","eb21e062":"code","073f6863":"code","1b701377":"code","604f3f6e":"code","666dec09":"code","69a885fe":"code","1a20ddc2":"code","8ad6b0d8":"code","c3b0970a":"code","ba4aac27":"code","3dff6c29":"code","ff44928c":"code","df8ba593":"code","183f83f3":"code","01e1d966":"code","a6337485":"code","fe059f8c":"code","af695fb3":"code","e7d9ba18":"code","383c8c58":"code","4591533e":"code","f9b95eb5":"markdown","4651bc22":"markdown","7bf3a57f":"markdown","ff79e52e":"markdown"},"source":{"ec81d27a":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile\nfrom scipy.io import wavfile\nimport pandas as pd\nfrom joblib import Parallel, delayed","bad6986b":"frequency_sampling, audio_signal  = wavfile.read(\"..\/input\/automatic-speech-recognition-in-wolof\/audio_wav_16000\/tmp\/WOLOF_ASR_dataset\/audio_wav_16000\/0047cd812e3db0cf28588f86b5c218822b556e6f9bc8c986919296da4b11e60080938fa69151bcb119b287b51db5e8bf6f6a82087f629cf03cfe86efab2c37af.wav\")","bba29692":"print('\\nSignal shape:', audio_signal.shape)\nprint('Signal Datatype:', audio_signal.dtype)\nprint('Signal duration:', round(audio_signal.shape[0] \/ \nfloat(frequency_sampling), 2), 'seconds')","bf30fcd9":"audio_signal = audio_signal \/ np.power(2, 15)","5296fd4e":"audio_signal = audio_signal [:1000]\ntime_axis = 1000 * np.arange(0, len(audio_signal), 1) \/ float(frequency_sampling)","08100a5d":"plt.plot(time_axis, audio_signal, color='blue')\nplt.xlabel('Time (milliseconds)')\nplt.ylabel('Amplitude')\nplt.title('Input audio signal')\nplt.show()","da645a34":"length_signal = len(audio_signal)\nhalf_length = np.ceil((length_signal + 1) \/ 2.0).astype(np.int)","9f078023":"signal_frequency = np.fft.fft(audio_signal)","135aff72":"signal_frequency = abs(signal_frequency[0:half_length]) \/ length_signal\nsignal_frequency **= 2","62d5d532":"len_fts = len(signal_frequency)","7af08eee":"if length_signal % 2:\n   signal_frequency[1:len_fts] *= 2\nelse:\n   signal_frequency[1:len_fts-1] *= 2","3c4700ff":"signal_power = 10 * np.log10(signal_frequency)","eb21e062":"x_axis = np.arange(0, half_length, 1) * (frequency_sampling \/ length_signal) \/ 1000.0","073f6863":"plt.figure()\nplt.plot(x_axis, signal_power, color='red')\nplt.xlabel('Frequency (kHz)')\nplt.ylabel('Signal power (dB)')\nplt.show()","1b701377":"!pip install SpeechRecognition","604f3f6e":"import speech_recognition as sr\nsr.__version__","666dec09":"r = sr.Recognizer()","69a885fe":"bangla1 = sr.AudioFile('..\/input\/automatic-speech-recognition-in-wolof\/audio_wav_16000\/tmp\/WOLOF_ASR_dataset\/audio_wav_16000\/0047cd812e3db0cf28588f86b5c218822b556e6f9bc8c986919296da4b11e60080938fa69151bcb119b287b51db5e8bf6f6a82087f629cf03cfe86efab2c37af.wav')","1a20ddc2":"with bangla1 as source:\n    audio = r.record(source)","8ad6b0d8":"type(audio)","c3b0970a":"#using google web speech API\nr.recognize_google(audio)","ba4aac27":"import os\nfrom pydub import AudioSegment\n\npath = \"..\/input\/automatic-speech-recognition-in-wolof\/audio_wav_16000\/tmp\/WOLOF_ASR_dataset\/audio_wav_16000\/\"\n\n#Change working directory\n# os.chdir(path)\n\naudio_files = os.listdir(path )","3dff6c29":"bangla2 = sr.AudioFile('..\/input\/automatic-speech-recognition-in-wolof\/audio_wav_16000\/tmp\/WOLOF_ASR_dataset\/audio_wav_16000\/005cea38ff6e71a7d1047aba774790ad04336b73fed6c02ebd144060725ada6861acf7821c9abfa408067dd345078c2eb74e34535e690957ade04ea985a13bff.wav')","ff44928c":"with bangla2 as source:\n    audio2 = r.record(source)","df8ba593":"type(audio2)","183f83f3":"r.recognize_google(audio2,language = \"FR\")","01e1d966":"from tqdm import tqdm\nimport os","a6337485":"root=\"..\/input\/automatic-speech-recognition-in-wolof\/audio_wav_16000\/tmp\/WOLOF_ASR_dataset\/audio_wav_16000\"","fe059f8c":"def save_fn(filename):\n    \n    path = f\"{root}\/{filename}.wav\"\n    if os.path.exists(path):\n        try:\n            wolof = sr.AudioFile(path)\n            with wolof as source:\n                audio2 = r.record(source)\n            transcript = r.recognize_google(audio2,language = \"fr\")\n        \n            return transcript\n        except:\n            print(path)","af695fb3":"wolof_trans","e7d9ba18":"\n\nTest = pd.read_csv(\"..\/input\/automatic-speech-recognition-in-wolof\/Test.csv\")\ntest_list = list(Test.ID.values)\nwolof_trans = Parallel(n_jobs=8, backend=\"multiprocessing\")(\n    delayed(save_fn)(filename) for filename in tqdm(test_list)\n)  \n   ","383c8c58":"\nSub = pd.read_csv(\"..\/input\/automatic-speech-recognition-in-wolof\/SampleSubmission.csv\")\nSub[\"transcription\"] = wolof_trans\nSub.head()","4591533e":"Sub.to_csv(\"submission.csv\",index=False)","f9b95eb5":"# Installing libraries","4651bc22":"# Characterizing the audio signal\nConverting the time domain signal into frequency domain. This is important because it gives a lot of information about the signal. A mathematical tool like fourier transformation can be used for this.","7bf3a57f":"# Loading an audio file","ff79e52e":"# Recognizer class"}}