{"cell_type":{"223a57aa":"code","50d757ec":"code","afc5ac52":"code","d38e0e59":"code","79433814":"code","135a3686":"code","084ab877":"code","6bfeddfd":"code","3b979a1a":"code","bc13385d":"code","7cf5d4c8":"code","6c4c7847":"code","95fc494a":"code","b0b50025":"code","70a49768":"code","7412b951":"code","36a80fd6":"code","23ca9551":"code","4e0d4be0":"code","57796ae9":"code","d3e57b8d":"code","324a57f3":"code","6342ea9c":"code","530e861a":"code","bfc1c1b6":"code","c5324d17":"code","153aba59":"code","5b1cca98":"code","7374e521":"markdown","f0ddc0de":"markdown"},"source":{"223a57aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport scipy.stats\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nfrom ml_metrics import rmse\nfrom sklearn.linear_model import LinearRegression\nfrom ml_metrics import rmse\nfrom sklearn.feature_selection import mutual_info_classif\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50d757ec":"train_df = pd.read_csv('..\/input\/neolen-house-price-prediction\/train.csv',index_col = 'Id')\ntest_df = pd.read_csv('..\/input\/neolen-house-price-prediction\/test.csv',index_col = 'Id')\ntrain_df.head()\n","afc5ac52":"train_df.info()","d38e0e59":"train_df.describe()\n","79433814":"train_df.var()\n","135a3686":"#pd.set_option(\"max_rows\", None)\n\ntrain_df.isnull().sum()\n","084ab877":"train_df['LotFrontage'].value_counts()","6bfeddfd":"train_df.boxplot('LotFrontage')\nplt.show()","3b979a1a":"train_df.hist('LotFrontage')\nplt.show()\ntrain_df['LotFrontage'].mean()","bc13385d":"_=plt.scatter(train_df['SalePrice'],train_df['Exterior2nd'].astype('str'))\nplt.show()","7cf5d4c8":"_=plt.hist(train_df['MasVnrArea'],bins=100)\nplt.show()","6c4c7847":"_=plt.scatter(train_df['LotArea'],train_df['PoolQC'].astype('str'))\nplt.show()","95fc494a":"#draw the pairplot to be able to select effective features  \ncol = train_df.select_dtypes('int64').columns\nfor i in col[:-1]:\n    sns.pairplot(train_df[[i,'SalePrice']])\n    plt.show()","b0b50025":"obj_cols = train_df.select_dtypes('object').columns\nobj_cols\n","70a49768":"column = ['ExterQual','ExterCond', 'FireplaceQu','KitchenQual', 'HeatingQC','BsmtCond','BsmtQual','GarageQual','GarageCond']\noe = OrdinalEncoder (categories = [[ 'nan' , 'Po' , 'Fa', 'TA','Gd','Ex' ]])\nfor col in column:\n    train_df[col] = oe.fit_transform(train_df[col].astype('str').values.reshape(-1, 1))\n    test_df[col] = oe.fit_transform(test_df[col].astype('str').values.reshape(-1, 1))\n\n","7412b951":"# lotFrontage: Linear feet of street connected to property\n# It got confused between NA as No alley and NA not a number\ntrain_df['Alley'].fillna('NAA', inplace=True) # replace with no alley access NAA\ntest_df['Alley'].fillna('NAA', inplace=True) # replace with no alley access NAA\n\ntrain_df['FireplaceQu'].fillna('NFP', inplace=True) # replace with no fireplace NFP\ntest_df['FireplaceQu'].fillna('NFP', inplace=True) # replace with no fireplace\n\ntrain_df['MasVnrType'].fillna('None', inplace=True) # None\ntest_df['MasVnrType'].fillna('None', inplace=True) # None\n\ntrain_df['PoolQC'].fillna('NP', inplace=True) # No Pool\ntest_df['PoolQC'].fillna('NP', inplace=True) # No Pool\n\ntrain_df['Fence'].fillna('NF', inplace=True) # No Fence\ntest_df['Fence'].fillna('NF', inplace=True) # No Fence\n\ntrain_df['MiscFeature'].fillna('None', inplace=True) # NONE\ntest_df['MiscFeature'].fillna('None', inplace=True) # NONE\n\n\nNB_columns = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']\nfor col in NB_columns:\n    train_df[col].fillna('NB', inplace=True) # No basement\n    test_df[col].fillna('NB', inplace=True) # No basement\n    \nNG_columns = ['GarageType','GarageFinish','GarageQual','GarageCond']\nfor col in NG_columns:\n    train_df['GarageType'].fillna('NG', inplace=True) # No Garage\n    test_df['GarageType'].fillna('NG', inplace=True) # No Garage","36a80fd6":"train_df.isnull().sum()\n","23ca9551":"train_df = pd.get_dummies(train_df,columns=obj_cols)\ntest_df = pd.get_dummies(test_df,columns=obj_cols)\n\n","4e0d4be0":"train_df.info()","57796ae9":"# initiate an Imputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n# start imputing\ntrain_median = train_df['LotFrontage'].median()\ntrain_df = pd.DataFrame(imputer.fit_transform(X = train_df, y = None), columns=train_df.columns)\ntest_df.fillna(train_median,inplace=True)\ntrain_df.isnull().sum()\nx_train_df,x_valid_df,y_train_df,y_valid_df = train_test_split(train_df,train_df['SalePrice'],train_size=0.8, test_size=0.2,random_state=0)\nx_train_df.drop(['SalePrice'], axis = 1, inplace = True)\nx_valid_df.drop(['SalePrice'], axis = 1, inplace = True)\n\n","d3e57b8d":"train_df.info()","324a57f3":"test_df.info()","6342ea9c":"MI_score = mutual_info_classif(x_train_df, y_train_df, random_state=0)\nfeature_names = x_train_df.columns\nselected_Features = []\n# Print the name and mutual information score of each feature\nfor feature in list(zip(feature_names, MI_score)):\n    if feature[1] >.2:\n        selected_Features.append(feature[0]) \nselected_Features","530e861a":"from xgboost import XGBRegressor\nxgbReg=XGBRegressor(seed=123,objective=\"reg:linear\",n_estimators=1000,max_depth = 15)\nxgbReg.fit(x_train_df[selected_Features], y_train_df)\npreds_valid=xgbReg.predict(x_valid_df[selected_Features])\nr_sq =rmse(y_valid_df, preds_valid)\nprint('Score', r_sq)\n","bfc1c1b6":"from sklearn.experimental import enable_hist_gradient_boosting\n\nfrom  sklearn.ensemble import HistGradientBoostingRegressor\n# Create a model\nmodel = HistGradientBoostingRegressor(\n    max_iter = 40,\n    max_depth =20,\n    max_leaf_nodes = 15,\n    max_bins = 200)\n# Fit the model\nmodel.fit(x_train_df[selected_Features], y_train_df)\n# Get the R-squared\npreds_valid = model.predict(x_valid_df[selected_Features])\n\nr_sq =rmse(y_valid_df, preds_valid)\nprint('Score', r_sq)\n","c5324d17":"test_df.head()","153aba59":"\npreds_test = model.predict(test_df[selected_Features])\noutput = pd.DataFrame({'Id': test_df.index,\n                      'SalePrice': preds_test})\n#output['SalePrice'] = np.exp(output['SalePrice'])\noutput.to_csv('submission.csv', index=False)\nprint('done')","5b1cca98":"output.head()","7374e521":"# Handle The Categorical Columns ","f0ddc0de":"# Some Exploration to understand our Data\u00b6\n"}}