{"cell_type":{"848c6867":"code","288d7124":"code","73da603d":"code","e67bef19":"code","12bfa43b":"code","1d000126":"code","87531270":"code","77bf8a5f":"code","a25ce04d":"code","bee4ebc9":"code","22fa72b4":"code","69c6a24c":"code","ec8f035c":"code","2cecc6ed":"code","ecdbb9ca":"code","cd781c28":"code","eddf2574":"code","40bc1e8b":"code","d9365385":"code","02dd8e91":"code","724a01be":"code","4750ad9e":"code","0d859fa5":"code","dead012d":"code","4d8bf9c9":"code","caf4d8f2":"code","96d90997":"code","ce851fe7":"code","c65dd292":"code","fe6a0c2f":"markdown","d653ef1d":"markdown","90dce6cc":"markdown","aac483d6":"markdown","1387ab0c":"markdown","ace5d2ce":"markdown","171e4c9e":"markdown","4e756611":"markdown","400a9049":"markdown","24157a38":"markdown","d4f6f8b9":"markdown","cdc8bfbc":"markdown","2d97d7cb":"markdown","27137860":"markdown","6773cfbf":"markdown","ed244b09":"markdown","827b2121":"markdown","5c6544d2":"markdown","830e8170":"markdown","23951398":"markdown","4980098b":"markdown"},"source":{"848c6867":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve,precision_recall_curve, auc,confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.impute import KNNImputer\n\nfrom xgboost import XGBClassifier\n\nfrom catboost import CatBoostClassifier\n","288d7124":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","73da603d":"df.head()","e67bef19":"df = df.drop('id', axis=1)\ndf.head()","12bfa43b":"df.info()","1d000126":"df.isnull().sum()","87531270":"labels =df['stroke'].value_counts(sort = True).index\nsizes = df['stroke'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\nexplode = (0.05,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Number of stroke in the dataset')\nplt.show()","77bf8a5f":"#  Using Lable encoding\nle = LabelEncoder()\nen_df = df.apply(le.fit_transform)\nen_df.head()","a25ce04d":"def plot_hist(col, bins=30, title=\"\",xlabel=\"\",ax=None):\n    sns.distplot(col, bins=bins,ax=ax)\n    ax.set_title(f'Histogram of {title}',fontsize=20)\n    ax.set_xlabel(xlabel)","bee4ebc9":"fig, axes = plt.subplots(1,3,figsize=(11,7),constrained_layout=True)\nplot_hist(df.bmi,\n          title='Bmi',\n          xlabel=\"Level of the BMI\",\n          ax=axes[0])\nplot_hist(df.age,\n          bins=30,\n          title='Age',\n          xlabel='Age',\n          ax=axes[1])\nplot_hist(df.avg_glucose_level,\n          title='Serum Creatinine', \n          xlabel='Level of serum creatinine in the blood (mg\/dL)',\n          ax=axes[2])\n\nplt.show()\n\n","22fa72b4":"sns.catplot(y=\"work_type\", hue=\"stroke\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df)","69c6a24c":"sns.catplot(y=\"smoking_status\", hue=\"stroke\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df)","ec8f035c":"plt.figure(figsize=(17,7))\nsns.catplot(x=\"gender\", y=\"stroke\", hue=\"heart_disease\", palette=\"pastel\", kind=\"bar\", data=df)\nsns.catplot(x=\"gender\", y=\"stroke\", hue=\"Residence_type\", palette=\"pastel\", kind=\"bar\", data=df)\nsns.catplot(x=\"gender\", y=\"stroke\", hue=\"hypertension\", palette=\"pastel\", kind=\"bar\", data=df)\nplt.show()\n","2cecc6ed":"len_data = len(df)\nlen_w = len(df[df[\"gender\"]==\"Male\"])\nlen_m = len_data - len_w\n\nmen_stroke = len(df.loc[(df[\"stroke\"]==1)&(df['gender']==\"Male\")])\nmen_no_stroke = len_m - men_stroke\n\nwomen_stroke = len(df.loc[(df[\"stroke\"]==1) & (df['gender']==\"Female\")])\nwomen_no_stroke = len_w - women_stroke\n\nlabels = ['Men with stroke','Men healthy','Women with stroke','Women healthy']\nvalues = [men_stroke, men_no_stroke, women_stroke, women_no_stroke]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,textinfo='label+percent',hole=0.4)])\nfig.update_layout(\n    title_text=\"Distribution of stroke EVENT according to their gender\")\nfig.show()","ecdbb9ca":"df.columns","cd781c28":"fig = px.parallel_categories(df[['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type',\n       'smoking_status', 'stroke']], color='stroke', color_continuous_scale=px.colors.sequential.Inferno)\nfig.show()\n\n","eddf2574":"features=['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type',\n       'smoking_status']\nfrom matplotlib.offsetbox import AnchoredText\ncorrelation_table = []\nfor cols in features:\n    y = en_df[\"stroke\"]\n    x = en_df[cols]\n    corr = np.corrcoef(x, y)[1][0]\n    dict ={\n        'Features': cols,\n        'Correlation coefficient' : corr,\n        'Feat_type': 'numerical'\n    }\n    correlation_table.append(dict)\ndF1 = pd.DataFrame(correlation_table)\nfig = plt.figure(figsize=(10,6), facecolor='#EAECEE')\nax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n                     data=dF1.sort_values(\"Correlation coefficient\", ascending=False),\n                     palette='viridis', alpha=0.75)\nax.grid()\n#ax.set_title(\"Correlation of numerical features with Target\", fontsize=20, y=1.05)\n\ntitle =  'Correlation features with target'\nsub_title = 'In comparison with categorical features \\\n\\nnumericals are less correlated with target.'\n\nplt.gcf().text(0.05, 1.02, title, fontsize=24)\n#plt.gcf().text(0.05, 0.9, sub_title, fontsize=14)\n\nat1 = AnchoredText(sub_title,\n                   loc='lower left', frameon=True,\n                   bbox_to_anchor=(-0.1, 1.01),\n                   bbox_transform=ax.transAxes,\n                   #prop=dict(size=8),\n                   )\nat1.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\nax.add_artist(at1)","40bc1e8b":"plt.figure(figsize=(16,8))\nsns.heatmap(en_df.corr(),cmap=\"Blues\");","d9365385":"from sklearn.ensemble import ExtraTreesClassifier\n\nX = en_df[features]\ny = en_df['stroke']\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    \n    \n# Plot the impurity-based feature importances of the forest\nplt.figure()\n\nplt.title(\"Feature importances\")\nsns.barplot(x=np.array(features)[indices], y=importances[indices], palette=\"deep\",yerr=std[indices])\nplt.xticks(range(X.shape[1]), np.array(features)[indices],rotation=60)\nplt.xlim([-1, X.shape[1]])\nplt.show()","02dd8e91":"en_df_imputed = en_df\nimputer = KNNImputer(n_neighbors=4, weights=\"uniform\")\nimputer.fit_transform(en_df_imputed)\n","724a01be":"en_df_imputed.isnull().sum()","4750ad9e":"from imblearn.over_sampling import SMOTE\nX , y = en_df_imputed[features],en_df_imputed[\"stroke\"]\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=23)\nsm = SMOTE()\nX_res, y_res = sm.fit_resample(x_train,y_train)\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_res==0)))","0d859fa5":"def plot_cm(cm,title):\n    z = cm\n    x = ['No stroke', 'stroke']\n    y = x\n    # change each element of z to type string for annotations\n    z_text = [[str(y) for y in x] for x in z]\n\n    # set up figure \n    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='deep')\n\n    # add title\n    fig.update_layout(title_text='<i><b>Confusion matrix {}<\/b><\/i>'.format(title),\n                      #xaxis = dict(title='x'),\n                      #yaxis = dict(title='x')\n                     )\n\n    # add custom xaxis title\n    fig.add_annotation({'font':{'color':\"black\",'size':14},\n                            'x':0.5,\n                            'y':-0.10,\n                            'showarrow':False,\n                            'text':\"Predicted value\",\n                            'xref':\"paper\",\n                            'yref':\"paper\"})\n    \n    fig.add_annotation({'font':{'color':\"black\",'size':14},\n                            'x':-0.15,\n                            'y':0.5,\n                            'showarrow':False,\n                            'text':\"Real value\",\n                            'textangle':-90,\n                            'xref':\"paper\",\n                            'yref':\"paper\"})\n\n\n    # adjust margins to make room for yaxis title\n    fig.update_layout(margin={'t':50, 'l':20},width=750,height=750)\n    \n\n\n    # add colorbar\n    fig['data'][0]['showscale'] = True\n    fig.show()\n\n\n\ndef hist_score(score):\n    models_names = [\n        'Logistic Regression',\n    'KNearest Neighbor',\n    'Decision Tree Classifier',\n    'Random Forest Classifier',\n    'Ada Boost',\n    'SVM',\n    'XG Boost',\n    'Cat Boost']\n\n    plt.rcParams['figure.figsize']=20,8\n    sns.set_style('darkgrid')\n    ax = sns.barplot(x=models_names, y=score, palette = \"inferno\", saturation =2.0)\n    plt.xlabel('Classifier Models', fontsize = 20 )\n    plt.ylabel('% of Accuracy', fontsize = 20)\n    plt.title('Accuracy of different Classifier Models on test set', fontsize = 20)\n    plt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\n    plt.yticks(fontsize = 12)\n    for i in ax.patches:\n        width, height = i.get_width(), i.get_height()\n        x, y = i.get_xy() \n        ax.annotate(f'{round(height,2)}%', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\n    plt.show()\n\n\n\n","dead012d":"def run_exp_on_feature(x_train,y_train,x_test,y_test):\n    #x_train,x_test,y_train,y_test = train_test_split(features,labels, test_size=0.2, random_state=23)\n    models= [['Logistic Regression ',LogisticRegression()],\n            ['KNearest Neighbor ',KNeighborsClassifier()],\n            ['Decision Tree Classifier ',DecisionTreeClassifier()],\n            ['Random Forest Classifier ',RandomForestClassifier()],\n            ['Ada Boost ',AdaBoostClassifier()],\n            ['SVM ',SVC()],\n            ['XG Boost',XGBClassifier()],\n            ['Cat Boost',CatBoostClassifier(logging_level='Silent')]]\n\n    models_score = []\n    for name,model in models:\n\n        model = model\n        model.fit(x_train,y_train)\n        model_pred = model.predict(x_test)\n        cm_model = confusion_matrix(y_test, model_pred)\n        print(cm_model)\n        models_score.append(accuracy_score(y_test,model.predict(x_test)))\n\n        print(name)\n        print('Validation Acuuracy: ',accuracy_score(y_test,model.predict(x_test)))\n        print('Training Accuracy: ',accuracy_score(y_train,model.predict(x_train)))\n        print('############################################')\n        plot_cm(cm_model,title=name+\"model\")\n        fpr, tpr, thresholds = roc_curve(y_test, model_pred)\n\n        fig = px.area(\n            x=fpr, y=tpr,\n            title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n            labels={'x':'False Positive Rate', 'y':'True Positive Rate'},\n            width=700, height=500\n        )\n        fig.add_shape(\n            type='line', line={'dash':'dash'},\n            x0=0, x1=1, y0=0, y1=1\n        )\n\n        fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n        fig.update_xaxes(constrain='domain')\n        fig.show()\n    \n        \n    return models_score\n\n\n","4d8bf9c9":"models_score = run_exp_on_feature(x_train,y_train,x_test,y_test)","caf4d8f2":"hist_score(models_score)","96d90997":"SEED=21\ndef xgb_tuning(new_features,labels):\n    list_scores=[]    \n    parameters = {\n        \n        'max_depth': [3, 5, 7, 9], \n        'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n        'learning_rate': [0.01, 0.05, 0.1]\n    }\n\n    model_xgb = XGBClassifier(objective = \"binary:logistic\",\n               eval_metric = \"logloss\",\n               learning_rate = 0.1,\n        random_state=SEED,\n    )\n\n    scores = ['f1','precision', 'recall']\n\n    for score in scores:\n        print(\"# Tuning hyper-parameters for %s\" % score)\n        print()\n\n        clf = GridSearchCV(\n            model_xgb, parameters,cv=5, scoring='%s_macro' % score\n        )\n        clf.fit(new_features, labels)\n\n        print(clf.cv_results_['mean_test_score']) \n        print(clf.cv_results_['std_test_score'])\n\n        list_scores.append([clf.cv_results_['mean_test_score'][clf.best_index_],clf.cv_results_['std_test_score'][clf.best_index_]])\n    return clf,list_scores","ce851fe7":"clf,list_scores = xgb_tuning(x_train,y_train)","c65dd292":"list_scores","fe6a0c2f":"<div class=\"alert alert-warning\" role=\"alert\">\n  <h4 class=\"alert-heading\">Observation \ud83d\udd0e\ud83d\udd0e\ud83d\udd0e.<\/h4>\n  <p>\ud83d\udccc 1. We can see from the plots that the gender is not a feature that descriminate a person having a stroke or not.<\/p>\n  <hr>\n  <p>\ud83d\udccc 2. Hypertension and heart disease features are correlated to stroke as we could expect.<\/p>\n  <hr>\n  <p>\ud83d\udccc 3. Rural person are less prone to strole than urban people, it seems that the polution is a correlatede to having stroke.<\/p>\n  <hr>\n   \n<\/div>\n","d653ef1d":"<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\">TABLE OF CONTENTS<\/h2>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#intro\">0&nbsp;&nbsp;&nbsp;&nbsp;Introduction<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_motivation\">I&nbsp;&nbsp;&nbsp;&nbsp;Background and motivation<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#package\">II&nbsp;&nbsp;&nbsp;&nbsp;Package<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#firstlook\">III&nbsp;&nbsp;&nbsp;&nbsp;First look on the data<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#EDA\">IV&nbsp;&nbsp;&nbsp;&nbsp;Exploratory data analasis<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#feature_to_target\">V&nbsp;&nbsp;&nbsp;&nbsp;Feature correlation to target <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#fixing_dataset\">VI&nbsp;&nbsp;&nbsp;&nbsp;Fixing the dataset (missing values and umbalanced target) <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#modeling\">VII&nbsp;&nbsp;&nbsp;&nbsp;Modeling<\/a><\/h3>\n\n---\n\n","90dce6cc":"<h1 class=\"list-group-item list-group-item-action active\" id=\"EDA\">IV&nbsp;&nbsp;Exploratory data analysis (EDA)<\/h1>","aac483d6":"<h1 class=\"list-group-item list-group-item-action active\" id=\"intro\">0&nbsp;&nbsp;Introduction<\/h1>","1387ab0c":"<div class=\"alert alert-info\" role=\"alert\">\n  I really like to plot this kind of map in order to have global view what brings you to a certain, in our case is stroke.\n<\/div>","ace5d2ce":"<h1 class=\"list-group-item list-group-item-action active\" id=\"firstlook\">III&nbsp;&nbsp;First look of the data<\/h1>","171e4c9e":"## Thank you for watching this notenook I hope you enjoyed it. Do not hesitate to give me feedback from your own analysis :D Stay safe !","4e756611":"<h1 class=\"list-group-item list-group-item-action active\" id=\"package\">II&nbsp;&nbsp;The Package<\/h1>","400a9049":"- Background: **A stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes.**\n  **A stroke is a medical emergency, and prompt treatment is crucial. Early action can reduce brain damage and other complications.**\n    **The good news is that many fewer Americans die of stroke now than in the past. Effective treatments can also help prevent disability from stroke.**\n\n- Motivation : **Our objective is to understand what are the reasons that cause stroke to peoeple and see if we can succefully detect stroke on some features using ML technics**","24157a38":"<div class=\"alert alert-success\" role=\"alert\">\n  From the histogramme we can report that decision three algorithms are performing better than the others, lets use some grid search and cv with Xgboost algoritm to see what we can get.\n<\/div>\n","d4f6f8b9":"![](https:\/\/media.giphy.com\/media\/3otPoNMCHgEfLaO1NK\/giphy.gif)","cdc8bfbc":" <div class=\"alert alert-success\" role=\"alert\">\n  <p>\ud83d\udca1 In order to make the dataset balanced we will use the package SMOTE for oversampling. Moreover to fix the missing values in the BMI columns we use a imputation technics based on the KNN. <\/p>\n  <hr>\n<\/div>\n\n","2d97d7cb":"<h1 class=\"list-group-item list-group-item-action active\" id=\"feature_to_target\">V&nbsp;&nbsp;Feature correlation to target<\/h1>","27137860":"\n<div class=\"alert alert-danger\" role=\"alert\">\n  <h4 class=\"alert-heading\">\u26d4\ufe0f\u26d4\ufe0f\u26d4\ufe0f<\/h4>\n  <p>Just by this brief looking we report two things this dataset is very unbanlaced and also we go some missing values on the BMI feauture.<\/p>\n  <hr>\n  <p class=\"mb-0\">Before using any modeling we should solve issues<\/p>\n<\/div>\n\n","6773cfbf":"<div class=\"alert alert-warning\" role=\"alert\">\n  <h4 class=\"alert-heading\">Observation \ud83d\udd0e\ud83d\udd0e\ud83d\udd0e.<\/h4>\n  <p> \ud83d\udccc 1. In term of proportion private and self-employed have the similar amount of people having a stroke. However people from the gouvernment are more likely to not have a stroke compared to both first gategories moreover chlidren are not very likekly to get a stroke. Maybe that could be explain due to the degree of pressure felt by workers<\/p>\n  <hr>\n  <p class=\"mb-0\"> \ud83d\udccc 2. Surprisingly, it seems that the stroke is not highly corralated to smokers since the proportion of person having a stroke is fairly the same among the different smoking status.<\/p>\n<\/div>","ed244b09":"<h1 class=\"list-group-item list-group-item-action active\" id=\"modeling\">VII&nbsp;&nbsp;Modeling<\/h1>","827b2121":"<div class=\"jumbotron\">\n  <h1 class=\"display-4\">Hello, Everyone!<\/h1>\n  <p class=\"lead\">We will learn some important information about stroke and what are the \"common\" trait of people having stroke. In this kernel we will be using some classical ML algorithms in order to detecting stroke using. I hope you will enjoy it :)<\/p>\n  <hr class=\"my-4\">\n  <p>Arthors: Rachid Zeghlache.<\/p>\n<\/div>","5c6544d2":"<h1 class=\"list-group-item list-group-item-action active\" id=\"fixing_dataset\">VI&nbsp;&nbsp;Fixing the dataset (Missing values and umblanced target class)<\/h1>","830e8170":"<h1 class=\"list-group-item list-group-item-action active\" id=\"background-information\">I&nbsp;&nbsp;Background and motivation<\/h1>","23951398":"![](https:\/\/media.giphy.com\/media\/gd6bBl4IExkXPK07Kx\/giphy.gif)","4980098b":"**What cause a stroke**        |  **How to detect a stroke**\n:-------------------------:|:-------------------------:\n![](https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/11\/15\/17\/44\/ds00150_ds01030_my00077_im00074_r7_ischemicstrokethu_jpg.jpg)  |  ![](https:\/\/www.cedars-sinai.org\/content\/dam\/cedars-sinai\/blog\/2019\/05\/signs-of-a-stroke.jpg)"}}