{"cell_type":{"3221bf0b":"code","ec13e76e":"code","a06ff708":"code","a5239a43":"code","77263be6":"code","d25b4e93":"code","36e48cec":"code","d6fbcf2d":"code","0b864ac1":"code","39848acc":"code","278ab10f":"code","96c8836a":"code","f73d323f":"code","ff68b782":"code","81e8527a":"code","34734c22":"code","7d095a6c":"code","eec75636":"code","e28e048e":"code","84ea31d6":"code","b5f4a478":"code","ccaa1846":"code","0c0d5280":"code","b28565fc":"code","6d4cce0c":"code","984040ad":"code","dc005356":"code","06c86d87":"code","45b1e7e9":"code","d5905a51":"code","f62a3cc7":"code","b1c3f6bd":"code","d4df2f79":"code","036597a8":"code","f400f57d":"code","9156069b":"code","7db0ff06":"code","c45b1797":"code","68f2b29b":"code","c4610ab9":"code","67730df4":"code","091238c2":"code","1600f9f5":"code","4ba59d67":"code","231c905d":"code","ef2a3cc1":"code","3afe0a77":"markdown","0d7f3d3a":"markdown","aacc9084":"markdown","200430b9":"markdown","78ffb711":"markdown","2f175993":"markdown","22a14593":"markdown","5b72d142":"markdown","6c3b9020":"markdown","354f30be":"markdown","b215c3cf":"markdown","7609db29":"markdown","de7bd0ff":"markdown"},"source":{"3221bf0b":"%%capture \n# command above avoids printing long console output\n\n# installing the pycaret module and its sub-modules (only run in GPU mode)\n!pip install pycaret[full]","ec13e76e":"## The \"magic four\"\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n# Scaler\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# garbage collection (clear up some RAM and optimise available memory)\nimport gc\n\n# imputer (fill in null values)\nfrom sklearn.impute import SimpleImputer\n\n# pycaret (only run in GPU mode)\nfrom pycaret.classification import *\n\n# cuDF\nimport cudf\n\n%matplotlib inline","a06ff708":"# this removes the many warnings that some functions and commands produce\n# it helps significantly declutter the workbook\nimport warnings\nwarnings.filterwarnings('ignore')","a5239a43":"# importing data and setting index column\n# using cuDF to dramatically speed up data import\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest = cudf.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')","77263be6":"train.head()","d25b4e93":"test.head()","36e48cec":"train.shape","d6fbcf2d":"train.dtypes","0b864ac1":"train.info()","39848acc":"train.describe()","278ab10f":"test.shape","96c8836a":"train.isnull().sum()","f73d323f":"test.isnull().sum()","ff68b782":"# defining the train and test datasets\n# transforming cuDF to pandas DataFrames for compatibility\nX_train = train.to_pandas()\ny_train = X_train.pop('claim')\nX_test = test.to_pandas()\n\n# saving the index of the test dataset for later use\nidx = X_test.index","81e8527a":"# saving a copy of column headings\ntrain_cols = X_train.columns\ntest_cols = X_test.columns","34734c22":"# fills null value in each column with column mean\n# use mean to fill nulls since the data is well distributed\n# use mean since the data does not contain substiantial outliers \n# use mean since data does not have a more frequent or particular value\n\nSI = SimpleImputer(strategy = 'mean')\nX_train_fill = SI.fit_transform(X_train)\nX_train_fill = pd.DataFrame(X_train_fill, columns = train_cols)","7d095a6c":"# fill null values in test and set index from orginal dataset\nX_test_fill = SI.fit_transform(X_test)\nX_test_fill = pd.DataFrame(X_test_fill, columns = test_cols)\nX_test_fill.set_index(idx, inplace = True)","eec75636":"gc.collect()","e28e048e":"# adding additional features to both train and test\nX_train_fill['n_missing'] = X_train.isnull().sum(axis=1).astype(int)\nX_train_fill['std'] = X_train_fill[train_cols].std(axis=1)\nX_train_fill['avg'] = X_train_fill[train_cols].mean(axis=1)\nX_train_fill['max'] = X_train_fill[train_cols].max(axis=1)\nX_train_fill['min'] = X_train_fill[train_cols].min(axis=1)\n\nX_test_fill['n_missing'] = X_test.isnull().sum(axis=1).astype(int) \nX_test_fill['std'] = X_test_fill[test_cols].std(axis=1)\nX_test_fill['avg'] = X_test_fill[test_cols].mean(axis=1)\nX_test_fill['max'] = X_test_fill[test_cols].max(axis=1)\nX_test_fill['min'] = X_test_fill[test_cols].min(axis=1)","84ea31d6":"# updated list of column headings\ntrain_cols = X_train_fill.columns\ntest_cols = X_test_fill.columns","b5f4a478":"# scaling train\nscaler = RobustScaler()\n\nscaled_X_train = scaler.fit_transform(X_train_fill)\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = train_cols)","ccaa1846":"# scaling test\nscaled_X_test = scaler.transform(X_test_fill)\nscaled_X_test = pd.DataFrame(scaled_X_test, columns = test_cols)\nscaled_X_test.set_index(idx, inplace = True)","0c0d5280":"# to reduce datset size in RAM\nscaled_X_train = scaled_X_train.astype(np.float32)\nscaled_X_test = scaled_X_test.astype(np.float32)","b28565fc":"# adding a column to say if the row contains nulls\n\nscaled_X_train['any_missing'] = X_train_fill['n_missing'] > 0\nscaled_X_test['any_missing'] = X_test_fill['n_missing'] > 0\n\nscaled_X_train['any_missing'] = scaled_X_train['any_missing'].astype(np.int8)\nscaled_X_test['any_missing'] = scaled_X_test['any_missing'].astype(np.int8)\n\ngc.collect()","6d4cce0c":"# eliminate unnecessary objects to reduce RAM usage\ntry:\n    del test, train, scaler, SI, X_train, X_test, X_train_fill, X_test_fill\nexcept:\n    print('already dropped!')\nfinally:\n    gc.collect()","984040ad":"# pycaret wants a single dataframe which includes the target column\n# try-except structure to avoid unhandled errors and reduce RAM usage\n# avoid error that would occur if the dataset we want to delete already does not exist\ntry:\n    clf_data = scaled_X_train.copy()\n    clf_data = clf_data.join(y_train)\n    del scaled_X_train\nexcept:\n    pass\nfinally:\n    gc.collect()","dc005356":"# setting up the pipeline\nclf = setup(data = clf_data, #DataFrame\n            target = 'claim', #specify the target column\n            data_split_stratify = True, #stratify by target value\n            fold = 5, #use 5-fold cross-validation\n            use_gpu = True, #use GPU acceleration\n            n_jobs = -1, #use maximum number of threads\n            silent = True #execute without need for confirmation\n           )","06c86d87":"# this lists all models that can be run\nmodels()","45b1e7e9":"# comparing all models\n# excluded some models that did not benefit from GPU acceleration and took too long to run (ada, gbc)\n# excluded SVM since it does not support AUC score\n# excluded some models that were run independently (dt, et)\ncompare_models(exclude = ['dt','ada','gbc','et','svm'], sort = 'AUC')","d5905a51":"# trains all available models and return top 3 by AUC score\ntop3 = compare_models(n_select = 3, exclude = ['dt','ada','gbc','et','svm'], sort = 'AUC')","f62a3cc7":"# lists top 3 models and their details\ntop3","b1c3f6bd":"# hyperparameter tuning\ntuned_top3 = [tune_model(i, choose_better = True, optimize = 'AUC') for i in top3]","d4df2f79":"# voting classifier on baseline models\nbasic_blend = blend_models(top3)","036597a8":"# voting classifier on tuned models\ntuned_blend = blend_models(tuned_top3)","f400f57d":"# predict baseline model on validation set\npredict_model(basic_blend);","9156069b":"# predict tuned model on validation set\npredict_model(tuned_blend);","7db0ff06":"# finalise model\n# fits the model onto the complete dataset including the test\/hold-out sample\nfinal_model = finalize_model(basic_blend)","c45b1797":"final_tuned_model = finalize_model(basic_blend)","68f2b29b":"# predict using entire dataset (train and validation)\npredict_model(final_model);","c4610ab9":"# tuned model does not perform better so simpler model will be used\npredict_model(final_tuned_model);","67730df4":"# performance on unseen data\nunseen_predictions_best = predict_model(final_model, data = scaled_X_test, raw_score = True, round = 6)\nunseen_predictions_best.head()","091238c2":"# model plotting\n# Area under the ROC curve\nplot_model(final_model, plot = 'auc')","1600f9f5":"# precision-recall plot\nplot_model(final_model, plot = 'pr')","4ba59d67":"# confusion matrix\nplot_model(final_model, plot = 'confusion_matrix')","231c905d":"# save model and parameters\nsave_model(final_model, 'Final model')","ef2a3cc1":"# check size of prediction vector\nassert(len(idx)==len(unseen_predictions_best))\n\n# create DataFrame with index of test dataset and predictions from the model\nsub = pd.DataFrame(list(zip(idx, unseen_predictions_best.Score_1)),columns = ['id', 'claim'])\n\n# DataFrame to csv to be submitted\nsub.to_csv('submission.csv', index = False)\n\n# print DataFrame contents for final inspection\nprint(sub)","3afe0a77":"# PyCaret Classification","0d7f3d3a":"# Importing data","aacc9084":"## Voting (Blending) Classifier","200430b9":"# Kaggle Tabular Playground Series\u200a-\u200aSep 2021","78ffb711":"## Importing packages and functions","2f175993":"# Submission","22a14593":"## Setup","5b72d142":"# Data Cleaning","6c3b9020":"# Scaler","354f30be":"The aim of this project was to take part in and complete the Kaggle \"Tabular Playground Series\u200a-\u200aSep 2021\" Prediction Competition. The competition consisted of predicting the probability of a customer making a claim on an insurance policy. The data used in this challenge came from a synthetic dataset. I chose this as my capstone project since it allowed me to put my knowledge and skills into practice but also explore new topics and learn new modelling techniques.\u00a0\n\nThe best solution I was able to achieve consisted of a voting classifier model containing the three highest-scoring available models I was able to train on my dataset. This solution was able to achieve good predictive performance both in-sample and out-of-sample, with AUC scores in excess of 83% when using the entire dataset to predict labels for the validation dataset. Other metrics scored highly as well, indicating the good performance of the chosen model.\u00a0\n\nThe project was concluded with the submission of the test predictions to the relevant competition submission page to finally  be scored. My best predictions allowed me to score in the top 37% of participants.","b215c3cf":"## Model comparison","7609db29":"# Exploratory Data Analysis","de7bd0ff":"## Filling in null values by column mean"}}