{"cell_type":{"b094b566":"code","a5f10c76":"code","ecfe8a1d":"code","42b76515":"code","ed32f819":"code","67ad0735":"code","aac1f359":"code","bf8ed7b7":"code","4b91bf71":"code","b8158770":"code","4237160a":"code","5e053f7e":"code","effb0ad3":"code","19b931c5":"code","17980ab4":"code","582e46a5":"code","1f956878":"code","e8dd04b6":"code","a02d79ff":"code","ea790066":"code","15f3d607":"code","e431fd7a":"code","0af34bfc":"markdown","8aeabb53":"markdown","fef09467":"markdown","3ef80934":"markdown","bd1db9ab":"markdown","96c4502f":"markdown","11916082":"markdown","8cb49672":"markdown","d44f8dff":"markdown","1aa0a50b":"markdown","de8e8c93":"markdown","e3d44885":"markdown","9686b5a9":"markdown","2774dc67":"markdown","b2452eb1":"markdown","aef8bc54":"markdown","07caba23":"markdown","470318ed":"markdown","1e7da3e9":"markdown","3ac317dc":"markdown","f300db45":"markdown","bd537db6":"markdown","d7578aac":"markdown","f4cfe818":"markdown","04f80622":"markdown","8804739c":"markdown","e7101df9":"markdown"},"source":{"b094b566":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5f10c76":"df = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndf.drop(['Id'], axis = 1)","ecfe8a1d":"df.dtypes","42b76515":"df.shape","ed32f819":"df.describe(include='all')","67ad0735":"df.duplicated().sum()\n","aac1f359":"df.Species.value_counts()","bf8ed7b7":"fig = px.scatter(df, x=\"SepalLengthCm\", y=\"SepalWidthCm\", facet_col=\"Species\")\n# sources of images\nsources = [\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fe\/Iris_setosa_var._setosa_%282595031014%29.jpg\/360px-Iris_setosa_var._setosa_%282595031014%29.jpg\",\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/38\/Iris_versicolor_quebec_1.jpg\/320px-Iris_versicolor_quebec_1.jpg\",\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/f8\/Iris_virginica_2.jpg\/480px-Iris_virginica_2.jpg\",\n]\n# add images\nfor col, src in enumerate(sources):\n    fig.add_layout_image(\n        row=1,\n        col=col + 1,\n        source=src,\n        xref=\"x domain\",\n        yref=\"y domain\",\n        x=1,\n        y=1,\n        xanchor=\"right\",\n        yanchor=\"top\",\n        sizex=0.3,\n        sizey=0.3,\n\n    )\n\nfig.show()","4b91bf71":"fig = px.scatter(df, x=\"PetalLengthCm\", y=\"PetalWidthCm\", facet_col=\"Species\")\n# sources of images\nsources = [\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fe\/Iris_setosa_var._setosa_%282595031014%29.jpg\/360px-Iris_setosa_var._setosa_%282595031014%29.jpg\",\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/38\/Iris_versicolor_quebec_1.jpg\/320px-Iris_versicolor_quebec_1.jpg\",\n    \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/f8\/Iris_virginica_2.jpg\/480px-Iris_virginica_2.jpg\",\n]\n# add images\nfor col, src in enumerate(sources):\n    fig.add_layout_image(\n        row=1,\n        col=col + 1,\n        source=src,\n        xref=\"x domain\",\n        yref=\"y domain\",\n        x=1,\n        y=1,\n        xanchor=\"right\",\n        yanchor=\"top\",\n        sizex=0.3,\n        sizey=0.3,\n\n    )\n\nfig.show()","b8158770":"fig = px.box(df, x=\"Species\", y=\"SepalLengthCm\",title=\"Box plot of Sepal Lengths\", points=\"all\", color=\"Species\")\nfig.show()","4237160a":"fig = px.box(df, x=\"Species\", y=\"SepalWidthCm\", points=\"all\", color=\"Species\",title=\"Box plot of Sepal Widths\")\nfig.show()","5e053f7e":"fig = px.box(df, x=\"Species\", y=\"PetalLengthCm\", points=\"all\", color=\"Species\", title=\"Box plot of Petal Lengths\")\nfig.show()","effb0ad3":"fig = px.box(df, x=\"Species\", y=\"PetalWidthCm\", points=\"all\", color=\"Species\", title=\"Box plot of Petal Width\")\nfig.show()","19b931c5":"df.corr().iplot(kind='heatmap',colorscale=\"Reds\",title=\"Feature Correlation Matrix\")","17980ab4":"x = df.iloc[:, [0, 1, 2, 3]].values\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)","582e46a5":"fig = px.line(df, x=range(1, 11), y=wcss)\nfig.show()","1f956878":"kmeans = KMeans(n_clusters = 3, init = 'k-means++',\n                max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","e8dd04b6":"plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], \n            s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], \n            s = 100, c = 'orange', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],\n            s = 100, c = 'blue', label = 'Iris-virginica')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","a02d79ff":"kmeans = KMeans(n_clusters = 7, init = 'k-means++',\n                max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], \n            s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], \n            s = 100, c = 'orange', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],\n            s = 100, c = 'blue', label = 'Iris-virginica')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","ea790066":"model = KMeans(3)\nvisualizer = SilhouetteVisualizer(model)\n\nvisualizer.fit(x)    # Fit the data to the visualizer\nvisualizer.show() ","15f3d607":"model = KMeans(4)\nvisualizer = SilhouetteVisualizer(model)\n\nvisualizer.fit(x)    # Fit the data to the visualizer\nvisualizer.show() ","e431fd7a":"import plotly.express as px\n\nfig = px.scatter_3d(df, x='SepalLengthCm', y='SepalWidthCm', z='PetalWidthCm',\n              color='PetalLengthCm', size='PetalLengthCm', size_max=18,\n              symbol='Species', opacity=0.7)\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","0af34bfc":"This is the **scatter plot of Sepal width vs Sepal Length for each type of species**. We notice that Iris Setosa Sepals with broader width and shorter length. Iris-versicolor has slighlty longer length but thinner width and Iris-Virginica has the longest length and medium width. Let us check how the petals compare to each other.","8aeabb53":"<h2> Silhouette Score <\/h>","fef09467":"Silhouette score is used to evaluate the **quality of clusters** created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. \nThe Silhouette score is calculated for each sample of different clusters. \n\nTo calculate the Silhouette score for each observation\/data point, the following distances need to be found out for each observations belonging to all the clusters:\n\n**Mean distance between the observation and all other data points in the same cluster.** This distance can also be called a mean intra-cluster distance. The mean distance is denoted by a\n\n**Mean distance between the observation and all other data points of the next nearest cluster.** This distance can also be called a mean nearest-cluster distance. The mean distance is denoted by b\n\nSilhouette score, S, for each sample is calculated using the following formula:\nS = (b - a)}\/{max(a, b)\n\n","3ef80934":"Playing around with the values of k we note that it could be detrimental to the quality of outcome if not chosen properly. \nIn some cases points may overlap or clusters may have no data.\n","bd1db9ab":"<h2> Correlation <\/h2>","96c4502f":"Bibliography: \n\nhttps:\/\/dzone.com\/articles\/kmeans-silhouette-score-explained-with-python-exam \n\nhttps:\/\/www.kaggle.com\/mhrizvi\/validating-clustering-model-thru-silhouette-score\n\nhttps:\/\/www.kaggle.com\/kautumn06\/yellowbrick-clustering-evaluation-examples\n\nhttps:\/\/www.toptal.com\/machine-learning\/clustering-algorithms\n","11916082":"![1_Hh53mOF4Xy4eORjLilKOwA.png](attachment:41934c77-7c16-4139-bba2-80701e71d3e6.png)","8cb49672":"**Main intuition behind \"Clusters\"**\n\nThe K-Means clustering algorithm tries to reduce the **intra cluster distances** and increase the **inter cluster distance** thus forming definitive clusters.\n\nIn order to do that we follow the K-Means clustering Algo:\n- The number of centroids are defined and are assigned to random locations on the graph.\n- In the first iteration, the data points closest to the centroids are clustered into one. (Calculated using the [Euclidean Distance](https:\/\/www.tutorialexample.com\/calculate-euclidean-distance-in-tensorflow-a-step-guide-tensorflow-tutorial\/) )\n\n\n Now this is a hit or miss step as the centroids are places randomly\n- Now the value of the centroid become the **mean** of the data points in that cluster. \n- Again the closest points are divided into each cluster.\n\nBelow is a visualisation of how the centroid moves in each iteration:\n\n![toptal-blog-image-1463672901961-c86610183bb2ba67f979c421f6748893.gif](attachment:2e3b22f9-4785-42ae-b5d9-0411a560d573.gif)","d44f8dff":"3-D visualisation.","1aa0a50b":"- Checking for duplicate values\n\nThere are none.","de8e8c93":"**Box plot of Sepals and Petals**","e3d44885":"From the **Scatter plot of Petal width vs Petal Length** it is clear the Iris-Virginica has the longest and broadest petals while Iris Setosa the shortest and thinnest!","9686b5a9":"<h2> Visualisation <\/h2>","2774dc67":"- Checking the important stattistical inferences.","b2452eb1":"<h2> Exploratory Data Analysis <h2>","aef8bc54":"**But...how do I know which elbow needs to be taken in consideration? There is a slight ambiguity between 2,3 and 4.**\n\nTo get a more trustworthy answer, we use another test called \n**Silhouette Score**","07caba23":"- The silhouette score falls within the range [-1, 1].\n\n- The silhouette score of 1 means that the clusters are very dense and nicely separated.\n\n- The score of 0 means that clusters are overlapping. \n\n- The score of less than 0 means that data belonging to clusters may be wrong\/incorrect.\n\n- The thickness of the silhouette plot representing each cluster also is a deciding point. The more uniform the thickness is, the better is the K value.\n\nThe silhouette plots can be used to select the most optimal value of the K (no. of cluster) in K-means clustering.\n\nIn the above examples we note at K=3; the silhouettes are close to 1, of equal thickness and are greater than 0. ","470318ed":"-The Iris flower data set or **Fisher's Iris data** set is a multivariatedata set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper *The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.*\n\n-The data set consists of 50 samples from each of three species of *Iris* (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.","1e7da3e9":"<h2> Elbow Method <\/h2>\n    \nThe elbow method involves running the algorithm multiple times over a loop, with an increasing number of cluster choice and then plotting the clustering score as a function of the number of clusters. \nThe no. of clusters is then decided based on where thee is an \"elbow\" or angle change in the graph. ","3ac317dc":"**What is K-Means clustering?**\n\nK-Means clustering is an iterative clustering algorithm of the **Partition Based-Unsupervised Learning**. The other types of learning under Unsupervised Learning include: Agglomerative and Divisive Hierachical and Density Based- DBSCAN.\n\n\n**Why Clustering?**\n\nK-Means clustering is very useful in  pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning..\n- Retail: Patterns in Buying, Item Recommendations.\n- Banking: Fraud Detection, Detecting Loyal Customers.\n- Insuarance: Fraud Detection.\n- Publication: Auto Categorizing news, recommending similiar news titles\n- Medicine: Disease etiology and factors associated with diseases, Patient characterizing.\n- Biology: Clustering Biological markers.","f300db45":"Thanks for reading! Happy learning :)","bd537db6":"**But wait..how do we select the number of clusters in the first place?**\n\nThis is one of the most important steps in K-Means clustering and decides the accuracy of a clustering model. \nThere are 2 ways of finding the optimal K:\n\n -Elbow Methods\n \n -Average Silhouette\n","d7578aac":"- Checking the data types of each Column.","f4cfe818":"- Checking the shape of the Dataframe","04f80622":"- Listing the 3 types of Species we will be trying to cluster. \n","8804739c":"<h1>K-Means Clustering.<\/h1>\n\n","e7101df9":"<h1> Data Preprocessing and EDA <\/h1>\n\n<h2> Imports <\/h2>"}}