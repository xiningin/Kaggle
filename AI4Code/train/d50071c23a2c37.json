{"cell_type":{"3be60aaf":"code","8d511264":"code","d3fc927a":"code","8759f54c":"code","796ba7df":"code","758151fe":"code","2fa69230":"code","065b2ac3":"code","d78946d9":"code","ecdf0740":"code","969be8d9":"code","e4e10f24":"code","b6d95ad4":"code","134c8132":"code","90e638b2":"code","c2f16cf8":"code","8ed58827":"code","a78011ba":"code","77ea7e1d":"code","22fbd381":"code","5367bf8b":"code","61148534":"code","81e4adc9":"code","fd563256":"code","bcbf55f2":"code","0cf19405":"code","76ce9708":"code","040bd5fd":"code","79d30d7e":"code","ff73999a":"code","a2b39e41":"code","2775a413":"code","ca113612":"code","34d96139":"code","d396ed57":"code","557d3ffa":"code","e95d8260":"code","2d13e397":"code","c2a0f933":"code","7c25e721":"code","adc18c69":"code","1d75743b":"code","8fd01f34":"code","f9624cd6":"markdown","45f38477":"markdown","11e5deb1":"markdown","c9bdc5b4":"markdown","d40b839e":"markdown","5e7987b6":"markdown","40fe7a04":"markdown","50f264b1":"markdown","6d52ff5c":"markdown","34a2a375":"markdown","80f1b0a0":"markdown","5aee867f":"markdown","ffe50ad1":"markdown","4cdd80e9":"markdown","7101ed2c":"markdown","052aa3c8":"markdown","5e746d72":"markdown","fcbe402b":"markdown","0bb6f667":"markdown","ad6e018b":"markdown","203b38f2":"markdown","b8562464":"markdown","6ced0482":"markdown","45e4e285":"markdown","eda18d59":"markdown","437473c5":"markdown","c247ba72":"markdown","44fa86b1":"markdown","a09e52f2":"markdown","cdbc3f90":"markdown","b614a530":"markdown","0b9aab2e":"markdown","7b190d9c":"markdown","a5a1b66f":"markdown","d1c825c4":"markdown","4f8e4560":"markdown","557b0345":"markdown","6b7ef9a6":"markdown","1fa73d6c":"markdown","d11b5683":"markdown","332e37ef":"markdown","7bd7cb08":"markdown","272c9b1d":"markdown","00d311cd":"markdown","e8fc251b":"markdown","da0a7798":"markdown","06dafe18":"markdown","49cd3236":"markdown","474ac470":"markdown","df169b44":"markdown"},"source":{"3be60aaf":"import pandas as pd\ndf_train=pd.read_csv('..\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv')\ndf_test=pd.read_csv('..\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv')","8d511264":"print(df_train.head())\nprint(df_train.tail())","d3fc927a":"print(df_train.columns)","8759f54c":"print(df_train['pet_category'].unique())","796ba7df":"y_train=df_train['pet_category'].values\nprint(y_train)","758151fe":"print(df_train['condition'].unique())\nprint(df_train['color_type'].unique())\nprint(df_train['breed_category'].unique())","2fa69230":"print(df_train['length(m)'].unique())\nprint(df_train['height(cm)'].unique())\nprint(df_train['X1'].unique())\nprint(df_train['X2'].unique())","065b2ac3":"print(df_train['length(m)'].isna().sum())\nprint(df_train['height(cm)'].isna().sum())\nprint(df_train['X1'].isna().sum())\nprint(df_train['X2'].isna().sum())\n\nprint(df_train['condition'].isna().sum())\nprint(df_train['color_type'].isna().sum())\nprint(df_train['breed_category'].isna().sum())","d78946d9":"print(df_test['length(m)'].isna().sum())\nprint(df_test['height(cm)'].isna().sum())\nprint(df_test['X1'].isna().sum())\nprint(df_test['X2'].isna().sum())\n\nprint(df_test['condition'].isna().sum())\nprint(df_test['color_type'].isna().sum())\n#print(df_test['breed_category'].isna().sum())","ecdf0740":"print(df_train.groupby(['condition']).size())\n\nprint(df_train[df_train['condition'].isnull()])","969be8d9":"print(df_train[df_train['condition'].isnull()]['breed_category'].unique())","e4e10f24":"df_train[df_train['breed_category']==2].count()","b6d95ad4":"import numpy as np\ndf_train['condition']=df_train['condition'].replace(np.nan,3)","134c8132":"df_test['condition']=df_test['condition'].replace(np.nan,3)","90e638b2":"print(df_train.groupby(['condition']).size())\n\nprint(df_train[df_train['condition'].isnull()])","c2f16cf8":"df_train['diff_days']=np.abs((pd.to_datetime(df_train['listing_date'].values)-pd.to_datetime(df_train['issue_date'].values)).days)\n\nprint(df_train['diff_days'].values)","8ed58827":"df_test['diff_days']=np.abs((pd.to_datetime(df_test['listing_date'].values)-pd.to_datetime(df_test['issue_date'].values)).days)\n\nprint(df_test['diff_days'].values)","a78011ba":"print(df_train['issue_date'][5], \" \", df_train['listing_date'][5], \" \",df_train['diff_days'][5])","77ea7e1d":"df_train_new=df_train.drop(columns=['issue_date','listing_date'])\n\nprint(df_train_new.head())\nprint(df_train_new.columns)","22fbd381":"df_test_new=df_test.drop(columns=['issue_date','listing_date'])\n\nprint(df_test_new.head())\nprint(df_test_new.columns)","5367bf8b":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\ndf_train_new[\"color_type_code\"] = lb_make.fit_transform(df_train_new[\"color_type\"])\ndf_train_new[[\"color_type\", \"color_type_code\"]].head(11)","61148534":"df_test_new[\"color_type_code\"] = lb_make.transform(df_test_new[\"color_type\"])\ndf_test_new[[\"color_type\", \"color_type_code\"]].head(11)","81e4adc9":"df_train_new['color_type_code'].unique()\n\ndf_train_new=df_train_new.drop(columns=['color_type'])\n\nprint(df_train_new.head(25))","fd563256":"df_test_new['color_type_code'].unique()\n\ndf_test_new=df_test_new.drop(columns=['color_type'])\n\nprint(df_test_new.head(25))","bcbf55f2":"print(df_train_new.columns)","0cf19405":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['length(m)'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Length\")\nax.set(title=\"Length distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['length(m)']))","76ce9708":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['height(cm)'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Height\")\nax.set(title=\"Height distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['height(cm)']))","040bd5fd":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['X1'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['X1']))","79d30d7e":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['X2'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X2\")\nax.set(title=\"X2 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['X2']))","ff73999a":"# to check skewness of X1 Score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nX1_trans=np.log(1+df_train_new['X1'].values)\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(X1_trans, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(X1_trans))","a2b39e41":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nX1_trans_test=np.log(1+df_test_new['X1'].values)\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(X1_trans, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(X1_trans_test))","2775a413":"df_train_norm=df_train_new\ndf_train_norm['X1']=X1_trans\n\ndf_test_norm=df_test_new\ndf_test_norm['X1']=X1_trans_test","ca113612":"df_train_norm['Low_Height']=np.where(df_train_norm['height(cm)']<=15,1,0)\ndf_train_norm['Medium_Height']=np.where(((df_train_norm['height(cm)']>15) & (df_train_norm['height(cm)']<=30)),1,0)\ndf_train_norm['High_Height']=np.where(df_train_norm['height(cm)']>30,1,0)\n\ndf_test_norm['Low_Height']=np.where(df_test_norm['height(cm)']<=15,1,0)\ndf_test_norm['Medium_Height']=np.where(((df_test_norm['height(cm)']>15) & (df_test_norm['height(cm)']<=30)),1,0)\ndf_test_norm['High_Height']=np.where(df_test_norm['height(cm)']>30,1,0)","34d96139":"df_train_norm['Low_Length']=np.where(df_train_norm['length(m)']<=0.3,1,0)\ndf_train_norm['Medium_Length']=np.where((df_train_norm['length(m)']>0.3) & (df_train_norm['length(m)']<=0.6),1,0)\ndf_train_norm['High_Length']=np.where(df_train_norm['length(m)']>0.6,1,0)\n\ndf_test_norm['Low_Length']=np.where(df_test_norm['length(m)']<=0.3,1,0)\ndf_test_norm['Medium_Length']=np.where((df_test_norm['length(m)']>0.3) & (df_test_norm['length(m)']<=0.6),1,0)\ndf_test_norm['High_Length']=np.where(df_test_norm['length(m)']>0.6,1,0)","d396ed57":"print(df_train_norm.head(20))","557d3ffa":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel=XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel2=XGBClassifier()\nmodel2.fit(X_train,y_train)\ny_pred=model2.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))","e95d8260":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\n\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\nmodel2.fit(X_train,y_train)\ny_pred=model2.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))","2d13e397":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\n\nmodelx.fit(X,Y)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\nmodelx2.fit(X,Y)\n","c2a0f933":"df_test_norm['length(cm)']=df_test_norm['length(m)']*100\ndf_train_norm['length(cm)']=df_train_norm['length(m)']*100\n\n\ndf_test_today=df_test_norm.drop(columns=['length(m)'])\ndf_train_today=df_train_norm.drop(columns=['length(m)'])","7c25e721":"df_train_today['ratio']=df_train_today['X2']\/(1+df_train_today['X1'])\ndf_test_today['ratio']=df_test_today['X2']\/(1+df_test_today['X1'])","adc18c69":"df_train_today['lhratio']=df_train_today['height(cm)']\/(1+df_train_today['length(cm)'])\ndf_test_today['lhratio']=df_test_today['height(cm)']\/(1+df_test_today['length(cm)'])","1d75743b":"from sklearn.model_selection import train_test_split\n\nY=df_train_today['pet_category'].values\nX=df_train_today.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.47,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1500, \n                      reg_alpha = 0.3,\n                      max_depth=7, \n                      gamma=4,\n                     random_state=42)\n\nmodelx.fit(X,Y)\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_today['breed_category'].values\nX=df_train_today.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4,\n                      random_state=42)\nmodelx2.fit(X,Y)\n","8fd01f34":"from sklearn.model_selection import train_test_split\n\n#Y_test_fin=df_test_new['pet_category'].values\nidx=df_test_today['pet_id'].values\nX_test_fin=df_test_today.drop(columns=['pet_id'])\n\n\ny_pred_fin=modelx.predict(X_test_fin)\n\n\nfrom sklearn.model_selection import train_test_split\n\n#Y_test_fin=df_test_new['pet_category'].values\nidx=df_test_today['pet_id'].values\nX_test_fin=df_test_today.drop(columns=['pet_id'])\n\n\ny_pred_fin2=modelx2.predict(X_test_fin)\n\n\ndf_sub = pd.DataFrame({'pet_id': idx,\n                   'breed_category': y_pred_fin2,\n                   'pet_category': y_pred_fin})\ndf_sub.to_csv('submit.csv',index=False)","f9624cd6":"Observations of performance with respect to learning rate:\n\n* learning rate=0.01 (89.75)\n* learning rate=0.1 (90.04)\n* learning rate=0.4 (90.17)\n* learning rate=0.6 (90.14)","45f38477":"# Calculating the no. of NAN values","11e5deb1":"# Clearing NAN value in 'condition' column","c9bdc5b4":"Apart from XgBoost classifier, I had tried with RandomForest, LGBM and CatBoost Classifier too, but it was found that XgBoost performed the best","d40b839e":"* Length: low (0 to 0.3) , medium (0.3 to 0.6) , high( 0.6 to 1.0)\n* Height: low (0 to 15) , medium (15 to 30) , high(30 to 45)","5e7987b6":"Dropping columns from testing dataset","40fe7a04":"Dropping columns from training dataset","50f264b1":"Training dataset stats","6d52ff5c":"For Testing Dataset","34a2a375":"# Arriving at the final model\n\nAfter tuning parameters like 'learning_rate' and 'max_depth' , I arrived at the final model which gave the final Public score of **91.06 (an improvement of 0.23)** ! Eureka!\n","80f1b0a0":"# XGBoost training and validation","5aee867f":"# Training the final XGBoost tuned model","ffe50ad1":"## Importing Dataset","4cdd80e9":"# Hackerearth ML Challenge : Adopt a Buddy \n\nProblem Description: [Hackerearth Link](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-pet-adoption\/machine-learning\/pet-adoption-9-5838c75b\/) \n\nLeaderboard [Link](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-pet-adoption\/leaderboard\/pet-adoption-9-5838c75b\/)\n\nRank: 115th\n\nFinal Score: 90.4\n\nWe have been given training and testing dataset which has columns like the Pet_Id , Condition , Color , Issue and Listing Date. The target variables are the breed_category and pet_category which we need to predict \n\nAs there are two classes , the approach taken is training two models for each classes and then testing seperately and appending the final result. ","7101ed2c":"So, we have added new feature 'diff_days' which describes difference in days between listing date and issue date. \\\n","052aa3c8":"Dropping the column from both the training and testing dataset","5e746d72":"**Observation:** Only For breed_category 2.0 , condition value is null","fcbe402b":"# Encoding columns of categorical names with numbers\n\nEncoding the color_type column in training dataset","0bb6f667":"So, we can give a unique value for condition where it is null","ad6e018b":"# Finding difference between issue_date and listing_date in days","203b38f2":"Now the 'max_depth' parameter is decreased from 4 and tried with 3, 2 and 1. \n\nIt is found that 4 is optimal","b8562464":"The subsample is increased from 0.8 but we found that it is optimal\n","6ced0482":"Testing dataset stats","45e4e285":"# Finding ratio of X2 : X1 and adding it as a feature\n\n* X1 , X2 columns were removed but the performance decreased, so we include those","eda18d59":"# Further Feature Engineering\n\nI did not stop at 90.83, but tried to add more features to train the model more efficiently and check whether any improvement is made.","437473c5":"Checking the correctness of difference (in days)","c247ba72":"# Creating features on basis of magnitude of Length and Height","44fa86b1":"Checking skewness of X2","a09e52f2":"For Training Dataset","cdbc3f90":"Encoding the color_type column in testing dataset","b614a530":"Checking skewness of X1","0b9aab2e":"# Removing date-time columns","7b190d9c":"Calculating Difference and adding feature for training data","a5a1b66f":"# Points to learn\n\n* Feature Engineering is very important, always try to incorporate new features to see performance improvement.\n\n* How to smartly convert DateTime values into numerical features in order to train your model more efficiently\n\n* Parameter Tuning is equally critical\n\n* Never lose Hope!","d1c825c4":"# Distribution of values in some features","4f8e4560":"Calculating Difference and adding feature for testing data","557b0345":"Performance with respect to 'gamma' parameter original \n* gamme= 5 (90.57)\n* gamma= 4 (90.7)\n\ngamma=4 is optimal","6b7ef9a6":"90.81 performance score without train_test_split, i.e training on whole training Dataset","1fa73d6c":"# Observations","d11b5683":"# Displaying the Unique values","332e37ef":"# Finding ratio of Height(cm) : Length(cm) and adding it as a feature","7bd7cb08":"Checking skewness of Length(m)","272c9b1d":"After this it is checked the diff_days column had negative values for **two cases** in the training dataset. So used np.abs() for calculation of difference. \n\nAfter retraining the model and testing it, final f1 score acheived on submission is 90.83","00d311cd":"Checking skewness of Height(cm)","e8fc251b":"**Observation:** The column 'condition' is having many Nan values","da0a7798":"# Converting Length to cm units, same as height","06dafe18":"# XGBoost Parameter tuning","49cd3236":"We noticed that only in column X1, there is high skewness, so we take the log transform of that column both in training and testing dataset","474ac470":"# Viewing the columns","df169b44":"XGBoost without any parameter tuning gave performance f1 score of 89.71 on submission "}}