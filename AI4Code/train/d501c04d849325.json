{"cell_type":{"dbb60267":"code","7a9a0768":"code","256dd136":"code","9915deed":"code","d911ce7b":"code","29690f7b":"code","20871453":"code","0a9d6071":"code","3ecee546":"code","2d408755":"code","5a0bb7b7":"code","45ac2d41":"code","06ab38ff":"code","a108a5ca":"code","15b10501":"code","dc9a34b6":"code","09ca55ff":"code","bca2979c":"code","09269c06":"code","82c51526":"code","ac09402f":"code","aa62fcff":"code","c13d4405":"code","f0793be7":"code","41f889e0":"code","7f8d2efc":"code","9403bac3":"markdown","4e5c6363":"markdown","5d98f0c4":"markdown","84fc9a6d":"markdown","f44dda1c":"markdown","37802627":"markdown","c2ea7e9f":"markdown","98683cfe":"markdown","daa45d6b":"markdown","ba0d71de":"markdown","bc479112":"markdown","add0196c":"markdown","0e427735":"markdown","c39599ff":"markdown","0c4be050":"markdown","d3fa4a3e":"markdown","a333e52b":"markdown"},"source":{"dbb60267":"## Importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n#from sklearn.ensemble import AdaBoostClassifier\n#from sklearn.linear_model import LogisticRegression\n#import lightgbm as lgb","7a9a0768":"# Load CSV\n\ndata = pd.read_csv('\/kaggle\/input\/hackerearth-pet-adoption-dataset\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hackerearth-pet-adoption-hackathon-dataset\/test.csv')\n\norigial=data.copy()\n\nprint('Train Data ---')\ndisplay(data.head(3))\nprint('Test Data ---')\ndisplay(data.tail(3))","256dd136":"## combine train and test data\n\ndata_com = pd.concat((data, test))","9915deed":"## checking for null values\n\ndata_com.isnull().sum()","d911ce7b":"## taking a look at some information about dataset\n\ndata_com.info()","29690f7b":"## modify listing and issue date coloumn from object to datetime format\n\ndata_com.issue_date = pd.to_datetime(data_com.issue_date)\ndata_com.listing_date = pd.to_datetime(data_com.listing_date)\n\n## creating a new coloumn (time duration for delivery)\n\ndata_com['time_diff'] = data_com.listing_date - data_com.issue_date\ndata_com['time_diff'] = data_com.time_diff.dt.total_seconds()\n\n## applying log\n\ndata_com['time_diff'] = np.log1p(data_com['time_diff'])","20871453":"col = ['color_type']\nfor c in col:\n    le = LabelEncoder()\n    data_com[c]=le.fit_transform(data_com[c])\n    \n## coverting height from cm to m\n\ndata_com['height'] = data_com['height(cm)']\/100\ndata_com.drop(['height(cm)'], axis=1, inplace=True)\ndata_com.head(3)","0a9d6071":"## I found pet id to be useless as all the values are unique but some people \n## used it after splitting it ( ANSL_6 and 9903)\n\ndata_ = data_com.drop(['pet_id'], axis=1)","3ecee546":"## Some new features which you can try \n\n#data_['L\/H'] = np.round(data_['length(m)'] \/ data_['height'], 3)\n#data_['X1\/X2'] = data_['X1'] \/ data_['X2']\n#data_ = data_com.drop(['pet_id'], axis=1)\n#data_['1'] = np.round(data_['length(m)'] * data_['height'], 3)\n#data_['2'] = np.round(data_['length(m)'] + data_['height'], 3)\n#data_['3'] = np.round(data_['length(m)'] - data_['height'], 3)\n#data_['4'] = data_['X1'] * data_['X2']\n#data_['5'] = data_['X1'] + data_['X2']\n#data_['6'] = data_['X1'] - data_['X2']","2d408755":"from xgboost import XGBClassifier\ndata_condition_null = data_[data_.condition.isnull()==True]\ndata_condition_not_null = data_[data_.condition.isnull()==False]","5a0bb7b7":"x = data_condition_not_null[['X1', 'X2', 'color_type', 'time_diff', 'length(m)', 'height']]\ny = data_condition_not_null['condition']\nto_predict = data_condition_null[['X1', 'X2', 'color_type', 'time_diff', 'length(m)', 'height']]","45ac2d41":"clf_xg_fill = XGBClassifier()\nclf_xg_fill.fit(x,y)\nabc = clf_xg_fill.predict(to_predict)","06ab38ff":"sns.countplot(abc)","a108a5ca":"## Splitting Data into train and test and fiiling missing values\n\ntrain = data_[data_.pet_category.isnull()==False]\ntrain = train.fillna(3)\n\ntest_ = data_[data_.pet_category.isnull()==True]\ntest_.drop(['breed_category', 'pet_category'], axis=1,inplace=True)\n\nx = train[['X1', 'X2', 'color_type', 'condition', 'time_diff', 'length(m)', 'height']]\ny1 = train['pet_category']\ny2 = train['breed_category']\nx['condition'] = x['condition'].astype('int')","15b10501":"## grid search for XGBClassifier\n\nparams = {'min_child_weight':[1,5,10],\n         'gamma':[0.5,1, 2, 5],\n         'max_depth':[3,5],\n         'subsample':[0.6, 1.0],\n         'learning_rate':[0.001,0.01,0.05,0.1],\n         'n_estimators':[100,300,500,700,900,1000]}\nxgb = XGBClassifier()\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_jobs=-1, cv=3, verbose=5)\nr_s = random_search.fit(x,y2)\nr_s.best_estimator_","dc9a34b6":"## grid search for RandomForestClassifier\n\nparams = {'bootstrap':[True, False], \n         'max_depth':[10,30,50,70,90,100,None],\n         'max_features':['auto', 'sqrt'],\n         'min_samples_leaf':[1,2,4],\n         'min_samples_split':[2,5,10],\n         'n_estimators':[200,600,1000,1400,1800]}\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(rf, param_distributions=params, n_jobs=-1, verbose=3, cv=3)\nr_f = rf_random.fit(x,y2)\n\nr_f.best_estimator_","09ca55ff":"## grid search for CatBoostClassifier\n\ncat_features = ['X2', 'color_type', 'condition']\ngrid = {\n    'learning_rate': [0.05, 0.07, 0.09, 0.3],\n    'depth': [5, 6, 7],\n    'l2_leaf_reg': [1, 3, 5, 7, 9],\n    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n}\ntrain_pool = Pool(x, label=y2, cat_features=cat_features)\nmodel = CatBoostClassifier(\n        early_stopping_rounds=100,\n        has_time=True,\n        iterations=5000\n    )\n\nmodel.randomized_search(grid, X=train_pool)\n","bca2979c":"cat_features = ['condition', 'color_type', 'X1']\nparams = {'depth': 7,\n          'l2_leaf_reg': 3,\n          'learning_rate': 0.07,\n          'grow_policy': 'SymmetricTree',\n         'cat_features': cat_features,\n         'verbose': 200,\n         'eval_metric': 'Accuracy'}\nxtrain, xtest, ytrain, ytest = train_test_split(x, y2, random_state=12)\n\nclf_rf = RandomForestClassifier(min_samples_leaf=2, n_estimators=200)\nclf_cat = CatBoostClassifier(**params)\nclf_xg =XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=5, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=3,\n              min_child_weight=5, missing=np.nan, monotone_constraints='()',\n              n_estimators=300, n_jobs=0, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=0.6,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n#clf_lgb = lgb.LGBMClassifier()\n#clf_ad = AdaBoostClassifier()\n\nclf2 = VotingClassifier(estimators=[('rf', clf_rf), ('xgb', clf_xg),('cat', clf_cat)], voting='soft')\n\nclf2.fit(x,y2)","09269c06":"## Adding bread_category to train_data\n\nx['breed_category'] = clf2.predict(x)\nx['breed_category'] = x['breed_category'].astype('int')","82c51526":"## grid search for RandomforestClassifier\n\nparams = {'bootstrap':[True, False], \n         'max_depth':[10,30,50,70,90,100,None],\n         'max_features':['auto', 'sqrt'],\n         'min_samples_leaf':[1,2,4],\n         'min_samples_split':[2,5,10],\n         'n_estimators':[200,600,1000,1400,1800]}\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(rf, param_distributions=params, n_jobs=-1, verbose=3, cv=3)\nr_f = rf_random.fit(x,y1)\nr_f.best_estimator_","ac09402f":"## grid search for XGBClassifier\n\nparams = {'min_child_weight':[1,5,10],\n         'gamma':[0.5,1, 2, 5],\n         'max_depth':[3,5],\n         'subsample':[0.6, 1.0],\n         'learning_rate':[0.001,0.01,0.05,0.1],\n         'n_estimators':[100,300,500,700,900,1000]}\nxgb = XGBClassifier()\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_jobs=-1, cv=3, verbose=5)\nr_s = random_search.fit(x,y1)\nr_s.best_estimator_","aa62fcff":"## grid search for CatBoostClassifier\n\ncat_features = ['X2', 'color_type', 'condition', 'breed_category']\ngrid = {\n    'learning_rate': [0.05, 0.07, 0.09, 0.3],\n    'depth': [5, 6, 7],\n    'l2_leaf_reg': [1, 3, 5, 7, 9],\n    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n}\ntrain_pool = Pool(x, label=y1, cat_features=cat_features)\nmodel = CatBoostClassifier(\n        early_stopping_rounds=100,\n        has_time=True,\n        iterations=5000\n    )\n\nmodel.randomized_search(grid, X=train_pool)","c13d4405":"cat_features = ['condition', 'color_type', 'breed_category', \"X1\"]\nparams = {'depth': 6,\n         'l2_leaf_reg': 9,\n         'learning_rate': 0.07,\n         'grow_policy': 'Depthwise',\n         'cat_features': cat_features,\n         'verbose':200,\n         'eval_metric':'Accuracy'}\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y1, random_state=12)\n\nclf_rf = RandomForestClassifier(max_depth=90, min_samples_split=10, n_estimators=1800)\n\nclf_cat = CatBoostClassifier(**params)\n\nclf_xg = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=1, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=10, missing=np.nan, monotone_constraints='()',\n              n_estimators=300, n_jobs=0, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1.0,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n#clf_ad = AdaBoostClassifier()\n\n#clf_lgb = lgb.LGBMClassifier()\n\nclf1 = VotingClassifier(estimators=[('rf', clf_rf), ('xgb', clf_xg), ('cat', clf_cat)], voting='soft')\n\nclf1.fit(x,y1)","f0793be7":"test_ = test_.drop(['listing_date', 'issue_date'], axis=1)\ntest_ = test_.fillna(4)\n\ntest_[['X1', 'X2', 'color_type', 'condition', 'time_diff','length(m)',\n       'height']] = test_[['X1', 'X2', 'color_type', 'condition', 'length(m)', 'time_diff',\n       'height']]\ntest_.columns = ['X1', 'X2', 'color_type', 'condition', 'time_diff','length(m)',\n       'height']\ntest_['condition'] = test_['condition'].astype('int')","41f889e0":"test_['color_type'] = test_['color_type'].astype('int')\ntest_['X1'] = test_['X1'].astype('int')\ny2 = clf2.predict(test_)\ny2 = np.ravel(y2)\n\ntest_['breed_category'] = y2\ntest_['breed_category'] = test_['breed_category'].astype('int')\ny1 = clf1.predict(test_)\ny1 = np.ravel(y1)","7f8d2efc":"dataframe = pd.DataFrame({'pet_id':test.pet_id, \n                         'breed_category':y2,\n                         'pet_category':y1})\ndataframe.breed_category = dataframe.breed_category.astype('int')\ndataframe.pet_category = dataframe.pet_category.astype('int')\n\n#dataframe.to_csv('submission\/votingclassifier21.csv', index=False)","9403bac3":"## Step 2: Explore and clean Dataset","4e5c6363":"### Voting Classifer for predicting breed_category","5d98f0c4":"## Voting Classifier 2","84fc9a6d":"### RandomizedSearchCV","f44dda1c":"## Step 3: Modelling ","37802627":"## APPROACH :\n\nThe idea here is to predict breed_category first and join it with train_data and train one more model and predict pet_category.<br>\n<br> **Model used:** Voting Classifier with CatBoostClassifer, RandomForestClassifer and XGBoost Classifier.<br>\nWe used RandomizedSearchCV for searching best parameters.<br>\n","c2ea7e9f":"## Mistakes and Improvement ","98683cfe":"## Filling condtion with BY PREDICTING with XGBClassifier","daa45d6b":"### Problem\nA leading pet adoption agency is planning to create a virtual tour experience for their customers showcasing all animals that are available in their shelter. To enable this tour experience, you are required to build a Machine Learning model that determines type and breed of the animal based on its physical attributes and other factors.","ba0d71de":"## Step 1: Load Dataset and Libraries","bc479112":"# Submission","add0196c":"### Label encoding color_type","0e427735":"### Features Description\n<ul>\n<li>pet_id-\tUnique Pet Id <\/li>\n<li>issue_date-\tDate on which the pet was issued to the shelter <\/li>\n<li>listing_date-- Date when the pet arrived at the shelter\n<li>condition- Condition of the pet\n<li>color_type- color of the pet\n<li>length(m)- Length of the pet (in meter)\n<li>height(cm)- Height of the pet (in centimeter)\n<li>X1,X2- Anonymous columns\n<li>breed_category- Breed category of the pet (target variable)\n<li>pet_category- Category of the pet (target variable)\n    \n<\/ul>","c39599ff":"**Lack of Data Visualization**: Using Data Visualization some hidden relationship among condtion, pet_category and breed_category can be seen. <br>\n**Lenght**: Lenght can not be zero, raplace it with median. <br>\n**CatBoost**: X2 and X1 column both should be used in Categorical Columns.<br>\n**time_diff**: Months and hours instead of second could be used. \n**new columns**: new columns can be created using X1, X2, length, height.","0c4be050":"Most of the missing sample are predicted as 2.0 but when i filled them with 2.0 accuracy goes down. So they can be filled with a new category (3.0)","d3fa4a3e":"# Summary of NoteBook\n### NoteBook consist of:\n#### 1. Load and  Explaore Dataset\n#### 2. Cleaning and Featue Engineering\n#### 3. Modelling and grid seach\n#### 4. Mistakes and Improvement (Please upvote if you found this notebook helpful )","a333e52b":"### Please do comment for any suggestions, queries, errors. ThankYou !!!"}}