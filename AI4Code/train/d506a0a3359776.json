{"cell_type":{"3da70bc1":"code","24b67622":"code","74ee2a98":"code","d09912c6":"code","3054d853":"code","d0d78ec0":"code","c4d5dbd0":"code","5946e532":"code","1dae52a5":"code","e1439714":"code","df860032":"code","c80f9a25":"code","83b7406a":"code","0e538aad":"code","d333c97b":"code","1233e19a":"code","08ca3f66":"code","9a5a78aa":"code","168733c5":"code","a4b9ed2e":"code","d01f2d40":"code","75c784e1":"code","04e2a897":"code","742e49f1":"code","2c6d8343":"code","0ecf9a76":"code","e5506971":"code","edfd1b7d":"code","bd308a78":"code","00a3d791":"code","5e49a078":"code","32a6c11b":"code","b624a0dd":"code","8a2ea6bb":"code","78cc6832":"code","323a9353":"code","02b38158":"code","35da9d21":"code","74481afd":"code","838137c6":"code","bf42e667":"code","a951370e":"code","8368f1cf":"code","c962c131":"code","2d9b14b5":"code","e894018d":"code","51296eb2":"code","341f677b":"code","b4397ab4":"code","6a785944":"code","c8d1cecf":"code","ba8e56de":"code","ebaa1a82":"code","e0801b03":"code","707deae0":"code","fe97aa74":"code","7e3fa195":"code","90efe42b":"code","d9798c52":"code","5233fb04":"code","4a31407f":"code","21d83a94":"code","08981034":"code","8a23b923":"code","5204528b":"code","6c5d036f":"code","8a0e684a":"markdown","6fa4ff61":"markdown","a6e576cf":"markdown","00c7ac4d":"markdown","248e2c51":"markdown","3a481fc9":"markdown","e5e0e566":"markdown","12effce2":"markdown","3a1bd609":"markdown","6131e1cb":"markdown","7136dc32":"markdown","f4f6eb58":"markdown","75351e5f":"markdown","9f4933f1":"markdown","7419a7b1":"markdown","88f97cdc":"markdown","6228f711":"markdown","d199572f":"markdown","e2b6502e":"markdown","037eefbe":"markdown"},"source":{"3da70bc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24b67622":"import pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv')","74ee2a98":"df.head()","d09912c6":"df.tail()","3054d853":"df.describe()","d0d78ec0":"print('Shape of tweets dataframe : {}'.format(df.shape))","c4d5dbd0":"df.info()","5946e532":"\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef return_missing_values(data_frame):\n    missing_values = data_frame.isnull().sum()\/len(data_frame)\n    missing_values = missing_values[missing_values>0]\n    missing_values.sort_values(inplace=True)\n    return missing_values\n\ndef plot_missing_values(data_frame):\n    missing_values = return_missing_values(data_frame)\n    missing_values = missing_values.to_frame()\n    missing_values.columns = ['count']\n    missing_values.index.names = ['Name']\n    missing_values['Name'] = missing_values.index\n    sns.set(style='whitegrid', color_codes=True)\n    sns.barplot(x='Name', y='count', data=missing_values)\n    plt.xticks(rotation=90)\n    plt.show()\n     ","1dae52a5":"\n\nreturn_missing_values(df)","e1439714":"plot_missing_values(df)","df860032":"# heatmap representation of missing values\n\n# plasma,visdir\n\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='ocean')","c80f9a25":"def return_unique_values(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['Features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['Uniques'] = uniques\n    return unique_dataframe","83b7406a":"udf = return_unique_values(df)\nprint(udf)","0e538aad":"f, ax = plt.subplots(1,1, figsize=(10,5))#plt.figure(figsize=(10, 5))\n\nsns.barplot(x=udf['Features'], y=udf['Uniques'], alpha=0.8)\nplt.title('Bar plot for #unique values in each column')\nplt.ylabel('#Unique values', fontsize=12)\nplt.xlabel('Features', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()","d333c97b":"def plot_frequency_charts(df, feature, title, pallete):\n    freq_df = pd.DataFrame()\n    freq_df[feature] = df[feature]\n    \n    f, ax = plt.subplots(1,1, figsize=(16,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n\n    plt.title('Frequency of {} tweeting about Corona'.format(feature))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel(title, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()\n    ","1233e19a":"plot_frequency_charts(df, 'user_name', 'User Names','Wistia')","08ca3f66":"plot_frequency_charts(df, 'user_location', 'User Locations', 'BuGn_r')","9a5a78aa":"plot_frequency_charts(df, 'source','Source', 'vlag')","168733c5":"from string import punctuation\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])\n\ndef punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms","a4b9ed2e":"df.head()","d01f2d40":"from collections import Counter\n\ndef draw_bar_graph_for_text_visualization(df, location):\n    tweets_from_loc = df.loc[df.user_location==location]\n    tweets_from_loc.loc[:, 'text'] = tweets_from_loc['text'].apply(punctuation_stopwords_removal)\n    loc_tweets_curated = tweets_from_loc['text'].tolist()\n    loc_tweet_list = []\n    for sublist in loc_tweets_curated:\n        for word in sublist:\n            loc_tweet_list.append(word)\n    loc_tweet_count = Counter(loc_tweet_list)\n    loc_top_30_words = pd.DataFrame(loc_tweet_count.most_common(50), columns=['word', 'count'])\n    fig, ax = plt.subplots(figsize=(16, 6))\n    sns.barplot(x='word', y='count', \n                data=loc_top_30_words, ax=ax)\n    plt.title(\"Top 50 Prevelant Words in {}\".format(location))\n    plt.xticks(rotation='vertical');\n    ","75c784e1":"from wordcloud import WordCloud, STOPWORDS\n\n\n\ndef draw_word_cloud(df, location, title):\n    loc_df = df.loc[df.user_location==location]\n    loc_df.loc[:, 'text'] = loc_df['text'].apply(punctuation_stopwords_removal)\n    word_cloud = WordCloud(\n                    background_color='white',\n                    stopwords=set(STOPWORDS),\n                    max_words=50,\n                    max_font_size=40,\n                    scale=5,\n                    random_state=1).generate(str(loc_df['text']))\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n    plt.imshow(word_cloud)\n    plt.show()\n    ","04e2a897":"draw_bar_graph_for_text_visualization(df, 'India')","742e49f1":"draw_word_cloud(df, 'India', 'Word Cloud for top 50 prevelant words in India')","2c6d8343":"draw_bar_graph_for_text_visualization(df, 'United Kingdom')","0ecf9a76":"draw_word_cloud(df, 'United Kingdom', 'Word Cloud for top 50 prevelant words in United Kingdom')","e5506971":"draw_bar_graph_for_text_visualization(df, 'Canada')","edfd1b7d":"draw_word_cloud(df, 'Canada', 'Word Cloud for top 50 prevelant words in Canada')","bd308a78":"draw_bar_graph_for_text_visualization(df, 'South Africa')","00a3d791":"draw_word_cloud(df, 'South Africa', 'Word Cloud for top 50 prevelant words in South Africa')","5e49a078":"draw_bar_graph_for_text_visualization(df, 'Switzerland')","32a6c11b":"draw_word_cloud(df, 'Switzerland', 'Word Cloud for top 50 prevelant words in Switzerland')","b624a0dd":"draw_bar_graph_for_text_visualization(df, 'London')","8a2ea6bb":"draw_word_cloud(df, 'London', 'Word Cloud for top 50 prevelant words in London')","78cc6832":"sentiment_df = pd.read_csv('\/kaggle\/input\/twitterdata\/finalSentimentdata2.csv')","323a9353":"sentiment_df.head()","02b38158":"sentiment_df.columns","35da9d21":"sentiment_df['sentiment'].nunique","74481afd":"sentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(punctuation_stopwords_removal)","838137c6":"reviews_split = []\nfor i, j in sentiment_df.iterrows():\n    reviews_split.append(j['text'])\n","bf42e667":"words = []\nfor review in reviews_split:\n    for word in review:\n        words.append(word)\n","a951370e":"print(words[:20])","8368f1cf":"from collections import Counter\n\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}","c962c131":"encoded_reviews = []\nfor review in reviews_split:\n    encoded_reviews.append([vocab_to_int[word] for word in review])\n","2d9b14b5":"print(len(vocab_to_int))\nprint(encoded_reviews[:10])","e894018d":"labels_to_int = []\nfor i, j in sentiment_df.iterrows():\n    if j['sentiment']=='joy':\n        labels_to_int.append(1)\n    else:\n        labels_to_int.append(0)\n    ","51296eb2":"reviews_len = Counter([len(x) for x in encoded_reviews])\nprint(max(reviews_len))","341f677b":"print(len(encoded_reviews))","b4397ab4":"non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\nencoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\nencoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])","6a785944":"print(len(encoded_reviews))\nprint(len(encoded_labels))","c8d1cecf":"def pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for i, row in enumerate(reviews_int):\n        if len(row)!=0:\n            features[i, -len(row):] = np.array(row)[:seq_length]\n    return features","ba8e56de":"seq_length = 50\npadded_features= pad_features(encoded_reviews, seq_length)\nprint(padded_features[:2])\n","ebaa1a82":"split_frac = 0.8\nsplit_idx = int(len(padded_features)*split_frac)\n\ntraining_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\ntraining_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","e0801b03":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader","707deae0":"# torch.from_numpy creates a tensor data from n-d array\ntrain_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n\nbatch_size = 1\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)","fe97aa74":"gpu_available = torch.cuda.is_available\n\nif gpu_available:\n    print('Training on GPU')\nelse:\n    print('GPU not available')","7e3fa195":"import torch.nn as nn\n\nclass CovidTweetSentimentAnalysis(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n        super(CovidTweetSentimentAnalysis, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","90efe42b":"vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1 # either happy or sad\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2","d9798c52":"net = CovidTweetSentimentAnalysis(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","5233fb04":"lr = 0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","4a31407f":"epochs = 4\ncount = 0\nprint_every = 100\nclip = 5 \nif gpu_available:\n    net.cuda()\n\nnet.train()\nfor e in range(epochs):\n    # initialize lstm's hidden layer \n    h = net.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        count += 1\n        if gpu_available:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        h = tuple([each.data for each in h])\n        \n        # training process\n        net.zero_grad()\n        outputs, h = net(inputs, h)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm(net.parameters(), clip)\n        optimizer.step()\n        \n        # print average training losses\n        if count % print_every == 0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                if gpu_available:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n            outputs, val_h = net(inputs, val_h)\n            val_loss = criterion(outputs.squeeze(), labels.float())\n            val_losses.append(val_loss.item())\n        \n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(count),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","21d83a94":"test_losses = []\nnum_correct = 0\n\nh = net.init_hidden(batch_size)\nnet.eval()\n\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    if gpu_available:\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    outputs, h = net(inputs, h)\n    test_loss = criterion(outputs.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(outputs.squeeze())\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n# printing average statistics\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n    \n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","08981034":"from string import punctuation\n\ndef tokenize_covid_tweet(tweet):\n    test_ints = []\n    test_ints.append([vocab_to_int[word] for word in tweet])\n    return test_ints","8a23b923":"def predict_covid_sentiment(net, test_tweet, seq_length=50):\n    print('Original Sentence :')\n    print(test_tweet)\n    \n    print('\\nAfter removing punctuations and stop-words :')\n    test_tweet = punctuation_stopwords_removal(test_tweet)\n    print(test_tweet)\n    \n    print('\\nAfter converting pre-processed tweet to tokens :')\n    tokenized_tweet = tokenize_covid_tweet(test_tweet)\n    print(tokenized_tweet)\n    \n    print('\\nAfter padding the tokens into fixed sequence lengths :')\n    padded_tweet = pad_features(tokenized_tweet, 50)\n    print(padded_tweet)\n    \n    feature_tensor = torch.from_numpy(padded_tweet)\n    batch_size = feature_tensor.size(0)\n    \n    if gpu_available:\n        feature_tensor = feature_tensor.cuda()\n    \n    h = net.init_hidden(batch_size)\n    output, h = net(feature_tensor, h)\n    \n    predicted_sentiment = torch.round(output.squeeze())\n    print('\\n==========Predicted Sentiment==========\\n')\n    if predicted_sentiment == 1:\n        print('Happy')\n    else:\n        print('Sad')\n    print('\\n==========Predicted Sentiment==========\\n')\n","5204528b":"test_sad_tweet = 'It is very sad to see the corona pandemic increasing at such an alarming rate'\npredict_covid_sentiment(net, test_sad_tweet)","6c5d036f":"test_happy_tweet = 'It is amazing to see that New Zealand reaches 100 days without Covid transmission!'\npredict_covid_sentiment(net, test_happy_tweet)","8a0e684a":"## Frequency of locations tweeting about Corona","6fa4ff61":"## Instantiate the network\nHere, I will define the model hyper-parameters -<br>\n\n1. `vocab_size` : Size of our vocabulary or the range of values for our input, word tokens.\n2. `output_size` : Size of our desired output; the number of class scores we want to output (pos\/neg).\n3. `embedding_dim` : Number of columns in the embedding lookup table; size of our embeddings.\n4. `hidden_dim` : Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n5. `n_layers`: Number of LSTM layers in the network. Typically between 1-3","a6e576cf":"## Encoding Sentiments\n\nFor simplicity purposes, I am encoding positive sentiment such as joy as 1 and rest (anger, sad) as 0","00c7ac4d":"# Sentiment Analysis on Covid19 Tweets\n\nFor this part of the notebook I will be using [Covid 19 Indian Sentiments on covid19 and lockdown](https:\/\/www.kaggle.com\/surajkum1198\/twitterdata) dataset.","248e2c51":"# EDA and Sentiment Analysis on COVID19 Tweets\n\nMy attempt to perform EDA and sentiment analysis on covid19 tweets. \n\nThis dataset contains covid19 tweet but since there is no column indicating tweet's sentiment, I have on-boarded another Kaggle dataset, namely [Covid 19 Indian Sentiments on covid19 and lockdown](https:\/\/www.kaggle.com\/surajkum1198\/twitterdata), which contains cleaned tweets from india on topics like coronavirus ,covid 19 and lockdown etc. The tweets have been collected betwen dates 23th march 2020 and 15th july 2020. I would also like to aknowledege Gabriel Preda's notebook on [covid-19 tweets](https:\/\/www.kaggle.com\/gpreda\/coronavirus-covid-19-tweets). \n\nThis notebook is organized as follows:\n\n1. EDA on Covid19 tweets<br> \n    * Plot missing values.\n    * Plot unique values.\n    * Plot frequency of users tweeting about Corona\n    * Plot frequency of locations tweeting about Corona\n    * Plot frequency of sources tweeting about Corona\n    * Visualizing location-wise top 50 prevelant words\n2. Sentiment Analysis on Covid19 Tweets<br>\n    * Exploring tweet data\n    * Encoding tweets\n    * Encoding sentiments\n    * Detecting outlier reviews\n    * Training, testing and validating\n    * Dataloaders and batching\n    * Sentiment network with PyTorch\n    * Instantiate the netork\n    * Calculating model's accuracy\n    * Testing model on a random covid19 tweet.","3a481fc9":"## Training, Testing and Validating","e5e0e566":"## Exploring Tweet Data\n\n* Sentiment Analysis on Covid19 Tweets\n    * Exploring tweet data\n    * Encoding tweets\n    * Encoding sentiments\n    * Detecting outlier reviews\n    * Training, testing and validating\n    * Dataloaders and batching\n    * Sentiment network with PyTorch\n    * Instantiate the netork\n    * Calculating model's accuracy\n    * Testing model on a random covid19 tweet.","12effce2":"# EDA on Covid19 Tweets","3a1bd609":"## Testing model on random tweet\n\nSince for performing sentiment analysis on covid 19 tweets, I on-boarded a completely different dataset in this notebook. Now that the our model is trained,we can use this model to perform sentiment analysis on tweets related to covid19 on this notebook.","6131e1cb":"## Dataloaders and Batching\n\nA neat way to create data-loaders and batch our training, validation and test Tensor datasets is as follows -<br>\n```python\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\n```\nThis is an alternative to creating a generator function for batching our data into full batches.","7136dc32":"## Encoding Tweets\nCreate an array that contains integer encoded version of words in reviews. The word appearing the most should have least integer value. Example if the appeared the most in reviews, then assign 'the' : 1","f4f6eb58":"### Acceptable color pallets in seaborn (i.e. we can experiment with `cmap` value below) : <br>\n\n```python\nAccent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\n```\n","75351e5f":"## Frequency of users tweeting about Corona","9f4933f1":"## Plot Unique Values ","7419a7b1":"## Frequency of sources tweeting about Corona","88f97cdc":"## Calculating model's accuracy\n\nThe `CovidTweetSentimentAnalysis` model achieved accuracy of 85.4 %","6228f711":"## Sentiment Network with PyTorch\nBelow are the various layers of our RNN that would perform sentiment analysis -<br>\n1. An *embedding layer* that converts our word tokens (integers) into embeddings of a specific size.\n2. A *LSTM layer* defined by a hidden_state size and number of layers\n3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n4. A sigmoid activation layer which turns all outputs into a value 0-1; return only the last sigmoid output as the output of this network.\"","d199572f":"## Detecting any outlier reviews\n\nThis step involves -<br>\n1. Getting rid of extremely long\/short reviews\n2. Padding\/truncating reaining data to maintain constant review length.","e2b6502e":"## Visualizing top 30 words location wise","037eefbe":"\n## Plot Missing Values"}}