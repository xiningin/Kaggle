{"cell_type":{"774423c2":"code","cb849af1":"code","f390957a":"code","111dfe69":"code","c6d04b44":"code","8180bfd3":"code","63089f6d":"code","6f7ffbb8":"code","a5e4a981":"code","25390758":"code","46dd25fd":"code","b5849fbd":"code","0da3a238":"code","d804cd42":"code","0fb8113f":"code","8dd31020":"code","7849ccba":"code","0789d9cd":"code","500fe97c":"code","7ebde1c5":"code","da3b4d91":"code","7e575f94":"code","945f9b1b":"code","8e8de108":"code","acabe79d":"code","de2d8a43":"code","bfd61f85":"code","06d7aef1":"code","4f4c168e":"code","bad884cd":"code","4b72f80b":"code","bdd7958b":"code","8a07f095":"code","70b04fec":"code","2a1cb9ab":"code","862a991f":"code","250e8fc7":"code","ca8fa3e4":"code","9a63acce":"code","4a938332":"code","cfd3c762":"code","7cb78a14":"code","14bcca78":"code","33202793":"code","63b6a128":"code","614a61cd":"code","2498ed9d":"code","c0b9d65b":"code","2ace02cd":"code","47f7e4bd":"code","ea120c1e":"code","d937eec0":"code","84498513":"code","1f0dce53":"code","66137952":"code","b2e40803":"code","de398d73":"code","1d28b14d":"code","97735a15":"code","4824817a":"markdown","d907ed8f":"markdown","fa81fc28":"markdown","e81d5b67":"markdown","2adb07b2":"markdown","793f461b":"markdown","d0825650":"markdown","d315bce2":"markdown","271b8b22":"markdown","873c537a":"markdown","698973e2":"markdown","23e8a000":"markdown","720c24f9":"markdown","2f05014c":"markdown","2371963f":"markdown","29499029":"markdown","965b744a":"markdown","128781ff":"markdown","4da669d3":"markdown","968539e0":"markdown","35139564":"markdown","c7e6e093":"markdown","1a1f3f43":"markdown","975a4cf9":"markdown","7c85c4b9":"markdown","2c6c7714":"markdown","5255a88b":"markdown","cead73c7":"markdown","a3a80a93":"markdown","cdf52f66":"markdown","56f00b98":"markdown","4f596f39":"markdown","b3c8974f":"markdown","e5a74475":"markdown","a9783e15":"markdown","57757d3f":"markdown"},"source":{"774423c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cb849af1":"# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f390957a":"# Uploading dataset from drive location using pandas library\naccidents=pd.read_csv(r'\/kaggle\/input\/us-accidents\/US_Accidents_Dec19.csv')","111dfe69":"accidents.head()","c6d04b44":"data=accidents\ndata","8180bfd3":"accidents.info()","63089f6d":"accidents.describe()","6f7ffbb8":"accidents.shape","a5e4a981":"accidents.columns","25390758":"len(accidents)","46dd25fd":"# Convert Start_Time and End_Time to datetypes\naccidents['Start_Time'] = pd.to_datetime(accidents['Start_Time'], errors='coerce')\naccidents['End_Time'] = pd.to_datetime(accidents['End_Time'], errors='coerce')","b5849fbd":"# Extract year, month, day, hour and weekday\naccidents['Year']=accidents['Start_Time'].dt.year\naccidents['Month']=accidents['Start_Time'].dt.strftime('%b')\naccidents['Day']=accidents['Start_Time'].dt.day\naccidents['Hour']=accidents['Start_Time'].dt.hour\naccidents['Weekday']=accidents['Start_Time'].dt.strftime('%a')","0da3a238":"# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\naccidents[td]=round((accidents['End_Time']-accidents['Start_Time'])\/np.timedelta64(1,'m'))\naccidents.info()","d804cd42":"# Check if there is any negative time_duration values\naccidents[td][accidents[td]<=0]","0fb8113f":"# Drop the rows with td<0\n\nneg_outliers=accidents[td]<=0\n\n# Set outliers to NAN\naccidents[neg_outliers] = np.nan\n\n# Drop rows with negative td\naccidents.dropna(subset=[td],axis=0,inplace=True)\naccidents.info()","8dd31020":"# Double check to make sure no more negative td\naccidents[td][accidents[td]<=0]","7849ccba":"# Remove outliers for Time_Duration(min): n * standard_deviation (n=3), backfill with median\n\nn=3\n\nmedian = accidents[td].median()\nstd = accidents[td].std()\noutliers = (accidents[td] - median).abs() > std*n\n\n# Set outliers to NAN\naccidents[outliers] = np.nan\n\n# Fill NAN with median\naccidents[td].fillna(median, inplace=True)\n\naccidents.info()","0789d9cd":"# Print time_duration information\nprint('Max time to clear an accident: {} minutes or {} hours or {} days; Min to clear an accident td: {} minutes.'.format(accidents[td].max(),round(accidents[td].max()\/60), round(accidents[td].max()\/60\/24), accidents[td].min()))","500fe97c":"# Set the list of features to include in Machine Learning\nfeature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat','Distance(mi)','Side','City','County','State','Timezone','Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)', 'Wind_Direction','Weather_Condition','Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']","7ebde1c5":"# Select the dataset to include only the selected features\naccidents_sel=accidents[feature_lst].copy()\naccidents_sel.info()","da3b4d91":"# Check missing values\naccidents_sel.isnull().mean()","7e575f94":"accidents_sel.dropna(subset=accidents_sel.columns[accidents_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\naccidents_sel.shape","945f9b1b":"# Set state\nstate='PA'\n\n# Select the state of Pennsylvania\naccidents_statePA=accidents_sel.loc[accidents_sel.State==state].copy()\naccidents_statePA.drop('State',axis=1, inplace=True)\naccidents_statePA.info()","8e8de108":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=accidents_statePA, hue='County', legend=False, s=20)\nplt.show()","acabe79d":"# Set county\ncounty='Montgomery'\n\n# Select the state of Pennsylvania\naccidents_countyMO=accidents_statePA.loc[accidents_statePA.County==county].copy()\naccidents_countyMO.drop('County',axis=1, inplace=True)\naccidents_countyMO.info()","de2d8a43":"# Map of accidents, color code by city\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=accidents_countyMO, hue='City', legend=False, s=20)\nplt.show()","bfd61f85":"# Set state\nstate='CA'\n\n# Select the state of California\naccidents_stateCA=accidents_sel.loc[accidents_sel.State==state].copy()\naccidents_stateCA.drop('State',axis=1, inplace=True)\naccidents_stateCA.info()","06d7aef1":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=accidents_stateCA, hue='County', legend=False, s=20)\nplt.show()","4f4c168e":"# Set county\ncounty='Sacramento'\n\n# Select the state of Pennsylvania\naccidents_countySA=accidents_stateCA.loc[accidents_stateCA.County==county].copy()\naccidents_countySA.drop('County',axis=1, inplace=True)\naccidents_countySA.info()","bad884cd":"# Map of accidents, color code by city\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=accidents_countySA, hue='City', legend=False, s=20)\nplt.show()","4b72f80b":"# Generate dummies for categorical data\naccidents_countyMO_dummy = pd.get_dummies(accidents_countyMO,drop_first=True)\naccidents_countyMO_dummy.info()","bdd7958b":"from sklearn.model_selection import train_test_split\n# Assign the data\naccidents=accidents_countyMO_dummy\n\n# Set the target for the prediction\ntarget='Severity'\n\n\n# Create arrays for the features and the response variable\n\n# set X and y\ny = accidents[target]\nX = accidents.drop(target, axis=1)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","8a07f095":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest','Naive Bayes']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","70b04fec":"# Import KNeighbors Classifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTree Classifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n#Import Navie Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Import SVM Classifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\n","2a1cb9ab":"# Logistic regression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))","862a991f":"# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))\nprint('[K-Nearest Neighbors (KNN)] accuracy_score: {:.3f}.'.format(acc))","250e8fc7":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, n_neighbor in enumerate(neighbors):\n    \n    # Setup a k-NN Classifier with n_neighbor\n    knn = KNeighborsClassifier(n_neighbors=n_neighbor)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","ca8fa3e4":"# Decision tree algorithm\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n\n# Print accuracy_entropy\nprint('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\n# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\n\n# Print accuracy_gini\nprint('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))","9a63acce":"# Random Forest algorithm\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Randon forest algorithm] accuracy_score: {:.3f}.\".format(acc))","4a938332":"feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=20\nsns.barplot(x=feature_imp[:20], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","cfd3c762":"# List top k important features\nk=20\nfeature_imp.sort_values(ascending=False)[:k]","7cb78a14":"# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.03\nsfm = SelectFromModel(clf, threshold=0.03)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeat_labels=X.columns\n\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    print(feat_labels[feature_list_index])","14bcca78":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)","33202793":"# Apply The Full Featured Classifier To The Test Data\ny_pred = clf.predict(X_test)\n\n# View The Accuracy Of Our Full Feature Model\nprint('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# Apply The Full Featured Classifier To The Test Data\ny_important_pred = clf_important.predict(X_important_test)\n\n# View The Accuracy Of Our Limited Feature Model\nprint('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))","63b6a128":"#Create a Gaussian Classifier\nmodel= GaussianNB()\n\n#Train the model using the training sets y_pred=model.predict(X_test)\nmodel.fit(X_train,y_train)\n\n# Predicting the Model\ny_pred = model.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Navie Bayes algorithm] accuracy_score: {:.3f}.\".format(acc))","614a61cd":"# Make a plot of the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\naccidents_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Export to a file\n# accidents_acc.to_csv('Accuracy_scores_algorithms_{}.csv'.format(state),index=False)\n\n\n# Make a plot\nax=accidents_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, accidents_acc['Algorithm'], rotation=0)\nplt.title('[{}-{}] Which algorithm is better?'.format(state, county))\n\nplt.show()","2498ed9d":"data[\"Severity\"].value_counts()","c0b9d65b":"# plot a histogram  \ndata['Severity'].hist(bins=10)","2ace02cd":"# Distribution of accidents according to their severity and when (Day\/Night) they occur.\nfig,ax = plt.subplots(1,2,figsize=(18,6))\n\n# Pie chart, that shows the distribution of accidents by severity from 1 to 4.\nsizes = data.groupby('Severity').size() # serie with severity as index and the number of row of each categorie as values.\nsizes = sizes[[2,1,3,4]] # the serie is reordered so that the values are clearly visible in the pie.\nlabels = 'Severity 2','Severity 1', 'Severity 3', 'severity 4' \nexplode = (0.1,0.1,0.1,0.1)\ncolors = ['red','purple','green','yellow']\nax[0].pie(sizes, explode=explode, labels= labels,colors=colors,autopct='%1.2f%%',shadow=True, startangle=0)\nax[0].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax[0].set_title('Accidents by Severity', size=20)\n\n# Seaborn countplot of accident distributions.\nsns.countplot(data=data,x='Severity', hue='Sunrise_Sunset',ax=ax[1])\nax[1].set_title('Accidents by Severity and Moment', size= 20)\nax[1].set_xlabel('Severity', size=18)\nax[1].set_ylabel('Number of Accidents', size=18)\n","47f7e4bd":"fig, ax = plt.subplots(figsize=(10, 6))\nax.set_title('Scatterplot', fontsize=22)\nax.plot(data['Severity'], data['Visibility(mi)'], 'ko')\n\n# This scatter plot gives the severity of accidents with respect to the visibility.","ea120c1e":"fig=sns.heatmap(data[['TMC','Severity','Distance(mi)',\n                    'Temperature(F)','Wind_Chill(F)','Humidity(%)',\n                    'Pressure(in)','Visibility(mi)','Wind_Speed(mph)']].corr(),\n                annot=True,cmap='RdBu',linewidths=0.2,annot_kws={'size':15})\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()\n\n#Heatmap is a two-dimensional graphical representation of data where the individual values that are contained in a matrix are represented as colors\n# Or we can also say that these Heat maps display numeric tabular data where the cells are colored depending upon the contained value. \n#Heat maps are great for making trends in this kind of data more readily apparent, particularly when the data is ordered and there is clustering.","d937eec0":"plt.figure(figsize =(10,5))\ndata.groupby(['Year']).size().sort_values(ascending=True).plot.bar()","84498513":"data['time'] = pd.to_datetime(data.Start_Time, format='%Y-%m-%d %H:%M:%S')\n\nplt.subplots(2,2,figsize=(15,10))\nfor s in np.arange(1,5):\n    plt.subplot(2,2,s)\n    plt.hist(pd.DatetimeIndex(data.loc[data[\"Severity\"] == s]['time']).month, bins=[1,2,3,4,5,6,7,8,9,10,11,12,13], align='left', rwidth=0.8)\n    plt.title(\"Accident Count by Month with Severity \" + str(s), fontsize=14)\n    plt.xlabel(\"Month\", fontsize=16)\n    plt.ylabel(\"Accident Count\", fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\nplt.tight_layout()\nplt.show()","1f0dce53":"plt.figure(figsize =(15,5))\ndata.groupby(['Month']).size().plot.bar()","66137952":"data['DayOfWeek'] = data['time'].dt.dayofweek\nplt.subplots(2,2,figsize=(15,10))\nfor s in np.arange(1,5):\n    plt.subplot(2,2,s)\n    plt.hist(data.loc[data[\"Severity\"] == s]['DayOfWeek'], bins=[0,1,2,3,4,5,6,7], align='left', rwidth=0.8)\n    plt.title(\"Accident Count by Day with Severity \" + str(s), fontsize=16)\n    plt.xlabel(\"Day\", fontsize=16)\n    plt.ylabel(\"Accident Count\", fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\nplt.tight_layout()\nplt.show()","b2e40803":"for s in np.arange(1,5):\n    plt.subplots(figsize=(12,5))\n    data.loc[data[\"Severity\"] == s]['Weather_Condition'].value_counts().sort_values(ascending=False).head(20).plot.bar(width=0.5,color='y',edgecolor='k',align='center',linewidth=1)\n    plt.xlabel('Weather Condition',fontsize=16)\n    plt.ylabel('Accident Count',fontsize=16)\n    plt.title('20 of The Main Weather Conditions for Accidents of Severity ' + str(s),fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)","de398d73":"for s in [\"Fog\",\"Light Rain\",\"Rain\",\"Heavy Rain\",\"Snow\"]:\n    plt.subplots(1,2,figsize=(12,5))\n    plt.suptitle('Accident Severity Under ' + s,fontsize=16)\n    plt.subplot(1,2,1)\n    data.loc[data[\"Weather_Condition\"] == s]['Severity'].value_counts().plot.bar(width=0.5,color='y',edgecolor='k',align='center',linewidth=1)\n    plt.xlabel('Severity',fontsize=16)\n    plt.ylabel('Accident Count',fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.subplot(1,2,2)\n    data.loc[data[\"Weather_Condition\"] == s]['Severity'].value_counts().plot.pie(autopct='%1.0f%%',fontsize=16)","1d28b14d":"factors = ['Temperature(F)','Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n\nfor factor in factors:\n    # remove some of the extreme values\n    factorMin = data[factor].quantile(q=0.0001)\n    factorMax = data[factor].quantile(q=0.9999)\n    # print df[\"Severity\"].groupby(pd.cut(df[factor], np.linspace(factorMin,factorMax,num=20))).count()\n    plt.subplots(figsize=(15,5))\n    for s in np.arange(1,5):\n        data[\"Severity\"].groupby(pd.cut(data[factor], np.linspace(factorMin,factorMax,num=20))).mean().plot()\n        plt.title(\"Mean Severity as a Function of \" + factor, fontsize=16)\n        plt.xlabel(factor + \" Range\", fontsize=16)\n        plt.ylabel(\"Mean Severity\", fontsize=16)\n        plt.xticks(fontsize=11)\n        plt.yticks(fontsize=16)","97735a15":"for s in ['Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']:\n    # check if infrastructure type is found in any record \n    if (data[s] == True).sum() > 0:\n        plt.subplots(1,2,figsize=(12,5))\n        plt.xticks(fontsize=14)\n        plt.suptitle('Accident Severity Near ' + s,fontsize=16)\n        plt.subplot(1,2,1)\n        data.loc[data[s] == True]['Severity'].value_counts().plot.bar(width=0.5,color='y',edgecolor='k',align='center',linewidth=1)\n        plt.xlabel('Severity',fontsize=16)\n        plt.ylabel('Accident Count',fontsize=16)\n        plt.xticks(fontsize=16)\n        plt.yticks(fontsize=16)\n        plt.subplot(1,2,2)\n        data.loc[data[s] == True]['Severity'].value_counts().plot.pie(autopct='%1.0f%%',fontsize=16)","4824817a":"# Algorithm 1 . Logistic regression","d907ed8f":"# By days","fa81fc28":"# Predict the accident severity with various supervised machine learning algorithms\n## Data preparation: train_test_split","e81d5b67":"# Fill outliers with median values","2adb07b2":"# Metadata process","793f461b":"There is a drop in the number of accidents for all severity levels during the weekend. Although, the relative drop for level 3 and 4 is smaller.","d0825650":"Across all levels of severity, most accidents happen under clear, cloudy, fair or similar weather conditions. Light rain and light snow are the top adverse weather conditions. Most likely these cause accidents since they can make roads slippery without causing concern in the drivers.","d315bce2":"# Importing important libraries and packages","271b8b22":"## Time\n### By year","873c537a":"## Infrastructure","698973e2":"## Weather\u00b6\n### Most Frequent Weather Conditions","23e8a000":"# Extract year, month, day, hour, weekday, and time to clear accidents","720c24f9":"# Algorithm 4 . Random Forest\u00b6\n## Select the top important features, set the threshold","2f05014c":"Any comments, suggestions are welcome.","2371963f":"# Select the state of interest: PA , CA or any other; and County of interest: Montgomery ,Sacramento or any other","29499029":"# Algorithm 2 . The K-Nearest Neighbors (KNN) algorithm\n## Optmize the number of neighors: plot the accuracy versus number of neighbors\n","965b744a":"# Drop rows with missing values","128781ff":"The proportion of level 3 and 4 accidents increases as weather changes from fog (25%) to light rain (36%) to rain (39%) to heavy rain (40%) to snow (41%).","4da669d3":"# Algorithm 5 . Navie Bayes","968539e0":"# ***Import the dataset***","35139564":" It is interesting that April, May, June and July have much lower accident counts. Mostly accidents occur in August, September and October.","c7e6e093":"# Dealing with outliers\nDrop rows with negative time_duration","1a1f3f43":"# Converting datatypes in datetime format","975a4cf9":"# Algorithm 3. Decision Tree","7c85c4b9":"# Select a list of features for machine learning algorithms","2c6c7714":"# Algorithm 2. The K-Nearest Neighbors (KNN) algorithm\n## KNN with 6 neighors","5255a88b":"### By months","cead73c7":"# Visualization Analysis\n## On the basis of severity","a3a80a93":"# Deal with categorical data: pd.get_dummies()","cdf52f66":"# Algorithm 4 . Random Forest## \nn_estimators=100","56f00b98":"# Algorithm 4 . Random Forest\n## Visualize important features","4f596f39":"## Other Weather Factors","b3c8974f":"## Severity by Fog, Light Rain, Rain, Heavy Rain and Snow","e5a74475":"Mean severity increases as conditions for freezing precipitation increase, and as we saw in the previous section rain and snow have higher proportion of level 3 and 4 severity. These conditions include decreasing temperature, wind chill, and air pressure [1] as well as increasing humidity. Severity also increases as a function of wind speed.","a9783e15":"# Plot the accuracy score versus algorithm","57757d3f":"Junctions, give way, and no exit have the highest proportion of level 3 and level 4 severity accidents. "}}