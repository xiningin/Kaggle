{"cell_type":{"6d7ad5f4":"code","22b7bde8":"code","85d41ed2":"code","4aa00b6f":"code","8def3961":"code","2253f081":"code","ced76dab":"code","2dc623a8":"code","670461b0":"code","fde24948":"code","2a8d16ff":"code","ce9ba80a":"code","af0d83ae":"code","935b0889":"code","11bf5b08":"code","0b213f0c":"code","81631be2":"code","3f8cffd3":"code","68219f3a":"code","71c6ed00":"code","eb047fe9":"code","d2de428f":"code","00f8b824":"code","bb7ad3b5":"code","f90c55d1":"code","02aa6808":"code","984940bb":"code","08172ee1":"code","6ed41340":"code","0cc1531d":"code","463230c6":"code","d34b781e":"code","4c131c0d":"code","67fa540c":"code","88f788bb":"code","c3322fa5":"code","622928ec":"code","76447840":"code","9133b79c":"code","7d0f16d2":"code","a45ce467":"code","3c350a87":"code","702944e8":"code","d82041da":"code","8248cc42":"code","39753bbc":"markdown","076f7b57":"markdown","fe5a8530":"markdown","3c4e9f8a":"markdown","cc40be07":"markdown","09f30bc8":"markdown","a7230687":"markdown","2e8fd879":"markdown","2986622d":"markdown","7dfcfed7":"markdown","893d78d0":"markdown","ab55b7c0":"markdown","d054817f":"markdown","1dde1ef6":"markdown","ebbffbcc":"markdown","8692e7c1":"markdown","abb99378":"markdown","14c02d3e":"markdown","a4a27ea4":"markdown","62533abc":"markdown","0daf9069":"markdown","fe47e405":"markdown","6e0a2756":"markdown","71a30a7d":"markdown","678e16dc":"markdown"},"source":{"6d7ad5f4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n\nfrom wordcloud import WordCloud,STOPWORDS\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking\n","22b7bde8":"df = pd.read_csv('..\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv')","85d41ed2":"#show dataframe\ndf.head()","4aa00b6f":"#show feature data types\ndf.info()","8def3961":"#show data basic stats by over_18 feature\ndf.groupby('over_18').describe()","2253f081":"#replace the boolean values on over_18 to numerical values\ndef replace_labels(x):\n    if x == False:\n        return 0\n    else:\n        return 1\n\ndf['over_18'] = df['over_18'].apply(replace_labels)","ced76dab":"plt.figure(figsize=(12,6))\nsns.heatmap(df.isnull(), yticklabels=False);","2dc623a8":"df.isnull().sum()","670461b0":"df.drop(['id', 'author_flair_text', 'removed_by',\n         'total_awards_received', 'awarders', 'created_utc', 'full_link'],\n        axis=1, inplace=True)","fde24948":"#add title character length feature\ndf['title_length'] = df['title'].apply(lambda x: len(str(x)))","2a8d16ff":"df = df.dropna()\nprint('DATAFRAME SHAPE: ',df.shape)\ndf.head()","ce9ba80a":"df.isnull().sum()","af0d83ae":"plt.figure(figsize=(7,5))\nplt.title('COUNTPLOT')\nplt.xlabel('Class')\nplt.ylabel('Count')\nsns.barplot(x=['Under 18', 'Over 18'],y= df.over_18.value_counts(), palette='viridis');","935b0889":"df.over_18.value_counts()","11bf5b08":"fig, ax = plt.subplots(1,2, figsize=(14,5))\nax[0].set_title('UNDER_18')\nax[1].set_title('OVER_18')\n\nsns.distplot(df[df['over_18']==0]['title_length'], ax=ax[0], color='steelblue');\nsns.distplot(df[df['over_18']==1]['title_length'], ax=ax[1], color='salmon');","0b213f0c":"#drop data which title character lenght is less than 5\ndf.drop(df[df['title_length']<5].index, inplace =True)","81631be2":"# combine author and title\ndf['text'] = df['title'] + ' ' + df['author']\ndf.drop(['title', 'author'], axis=1, inplace=True)","3f8cffd3":"df.over_18.value_counts()","68219f3a":"SAMPLES = 10000\n\nunder_18 = df[df['over_18']==0]\nunder_18 = under_18[(under_18['title_length']>25) & (under_18['title_length']<75)].sample(frac=1)\nunder_18 = under_18[:SAMPLES]\nover_18 = df[df['over_18']==1]\n\ndf_train = pd.concat([under_18, over_18])\ndf_train.index = np.arange(len(df_train))\n\n#check data frame\nprint('SHAPE: ', df_train.shape)\ndf_train.head()","71c6ed00":"\nX = df_train.drop(['score', 'num_comments', 'over_18', 'title_length'], axis=1)\ny = df_train['over_18']","eb047fe9":"plt.figure(figsize=(14,10))\nwc_under18 = WordCloud(min_font_size=3, max_font_size=3000, \n                       width=1920, height=1080, \n                       stopwords=STOPWORDS).generate(str(''.join(df_train[df_train['over_18']==0]['text'])))\n\nplt.imshow(wc_under18, interpolation='bilinear');","d2de428f":"plt.figure(figsize=(14,10))\nwc_over18 = WordCloud(min_font_size=3, max_font_size=3000, \n                       width=1920, height=1080, \n                       stopwords=STOPWORDS).generate(str(''.join(df_train[df_train['over_18']==1]['text'])))\n\nplt.imshow(wc_over18, interpolation='bilinear');","00f8b824":"topwords_over_18 = pd.Series(wc_over18.process_text(str(''.join(df_train[df_train['over_18']==1]['text'])))).sort_values(ascending=False)[:15]\n\nplt.figure(figsize=(10,7))\nsns.barplot(topwords_over_18.values, topwords_over_18.index, palette='magma');","bb7ad3b5":"topwords_under_18 = pd.Series(wc_over18.process_text(str(''.join(df_train[df_train['over_18']==0]['text'])))).sort_values(ascending=False)[:15]\n\nplt.figure(figsize=(10,7))\nsns.barplot(topwords_under_18.values, topwords_under_18.index, palette='coolwarm');","f90c55d1":"# get the stop words, punctuation, and also add the OC, deleted and OC deleted\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)\nremoved_topwords = ['deleted', 'oc', 'deleted oc', 'oc deleted']\nstop.update(removed_topwords)","02aa6808":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","984940bb":"#lematizing function\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text","08172ee1":"#process the text data\nX.text = X.text.apply(lemmatize_words)","6ed41340":"X.text = X.text.apply(lambda i: ' '.join(i))\n#check data\nX.text","0cc1531d":"#split the data\nX_train, X_test, y_train, y_test = train_test_split(X.text, y, test_size=0.2, random_state=101)","463230c6":"pipeline = Pipeline([\n    ('count', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('model', RandomForestClassifier())\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","d34b781e":"#classification report\nprint(classification_report(y_test, predictions))","4c131c0d":"plt.figure(figsize=(5,5))\ncon_mat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(con_mat, annot=True, square=True);\n\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","67fa540c":"pipeline = Pipeline([\n    ('count', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('model', LogisticRegression())\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","88f788bb":"#classification report\nprint(classification_report(y_test, predictions))","c3322fa5":"plt.figure(figsize=(5,5))\ncon_mat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(con_mat, annot=True, square=True);\n\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","622928ec":"pipeline = Pipeline([\n    ('count', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('model', LinearSVC())\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","76447840":"#classification report\nprint(classification_report(y_test, predictions))","9133b79c":"plt.figure(figsize=(5,5))\ncon_mat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(con_mat, annot=True, square=True);\n\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","7d0f16d2":"pipeline = Pipeline([\n    ('count', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('model', XGBClassifier(loss = 'deviance',\n                                    learning_rate = 0.02,\n                                    n_estimators = 10,\n                                    max_depth = 7,\n                                    random_state=101))\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","a45ce467":"#classification report\nprint(classification_report(y_test, predictions))","3c350a87":"plt.figure(figsize=(5,5))\ncon_mat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(con_mat, annot=True, square=True);\n\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","702944e8":"pipeline = Pipeline([\n    ('count', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('model', SGDClassifier(n_jobs=-1))\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","d82041da":"#classification report\nprint(classification_report(y_test, predictions))","8248cc42":"plt.figure(figsize=(5,5))\ncon_mat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(con_mat, annot=True, square=True);\n\nplt.xlabel('Y_TRUE');\nplt.ylabel('PREDICTIONS');","39753bbc":"### LOGISTIC REGRESSION","076f7b57":"## EDA\n---","fe5a8530":"### RANDOM FOREST CLASSIFIER","3c4e9f8a":"#### CONFUSION MATRIX","cc40be07":"### DISTRIBUTION PER TITLE CHARACTER LENGHT","09f30bc8":"### MOST FREQUENT WORDS OF UNDER 18","a7230687":"### GET THE DATA","2e8fd879":"### CHECK NULL VALUES","2986622d":"## PREDICTIVE MODELLING\n---","7dfcfed7":"* OC, OC deleted, deleted are on top words, I'll remove them later by including them on stopwords because it may confuse the model.","893d78d0":"## OVERVIEW\n---\n* Exploratory Data Analysis\n* Data Cleaning and Feature selection\n* Data Sampling\n* Text Preprocessing\n    * Punctuation, Stopwords\n    * Stemming and Lemmatizing\n* Predictive Modelling\n    * Random Forest Classifier\n    * LinearSVC\n    * Logistic Regression\n    * XGBClassifier\n    * Stochastic Gradient Descent","ab55b7c0":"#### CONFUSION MATRIX","d054817f":"### TEXT PREPROCESSING","1dde1ef6":"#### CONFUSION MATRIX","ebbffbcc":"#### CONFUSION MATRIX","8692e7c1":"#### REDUCING THE DATA BY ONLY TAKING SAMPLES\n* From the distribution plot above, we can see that most of data in under_18 class have a 25-75 character length. so i'll take samples from that range.","abb99378":"### SHOW COUNTPLOT OF OVER_18 LABELS","14c02d3e":"#### CONFUSION MATRIX","a4a27ea4":"### LINEAR SVC","62533abc":"### SGD CLASSIFIER","0daf9069":"### MOST FREQUENT WORDS OF OVER 18","fe47e405":"### FEATURE SELECTION","6e0a2756":"### WORDCLOUD UNDER 18","71a30a7d":"### XGBOOST CLASSIFIER","678e16dc":"### WORDCLOUD OVER_18"}}