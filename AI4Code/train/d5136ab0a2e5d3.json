{"cell_type":{"f7e38f5d":"code","24c94059":"code","92f29fd1":"code","c197c624":"code","56327466":"code","abafc2e8":"code","137ed8dd":"code","8934c07f":"code","af9aa980":"code","6173de8b":"code","c0d0bfad":"code","0164b65e":"code","7df16cff":"code","edeea281":"code","d7374488":"code","55c66a99":"code","09c9ac77":"code","e71b852c":"code","7e4b19cc":"code","70c61c2b":"code","c1fdcb19":"code","ee9ddca2":"code","36cbf7a1":"code","445914d6":"code","df76a1a0":"code","6ec0f49c":"code","071783b2":"code","8fef1ade":"code","a4852330":"code","9823bfa2":"code","692c8d95":"code","a9d67b62":"markdown","e63e980f":"markdown","a5daf08e":"markdown","e77a772d":"markdown","07ff6e33":"markdown","9c865350":"markdown","d543599b":"markdown","bc736512":"markdown","cb5a7d2b":"markdown","5c6b0209":"markdown","4be3c072":"markdown","ad518f41":"markdown"},"source":{"f7e38f5d":"from keras.models import Model, Sequential\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import glorot_uniform\nfrom keras.preprocessing import text, sequence\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np \nimport pandas as pd \nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom matplotlib import rcParams\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(0)\n","24c94059":"!kaggle datasets download -d watts2\/glove6b50dtxt","92f29fd1":"df=pd.read_csv('..\/input\/phone-reviews\/phone_reviews.csv')\ndf.info()","c197c624":"df['star']-=1\nx=df['body']\ny=df['star']","56327466":"\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)","abafc2e8":"sentence_lengths=X_train.apply(lambda x:len(x.split()))","137ed8dd":"sentence_lengths.describe()","8934c07f":"maxlen=100\nloss='categorical_crossentropy'\nbatch_size=32\noptimizer='adam'\nepochs=10","af9aa980":"\n\n\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r',encoding='UTF-8') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\n\n\n#convert number to one_hot vector\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)]\n    return Y\n\n\n#sentences to word indices\ndef sentences_to_indices(X, word_to_index):  \n    m = X.shape[0]                                   # number of training examples\n\n   \n    X_indices = np.zeros(m)\n    X_indices=[]\n    for i in range(m):                               # loop over training examples\n\n        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n        sentence_words = [word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n        #sentence_words=X[i].split(' ')\n        # Initialize j to 0\n        j=0\n        \n        # Loop over the words of sentence_words\n        li=np.zeros(len(sentence_words))\n        for w in sentence_words:\n            \n            # Set the (i,j)th entry of X_indices to the index of the correct word.\n            try:\n                li[j] = word_to_index[w]\n            except:\n               # print(w)\n                li[j]= 0\n            # Increment j to j + 1\n            j += 1\n        X_indices.append(li)    \n    X_indices=np.array(X_indices)\n    return X_indices\n\n\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    \n    vocab_len = len(word_to_index) + 1              \n    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n   \n\n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output\/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","6173de8b":"word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('..\/input\/glove6b50dtxt\/glove.6B.50d.txt')\n\n#testing\nword = \"good\"\nidx = 567\nprint(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\nprint(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])","c0d0bfad":"X1 = np.array([\"the phone is good\", \"very bad\", \"no star rating\"])\nX1_indices = sentences_to_indices(X1,word_to_index)\nprint(\"X1 =\", X1)\nprint(\"X1_indices =\\n\", X1_indices)","0164b65e":"embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n\nprint(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])","7df16cff":"def lstmModel(input_shape, word_to_vec_map, word_to_index):\n    \n    sentence_indices = Input(shape=input_shape, dtype='int32')\n    \n    # Create the embedding layer pretrained with GloVe Vectors (\u22481 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)   \n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(128, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(128, return_sequences=False)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(5, activation=None)(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=[sentence_indices], outputs=X)\n    \n    return model","edeea281":"model = lstmModel((100,), word_to_vec_map, word_to_index)\nmodel.summary()","d7374488":"model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])","55c66a99":"X_train_np=np.array(X_train)\ny_train_np=np.array(y_train)\nX_test_np=np.array(X_test)\ny_test_np=np.array(y_test)\n\nY_train_oh = convert_to_one_hot(y_train_np, C = 5)\nY_test_oh = convert_to_one_hot(y_test_np, C = 5)\n\nX_test_indices = sentences_to_indices(X_test_np, word_to_index)\nX_train_indices = sentences_to_indices(X_train_np, word_to_index)\n\nX_train_indices = sequence.pad_sequences(X_train_indices, maxlen=100)\nX_test_indices = sequence.pad_sequences(X_test_indices, maxlen=100)","09c9ac77":"model_history=model.fit(X_train_indices, Y_train_oh, epochs = epochs, batch_size = batch_size, shuffle=True)","e71b852c":"test_preds=model.predict(X_test_indices)\ntest_preds = [np.argmax(pred) for pred in test_preds]\ntrain_preds=model.predict(X_train_indices)\ntrain_preds = [np.argmax(pred) for pred in train_preds]","7e4b19cc":"from sklearn.metrics import cohen_kappa_score,accuracy_score\nprint(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(test_preds, y_test.astype('int'), weights='quadratic'))\nprint(\"Train Accuracy score : %.3f\" % accuracy_score(y_test.astype('int'),test_preds))\nprint(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds, y_train.astype('int'), weights='quadratic'))\nprint(\"Train Accuracy score : %.3f\" % accuracy_score(y_train.astype('int'),train_preds))","70c61c2b":"def sentiment_category(score):\n    if score >= 4:\n        return \"positive\"\n    elif score <= 2:\n        return \"negative\"\n    else:\n        return \"neutral\"","c1fdcb19":"\ndf_indices = sentences_to_indices(np.array(df['body']), word_to_index)\ndf_indices = sequence.pad_sequences(df_indices, maxlen=100)\ndf_preds=model.predict(df_indices)\ndf_preds = [np.argmax(pred) for pred in df_preds]\ndf['sentiment_score']=df_preds\ndf['review_category']=df['sentiment_score'].apply(lambda x:sentiment_category(x))","ee9ddca2":"plt.figure(figsize=(6,6))\nsns.countplot(df['review_category']).set_title(\"Distribution of Reviews Category\")","36cbf7a1":"positive_reviews=df.loc[df['review_category']=='positive','body'].tolist() # extracting all positive reviews and converting to a list\npositive_reviews[0:5]","445914d6":"negative_reviews=df.loc[df['review_category']=='negative','body'].tolist() # extracting all negative reviews and converting to a list\nfor review in negative_reviews[0:5]:\n    print(review)","df76a1a0":"\nplt.figure(figsize=(12,12))\nwordcloud = WordCloud(height=4000, width=4000, background_color='black')\nwordcloud = wordcloud.generate(' '.join(df.loc[df['review_category']=='positive','body'].tolist()))\nplt.imshow(wordcloud)\nplt.title(\"Most common words in positive customer comments\")\nplt.axis('off')\nplt.show()","6ec0f49c":"plt.figure(figsize=(12,12))\nwordcloud = WordCloud(height=4000, width=4000, background_color='black')\nwordcloud = wordcloud.generate(' '.join(df.loc[df['review_category']=='negative','body'].tolist()))\nplt.imshow(wordcloud)\nplt.title(\"Most common words in negative customer comments\")\nplt.axis('off')\nplt.show()","071783b2":"def getMostCommon(reviews_list,topn=20):\n    reviews=\" \".join(reviews_list)\n    tokenised_reviews=reviews.split(\" \")\n    freq_counter=Counter(tokenised_reviews)\n    return freq_counter.most_common(topn)","8fef1ade":"def generateNGram(text,n):\n    tokens=text.split(\" \")\n    ngrams = zip(*[tokens[i:] for i in range(n)])\n    return [\"_\".join(ngram) for ngram in ngrams]","a4852330":"positive_reviews_bigrams=[\" \".join(generateNGram(review,2)) for review in positive_reviews]\nnegative_reviews_bigrams=[\" \".join(generateNGram(review,2)) for review in negative_reviews]","9823bfa2":"def plotMostCommonWords(reviews_list,topn=30,title=\"Common Review Words\",color=\"blue\",axis=None): #default number of words is given as 30\n    top_words=getMostCommon(reviews_list,topn=topn)\n    data=pd.DataFrame()\n    data['words']=[val[0] for val in top_words]\n    data['freq']=[val[1] for val in top_words]\n    if axis!=None:\n        sns.barplot(y='words',x='freq',data=data,color=color,ax=axis).set_title(title+\" top \"+str(topn))\n    else:\n        sns.barplot(y='words',x='freq',data=data,color=color).set_title(title+\" top \"+str(topn))","692c8d95":"rcParams['figure.figsize'] = 15,20\nrcParams['font.size']=15\nfig,ax=plt.subplots(1,2)\nfig.subplots_adjust(wspace=1)\nplotMostCommonWords(positive_reviews_bigrams,40,'Positive Review Bigrams',axis=ax[0])\n\nplotMostCommonWords(negative_reviews_bigrams,40,'Negative Review Bigrams',color=\"red\",axis=ax[1])","a9d67b62":"*Note:* There are some words that should not be there such as \"galaxym21,samsung galaxy,redmi note\"","e63e980f":"### Prepare the data for training","a5daf08e":"So, by the above we can take the the length of  the sentence as 100. ","e77a772d":"### Split the data of training and evaluating the model","07ff6e33":"# **Preparing and Training the Sentiment analysis Model**","9c865350":"# Generating Bigrams\nBigrams are more effective than unigrams","d543599b":"### Import the pretrained word embeddings and Data\n<b>Note:<\/b> \n    If you are running this notebook in local or colab you have to get the kaggle auth keys.","bc736512":"Training","cb5a7d2b":"# *Import all the packges required*","5c6b0209":"### Building the Model of 2 layers of LSTM","4be3c072":"This module is imported from coursera sequential models assignment","ad518f41":"# Hyperparameters"}}