{"cell_type":{"b2f0684e":"code","78cd3641":"code","43164fc5":"code","38dea267":"code","e659be09":"code","026f7774":"code","c35e2253":"code","a2e00e8b":"code","d06494cb":"code","29ca83cb":"code","f746d1ee":"code","7bf6d4fc":"code","5f8f61ec":"code","e5c0cf66":"code","97b15863":"code","457400e6":"code","be74e395":"code","897ef702":"code","bfcfff98":"code","4fa98284":"code","822a8fb0":"code","10809208":"code","3d3fa6de":"markdown","6fdbf992":"markdown","885505b4":"markdown","85ff7bd9":"markdown","207bbe56":"markdown","c3a68d1c":"markdown","5a958b02":"markdown","55c98339":"markdown","990c2b44":"markdown","10d763c1":"markdown","3e26f51f":"markdown","8f163e48":"markdown","29206da0":"markdown","6c27c518":"markdown","afba9828":"markdown","a1c67c16":"markdown"},"source":{"b2f0684e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split # train-test split\nfrom sklearn.metrics import confusion_matrix, classification_report # classification metrics\nfrom imblearn.over_sampling import SMOTE # SMOTE\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler # scaling methods\n\nfrom sklearn.model_selection import GridSearchCV # grid search cross validation\nfrom sklearn.model_selection import RandomizedSearchCV # randomized search cross validation\n\n# supervised learning algorithms\nfrom sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbbors\nfrom sklearn.naive_bayes import GaussianNB # Gaussain Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier # Adaptive Boosting Classifier\nfrom sklearn.ensemble import BaggingClassifier # Bootstrap Aggregating Classifier\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78cd3641":"# combining trian and test datasets\n\ndf = pd.concat([pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTrain.csv'),pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTest.csv')], ignore_index=True)\ndf.drop('Unnamed: 0',axis=1,inplace=True) # unnecessary column\ndf.head()","43164fc5":"import pandas_profiling\n\ndf.profile_report()","38dea267":"# Checking Null values\npd.DataFrame(df.isnull().value_counts())","e659be09":"# Binarizing Gender column\ndef gender_binarizer(x):\n    if x=='F':\n        return 1\n    if x=='M':\n        return 0\n    \ndf['gender'] = df['gender'].transform(gender_binarizer)","026f7774":"# Seperating nominal from numeric\n# Note:There are almost 2M records in dfz.In order to avoid the heavy calculation,only the first 100000 rows were selected.\ndf2 = df.loc[:99999,df.dtypes!=np.object]\ndf2","c35e2253":"X = df2.drop(['cc_num','is_fraud'],axis=1)\ny = df2['is_fraud']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","a2e00e8b":"sm = SMOTE()\nX_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\n\n# to demonstrate the effect of SMOTE over imbalanced datasets\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize =(15, 5))\nax1.set_title('Before SMOTE')\npd.Series(y_train).value_counts().plot.bar(ax=ax1)\n\nax2.set_title('After SMOTE')  \npd.Series(y_train_new).value_counts().plot.bar(ax=ax2)\n\nplt.show()","d06494cb":"X_train, y_train = sm.fit_resample(X_train, y_train.ravel())","29ca83cb":"# to compare the effect of each scaler on our dataset\nscaler = RobustScaler()\nrobust_df = scaler.fit_transform(df2)\nrobust_df = pd.DataFrame(robust_df)\n  \nscaler = StandardScaler()\nstandard_df = scaler.fit_transform(df2)\nstandard_df = pd.DataFrame(standard_df)\n  \nscaler = MinMaxScaler()\nminmax_df = scaler.fit_transform(df2)\nminmax_df = pd.DataFrame(minmax_df)\n\n# using KDE plot\n#Note: some columns are opted out in order to speed up the process\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df2['merch_long'], ax = ax1)\nsns.kdeplot(df2['merch_lat'], ax = ax1)\nsns.kdeplot(df2['city_pop'], ax = ax1)\nsns.kdeplot(df2['long'], ax = ax1)\nsns.kdeplot(df2['lat'], ax = ax1)\n\n\nax2.set_title('After Robust Scaling')  \nsns.kdeplot(robust_df[9], ax = ax2)\nsns.kdeplot(robust_df[8], ax = ax2)\nsns.kdeplot(robust_df[7], ax = ax2)\nsns.kdeplot(robust_df[5], ax = ax2)\nsns.kdeplot(robust_df[4], ax = ax2)\n\n\nax3.set_title('After Standard Scaling')  \nsns.kdeplot(standard_df[9], ax = ax3)\nsns.kdeplot(standard_df[8], ax = ax3)\nsns.kdeplot(standard_df[7], ax = ax3)\nsns.kdeplot(standard_df[5], ax = ax3)\nsns.kdeplot(standard_df[4], ax = ax3)\n\n\nax4.set_title('After Min-Max Scaling')  \nsns.kdeplot(minmax_df[9], ax = ax4)\nsns.kdeplot(minmax_df[8], ax = ax4)\nsns.kdeplot(minmax_df[7], ax = ax4)\nsns.kdeplot(minmax_df[5], ax = ax4)\nsns.kdeplot(minmax_df[4], ax = ax4)\n\nplt.show()","f746d1ee":"scaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","7bf6d4fc":"param_grid = {'n_neighbors': range(1,20)}\nclf = RandomizedSearchCV(KNeighborsClassifier(), param_grid)\nclf.fit(X_train,y_train)\nclf_pred = clf.predict(X_test)","5f8f61ec":"### Determining the number of neighbors using RandomizedSearchCV\nparam_grid = {'n_neighbors': range(1,20)}\nknn = RandomizedSearchCV(KNeighborsClassifier(), param_grid, verbose=3)\nknn.fit(X_train,y_train)","e5c0cf66":"knn.best_params_ # best parameter","97b15863":"knn_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","457400e6":"gnb = GaussianNB()\ngnb.fit(X_train,y_train)\ngnb_pred = gnb.predict(X_test)\n\nprint(confusion_matrix(y_test,gnb_pred))\nprint('\\n')\nprint(classification_report(y_test,gnb_pred))","be74e395":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_pred = dtree.predict(X_test)\n\nprint(confusion_matrix(y_test,dtree_pred))\nprint('\\n')\nprint(classification_report(y_test,dtree_pred))","897ef702":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\nrfc_pred = rfc.predict(X_test)\n\nprint(confusion_matrix(y_test,rfc_pred))\nprint('\\n')\nprint(classification_report(y_test,rfc_pred))","bfcfff98":"adabc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200)\nadabc.fit(X_train,y_train)\nadabc_pred = adabc.predict(X_test)\n\nprint(confusion_matrix(y_test,adabc_pred))\nprint('\\n')\nprint(classification_report(y_test,adabc_pred))","4fa98284":"bgc = BaggingClassifier(DecisionTreeClassifier(),n_estimators=200)\nbgc.fit(X_train,y_train)\nbgc_pred = bgc.predict(X_test)\n\nprint(confusion_matrix(y_test,bgc_pred))\nprint('\\n')\nprint(classification_report(y_test,bgc_pred))","822a8fb0":"from sklearn.metrics import roc_curve, roc_auc_score\n\n\n# Instantiate the classfiers and make a list\nclassifiers = [GaussianNB(), \n               KNeighborsClassifier(n_neighbors= knn.best_params_.get('n_neighbors')),\n               DecisionTreeClassifier(random_state=42),\n               RandomForestClassifier(random_state=42),\n               AdaBoostClassifier(random_state=42),\n               BaggingClassifier(random_state=42)\n              ]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    yproba = model.predict_proba(X_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","10809208":"# Plotting ROC curve \n\nfig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","3d3fa6de":"# Classification Evaluation via AUROC","6fdbf992":"## Resampling via SMOTE","885505b4":"# Gaussian Naive Bayes","85ff7bd9":"# Pre-Processing","207bbe56":"# Decision Tree","c3a68d1c":"# Random Forest","5a958b02":"Since we have a huge amount of data, its better to normalize the dataset by using RobustScaler which scales the data according to the quantile range.\n","55c98339":"# Bagging","990c2b44":"# KNN","10d763c1":"## Dataset descriptions","3e26f51f":"## Train Test Split","8f163e48":"## Scaling","29206da0":"## Data Cleaning","6c27c518":"# AdaBoost","afba9828":"### Robust Scaler VS MinMaxScaler VS Standard Scaler","a1c67c16":"The dataset is heavily imbalanced. Through resampling, fraud transactions (Class = 1) are randomly increased to the same amount as non-fraud transactions (Class = 0) in order to avoid the bias results toward the non-fraudulent class."}}