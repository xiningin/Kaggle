{"cell_type":{"4ceba4ff":"code","2d96eb90":"code","787c1ef1":"code","1b3b2df6":"code","0a0b15aa":"code","5dab518c":"code","bb08042f":"code","5bc8a4a5":"code","86f525ee":"code","7c53e83e":"code","6d8d1827":"code","662b2693":"code","670f4fad":"code","72701ae9":"code","7e8127df":"code","691ba430":"code","35e5f5b7":"code","ce3d0090":"code","077027bb":"code","4d95904e":"code","5d7f2d68":"code","3ae46849":"code","af4fb6b6":"code","4c4c8061":"code","923f1380":"code","6d681d39":"code","1931c23f":"code","d604d06e":"code","580bb370":"code","57b10deb":"code","fd792157":"code","a9708a74":"code","ef81a525":"code","3faa4420":"code","2c347b09":"code","83083363":"code","5c72bb52":"code","6d2a0e43":"code","dd15451a":"code","ecc0b1ef":"code","3d90e1b8":"code","63e6a83e":"code","156de17c":"code","e1e5edff":"markdown","5188978f":"markdown","611e0b51":"markdown","e977310f":"markdown","c3bfc84b":"markdown","a8f11a16":"markdown","478fec1a":"markdown","03b7fa37":"markdown","d06878d6":"markdown","b280b0a8":"markdown","6f56ecb3":"markdown","fca862e2":"markdown"},"source":{"4ceba4ff":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\n\n# from category_encoders import MEstimateEncoder\n# from sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\n# from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# from IPython.display import clear_output\n# !pip install -q -U keras-tuner\n# clear_output()\nimport keras_tuner as kt\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","2d96eb90":"# -----------------------------------------------------------------\n# Some parameters to config \nMAX_TRIAL = 1 # speed trial any%\nEPOCHS = 60\n\n# not used\nBATCH_SIZE = 2048 # large enough to fit RAM\nACTIVATION = 'selu'\nKERNEL_INIT = \"glorot_uniform\" # lecun_normal\nLEARNING_RATE = 0.000965713 # Optimal lr is about half the maximum lr\nLR_FACTOR = 0.5 # LEARNING_RATE * LR_FACTOR = New Learning rate on ReduceLROnPlateau\nES_PATIENCE = 10\nRLRP_PATIENCE = 5\nDROPOUT = 0.15\n\nOPTIMIZER = 'adam' # adam adamax nadam\nLOSS ='sparse_categorical_crossentropy' # sparse_categorical_crossentropy categorical_crossentropy\nMETRICS ='accuracy'  # acc accuracy categorical_accuracy sparse_categorical_accuracy\nACC_VAL_METRICS = 'val_accuracy' # 'val_acc' val_accuracy val_sparse_categorical_accuracy\nACC_METRICS = 'accuracy' # acc accuracy 'sparse_categorical_accuracy'\n\nRANDOM_STATE = 31\nVERBOSE = 1\n\n# The dataset is too huge for free contrainer. Sampling it for more fun!\nSAMPLE = 195712 # [1468136, 2262087, 195712, 377, 1, 11426, 62261] # 4000000 total rows\nVALIDATION_SPLIT = 0.25\n\n# Admin\nID = \"Id\" # Id id x X index\nINPUT = \"..\/input\/tabular-playground-series-dec-2021\"","787c1ef1":"def load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=ID)\n    # Merge the splits so we can process them together\n#     df = pd.concat([df_train, df_test])\n    # Preprocessing\n#     df = clean(df)\n#     df = encode(df)\n    df_train = impute(df_train)\n    df_test = impute(df_test)\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    # Reform splits\n#     df_train = df.loc[df_train.index, :]\n#     df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n","1b3b2df6":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","0a0b15aa":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","5dab518c":"%%time\ndf_train, df_test = load_data()","bb08042f":"# Peek at the values\ndisplay(df_train)\n# display(df_test)\n\n# Display information about dtypes and missing values\n# display(df_train.info())\n# display(df_test.info())","5bc8a4a5":"TARGET_FEATURE_NAME = df_train.columns.difference(df_test.columns)[0]","86f525ee":"# customized XY TBR\nidx = df_train[df_train[TARGET_FEATURE_NAME] == 4].index\ndf_train.drop(idx, axis = 0, inplace = True)\nidx = df_train[df_train[TARGET_FEATURE_NAME] == 5].index\ndf_train.drop(idx, axis = 0, inplace = True)\n# idx = train_data[train_data[column_y] == 6].index # Less then 0.5% significant different, dropped\n# train_data.drop(idx, axis = 0, inplace = True)\n\ncols = [\"Soil_Type7\", \"Soil_Type15\"]\ndf_train.drop(cols, axis = 1, inplace= True)\ndf_test.drop(cols, axis = 1, inplace = True)","7c53e83e":"X_raw = df_train.drop(columns=TARGET_FEATURE_NAME)\ny_raw = df_train[TARGET_FEATURE_NAME]\n\nX_test_raw = df_test.iloc[:,:]\nTARGET_FEATURE_NAME","6d8d1827":"from sklearn.model_selection import train_test_split\n# Check NA\nmissing_val = X_raw.isnull().sum()\nprint(missing_val[missing_val > 0])","662b2693":"sampling_key, sampling_count = np.unique(y_raw, return_counts=True)\nsampling_count[sampling_count > SAMPLE] = SAMPLE\nzip_iterator = zip(sampling_key, sampling_count)\nsampling_params = dict(zip_iterator)\n\nundersample = RandomUnderSampler(\n    sampling_strategy=sampling_params)\n\nX_raw, y_raw = undersample.fit_resample(X_raw, y_raw)","670f4fad":"np.unique(y_raw, return_counts=True)","72701ae9":"transformer_all_cols = make_pipeline(\n    RobustScaler(),\n#     StandardScaler(),\n#     MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_all_cols, X_raw.columns[:]),\n)","7e8127df":"X_train = preprocessor.fit_transform(X_raw)\nX_test = preprocessor.transform(X_test_raw)","691ba430":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_raw) # Prepare for multiclass classification","35e5f5b7":"le.classes_","ce3d0090":"np.unique(y_train, return_counts=True)","077027bb":"X_raw[TARGET_FEATURE_NAME] = y_train","4d95904e":"train, val = train_test_split(X_raw, test_size=VALIDATION_SPLIT, stratify=X_raw[[TARGET_FEATURE_NAME]], random_state=RANDOM_STATE)","5d7f2d68":"X_train = train.drop(columns=TARGET_FEATURE_NAME)\ny_train = train[[TARGET_FEATURE_NAME]]\nX_val = val.drop(columns=TARGET_FEATURE_NAME)\ny_val = val[[TARGET_FEATURE_NAME]]","3ae46849":"import gc\ndel df_train\ndel df_test\ndel X_raw\ndel y_raw\ndel X_test_raw\ngc.collect()","af4fb6b6":"TARGET_FEATURE_LABELS = np.unique(y_train)\nNUM_CLASSES = len(TARGET_FEATURE_LABELS)\n\nINPUT_SHAPE = X_train.shape[-1]\nOUTPUT_SHAPE = le.classes_.shape[-1]","4c4c8061":"# y_train = tf.keras.utils.to_categorical(y_train, OUTPUT_SHAPE)","923f1380":"# Search for the best model with EarlyStopping.\nstop_early = tf.keras.callbacks.EarlyStopping(\n                                    patience=ES_PATIENCE,\n                                    monitor='val_loss',\n#                                     mode='min',\n#                                     restore_best_weights=True,       \n#                                     verbose=VERBOSE,\n                                    )\nreduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(\n                                    factor=LR_FACTOR,\n                                    patience=RLRP_PATIENCE,\n                                    monitor='val_loss',\n#                                     mode='min',\n#                                     verbose=VERBOSE,\n                                    )","6d681d39":"def model_builder(hp):\n    model = keras.Sequential()\n    model.add(keras.layers.Flatten(input_shape=(INPUT_SHAPE,)))\n\n    # Tune the number of units in the first Dense layer\n    # Choose an optimal value between 32-512\n    hp_units1 = hp.Int('units1', min_value=64, max_value=256, step=64)\n    hp_units2 = hp.Int('units2', min_value=16, max_value=256, step=16)\n    hp_dropout = hp.Float('dropout_rate', min_value=0.05, max_value=0.5, step=0.05)\n    \n    model.add(keras.layers.Dense(units=hp_units1, kernel_initializer = KERNEL_INIT, activation=ACTIVATION))\n    model.add(layers.AlphaDropout(rate = hp_dropout))\n    model.add(layers.BatchNormalization())\n    model.add(keras.layers.Dense(units=hp_units2, kernel_initializer = KERNEL_INIT, activation=ACTIVATION))\n    model.add(layers.AlphaDropout(rate = hp_dropout))\n    model.add(layers.BatchNormalization())\n    model.add(keras.layers.Dense(units=hp_units2, kernel_initializer = KERNEL_INIT, activation=ACTIVATION))\n    model.add(keras.layers.Dense(OUTPUT_SHAPE)) #, activation = 'softmax'\n\n    # Tune the learning rate for the optimizer\n    # Choose an optimal value from 0.01, 0.001, or 0.0001\n#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n    return model","1931c23f":"tuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy', # val_accuracy\n                     max_epochs=21,\n                     factor=3,\n                     directory='kt_softmax',\n                     project_name='kt_softmax',\n                     seed=RANDOM_STATE)","d604d06e":"%%time\ntuner.search(X_train, y_train, epochs=EPOCHS, validation_data=(X_val, y_val),\n                callbacks=[stop_early, reduceLROnPlateau])\n\n# Get the optimal hyperparameters ,seed=RANDOM_STATE\nbest_hps=tuner.get_best_hyperparameters(num_trials=MAX_TRIAL)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units1')}  {best_hps.get('units2')} and the optimal dropout rate for the optimizer\nis {best_hps.get('dropout_rate')}.\n\"\"\")\ntuner.results_summary()","580bb370":"# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_val, y_val), callbacks=[reduceLROnPlateau])\n\nval_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","57b10deb":"%%time\nhypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhistory = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_val, y_val), callbacks=[reduceLROnPlateau],\n                verbose=VERBOSE)","fd792157":"history1 = model.evaluate(X_val, y_val)","a9708a74":"# model = model.export_model()\nmodel.summary()","ef81a525":"# Predict with the best model.\npredicted_y = model.predict(X_test)\n# predicted_y = le.inverse_transform(clf.predict(X_test))\n","3faa4420":"predicted_y","2c347b09":"predicted_y.shape","83083363":"# Auto Keras converted y from int to string bug le.inverse_transform(np.argmax(predicted_y, axis=1))\npredicted_y = le.inverse_transform(np.argmax(predicted_y, axis=1))","5c72bb52":"# output = pd.DataFrame({ID: df_test.index, target_col: predicted_y[:,0]})\n\noutput = pd.read_csv(INPUT + \"\/sample_submission.csv\")\noutput[TARGET_FEATURE_NAME] = predicted_y\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\noutput","6d2a0e43":"def plot_acc(history_f):\n    fold = 0\n    best_epoch = np.argmin(np.array(history_f['val_loss']))\n    best_val_loss = history_f['val_loss'][best_epoch]\n\n    fig, ax1 = plt.subplots(1, 2, tight_layout=True, figsize=(15,4))\n\n    fig.suptitle('Fold : '+ str(fold+1) +\n                 \" Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()) +\n                 \" Validation Accuracy: {:0.4f}\".format(history_f[ACC_VAL_METRICS].max()) +\n                 \" LR: {:0.8f}\".format(history_f['lr'].min())\n                 , fontsize=14)\n\n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n\n    from_epoch = 0\n    if best_epoch >= from_epoch:\n        plt.scatter([best_epoch], [best_val_loss], c = 'r', label = f'Best val_loss = {best_val_loss:.5f}')\n    if best_epoch > 0:\n        almost_epoch = np.argmin(np.array(history_f['val_loss'])[:best_epoch])\n        almost_val_loss = history_f['val_loss'][almost_epoch]\n        if almost_epoch >= from_epoch:\n            plt.scatter([almost_epoch], [almost_val_loss], c='orange', label = 'Second best val_loss')\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper left')   \n\n    ax2 = plt.gca().twinx()\n    ax2.plot(history_f.loc[:, ['lr']], 'y:', label='lr' ) # default color is same as first ax\n    ax2.set_ylabel('Learning rate')\n    ax2.legend(loc = 'upper right')\n    ax2.grid()\n\n    best_epoch = np.argmax(np.array(history_f[ACC_VAL_METRICS]))\n    best_val_acc = history_f[ACC_VAL_METRICS][best_epoch]\n\n    plt.subplot(1,2,2)\n    plt.plot(history_f.loc[:, [ACC_METRICS, ACC_VAL_METRICS]],label= [ACC_METRICS, ACC_VAL_METRICS])\n    if best_epoch >= from_epoch:\n        plt.scatter([best_epoch], [best_val_acc], c = 'r', label = f'Best val_acc = {best_val_acc:.5f}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc = 'lower left')\n    plt.legend(fontsize = 15)\n    plt.grid(b = True, linestyle = '-')\n\nplot_acc(pd.DataFrame(history.history))","dd15451a":"np.unique(output[TARGET_FEATURE_NAME], return_counts=True)","ecc0b1ef":"# Plot the distribution of the test predictions\nfig, ax =plt.subplots(1,2,figsize=(10,4))\nsns.countplot(x=predicted_y, ax=ax[0], orient=\"h\").set_title(\"Prediction\")\n# Plot the distribution of the training set\nsns.countplot(x = y_train[TARGET_FEATURE_NAME], ax=ax[1], orient=\"h\").set_title(\"Training labels\")\nfig.show()","3d90e1b8":"kt.__version__","63e6a83e":"keras.__version__","156de17c":"tf.__version__","e1e5edff":"You can also export the best model found by AutoKeras as a Keras Model.","5188978f":"# Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file","611e0b51":"## Load Data ##\n\nAnd now we can call the data loader and get the processed data splits:","e977310f":"# Parameters\n","c3bfc84b":"# Hyperparameter Tuning #\n\nAt this stage, you might like to do auto hyperparameter tuning with AutoKeras before creating your final submission.\nAutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone.\n\nBy default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.","a8f11a16":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the competition dataset, we'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","478fec1a":"# Introduction #\nWelcome. Let automate machine learning as much as possible.\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!<\/strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit<\/strong> button in the top right corner.\n<\/blockquote>\n\nBugs:\nAK - No val_loss, val_accuracy from history.\n\n## Imports and Configuration ##\n\nWe'll start by importing the packages we used in the exercises and setting some notebook defaults. Unhide this cell if you'd like to see the libraries we'll use:","03b7fa37":"# Resampling\n\nAuto Keras y categories calculation wrong when cat 5 is missing etc","d06878d6":"TPS always have huge dataset.","b280b0a8":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","6f56ecb3":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/notebooks) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","fca862e2":"## Scaler transformer\nBy using RobustScaler(), we can remove the outliers. No good for this dataset test.\n![](https:\/\/github.com\/furyhawk\/kaggle_practice\/blob\/main\/images\/Scalers.png?raw=true)"}}