{"cell_type":{"841a8267":"code","756843e0":"code","19505a3e":"code","b0a4e331":"code","4d77715a":"code","31f08309":"code","de803ba7":"code","4e6e5be6":"code","5154b843":"code","79cfd0c3":"code","78074b12":"code","758a0a8f":"code","00d07b3f":"code","c6f37caf":"code","c82556b8":"code","5013b276":"code","ea01e181":"code","37b7a017":"code","00623571":"code","c76204b3":"code","34198096":"code","588b6d85":"code","1a5c0be7":"code","e7f47b3e":"code","51430796":"code","c02090d6":"code","2f542acd":"code","141ecf4e":"code","87d45eaf":"code","cdaad322":"code","8dd9fe36":"code","6319d043":"code","9ed993c1":"code","8d9ab653":"code","48bc1665":"code","cabf1ce7":"code","7c6226f8":"code","2ce81d81":"code","b6380057":"code","77a7c98d":"code","21756ba5":"code","9078d316":"code","051e04da":"code","2a4ced08":"code","296bf352":"code","051916a2":"code","17c70982":"code","0a8b0e3b":"code","826098df":"code","2efcc4d0":"code","fba74389":"code","3cdbfdae":"code","c10d3d49":"code","b50d8a08":"code","21592d78":"code","cf338a23":"code","4d29c520":"code","60b4c978":"code","e49289ff":"code","58f815ce":"code","42ad953d":"code","e2d8ea79":"code","d35643e1":"code","69214df1":"code","5a864d84":"code","52b696f9":"code","97c64385":"code","d4de70c9":"code","aedcb9b2":"code","69308ceb":"code","89856b73":"code","1b378967":"code","3b5d100f":"code","7650c22a":"code","972b4849":"code","78d532dd":"code","20bf776c":"code","70d8fc39":"code","ea9eadf3":"code","241ff1fa":"code","f5866de8":"code","00fe4da1":"code","3e355535":"code","85013926":"code","3b4a9fb0":"code","79d46ef1":"code","d0337914":"code","003260a9":"code","11dbcde8":"code","bc5fb747":"markdown","a53bcf4b":"markdown","735f5e47":"markdown","245b2ab9":"markdown","d0ca9f11":"markdown","4240ac0e":"markdown","8f370715":"markdown","b6cfc05b":"markdown","3bb6efb0":"markdown","ca27eb23":"markdown","d8b5df26":"markdown","95f5e744":"markdown","26cde1e3":"markdown","ea07e13b":"markdown","bc9b0800":"markdown","616dcba7":"markdown","fd719cc7":"markdown","ea8bbba6":"markdown","2469910a":"markdown","77f61723":"markdown","ec6c37cc":"markdown","c472cede":"markdown","9c0a4393":"markdown","83da0dc3":"markdown","9faa8082":"markdown","c28a94b5":"markdown","13670ff8":"markdown","06bd2bad":"markdown","0357a598":"markdown","19bcdb63":"markdown","4cfbf08f":"markdown","e9a4ab0e":"markdown","0eefa065":"markdown","f0bc054e":"markdown","e1ac8c05":"markdown","dfb40813":"markdown","f6454ebd":"markdown","958d7810":"markdown","c97509a2":"markdown","6e6aeb28":"markdown","cbd8741f":"markdown","b05a9564":"markdown","e52a8991":"markdown","3ed96538":"markdown","c1aab896":"markdown","82a66d6e":"markdown","8dfd7ecd":"markdown","2f772f7f":"markdown","3a4c4567":"markdown","90f17730":"markdown","5e07ec90":"markdown","bbb8cbae":"markdown","35958602":"markdown","03aaafd7":"markdown","cbce14a1":"markdown","35fa9a32":"markdown","d17c705f":"markdown","391062d1":"markdown","726fb6e3":"markdown","fe3eb0a5":"markdown","59533727":"markdown","d875e685":"markdown","a33c3249":"markdown","6ca6a225":"markdown","4c4486bf":"markdown","139e3a96":"markdown","6dd357dc":"markdown","a490be62":"markdown","29b4f18e":"markdown","0c730209":"markdown","f4856dd4":"markdown","d92c06f0":"markdown","3b4062fe":"markdown","a183ef85":"markdown","39af689c":"markdown","ccc5e89a":"markdown","0bb1b498":"markdown","ace1678b":"markdown","bdc3717b":"markdown","774fa9c9":"markdown","d5561b35":"markdown","87820faa":"markdown","86ea7a72":"markdown","9acd82b2":"markdown","a161377a":"markdown","9cfb6612":"markdown","51e549f3":"markdown","3af74c62":"markdown","73e0182d":"markdown"},"source":{"841a8267":"from time import time\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","756843e0":"# Reading the dataset with no columns titles and with latin encoding \ndf_raw = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", header=None)\n\n # As the data has no column titles, we will add our own\ndf_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n\n# Show the first 5 rows of the dataframe.\n# You can specify the number of rows to be shown as follows: df_raw.head(10)\ndf_raw.head()","19505a3e":"# Checking the data's output balance\n# The label '4' denotes positive sentiment and '0' denotes negative sentiment\ndf_raw['label'].value_counts()","b0a4e331":"# Ommiting every column except for the text and the label, as we won't need any of the other information\ndf = df_raw[['label', 'text']]\ndf.head()","4d77715a":"# Separating positive and negative rows\ndf_pos = df[df['label'] == 4]\ndf_neg = df[df['label'] == 0]\nprint(len(df_pos), len(df_neg))","31f08309":"# Only retaining 1\/4th of our data from each output group\n# Feel free to alter the dividing factor depending on your workspace\n# 1\/64 is a good place to start if you're unsure about your machine's power\ndf_pos = df_pos.iloc[:int(len(df_pos)\/4)]\ndf_neg = df_neg.iloc[:int(len(df_neg)\/4)]\nprint(len(df_pos), len(df_neg))","de803ba7":"# Concatinating both positive and negative groups and storing them back into a single dataframe\ndf = pd.concat([df_pos, df_neg])\nlen(df)","4e6e5be6":"start_time = time()\n\nfrom nltk.tokenize import TweetTokenizer\n# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\ntk = TweetTokenizer(reduce_len=True)\n\ndata = []\n\n# Separating our features (text) and our labels into two lists to smoothen our work\nX = df['text'].tolist()\nY = df['label'].tolist()\n\n# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n# and its corresponding label\nfor x, y in zip(X, Y):\n    if y == 4:\n        data.append((tk.tokenize(x), 1))\n    else:\n        data.append((tk.tokenize(x), 0))\n        \n# Printing the CPU time and the first 5 elements of our 'data' list\nprint('CPU Time:', time() - start_time)\ndata[:5]","5154b843":"from nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Previewing the pos_tag() output\nprint(pos_tag(data[0][0]))","79cfd0c3":"def lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n# Previewing the WordNetLemmatizer() output\nprint(lemmatize_sentence(data[0][0]))","78074b12":"import re, string\n\n# Stopwords are frequently-used words (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that do not hold any meaning useful to extract sentiment.\n# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words('english')\n\n# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance. \ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '\u00bd25':\n        return ''\n    return token\n\n# This function will be our all-in-one noise removal function\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Eliminating the token if it is a link\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        # Eliminating the token if it is a mention\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        cleaned_token = cleaned(token.lower())\n        \n        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens\n\n# Prevewing the remove_noise() output\nprint(remove_noise(data[0][0]))","758a0a8f":"start_time = time()\n\n# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n# we have to define a function that transforms our data into the required input structure\ndef list_to_dict(cleaned_tokens):\n    return dict([token, True] for token in cleaned_tokens)\n\ncleaned_tokens_list = []\n\n# Removing noise from all the data\nfor tokens, label in data:\n    cleaned_tokens_list.append((remove_noise(tokens), label))\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nfinal_data = []\n\n# Transforming the data to fit the input structure of the Naive Bayesian classifier\nfor tokens, label in cleaned_tokens_list:\n    final_data.append((list_to_dict(tokens), label))\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n# Previewing our final (tokenized, cleaned and lemmatized) data list\nfinal_data[:5]","00d07b3f":"from wordcloud import WordCloud, STOPWORDS\n\nstart_time = time()\n\npositive_words = []\nnegative_words = []\n\n# Separating out positive and negative words (i.e., words appearing in negative and positive tweets),\n# in order to visualize each set of words independently\nfor i in range(len(cleaned_tokens_list)):\n    if cleaned_tokens_list[i][1] == 1:\n        positive_words.extend(cleaned_tokens_list[i][0])\n    else:\n        negative_words.extend(cleaned_tokens_list[i][0])\n\n# Defining our word cloud drawing function\ndef wordcloud_draw(data, color = 'black'):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                          background_color = color,\n                          width = 2500,\n                          height = 2000\n                         ).generate(' '.join(data))\n    plt.figure(1, figsize = (13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nprint(\"Positive words\")\nwordcloud_draw(positive_words, 'white')\nprint(\"Negative words\")\nwordcloud_draw(negative_words)        \n\nprint('CPU Time:', time() - start_time)","c6f37caf":"# As our data is currently ordered by label, we have to shuffle it before splitting it\n# .Random(140) randomizes our data with seed = 140. This guarantees the same shuffling for every execution of our code\n# Feel free to alter this value or even omit it to have different outputs for each code execution\nrandom.Random(140).shuffle(final_data)\n\n# Here we decided to split our data as 90% train data and 10% test data\n# Once again, feel free to alter this number and test the model accuracy\ntrim_index = int(len(final_data) * 0.9)\n\ntrain_data = final_data[:trim_index]\ntest_data = final_data[trim_index:]","c82556b8":"start_time = time()\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\n# Output the model accuracy on the train and test data\nprint('Accuracy on train data:', classify.accuracy(classifier, train_data))\nprint('Accuracy on test data:', classify.accuracy(classifier, test_data))\n\n# Output the words that provide the most information about the sentiment of a tweet.\n# These are words that are heavily present in one sentiment group and very rarely present in the other group.\nprint(classifier.show_most_informative_features(20))\n\nprint('\\nCPU Time:', time() - start_time)","5013b276":"custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","ea01e181":"custom_tweet = \"I loved the show today! It was amazing.\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","37b7a017":"custom_tweet = \"No idea\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","00623571":"custom_tweet = \"Good\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","c76204b3":"custom_tweet = \"Not good\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","34198096":"custom_tweet = \"The reward for good work is more work!\"\n\ncustom_tokens = remove_noise(tk.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","588b6d85":"from sklearn import metrics\n\n# Storing all desired values in a list\ny = [test_data[i][1] for i in range(len(test_data))]\n# Storing all probabilities of having a positive sentiment in a list\nprobs = [classifier.prob_classify(test_data[i][0]).prob(1) for i in range(len(test_data))]\n\n# Making sure both values a re equal\nprint(len(y), len(probs))\n\n# performing the roc curve calculations\nfpr, tpr, thresholds = metrics.roc_curve(y, probs)\n\n# Printing the Area Under Curve (AUC) of the ROC curve (the closer to 1, the better)\nauc = metrics.roc_auc_score(y, probs)\nprint('AUC: %.3f' % auc)","1a5c0be7":"# Plotting the ROC Curve\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='NB')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Naive Bayesian ROC curve')\nplt.show()","e7f47b3e":"# Creating a confusion matrix (this specific confusion matrix function accepts numpy arrays rather than python lists)\n\nprobs = [classifier.classify(test_data[i][0]) for i in range(len(test_data))]\ny = np.asarray(y)\nprobs = np.asarray(probs)\npd.crosstab(y, probs, rownames=['Actual'], colnames=['Predicted'], margins=True)","51430796":"# Downloading and unzipping the glove word embeddings from the official website\n\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","c02090d6":"# Defining a handy function in order to load a given glove file\n\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r', encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","2f542acd":"# Loading the 50-dimensional GloVe embeddings\n# This method will return three dictionaries:\n# * word_to_index: a dictionary mapping from words to their indices in the vocabulary\n# * index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\n# * word_to_vec_map: dictionary mapping words to their GloVe vector representation\n# Note that there are 400,001 words, with the valid indices ranging from 0 to 400,000\n\nword_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')","141ecf4e":"word_to_index['hello']","87d45eaf":"word_to_vec_map['hello']","cdaad322":"word_to_index['unk']","8dd9fe36":"def cosine_similarity(u, v):\n    dot = np.dot(u, v)\n    norm_u = np.sqrt(np.sum(u**2))\n    norm_v = np.sqrt(np.sum(v**2))\n    cosine_similarity = dot \/ (norm_u * norm_v)\n    return cosine_similarity","6319d043":"cosine_similarity(word_to_vec_map['cucumber'], word_to_vec_map['tomato'])","9ed993c1":"cosine_similarity(word_to_vec_map['cucumber'], word_to_vec_map['phone'])","8d9ab653":"start_time = time()\n\nunks = []\nUNKS = []\n\n# This function will act as a \"last resort\" in order to try and find the word\n# in the words embedding layer. It will basically eliminate contiguously occuring\n# instances of a similar character\ndef cleared(word):\n    res = \"\"\n    prev = None\n    for char in word:\n        if char == prev: continue\n        prev = char\n        res += char\n    return res\n\n\ndef sentence_to_indices(sentence_words, word_to_index, max_len, i):\n    global X, Y\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            UNKS.append(w)\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n                unks.append(w)\n        X[i, j] = index\n\n        \n# Here we will utilize the already computed 'cleaned_tokens_list' variable\n   \nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nlist_len = [len(i) for i, j in cleaned_tokens_list]\nmax_len = max(list_len)\nprint('max_len:', max_len)\n\nX = np.zeros((len(cleaned_tokens_list), max_len))\nY = np.zeros((len(cleaned_tokens_list), ))\n\nfor i, tk_lb in enumerate(cleaned_tokens_list):\n    tokens, label = tk_lb\n    sentence_to_indices(tokens, word_to_index, max_len, i)\n    Y[i] = label\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n\nprint(X[:5])\nprint(Y[:5])","48bc1665":"import keras\nfrom keras import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split","cabf1ce7":"# Defining a function that will initialize and populate our embedding layer\n\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n    vocab_len = len(word_to_index) + 1\n    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n    \n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    for word, idx in word_to_index.items():\n        emb_matrix[idx, :] = word_to_vec_map[word]\n        \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","7c6226f8":"# Defining a sequencial model composed of firstly the embedding layer, than a pair of Bidirectional LSTMs,\n# that finally feed into a sigmoid layer that generates our desired output betwene 0 and 1.\n\nmodel = Sequential()\n\nmodel.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=False)))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmodel.summary()","2ce81d81":"# Compiling our model with a binary cross-entropy loss function, using the default adam optimizer\n# and setting the accurary as the metric to track and ameliorate\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b6380057":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)","77a7c98d":"len(X_train)","21756ba5":"len(X_test)","9078d316":"# Setting a batch size of 20 and training our model for 20 epochs\n\nmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","051e04da":"# Defnining a handy function in order to plot various models accuracy and loss progress\n\ndef plot_acc_loss(history):\n\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","2a4ced08":"# Plotting the obtained training and validation loss and accuracy progressions.\nplot_acc_loss(model.history)","296bf352":"keras.backend.clear_session()\n\nmodel_dropout = Sequential()\n\nmodel_dropout.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\nmodel_dropout.add(Dropout(rate=0.4))\nmodel_dropout.add(Bidirectional(LSTM(units=128, return_sequences=True)))\nmodel_dropout.add(Dropout(rate=0.4))\nmodel_dropout.add(Bidirectional(LSTM(units=128, return_sequences=False)))\nmodel_dropout.add(Dense(units=1, activation='sigmoid'))\n\nmodel_dropout.summary()","051916a2":"model_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","17c70982":"model_dropout.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","0a8b0e3b":"plot_acc_loss(model_dropout.history)","826098df":"model_dropout.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","2efcc4d0":"plot_acc_loss(model_dropout.history)","fba74389":"unk = word_to_index['unk']\n\nn_unk_words = 0\n\nfor x in X:\n    for y in x:\n        if y == unk:\n            n_unk_words += 1\n\nn_unk_words","3cdbfdae":"len(unks)","c10d3d49":"len(UNKS)","b50d8a08":"from collections import Counter\nCounter(unks).most_common(50)","21592d78":"# A custom function defined in order to fine-tune the cleaning of the input text.\n# This function is being \"upgraded\" such that it performs a more thourough cleaning of the data\n# in order to better fit our words embedding layer\ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow' or token == '2moro':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token in ['hahah', 'hahaha', 'hahahaha']:\n        return 'haha'\n    if token == \"mother's\":\n        return \"mother\"\n    if token == \"mom's\":\n        return \"mom\"\n    if token == \"dad's\":\n        return \"dad\"\n    if token == 'bday' or token == 'b-day':\n        return 'birthday'\n    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\n                 \"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n        return token.replace(\"'\", \"\")\n    if token in ['lmao', 'lolz', 'rofl']:\n        return 'lol'\n    if token == '<3':\n        return 'love'\n    if token == 'thanx' or token == 'thnx':\n        return 'thanks'\n    if token == 'goood':\n        return 'good'\n    if token in ['amp', 'quot', 'lt', 'gt', '\u00bd25', '..', '. .', '. . .']:\n        return ''\n    return token\n\n\n# This function will be our all-in-one noise removal function\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token in tweet_tokens:\n        # Eliminating the token if it is a link\n        token = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', \" \", token)\n        # Eliminating the token if it is a mention\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n        \n        cleaned_token = cleaned(token.lower())\n        \n        if cleaned_token == \"idk\":\n            cleaned_tokens.append('i')\n            cleaned_tokens.append('dont')\n            cleaned_tokens.append('know')\n            continue\n        if cleaned_token == \"i'll\":\n            cleaned_tokens.append('i')\n            cleaned_tokens.append('will')\n            continue\n        if cleaned_token == \"you'll\":\n            cleaned_tokens.append('you')\n            cleaned_tokens.append('will')\n            continue\n        if cleaned_token == \"we'll\":\n            cleaned_tokens.append('we')\n            cleaned_tokens.append('will')\n            continue\n        if cleaned_token == \"it'll\":\n            cleaned_tokens.append('it')\n            cleaned_tokens.append('will')\n            continue\n        if cleaned_token == \"it's\":\n            cleaned_tokens.append('it')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"i've\":\n            cleaned_tokens.append('i')\n            cleaned_tokens.append('have')\n            continue\n        if cleaned_token == \"you've\":\n            cleaned_tokens.append('you')\n            cleaned_tokens.append('have')\n            continue\n        if cleaned_token == \"we've\":\n            cleaned_tokens.append('we')\n            cleaned_tokens.append('have')\n            continue\n        if cleaned_token == \"they've\":\n            cleaned_tokens.append('they')\n            cleaned_tokens.append('have')\n            continue\n        if cleaned_token == \"you're\":\n            cleaned_tokens.append('you')\n            cleaned_tokens.append('are')\n            continue\n        if cleaned_token == \"we're\":\n            cleaned_tokens.append('we')\n            cleaned_tokens.append('are')\n            continue\n        if cleaned_token == \"they're\":\n            cleaned_tokens.append('they')\n            cleaned_tokens.append('are')\n            continue\n        if cleaned_token == \"let's\":\n            cleaned_tokens.append('let')\n            cleaned_tokens.append('us')\n            continue\n        if cleaned_token == \"she's\":\n            cleaned_tokens.append('she')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"he's\":\n            cleaned_tokens.append('he')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"that's\":\n            cleaned_tokens.append('that')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"i'd\":\n            cleaned_tokens.append('i')\n            cleaned_tokens.append('would')\n            continue\n        if cleaned_token == \"you'd\":\n            cleaned_tokens.append('you')\n            cleaned_tokens.append('would')\n            continue\n        if cleaned_token == \"there's\":\n            cleaned_tokens.append('there')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"what's\":\n            cleaned_tokens.append('what')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"how's\":\n            cleaned_tokens.append('how')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"who's\":\n            cleaned_tokens.append('who')\n            cleaned_tokens.append('is')\n            continue\n        if cleaned_token == \"y'all\" or cleaned_token == \"ya'll\":\n            cleaned_tokens.append('you')\n            cleaned_tokens.append('all')\n            continue\n\n        if cleaned_token.strip() and cleaned_token not in string.punctuation: \n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens\n\n\n# Prevewing the remove_noise() output\nprint(remove_noise(data[0][0]))","cf338a23":"start_time = time()\n\nunks = []\nUNKS = []\n\ndef cleared(word):\n    res = \"\"\n    prev = None\n    for char in word:\n        if char == prev: continue\n        prev = char\n        res += char\n    return res\n\ndef sentence_to_indices(sentence_words, word_to_index, max_len, i):\n    global X, Y\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            UNKS.append(w)\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n                unks.append(w)\n        X[i, j] = index\n\ncleaned_tokens_list = []\n\n# Removing noise from all the data, using the newly defined function\nfor tokens, label in data:\n    x = remove_noise(tokens)\n    if x:\n        cleaned_tokens_list.append((x, label))\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nlist_len = [len(i) for i, j in cleaned_tokens_list]\nmax_len = max(list_len)\nprint('max_len:', max_len)\n\n\nX = np.zeros((len(cleaned_tokens_list), max_len))\nY = np.zeros((len(cleaned_tokens_list), ))\n\n\nfor i, tk_lb in enumerate(cleaned_tokens_list):\n    tokens, label = tk_lb\n    sentence_to_indices(tokens, word_to_index, max_len, i)\n    Y[i] = label\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n\nprint(X[:5])\nprint(Y[:5])","4d29c520":"unk = word_to_index['unk']\n\nn_unk_words = 0\n\nfor x in X:\n    for y in x:\n        if y == unk:\n            n_unk_words += 1\n\nn_unk_words","60b4c978":"from collections import Counter\nCounter(unks).most_common(50)","e49289ff":"keras.backend.clear_session()\n\nmodel_clean_data = Sequential()\n\nmodel_clean_data.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\nmodel_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=True)))\nmodel_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=False)))\nmodel_clean_data.add(Dense(units=1, activation='sigmoid'))\n\nmodel_clean_data.summary()","58f815ce":"model_clean_data.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","42ad953d":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)","e2d8ea79":"len(X_train)","d35643e1":"len(X_test)","69214df1":"model_clean_data.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","5a864d84":"history = model_clean_data.history\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\ny_arrow = max(val_acc)\nx_arrow = val_acc.index(y_arrow) + 1\nplt.annotate(str(y_arrow)[:6],\n             (x_arrow, y_arrow),\n             xytext=(x_arrow + 5, y_arrow + .02),\n             arrowprops=dict(facecolor='orange', shrink=0.05))\nplt.xticks(epochs)\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.xticks(epochs)\nplt.show()","52b696f9":"def sentence_to_indices(sentence_words, max_len):\n    X = np.zeros((max_len))\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n        X[j] = index\n    return X\n\ndef predict_custom_tweet_sentiment(custom_tweet):\n    # Convert the tweet such that it can be fed to the model\n    x_input = sentence_to_indices(remove_noise(tk.tokenize(custom_tweet)), max_len)\n    \n    # Retrun the model's prediction\n    return model_clean_data.predict(np.array([x_input])).item()","97c64385":"predict_custom_tweet_sentiment(\"I'm happy you're here!\")","d4de70c9":"predict_custom_tweet_sentiment(\"I'm not happy you're here!\")","aedcb9b2":"predict_custom_tweet_sentiment(\"I disliked his attitude...\")","69308ceb":"predict_custom_tweet_sentiment(\"I'm infatuated with you\")","89856b73":"(negative_words + positive_words).count('love')","1b378967":"(negative_words + positive_words).count('infatuated')","3b5d100f":"def i_to_sentence(I):\n    sentence = \"\"\n    for i in I:\n        if i:\n            sentence += index_to_word[int(i)] + \" \"\n        else:\n            break\n    return sentence","7650c22a":"C = 0\n\npred = model_clean_data.predict(X_test)\n\nfor i in range(len(X_test)):\n    final_pred = 1 if pred[i] > 0.5 else 0\n    \n    if(final_pred != Y_test[i]):\n        print('Expected sentiment: ' + str(int(Y_test[i])) + '. Input: ' + i_to_sentence(X_test[i]))\n        C += 1\n        \n    if C > 100:\n        break","972b4849":"# Taking only the label and the date in order to work on them\ndf_date = df_raw.copy()[['label', 'date']]\ndf_date.head()","78d532dd":"# Adding a field to our dataframe, hour, containing the hour in which the tweet was published\ndf_date['hour'] = df_date.date.apply(lambda x: x[11:13]).astype('int32')\ndf_date.head()","20bf776c":"# Adding a field to our dataframe, dow, containing the day of week in which the tweet was published\ndf_date['dow'] = df_date.date.apply(lambda x: x[:3]).astype('str')\ndf_date.head()","70d8fc39":"# Building a pivot table that breaks-down the number of positive and negative tweets in each hour of the day\ntemporal_hour = pd.pivot_table(df_date, index='label', columns='hour', aggfunc='size', fill_value=0)\ntemporal_hour","ea9eadf3":"# Plotting our results\ntemporal_hour.transpose().plot()","241ff1fa":"# Transforming the above pilot table such that it now contains the percentage\n# of negative\/positive tweets in each hour of the day\ntemporal_hour = temporal_hour.iloc[:, :].apply(lambda x: x \/ x.sum())\ntemporal_hour","f5866de8":"# Plotting our results\ntemporal_hour.transpose().plot()","00fe4da1":"# Building a pivot table that breaks-down the number of positive and negative tweets in each day of the week\ntemporal_dow = pd.pivot_table(df_date, index='label', columns='dow', aggfunc='size', fill_value=0)\ntemporal_dow","3e355535":"# Re-ordering our table\ntemporal_dow = temporal_dow[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']]\ntemporal_dow","85013926":"# Plotting our results\ntemporal_dow.transpose().plot()","3b4a9fb0":"# Transforming the above pilot table such that it now contains the percentage\n# of negative\/positive tweets in each day of the week\ntemporal_dow = temporal_dow.iloc[:, :].apply(lambda x: x \/ x.sum())\ntemporal_dow","79d46ef1":"temporal_dow.transpose().plot()","d0337914":"import pickle","003260a9":"# Saving a python object to disk\npickle.dump(classifier, open(\"classifier.p\", \"wb\"))","11dbcde8":"# Loading a python object from disk\nclassifier = pickle.load(open(\"classifier.p\", \"rb\"))","bc5fb747":"Now that our data is *somewhat* clean, we can use it to build our classification model. One of the most commonly used classification models in Natural Language Processing (NLP) is the Naive Bayesian.<br>\n**Naive Bayesian** classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but rather a family of algorithms where all of them make the following *naive* assumptions:\n* All features are independent from each other.\n* Every feature contributes equally to the output.\n\nIn our case, these two assumptions can be interpreted as:\n* Each word is independent from the other words, no relation between any two words of a given sentence.\n* Each word contributes equally, throughout all sentences, to the decision of our model, regardless of its relative position in the sentence.\n\n<u>Example:<\/u> \"This is bad\" \/ \"This is very bad\" or \"Such a kind person\" \/ \"This kind of chocolate is disgusting\", in both cases the Naive Bayesian classifier would give the same importance for the words 'bad' and 'kind', albeit them having a stronger meaning and a different meaning respectively in first and second sentences.\n\nNevertheless, Naive Bayesian are widely used in NLP and they often output great results.<br>\n\nThe **Bayes' Theorem** describes the probability of an event $A$, based on prior knowledge of conditions $B$ that might be related to the event:\n$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$.<br>\nIn our case, this can be intuitively interpreted as the probability of a tweet being positive, based on prior knowledge of the words inside the input text.\nIn a nutshell, this probability is: the probability of the first word occuring in a positive tweet, times, the probability of the second word occuring in a positive tweet, ..., times, the probability of a tweet being positive. This can be mathematically written as: $P(A \\mid B) \\propto P(B_1 \\mid A)\\times P(B_2 \\mid A)  \\times  \\cdot \\cdot \\cdot  \\times  P(B_n \\mid A)\\times P(A)$.\n\nMore details can be found at: https:\/\/www.geeksforgeeks.org\/naive-bayes-classifiers\/","a53bcf4b":"Further optimizing the number of unknown words at this point would not be the best use of our time. As now we are left with a big number of minimally occuring words in the data. Thus, spending more time cleaning the data would result in a very little return on investment.\n\nNow, let's try to train our model on the newly generated *cleaner* data!","735f5e47":"Wow, a 75.5% accuracy on the test set training a very *Naive* (\ud83d\ude09) algorithm and in just 36 seconds!\n\nTaking a look at the 20 most informative features of the model, we can notice the high volume of negative to positive (0:1) informative features. This is very interesting as it means that negative tweets have a much more concentrated and limited vocabulary when compared to positive tweets.\n\nI personally interpret this as follows: *Whenever people are in a bad mood, they are confined in such a limited space of words and creativity, in contrast with when they are in a happy mood*.\n\n(Kudos for movies \ud83c\udfac for keeping our moods up \ud83d\ude0a)","245b2ab9":"Also, the model isn't robust against sarcastic sentences.","d0ca9f11":"## 5.2. Data Transformation <a class=\"anchor\" id=\"head-5-2\"><\/a>","4240ac0e":"# 5. Deep Learning Model - LSTM <a class=\"anchor\" id=\"head-5\"><\/a>","8f370715":"### 5.5.2. Inspecting the Data - Unknown Words <a class=\"anchor\" id=\"head-5-5-2\"><\/a>","b6cfc05b":"**Deep Learning** is a very rapidly growing field, that is proving to be extremely beneficial in various scenarios. One of those scenarios, which we will be studying in this notebook, is the ability to process text data in a much more complex and powerful manner. In fact, in the next section of the notebook we will be focusing on implementing a Deep Learning model that will successfully tackle and solve the above mentioned shortcomings of the Naive Bayes model, such as the lack of relationship between words in a sentence and the poor generalization on previously unseen data.\n\n---","3bb6efb0":"## 2.3. Cleaning the Data <a class=\"anchor\" id=\"head-2-3\"><\/a>","ca27eb23":"**Regularization** is the process of preventing a model from over-fitting the training data. You can conceptualize regularization as being a tool we use in order to render our model less sensible to every detail, and possibly outliers, in the training data. This should allow the model to better generalize and have a better performance on the validation data, or any data it wasn't trained on.\n\n**Dropout** is one of the many regularization techniques, and also one of the simplest to implement and most commonly used. Basically, what dropout does is that it randomly eliminates several (based on a parametrized percentage rate) neurons connections in the network, rendering the model less complex, and forcing the model to only look at part of a given example. The random elimination of connections in the model is repeated randomly for each example training data.\n\n\n![dropout.png](attachment:dropout.png)\n<center><i>Srivastava, Nitish, et al. \u201dDropout: a simple way to prevent neural networks from overfitting\u201d, JMLR 2014<\/i><\/center>\n\n<br\/>\n\nFor example, let's consider the following sentences, with a dropout layer with a rate of 0.5 (50% of connections will be eliminated): \n\n> \"Another kind of regularization can be directly applied to the cost function\"\n>\n> \"This is my first ever notebook. Hope you're enjoying it so far!\"\n\nThe output of the dropout layer could look like the following:\n\n> \"kind of regularization be to function\"\n>\n> \"This my notebook. you enjoying it far!\"\n\nThus, the model will only have information on a part of the input example, and should be able to escape over-fitting particular characteristics of the training data.","d8b5df26":"Let's take a look at some of the wrongly classified data from the model.","95f5e744":"Great, let's take a look at the progress accomplished:","26cde1e3":"Further in our training we would like to speed the process up by splitting data into *mini-batches*. Batch learning is basically the process of training on several examples at the same time, which greatly decreases the training time!\n\nHowever, and in order to be able to utilize batch learning, keras (and similarly to most machine learning frameworks) requires all data within the same batch to have the same length or *dimension*. Whereas in our text data, each example could have a variable sentence length. In order to overcome this issue, we will go over all of our data, and calculate the length of the longest phrase (in terms of words). Then, we will 0-pad all of the data sequences so that they will all have the same *max_len* calculated.\n\nLet's consider a *max_len* of 5 words, and the two sentences *I love you* and *I will be ready*.\nFirst, we will convert these sentences to their corresponding index representation, then 0-pad them for the *max_len* 5. After we've done that, we can now feed the resulting lists into a word embedding layer in order to get the representational vectors for each index (representing a given word).\n\n![embedding_small.png](attachment:embedding_small.png)\n\n----------------\n\nThe image above, as well as various helper functions written in this section are inspired from Coursera's [Sequence Models Course](https:\/\/www.coursera.org\/learn\/nlp-sequence-models) in the [Deep Learning Specialization](https:\/\/www.coursera.org\/specializations\/deep-learning) by [deeplearning.ai](deeplearning.ai).","ea07e13b":"## 5.3. Building the Model <a class=\"anchor\" id=\"head-5-3\"><\/a>","bc9b0800":"Wow, now that's a considerable performance gain! The model managed to reach an impressive **81.1%** validation accuracy on the 5th epoch!\n\nThis goes to prove the following: **The model is as good as the data**.\n\nNonetheless, the over-fitting problem is still persistent in the model. This could be further reduced by introducing a more aggressive regularization and training the model for a much bigger number of epochs, and by also training the model on a bigger, more diverse, cleaner data.","616dcba7":"First, we need to split our data into two sets: Training and Testing sets.<br>\n* **Train Data** is data used in order to build and train our classification model.\n* **Test Data** is data, that our classifier model has never seen before, used in order to assert the accuracy and test our classification model.","fd719cc7":"## 5.7. Inspecting Wrongly Predicted Data <a class=\"anchor\" id=\"head-5-7\"><\/a>","ea8bbba6":"If you remember in the above defined function `sentence_to_indices`, we have incorporated a \"last resort\" function `cleared`, which eliminates contiguous similar characters. It seems like our last resort of clearing the words helped us reduce the number of unknown words by 28,000 !","2469910a":"Woohoo, our model can now easily understand the relationship between words! It managed to identify that adding a *not* before a *happy*, would completely **switch** its meaning!","77f61723":"As this word embedding only considers 400,000 unique words, it might encounter words it has never seen before. For example, the word \"LSTM\" is most likely not included in those 400,000 words.\n\nIn order to overcome such scenarios, word embeddings reserve an extra spot for \"unknown\" words, also denoted with the keyword **unk**.","ec6c37cc":"## 5.1. Data Pre-processing <a class=\"anchor\" id=\"head-5-1\"><\/a>","c472cede":"This almost looks like it's oscillating!","9c0a4393":"### Further data cleaning","83da0dc3":"Finally, let's further assert our model by plotting the AUC and the confusion matrix of the model.","9faa8082":"# 8. Further Work <a class=\"anchor\" id=\"head-8\"><\/a>","c28a94b5":"Several directions could be undertaken at this stage in order to improve our model's performance. Arguably, the most promising direction to firstly look into is to introduce some kind of regularization in our model in order to try to reduce the clearly apparent over-fitting problem our model is facing. Let's specifically  look at the *dropout* regularization technique.","13670ff8":"Word embeddings are basically a way for us to convert words to *representational vectors*. What I mean by this is that, instead of mapping each word to an index, we want to map each word to a vector of real numbers, representing this word.\n\nThe goal here is to be able to generate similar or close representational vectors for words that have similar meaning. For example, when feeding the words \"excited\" and \"thrilled\" to the word embedding model, we would like the model to output \"close\" representations for both words. Whereas if we feed the words \"excited\" and \"Java\", we would like the model to output \"far\" representations for both words.\n\n> The concept of \"close\" and \"far\" vectors is actually implemented using the [cosine similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity). In fact, word embeddings and distance between words or relation between words is an immense discussion in its own. So I'll just keep my explanation to a minimum in this notebook.\n\n----------\n\n[This](https:\/\/papers.nips.cc\/paper\/2016\/file\/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) is a very interesting paper to read for advanced readers on the subject of word embeddings.","06bd2bad":"In order to feed our text data to our LSTM model, we'll have to go through several extra preprocessing steps.\n\nMost neural networks expect *numbers* as inputs. Thus, we'll have to convert our *text* data to *numerical* data.\n\nOne way of doing so would be the following: collect all possible words in our dataset and generate a *dictionary* containing all unique words in our text corpus, then sort all of these words alphabetically and assign to each word an *index*. So for example, let's say our dictionary's length turned out to be 100,000 words. The word \"a\" would be assigned the index 0, the word \"aaron\" would be assigned the index 1, and so on, until we reach the last word in our dictionary, say \"zulu\", and assign to it the index 99,999. Great! Now each word is represented with a numerical value, and we can feed the numerical value of each word to our model.\n\nIt turns out that this step alone is not enough to be able to train good Deep Learning models. If you think about it, when the model reads an input 20,560 and then another input 20,561 for example, it would assume that these values are \"close\". However, those inputs could be the indexes of totally unrelated words, such as \"cocktail\" and \"code\", appearing right next to each  other in the sorted dictionary. Hoping I've convinced you with this example, and that you hopefully believe that \"cocktail\" and \"code\" are, and should always be, completely unrelated, let's take a look at one solution that is widely adopted in various NLP implementations.\n\n----------\n\n*Also, one simple solution for this problem is to use [One-Hot](https:\/\/en.wikipedia.org\/wiki\/One-hot#:~:text=In%20natural%20language%20processing%2C%20a,uniquely%20to%20identify%20the%20word.) vectors to represent each word, but we won't bother with One-Hot vectors in this notebook, as we will be discussing a much more robust solution.*","0357a598":"Now that our classifier is built, we can have fun and test it with custom tweets!","19bcdb63":"# 2. Cleaning and Processing the Data <a class=\"anchor\" id=\"head-2\"><\/a>","4cfbf08f":"Our model is even capable of correctly predicting the sentiment of words **it never encountered** during training!\n\nThe word *Infatuated* never appeared neither in the positive nor negative tweets. However, given that it\u2019s strongly related to the word *love* (a very frequently appearing word in our data), and thanks to the use of **word embeddings**, the model was capable of assigning a positive sentiment to this word!","e9a4ab0e":"We can observe that the accuracy has plateaued, reaching its best validation value of **77.2%**.","0eefa065":"It's always cool to have real world data reflect what one would guess as: *yeah, it should look like this*.\n\nHere also, two insights can be drawn from the graph:\n\n* People tend to tweet more positively during the weekends, when they relax and attend events, whereas negative tweets are dominant during the rest of the week, showcasing people's dissatisfaction with their work.\n* A much bigger number of tweets is circulated during the weekend, relative to the rest of the week.\n\nThis actually slightly reinforces our prior hypotheses that *positive tweets come in big batches, maybe relating to a similar subject*, as such events usually occur on the weekends.\n\nAlso, keep in mind that this graph could also be shifted either to the left or to the right, as the tweets appearing to have a date *Monday 1AM* for example could end up being on *Sunday 8PM*, or vise versa.","f0bc054e":"*\u201cPickling\u201d is the process whereby a Python object hierarchy is converted into a byte stream, and \u201cunpickling\u201d is the inverse operation, whereby a byte stream is converted back into an object hierarchy.*<br>\nThis means that we can save on our drive any object in python! In our case, the \"*final_data*\" list took 11 minutes to compute! So, if were to come back to our code and decide to add a visualization or extra code, and instead of re-processing this list, we can *pickle* it on our drive and later load it back from our drive in less than a second!","e1ac8c05":"In order to feed our text data to a classification model, we first need to *tokenize* it.  \n**Tokenization** is the process of splitting up a single string of text into a list of individual words, or *tokens*.\n<br>\n\nPython has a built in string method: *string.split()*, that splits up any given string into a list based on a splitting character (if not specified, will default to *white space*).\n<br>\n\nIn this example, we will use the **TweetTokenizer**; a Twitter-aware tokenizer provided by the *nltk* library. In addition to a standard tokenizer, this tokenizer will split the input text based on various criterions that are well suited for the tweets use case.\n\nMore info can be found at: https:\/\/www.nltk.org\/api\/nltk.tokenize.html#module-nltk.tokenize.casual.","dfb40813":"# Introduction","f6454ebd":"As someone who's extremely passionate about data, I couldn't help but try and explore further correlations in the data. And I'm excited to share with you a couple of my findings!\n\nEven though this project is specifically focused on NLP, the following is completely unrelated to NLP.","958d7810":"As explained earlier, whenever a word is not included in the words embedding mapping, it is referred to as an unknown word, or *unk*. Let's count the number of words that are being flagged as unknowns in our data. ","c97509a2":"### Model Training - Cleaner Data","6e6aeb28":"### 5.1.1. Word Embeddings <a class=\"anchor\" id=\"head-5-1-1\"><\/a>","cbd8741f":"# Twitter Sentiment Analysis - Classical Approach VS Deep Learning","b05a9564":"Below is a short example of the usage of the cosine similarity in order to find correlations in the data.\n\nWe can observe that the words *cucumber* and *tomato* are highly positively similar. Whereas the words *cucumber* and *phone* are not related at all, having a close to 0 similarity measure.\n\n> Note that the cosine similarity return values between -1 and +1, where **-1** denote completely opposite meanings, **+1** denote completely identical meanings and **0** denote totally unrelated meanings.","e52a8991":"In the next cell we will write some code in order to perform the above-described process, with the exception of passing the lists to the embedding layer, as this step will be handled by the model.\n\nNote that the first step in this process is similar to the process previously performed for the Naive Bayes model.","3ed96538":"GloVe embeddings come in various flavors. They basically differ depending on the type of data they were trained on, the length of the vocabulary, the size of the representational vectors and so on.\n\nWe've previously downloaded GloVe vectors trained on 6 Billion tokens and having a dictionary, or *vocabulary*, size of 400,001 unique words. Believe it or not, this is the smallest model from GloVe! Next we will load the vectors that are 50-dimensional.","c1aab896":"Thus, we can conclude that the regularization process did not really help us in our case. A tiny **0.5%** improvement was observed after adding dropout to the model.\n\nThis leads us to the second direction to investigate in order to improve our model: **data**.","82a66d6e":"That's it for the small detour! It's really fun to try and squeeze out every ounce of information from the data! And who knows, maybe we can combine this information with our current model, as to give more weights to a certain class depending on the date of the tweet!","8dfd7ecd":"Skimming through the above output, we can \"understand\" why the model wouldn't properly classify several examples. Some examples seem wrongly labeled in the first place, whereas some other examples are really hard to classify without further context.\n\nSo all in all, I would say that our current model is relatively robust in classifying the sentiment in a given sentence!","2f772f7f":"A **L**ong **S**hort-**T**erm **M**emory, or **LSTM**, is a type of machine learning neural networks. More specifically, it belongs to the family of **R**ecurrent **N**eural **N**etwords (**RNN**) in Deep Learning, which are specifically conceived in order to process *temporal data*. Temporal data is defined as data that is highly influenced by the order that it is presented in. This means that data coming before or after a given datum (singular for *data*) can greatly affect this datum. Text data is an example of temporal data. For example, let's consider the following sentence:\n\n>*Jane is not very happy. She's still mad at you!*\n\nIn the above sentence, the word *not* greatly influences the meaning of the upcoming words *very happy*. Also, we used the word *she* as we are speaking about a female subject.\n\nAlso, here's a fun example conveying the influence of words' positions directly influencing a sentence's meaning:\n\n>*Are you as clever as I am?*\n>\n>*Am I as clever as you are?*\n\n------------\n\nLSTM is an advanced and complex deep learning architecture, so we will avoid explaining it in detail in this notebook as it will result in a huge notebook! (Maybe it's a project for the future? \ud83d\ude09)\n\nThat being said, you don't really need to know the ins and outs of LSTM in order to walk through the rest of this notebook, so don't worry about it for the moment!","3a4c4567":"\\*\\* *Feel free to skip this step if you're working with a powerful machine, or have lots (and I mean lots) of time to spare* \ud83d\ude05 \\*\\*  \nAs our data is huge (1,600,000 rows), working with it on a regular machine is very challenging. For this reason, we will trim our dataframe to $\\frac{1}{4} $th of its original size.  \nAs data output balance is key for a better performing algorithm, we will make sure to maintain the data balance while trimming the dataframe.","90f17730":"### Model Building - Dropout","5e07ec90":"Now it's time to train our model on our training data.\n<br>\nIn this notebook, we will utilize *nltk*'s **NaiveBayesClassifier**.","bbb8cbae":"Building and training good word embeddings is a tremendous process requiring millions of data samples and exceptional computational power. Luckily for us, folks at the University of Stanford already did this for us and published their results for free on their official [website](https:\/\/nlp.stanford.edu\/projects\/glove\/)! Their model is called **GloVe**, and it's going to be what we'll use in the next steps","35958602":"# 6. Bonus Section <a class=\"anchor\" id=\"head-6\"><\/a>","03aaafd7":"The training accuracy is sky-rocketing, exceeding **95%** after 20 epochs! However, the validation accuracy increased slightly in the early epochs, reaching **76.7%** on the 5th epoch, after which it experienced a consistently gradual decrease. In data science, we would classify this model as having very high variance and low bias. This is also referred to as \"over-fitting\".\n\n**Over-Fitting** is basically the phenomenon where the model's performance on validation data starts degrading, while still achieving great progress on the test set. In other words, the model is doing exceptionally well on learning specific examples it has been trained on, but is failing to generalize to data it never saw in its training phase. ","cbce14a1":"# 4. Naive Bayesian Model <a class=\"anchor\" id=\"head-4\"><\/a>","35fa9a32":"![love_scrable.jpg](attachment:love_scrable.jpg)\n\n<span>Photo by <a href=\"https:\/\/unsplash.com\/@gaellemarcel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Gaelle Marcel<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/computer-text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>.","d17c705f":"## 5.6. Predicting on Custom Data <a class=\"anchor\" id=\"head-5-6\"><\/a>","391062d1":"We can observe that most of the unknown words are very common words, however the word embeddings layer does not support quotes. For example, if you try to get the index of the word \"i'm\", you'll be prompted with a key error, whereas the word \"im\" is available as a valid key. Other words, such as \"idk\", \"lmao\" or \"b-day\" reminds us that our data comes from twitter, and we'll have to individually handle each of these words such that our words embedding layer recognise them.","726fb6e3":"## 4.2. Training the Model <a class=\"anchor\" id=\"head-4-2\"><\/a>","fe3eb0a5":"This project's aim, is to explore the world of *Natural Language Processing* (NLP) by building what is known as a **Sentiment Analysis Model**. A sentiment analysis model is a model that analyses a given piece of text and predicts whether this piece of text expresses positive or negative sentiment.\n\n![sentiment_classification.png](attachment:sentiment_classification.png)\n\nTo this end, we will be using the `sentiment140` dataset containing data collected from twitter. An impressive feature of this dataset is that it is *perfectly* balanced (i.e., the number of examples in each class is equal).\n\nCiting the [creators](http:\/\/help.sentiment140.com\/for-students\/) of this dataset:\n\n> *Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search*\n\nAfter a series of **cleaning and data processing**, and after visualizing our data in a **word cloud**, we will be building a **Naive Bayezian** model. This model's goal would be to properly classify positive and negative tweets in terms of sentiment.\nNext, we will propose a much more advanced solution using a **deep learning** model: **LSTM**. This process will require a different kind of data cleaning and processing. Also, we will discover **Word Embeddings**, **Dropout** and many other machine learning related concepts.\n\nThroughout this notebook, we will take advantage of every result, visualization and failure in order to try and further understand the data, extract insights and information from it and learn how to improve our model. From the type of words used in positive\/negative sentiment tweets, to the vocabulary diversity in each case and the day of the week in which these tweets occur, to the overfitting concept and grasping the huge importance of the data while building a given model, I really hope that you'll enjoy going through this notebook and gain not only technical skills but also analytical skills from it.\n\n---\n\nThis notebook is written by **Joseph Assaker**. Feel free to reach out for any feedback on this notebook via [email](mailto:lb.josephassaker@gmail.com) or [LinkedIn](https:\/\/www.linkedin.com\/in\/joseph-assaker\/).\n\n---\n\nNow, let's start with the fun \ud83c\udf89\n\n### **Table of Content:**\n\n 1. [Importing and Discovering the Dataset](#head-1)  \n 2. [Cleaning and Processing the Data](#head-2)  \n  2.1. [Tokenization](#head-2-1)  \n  2.2. [Lemmatization](#head-2-2)  \n  2.3. [Cleaning the Data](#head-2-3)  \n 3. [Visualizing the Data](#head-3)\n 4. [Naive Bayesian Model](#head-4)  \n  4.1. [Splitting the Data](#head-4-1)  \n  4.2. [Training the Model](#head-4-2)  \n  4.3. [Testing the Model](#head-4-3)  \n  4.4. [Asserting the Model](#head-4-4)    \n 5. [Deep Learning Model - LSTM](#head-5)  \n  5.1. [Data Pre-processing](#head-5-1)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.1.1. [Word Embeddings](#head-5-1-1)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.1.2. [Global Vectors for Word Representation (GloVe)](#head-5-1-2)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.1.3. [Data Padding](#head-5-1-3)  \n  5.2. [Data Transformation](#head-5-2)  \n  5.3. [Building the Model](#head-5-3)  \n  5.4. [Training the Model](#head-5-4)  \n  5.5. [Investigating Possibilties to Improve the Model](#head-5-5)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.5.1. [Regularization - Dropout](#head-5-5-1)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.5.2. [Inspecting the Data - Unknown Words](#head-5-5-2)  \n  5.6. [Predicting on Custom Data](#head-5-6)  \n  5.7. [Inspecting Wrongly Predicted Data](#head-5-7)  \n 6. [Bonus Section](#head-6)\n 7. [Extra Tip: Pickling !](#head-7)\n 8. [Further Work](#head-8)","59533727":"* Further data cleaning and relabelling. As the data origin is from twitter, it is expected to contain a wide range of not \"official\" english words, so data cleaning is crucial in such a scenario. Furthermore, as the data labelling has been done automatically based on the reactions of the tweet, this labelling is by no means perfect and a human re-labelling of the whole data would certainly be beneficial.\n* Introduce a neutral class, transforming the problem to a multi-class classification problem.\n* Try out several other word embeddings or model architectures.\n* Augment the data by diversifying it in order to make the model more robust, especially against sarcasm.","d875e685":"## 2.1. Tokenization <a class=\"anchor\" id=\"head-2-1\"><\/a>","a33c3249":"Now let\u2019s have some fun testing our **new and all-powerful** model on some custom data!\n\nThis model will return values between 0 and 1, representing it\u2019s confidence on whether a tweet holds a negative or a positive sentiment. The closer the value is to 0, the more confident the model is that this tweet is negative. The closer the value is to 1, the more confident the model is that this tweet is negative.\n","6ca6a225":"### 5.1.3. Data Padding  <a class=\"anchor\" id=\"head-5-1-3\"><\/a>","4c4486bf":"## 4.1. Splitting the Data <a class=\"anchor\" id=\"head-4-1\"><\/a>","139e3a96":"## 4.3. Testing the Model <a class=\"anchor\" id=\"head-4-3\"><\/a>","6dd357dc":"We can clearly see the effects of adding dropout layers on the training progress. The training accuracy is progressing at a much slower pace than it previously did. Nevertheless, the validation accuracy is steadily increasing, reaching 76.85%.\n\nLet's keep on training the model for 20 more epochs.","a490be62":"Nevertheless, this model has various shortcomings.\n\nAs the model only evaluates sentences at an independent word level, it performs very poorly when it comes to negations and other multi-words constructs. For example, is the model gets the following input: **The concert was good!**, it would simply take each individual word (here, and after cleaning the input, *\\\"concert\\\"* and *\\\"good\\\"*) and calculate each word's probability to be either positive and negative and finally multiply everything together. Thus, we would expect the model to perform poorly on examples such: **The concert was not good!** or **I'm not very happy :(**.","29b4f18e":"## 2.2. Lemmatization <a class=\"anchor\" id=\"head-2-2\"><\/a>","0c730209":"## 4.4. Asserting the Model <a class=\"anchor\" id=\"head-4-4\"><\/a> ","f4856dd4":"**Word Clouds** are one of the best visualizations for words frequencies in text documents.<br>Essentially, what it does is that it produces an image with frequently-appearing words in the text document, where the most frequent words are showcased with bigger font sizes, and less frequent words with smaller font sizes.","d92c06f0":"# 1. Importing and Discovering the Dataset <a class=\"anchor\" id=\"head-1\"><\/a>","3b4062fe":"### 5.1.2. Global Vectors for Word Representation (GloVe) <a class=\"anchor\" id=\"head-5-1-2\"><\/a>","a183ef85":"### Model Training - Dropout","39af689c":"Let's investigate some of the most commonly occuring unknowns words in our data.","ccc5e89a":"So, while looking at the data I thought to myself: *could the tweet date have any correlation(s) with the tweet sentiment?*.\n\nLet's find out!","0bb1b498":"It's super interesting to see that it turns out that there's a relationship between the time of the tweet (in terms of hours of day) and whether this tweet has a positive or negative sentiment!\n\nIn fact, from this graph I concluded two things:\n\n* There's a specific interval of time in which positive tweets outnumber the negative tweets, and another interval in which the opposite occurs.\n* When the total number of tweets is relatively high, the majority of the tweets tend to have a positive sentiment. This *could* also mean that positive tweets come in big batches, maybe relating to a similar subject (soccer game, movie release, etc), whereas negative tweets are more sparsely distributed.\n\nAlso, you might find it weird that the biggest number of tweets occurs between 12AM and 7AM, however keep in mind that this is the raw date we've read from the data, and could quite possibly need to be converted to another timezone. As it stands, I cannot directly draw a conclusion as to which specific time frame favors positive or negative tweets, as I'm not entirely sure how to convert the given dates to the appropriate timezone of the collected data. So just look at the shape of the graph, without necesseraly pinpointing the hour coordinates. I.e., the graph could be shifted in any (horizontal) direction and still hold the same meaning.","ace1678b":"### 5.5.1. Regularization - Dropout <a class=\"anchor\" id=\"head-5-5-1\"><\/a>","bdc3717b":"Finally, it is worth mentioning one more weakness of such a model: it does not generalize well.\n\nThe model would perform greatly on data similar to the data it trained on. For example, if the model learned that **I love football** and **I love cooking** carry a positive sentiment, it would be pretty easy for it to classify **I love machine learning** as a sentence carrying a positive sentiment. However, if the model were to classify **I adore embeddings**, it would most probably miss-classify it. As it never encountered any of these words before, therefore it is unable to properly classify it, and would simply output a random choice.","774fa9c9":"Those world clouds are beautiful!\n\nIt's quite interesting to observe the (higher) occurence of *\\\"love\\\"*, *\\\"lol\\\"* and *\\\"thank\\\"* in the positive tweets, and of *\\\"work\\\"* and *\\\"miss\\\"* in the negative tweets. However, and in my opinion, the most compelling information observed in those images is without a doubt the huge occurence of the word **today** in the negative tweets.\n\nI personally interpret this information as follows: *Bad news travel faster than good news*. People tend to talk and share bad news much more frequently and much more quickly than good news.","d5561b35":"Gathering insights from the above output, and from manually skimming through the data, the `remove_noise` function has been redefined as follows:","87820faa":"Now let's re-compute our X and Y arrays according to the newly defined `remove_noise` function.","86ea7a72":"That's amazing! We've managed to bring down the number of unknown words from 200K to 120K! Calculating the percentage of unknown words now, it shrank down to approximately **2.5%**.","9acd82b2":"## 5.4. Training the Model <a class=\"anchor\" id=\"head-5-4\"><\/a>","a161377a":"200K unknown words are kind of a lot. In fact, after counting the total number of words in our data, this equates to ~7% of all words. ","9cfb6612":"## 5.5. Investigating Possibilties to Improve the Model <a class=\"anchor\" id=\"head-5-5\"><\/a>","51e549f3":"According to the Cambridge English Dictionary, **Lemmatization** is the process of reducing the different forms of a word to one single form, for example, reducing \"builds\", \"building\", or \"built\" to the lemma \"build\". This will greatly help our classifier by treating all variants of a given word as being references to the original lemma word. For example, it will avoid interpreting \"running\" and \"run\" as completely different inputs.\n<br>\n\nIn this example, we will use *nltk*'s **WordNetLemmatizer** to accomplish this task. This lemmatizer however takes as input two arguments: a list of tokens to be lemmatized as well as their corresponding *part of speech*. The most common parts of speech in english are nouns and verbs. In order to extract each token's part of speech, we will utilize *nltk*'s *post_tag* function, that takes an input a list of tokens, and returns a list of tuples, where each tuple is composed of a token and its corresponding position tag. Various position tags can be outputted from the pos_tag function, however the most notable ones are:\n* **NNP**: Noun, proper, singular\n* **NN**: Noun, common, singular or mass.\n* **VBG**: Verb, gerund or present participle.\n* **VBN**: Verb, past participle.\n\nA full list of position tags can be found at: https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html","3af74c62":"# 7. Extra Tip: Pickling ! <a class=\"anchor\" id=\"head-7\"><\/a>","73e0182d":"# 3. Visualizing the Data <a class=\"anchor\" id=\"head-3\"><\/a>"}}