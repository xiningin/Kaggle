{"cell_type":{"6865a752":"code","9996f6f7":"code","3c4bbde6":"code","28070aca":"code","88a6dd9a":"code","acbdfd8c":"code","6c559e85":"code","2db3df06":"code","370fb6fb":"code","1d94c1e9":"code","8af17b7e":"code","5ac5b42f":"markdown","b478da4e":"markdown","5e189cbb":"markdown","9b6808c0":"markdown","a5ae6e6e":"markdown"},"source":{"6865a752":"import os\nimport glob\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport cudf\n\nfrom tqdm import tqdm\nfrom joblib import Parallel,delayed\n\nlist_order_book_file_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')","9996f6f7":"# Check for nvidia GPU availability\n!nvidia-smi","3c4bbde6":"def wap1(df_book_data):\n    wap =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  \/ (\n                                      df_book_data['bid_size1']+ df_book_data['ask_size1'])\n    return wap\n\ndef wap2(df_book_data):\n    wap =(df_book_data['bid_price2'] * df_book_data['ask_size2']+df_book_data['ask_price2'] * df_book_data['bid_size2'])  \/ (\n                                      df_book_data['bid_size2']+ df_book_data['ask_size2'])\n    \n    return wap\n\ndef wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    \n    return wap\n\ndef wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    \n    return wap\n\n\n\ncolumns = ['time_id','wap1','wap2','wap3','wap4','wap12','ask_volume','bid_volume','total_volume','wap_balance','price_spread1','price_spread2',\n    'bid_spread','ask_spread','bid_ask_spread','weighted_spread1','weighted_spread2','weighted_total_spread','volume_imbalance']\n\ndef compute_rv(df_book_data,cols=columns):\n    \n        cp_df = df_book_data[cols].values.get()\n        x = np.unique(cp_df[:, 0],return_index=True)\n        wap_grouby_timeid_flatten = np.split(cp_df[:,1:],list(x[1][1:]))\n        rv = np.zeros((len(wap_grouby_timeid_flatten),81))\n        for i in range(len(wap_grouby_timeid_flatten)):\n            \n            \n            ## Stats for wap1\n            rv[i,5] = wap_grouby_timeid_flatten[i][:,0].max()\n            rv[i,6] = wap_grouby_timeid_flatten[i][:,0].mean()\n            rv[i,7] = wap_grouby_timeid_flatten[i][:,0].max()-wap_grouby_timeid_flatten[i][:,0].min()\n            rv[i,8] = wap_grouby_timeid_flatten[i][:,0].sum()\n\n            \n            ## Stats for wap2\n            rv[i,9] = wap_grouby_timeid_flatten[i][:,1].max()\n            rv[i,10] = wap_grouby_timeid_flatten[i][:1].mean()\n            rv[i,11] = wap_grouby_timeid_flatten[i][:,1].max()-wap_grouby_timeid_flatten[i][:,1].min()\n            rv[i,12] = wap_grouby_timeid_flatten[i][:,1].sum()\n            \n            \n            # Stats for wap3\n            rv[i,13] = wap_grouby_timeid_flatten[i][:,2].max()\n            rv[i,14] = wap_grouby_timeid_flatten[i][:,2].mean()\n            rv[i,15] = wap_grouby_timeid_flatten[i][:,2].max()-wap_grouby_timeid_flatten[i][:,2].min()\n            rv[i,16] = wap_grouby_timeid_flatten[i][:,2].sum()\n            \n            \n            ## Stats for wap4\n            rv[i,17] = wap_grouby_timeid_flatten[i][:,3].max()\n            rv[i,18] = wap_grouby_timeid_flatten[i][:,3].mean()\n            rv[i,19] = wap_grouby_timeid_flatten[i][:,3].max()-wap_grouby_timeid_flatten[i][:,3].min()\n            rv[i,20] = wap_grouby_timeid_flatten[i][:,3].sum()\n            \n            ## Stats for wap1+wap2\n            rv[i,21] = wap_grouby_timeid_flatten[i][:,4].max()\n            rv[i,22] = wap_grouby_timeid_flatten[i][:,4].mean()\n            rv[i,23] = wap_grouby_timeid_flatten[i][:,4].max()-wap_grouby_timeid_flatten[i][:,4].min()\n            rv[i,24] = wap_grouby_timeid_flatten[i][:,4].sum()\n\n            wap_grouby_timeid_flatten[i][:,0] = np.insert(np.diff(np.log(wap_grouby_timeid_flatten[i][:,0]),axis=0),0,0)\n            wap_grouby_timeid_flatten[i][:,1] = np.insert(np.diff(np.log(wap_grouby_timeid_flatten[i][:,1]),axis=0),0,0)\n            wap_grouby_timeid_flatten[i][:,2] = np.insert(np.diff(np.log(wap_grouby_timeid_flatten[i][:,2]),axis=0),0,0)\n            wap_grouby_timeid_flatten[i][:,3] = np.insert(np.diff(np.log(wap_grouby_timeid_flatten[i][:,3]),axis=0),0,0)\n            wap_grouby_timeid_flatten[i][:,4] = np.insert(np.diff(np.log(wap_grouby_timeid_flatten[i][:,4]),axis=0),0,0)\n\n            \n            ## predicted rv.\n            rv[i,0]=np.sqrt(np.sum(wap_grouby_timeid_flatten[i][:,0]**2,axis=0))\n            rv[i,1]=np.sqrt(np.sum(wap_grouby_timeid_flatten[i][:,1]**2,axis=0))\n            rv[i,2]=np.sqrt(np.sum(wap_grouby_timeid_flatten[i][:,2]**2,axis=0))\n            rv[i,3]=np.sqrt(np.sum(wap_grouby_timeid_flatten[i][:,3]**2,axis=0))\n            rv[i,4]=np.sqrt(np.sum(wap_grouby_timeid_flatten[i][:,4]**2,axis=0))\n            \n            #####################################\n            \n            ## Ask volume\n            rv[i,25] = wap_grouby_timeid_flatten[i][:,5].max()\n            rv[i,26] = wap_grouby_timeid_flatten[i][:,5].mean()\n            rv[i,27] = wap_grouby_timeid_flatten[i][:,5].max()-wap_grouby_timeid_flatten[i][:,5].min()\n            rv[i,28] = wap_grouby_timeid_flatten[i][:,5].sum()\n\n            ## bid volume\n            rv[i,29] = wap_grouby_timeid_flatten[i][:,6].max()\n            rv[i,30] = wap_grouby_timeid_flatten[i][:,6].mean()\n            rv[i,31] = wap_grouby_timeid_flatten[i][:,6].max()-wap_grouby_timeid_flatten[i][:,6].min()\n            rv[i,32] = wap_grouby_timeid_flatten[i][:,6].sum()\n            \n            ## total volume\n            rv[i,33] = wap_grouby_timeid_flatten[i][:,7].max()\n            rv[i,34] = wap_grouby_timeid_flatten[i][:,7].mean()\n            rv[i,35] = wap_grouby_timeid_flatten[i][:,7].max()-wap_grouby_timeid_flatten[i][:,7].min()\n            rv[i,36] = wap_grouby_timeid_flatten[i][:,7].sum()\n            \n            \n            ## wap balance\n            rv[i,37] = wap_grouby_timeid_flatten[i][:,8].max()\n            rv[i,38] = wap_grouby_timeid_flatten[i][:,8].mean()\n            rv[i,39] = wap_grouby_timeid_flatten[i][:,8].max()-wap_grouby_timeid_flatten[i][:,8].min()\n            rv[i,40] = wap_grouby_timeid_flatten[i][:,8].sum()\n\n            \n            ## price spread1\n            rv[i,41] = wap_grouby_timeid_flatten[i][:,9].max()\n            rv[i,42] = wap_grouby_timeid_flatten[i][:,9].mean()\n            rv[i,43] = wap_grouby_timeid_flatten[i][:,9].max()-wap_grouby_timeid_flatten[i][:,9].min()\n            rv[i,44] = wap_grouby_timeid_flatten[i][:,9].sum()\n\n            \n            ## price spread2\n            rv[i,45] = wap_grouby_timeid_flatten[i][:,10].max()\n            rv[i,46] = wap_grouby_timeid_flatten[i][:,10].mean()\n            rv[i,47] = wap_grouby_timeid_flatten[i][:,10].max()-wap_grouby_timeid_flatten[i][:,10].min()\n            rv[i,48] = wap_grouby_timeid_flatten[i][:,10].sum()\n\n            \n            ## bid_spread\n            rv[i,49] = wap_grouby_timeid_flatten[i][:,11].max()\n            rv[i,50] = wap_grouby_timeid_flatten[i][:,11].mean()\n            rv[i,51] = wap_grouby_timeid_flatten[i][:,11].max()-wap_grouby_timeid_flatten[i][:,11].min()\n            rv[i,52] = wap_grouby_timeid_flatten[i][:,11].sum()\n\n            ## bid_spread\n            rv[i,53] = wap_grouby_timeid_flatten[i][:,12].max()\n            rv[i,54] = wap_grouby_timeid_flatten[i][:,12].mean()\n            rv[i,55] = wap_grouby_timeid_flatten[i][:,12].max()-wap_grouby_timeid_flatten[i][:,12].min()\n            rv[i,56] = wap_grouby_timeid_flatten[i][:,12].sum()\n\n            \n            ## ask_spread\n            rv[i,57] = wap_grouby_timeid_flatten[i][:,13].max()\n            rv[i,58] = wap_grouby_timeid_flatten[i][:,13].mean()\n            rv[i,59] = wap_grouby_timeid_flatten[i][:,13].max()-wap_grouby_timeid_flatten[i][:,13].min()\n            rv[i,60] = wap_grouby_timeid_flatten[i][:,13].sum()\n\n            ## bid ask spread\n            rv[i,61] = wap_grouby_timeid_flatten[i][:,14].max()\n            rv[i,62] = wap_grouby_timeid_flatten[i][:,14].mean()\n            rv[i,63] = wap_grouby_timeid_flatten[i][:,14].max()-wap_grouby_timeid_flatten[i][:,14].min()\n            rv[i,64] = wap_grouby_timeid_flatten[i][:,14].sum()\n\n\n            ## weighted spread1\n            rv[i,65] = wap_grouby_timeid_flatten[i][:,15].max()\n            rv[i,66] = wap_grouby_timeid_flatten[i][:,15].mean()\n            rv[i,67] = wap_grouby_timeid_flatten[i][:,15].max()-wap_grouby_timeid_flatten[i][:,15].min()\n            rv[i,68] = wap_grouby_timeid_flatten[i][:,15].sum()\n    \n            ## weighted spread 2\n            rv[i,69] = wap_grouby_timeid_flatten[i][:,16].max()\n            rv[i,70] = wap_grouby_timeid_flatten[i][:,16].mean()\n            rv[i,71] = wap_grouby_timeid_flatten[i][:,16].max()-wap_grouby_timeid_flatten[i][:,16].min()\n            rv[i,72] = wap_grouby_timeid_flatten[i][:,16].sum()\n            \n            ## weighted total spread\n            rv[i,73] = wap_grouby_timeid_flatten[i][:,17].max()\n            rv[i,74] = wap_grouby_timeid_flatten[i][:,17].mean()\n            rv[i,75] = wap_grouby_timeid_flatten[i][:,17].max()-wap_grouby_timeid_flatten[i][:,17].min()\n            rv[i,76] = wap_grouby_timeid_flatten[i][:,17].sum()\n            \n\n            \n            \n        return rv,x[0]\n","28070aca":"def rapids_compute_features(file_path, pred_col_name):\n\n    # Read a specific stock file.\n    df_book_data = cudf.read_parquet(file_path)\n    # Compute the wap and create a wap column.\n    df_book_data['wap1'] = wap1(df_book_data)\n    df_book_data['wap2'] = wap2(df_book_data)\n    df_book_data['wap3'] = wap3(df_book_data)\n    df_book_data['wap4'] = wap4(df_book_data)\n    df_book_data['wap12'] = df_book_data['wap1'] + df_book_data['wap2']\n    \n    \n    ['wap1','wap2','wap3','wap4','wap12','ask_volume','bid_volume','total_volume','wap_balance','price_spread1','price_spread2',\n    'bid_spread','ask_spread','bid_ask_spread','weighted_spread1','weighted_spread2','weighted_total_spread','volume_imbalance']\n    \n    \n    df_book_data['ask_volume'] = df_book_data['ask_size1']+df_book_data['ask_size2']\n    df_book_data['bid_volume'] = df_book_data['bid_size1']+df_book_data['bid_size2']\n    df_book_data['total_volume'] = df_book_data['ask_volume']+df_book_data['bid_volume']\n    \n    # Calculate wap balance\n    df_book_data['wap_balance'] = abs(df_book_data['wap1'] - df_book_data['wap2'])\n    # Calculate spread\n    df_book_data['price_spread1'] = (df_book_data['ask_price1'] - df_book_data['bid_price1']) \/ ((df_book_data['ask_price1'] + df_book_data['bid_price1']) \/ 2)\n    df_book_data['price_spread2'] = (df_book_data['ask_price2'] - df_book_data['bid_price2']) \/ ((df_book_data['ask_price2'] + df_book_data['bid_price2']) \/ 2)\n    \n    df_book_data['bid_spread'] = df_book_data['bid_price1'] - df_book_data['bid_price2']\n    df_book_data['ask_spread'] = df_book_data['ask_price1'] - df_book_data['ask_price2']\n    df_book_data[\"bid_ask_spread\"] = abs(df_book_data['bid_spread'] - df_book_data['ask_spread'])\n    \n    df_book_data[\"weighted_spread1\"] = df_book_data['ask_price1']* df_book_data['ask_size1'] - df_book_data['bid_price1']* df_book_data['bid_size1'] \n    df_book_data[\"weighted_spread2\"] = df_book_data['ask_price2']* df_book_data['ask_size2'] - df_book_data['bid_price2']* df_book_data['bid_size2'] \n    df_book_data[\"weighted_total_spread\"] = df_book_data[\"weighted_spread1\"] + df_book_data[\"weighted_spread2\"]\n    \n    df_book_data['volume_imbalance'] = abs((df_book_data['ask_size1'] + df_book_data['ask_size2']) - (df_book_data['bid_size1'] + df_book_data['bid_size2']))\n\n\n    rv,time_id = compute_rv(df_book_data)\n\n#       ['wap1','wap2','wap3','wap4','wap12','ask_volume','bid_volume','total_volume','wap_balance','price_spread1','price_spread2',\n #   'bid_spread','ask_spread','bid_ask_spread','weighted_spread1','weighted_spread2','weighted_total_spread','volume_imbalance']\n        \n    cols = [pred_col_name+'1', pred_col_name+'2', pred_col_name+'3', pred_col_name+'4', pred_col_name+'12',\n     'wap1max','wap1mean','wap1range','wap1sum',\n     'wap2max','wap2mean','wap2range','wap2sum',\n     'wap3max','wap3mean','wap3range','wap3sum',\n     'wap4max','wap4mean','wap4range','wap4sum',\n     'wap12max','wap12mean','wap12range','wap12sum',\n     'ask_volume_max','ask_volume_mean','ask_volume_range','ask_volume_sum',\n     'bid_volume_max','bid_volume_mean','bid_volume_range','bid_volume_sum',\n     'total_volume_max','total_volume_mean','total_volume_range','total_volume_sum',  \n     'wap_balance_max','wap_balance_mean','wap_balance_range','wap_balance_sum',\n     'price_spread1_max','price_spread1_mean','price_spread1_range','price_spread1_sum',\n     'price_spread2_max','price_spread2_mean','price_spread2_range','price_spread2_sum',\n     'bid_spread_max','bid_spread_mean','bid_spread_range','bid_spread_sum',\n     'ask_spread_max','ask_spread_mean','ask_spread_range','ask_spread_sum',\n     'bid_ask_spread_max','bid_ask_spread_mean','bid_ask_spread_range','bid_ask_spread_sum',\n     'w_spread1_max','w_spread1_mean','w_spread1_range','w_spread1_sum',\n     'w_spread2_max','w_spread2_mean','w_spread2_range','w_spread2_sum',\n     'wt_spread_max','wt_spread_mean','wt_spread_range','wt_spread_sum',\n     'vol_imb_max','vol_imb_mean','vol_imb_range','vol_imb_sum']\n\n      \n    \n    df_realized_vol_per_stock = cudf.DataFrame()\n    df_realized_vol_per_stock['time_id']=time_id\n\n    for i in range(len(cols)):\n        \n        df_realized_vol_per_stock[cols[i]] = rv[:,i]\n        \n        \n    #df_realized_vol_per_stock[pred_col_name+'1']=rv[:,0]\n\n\n    stock_id = file_path.split('=')[1]\n\n    # Compute an ID for the combinaison of stock-time IDs.\n    df_realized_vol_per_stock['row_id'] = stock_id+'-'+df_realized_vol_per_stock['time_id'].astype(int).astype(str)    \n\n    df_realized_vol_per_stock['fft_rv1'] = np.abs(np.fft.fft((rv[:,0])))\n    df_realized_vol_per_stock['fft_rv2'] = np.abs(np.fft.fft((rv[:,1])))\n\n    \n    return df_realized_vol_per_stock\n\n\n\n\n\n\n\n\ndef rapids_features_per_stock(list_file,prediction_column_name):\n    \"\"\"\n   # Loops over all the files of the books and apply the rapids_compute_features function.\n    \"\"\"\n    df_past_realized = cudf.DataFrame()\n    for file in tqdm(list_file):\n        df_past_realized = cudf.concat([df_past_realized,\n                                     rapids_compute_features(file,prediction_column_name)])\n    return df_past_realized","88a6dd9a":"train = cudf.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')","acbdfd8c":"%time df_past_realized_train = rapids_features_per_stock(list_file=list_order_book_file_train, prediction_column_name='rv')","6c559e85":"df_past_realized_train.to_csv('df_past_realized_train.csv',index=False)","2db3df06":"list_order_book_file_test = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\ndf_naive_pred_test = rapids_features_per_stock(list_file=list_order_book_file_test,prediction_column_name='rv')\n","370fb6fb":"df_naive_pred_test.to_csv('test.csv',index=False)","1d94c1e9":"def log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n\n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_files):\n    \n    # Parrallel for loop\n    def for_joblib(file):\n     \n            \n        file_path_trade = file \n        # Preprocess book and trade data and merge them\n        df_tmp = trade_preprocessor(file_path_trade)\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_file) for stock_file in list_stock_files)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n","8af17b7e":"list_trade_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/*')\ntrain_ = preprocessor(list_trade_train_files)\nlist_trade_test = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/*')\ntest_ = preprocessor(list_trade_test)","5ac5b42f":"## Generate training data features:","b478da4e":"# Trade data:","5e189cbb":"## Create test data features:","9b6808c0":"## Helper function:\n\n* **wap1**,**wap2**,**wap3**,**wap4**: compute different types of wap using both book data.\n* **compute_rv**: Function that computes the different variables using bid, ask price and size as well as the wap values.\n* **rapids_compute_features**: compute features for each time ID.\n* **rapids_features_per_stock**: compute features for each stock ID.","a5ae6e6e":"# Feature extraction using GPU Accelerated library RAPIDS.\n\nRAPIDS is a library of open source software that runs exclusively on GPUs. It works with different machine learning algorithms to provide a faster processing. In this notebook, I used \"cudf\" which is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. It has a pandas like API so its really easy to understand the code while getting a performance optimization of ~18-20 times faster!!\nThe notebook provides a feature generation techniques for generating 80 variables that can help improve the performances of the models.\n"}}