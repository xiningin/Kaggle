{"cell_type":{"e378fef3":"code","cbeab7f8":"code","32130f84":"code","b0241d4b":"code","17e555af":"code","9926118f":"code","89a59fb8":"code","371c2d35":"code","65cb4ff2":"code","1d2f757e":"code","122e70c9":"code","658691d0":"code","680e4522":"code","7c75dfd1":"code","9d8736d6":"code","29a77109":"code","e1e1783f":"code","1ec20db5":"code","906d8387":"code","4ebca7eb":"code","69aeb62f":"code","e25e93ec":"code","7f190f2a":"code","1cd37ec3":"code","704854c4":"code","557fe38e":"code","7bfe7917":"code","cb1ac4a6":"code","95c5d492":"code","f7fc5e33":"code","87a0cbdc":"code","ec4d4b53":"code","34f054fd":"code","0b199e51":"code","34d087be":"code","5732164f":"code","58ff04c0":"code","7df36e45":"code","48a9a24e":"code","e4b5d7b6":"code","470ebcd9":"code","8a0eb8b4":"code","381c2897":"code","2a6313fd":"code","bc853ce7":"code","5f23c701":"code","f46ed414":"code","25a2239a":"code","e9428ecd":"code","a19f7a99":"code","09a76cdb":"code","1be2b444":"code","fcec1360":"code","59a0bdcc":"code","3131ccdc":"code","2da58868":"markdown","911ac14c":"markdown","d1b55f00":"markdown","3b98e422":"markdown","5ef12650":"markdown","18ec25ce":"markdown","88e48a80":"markdown","9ce2c5fd":"markdown","76b97ccb":"markdown","06c928ac":"markdown","d41b0cf2":"markdown","c2c8b2d3":"markdown","14c8e866":"markdown","5839f2ca":"markdown","6895e1c5":"markdown","96250823":"markdown","0b774a93":"markdown","fb00e4ac":"markdown","1a41b4f0":"markdown","544f6e84":"markdown","db0e16f6":"markdown","a27215ca":"markdown"},"source":{"e378fef3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbeab7f8":"import warnings\nwarnings.filterwarnings('ignore')","32130f84":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic.head()\n## checking the head of our data set","b0241d4b":"titanic.info()\n## checking info of all columns","17e555af":"titanic.shape\n## checking shape of data set","9926118f":"titanic.describe()\n## statistical information about numerical variable","89a59fb8":"round(100*(titanic.isnull().sum()\/len(titanic)),2)\n## checking missing value percentage in all columns","371c2d35":"titanic.drop('Cabin',axis=1,inplace=True)\n## cabin almost have 77% of missing values hence remove this column from data set","65cb4ff2":"age_median = titanic['Age'].median(skipna=True)\ntitanic['Age'].fillna(age_median,inplace=True)\n## as there is 19% of missing values in age column hence it is not a good idea to remove this row wise or column wise hence impute those missing values with the median of age \n","1d2f757e":"titanic = titanic[titanic['Embarked'].isnull()!=True]\n## as embarked has a very small amount of missing values hence remove those rows which have missing values in embarked column \n","122e70c9":"titanic.shape\n## checking shape after removing null values","658691d0":"titanic_dub = titanic.copy()\n## creating copy of the data frame to check duplicate values","680e4522":"titanic_dub.shape\n## comparing shapes of two data frames","7c75dfd1":"titanic.shape\n## shape of original data frame","9d8736d6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n## importing libraries for data visualitation","29a77109":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.boxplot(y=titanic['Age'])\nplt.title(\"Outliers in 'Age'\")\n\nplt.subplot(1,4,2)\nax = sns.boxplot(y=titanic['Fare'])\nax.set_yscale('log')\nplt.title(\"Outliers in 'Fare'\")\n\nplt.subplot(1,4,3)\nsns.boxplot(y=titanic['SibSp'])\nplt.title(\"Outliers in 'SibSp'\")\n\n\nplt.subplot(1,4,4)\nsns.boxplot(y=titanic['Parch'])\nplt.title(\"Outliers in 'Parch'\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n\n## plotting all four variables to check for outliers\n## it clearly shows that all four variables has some outliers","e1e1783f":"\n\nsns.catplot(x=\"SibSp\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nsns.catplot(x=\"Parch\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nplt.tight_layout()\nplt.show()\n\n## plotting of sibsp and parch in basis of survived and not survived","1ec20db5":"def alone(x):\n    if (x['SibSp']+x['Parch']>0):\n        return (1)\n    else:\n        return (0)\ntitanic['Alone'] = titanic.apply(alone,axis=1)\n## creating a function to make one variable which tells us whether a person is single or accompanied by some on the ship","906d8387":"sns.catplot(x=\"Alone\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nplt.show()","4ebca7eb":"## drop parch and sibsp\ntitanic = titanic.drop(['Parch','SibSp'],axis=1)\ntitanic.head()\n","69aeb62f":"sns.distplot(titanic['Fare'])\nplt.show()","e25e93ec":"sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\", data=titanic, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')\nsns.catplot(x=\"Sex\", y=\"Survived\", col=\"Embarked\", data=titanic, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')\nplt.show()\n\n## plotting of survive on basis of pclass","7f190f2a":"survived_0 = titanic[titanic['Survived']==0]\nsurvived_1 = titanic[titanic['Survived']==1]\n## divided our dataset into survived or not survived to check the distribution of age in both the cases ","1cd37ec3":"survived_0.shape\n## checking shape of the data set that contains the data of passengers who not survived","704854c4":"survived_1.shape\n## checking shape of the data set that contains the data of passengers who survived","557fe38e":"sns.distplot(survived_0['Age'])\nplt.show()\n## checking distribution of age in not survived data set","7bfe7917":"sns.distplot(survived_1['Age'])\nplt.show()\n## checking distribution of age in survived dataset","cb1ac4a6":"sns.boxplot(x='Survived',y='Fare',data=titanic)\nplt.show()\n## checking survival rate on basis of fare","95c5d492":"Pclass_dummy = pd.get_dummies(titanic['Pclass'],prefix='Pclass',drop_first=True)\nPclass_dummy.head()\n## creating dummy variables for pclass\n\n","f7fc5e33":"## joing dummy variables\ntitanic = pd.concat([titanic,Pclass_dummy],axis=1)\ntitanic.head()","87a0cbdc":"titanic.drop('Pclass',axis=1,inplace=True)\n## as there is no use of pclass after joining the columns that contains dummy variables  for pclass","ec4d4b53":"Embarked_dummy = pd.get_dummies(titanic['Embarked'],drop_first=True)\nEmbarked_dummy.head()\n## creating dummy variables for embarked and dropping first column","34f054fd":"titanic = pd.concat([titanic,Embarked_dummy],axis=1)\ntitanic.drop('Embarked',axis=1,inplace=True)\n## joining dummy variables","0b199e51":"titanic.head()\n## checking head of the data set after joining dummy variables","34d087be":"def sex_map(x):\n    if x == 'male':\n        return (1)\n    elif x == 'female':\n        return (0)\ntitanic['Sex'] = titanic['Sex'].apply(lambda x:sex_map(x))\n\n## creating function for convert sex into binary values","5732164f":"titanic = titanic[['Survived','Sex','Age','Fare','Alone','Pclass_2','Pclass_3','Q','S']]","58ff04c0":"## let's plot one heatmap to check the corelations \nplt.figure(figsize=(10,10))\nsns.heatmap(titanic.corr(),annot=True)\nplt.show()","7df36e45":"## read test data \n\ntitanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","48a9a24e":"## check shape and info of titanic_test\n\ntitanic_test.shape","e4b5d7b6":"titanic_test.info()","470ebcd9":"## imputing missing values of age \ntitanic_test['Age'].fillna(age_median,inplace=True)\n## create alone column\ntitanic_test['Alone'] = titanic_test.apply(alone,axis=1)\n## create sex column with binary value 0 for 'female' and 1 for 'male'\ntitanic_test['Sex'] = titanic_test['Sex'].apply(lambda x:sex_map(x))","8a0eb8b4":"## compute dummy encoding in test data \n\nPclass_dummy_test = pd.get_dummies(titanic_test['Pclass'],prefix='Pclass',drop_first=True)\n\ntitanic_test = pd.concat([titanic_test,Pclass_dummy_test],axis=1)","381c2897":"## compute dummy encoding in test data \n\n\nEmbarked_dummy_test = pd.get_dummies(titanic_test['Embarked'],drop_first=True)\n\ntitanic_test = pd.concat([titanic_test,Embarked_dummy_test],axis=1)","2a6313fd":"## let's seperate the target variable and divide train data into train and test (validation data) our actual test data is already seperated\n\ntarget = titanic.pop('Survived')\n\n## define train and test to from train to evaluate our result\n\n## import libraries \nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(titanic,target,random_state=42,stratify=target)\n\n## 'stratify' makes sure that both train and test contains same percentage of target '0' and '1'","bc853ce7":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier(objective='binary:logistic',seed=42)\nxgb_clf.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_metric='aucpr',eval_set=[(X_test,y_test)])","5f23c701":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(xgb_clf,X_test,y_test,values_format='d',display_labels=[\"Not Survived\",\"Survived\"])\nplt.show()\n\n## plot confusion matrix on seperated test set of the train data ","f46ed414":"## define my parameters\n\nfrom sklearn.model_selection import GridSearchCV\n\n##ROUND1\n\nparam_grid = {\n               'max_depth':[2,3,4],\n               'learning_rate':[0.1,0.01,0.05],\n               'gamma':[0,0.25,1.0],\n               'reg_lambda':[0,1.0,10.0]\n    \n}\n\noptimal_model1 = GridSearchCV(estimator=xgb.XGBClassifier(objctive='binary:logistic',seed=42,subsample=0.9,colsample_bytree=0.6),\n                              param_grid=param_grid,\n                              scoring='roc_auc',\n                              verbose=0,\n                              n_jobs=10,\n                              cv=5\n                             ).fit(\n                                   X_train,y_train,early_stopping_rounds=10,eval_metric='auc',eval_set=[(X_test,y_test)]\n)","25a2239a":"optimal_model1.best_params_","e9428ecd":"##ROUND2\nparam_grid = {\n               'max_depth':[5,6,7],\n               'learning_rate':[0.001,0.003,0.005],\n               'gamma':[2.0,5.0,10.0],\n               'reg_lambda':[0,1.0,10.0]\n    \n}\n\noptimal_model2 = GridSearchCV(estimator=xgb.XGBClassifier(objctive='binary:logistic',seed=42,subsample=0.9,colsample_bytree=0.6),\n                              param_grid=param_grid,\n                              scoring='roc_auc',\n                              verbose=0,\n                              n_jobs=10,\n                              cv=5\n                             ).fit(\n                                   X_train,y_train,early_stopping_rounds=10,eval_metric='auc',eval_set=[(X_test,y_test)]\n)","a19f7a99":"optimal_model2.best_params_","09a76cdb":"##ROUND3\nparam_grid = {\n               'max_depth':[8,9,10],\n               'learning_rate':[0.0001,0.0003,0.0005],\n               'gamma':[1.25,1.50,1.75],\n               'reg_lambda':[0,1.0,10.0]\n    \n}\n\noptimal_model3 = GridSearchCV(estimator=xgb.XGBClassifier(objctive='binary:logistic',seed=42,subsample=0.9,colsample_bytree=0.6),\n                              param_grid=param_grid,\n                              scoring='roc_auc',\n                              verbose=0,\n                              n_jobs=10,\n                              cv=5\n                             ).fit(\n                                   X_train,y_train,early_stopping_rounds=10,eval_metric='auc',eval_set=[(X_test,y_test)]\n)","1be2b444":"optimal_model3.best_params_","fcec1360":"optimal_model2.best_estimator_","59a0bdcc":"## hence pick our final params {'gamma': 2.0, 'learning_rate': 0.03, 'max_depth': 7, 'reg_lambda': 0}\n\nxgb_final = optimal_model2.best_estimator_\n\n","3131ccdc":"titanic_test['Survived'] = xgb_final.predict(titanic_test[['Sex', 'Age', 'Fare', 'Alone', 'Pclass_2', 'Pclass_3', 'Q', 'S']])","2da58868":"# EDA","911ac14c":"Again scale values.","d1b55f00":"sibsp and parch basically tells us that whether a person is accompanied by someone else or not \nso we can make two category by merging them to find whether a single person is acompanied by some one else or not ","3b98e422":"we can use others way to tune our hyper parameters.","5ef12650":"those who are survived paid more fares","18ec25ce":"it clearly shows that those person who are not alone survived more","88e48a80":"there is some skewness in the fare column \nhence removing the skewness using log function","9ce2c5fd":"# duplicate check","76b97ccb":"There is not much higher co-relations between two variables.","06c928ac":"females are more likely to be survived","d41b0cf2":"young persons are survived more (age group between 20-40)","c2c8b2d3":"# **Data Quality Check**\n\nhandling missing values as well\n","14c8e866":"Those params are end of their range we will explore them more . Hence increase max_depth ,learning rate and gamma.we need to focous on our score so that model will not became overfit. ","5839f2ca":"**Optimize Parameter using cross validation and grid search**\n\nsome informations about the parameters used :\n\n**Maximum depth** of a tree. Increasing this value will make the model more complex and more likely to overfit.\nmax_depth is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n\n**learning_rate\/ETA** Step size shrinkage used in update to prevents overfitting.\nThe ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step (remember how each boosting round is correcting the errors of the previous? if not, check our first tutorial here).\nIn practice, having a lower eta makes our model more robust to overfitting thus, usually, the lower the learning rate, the best. But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements.\n\n**gamma** Minimum loss reduction required to make a further partition on a leaf node of the tree (used for pruning)\n\n**lambda** Increasing this value will make model more conservative.\n\n**subsample** corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n**colsample_bytree** corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.\n\nReference [https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html](http:\/\/)\n[https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f](http:\/\/)","6895e1c5":"check on test.","96250823":"**Default Xgboost Model**\n\n**some info about the parameters used**\n\nFortunately XGBoost provides a nice way to find the best number of rounds whilst training by **early_stopping_rounds**. Since trees are built sequentially, instead of fixing the number of rounds at the beginning, we can test our model at each step and see if adding a new tree\/round improves performance.\nTo do so, we define a test dataset and a metric that is used to assess performance at each round. If performance haven\u2019t improved for N rounds (N is defined by the variable early_stopping_round), we stop the training and keep the best number of boosting rounds.\n\n**seed**: random seed. It's important to set a seed here, to ensure we are using the same folds for each step so we can properly compare the scores with different parameters.\n\n**metrics**: the metrics to use to evaluate our model, here we use aucpr: Area under the PR curve.\n\n**eval_set** : a list of pairs to evaluate.","0b774a93":"# Hyper Parameter Tuning","fb00e4ac":"**XgBoost**\n\nXGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n\nA Gentle Introduction to XGBoost for Applied Machine Learning\n\n**What is XGBoost?**\nXGBoost stands for eXtreme Gradient Boosting.\n\nThe name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.\n\n\u2014 Tianqi Chen, in answer to the question \u201cWhat is the difference between the R gbm (gradient boosting machine) and xgboost (extreme gradient boosting)?\u201d on Quora\n\nIt is an implementation of gradient boosting machines created by Tianqi Chen, now with contributions from many developers. It belongs to a broader collection of tools under the umbrella of the Distributed Machine Learning Community or DMLC who are also the creators of the popular mxnet deep learning library.\n\nTianqi Chen provides a brief and interesting back story on the creation of XGBoost in the post Story and Lessons Behind the Evolution of XGBoost.\n\nXGBoost is a software library that you can download and install on your machine, then access from a variety of interfaces. Specifically, XGBoost supports the following main interfaces:\n\nCommand Line Interface (CLI).\nC++ (the language in which the library is written).\nPython interface as well as a model in scikit-learn.\nR interface as well as a model in the caret package.\nJulia.\nJava and JVM languages like Scala and platforms like Hadoop.\nXGBoost Features\nThe library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.\n\n**Model Features**\nThe implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:\n\nGradient Boosting algorithm also called gradient boosting machine including the learning rate.\nStochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\nRegularized Gradient Boosting with both L1 and L2 regularization.\n\n**System Features**\nThe library provides a system for use in a range of computing environments, not least:\n\nParallelization of tree construction using all of your CPU cores during training.\nDistributed Computing for training very large models using a cluster of machines.\nOut-of-Core Computing for very large datasets that don\u2019t fit into memory.\nCache Optimization of data structures and algorithm to make best use of hardware.\n\n**Algorithm Features**\nThe implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:\n\nSparse Aware implementation with automatic handling of missing data values.\nBlock Structure to support the parallelization of tree construction.\nContinued Training so that you can further boost an already fitted model on new data.\nXGBoost is free open source software available for use under the permissive Apache-2 license.\n\n**Why Use XGBoost?**\nThe two reasons to use XGBoost are also the two goals of the project:\n\nExecution Speed.\nModel Performance.\n\nReference:[https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/](http:\/\/)","1a41b4f0":"# creating dummy variables","544f6e84":"Select variables needed for creating model.","db0e16f6":"# Model Build","a27215ca":"As score is getting decreased hence select the previous models params ."}}