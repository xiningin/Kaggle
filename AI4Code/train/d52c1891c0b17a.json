{"cell_type":{"1e12ea78":"code","bbf611f8":"code","370d28f9":"code","f81909e8":"code","dc4930e5":"code","91f1d8f9":"code","aa788d5f":"code","8e2aca13":"code","203b2d81":"code","aa28a866":"code","03d2ad89":"code","4e9cf174":"code","a6c2e717":"code","9a441a97":"code","b3ed7d9e":"code","86b13b7b":"code","3b26cc94":"code","5371ebed":"code","4fe79463":"code","7e8efe28":"code","83f6f737":"code","ce7dd49d":"markdown","51387432":"markdown","ab4fa043":"markdown","87bf290b":"markdown","2c600e0e":"markdown","379510a7":"markdown","44a9dccc":"markdown","4404deb5":"markdown","4c9e86f0":"markdown","348fafcb":"markdown","213a4f54":"markdown","fef4a602":"markdown","75946347":"markdown","db07d884":"markdown","7f26fa9e":"markdown","ec47056c":"markdown","85b56760":"markdown","34834fe6":"markdown","c6e0f74c":"markdown"},"source":{"1e12ea78":"import numpy as np\nimport pandas as pd \nimport re\nimport math\nimport gensim\nimport spacy\nfrom nltk.tokenize import word_tokenize\n\n# Bokeh libraries\nfrom bokeh.io import output_file, output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import row, column, gridplot\nfrom bokeh.models.widgets import Tabs, Panel\n\nimport os\nprint(os.listdir(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\")[0:5])\n","bbf611f8":"#Loading wiki's word2vec embedding\nembed_model = gensim.models.KeyedVectors.load_word2vec_format(\"..\/input\/ai-bootcamp\/wiki-news-300d-1M.vec\")  ","370d28f9":"def clean_with_regex(df0,col_name,regex):\n    df = df0.copy()\n    \n    for idx,each in enumerate(df[col_name]):\n        try:\n            df.iloc[idx,df.columns.get_loc(col_name)] = re.sub(regex, '', each) \n        except:\n            #print(each)\n            #print(\"Error with row: \"+str(idx)+\" and column: \"+col_name)\n            continue\n    return df[col_name].copy()","f81909e8":"def remove_stop_words(df,col_name):\n    \n    #Lowercase all words\n    for idx,each in enumerate(df[col_name]):\n        try:\n            df.iloc[idx][col_name] = each.lower()\n        except:\n            continue\n    \n    #Remove stopwords\n    \n    for idx,each in enumerate(df[col_name]):\n        df.iloc[idx][col_name] = \" \".join(select_stop_words(each))\n    \n    return df","dc4930e5":"\n#Function obtained from https:\/\/www.programcreek.com\/python\/example\/106181\/sklearn.feature_extraction.stop_words.ENGLISH_STOP_WORDS and is open source & modified\ndef select_stop_words(word_list):\n    out = []\n    nlp = spacy.load(\"en\")\n    for word in word_list.split(' '):\n        #Removing special characters - \u2022,! etc\n        word = re.sub('[^\\s\\w]','',word)\n        sentence = nlp(word)\n        \n        if sentence[0].is_stop:\n            continue\n        out.append(word)\n    return out ","91f1d8f9":"def tf_idf(df,col_name):\n    \n    vec = TfidfVectorizer(ngram_range=(1,3))\n    resp = vec.fit_transform(df[col_name])\n    \n    new_cols_vocab = []\n    \n    sorted_x = sorted(vec.vocabulary_.items(), key=lambda kv: kv[1])\n    for each in sorted_x:\n        new_cols_vocab.append(each[0])\n        \n    dfz1 = pd.DataFrame(resp.toarray(),columns=new_cols_vocab)\n    dfz2 = df.merge(dfz1, how = 'inner', left_index = True, right_index=True)\n        \n    return resp,vec,dfz2","aa788d5f":"\ndef svd_apply_train(tfidf_matrix, k):\n    \n    svd = TruncatedSVD(n_components=k, n_iter=100)\n    svd_lsa = svd.fit_transform(tfidf_matrix)\n    svd_lsa = Normalizer(copy=False).fit_transform(svd_lsa)\n    \n    return svd,svd_lsa","8e2aca13":"zx = open(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/ARTS ASSOCIATE 2454 072117 REV 072817.txt\")\nprint(zx.read())\nzx.close()","203b2d81":"#Column name for dataframe\ncol_lst = ['ANNUAL SALARY','DUTIES','REQUIREMENTS','PROCESS','WHERE TO APPLY','APPLICATION DEADLINE','SELECTION','NOTE','NOTICE']\ncol_lst2 = ['position','ANNUAL SALARY','DUTIES','REQUIREMENTS','PROCESS','WHERE TO APPLY','APPLICATION DEADLINE','SELECTION','NOTE','NOTICE']\nrow_lst = []\ndf = pd.DataFrame(columns=col_lst)\n\nij=0\nfor each in os.listdir(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\"):\n    base_pth = \"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"\n    fp = open(base_pth+each)\n    \n    try:\n        section_start_pos = []\n        emp_dict = {}\n\n        #Extracting job title\n        title = fp.readline()\n        emp_dict['position'] = title\n\n        txt = fp.read()\n        for section in re.finditer(r'\\n[A-Z]+[ A-Z\/]*[:]?\\n{2}.*', txt):\n            section_start_pos.append(section.span()[0])\n\n        for i in range (0,len(section_start_pos)):\n            start_pos = section_start_pos[i]\n\n            if (i == len(section_start_pos)-1):\n                end_pos = len(txt)\n            else:\n                end_pos = section_start_pos[i+1]\n\n            tmp_txt = txt[start_pos:end_pos]\n\n            for each3 in col_lst:\n                if (re.finditer(each3,tmp_txt)):\n                    for each2 in re.finditer(each3,tmp_txt):\n                        emp_dict[each3] = str(tmp_txt[0:each2.span()[0]] + tmp_txt[each2.span()[1]:])  \n    \n    except:\n        print(\"Error in : \"+each+\" - \"+str(ij))\n        continue\n    ij+=1\n    row_lst.append(emp_dict)\n    \n#print(row_lst)\ndf = pd.DataFrame(row_lst,columns=col_lst2)\n","aa28a866":"df.to_csv('conv_from_files.csv')\ndf.head()","03d2ad89":"print(df['position'].head())","4e9cf174":"#Removing \\t & \\n from differnt columns\ncol_to_be_clean = ['position','ANNUAL SALARY','DUTIES','REQUIREMENTS','PROCESS','WHERE TO APPLY','APPLICATION DEADLINE','SELECTION','NOTE','NOTICE']\nfor each in col_to_be_clean:\n    regex = '(\\n|\\t)+'\n    df[each] = clean_with_regex(df,each,regex)\n    ","a6c2e717":"df['position'].head()","9a441a97":"df.head()","b3ed7d9e":"#Split annual income from text to numbers\ncol = 'ANNUAL SALARY'\nregex_whole = '\\$[0-9,]+ to \\$[0-9,]+'\ndf['low_end_sal'] = -999\ndf['high_end_sal'] = -1\n\nfor idx,each in enumerate(df[col]):\n    #print(each)\n    if (type(each) is str):\n        for section in re.finditer(regex_whole, each):\n            s = section.start()\n            e = section.end()\n            \n            each3 = each[s:e].split(' to ')\n            \n            regex_to_be_rem = '[$,]*'\n            tmp_low_end = re.sub(regex_to_be_rem,'' , each3[0])\n            tmp_high_end = re.sub(regex_to_be_rem, '', each3[1])\n            \n            df.iloc[idx,df.columns.get_loc('low_end_sal')] = int(tmp_low_end)\n            df.iloc[idx,df.columns.get_loc('high_end_sal')] = int(tmp_high_end)\n            break\n       \n    else:\n        continue","86b13b7b":"df.head()","3b26cc94":"df.isna().sum()","5371ebed":"df2 = pd.DataFrame(columns = [x for x in range(0,len(embed_model['example']))])\n\n\nfor idx,each in enumerate(df['position']):\n    df2.loc[idx] = np.zeros(len(embed_model['example']))\n    for each2 in each.split(' '): \n        try:\n            df2.loc[idx] += embed_model[each2.lower()]\n        except:\n            continue\n    \nfrom minisom import MiniSom    \nsom = MiniSom(20, 20, 300, sigma=1.0, learning_rate=0.5) # initialization of 6x6 SOM\nsom.train_random(df2.values, 100) # trains the SOM with 100 iterations","4fe79463":"X = df2.values\n\nfrom pylab import bone, pcolor, colorbar, plot, show\nbone()\npcolor(som.distance_map().T)\ncolorbar()\nmarkers = ['o', 's']\ncolors = ['r', 'g']\nfor i, x in enumerate(X):\n    w = som.winner(x)\n    plot(w[0] + 0.5,\n         w[1] + 0.5,\n         markerfacecolor = 'None',\n         markersize = 10,\n         markeredgewidth = 2)\nshow()\n","7e8efe28":"df['vec'] = \n\nfor idx,each in enumerate(df['position']):\n    for each2 in each.split(' '):\n        df.iloc[idx,df.columns.get_loc('vec')] += embed_model[each2]\n    break\n    \n    \n# Determine where the visualization will be rendered\noutput_notebook()  # Render inline in a Jupyter Notebook\n\n# Set up the figure(s)\nfig = figure()  # Instantiate a figure() object\n\n# Connect to and draw the data\n\n-----xxxx------\ndot = figure(title=\"Categorical Dot Plot\", tools=\"\", toolbar_location=None,\n            y_range=factors, x_range=[0,100])\n\ndot.segment(0, factors, x, factors, line_width=2, line_color=\"green\", )\ndot.circle(x, factors, size=15, fill_color=\"orange\", line_color=\"green\", line_width=3, )\n\n-----xxxx------\n# Organize the layout\n\n# Preview and save \nshow(fig)","83f6f737":"col_name = 'DUTIES'\ndf = remove_stop_words(df,col_name)","ce7dd49d":"- Function for getting TF-IDF matrix","51387432":"### Step 2: Using Regex to extract data to a csv file.\n- Here every row in CSV file is a text file and regular expression is used to extract the data. Every section is in the format - \n\n   - [UPPERCASE TITLE][:] [\\n - Newline Character] - (Colon may or may not be present *)\n   - [New Line]\n   - [Text] [\\n - Newline Character] - (Content of section)\n   - [New Line]","ab4fa043":"- Function to remove stop words","87bf290b":"# Functions ","2c600e0e":"# Try to group all Job Titles and make salary visualization","379510a7":"# Next try to check using 'Duties' section how different jobs are:","44a9dccc":"## Step 1: Conversion of text files to csv\n#### We converted traditional text files to csv file, extracting the common pattern. We extracted bunch of fields such as *Salary, Duty, Requirements, where to Apply, Deadline, etc.* The extraction was done with regular expression and not AI or Machine Learning. Although doing it through Machine Learning would be fun too.","4404deb5":"# Trying Self Organizing Maps","4c9e86f0":"*Conversion and basic sectioning of text files end here.*","348fafcb":"- Function for applying LSA","213a4f54":"## This is how the csv file looks.\n- The file is not at all cleaned and as you can see has newline characters.","fef4a602":"# What have we done?","75946347":"# Number of NaN in columns \n- Meaning entries where regex did not do a good job ","db07d884":"- Function to delete special regex. Eg - special chars in text or $, in numbers.","7f26fa9e":"# What were the next steps? (Incomplete from here on)\n### Next we try to use the pre-trained word2vec wikipedia model to obtain aggregate summation vector of job titles. Thus titles such as Data Scienist and Data Engineer will be relatively closer. Next we use Self Organizing Map to create a map which will show all the relevant job titles close. After that we can use this map to do -\n1. Create a radio visualization which could allow us to select one job title and show us job titles which are similar, which require similar skill and pay more.\n2. To create a radio visualization where iterating through all jobs, one job title at a time will allow us to decide the similarity (distance from other) of job titles from real world pre-trained word2vec model. Using this similariy\/distance to decide whether the job description\/duty needs to be changed or re-designed. For eg - if Data Scientist and Data Engineer would have less distance, from the Word2Vec embedding model, so by using TF-IDF and LSA (Latent Semantic Analysis) on Job Duties and try to get the distance between them and depending on the distance between duties and distance from Self Organising Maps re-design one or both of them.","ec47056c":"### Step 1: Visualizing text file (If you've seen the file a lot of times please do skip)","85b56760":"- Now the titles look much cleaner.","34834fe6":"### Step 0 - Importing all libraries defining useful functions\n- Importing Libraries","c6e0f74c":"### Next we convert the text salary to actual numbers.\n- 2 Additional columns for lower bound and upper bound"}}