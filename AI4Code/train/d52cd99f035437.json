{"cell_type":{"057e5f6b":"code","427a85aa":"code","ef36bd3b":"code","6b1393b3":"markdown"},"source":{"057e5f6b":"# %%bash\n# pip install 'kaggle-environments>=0.1.6'","427a85aa":"%%writefile agent.py\n\nimport random\nimport pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom joblib import Parallel, delayed\nfrom sklearn.base import BaseEstimator\nfrom datetime import datetime\nfrom typing import List, Tuple, Optional\n\nnp.seterr(divide='ignore', invalid='ignore')\n\n\nclass _ChainBase(BaseEstimator):\n    \"\"\"\n    Base class for chain processing and estimation.\n\n\n    Parameters\n    ----------\n\n    order: int\n        Number of lags of the model.\n    \"\"\"\n\n    def __init__(self, order: int = None, values: List = None) -> None:\n        self.log_likelihood = None\n        self.aic = None\n        self.bic = None\n        self._n_parameters = None\n        self.samples = None\n        self._n_dimensions = None\n        self.order = order\n        self._transition_matrix = None\n        self.transition_matrices = None\n        self.lambdas = None\n        self._indexes = None\n        self.values = values\n        self.expanded_matrix = None\n\n    def _create_indexes(self) -> None:\n        idx_gen = product(range(self._n_dimensions), repeat=self.order + 1)\n        self._indexes = [i for i in idx_gen]\n\n    @property\n    def transition_matrix(self) -> np.ndarray:\n        return self._transition_matrix\n\n    @transition_matrix.setter\n    def transition_matrix(self, new_transition_matrix: np.ndarray) -> None:\n        self._transition_matrix = new_transition_matrix\n\n    def _calculate_dimensions(self, array: np.ndarray) -> None:\n        max_value = array.max()\n        min_value = array.min()\n        n_dim = np.unique(array).shape[0]\n        if min_value != 0:\n            raise ValueError('Lowest label should be equal to zero')\n        if max_value + 1 != n_dim:\n            raise ValueError('Highest label should be equal to number of unique labels minus one')\n        self._n_dimensions = n_dim\n\n    def _aggregate_chain(self,\n                         x: np.ndarray,\n                         sample_weight: Optional[np.ndarray] = None) -> np.ndarray:\n\n        if sample_weight is None:\n            sample_weight = np.ones(x.shape[0], dtype=np.int)\n\n        n_columns = x.shape[1]\n        values_dict = {i: 0 for i in range(self._n_dimensions ** (x.shape[1]))}\n        idx = [self._n_dimensions ** i for i in range(n_columns)]\n\n        idx = np.array(idx[::-1])\n        data_indexes = np.dot(x, idx)\n\n        for n, index in enumerate(data_indexes):\n            values_dict[index] += sample_weight[n]\n\n        return np.array(list(values_dict.values()))\n\n    def _calculate_aic(self) -> None:\n\n        self.aic = -2 * self.log_likelihood + 2 * self._n_parameters\n\n    def _calculate_bic(self) -> None:\n\n        self.bic = -2 * self.log_likelihood + np.log(self.samples) * self._n_parameters\n\n    def predict_proba(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Probability estimates.\n\n        The returned estimates for all states are ordered by the\n        label of state.\n        :param x: NumPy array of shape (n_samples, order)\n        :return:  NumPy array of shape (n_samples, _n_dimensions)\n        \"\"\"\n\n        if self.order == 0:\n            x = np.zeros((x.shape[0], 1), dtype=int)\n\n        idx = [self._n_dimensions ** i for i in range(x.shape[1])]\n\n        idx = np.array(idx[::-1])\n        indexes = np.dot(x, idx)\n\n        return self.transition_matrix[indexes, :]\n\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict state.\n\n        :param x: NumPy array of shape (n_samples, order)\n        :return:  NumPy array of shape (n_samples,)\n        \"\"\"\n\n        prob = self.predict_proba(x)\n\n        return prob.argmax(axis=1)\n\n    def _calculate_log_likelihood(self, transition_matrix_num: np.ndarray) -> None:\n        logs = np.nan_to_num(np.log(self.transition_matrix), nan=0.0)\n        self.log_likelihood = (transition_matrix_num * logs).sum()\n\n    def _create_transition_matrix(self,\n                                  x: np.ndarray,\n                                  y: np.ndarray,\n                                  sample_weight: np.ndarray) -> np.ndarray:\n        x = np.hstack([x, y.reshape(-1, 1)])\n        self._calculate_dimensions(x)\n        transition_matrix = self._aggregate_chain(x, sample_weight)\n        transition_matrix_num = transition_matrix.reshape(-1, self._n_dimensions)\n        self.transition_matrix = transition_matrix_num \/ transition_matrix_num.sum(1).reshape(-1, 1)\n        return transition_matrix_num\n\n    def _check_and_reshape_input(self, x: np.ndarray) -> np.ndarray:\n        if x.shape[1] > self.order:\n            print(f'WARNING: The input has too many columns. Expected: {self.order}, got: {x.shape[1]}. '\n                  f'The columns were trimmed.')\n            x = x[:, -self.order:]\n        if x.shape[1] < self.order:\n            raise ValueError(f'WARNING: The input has less columns than order. Expected: {self.order}, '\n                             f'got: {x.shape[1]}.')\n        return x\n\n    def _create_markov(self) -> None:\n\n        array_coords = product(range(self._n_dimensions), repeat=self.order)\n\n        transition_matrix_list = []\n        for idx in array_coords:\n            t_matrix_part = np.array([self.transition_matrices[i, idx[i], :] for i in range(self.order)]).T\n            transition_matrix_list.append(np.dot(t_matrix_part,\n                                                 self.lambdas))\n        self.transition_matrix = np.array(transition_matrix_list)\n\n    def predict_random(self, x: np.ndarray) -> List:\n        \"\"\"\n        Return state sampled from probability distribution from transition matrix. Used primarily for data generation.\n\n        :param x: NumPy array of shape (n_samples, order)\n        :return:  NumPy array of shape (n_samples,)\n        \"\"\"\n        prob = self.predict_proba(x)\n        x_new = [np.random.choice(self.values, p=i) for i in prob]\n        return x_new\n\n    def create_expanded_matrix(self) -> None:\n        \"\"\"\n        Transforms transition matrix into first order transition matrix.\n        See 1.1 in The Mixture Transition Distribution Model for High-Order Markov Chains and Non-Gaussian Time Series.\n\n        :return: self\n        \"\"\"\n\n        if self.order > 1:\n            idx_gen = product(range(self._n_dimensions), repeat=self.order)\n            idx = [i for i in idx_gen]\n\n            self.expanded_matrix = np.zeros((len(idx), len(idx)))\n            for i, row in enumerate(idx):\n                for j, col in enumerate(idx):\n                    if row[-(self.order - 1):] == col[:(self.order - 1)]:\n                        self.expanded_matrix[i, j] = self.transition_matrix[i, j % self._n_dimensions]\n\n        else:\n            self.expanded_matrix = self.transition_matrix.copy()\n\n\nclass MTD(_ChainBase):\n    \"\"\"\n    Mixture Transition Distribution (MTD) model with separate transition matrices for each lag.\n\n\n    Parameters\n    ----------\n\n    order: int\n        Number of lags of the model.\n\n    number_of_initiations: int, optional (default=10)\n        Number of parameters sets to be initiated. 1 is minimum.\n\n    max_iter: int, optional (default=100)\n        Maximum number of iterations for the EM algorithm.\n\n    min_gain: float, optional (default=0.1)\n        Minimum change of the log-likelihood function value for a step in the EM optimization algorithm.\n\n    lambdas_init: NumPy array, optional (default=None)\n        Starting value for lambdas.\n\n    transition_matrices_init: NumPy array, optional (default=None)\n        Starting value for transition_matrices.\n\n    verbose: int, optional (default=1)\n        Controls the verbosity when fitting and predicting.\n\n    n_jobs: int, optional (default=-1)\n        Number of threads to be used for estimation. Every initiation set can be estimated on one thread only.\n\n\n    Attributes\n    ----------\n\n    _n_dimensions: int\n        Number of states of the process.\n\n    _n_parameters: int\n        Number of independent parameters of the model following [1] section 2\n\n    lambdas: NumPy array\n        Weights vector\n\n    transition_matrices: NumPy array\n        Transition matrices of the model\n\n    transition_matrix: NumPy array\n        Reconstructed Markov Chain transition matrix\n\n    log_likelihood: float\n        Log-likelihood the the MTD model\n\n    aic: float\n        Value of the Akaike's Information Criterion (AIC)\n\n    bic: float\n        Value of the Bayesian Information Criterion (BIC)\n\n    Example\n    ----------\n    import numpy as np\n    from mtdlearn.mtd import MTD\n\n    np.random.seed(42)\n\n    _n_dimensions = 3\n    order = 2\n\n    m = MTD(_n_dimensions, order, n_jobs=-1)\n\n    x = np.array([[0, 0],\n                  [1, 1],\n                  [2, 2],\n                  [0, 1],\n                  [2, 1],\n                  [2, 0],\n                  [0, 1],\n                  [2, 1],\n                  [1, 1],\n                  [1, 0]])\n    y = np.array([0, 0, 2, 1, 1, 2, 0, 1, 2, 1])\n\n    m.fit(x, y)\n\n    x = np.array([[0, 0],\n                  [1, 1],\n                  [2, 2]])\n\n    m.predict_proba(x)\n\n    m.predict(x)\n\n    References\n    ----------\n    -- [1] S. L\u00e8bre, P Bourguinon \"An EM algorithm for estimation in the Mixture Transition Distribution model\", 2008\n\n    \"\"\"\n\n    def __init__(self,\n                 order: int,\n                 number_of_initiations: int = 10,\n                 max_iter: int = 100,\n                 min_gain: float = 0.1,\n                 lambdas_init: Optional[np.ndarray] = None,\n                 transition_matrices_init: Optional[np.ndarray] = None,\n                 verbose: np.int = 1,\n                 n_jobs: int = -1) -> None:\n\n        super().__init__(order)\n        self.number_of_initiations = number_of_initiations\n        self.lambdas_init = lambdas_init\n        self.transition_matrices_init = transition_matrices_init\n        self.max_iter = max_iter\n        self.min_gain = min_gain\n        self.verbose = verbose\n        self.n_jobs = n_jobs\n\n    def fit(self,\n            x: np.ndarray,\n            y: np.ndarray,\n            sample_weight: Optional[np.ndarray] = None) -> None:\n        \"\"\"\n        Fit MTD model.\n\n        Note that the labels (for combined x and y) has to start from zero and be consecutive.\n\n        :param x: NumPy array of shape (n_samples, order)\n                  Training data\n        :param y: NumPy array of shape (n_samples,)\n                  Target values\n        :param sample_weight: NumPy array of shape (n_samples,), default=None\n                              Individual weights for each sample\n        :return: self\n        \"\"\"\n\n        if sample_weight is not None:\n            self.samples = sample_weight.sum()\n        else:\n            self.samples = y.shape[0]\n\n        x = self._check_and_reshape_input(x)\n        x = np.hstack([x, y.reshape(-1, 1)])\n        self._calculate_dimensions(x)\n        x = self._aggregate_chain(x, sample_weight)\n\n        self._create_indexes()\n\n        n_direct = [x.reshape(-1,\n                              self._n_dimensions,\n                              self._n_dimensions ** (self.order - i - 1),\n                              self._n_dimensions).sum(0).sum(1) for i in range(self.order)]\n        n_direct = np.array(n_direct)\n\n        candidates = Parallel(n_jobs=self.n_jobs)(delayed(MTD._fit_one)(x,\n                                                                        self._indexes,\n                                                                        self.order,\n                                                                        self._n_dimensions,\n                                                                        self.min_gain,\n                                                                        self.max_iter,\n                                                                        self.verbose,\n                                                                        n_direct,\n                                                                        self.lambdas_init,\n                                                                        self.transition_matrices_init)\n                                                  for _ in range(self.number_of_initiations))\n\n        self.log_likelihood, self.lambdas, self.transition_matrices = self._select_the_best_candidate(candidates)\n\n        if self.verbose > 0:\n            print(f'log-likelihood value: {self.log_likelihood}')\n\n        self._create_markov()\n        self._n_parameters = (1 + self.order * (self._n_dimensions - 1)) * (self._n_dimensions - 1)\n        self._calculate_aic()\n        self._calculate_bic()\n\n    @staticmethod\n    def _fit_one(x: np.ndarray,\n                 indexes: List[Tuple[int]],\n                 order: int,\n                 n_dimensions: int,\n                 min_gain: float,\n                 max_iter: int,\n                 verbose: int,\n                 n_direct: np.ndarray,\n                 lambdas: Optional[np.ndarray] = None,\n                 transition_matrices: Optional[np.ndarray] = None) -> Tuple[float, np.ndarray, np.ndarray]:\n\n        if lambdas is None:\n            lambdas = np.random.rand(order)\n            lambdas = lambdas \/ lambdas.sum()\n        if transition_matrices is None:\n            transition_matrices = np.random.rand(order, n_dimensions, n_dimensions)\n            transition_matrices = transition_matrices \/ transition_matrices.sum(2).reshape(order, n_dimensions, 1)\n\n        iteration = 0\n        gain = min_gain * 2\n        log_likelihood = MTD._calculate_log_likelihood_mtd(indexes,\n                                                           x,\n                                                           transition_matrices,\n                                                           lambdas)\n        while iteration < max_iter and gain > min_gain:\n            old_ll = log_likelihood\n            p_expectation, p_expectation_direct = MTD._expectation_step(n_dimensions,\n                                                                        order,\n                                                                        indexes,\n                                                                        transition_matrices,\n                                                                        lambdas)\n            lambdas, transition_matrices = MTD._maximization_step(n_dimensions,\n                                                                  order,\n                                                                  x,\n                                                                  n_direct,\n                                                                  p_expectation,\n                                                                  p_expectation_direct)\n            log_likelihood = MTD._calculate_log_likelihood_mtd(indexes,\n                                                               x,\n                                                               transition_matrices,\n                                                               lambdas)\n            gain = log_likelihood - old_ll\n            iteration += 1\n\n            if verbose > 1:\n                current_time = datetime.now()\n                print(f'{current_time.time()} iteration: {iteration}  '\n                      f'gain: {round(gain, 5)} ll_value: {round(log_likelihood, 5)}')\n\n        if iteration == max_iter:\n            print('WARNING: The model has not converged. Consider increasing the max_iter parameter.')\n\n        if verbose > 0:\n            print(f\"log-likelihood value: {log_likelihood}\")\n\n        return log_likelihood, lambdas, transition_matrices\n\n    @staticmethod\n    def _calculate_log_likelihood_mtd(indexes: List[Tuple[int]],\n                                      n_occurrence: np.ndarray,\n                                      transition_matrices: np.ndarray,\n                                      lambdas: np.ndarray) -> float:\n\n        log_likelihood = 0\n\n        for i, idx in enumerate(indexes):\n            if n_occurrence[i] > 0:\n                mtd_value = sum([lam * transition_matrices[i, idx[i], idx[-1]] for i, lam in enumerate(lambdas)])\n                log_likelihood += n_occurrence[i] * np.log(mtd_value)\n\n        return log_likelihood\n\n    @staticmethod\n    def _expectation_step(n_dimensions: int,\n                          order: int,\n                          indexes: List[Tuple[int]],\n                          transition_matrices: np.ndarray,\n                          lambdas: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n\n        p_expectation = []\n        for i in range(n_dimensions):\n            parts = product(*transition_matrices[:, :, i].tolist())\n            p_expectation.append(np.array([i for i in parts]))\n\n        p_expectation = np.hstack(p_expectation).reshape(-1, order) * lambdas\n\n        p_expectation = p_expectation \/ p_expectation.sum(axis=1).reshape(-1, 1)\n        p_expectation = np.nan_to_num(p_expectation, nan=1. \/ order)\n\n        p_expectation_direct = [p_expectation[:, i].reshape(-1,\n                                                            n_dimensions,\n                                                            n_dimensions ** (order - i - 1),\n                                                            n_dimensions).sum(0).sum(1) for i in range(order)]\n\n        p_expectation_direct = np.array(p_expectation_direct)\n        p_expectation_direct = p_expectation_direct \/ p_expectation_direct.sum(axis=0)\n\n        return p_expectation, p_expectation_direct\n\n    @staticmethod\n    def _maximization_step(n_dimensions: int,\n                           order: int,\n                           n_occurrence: np.ndarray,\n                           n_direct: np.ndarray,\n                           p_expectation: np.ndarray,\n                           p_expectation_direct: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n\n        denominator = 1 \/ sum(n_occurrence)\n        sum_part = (p_expectation * n_occurrence.reshape(-1, 1)).sum(0)\n        lambdas = denominator * sum_part\n\n        transition_matrices = n_direct * p_expectation_direct\n        transition_matrices = transition_matrices \/ transition_matrices.sum(2).reshape(order, n_dimensions, 1)\n\n        return lambdas, transition_matrices\n\n    @staticmethod\n    def _select_the_best_candidate(candidates: List) -> Tuple[float, np.ndarray, np.ndarray]:\n\n        log_likelihood = candidates[0][0]\n        lambdas = candidates[0][1]\n        transition_matrices = candidates[0][2]\n\n        for c in candidates[1:]:\n            if c[0] > log_likelihood:\n                log_likelihood = c[0]\n                lambdas = c[1]\n                transition_matrices = c[2]\n\n        return log_likelihood, lambdas, transition_matrices\n\n\nclass MarkovChain(_ChainBase):\n    \"\"\"\n    Markov Chain model.\n\n\n    Parameters\n    ----------\n\n    order: int\n        Number of lags of the model.\n\n    verbose: int, optional (default=1)\n        Controls the verbosity when fitting and predicting.\n\n\n    Attributes\n    ----------\n\n    _n_dimensions: int\n        Number of states of the process.\n\n    _n_parameters: int\n        Number of independent parameters of the model\n\n    transition_matrix: NumPy array\n        Markov Chain transition matrix\n\n    log_likelihood: float\n        Log-likelihood the the Markov Chain model\n\n    aic: float\n        Value of the Akaike's Information Criterion (AIC)\n\n    bic: float\n        Value of the Bayesian Information Criterion (BIC)\n    \"\"\"\n\n    def __init__(self, order: int, verbose: int = 1) -> None:\n\n        super().__init__(order)\n        self.verbose = verbose\n\n    def fit(self,\n            x: np.ndarray,\n            y: np.ndarray,\n            sample_weight: Optional[np.ndarray] = None) -> None:\n        \"\"\"\n        Fit Markov Chain model.\n\n        :param x: NumPy array of shape (n_samples, order)\n                  Training data\n        :param y: NumPy array of shape (n_samples,)\n                  Target values\n        :param sample_weight: NumPy array of shape (n_samples,), default=None\n                              Individual weights for each sample\n        :return: self\n        \"\"\"\n\n        if sample_weight is not None:\n            self.samples = sample_weight.sum()\n        else:\n            self.samples = y.shape[0]\n\n        x = self._check_and_reshape_input(x)\n        self._n_dimensions = np.unique(np.hstack([x, y.reshape(-1, 1)])).shape[0]\n        self._n_parameters = (self._n_dimensions ** self.order) * (self._n_dimensions - 1)\n        self._create_indexes()\n\n        transition_matrix_num = self._create_transition_matrix(x, y, sample_weight)\n\n        self._calculate_log_likelihood(transition_matrix_num)\n        self._calculate_aic()\n        self._calculate_bic()\n\n        if self.verbose > 0:\n            print(f'log-likelihood value: {self.log_likelihood}')\n\n\nhist = []\nbeat = {0: [0, 1, 1], 1: [1, 2, 2], 2: [2, 0, 0]}\nlag = 3\noutput = 0\n\n\ndef run_agent(observation, configuration):\n\n    global hist, output\n    \n    if observation.step == 0:\n        output = random.choice([0, 1, 2])\n        \n    else:\n        hist.append(observation['lastOpponentAction'])\n        if observation.step < 100:\n            output = random.choice([0, 1, 2])\n        else:\n            hist = hist[-200:]\n            x = pd.Series(hist).to_frame()\n            x.columns = ['x']\n\n            for i in range(lag * 2):\n                x['col{}'.format(i)] = x.x.shift(i)\n\n            x.dropna(inplace=True)\n            x = x.astype(int)\n            columns = x.columns[::-1]\n            x = x[columns]\n            x = x.iloc[::2, :]\n\n            y = x.iloc[:, -1].values.reshape(-1, 1)\n            x = x.iloc[:, :-1].values\n\n            model = MTD(order=lag * 2, \n                        n_jobs=1, \n                        verbose=0, \n                        number_of_initiations=4, #this seems like a safe bet it Kaggle RPS competition\n                        min_gain=0.01)\n            model.fit(x, y)\n            print()\n            pred = model.predict(x[-1].reshape(1, -1))\n            output = random.choice(beat[pred[0]])\n    hist.append(output)\n    return output","ef36bd3b":"# from kaggle_environments import evaluate, make\n# env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)\n# env.reset()\n# len(env.run(['agent.py', 'agent.py']))","6b1393b3":"# Mixture Transition Distribution\n\nThis notebook aims to show you how to use Mixture Transition Distribution model in this competition. You can find some more details in the [discussion](https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/200200)."}}