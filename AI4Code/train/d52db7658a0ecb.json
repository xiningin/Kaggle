{"cell_type":{"7751bce0":"code","da77e75b":"code","f18382a8":"code","a4d81f33":"code","3c8673fe":"code","9a361b41":"code","5f04924e":"code","831393e9":"code","29062e45":"code","d92bfb1e":"code","8f8b5178":"code","886b46c0":"code","466111be":"code","3e7d57da":"code","896ddc66":"code","5afbffa6":"code","b7ea7c02":"code","8f2acbd8":"code","b696ef14":"code","d622c635":"code","73ebb9be":"code","5ae1e1c9":"code","66b3da48":"code","5a97b654":"code","0cc23448":"code","e0827e16":"code","1df2bd3a":"code","756dc758":"code","08755660":"code","964f807f":"code","889cc47e":"code","95d80218":"code","95c009e9":"code","6bf6d472":"code","00ad4a60":"code","bc4c2d64":"code","0bbc2a76":"code","fb801e63":"code","02cdbbea":"code","5ba59cad":"code","c920ea8e":"code","0c823b25":"code","58962c96":"code","8a84079b":"markdown","00909ddc":"markdown","ab7654df":"markdown","38ded982":"markdown"},"source":{"7751bce0":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom collections import OrderedDict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport json\nimport pickle\nimport sys\nimport math\nimport gc\n\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\nimport os\nprint(os.listdir(\"..\/input\"))","da77e75b":"def load(csv_path, nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows,)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","f18382a8":"train = load('..\/input\/train.csv')\ntest = load('..\/input\/test.csv')","a4d81f33":"def to_numeric(data):\n    for feature in data.dtypes[data.dtypes == 'object'].index: \n        data[feature] = pd.to_numeric(data[feature],errors='ignore')","3c8673fe":"to_numeric(train)\nto_numeric(test)","9a361b41":"# Customers present in train and test\nrepeat_clients = set(train.fullVisitorId) & set(test.fullVisitorId)\nlen(set(train.fullVisitorId)), len(repeat_clients)","5f04924e":"train['isBuy'] = train['totals.transactionRevenue'] > 0 \ntrain['isBuy'].mean(), train[train.fullVisitorId.isin(repeat_clients)].isBuy.mean()","831393e9":"# Drop columns with constant value \nconst_columns = [column for column in train.columns if len(train[column].value_counts(dropna=False)) == 1]\ntrain.drop(columns=const_columns, axis=1, inplace=True)\ntest.drop(columns=const_columns, axis=1, inplace=True)","29062e45":"# Drop columns containing only in train\nset(train.columns) ^ set(test.columns)","d92bfb1e":"del train['trafficSource.campaignCode']","8f8b5178":"def get_time_feature(df):\n    df.date = pd.to_datetime(df.date, format='%Y%m%d')\n    df.visitStartTime = pd.to_datetime(df.visitStartTime, unit='s')\n    df['hour'] = df.visitStartTime.dt.hour\n    df['dayofweek'] = df.visitStartTime.dt.dayofweek\n    df['weekofyear'] = df.visitStartTime.dt.weekofyear\n    df['month'] = df.visitStartTime.dt.month","886b46c0":"get_time_feature(train)\nget_time_feature(test)","466111be":"class ThrColumnEncoder:\n    \"\"\"\n    The threshold label encoder. \n    To avoid overfitting we can combine rare values into one group. Class work with pd.Series.\n    thr: Threshold as a percentage, values whose number is less than the threshold will be replaced by a single label.\n    \"\"\"\n    def __init__(self, thr=0.5):\n        self.thr = thr\n        self.categories = defaultdict(lambda:-1) # Those values that are X_test, but not in X_train will be replaced by -1.\n        \n    def fit(self, x):\n        values = x.value_counts(dropna=False)*100\/len(x)\n        for value, key in enumerate(values[values >= self.thr].index):\n            self.categories[key] = value\n        for value, key in enumerate(values[values < self.thr].index):\n            self.categories[key] = -1 # Rare values replace -1\n            \n    def transform(self, x):   \n        return x.apply(self.categories.get)\n    \n    def fit_transform(self, x):\n        self.fit(x)\n        return self.transform(x)","3e7d57da":"class ThrLabelEncoder:\n    \"\"\"\n    Work with pd.DataFrame.\n    \"\"\"\n    def __init__(self, thr=0.5):\n        self.thr = thr\n        self.column_encoders = {}\n        self.features = None\n        \n    def fit(self, X):\n        self.features = X.columns\n        for feature in self.features:\n            ce = ThrColumnEncoder()\n            ce.fit(X[feature])\n            self.column_encoders[feature] = ce\n            \n    def transform(self, X):\n        X = X.copy()\n        for feature in self.features: \n            ce = self.column_encoders[feature]\n            X.loc[:, feature] = ce.transform(X[feature])\n        return X\n            \n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)","896ddc66":"id_cols = 'fullVisitorId', 'sessionId'\n\ntfidf_cols = 'geoNetwork.networkDomain', 'trafficSource.referralPath', 'trafficSource.source'\n\ncat_cols = list(set(train.dtypes[train.dtypes == 'object'].index) - set(id_cols) - set(tfidf_cols)  |  set(('dayofweek', 'hour', 'month')))","5afbffa6":"class DummyEncoder(ThrLabelEncoder):\n    \"\"\"\n    For each unique value of a categorical feature, we can create our own binary feature.\n    Generally speaking, this transformation is not necessary for gradient boosting.\n    But for this task, I decided to move from the feature description of the visit to the feature description\n    of the client and without the dummy-encoding can not do it.\n    \"\"\"\n    def transform(self, X):\n        result = []\n        for feature in self.features: \n            ce = self.column_encoders[feature]\n            tf_feature = ce.transform(X[feature])\n            popular_values = [value for key, value in self.column_encoders[feature].categories.items() if value != -1]\n            popular_keys = [key for key, value in self.column_encoders[feature].categories.items() if value != -1]\n            columns = ['%s_%s'%(feature, key) for key in popular_keys + ['rare']]\n            feature_dummies = pd.concat([tf_feature == value for value in popular_values] + [tf_feature == -1], axis=1)\n            feature_dummies.columns = columns\n            result.append(feature_dummies)\n        return pd.concat(result, axis=1)","b7ea7c02":"de = DummyEncoder()","8f2acbd8":"train_de = de.fit_transform(train[cat_cols])\ntest_de = de.transform(test[cat_cols])\n\ntrain_de.index = train.fullVisitorId\ntest_de.index = test.fullVisitorId","b696ef14":"def life_time(x):\n    return (x.max() - x.min()).total_seconds()","d622c635":"aggregates = {'totals.pageviews': [sum, min, max, np.mean], \n              'totals.hits': [sum, min, max, np.mean], \n              'date': life_time,\n              'visitNumber': [max, min],\n              'totals.bounces': sum,\n              'totals.transactionRevenue': sum\n             }","73ebb9be":"%time train_gr = train.groupby('fullVisitorId').agg(aggregates)","5ae1e1c9":"def groupby_rename(df):\n    df.columns = ['%s_%s'% (df.columns.levels[0][i],df.columns.levels[1][j]) for i,j in \\\n                  zip(df.columns.labels[0], df.columns.labels[1])]","66b3da48":"groupby_rename(train_gr)","5a97b654":"del aggregates['totals.transactionRevenue']","0cc23448":"%time test_gr = test.groupby('fullVisitorId').agg(aggregates)","e0827e16":"groupby_rename(test_gr)","1df2bd3a":"train_de_sum = train_de.groupby(level=0).sum()\n\ntest_de_sum = test_de.groupby(level=0).sum()\n\nX = pd.concat([train_de_sum, train_gr], axis=1)\nX_test = pd.concat([test_de_sum, test_gr], axis=1)\n\nY = np.log1p(X['totals.transactionRevenue_sum'])\ndel X['totals.transactionRevenue_sum']","756dc758":"del train_de_sum\ndel test_de_sum\ndel train_gr\ndel test_gr\ndel train_de\ndel test_de","08755660":"class TFIDFER:\n    \"\"\"\n    Class encapsulating tfidf transformation and renaming column of pd.Dataframe with using name tfidf-features.\n    \"\"\"\n    def __init__(self, max_df=0.9, min_df=0.01, max_features=100, ngram_range=(1,2)):\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features  = max_features\n        self.ngram_range = ngram_range\n        \n        self.column_encoders = {}\n        self.features = None\n        \n    def fit(self, X): \n        self.features = X.columns\n        for feature in self.features:\n            tfidf = TfidfVectorizer(max_df=self.max_df, min_df=self.min_df, max_features=self.max_features, ngram_range=self.ngram_range)\n            tfidf.fit(X[feature])\n            self.column_encoders[feature] = tfidf\n            \n    def transform(self, X):\n        result = []\n        for feature in self.features: \n            items_tfidf = pd.DataFrame(self.column_encoders[feature].transform(X[feature]).toarray(), X.index)\n            col_names = [word.replace(' ','_') for word, index in sorted(self.column_encoders[feature].vocabulary_.items(), key = lambda x:x[1])]\n            items_tfidf.columns =  ['tfidf_%s_%s'%(feature, name) for name in col_names] \n            result.append(items_tfidf.copy())\n        return pd.concat(result, axis=1)\n            \n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)","964f807f":"def get_tfidf(df):\n    df['geoNetwork.networkDomain'] = df['geoNetwork.networkDomain'].apply(lambda x: x.replace('.', ' ').replace(':', ' ') + ' ')\n    df['trafficSource.source'] = df['trafficSource.source'].apply(lambda x: x.replace('.', ' ').replace(':', ' ') + ' ')\n    df['trafficSource.referralPath'] = df['trafficSource.referralPath'].astype(str).apply(lambda x: x.replace('\/', ' ') + ' ')\n    aggregates = {'geoNetwork.networkDomain': sum, \n                  'trafficSource.source': sum, \n                  'trafficSource.referralPath': sum}\n    return df.groupby('fullVisitorId').agg(aggregates)","889cc47e":"aggregates = {'geoNetwork.networkDomain': sum, \n              'trafficSource.source': sum, \n              'trafficSource.referralPath': sum}","95d80218":"%time train_ftidf_sum = get_tfidf(train)","95c009e9":"%time test_ftidf_sum = get_tfidf(test)","6bf6d472":"tfidfer = TFIDFER()","00ad4a60":"train_tfidf = tfidfer.fit_transform(train_ftidf_sum)\ntest_tfidf = tfidfer.transform(test_ftidf_sum)","bc4c2d64":"X_test = pd.concat([X_test, test_tfidf], axis=1)\nX = pd.concat([X, train_tfidf],axis =1)","0bbc2a76":"del train \ndel test \ngc.collect()","fb801e63":"X_test.head()","02cdbbea":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.33, random_state=0)","5ba59cad":"gbm = lgb.LGBMRegressor(objective = 'regression',  \n                        max_depth = 11,\n                        colsample_bytre = 0.8,\n                        subsample = 0.8, \n                        learning_rate = 0.1,\n                        n_estimators = 300)","c920ea8e":"gbm.fit(X_train, Y_train, \n        eval_set=[(X_valid, Y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=5)","0c823b25":"%matplotlib inline\nlgb.plot_importance(gbm, max_num_features=120, figsize=(10,40))","58962c96":"Y_test = pd.Series(gbm.predict(X_test),index= X_test.index)\n\nY_test[Y_test<0] = 0\n\nY_test.name = \"PredictedLogRevenue\"\n\nY_test.head()","8a84079b":"## 4. Model building","00909ddc":"## 2. Feature generation","ab7654df":"##  3. Aggregation of data to the client level","38ded982":"## 1. Data loading"}}