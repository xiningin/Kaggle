{"cell_type":{"7d1cb144":"code","e708ed8c":"code","4b2af39c":"code","5bb951e5":"code","3d72df2e":"code","28e8ace1":"code","4cd4bde5":"code","6c08495f":"code","85dbe0a3":"code","e4c8b066":"code","9715fbca":"code","935d676d":"code","600e771e":"code","c5a6a43f":"code","73a07c37":"code","4fd181b4":"code","6b97228d":"code","5a41a8b0":"code","380153bb":"code","36111544":"code","384100bd":"code","0ff463bc":"code","a2d348a0":"code","293d8833":"code","943005ee":"code","c9e94184":"code","9d3f8798":"code","081562ff":"code","1b2bf113":"code","7166c8f6":"code","9e7619f3":"code","c17b488a":"code","d14a796c":"code","47a247d7":"code","7f5a4f34":"code","87206985":"code","4da53f6c":"code","6d63b0b0":"code","bd4e4141":"code","42e990c3":"code","0af6bfeb":"code","1dcb9df0":"code","c2dedf49":"code","80b60fe7":"code","b15f144f":"code","9c33b78f":"code","281d00be":"code","84650743":"code","7411b95a":"code","e87b3cdd":"code","dea3d128":"code","30e51124":"code","34ce223c":"code","7895000b":"code","a807a73e":"code","7a434052":"code","7d99f4c5":"code","633c32ae":"code","80532066":"markdown","38b79b28":"markdown","9075baba":"markdown"},"source":{"7d1cb144":"# for hyper parameter search\n!pip install optuna","e708ed8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4b2af39c":"import pyarrow.parquet as pq","5bb951e5":"from pathlib import Path","3d72df2e":"class DataPaths(object):\n    TRAIN_PARQUET_PATH = Path('..\/input\/train.parquet')\n    TRAIN_METADATA_PATH = Path('..\/input\/metadata_train.csv')\n    TEST_PARQUET_PATH = Path('..\/input\/test.parquet')\n    TEST_MATADATA_PATH = Path('..\/input\/metadata_test.csv')","28e8ace1":"train_meta_df = pd.read_csv('..\/input\/metadata_train.csv')","4cd4bde5":"train_meta_df[:10]","6c08495f":"# for debug\n# train_meta_df = train_meta_df.iloc[:600]","85dbe0a3":"train_meta_df.info()","e4c8b066":"train_meta_df.describe()","9715fbca":"from scipy import signal","935d676d":"import pywt","600e771e":"WAVELET_WIDTH = 30","c5a6a43f":"from sklearn.preprocessing import FunctionTransformer","73a07c37":"subset_train = pq","4fd181b4":"class SummaryTransformer(FunctionTransformer):\n    def __init__(self, \n                 kw_args=None, inv_kw_args=None):\n        validate = False\n        inverse_func = None\n        accept_sparse = False\n        pass_y = 'deprecated'\n        super().__init__(self.f, inverse_func, validate, accept_sparse, pass_y, kw_args, inv_kw_args)\n    \n    def f(self, X):\n        avgs = np.mean(X)\n        stds = np.std(X)\n        maxs = np.max(X)\n        mins = np.min(X)\n        medians = np.median(X)\n        return np.array([avgs, stds, maxs, mins, medians])","6b97228d":"class WaevletSummaryTransformer(FunctionTransformer):\n    def __init__(self, wavelet_width,\n                 kw_args=None, inv_kw_args=None):\n        validate = False\n        inverse_func = None\n        accept_sparse = False\n        pass_y = 'deprecated'\n        self.wavelet_width = wavelet_width\n        super().__init__(self.f, inverse_func, validate, accept_sparse, pass_y, kw_args, inv_kw_args)\n    \n    def f(self, X):\n#         wavelets = signal.cwt(X, signal.ricker, np.arange(1, self.wavelet_width + 1))\n        wavelets, _ = pywt.cwt(X, np.arange(1, self.wavelet_width + 1), 'mexh')\n        avgs = np.mean(wavelets, axis=1)\n        stds = np.std(wavelets, axis=1)\n        maxs = np.max(wavelets, axis=1)\n        mins = np.min(wavelets, axis=1)\n        medians = np.median(wavelets, axis=1)\n        return np.concatenate([avgs, stds, maxs, mins, medians])","5a41a8b0":"class SpectrogramSummaryTransformer(FunctionTransformer):\n    def __init__(self, sample_rate, fft_length, stride_length,\n                 kw_args=None, inv_kw_args=None):\n        validate = False\n        inverse_func = None\n        accept_sparse = False\n        pass_y = 'deprecated'\n        self.sample_rate = sample_rate\n        self.fft_length = fft_length\n        self.stride_length = stride_length\n        super().__init__(self.f, inverse_func, validate, accept_sparse, pass_y, kw_args, inv_kw_args)\n    \n    def f(self, X):\n        X = self.to_spectrogram(X)\n        avgs = np.mean(X, axis=1)\n        stds = np.std(X, axis=1)\n        maxs = np.max(X, axis=1)\n        mins = np.min(X, axis=1)\n        medians = np.median(X, axis=1)\n        return np.concatenate([avgs, stds, maxs, mins, medians])\n\n    def to_spectrogram(self, series):\n        f, t, Sxx = signal.spectrogram(series, fs=self.sample_rate, nperseg=self.fft_length,\n                                   noverlap=self.fft_length - self.stride_length, window=\"hanning\", axis=0,\n                                   return_onesided=True, mode=\"magnitude\", scaling=\"density\")\n        return Sxx","380153bb":"from typing import List","36111544":"from sklearn.base import TransformerMixin","384100bd":"train_meta_df.columns","0ff463bc":"def read_column(parquet_path, column_id):\n    return pq.read_pandas(parquet_path, columns=[str(column_id)]).to_pandas()[str(column_id)]","a2d348a0":"import itertools","293d8833":"from tqdm import tqdm_notebook","943005ee":"from multiprocessing.pool import Pool","c9e94184":"class FeatureExtractor(object):\n    def __init__(self, transformers):\n        self.transformers: List[TransformerMixin] = transformers\n        self._parquet_path = None\n        self._meta_df = None\n    \n    def fit(self, parquet_path, meta_df):\n        pass\n    \n    def from_signal(self, parquet_path, signal_id):\n        return [ transformer.transform(read_column(parquet_path, signal_id).values)  \n                                          for transformer in self.transformers]\n    \n    def from_measurement(self, measure_id):\n        temp = np.concatenate(\n            list(itertools.chain.from_iterable(\n                [ self.from_signal(self._parquet_path, signal_id) for signal_id \n                 in self._meta_df[self._meta_df[\"id_measurement\"] == measure_id].signal_id\n                ]\n            ))\n        )\n        return temp\n    \n    def transform(self, parquet_path, meta_df, n_jobs=2):\n        self._parquet_path = parquet_path\n        self._meta_df = meta_df\n        with Pool(n_jobs) as pool:\n            rows = pool.map(self.from_measurement, self._meta_df.id_measurement.unique())\n        return np.vstack(rows)","9d3f8798":"N_MEASUREMENTS = 800000","081562ff":"TOTAL_DURATION = 20e-3","1b2bf113":"sample_rate = N_MEASUREMENTS \/ TOTAL_DURATION","7166c8f6":"# wavelet transform takes too much time\n# extractor = FeatureExtractor([SummaryTransformer(), WaevletSummaryTransformer(WAVELET_WIDTH), SpectrogramSummaryTransformer(\n#     sample_rate= sample_rate, fft_length=200, stride_length=100)])","9e7619f3":"extractor = FeatureExtractor([SummaryTransformer(), SpectrogramSummaryTransformer(\n    sample_rate= sample_rate, fft_length=200, stride_length=100)])","c17b488a":"X = extractor.transform(DataPaths.TRAIN_PARQUET_PATH, train_meta_df, n_jobs=4)","d14a796c":"X.shape","47a247d7":"from sklearn.metrics import matthews_corrcoef","7f5a4f34":"from lightgbm import LGBMClassifier","87206985":"import optuna","4da53f6c":"y = train_meta_df.target[list(range(train_meta_df.signal_id.values[0], \n                                        train_meta_df.signal_id.values[-1], 3))]","6d63b0b0":"RANDOM_STATE=10","bd4e4141":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer","42e990c3":"def objective(trial:optuna.trial.Trial):\n    boosting_type = trial.suggest_categorical(\"boosting_type\", ['gbdt', 'dart'])\n    num_leaves = trial.suggest_int('num_leaves', 30, 80)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 10, 100)\n#     max_depth = trial.suggest_int('max_depth', )\n    lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1e-2)\n    lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-5, 1e-2)\n#     num_iterations = trial.suggest_int(\"num_iterations\", 100, 500)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n    \n    clf = LGBMClassifier(boosting_type=boosting_type, num_leaves=num_leaves, \n                        learning_rate=learning_rate, reg_alpha=lambda_l1, \n                        min_child_samples=min_data_in_leaf,\n                         reg_lambda=lambda_l2, random_state=RANDOM_STATE)\n#     fit_params = {\"early_stopping_rounds\":20, \n#                  \"eval_metric\": matthews_corrcoef}\n    scores = cross_validate(clf, X, y, verbose=1,  \n                  n_jobs=-1, scoring=make_scorer(matthews_corrcoef), cv=5)\n    return - scores[\"test_score\"].mean()\n    ","0af6bfeb":"study = optuna.create_study()","1dcb9df0":"study.optimize(objective, n_trials=10)","c2dedf49":"study.best_params","80b60fe7":"study.best_value","b15f144f":"best_params = study.best_params","9c33b78f":"best_params[\"random_state\"] = RANDOM_STATE","281d00be":"clf = LGBMClassifier(**best_params)","84650743":"clf.fit(X, y, eval_metric=matthews_corrcoef, \n       verbose=1)","7411b95a":"test_meta_df = pd.read_csv(DataPaths.TEST_MATADATA_PATH)","e87b3cdd":"# test_meta_df = test_meta_df.iloc[:15]","dea3d128":"test_meta_df.shape","30e51124":"X = extractor.transform(DataPaths.TEST_PARQUET_PATH, test_meta_df, n_jobs=4)","34ce223c":"predictions = clf.predict(X)","7895000b":"submit_df = pd.DataFrame()","a807a73e":"submit_df[\"signal_id\"] = test_meta_df.signal_id","7a434052":"submit_df[\"target\"] = np.repeat(predictions, 3)","7d99f4c5":"submit_df[:10]","633c32ae":"submit_df.to_csv(\"submission.csv\", index=None)","80532066":"## predict","38b79b28":"# feature extraction","9075baba":"## train model"}}