{"cell_type":{"cef01961":"code","79410f4c":"code","bc88d121":"code","82529e44":"code","ae15432f":"code","dc7bb7e3":"code","c2cb7549":"code","4e411dec":"code","4af2efbd":"code","1ea24ca2":"code","4f2841fa":"code","4426b4bf":"code","2aabf00f":"code","d2be62c4":"code","c5ee912a":"code","70822ff5":"code","a3ab9c35":"code","dcfcd3f8":"code","8016a865":"code","97443200":"code","a3f68d0e":"code","6dd6c2ec":"markdown","e75405b4":"markdown","1130feef":"markdown","5735f672":"markdown","7a068764":"markdown","88f0b180":"markdown","f60d8b93":"markdown","261fbdb8":"markdown","85b7513e":"markdown","018883f1":"markdown","39f88cee":"markdown","febd2a99":"markdown","8e20a643":"markdown","c208370a":"markdown","7047918e":"markdown","6d7893c0":"markdown","571347fb":"markdown"},"source":{"cef01961":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression as LogR\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","79410f4c":"# First load training and test data and remove features believed to be of low improtance\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n# remove unimportant features. \ntrain_name = list(train['Name'])\ntrain = train.drop(columns = ['Name', 'Ticket', 'Cabin'])\n\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_name = list(test['Name'])\ntest = test.drop(columns = ['Name', 'Ticket', 'Cabin'])","bc88d121":"train.isnull().sum()","82529e44":"test.isnull().sum()","ae15432f":"test.corr().Fare","dc7bb7e3":"# replace 0 fare values with NaN, since it does not make sense\ntrain['Fare'] = train['Fare'].map(lambda x: np.nan if x ==0 else x)\ntest['Fare'] = test['Fare'].map(lambda x: np.nan if x ==0 else x)","c2cb7549":"# impute missing Age and Fare data using sklearn's SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\n\ntrain_data = train[['Age','Fare']]\nimp.fit(train_data)\ntrain_data = pd.DataFrame(imp.transform(train_data))\ntrain[['Age','Fare']] = train_data\n\ntest_data = test[['Age','Fare']]\nimp.fit(train_data)\ntest_data = pd.DataFrame(imp.transform(test_data))\ntest[['Age','Fare']] = test_data\n","4e411dec":"# Change Pclass dtype to str\ntrain['Pclass'] = train['Pclass'].astype(str)\ntest['Pclass'] = test['Pclass'].astype(str)\n\ntrain.corr().Survived.sort_values(ascending = False)","4af2efbd":"# categorize Age data into child (0-14), Adult (15-60) and seniors (61+)\ntrain_child = [0]*len(train)\ntrain_adult = [0]*len(train)\ntrain_senior = [0]*len(train)\nfor i in range(len(train)):\n    if train['Age'][i] <= 14:\n        train_child[i] = 1\n    elif train['Age'][i] <= 60:\n        train_adult[i] = 1\n    else:\n        train_senior[i] = 1\ntrain['isChild'] = train_child\ntrain['isAdult'] = train_adult\ntrain['isSenior'] = train_senior\ntrain = train.drop(columns = 'Age')\n\ntest_child = [0]*len(test)\ntest_adult = [0]*len(test)\ntest_senior = [0]*len(test)\nfor i in range(len(test)):\n    if test['Age'][i] <= 14:\n        test_child[i] = 1\n    elif test['Age'][i] <= 60:\n        test_adult[i] = 1\n    else:\n        test_senior[i] = 1\ntest['isChild'] = test_child\ntest['isAdult'] = test_adult\ntest['isSenior'] = test_senior\ntest = test.drop(columns = 'Age')","1ea24ca2":"train.corr().Survived","4f2841fa":"# categorize Fare data into class1 (0-10), class2(10-20), class3(20-30), class4(30+)\ntrain_fare = ['class1']*len(train)\nfor i in range(len(train)):\n    if 10 <= train['Fare'][i] < 20:\n        train_fare[i] = 'class2'\n    elif 20 <= train['Fare'][i] < 30:\n        train_fare[i] = 'class3'\n    elif 30 <= train['Fare'][i]:\n        train_fare[i] = 'class4'\ntrain['Fare'] = train_fare\n\ntest_fare = ['class1']*len(test)\nfor i in range(len(test)):\n    if 10 <= test['Fare'][i] < 20:\n        test_fare[i] = 'class2'\n    elif 20 <= test['Fare'][i] < 30:\n        test_fare[i] = 'class3'\n    elif 30 <= test['Fare'][i]:\n        test_fare[i] = 'class4'\ntest['Fare'] = test_fare\n    ","4426b4bf":"# Assign age group based on title (some children were imputed as adults but can easily be seen to be children based on their titles)\nfor i in range(len(train)):\n    if 'Master.' in train_name[i]:\n        train.set_value(i,'isChild',1)\n        train.set_value(i,'isAdult',0)\n        train.set_value(i,'isSenior',0)\nfor i in range(len(test)):\n    if 'Master.' in test_name[i]:\n        test.set_value(i,'isChild',1)\n        test.set_value(i,'isAdult',0)\n        test.set_value(i,'isSenior',0)","2aabf00f":"# Add family size as a feature to the training and test data\ntrain_famil = [1]*len(train)\nfor i in range(len(train)):\n    train_famil[i] = train_famil[i] + train['Parch'][i] + train['SibSp'][i]\ntrain['family size'] = train_famil\n\ntest_famil = [1]*len(test)\nfor i in range(len(test)):\n    test_famil[i] = test_famil[i] + test['Parch'][i] + test['SibSp'][i]\ntest['family size'] = test_famil","d2be62c4":"\"\"\"\nLogistic Regression using Pclass, Sex, Fare and Age, Pclass, family size, Embarked port as features\n\"\"\"\n\nY_train = train['Survived']\nX_train = train[['isChild','isAdult','isSenior','Fare','Sex','Pclass', 'family size','Embarked']]\nX_test = test[['isChild','isAdult','isSenior','Fare','Sex','Pclass', 'family size','Embarked']]\n\n# Convert Embarked to One-hot-class encoding via the pd.get_dummies method \nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\n\n# fit model\nLog_R_model = LogR(solver = 'lbfgs').fit(X_train,Y_train)\n\nY_test = Log_R_model.predict(X_test)\npassId = np.array(test['PassengerId'])\n\nprediction = pd.DataFrame(data = np.transpose([passId, Y_test]), columns = ['PassengerId','Survived'])\nprediction = prediction.astype('int32')\n\n# write prediction to .csv file\n# This line is commented out since this .csv file is written locally\n# prediction.to_csv('prediction_LR.csv', index = False)\n\n\"\"\"\ntenfold CV using GridsearchCV\n\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n# penalty space\npenalty = ['l1','l2']\n# regularization hyperparameter space\nCspace = np.linspace(0.01,1)\nhyperparameters = dict(C=Cspace, penalty=penalty)\n\n# create new logistic regression model\nLRmodel = LogR() \nclf = GridSearchCV(LRmodel, hyperparameters, cv=10, verbose=False)\n\nbest_LR = clf.fit(X_train, Y_train)\n\nY_test = best_LR.predict(X_test)\n# predict probability of survival being 0\/1\n\nLR_proba = best_LR.predict_proba(X_test)\npassId = np.array(test['PassengerId'])\n\nLR_prediction = pd.DataFrame(data = np.transpose([passId, Y_test]), columns = ['PassengerId','Survived'])\nLR_prediction = LR_prediction.astype('int32')","c5ee912a":"best_LR.score(X_train,Y_train)","70822ff5":"best_LR.best_estimator_","a3ab9c35":"from sklearn.ensemble import GradientBoostingClassifier\nparam_grid = {'max_features':['sqrt',4,5,6,7,8], 'max_depth': [i for i in range(1,7)]}\n\n# create new logistic regression model\nGB_model = GradientBoostingClassifier( random_state = 0) \nGridSearch = GridSearchCV(GB_model, param_grid, cv=10, verbose=False)\n\nbest_GB = GridSearch.fit(X_train, Y_train)\n\n# predict GB probabilities \nGB_proba = best_GB.predict_proba(X_test)\nY_test = best_GB.predict(X_test)\npassId = np.array(test['PassengerId'])\n\nGB_prediction = pd.DataFrame(data = np.transpose([passId, Y_test]), columns = ['PassengerId','Survived'])\nGB_prediction = GB_prediction.astype('int32')","dcfcd3f8":"best_GB.score(X_train,Y_train)","8016a865":"# find predictions where LR and GB models differ:\n\ndiff = []\n\nfor i in range(len(X_test)):\n    if GB_prediction.Survived[i] != LR_prediction.Survived[i]:\n        diff.append(i)\n    \ndisagree = len(diff)\/len(X_test)\n    \n\nLR_disagree_prob = []\nfor i in diff:\n    LR_disagree_prob.append(LR_proba[i])\n\nprint('number of disagreeing instances:')\nprint(len(diff))\n\nprint('percentage of disagreeing instances:')\nprint(round(len(diff)\/len(X_test)*100,2))","97443200":"print('probabilities associated with differing predictions using LR model:')\nLR_disagree_prob","a3f68d0e":"# make table of final prediction of blended model\nfinal_prediction = GB_prediction.copy()\nfor i in range(len(X_test)):\n    if i in diff:\n        if LR_disagree_prob[diff.index(i)][1] > 0.75:\n            final_prediction.Survived[i] = 1\n        elif LR_disagree_prob[diff.index(i)][1] < 0.25:\n            final_prediction.Survived[i] = 0\n        else:\n            continue\n# write prediction to .csv file\nfinal_prediction.to_csv('prediction_Blended.csv', index = False)","6dd6c2ec":"We note there there are several features missing data in the training and test set - age, embarked and fare.\n\n**Embarked** - From the data, we can see that most of the passengers embarked from port 'S'. Hence the missing embarked data shall be imputed as 'S' simply because of the overwhelmingly large number of observations.\n\n**Age** - With such a large number (177) of missing ages, it is not advised to simply discard the 177 training data points. It will be imputed using its median value with respect to fare, since fare has the largest correlation with age in the test data set.\n\n**Fare** - Missing fare values will will be imputed based on age, since it has the largest correlation with age. \n","e75405b4":"We then note an interesting observation in the training dataset: despite the women and children first policy, the correlations between a passenger's survival is only 0.06 - compared to 0.255 for fare value - representative of the passenger class. What is more unreasonable, is that the value of age seems negatively correlated with survival - suggesting that younger passengers are somehow more likely to perish!\n\nThis could of course be partially explained by the fact that there are always dependencies between variables. Another factor that we seek to address would be that different passengers were treated based on their age group - child, adult, senior etc. instead of their actual age during the chaos of evacuation!\n\nTherefore it would be sensible to categorize the age data along these lines:","1130feef":"Some further data cleaning by changing the Pclass dataype to string:","5735f672":"The training accuracy for the final logistic regression model is **0.803**, using **L2 norm**. The test accuracy of this model is **0.7703**.","7a068764":"We initially notice that there are a certain number of rows in the training and test data that have missing age\n","88f0b180":"# 2) Gradient Boosted Decision Tree #\n\nGradient boosting is an ensemble methond that has become popular in recent years for the simplicity of decision trees and its high accuracy comparable to other classifiers. The reason I have chosen decision tree over say, LDA or SVMs is that 1)LDA makes several assumptions (Gaussianity of variable distribution, equal variance) which is simply not true for some of the features (fare amount, for example, is skewed heavily towards lower values). 2) SVMs are similar to logistic regressions in their behaviors. \n\nSince the data has already been cleaned, we directly perform 10-fold CV on gradient-boosted decision trees with varying *max_features* and *depth*:","f60d8b93":"# 3) Blended Model #\n\nSometimes it may be advantageous to blend models together to overcome their limits. Regression and decision trees both have their limitations but could perhaps combined together, the could help each model resolve some of these issuses. While gradient boosting is not very interpretable, the probability results from logistic regression are quite straightforward to understand. We first output the instances where LR model and GB models differ in their predictions of the test data, and look at the associated probabilities of survival for the logistic regression model:","261fbdb8":"The fare data, seen before, is highly correlated with survival. This can be explained with the intuition that people affording higher fares get placed into higher passenger classes, increasing their chances of survival. We again categorize fares into 4 categories: class1 (0-10), class2(10-20), class3(20-30), class4(30+). \n\nNote that 10 pounds in 1912 is worth 1133 pounds in 1912 (http:\/\/www.in2013dollars.com\/uk\/inflation\/1912?amount=10) and if we consider a comparable trip today,such as a flight from London to New York, a 2000 pound ticket would land someone in a comfortable business class seat!","85b7513e":"# 1) Logistic Regression #\n\nLogistic regression is an interpreble classifier good for binary data classification. We will attempt to use this model first to set a basis for the other less interpreble models. 10-fold cross-validation is used, with L1 and L2 penalties being considered. Since L1 performs variable selection, we initially start the model fitting with all parameters being included. ","018883f1":"# Closing Remarks #\n\nThe exercise has been helpful in re-familiarizing myself with the common classification models used, as well as some feature engineering principles. The results could always be further improved, for example by filling missing ages with a more accurate imputer (KNN is usually used for this), extracting more passenger names, using different models and\/or blending more models.","39f88cee":"Before we jump in on the NaN values, we also notice that a few fare values from the training dataset are 0 - These do not make much sense and would only skew our fare data farther to the left. We therefore replace the zero values with NaN and treat them as having missing data. ","febd2a99":"The blended model prediction had a test accuracy of **0.79425**, which places the result in the top 20% of all submissions.","8e20a643":"Here we notice something else interesting about age:\n\nFor many individuals missing age information, their names can provide highly correlated information with regard to their age group! Most notably, young men and women were usually referred to as \"master\" and \"miss\" in the early 1900s. Without this information, these people were being classified as adults based on the median age.\n\n<a href=\"https:\/\/imgur.com\/c6c6QFy\"><img src=\"https:\/\/i.imgur.com\/c6c6QFy.png\" title=\"source: imgur.com\" \/><\/a>\n\nWe can further improve our age imputation by directly replacing the category \"isChild\" by and everything else to be 0 when we see a keyword such as \"master\" or \"miss.\"","c208370a":"We can now see that there is a positive (and quite relatively strong) correlation between being a child and survival.","7047918e":"The training accuracy for the final gradient boosting classifier with 100 estimators is **0.83**. The test accuracy of this model is **0.77990**, a notable improvement over logistic regression's results.","6d7893c0":"We then add another feature of \"family size\" by tweeking the \"Parch\" and \"Sibsp\" data:","571347fb":"If we look at the first passenger, we see that our LR model will predict him\/her to not have survived, with a probability of survival of 42%. This is could be a somewhat shaky prediction, since the model only returned 0 for surival because the decision boundary was set at 50%. Conversely, results where probabilities of survival\/death are high (in the 70s) could suggest that we should place more trust in our LR model instead of GB model. \n\nSince the gradient boosting classifier performed better than the logistic regression model, we seek to blend results of logistic regression into the gradient boosting model only when the LR model has survival probabilities of < 0.25 or > 0.75, suggesting high confidence in our LR model prediction. "}}