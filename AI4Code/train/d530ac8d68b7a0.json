{"cell_type":{"eb478543":"code","2fc92b5a":"code","a752b742":"code","0050bc03":"code","3a843828":"code","e7127e36":"code","e1cacb9e":"code","1f6fc49c":"code","4e98e4a5":"code","f1b43bce":"code","2502e503":"code","78d84394":"code","c58afc4d":"code","d7e6c1c6":"code","c91af352":"code","a4571318":"code","ffbb7fc0":"code","cd95855b":"code","e6c817ab":"markdown","04f01e7b":"markdown","0cd5b56f":"markdown","adb3bca1":"markdown","2339fd91":"markdown","a0056b5a":"markdown","1f2304a0":"markdown"},"source":{"eb478543":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom pathlib import Path\nimport os\nimport os, gc\nimport random\nimport datetime\n\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#import xgboost as xgb","2fc92b5a":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","a752b742":"def dataset_reader():\n    lists=['weather_test.feather'\n          ,'weather_train.feather'\n          ,'test.feather'\n          ,'train.feather'\n          ,'building_metadata.feather']\n    Input = Path('\/kaggle\/input\/ashrae-feather-format-for-fast-loading\/')\n    \n    wtest = pd.read_feather(Input\/lists[0])\n    wtrain = pd.read_feather(Input\/lists[1])\n    test = pd.read_feather(Input\/lists[2])\n    train = pd.read_feather(Input\/lists[3])\n    bmdata = pd.read_feather('\/kaggle\/input\/ashrae-feather-format-for-fast-loading\/building_metadata.feather')\n    \n    #train = train.sort_values(by=['building_id','meter','timestamp']).reset_index()\n\n    gc.collect()\n    #wfull = pd.concat([wtrain,wtest],sort=True,ignore_index = True)\n    train = reduce_mem_usage(train)\n    test = reduce_mem_usage(test)\n    wtrain = reduce_mem_usage(wtrain)\n    wtest = reduce_mem_usage(wtest)\n    bmdata = reduce_mem_usage(bmdata)\n\n    return train,test,wtrain,wtest,bmdata\ntrain,test,wtrain,wtest,bmdata = dataset_reader()","0050bc03":"bm1 = train.groupby('building_id').meter_reading.mean().to_dict()\nbm2 = train.groupby('building_id').meter_reading.median().to_dict()\nbm3 = train.groupby('building_id').meter_reading.min().to_dict()\nbm4 = train.groupby('building_id').meter_reading.max().to_dict()\nbsd = train.groupby('building_id').meter_reading.std().to_dict()\nbmdata['bm1'] = bmdata.building_id.map(bm1)\nbmdata['bm2'] = bmdata.building_id.map(bm2)\nbmdata['bm3'] = bmdata.building_id.map(bm3)\nbmdata['bm4'] = bmdata.building_id.map(bm4)\nbmdata['bsd'] = bmdata.building_id.map(bsd)\n\nprimary_use = {0: 'Religious worship',\n  1: 'Warehouse\/storage',\n  2: 'Technology\/science',\n  3: 'Other',\n  4: 'Retail',\n  5: 'Parking',\n  6: 'Lodging\/residential',\n  7: 'Manufacturing\/industrial',\n  8: 'Public services',\n  9: 'Food sales and service',\n  10: 'Entertainment\/public assembly',\n  11: 'Utility',\n  12: 'Office',\n  13: 'Healthcare',\n  14: 'Services',\n  15: 'Education'}\n\ninv_map = {v: k for k, v in primary_use.items()}\nfloor_avg = bmdata.groupby('primary_use').floor_count.mean().apply(np.ceil).to_dict()\n\ndef addTime(df):\n    df['Month']= df.timestamp.dt.month.astype(np.uint8)\n    df['Day']= df.timestamp.dt.day.astype(np.uint8)\n    df['Hour'] = df.timestamp.dt.hour.astype(np.uint8)\n    df['Weekday'] = df.timestamp.dt.weekday.astype(np.uint8)\n    #df['Date'] = df.timestamp.dt.date\n    return df\n\n#Rolling!\n\nlists = ['air_temperature','cloud_coverage'\n            ,'dew_temperature','precip_depth_1_hr'\n            , 'sea_level_pressure','wind_direction','wind_speed']\n\ndef get_rolling(df,f,p):\n    \n    if f == 'meter_reading':\n        df[f'{f}_{p}_mean'] = df.groupby('building_id')[f].rolling(p).mean().reset_index().astype(np.float16)[f]\n        df[f'{f}_{p})_std'] = df.groupby('building_id')[f].rolling(p).std().reset_index().astype(np.float16)[f]\n    else:\n        #df[f'{f}_{p}'] =\n        gp= df.groupby('site_id')[f].rolling(p).mean().reset_index().astype(np.float16)[f]\n        gp2 = df.groupby('site_id')[f].rolling(p).std().reset_index().astype(np.float16)[f]\n        gp3 = df.groupby('site_id')[f].rolling(p).min().reset_index().astype(np.float16)[f]\n        for i in f:\n            df[f'{i}_{p}_mean'] = gp[i]\n            df[f'{i}_{p}_std'] = gp2[i]\n            df[f'{i}_{p}_min'] = gp3[i]\n    #for i in [24,24*7,24*7*4,24*7*4*4]:\n    #df[f'{f}_{p}'] = train.groupby('building_id')['meter_reading'].rolling(24).mean().reset_index()\n    return df","3a843828":"gc.collect()\ngc.collect()","e7127e36":"def Create_train_feature(df,wdf,meter,smooth,types=None):\n    \n    new_df = df.loc[df.meter==meter]\n    #new_df = get_rolling(new_df,'meter_reading',smooth)\n    new_df = new_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n    new_df = new_df.query('not(building_id>=1325 & timestamp >= \"2016-02-01\" & timestamp <= \"2016-04-20\")')\n\n    bmdata['log_sqf'] = np.log(bmdata.square_feet)\n    if types!='test':\n        new_df['m_log1p'] = np.log1p(new_df.meter_reading)\n    ##\n    bmdata['floor_count'] = bmdata.primary_use.map(floor_avg)\n    bmdata['primary_use'] = bmdata['primary_use'].map(inv_map)\n    ##\n    new_df = new_df.merge(bmdata, on='building_id',how='left')\n    #use it later\n    new_wdf = get_rolling(wdf,lists,smooth)\n    new_wdf = new_wdf.query('not(site_id==15 & timestamp >= \"2016-02-01\" & timestamp <= \"2016-04-20\")')\n    \n    #Temporary\n    #wdf = wdf.query('not(site_id==15 & timestamp >= \"2016-02-01\" & timestamp <= \"2016-04-20\")')\n    \n    new_df = new_df.merge(wdf, on=['site_id','timestamp'],how='left')\n    new_df = addTime(new_df)\n    \n    return new_df","e1cacb9e":"import lightgbm as lgb\nimport optuna\nfrom optuna import Trial\nfrom sklearn.metrics import mean_squared_error\n\ndef objective(trial,meter,rounds):\n    Losses = 0\n    folds = 4\n    shuffle = True\n    seed = 7\n    kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n    selective_train = Create_train_feature(train,wtrain,meter,30)\n    selective_train = reduce_mem_usage(selective_train)\n    target= 'm_log1p'\n\n    do_not_use = ['meter_reading','m_log1p'\n                     ,'is_train'\n                    ,'row_id'\n                    ,'square_feet'\n                    ,'timestamp'\n                  ,'index'\n                     ]\n\n    feature_columns = [c for c in selective_train.columns if c not in do_not_use ]\n\n  \n    for train_idx, valid_idx in tqdm(kf.split(selective_train,selective_train['building_id']),total=folds):\n        print(f'###############Starting_meter :{meter}###############')\n        print(f'Training and predicting for target {target}')\n        Xtr = selective_train[feature_columns].iloc[train_idx]\n        Xv = selective_train[feature_columns].iloc[valid_idx]\n        ytr = selective_train[target].iloc[train_idx].values\n        yv = selective_train[target].iloc[valid_idx].values\n\n        dtrain = lgb.Dataset(Xtr, label=ytr)\n        dvalid = lgb.Dataset(Xv, label=yv ,reference= dtrain)\n\n        print('Train_size: ',Xtr.shape[0],'Validation_size: ', Xv.shape[0])\n\n  \n        Loss = opt_lgbm(trial,rounds,dtrain,dvalid,Xv,yv)\n        Losses = Losses + Loss\n    print(print(f'Cumulative MSLE: {Losses}'))\n    return Losses\n\ndef opt_lgbm(trial,rounds,dtrain,dvalid,Xv,yv,on_gpu=0):\n\n    param = {\"objective\": \"regression\",\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        }\n    if on_gpu==1:\n        print('Use_GPU')\n        gpu={'device': 'gpu'}\n        param.update(gpu)\n\n    print(param)\n    model = lgb.train(param,\n              dtrain,\n              num_boost_round=rounds,\n              valid_sets=(dtrain, dvalid),\n              early_stopping_rounds=20,\n              verbose_eval = 200)\n    valid_prediction = model.predict(Xv, num_iteration=model.best_iteration)\n    #Loss = np.sqrt(np.mean((np.log1p(valid_prediction)-np.log1p(yv))**2))\n    #Loss = np.sqrt( np.mean(np.subtract( np.log1p( valid_prediction ), np.log1p( yv ) )**2 ) )\n    Loss = np.sqrt(mean_squared_error(valid_prediction, yv))\n    gc.collect()\n    print(print(f'RMSLE: {Loss}'))\n    return Loss\n","1f6fc49c":"#Find The Best Hyper Parameter 'minimize' RMSE(RMSLE)\nrounds = 1\nmeters = [0,1,2,3]\nn_trials = 1\n#Meter 0\nstudy0 = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\nstudy0.optimize(lambda trial: objective(trial, meters[0],rounds), n_trials=n_trials)\n#Meter 1\nstudy1 = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\nstudy1.optimize(lambda trial: objective(trial, meters[1],rounds), n_trials=n_trials)\n#Meter 2\nstudy2 = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\nstudy2.optimize(lambda trial: objective(trial, meters[2],rounds), n_trials=n_trials)\n#Meter 3\nstudy3 = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\nstudy3.optimize(lambda trial: objective(trial, meters[3],rounds), n_trials=n_trials)","4e98e4a5":"#Extract Best Hyper Parameters For Each Meters\nBestHPMeter_0 = study0.best_trial.params\nBestHPMeter_1 = study1.best_trial.params\nBestHPMeter_2 = study2.best_trial.params\nBestHPMeter_3 = study3.best_trial.params","f1b43bce":"print(BestHPMeter_0)\nprint(BestHPMeter_1)\nprint(BestHPMeter_2)\nprint(BestHPMeter_3)","2502e503":"import lightgbm as lgb\ndef lgbm(df,wdf,meter,rounds,on_gpu=0):\n    Losses = []\n    models = []\n    selective_train = Create_train_feature(df,wdf,meter,30)\n\n    folds = 4\n    shuffle = True\n    seed = 7\n    kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n\n    target= 'm_log1p'\n\n    do_not_use = ['meter_reading','m_log1p'\n                     ,'is_train'\n                    ,'row_id'\n                    ,'square_feet'\n                    ,'timestamp'\n                  ,'index'\n                     ]\n\n    feature_columns = [c for c in selective_train.columns if c not in do_not_use ]\n    \n    print(f'Feature Colums : {len(feature_columns)}')\n\n    best_params = {\"objective\": \"regression\",\n          \"boosting_type\": \"gbdt\",\n          \"metric\": \"rmse\",\n          \"verbose\": 0,\n         }\n    if on_gpu==1:\n        print('Use_GPU')\n        gpu={'device': 'gpu'}\n        best_params.update(gpu)\n\n    if meter == 0:\n        best_params.update(BestHPMeter_0)\n    elif meter == 1:\n        best_params.update(BestHPMeter_1)\n    elif meter == 2:\n        best_params.update(BestHPMeter_2)\n    else:\n        best_params.update(BestHPMeter_3)\n\n  \n    for train_idx, valid_idx in tqdm(kf.split(selective_train,selective_train['building_id']),total=folds):\n\n        print(f'###############Starting_meter :{meter}###############')\n        print(f'Training and predicting for target {target}')\n        Xtr = selective_train[feature_columns].iloc[train_idx]\n        Xv = selective_train[feature_columns].iloc[valid_idx]\n        ytr = selective_train[target].iloc[train_idx].values\n        yv = selective_train[target].iloc[valid_idx].values\n\n        dtrain = lgb.Dataset(Xtr, label=ytr)\n        dvalid = lgb.Dataset(Xv, label=yv ,reference= dtrain)\n\n        print('Train_size: ',Xtr.shape[0],'Validation_size: ', Xv.shape[0])\n        print(f'Train with {best_params}')\n        model = lgb.train(best_params,\n              dtrain,\n              num_boost_round=rounds,\n              valid_sets=(dtrain, dvalid),\n              early_stopping_rounds=20,\n              verbose_eval = 50)\n        models.append(model)\n        gc.collect()\n\n    return models\n\n  \ndef Lgbm_Training_LOOP(df,wdf,meter,rounds):\n    \n    print(\"LGBM Main Training Meter_{meter}\")\n\n    models  =  lgbm(df,wdf,meter,rounds)\n    \n    print(models)\n    print('End')\n    gc.collect()\n\n    return models\n    \n","78d84394":"models0 = Lgbm_Training_LOOP(train,wtrain,0,1)\nmodels1 = Lgbm_Training_LOOP(train,wtrain,1,1)\nmodels2 = Lgbm_Training_LOOP(train,wtrain,2,1)\nmodels3 = Lgbm_Training_LOOP(train,wtrain,3,1)\ngc.collect()","c58afc4d":"def prediction(meter,models):\n    meter_reading = []\n    selective_test = Create_train_feature(test,wtest,meter,30,types='test')\n    selective_test = reduce_mem_usage(selective_test)\n    set_size = len(selective_test)\n    print(f'meter {meter} : test_size {set_size}')\n    iterations = 100\n    batch_size = int(np.ceil(set_size \/ iterations))\n    do_not_use = ['meter_reading','m_log1p'\n                     ,'is_train'\n                    ,'row_id'\n                    ,'square_feet'\n                    ,'timestamp'\n                  ,'index'\n                     ]\n    feature_columns = [c for c in selective_test.columns if c not in do_not_use ]\n    \n    print(f'Feature Columns : {len(feature_columns)}')\n\n    for i in tqdm(range(iterations)):\n        pos = i*batch_size\n        fold_preds = [np.expm1(model.predict(selective_test[feature_columns].iloc[pos : pos+batch_size],num_iteration=model.best_iteration)) for model in models]\n        meter_reading.extend(np.mean(fold_preds, axis=0))\n    print(np.mean(meter_reading))\n    gc.collect()\n    assert len(meter_reading) == set_size\n    return meter_reading","d7e6c1c6":"submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\ngc.collect()","c91af352":"prediction0 = prediction(0,models0)\nsubmission.loc[test['meter'] == 0, 'meter_reading'] = np.clip(prediction0, a_min=0, a_max=None)\nprediction1 = prediction(1,models1)\nsubmission.loc[test['meter'] == 1, 'meter_reading'] = np.clip(prediction1, a_min=0, a_max=None)\nprediction2 = prediction(2,models2)\nsubmission.loc[test['meter'] == 2, 'meter_reading'] = np.clip(prediction2, a_min=0, a_max=None)\nprediction3 = prediction(3,models3)\nsubmission.loc[test['meter'] == 3, 'meter_reading'] = np.clip(prediction3, a_min=0, a_max=None)","a4571318":"submission.head(10)","ffbb7fc0":"submission.describe()","cd95855b":"submission.to_csv('submission.csv', index=False, float_format='%.4f')","e6c817ab":"#### I will share some of my work for who think it is useful.","04f01e7b":"#### Feature Extraction","0cd5b56f":"## Hyper Parameter Optimization","adb3bca1":"## Main Training","2339fd91":"reference : <br>\nhttps:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization <br>\nhttps:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type by corochann <br><br>\nhttps:\/\/colab.research.google.com\/drive\/1ZKUIL4WiOYLZP6FII4H7CwWRRU1Ter3W\n","a0056b5a":"## Prediction","1f2304a0":"## Data Preprocessing"}}