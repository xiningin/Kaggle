{"cell_type":{"c0e855b1":"code","207acd00":"code","3b07f079":"code","278ac6f6":"code","527eeee0":"code","a5d6f37b":"code","bf9a77ae":"code","3d26fdda":"code","6f8f2d94":"code","3766b3ea":"code","c4e47ab2":"code","61ea9a19":"code","46c67b79":"markdown","bd206ea1":"markdown"},"source":{"c0e855b1":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, math\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy.stats import ks_2samp\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","207acd00":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","3b07f079":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = True\nMAKE_MODEL_TEST = True\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","278ac6f6":"########################### Model params\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':80000,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","527eeee0":"########################### Model\nimport lightgbm as lgb\n\ndef make_test_predictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n    \n    new_columns = set(list(train_df)).difference(base_columns + remove_features)\n    features_columns = base_columns + list(new_columns)\n    \n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n\n    for col in list(X):\n        if X[col].dtype=='O':\n            X[col] = X[col].fillna('unseen_before_label')\n            P[col] = P[col].fillna('unseen_before_label')\n\n            X[col] = train_df[col].astype(str)\n            P[col] = test_df[col].astype(str)\n\n            le = LabelEncoder()\n            le.fit(list(X[col])+list(P[col]))\n            X[col] = le.transform(X[col])\n            P[col]  = le.transform(P[col])\n\n            X[col] = X[col].astype('category')\n            P[col] = P[col].astype('category')\n        \n    tt_df = tt_df[['TransactionID',target]]    \n    predictions = np.zeros(len(tt_df))\n\n    tr_data = lgb.Dataset(X, label=y)\n    vl_data = lgb.Dataset(P, label=P_y) \n    estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n    pp_p = estimator.predict(P)\n    predictions += pp_p\/NFOLDS\n\n    if LOCAL_TEST:\n        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n        print(feature_imp)\n        \n    tt_df['prediction'] = predictions\n    \n    return tt_df\n## -------------------","a5d6f37b":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('..\/input\/ieee-data-minification\/train_transaction.pkl')\n\nif LOCAL_TEST:\n    \n    # Convert TransactionDT to \"Month\" time-period. \n    # We will also drop penultimate block \n    # to \"simulate\" test set values difference\n    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n    del train_df['DT_M'], test_df['DT_M']\n    \nelse:\n    test_df = pd.read_pickle('..\/input\/ieee-data-minification\/test_transaction.pkl')\n    \nprint('Shape control:', train_df.shape, test_df.shape)","bf9a77ae":"########################### Features\n#################################################################################\n# Add list of feature that we will\n# remove later from final features list\nremove_features = [\n    'TransactionID','TransactionDT', # These columns are pure noise right now\n    TARGET,\n    ]\n\n# Let's also remove all V columns for tests\nremove_features += ['V'+str(i) for i in range(1,340)]\n\nbase_columns = [col for col in list(train_df) if col not in remove_features]","3d26fdda":"#### Let's make baseline model \nif MAKE_MODEL_TEST:\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n####","6f8f2d94":"########################### Let's check how many nans we have\ni_cols = ['card1','card2','card3','card4','card5','card6']\n\nfor col in i_cols:\n    print(col,':',train_df[col].isna().sum())\n    \ntrain_df[i_cols].head()","3766b3ea":"########################### Let's play \"sudoku\" and fill nans in cards columns\ni_cols = ['TransactionID','card1','card2','card3','card4','card5','card6']\n\nfull_df = pd.concat([train_df[i_cols], test_df[i_cols]])\n\n## I've used frequency encoding before so we have ints here\n## we will drop very rare cards\nfull_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\nfull_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n\ni_cols = ['card2','card3','card4','card5','card6']\n\n## We will find best match for nan values and fill with it\nfor col in i_cols:\n    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False).reset_index(drop=True)\n    del temp_df['count']\n    temp_df = temp_df.drop_duplicates(keep='first').reset_index(drop=True)\n    temp_df.index = temp_df['card1'].values\n    temp_df = temp_df[col].to_dict()\n    full_df[col] = np.where(full_df[col].isna(), full_df['card1'].map(temp_df), full_df[col])\n    \n    \ni_cols = ['card1','card2','card3','card4','card5','card6']\nfor col in i_cols:\n    train_df[col] = full_df[full_df['TransactionID'].isin(train_df['TransactionID'])][col].values\n    test_df[col] = full_df[full_df['TransactionID'].isin(test_df['TransactionID'])][col].values\n","c4e47ab2":"########################### Let's check how many nans left\ni_cols = ['card1','card2','card3','card4','card5','card6']\n\nfor col in i_cols:\n    print(col,':',train_df[col].isna().sum())\n    \ntrain_df[i_cols].head()","61ea9a19":"#### Let's find out if we have boost\nif MAKE_MODEL_TEST:\n    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n####","46c67b79":"----\n\n## NOTES!\n\n1. Can you do it better? Yes? Look for combinations in cards columns and make more specific fillup.\n\n2. Does it help? For me yes. Please trust your CV and not my word.\n\n3. Should it be done before all fe or after -> trust your cv\n\n4. This is just an example.\n","bd206ea1":"----"}}