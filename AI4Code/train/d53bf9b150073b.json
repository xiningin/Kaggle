{"cell_type":{"ebe0f0ca":"code","fe2a321e":"code","5947a8ab":"code","5dbb031f":"code","83d20b34":"code","88296e32":"code","0ea71599":"code","082dffe4":"code","2586817f":"code","a8da5cfd":"code","22d7fa0b":"code","ea010e41":"code","0bb86880":"markdown","4e8217d1":"markdown","14cbb553":"markdown","8df77be3":"markdown","24bdc179":"markdown"},"source":{"ebe0f0ca":"import json \n\nwith open('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json','r') as f:\n    \n    data = f.read()\n    data = \"[\" + data.replace(\"}\", \"},\", data.count(\"}\")-1) + \"]\"\n    datastore = json.loads(data)","fe2a321e":"headlines = []\nlabels = []\n\nfor item in datastore:\n    headlines.append(item[\"headline\"])\n    labels.append(item[\"is_sarcastic\"])","5947a8ab":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","5dbb031f":"# configure important hyper-parameters \n\nvocab_size = 2000\nmax_len = 100\nembedding_dim = 32\noov_tok = \"<OOV>\"\npadding_type = \"post\"\ntrunc_type = \"post\"\ntraining_size = 20000","83d20b34":"# split the data into sets for training and testing\n\ntrain_data, test_data = headlines[:training_size], headlines[training_size:]\ntrain_labels, test_labels = labels[:training_size], labels[training_size:]","88296e32":"# tokenize the train_data and test_data\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_data)\n\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_data)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\ntest_padded = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n\ntrain_padded = np.array(train_padded)\ntrain_labels = np.array(train_labels)\n\ntest_padded = np.array(test_padded)\ntest_labels = np.array(test_labels)","0ea71599":"# define a model (version 1 - 1 bidirectional LSTM)\n\nmodel_ver1 = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                          layers.Bidirectional(layers.LSTM(16, return_sequences=True)),\n                          layers.Dense(16, activation=\"relu\"),\n                          layers.Dense(1, activation=\"sigmoid\")])\n                         \nmodel_ver1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n                         \nmodel_ver1.summary()\nkeras.utils.plot_model(model_ver1)","082dffe4":"# train the model\n\nnum_epochs = 20\n\nhistory_1 = model_ver1.fit(train_padded, train_labels,\n                   epochs=num_epochs,\n                   validation_data=(test_padded, test_labels),\n                   verbose=1)","2586817f":"# plot accuracy and loss\n\nacc = history_1.history[\"accuracy\"]\nval_acc = history_1.history[\"val_accuracy\"]\nloss = history_1.history[\"loss\"]\nval_loss = history_1.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\n# accuracy\n\nplt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b--\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show()\n\n# loss\n\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","a8da5cfd":"# define a model (version 2)\n\nmodel_ver2 = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                               layers.Dropout(0.4),\n                               layers.Conv1D(64, 5, activation=\"relu\"),\n                               layers.MaxPooling1D(pool_size=4),\n                               layers.LSTM(128),\n                               layers.Dense(1, activation=\"sigmoid\")])\n                         \nmodel_ver2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n                         \nmodel_ver2.summary()\nkeras.utils.plot_model(model_ver2)","22d7fa0b":"# train the second model\n\nnum_epochs = 20\n\nhistory_2 = model_ver2.fit(train_padded, train_labels,\n                   epochs=num_epochs,\n                   validation_data=(test_padded, test_labels),\n                   verbose=1)","ea010e41":"# plot accuracy and loss of the second model\n\nacc = history_2.history[\"accuracy\"]\nval_acc = history_2.history[\"val_accuracy\"]\nloss = history_2.history[\"loss\"]\nval_loss = history_2.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\n# accuracy\n\nplt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b--\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show()\n\n# loss\n\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","0bb86880":"### Model 2: Adding a 1D convolutional layer and a max pooling 1D layer","4e8217d1":"### Conclusion for this version of the notebook\n\nThe second model is better and more stable than the first one. However, the validation loss is relatively high. I think there should be another way to lower it down and increase the overall performace of the model.\n\nPlease give me some advice if you know how to improve the second model. Thanks in advance!","14cbb553":"### Model 1: Using a bidirectional LSTM layer and 2 densely connected layers.","8df77be3":"## Sarcasm Detection from News Headlines\n\n### In this notebook, I am going to predict if the news is sarcastic or not using 2 deep learning models.\n\n### Dataset: news-headlines-dataset-for-sarcasm-detection.json  \n\nThis is a binary classification task with 2 labels: 0 for is_sarcastic and 1 for not_sarcastic.\n\n![](https:\/\/miro.medium.com\/max\/1200\/1*draHTh0KlC2oC2uJqM4UkA.jpeg)","24bdc179":"### Data Preprocessing "}}