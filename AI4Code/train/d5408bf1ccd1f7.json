{"cell_type":{"35b835ea":"code","d1d8ebca":"code","2d62a238":"code","9d972e6d":"code","4a1ad9bf":"code","e98c9c16":"code","6b9314b7":"code","d359102c":"code","c18e712e":"code","c6e495c0":"code","f458b33a":"code","32110e65":"code","56b0549c":"code","4f99a836":"code","c5a90b9c":"code","85d766ee":"code","ebd0080f":"code","5100d710":"code","0664ccf5":"code","fa32aee4":"code","cab67747":"code","d2151769":"code","1fdc924b":"code","4d438625":"code","acef403c":"code","f76f9c24":"code","a0a51f5b":"code","2f78e175":"code","066cfd54":"code","e0896620":"code","412b92ba":"code","848f1f17":"code","e9aa6850":"code","68ecfb97":"code","391d336a":"code","c681a129":"code","a87310e5":"code","0fb6e0b9":"code","3a11bfce":"code","5eaecec2":"code","7fd5f943":"code","f39a5221":"code","1227adb3":"markdown","e7029adf":"markdown","3be1d648":"markdown","d46b63bf":"markdown","866daf88":"markdown","526b2bc9":"markdown","2e7136d6":"markdown","e3c71e59":"markdown","4966c435":"markdown","533daa09":"markdown","e89ce5e5":"markdown","486ac8c5":"markdown","62770c94":"markdown","784b967b":"markdown","864670e8":"markdown","0c9960c6":"markdown","01edd9af":"markdown","eaf359be":"markdown","4610571f":"markdown","03d1775c":"markdown","b556e4ce":"markdown","317057af":"markdown","c484ea72":"markdown","2ebe9a5f":"markdown","ce6f19f4":"markdown","f7add37e":"markdown"},"source":{"35b835ea":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, StratifiedKFold, GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom xgboost import XGBClassifier\n\nplt.style.use(\"Solarize_Light2\")\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d1d8ebca":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2d62a238":"data = pd.read_csv('\/kaggle\/input\/diabetes-health-indicators-dataset\/diabetes_binary_5050split_health_indicators_BRFSS2015.csv')","9d972e6d":"data","4a1ad9bf":"data.isna().sum()","e98c9c16":"data.nunique()","6b9314b7":"cat_col = ['GenHlth', 'Education', 'Income']","d359102c":"num = ['BMI', 'MentHlth', 'PhysHlth', 'Age']","c18e712e":"data = pd.get_dummies(data, columns=cat_col)","c6e495c0":"data['bmi_group'] = pd.cut(data['BMI'], (0, 16, 18.5, 25, 30, 35, 40, np.inf), labels=[1, 2, 3, 4, 5, 6, 7])","f458b33a":"data.BMI = data['bmi_group']\ndata.drop('bmi_group', axis=1, inplace=True)","32110e65":"data.BMI = data.BMI.astype('float')","56b0549c":"X = data.drop(['Diabetes_binary'], axis=1)\ny = data['Diabetes_binary']","4f99a836":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","c5a90b9c":"num","85d766ee":"scaler = StandardScaler()\nx_train_sc = scaler.fit_transform(x_train[num])\nx_test_sc = scaler.transform(x_test[num])\n\nx_train[num] = x_train_sc\nx_test[num] = x_test_sc","ebd0080f":"def xgb_class(clf, param_test, feat, target):\n   \n    kfold = StratifiedKFold(n_splits=3,\n                            shuffle=True,\n                            random_state=42)\n\n    grid_search = GridSearchCV(clf,\n                               param_test,\n                               scoring=\"neg_log_loss\",\n                               n_jobs=-1,\n                               cv=kfold)\n\n    global grid_result\n    grid_result = grid_search.fit(feat, target)\n    \n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))","5100d710":"def logistic_loss(clf, act, feat):\n    return log_loss(act, clf.predict_proba(feat))","0664ccf5":"def quality_report(actual, prediction):\n    print(\"Accuracy: {:.3f}\\nPrecision: {:.3f}\\nRecall: {:.3f}\\nf1_score: {:.3f}\\nRoc_auc: {:.3f}\".format(\n        accuracy_score(actual, prediction),\n        precision_score(actual, prediction),\n        recall_score(actual, prediction),\n        f1_score(actual, prediction),\n        roc_auc_score(actual, prediction)))","fa32aee4":"def plot_features(clf):\n    feature_importances = clf.feature_importances_\n    pd.DataFrame({'features': x_train.columns,\n                                           'feature_importances': feature_importances})\\\n    .sort_values('feature_importances', ascending=False).plot.barh(x ='features', figsize=(10, 7))","cab67747":"xgbc_start = XGBClassifier(random_state = 42, eval_metric='auc')\n\nxgbc_start.fit(x_train, y_train)\n\npredict_start = xgbc_start.predict(x_test)\n\nprint('\\nTest quality: \\n')\nquality_report(y_test, predict_start)\nprint('\\nLogistic Loss:')\nlogistic_loss(xgbc_start, y_test, x_test)","d2151769":"xgbc_clf = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False) \n\n\nn_estimators = [100, 200, 300, 400, 500]\nlearning_rate = [0.0001, 0.001, 0.01, 0.05, 0.1]\n\nparam_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n\nkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n\ngrid_search = GridSearchCV(xgbc_clf, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n\ngrid_result = grid_search.fit(x_train, y_train)\n\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot results\nscores = np.array(means).reshape(len(learning_rate), len(n_estimators))\nplt.figure(figsize=(13,10))\nfor i, value in enumerate(learning_rate):\n    plt.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\nplt.legend()\nplt.xlabel('n_estimators')\nplt.ylabel('Log Loss')","1fdc924b":"xgbc_clf = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False)\n\nlow_e = int((grid_result.best_params_.get('learning_rate'))*100 - 5)\nif low_e <= 0:\n    low_e = int(1)\nhigh_e = int((grid_result.best_params_.get('learning_rate'))*100 + 6)\nif high_e >= 10:\n    high_e = int(11)\n\nparam_test = {\n'n_estimators':[100, 200, 300, 400, 500],\n'learning_rate':[i\/100.0 for i in range(low_e,high_e)]}\n\n\nxgb_class(xgbc_clf, param_test, x_train, y_train)","4d438625":"params = {}\nparams.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","acef403c":"xgbc_depth = XGBClassifier(random_state = 42,\n                     eval_metric='auc',\n                     use_label_encoder=False, **params) \n\nparam_test = {\n'max_depth':range(1,6,1),\n'min_child_weight':range(1,6,1)}\n\n\nxgb_class(xgbc_depth, param_test, x_train, y_train)","f76f9c24":"params.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","a0a51f5b":"xgbc_s_c = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nparam_test = {\n 'subsample':[i\/10.0 for i in range(6,11)],\n 'colsample_bytree':[i\/10.0 for i in range(6,11)]\n}\n\n\nxgb_class(xgbc_s_c, param_test, x_train, y_train)","2f78e175":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","066cfd54":"xgbc_s_c = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params)\n\nlow_s = int((grid_result.best_params_.get('subsample'))*100 - 5)\nif low_s <= 0:\n    low_s = int(1)\nhigh_s = int((grid_result.best_params_.get('subsample'))*100 + 6)\nif high_s >= 100:\n    high_s = int(101)\n\n\nlow_c = int((grid_result.best_params_.get('colsample_bytree'))*100 - 5)\nif low_c <= 0:\n    low_c = int(1)\nhigh_c = int((grid_result.best_params_.get('colsample_bytree'))*100 + 6)\nif high_c >= 100:\n    high_c = int(101)\n\nparam_test = {\n 'subsample':[i\/100.0 for i in range(low_s,high_s)],\n 'colsample_bytree':[i\/100.0 for i in range(low_c, high_c)]\n}\n\n\nxgb_class(xgbc_s_c, param_test, x_train, y_train)","e0896620":"\nparams.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","412b92ba":"xgbc_g = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nparam_test = {\n 'gamma':[i\/10.0 for i in range(0,11)]\n}\n\n\nxgb_class(xgbc_g, param_test, x_train, y_train)","848f1f17":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","e9aa6850":"xgbc_g = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nlow_g = int((grid_result.best_params_.get('gamma'))*100 - 5)\nif low_g <= 0:\n    low_g = int(1)\nhigh_g = int((grid_result.best_params_.get('gamma'))*100 + 6)\nif high_g >= 100:\n    high_g = int(101)\n\nparam_test = {\n 'gamma':[i\/100.0 for i in range(low_g,high_g)]\n}\n\n\nxgb_class(xgbc_g, param_test, x_train, y_train)","68ecfb97":"params.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","391d336a":"xgbc_fin = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params)\n\n\nxgbc_fin.fit(x_train, y_train)\n\npredict_xgbc = xgbc_fin.predict(x_test)\n\nprint('\\nTest quality: \\n')\nquality_report(y_test, predict_xgbc)","c681a129":"print('The first result: %s' % logistic_loss(xgbc_start, y_test, x_test))\nprint('Result with parameter tuning: %s' % logistic_loss(xgbc_fin, y_test, x_test))","a87310e5":"plot_features(xgbc_fin)","0fb6e0b9":"pred_proba = xgbc_fin.predict_proba(x_test)","3a11bfce":"class_1 = pred_proba[:, 1][y_test == 1]\nclass_0 = pred_proba[:, 1][y_test == 0]\nthreshold = 0.5\nplt.figure(figsize=(17,10))\nplt.scatter(np.arange(len(class_1)), class_1, label='class_1')\nplt.scatter(np.arange(len(class_1), len(class_1)+len(class_0)), class_0, label='class_0')\nplt.plot([-0.2, len(pred_proba[:, 1])], [threshold, threshold], c='b')\nplt.title('Probability of class 1')\nplt.legend();","5eaecec2":"msg_row = pd.DataFrame(columns=['threshold', 'precision', 'recall',\n               'F1', 'accuracy'])\nthreshold = 0.3\ndata_threshold = pd.DataFrame()\nfor i in range(300):\n    threshold += 0.001\n    pred = np.where(pred_proba[:, 1] >= threshold, 1, 0)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    new_row = {'threshold': threshold, 'precision':precision, 'recall':recall,\n               'F1':f1, 'accuracy':accuracy, 'SUM': precision+recall+f1+accuracy}\n    msg_row = msg_row.append(new_row, ignore_index=True)","7fd5f943":"msg_row.head()","f39a5221":"msg_row.plot(x='threshold', y=['precision', 'recall',\n               'F1', 'accuracy'], figsize=(19, 10), fontsize=15)","1227adb3":"When the threshold is increased, the `recall` drops sharply and the `precision` increases. At the threshold value of 0.54, the metrics intersect and have an equal value of 0.751.","e7029adf":"Now let's set up `max_depth` and `min_child_weight`, set a spread of values and look for the best. ","3be1d648":"+ Divide the values of `BMI` (body mass index) into groups, update `BMI`","d46b63bf":" let's run it again, but with boundary values `learning_rate`.","866daf88":"Let's add new parameters to the `params` dictionary, which we will use in the next training.","526b2bc9":"+ Let's break all the values into separate variables","2e7136d6":"+ Let's scaled all numeric signs - `'BMI'`, `'MentHlth'`, `'PhysHlth'`, `'Age'`","e3c71e59":"# Data overview & Data preprocessing","4966c435":"Let's run the final model with the values obtained in the sample `test` ","533daa09":"The blue labels are class 1, the green labels are class 0 in `y_test`, the blue line is a threshold of 0.5, which are separated by class 1 and 0 in the predicted model. Green labels above the blue line and blue labels below the blue line are incorrectly classified classes.\n\nLet's change the threshold 0.5  to see how the values of `precision`, `recall`, `F1` and `accuracy` will change.","e89ce5e5":"+ Let's run the model with default settings:","486ac8c5":"Now let's fine-tune the `XGBClassifier`. To begin with, we determine the optimal learning rate and the number of trees. Let's take `net_log_loss` as a criterion. Although our data is balanced, but due to a logistical error, we will be able to better see the progress in finding parameters. And so it's easier to use any of the `confusion_matrix` metrics.","62770c94":"There are 70692 values in the dataset, so we will divide the dataset into 2 parts: `train` and `test`.","784b967b":"let's run it again, but with boundary values.","864670e8":"Let's add new parameters to the `params` dictionary, which we will use in the next training.","0c9960c6":"# Split and Scaler","01edd9af":"Now let's configure `subsample` and `com sample_by three`, set a spread of values from 0.6 to 1.","eaf359be":"Configure the `gamma` parameter, set values from 0 to 1","4610571f":"Let's run it again, but with boundary values.","03d1775c":"Let's see how the probabilities of classes 1 and 0 were distributed in the predicted model","b556e4ce":"+ No missing values    \n  \nLet's see how many unique values are in each attribute:","317057af":"# XGBClassifier","c484ea72":"+ Let's define the functions","2ebe9a5f":"Let's see which variabilities  had the greatest impact on learning.","ce6f19f4":"+ Non-binary categorical features - `GenHlth`, `Education`, `Income`.","f7add37e":"+ Continuous features - `BMI`, `MentHlth`, `PhysHlth`, `Age`"}}