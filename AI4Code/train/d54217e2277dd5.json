{"cell_type":{"74c12c1a":"code","6711e082":"code","22109d98":"code","60e0e9e7":"code","a6bedb73":"code","73205341":"code","c8f24772":"code","133d7ca8":"code","f5723a1e":"code","dc794e89":"code","461b6bda":"code","4014b8e0":"code","cfb74d6f":"code","658f4908":"code","30a62d4d":"code","1538a66b":"code","2a8ad04c":"code","6f4bcc43":"code","dcad16cf":"code","c449644c":"code","31378a74":"code","524f9a7e":"code","919abbeb":"code","506b87e2":"code","cbd3d7d0":"code","5e7f4e2b":"code","6a4ea716":"code","3db74bb4":"code","0767646b":"code","bae5469a":"code","0ea3082d":"code","e09397cb":"code","6f11f410":"code","799096c4":"code","586d4b63":"code","20e17b74":"code","eed070cd":"code","bcb764b7":"markdown","e1a98d25":"markdown","6347f708":"markdown","7bd152f7":"markdown","3b84e74f":"markdown"},"source":{"74c12c1a":"# import usual stuff\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6711e082":"# loading data\nfrom sklearn.datasets import load_boston\nboston = load_boston()","22109d98":"boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)","60e0e9e7":"boston_df.head()","a6bedb73":"boston_df['MEDV'] = boston.target","73205341":"boston_df.head()","c8f24772":"boston_df.info()","133d7ca8":"boston_df.describe()","f5723a1e":"sns.boxplot('CHAS','MEDV',data=boston_df)","dc794e89":"sns.pairplot(boston_df)","461b6bda":"sns.scatterplot('LSTAT','MEDV',data = boston_df)","4014b8e0":"X = boston_df.drop('MEDV', axis=1)\ny= boston_df['MEDV']","cfb74d6f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","658f4908":"# lets first use linear regression\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)","30a62d4d":"sns.scatterplot(y_test,y_pred)","1538a66b":"sns.distplot(y_test-y_pred)","2a8ad04c":"coef = model.coef_\ninter = model.intercept_\ncoefficients = pd.DataFrame(coef,X.columns,columns=['Coefficients'])\ncoefficients","6f4bcc43":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nprint('MAE: ',mean_absolute_error(y_test,y_pred))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred)))\nprint('R2: ',r2_score(y_test,y_pred))","dcad16cf":"# let's try lasso regression with L1 Penalty\nfrom sklearn.linear_model import Lasso\nlassomod = Lasso()\nlassomod.fit(X_train,y_train)\nyls_pred = lassomod.predict(X_test)","c449644c":"sns.scatterplot(y_test,yls_pred)","31378a74":"sns.distplot(y_test-yls_pred)","524f9a7e":"coefls = lassomod.coef_\ninterls = lassomod.intercept_\ncoefficients_ls = pd.DataFrame(coefls,X.columns,columns=['Coefficients'])\ncoefficients_ls","919abbeb":"print('MAE: ',mean_absolute_error(y_test,yls_pred))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,yls_pred)))\nprint('R2: ',r2_score(y_test,yls_pred))","506b87e2":"# let's try Ridge Regression\nfrom sklearn.linear_model import Ridge\nmodelridge = Ridge()\nmodelridge.fit(X_train,y_train)\nyridge_pred = modelridge.predict(X_test)","cbd3d7d0":"sns.scatterplot(y_test, yridge_pred)","5e7f4e2b":"sns.distplot(y_test-yridge_pred)","6a4ea716":"coefridge = modelridge.coef_\ninterridge = modelridge.intercept_\ncoefficients_ridge = pd.DataFrame(coefridge,X.columns,columns=['Coefficients'])\ncoefficients_ridge","3db74bb4":"print('MAE: ',mean_absolute_error(y_test,yridge_pred))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,yridge_pred)))\nprint('R2: ',r2_score(y_test,yridge_pred))","0767646b":"# now let's try neural networks for prediction and see how much we imporve\n# for getting number of nodes in a single layer, thumb rule is to have as many as your features\nX_train.shape","bae5469a":"#scaling for input into tf model\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled_train = scaler.fit_transform(X_train)\nX_scaled_test = scaler.transform(X_test)","0ea3082d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam","e09397cb":"tfmodel = Sequential()\n\ntfmodel.add(Dense(13,activation='relu'))\ntfmodel.add(Dense(13,activation='relu'))\ntfmodel.add(Dense(13,activation='relu'))\ntfmodel.add(Dense(13,activation='relu'))\n\ntfmodel.add(Dense(1))\n\ntfmodel.compile(optimizer='adam', loss='mse')\n","6f11f410":"tfmodel.fit(x=X_scaled_train,y=y_train.values,\n          validation_data=(X_scaled_test,y_test.values),\n          batch_size=128,epochs=400)","799096c4":"lossdf = pd.DataFrame(tfmodel.history.history)","586d4b63":"lossdf.plot()","20e17b74":"y_pred = tfmodel.predict(X_scaled_test)","eed070cd":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nprint('MAE:', mean_absolute_error(y_test,y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred)))\nprint('R2:', r2_score(y_test,y_pred))","bcb764b7":"we can clearly see that neural networks worked significantly better, specially in the cases where linear regression wasn't fitting the data nicely across the range.","e1a98d25":"Defining target and features and doing train test split","6347f708":"Let's do quick exploratory analysis but I have worked on this data before so not really spending ton of time here.","7bd152f7":"In this workbook, i am going to work withn sklearn provided boston dataset and compare regression approaches and also use neural networks and compare the performance","3b84e74f":"as you can see above that linear regression is not predicting well at the higher range. yvyhat curve should be a straight line and here line is curving post MEDV of 40. There are several methods to solve this problem like only training models for less expensive range or creating a feature for more expensive houses and pass that in the model among others. Since we want to actually see if neural network works better, this data is an ideal candidate for this exercise as linear regression is not fitting well. \n\nnext I am going to try ridge and lasso just to compare."}}