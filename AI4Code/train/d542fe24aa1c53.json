{"cell_type":{"ec0ec8e7":"code","b017ffb9":"code","e834e8ac":"code","adf899ea":"code","5a89ad9c":"code","72f83572":"code","bbfa481b":"code","66c4f099":"code","d98838f8":"code","5b10d228":"code","2ad81c17":"code","263b9600":"code","431b49bf":"code","0bc51341":"code","3ab2f2c3":"code","c3a9cec2":"code","d1b1f6be":"code","c632df84":"code","97a3e9a3":"code","e4cde472":"code","09f1a447":"code","575efbbb":"markdown","d85065e8":"markdown","e3245ab2":"markdown","dc30ff72":"markdown","814499a3":"markdown"},"source":{"ec0ec8e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b017ffb9":"import warnings\nimport gc\nimport tensorflow as tf\nimport holidays\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split","e834e8ac":"def smape(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    num = tf.math.abs(tf.math.subtract(y_true, y_pred))\n    denom = tf.math.add(tf.math.abs(y_true), tf.math.abs(y_pred))\n    denom = tf.math.divide(denom,200.0)\n    \n    val = tf.math.divide(num,denom)\n    val = tf.where(denom == 0.0, 0.0, val) \n    return tf.reduce_mean(val)","adf899ea":"def split_sequences(sequences, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the dataset\n\t\tif end_ix > len(sequences):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn np.array(X), np.array(y)","5a89ad9c":"class Time2Vector(tf.keras.layers.Layer):\n    def __init__(self, seq_len, **kwargs):\n        super(Time2Vector, self).__init__()\n        self.seq_len = seq_len\n\n    def build(self, input_shape):\n        self.weights_linear = self.add_weight(name='weight_linear',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_linear = self.add_weight(name='bias_linear',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.weights_periodic = self.add_weight(name='weight_periodic',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_periodic = self.add_weight(name='bias_periodic',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n    def call(self, x):\n        x = tf.math.reduce_mean(x[:,:,:], axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)\n        time_linear = self.weights_linear * x + self.bias_linear\n        time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)\n        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n        time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)\n        return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2","72f83572":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.attn(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass Transformer(keras.Model):\n    def __init__(\n            self,\n            num_hid=64, # embed_dim - num of features\n            time_steps=7,\n            num_head = 2,\n            num_feed_forward=128, # pointwise dim\n            num_layers_enc = 4,\n            time_embedding = False,\n    ):\n        super().__init__()\n        self.num_hid = num_hid\n        if time_embedding:\n            self.num_hid += 2\n            self.tv = Time2Vector(time_steps)\n        else:\n            self.tv = None\n        self.numlayers_enc = num_layers_enc\n        self.enc_input = layers.Input((time_steps, self.num_hid))\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(self.num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n        self.GlobalAveragePooling1D = layers.GlobalAveragePooling1D(data_format='channels_last')\n        self.out = layers.Dense(units=1, activation='linear')        \n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n        \n    def call(self, inputs):\n        if self.tv:\n            x = self.tv(inputs)\n            x = self.concat([inputs, x])\n            x = self.encoder(x)\n        else:\n            x = self.encoder(inputs)\n        x = self.GlobalAveragePooling1D(x)\n        y = self.out(x)\n        return y","bbfa481b":"def label_encoder(df):\n    country = {c : i for i, c in enumerate(df['country'].unique())}\n    store = {s : i for i, s in enumerate(df['store'].unique())}\n    product = {p : i for i, p in enumerate(df['product'].unique())}\n    df = df.copy()\n    df['country'] = df['country'].replace(country)\n    df['store'] = df['store'].replace(store)\n    df['product'] = df['product'].replace(product)\n    return df","66c4f099":"# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html\ndef preprocess_dates(df):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df['weekday'] = df['date'].dt.weekday\n    df['quarter'] = df['date'].dt.quarter\n    df['day_of_year'] = df['date'].dt.day_of_year\n    df['is_month_start'] = df['date'].dt.is_month_start.astype(\"int8\")\n    df['is_month_end'] = df['date'].dt.is_month_end.astype(\"int8\")\n    df['month'] = df['date'].dt.month\n    return df","d98838f8":"def preprocess_holidays(df):\n    holiday_finland = holidays.CountryHoliday(country='FI', years=[2015, 2016, 2017, 2018, 2019])\n    holiday_norway = holidays.CountryHoliday(country='NO', years=[2015, 2016, 2017, 2018, 2019])\n    holiday_sweden = holidays.CountryHoliday(country='SE', years=[2015, 2016, 2017, 2018, 2019])\n    holidays_fin_nor_swe = holiday_finland.copy()\n    holidays_fin_nor_swe.update(holiday_norway)\n    holidays_fin_nor_swe.update(holiday_sweden)\n    dates = list(holidays_fin_nor_swe.keys())\n    dates = sorted(pd.to_datetime(dates))\n    df = df.copy()\n    df['is_holiday'] = df['date'].apply(lambda x : 1 if x in dates else 0)\n    return df","5b10d228":"def preprocess_timeseries(df):\n    df = df.copy()\n    df['sin_day_of_year'] = np.sin(df['day_of_year'])\n    df['sin_month'] = np.sin(df['month'])\n    return df","2ad81c17":"seed = 47\nTIMESTEPS = 1\nwarnings.filterwarnings(\"ignore\")","263b9600":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv\", sep=',')","431b49bf":"train_df = label_encoder(train_df)\ntrain_df = preprocess_dates(train_df)\ntrain_df = preprocess_holidays(train_df)\ntrain_df = preprocess_timeseries(train_df)\nx_train = train_df.drop(['row_id', 'date', 'num_sold'], axis=1)\ny_train = train_df['num_sold']\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=False)","0bc51341":"x_train = np.append(x_train, y_train.values.reshape(-1, 1), axis=1)\nx_test = np.append(x_test, y_test.values.reshape(-1, 1), axis=1)\nx_train, y_train = split_sequences(x_train, TIMESTEPS)\nx_test, y_test = split_sequences(x_test, TIMESTEPS)","3ab2f2c3":"num_heads=2\nnum_layers_enc=2\nnum_feed_forward=64\nnum_features = x_train.shape[-1]\ntime_steps = TIMESTEPS\nepochs = 100\nbatch_size = 128\n\nmodel = Transformer(num_hid=num_features,\n                        time_steps=time_steps,\n                        time_embedding=True,\n                        num_head=num_heads,\n                        num_layers_enc=num_layers_enc,\n                        num_feed_forward=num_feed_forward)\n\nopt = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.mse\nmodel.compile(optimizer=opt, loss=smape)\nmodel.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\nprint()\nresults = model.evaluate(x_test, y_test)\nprint(results)","c3a9cec2":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","d1b1f6be":"test_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv\", sep=',')","c632df84":"test_df = label_encoder(test_df)\ntest_df = preprocess_dates(test_df)\ntest_df = preprocess_holidays(test_df)\ntest_df = preprocess_timeseries(test_df)\nx_test = test_df.drop(['row_id', 'date'], axis=1)\nx_test = np.append(x_test, np.ones((x_test.shape[0], 1)), axis=1)\nx_test, _ = split_sequences(x_test, TIMESTEPS)","97a3e9a3":"target = model.predict(x_test).squeeze()\nrow_id =  test_df['row_id'].values\nsubmission = pd.DataFrame({'row_id' : row_id, 'num_sold' : target})","e4cde472":"submission.head()","09f1a447":"submission.to_csv('submission.csv', index=False)","575efbbb":"# Transformer with TIME2VEC\n\nHere I tried a transformer model with almost nothing of preprocessing on the data. The results aren't that good yet, so next time\nI am going do some feature engineering, like adding information extracted from the dates, like holidays, weekends and month of the year for instance.","d85065e8":"# Preprocessing","e3245ab2":"# Submission","dc30ff72":"# Reading the dataset","814499a3":"<h3>Converte the data to 3D array shape<\/h3>"}}