{"cell_type":{"e0fcb43a":"code","510860b3":"code","ad35b4e1":"code","393ceddf":"code","158b15e6":"code","73750ca3":"code","5c8eec5b":"code","d9dd28a9":"code","4e109c80":"code","204a8d08":"code","8151edb1":"code","6273687a":"code","3386ba24":"code","beb71902":"code","7182a23e":"code","ea15d314":"code","d194cd13":"code","74726e2c":"code","2f742854":"markdown","c485dc02":"markdown","bfe71536":"markdown","8a62cf84":"markdown","1da08e13":"markdown","03511a4e":"markdown","9a865c1a":"markdown","32fc5b97":"markdown"},"source":{"e0fcb43a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","510860b3":"battles = pd.read_csv(\"..\/input\/battles.csv\")\npokemon = pd.read_csv(\"..\/input\/pokemon.csv\")\npokemon.describe()","ad35b4e1":"def normalizeColumns(columns, dataset):\n    \"\"\"\n    Function that use standard normalization\n    to normalize a column in a dataset\n    \"\"\"\n    for column in columns:\n       dataset[column] = (dataset[column] - dataset[column].mean()) \/ dataset[column].std()","393ceddf":"# Types NaN filling and one-hot\npokemon.fillna(\"noType\", inplace=True)\npokemon = pd.concat([\n    pokemon, \n    pd.get_dummies(pokemon[\"Type 1\"], prefix=\"t1\"),\n    pd.get_dummies(pokemon[\"Type 2\"], prefix=\"t2\"),\n], axis=1)\n\n# Drop linear dependent\/useless columns\npokemon.drop([\"Type 1\", \"Type 2\", \"Name\", \"Generation\", \"Legendary\"], axis=1, inplace=True)\n\n# Normalization\nnormalizeColumns([\"HP\", \"Sp. Atk\", \"Sp. Def\",  \"Attack\", \"Defense\", \"Speed\"], pokemon)","158b15e6":"def mergePokemonStats(battles, pokemon):\n    data = battles \\\n        .merge(pokemon, left_on=\"First_pokemon\", right_on=\"#\", suffixes=[\"_1\",\"_2\"]) \\\n        .merge(pokemon, left_on=\"Second_pokemon\", right_on=\"#\",  suffixes=[\"_1\",\"_2\"]) \\\n        .sort_values(['battle_number']) \\\n        .drop([\"#_1\", \"#_2\", \"First_pokemon\", \"Second_pokemon\", \"battle_number\"], axis=1) \n    data = data.reindex(sorted(data.columns), axis=1)\n    return data","73750ca3":"# Merging columns\ndata = mergePokemonStats(battles, pokemon)\ndata.head()","5c8eec5b":"y = data[\"Winner\"].values\nx = data.drop([\"Winner\"],axis=1).values","d9dd28a9":"import tensorflow as tf\nimport sklearn.model_selection\nimport sklearn.ensemble\nimport sklearn.metrics","4e109c80":"mlp = tf.keras.Sequential([\n    tf.keras.layers.Dense(310, activation=tf.nn.relu),\n    tf.keras.layers.GaussianNoise(0.17),\n    tf.keras.layers.Dropout(0.50),\n    tf.keras.layers.Dense(310, activation=tf.nn.relu),  \n    tf.keras.layers.Dropout(0.50),\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])","204a8d08":"mlp.compile(optimizer=\"adam\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])","8151edb1":"history = mlp.fit(\n    x,\n    y,\n    epochs=350,\n    workers=4,\n    batch_size=512,\n    shuffle=True,\n    validation_split=0.1\n)","6273687a":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","3386ba24":"x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)\n\ngradientBoosting = sklearn.ensemble.GradientBoostingClassifier(\n    n_estimators=1000, \n    max_depth=10,\n    verbose = 1\n)\n\ngradientBoosting.fit(x_train, y_train)","beb71902":"sklearn.metrics.accuracy_score(y_test, gradientBoosting.predict(x_test), normalize=True, sample_weight=None)","7182a23e":"submission = pd.read_csv(\"..\/input\/test.csv\")\ntest = mergePokemonStats(submission, pokemon)","ea15d314":"predictionMLP = np.transpose(mlp.predict(test.values))\npredictionGB = gradientBoosting.predict_proba(test.values)\n\npredictionGB = np.array([predict[1] for predict in predictionGB])\n\nprediction = ((predictionMLP) + (0.10*predictionGB))\/1.10\n\nprediction[prediction <= 0.5] = 0\nprediction[prediction > 0.5] = 1","d194cd13":"submission[\"Winner\"] = prediction[0].astype(\"int\")\nsubmission.drop([\"First_pokemon\", \"Second_pokemon\"], axis=1, inplace=True)\nsubmission.head()","74726e2c":"submission.to_csv(\"submission.csv\", index=False)","2f742854":"### Multilayer Perceptron\n\n(Para el resultado \u00f3ptimo, los hiperpar\u00e1metros pueden variar)","c485dc02":"Lo siguiente ser\u00e1 tratarlos para obtener el m\u00e1ximo rendimiento de ellos:\n\n- Rellenamos los NA con un tipo dummy, \"noType\"\n- Convertimos los dos tipos\n- Eliminamos las columnas Tipo 1, Tipo 2, Generaci\u00f3n y Legendario\n- Normalizamos el resto de columnas num\u00e9ricas\n\nPor \u00faltimo, para cada batalla, a\u00f1adimos los datos de cada uno de los Pok\u00e9mon de las mismas.","bfe71536":"## Tratamiento de los datos","8a62cf84":"## Submission","1da08e13":"# Multilayer Perceptron + Gradient Boosting - Ensamblado mediante Bagging\n\nAqu\u00ed ten\u00e9is el kernel con el que he conseguido mejores resultados hasta hora. Para conseguir mejores resultados, probablemente debas modificar los hiperpar\u00e1metros, ya que estos se encuentran alterados por las muchas pruebas que he realizado.","03511a4e":"### Gradient Boosting Classifier\n\n(Para el resultado \u00f3ptimo, los hiperpar\u00e1metros pueden variar)","9a865c1a":"## Modelado\n\nPara crear el modelo final, realizaremos un ensamblado de otros dos modelos: un Multilayer Perceptron y un Gradient Boosting Classifier. Para hacer el ensamblaje, har\u00e9 uso de un simple bagging. A la hora de la verdad, la importancia que posee el Gradient Boosting Classifier es **muy bajo**. Para el bagging, realizar\u00e9 una media de los pesos, en la que el peso de mayor importancia ser\u00e1 el del MLP.\n\nLo primero ser\u00e1 obtener los datos con los que vamos a entrenar\/validar los modelos","32fc5b97":"Importamos los datos, como de costumbre:"}}