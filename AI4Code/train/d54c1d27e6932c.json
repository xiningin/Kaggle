{"cell_type":{"8b5e0b8e":"code","551a4cca":"code","39995ca1":"code","a09b0924":"code","a72bbc83":"code","394ca2a6":"code","1bb5e328":"code","315c452a":"code","37fba214":"code","c3742ab9":"code","a09eff37":"code","13c7ab9a":"code","32b12f49":"code","02100138":"code","077abae7":"code","db5d8109":"code","4f6bb97a":"code","958bfc29":"markdown","b1c9c46f":"markdown","285ae6ec":"markdown","d629f28e":"markdown","df146ae3":"markdown","d0ded6b6":"markdown","7d83830c":"markdown","5f1af3ed":"markdown","10d5a885":"markdown"},"source":{"8b5e0b8e":"import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport timeit\nfrom pandas.core.frame import DataFrame","551a4cca":"features = range(1,35)\nbatch_size = 1\nepochs = 600\nlearn_rate = 0.003\nhl = 2                          # Numbers of hidden layers\nnodes = [20, 10]                 # All Hidden Layer have same number of nodes\noutput_layer = 2                # Output node7","39995ca1":"def feature_select(data:DataFrame, index:list):\n    new = data.iloc[:,index]\n    new = normalize(new)\n    new = new.fillna(0)\n    return new\n\ndef normalize(x:DataFrame):                           # Max-min normalize\n    m  = (x-x.min())\/(x.max()-x.min())\n    return m\n\ndef y_trans(x):\n    y_new = pd.get_dummies(x, prefix='', prefix_sep='')\n    return y_new","a09b0924":"def init_model(x, hl, node, p, batch):\n    w = {}\n    b = {}\n    if len(node) < hl:\n        node = np.tile(node,hl)\n    n = np.hstack((x,node,p)).ravel()                     # Number of layer, including in and out\n    for i in range(0,(len(n)-1)):\n        w[i] = np.random.rand(n[i+1],n[i]) - 0.5          # row = output ; column = input\n        b[i] = np.random.rand(n[i+1],batch) - 0.5         # Row = nodes ; column = batch = 1\n    return w, b","a72bbc83":"def sig(x):\n    return 1.0\/(1.0 + np.exp(-x))                     # Sigmoid Function\n\ndef sig_deriv(x):\n    return x*(1.0-x)                                # X --> Result of sigmoid function\n\ndef relu(x):\n    return np.maximum(x,0)\n\ndef relu_deriv(x):\n    return x > 0\n\ndef softmax(x):\n    #z = np.exp(x-x.max())\n    z = np.exp(x)\n    s = z.sum()\n    out = z \/ s\n    return out","394ca2a6":"def forward_prop(x, w, b):\n    z = {}\n    a = {}\n    a[0] = np.array(x).T\n    #print(a[0].shape)\n    last = len(w)\n    for i in range(1,(last+1)):\n        z[i] = w[i-1].dot(a[i-1]) + b[i-1]\n        #print(z[i].shape)\n        if i == last:\n            a[i] = softmax(z[i])\n        else:\n            a[i] = sig(z[i])\n    pred = a[last]\n    #print('prediction')\n    #print(pred)\n    #print(pred.shape)\n    return pred, a, z     # A is output of layers , last A is the output, Z is before the activation function\n\ndef back_prop(y, p, a, w, b):\n    d = {}\n    d_w = {}\n    d_b = {}\n    last = len(w)\n    d[last] = (p-y.T)           #1x1\n    #print('p - y')\n    #print(d[last])\n    for i in range(last-1, -1, -1):\n        #print('back-prop')\n        if i == last-1:\n            d[i] = w[i].T.dot(d[i+1])      # 10 x 1 --> d2\n            #print(d[i])\n        elif i != 0:           # No need to get d0\n            d[i] = w[i].T.dot(d[i+1]) * sig_deriv(a[i])\n            #print(d[i])\n        d_w[i] = d[i+1].dot(a[i].T)        # dw2 \n        #d_b[i] = np.sum(d[i+1], axis=1, keepdims=True)\n        d_b[i] = d[i+1]\n    return d_w, d_b\n\ndef update(w, b, d_w, d_b, a):\n    for i in range (0,len(w)):\n        #print(w[i].shape)\n        #print(w[i])\n        w[i] = w[i] - a * d_w[i]\n        #print('update w')\n        #print(w[i].shape)\n        #print(w[i])\n        #print(b[i].shape)\n        #print(b[i])\n        b[i] = b[i] - a * d_b[i]\n    return w, b\n\ndef sum_error(y, p):\n    z = np.sum(y.T * np.log(p))\n    #print('cost')\n    #print(z)\n    return z","1bb5e328":"def cal_acc(y, p):\n    tp = tn = fp = fn = 0   \n    for i in range(0,len(p)):\n        if y[i] == 1 and p[i] == 1:\n            tp = tp + 1\n        elif y[i] == 1 and p[i] == 0:\n            fn = fn + 1\n        elif y[i] == 0 and p[i] == 1:\n            fp = fp + 1\n        elif y[i] == 0 and p[i] == 0:\n            tn = tn + 1\n    err_rate = (fp+fn) \/ len(p)\n    return err_rate","315c452a":"df = pd.read_csv('..\/input\/ionosphere-data\/ionosphere_data.csv')\nx_test = df.sample(frac=0.20).reset_index()     # Sample 20%\nx_train = df.drop(x_test.index).reset_index()    # Put the rest for training\ny_train = y_trans(x_train.iloc[:,-1])                           # Train Output\ny_test = y_trans(x_test.iloc[:,-1])                             # Test Output\nx_test = x_test.drop(x_test.columns[-1], axis=1)                # Drop output\nx_train = x_train.drop(x_train.columns[-1], axis=1)\nx_test = feature_select(x_test, features)\nx_train = feature_select(x_train, features)\nx_test = np.array(x_test)\nx_train = np.array(x_train)\ny_test = np.array(y_test)\ny_train = np.array(y_train)","37fba214":"def train(x, y, w, b, epoch, learn_rate, batch):\n    c = []           \n    lf_d = {}\n    t_v = np.argmax(y, axis=1)           # True Value ( 1 \/ 0 )\n    err_rate = np.zeros(epoch)           # Error rate for each epoch\n    for i in range(0, epoch):\n        e = 0                            # Reset\n        lf = {}                          # Reset\n        p = np.zeros(len(x))             # Prediction ( 1 \/ 0 )\n        #print('epoch', i)\n        for k in range(0, len(x)):       # Loop through training datasets\n            pred, layer, z = forward_prop(x[k:k+batch], w, b)\n            w_grad, b_grad = back_prop(y[k:k+batch], pred, layer, w, b)\n            w, b = update(w, b, w_grad, b_grad, learn_rate)\n            e = e + sum_error(y[k:k+batch], pred)\n            p[k] = np.argmax(pred, axis=0)  # Prediction ( 1 \/ 0 )\n            #print(z[len(z)-1])\n            lf[k] = z[len(z)-1]\n        lf_d[i] = {'l_f':lf, 'p':t_v}\n        cost = -e\/len(x)\n        c.append(cost)\n        err_rate[i] = cal_acc(t_v, p)\n        if (i%(epoch\/12)) == 0:\n            print('iteration:',i,' cost:' ,cost)\n    return c, err_rate, w, b, lf_d","c3742ab9":"def predict(x, y, w, b, batch):\n    #er = 0\n    s = len(x)\n    p = np.zeros(s)\n    #p_e = np.zeros(s)\n    t_v = np.argmax(y, axis=1)    # True Value ( 1 \/ 0 )\n    for i in range(0,s):\n        pred, c, z = forward_prop(x[i:i+batch], w, b)\n        #er = er + sum_error(y[i:i+batch], pred)\n        p[i] = np.argmax(pred, axis=0)  # Prediction ( 1 \/ 0 )\n        #print(pred)\n        #p_e[i] = sum_error(y[i:i+batch], pred)\n    err_rate = cal_acc(t_v, p)\n    return p, err_rate","a09eff37":"m, n = x_train.shape                    # m = number of data, n = features\n\nweight, bias = init_model(n, hl, nodes, output_layer, batch_size)\n\ncost, err_rate, weight_N, bias_N, latent_f = train(x_train, y_train, weight, bias, epochs, learn_rate, batch_size)\n\ner = np.sum(err_rate, keepdims=True)\/epochs\n#print(err_rate.shape)\nprint('Error Rate = %f' %er)","13c7ab9a":"print(err_rate[-1])","32b12f49":"fig, (ax1, ax2) = plt.subplots(2,figsize=(20,10))\nax1.plot(err_rate)\nax1.set_title('Error rate')\nax2.plot(cost)\nax2.set_title('Cost')","02100138":"predict, err = predict(x_test, y_test, weight_N, bias_N, batch_size)\ntrue_y = np.argmax(y_test, axis=1)\nprint('Error rate = %f' %err)","077abae7":"fig, (ax1, ax2) = plt.subplots(2,figsize=(20,10))\nax1.plot(predict)\nax1.set_title('Prediction')\nax2.plot(true_y)\nax2.set_title('True Value')","db5d8109":"if nodes[-1] == 2:\n    fig = plt.figure(figsize =(20,10))\n    p1_data = latent_f[100]\n    p2_data = latent_f[599]\n    colors = {'red','blue'}\n    # First subplot\n    ax1 = fig.add_subplot(1,2,1)\n    data = p1_data['l_f']\n    color = p1_data['p']\n    x = []\n    y = []\n    for i in data:\n        x.append(data[i][0])\n        y.append(data[i][1])\n    ax1.scatter(x , y, c=color, cmap=matplotlib.colors.ListedColormap(colors))\n    ax1.set_title('Latent Feature Epoch 101')\n    # Second subplot\n    ax2 = fig.add_subplot(1,2,2)\n    data2 = p2_data['l_f']\n    color2 = p2_data['p']\n    x2 = []\n    y2 = []\n    for i in data:\n        x2.append(data2[i][0])\n        y2.append(data2[i][1])\n    ax2.scatter(x2, y2, c=color2, cmap=matplotlib.colors.ListedColormap(colors))\n    ax2.set_title('Latent Feature Epoch 600')","4f6bb97a":"if nodes[-1] == 3 :\n    fig = plt.figure(figsize =(20,20))\n    p1_data = latent_f[100]\n    p2_data = latent_f[599]\n    colors = {'red','blue'}\n    # First subplot\n    ax1 = fig.add_subplot(1,2,1, projection='3d')\n    data = p1_data['l_f']\n    #print(data)\n    color = p1_data['p']\n    x = []\n    y = []\n    z = []\n    for i in data:\n        x.append(data[i][0])\n        y.append(data[i][1])\n        z.append(data[i][2])\n    ax1.scatter(x , y, z, c=color, cmap=matplotlib.colors.ListedColormap(colors))\n    # Second subplot\n    ax2 = fig.add_subplot(1,2,2, projection='3d')\n    data2 = p2_data['l_f']\n    #print(data2)\n    color2 = p2_data['p']\n    x2 = []\n    y2 = []\n    z2 = []\n    for i in data:\n        x2.append(data2[i][0])\n        y2.append(data2[i][1])\n        z2.append(data2[i][2])\n    ax2.scatter(x2, y2, z2, c=color2, cmap=matplotlib.colors.ListedColormap(colors))","958bfc29":"# **Constants**","b1c9c46f":"# **Prediction**","285ae6ec":"# Plot 3D Latent Features","d629f28e":"# **Training Functions**","df146ae3":"# **Predict**","d0ded6b6":"# **NN Classification**","7d83830c":"# **Load Data**","5f1af3ed":"# Plot 2D Latent Feature","10d5a885":"# **Training**"}}