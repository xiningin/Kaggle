{"cell_type":{"a4e91c88":"code","2fdc2929":"code","5f82e151":"code","20a91b32":"code","c869ef81":"code","852e629d":"code","2e3f4866":"code","7dd5dd27":"code","621e49e3":"code","cf5a1db2":"code","e00bc878":"code","6c636b3e":"code","6ffe12e3":"code","9a3912a7":"code","5ea486d8":"code","6963ff1b":"code","d779179a":"code","2c359d1f":"code","04c0bea0":"code","9af59827":"code","7b818a1a":"code","77ffdb25":"code","64b2c27d":"code","d0df5800":"code","2b571e07":"code","68302f6a":"code","0fcd9c12":"code","d8673fac":"code","eb567692":"code","a1ff0680":"code","f53464cb":"code","d8beed57":"code","5cbbaee2":"markdown","62571a4e":"markdown","99a80467":"markdown","44d17278":"markdown","54e7e70a":"markdown","74722f1d":"markdown","729b1349":"markdown","1e9918ff":"markdown","2b1a132f":"markdown","cba55780":"markdown","c314eea2":"markdown","d110eb90":"markdown","392dc7d8":"markdown","7c5f09f2":"markdown","2e0ef917":"markdown","0ac2ed7f":"markdown","35f0f332":"markdown","3bd4afa4":"markdown","adc92217":"markdown","3ff81015":"markdown","9d98b508":"markdown"},"source":{"a4e91c88":"# Importing Libraries\nimport numpy as np \nimport pandas as pd\nimport re\nimport os\nimport random\nfrom collections import Counter\nfrom tqdm import tqdm\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\ntqdm.pandas()","2fdc2929":"df_train = pd.read_csv('\/kaggle\/input\/pec-dl-202101\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/pec-dl-202101\/test.csv')","5f82e151":"n_train_samples = len(df_train)\nprint('Train samples: ', n_train_samples, '\\tTest samples: ', len(df_test))","20a91b32":"df_train.head()","c869ef81":"df_test.head()","852e629d":"y = df_train['sentiment'] #save targets\nX_train = df_train.drop('sentiment', axis=1)\nX_test = df_test\n#Concatenate Train and Test to transform data only once.\nX = pd.concat([X_train,X_test]) \nprint(X.shape)","2e3f4866":"# Count label distribution\ny.value_counts()","7dd5dd27":"# Now, let's see the average number of words per sample\nlen_of_review = X['review'].apply(lambda x : len(x.split(' ')))\nplt.figure(figsize=(10, 6))\nplt.hist(len_of_review, 50)\nplt.xlabel('Length of reviews')\nplt.ylabel('Number of samples')\nplt.title('Number of Words distribution')\nplt.show()","621e49e3":"word_counter = Counter(\" \".join(X[\"review\"]).split())","cf5a1db2":"print('Found ', len(word_counter), ' distinct words!')\n\nprint('List of most common words in reviews: \\n')\nword_counter.most_common(50)","e00bc878":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nSTOP = stopwords.words('english') #List of english most common words","6c636b3e":"print('Sample of most common words in english (stop words): \\n', random.sample(STOP,50))","6ffe12e3":"# put into lowercase\nX['review'] = X['review'].str.lower()\n# Strip HTML tags\nX['review'] = X['review'].progress_apply(lambda review : re.sub('<[^>]*>', '', review))","9a3912a7":"# remove stop words and do stemming\n# save to another field to preserve original data for later models.\nX['review_processed'] = X['review'].progress_apply(lambda review: ' '.join(\n    [PorterStemmer().stem(word) for word in word_tokenize(review) if word not in (STOP)]))","5ea486d8":"word_counter_processed = Counter(\" \".join(X[\"review_processed\"]).split())\nprint('Found ', len(word_counter_processed), ' distinct words!')\nprint('List of most common words in reviews after preprocessing: \\n')\nword_counter_processed.most_common(50)","6963ff1b":"count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2)\nBoW = count_vectorizer.fit_transform(X['review_processed'])","d779179a":"BoW.shape","2c359d1f":"# Split train and test\nBoW_train = BoW[:n_train_samples]\nBoW_test = BoW[n_train_samples:]\nprint(BoW_train.shape, BoW_test.shape)","04c0bea0":"N_columns = 20000\nselector = SelectKBest(f_classif, k=min(N_columns, BoW_train.shape[1]))\nselector.fit(BoW_train, y)\nBoW_train = selector.transform(BoW_train).astype('float32')\nBoW_test = selector.transform(BoW_test).astype('float32')\nprint('New data shapes: ', BoW_train.shape, BoW_test.shape)","9af59827":"# Split data for train \/ validation \nBoW_train, BoW_valid, y_train, y_valid = train_test_split(BoW_train, y, test_size=0.25, random_state=42)","7b818a1a":"gbt = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42).fit(BoW_train, y_train)\npredictions = gbt.predict_proba(BoW_valid)\nprint('Accuracy ', accuracy_score(y_valid, np.argmax(predictions,axis=1)), 'ROC-AUC', roc_auc_score(y_valid, predictions[:,1]))","77ffdb25":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","64b2c27d":"#Max number of words to consider in our network. \n#Larger values means more processing time. Smaller means lost of information.\n#Remember the first histogram we build?\nmaxlen = 200 \n\n#Using same number of distinct words as previous algorithm\ntokenizer = Tokenizer(num_words=N_columns)\n\n#Using review, not review_processed\ntokenizer.fit_on_texts(X['review'])\ntokenized_data = tokenizer.texts_to_sequences(X['review'])\n\nX_t = pad_sequences(tokenized_data, maxlen=maxlen)","d0df5800":"print('===>This review: \\n ', X.iloc[0]['review'], '\\n===>Was tokenized to:\\n', X_t[0,:], '\\nSize of tokens: ', X_t[0,:].shape)","2b571e07":"#Split data back into train and test\nX_t_train = X_t[:n_train_samples,:]\nX_t_test = X_t[n_train_samples:,:]\n#Split train into train and validation\nX_t_train, X_t_valid, y_train, y_valid = train_test_split(X_t_train, y, test_size=0.25, random_state=42)\nprint(X_t_train.shape, X_t_valid.shape, X_t_test.shape)","68302f6a":"# Input for variable-length sequences of integers\ninputs = keras.Input(shape=(None,), dtype=\"int32\")\n# Embed each integer in a 128-dimensional vector\nx = layers.Embedding(N_columns, 128)(inputs)\n# Add 2 bidirectional LSTMs\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\n# Add a classifier\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()","0fcd9c12":"model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\nmodel.fit(X_t_train, y_train, batch_size=32, epochs=3, validation_data=(X_t_valid, y_valid))","d8673fac":"test_preds = model.predict(X_t_test)","eb567692":"test_preds[:10]","a1ff0680":"df_submission = pd.DataFrame({'id': df_test['id'].values, 'sentiment': test_preds.T[0]})\ndf_submission.head()","f53464cb":"#Save to file\n#After Save (commit) this notebook (button in right upper corner) you may submit to competition\ndf_submission.to_csv('submission.csv', index=False)","d8beed57":"print('Done!')","5cbbaee2":"# 6. Suggestions to improve your score","62571a4e":"Here we put text in lowercase and remove all html tags using regex.","99a80467":"Our BoW model generated 692,081 distinct \"words\". Since we defined ngram_range = (1,2), we included bigrams into our model as distinct words, so this is not a surprise the number of columns is bigger than number of distinct words we counted before.\n\nBelow we use the [SelectKBest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html) from Scikit-Learn to select N best columns which correlate more with our labels. ","44d17278":"We have almost 50-50% positive-negative reviews on train dataset.","54e7e70a":"Finally we train a gradient boosting algorithm to fit our BoW data and score against validation split.","74722f1d":"Note that test set doesn't have sentiment column (target). Our model has to be built on train dataset and predictions made on test dataset to submit to competition's grader.","729b1349":"# 2. Preprocessing Text\n\nIn this section we'll preprocess the text to clean it. ","1e9918ff":"Our simplest model is based on a concept called Bag of Words (BoW). We'll create a Matrix whereas lines represents reviews and columns distincts words. Cells of this matrix counts number of occurences of each word in each review. As you can imagine, this is a very sparse matrix.\n\nTo accomplish this we'll use a library from scikit-learn, [CountVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) (Please, see docs for details of parameters).\n\nFor BoW model, we'll use the version without stopwords and with stemmed processing step ('review_processed' column)","2b1a132f":"# 1. Read and Explore Data","cba55780":"As we can see, few duplicate entries ('the', 'The') because of case insensitive, lots of \"stop words\" and a HTML tag garbage.","c314eea2":"# 4. RNN Model","d110eb90":"For some kind of algorithms, it's interesting to remove stop words and do stemming on text. Below we create a new column with this processing step. ","392dc7d8":"Now we start training our model for 3 epochs","7c5f09f2":"* Try to use pretrained embeddings (glove, word2vec, fasttext) instead of training from sketch on your Deep Learning Models. See [here](https:\/\/keras.io\/examples\/nlp\/pretrained_word_embeddings\/). \n* Try other hyperparameters on your models (number of max columns, range of n-grams, max length of sentences)\n* Try other combination of preprocessing steps. We assumed it's not a good idea remove stop words for RNN. Is it true? We put data in lowercase. Is it better?\n* You can create another handcraft features such as \"number of words entirely on CAPS LOCK\". Your predictions could mix this kind of features with the others.\n* Try to ensemble and\/or stack models: https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python ","2e0ef917":"As you can see, our RNN model improves the ROC-AUC score from previous model. This is the metric we care about in the competition. So next, we'll use it to create predictions on test file and submit results to grader.","0ac2ed7f":"# 5. Create submission file","35f0f332":"Now we'll try to build a more sophisticated model, using Deep Learning. We'll use Keras to build a Bidirectional LSTM model. Most of the code below was borrowed from [here](https:\/\/keras.io\/api\/datasets\/imdb\/).\n\nBut unlike the previous algorithm, here we'll use the less preprocessed version of reviews, because RNN could take advantage of all words in its all forms in the context, including stop words. ","3bd4afa4":"# Sentiment Analysis for Movie Reviews - Starter Code\n\nThis Jupyter notebook was created as part of an academic activity of [COC891-Deep Learning](http:\/\/www.coc.ufrj.br\/pt\/disciplinas\/catalogo\/8673-coc-891-deep-learning) course from professor Alexandre Evsukoff at PEC\/Coppe\/UFRJ. Videos from past offer from this course can be found [here](http:\/\/www.coc.ufrj.br\/pt\/sistema-online2\/598-cursos-online\/9417-cursos-online-disciplina-coc891-deep-learning)\n\nThe activity consists of a [competition](https:\/\/www.kaggle.com\/c\/pec-dl-202101) between students to create deep learning models to predict sentiment (positive or negative) of movie reviews. \n\nThis Starter Code presentes the data and two baseline models for students get started with the problem. At the end, a submission file is created and can be send directly to competition grader. You may turn GPU on for faster processing, but remember you have limited budget for it.","adc92217":"# 3. First Try: Traditional ML Model with Bag of Words","3ff81015":"The tokenizer creates an ID for each token (word). The pad_sequences put all reviews into a fixed length format.","9d98b508":"Below we define the achitecture of our RNN model."}}