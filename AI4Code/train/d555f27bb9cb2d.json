{"cell_type":{"f53d16ac":"code","c7f8205e":"code","94e0e712":"code","590fc0e0":"code","d46d0722":"code","025f7006":"code","aed6c3ed":"code","cf8f3e3c":"code","b57c5a1d":"code","5ecc8d4e":"code","6da91770":"code","8e465e9f":"code","064d81c3":"code","90f15c13":"code","d01e0779":"code","78a8f85e":"code","3db37f6e":"code","751a2395":"code","f7623382":"code","df57b7f2":"code","931818f7":"code","583c9b2c":"code","4ea994c2":"code","8d057f5d":"code","71da5aa4":"code","35f850ad":"code","cec99235":"code","091ae89e":"code","2bc3a6a8":"code","f1afd2d8":"code","8bc1255f":"markdown","c1916fc2":"markdown","1bf64059":"markdown","3d042181":"markdown","b008e374":"markdown","2fe082ef":"markdown","8ef8570a":"markdown","37055b9c":"markdown","9e8040f3":"markdown","020210e4":"markdown","71deea40":"markdown"},"source":{"f53d16ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom tqdm import tqdm, tqdm_notebook\n\nimport matplotlib.pyplot as plt\n\n#Audio\nimport IPython.display as ipd  # To play sound in the notebook\nfrom scipy.io import wavfile\nimport gc\n\n# Parallelization\nfrom joblib import Parallel, delayed\n\n#Classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\n# Signal processing\nimport scipy.stats","c7f8205e":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# PATH=\"..\/input\/Santander\/\" \nPATH=\"..\/input\/\" \nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output.","94e0e712":"%%time\ntrain_curated = pd.read_csv(PATH+\"train_curated.csv\")\n# train_noisy = pd.read_csv(PATH+\"train_noisy.csv\")\ntest = pd.read_csv(PATH+\"sample_submission.csv\")","590fc0e0":"print('Train curated size:' + format(train_curated.shape))\n# print('Train noisy size:' + format(train_noisy.shape))\nprint('Test size: ' + format(test.shape))","d46d0722":"train_curated.head()","025f7006":"test.head()","aed6c3ed":"#Creating a dictionary of labels\nlabel_columns = list( test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\nlabel_mapping","cf8f3e3c":"def split_and_label(rows_labels):\n    \n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((80))\n        \n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        \n        row_labels_list.append(labels_array)\n    \n    return row_labels_list","b57c5a1d":"train_curated_labels = split_and_label(train_curated['labels'])\n# train_noisy_labels   = split_and_label(train_noisy  ['labels'])\nlen(train_curated_labels) #, len(train_noisy_labels)","5ecc8d4e":"for f in label_columns:\n    train_curated[f] = 0.0\n#     train_noisy[f] = 0.0\n\ntrain_curated[label_columns] = train_curated_labels\n# train_noisy[label_columns]   = train_noisy_labels\n\ntrain_curated['num_labels'] = train_curated[label_columns].sum(axis=1)\n# train_noisy['num_labels']   = train_noisy[label_columns].sum(axis=1)\n\ntrain_curated['path'] = PATH+'train_curated\/'+train_curated['fname']\n# train_noisy  ['path'] = PATH+'train_noisy\/'+train_noisy['fname']\n\ntrain_curated.head()","6da91770":"# train_noisy.head()","8e465e9f":"train = train_curated\n# train = pd.concat([train_curated, train_noisy],axis=0) # Using both datasets\n\ndel train_curated  #, train_noisy\ngc.collect()\n\ntrain.shape","064d81c3":"train.describe()","90f15c13":"import pywt\nimport scipy as sc","d01e0779":"def zero_padding(data, seconds):\n    fs = 44100  # 2 seconds =  88200 samples    \n    if data.shape[0] < seconds*fs:\n        zeros = np.zeros(seconds*fs - data.shape[0])\n        data = np.concatenate((data, zeros), axis=None)    \n    return data","78a8f85e":"from collections import defaultdict, Counter\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))\/den\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 2\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\n","3db37f6e":"from scipy.stats import kurtosis\nfrom scipy.stats import skew #skewness\n\ndef calculate_entropy(list_values):\n    counter_values = Counter(list_values).most_common()\n    probabilities = [elem[1]\/len(list_values) for elem in counter_values]\n    entropy=scipy.stats.entropy(probabilities)\n    return entropy\n \ndef calculate_statistics(list_values):\n    n5 = np.nanpercentile(list_values, 5)\n    n25 = np.nanpercentile(list_values, 25)\n    n75 = np.nanpercentile(list_values, 75)\n    n95 = np.nanpercentile(list_values, 95)\n    median = np.nanpercentile(list_values, 50)\n    mean = np.nanmean(list_values)\n    std = np.nanstd(list_values)\n    var = np.nanvar(list_values)\n    rms = np.nanmean(np.sqrt(list_values**2))\n    # New features\n    kur = kurtosis(list_values)\n    MeanAbs = mean_abs(list_values)\n    norm_ent = norm_entropy(list_values)\n    skewness = skew(list_values)\n    CPT_5 = CPT5(list_values)\n    SSC_1 = SSC(list_values)\n    WL = wave_length(list_values)\n    SRAV_1 = SRAV(list_values)\n    return [n5, n25, n75, n95, median, mean, std, var, rms, kur, MeanAbs, norm_ent, skewness, CPT_5, SSC_1, WL, SRAV_1]\n \ndef calculate_crossings(list_values):\n    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n    no_zero_crossings = len(zero_crossing_indices)\n    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n    no_mean_crossings = len(mean_crossing_indices)\n    return [no_zero_crossings, no_mean_crossings]\n ","751a2395":"def get_features(list_values):\n    entropy = calculate_entropy(list_values)\n    crossings = calculate_crossings(list_values)\n    statistics = calculate_statistics(list_values)\n    return [entropy] + crossings + statistics    ","f7623382":"from sklearn.preprocessing import StandardScaler\n\n\ndef feature_extraction_wpd(path_names, level, seconds):\n    # Sampling rate\n    fs = 44100    \n    corpus = []\n    \n    for fname in tqdm_notebook(path_names):        \n\n        fs, data = wavfile.read(fname)    \n        data = data.astype(float)\n        \n        # Zero padding\n        if data.shape[0] < (seconds*fs):\n            data = zero_padding(data,seconds)\n        elif data.shape[0] > (seconds*fs):\n            data = data[0:seconds*fs]\n        elif data.shape[0] == 0:\n            raise Exception('Lenght of x should not be 0. The value of lenght of x was: {}'.format(data.shape[0]))\n            \n        # Signal standarization\n        data_std = StandardScaler().fit_transform(data.reshape(-1,1)).reshape(1,-1)[0]            \n        \n        # WPD tree\n        wptree = pywt.WaveletPacket(data=data_std, wavelet='db5', mode='symmetric', maxlevel=level)\n        levels = wptree.get_level(level, order = \"freq\")            \n        \n        #Feature extraction for each node\n        features = []        \n        for node in levels:\n            data_wp = node.data\n            # Features group\n            features.extend(get_features(data_wp))\n        corpus.append(features)\n    # Delate first row\n    return np.array(corpus)\n     ","df57b7f2":"%%time\npath_names = train.path.values\nlevel = 6\nseconds = 2\nX_train = feature_extraction_wpd(path_names,level,seconds)","931818f7":"path_names.shape, X_train.shape","583c9b2c":"test.shape","4ea994c2":"%%time\npath_test = PATH+'test\/'\npath_names = path_test + test['fname'].values\nX_test = feature_extraction_wpd(path_names,level, seconds) ","8d057f5d":"X_train[~np.isfinite(X_train)] = 0\nX_test[~np.isfinite(X_test)] = 0","71da5aa4":"X_train = np.float32(X_train)\nX_test = np.float32(X_test)","35f850ad":"X_train[~np.isfinite(X_train)] = 0\nX_test[~np.isfinite(X_test)] = 0","cec99235":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=69)\n\nPREDTRAIN = np.zeros( (X_train.shape[0],80))\nPREDTEST  = np.zeros( (X_test.shape[0],80))\nfor f in range(len(label_columns)):\n    y = train[ label_columns[f]].values\n    oof      = np.zeros( X_train.shape[0] )\n    oof_test = np.zeros( X_test.shape[0] )\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train,y)):\n        \n        # Random Forest classifier\n        model = RandomForestClassifier(n_estimators=500, random_state=0, n_jobs=-1)\n        model.fit(X_train[trn_idx,:], y[trn_idx])\n        \n        oof[val_idx] = model.predict_proba(X_train[val_idx,:])[:,1] \n        oof_test += model.predict_proba(X_test)[:,1]\/5.0\n\n    PREDTRAIN[:,f] = oof    \n    PREDTEST [:,f] = oof_test\n    \n    print( f, str(roc_auc_score( y, oof ))[:6], label_columns[f] )","091ae89e":"def calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\nprint( 'CV:', calculate_overall_lwlrap_sklearn( train[label_columns].values, PREDTRAIN ) )","2bc3a6a8":"PREDTEST.shape, test.shape, X_test.shape","f1afd2d8":"test[label_columns] = PREDTEST \ntest.to_csv('submission.csv', index=False)\ntest.head()","8bc1255f":"# Freesound Audio Tagging 2019\n## Automatically recognize sounds and apply tags of varying natures","c1916fc2":"![image.png](attachment:image.png)\nWe will use WPD to Level 6. Where, a total of 64 new signal are obtained. For each new signal, a statistics feature group are obtanied. ","1bf64059":"Hello everyone, this kernel was developed to show a new approach of signal processing in machine learning.\nThe key here is the Wavelet Transform (WT), more information in [link 1](https:\/\/en.wikipedia.org\/wiki\/Wavelet_packet_decomposition) and [link 2](https:\/\/file.scirp.org\/pdf\/IJCNS20100300011_40520775.pdf). This is a useful tool for the analysis and classification of time-series and signal. There are diferentes implementations of WT: Continuous Wavelet Transform, Discrete Wavelet Transform and Wavelet Packet Decomposition.\n\nIn this kernel, we will use Wavelet Packet Decomposition and Random Forest Classifier.  \n(Only train_curated dataset will be used)","3d042181":"Train set ready!","b008e374":"### Feature extraction methods","2fe082ef":"Cleaning Nan and Inf values","8ef8570a":"# Classification\n\nInformation about Random Forest classifier:\n[Link](https:\/\/medium.com\/machine-learning-101\/chapter-5-random-forest-classifier-56dc7425c3e1)","37055b9c":"### Zero pading\nUseful information\n[Link 1](http:\/\/www.bitweenie.com\/listings\/fft-zero-padding\/)\n[Link 2](https:\/\/www.youtube.com\/watch?v=ukHTfD37THI)","9e8040f3":"# Preparing data","020210e4":"Test set ready!","71deea40":"# Wavelet packet descomposition (WPD)"}}