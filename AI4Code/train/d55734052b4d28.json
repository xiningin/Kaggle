{"cell_type":{"ab8ab3a0":"code","c743a747":"code","bf397d56":"code","ec29d224":"code","759a85a2":"code","594fea92":"code","62b17e69":"code","fa151c1d":"code","a090c708":"code","3b56a36b":"code","b20260ff":"code","7f44b55c":"code","e0fa43b7":"code","7f175112":"code","f770f8fb":"markdown","856eb5aa":"markdown","ac2483ff":"markdown","5d2f6b4e":"markdown","0b9a72df":"markdown","16ddb597":"markdown","342567ad":"markdown","b3b1288c":"markdown","513ff764":"markdown","e0d3d4ed":"markdown"},"source":{"ab8ab3a0":"import IPython.display\nIPython.display.YouTubeVideo('hhbMpe17fzA', width=800, height=500)","c743a747":"!pip install -q nnAudio","bf397d56":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='tf_efficientnet_b7_ns'\n    model_dir='..\/input\/g2net-efficientnet-b7-baseline-training\/'\n    batch_size=512\n    qtransform_params={\"sr\": 2048, \"fmin\": 20, \"fmax\": 1024, \"hop_length\": 32, \"bins_per_octave\": 8}\n    seed=42\n    target_size=1\n    target_col='target'\n    n_fold=5\n    trn_fold=[0] # [0, 1, 2, 3, 4]","ec29d224":"# ====================================================\n# Library\n# ====================================================\nimport sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom nnAudio.Spectrogram import CQT1992v2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","759a85a2":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = roc_auc_score(y_true, y_pred)\n    return score\n\n\ndef get_result(result_df):\n    preds = result_df['preds'].values\n    labels = result_df['target'].values\n    score = get_score(labels, preds)\n    return score\n    \n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","594fea92":"test = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\n\nif CFG.debug:\n    test = test.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/g2net-gravitational-wave-detection\/test\/{}\/{}\/{}\/{}.npy\".format(\n        image_id[0], image_id[1], image_id[2], image_id)\n\ntest['file_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(test.head())","62b17e69":"# ====================================================\n# Dataset\n# ====================================================\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.wave_transform = CQT1992v2(**CFG.qtransform_params)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def apply_qtransform(self, waves, transform):\n        waves = np.hstack(waves)\n        waves = waves \/ np.max(waves)\n        waves = torch.from_numpy(waves).float()\n        image = transform(waves)\n        return image\n    \n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        waves = np.load(file_path)\n        image = self.apply_qtransform(waves, self.wave_transform)\n        image = image.squeeze().numpy()\n        if self.transform:\n            image = self.transform(image=image)['image']\n        return image","fa151c1d":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            ToTensorV2(),\n        ])","a090c708":"from matplotlib import pyplot as plt\n\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\n\nfor i in range(5):\n    plt.figure(figsize=(16,12))\n    image = test_dataset[i]\n    plt.imshow(image[0])\n    plt.show() ","3b56a36b":"# ====================================================\n# MODEL\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.model = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=1)\n        self.n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(self.n_features, self.cfg.target_size)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output","b20260ff":"# ====================================================\n# inference\n# ====================================================\ndef inference(model, states, test_loader, device):\n    model.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for state in states:\n            model.load_state_dict(state['model'])\n            model.eval()\n            with torch.no_grad():\n                y_preds = model(images)\n            avg_preds.append(y_preds.sigmoid().to('cpu').numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs","7f44b55c":"model = CustomModel(CFG, pretrained=False)\nstates = [torch.load(CFG.model_dir+f'{CFG.model_name}_fold{fold}_best_score.pth') for fold in CFG.trn_fold]\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True)\npredictions = inference(model, states, test_loader, device)","e0fa43b7":"test['target'] = predictions\ntest[['id', 'target']].to_csv('submission.csv', index=False)\ntest.head()","7f175112":"test['target'].hist()","f770f8fb":"# Utils","856eb5aa":"# CFG","ac2483ff":"# Dataset","5d2f6b4e":"# inference","0b9a72df":"# MODEL","16ddb597":"# Transforms","342567ad":"# About this notebook\n- PyTorch tf_efficientnet_b7_ns starter code\n- StratifiedKFold 5 folds\n- Training notebook is [here](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-efficientnet-b7-baseline-training)\n- Spectrogram generation code\n    - https:\/\/www.kaggle.com\/yasufuminakama\/g2net-n-mels-128-train-images is generated by https:\/\/www.kaggle.com\/yasufuminakama\/g2net-spectrogram-generation-train\n    - https:\/\/www.kaggle.com\/yasufuminakama\/g2net-n-mels-128-test-images is generated by https:\/\/www.kaggle.com\/yasufuminakama\/g2net-spectrogram-generation-test\n- version 3: melspectrogram approach using above dataset\n- version 4: nnAudio Q-transform approach\n    - Here is nnAudio Constant Q-transform Demonstration\n        - https:\/\/www.kaggle.com\/atamazian\/nnaudio-constant-q-transform-demonstration\n        - https:\/\/www.kaggle.com\/c\/g2net-gravitational-wave-detection\/discussion\/250621\n    - Thanks for sharing @atamazian\n- version 6: tf_efficientnet_b0_ns -> tf_efficientnet_b7_ns\n- version 7: update model\n\nIf this notebook is helpful, feel free to upvote :)","b3b1288c":"# Data Loading","513ff764":"# Library","e0d3d4ed":"# Submission"}}