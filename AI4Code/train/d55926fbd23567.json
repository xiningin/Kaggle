{"cell_type":{"65b70de9":"code","520e64a6":"code","0ec28944":"code","9878f147":"code","3ae7cf53":"code","85a13f46":"code","add09766":"code","8d091665":"code","f7b9017a":"code","3aef01c6":"code","23187b74":"code","38c7f555":"code","e7779e4d":"code","ec97f9b7":"code","0eb9beaf":"code","35612642":"code","e79e859e":"code","13dd0172":"markdown","4f5b1eb2":"markdown","82994abd":"markdown","ab59bf95":"markdown","7ace64d4":"markdown","9845a2c0":"markdown","4ca6a51d":"markdown","42676418":"markdown"},"source":{"65b70de9":"import os\nimport numpy as np\nimport pandas as pd \nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nfrom os import listdir\nfrom os.path import isfile, join\nfrom PIL import Image\nimport glob\n\n\nseed = 42\nnp.random.seed(seed)\n%matplotlib inline","520e64a6":"tf.__version__","0ec28944":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","9878f147":"    dirname = '\/kaggle\/input'\n    train_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/train')\n    train_normal_pth = os.path.join(train_path, 'NORMAL')\n    train_dme_pth = os.path.join(train_path, 'DME')\n    train_drusen_pth = os.path.join(train_path, 'DRUSEN')\n    train_cnv_pth = os.path.join(train_path, 'CNV')\n    \n    test_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/test')\n    test_normal_pth = os.path.join(test_path, 'NORMAL')\n    test_dme_pth = os.path.join(test_path, 'DME')\n    test_drusen_pth = os.path.join(test_path, 'DRUSEN')\n    test_cnv_pth = os.path.join(test_path, 'CNV')\n    \n    val_path = os.path.join(dirname, 'kermany2018\/OCT2017 \/val')\n    val_normal_pth = os.path.join(val_path, 'NORMAL')\n    val_dme_pth = os.path.join(val_path, 'DME')\n    val_drusen_pth = os.path.join(val_path, 'DRUSEN')\n    val_cnv_pth = os.path.join(val_path, 'CNV')","3ae7cf53":"test_normal_pth","85a13f46":"def plot_imgs(item_dir, num_imgs=4):\n    all_item_dirs = os.listdir(item_dir)\n    item_files = [os.path.join(item_dir, file) for file in all_item_dirs][:num_imgs]\n\n    plt.figure(figsize=(16, 16))\n    for idx, img_path in enumerate(item_files):\n        plt.subplot(1, 4, idx+1)\n\n        img = plt.imread(img_path)\n        plt.imshow(img, cmap='gray')\n\n    plt.tight_layout()","add09766":"plot_imgs(train_normal_pth)","8d091665":"plot_imgs(train_dme_pth)","f7b9017a":"plot_imgs(train_drusen_pth)","3aef01c6":"plot_imgs(train_cnv_pth)","23187b74":"def Images_details_Print_data(data, path):\n    print(\" ====== Images in: \", path)    \n    for k, v in data.items():\n        print(\"%s:\\t%s\" % (k, v))\n\ndef Images_details(path):\n    files = [f for f in glob.glob(path + \"**\/*.*\", recursive=True)]\n    data = {}\n    data['images_count'] = len(files)\n    data['min_width'] = 10**100  # No image will be bigger than that\n    data['max_width'] = 0\n    data['min_height'] = 10**100  # No image will be bigger than that\n    data['max_height'] = 0\n\n\n    for f in files:\n        im = Image.open(f)\n        width, height = im.size\n        data['min_width'] = min(width, data['min_width'])\n        data['min_height'] = min(height, data['min_height'])\n        data['max_width'] = max(width, data['max_height'])\n        \n        data['max_height'] = max(height, data['max_height'])\n\n    Images_details_Print_data(data, path)\n","38c7f555":"Images_details(train_normal_pth)\nImages_details(train_dme_pth)\nImages_details(train_drusen_pth)\nImages_details(train_cnv_pth)","e7779e4d":"Images_details(test_normal_pth)\nImages_details(test_dme_pth)\nImages_details(test_drusen_pth)\nImages_details(test_cnv_pth)","ec97f9b7":"Images_details(val_normal_pth)\nImages_details(val_dme_pth)\nImages_details(val_drusen_pth)\nImages_details(val_cnv_pth)","0eb9beaf":"input_path = \"\/kaggle\/input\/kermany2018\/OCT2017 \/\"\n\nfor _set in ['train', 'test', 'val']:\n    normal = len(os.listdir(input_path + _set + '\/NORMAL'))\n    dme = len(os.listdir(input_path + _set + '\/DME'))\n    drusen = len(os.listdir(input_path + _set + '\/DRUSEN'))\n    cnv = len(os.listdir(input_path + _set + '\/CNV'))\n    print('{}, Normal images: {}, DME images: {}, DRUSEN images: {}, CNV images: {}'.format(_set, normal, dme, drusen, cnv))","35612642":"def process_data(img_dims, batch_size):\n    # Data generation objects\n    train_datagen = ImageDataGenerator(\n        rescale = 1.\/255,\n      #  featurewise_center=True,\n      #  featurewise_std_normalization=True,\n        zoom_range = 0.3,\n        horizontal_flip = True)\n    \n    test_datagen = ImageDataGenerator(\n      #  featurewise_center=True,\n      #  featurewise_std_normalization=True,\n        rescale=1.\/255)\n    \n    # This is fed to the network in the specified batch sizes and image dimensions\n    train_gen = train_datagen.flow_from_directory(\n    directory = train_path, \n    target_size = (img_dims, img_dims), \n    batch_size = batch_size, \n    class_mode = 'categorical', \n    shuffle=True)\n\n    test_gen = test_datagen.flow_from_directory(\n    directory=test_path, \n    target_size=(img_dims, img_dims), \n    batch_size=batch_size, \n    class_mode='categorical', \n    shuffle=True)\n    \n    # I will be making predictions off of the test set in one batch size\n    # This is useful to be able to get the confusion matrix\n    test_data = []\n    test_labels = []\n\n    for cond in ['\/NORMAL\/', '\/DME\/', '\/DRUSEN\/', '\/CNV\/']:\n        for img in (os.listdir(test_path + cond)):\n            img = plt.imread(test_path + cond + img)\n            img = cv2.resize(img, (img_dims, img_dims))\n            img = np.dstack([img, img, img])\n            img = img.astype('float32') \/ 255\n            if cond=='\/NORMAL\/':\n                label = 0\n            elif cond=='\/DME\/':\n                label = 1\n            elif cond=='\/DRUSEN\/':\n                label = 2\n            elif cond=='\/CNV\/':\n                label = 3\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    \n    return train_gen, test_gen, test_data, test_labels","e79e859e":"train_gen, test_gen, test_data, test_labels = process_data(160, 32)","13dd0172":"## DME","4f5b1eb2":"# Details about images dimensions","82994abd":"## DRUSEN","ab59bf95":"# Data augmentation\nThe practice of data augmentation is an effective way to increase the size of the training set.\n\nAugmenting the training examples allow the network to \u201csee\u201d more diversified, but still representative, data points during training.\n\nThere's two data generators: one for training data, and the other for validation data. \nA data generator is capable of loading the required amount of data (a mini batch of images) directly from the source folder, convert them into training data (fed to the model) and training targets (a vector of attributes \u2014 the supervision signal).","7ace64d4":"## CNV","9845a2c0":"## Normal","4ca6a51d":"# Path sets","42676418":"# Let's see what xray photos look like"}}