{"cell_type":{"b8227e10":"code","c5d3d2ea":"code","19f73238":"code","9a31bd23":"code","0a419d56":"code","3e9d436b":"code","89afb846":"code","fff34dff":"code","7ffc0e67":"code","616684f2":"code","824308f1":"code","cdf3f447":"code","51248834":"code","fc7e26f1":"code","c7f8a699":"code","e11ac750":"code","8d3958ca":"code","1a174ac2":"code","654522c3":"code","a265b887":"code","67ce5c85":"code","4f0b4c2a":"code","2741c602":"code","850f12f2":"code","d80d1b72":"code","d40957f4":"code","62c09b6e":"code","2c024fa7":"code","c861a30f":"code","2f06a2bf":"code","c354a59d":"code","a26c2e23":"code","e87dea26":"code","63ee21cb":"code","3080cc7c":"code","93f16ee7":"code","271fd8b4":"markdown","74b7a972":"markdown","cb4a193b":"markdown","0fb8a95d":"markdown","6fc2d821":"markdown","3de2b3b4":"markdown","16157e5e":"markdown","4efb3e63":"markdown","680711b9":"markdown","a541c303":"markdown","7296885f":"markdown","be9cb1eb":"markdown","33df3a80":"markdown","7f0e7f29":"markdown","97fcb5b3":"markdown","81edc7b8":"markdown","1c759c06":"markdown","bec5f1a6":"markdown","c68dc94e":"markdown","4143c62b":"markdown","b8f605c3":"markdown","da196693":"markdown","941b0cee":"markdown","f141a401":"markdown","6e3f210b":"markdown"},"source":{"b8227e10":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport re\nimport gensim \nfrom gensim.models import Word2Vec","c5d3d2ea":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(train_df.shape, test_df.shape)","19f73238":"def clean_text(text):\n    \"\"\"\n    Convert all to lowercase and remove punctuations\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text) # remove everything that isn't word or space\n    text = re.sub(r'\\_', '', text)      # remove underscore\n    return text","9a31bd23":"# clean train_df['text']\ntrain_df['text'] = train_df['text'].map(lambda x: clean_text(x))\ntrain_df['text'] = train_df['text'].map(lambda x: x.strip().split())\ntrain_df.head()","0a419d56":"# clean test_df['text']\ntest_df['text'] = test_df['text'].map(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].map(lambda x: x.strip().split())\ntest_df.head()","3e9d436b":"data = []  \n# iterate through each row in train_df \nfor i in range(len(train_df)):\n    data.append(train_df['text'][i])\nfor j in range(len(test_df)):\n    data.append(test_df['text'][j])","89afb846":"print(len(data))","fff34dff":"# Create Word2Vec model using CBOW (sg=0)\n# Set min_count to 1 so as to include all words\nembedding = gensim.models.Word2Vec(data, size=50, window=10, min_count=1, sg=0)","7ffc0e67":"print(embedding)","616684f2":"# train & generate the embeddings\nembedding.train(data,total_examples=len(data),epochs=30)","824308f1":"words = list(embedding.wv.vocab)\nprint(len(words))","cdf3f447":"print(embedding['capered'])","51248834":"embedding.most_similar('dark', topn=5)","fc7e26f1":"embedding.most_similar('shocked', topn=5)","c7f8a699":"embedding.most_similar('sprang', topn=5)","e11ac750":"embedding.most_similar('pride', topn=5)","8d3958ca":"# convert author labels into one-hot encodings\ntrain_df['author'] = pd.Categorical(train_df['author'])\ndf_Dummies = pd.get_dummies(train_df['author'], prefix='author')\ntrain_df = pd.concat([train_df, df_Dummies], axis=1)\n# Check the conversion\ntrain_df.head()","1a174ac2":"X = train_df['text'].str[:100]\nY = train_df[['author_EAP', 'author_HPL', 'author_MWS']].values\nprint(X.shape, X[0], Y.shape, Y[0])","654522c3":"X_test = test_df['text'].str[:50]\nprint(X_test.shape, X_test[0])","a265b887":"def text_to_avg(text):\n    \"\"\"\n    Given a list of words, extract the respective word embeddings\n    and average the values into a single vector encoding the text meaning.\n    \"\"\"\n    # initialize the average word vector\n    avg = np.zeros((50,))\n    # average the word vector by looping over the words in text\n    for w in text:\n        avg += embedding[w]\n    avg = avg\/len(text)\n    return avg","67ce5c85":"X_avg = np.zeros((X.shape[0], 50)) # initialize X_avg\nfor i in range(X.shape[0]):\n    X_avg[i] = text_to_avg(X[i])","4f0b4c2a":"print(X_avg.shape)\nprint(X_avg[0])","2741c602":"X_test_avg = np.zeros((X_test.shape[0], 50)) # initialize X_test_avg\nfor i in range(X_test.shape[0]):\n    X_test_avg[i] = text_to_avg(X_test[i])","850f12f2":"print(X_test_avg.shape)\nprint(X_test_avg[0])","d80d1b72":"from sklearn.model_selection import train_test_split\nX_train, X_dev, Y_train, Y_dev = train_test_split(X_avg, Y, test_size=0.2, random_state=123)\nprint(X_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape)","d40957f4":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_shape=(50,)))\nmodel.add(layers.Dense(3, activation='softmax'))\n\nmodel.summary()","62c09b6e":"# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","2c024fa7":"# train and validate the model\nepochs = 50\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=128, validation_data=(X_dev, Y_dev))","c861a30f":"# plot and visualise the training and validation losses\nloss = history.history['loss']\ndev_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='training loss')\nplt.plot(epochs, dev_loss, 'b', label='validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","2f06a2bf":"model = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_shape=(50,)))\nmodel.add(layers.Dense(3, activation='softmax'))\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","c354a59d":"# train the model\nepochs = 10\nmodel.fit(X_avg, Y, epochs=epochs, batch_size=128)","a26c2e23":"# predict on test set\npreds = model.predict(X_test_avg)\nprint(preds.shape)\nprint(preds[7])","e87dea26":"# set the predicted labels to be the one with the highest probability\npred_labels = []\nfor i in range(len(X_test_avg)):\n    pred_label = np.argmax(preds[i])\n    pred_labels.append(pred_label)","63ee21cb":"print(pred_labels[7])","3080cc7c":"result = pd.DataFrame(preds, columns=['EAP','HPL','MWS'])\nresult.insert(0, 'id', test_df['id'])\nresult.head()","93f16ee7":"# Generate submission file in csv format\nresult.to_csv('submission.csv', index=False, float_format='%.20f')","271fd8b4":"#### Let's use the generated Word2Vec embeddings for the \"Spooky Authors Identification\" problem.","74b7a972":"* One-hot encode the target variable to facilitate modelling.","cb4a193b":"#### Each word in the above corpus is expressed as a 50-dimension vector as shown below.","0fb8a95d":"# Generate word embeddings with Word2Vec","6fc2d821":"#### We can try pick random words in the corpus and find out what are the most similar words according to this generated Word2Vec model. Let's try a few examples.","3de2b3b4":"#### Create corpus from all the words in the train & test datasets, and use Word2Vec to generate the word embeddings.","16157e5e":"#### Thank you for reading this.\n#### Please upvote if you find it useful. Cheers!","4efb3e63":"# Introduction","680711b9":"#### Let's examine the generated Word2Vec model we have created.","a541c303":"# Use the word embeddings","7296885f":"#### The size of the vocabulary is 28,727 words. I am using a smaller dimension of 50 as I thought the vocab size is small.","be9cb1eb":"### Train the model","33df3a80":"#### Let's clean up the text; make lowercase & remove punctuations. Then split the text into words. This prepares the text for use in Gensim's Word2Vec.","7f0e7f29":"## Re-train with the entire training set","97fcb5b3":"### Build the dense neural network model from keras","81edc7b8":"### Predict on the test set and compute the probabilities for submission","1c759c06":"# Examine the generated word embeddings","bec5f1a6":"* Create X and Y from train_df, test_df, limiting to first 100 words.","c68dc94e":"### Split train data into a train and a dev set","4143c62b":"## Baseline model\n#### One simple way to use word embeddings would be to average the word embeddings of words in the text, and then feed into a softmax layer (3 classes) for training. To predict the class, I would pass the average word embeddings of the test text and determine the class with the highest probability.","b8f605c3":"Started on 30 May 2019","da196693":"# Create submission file","941b0cee":"#### Looking at the above words listed as similar (vectors which are closer to each other), I thought the word embeddings are quite decent.","f141a401":"#### Here I explore using word embeddings on the \"Spooky Author Identification\" datasets.\n#### I am using Gensim's Word2Vec to generate the word representations from all the text in the training and test sets.","6e3f210b":"# Load data"}}