{"cell_type":{"c7b0c171":"code","4de3f893":"code","3ea01e20":"code","cd937e5f":"code","c56454e4":"markdown","0169f1ab":"markdown","c6d819f8":"markdown","aa4acbf1":"markdown"},"source":{"c7b0c171":"# imports and data loading. remove instances where there are nan values\nimport numpy as np \nimport pandas as pd \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_validate\n\nhr_temp= pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\nhr= hr_temp.dropna()\nprint(np.shape(hr))\nhr.head()","4de3f893":"# for categorical fields with only 2 possible values, convert those to be numerical (i.e. value of 0\/1)\nhr_num= hr.copy()\nhr_num.loc[hr['relevent_experience'] == 'No relevent experience', 'relevent_experience'] = 0\nhr_num.loc[hr['relevent_experience'] == 'Has relevent experience', 'relevent_experience'] = 1\nhr_num['relevent_experience']= hr_num['relevent_experience'].astype(int)\n\n# for categorical fields with a mix of numerical and text values, convert the text values to numbers\nhr_num.loc[hr['experience'] == '>20', 'experience']= 20\nhr_num.loc[hr['experience'] == '<1', 'experience']= 1\nhr_num['experience']= hr_num['experience'].astype(int)\n\nhr_num.loc[hr['last_new_job'] == '>4', 'last_new_job']= 4\nhr_num.loc[hr['last_new_job']== 'never', 'last_new_job']= 100\nhr_num['last_new_job']= hr_num['last_new_job'].astype(int)\n\n# use ordinal encoding for gender, education_level, enrolled_university, company_size\ndef ord_encode(df, col_name, cats):\n    df[col_name]= OrdinalEncoder(categories= [cats], dtype= int).fit_transform(df[col_name].values.reshape(-1,1))\n    return df\nhr_num= ord_encode(hr_num, 'gender', ['Male', 'Female', 'Other'])\nhr_num= ord_encode(hr_num, 'education_level', ['Graduate', 'Masters', 'Phd'])\nhr_num= ord_encode(hr_num, 'enrolled_university', ['no_enrollment', 'Part time course', 'Full time course'])\nhr_num= ord_encode(hr_num, 'company_size', ['<10', '10\/49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+'])\n\n# use one-hot-encoding for remaing categorical variables: major_discipline, and company_type\n# first specify which columns are the features and which column is the labels\nX= hr_num.drop(['city','enrollee_id', 'training_hours', 'target'], axis= 1)\nY= hr_num['target'].astype(int)\n\n# define function that uses the names of the categorical columns to be one-hot-encoded and the original feature matrix\n# function returns a strictly numeric feature matrix (i.e. categorical columns are one-hot-coded and the original categorical columns are removed)\ndef ohe_features(cat_cols, X):\n    ohe_transform= ColumnTransformer(transformers= [('cat', OneHotEncoder(), cat_cols)], sparse_threshold= 0)\n    ohe_df= pd.DataFrame(ohe_transform.fit_transform(X), columns= ohe_transform.get_feature_names())\n    X[ohe_transform.get_feature_names()]= ohe_transform.fit_transform(X)\n    X= X.drop(cat_cols, axis= 1)\n    return X \n\ncat_cols= ['major_discipline', 'company_type']\nX= ohe_features(cat_cols, X)","3ea01e20":"# define decision tree model, the different hyperparameter settings to test and the grid search\ndt= DecisionTreeClassifier()\ndt_grid= {'max_depth': [None, 2, 5, 10, 15], 'class_weight': [None, 'balanced'], 'min_samples_split': [2, 10, 100, 500], 'random_state': [24]}\nsearch= GridSearchCV(dt, dt_grid, scoring= 'roc_auc', n_jobs= -1, cv= 10)\n\n# run the grid search and get the best hyperparameter settings (the settings that give the best ROC-AUC score)\nsearch.fit(X, Y)\nprint('Best hyperparameter settings:', search.best_params_)\nprint('Corresponding ROC-AUC score (avg of 10-fold CV):', search.best_score_)","cd937e5f":"xgb= XGBClassifier(objective= 'binary:logistic', random_state= 24, n_estimators= 100, max_depth= 3, learning_rate= 0.1, alpha= 10)\nxgb_cv= cross_validate(xgb, X, Y, n_jobs= -1, scoring= 'roc_auc', cv= 10)\nprint('XGBoost model ROC-AUC score (avg of 10-fold CV):', np.mean(xgb_cv['test_score']))","c56454e4":"# Hyperparameter Grid Search\n\nEvery machine learning model has settings (\"hyperparameters\") that are not learned by the dataset, but are instead set by the person implementing the machine learning model. To know which values to assign these hyperparameters, we run something called a hyperparameter grid search, where we try a bunch of different combinations of hyperparameter values and see which gives us the best results. The hyperparameters differ depending on the model, so here we will use a decision tree and try out different values for the following hyperparameter settings: max depth of tree, class weight, and min number of samples needed to split an internal node.\n\n*The hyperparameter grid search should only be run on the training set! For this particular dataset, there are two separate files for the training and test sets, so we don't need to split the data ourselves.*","0169f1ab":"We can see that we achieve slightly better performance with the XGBoost model compared to the decision tree model. From here, we can continue to try out different models until we achieve a ROC-AUC score we are happy with and then use our final model to evaluate predictions on our test set.\n\n# Conclusions\nRecognizing that the standalone variables in the HR Analytics dataset are not good indicators of whether a candidate will quit, this notebook aimed to develop and train 2 machine learning models with the goal of predicting whether a candidate will quit. Although the performance of the models were not outstanding, this notebook provides examples on how to implement ordinal encoding, one-hot encoding, a hyperparameter grid search, and cross-validation of a model. For more examples of my work, including feature selection techniques, check out the links below:\n\nhttps:\/\/www.kaggle.com\/valbauman\/random-forests-for-thermal-comfort\n\nhttps:\/\/www.kaggle.com\/valbauman\/feature-selection-for-heart-disease-prediction\n\n(don't get me wrong, I love implementing ML models as much as the next person, but data wrangling and cleaning before implementing a ML model is just as important!)","c6d819f8":"# Preamble\n\nThis Kaggle notebook is a follow-up on the exploratory analysis conducted in a separate Power BI dashboard. In the dashboard, it was realized that no standalone single variable is indicative of whether a candidate will quit after training. **Based on this, if the company wants to predict whether a person will quit after completing their training courses, a more sophisticated approach (compared to just looking at the values of certain fields) is needed. This motivates the development and validation of a machine learning model that uses only the information from the candidate that would be known prior to them being hired.** If such a model is successful, the company could use this to better predict whether a candidate will quit after completing their training courses.\n\n*Since it was already completed in the Power BI dashboard, this notebook doesn't include any exploratory data visualization. Instead, the model jumps right into necessary data transformations and machine learning model devleopment.*\n\n## Illustrated concepts\n- Ordinal encoding\n- One-hot encoding\n- Hyperparameter grid search for a machine learning model\n- Cross-validation \n\n# Imports, Data Encoding\nData cleaning steps include omitting rows that contain nan values and encoding categorical data. Ordinal encoding is used for \"gender\", \"education level\", \"enrolled university\", and \"company_size\". Categories with a mix of numerical and text values (i.e. 20 vs. \">20\") are changed to be strictly numerical with the largest value being the value included with a \">\" or \"<\" symbol (e.g. \">20\" becomes 20). For categorical variables \"major discipline\" and \"company type\", one hot encoding is used. All other variables are dropped because of either high cardinality (\"city\"), they use information that would be available only after hiring (\"training hours\"), they are the label (\"target\"), or they contain person-specific information that won't help with predictions (\"enrollee id\"). ","aa4acbf1":"An ROC-AUC score of 0.76 is not very good - 1 is a perfect score and means that the model has no trouble distinguishing between the two classes. We will try one more model to see if we can improve our cross validation score before we move on to evaluating the test set. XGBoost is an ensemble method that combines the opinions of multiple weak learners (e.g. weak decision trees) to come up with the final prediction for an observation. Here we won't run a grid search to determine the best hyperparameter settings, we will just use arbitary settings and run our cross-validation."}}