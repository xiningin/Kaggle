{"cell_type":{"235f4f43":"code","7dc3dc5c":"code","70473e58":"code","15e82a31":"code","e06ffb77":"code","495bddf4":"code","ed520c04":"code","b8d7abf3":"code","dfefcf37":"code","fcf3ee95":"code","bf791566":"code","fc9f3da0":"code","f45ea6f2":"code","c0137eb7":"code","70badc90":"code","3766eb27":"code","dcb95852":"code","914780e4":"code","c5b8fe4a":"code","dc32c579":"code","572134d2":"code","852b5f26":"code","ec1754e0":"code","0d5ed54d":"code","47e1eb15":"code","ae19fed9":"code","807e54ea":"code","970fe079":"code","67f3af2d":"code","3d4db298":"code","cb7ef13b":"code","c9a2af85":"code","832cd29f":"code","082f72f5":"code","440e5250":"code","e91f1a50":"code","b207eaa3":"code","fec6977b":"code","4a85cdb7":"code","fa583101":"code","518332f9":"code","4ad1d119":"code","1576eb62":"code","74261342":"code","fab45db7":"code","01944be4":"code","f6304fd3":"code","494a5345":"code","e42859c1":"code","48149240":"markdown","746ca7ad":"markdown","55edf17a":"markdown","64efa459":"markdown","8a5ef080":"markdown","4c3110ca":"markdown","edc93173":"markdown","5139eef8":"markdown","5ec233bf":"markdown","b616701e":"markdown","a1c8c405":"markdown","0415799f":"markdown","ab2c9145":"markdown","ba360062":"markdown","81c04c55":"markdown","e97ff784":"markdown","38b8e536":"markdown","98aa8337":"markdown","a16d6633":"markdown","ea0a3fd5":"markdown","f31efdb8":"markdown","d8cbffde":"markdown","511104be":"markdown","5c90fa19":"markdown","d370762b":"markdown","ab5cffcc":"markdown","871a2154":"markdown","33ae881a":"markdown","9b06b257":"markdown","393ac082":"markdown","54b224b8":"markdown","8d6287db":"markdown","0beff618":"markdown","fc1a3474":"markdown","26632193":"markdown","cae2616b":"markdown","96ee8bdc":"markdown","583e537a":"markdown","11ee81a0":"markdown","067f698e":"markdown","2b7eb124":"markdown","af07b15b":"markdown","fe32c144":"markdown","d6b70b92":"markdown","31c1256a":"markdown","8a54048c":"markdown","36a1543d":"markdown","be052bf2":"markdown","ef5bd63d":"markdown"},"source":{"235f4f43":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dc3dc5c":"import math\nimport os\nimport gc\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\n\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n  display(Markdown(string))\n\n\n%matplotlib inline\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nplt.rcParams['figure.figsize'] = 12, 8\n# plt.rcParams['figure.facecolor'] = 'white'\n# sns.set_style('whitegrid')\n\n!pip install --upgrade plotly\n!pip install -U seaborn\n\nfrom sklearn import preprocessing\nfrom yellowbrick.target import FeatureCorrelation\nfrom sklearn.feature_selection import mutual_info_classif\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, RepeatedStratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import matthews_corrcoef, roc_auc_score, precision_recall_curve, confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.utils import resample\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Boosting Algorithms \n!pip install catboost\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier, Pool\nfrom lightgbm                         import LGBMClassifier\n\n#Conventional Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n#Hyper Parameter Tuning\n!pip install optuna\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# For statistical Test\n!pip install scipy==1.7.1\nfrom scipy import stats                         \n# from scipy.stats.contingency import association\nfrom scipy.stats import contingency\n\nimport multiprocessing\nimport pickle, joblib\n","70473e58":"# set optuna verbosity level\noptuna_verbosity = optuna.logging.WARNING # https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/logging.html#module-optuna.logging\n\n\nSEED = 42\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","15e82a31":"df = pd.read_csv('\/kaggle\/input\/alc-datathon-2021\/covid_mental_health_train.csv')\ndf.head(10)\n","e06ffb77":"df.info()","495bddf4":"df.describe().T","ed520c04":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent*100], axis=1, keys=['Total', 'Percent'])\ndisplay(missing_data.head(5))\nprintmd(\"Only **income_group** has 15.42% missing values\")","b8d7abf3":"printmd(\"### Missing Values in Income Group\")\ndf_null = df[df['income_group'].isnull() == True].reset_index(drop=True)\ndisplay(df_null.head(5))\n\nprintmd(\"### Income Group 'Unknown (10.0)'\")\ndf_unknown = df[df['income_group'] == 10]\ndisplay(df_unknown.head(5))\n\nprintmd(\"<br>\")\nprintmd(\"#### As it is not possible to identify income groups using conventional imputation methods, we are including the missing values into the 'Unknown (10.0)' income group\")\ndf.fillna(10.0, inplace=True)\n\ndisplay(df.isnull().sum())\n","dfefcf37":"# Convert the float64 into int64\ndisplay(df[['income_group']].dtypes)\nprintmd(\"#### As 'income_group' is a category, we will convert the data type into int64 from float64\")\n\ndf[\"income_group\"] = df['income_group'].astype('int64')\ndisplay(df[['income_group']].dtypes)","fcf3ee95":"# Transforming the Gender into Male and Female for a better visualization\ndef label_gender(row):\n  if row['gender'] == 1:\n    return \"male\"\n  else:\n    return \"female\"\ndf['gender_label'] = df.apply(lambda row: label_gender(row), axis=1)\n\n# Transforming the Status into Single and Married for a better visualization\ndef label_family(row):\n  if row['family_status'] == 1:\n    return \"single\"\n  else:\n    return \"married\"\ndf['marital_status'] = df.apply(lambda row: label_family(row), axis=1)\n","bf791566":"p = sns.catplot(data= df, x=\"city\", hue = 'depression', col = 'gender_label',  kind = 'count', col_wrap = 2)\n\np.fig.suptitle(\"Participants' Count From Different Cities\")\np.fig.subplots_adjust(top=0.81,right=0.86)\nplt.show()\n\nfemale = df[((df['gender_label']==\"female\") & (df['depression']==1))].id.value_counts().sum()\/df[df['depression']==1].id.value_counts().sum()*100\nmale = df[((df['gender_label']==\"male\") & (df['depression']==1))].id.value_counts().sum()\/df[df['depression']==1].id.value_counts().sum()*100\n\nprintmd(\"From the above graph it can be seen that among the depressed participants, females are more than the males. Here, \"+ \"**{:0.2f}**\".format(female) + \"% of the depressed participants are females whereas only \" + \"**{:0.2f}**\".format(male) + \"% are males\")","fc9f3da0":"min = df['age'].min()\nmax = df['age'].max()\nfig = sns.histplot(data=df, x='age')\nfig.set_title(\"Age Distribution of the Participants\")\nfig.set_xticks(range(min, max, 10))\nfig.set_xticklabels(['18','28','38', '48', '58', '68', '78', '89'])\nplt.show()\n\nprintmd(\"Participants' age vary from 18 to 89 years. For a better understanding of the relationship between age and other attributes, we are grouping the participants into three age groups\")","f45ea6f2":"age_labels = ['young_adults', 'middle_aged', 'older_adults']\n# cut_bins = np.linspace(min, max, 4)\ncut_bins = [17, 35, 55, 90]\n\ndf['age_group'] = pd.cut(df['age'], bins=cut_bins, labels=age_labels)\n\nfig = sns.countplot(data=df, x='age_group')\nplt.title(\"Age Distribution after binned\")\nplt.show()\n\nprintmd(\"**Age Group Range**\")\nprintmd(\"Young Adults  (18 - 35) <br> Middle Aged  (36 - 55) <br> Older Adult  (56 - 89) <br>\")\ndisplay(df[['age', 'age_group']].head())\n\nprintmd(\"<br> **Participants per Age Group**\")\ndisplay(df['age_group'].value_counts())","c0137eb7":"plt_df = df['depression'].value_counts().reset_index().rename(columns={'index':'depression', 'depression':'count'})\nfig = px.pie(plt_df, values='count', names='depression', title='Distribution of the Target (Depression 1 = Yes, 0 = No)')\nfig.show()\n\nprintmd(\"From the above chart, it is certain that the target variable is heavily imbalanced\")","70badc90":"fig = px.sunburst(df, path=[\"age_group\",\"gender_label\", \"depression\"], title=\"Depression among Different Age & Gender Demographic (0 = Depression No, 1 = Depression Yes)\")\nfig.show()\n\nprintmd(\"From the above chart we can say,\")\nprintmd(\"- The majority of our participants are middle aged (36-55 years)\")\nprintmd(\"- Older (56-89 years) participants are the least depressed (~15%)\")\nprintmd(\"- Middle aged participants are the most depressed (~48%)\")\nprintmd(\"- Conversely, depression rate among the young adults (18-35 years) are ~23% whereas, among the middle aged, its ~19%\")","3766eb27":"f = sns.catplot(data= df, x=\"marital_status\", hue = 'gender_label', col = 'depression',  kind = 'count', col_wrap = 2)\nf.fig.suptitle('Relation between Depression and Marital Status with respect to Gender')\nf.fig.subplots_adjust(top=0.81,right=0.86)\nplt.show()\n\nprintmd(\"#### Most of the participants are married. Moreover, both married and single women are more depressed than the men.\")","dcb95852":"# printmd(\"##Effects of Physical and Mental Health on Depression\")\n\nfig,((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\nfig.set_size_inches(15, 10)\n\nsns.countplot(data=df, x='current_physical', hue='depression', ax=ax1)\nsns.countplot(data=df, x='past_physical',  hue='depression', ax=ax2)\nsns.countplot(data=df, x='current_mental',  hue='depression', ax=ax3)\nsns.countplot(data=df, x='past_mental',  hue='depression', ax=ax4)\nfig.suptitle(\"Effects of Physical and Mental Health (1 = Treatment Yes, 2 = Treatment No) on Depression (0 = No, 1 = Yes)\", fontsize=20)\nplt.show()\n\nprintmd(\"#### The majority of the participants are free from physical and mental ailments. Participants currently seeking treatment for mental health are slightly more depressed.\")","914780e4":"df_non_depressed = df[df['depression'] == 0].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=2, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_non_depressed['optimism'].value_counts().index, \n                     values=df_non_depressed['optimism'].value_counts().values, name=\"Optimism\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['frustration'].value_counts().index, \n                     values=df_non_depressed['frustration'].value_counts().values, name=\"Frustration\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['covid_anxiety'].value_counts().index, \n                     values=df_non_depressed['covid_anxiety'].value_counts().values, name=\"Covid Anxiety\"), 2, 1)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['covid_sleepless'].value_counts().index, \n                     values=df_non_depressed['covid_sleepless'].value_counts().values, name=\"Covid Sleepless\"), 2, 2)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations\n    annotations=[dict(text='Optimism', x=0.170, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Frustration', x=0.820, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Covid Anxiety', x=0.150, y=0.45, font_size=20, showarrow=False),\n                 dict(text='Covid Sleepless', x=0.850, y=0.45, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=20, l=0, r=0), title = 'Mental State of Participants identified without Depression during covid (1 = not at all, 7 = extremely)')\nfig.show()\n\nprintmd(\"For the charts we can assert that -\")\nprintmd(\"- Majority did not have frustration\")\nprintmd(\"- Majority had an optimistic outlook\")\nprintmd(\"- Majority did not suffer sleeplessness due to covid, although they had some covid related anxiety\")","c5b8fe4a":"df_depressed = df[df['depression'] == 1].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=2, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_depressed['optimism'].value_counts().index, \n                     values=df_depressed['optimism'].value_counts().values, name=\"Optimism\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_depressed['frustration'].value_counts().index, \n                     values=df_depressed['frustration'].value_counts().values, name=\"Frustration\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_depressed['covid_anxiety'].value_counts().index, \n                     values=df_depressed['covid_anxiety'].value_counts().values, name=\"Covid Anxiety\"), 2, 1)\n\nfig.add_trace(go.Pie(labels=df_depressed['covid_sleepless'].value_counts().index, \n                     values=df_depressed['covid_sleepless'].value_counts().values, name=\"Covid Sleepless\"), 2, 2)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations\n    annotations=[dict(text='Optimism', x=0.170, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Frustration', x=0.820, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Covid Anxiety', x=0.150, y=0.45, font_size=20, showarrow=False),\n                 dict(text='Covid Sleepless', x=0.850, y=0.45, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=20, l=0, r=0), title = 'Mental State of Participants identified with Depression during covid (1 = not at all, 7 = extremely)')\nfig.show()\n\nprintmd(\"For the charts we can assert that -\")\nprintmd(\"- Majority had an high level of frustration\")\nprintmd(\"- Majority were somewhat optimistic\")\nprintmd(\"- They suffered more sleeplessness in comparison with non depressed participants\")\nprintmd(\"- They also had high covid related anxiety\")","dc32c579":"df_depressed = df[df['depression'] == 1].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=2, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_depressed['exercise'].value_counts().index, \n                     values=df_depressed['exercise'].value_counts().values, name=\"Exercise\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_depressed['activity'].value_counts().index, \n                     values=df_depressed['activity'].value_counts().values, name=\"Activity\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_depressed['healthy_diet'].value_counts().index, \n                     values=df_depressed['healthy_diet'].value_counts().values, name=\"Healthy diet\"), 2, 1)\n\nfig.add_trace(go.Pie(labels=df_depressed['healthy_sleep'].value_counts().index, \n                     values=df_depressed['healthy_sleep'].value_counts().values, name=\"Healthy sleep\"), 2, 2)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations\n    annotations=[dict(text='Exercise', x=0.170, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Activity', x=0.820, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Healthy diet', x=0.150, y=0.45, font_size=20, showarrow=False),\n                 dict(text='Healthy sleep', x=0.850, y=0.45, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=20, l=0, r=0), title = 'Life Style of Participants identified with Depression during covid (1 = not at all, 7 = extremely)')\nfig.show()\n","572134d2":"df_non_depressed = df[df['depression'] == 0].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=2, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_non_depressed['exercise'].value_counts().index, \n                     values=df_non_depressed['exercise'].value_counts().values, name=\"Exercise\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['activity'].value_counts().index, \n                     values=df_non_depressed['activity'].value_counts().values, name=\"Activity\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['healthy_diet'].value_counts().index, \n                     values=df_non_depressed['healthy_diet'].value_counts().values, name=\"Healthy diet\"), 2, 1)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['healthy_sleep'].value_counts().index, \n                     values=df_non_depressed['healthy_sleep'].value_counts().values, name=\"Healthy sleep\"), 2, 2)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations\n    annotations=[dict(text='Exercise', x=0.170, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Activity', x=0.820, y=0.55, font_size=20, showarrow=False),\n                 dict(text='Healthy diet', x=0.150, y=0.45, font_size=20, showarrow=False),\n                 dict(text='Healthy sleep', x=0.850, y=0.45, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=20, l=0, r=0), title = 'Life Style of Participants identified without Depression during covid (1 = not at all, 7 = extremely)')\nfig.show()","852b5f26":"f = sns.catplot(data= df, x=\"job\", hue = 'gender_label', col = 'depression',  kind = 'count', col_wrap = 2)\nf.fig.suptitle('Relation between Depression and Job with respect to Gender')\nf.fig.subplots_adjust(top=0.81,right=0.86)\nplt.show()","ec1754e0":"df_depressed = df[df['depression'] == 1].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_depressed['difficulty_living'].value_counts().index, \n                     values=df_depressed['difficulty_living'].value_counts().values, name=\"Living\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_depressed['difficulty_work'].value_counts().index, \n                     values=df_depressed['difficulty_work'].value_counts().values, name=\"Work\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_depressed['deterioration_economy'].value_counts().index, \n                     values=df_depressed['deterioration_economy'].value_counts().values, name=\"Economy\"), 1, 3)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations \n    annotations=[dict(text='Difficulty Living', x=0.100, y=0.01, font_size=20, showarrow=False),\n                 dict(text='Difficulty Work', x=0.480, y=0.01, font_size=20, showarrow=False),\n                 dict(text='Deterioration Economy', x=0.980, y=0.01, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=0, l=0, r=0), title = 'Living, Work & Economic State of Participants identified with Depression (1 = not at all, 7 = extremely)')\nfig.show()\n\nprintmd(\"#### People who had depression responded to have more difficulty in their living and work situation, in addition with a worsening economical state\")","0d5ed54d":"df_non_depressed = df[df['depression'] == 0].reset_index(drop=True)\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df_non_depressed['difficulty_living'].value_counts().index, \n                     values=df_non_depressed['difficulty_living'].value_counts().values, name=\"Living\"), 1, 1)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['difficulty_work'].value_counts().index, \n                     values=df_non_depressed['difficulty_work'].value_counts().values, name=\"Work\"), 1, 2)\n\nfig.add_trace(go.Pie(labels=df_non_depressed['deterioration_economy'].value_counts().index, \n                     values=df_non_depressed['deterioration_economy'].value_counts().values, name=\"Economy\"), 1, 3)\n\n# donut-like pie chart\n# fig.update_traces(hoverinfo=\"label+percent\")\n\nfig.update_layout(\n\n    # Add annotations \n    annotations=[dict(text='Difficulty Living', x=0.100, y=0.01, font_size=20, showarrow=False),\n                 dict(text='Difficulty Work', x=0.480, y=0.01, font_size=20, showarrow=False),\n                 dict(text='Deterioration Economy', x=0.980, y=0.01, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=50, b=0, l=0, r=0), title = 'Living, Work & Economic State of Participants identified without Depression (1 = not at all, 7 = extremely)')\nfig.show()\n\nprintmd(\"#### People who did not have depression responded to have less difficulty in their living, working and economical state\")","47e1eb15":"health_worker_df = df[df['health_worker_self'] == 1]\nhealth_family_df = df[df['health_worker_family'] == 1]\n\nw = sns.catplot(data=health_worker_df, x='depression', hue='gender_label', col='gender_label', kind = 'count', col_wrap = 2)\nw.fig.subplots_adjust(top=0.81,right=0.86,bottom=0.10)\nw.fig.suptitle('Depression among the Participants who work in Health care')\n\nf = sns.catplot(data=health_family_df, x='depression', hue='gender_label', col='gender_label', kind = 'count', col_wrap = 2)\nf.fig.subplots_adjust(top=0.81,right=0.86)\nf.fig.suptitle('Depression among the Participants whom family members work in Health care')\nplt.show()\n\nprintmd(\"From the above graphs we can see,\")\nprintmd(\"- More female participants work in the health care sector than male\")\nprintmd(\"- Women health care workers are relatively less susceptible to depression\")\nprintmd(\"- Women participants who reported to have family members working in health care had more depression than their male counterparts\")","ae19fed9":"# Dropping the unnecessary columns\ndf.drop(columns=['gender_label', 'marital_status', 'age', 'id'], axis=1, inplace=True)\ndf.columns","807e54ea":"# Converting age_group \nprintmd(\"Label Encoding **age_group**\")\ndisplay(\"Before Label encoding\", df['age_group'].unique())\nle = preprocessing.LabelEncoder()\ndf['age_group'] = le.fit_transform(df['age_group'])\ndisplay(\"After Label Encoding\", df['age_group'].unique())","970fe079":"features = df.drop(columns=['depression'], axis=1)\ntarget = df['depression']\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: features[col].nunique(), features))\ndict_features_by_col = dict(zip(features, object_nunique))\n\n\n# Print number of unique entries by column, in ascending order\nprint(\"Number of Unique Categories per Feature:\")\nprint(sorted(dict_features_by_col.items(), key=lambda x: x[1]))\n\nordinal_features = ['exercise', 'healthy_diet', 'healthy_sleep', 'activity', 'interaction_offline', 'interaction_online', 'preventive_behaviors', 'optimism', 'deterioration_economy', 'deterioration_interact', 'frustration', 'covid_anxiety', 'covid_sleepless', 'difficulty_living', 'difficulty_work']\ndichotomous_features = ['gender', 'health_worker_self', 'health_worker_family', 'family_status', 'current_physical', 'past_physical', 'current_mental', 'past_mental']\npolytomous_features = ['age_group', 'job', 'city', 'income_group']","67f3af2d":"def phi_coeff(crosstab):\n\n  a = crosstab[0][1]\n  c = crosstab[0][2]\n  b = crosstab[1][1]\n  d= crosstab[1][2]\n\n  demon = np.sqrt((a+b)*(c+d)*(a+c)*(b+d))\n  nom = (a*d) - (b*c)\n\n  phi = nom \/ demon \n  return phi\n\ncoeff_dic = {}\n\nfor feature in dichotomous_features:\n  crosstab = pd.crosstab(df[feature], df['depression'])\n  coeff_dic[feature] = phi_coeff(crosstab)\n\ncoeff_dic_sorted = sorted(coeff_dic.items(), key=lambda x:x[1], reverse=True)\n\nprintmd(\"**Correlation Between Dichotomous Features and Target**\")\nfor k,v in coeff_dic_sorted:\n  print(k, \": \", v)\n\n#Credit https:\/\/www.statisticshowto.com\/phi-coefficient-mean-square-contingency-coefficient\/","3d4db298":"def cramer_v(crosstab):\n  coeff = stats.contingency.association(crosstab, method='cramer')\n  return coeff\n\ncoeff_dic = {}\n\nfor feature in polytomous_features:\n  crosstab = pd.crosstab(df[feature], df['depression'])\n  coeff_dic[feature] = cramer_v(crosstab)\n\ncoeff_dic_sorted = sorted(coeff_dic.items(), key=lambda x:x[1], reverse=True)\n\nprintmd(\"Correlation Between Polytomous Features and Target\")\nfor k,v in coeff_dic_sorted:\n  print(k, \": \", v)\n\nprintmd(\"**Here ```age_group``` has a Strong Relationship**\")\n\n#Credit https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/","cb7ef13b":"def kendall_tau(feature):\n\n  coef, p_value = stats.kendalltau(df[feature], df['depression'])\n  print('Kendall Tau coefficient = %.5f, P value = %.5f' % (coef, p_value))\n\n  # interpret the significance\n  alpha = 0.05\n  if p_value > alpha:\n    print('Samples are uncorrelated (fail to reject Null Hypothesis) p=%.3f' % p_value)\n  else:\n    print('Samples are correlated (reject Null Hypothesis, accept the Alternative Hypothesis) p=%.3f' % p_value)\n  print('----\\n')\n\nfor feature in ordinal_features:\n  printmd(f\"Correlation with **{feature}** and **depression**\")\n  kendall_tau(feature)","c9a2af85":"df_dummies = df.copy()\n\ncolumns = ['city', 'gender', 'job', 'health_worker_self', 'health_worker_family',\n       'family_status', 'income_group', 'current_physical', 'past_physical',\n       'current_mental', 'past_mental', 'exercise', 'healthy_diet',\n       'healthy_sleep', 'activity', 'interaction_offline',\n       'interaction_online', 'preventive_behaviors', 'optimism',\n       'deterioration_economy', 'deterioration_interact', 'frustration',\n       'covid_anxiety', 'covid_sleepless', 'difficulty_living',\n       'difficulty_work', 'age_group']\n\nfor column in columns:\n  df_dummies = pd.concat([df_dummies, pd.get_dummies(df_dummies[column], prefix=column)], axis=1)\n  df_dummies.drop(columns=[column], axis=1, inplace=True)\n\ndisplay(df_dummies)","832cd29f":"# Train Test Split\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\nfor train_index, test_index in strat_split.split(df_dummies, df_dummies[\"depression\"]):\n    strat_train_set = df_dummies.loc[train_index]\n    strat_test_set = df_dummies.loc[test_index]\n\nprint('Target Labels Ratio in Original Dataset\\n')\nprint(df_dummies[\"depression\"].value_counts(normalize=True).sort_index())\n\n\nprint('\\nTarget Labels Ratio in Test Dataset\\n')\nprint(df_dummies[\"depression\"].value_counts(normalize=True).sort_index())\n","082f72f5":"# train Dataset\nX = strat_train_set.drop(\"depression\", axis=1)\ny = strat_train_set[\"depression\"].copy()\n\n# test dataset\ny_test = strat_test_set['depression']\nX_test = strat_test_set.drop('depression',axis=1)\n\n# X.reset_index(drop=True)\n# y.reset_index(drop=True)\n# X_test.reset_index(drop=True)\n# y_test.reset_index(drop=True)\n\nprint(\"Printing Train-Test Dimensions\")\ndisplay(X.shape, y.shape, X_test.shape, y_test.shape)\n# print(X_test, y_test)","440e5250":"def plot_feature_importance(importance,names,model_type):\n\n  #Create arrays from feature importance and feature names\n  feature_importance = np.array(importance)\n  feature_names = np.array(names)\n\n  #Create a DataFrame using a Dictionary\n  data={'feature_names':feature_names,'feature_importance':feature_importance}\n  fi_df = pd.DataFrame(data)\n\n  #Sort the DataFrame in order decreasing feature importance\n  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n  # display(list(fi_df['feature_names'][:50]))\n  # display(list(fi_df['feature_names'][:100]))\n  #Define size of bar plot\n  plt.figure(figsize=(14,50),dpi=100)\n  #Plot Searborn bar chart\n  sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n  #Add chart labels\n  plt.title(model_type + ' FEATURE IMPORTANCE')\n  plt.xlabel('FEATURE IMPORTANCE')\n  plt.ylabel('FEATURE NAMES')","e91f1a50":"def train_model(model, model_name, X, y, X_test, fold):\n  printmd(f'**{model_name} Init**')\n  auc_scores = []\n\n  test_preds=None\n\n  strat_kf = StratifiedKFold(n_splits=fold, random_state=SEED, shuffle=True)\n\n  for fold, (train_index, valid_index) in enumerate(strat_kf.split(X, y)):\n\n    X_train, X_valid = X.iloc[train_index] , X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index] , y.iloc[valid_index]\n\n    #### to SMOTE sampling\n    # sm = SMOTE(sampling_strategy='all', random_state=SEED)\n    # X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n    # X_val_oversampled, y_val_oversampled = sm.fit_resample(X_valid, y_valid)\n \n    eval_set = [(X_valid, y_valid)]\n\n    print(\"-\" * 50)\n    print(f\"Fold {fold + 1}\")\n\n    if model_name == 'cat':\n      model.fit(X_train, y_train, eval_set= eval_set, verbose=False)\n    elif model_name == 'xgb':\n      model.fit(X_train, y_train, eval_set= eval_set, eval_metric = 'auc', verbose = False, early_stopping_rounds = 200)\n    else:\n      model.fit(X_train, y_train)\n    \n\n    val_pred = model.predict_proba(X_valid)[:,1]\n    auc = roc_auc_score(y_valid, val_pred) # AUROC requires probabilities of the predictions\n    print(\"AUC Score : \",auc)\n\n    auc_scores.append(auc)\n\n    if test_preds is None:\n      test_preds = model.predict_proba(X_test)[:,1] \n    else:\n      test_preds += model.predict_proba(X_test)[:,1] \n\n\n    del X_train, y_train, X_valid, y_valid\n    gc.collect()\n      \n  print(\"-\" * 50)\n  test_preds \/= fold\n\n  print(f'Train : Base Model - {model_name} - AUC score : mean ---> {np.mean(auc_scores)}, std ---> {np.std(auc_scores)}')\n  \n  # evaluation on test set\n  print(f'Test  : Base Model - {model_name} - AUC score : {roc_auc_score(y_test, test_preds)}')\n\n  del test_preds\n  gc.collect()\n  \n  print('Done!')\n\n  if model_name == 'cat':\n    plot_feature_importance(model.get_feature_importance(), X.columns, model_name)\n    model.save_model(\"model_catboost\")\n\n","b207eaa3":"%%time\ndef objective(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=int(SEED), shuffle=True, stratify=y)\n    \n    # parameters\n    params = {\n        'iterations' : trial.suggest_int('iterations', 6000, 8000), \n        'depth' : trial.suggest_int('depth', 3, 12),\n        'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\"]),\n        'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1), #  # does not support on gpu \n        'random_strength' :trial.suggest_int('random_strength', 0, 100),   \n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]  # https:\/\/catboost.ai\/en\/docs\/concepts\/algorithm-main-stages_bootstrap-options\n        ),\n        'random_state': trial.suggest_categorical('random_state',[SEED]),\n    }\n\n    # learning\n    model = CatBoostClassifier(\n        loss_function=\"Logloss\",\n        eval_metric=\"AUC\",\n        # task_type=\"GPU\",\n        l2_leaf_reg=50,\n#         border_count=64,\n        **params\n    )        \n    model.fit(X_train, y_train, \n              verbose=False) # 1000\n    val_preds = model.predict_proba(X_test)[:,1]\n    auc = roc_auc_score(y_test, val_preds) # AUROC requires probabilities of the predictions\n    print(\"AUC Score : \",auc) # check the auc score in each trial\n    \n    return auc","fec6977b":"%%time\nn_trials = int(50)\n\n# set logging level\noptuna.logging.set_verbosity(optuna_verbosity)\n\nstudy = optuna.create_study(direction = \"maximize\", sampler = optuna.samplers.TPESampler(seed=int(SEED)))\nstudy.optimize(objective, n_trials = n_trials, n_jobs = multiprocessing.cpu_count())\n\nprintmd('**BEST TRIAL**')\nprint(\"Best Score: \", study.best_value)\nprintmd('**CatBoost Tuned Hyperparameters**')\npprint.pprint(study.best_trial.params)","4a85cdb7":"# Save\npickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\nprint(\"Best Score: \", study.best_value)\n\nprintmd('**CatBoost Tuned Hyperparameters**')\npprint.pprint(study.best_trial.params)\n\n# history\ndisplay(optuna.visualization.plot_optimization_history(study))\n\n# Importance\ndisplay(plot_param_importances(study))","fa583101":"fold_num = 10\n\ncat_params  = {\n    'eval_metric':\"AUC\",\n    'loss_function': 'logloss',\n    'objective': 'Logloss',\n    'boosting_type': 'Plain',\n    'bootstrap_type': 'MVS', \n    'colsample_bylevel': 0.0841793141263058, # does not support on gpu https:\/\/catboost.ai\/en\/docs\/references\/training-parameters\/common#rsm\n    'depth': 8,\n    'iterations': 7117,\n    'learning_rate': 0.001307076431605048,\n    'random_strength': 65, \n    'l2_leaf_reg': 50,\n    'random_state': SEED,\n#     'task_type':\"GPU\",\n    'devices' : '0',\n    # 'cat_features':cat_cols\n    }\n\n\ncat = CatBoostClassifier(**cat_params)\n\n\n\ntrain_model(cat, 'cat', X, y , X_test, fold_num)\n\n","518332f9":"features_50 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', \n               'age_group_1', 'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n               'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', 'healthy_sleep_7', \n               'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', 'preventive_behaviors_4', \n               'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', \n               'healthy_diet_3', 'difficulty_living_4', 'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0']\n\n\ntrain_model(cat, 'cat', X[features_50], y , X_test[features_50], fold_num)\n","4ad1d119":"features_100 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', 'age_group_1', \n                'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n                'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', \n                'healthy_sleep_7', 'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', \n                'preventive_behaviors_4', 'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', 'healthy_diet_3', 'difficulty_living_4', \n                'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0', 'deterioration_economy_4', 'preventive_behaviors_6', 'interaction_offline_1', \n                'deterioration_interact_7', 'covid_anxiety_5', 'activity_3', 'optimism_5', 'exercise_4', 'deterioration_interact_2', 'difficulty_living_5', 'healthy_diet_6', 'healthy_sleep_6', 'activity_4', \n                'interaction_online_7', 'preventive_behaviors_7', 'covid_sleepless_2', 'interaction_online_1', 'covid_sleepless_6', 'healthy_sleep_5', 'difficulty_living_3', 'city_1', 'healthy_sleep_2', 'exercise_2', \n                'covid_anxiety_4', 'optimism_6', 'job_1', 'interaction_offline_4', 'covid_sleepless_7', 'activity_7', 'income_group_2', 'exercise_6', 'city_6', 'covid_sleepless_3', 'interaction_online_3', \n                'deterioration_economy_5', 'interaction_offline_3', 'interaction_online_5', 'healthy_sleep_3', 'healthy_diet_4', 'difficulty_living_6', 'job_2', 'income_group_1', 'city_5', 'city_2', 'covid_anxiety_3', \n                'exercise_5', 'deterioration_economy_2', 'activity_5', 'preventive_behaviors_5', 'income_group_3']\n\n\ntrain_model(cat, 'cat', X[features_100], y , X_test[features_100], fold_num)","1576eb62":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n\n#Random Forest\nforest = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 50, cv = 10, \n                               verbose=2, random_state=42, n_jobs = -1)\n\n#Train Test Split\nX_train, test_X, y_train, test_y = train_test_split(X, y, test_size=0.25, random_state=int(SEED), shuffle=True, stratify=y)\n\nrf_random.fit(X_train, y_train)\nprint(rf_random.best_params_)                               \n","74261342":"fold_num = 10\nrandom_forest = RandomForestClassifier(n_estimators=800, min_samples_split=10, min_samples_leaf=2, max_features='sqrt', \n                                       max_depth=30, bootstrap=False)\ntrain_model(random_forest, 'rf', X, y , X_test, fold_num)","fab45db7":"features_50 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', \n               'age_group_1', 'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n               'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', 'healthy_sleep_7', \n               'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', 'preventive_behaviors_4', \n               'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', \n               'healthy_diet_3', 'difficulty_living_4', 'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0']\n\n\ntrain_model(random_forest, 'rf', X[features_50], y , X_test[features_50], fold_num)","01944be4":"features_100 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', 'age_group_1', \n                'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n                'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', \n                'healthy_sleep_7', 'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', \n                'preventive_behaviors_4', 'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', 'healthy_diet_3', 'difficulty_living_4', \n                'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0', 'deterioration_economy_4', 'preventive_behaviors_6', 'interaction_offline_1', \n                'deterioration_interact_7', 'covid_anxiety_5', 'activity_3', 'optimism_5', 'exercise_4', 'deterioration_interact_2', 'difficulty_living_5', 'healthy_diet_6', 'healthy_sleep_6', 'activity_4', \n                'interaction_online_7', 'preventive_behaviors_7', 'covid_sleepless_2', 'interaction_online_1', 'covid_sleepless_6', 'healthy_sleep_5', 'difficulty_living_3', 'city_1', 'healthy_sleep_2', 'exercise_2', \n                'covid_anxiety_4', 'optimism_6', 'job_1', 'interaction_offline_4', 'covid_sleepless_7', 'activity_7', 'income_group_2', 'exercise_6', 'city_6', 'covid_sleepless_3', 'interaction_online_3', \n                'deterioration_economy_5', 'interaction_offline_3', 'interaction_online_5', 'healthy_sleep_3', 'healthy_diet_4', 'difficulty_living_6', 'job_2', 'income_group_1', 'city_5', 'city_2', 'covid_anxiety_3', \n                'exercise_5', 'deterioration_economy_2', 'activity_5', 'preventive_behaviors_5', 'income_group_3']\n\n\n\ntrain_model(random_forest, 'rf', X[features_100], y , X_test[features_100], fold_num)","f6304fd3":"fold_num = 10\nlog_model = LogisticRegression(max_iter=450)\n\ntrain_model(log_model, 'logistic_reg', X, y , X_test, fold_num)\n","494a5345":"features_50 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', \n               'age_group_1', 'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n               'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', 'healthy_sleep_7', \n               'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', 'preventive_behaviors_4', \n               'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', \n               'healthy_diet_3', 'difficulty_living_4', 'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0']\n\nfold_num = 10\nlog_model = LogisticRegression(max_iter=450)\n\ntrain_model(log_model, 'logistic_reg', X[features_50], y , X_test[features_50], fold_num)","e42859c1":"features_100 = ['frustration_1', 'deterioration_interact_1', 'past_mental_1', 'covid_sleepless_1', 'current_mental_1', 'current_mental_2', 'past_mental_2', 'age_group_1', \n                'deterioration_interact_4', 'covid_anxiety_7', 'frustration_7', 'difficulty_work_1', 'optimism_1', 'deterioration_interact_5', 'difficulty_work_7', 'age_group_2', \n                'family_status_1', 'family_status_2', 'difficulty_living_1', 'deterioration_economy_7', 'covid_sleepless_4', 'difficulty_living_7', 'healthy_sleep_1', 'frustration_6', \n                'healthy_sleep_7', 'covid_anxiety_1', 'frustration_2', 'optimism_2', 'covid_anxiety_6', 'gender_1', 'deterioration_economy_1', 'optimism_4', 'interaction_online_4', 'exercise_1', \n                'preventive_behaviors_4', 'frustration_5', 'activity_1', 'gender_2', 'covid_sleepless_5', 'deterioration_economy_6', 'optimism_3', 'frustration_4', 'healthy_diet_3', 'difficulty_living_4', \n                'healthy_sleep_4', 'income_group_10', 'healthy_diet_1', 'difficulty_work_4', 'healthy_diet_5', 'age_group_0', 'deterioration_economy_4', 'preventive_behaviors_6', 'interaction_offline_1', \n                'deterioration_interact_7', 'covid_anxiety_5', 'activity_3', 'optimism_5', 'exercise_4', 'deterioration_interact_2', 'difficulty_living_5', 'healthy_diet_6', 'healthy_sleep_6', 'activity_4', \n                'interaction_online_7', 'preventive_behaviors_7', 'covid_sleepless_2', 'interaction_online_1', 'covid_sleepless_6', 'healthy_sleep_5', 'difficulty_living_3', 'city_1', 'healthy_sleep_2', 'exercise_2', \n                'covid_anxiety_4', 'optimism_6', 'job_1', 'interaction_offline_4', 'covid_sleepless_7', 'activity_7', 'income_group_2', 'exercise_6', 'city_6', 'covid_sleepless_3', 'interaction_online_3', \n                'deterioration_economy_5', 'interaction_offline_3', 'interaction_online_5', 'healthy_sleep_3', 'healthy_diet_4', 'difficulty_living_6', 'job_2', 'income_group_1', 'city_5', 'city_2', 'covid_anxiety_3', \n                'exercise_5', 'deterioration_economy_2', 'activity_5', 'preventive_behaviors_5', 'income_group_3']\n\nfold_num = 10\nlog_model = LogisticRegression(max_iter=450)\n\ntrain_model(log_model, 'logistic_reg', X[features_100], y , X_test[features_100], fold_num)","48149240":"<a id='4.1'><\/a>\n## 4.1 Participants' Count per Cities","746ca7ad":"The dataset is a part of ADA Lovelace Datathon 2021. The task was to perform EDA on the dataset and predict depression among the demographic of japan.","55edf17a":"<a id='5.2.1'><\/a>\n### 5.2.1 Feature Types\n\n#### Ordinal Features\n> ```'exercise', 'healthy_diet', 'healthy_sleep', 'activity', 'interaction_offline', 'interaction_online', 'preventive_behaviors', 'optimism', 'deterioration_economy', 'deterioration_interact', 'frustration', 'covid_anxiety', 'covid_sleepless', 'difficulty_living', 'difficulty_work'```\n\n#### Dichotomous Features\n> ```'gender', 'health_worker_self', 'health_worker_family', 'family_status', 'current_physical', 'past_physical', 'current_mental', 'past_mental'```\n\n#### Polytomous Features\n> ```'age_group', 'job', 'city', 'income_group'```\n\n\n\n\n\n","64efa459":"<a id='6.3.3'><\/a>\n### 6.3.3 With Selected 50 Features","8a5ef080":"<a id='4.3'><\/a>\n## 4.3 Depression Ratio of the Participants","4c3110ca":"<a id='6.4'><\/a>\n## 6.4 Logistic Regression","edc93173":"<a id='6.2.2'><\/a>\n### 6.2.2 Without Feature Selection\n","5139eef8":"<a id='6.2.4'><\/a>\n### 6.2.4 With Selected 100 Features","5ec233bf":"<a id='3.1'><\/a>\n## 3.1 Features and their Values \n\n| Items                   | Comments                                                                                                                                                                                                               |\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| City                    | 1 = Tokyo, 2 = Kanagawa, 3 = Saitama, 4 = Chiba, 5 = Osaka, 6 = Hyogo, 7 = Fukuoka                                                                                                                                     |\n| Gender                  | 1 = male, 2 = female                                                                                                                                                                                                   |\n| Age                     | Age in responding the questionnaire                                                                                                                                                                                    |\n| Job                     | 1 = employed, 2 = home maker, 3 = student, 4 = unemployed, 5 = others                                                                                                                                                  |\n| Health\\_Worker\\_Self    | \"Are you a healthcare worker?\" 1 = yes, 2 = no                                                                                                                                                                         |\n| Health\\_Worker\\_Family  | \"Is your family member a healthcare worker?\" 1 = yes, 2 = no                                                                                                                                                           |\n| Family\\_Status          | 1 = single, 2 = married                                                                                                                                                                                                |\n| Income\\_Group           | Annual household income (JPY), 1 = < 2 million, 2 = 2-4 million, 3 = 4-6 million, 4 = 6-8 million, <br>5 = 8-10 million, 6 = 10-12 million, 7 = 12-15 million, 8 = 15-20 million, 9 = \u226520 million, 10 = unknown            |\n| Current\\_Physical       | Current treatment for severe physical illness, 1 = yes, 2 =no                                                                                                                                                          |\n| Past\\_Physical          | Past treatment for severe physical illness, 1 = yes, 2 =no                                                                                                                                                             |\n| Current\\_Mental         | Current treatment for mental problem, 1 = yes, 2 =no                                                                                                                                                                   |\n| Past\\_Mental            | Past treatment for mental problem, 1 = yes, 2 =no                                                                                                                                                                      |\n| Exercise                | \"I exercised for my health (whether indoors or outdoors).\" 1 = not at all, 7 = extremely                                                                                                                               |\n| Healthy\\_Diet           | \"I took meals considering the nutrition balance. \" 1 = not at all, 7 = extremely                                                                                                                                       |\n| Healthy\\_Sleep          | \"I kept regular awakening time and bedtime approximately. \" 1 = not at all, 7 = extremely                                                                                                                              |\n| Activity                | \"I engaged in activities such as hobbies with absorbing interest. \" 1 = not at all, 7 = extremely                                                                                                                      |\n| Interaction\\_Offline    | \"I interacted with my family or friends on a face-to-face basis (except work or class).\" 1 = not at all, 7 = extremely                                                                                                 |\n| Interaction\\_Online     | \"I interacted with my family or friends online using chat or video calling (except work or class).\" 1 = not at all, 7 = extremely                                                                                      |\n| Preventive\\_Behaviors   | \"I spontaneously refrained from going out or altruistically took preventive behaviors (e.g. wearing a mask) <br>to prevent coronavirus disease 2019 infection to my family or other people.\" 1 = not at all, 7 = extremely |\n| Optimism                | \"I thought about the future positively. \" 1 = not at all, 7 = extremely                                                                                                                                                |\n| Deterioration\\_Economy  | \"The family budget has tightened.\" 1 = not at all, 7 = extremely                                                                                                                                                       |\n| Deterioration\\_Interact | \"A personal relationship with a close person such as family or friends got worse.\" 1 = not at all, 7 = extremely                                                                                                       |\n| Frustration             | \"I have become easily annoyed or irate due to life-change.\" 1 = not at all, 7 = extremely                                                                                                                              |\n| Covid\\_Anxiety          | \"I felt nervous or anxious when I watched news about coronavirus disease 2019.\" 1 = not at all, 7 = extremely                                                                                                          |\n| Covid\\_Sleepless        | \"I could not sleep because I worried about getting coronavirus disease 2019.\" 1 = not at all, 7 = extremely                                                                                                            |\n| Difficulty\\_Living      | \"My daily life was interrupted due to the shortage of materials relating to prevention for coronavirus disease 2019 <br>infection (e.g. mask or thermometer) or other daily supplies.\" 1 = not at all, 7 = extremely       |\n| Difficulty\\_Work        | \"My work or schoolwork was interrupted due to life-change. \" 1 = not at all, 7 = extremely                                                                                                                             |\n| Depression              | Target Variable to predict, 1 = depression, 0 = no depression                                                                                                                                                          |","b616701e":"<a id='5.4'><\/a>\n## 5.4 Train-Test Split","a1c8c405":"<a id='4.7'><\/a>\n## 4.7 Mental State of the Participants","0415799f":"<a id='4.5'><\/a>\n## 4.5 Depression and Marital Status","ab2c9145":"<a id='5.1'><\/a>\n## 5.1 Dataset Preparation\n- Remove the unnecessary columns ```(id, gender_label, marital_status, and age)``` for further analysis\n- Convert the ```age_group``` to nominal form\n- Convert the nominal values to **One hot encoding**","ba360062":"<a id='5'><\/a>\n# 5 Feature Engineering\n\n---\n","81c04c55":"<a id='6.3.2'><\/a>\n### 6.3.2 With All Features","e97ff784":"<a id='7'><\/a>\n# 7 Conclusion\n\n---\nFrom the above analysis, it can be concluded\n\n*   Among the participants, females are more depressed than the males\n*   Middle aged participants are the most depressed (~48%) age group. \n* Although, the depression rate among the young adults (18-35 years) is the highest (~23%).\n*   Depressed Participants also had high frustration, sleeplessness and high convid anxiety in comparison with non-depressed participants.\n*   People with depression responded to have more difficulty in their living and work situation, in addition with a worsening economical state.\n*   Women participants who reported to have family members working in health care had more depression than their male counterparts\n\n\n\n\n","38b8e536":"<a id='3.6'><\/a>\n## 3.6 Nominal Variables into Labels\n\nFor a better visualization of some features, we are going to convert them into Labels","98aa8337":"<a id='6.2'><\/a>\n## 6.2 CatBoost","a16d6633":"<a id='5.2'><\/a>\n## 5.2 Bivariate Analysis","ea0a3fd5":"<a id='1'><\/a>\n# 1 Introduction \n\n---\nThe objective of this notebook is to present an extensive analysis of the **Covid-19 Mental Health Dataset provided by [Ada Lovelace Datathon 2021](https:\/\/www.kaggle.com\/c\/alc-datathon-2021)** and to predict the mental health status(i.e., depression) of the participants.\n\nThis notebook manages to figure out relationship between depression and the other features both visually and statistically. In this version, I applied two different types of tuners for hyperparamater optimization of the models.\n\n","f31efdb8":"<a id='4.8'><\/a>\n## 4.8 Effects of Life Style on Depression","d8cbffde":"<a id='4'><\/a>\n# 4 Visualizations\n---\nIn this section, we will try to uncover useful information about the participants. Also we will figure out the relationship between depression and certain features from the dataset.\n","511104be":"<a id='5.2.2'><\/a>\n### 5.2.2 Dichotomous Features\n---\n#### Phi Coefficient\n\n**Null Hypothesis:** The two variables dont have monotonic relationship <br>\n**Alternate Hypothesis:** The two variables have a monotonic relationship\n\nTo test this hypothesis between Dichotomous features (binary) and Target variable, we are going to use *Phi Coefficient*","5c90fa19":"<a id='3.5'><\/a>\n## 3.5 Imputation","d370762b":"<a id='6.3.1'><\/a>\n### 6.3.1 RandomizedSearchCV ","ab5cffcc":"<a id='2'><\/a>\n# 2 Important Libraries & Configuration\n\n---\n","871a2154":"<a id='3'><\/a>\n# 3 Descriptive Analysis\n\n---\n\n","33ae881a":"<a id='6.2.3'><\/a>\n### 6.2.3 With Selected 50 Features","9b06b257":"<a id='3.3'><\/a>\n## 3.3 Summary of the dataset","393ac082":"<a id='3.2'><\/a>\n## 3.2 Preview of the dataset","54b224b8":"<a id='6.4.1'><\/a>\n### 6.4.1 With Selected 50 Features","8d6287db":"<a id='6.3.4'><\/a>\n### 6.3.4 With Selected 100 Features","0beff618":"<a id='6.4.2'><\/a>\n### 6.4.2 With Selected 100 Features","fc1a3474":"<a id='6'><\/a>\n# 6 Model Building\n\n---\n\n","26632193":"<a id='4.6'><\/a>\n## 4.6 Depression with Physical and Mental Health","cae2616b":"<a id='6.5'><\/a>\n## 6.5 Model Summary\n\n\n| Model                   | All Features            |                         | Selected 50 Features    |                         |Selected 100 Features    |                         |\n| :-----------------------: | :-----------------------: | :-----------------------: | :-----------------------: | :-----------------------: | :-----------------------: | :-----------------------: |\n|                         | **Train AUC**               | **Test AUC**                | **Train AUC**               | **Test AUC**                | **Train AUC**               | **Test AUC**                |\n| CatBoost                | 0.8198                  | 0.8056                   | 0.8161                  | 0.7959                  | **0.8237**                  | **0.8066**                 |\n| Random Forest           | 0.8096                  | 0.7972                  | 0.7865                  | 0. 7743                | 0.8083                  | 0.7925                   |\n| Logistic Regression     | 0.8122                  | 0.7996                  | 0.8102                  | 0.7946                  | 0.8131                  | 0.7982                  |","96ee8bdc":"<a id='5.2.4'><\/a>\n### 5.2.4 Ordinal Features\n\n---\n\n#### Kendall's Tau\n\n**Null Hypothesis:** The two variables dont have monotonic relationship <br>\n**Alternate Hypothesis:** The two variables have a monotonic relationship\n\nTo test this hypothesis between Ordinal features and Target variable, we are going to use *Kendall's Tau*","583e537a":"<a id='5.3'><\/a>\n## 5.3 Converting to One Hot Encoding","11ee81a0":"<a id='4.11'><\/a>\n## 4.11 Depression and Participants Related to Health Care","067f698e":"<a id='3.4'><\/a>\n## 3.4 Check for Missing Values","2b7eb124":"<a id='4.2'><\/a>\n## 4.2 Age Distribution","af07b15b":"<a id='4.10'><\/a>\n## 4.10 Effects of Living, Work and Economy on Depression","fe32c144":"<a id='6.2.1'><\/a>\n### 6.2.1 Optuna Hyperparameter Tuning ","d6b70b92":"# Table of Contents\n\n* [1 Introduction](#1)\n* [2 Important Libraries & Configuration](#2)\n* [3 Descriptive Analysis](#3)\n  * [3.1 Features and their Values](#3.1)\n  * [3.2 Preview of the dataset](#3.2)\n  * [3.3 Summary of the dataset](#3.3)\n  * [3.4 Check for Missing Values](#3.4)\n  * [3.5 Imputation](#3.5)\n  * [3.6 Nominal Variables into Labels](#3.6)\n* [4 Visualizations](#4)\n  * [4.1 Participants' Count per Cities](#4.1)\n  * [4.2 Age Distribution](#4.2)\n  * [4.3 Depression Ratio of the Participants](#4.3)\n  * [4.4 Depression among the Demographic](#4.4)\n  * [4.5 Depression and Marital Status](#4.5)\n  * [4.6 Depression with Physical and Mental Health](#4.6)\n  * [4.7 Mental State of the Participants](#4.7)\n  * [4.8 Effects of Life Style on Depression](#4.8)\n  * [4.9 Depression and Job](#4.9)\n  * [4.10 Effects of Living, Work and Economy on Depression](#4.10)\n  * [4.11 Depression and Participants Related to Health Care](#4.11)\n* [5 Feature Engineering](#5)\n  * [5.1 Dataset Preparation](#5.1)\n  * [5.2 Bivariate Analysis](#5.2)\n    * [5.2.1 Feature Types](#5.2.1)\n    * [5.2.2 Dichotomous Features](#5.2.2)\n    * [5.2.3 Polytomous Features](#5.2.3)\n    * [5.2.4 Ordinal Features](#5.2.4)\n  * [5.3 Converting to One Hot Encoding](#5.3)\n  * [5.4 Train-Test Split](#5.4)\n* [6 Model Building](#6)\n  * [6.1 Utility Functions](#6.1)\n  * [6.2 CatBoost](#6.2)\n    * [6.2.1 Optuna Hyperparameter Tuning](#6.2.1)\n    * [6.2.2 Without Feature Selection](#6.2.2)\n    * [6.2.3 With Selected 50 Features](#6.2.3)\n    * [6.2.4 With Selected 100 Features](#6.2.4)\n  * [6.3 Random Forest](#6.3)\n    * [6.3.1 RandomizedSearchCV](#6.3.1)\n    * [6.3.2 With All Features](#6.3.2)\n    * [6.3.3 With Selected 50 Features](#6.3.3)\n    * [6.3.4 With Selected 100 Features](#6.3.4)\n  * [6.4 Logistic Regression](#6.4)\n    * [6.4.1 With Selected 50 Features](#6.4.1)\n    * [6.4.2 With Selected 100 Features](#6.4.2)\n  * [6.5 Model Summary](#6.5)\n* [7 Conclusion](#7)","31c1256a":"<a id='6.1'><\/a>\n## 6.1 Utility Functions","8a54048c":"<a id='6.3'><\/a>\n## 6.3 Random Forest","36a1543d":"<a id='4.9'><\/a>\n## 4.9 Depression and Job","be052bf2":"<a id='5.2.3'><\/a>\n### 5.2.3 Polytomous Features\n---\n\n#### Cramer's V\n\n|  **Coefficient Value Interpretation**  |   |\n|---|---|\n|__.25 or higher__| Very strong relationship  |\n|__.15 to .25__| Strong relationship  |\n|__.11 to .15__| Moderate relationship  |\n|__.06 to .10__| Weak relationship  |\n|__.01 to .05__| No or negligible relationship  |\n\n**Null Hypothesis:** The two variables dont have monotonic relationship <br>\n**Alternate Hypothesis:** The two variables have a monotonic relationship\n\nTo test this hypothesis between Polytomous features and Target variable, we are going to use *Cramer's V*","ef5bd63d":"<a id='4.4'><\/a>\n## 4.4 Depression among the Demographic"}}