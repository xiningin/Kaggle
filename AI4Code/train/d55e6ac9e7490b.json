{"cell_type":{"0e9de90e":"code","7411ad52":"code","404f8c60":"code","3ef704cf":"code","f97de654":"code","afb7908d":"code","f060cf48":"code","3b68245b":"code","65af58f9":"code","c78f1bab":"code","ee50295d":"code","b8851a5d":"code","dd5cfbfb":"code","cd0c4837":"code","3d34a9bd":"code","5116c9dd":"code","19806389":"markdown","3faed424":"markdown","ae85b382":"markdown","9325e577":"markdown","bbdf82fd":"markdown","ac26e66d":"markdown","96290187":"markdown","8e0801b7":"markdown","51a34929":"markdown","690c7add":"markdown","33828018":"markdown","eaad5382":"markdown"},"source":{"0e9de90e":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7411ad52":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nX = np.array(train.drop(['id','target'], axis=1))\ny = np.array(train['target'])\nX_test = np.array(test.iloc[:,1:])","404f8c60":"# get correlation of each feature with target\ncorr = train.corr()['target'][2:]\nsns.boxplot(corr)","3ef704cf":"from sklearn.model_selection import StratifiedKFold\n\ndef repeat_cross_val(model, X, y, n_iters=10, n_folds=5):\n    \n    folds = StratifiedKFold(n_splits=n_folds, shuffle=True)\n    scores = np.zeros([n_iters, n_folds])\n    \n    for i in range(n_iters):\n        for j, (cv_train, cv_test) in enumerate(folds.split(X,y)):\n            model.fit(X[cv_train], y[cv_train])\n            scores[i,j] = model.score(X[cv_test],y[cv_test])    \n    return scores.mean()","f97de654":"#set correltaion threshold and filter training data\ncorr_thresh = 0.1\nhigh_corr = abs(corr)>corr_thresh\nX_corr = X[:,high_corr]\n\nX_corr.shape[1]\/X.shape[1]","afb7908d":"# give it a quick test\nfrom sklearn.linear_model import LogisticRegression\n\nlrc = LogisticRegression(penalty='l1', solver='liblinear')\nprint(repeat_cross_val(model=lrc, X=X_corr, y=y))","f060cf48":"# Testing values from 0-0.3\ncorr_test = np.arange(0, 0.3, 0.01)\ncv_score = np.zeros(corr_test.shape[0])\n\nlrc = LogisticRegression(penalty='l1', solver='liblinear')\n\nfor i, c in enumerate(corr_test):\n    high_corr = abs(corr)>c\n    X_corr = X[:,high_corr]\n    cv_score[i] = repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=25)\n\nplt.scatter(x=corr_test, y=cv_score)\nplt.xlabel('correlation threshold')\nplt.ylabel('cv score')\nplt.title('Testing correlation threshold')","3b68245b":"corr_thresh = 0.11\nhigh_corr = abs(corr)>corr_thresh\nX_corr = X[:,high_corr]\nX_corr.shape","65af58f9":"lrc = LogisticRegression(penalty='l1', solver='liblinear')\nprint('l1: {0:.3f}'.format(repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=250)))\nlrc = LogisticRegression(penalty='l2', solver='liblinear')\nprint('l2: {0:.3f}'.format(repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=250)))","c78f1bab":"# Testing values from 0-0.3\nC_test = np.array([0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]) \ncv_score = np.zeros(C_test.shape[0])\nfor i, C in enumerate(C_test):\n    lrc = LogisticRegression(penalty='l2', solver='liblinear', C=C)\n    cv_score[i] = repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=50)\nplt.scatter(x=C_test, y=cv_score)\nplt.xlabel('C-value')\nplt.ylabel('cv score')\nplt.title('Testing C-value')\nplt.xscale('log')\nplt.xlim((0.00000001,10000))","ee50295d":"%%time\nC_test = np.logspace(-4.0, 0, 60)\ncv_score = np.zeros(C_test.shape[0])\nfor i, C in enumerate(C_test):\n    lrc = LogisticRegression(penalty='l2', solver='liblinear', C=C)\n    cv_score[i] = repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=50)\nplt.scatter(x=C_test, y=cv_score)\nplt.xlabel('C-value')\nplt.ylabel('cv score')\nplt.title('Testing C-value')\nplt.xscale('log')\nplt.xlim((10**-4.2,1))","b8851a5d":"# Testing values from 0-0.3\ncorr_test = np.arange(0, 0.3, 0.01)\ncv_score = np.zeros(corr_test.shape[0])\nlrc = LogisticRegression(penalty='l1', solver='liblinear', C=0.5)\nfor i, c in enumerate(corr_test):\n    high_corr = abs(corr)>c\n    X_corr = X[:,high_corr]\n    cv_score[i] = repeat_cross_val(model=lrc, X=X_corr, y=y, n_iters=25)\nplt.scatter(x=corr_test, y=cv_score)\nplt.xlabel('correlation threshold')\nplt.ylabel('cv score')\nplt.title('Testing correlation threshold')","dd5cfbfb":"# reset X_corr\ncorr_thresh = 0.11\nhigh_corr = abs(corr)>corr_thresh\nX_corr = X[:,high_corr]\n\nlrc = LogisticRegression(penalty='l2', solver='liblinear', C=0.05)\nlrc.fit(X_corr, y)","cd0c4837":"predict = lrc.predict(X_test[:,high_corr]) # this got a .704\npredict_prob = lrc.predict_proba(X_test[:,high_corr]) # wow! this got a 0.786\nprint(predict[0], predict_prob[:,1])","3d34a9bd":"sub = pd.DataFrame({\n    'id': test['id'],\n    'target': predict_prob[:,1]\n})\nprint(sub.head())\nprint(pd.read_csv('..\/input\/sample_submission.csv').head())","5116c9dd":"sub.to_csv('submission.csv', index=False)","19806389":"Looks about the same. Lets see how it does.","3faed424":"Hey all. I'm really new at this but I'm trying to get my hands dirty. Please jump in and correct me if you see anything that doesn't make sense, or if you have any pointers I'm all ears! \n\nI'm taking a lot of ideas from Tak's kernel (https:\/\/www.kaggle.com\/takaishikawa\/experiment02-corr-select-logistic-reg) and trying to expand on that a bit.","ae85b382":"Down form 300 to 45.","9325e577":"Lots of very low correlation features. Seems like ditching some might help in avoiding overfitting.\n\nFirst I'll make a quick function for repeating cross validation. I realized after I wrote this that there it's built in to sklearn, but this way seems to work too. ","bbdf82fd":"Using the probabilities instead of binary predictions got a much better score (0.786 vs 0.704). This surprised me, time to learn more about AUCROC.","ac26e66d":"Seems alright.\n\nLet's see if we can get some idea of what a good correlation threshold will be.","96290187":"Take a look at correlation between each feature and the target","8e0801b7":"l2 seems to be just a touch better. Next up is the C value. Smaller C means more regularization (i.e., the algorithm is more willing to let a training point be on the wrong side of the decision boundry), intuitively it seems like this will help with the overfitting problem. Let's take a look...","51a34929":"I'm going to define a correlation threshold, this will be a value that used to decide whether a feature is going to be removed from the training data. I'll get the correation with each feature and the target, and then set a value. If the absolute value of correlation is above this, I'll leave that feature in.","690c7add":"Looks like something around 0.05 will be a good option. I'm going to check the corr_threshold again with this C value. ","33828018":"It looks like 0.1 - 0.13 is the sweet spot. I'll check how many features that leaves.","eaad5382":"It looks like smaller C values are the way to go! (default is 1.0) I'll zome in on the peak."}}