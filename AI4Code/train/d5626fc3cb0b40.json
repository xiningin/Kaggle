{"cell_type":{"5d21ca16":"code","166489ec":"code","89bdaeb6":"code","409d6f69":"code","8eb6d56a":"code","f62c0956":"code","b1b8432a":"code","5e8b5b84":"code","a5cf5b0c":"code","dea78e30":"code","38e966a8":"code","9b78a712":"code","affd65f2":"code","4468849c":"code","6449b72d":"code","395899bb":"code","7d2e078c":"code","c9508e25":"code","228a258d":"code","deeb03b7":"code","3e6395fa":"code","a62ff8be":"code","7fce3948":"code","0ce2875c":"markdown"},"source":{"5d21ca16":"import pandas as pd\nimport numpy as np\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\n#import umap.plot\n\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\nimport os, time, gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns","166489ec":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","89bdaeb6":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","409d6f69":"\ntrain = pd.read_csv(\"\/kaggle\/input\/kannada-digits\/Dig-MNIST.csv\")","8eb6d56a":"train.shape\ntrain.head(2)\n\ntrain.dtypes.value_counts()","f62c0956":"train.label.value_counts()\nprint(\"\\n\\n--normalized--\\n\")\ntrain.label.value_counts(normalize=True)\n#the labels have equal records","b1b8432a":"y = train.pop('label')   # Target variable","5e8b5b84":"train.describe()","a5cf5b0c":"print(\"--Current memory usage (MB) --\\n\")\ntrain.memory_usage().sum()\/1000000  #64.307 MB\n\n# 3.1\nprint(\"\\n\\n--Max and min values---\\n\")\ntrain.max().max()               \nprint()\ntrain.min().min()               \n\n# 3.2 Change dtype\ntrain = train.astype('uint16')\n\n# 3.3\nprint(\"\\n\\n--Revised memory usage (MB) --\\n\")\ntrain.memory_usage().sum()\/1000000  # 16.05 MB","dea78e30":"#Encode labels is not necessary as the class labels are already in integer format\n#perform standardization of the dataset\nss = StandardScaler()\nX = ss.fit_transform(train)","38e966a8":"X.shape","9b78a712":"#since the dataset is small sampling is not necessary\n#applying PCA\n\npca = PCA(n_components=0.95)   # Explain 95% variance\nXf = pca.fit_transform(X)\nXf.shape\n#we could reduce 376 variables for 95% variance","affd65f2":"#Perform t-sne \n\nimport time\nstart = time.time()\n# 4.3.1 Instantiate TSNE class\ntsne = TSNE(\n            perplexity = 30,  # Larger datasets usually require a larger perplexity.\n                              # Experiment with selecting a value between 5 and 50.\n            n_jobs = 2,\n            n_iter=500,\n            verbose =1\n            )\n\n# Fit and transform dataset to 2D\n\nres = tsne.fit_transform(Xf)\nend = time.time()\n(end - start)\/60 ","4468849c":"res = pd.DataFrame(res, columns= ['x','y'])\nfig = plt.figure(figsize = (16,10))\n_=sns.scatterplot(data=res, x='x', y='y', hue=y, palette='tab20')\n#we could observe that the labels are clearly classified except for labels 6 and 7 ","6449b72d":"#perform umap for default metrics\nimport umap\nstart = time.time()\nmapper = umap.UMAP(\n                    n_epochs = 200,\n                    verbose = True  \n                   ).fit(X)       \nembedding =  mapper.transform(X)  \nend = time.time()\n(end-start)\/60","395899bb":"embedding = pd.DataFrame(embedding, columns= ['x','y'])\nfig = plt.figure(figsize = (16,10))\n_=sns.scatterplot(\n                   data=embedding,\n                   x='x',\n                   y='y',\n                   hue=y,          \n                   palette='tab20')","7d2e078c":"#Now we will map in 3d spherical plane with haversine distant metric wich measures distance in radians\nsphere_mapper = umap.UMAP(output_metric='haversine', random_state=42).fit(X)","c9508e25":"alpha = np.sin(sphere_mapper.embedding_[:, 0]) * np.cos(sphere_mapper.embedding_[:, 1])\nbeta = np.sin(sphere_mapper.embedding_[:, 0]) * np.sin(sphere_mapper.embedding_[:, 1])\ngamma = np.cos(sphere_mapper.embedding_[:, 0])","228a258d":"fig = plt.figure(figsize = (24,16))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(alpha, beta, gamma, c=y, cmap='Spectral')","deeb03b7":"#plotting in 2D frame we get\na = np.arctan2(alpha, beta)\nb = -np.arccos(gamma)\nfig = plt.figure(figsize = (16,10))\n_=sns.scatterplot(\n                   \n                   x=a,\n                   y=b,\n                   hue=y,\n                   palette='tab20')\n#Haversine distance metric provided for better visualization than Euclidean. We can predict logloss for both the metrics and evaluate.\n#However we can observe that both eucledian and haversine picture are interpreting label 6 and label 7 are similar in nature","3e6395fa":"#Now we can try UMAP for predictions concatenating with Random forest classifier. We will be using above both metrics for computing logloss\nsss =  StratifiedShuffleSplit(n_splits = 1, test_size = 0.3)\n\nfor train_index, test_index in sss.split(train, y):\n    X_train, X_test = train.loc[train_index,:],train.loc[test_index, :] \n    y_train, y_test = y[train_index], y[test_index]","a62ff8be":"# we are keeping variance retrieval of 95% for PCA and all parameters are same except for distance metric\npipe1 = make_pipeline(\n                     StandardScaler(),\n                     PCA(n_components=0.95), \n                     UMAP(\n                           n_neighbors=15,  \n                           n_components = 5, \n                           verbose = True,\n                           n_epochs = 150\n                          ),\n                      \n                     RandomForestClassifier(max_depth =35)\n                     )\n\n# 6.1.2\npipe1.fit(X_train,y_train)\n# 6.1.3\ny_pred = pipe1.predict_proba(X_test)\n# 6.1.4\nprint()\nf\"log loss: {log_loss(y_test,y_pred)} \"\nend = time.time()\nprint()\n(end-start)\/60","7fce3948":"pipe2 = make_pipeline(\n                     StandardScaler(),\n                     PCA(n_components=0.95), \n                     UMAP(\n                           n_neighbors=15,  \n                           n_components = 5, \n                           metric= 'haversine',\n                           verbose = True,\n                           n_epochs = 150\n                          ),\n                      \n                     RandomForestClassifier(max_depth =35)\n                     )\n\n# 6.1.2\npipe1.fit(X_train,y_train)\n# 6.1.3\ny_pred = pipe1.predict_proba(X_test)\n# 6.1.4\nprint()\nf\"log loss: {log_loss(y_test,y_pred)} \"\nend = time.time()\nprint()\n(end-start)\/60","0ce2875c":"#logloss of haversine: 1.82\n#logloss of euclidean: 3.03\nIn this case it can be observed that log loss for haversine distance is significantly lower than euclidean distance."}}