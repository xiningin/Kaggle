{"cell_type":{"3fc7bfa2":"code","9eac0bc5":"code","894c4ec5":"code","3389e27f":"code","b8f882e2":"code","ad2e82b2":"code","721a864a":"code","1cb13733":"code","c2b69c62":"code","190b9ed6":"code","b3c86e09":"code","dd3fea01":"code","0a4ad520":"code","a6415712":"code","92a17848":"code","7e07f944":"code","4067da54":"code","dafe0e40":"code","690dfa2f":"code","4f024e30":"code","c619eab3":"code","d31d6cec":"code","f3775b9b":"code","edaaa71e":"code","b5c8fc58":"code","6fa415e3":"code","b88653ae":"code","e07a76a5":"code","caf2b544":"code","054f4dc5":"code","49348835":"code","fef03368":"code","c828e113":"code","23ba8923":"code","b0254c65":"code","f4bf5bca":"code","25e3dfcf":"code","d613ff97":"code","18ee933f":"code","d2a4c18e":"code","3b56b580":"code","28de0779":"code","b01ea96e":"code","95493b64":"code","2d72724a":"code","0402f09f":"code","f56a42a5":"code","d56208ca":"markdown","7367bf22":"markdown","c4fcc9e4":"markdown","1f21f2f0":"markdown","ea352293":"markdown","75bf75c8":"markdown","006a7030":"markdown","8223f4c8":"markdown","9498fc6a":"markdown","abfa2391":"markdown","2c4a6390":"markdown","5d51d3b3":"markdown","df316f15":"markdown","aabfd0fe":"markdown","b35818c2":"markdown","1b4745db":"markdown","efe24971":"markdown","8a6c5409":"markdown","90cccd1c":"markdown","6ee754bb":"markdown","f8c7a20c":"markdown","14a4a1c4":"markdown","e575e8af":"markdown","f11e536c":"markdown","eafec8e0":"markdown","357fc454":"markdown","25b9ba35":"markdown","6ece3e6f":"markdown","176e86f7":"markdown","0eac672e":"markdown","3cf1b83e":"markdown","25f085c1":"markdown","fa84c7a7":"markdown","d3add4c6":"markdown","e6d04a91":"markdown","5372d170":"markdown","611e2c65":"markdown","e156b12f":"markdown","d4a17702":"markdown","e91640b1":"markdown","81f510b1":"markdown","bc485a57":"markdown","580ceed5":"markdown","6db96aa0":"markdown","84f3391a":"markdown","1d66a303":"markdown","3b7e124f":"markdown","cf3ddb54":"markdown","613a096f":"markdown","b7e39af2":"markdown","895276ae":"markdown","7385d452":"markdown","398fb95d":"markdown","6372b688":"markdown","a8a5bfc6":"markdown","6f44fe70":"markdown","741c5a66":"markdown","37ae29ef":"markdown","52130723":"markdown","a3b8b9fa":"markdown","9db3cb24":"markdown","dcab6939":"markdown","8511d332":"markdown","8441fbbe":"markdown","9c55982f":"markdown","61e4ddb1":"markdown","e58d9d38":"markdown"},"source":{"3fc7bfa2":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport missingno as msno\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nlabel_size = 17\nplt.rcParams['axes.labelsize'] = label_size\nplt.rcParams['axes.titlesize'] = label_size\nplt.rcParams['xtick.labelsize'] = label_size - 3  # tick labels should be smaller than axes labels (by 3 in our case)\nplt.rcParams['ytick.labelsize'] = label_size - 3\n\nrandom_state = 42\nscoring_metric = 'accuracy'\n\nprint ('Libraries Loaded!')","9eac0bc5":"def plot_learning_curve(estimator, estimator_name, X, y, ax, cv = None, train_sizes = np.linspace(0.1, 1.0, 5)):\n                 \n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv = cv, n_jobs = -1, \n                                                            train_sizes = train_sizes, scoring = 'accuracy')\n    \n    train_scores_mean, train_scores_std = np.mean(train_scores, axis = 1), np.std(train_scores, axis = 1)\n    test_scores_mean, test_scores_std = np.mean(test_scores, axis = 1), np.std(test_scores, axis = 1)\n            \n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha = 0.1, color = 'dodgerblue')\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha = 0.1, color = 'darkorange')\n    \n    ax.plot(train_sizes, train_scores_mean, color = 'dodgerblue', marker = 'o', linestyle = '-', label = 'Training Score')\n    ax.plot(train_sizes, test_scores_mean, color = 'darkorange', marker = 'o', linestyle = '-', label = 'Cross-validation Score')\n    ax.set_title(estimator_name)\n    ax.set_xlabel('Training Examples')\n    ax.set_ylabel('Accuracy Score')\n    ax.legend(loc = 'best');\n    \ndef clf_performance(clf, clf_name):\n    \n    cv_scores = cross_val_score(clf, X_train, y_train, \n                                cv = StratifiedKFold(shuffle = True, random_state = random_state))\n    \n    print (clf_name)\n    print('-------------------------------')\n    print ('CV scores: ', cv_scores)\n    print ('     Mean: ', np.round(cv_scores.mean(), 3))\n    print ('      STD: ', np.round(cv_scores.std(), 3))\n    \n    y_pred_pp = cross_val_predict(clf, X_train, y_train, cv = 5, method = 'predict_proba')[:, 1]\n    y_pred = y_pred_pp.round()\n    \n    cm = confusion_matrix(y_train, y_pred, normalize = 'true')\n    \n    fpr, tpr, _ = roc_curve(y_train, y_pred_pp)\n    auc = roc_auc_score(y_train, y_pred_pp)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n    \n    sns.heatmap(cm, annot = True, cmap = 'Blues', annot_kws = {'fontsize': 18}, ax = ax1)\n    ax1.set_xlabel('Predicted Label')\n    ax1.set_xticks([0.5, 1.5])\n    ax1.set_xticklabels(['0', '1'])\n    ax1.set_ylabel('True Label') \n    ax1.set_yticks([0.5, 1.5])\n    ax1.set_yticklabels(['0', '1'])\n    \n    plot_learning_curve(clf, clf_name, X_train, y_train, ax2, cv = StratifiedKFold(shuffle = True, random_state = random_state))\n    plt.tight_layout()\n    \n    return fpr, tpr, auc\n\ndef plot_feature_imp(classifier, classifier_name, color, ax):\n\n    importances = pd.DataFrame({'Feature': X.columns,\n                                'Importance': np.round(classifier.feature_importances_, 3)})\n\n    importances = importances.sort_values('Importance', ascending = True).set_index('Feature')\n\n    importances.plot.barh(color = color, edgecolor = 'firebrick', legend = False, ax = ax)\n    ax.set_title(classifier_name)\n    ax.set_xlabel('Importance');\n    \nprint('Functions defined!')","894c4ec5":"df = pd.read_csv('..\/input\/311-nyc-open-data-hpd\/fhrw-4uyv.csv', \n                 parse_dates = ['created_date', 'closed_date'])\n\ndf['Year'] = df['created_date'].dt.year\n\nprint('The 311 Dataset contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\ndf.head()","3389e27f":"borough_map = gpd.read_file('https:\/\/github.com\/codeforamerica\/click_that_hood\/blob\/master\/public\/data\/new-york-city-boroughs.geojson?raw=truen')\n\nfig, ax = plt.subplots(figsize = (4, 4)) \n\nborough_map['geometry'].boundary.plot(edgecolor = 'firebrick', linewidth = 1, ax = ax)\nborough_map.plot(color = 'grey', alpha = 0.7, ax = ax);","b8f882e2":"df.info()","ad2e82b2":"print ('There are {} different complaint types.'.format(df['complaint_type'].nunique()))","721a864a":"df['complaint_type'].value_counts()   # Returns a Series containing counts of unique values.","1cb13733":"df.loc[df['complaint_type'] == 'HEATING', 'complaint_type'] = 'HEAT\/HOT WATER'\ndf.loc[df['complaint_type'] == 'PAINT - PLASTER', 'complaint_type'] = 'PAINT\/PLASTER'\n\n# or\n# df['Complaint Type'] = np.where(df['Complaint Type'] == 'HEATING','HEAT\/HOT WATER', df['Complaint Type'])\n\nprint ('Top 5 complaints:')\ndf['complaint_type'].value_counts().head()","c2b69c62":"df_new = df.copy()\n\n# find the name of the less common complaints and rename all corresponding rows\nidx = df['complaint_type'].value_counts().sort_values().head(13).index\ndf_new.loc[df_new['complaint_type'].isin(idx), 'complaint_type'] = 'Rest'\n\ndf_complaints = df_new['complaint_type'].value_counts().sort_values()\n\ndf_complaints.plot(kind = 'barh', figsize = (12, 9), fontsize = 11, color = sns.color_palette('coolwarm', len(df_complaints)))\nplt.ylabel('Complaint Type', fontsize = 14)\nplt.xlabel('Number of Complaints', fontsize = 14)\n\n# Includes the number of incidents and the corresponding percentage for every type\nfor index, value in enumerate(df_complaints):    \n    label = str(format(int(value), ',')) + '  -  {}%'.format(round( (value\/df_complaints.sum())*100, 1)) \n    plt.annotate(label, xy = (value + 20000, index - 0.2 ), color = 'red')","190b9ed6":"df.groupby(['Year'])['complaint_type'].agg(pd.Series.mode)","b3c86e09":"df_tc = df[['borough', 'incident_zip', 'street_name', 'latitude', 'longitude', 'complaint_type']]\ndf_tc = df_tc[df_tc['complaint_type'] == 'HEAT\/HOT WATER']\n\ndf_tc.head()","dd3fea01":"missing_counts = df_tc.isnull().sum().sort_values(ascending = False)\npercent = (df_tc.isnull().sum()*100\/df_tc.shape[0]).sort_values(ascending = False)\n\nmissing_df = pd.concat([missing_counts, percent], axis = 1, keys = ['Counts', '%'])\nprint('Missing values: ')\nmissing_df.head()","0a4ad520":"msno.matrix(df_tc);","a6415712":"df_tc['borough'].value_counts()","92a17848":"df_tc = df_tc[df_tc['borough'] != 'Unspecified']\ndf_tc['borough'].value_counts()","7e07f944":"boroughs = list(df_tc['borough'].value_counts().index)\ncolors = ['#9b59b6', '#3498db', '#95a5a6', '#e74c3c', '#34495e', '#2ecc71']\npatches = []\n\nfig, ax = plt.subplots(figsize = (6, 6))\n\nfor index, value in enumerate(boroughs):\n    borough_df = df_tc[df_tc['borough'] == value]\n    ax.scatter(borough_df['longitude'], borough_df['latitude'], c = colors[index], s = 0.5, alpha = 0.01, marker = '.')\n    patches.append(mpatches.Patch(label = value, color = colors[index]))\n \nborough_map['geometry'].boundary.plot(edgecolor = 'black', linewidth = 0.2, ax = ax)\n\nplt.title(\"Distribution of 'HEAT\/HOT WATER' complaints\", size = 13)\nplt.xlabel('Longitude', size = 12)\nplt.ylabel('Latitude', size = 12)\nplt.legend(patches, boroughs);","4067da54":"colors = ['Grey', 'Grey', 'Grey', 'Firebrick', 'Firebrick']\nny_borough = df_tc['borough'].value_counts().sort_values()\n\nny_borough.plot(kind = 'barh', figsize = (8, 5), fontsize = 11, color = colors)\nplt.ylabel('Borough', fontsize = 14)\nplt.xlabel('Number of Complaints', fontsize = 14)\n\nfor index, value in enumerate(ny_borough):   \n    label = str(format(int(value), ',')) + '  -  {}%'.format(round( (value\/ny_borough.sum())*100, 1)) \n    plt.annotate(label, xy = (value + 10000, index - 0.05), color = 'red')","dafe0e40":"zip_codes = list(df_tc['incident_zip'].value_counts().head(10).index)\ncolors = sns.color_palette('Reds_r', len(zip_codes)).as_hex()\npatches = []\n\nfig, ax = plt.subplots(figsize = (6, 6))\n\nfor index, value in enumerate(zip_codes):\n    zip_df = df_tc[df_tc['incident_zip'] == value]\n    ax.scatter(zip_df['longitude'], zip_df['latitude'], c = colors[index], s = 1, alpha = 0.01, marker = '.')\n    patches.append(mpatches.Patch(label = value, color = colors[index]))\n    \nborough_map['geometry'].boundary.plot(edgecolor = 'black', linewidth = 0.2, ax = ax)\n    \nplt.title(\"Distribution of 'HEAT\/HOT WATER' complaints in different zip codes\", size = 13)\nplt.xlabel('Longitude', size = 12)\nplt.ylabel('Latitude', size = 12)\n\nplt.legend(patches, zip_codes);","690dfa2f":"ny_zip = df_tc['incident_zip'].value_counts().head(5).sort_values()\nny_zip.index = ny_zip.index.map(str)\ncolor = sns.color_palette('Reds', len(ny_zip))\n\nny_zip.plot(kind = 'barh', figsize = (8, 5), fontsize = 11, color = color)\nplt.ylabel('Zip code', fontsize = 14)\nplt.xlabel('Number of Complaints', fontsize = 14)\n\nfor index, value in enumerate(ny_zip):   \n    label = str(format(int(value), ',')) \n    plt.annotate(label, xy = (value + 500, index - 0.1), color = 'red')","4f024e30":"print('11226 Zip code belongs to: ', df[df['incident_zip'] == 11226.0].head().iloc[0]['borough'])","c619eab3":"colors = ['Grey', 'Grey', 'Grey', 'Grey', 'Grey', 'Grey', 'Grey', 'Grey', 'Grey', 'firebrick']\nny_streets = df_tc['street_name'].value_counts().head(10).sort_values()\n\nny_streets.plot(kind = 'barh', figsize = (10, 7), fontsize = 11, color = colors)\nplt.ylabel('Street', fontsize = 14)\nplt.xlabel('Number of Complaints', fontsize = 14)\n\nfor index, value in enumerate(ny_streets):   \n    label = str(format(int(value), ',')) \n    plt.annotate(label, xy = (value + 500, index - 0.1), color = 'red')","d31d6cec":"print('Grand Concourse\\n\\tBorough: ', df[df['street_name'] == 'GRAND CONCOURSE'].iloc[0]['borough'])","f3775b9b":"df_311_BX = df[(df['complaint_type'] == 'HEAT\/HOT WATER') & (df['borough'] == 'BRONX')].reset_index(drop = True)\ndf_311_BX.head()","edaaa71e":"columns = ['Address', 'BldgArea', 'BldgDepth', 'BuiltFAR', 'CommFAR', 'FacilFAR', 'Lot', 'LotArea', 'LotDepth', \n           'NumBldgs', 'NumFloors', 'OfficeArea', 'ResArea', 'ResidFAR', 'RetailArea', 'YearBuilt', 'YearAlter1', \n           'ZipCode', 'YCoord', 'XCoord']\n\ndf_pluto_BX = pd.read_csv('..\/input\/311-nyc-open-data-hpd\/BX_18v1.csv', usecols = columns)\ndf_pluto_BX.head()","b5c8fc58":"df_311_BX.dropna(subset = ['incident_address'], axis = 0, inplace = True)  # drop rows with missing values\nprint('311 Dataset: Rows with a missing address dropped successfully!')\n\ndf_311_total = df_311_BX.groupby('incident_address').agg('count')['borough'].to_frame()\ndf_311_total.columns = ['Complaints #']\n\nprint('New 311 Dataset: {} rows'.format(df_311_total.shape[0]))\ndf_311_total.head()","6fa415e3":"print('{} missing addresses in the PLUTO dataset'.format(df_pluto_BX['Address'].isnull().sum()))\n\ndf_pluto_BX.dropna(subset = ['Address'], axis = 0, inplace = True)\nprint('PLUTO Dataset: Rows with a missing address dropped successfully!')","b88653ae":"df_pluto_BX['Address'].value_counts() ","e07a76a5":"df_pluto_BX.drop_duplicates(subset = 'Address', keep = 'first', inplace = True)\n\nprint('PLUTO Dataset: Duplicate addresses dropped successfully!')\nprint('New PLUTO Dataset: {} rows x {} columns'.format(df_pluto_BX.shape[0], df_pluto_BX.shape[1]))","caf2b544":"df_merged = pd.merge(df_311_total, df_pluto_BX, right_on = 'Address', left_index = True, how = 'right')\n\nprint('The merged dataset has {} rows x {} columns.'.format(df_merged.shape[0], df_merged.shape[1]))\ndf_merged.head()","054f4dc5":"corr_p = df_merged.corr()['Complaints #'].sort_values(ascending = False)\n\ndisplay(corr_p.to_frame().style.background_gradient(cmap = 'Reds', axis = 0))","49348835":"corr_sp = df_merged.corr(method = 'spearman')['Complaints #'].sort_values(ascending = False)\n\ndisplay(corr_sp.to_frame().style.background_gradient(cmap = 'Reds', axis = 0))","fef03368":"# delete rows with NaN values \ndf_merged_ = df_merged.dropna(subset = ['Complaints #'], axis = 0)\n\n# create a new dataframe for storing the results\ndf_merged_stats = pd.DataFrame(columns = df_merged_.columns[1:], \n                               index = ['Correlation', 'P-value'])\n\n# iterate over all columns, calculate Spearman's coefficient and the p-value\nfor i in df_merged_stats.columns:\n    spearman, p_value = stats.spearmanr(df_merged_['Complaints #'], df_merged_[i])\n    df_merged_stats[i]['Correlation'] = spearman\n    df_merged_stats[i]['P-value'] = p_value\n    \ndf_merged_stats.T.sort_values(by = 'Correlation', ascending = False)","c828e113":"corr_table = df_merged.corr(method = 'spearman')\n\nplt.figure(figsize = (10, 8))\n\nmask = np.triu(np.ones_like(corr_table, dtype = np.bool))\nax = sns.heatmap(corr_table, mask = mask, cmap = 'Reds')","23ba8923":"df_model = df_merged.copy()\nprint('Copy created!')","b0254c65":"df_model.isnull().sum().sort_values(ascending = False).head(7)","f4bf5bca":"df_model['Complaints #'] = df_model['Complaints #'].fillna(0)\ndf_model['Complaints #'] = np.where(df_model['Complaints #'] > 0, 1, 0)\n\nprint('Complaints # - Value counts:')\ndf_model['Complaints #'].value_counts()","25e3dfcf":"# Uncomment below to display those features\n\n# corr_sp = df_model.corr(method = 'spearman')['Complaints #'].sort_values(ascending = False)\n# print (corr_sp[np.abs(corr_sp) > 0.15].index[:])\n\ny = df_model['Complaints #']\nX = df_model[['ResArea', 'NumFloors', 'BuiltFAR', 'BldgArea',\n              'BldgDepth', 'LotArea', 'ResidFAR', 'FacilFAR', 'YearAlter1']] #'ZipCode' and 'XCoord' are excluded\n\nprint('Features Selected!')","d613ff97":"# Split the dataset into 20% test and 80% training ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nprint('X Set - Shape')\nprint('Train set: {} rows x {} columns'.format(X_train.shape[0], X_train.shape[1]))\nprint(' Test set: {} rows x {} columns'.format(X_test.shape[0], X_test.shape[1]))","18ee933f":"print('y training set:')\ny_train.value_counts()","d2a4c18e":"over = SMOTE(sampling_strategy = 0.5, random_state = 42)\nunder = RandomUnderSampler(sampling_strategy = 1, random_state = 42)\n\nsteps = [('o', over), \n         ('u', under)]\npipeline = Pipeline(steps = steps)\n\nX_train, y_train = pipeline.fit_resample(X_train, y_train)\n\nprint('y training set after SMOTE:')\ny_train.value_counts()","3b56b580":"scaler = StandardScaler()  \n\nX_train = scaler.fit_transform(X_train)  \nX_test = scaler.transform(X_test)\n\nprint('Features scaled!')","28de0779":"rf = RandomForestClassifier(max_depth = 4, random_state = random_state)\n\nfpr_rf, tpr_rf, auc_rf = clf_performance(rf, 'Random Forest Classifier')","b01ea96e":"xgb = XGBClassifier(max_depth = 4, random_state = random_state)\n\nfpr_xgb, tpr_xgb, auc_xgb = clf_performance(xgb, 'XGB')","95493b64":"plt.plot(fpr_rf, tpr_rf, color = 'b', lw = 2, label = '   RF - AUC = {}'.format(np.round(auc_rf, 3)))\nplt.plot(fpr_xgb, tpr_xgb, color = 'firebrick', lw = 2, label = 'XGB - AUC = {}'.format(np.round(auc_xgb, 3)))\n\nplt.plot([0, 1], [0, 1], 'k--', lw = 2, label = 'Baseline')\n\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.xlim([0, 1])\nplt.xticks([0, 0.25, 0.5, 0.75, 1])\nplt.ylabel('False Positive Rate')\nplt.ylim([0, 1])\nplt.yticks([0, 0.25, 0.5, 0.75, 1])\nplt.legend(fontsize = 12);","2d72724a":"rf.fit(X_train, y_train)\nxgb.fit(X_train, y_train);\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n\nplot_feature_imp(rf, 'RF', 'steelblue', ax1)\nplot_feature_imp(xgb, 'XGB', 'darkgray', ax2)\n\nplt.tight_layout();","0402f09f":"y_pred = rf.predict(X_test)\n\nprint('RF - Accuracy = ', np.round(accuracy_score(y_test, y_pred), 3))","f56a42a5":"y_pred = xgb.predict(X_test)\n\nprint('XGB - Accuracy = ', np.round(accuracy_score(y_test, y_pred), 3))","d56208ca":"We have already mentioned that our models do **not** suffer from overfitting. Consequently, it's not a surprise that their performance on the test set is similar to the training set.\n\n<br>\n\n## Future Development\n\nThere are several things that we could improve:\n\n- Feature Engineering,\n- Different techniques for addressing the class imbalance,\n- Add more classifiers,\n- Perform hyperparameter tuning and find the optimal set of hyperparameters. ","7367bf22":"<font color=\"firebrick\">11226<\/font> is the Zip Code with the highest number of submitted complaints (56,009). As we can see from the map but also the code below, it belongs to Brooklyn.","c4fcc9e4":"The last scatter plot shows that the boroughs with the highest number of 'HEAT\/HOT WATER' complaints are the Bronx, Brooklyn, and Manhattan.\n\nWe can quantify this observation with a bar plot:","1f21f2f0":"To visualise the results, we will first merge the less common complaints into a new one titled \u2018Rest\u2019.","ea352293":"Grand Concourse belongs in the Bronx, the most affected borough of NY.","75bf75c8":"There are indeed duplicate addresses. We need to remove them before proceeding.","006a7030":"The 'XCoord', 'YCoord', and 'ZipCode' features will not be used for our models, so we don\u2019t need to care about missing values in them.\n\n### Feature Selection\n\nFor our models, we will use features with an absolute Spearman\u2019s correlation coefficient higher than 0.15.","8223f4c8":"<br>\n\n## Question 4 - Answer \n\nIn this notebook, we built <font color=\"firebrick\"><b>two predictive models<\/b><\/font> for future prediction of the possibility of getting a 'HEAT\/HOT WATER' complaint in the Bronx. Our models are based on typical building characteristics such as the number of floors, residential and lot area, the building's year, etc. The <font color=\"firebrick\"><b>XGB classifier<\/b><\/font> (accuracy <font color=\"firebrick\"><b>~ 82%<\/b><\/font>) performs better than the Random Forest classifier (accuracy ~ 77%).","9498fc6a":"Evidently, <font color=\"firebrick\">'HEAT\/HOT WATER'<\/font> is the <font color=\"firebrick\">most frequent complaint<\/font> during the studied period. It amounts to approximately 35% of all complaints.\n\nIs it also the most common complaint for each year individually?","abfa2391":"### Feature Importance","2c4a6390":"### Feature Scaling","5d51d3b3":"We could interpret NaN values for 'Complaints #' as the corresponding building not showing any heating problems. Of course, this could be false for some buildings if the owners\/tenants don\u2019t report any problem. But we will stick to our hypothesis. \n\nWe will fill these values with 0 and turn all non-zero values into 1, thus making 'Complaints #\u2019 a **binary** column.","df316f15":"# Datasets\n\nWe will use two datasets from the Department of Housing Preservation and Development of New York City to address their problems. You can have a look at the [Quiz on Data Ingestion](https:\/\/github.com\/KOrfanakis\/MOOCs\/blob\/main\/Python_Data_Science_IBM\/05-Data_Science_and_Machine_Learning_Capstone_Project\/Quiz_on_Data_Ingestion.ipynb) notebook for a quick overview of the datasets.\n\n## 311 Complaint Dataset\n\nThe 311 Complaint Dataset is the primary dataset for our analysis. It is essentially the log of complaints filed by residents of the five boroughs in New York City. It is updated daily and is available at the [NYC OpenData website](https:\/\/data.cityofnewyork.us\/Social-Services\/311-Service-Requests-from-2010-to-Present\/erm2-nwe9). Kaggle user [sousablde](https:\/\/www.kaggle.com\/sousablde) has uploaded a version of the original dataset that contains entries from 2010 to (August) 2019. It also contains data only for relevant columns so that the data volume is manageable.\n\nAgain, I should mention that the dataset I used for this notebook is slightly different than the one I used for notebooks found on my GitHub repository. This fact caused a minor discrepancy between the numbers reported in these two analyses; however, the results are the same.\n\n## PLUTO Dataset for Housing\n\nThe second dataset contains building characteristics for each house in New York City. We can download the PLUTO dataset as a zip file from [this link](https:\/\/data.cityofnewyork.us\/City-Government\/Primary-Land-Use-Tax-Lot-Output-PLUTO-\/xuk2-nczf). After extracting the file, we should have five CSV files for the five New York City boroughs: Bronx, Brooklyn, Manhattan, Queens, and Staten Island. These CSV files along with the 311 Complaint Dataset are included in the [311 NYC Open Data HPD](https:\/\/www.kaggle.com\/sousablde\/311-nyc-open-data-hpd) dataset on Kaggle.","aabfd0fe":"For the sake of completeness, we can check for correlations between every possible pair of features by plotting a heatmap:","b35818c2":"### Splitting Sets","1b4745db":"---\n\n# Question 3: Identify Relationship between the Building Characteristics and the Top Complaint Type\n\nThe goal of this exercise is to find the answer to the third question of the problem statement:\n\n<font color=\"royalblue\"><center><em>Does the Complaint Type you identified in response to Question 1 have an apparent relationship with any particular characteristic or characteristics of the houses?<\/em><\/center><\/font>","efe24971":"We can plot a map to visualise the distribution of 'HEAT\/HOT WATER' complaints.","8a6c5409":"---\n\n# Question 1: Identify the Top Complaint Type\n\nThe goal of this section is to find the answer to the first question of the problem statement:\n\n<font color=\"royalblue\"><center><em>Which type of complaint should the Department of Housing Preservation and Development of New York City focus on first?<\/em><\/center><\/font>","90cccd1c":"The PLUTO dataset also contains missing values for 'Address'.","6ee754bb":"# Results\n\nThe answers to the questions are summarised in the [Conclusions](#Conclusions) section. You can also view a slightly more  descriptive version of each answer at the end of each question subsection ([Answer #1](#Question-1---Answer), [Answer #2](#Question-2---Answer), [Answer #3](#Question-3---Answer), and [Answer #4](#Question-4---Answer)).","f8c7a20c":"---\n\n<br>\n\n# Conclusions\n\nIn summary, my findings are the following:\n\n- **Question 1**: 'HEAT\/HOT WATER' is the most frequent complaint type.\n- **Question 2**: The Bronx is the most affected borough by this complaint type. The ZIP code with the highest number of complaints about 'HEAT\/HOT WATER' is 11226 and belongs in Brooklyn. Finally, Grand Concourse is the most affected street in New York City and belongs in the Bronx.\n- **Question 3**: Correlation analysis shows a (moderate positive) correlation between the total number of 'HEAT\/HOT WATER' complaints and several building characteristics\/features. It is not surprising that these features are related to spatial dimensions (larger area leads to a higher possibility for a heating-related complaint).\n- **Question 4**: We could build two predictive models for future prediction of the possibility of getting a 'HEAT\/HOT WATER' complaint in the Bronx. The XGB classifier (accuracy ~ 82%) performs better than the Random Forest classifier (accuracy ~ 76%).\n","14a4a1c4":"The first two types appear to be similar. That's because in 2014 the label changed from 'HEATING' to 'HEAT\/HOT WATER'. Therefore, we need to merge these complaint types and analyse them together. We will perform the same thing for the 'PAINT - PLASTER'-'PAINT\/PLASTER' types. For a complete analysis, we also need to merge other pairs of features. However, this is not important here. ","e575e8af":"## Exploratory Data Analysis\n\nFirst, let\u2019s look at a summary of the 311 Dataset:","f11e536c":"---\n\n# Question 2: Identify Areas Most Affected by the Top Complaint Type\n\nThe goal of this exercise is to explore the data further and find the answer to the second question of the problem statement:\n\n<font color=\"royalblue\"><center><em>Should the Department of Housing Preservation and Development of New York City focus on any particular set of boroughs, ZIP codes, or street (where the complaints are severe) for the specific type of complaints you identified in response to Question 1?<\/em><\/center><\/font>","eafec8e0":"Let\u2019s see how many complaints fall in each category:","357fc454":"<br>\n\n## Question 3 - Answer \n\n- Using <font color=\"firebrick\"><b>Spearman's correlation coefficient<\/b><\/font>, we showed that the total number of 'HEAT\/HOT WATER' complaints displays a <font color=\"firebrick\"><b>moderate positive correlation<\/b><\/font> with <font color=\"firebrick\"><b>six features<\/b><\/font> ('ResArea', 'NumFloors', 'BuiltFAR', 'BldgArea', 'BldgDepth', and 'LotArea').\n- Only <font color=\"royalblue\"><b>two<\/b><\/font> of these features ('NumFloors' and 'BldgDepth') show a <font color=\"royalblue\"><b>weak linear correlation<\/b><\/font> measured by <font color=\"royalblue\"><b>Pearson\u2019s correlation coefficient<\/b><\/font>. The remaining four exhibit a very weak linear correlation.","25b9ba35":"Three columns contain missing values. Luckily, the number of missing values amounts to only a small portion (~1%) for all these columns.\n\n## Boroughs\n\nLet\u2019s use the `value_counts()` method to list all boroughs and the total number of \u2018HEAT\/HOT WATER\u2019 complaints in each. ","6ece3e6f":"Consequently, it has been the most common complaint for each year individually.","176e86f7":"### Class Imbalance\n\nThere is an imbalance in the classes to be predicted, with one class (0 \u2013 No 'HEAT\/HOT WATER' complaint) much more prevalent than the other (1 - at least one complaint):","0eac672e":"To answer Question 2, we also need a geojson file for the boundary of each borough.","3cf1b83e":"We should check for missing values in our new dataset.","25f085c1":"# Approach\n\nWe can answer the first two problem statements by performing Exploratory Data Analysis (EDA) on the 311 Complaint Dataset. We essentially need two tools: Pandas, for extracting useful information from our dataset, and Matplotlib for visualising the results.\n\nFor the third problem, we need to include building characteristics in our analysis. Therefore, we will first merge the two datasets and then examine the correlation between the most common complaint type and the building characteristics.\n\nFinally, the fourth problem statement requires the basic Machine Learning (ML) workflow. After preprocessing the merged dataset, we will build two ML models for predicting a building's chance of showing the most common complaint type. The algorithms used are Random Forest and XGBoost classifier.","fa84c7a7":"---\n\n# Question 4: Predict Complaint Types\n\nThe goal of this exercise is to develop a model and find the answer to the fourth question of the problem statement:\n\n<font color=\"royalblue\"><center><em>Can a predictive model be built for future prediction of the possibility of complaints of the specific type you identified in response to Question 1?<\/em><\/center><\/font>","d3add4c6":"### Missing Values\n\nAs we discussed already, some rows contain NaN values for the 'Complaints #' column. ","e6d04a91":"Our Random Forest classifier achieves an accuracy approximately equal to <font color=\"firebrick\">77%<\/font>. Notice that it has a similar performance in classifying both 1s and 0s. Additionally, our model does **not** suffer from overfitting at the end of training.","5372d170":"Notice that some instances have an \u2018Unspecified\u2019 borough. We can assign the actual borough name since other information (street name, zip code, etc.) are available. However, we can safely ignore those instances for now. ","611e2c65":"Apart from missing values, we also need to check if there are duplicate entries for the same address.","e156b12f":"The ROC curve is another way of comparing the two classifiers. The XGB classifier is superior to the Random Forest classifier because its ROC curve is much closer to the top-left corner (or similarly, further away from the baseline random classifier). As a result, its area under the ROC curve (AUC) is also better.","d4a17702":"We need to focus on the borough with the highest number of 'HEAT\/HOT WATER' reports, i.e. the Bronx. Therefore, we will create a new DataFrame containing only BRONX samples and with 'HEAT\/HOT WATER' as the complaint type.","e91640b1":"Therefore, <font color=\"firebrick\">the Bronx<\/font> is the NY borough with the highest number of 'HEAT\/HOT WATER' reports (569,194) amounting to almost 1\/3 of the total number. Brooklyn follows closely with 542,245 complaints.\n\n## Zip Codes","81f510b1":"'ResArea' is the most important feature for both classifiers. Moreover, features 'BuiltFAR', 'NumFloors', and 'ResidFAR' rank high in importance. On the other hand, 'YearAlter' and 'LotArea' do not seem that critical for our predictive models. ","bc485a57":"We notice only a <font color=\"firebrick\">weak positive linear correlation<\/font> (absolute correlation coefficient ~ 0.20) with <font color=\"firebrick\">two features<\/font>: 'NumFloors' and 'BldgDepth'.\n\n\n### Spearman\u2019s Correlation\n\nAfter a post in the discussion section, I also decided to use Spearman's correlation coefficient. It assesses how well the relationship between two variables can be described using a monotonic function.","580ceed5":"<br>\n\n## Question 1 - Answer \n\nBased on the previous analysis, the Department of Housing Preservation and Development of New York City should address the <font color=\"firebrick\"><b>'HEAT\/HOT WATER'<\/b><\/font> complaint first. This type of complaint has the highest number of reports (more than 2 million, amounting to 35% of all complaints) and has always been the most frequent complaint in every individual year since 2010.","6db96aa0":"In Problem I, we concluded that 'HEAT\/HOT WATER' is the most severe type of complaint in New York between 2010 and 2019. For the current problem set, we need to investigate if particular locations are more critical in addressing the issue.\n\nFor this purpose, we will make a new dataset containing columns only for 'borough', 'incident_zip', 'street_name', latitude', 'longitude' and 'complaint_type', and rows corresponding to 'HEAT\/HOT WATER'.","84f3391a":"### Random Forest","1d66a303":"<font color=\"firebrick\">Grand Concourse<\/font> is the street with the highest number of submitted complaints (31,434).","3b7e124f":"# Importing the Data\n","cf3ddb54":"## Data Preprocessing\n\nBefore we start pre-processing our merged dataset, we will create a copy of it. ","613a096f":"### ROC Curve","b7e39af2":"For this exercise, we also need to import the PLUTO dataset for the Bronx.","895276ae":"## Performance on the Test Set","7385d452":"The total number of 'HEAT\/HOT WATER' complaints has a <font color=\"firebrick\">moderate positive correlation<\/font> (absolute correlation coefficient between 0.40 - 0.59) with <font color=\"firebrick\">6 features<\/font>: 'ResArea', 'NumFloors', 'BuiltFAR', 'BldgArea', 'BldgDepth', 'LotArea'. It's not a surprise that these features are related to spatial dimensions.\n\nAll other features show a weak correlation (0.2 - 0.39) or a very week correlation (0-0.19). \n\nWe can confirm that our results are statistically significant by calculating the p-value for each feature.","398fb95d":"<br>\n\n## Question 2 - Answer \n\nFrom the above analysis of the 'HEAT\/HOT WATER' complaint, we can conclude that the Department of Housing Preservation and Development of New York City should focus on:\n\n- <font color=\"firebrick\"><b>Bronx<\/b><\/font> and <font color=\"firebrick\"><b>Brooklyn<\/b><\/font>, since they are the first and second most affected boroughs in NYC, respectively,\n- Houses in Brooklyn with <font color=\"firebrick\"><b>11226<\/b><\/font> as their Zip code, and\n- <font color=\"firebrick\"><b>Grand Concourse<\/b><\/font>, a severely affected street in the Bronx.\n","6372b688":"There are several strategies for addressing the class imbalance. Motivated from [this article](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/), I have decided first to oversample the minority class to have 50 percent of the number of instances of the majority class (e.g. 27846), then use random undersampling to reduce the number of instances in the majority class to have the same number of instances as the minority class.\n\nTo implement this, we can specify the desired ratios as arguments to the `SMOTE` and `RandomUnderSampler` classes:","a8a5bfc6":"## Building a Predictive Model\n\nWe are now ready to build Machine Learning models! Specifically, we will use two classifiers: scikit-learn's `RandomForestClassifier` and xgboost's `XGBClassifier`. \n\nThe exercise does **not** ask for hyperparameter tuning; therefore, I decided to stick to the default parameters (except for reducing the `max_depth` hyperparameter to avoid overfitting). A complete analysis would require a hyperparameter tuning step.","6f44fe70":"# Libraries and Functions\n\nWe start by importing the necessary libraries and setting some parameters for the whole notebook (such as parameters for the plots, etc.).","741c5a66":"**Table of Contents**\n\n1. [Problem Statement](#Problem-Statement)\n2. [Datasets](#Datasets)\n3. [Approach](#Approach)\n4. [Results](#Results)\n5. [Libraries and Functions](#Libraries-and-Functions)\n6. [Importing the Data](#Importing-the-Data)\n7. [Question 1: Identify the Top Complaint Type](#Question-1:-Identify-the-Top-Complaint-Type) <br>\n    i. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n    ii. [Question 1 - Answer](#Question-1---Answer)<br>\n8. [Question 2: Identify Areas Most Affected by the Top Complaint Type](#Question-2:-Identify-Areas-Most-Affected-by-the-Top-Complaint-Type)<br>\n    i. [Boroughs](#Boroughs) <br>\n    ii. [Zip Codes](#Zip-Codes) <br>\n    iii. [Streets](#Streets) <br>\n    iv. [Question 2 - Answer](#Question-2---Answer)\n9. [Question 3: Identify Relationship between the Building Characteristics and the Top Complaint Type](#Question-3:-Identify-Relationship-between-the-Building-Characteristics-and-the-Top-Complaint-Type)<br>\n    i. [Merging the two Datasets](#Merging-the-two-Datasets)<br>\n    ii. [Looking for Correlations](#Looking-for-Correlations)<br>\n    iii. [Question 3 - Answer](#Question-3---Answer)<br>\n10. [Question 4: Predict Complaint Types](#Question-4:-Predict-Complaint-Types)<br>\n    i. [Data Preprocessing](#Data-Preprocessing)<br>\n    ii. [Building a Predictive Model](#Building-a-Predictive-Model)<br>\n    iii. [Performance on the Test Set](#Performance-on-the-Test-Set)<br>\n    iv. [Future Development](#Future-Development)<br>\n    v. [Question 4 - Answer](#Question-4---Answer)<br>\n11. [Conclusions](#Conclusions)\n\n\n# Problem Statement\n\nNew York City residents use the 311 system to report complaints about non-emergency problems to local authorities. Various agencies in New York are assigned these problems. The Department of Housing Preservation and Development of New York City is the agency that processes 311 complaints that are related to housing and buildings.\n\nIn the last few years, the number of 311 complaints coming to the Department of Housing Preservation and Development has increased significantly. Although these complaints are not necessarily urgent, the large volume of complaints and the sudden increase impact the agency's overall efficiency.\n\nTherefore, the Department of Housing Preservation and Development has approached your organisation to help them manage the large volume of 311 complaints they are receiving every year.\n\nThe agency needs answers to several questions. Data and analytics must support the answers to those questions. These are their questions:\n\n- Which type of complaint should the Department of Housing Preservation and Development of New York City focus on first?\n- Should the Department of Housing Preservation and Development of New York City focus on any particular set of boroughs, ZIP codes, or street (where the complaints are severe) for the specific type of complaints you identified in response to Question 1?\n- Does the Complaint Type you identified in response to question 1 have an apparent relationship with any particular characteristic or characteristics of the houses or buildings?\n- Can a predictive model be built for a future prediction of the possibility of complaints of the type you have identified in response to question 1?\n\nYour organisation has assigned you as the lead data scientist to provide the answers to these questions. You need to work on getting answers to them in this Capstone Project by following the standard approach of data science and machine learning.","37ae29ef":"## Merging the two Datasets\n\nAt this stage, we need to **merge** the two datasets into one. I chose to merge them based on an instance\u2019s address.\n\nWe can use the `groupby()` method on the 311 Dataset to create a dataset with the total number of complaints about each address. We first need to drop all rows with a missing address. ","52130723":"<font size=+3 color=\"black\"><center><b>Data Science and Machine Learning Capstone Project - NYC 311 Complaint Dataset<\/b><\/center><\/font>\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1496588152823-86ff7695e68f?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1350&q=80\" width = 500>\n<center><em>Photo by Luca Bravo (Unsplash)<\/em><\/center>\n\n<br>\n\nThis notebook contains the analysis I used to complete IBM's online course '[Data Science and Machine Learning Capstone Project](https:\/\/www.edx.org\/course\/data-science-and-machine-learning-capstone-project)' on edX. I completed the course in July 2020.\n\nThe course is organised into four modules, one for each of the problem statement's four questions. I decided to follow the same structure and divide the main part of this notebook into four sections, one for each question. There is also an introductory module (titled 'Quiz on Data Ingestion') which is not included here but can be found on my [GitHub repository](https:\/\/github.com\/KOrfanakis\/MOOCs\/blob\/main\/Python_Data_Science_IBM\/05-Data_Science_and_Machine_Learning_Capstone_Project\/Quiz_on_Data_Ingestion.ipynb).\n\nAt the end of each module, a quiz section is used for assessment. Again, this section is not included in this notebook, but you can find it on my [GitHub repository](https:\/\/github.com\/KOrfanakis\/MOOCs\/tree\/main\/Python_Data_Science_IBM\/05-Data_Science_and_Machine_Learning_Capstone_Project). \n\n\nI should mention that the dataset I used for this notebook is slightly different than the one I used for notebooks found on my GitHub repository. This fact caused a minor discrepancy between the numbers reported in these two analyses; however, the results are the same.","a3b8b9fa":"<br>\n\nThe two datasets have a different number of rows. Since the PLUTO dataset has more entries\/addresses (87017 compared to only 22103 for the 311 Dataset), some rows will contain NaN values for the 'Complaints #' column. \n\nWe are now ready to merge the two datasets:","9db3cb24":"## Streets","dcab6939":"The XGB classifier has higher accuracy (~ <font color=\"firebrick\">82%<\/font>), but it performs better when classifying 0s. There is a small gap between the curves at the end of training, which we could easily close by regularising our model further.","8511d332":"### XGBClassifier","8441fbbe":"---\n\nPlease feel free to make any suggestions for improving my analysis. Also, please consider <font color=\"red\"><b>upvoting<\/b><\/font> if you found this notebook useful. Thank you! \ud83d\ude09","9c55982f":"There are 16 columns in total. The column of interest for this exercise is the 'complaint_type'.\n\nHow many different complaint types exist in our dataset, including duplicates entries of the same type?","61e4ddb1":"## Looking for Correlations\n\nLet\u2019s look at how much each attribute correlates with the number of complaints. \n\n### Pearson\u2019s Correlation\n\nWe will initially use Pearson\u2019s correlation which measures linear correlations.","e58d9d38":"We need three functions for our task:\n\n- **plot_learning_curve**: As the name suggests, this function plots the learning curve for our estimator\/classifier,\n- **clf_performance**: This function does several things. It firstly outputs the cross-validation scores for our classifier, their mean and standard deviation. It then plots a confusion matrix next to the plot for the learning curve. Finally, it returns the False Positive Rate (fpr), the True Positive Rate (tpr), and the AUC score,\n- **plot_feature_imp**: The final functions plots a horizontal bar plot with each bar showing the importance of the corresponding feature (in descending order)."}}