{"cell_type":{"71de75e8":"code","7915842e":"code","4cf54f75":"code","cc29afb1":"code","432813c3":"code","402d13c4":"code","e9418ffa":"code","d3c01fea":"code","ab924091":"code","208d43ac":"code","5666e81b":"code","568bb0b5":"code","6c08b1d5":"code","324f5cc2":"code","e29d0615":"code","88f64c04":"code","4fb35038":"code","d6c28e5d":"code","5bb721de":"code","67f46e67":"code","69910ff8":"code","eb7856f5":"code","9ccb1e06":"code","f9f4b42c":"code","9ee7ab85":"markdown"},"source":{"71de75e8":"import pickle\nimport pandas as pd\nhpa_df = pd.read_pickle(\"..\/input\/gtdataframes\/dataset_partial_300.pkl\")","7915842e":"hpa_df","4cf54f75":"LABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","cc29afb1":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt","432813c3":"IMG_WIDTH = 300\nIMG_HEIGHT = 300\nBATCH_SIZE = 16\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","402d13c4":"#%%script echo skipping\n#set an amount of folds to split dataframe into --> k-fold cross validation\n# explanation: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\nN_FOLDS=5\n#choose which one of the 5 folds will be used as validation set this time\ni_VAL_FOLD=1\nhpa_df=np.array_split(hpa_df, N_FOLDS+1) #add one extra part for testing set\ndf_test_split=hpa_df[-1]\nhpa_df=hpa_df[:-1]\ni_training = [i for i in range(N_FOLDS)]\ni_training.pop(i_VAL_FOLD-1)\ni_validation=i_VAL_FOLD-1\ndf_train_split=list()\nfor i in i_training:\n    df_train_split.append(hpa_df[i])\ndf_train_split=pd.concat(df_train_split)\ndf_val_split=hpa_df[i_validation]","e9418ffa":"print(len(df_train_split))\nprint(len(df_val_split))\nprint(len(df_test_split))","d3c01fea":"#%%script echo skipping\n#analyze class imbalance and set up class weights here\n#https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/improve-class-imbalance-class-weights\/\ny_train=df_train_split[\"Label\"].apply(lambda x:list(map(int, x.split(\"|\"))))\ny_train=y_train.values\ny_train=np.concatenate(y_train)\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","ab924091":"#%%script echo skipping\ntmp_dict={}\nfor i in range(len(LABELS)):\n    tmp_dict[i]=class_weights[i]\nclass_weights=tmp_dict\nclass_weights","208d43ac":"#%%script echo skipping\n# adapted from https:\/\/www.kaggle.com\/ayuraj\/hpa-multi-label-classification-with-tf-and-w-b\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n    return one_hot_enc_tensor\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    rgb = tf.io.read_file(df_dict['path'])\n    image = tf.image.decode_png(rgb, channels=3)\n    #https:\/\/medium.com\/@kyawsawhtoon\/a-tutorial-to-histogram-equalization-497600f270e2\n    image=tf.image.per_image_standardization(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","5666e81b":"#%%script echo skipping\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","568bb0b5":"#%%script echo skipping\ndef get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(10):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","6c08b1d5":"#%%script echo skipping\n# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n#print(label_batch)","324f5cc2":"#%%script echo skipping\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB3(include_top=False, weights='imagenet')\n    base_model.trainable = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","e29d0615":"#%%script echo skipping\ntime_stopping_callback = tfa.callbacks.TimeStopping(seconds=int(round(60*60*8)), verbose=1) #8h to not exceed allowance\n\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\nlronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=5, verbose=0,\n    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0\n)","88f64c04":"#%%script echo skipping\n#set up checkpoint save\n#source:https:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load\n!pip install -q pyyaml h5py\nimport os\ncheckpoint_path_input = \"..\/input\/traineffnetb3versionapart1\/cp.ckpt\"\ncheckpoint_path = \".\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","4fb35038":"#%%script echo skipping\nimport keras.backend as K\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef f1_loss(y_true, y_pred):\n    \n    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","d6c28e5d":"#%%script echo skipping\nimport tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","5bb721de":"#https:\/\/stackoverflow.com\/questions\/47490834\/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras\nimport keras\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer._decayed_lr(tf.float32)\n    return lr\noptimizer = keras.optimizers.Adam()\nlr_metric = get_lr_metric(optimizer)","67f46e67":"#%%script echo skipping\n# Initialize model\ntf.keras.backend.clear_session()\n\n#model = get_model()\nmodel.load_weights(checkpoint_path_input)\n\n# Compile model\nmodel.compile(\n    optimizer=optimizer, \n    loss=f1_loss, \n    metrics=[f1, lr_metric])\n\n# Train\nhistory=model.fit(\n    train_ds,\n    epochs=1000,\n    validation_data=val_ds,\n    class_weight=class_weights,\n    callbacks=[cp_callback,earlystopper,time_stopping_callback])","69910ff8":"#%%script echo skipping\nhistory.history","eb7856f5":"#%%script echo skipping\n#source: https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n# list all data in history\nprint(history.history.keys())\n# summarize history for f1\nplt.plot(history.history['f1'])\nplt.plot(history.history['val_f1'])\nplt.title('f1')\nplt.ylabel('f1')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for lr\nplt.plot(history.history['lr'])\nplt.title('learning rate')\nplt.ylabel('learning rate')\nplt.xlabel('epoch')\nplt.legend('train', loc='upper left')\nplt.show()","9ccb1e06":"with open('.\/historyhistory.pkl', 'wb') as handle:\n    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)","f9f4b42c":"with open('.\/history.pkl', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","9ee7ab85":"#### this notebook is part of the documentation on my HPA approach  \n    -> main notebook: https:\/\/www.kaggle.com\/philipjamessullivan\/0-hpa-approach-summary\n\n\n## 7: network training\n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb0-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb1-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb2-version-a-part-1  \n    -> https:\/\/www.kaggle.com\/philipjamessullivan\/7-train-effnetb3-version-a-part-1  \n    \n##### Network Architecture: Efficientnet (B0 to B3) with a Dropout layer of 0.3  \n\n#### Goal: further refine model  \n\n##### INPUT: \n**resized cropped RGB cell images**:  \n\nEfficientnetB0 -> 224x224  \nEfficientnetB1 -> 240x240  \nEfficientnetB2 -> 260x260  \nEfficientnetB3 -> 300x300  \n\n**specific subset of ground truth dataframe**:  \n\nColums: img_id, Label, Cell#, bbox coords, path  \n\n##### OUTPUT:\n**model checkpoints**:  \n\none after each epoch\n\n**training history**:  \n\nshown as printed array on screen with f1 loss and f1 score per step"}}