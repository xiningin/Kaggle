{"cell_type":{"981a1553":"code","e20c5105":"code","a7b97571":"code","e02a8003":"code","b3320058":"code","be600b86":"code","c7c9a61a":"code","3bcf8148":"code","e4f94feb":"code","5d1569a7":"code","77cdb8de":"code","426d3352":"code","1c160098":"code","c05b61c7":"code","36cac895":"code","0450b40f":"code","029c13ff":"code","ad491b69":"code","9b88b718":"code","9a2d2c78":"code","c0d5b520":"code","f5ab478b":"code","ef1a9466":"code","a7de1f68":"code","a8bcdb77":"code","a929d08d":"code","54b31ae3":"code","4f9e69e7":"code","dfde9649":"code","e7dab0ae":"code","f24116f7":"code","78f1dbec":"code","012cbd26":"code","bfa640c7":"code","cb21b53f":"code","d389c48f":"code","9099870a":"code","b125b45b":"code","b47f8e8b":"code","01b1f9ef":"code","8bf0d845":"code","4ca9c865":"code","f10c86ca":"code","26b385f2":"code","e0f3d0db":"code","a1b4cc02":"code","f5794da9":"code","293bf1d0":"code","c434dab0":"code","5efdf232":"code","71d1598b":"code","0b73a64e":"code","f38a7f6d":"code","9b490ffd":"code","051d4eff":"code","dcde4ffc":"code","c7b70cfe":"code","32a39733":"code","efdf2f5b":"code","c349a0ed":"code","cc9d6863":"code","8d9bbbe0":"code","46f5f4fd":"code","6ea1d474":"code","2b567136":"code","9848a470":"code","5dd6da66":"code","0f69a2f1":"code","2c0471e2":"code","b5941f2e":"code","c90922ef":"code","2c826a79":"code","97923aff":"code","73811abc":"code","8fe683a3":"code","daff0c31":"code","f818d6ce":"code","579ca062":"code","8cb53b1d":"code","feba358e":"code","4d67ae84":"code","cc6a93cc":"code","a980ed79":"code","a1048d46":"code","faa2ec88":"code","b67285b6":"code","a851ea1f":"code","85f8777c":"code","15932bda":"code","fa5b9011":"code","e724980b":"code","ec12b147":"code","c63a8255":"code","418e85e4":"code","cb6c5cfc":"code","e08ad17a":"code","0fe131a9":"code","e6f2f3bf":"code","d3c9bd6e":"code","d6255648":"code","6cea64b1":"code","f969946a":"code","46bbcda8":"code","fda33800":"code","004aeef1":"code","97554acc":"code","fde626fe":"code","14efdf0b":"code","a735d0f7":"code","7e033308":"code","fbd6649c":"code","36f1d6f8":"code","67163d9e":"code","7a283bcb":"code","b1bcb3ec":"code","da9b8cc8":"code","ccc30b57":"code","65e028ca":"code","b21e496f":"code","214b31c4":"code","4916ba02":"code","e797945b":"code","da775335":"code","fed62397":"code","de937c87":"code","0e3ec15e":"code","1ade11dd":"code","33ac312d":"code","df8bc551":"code","7e8b3ae8":"code","fbfe95c8":"code","ad8d67ae":"code","32a78daf":"code","295cd217":"code","a2683a2c":"code","40c11e66":"code","c58aad04":"code","c3df739b":"code","9876f29c":"code","03eeae48":"code","e974976b":"code","34bb4dac":"code","480294c8":"code","16b58ad6":"code","e83dda92":"code","70a5f29d":"code","b46fc194":"code","b58e84e3":"code","c6a74718":"code","0c22bcb0":"code","4687ee0f":"code","7af0a198":"code","fd29bc34":"code","0a39fe65":"code","996cb713":"code","8ef5527a":"code","8fd1cea0":"code","de76431d":"code","1143964a":"code","ba17cc8c":"code","16df19f2":"code","2b25c324":"code","47827ef7":"code","eb19ceae":"code","b92f5694":"code","0c76cc30":"code","1dece138":"code","b05f00c4":"code","20db88b1":"code","4c487032":"code","3190ed4c":"code","88dd240c":"code","56b162f0":"code","505d6e5e":"code","7c0d425b":"code","7269a4c3":"code","b4eaa52a":"code","f267a0e5":"code","8ee5b8ca":"code","fb3681d3":"code","92337a92":"code","ff4de999":"code","fed00df7":"code","27cd532b":"markdown","7976186f":"markdown","a33dc319":"markdown","9ce26655":"markdown","2431ddf0":"markdown","cfe2b200":"markdown","0d53de66":"markdown","4d88a061":"markdown","19c89821":"markdown","84454307":"markdown","55d6130f":"markdown","77d366b0":"markdown","490adbe2":"markdown","a9fe0dd0":"markdown","03bfcfb6":"markdown","2eed4356":"markdown","2ae77700":"markdown","384052b3":"markdown","33a46372":"markdown","2add31da":"markdown","9193f1d3":"markdown","e7083b8d":"markdown","1adaf0ae":"markdown","e02dc1d6":"markdown","a5c99b18":"markdown","1eac3f76":"markdown","cc0d56d8":"markdown","2b69c02e":"markdown","003d8c5d":"markdown","e43d9dbe":"markdown","8af6def8":"markdown","674cc262":"markdown","1fbf1c83":"markdown","86d72a66":"markdown","223f71ca":"markdown","31edcf6d":"markdown","51ff86a5":"markdown","0639eee1":"markdown","da122614":"markdown","b6a0f48a":"markdown","2c6b0dd3":"markdown","ff35ebfa":"markdown","750a1f7a":"markdown","16091cea":"markdown","4f74ba8d":"markdown","5b0651af":"markdown","9e998b58":"markdown","5ab2fc2a":"markdown","47656fee":"markdown","47540364":"markdown","73a9fa90":"markdown","e52e6520":"markdown","3987c558":"markdown","768eda5f":"markdown","0ecaa902":"markdown","5f15f605":"markdown","7d39182f":"markdown","f753c4e0":"markdown","7324d717":"markdown","39848679":"markdown","076f9c72":"markdown","9a1f0b0b":"markdown","f98719e4":"markdown","8280697c":"markdown","002ffbaf":"markdown","742bf4d1":"markdown","fc4986d1":"markdown","c0e344a3":"markdown","905b7a18":"markdown","d84664af":"markdown","b4c397cd":"markdown","3f762d6d":"markdown","2fc796e8":"markdown","926413e4":"markdown","4abf59ca":"markdown","0a0177d8":"markdown","0c81e91d":"markdown","2aa017de":"markdown","05e78581":"markdown","3f34527f":"markdown","f0c113b7":"markdown","22f3e1f5":"markdown","496abc84":"markdown","9a4d31d6":"markdown","0bc467a6":"markdown","745b3639":"markdown","5fba97c5":"markdown","b41e74ca":"markdown","22ceff26":"markdown","46844af8":"markdown","601bbad3":"markdown","c2e6e06f":"markdown","aeba83c3":"markdown","07a1ac11":"markdown","ba56de12":"markdown","534c5a29":"markdown","d9b52a19":"markdown","9686dfa1":"markdown","0e185581":"markdown","e541d704":"markdown","b0261a06":"markdown","7b43e062":"markdown","7779a6d5":"markdown","ac3087a1":"markdown","40e9aa28":"markdown","78118888":"markdown","07032605":"markdown","261ab2d5":"markdown","0f613510":"markdown","b4c84307":"markdown","e93f1a4e":"markdown","d670226c":"markdown","79653839":"markdown","f3773852":"markdown","cd4e902b":"markdown","dac2867f":"markdown","b896cf15":"markdown","0a2b20f8":"markdown","b1fc8d1b":"markdown","97c69e0f":"markdown","489ef1e3":"markdown","03587ba4":"markdown","d63a4d7f":"markdown","c6f21363":"markdown","0ceef549":"markdown","17d45ade":"markdown","2330db81":"markdown","552ff36e":"markdown","8d4d6146":"markdown","2b33a93a":"markdown","4006acf1":"markdown","cd889915":"markdown","c6bfa3a1":"markdown","4a957af2":"markdown","1a997a33":"markdown","10b102a0":"markdown","e2afb18b":"markdown","ae96eda1":"markdown","8e53945a":"markdown","a504ce9e":"markdown","9ea1b2fa":"markdown","0338c97e":"markdown","02eb47d1":"markdown","a62b535f":"markdown","ab8b1865":"markdown","a44f4260":"markdown","cabaddc1":"markdown","a9be6aae":"markdown","a8ae9b69":"markdown","20f17704":"markdown","e4d27c9e":"markdown","46f75349":"markdown","cf6fd84b":"markdown","9823ad64":"markdown","fc50c26a":"markdown","da2f6cd8":"markdown","334435e2":"markdown","413bc559":"markdown","7a5a9f0c":"markdown","080b6123":"markdown","e2756c1d":"markdown","ac0a7c40":"markdown","e2b8126c":"markdown","9980454f":"markdown","11c06744":"markdown","caf4a33e":"markdown","34490697":"markdown","25670718":"markdown","8b3b108e":"markdown","f78532d0":"markdown","15ea73f0":"markdown","4e39c53e":"markdown","81dd3e99":"markdown","29d44635":"markdown","3ddf9887":"markdown","2989a360":"markdown","06079e3e":"markdown","2e446532":"markdown","c2e9c7c2":"markdown","59462276":"markdown","4b04ae25":"markdown","d2daa8f9":"markdown","f61aaf8e":"markdown","ec382b29":"markdown","905674a4":"markdown","abb797d1":"markdown","c6367bf6":"markdown","84d39b66":"markdown","db48b2c3":"markdown","0381425b":"markdown","b2960e1a":"markdown","f6058b19":"markdown","a1077a50":"markdown","ae52cbe2":"markdown","d2869718":"markdown","e3acb6ba":"markdown","f06726ef":"markdown","14f37935":"markdown","51bf85cb":"markdown","302b016c":"markdown","c82073a0":"markdown","209dfc7d":"markdown","949c6e7f":"markdown","0eda3041":"markdown","6c2dd75d":"markdown"},"source":{"981a1553":"# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualisations\n# (a) ggplot-like graphs for EDA\nfrom plotnine import *\nimport plotnine\nplotnine.options.figure_size = (5.2,3.2)\n# (b) for plotting other plots\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","e20c5105":"# setting random seed for notebook reproducability\nimport random\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)","a7b97571":"# load datasets\nass = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/assessments.csv')\ncourses = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/courses.csv')\nresults = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/studentAssessment.csv')\ninfo = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/studentInfo.csv')\nreg = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/studentRegistration.csv')\nvle = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/studentVle.csv')\nmaterials = pd.read_csv('\/kaggle\/input\/open-university-learning-analytics-dataset\/vle.csv')","e02a8003":"ass.head()","b3320058":"# Percentage of missing values\nass.isnull().sum() * 100 \/ len(ass)","be600b86":"ass.nunique()","c7c9a61a":"ass[ass.duplicated()]","3bcf8148":"ass.info()","e4f94feb":"ass['id_assessment'] = ass['id_assessment'].astype(object)","5d1569a7":"# Group by module presentation and sum the weights of assessments\nass\\\n.groupby(['code_module','code_presentation'])\\\n.agg(total_weight = ('weight',sum))","77cdb8de":"# See what are the weights of exams in module presentations\nass[ass['assessment_type'] == 'Exam']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(total_weight = ('weight',sum))","426d3352":"# Count how many exams there are in every module presentation\nass[ass['assessment_type'] == 'Exam'][['code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['code_module', 'code_presentation'])\\\n.count()","1c160098":"# Sum the weights of all course work assignments per module presentation\nass[ass['assessment_type'] != 'Exam']\\\n.groupby(['code_module', 'code_presentation'])\\\n.agg(total_weight = ('weight',sum))","c05b61c7":"ass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight', sum))","36cac895":"ass[(ass['assessment_type'] == 'CMA') & (ass['weight'] == 0) & (ass['code_module'] != 'GGG')]['weight'].count()","0450b40f":"ass[(ass['assessment_type'] == 'TMA') & (ass['weight'] == 0) & (ass['code_module'] != 'GGG')]['weight'].count()","029c13ff":"ass[(ass['assessment_type'] == 'TMA') & (ass['weight'] == 0)]","ad491b69":"ass[ass['code_module'] == 'BBB']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight',sum))","9b88b718":"column = ass[(ass['assessment_type'] == 'CMA') & (ass['code_module'] != 'GGG')]['weight']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","9a2d2c78":"column = ass[(ass['assessment_type'] == 'TMA') & (ass['code_module'] != 'GGG')]['weight']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","c0d5b520":"# How many total assignments in GGG module are there?\nass[ass['code_module'] == 'GGG'][['code_module', 'code_presentation', 'assessment_type', 'id_assessment']]\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.count()","f5ab478b":"# Assign new weights to module GGG assessments\nass.loc[(ass.code_module=='GGG') & (ass.assessment_type=='TMA'),'weight'] = (100\/3)\nass.loc[(ass.code_module=='GGG') & (ass.assessment_type=='CMA'),'weight'] = (0)","ef1a9466":"# Check that TMA now sums to 100\nass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight', sum))","a7de1f68":"# check that all assessments now sum to 200\nass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation'])\\\n.agg(total_weight = ('weight', sum))","a8bcdb77":"def compareCols(df1, df2):\n    '''\n    Check what columns are shared between two dataframes\n    and count values of df1 present and absent in df2 (in the shared\n    columns)\n    '''\n\n    # Show shared columns between dataframes\n    # (a) Make lists of columns for each data frame\n    df1Columns = df1.columns.values.tolist()\n    df2Columns = df2.columns.values.tolist()\n\n    # (b) Find column names that are the same\n    diffDict = set(df1Columns) & set(df2Columns)\n    \n    print('Shared columns : ', diffDict, '\\n')\n\n    # (c) Make a list of the dictinary\n    diffList = list(diffDict)\n    # (d) Check that if values in\n    # every shared column match in\n    # the two dataframes\n    for col in diffList:\n        x = df1[col].isin(df2[col]).value_counts()\n        print('Check if values are present in both dataframes:')\n        print(x, '\\n')\n\ncompareCols(ass, results)","a929d08d":"def findDiffValues(df1, df2, col):\n    '''\n    Find all df1.col values not present in df2.col\n    '''\n    # Pull out all unique values of col\n    df1_IDs = df1[col].unique()\n    df2_IDs = df2[col].unique()\n\n    # Compare the two lists\n    # (a) Find what values are different\n    diff = set(df1_IDs).difference(set(df2_IDs))\n    # (b) Count how many are different\n    numberDiff = len(diff)\n\n    print(\"Values from df1 not in df2: \" + str(diff))\n    print(\"Number of missing values: \" + str(numberDiff))\n\nfindDiffValues(ass, results, 'id_assessment')","54b31ae3":"def printDiffValues(df1, df2, col):\n    '''\n    Show all df1.col values not present in df2.col\n    '''\n    # Pull out all unique values id_assessments\n    df1_IDs = df1[col].unique()\n    df2_IDs = df2[col].unique()\n\n    # Compare the two lists\n    # (a) Find what values are different\n    diff = set(df1_IDs).difference(set(df2_IDs))\n    \n    # Show information for all df1.col values not presentin df2.col\n    # (a) Make a list of missing values\n    missingList = list(diff)\n    # (b) Find these IDs in df2\n    missingDf = df1[df1[col].isin(missingList)]\n\n    return missingDf\n\nprintDiffValues(ass, results, 'id_assessment')","4f9e69e7":"# Make a list of missing IDs\nmissingList = [30723, 1763, 34885, 15014, 37444, 14990, 30713, 37424, 15025, 34898, 37434, 40087, 34872, 40088, 15002, 1757, 30718, 34911]\n\n# Get all rows with weight 100 from Assessments table\nweight100 = ass[ass['weight'] == 100]\n# Get all unique assessment IDs\nweight100List = weight100['id_assessment'].unique()\n\n# Compare this list with the list of all assessment IDs missing from results table\ncompare = set(weight100List).difference(set(missingList))\nnumberCompare = len(compare)\n\nprint(\"100 weighted assessments in the Results table (that are not missing exams): \" + str(compare))\nprint(\"Number of 100 weighted assessments (that are not missing exams) in the Results table: \" + str(numberCompare))","dfde9649":"# Show information for weight 100 assessments in the results table\n# (a) Make a list of IDs to look for\nmatchList = [24290, 25354, 24299, 25361, 25368, 25340]\n# (b) Find these IDs in the Assessments table\nmatchDf = ass[ass['id_assessment'].isin(matchList)]\n\nmatchDf","e7dab0ae":"results.head()","f24116f7":"# Percentage of missing values\nresults.isnull().sum() * 100 \/ len(results)","78f1dbec":"results.nunique()","012cbd26":"results[results.duplicated()]","bfa640c7":"results.info()","cb21b53f":"results['id_assessment'] = results['id_assessment'].astype(object)\nresults['id_student'] = results['id_student'].astype(object)","d389c48f":"# Have a look at NaN values\nresults[results['score'].isnull()]","9099870a":"# Replace all null values with 0s\nresults.fillna(0, inplace=True)","b125b45b":"courses.head()","b47f8e8b":"# Percentage of missing values\ncourses.isnull().sum() * 100 \/ len(courses)","01b1f9ef":"courses.nunique()","8bf0d845":"courses[courses.duplicated()]","4ca9c865":"courses.info()","f10c86ca":"reg.head()","26b385f2":"# Percentage of missing values\nreg.isnull().sum() * 100 \/ len(reg)","e0f3d0db":"reg.nunique()","a1b4cc02":"reg[reg.duplicated()]","f5794da9":"reg.info()","293bf1d0":"reg['id_student'] = reg['id_student'].astype(object)","c434dab0":"compareCols(reg, results)","5efdf232":"compareCols(info, results)","71d1598b":"# Pull out all unique values id_assessments\ndf1_IDs = reg['id_student'].unique()\ndf2_IDs = info['id_student'].unique()\n\n# Compare the two lists\n# (a) Find what assessment IDs are different\ndiff = set(df1_IDs).difference(set(df2_IDs))\n# (b) Count how many are different\nnumberDiff = len(diff)\n\nnumberDiff","0b73a64e":"compareCols(reg, info)","f38a7f6d":"info_not_in_results = printDiffValues(info, results, 'id_student')\ninfo_not_in_results.head(10)","9b490ffd":"# What are their final results?\ncolumn = info_not_in_results['final_result']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","051d4eff":"reg_not_in_results = printDiffValues(reg, results, 'id_student')\nreg_not_in_results.head(10)","dcde4ffc":"# What are their unregistration status?\nreg_not_in_results['date_unregistration'].notnull().sum()","c7b70cfe":"# Show rows with passes\ninfo_not_in_results[info_not_in_results['final_result'] == 'Pass']","32a39733":"# Find their date unregistration\nreg_not_in_results[reg_not_in_results['id_student'] == 1336190]","efdf2f5b":"reg_not_in_results[reg_not_in_results['id_student'] == 1777834]","c349a0ed":"materials.head()","cc9d6863":"# Percentage of missing values\nmaterials.isnull().sum() * 100 \/ len(materials)","8d9bbbe0":"materials.nunique()","46f5f4fd":"materials[materials.duplicated()]","6ea1d474":"materials.info()","2b567136":"materials['id_site'] = materials['id_site'].astype(object)","9848a470":"vle.head()","5dd6da66":"# Percentage of missing values\nvle.isnull().sum() * 100 \/ len(vle)","0f69a2f1":"vle.nunique()","2c0471e2":"vle[vle.duplicated()].head()","b5941f2e":"vle.info()","c90922ef":"vle['id_student'] = vle['id_student'].astype(object)\nvle['id_site'] = vle['id_site'].astype(object)","2c826a79":"info.head()","97923aff":"info.isnull().sum() * 100 \/ len(info)","73811abc":"info.nunique()","8fe683a3":"info[info.duplicated()]","daff0c31":"info.info()","f818d6ce":"info['id_student'] = info['id_student'].astype(object)","579ca062":"compareCols(materials, vle)","8cb53b1d":"findDiffValues(materials, vle, 'id_site')","feba358e":"printDiffValues(materials, vle, 'id_site')","4d67ae84":"# Merge with an inner join\nVLEmaterials = pd.merge(vle, materials, on=['code_module', 'code_presentation', 'id_site'], how='inner')\n# Drop columns\nVLEmaterials.drop(columns=['week_from', 'week_to', 'date'], inplace=True)\n\nVLEmaterials.head()","cc6a93cc":"VLEmaterials\\\n.groupby(['code_module', 'code_presentation', 'id_student'])\\\n.agg(total_click = (\"sum_click\",sum))","a980ed79":"total_click_per_student = VLEmaterials\\\n.groupby(['code_module', 'code_presentation', 'id_student'])\\\n.agg(total_click = (\"sum_click\",sum))\\\n.reset_index()\n\ntotal_click_per_student.head(7)","a1048d46":"# Check that all module presentations in\n# Registration table are present in Courses table\ncompareCols(reg, courses)","faa2ec88":"# Have a look at all unique module lengths\ncourses['module_presentation_length'].unique()","b67285b6":"# Merge with an inner join\nregCourses = pd.merge(reg, courses, on=['code_module', 'code_presentation'], how='inner')\n\nregCourses.head()","a851ea1f":"# Merge with an inner join\nregCoursesInfo = pd.merge(regCourses, info, on=['code_module', 'code_presentation', 'id_student'], how='inner')\n\nregCoursesInfo.head()","85f8777c":"# merge with an inner join\nassResults = pd.merge(ass, results, on=['id_assessment'], how='inner')\n# Rearrange column names\nassResults = assResults[['id_student', 'code_module', 'code_presentation', 'id_assessment', 'assessment_type', 'date', 'date_submitted', 'weight', 'is_banked', 'score']]\n\nassResults.head()","15932bda":"assResults.isnull().sum()","fa5b9011":"# Make a copy of dataset\nscores = assResults\n\n# Count how many exams there are in Results for every module presentation\nscores[scores['assessment_type'] == 'Exam'][['code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['code_module', 'code_presentation'])\\\n.nunique()","e724980b":"### Make helper columns ###\n# (a) Add column multiplying weight and score\nscores['weight*score'] = scores['weight']*scores['score']\n# (b) Aggregate recorded weight*score per student\n    # per module presentation\nsum_scores = scores\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(weightByScore = ('weight*score', sum))\\\n.reset_index()\n# (c) Calculate total recorded weight of module\n# (c.i) Get total weight of modules\ntotal_weight = ass\\\n.groupby(['code_module', 'code_presentation'])\\\n.agg(total_weight = ('weight', sum))\\\n.reset_index()\n# (c.ii) Subtract 100 to account for missing exams\ntotal_weight['total_weight'] = total_weight['total_weight']-100\n# (c.iii) Mark module DDD as having 200 credits \ntotal_weight.loc[(total_weight.code_module == 'DDD'), 'total_weight'] = 200\n\n### Calculate weighted score ###\n# (a) Merge sum_scores and total_weight tables\nscore_weights = pd.merge(sum_scores, total_weight, on=['code_module', 'code_presentation'], how='inner')\n# (b) Calculate weighted score\nscore_weights['weighted_score'] = score_weights['weightByScore'] \/ score_weights['total_weight']\n# (c) Drop helper columns\nscore_weights.drop(columns=['weightByScore', 'total_weight'], inplace=True)","ec12b147":"score_weights.head()","c63a8255":"# Calculate the difference between the submission dates\nlateSubmission = assResults.assign(submission_days=assResults['date_submitted']-assResults['date'])\n# Make a column indicating if the submission was late or not \nlateSubmission = lateSubmission.assign(late_submission=lateSubmission['submission_days'] > 0)\n\nlateSubmission.head()","418e85e4":"lateSubmission[(lateSubmission['assessment_type'] == 'Exam') & (lateSubmission['late_submission'] == True)]","cb6c5cfc":"# Aggregate per student per module presentation\ntotal_late_per_student = lateSubmission\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(total_late_submission = ('late_submission', sum))\\\n.reset_index()\n\ntotal_late_per_student.head()","e08ad17a":"# Make a df with total number of all assessments per student per module presentation\ntotal_count_assessments = lateSubmission[['id_student', 'code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.size()\\\n.reset_index(name='total_assessments')\n\ntotal_count_assessments.head()","0fe131a9":"# Merge df with total late assessements and total count assessments\nlate_rate_per_student = pd.merge(total_late_per_student, total_count_assessments, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n# Make a new column with late submission rate\nlate_rate_per_student['late_rate'] = late_rate_per_student['total_late_submission'] \/ late_rate_per_student['total_assessments']\n# Drop helper columns\nlate_rate_per_student.drop(columns=['total_late_submission', 'total_assessments'], inplace=True)\n\nlate_rate_per_student","e6f2f3bf":"# Define function for marking failed assignments\npassRate = assResults\npassRate = passRate.assign(fail=passRate['score'] < 40)\n\npassRate.head()","d3c9bd6e":"passRate.head()","d6255648":"# Aggregate per student per module presentation\ntotal_fails_per_student = passRate\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(total_fails = (\"fail\",sum))\\\n.reset_index()\n\ntotal_fails_per_student.head()","6cea64b1":"# Merge df with total fails and total count assessments\nfail_rate_per_student = pd.merge(total_fails_per_student, total_count_assessments, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n# Make a new column with late submission rate\nfail_rate_per_student['fail_rate'] = fail_rate_per_student['total_fails'] \/ fail_rate_per_student['total_assessments']\n# Drop helper columns\nfail_rate_per_student.drop(columns=['total_fails', 'total_assessments'], inplace=True)\n\nfail_rate_per_student","f969946a":"assessments = pd.merge(score_weights, late_rate_per_student, on=['id_student', 'code_module', 'code_presentation'], how='inner')\nassessments = pd.merge(assessments, fail_rate_per_student, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n\nassessments.head()","46bbcda8":"merged = pd.merge(regCoursesInfo, total_click_per_student, on=['id_student', 'code_module', 'code_presentation'], how='left')\n\nmerged.head()","fda33800":"merged = pd.merge(merged, assessments, on=['id_student', 'code_module', 'code_presentation'], how='left')\n\nmerged.head()","004aeef1":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(merged, test_size=0.2, random_state=42, stratify=merged['code_module'])","97554acc":"train.isnull().sum()","fde626fe":"train\\\n[train['imd_band'].isnull()]\\\n.head()","14efdf0b":"# Find what is the most frequent band in each region\nregions_list = list(train\\\n                    [train['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    result = train[train['region'] == i].imd_band.mode()\n    print(f'{i} IMD band : \\n', result)","a735d0f7":"# Replace all null values with respective most frequent imd_bands\nregions_list = list(train\\\n                    [train['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    train['imd_band'] = np.where( ( (train['imd_band'].isnull()) & (train['region'] == i) ),\n                                           train[train['region'] == i].imd_band.mode(),\n                                           train['imd_band']\n                                    )","7e033308":"# Make a new dataframe just with rows that have null values for the registration date\nreg_date_nulls_in_reg = train\\\n[train['date_registration'].isnull()]","fbd6649c":"# What are their final results?\ncolumn = reg_date_nulls_in_reg['final_result']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","36f1d6f8":"# Get median registration date\ntrain.date_registration.median()","67163d9e":"# Replace NaN values with date_unreg minus the median (note, the median is negative)\ntrain['date_registration'] = np.where( (train['date_registration'].isnull()),\n                                           train['date_unregistration'] + train.date_registration.median(),\n                                           train['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntrain['date_registration'] = np.where( (train['date_registration'].isnull()),\n                                           train.date_registration.median(),\n                                           train['date_registration']\n                                    )","7a283bcb":"train['total_click'] = train['total_click'].replace(np.nan).fillna(0)","b1bcb3ec":"train['weighted_score'] = train['weighted_score'].replace(np.nan).fillna(0)","da9b8cc8":"train['late_rate'] = train['late_rate'].replace(np.nan).fillna(1.0)","ccc30b57":"train['fail_rate'] = train['fail_rate'].replace(np.nan).fillna(1.0)","65e028ca":"# Make a copy of training and test datasets for classification\ntrain_class = train.copy()\ntest_class = test.copy()","b21e496f":"# Drop unneeded columns\ntrain.drop(columns=['id_student'], inplace=True)\ntrain.drop(columns=['final_result'], inplace=True)\ntrain.drop(columns=['date_unregistration'], inplace=True)\n\ntrain.head()","214b31c4":"# Drop unneeded columns\ntrain_class.drop(columns=['id_student'], inplace=True)\ntrain_class.drop(columns=['date_unregistration'], inplace=True)\n# Drop columns on assessments\ntrain_class.drop(columns=['weighted_score'], inplace=True)\ntrain_class.drop(columns=['late_rate'], inplace=True)\ntrain_class.drop(columns=['fail_rate'], inplace=True)\n\n\ntrain_class.head()","4916ba02":"'''IMD BAND'''\n# Replace all null values with respective most frequent imd_bands\nregions_list = list(test\\\n                    [test['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    test['imd_band'] = np.where( ( (test['imd_band'].isnull()) & (test['region'] == i) ),\n                                           test[test['region'] == i].imd_band.mode(),\n                                           test['imd_band']\n                                    )\n\n'''DATE REGISTRATION'''\n# Get registration date median\nreg_date_median = test.date_registration.median()\n\n\n# Replace NaN values with date_unreg minus 57 days\ntest['date_registration'] = np.where( (test['date_registration'].isnull()),\n                                           test['date_unregistration'] + reg_date_median,\n                                           test['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntest['date_registration'] = np.where( (test['date_registration'].isnull()),\n                                           reg_date_median,\n                                           test['date_registration']\n                                    )\n\n'''Rest of null values'''\ntest['total_click'] = test['total_click'].replace(np.nan).fillna(0)\ntest['weighted_score'] = test['weighted_score'].replace(np.nan).fillna(0)\ntest['late_rate'] = test['late_rate'].replace(np.nan).fillna(1.0)\ntest['fail_rate'] = test['fail_rate'].replace(np.nan).fillna(1.0)\n\n'''Drop unneeded columns'''\n# Drop ID, final result, and date unregistration columns\ntest.drop(columns=['id_student'], inplace=True)\ntest.drop(columns=['final_result'], inplace=True)\ntest.drop(columns=['date_unregistration'], inplace=True)","e797945b":"'''IMD BAND'''\n# Replace all null values with respective most frequent imd_bands\nregions_list = list(test_class\\\n                    [test_class['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    test_class['imd_band'] = np.where( ( (test_class['imd_band'].isnull()) & (test_class['region'] == i) ),\n                                           test_class[test_class['region'] == i].imd_band.mode(),\n                                           test_class['imd_band']\n                                    )\n\n'''DATE REGISTRATION'''\n# Get registration date median\nreg_date_median = test_class.date_registration.median()\n\n\n# Replace NaN values with date_unreg minus 57 days\ntest_class['date_registration'] = np.where( (test_class['date_registration'].isnull()),\n                                           test_class['date_unregistration'] + reg_date_median,\n                                           test_class['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntest_class['date_registration'] = np.where( (test_class['date_registration'].isnull()),\n                                           reg_date_median,\n                                           test_class['date_registration']\n                                    )\n\n'''Rest of null values'''\ntest_class['total_click'] = test_class['total_click'].replace(np.nan).fillna(0)\n\n'''Drop unneeded columns'''\n# Drop ID, final result, and date unregistration columns\ntest_class.drop(columns=['id_student'], inplace=True)\ntest_class.drop(columns=['date_unregistration'], inplace=True)\n# Drop columns on assessments\ntest_class.drop(columns=['weighted_score'], inplace=True)\ntest_class.drop(columns=['late_rate'], inplace=True)\ntest_class.drop(columns=['fail_rate'], inplace=True)","da775335":"train.describe().transpose()","fed62397":"# Create statistics summaries with skew, mean, and median\n# Produce a dataframe with just numerical columns\ndf_num = train.select_dtypes(include=np.number)\n\nfor col in df_num.columns:\n\n    skew = df_num[col].skew()\n    mean = df_num[col].mean()\n    median = df_num[col].median()\n    \n    print(f'\\tSummary for {col.upper()}')\n    print(f'Skewness of {col}\\t: {skew}')\n    print(f'Mean {col} :\\t {mean}')\n    print(f'Median {col} :\\t {median} \\n')","de937c87":"train.hist(bins=50, figsize=(20,15))\nplt.show()","0e3ec15e":"(\n    ggplot(train)\n    + aes(x=0, y='weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per module\")\n    + coord_flip()\n)","1ade11dd":"# Let's make a correlation heatmap\nplt.figure(figsize=(6,4))\nsns.heatmap(df_num.corr(), annot=True, cmap=\"coolwarm\", );","33ac312d":"train\\\n.drop(columns=['weighted_score'])\\\n.corrwith(train['weighted_score']).plot.bar(\n        figsize = (6, 4), title = \"Correlation with Target\", fontsize = 12,\n        rot = 90, grid = True);","df8bc551":"train.corrwith(train['weighted_score']).sort_values(ascending=False)","7e8b3ae8":"# Produce a dataframe with just categorical columns\ndf_cat = train.select_dtypes(exclude=np.number)\n\ndf_cat.head()","fbfe95c8":"# Set the plot number for the first subplot function\nplot_number = 1\n\n# Set sizes for all plots\nplt.figure(figsize=(15, 15)) # create a figure object\nplt.subplots_adjust(hspace = 0.5) # set the size of subplots\n\nfor col in df_cat[['code_module', 'code_presentation', 'gender', 'region']]:\n    \n    # Call countplot on each column\n    plt.subplot(4, 2, plot_number)\n    sns.countplot(\n        y=col,\n        data=df_cat,\n        order=df_cat[col].value_counts().index\n    )\n    plt.title(f'{col.capitalize()} Countplot')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plot_number = plot_number + 1 # set a new plot number for the next subplot function\n    \n    # Add relative frequency labels:\n    n_points = df_cat.shape[0]\n    col_counts = df_cat[col].value_counts()\n    locs, labels = plt.yticks()   # get the current tick locations and labels\n\n    # loop through each pair of locations and labels\n    for loc, label in zip(locs, labels):\n\n        # get the text property for the label to get the correct count\n        count = col_counts[label.get_text()]\n        pct_string = '{:0.1f}%'.format(100*count\/n_points)\n\n        # print the annotation at the top of the bar\n        plt.text(x=count, y=loc, s=pct_string, ha='left', va='center', color='k')\n    \nplt.tight_layout()","ad8d67ae":"# Set the plot number for the first subplot function\nplot_number = 1\n\n# Set sizes for all plots\nplt.figure(figsize=(15, 15)) # create a figure object\nplt.subplots_adjust(hspace = 0.5) # set the size of subplots\n\nfor col in df_cat[['highest_education', 'imd_band', 'age_band', 'disability']]:\n    \n    # Call countplot on each column\n    plt.subplot(4, 2, plot_number)\n    sns.countplot(\n        y=col,\n        data=df_cat,\n        order=df_cat[col].value_counts().index\n    )\n    plt.title(f'{col.capitalize()} Countplot')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plot_number = plot_number + 1 # set a new plot number for the next subplot function\n    \n    # Add relative frequency labels:\n    n_points = df_cat.shape[0]\n    col_counts = df_cat[col].value_counts()\n    locs, labels = plt.yticks()   # get the current tick locations and labels\n\n    # loop through each pair of locations and labels\n    for loc, label in zip(locs, labels):\n\n        # get the text property for the label to get the correct count\n        count = col_counts[label.get_text()]\n        pct_string = '{:0.1f}%'.format(100*count\/n_points)\n\n        # print the annotation at the top of the bar\n        plt.text(x=count, y=loc, s=pct_string, ha='left', va='center', color='k')\n    \nplt.tight_layout()","32a78daf":"# Rename 'no formal quals' into 'lower than a level'\ntrain['highest_education'] = np.where( (train['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           train['highest_education']\n                                    )\n\n# Rename post-grads\ntrain['highest_education'] = np.where( (train['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           train['highest_education']\n                                    )\n\n\n# Do the same for the test set\ntest['highest_education'] = np.where( (test['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           test['highest_education']\n                                    )\n\ntest['highest_education'] = np.where( (test['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           test['highest_education']\n                                    )","295cd217":"# Have a closer look at the category\n(\n    ggplot(train)\n    + aes(x='age_band', fill='age_band')\n    + geom_bar()\n    + geom_text(\n     aes(label='stat(prop)*100', group=1),\n     stat='count',\n     nudge_y=0.125,\n     va='bottom',\n     format_string='{:.1f}%'\n )\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","a2683a2c":"# Replace 55+ and 35-55 groups with 35+\ntrain['age_band'] = np.where( (train['age_band'] == '55<='),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\ntrain['age_band'] = np.where( (train['age_band'] == '35-55'),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\n# Do the same for the test set\ntest['age_band'] = np.where( (test['age_band'] == '55<='),\n                                           '35+',\n                                           test['age_band']\n                                    )\n\ntest['age_band'] = np.where( (test['age_band'] == '35-55'),\n                                           '35+',\n                                           test['age_band']\n                                    )","40c11e66":"# See the changes\n(\n    ggplot(train)\n    + aes(x='age_band', fill='age_band')\n    + geom_bar()\n    + geom_text(\n     aes(label='stat(prop)*100', group=1),\n     stat='count',\n     nudge_y=0.125,\n     va='bottom',\n     format_string='{:.1f}%'\n )\n)","c58aad04":"sns.pairplot(train)","c3df739b":"train = train.assign(fail_final=train['weighted_score'] < 40)\n\ntrain.head()","9876f29c":"(\n    ggplot(train)\n    + aes(x='code_module', fill='fail_final')\n    + geom_bar(position='fill')\n    + ggtitle(\"Count frequency of different modules by pass rate\")\n)","03eeae48":"(\n    ggplot(train)\n    + aes('code_module', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per module\")\n    + coord_flip()\n)","e974976b":"(\n    ggplot(train)\n    + aes(x='code_presentation', fill='fail_final')\n    + geom_bar(position='fill')\n)","34bb4dac":"(\n    ggplot(train)\n    + aes('code_presentation', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per presentation\")\n    + coord_flip()\n)","480294c8":"(\n    ggplot(train)\n    + aes(x='gender', fill='fail_final')\n    + geom_bar(position='fill')\n)","16b58ad6":"(\n    ggplot(train)\n    + aes('gender', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per sex\")\n    + coord_flip()\n)","e83dda92":"(\n    ggplot(train)\n    + aes(x='region', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","70a5f29d":"(\n    ggplot(train)\n    + aes('region', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per region\")\n    + coord_flip()\n)","b46fc194":"(\n    ggplot(train)\n    + aes(x='highest_education', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","b58e84e3":"(\n    ggplot(train)\n    + aes(x='imd_band', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","c6a74718":"(\n    ggplot(train)\n    + aes(x='age_band', fill='fail_final')\n    + geom_bar(position='fill')\n)","0c22bcb0":"(\n    ggplot(train)\n    + aes(x='disability', fill='fail_final')\n    + geom_bar(position='fill')\n)","4687ee0f":"# Separate features from target\n\n'''Training set'''\n# Drop target and helper columns\nX_train = train.drop(columns=['fail_final', 'weighted_score'])\n# Make an array with target\nY_train = train['weighted_score'].copy()\n\n'''Test set'''\n# Drop target column\nX_test = test.drop(columns=['weighted_score'])\n# Make an array with target\nY_test = test['weighted_score'].copy()\n\nX_train.head()","7af0a198":"'''Make a copy for the subsequent last evaluation'''\nX_train_eval = X_train.copy()\nY_train_eval = Y_train.copy()\nX_test_eval = X_test.copy()\nY_test_eval = Y_test.copy()","fd29bc34":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.compose import make_column_transformer\n\n# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'age_band', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band']),\n    (RobustScaler(), ['date_registration', 'module_presentation_length',\n                       'num_of_prev_attempts', 'studied_credits', 'total_click', 'late_rate',\n                       'fail_rate'])\n)\n\n# Apply column transformer to features\nX_encoded = column_transform.fit_transform(X_train)","0a39fe65":"# Have a look at what the scaled and encoded data looks like\npd.DataFrame(X_encoded).head()","996cb713":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Setting up the pipeline\nlm = LinearRegression()\n\nlm_pipeline = make_pipeline(column_transform, lm)\n\n# Fit the training data\nlm_pipeline.fit(X_train, Y_train)\n# Predict the training data\nlm_train_predictions = lm_pipeline.predict(X_train)","8ef5527a":"# Now let's evaluate the model\nimport sklearn.metrics as metrics\n\ndef regression_eval(X, y, predictions):\n    MSE = metrics.mean_squared_error(y, predictions)\n    RMSE = np.sqrt(MSE)\n    R2 = metrics.r2_score(y, predictions)\n    adj_R2 = 1 - ( (1-R2)*(len(y)-1)\/(len(y)-X.shape[1]-1) )\n\n    print(\"-----------------------\")\n    print('RMSE is {}'.format(RMSE))\n    print('Adjusted R2 score is {}\\n'.format(adj_R2))\n\n### For training set ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, lm_train_predictions)","8fd1cea0":"# Perform cross-validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lm, column_transform.fit_transform(X_train), Y_train, cv=10, scoring='neg_mean_squared_error')\nlm_rmse_scores = np.sqrt(-scores)\n\ndef display_scores(scores):\n    print('Scores\\t:', scores)\n    print('Mean\\t:', scores.mean())\n    print('SD\\t:', scores.std())\n    \ndisplay_scores(lm_rmse_scores)","de76431d":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha =0.0005, random_state=1)\n\nlasso_pipeline = make_pipeline(column_transform, lasso)\n\nlasso_pipeline.fit(X_train, Y_train)\n\nX_lasso_predictions = lasso_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, X_lasso_predictions)","1143964a":"# Perform cross-validation\nscores = cross_val_score(lasso, column_transform.fit_transform(X_train), Y_train, cv=10, scoring='neg_mean_squared_error')\n\nlasso_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(lasso_rmse_scores)","ba17cc8c":"from sklearn.svm import SVR\n\nSVR = SVR(kernel='rbf')\n\nSVR_pipeline = make_pipeline(column_transform, SVR)\n\nSVR_pipeline.fit(X_train, Y_train)\n\ntrain_SVR_predictions = SVR_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_SVR_predictions)","16df19f2":"scores = cross_val_score(SVR, column_transform.fit_transform(X_train), Y_train, cv=4, scoring='neg_mean_squared_error')\n\nSVR_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(SVR_rmse_scores)","2b25c324":"from sklearn.tree import DecisionTreeRegressor\n\nDtree = DecisionTreeRegressor(min_samples_leaf=15, min_samples_split=10, max_features=13)\n\nDtree_pipeline = make_pipeline(column_transform, Dtree)\n\nDtree_pipeline.fit(X_train, Y_train)\n\ntrain_Dtree_predictions = Dtree_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_Dtree_predictions)","47827ef7":"scores = cross_val_score(Dtree, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nDtree_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(Dtree_rmse_scores)","eb19ceae":"from sklearn.ensemble import GradientBoostingRegressor\n\nGBoost = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05,\n                                   max_depth=4, max_features=13,\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost_pipeline = make_pipeline(column_transform, GBoost)\n\nGBoost_pipeline.fit(X_train, Y_train)\n\ntrain_GBoost_predictions = GBoost_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_GBoost_predictions)","b92f5694":"scores = cross_val_score(GBoost, column_transform.fit_transform(X_train), Y_train, cv=4, scoring='neg_mean_squared_error')\n\nGBoost_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(GBoost_rmse_scores)","0c76cc30":"from sklearn.neighbors import KNeighborsRegressor\n\nKNReg = KNeighborsRegressor(n_neighbors=2)\n\nKNReg_pipeline = make_pipeline(column_transform, KNReg)\n\nKNReg_pipeline.fit(X_train, Y_train)\n\ntrain_KNReg_predictions = KNReg_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_KNReg_predictions)","1dece138":"scores = cross_val_score(KNReg, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nKNReg_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(KNReg_rmse_scores)","b05f00c4":"from sklearn.ensemble import RandomForestRegressor\n\nRForest = RandomForestRegressor(min_samples_leaf=15, min_samples_split=10,\n                                max_features=13, n_estimators=20)\n\nRForest_pipeline = make_pipeline(column_transform, RForest)\n\nRForest_pipeline.fit(X_train, Y_train)\n\ntrain_RForest_predictions = RForest_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_RForest_predictions)","20db88b1":"scores = cross_val_score(RForest, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nRForest_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(RForest_rmse_scores)","4c487032":"# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'age_band', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band']),\n    remainder='passthrough')\n    \nGBoost = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05,\n                                   max_depth=4, max_features=13,\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost_pipeline = make_pipeline(column_transform, GBoost)\n\n# Fit the training data\nGBoost_pipeline.fit(X_train_eval, Y_train_eval)\n\n# Transform the test set (don't fit)\nX_prepared_eval = column_transform.transform(X_test_eval)\n# Predict the test data\ntest_GBoost_predictions = GBoost.predict(X_prepared_eval)\n\nregression_eval(X_test_eval, Y_test_eval, test_GBoost_predictions)","3190ed4c":"train = train_class.copy()\ntest = test_class.copy()\n\ntrain.head()","88dd240c":"# Distinction as a Pass\ntrain['final_result'] = np.where( (train['final_result'] == 'Distinction'),\n                                           'Pass',\n                                           train['final_result']\n                                    )\n# Withdrawn as a Fail (to make the target binary)\ntrain['final_result'] = np.where( (train['final_result'] == 'Withdrawn'),\n                                           'Fail',\n                                           train['final_result']\n                                    )\n# Same for test set\ntest['final_result'] = np.where( (test['final_result'] == 'Distinction'),\n                                           'Pass',\n                                           test['final_result']\n                                    )\ntest['final_result'] = np.where( (test['final_result'] == 'Withdrawn'),\n                                           'Fail',\n                                           test['final_result']\n                                    )","56b162f0":"# Rename 'no formal quals' into 'lower than a level'\ntrain['highest_education'] = np.where( (train['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           train['highest_education']\n                                    )\n\n# Rename post-grads\ntrain['highest_education'] = np.where( (train['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           train['highest_education']\n                                    )\n\n\n# Do the same for the test set\ntest['highest_education'] = np.where( (test['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           test['highest_education']\n                                    )\n\ntest['highest_education'] = np.where( (test['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           test['highest_education']\n                                    )\n### Age bands ###\ntrain['age_band'] = np.where( (train['age_band'] == '55<='),\n                                           '35-55',\n                                           train['age_band']\n                                    )\n\ntrain['age_band'] = np.where( (train['age_band'] == '35-55'),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\n# Do the same for the test set\ntest['age_band'] = np.where( (test['age_band'] == '55<='),\n                                           '35-55',\n                                           test['age_band']\n                                    )\n\ntest['age_band'] = np.where( (test['age_band'] == '35-55'),\n                                           '35+',\n                                           test['age_band']\n                                    )","505d6e5e":"# Separate features from target\n\n'''Training set'''\n# Drop target column\nX_train = train.drop(columns=['final_result'])\n# Make an array with target\nY_train = train['final_result'].copy()\n\n'''Test set'''\n# Drop target column\nX_test = test.drop(columns=['final_result'])\n# Make an array with target\nY_test = test['final_result'].copy()\n\nX_train.head()","7c0d425b":"X_train.shape","7269a4c3":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import make_column_transformer\n\n# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band', 'age_band']),\n    remainder='passthrough'\n)\n\n# Apply column transformer to features\nX_encoded = column_transform.fit_transform(X_train)","b4eaa52a":"pd.DataFrame(X_encoded).head()","f267a0e5":"from sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(min_samples_leaf=15, min_samples_split=10, max_features=8)\n\nDtree_pipeline = make_pipeline(column_transform, Dtree)\n\n# Cross-validate\ndef display_accuracy_scores(pipeline, X, Y):\n    scores = cross_val_score(pipeline, X, Y, cv=5, scoring='accuracy')\n    print('Scores\\t:', scores)\n    print('Mean\\t:', scores.mean())\n    print('SD\\t:', scores.std())\n\n### Cross-validate ###\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(Dtree_pipeline, X_train, Y_train)","8ee5b8ca":"from sklearn.ensemble import RandomForestClassifier\n\nRForest = RandomForestClassifier(min_samples_leaf=15, min_samples_split=10,\n                                max_features=8, n_estimators=20)\n\nRForest_pipeline = make_pipeline(column_transform, RForest)\n\n### Cross-validate ###\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(RForest_pipeline, X_train, Y_train)","fb3681d3":"# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band', 'age_band']),\n    (RobustScaler(), ['date_registration', 'module_presentation_length',\n                       'num_of_prev_attempts', 'studied_credits', 'total_click'])\n)","92337a92":"from sklearn.svm import SVC\n\nSVClass = SVC(gamma='auto')\n\nSVClass_pipeline = make_pipeline(column_transform, SVClass)\n\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(SVClass_pipeline, X_train, Y_train)","ff4de999":"from sklearn.linear_model import SGDClassifier\n\nSDG = SGDClassifier(max_iter=1000, tol=1e-3)\n\nSDG_pipeline = make_pipeline(column_transform, SDG)\n    \n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(SDG_pipeline, X_train, Y_train)","fed00df7":"# Test set evaluation for SVC\nprint('Evaluation of the test set')\n\n# Fit the training data\nSVClass_pipeline.fit(X_train, Y_train)\n# Transform the test data\nX_test_prepared = column_transform.transform(X_test)\n# Predict the test data\nSVClass_predictions_test = SVClass.predict(X_test_prepared)\n\nprint('Accuracy score:', metrics.accuracy_score(Y_test, SVClass_predictions_test))","27cd532b":"### For Regression","7976186f":"### 3.2.3. Non-submissions","a33dc319":"5.3.2. Merged all result tables","9ce26655":"### disability","2431ddf0":"### 3.6.1. Missing values and duplicate rows","cfe2b200":"<a id=\"regression\"><\/a>\n# 11. Regression\n***","0d53de66":"Linear regression is most likely going to be a bad model as the data breaks several of the assumptions of this model.\n\n**The assumptions are as follows**:\n* Linear relationship between the target and features.\n    * The pair plots show this isn't the case.\n* Multivariate normality - all variables must be normal.\n    * The histograms of the numerical variables show that their distributions aren't normal.\n* Little to no multicollinearity - all variables must be independent from each other.\n    * The correlation matrix shows that this isn't so.\n* No auto-correlation - when the value of y(x+1) is independent from the value of y(x).\n* Homoscedasticity - residuals must be equal along the regression line.\n\nLinear regression can show us an example of a bad model, showing how other models can vastly improve the predictions.","4d88a061":"Very little difference in pass rates amongst students in different deprivation bands, but there does seem to be a trend - the more deprived the area is, the higher the fail rate. 0-10% IMD means the student lives in an area that that falls amongst top 0-10% most deprived small areas (the higher the percentage, the more deprived the area).","19c89821":"## Change education categories","84454307":"Duplication is entirely acceptable here as the system most likely records the clicks at different points on the same day, leading to duplicates.","55d6130f":"<a id=\"frame\"><\/a>\n# 4. Frame the problem\n***","77d366b0":"All modules show weight of 100 for exams apart from module CCC (for both presentations). Let's count the exams in each module presentation.","490adbe2":"This is a predictive model using an ensemble of weak predictive models (decision trees). We expect it will perform better than a simple Decision Tree.\n\nThe model is trained with huber loss, making it more robust to outliers.","a9fe0dd0":"Encoding for the last two models requires scaling.","03bfcfb6":"<a id=\"info\"><\/a>\n## 3.7. Student information","2eed4356":"### 3.4.3. Check if in Results table","2ae77700":"### 3.7.1. Missing values and duplicate rows","384052b3":"Two notebooks were very helpful in starting this analysis:\n* [Data Cleaning-Feature Generation-EDA-Segmentation by Anil](https:\/\/www.kaggle.com\/anlgrbz\/data-cleaning-feature-generation-eda-segmentation)\n* [Student Performance Prediction: Complete analysis by Victor R\u00e9gis](https:\/\/www.kaggle.com\/devassaxd\/student-performance-prediction-complete-analysis)","33a46372":"Support Vector Machine classifier model performed the best. Althrough its accuracy scores (0.78, SD = 0.002) are similar to the RF model (0.79, SD = 0.004), the SVC model performs with slightly less variance between the scores during cross-validation as shown by lower standard deviation.","2add31da":"<a id=\"ass\"><\/a>\n## 3.1. Assessments info","9193f1d3":"It's important to split the dataset before doing serious exploratory analysis as we do not want to peak at the testing data. Any pre-processing and further feature engineering will also be done to the test set with the same parameters as are set for the training set. We'' stratify by code module to make sure that each module is represented equally in both the test and the training sets.","e7083b8d":"## Numerical","1adaf0ae":"This file contains demographic information about the students together with their results. File\ncontains the following columns:\n1. **code_module** \u2013 an identification code for a module on which the student is registered.\n2. **code_presentation** - the identification code of the presentation during which the\nstudent is registered on the module.\n3. **id_student** \u2013 a unique identification number for the student.\n4. **gender** \u2013 the student\u2019s gender.\n5. **region** \u2013 identifies the geographic region, where the student lived while taking the\nmodule-presentation.\n6. **highest_education** \u2013 highest student education level on entry to the module\npresentation.\n7. **imd_band** \u2013 specifies the Index of Multiple Depravation band of the place where the\nstudent lived during the module-presentation.\n8. **age_band** \u2013 band of the student\u2019s age.\n9. **num_of_prev_attempts** \u2013 the number times the student has attempted this module.\n10. **studied_credits** \u2013 the total number of credits for the modules the student is currently\nstudying.\n11. **disability** \u2013 indicates whether the student has declared a disability.\n12. **final_result** \u2013 student\u2019s final result in the module-presentation.","e02dc1d6":"Let's see what our count plot for the age variable looks like after merging the categories.","a5c99b18":"The csv file contains information about the available materials in the VLE. Typically, these are\nhtml pages, pdf files, etc. Students have access to these materials online and their interactions\nwith the materials are recorded. The table comprises of the following columns:\n1. **id_site** \u2013 an identification number of the material.\n2. **code_module** \u2013 an identification code for module.\n3. **code_presentation** - the identification code of presentation.\n4. **activity_type** \u2013 the role associated with the module material.\n5. **week_from** \u2013 the week from which the material is planned to be used.\n6. **week_to** \u2013 week until which the material is planned to be used.","1eac3f76":"The results are much better than the linear models or SVR. Adjusted-R2 is 0.66% and cross-validation shows RMSE of 19. Interestingly, even though the model is capable of explaining significantly more variance, RMSE is not improved drastically.","cc0d56d8":"<a id=\"classification\"><\/a>\n# 12. Classification\n***","2b69c02e":"### Linear Regression","003d8c5d":"### Support Vector Machine","e43d9dbe":"Next steps to take would be to find which features are most important and which can be dropped. Hyperparameter tuning can be used to find the best set of parameters for the models. Various dimensionality reduction tools can be used to improve the performance of the models. Another point to make is that accuracy score isn't the best way to evaluate classification models, especially when the target is imbalanced. We have an imbalanced target for this classification problem, so dealing with this imbalance and using a different evaluation metric would be advantageous.\n\nIt's possible to engineer some more features too, for example, we know VLE interactions are important for student success, but maybe the type of the resource the student interacts with will be a better signal than total clicks for all resources?","8af6def8":"For the withdrawn students let's not put the registration date after unregistation date. Let's substract the median value from the unregistration date to fill these.","674cc262":"Let's look at the numbers for this: ","1fbf1c83":"Get toal clicks per student per module presentation.","86d72a66":"### Random Forest","223f71ca":"The model is an improvement on the linear models. Adjusted R2 is a bit higher (39%) and the standard error for RMSE scores in the cross-validated sets is lower (from SD = 0.31 for LASSO to SD = 0.09). It is still not a great predictor for the dataset.","31edcf6d":"<a id=\"results\"><\/a>\n## 3.2. Assessments results","51ff86a5":"### weighted_score","0639eee1":"<a id=\"courses\"><\/a>\n## 3.3. Courses info","da122614":"## 12.3 Best Classification Model - evaluation ","b6a0f48a":"Can exams be late submissions?","2c6b0dd3":"<a id=\"vle\"><\/a>\n## 3.5. VLE resources","ff35ebfa":"KNN-Regression examines the point close to the target point and then makes a prediction on which class these data points belong to.","750a1f7a":"The lengths of modules are not drastically different, but it might make an impact.","16091cea":"The studentVle.csv file contains information about each student\u2019s interactions with the\nmaterials in the VLE. This file contains the following columns:\n1. **code_module** \u2013 an identification code for a module.\n2. **code_presentation** - the identification code of the module presentation.\n3. **id_student** \u2013 a unique identification number for the student.\n4. **id_site** - an identification number for the VLE material.\n5. **date** \u2013 the date of student\u2019s interaction with the material measured as the number of\ndays since the start of the module-presentation.\n6. **sum_click** \u2013 the number of times a student interacts with the material in that day.","4f74ba8d":"### 3.3.2. Data types","5b0651af":"Again, there's not much difference between men and women passing or failing modules.","9e998b58":"*  [1. Introduction](#introduction)\n> * 1.1 Acknowledgements\n\n*  [2. Import libraries](#import-libraries)\n\n* [3. Data overview](#data-overview)\n> * [3.1. Assessments info](#ass)\n     * 3.1.1. Missing values and duplicate rows\n     * 3.1.2. Data types\n     * 3.1.3. Inconsistent weights\n         * 3.1.3.1. Fix inconsistent weights\n     * 3.1.4. Check if Assessments Info is the in Results table\n> * [3.2. Assessments results](#results)\n    * 3.2.1. Missing values and duplicate rows\n    * 3.2.2. Data types\n    * 3.2.3. Non-submissions\n> * [3.3. Courses info](#courses)\n    * 3.3.1. Missing values and duplicate rows\n    * 3.3.2. Data types\n> * [3.4. Student registration](#reg)\n    * 3.4.1. Missing values and duplicate rows\n    * 3.4.2. Data types\n    * 3.4.3. Check if in results table\n> * [3.5. VLE resources](#vle)\n    * 3.5.1. Missing values and duplicate rows\n    * 3.5.2. Data types\n> * [3.6. VLE Interactions](#vle-int)\n    * 3.6.1. Missing values and duplicate rows\n    * 3.6.2. Data types\n> * [3.7. Student information](#info)\n    * 3.7.1. Missing values and duplicate rows\n    * 3.7.2. Data types\n\n* [4. Frame the problem](#frame)\n\n* [5. Merge tables and Feature engineering](#merge)\n> * 5.1. VLE + VLE Interactions\n    * 5.1.1. Pre-prosessing\n> * 5.2. Registration info + Courses + Info\n> * 5.3. Assessment info + Assessment Results\n    * 5.3.1. Feature engineering\n        * 5.3.1.2. Late Submission\n        * 5.3.1.3. Fail rate\n    * 5.3.2. Merged all result tables\n> * 5.4. Merge all tables\n\n* [6. Split the dataset](#split)\n\n* [7. Final cleaning](#final-cleaning)\n\n* [8. Univariate analysis: numerical data](#num)\n\n* [9. Univariate analysis: categorical data](#cat)\n\n* [10. Bivariate analysis: final scores vs other variables](#scores-vs-variables)\n\n* [11. Regression](#regression)\n   > * 11.1 Model preparation\n   > * 11.2. Models\n   > * 11.3 Best Regression Model - evaluation\n\n* [12. Classification](#classification)\n   > * 12.1. Model preparation\n   > * 12.2. Models\n   > * 12.3. Best Classification Model - evaluation\n\n* [13. Discussion](#discussion)","5ab2fc2a":"This model uses a very different approach - building a what is essentially a flow chart based on probabilities and likelihoods. It is a simple algorithm with many models using it as a base algorithm (e.g. Random Forest).","47656fee":"### 3.4.1. Missing values and duplicate rows","47540364":"## 11.2. Models:","73a9fa90":"GradientBoost and Random Forest are our best models. Cross-validation for the GBoost model shows it to be the most accurate model, even though the error for the training set without cross-validation for the GBoost is higher than for the RF. KNN had the lowest RMSE score for the training set without cross-validation, but we can see how this model's performance degraded in the cross-validation, meaning this model is overfitting.","e52e6520":"Since CMA assignment is often weight 0, we will just assign 100 total weight to TMA assignment for simplicity.","3987c558":"Here we can see most that module presentations have total weight of 200, apart from module CCC which is 300 and module GGG which is 100. Let's have a closer look.","768eda5f":"This probbaly means these resources were not used by any students or that these resources did not record activity. And as such, we can merge these two tables with an inner merge as resources with no activity for any student provide zero information. Week_from and week_to columns can be dropped as they are over 82% empty. Drop date as it won't provide any extra information after grouping by module presentation per student.","0ecaa902":"All assignments missing from the Results (and consequently Merged) table are exams with 100% module weight. Are there any other 100% weighted assignments in the Assessment table apart from these?","5f15f605":"Here, again, we see an inconsistency as all withdrawn students should have their date_unregistration field filled in. According to the Student Information table 4648 students have withdrawn, however, according to the Registration table 4594 students have unregistered. This leaves 54 withdrawn students without an unregistration date.\n\nLet's check unregistration dates for 2 students with passes that have no recorded assessment results. If we find unregistration dates for these students we'll know it's a clerical error.","7d39182f":"## 5.4 Merge all tables","f753c4e0":"## Categorical","7324d717":"Immediately we can see weighted_score is most strongly positively correlated with total_click. The more students engaged with Blackboard, the better results they got. Theyre is also weak negative correlation with the number of the previous attempts. Late_rate and fail_rate also negatively correlated with weighted score, albeit weakly.\n\nThere's no correlation with module_presentation_length or date_registration, or studied_credits.","39848679":"How to fill out missing values here - fill out according to the most frequent band for that region:\n1. Find which regions have null imd_band values\n2. Find what band is the most frequent one for that region\n3. Replace null values with most frequent values for that region.","076f9c72":"Yes, these are the same students. Let's have a closer look.","9a1f0b0b":"### Last cleaning","f98719e4":"Date unregistration has been dropped as it should be the same metric as Withdrawal in the final results column of Student Information table. I'm assuming the prediction for who is likely to withdraw would only be useful if trying to predict future withdrawals, trying to predict if a student has withdrawn when we know they have withdrawn is useless (date_unregistration is essentially another target).\n\nFor the regression problem due to the contuinous nature of the target variable, we can't distinguish between fails and witdrawals, so all withdrawals will be treated as fails (score < 40%).","8280697c":"Calculate the rate of late submission for the assignments that the student did submit.","002ffbaf":"<a id=\"vle-int\"><\/a>\n## 3.6. VLE Interactions","742bf4d1":"This model was not in any way an improvement.","fc4986d1":"### IMD band","c0e344a3":"## 12.2. Models:","905b7a18":"Assessments IDs are denoted as integers. This is incorrect - IDs by definition are categorical. Below code corrects this.","d84664af":"LASSO (least absolute shrinkage and selection operator) regression is a modification of linear regression. In very simple terms, this algorithm can drop some features based on those features' coefficients (if they are too low).\n\nThe assumptions of this model as the same as for the linear model, except normality is not assumed.","b4c397cd":"There's a lot of skewed variables in this dataset. Something to keep in mind when using linear models as these assume normal distributions.","3f762d6d":"Cross-validation uses stratified k-fold cross-validation which is different from validation with randomised values. The training set is split into a smaller training set and an even smaller validation set. Each of these sets are then used for training and validation sequentially. Cross-validation also shows us the model performs abysmally, just as expected.","2fc796e8":"Same can be done for the age bands, merging 35-55 and 55+ groups into one. First, let's have a closer look at this variable.","926413e4":"### 3.1.2. Data types","4abf59ca":"### 3.2.2. Data types","0a0177d8":"### Encoding for trees","0c81e91d":"We can see the target variable has two peaks and is not normally distributed, but it doesn't have any outliers. We may wish to transform the target at some point to improve our models. This notebook shows very basic analysis though, so we will not be doing this, but it's something to keep in mind when using certain models (like the linear regression that assumes the distributions are normal).","2aa017de":"Yes, there are also 5847 students recorded in the Students Information table missing from the Assessment Results table. Are these the same students?","05e78581":"Module CCC has two exams, this can explain the hight assessments weight for this module. Now let's have a look at all the assignments that are not exams and see if everything is as it should be.","3f34527f":"Linear models are doing badly, as expected. We will next use non-linear models and see if our predictions improve.\n\nSupport Vector Regression seeks not to minimise the squared error as in the linear regression, but to minimise coefficients.","f0c113b7":"Note that there are null values.","22f3e1f5":"Students that have lower than A level previous education seem to fail more, however, the difference is so slight it may not be statistically significant.","496abc84":"Those students who have null values for total_click are the students who did not have any records in the VLE Interactions table, meaning they did not interact with VLE. Therefore, we can replace them with 0s.","9a4d31d6":"There are 96 entries in id_site in Materials table that are not in the VLE table.","0bc467a6":"## 5.2. Registration info + Courses + Info","745b3639":"## Distribution plots","5fba97c5":"RobustScaler is used to make the models more robust to putliers. More specifically, RobustScaler  scales the data according to the interquartile range.","b41e74ca":"#### 5.3.1.3. Fail rate","22ceff26":"The model looks great. Training error (RMSE = 17.0) is not much lower than validation error (mean RMSE = 18.6, SD = 0.2). Let's compare all non-linear models below and chose the best performing one.","46844af8":"Students who have nan values for their late submission rate have not submitted any of their assignments. We can replace the nan values with 1.00 (100% late rate).","601bbad3":"* **CCC module** only has results for 1 exam when the module should have 2 exams in total.\n* **DDD module** has results for the final exam (DDD module should have one exam in total).","c2e6e06f":"There's a few missing values we need to impute.","aeba83c3":"This is our final regression model with RMSE = 18.04 and adjusted R2 = 63%.","07a1ac11":"## 11.1 Model preparation","ba56de12":"### For Classification","534c5a29":"Do the same as above to calculate the fail rate.","d9b52a19":"We can think of this work as a regression *and* a classification problem designed to predict student academic failure and student withdrawal from module presentations.\n\nConsidering the incompleteness of data, the above is tricky.\n\nThe scores in the Assessment Results table are not complete - all modules but one are missing their final exam results for all students. This means that using the table as a whole with scores as a response variable for regression can lead to less robust results as information is not complete. In other words, it is possible for a student to pass their assignments and fail their final exam resulting in overall fail for the module.\n\nAnother point is that score is the same thing as the final result (in the Student Information table), so predicting the likelihood of someone failing knowing that they got less than 40% as their final mark is not a prediction at all. And, it would be quite interesting to see if it is possible to identify students at risk of withdrawing or failing without knowing anything about their actual academic performance.\n\nAll of these points considered, this is the plan:\n\n1. **Classification problem**: merge all tables apart from Assessment Results and use the final result column from Student Information table as target.\n2. **Regression problem**: merge all tables, deleting the final result column from Student Information and using scores as target.\n\nWe can then see which method gives the best predictions.","9686dfa1":"Here we see that module GGG doesn't have any weight to its assignments. Is it because there's no assingments for this module?","0e185581":"### imd_band","e541d704":"This file contains information about assessments in module-presentations. Usually, every\npresentation has a number of assessments followed by the final exam. CSV contains columns:\n1. **code_module** \u2013 identification code of the module, to which the assessment belongs.\n2. **code_presentation** - identification code of the presentation, to which the assessment\nbelongs.\n3. **id_assessment** \u2013 identification number of the assessment.\n4. **assessment_type** \u2013 type of assessment. Three types of assessments exist: Tutor\nMarked Assessment (TMA), Computer Marked Assessment (CMA) and Final Exam\n(Exam).\n5. **date** \u2013 information about the final submission date of the assessment calculated as\nthe number of days since the start of the module-presentation. The starting date of\nthe presentation has number 0 (zero).\n6. **weight** - weight of the assessment in %. Typically, Exams are treated separately and\nhave the weight 100%; the sum of all other assessments is 100%.\nIf the information about the final exam date is missing, it is at the end of the last presentation\nweek.","b0261a06":"Code for the below cell is adapted from a [Kaggle notebook](https:\/\/www.kaggle.com\/teertha\/us-health-insurance-eda) by Anirban Datta.","7b43e062":"#### 3.1.3.1. Fix inconsistent weights","7779a6d5":"Yes, exams can be submitted late.","ac3087a1":"Project brief states that typically, exams have a weight of 100 and the sum of all other assessments is 100. This would man that a module with one exam only would have a weight of 100 and a module with one exam and some assessments would have a weight of 200. Let\u2019s check if this so in the table provided.","40e9aa28":"There are 5847 students missing from the Results table. Are there any students from the Student Information table missing from the Results table?","78118888":"It seems that some modules have higher fail rates than others. For example, for module CCC the pass rate is just a little over 50%. The boxplot also reveals some outliers.","07032605":"### total_click","261ab2d5":"### fail_rate","0f613510":"We can see that the number of assessments in the results table does not match the number of assessments in the Assessments table.","b4c84307":"### For Regression","e93f1a4e":"This chapter presents a quick overview of the data before the training\/test split.","d670226c":"### Date registration","79653839":"## Cleaning test sets","f3773852":"Next, we approach the task as a classification problem. Same steps as with the regression apply - model is prepared with last cleaning steps, then categorical values are encoded, models are fitted and then evaluated.","cd4e902b":"### highest_education","dac2867f":"This is my first attempt to clean data, engineer features, and train some machine learning models in Python. Any feedback will be appreciated.","b896cf15":"Module presentation (semester) seems to have no effect on the pass\/fail rate.","0a2b20f8":"<a id=\"num\"><\/a>\n# 8. Univariate analysis: numerical data\n***","b1fc8d1b":"To quickly summarise, the best model for the regression task was Gradient Boost (RMSE = 18.4, SD = 0.2 with 4-fold cross-validation) which gave us RMSE = 18.1 and adjusted R2 = 0.63 when evaluated on the test set). This is without any fine-tuning of the hyperparameters. The best model for the classification task was a Support Vector Machine classifier (0.78% accuracy score on the test set). Again, this is without any hyperparameter tuning.","97c69e0f":"### gender","489ef1e3":"### 3.7.2. Data types","03587ba4":"Are there any other CMA and TMA assignments with a weight of 0?","d63a4d7f":"As expected, people with disabilities do worse academically. This is to be expected as disabled students would face more challenges due to ill health.","c6f21363":"## Target variable - weighted score","0ceef549":"This can show us that there is little colinearity among the variables. Let's have a closer look at linear correlations between features and the target.","17d45ade":"Now let's merge the least frequent categories.","2330db81":"**How it will be calculated:**\n\nTo calculate the total weight of all modules, we need to remember that most final exams are missing from the Results table.\n\n1. Multiply the weight of the assignment with its score.\n2. Aggregate the dataframe per weight\\*score per module per module presentation with the sum function.\n3. Calculate total recorded weight of module (recorded total is key here as most modules are missing their final exam).\n4. Now calculate weighted scores - divide summed weight\\*score by total recorded weight of module.","552ff36e":"File contains the list of all available modules and their details. The columns are:\n1. **code_module** \u2013 code name of the module, which serves as the identifier.\n2. **code_presentation** \u2013 code name of the presentation. It consists of the year and \u201cB\u201d for\nthe presentation starting in February and \u201cJ\u201d for the presentation starting in October.\n3. **length** - length of the module-presentation in days.","8d4d6146":"### GradientBoost","2b33a93a":"What is the usual weight of assignments?","4006acf1":"### SGD","cd889915":"<a id=\"reg\"><\/a>\n## 3.4. Student registration","c6bfa3a1":"The above evaluation of the model is using our uses a randomised training and test set split, then calculates RMSE and adjusted-R2. Adjusted-R2 shows the model explains 35% of the total variance in the sample. RMSE of 23.8 shows us the error - predictions are off by 23.8 points. This is a large error considering the fail mark is only 40%.","4a957af2":"The students who have null values for weighted_score have not submitted any assignments. We can replace the nan values with 0s.","1a997a33":"### 3.5.2. Data types","10b102a0":"<a id=\"import-libraries\"><\/a>\n# 2. Import libraries\n***","e2afb18b":"The accuracy score is almost the same for both the training and the test sets.","ae96eda1":"### region","8e53945a":"### Random Forest","a504ce9e":"### 5.3.1. Feature engineering","9ea1b2fa":"### late_rate","0338c97e":"### For Classification","02eb47d1":"This file contains the results of students\u2019 assessments. If the student does not submit the\nassessment, no result is recorded. The final exam submissions is missing, if the result of the\nassessments is not stored in the system. This file contains the following columns:\n1. **id_assessment** \u2013 the identification number of the assessment.\n2. **id_student** \u2013 a unique identification number for the student.\n3. **date_submitted** \u2013 the date of student submission, measured as the number of days\nsince the start of the module presentation.\n4. **is_banked** \u2013 a status flag indicating that the assessment result has been transferred\nfrom a previous presentation.\n5. **score** \u2013 the student\u2019s score in this assessment. The range is from 0 to 100. The score\nlower than 40 is interpreted as Fail. The marks are in the range from 0 to 100.","a62b535f":"One thing to note is that is_banked column is dropped along with date_submitted and assessment_type. We can add these as features to see if it impoves our model after we build a basic model.","ab8b1865":"The dataset for this machine learning project has been provided by the learning analytics research group at the Knowledge Media institute, The Open University. The dataset is publicly available and consists of tables with information on student demographics, modules undertaken, time of year the modules start (module presentations), and information on student academic success in terms of grades for assignments and exams, as well as students\u2019 interactions with the university\u2019s Virtual Learning Environment (VLE).\n\nThe task at hand is to predict which students are to fail or withdraw and which are to pass their modules.\n\nThe dataset is rather messy, with many values missing and some inconsistencies between tables. All cleaning steps are detailed in the first part of the notebook. Various inconsistencies are reported and dealt with or suggestions are made as to how to deal with them in future work.\n\nSome feature engineering is done with suggestions for more features that could be of help in this project.\n\nFinally, several classification and regression models are used to predict student academic success.","a44f4260":"### 3.2.1. Missing values and duplicate rows","cabaddc1":"Date_registration may turn out to be a predictor of future fail or withdrawal as early registration may predict keen interest and future success, or in an opposite way, early registration means students become disinterested in the module by the time it starts and are likely to withdraw.","a9be6aae":"### 3.1.1. Missing values and duplicate rows","a8ae9b69":"Assessment information table will provide just that - information on weights for assessmtn scores.","20f17704":"<a id=\"discussion\"><\/a>\n# 13. Discussion\n***","e4d27c9e":"Let's make a helper column to indicate if the student failed or not so we can compare categorical variables for failed and passing students.","46f75349":"Check if all student IDs recorded in the Registration tables are recorded in the Results table.","cf6fd84b":"Let's chose the best model. To do this we need to have a look at how the model performed on the training set and in cross-validation.\n\n\n**On the training set:**\n\n|Models| RMSE score|\n| ----------- | ----------- |\n|**SVR**|23.1|\n|**DT**|17.3|\n|**GB**|17.7|\n|**KNN**|16.9|\n|**RF**|17.0|\n\n**Cross-validation:**\n\n|Models| Mean RMSE|SD|\n| ----------- | ----------- |-------|\n|**SVR**|23.4|0.1|\n|**DT**|20.4|0.5|\n|**GB**|18.4|0.2|\n|**KNN**|29.1|0.3|\n|**RF**|18.6|0.2|\n","9823ad64":"* Very few students with no formal education (1%)\n* Very few students with post-grad qualifications.\n\nThese two categories should be merged with 'Lower Than A Level' and 'HE Qualification', respectivelly, as with so little data these two categories are not likely to bring much insight.","fc50c26a":"### 3.6.2. Data types","da2f6cd8":"<a id=\"introduction\"><\/a>\n# 1. Introduction\n***","334435e2":"### LASSO Regression","413bc559":"### 5.1.1. Pre-prosessing","7a5a9f0c":"<a id=\"final-cleaning\"><\/a>\n# 7. Final cleaning\n***","080b6123":"## 12.1 Model preparation","e2756c1d":"The brief stated that assignments not recorded in the Results table and not recorded due to the student not submitting them. However, here we have 2 students with no submissions recorded who have passed their modules. This may be due to two reasons:\n* The recorded pass is a clerical error.\n* The brief is wrong.","ac0a7c40":"This may be a small improvement. Further tests needed to determine if the results of this model are significantly different than a simple Decision Tree. RMSE is 18.4 based on cross-validation, which is lower than we've seen before. Adjusdted R2 score is one point lower (65%).","e2b8126c":"Course length may well be a good predictor of withdrawal simply due to the fact that longer courses will have more time for students to decide to drop out.","9980454f":" The dataframes created previously:\n \n 1. VLE + VLE materials = total_click_per_student\n 2. Registration Info + Courses + Student Info = regCoursesInfo\n 3. Assessments + Results = assessments","11c06744":"The results are vague. There doesn't seem to be any strong relationships between any variables and the target except, perhaps, the number of total clicks.","caf4a33e":"<a id=\"merge\"><\/a>\n# 5. Merge tables and engineer features\n***","34490697":"<a id=\"cat\"><\/a>\n# 9. Univariate analysis: categorical data\n***","25670718":"Older people seem to do bettwer academically, however, the difference is fairly small.","8b3b108e":"### code_module","f78532d0":"### 3.1.3. Inconsistent weights","15ea73f0":"### 3.5.1. Missing values and duplicate rows","4e39c53e":"Encoding for tree models does not require scaling.    ","81dd3e99":"RMSE is the lowest yet at 16.9. Let's see how the model fairs with cross-validation.","29d44635":"Students who have nan values for their fail rate have not submitted any assingments. Their fail rate is therefore 100% (1.0).","3ddf9887":"## Change age categories","2989a360":"<a id=\"data-overview\"><\/a>\n# 3. Data overview\n***","06079e3e":"## 5.3. Assessment info + Assessment Results","2e446532":"### Decision Tree","c2e9c7c2":"## Drop columns","59462276":"## 5.1. VLE + VLE Interactions","4b04ae25":"### code_presentation","d2daa8f9":"There is very little difference in pass rates between regions.","f61aaf8e":"### Decision Tree","ec382b29":"**How will be calculated**\n\n1. Calculate the difference between the deadline and the actual submission date.\n2. Make a new column - if the difference between dates is more that ), the submission was late.\n3. Aggregate by student ID, module, and module presenation.","905674a4":"<a id=\"split\"><\/a>\n# 6. Split the dataset\n***","abb797d1":"Due to the above we can't say that all final exams are missing from the results table, just some exams.","c6367bf6":"<a id=\"scores-vs-variables\"><\/a>\n# 10. Bivariate analysis: final scores vs other variables\n***","84d39b66":"#### 5.3.1.2. Late submission","db48b2c3":"Null scores will be assigned not a fail. It is ok as most submissions are not fails, so it would make sense to automatically assign them as passes.","0381425b":"Mean RMSE is at 29.1 which is the worst performance among all non-linear models. Validation error is expected to be higher than training error, but this differenc is quite stark. This means the model is overfitting badly.","b2960e1a":"We know that if the student does not submit the assessment, no result is recorded. Therefore, all null scores can be interpreted as non-submissions. This means we can fill them out with zeros.\n\nIt is, however, a little strange that there are recorded submission days for assessments with null scores. One would expect a null value for the submission date for an assessment that has not been submitted. Ideally, this should be clarified with data providers.","f6058b19":"### age_band","a1077a50":"### Support Vector Regression","ae52cbe2":"## Correlation matrix","d2869718":"All module presentations from Registration table are present in the Courses table.","e3acb6ba":"## Missing values","f06726ef":"### 3.1.4. Check if Assessments Info is the in Results table","14f37935":"### 3.3.1. Missing values and duplicate rows","51bf85cb":"#### 5.3.1.1. Weighted score","302b016c":"There are no unregistration dates for these 2 students, however, we know there are 54 withdrawn students that have no unregistration dates, so it's unclear how much we can trust this data.","c82073a0":"### K Nearest Neighbours Regression","209dfc7d":"## 1.1. Acknowledgements","949c6e7f":"This file contains information about the time when the student registered for the module\npresentation. For students who unregistered the date of unregistration is also recorded. File\ncontains five columns:\n1. **code_module** \u2013 an identification code for a module.\n2. **code_presentation** - the identification code of the presentation.\n3. **id_student** \u2013 a unique identification number for the student.\n4. **date_registration** \u2013 the date of student\u2019s registration on the module presentation, this\nis the number of days measured relative to the start of the module-presentation (e.g.\nthe negative value -30 means that the student registered to module presentation 30\ndays before it started).\n5. **date_unregistration** \u2013 date of student un-registration from the module presentation,\nthis is the number of days measured relative to the start of the module-presentation.\nStudents, who completed the course have this field empty. Students who unregistered\nhave Withdrawal as the value of the final_result column in the studentInfo.csv file.","0eda3041":"### 3.4.2. Data types","6c2dd75d":"## 11.3 Best Regression Model - evaluation"}}