{"cell_type":{"b90ae563":"code","e8072c07":"code","6fbd702b":"code","b2fceb47":"code","bc566764":"code","9f0a491b":"code","eeb91e0f":"code","e5b67ff9":"code","9ef34b3e":"code","9e5801e4":"code","f7dc7d1b":"code","98f0ad59":"code","6b86b635":"code","20cb6f62":"code","e9ebe38e":"code","c7dd365b":"code","76e67933":"code","951c07d9":"code","a2882ffa":"code","067b26f8":"code","1d67fbfb":"code","9e44d10d":"code","6097c449":"code","78ae551b":"code","584b30b3":"code","f4fa2f84":"markdown","da0258d5":"markdown","6d7b19b8":"markdown","5bbef447":"markdown","9b41dda9":"markdown","05b99fb6":"markdown","a8c76791":"markdown","892cacfe":"markdown","3b1f8e85":"markdown","b4db758a":"markdown","caf31c32":"markdown","c9306e4f":"markdown","b5a1fbd3":"markdown","34f3f761":"markdown","25aa5431":"markdown","797eb176":"markdown","dd0830c9":"markdown"},"source":{"b90ae563":"import bz2\nfrom collections import Counter\nimport re\nimport nltk\nimport numpy as np\nnltk.download('punkt')","e8072c07":"train_file = bz2.BZ2File('..\/input\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/test.ft.txt.bz2')","6fbd702b":"train_file = train_file.readlines()\ntest_file = test_file.readlines()","b2fceb47":"print(\"Number of training reivews: \" + str(len(train_file)))\nprint(\"Number of test reviews: \" + str(len(test_file)))","bc566764":"num_train = 800000 #We're training on the first 800,000 million reviews in the dataset\nnum_test = 200000 #Using 200,000 reviews from test set\n\ntrain_file = [x.decode('utf-8') for x in train_file[:num_train]]\ntest_file = [x.decode('utf-8') for x in test_file[:num_test]]","9f0a491b":"print(train_file[0])","eeb91e0f":"# Extracting labels from sentences\n\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n\n# Some simple cleaning of data\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n\n# Modify URLs to <url>\n\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","e5b67ff9":"del train_file, test_file","9ef34b3e":"words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\nfor i, sentence in enumerate(train_sentences):\n    #The sentences will be stored as a list of words\/tokens\n    train_sentences[i] = []\n    for word in nltk.word_tokenize(sentence): #Tokenizing the words\n        words.update([word.lower()]) #Converting all the words to lower case\n        train_sentences[i].append(word)\n    if i%20000 == 0:\n        print(str((i*100)\/num_train) + \"% done\")\nprint(\"100% done\")","9e5801e4":"# Removing the words that only appear once\nwords = {k:v for k,v in words.items() if v>1}\n# Sorting the words according to the number of appearances, with the most common word being first\nwords = sorted(words, key=words.get, reverse=True)\n# Adding padding and unknown to our vocabulary so that they will be assigned an index\nwords = ['_PAD','_UNK'] + words\n# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","f7dc7d1b":"for i, sentence in enumerate(train_sentences):\n    # Looking up the mapping dictionary and assigning the index to the respective words\n    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n\nfor i, sentence in enumerate(test_sentences):\n    # For test sentences, we have to tokenize the sentences as well\n    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]","98f0ad59":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features","6b86b635":"seq_len = 200 #The length that the sentences will be padded\/shortened to\n\ntrain_sentences = pad_input(train_sentences, seq_len)\ntest_sentences = pad_input(test_sentences, seq_len)","20cb6f62":"# Converting our labels into numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)","e9ebe38e":"test_sentences[0]","c7dd365b":"split_frac = 0.5\nsplit_id = int(split_frac * len(test_sentences))\nval_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\nval_labels, test_labels = test_labels[:split_id], test_labels[split_id:]","76e67933":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\nval_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\ntest_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n\nbatch_size = 400\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","951c07d9":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","a2882ffa":"dataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint(sample_x.shape, sample_y.shape)","067b26f8":"import torch.nn as nn\n\nclass SentimentNet(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        super(SentimentNet, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        return hidden","1d67fbfb":"vocab_size = len(word2idx) + 1\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 512\nn_layers = 2\n\nmodel = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel.to(device)\nprint(model)","9e44d10d":"lr=0.005\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","6097c449":"epochs = 2\ncounter = 0\nprint_every = 1000\nclip = 5\nvalid_loss_min = np.Inf\n\nmodel.train()\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        counter += 1\n        h = tuple([e.data for e in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if counter%print_every == 0:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            for inp, lab in val_loader:\n                val_h = tuple([each.data for each in val_h])\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = criterion(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                \n            model.train()\n            print(\"Epoch: {}\/{}...\".format(i+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), '..\/working\/state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","78ae551b":"model.load_state_dict(torch.load('..\/working\/state_dict.pt'))","584b30b3":"test_losses = []\nnum_correct = 0\nh = model.init_hidden(batch_size)\n\nmodel.eval()\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    inputs, labels = inputs.to(device), labels.to(device)\n    output, h = model(inputs, h)\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(output.squeeze()) #rounds the output to 0\/1\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n        \nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}%\".format(test_acc*100))","f4fa2f84":"With the mappings, we'll convert the words in the sentences to their corresponding indexes.","da0258d5":"We can also check if we have any GPUs to speed up our training time by many folds. If you\u2019re using FloydHub with GPU to run this code, the training time will be significantly reduced.","6d7b19b8":"This dataset contains a total of 4 million reviews - 3.6 million training and 0.4 million testing. We won't be using the entire dataset to save time. However, if you have the computing power and capacity, go ahead and train the model on a larger portion of data.","5bbef447":"At this point, we will be defining the architecture of the model. At this stage, we can create Neural Networks that have deep layers or and large number of LSTM layers stacked on top of each other. However, a simple model such as the one below works quite well and requires much less training time. We will be training our own word embeddings in the first layer before the sentences are fed into the LSTM layer.\n\nThe final layer is a fully connected layer with a sigmoid function to classify whether the review is of positive\/negative sentiment.","9b41dda9":"Next, this is the point where we\u2019ll start working with the PyTorch library. We\u2019ll first define the datasets from the sentences and labels, followed by loading them into a data loader. We set the batch size to 256. This can be tweaked according to your needs.","05b99fb6":"A padded sentence will look something like this, where 0 represents the padding:","a8c76791":"With this, we can instantiate our model after defining the arguments. The output dimension will only be 1 as it only needs to output 1 or 0. The learning rate, loss function and optimizer are defined as well.","892cacfe":"We managed to achieve an accuracy of ~93.8% with this simple LSTM model! This shows the effectiveness of LSTM in handling such sequential tasks.\n\nThis result was achieved with just a few simple layers and without any hyperparameter tuning. There are so many other improvements that can be made to increase the model\u2019s effectiveness, and you are free to attempt to beat this accuracy by implementing these improvements!\n\nSome improvement suggestions are as follow:\n\n- Running a hyperparameter search to optimize your configurations.\n- Increasing the model complexity\n    - E.g. Adding more layers\/ using bidirectional LSTMs\n1. - Using pre-trained word embeddings such as GloVe embeddings","3b1f8e85":"Our dataset is already split into training and testing data. However, we still need a set of data for validation during training. Therefore, we will split our test data by half into a validation set and a testing set.","b4db758a":"To remove typos and words that likely don't exist, we'll remove all words from the vocab that only appear once throughout. To account for **unknown** words and **padding**, we'll have to add them to our vocabulary as well. Each word in the vocabulary will then be assigned an integer index and thereafter mapped to this integer.","caf31c32":"For our data pre-processing steps, we'll be using regex, numpy and the NLTK (Natural Language Toolkit) library for some simple NLP helper functions. As the data is compressed in the bz2 format, we'll use the Python bz2 module to read the data.","c9306e4f":"Next, we'll have to extract out the labels from the sentences. The data is the format \\__label__1\/2 sentence, therefore we can easily split it accordingly. Positive sentiment labels are stored as 1 and negative are stored as 0.\n\nWe will also change all urls to a standard \"<url\\>\" as the exact url is irrelevant to the sentiment in most cases.","b5a1fbd3":"After quickly cleaning the data, we will do tokenization of the sentences, which is a standard NLP task. Tokenization is the task of splitting a sentence into the individual tokens, which can be words or punctuation, etc. There are many NLP libraries that are able to do this, such as *spaCy* or *Scikit-learn*, but we will be using *NLTK* here as it has one of the faster tokenizers.\n\nThe words will then be stored in a dictionary mapping the word to its number of appearances. These words will become our **vocabulary**.","34f3f761":"Finally, we can start training the model. For every 1000 steps, we\u2019ll be checking the output of our model against the validation dataset and saving the model if it performed better than the previous time. The state_dict is the model\u2019s weights in PyTorch and can be loaded into a model with the same architecture at a separate time or script altogether.","25aa5431":"This kernel is the project implementation code from my [FloydHub blogpost on LSTMs](https:\/\/blog.floydhub.com\/long-short-term-memory-from-zero-to-hero-with-pytorch\/). The link to the GitHub repository can be found [here](https:\/\/github.com\/gabrielloye\/LSTM_Sentiment-Analysis). I'll be creating a simple LSTM classification model for Sentiment Analysis on the Amazon reviews dataset.","797eb176":"After we\u2019re done training, it's time to test our model on a dataset it has never seen before - our test dataset. We'll first load the model weights from the point where the validation loss is the lowest. We can calculate the accuracy of the model to see how accurate our model\u2019s predictions are.","dd0830c9":"In the last pre-processing step, we'll be padding the sentences with 0s and shortening the lengthy sentences so that the data can be trained in batches to speed things up."}}