{"cell_type":{"aeb8ffd4":"code","6567f8f1":"code","3f064ca9":"code","f4894dd0":"code","dbd3346a":"code","57624d51":"code","b8ce62a7":"code","35c2066f":"code","cc82afc8":"code","523f272a":"code","f4f8c6a6":"code","a22a4eb9":"code","4f662903":"code","3da8496a":"code","d7635682":"code","b3300d6b":"code","3eec67d8":"code","d8193111":"code","d0fe2ddf":"code","7bb56553":"code","29469d08":"code","9c145396":"code","054eca55":"code","bcb98ceb":"code","31b0f5c2":"code","907826d4":"code","379ad5a0":"code","cde3ddb2":"code","5cbb0807":"code","81deebe1":"code","7591d1c7":"code","25c0dec3":"markdown","fb3e4c5e":"markdown","5f6a09e2":"markdown","f9db7cb3":"markdown","9bae0fc1":"markdown","cef04e4b":"markdown","fb58b3d2":"markdown"},"source":{"aeb8ffd4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6567f8f1":"train = pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/sample_submission.csv')","3f064ca9":"print(\"Train data dimensions: \", train.shape)\nprint(\"Test data dimensions: \", test.shape)","f4894dd0":"print(train.columns.tolist())","dbd3346a":"train.head()","57624d51":"print(\"Number of missing values\",train.isnull().sum().sum())","b8ce62a7":"train.describe()","35c2066f":"cont_features = train.iloc[:,-15:-1]\ncont_features.head()","cc82afc8":"print(cont_features.skew())","523f272a":"print(cont_features.kurtosis())","f4f8c6a6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(13,9))\nsns.boxplot(train[\"loss\"])","a22a4eb9":"plt.figure(figsize=(13,9))\nsns.distplot(train[\"loss\"])","4f662903":"plt.figure(figsize=(13,9))\nsns.boxplot(np.log1p(train[\"loss\"]))","3da8496a":"plt.figure(figsize=(13,9))\nsns.distplot(np.log1p(train[\"loss\"]))","d7635682":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nfor i in train:\n    if 'cat' in i:\n        train[i] = enc.fit_transform(train[i])","b3300d6b":"train.head()","3eec67d8":"X = train.drop([\"id\",\"loss\"],axis=1)\nY = train[\"loss\"]","d8193111":"from sklearn.model_selection import train_test_split, cross_val_score\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=10)","d0fe2ddf":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\nX_test = sc.transform(X_test)","7bb56553":"from xgboost import XGBRegressor\nmodel = XGBRegressor(n_estimators=1000)\nmodel.fit(X,Y)","29469d08":"Y_predict = model.predict(X_test)","9c145396":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nprint(\"r2 score is \", r2_score(Y_predict,Y_test))\nprint(\"MAE is \",mean_absolute_error(Y_predict,Y_test))\nprint(\"MSE score is \", mean_squared_error(Y_predict,Y_test))","054eca55":"test_id = test['id']\ntest.head()","bcb98ceb":"print(\"Number of missing values\",train.isnull().sum().sum())","31b0f5c2":"enc = LabelEncoder()\nfor i in test:\n    if 'cat' in i:\n        test[i] = enc.fit_transform(test[i])","907826d4":"test.head()","379ad5a0":"test = test.drop([\"id\"],axis= 1)\ntest = sc.transform(test)","cde3ddb2":"prediction = model.predict(test)","5cbb0807":"submission = pd.DataFrame(test_id)\nsubmission['prediction'] = prediction","81deebe1":"submission.head()","7591d1c7":"submission.to_csv('Submission_ACS.csv', index = False)","25c0dec3":"#### Loss is highly skewed to the right because there are many outliers in the data as we can see from box plot. So we can use log function to see if we can get a normal distribution.","fb3e4c5e":"## Evaluation and prediction\n ### XGBoost Regressor","5f6a09e2":"## TEST DATA","f9db7cb3":"### Convert categorical string values to numeric values","9bae0fc1":"### Checking skewness and kurtosis to see if normally distributed continuous features\n#### The acceptable values are between -1.5 to +1.5","cef04e4b":"### Analysis of loss feature","fb58b3d2":"#### Now we have normal distribution by applying logarithm on loss function and we can train model using the same target feature without removing outliers."}}