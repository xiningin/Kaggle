{"cell_type":{"62c07d55":"code","07fc55d4":"code","8c939d40":"code","985e0ffc":"code","652ee6d9":"code","856867a1":"code","f656c37f":"code","f9949580":"code","53d6db40":"code","b145c483":"code","fc7a03e7":"code","37eb48a3":"code","f8144b03":"code","e3cf6d88":"code","ef85dace":"code","1c8da7c8":"code","9f3d6d91":"code","5c6f9a06":"code","48df4bb1":"code","54765e20":"code","89bdccad":"code","b888411f":"code","19ed6f0a":"code","33e85567":"code","3bf1fb31":"code","7729953d":"code","54e33be0":"code","279f899e":"code","4cd723e7":"code","b1159c8e":"code","fca5b512":"code","fdfc2c32":"code","90e61d84":"code","d9fd5d01":"code","c075867d":"code","63de4604":"code","0ecdd921":"code","8da82fe3":"code","30fc7179":"code","bee0e759":"code","138baacc":"code","f9a7b725":"code","7a3c8760":"code","611f08e2":"code","a365b036":"code","31785e65":"code","79dfc69e":"code","61a85fbc":"code","cb317c29":"code","2592526d":"code","2b8629d5":"code","d3834aa9":"code","3d3cc342":"code","f9dae136":"code","9410b7df":"code","a3c81c99":"code","28b5c262":"code","68ad76a3":"code","bd5f0174":"code","71cf4976":"code","556278f4":"code","685c0ba7":"code","5ff5d4cc":"code","fca3b4a9":"code","4d04286c":"code","a85d7f25":"code","996f14a6":"code","a539b239":"code","8e2df1b9":"code","e8b3a222":"code","5abe4c84":"code","29273418":"code","b8940c75":"code","3384a1c2":"code","dfeee511":"markdown","d1701f90":"markdown","b17a75bb":"markdown","95cb4801":"markdown","806880ad":"markdown","16def7d5":"markdown","23cf9f48":"markdown","81a5a2fe":"markdown","e05e55c4":"markdown","9e90b23b":"markdown","96afee82":"markdown"},"source":{"62c07d55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07fc55d4":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sbn\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nimport pandas_profiling","8c939d40":"data  = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndata.head()","985e0ffc":"data.profile_report()","652ee6d9":"data.isnull().sum()","856867a1":"data.corr()","f656c37f":"data.describe()","f9949580":"s = data.groupby(\"Sex\")","53d6db40":"s = s.size().sort_values()","b145c483":"plt.pie(s, labels = [\"F\", \"M\"],autopct='%1.1f%%',radius = 2, textprops = {\"fontsize\" : 14})","fc7a03e7":"c = data.groupby(\"ChestPainType\")","37eb48a3":"c = c.size().sort_values()","f8144b03":"plt.pie(c, labels = [\"TA\", \"ATA\", \"NAP\", \"ASY\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","e3cf6d88":"r = data.groupby(\"RestingECG\")","ef85dace":"r = r.size().sort_values()","1c8da7c8":"r","9f3d6d91":"plt.pie(r, labels = [\"ST\", \"LVH\", \"Normal\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","5c6f9a06":"ea = data.groupby(\"ExerciseAngina\")","48df4bb1":"ea = ea.size().sort_values()","54765e20":"ea","89bdccad":"plt.pie(ea, labels = [\"Y\", \"N\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","b888411f":"plt.figure(figsize = (7,5))\nsbn.distplot(data[\"HeartDisease\"])","19ed6f0a":"sbn.countplot(data[\"HeartDisease\"])","33e85567":"sbn.scatterplot(x = \"HeartDisease\", y=\"Cholesterol\", data = data)","3bf1fb31":"st = data.groupby(\"ST_Slope\")","7729953d":"st = st.size().sort_values()","54e33be0":"st","279f899e":"plt.pie(st, labels = [\"Down\", \"Up\", \"Flat\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","4cd723e7":"chest = data.iloc[:,2:3].values","b1159c8e":"le = preprocessing.LabelEncoder()\nchest[:,0]  = le.fit_transform(data.iloc[:,2:3])","fca5b512":"ohe = preprocessing.OneHotEncoder()\nchest = ohe.fit_transform(chest).toarray()","fdfc2c32":"chestt = pd.DataFrame(data = chest, index = range(918), columns = [\"cpt_asy\",\"cpt_ata\", \"cpt_nap\", \"cpt_ta\" ])","90e61d84":"chestt.head(5)","d9fd5d01":"sex = data.iloc[:,1:2].values\nle1 = preprocessing.LabelEncoder()\nsex[:,0] = le1.fit_transform(data.iloc[:,1:2])","c075867d":"ohe1 = preprocessing.OneHotEncoder()\nsex = ohe1.fit_transform(sex).toarray()","63de4604":"sex = pd.DataFrame(data = sex, index = range(918), columns = [\"F\", \"M\"] )","0ecdd921":"data.head(5)","8da82fe3":"res = data.iloc[:,6:7].values\nres.shape","30fc7179":"le2 = preprocessing.LabelEncoder()\nres[:,0] = le2.fit_transform(data.iloc[:,6:7])","bee0e759":"ohe2 = preprocessing.OneHotEncoder()\nres = ohe2.fit_transform(res).toarray()","138baacc":"res = pd.DataFrame(data = res, index = range(918), columns = [\"re_lvh\", \"re_normal\", \"le_st\"] )","f9a7b725":"res.head(5)","7a3c8760":"ex = data.iloc[:,8:9].values\nle3 = preprocessing.LabelEncoder()\nex[:,0] = le3.fit_transform(data.iloc[:,8:9])","611f08e2":"ohe3 = preprocessing.OneHotEncoder()\nex = ohe3.fit_transform(ex).toarray()","a365b036":"ex = pd.DataFrame(data = ex, index = range(918), columns = [\"ex_no\", \"ex_yes\"] )","31785e65":"ex.head(5)","79dfc69e":"st = data.iloc[:,10:11].values\nle4 = preprocessing.LabelEncoder()\nst[:,0] = le4.fit_transform(data.iloc[:,10:11])","61a85fbc":"ohe4 = preprocessing.OneHotEncoder()\nst = ohe3.fit_transform(st).toarray()","cb317c29":"st = pd.DataFrame(data = st, index = range(918), columns = [\"ST_S_down\", \"ST_S_flat\", \"ST_S_up\"] )","2592526d":"st.head(5)","2b8629d5":"data1 = pd.concat([sex,chestt,res,ex,st], axis = 1)","d3834aa9":"data1.head(5)","3d3cc342":"data.drop([\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"], axis = 1, inplace = True)","f9dae136":"data.head(5)","9410b7df":"data = pd.concat([data1, data], axis = 1)","a3c81c99":"data.head(5)","28b5c262":"data.columns","68ad76a3":"x = data.iloc[:,0:20].values","bd5f0174":"y = data.iloc[:,20:].values","71cf4976":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\nsc = preprocessing.StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","556278f4":"myList = []\nclass Logistic():\n    def __init__(self,y_pred, cm, accuracy, report):\n        self.y_pred = y_pred \n        self.cm = cm\n        self.accuracy = accuracy\n        self.report = report\n        logr = LogisticRegression()\n        logr.fit(X_train, y_train)\n        y_pred = logr.predict(X_test)\n        print(f' Logistic Regression : \\n{y_pred}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy = accuracy_score(y_pred, y_test)\n        print(f'accuracy score: {accuracy}')\n        print(\"------------------------------------\")\n        myList.append(accuracy)\n        report = classification_report(y_test, y_pred)\n        print(f'classification_report: {report}')\nLogistic = Logistic(\"y_pred\",\"cm\", \"accuracy\",\"report\")   \nprint(Logistic) ","685c0ba7":"myList1 = []\nclass Svc():\n    def __init__(self,y_pred1, cm1, accuracy1, report1, success1):\n        self.y_pred1 = y_pred1\n        self.cm1 = cm1\n        self.accuracy1 = accuracy1\n        self.report1 = report1\n        self.success1 = success1\n        svc = SVC(C = 2, kernel = \"rbf\", gamma = 0.01, random_state = 0)\n        svc.fit(X_train, y_train)\n        y_pred1 = svc.predict(X_test)\n        print(f' SVC : \\n{y_pred1}')\n        print(\"------------------------------------\")\n        cm1 = confusion_matrix(y_test, y_pred1)\n        print(f'confusion_ matrix: \\n{cm1}') \n        print(\"------------------------------------\")\n        accuracy1 = accuracy_score(y_pred1, y_test)\n        print(f'accuracy score: {accuracy1}')\n        print(\"------------------------------------\")\n        myList1.append(accuracy1)\n        report1 = classification_report(y_test, y_pred1)\n        print(f'classification_report: {report1}')\n        print(\"------------------------------------\")\n        success1 = cross_val_score(estimator = svc, X= X_train, y=y_train, cv=4)\n        print(f'success: {success1}')\n        p = [{'C': [1,2,3,4,5,6,7,8,9,10], 'kernel': ['linear']}, {'C': [1,2,3,4,5,6,7,8,9,10], 'kernel': ['rbf']}, {'gamma': [1,0.5,0.1,0.01, 0.001]}]\n        gs = GridSearchCV(estimator = svc, param_grid = p, scoring=\"accuracy\", cv = 10, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nSvc = Svc(\"y_pred1\",\"cm1\", \"accuracy1\",\"report1\",\"success1\")","5ff5d4cc":"myList2 = []\nclass Knn():\n    def __init__(self,y_pred2, cm2, accuracy2, report2,success2):\n        self.y_pred2 = y_pred2\n        self.cm2 = cm2\n        self.accuracy2 = accuracy2\n        self.report2 = report2\n        self.success2 = success2\n        knn = KNeighborsClassifier(n_neighbors=19, metric = \"manhattan\", weights = \"distance\")\n        knn.fit(X_train, y_train)\n        y_pred2 = knn.predict(X_test)\n        print(f' KNN : \\n{y_pred2}')\n        print(\"------------------------------------\")\n        cm2 = confusion_matrix(y_test, y_pred2)\n        print(f'confusion_ matrix: \\n{cm2}') \n        print(\"------------------------------------\")\n        accuracy2 = accuracy_score(y_test, y_pred2)\n        print(f'accuracy score: {accuracy2}')\n        print(\"------------------------------------\")\n        myList2.append(accuracy2)\n        report2 = classification_report(y_test, y_pred2)\n        print(f'classification_report: {report2}')\n        print(\"------------------------------------\")\n        success2 = cross_val_score(estimator = knn, X= X_train, y=y_train, cv=4)\n        print(f'success : {success2}')\n        p1 = [{\"n_neighbors\": range(20), \"weights\": ['uniform' ,'distance'], \"metric\": ('minkowski', 'euclidean', 'manhattan')} ]\n        gs = GridSearchCV(estimator = knn, param_grid = p1, scoring=\"accuracy\", cv = 30, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nKnn(\"y_pred2\",\"cm2\", \"accuracy2\",\"report2\", \"success2\")","fca3b4a9":"myList3 = []\nclass Decisiontree():\n    def __init__(self,y_pred3, cm3, accuracy3, report3):\n        self.y_pred3 = y_pred3\n        self.cm3 = cm3\n        self.accuracy3 = accuracy3\n        self.report3 = report3\n        dt = DecisionTreeClassifier(criterion = 'entropy')\n        dt.fit(X_train, y_train)\n        y_pred3 = dt.predict(X_test)\n        print(f' Decision Tree : \\n{y_pred3}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred3)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy3 = accuracy_score(y_pred3, y_test)\n        print(f'accuracy score: {accuracy3}')\n        print(\"------------------------------------\")\n        myList3.append(accuracy3)\n        report3 = classification_report(y_test, y_pred3)\n        print(f'classification_report: {report3}')\n        \nDecisiontree(\"y_pred3\", \"cm3\", \"accuracy3\", \"report3\")","4d04286c":"myList4 = []\nclass Randomforest():\n    def __init__(self,y_pred4, cm4, accuracy4, report4,success4):\n        self.y_pred4 = y_pred4\n        self.cm4= cm4\n        self.accuracy4= accuracy4\n        self.report4= report4\n        self.success4 = success4\n        rfc = RandomForestClassifier(n_estimators=95, criterion = 'gini', random_state = 0)\n        rfc.fit(X_train, y_train)\n        y_pred4 = rfc.predict(X_test)\n        print(f'Random Forest : \\n{y_pred4}')\n        print(\"------------------------------------\")\n        cm4 = confusion_matrix(y_test, y_pred4)\n        print(f'confusion_ matrix: \\n{cm4}') \n        print(\"------------------------------------\")\n        accuracy4 = accuracy_score(y_pred4, y_test)\n        print(f'accuracy score: {accuracy4}')\n        print(\"------------------------------------\")\n        myList4.append(accuracy4)\n        report4 = classification_report(y_test, y_pred4)\n        print(f'classification_report: {report4}')\n        print(\"------------------------------------\")\n        success4 = cross_val_score(estimator = rfc, X= X_train, y=y_train, cv=4)\n        print(f'success : {success4}')\n        p2 = {'n_estimators': range(100), 'criterion' :['gini', 'entropy']}\n        gs = GridSearchCV(estimator=rfc, param_grid=p2, scoring = \"accuracy\", cv= 4, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\n\nRandomforest(\"y_pred4\",\"cm4\", \"accuracy4\", \"report4\",\"success4\")","a85d7f25":"myList5 = []\nclass Naivebayes():\n    def __init__(self,y_pred5, cm5, accuracy5, report5):\n        self.y_pred5 = y_pred5\n        self.cm5= cm5\n        self.accuracy5= accuracy5\n        self.report5= report5\n        gnb = GaussianNB(var_smoothing = 10.0)\n        gnb.fit(X_train, y_train)\n        y_pred5 = gnb.predict(X_test)\n        print(f'Gaussian NB : \\n{y_pred5}')\n        print(\"------------------------------------\")\n        cm5 = confusion_matrix(y_test, y_pred5)\n        print(f'confusion_ matrix: \\n{cm5}') \n        print(\"------------------------------------\")\n        accuracy5 = accuracy_score(y_pred5, y_test)\n        print(f'accuracy score: {accuracy5}')\n        print(\"------------------------------------\")\n        myList5.append(accuracy5)\n        report5 = classification_report(y_test, y_pred5)\n        print(f'classification_report: {report5}')\n        print(\"---------------------------------\")\n        success5 = cross_val_score(estimator = gnb, X= X_train, y=y_train, cv=4)\n        print(f'success : {success5}')\n        p4 = {'var_smoothing': np.logspace(0,9, num=100)}\n        gs = GridSearchCV(estimator=gnb, \n                 param_grid=p4, \n                 cv=10,  \n                 verbose=1, \n                 scoring='accuracy')\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nNaivebayes(\"y_pred5\",\"cm5\", \"accuracy5\", \"report5\")","996f14a6":"myList6 = []\nclass XGBoost():\n    def __init__(self,y_pred, cm, accuracy, report):\n        self.y_pred = y_pred \n        self.cm = cm\n        self.accuracy = accuracy\n        self.report = report\n        xgb = XGBClassifier()\n        xgb.fit(X_train, y_train)\n        y_pred = xgb.predict(X_test)\n        print(f' XGB : \\n{y_pred}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy = accuracy_score(y_pred, y_test)\n        print(f'accuracy score: {accuracy}')\n        print(\"------------------------------------\")\n        myList6.append(accuracy)\n        report = classification_report(y_test, y_pred)\n        print(f'classification_report: {report}')\nXGBoost = XGBoost(\"y_pred\",\"cm\", \"accuracy\",\"report\")   \nprint(XGBoost) ","a539b239":"models = myList + myList1 + myList2 + myList3 + myList4 + myList5 + myList6\naccuracy_scores = []\nfor model in models:\n    accuracy_scores.append(model)\nprint(accuracy_scores)  ","8e2df1b9":"models1 = {'LG':myList, 'SVC':myList1, 'KNN':myList2, 'DTC':myList3, 'RFC':myList4, 'GB':myList5, 'XGB':myList6}\nprint(models1)","e8b3a222":"md = list(models1.keys())\nv = list(models1.values())","5abe4c84":"'''\nplotdata = pd.DataFrame({'LG':myList, 'SVC':myList1, 'KNN':myList2, 'DTC':myList3, 'RFC':myList4, 'GB':myList5, 'XGB':myList6}, \n                        index=['LG', 'SVC', 'KNN', 'DTC', 'RFC', 'GB', 'XGB'])\n\nplotdata.plot(kind=\"bar\",figsize=(15, 8))\n\nplt.title(\"Accuracy comparison for various models\")\n\nplt.xlabel(\"Models\")\n'''","29273418":"'''import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.barplot(x = 'Models',y = 'Accuracy Score',data = accuracy_scores)\nplt.show()'''","b8940c75":"'''fig = plt.figure(figsize = (10, 5))\n#  Bar plot\nplt.bar(md, v, color ='green', width = 0.5)\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Accuracy comparison for various models\")\nplt.show()'''","3384a1c2":"'''\nplt.bar(['LG', 'SVC', 'KNN', 'DTC', 'RFC', 'GB', 'XGB'], accuracy_scores)\nplt.ylim(0.6,0.95)\nplt.title('Accuracy comparison for various models', fontsize=15, color='r')\nplt.xlabel('Models', fontsize=15, color='r')\nplt.ylabel('Accuracy Score', fontsize=15, color='r')\nplt.tight_layout()\nplt.show()\n'''","dfeee511":"# XGBoost","d1701f90":"# Data Preprocessing","b17a75bb":"# SVC","95cb4801":"# MODELS","806880ad":"#  Logistic Regression","16def7d5":"# KNN","23cf9f48":"\n# Comparison","81a5a2fe":"# Data Visualization","e05e55c4":"# DecisionTree","9e90b23b":"# Naive Bayes","96afee82":"# Random Forest"}}