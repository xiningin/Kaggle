{"cell_type":{"0ffcceb1":"code","e21b3eb5":"code","6821d5b8":"code","0012a006":"code","15320a3a":"code","f6c4964d":"code","0795a3d8":"code","e958fbb6":"code","95a19fab":"code","ea14be64":"code","fb8c93b6":"code","15d23e16":"code","3fc2fdac":"code","71583ae5":"code","84a959f6":"code","9c92b620":"code","f826d119":"code","96b5d8f1":"code","9077a435":"code","43c3fd91":"code","2bd170a1":"code","970ef817":"code","ed103749":"code","94829af7":"markdown"},"source":{"0ffcceb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n%matplotlib inline\nimport itertools\nfrom time import sleep\n\nimport gc\nfrom pathlib2 import Path\nfrom tqdm import tqdm_notebook\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e21b3eb5":"\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample_submission=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\n\ngroupby_cols = ['date_block_num', 'shop_id', 'item_id']\n","6821d5b8":"train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (\n            train.item_price > 0)].item_price.median()\ntrain = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day < 1001]\n\nmedian = train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (\n            train.item_price > 0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median","0012a006":"train.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","15320a3a":"test['date_block_num'] = 34","f6c4964d":"category = items[['item_id', 'item_category_id']].drop_duplicates()\ncategory.set_index(['item_id'], inplace=True)\ncategory = category.item_category_id\ntrain['category'] = train.item_id.map(category)\ncategory\ntrain","0795a3d8":"item_categories['meta_category'] = item_categories.item_category_name.apply(lambda x: x.split(' ')[0])\nitem_categories['meta_category'] = pd.Categorical(item_categories.meta_category).codes\nitem_categories.set_index(['item_category_id'], inplace=True)\nmeta_category = item_categories.meta_category\ntrain['meta_category'] = train.category.map(meta_category)\n","e958fbb6":"train","95a19fab":"shops['city'] = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city'] = pd.Categorical(shops['city']).codes\ncity = shops.city\ntrain['city'] = train.shop_id.map(city)\n\n","ea14be64":"year = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[2]))], axis=1).drop_duplicates()\nyear.set_index(['date_block_num'], inplace=True)\nyear = year.date.append(pd.Series([2015], index=[34]))\n\n","fb8c93b6":"month = pd.concat([train.date_block_num, train.date.apply(lambda x: int(x.split('.')[1]))], axis=1).drop_duplicates()\nmonth.set_index(['date_block_num'], inplace=True)\nmonth = month.date.append(pd.Series([11], index=[34]))\n\n","15d23e16":"all_shops_items = []\n\nfor block_num in train['date_block_num'].unique():\n    unique_shops = train[train['date_block_num'] == block_num]['shop_id'].unique()\n    unique_items = train[train['date_block_num'] == block_num]['item_id'].unique()\n    all_shops_items.append(np.array(list(itertools.product([block_num], unique_shops, unique_items)), dtype='int32'))\n\ndf = pd.DataFrame(np.vstack(all_shops_items), columns=groupby_cols, dtype='int32')\ndf = df.append(test, sort=True)","3fc2fdac":"df['ID'] = df.ID.fillna(-1).astype('int32')\ndf['year'] = df.date_block_num.map(year)\ndf['month'] = df.date_block_num.map(month)\ndf['category'] = df.item_id.map(category)\ndf['meta_category'] = df.category.map(meta_category)\ndf['city'] = df.shop_id.map(city)\ntrain['category'] = train.item_id.map(category)\n\n","71583ae5":"df","84a959f6":"%%time\n\ngb = train.groupby(by=groupby_cols, as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=groupby_cols)\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_item'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])\n\ngb = train.groupby(by=['date_block_num', 'shop_id'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_shop'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'shop_id'])\n\ngb = train.groupby(by=['date_block_num', 'category'], as_index=False).agg({'item_cnt_day': ['sum']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_cnt_day_sum': 'target_category'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'category'])\n\ngb = train.groupby(by=['date_block_num', 'item_id'], as_index=False).agg({'item_price': ['mean', 'max']})\ngb.columns = [val[0] if val[-1] == '' else '_'.join(val) for val in gb.columns.values]\ngb.rename(columns={'item_price_mean': 'target_price_mean', 'item_price_max': 'target_price_max'}, inplace=True)\ndf = pd.merge(df, gb, how='left', on=['date_block_num', 'item_id'])\n","9c92b620":"df['target_price_mean'] = np.minimum(df['target_price_mean'], df['target_price_mean'].quantile(0.99))\ndf['target_price_max'] = np.minimum(df['target_price_max'], df['target_price_max'].quantile(0.99))\n\ndf.fillna(0, inplace=True)\ndf['target'] = df['target'].clip(0, 20)\ndf['target_zero'] = (df['target'] > 0).astype('int32')","f826d119":"df","96b5d8f1":"%%time\n\nfor enc_cols in [['shop_id', 'category'], ['shop_id', 'item_id'], ['shop_id'], ['item_id']]:\n\n    col = '_'.join(['enc', *enc_cols])\n    col2 = '_'.join(['enc_max', *enc_cols])\n    df[col] = np.nan\n    df[col2] = np.nan\n\n    for d in tqdm_notebook(df.date_block_num.unique()):\n        f1 = df.date_block_num < d\n        f2 = df.date_block_num == d\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].mean().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col] = enc['target']\n\n        gb = df.loc[f1].groupby(enc_cols)[['target']].max().reset_index()\n        enc = df.loc[f2][enc_cols].merge(gb, on=enc_cols, how='left')[['target']].copy()\n        enc.set_index(df.loc[f2].index, inplace=True)\n        df.loc[f2, col2] = enc['target']\n\n","9077a435":"\n\ndef downcast_dtypes(df):\n    float32_cols = [c for c in df if df[c].dtype == 'float64']\n    int32_cols = [c for c in df if df[c].dtype in ['int64', 'int16', 'int8']]\n\n    df[float32_cols] = df[float32_cols].astype(np.float32)\n    df[int32_cols] = df[int32_cols].astype(np.int32)\n\n    return df\n\n","43c3fd91":"df.fillna(0, inplace=True)\ndf = downcast_dtypes(df)\n\n","2bd170a1":"\n\n%%time\n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nshifted_columns = [c for c in df if 'target' in c]\n\nfor shift in tqdm_notebook(shift_range):\n    shifted_data = df[groupby_cols + shifted_columns].copy()\n    shifted_data['date_block_num'] = shifted_data['date_block_num'] + shift\n\n    foo = lambda x: '{}_lag_{}'.format(x, shift) if x in shifted_columns else x\n    shifted_data = shifted_data.rename(columns=foo)\n\n    df = pd.merge(df, shifted_data, how='left', on=groupby_cols).fillna(0)\n    df = downcast_dtypes(df)\n\n    del shifted_data\n    gc.collect()\n    sleep(1)\n\n","970ef817":"df['target_trend_1_2'] = df['target_lag_1'] - df['target_lag_2']\ndf['target_predict_1_2'] = df['target_lag_1'] * 2 - df['target_lag_2']\n\ndf['target_trend_3_4'] = df['target_lag_1'] + df['target_lag_2'] - df['target_lag_3'] - df['target_lag_4']\ndf['target_predict_3_4'] = (df['target_lag_1'] + df['target_lag_2']) * 2 - df['target_lag_3'] - df['target_lag_4']\n\ndf['target_item_trend_1_2'] = df['target_item_lag_1'] - df['target_item_lag_2']\ndf['target_item_trend_3_4'] = df['target_item_lag_1'] + df['target_item_lag_2'] - df['target_item_lag_3'] - df['target_item_lag_4']\ndf['target_shop_trend_1_2'] = df['target_shop_lag_1'] - df['target_shop_lag_2']\ndf['target_shop_trend_3_4'] = df['target_shop_lag_1'] + df['target_shop_lag_2'] - df['target_shop_lag_3'] - df['target_shop_lag_4']","ed103749":"df = downcast_dtypes(df)\ndf.to_pickle('df.pkl')\n","94829af7":"# **Adding new features**"}}