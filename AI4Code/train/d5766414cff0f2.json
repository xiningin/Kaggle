{"cell_type":{"27cd1527":"code","a4e02bd6":"code","6587d00b":"code","1a3963e6":"code","0b509aaa":"code","51f8eff1":"code","4152bede":"code","fb3418b6":"code","c2471d2c":"code","610b90d3":"code","2c014ba6":"code","b726aa31":"code","9e59af8e":"code","92956f03":"code","6dd9b98e":"code","e45aa160":"code","6dcbfc8f":"code","5897c04d":"code","ecb089c4":"code","6c7034b8":"markdown"},"source":{"27cd1527":"%%capture\n!pip install --no-index --find-links ..\/input\/huggingface-datasets datasets -qq","a4e02bd6":"%%html\n<style>\ntable {float:left}\n<\/style>","6587d00b":"import pandas as pd\nimport numpy as np\nimport os\nimport json\nimport collections\n\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nfrom pydantic import BaseModel\n\nimport datasets\nfrom transformers.trainer_utils import set_seed\nfrom transformers import (AutoTokenizer, PreTrainedTokenizerFast,\n                          AutoModelForQuestionAnswering, TrainingArguments,\n                          Trainer, default_data_collator, DataCollatorWithPadding,)\n\nfrom tqdm.auto import tqdm\nimport gc","1a3963e6":"def read_json(from_path: Path) -> dict:\n    with open(from_path, 'r', encoding='utf-8') as out_file:\n        return json.load(out_file)\n        \ndef write_json(data: dict, out_path: Path) -> None:\n    with open(out_path, 'w', encoding='utf-8') as out_file:\n        json.dump(data, out_file, indent=2, sort_keys=True, ensure_ascii=False)","0b509aaa":"path = '..\/input\/chaii-roberta-large\/'\nconfig = read_json(f'{path}xlm_roberta_large_squad2_finetuned_v11.json')\nconfig['model_path'] = f'{path}xlm-roberta-large-squad2-finetuned-v11\/xlm-roberta-large-squad2-finetuned-v11'\nconfig['LB'] = 0.736\nwrite_json(config, '.\/xlm_roberta_large_squad2_finetuned_v11.json')","51f8eff1":"set_seed(config['seed'])","4152bede":"def df_to_squad_format(path: Path, out_name: str, lang: Optional[str] = None) -> Path:\n    df = pd.read_csv(path)\n    if lang:\n        df = df.loc[df.language == lang].copy()\n        out_name = f'{out_name}_{lang}'\n    \n    data = []\n    for _, row in df.iterrows():\n        answers = {}\n        try:\n            answers['answer_start'] = [int(row['answer_start'])]\n            answers['text'] = [row['answer_text']]\n        except:\n            answers = {'answer_start': [-1], 'text': ['']}\n        data.append(\n            {\n            'answers': answers,\n            'context': row['context'],\n            'id': row['id'],\n            'question': row['question'],\n            'title': ''\n            }\n        )\n    \n    df_as_squad = {'data': data, 'version': out_name}\n    \n    out_path = f'.\/{out_name}.json'\n    write_json(df_as_squad, out_path)\n    print('The data has been converted to SQuAD format and saved as a JSON object.')\n    return out_path","fb3418b6":"chaii_test = datasets.load_dataset(\n    'json',\n    data_files=df_to_squad_format(config['test_path'], 'chaii_test'), \n    field='data',\n    split='train'\n)","c2471d2c":"chaii_test","610b90d3":"tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\nmodel = AutoModelForQuestionAnswering.from_pretrained(config['model_path'])","2c014ba6":"assert isinstance(tokenizer, PreTrainedTokenizerFast)\npad_on_right = tokenizer.padding_side == \"right\"","b726aa31":"def prepare_validation_features(examples):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=config['max_length'],\n        stride=config['doc_stride'],\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    \n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","9e59af8e":"test_features = chaii_test.map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=chaii_test.column_names\n)","92956f03":"trainer = Trainer(\n    model,\n    data_collator=default_data_collator,\n    tokenizer=tokenizer,\n)","6dd9b98e":"test_predictions = trainer.predict(test_features)\ntest_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))","e45aa160":"def postprocess_qa_predictions(examples, features, raw_predictions, tokenizer=tokenizer,\n                               squad_v2=config['squad_v2'], n_best_size=config['n_best_size'], \n                               max_answer_length=config['max_answer_length']):\n    \n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        \n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions","6dcbfc8f":"predictions = postprocess_qa_predictions(chaii_test, test_features, test_predictions.predictions)","5897c04d":"test_df = pd.read_csv(config['test_path'])\ntest_df['PredictionString'] = test_df['id'].apply(lambda x: predictions[x])\ntest_df[['id', 'PredictionString']].to_csv('submission.csv', index=False)","ecb089c4":"pd.read_csv('.\/submission.csv')","6c7034b8":"This is an inference notebook from the finetuned [XLM-Roberta model pretrained on SQUAD](https:\/\/www.kaggle.com\/oleksandrsirenko\/chaii-fine-tuning-model). For training, we used [formatted datasets](https:\/\/www.kaggle.com\/oleksandrsirenko\/chaii-squad) obtained from the original and external data. Scrips for converting data to SQuAD format you can [find here](https:\/\/www.kaggle.com\/oleksandrsirenko\/chaii-dataframe-and-external-data-to-squad).\n\n\n| Model              | Version  | LB Score|\n|:-------------------|:--------:|:-------:|\n| XLM Roberta large  | v11      | 0.736   |\n| XLM Roberta large  | v10      | 0.732   |\n| XLM Roberta large  | v9       | 0.717   |\n| XLM Roberta base   | v5       | 0.617   | \n| XLM Roberta base   | v4       | 0.629   |\n| XLM Roberta base   | v2       | 0.645   |"}}