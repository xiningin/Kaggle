{"cell_type":{"87d53787":"code","ed7d3c02":"code","2f2703cf":"code","b1b89a07":"code","cef00a8e":"code","aad1659e":"code","68d0f3a2":"code","64709585":"code","cffa3d9f":"code","95a8808e":"code","a4837f0a":"code","4b5a8355":"code","a9bf252a":"code","5d189c58":"code","1a1ed708":"code","0732c582":"code","9984a841":"code","cd6f909d":"code","f975ed83":"code","58409d26":"code","633f8bc8":"code","82d29a0b":"code","2c5d54f3":"code","f2fab780":"code","c93e4b91":"code","ea92eccd":"code","959a9c39":"code","a095b720":"code","e3ca7360":"code","a09bf479":"code","90939a80":"code","f876530b":"code","0f622a35":"code","58eef9dc":"code","fba80289":"code","f4fef7e5":"code","fd8016b9":"code","10fc0537":"code","b7e1f49b":"code","bf3310e0":"code","58c0417e":"code","c7817110":"code","55408eba":"code","e2009a54":"code","3cceb1c3":"code","dc1b3b6a":"code","1cbc98d2":"code","6595eda4":"code","5b546189":"code","796d9bd1":"code","5f41eb5d":"code","0efecbaa":"code","dff56bb5":"code","b138f376":"code","197ec871":"code","71b286eb":"code","5f397f5e":"code","4ad02246":"code","b7e1882c":"code","53aad8e9":"code","f0d79bb1":"markdown","999c53b7":"markdown","1ec19388":"markdown","80ce9c8d":"markdown","1c49e0d7":"markdown","305fe9ec":"markdown"},"source":{"87d53787":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.applications.densenet import DenseNet169\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint","ed7d3c02":"data_dir = '..\/input\/mura-v11\/MURA-v1.1'\n\ntrain_dir = data_dir + '\/train' # Chemin d'acc\u00e8s au r\u00e9pertoire de train set\nval_dir = data_dir + '\/valid' # Chemin d'acc\u00e8s au r\u00e9pertoire de test set","2f2703cf":"# Affichage d'une image osseuse anormale\nimg_abnormal = load_img('..\/input\/mura-v11\/MURA-v1.1\/train\/XR_ELBOW\/patient00069\/study1_positive\/image2.png')\nprint('ABNORMAL')\nplt.imshow(img_abnormal)\nplt.show()","b1b89a07":"# Affichage d'une image osseuse normale\nimg_normal = load_img('..\/input\/mura-v11\/MURA-v1.1\/train\/XR_ELBOW\/patient00011\/study1_negative\/image1.png')\nprint('NORMAL')\nplt.imshow(img_normal)\nplt.show()","cef00a8e":"#les etudes de train set avec labels\ndf=pd.read_csv('..\/input\/mura-v11\/MURA-v1.1\/train_labeled_studies.csv', names=['Train_Image','Train_Label'])","aad1659e":"df.head(15)","68d0f3a2":"# Compter les \u00e9tiquettes dans le train set\ncases_count = df['Train_Label'].value_counts()\nprint(cases_count)\n\n# Tracer les r\u00e9sultats\nplt.figure(figsize=(4,4))\nsns.barplot(x=cases_count.index, y=cases_count.values)\nplt.title('Number of cases', fontsize=12)\nplt.xlabel('Case type', fontsize=10)\nplt.ylabel('Count', fontsize=10)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Abnormal(1)'])\nplt.show()","64709585":"# Ajout d'\u00e9tiquettes aux images individuelles dans le train set\ndf=pd.read_csv('..\/input\/mura-v11\/MURA-v1.1\/train_image_paths.csv', names=['Train_Image'])\nnames=df['Train_Image'].values\ntrain_labels=[]\n\nfor i in names:\n  if ('positive' in i):\n    train_labels.append('1')\n  elif('negative' in i):\n    train_labels.append('0')\n\ntrain_labels = np.array(train_labels)\n#labels = pd.DataFrame(labels, columns=['Image', 'Label'])\ndf.insert(1, 'Train_Label', train_labels)\ndf.to_csv('Train_set.csv', index=False)","cffa3d9f":"df.head(20)","95a8808e":"# Compter le nombre d'\u00e9tiquettes individuelles des images dans le train set\ncases_count = df['Train_Label'].value_counts()\nprint(cases_count)\n\n# Tracer les r\u00e9sultats\nplt.figure(figsize=(4,4))\nsns.barplot(x=cases_count.index, y=cases_count.values)\nplt.title('Number of labels', fontsize=12)\nplt.xlabel('Case type', fontsize=10)\nplt.ylabel('Count', fontsize=10)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Abnormal(1)'])\nplt.show()","a4837f0a":"#les etudes d'ensembles de validation avec labels\ndf=pd.read_csv('..\/input\/mura-v11\/MURA-v1.1\/valid_labeled_studies.csv', names=['Valid_Image','Valid_Label'])\n","4b5a8355":"df.head(20)","a9bf252a":"# Compter les \u00e9tiquettes dans le jeu de validation\ncases_count = df['Valid_Label'].value_counts()\nprint(cases_count)\n\n# Tracer les r\u00e9sultats \nplt.figure(figsize=(4,4))\nsns.barplot(x=cases_count.index, y=cases_count.values)\nplt.title('Number of cases', fontsize=12)\nplt.xlabel('Case type', fontsize=10)\nplt.ylabel('Count', fontsize=10)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Abnormal(1)'])\nplt.show()","5d189c58":"# Ajout d'\u00e9tiquettes aux images individuelles dans le jeu de validation\ndf=pd.read_csv('..\/input\/mura-v11\/MURA-v1.1\/valid_image_paths.csv', names=['Valid_Image'])\nnames=df['Valid_Image'].values\nvalid_labels=[]\n\nfor i in names:\n  if ('positive' in i):\n    valid_labels.append('1')\n  elif('negative' in i):\n    valid_labels.append('0')\n\nvalid_labels = np.array(valid_labels)\n#labels = pd.DataFrame(labels, columns=['Image', 'Label'])\ndf.insert(1, 'Valid_Label', valid_labels)\ndf.to_csv('Valid_set.csv', index=False)","1a1ed708":"df.head(20)","0732c582":"# Compter le nombre d'\u00e9tiquettes individuelles des images dans l'ensemble de validation\ncases_count = df['Valid_Label'].value_counts()\nprint(cases_count)\n\n# Tracer les r\u00e9sultats \nplt.figure(figsize=(4,4))\nsns.barplot(x=cases_count.index, y=cases_count.values)\nplt.title('Number of labels', fontsize=12)\nplt.xlabel('Case type', fontsize=10)\nplt.ylabel('Count', fontsize=10)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Abnormal(1)'])\nplt.show()","9984a841":"# Lire les fichiers .csv de training set et de valid set\ntrain_df = pd.read_csv(\"..\/input\/mydata\/Train_set.csv\", dtype=str)\nvalid_df = pd.read_csv(\"..\/input\/mydata\/Valid_set.csv\", dtype=str)\n\n\nprint(train_df.shape)\nprint(valid_df.shape)\n","cd6f909d":"# Image Preprocessing\ndatagen = ImageDataGenerator(rescale=1.\/255, rotation_range=30)\n\ntrain_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=None,\n                                              x_col=\"Train_Image\", y_col=\"Train_Label\",\n                                              target_size=(224,224), class_mode=\"binary\",\n                                              batch_size=16, validate_filenames=False)\n\nvalid_generator = datagen.flow_from_dataframe(dataframe=valid_df, directory=None,\n                                              x_col=\"Valid_Image\", y_col=\"Valid_Label\",\n                                              target_size=(224,224), class_mode=\"binary\",\n                                              batch_size=16, shuffle=True, validate_filenames=False)","f975ed83":"print(train_generator.n)\nprint(valid_generator.n)","58409d26":"#Import densenet169 pre-trained model\ndense_model = DenseNet169(include_top=True, weights='imagenet')","633f8bc8":"dense_model.layers.pop()","82d29a0b":"# Compile the model\npredictions = Dense(1, activation='sigmoid')(dense_model.layers[-1].output)\nmodel = Model(inputs=dense_model.input, outputs=predictions)\n\nmodel.compile(optimizer = Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","2c5d54f3":"# Number of train and validation steps\ntrain_steps=train_generator.n\/\/train_generator.batch_size\nvalid_steps=valid_generator.n\/\/valid_generator.batch_size","f2fab780":"print(train_steps)\nprint(valid_steps)","c93e4b91":"filepath = \"weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, verbose=1, mode='max')\nepoche_nbr = 15","ea92eccd":"# history = model.fit_generator(generator=train_generator,\n#                     steps_per_epoch=train_steps,\n#                     validation_data=valid_generator,\n#                     validation_steps=valid_steps,\n#                     epochs=epoche_nbr,\n#                     callbacks=[checkpoint])","959a9c39":"# Plots for training and testing process: loss and accuracy\ndef plot_model_history(model_name, history, epochs):\n  \n  print(model_name)\n  plt.figure(figsize=(15, 5))\n  \n  # summarize history for accuracy\n  plt.subplot(1, 2 ,1)\n  plt.plot(np.arange(0, len(history['accuracy'])), history['accuracy'], 'r')\n  plt.plot(np.arange(1, len(history['val_accuracy'])+1), history['val_accuracy'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Accuracy vs. Validation Accuracy')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['train', 'validation'], loc='best')\n  \n  plt.subplot(1, 2, 2)\n  plt.plot(np.arange(1, len(history['loss'])+1), history['loss'], 'r')\n  plt.plot(np.arange(1, len(history['val_loss'])+1), history['val_loss'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Loss vs. Validation Loss')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Loss')\n  plt.legend(['train', 'validation'], loc='best')\n  \n  \n  plt.show()","a095b720":"# plot_model_history('Densenet169', history.history, epoche_nbr)","e3ca7360":"import tensorflow as tf \nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nbase_model_vgg = VGG16(input_shape = (224, 224, 3), # Shape of our images\ninclude_top = False, # Leave out the last fully connected layer\nweights = 'imagenet')","a09bf479":"for layer in base_model_vgg.layers:\n    layer.trainable = False","90939a80":"# Flatten the output layer to 1 dimension\nx = layers.Flatten()(base_model_vgg.output)\n\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = layers.Dense(512, activation='relu')(x)\n\n# Add a dropout rate of 0.5\nx = layers.Dropout(0.5)(x)\n\n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(base_model_vgg.input, x)    \n\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])\n","f876530b":"# vgghist = model.fit(train_generator, validation_data = valid_generator, steps_per_epoch = train_steps, epochs = epoche_nbr)","0f622a35":"# Plots for training and testing process: loss and accuracy\ndef plot_model_history1(model_name, history, epochs):\n  \n  print(model_name)\n  plt.figure(figsize=(15, 5))\n  \n  # summarize history for accuracy\n  plt.subplot(1, 2 ,1)\n  plt.plot(np.arange(0, len(history['acc'])), history['acc'], 'r')\n  plt.plot(np.arange(1, len(history['val_acc'])+1), history['val_acc'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Accuracy vs. Validation Accuracy')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['train', 'validation'], loc='best')\n  \n  plt.subplot(1, 2, 2)\n  plt.plot(np.arange(1, len(history['loss'])+1), history['loss'], 'r')\n  plt.plot(np.arange(1, len(history['val_loss'])+1), history['val_loss'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Loss vs. Validation Loss')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Loss')\n  plt.legend(['train', 'validation'], loc='best')\n  plt.show()","58eef9dc":"# plot_model_history1('VGG16', vgghist.history, epoche_nbr)","fba80289":"from tensorflow.keras.applications import ResNet50\n\nbase_model_res = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")","f4fef7e5":"for layer in base_model_res.layers:\n    layer.trainable = False","fd8016b9":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n\nbase_model_res = Sequential()\nbase_model_res.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\nbase_model_res.add(Dense(1, activation='sigmoid'))","10fc0537":"base_model_res.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])","b7e1f49b":"# base_model_res.summary()","bf3310e0":"# resnet_history = base_model_res.fit(train_generator, validation_data = valid_generator, steps_per_epoch = train_steps, epochs = epoche_nbr)","58c0417e":"# plot_model_history1('ResNet50', resnet_history.history, epoche_nbr)","c7817110":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n\nbase_model_inc = InceptionV3(input_shape = (224, 224,3), include_top = False, weights = 'imagenet')\n","55408eba":"for layer in base_model_inc.layers:\n    layer.trainable = False","e2009a54":"from tensorflow.keras.optimizers import RMSprop\n\nx = layers.Flatten()(base_model_inc.output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(base_model_inc.input, x)\n\nmodel.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])\n","3cceb1c3":"# model.summary()","dc1b3b6a":"# inc_history = model.fit_generator(train_generator, validation_data = valid_generator, steps_per_epoch=train_steps, epochs = epoche_nbr, validation_steps=valid_steps, verbose = 1)","1cbc98d2":"# plot_model_history1('InceptionV3', inc_history.history, epoche_nbr)","6595eda4":"import tensorflow as tf\nimport os\nimport pandas as pd\nimport cv2\nfrom skimage.transform import rescale, resize\nfrom tensorflow import keras\nimport numpy as np\nfrom sklearn.utils import class_weight\nimport tensorflow_addons as tfa\nimport pickle\nfrom skimage.io import imread\nfrom sklearn.utils import shuffle\nfrom matplotlib import pyplot as plt","5b546189":"#To get the filenames for a task\ndef filenames(part,train=True):\n    root='..\/input\/mura-v11\/'\n    if train:\n        csv_path=\"..\/input\/mura-v11\/MURA-v1.1\/train_image_paths.csv\"\n    else:\n        csv_path=\"..\/input\/mura-v11\/MURA-v1.1\/valid_image_paths.csv\"\n    \n    with open(csv_path, 'rb') as F:\n        d = F.readlines()\n        if part == 'all':\n            imgs = [root + str(x, encoding='utf-8').strip() for x in d]  # \u6240\u6709\u56fe\u7247\u7684\u5b58\u50a8\u8def\u5f84, [:-1]\u76ee\u7684\u662f\u629b\u5f03\u6700\u672b\u5c3e\u7684\\n\n        else:\n            imgs = [root + str(x, encoding='utf-8').strip() for x in d if\n                            str(x, encoding='utf-8').strip().split('\/')[2] == part]\n\n    #imgs= [x.replace(\"\/\", \"\\\\\") for x in imgs]\n    labels= [x.split('_')[-1].split('\/')[0] for x in imgs]\n    return imgs,labels\n\n\n#To icrop a image from center\ndef crop_center(img,cropx,cropy):\n    y,x,_ = img.shape\n    startx = x\/\/2-(cropx\/\/2)\n    starty = y\/\/2-(cropy\/\/2)    \n    return img[starty:starty+cropy,startx:startx+cropx]","796d9bd1":"from albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,\n    ToFloat, ShiftScaleRotate\n)\nfrom albumentations.augmentations.transforms import Resize\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    RandomContrast(limit=0.2, p=0.5),\n    RandomGamma(gamma_limit=(80, 120), p=0.5),\n    RandomBrightness(limit=0.2, p=0.5),\n    ShiftScaleRotate(\n        shift_limit=0.0625, scale_limit=0.1, \n        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n    ToFloat(max_value=255)\n])\nAUGMENTATIONS_TEST = Compose([\n    # CLAHE(p=1.0, clip_limit=2.0),\n    ToFloat(max_value=255)\n])\n","5f41eb5d":"albumentation_list =  [\n    HorizontalFlip(p=0.5),\n    RandomContrast(limit=0.2, p=0.5),\n    RandomGamma(gamma_limit=(80, 120), p=0.5),\n    RandomBrightness(limit=0.2, p=0.5),\n    ShiftScaleRotate(\n        shift_limit=0.0625, scale_limit=0.1, \n        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n    ToFloat(max_value=255)\n]\nroot='..\/input\/mura-v11\/'\nchosen_image= imread(root+'MURA-v1.1\/train\/XR_ELBOW\/patient01055\/study1_positive\/image3.png')\nimg_matrix_list = []\nbboxes_list = []\nfor aug_type in albumentation_list:\n    img = aug_type(image = chosen_image)['image']\n    img_matrix_list.append(img)\nimg= resize(chosen_image,(300,300,3))\nimg_matrix_list.append(img)\nimg_matrix_list.append(crop_center(img,224,224))\n\nimg_matrix_list.insert(0,chosen_image)    \n\ntitles_list = [\"Original\",\"Horizontal Flip\",\"Random Contrast\",\"Random Gamma\",\"RandomBrightness\",\n               \"Shift Scale Rotate\",\"Resizing\", \"Cropping\"]\n\ndef plot_multiple_img(img_matrix_list, title_list, ncols, main_title=\"Data Augmentation\"):\n    fig, myaxes = plt.subplots(figsize=(20, 15), nrows=2, ncols=ncols, squeeze=True)\n    fig.suptitle(main_title, fontsize = 30)\n    #fig.subplots_adjust(wspace=0.3)\n    #fig.subplots_adjust(hspace=0.3)\n    for i, (img, title) in enumerate(zip(img_matrix_list, title_list)):\n        myaxes[i \/\/ ncols][i % ncols].imshow(img)\n        myaxes[i \/\/ ncols][i % ncols].set_title(title, fontsize=15)\n    plt.show()\n    \nplot_multiple_img(img_matrix_list, titles_list, ncols = 4)","0efecbaa":"class My_Custom_Generator(keras.utils.Sequence) :\n  \n  def __init__(self, image_filenames, labels, batch_size,transform) :\n    self.image_filenames = image_filenames\n    self.labels = labels\n    self.batch_size = batch_size\n    self.t= transform\n    \n  def __len__(self) :\n    return (np.ceil(len(self.image_filenames) \/ float(self.batch_size))).astype(np.int)\n  \n  \n  def __getitem__(self, idx) :\n    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n    x=[]\n    for file in batch_x:\n        img= imread(file)\n        img= self.t(image=img)[\"image\"]\n        img= resize(img,(300,300,3))\n        img= crop_center(img,224,224)\n        x.append(img)\n    x=np.array(x)\/255.0\n    y= np.array(batch_y)\n    return x,y\n","dff56bb5":"part='XR_WRIST'\nimgs,labels= filenames(part=part)\nvimgs,vlabels= filenames(part=part,train=False)\nprint(labels.count('positive'),labels.count('negative'))\ntraining_data= labels.count('positive')+labels.count('negative')\nprint(\"Training Data: \", training_data)\ny_data= [0 if x=='positive' else 1 for x in labels]\ny_data= keras.utils.to_categorical(y_data)\nprint(vlabels.count('positive'),vlabels.count('negative'))\nvalidation_data= vlabels.count('positive')+vlabels.count('negative')\nprint(\"Validation Data: \", validation_data)\nvy_data= [0 if x=='positive' else 1 for x in vlabels]\nvy_data= keras.utils.to_categorical(vy_data)","b138f376":"from sklearn.utils.class_weight import compute_class_weight\n\ny_integers = np.argmax(y_data, axis=1)\nclass_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\nd_class_weights = dict(enumerate(class_weights))\n\n","197ec871":"batch_size = 32\nimgs, y_data = shuffle(imgs, y_data)\n#vimgs, vy_data = shuffle(vimgs, vy_data)\nmy_training_batch_generator = My_Custom_Generator(imgs, y_data, batch_size,AUGMENTATIONS_TRAIN)\nmy_validation_batch_generator = My_Custom_Generator(vimgs, vy_data, batch_size,AUGMENTATIONS_TEST)\n","71b286eb":"part='XR_WRIST'\ncheckpoint_path = root+\"MURA-v1.1\/\"+part+\"\/WRIST.h5\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\nmy_callbacks = [\n    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=0, save_best_only=True,\n                                       save_weights_only=False, mode='auto'),\n    keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,patience=3,\n                                         min_delta=0.001, verbose=1, min_lr=0.000000001)]","5f397f5e":"Inception=keras.applications.InceptionResNetV2(include_top=False,input_shape=(224,224,3))\n#for layer in Inception.layers[:4]:\n#  layer.trainable=False\ninput_image=keras.layers.Input((224,224,3))\nx=Inception (input_image)\n\n#x=keras.layers.GlobalAveragePooling2D()(x)\nx=keras.layers.Flatten()(x)\n#x=keras.layers.Dense(1024)(x)\n#x=keras.layers.Activation(activation='relu')(x)\n#x= keras.layers.Dropout(0.5)(x)\nx=keras.layers.Dense(256)(x)\nx=keras.layers.Activation(activation='relu')(x)\nx= keras.layers.Dropout(0.5)(x)\nx=keras.layers.Dense(2)(x)\nout=keras.layers.Activation(activation='softmax')(x)\n\nmodel=keras.Model(inputs=input_image,outputs=out)\nmodel.compile(optimizer=keras.optimizers.RMSprop(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\nprint(model.summary())","4ad02246":"history=model.fit_generator(generator=my_training_batch_generator,\n                   steps_per_epoch = int(training_data \/\/ batch_size),\n                   epochs = 10,\n                   verbose = 1,\n                   class_weight=d_class_weights,\n                   validation_data = my_validation_batch_generator,\n                   validation_steps = int(validation_data \/\/ batch_size), \n                   callbacks=my_callbacks)","b7e1882c":"def plot_model_history2(model_name, history, epochs):\n  \n  print(model_name)\n  plt.figure(figsize=(15, 5))\n  \n  # summarize history for accuracy\n  plt.subplot(1, 2 ,1)\n  plt.plot(np.arange(0, len(history['accuracy'])), history['accuracy'], 'r')\n  plt.plot(np.arange(1, len(history['val_accuracy'])+1), history['val_accuracy'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Accuracy vs. Validation Accuracy')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['train', 'validation'], loc='best')\n  \n  plt.subplot(1, 2, 2)\n  plt.plot(np.arange(1, len(history['loss'])+1), history['loss'], 'r')\n  plt.plot(np.arange(1, len(history['val_loss'])+1), history['val_loss'], 'g')\n  plt.xticks(np.arange(0, epochs+1, epochs\/10))\n  plt.title('Training Loss vs. Validation Loss')\n  plt.xlabel('Num of Epochs')\n  plt.ylabel('Loss')\n  plt.legend(['train', 'validation'], loc='best')\n  plt.show()","53aad8e9":"plot_model_history2('InceptionV2', history.history, 10)","f0d79bb1":"# **ResNet pre-trained model**","999c53b7":"**Inception V2**","1ec19388":"# **VGG pre-trained model**","80ce9c8d":"# **Image Preprocessing and augmentation**","1c49e0d7":"# **Densenet169 pre-trained model**\n","305fe9ec":"# **InceptionV3 pre-trained model**"}}