{"cell_type":{"e50aa199":"code","b13a9d0c":"code","17107271":"code","4fcc99f8":"code","bc783451":"code","1081a7f7":"code","bf6cf248":"code","60397bee":"code","80bec999":"code","9977b0de":"code","718bce67":"code","0dc5a70b":"code","5ca69f33":"code","ed3b5eb5":"code","0a0f4602":"code","fbac051c":"code","069bc8f0":"code","6fa5eb02":"code","1754deba":"code","2efbe4b0":"code","307fe91e":"code","891c5f76":"code","2b8dc73a":"code","d012c355":"code","5b4e55b6":"code","3a7dba2b":"code","11ffec66":"code","1d5fb735":"code","2104a00d":"code","1e2635ca":"code","4f1f4707":"code","0cc29058":"code","f26f4184":"code","8be85e97":"markdown","3512a63c":"markdown","476b7972":"markdown","9849b087":"markdown","59253473":"markdown","3166c0e2":"markdown","0b38811a":"markdown","ccf1625b":"markdown","a48f35a4":"markdown","b12ee26b":"markdown","264d7215":"markdown","b72411a6":"markdown","9c98ed15":"markdown","d4067693":"markdown","bf443378":"markdown","a0ad3bff":"markdown","0f20eefb":"markdown","f367f4ce":"markdown","cc9574cc":"markdown","eff82a75":"markdown","d93eb3e2":"markdown","f46ccea2":"markdown","5d87c41c":"markdown","98f1502c":"markdown","a4a50af1":"markdown","6d99fddf":"markdown","7401bbc0":"markdown","0c71c93e":"markdown","3a745887":"markdown","6ffc032d":"markdown","3167a21b":"markdown","bc9fd594":"markdown"},"source":{"e50aa199":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.model_selection import RandomizedSearchCV\n\nsns.set_style('darkgrid')\nplt.style.use(\"dark_background\") \n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)","b13a9d0c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape","17107271":"trainID = train.Id\ntestID = test.Id\n\ntrain.drop('Id', axis = 1, inplace=True)\ntest.drop('Id', axis = 1, inplace=True)\n\ntrain.shape","4fcc99f8":"plt.figure(figsize=(14,10))\n\nplt.subplot(2,2,1)\ng = sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\nplt.title('Before removing outliers')\n\ntrain = train[((train.GrLivArea<4000) | (train.SalePrice>200000))]\n\nplt.subplot(2,2,2)\nplt.title('After removing outliers')\ng = sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\n\nplt.subplot(2,2,3)\ng = sns.scatterplot(x = 'OverallQual', y = 'SalePrice', data = train)\nplt.title('Before removing outliers')\n\ntrain = train[((train.OverallQual>5) | (train.SalePrice<200000))]\ntrain = train[((train.OverallQual>=9) | (train.SalePrice<500000))]\n\nplt.subplot(2,2,4)\nplt.title('After removing outliers')\ng = sns.scatterplot(x = 'OverallQual', y = 'SalePrice', data = train)","bc783451":"g = sns.distplot(train.SalePrice, fit = norm)\n(mu, sigma) = norm.fit(train.SalePrice)\nprint('mu = ', mu, ', sigma = ', sigma)\n\nplt.figure()\ng = stats.probplot(train.SalePrice, plot = plt)","1081a7f7":"train.SalePrice = np.log(train.SalePrice)\ng = sns.distplot(train.SalePrice, fit = norm)\n\nplt.figure()\ng = stats.probplot(train.SalePrice, plot = plt)","bf6cf248":"ntrain = train.shape[0]\nntest = test.shape[0]\ny = train.SalePrice.values\nfull_data = pd.concat((train,test)).reset_index(drop=True)\nfull_data.drop('SalePrice', axis = 1, inplace=True)\nfull_data.shape\n\n#concatenate train and test rows","60397bee":"#which features have maximum missing values?\nfull_data.isnull().sum().sort_values(ascending=False).head()","80bec999":"full_data.PoolQC.fillna('None', inplace=True)\nfull_data.MiscFeature.fillna('None', inplace=True)\nfull_data.Alley.fillna('None', inplace=True)\nfull_data.Fence.fillna('None', inplace=True)\nfull_data.FireplaceQu.fillna('None', inplace=True)\nfull_data.LotFrontage = full_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nfor col in ['GarageFinish','GarageQual','GarageCond', 'GarageType','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual']:\n    full_data[col].fillna('None', inplace=True)\nfor col in ['GarageCars','GarageArea','GarageYrBlt','BsmtHalfBath','BsmtFullBath','TotalBsmtSF','BsmtUnfSF','BsmtFinSF1','BsmtFinSF2']:\n    full_data[col].fillna(0, inplace=True)\nfull_data.MasVnrType.fillna('None', inplace=True)\nfull_data.MasVnrArea.fillna(0, inplace=True)\nfull_data.MSZoning.fillna(full_data.MSZoning.mode()[0], inplace=True)\nfull_data.Functional.fillna('Typ', inplace=True) #fill with most common value\nfull_data.drop('Utilities',axis=1,inplace=True) #all values are same, except one, which is in train set\nfull_data.Electrical.fillna('SBrkr', inplace=True)\nfull_data['Exterior1st'].fillna(full_data['Exterior1st'].mode()[0], inplace=True)\nfull_data['Exterior2nd'].fillna(full_data['Exterior2nd'].mode()[0], inplace=True)\nfull_data.KitchenQual.fillna('TA', inplace=True)\nfull_data.SaleType.fillna('WD', inplace=True)\n\nfull_data.isnull().sum().max()","9977b0de":"#plot heatmap of correlation coefficients\nplt.figure(figsize=(10,8))\nsns.heatmap(train.corr(), vmax = 0.8)\n\nprint(train.corr(method = 'spearman')['SalePrice'].abs().nlargest(15).keys())","718bce67":"for col in ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', ]:\n    full_data[col] = pd.Categorical(full_data[col], categories=['None','Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered=True).codes\n\nfor col in ['BsmtFinType1', 'BsmtFinType2']:\n    full_data[col] = pd.Categorical(full_data[col], categories=['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ'], ordered=True).codes\n\nfull_data['Functional'] = pd.Categorical(full_data['Functional'], categories=['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], ordered=True).codes\nfull_data['Fence'] = pd.Categorical(full_data['Fence'], categories=['None','MnWw','GdWo','MnPrv','GdPrv'], ordered=True).codes\nfull_data['BsmtExposure'] = pd.Categorical(full_data['BsmtExposure'], categories=['None','No','Mn','Av','Gd'], ordered=True).codes\nfull_data['GarageFinish'] = pd.Categorical(full_data['GarageFinish'], categories=['None','Unf','RFn','Fin'], ordered=True).codes\nfull_data['LandSlope'] = pd.Categorical(full_data['LandSlope'], categories=['Sev','Mod','Gtl'], ordered=True).codes\nfull_data['LotShape'] = pd.Categorical(full_data['LotShape'], categories=['IR3','IR2','IR1','Reg'], ordered=True).codes\nfull_data['PavedDrive'] = pd.Categorical(full_data['PavedDrive'], categories=['N','P','Y'], ordered=True).codes\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlbl = LabelEncoder()\nfor col in ['Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond']:\n    full_data[col] = lbl.fit_transform(full_data[col])","0dc5a70b":"full_data['TotalSF'] = full_data.TotalBsmtSF + full_data['1stFlrSF'] + full_data['2ndFlrSF']\nfull_data['YearsSinceRemodel'] = full_data.YrSold.astype(int) - full_data.YearRemodAdd.astype(int)\nfull_data['TotalHomeQuality'] = full_data.OverallQual + full_data.OverallCond","5ca69f33":"#select numeric features\nnumeric_feats = full_data.loc[:,full_data.dtypes != object].columns \n#select features with high skew\nskewed_feats = full_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed_feats = skewed_feats[skewed_feats > 0.75]\n\n#log(1+x) applied to all columns\nfor col in skewed_feats.keys().to_list():\n    full_data[col] = np.log1p(full_data[col])","ed3b5eb5":"def addCrossSquared(temp, plist):\n    m = temp.shape[1]\n    for i in range(len(plist)-1):\n        for j in range(i+1,len(plist)):\n            temp = temp.assign(newcol=pd.Series(temp[plist[i]]*temp[plist[j]]).values)   \n            temp.columns.values[m] = plist[i] + '*' + plist[j]\n            m += 1\n    return temp","0a0f4602":"poly_features_list = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF','1stFlrSF']\nfull_data = addCrossSquared(full_data, poly_features_list)","fbac051c":"full_data = pd.get_dummies(full_data)\nfull_data.shape","069bc8f0":"full_data['MSZoning_C (all)'].value_counts()\nfull_data.drop('MSZoning_C (all)', axis=1, inplace=True)","6fa5eb02":"train = full_data[:ntrain]\ntest = full_data[ntrain:]","1754deba":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, Ridge, Lasso","2efbe4b0":"scaler = RobustScaler() \nX_train = scaler.fit_transform(train)\nX_test = scaler.transform(test)","307fe91e":"def rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y ,scoring = 'neg_mean_squared_error', cv=5))\n    return rmse","891c5f76":"ridge = Ridge()\nalphas = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100,200,300,500,1000]\nerror_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\nsns.lineplot(x = alphas, y = error_ridge, marker = 'o')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.xlim(0,100)","2b8dc73a":"ridgecv = RidgeCV(cv = 5).fit(X_train,y)\nrmse_cv(ridgecv).mean()","d012c355":"lassocv = LassoCV(cv=5).fit(X_train,y)\nprint(\"Number of values searched through: \", lassocv.alphas_.shape[0])\nprint(\"Selected alpha: \", lassocv.alpha_)\nprint(\"Smallest alpha tried: \", lassocv.alphas_[-1])","5b4e55b6":"alphas = np.linspace(0.00001, 0.055, 50)\nlassocv = LassoCV(cv = 5, alphas = alphas).fit(X_train,y)\nprint(\"Value of alpha selected: \", lassocv.alpha_)\nrmse_cv(lassocv).mean()","3a7dba2b":"coeffs = pd.Series(lassocv.coef_, index = train.columns)\nimp_coeffs = pd.concat([coeffs.sort_values().head(10),coeffs.sort_values().tail(10)])\nsns.barplot(x = imp_coeffs.values, y = imp_coeffs.keys())\nplt.xlabel('Coefficient')\nplt.ylabel('Feature')","11ffec66":"alphas = np.linspace(0.002, 0.0002, 50)\nelasticcv = ElasticNetCV(cv = 5, l1_ratio = 0.7, alphas = alphas).fit(X_train,y)\nrmse_cv(elasticcv).mean()","1d5fb735":"import xgboost as xgb","2104a00d":"'''brute_xgb = xgb.XGBRegressor()\n\nmax_depth = [3,4,5,6,8,10]\nmin_child_weight = [3,4,5,6,7,9]\ngamma = [i\/10.0 for i in range(0,2)]\nsubsample = [i\/100.0 for i in range(60,90)]\ncolsample_bytree = [i\/100.0 for i in range(60,90)]\nbase_score = [0.3, 0.4, 0.45, 0.5, 0.55, 0.6]\nlearning_rate = [0.01,0.05,0.1,0.2,0.3,0.5]\nn_estimators = [200, 300, 400, 500, 600, 700, 800 ,900, 1000, 1200, 1400]\n\nparams = {\n    'max_depth': max_depth,\n    'gamma': gamma,\n    'subsample': subsample,\n    'colsample_bytree': colsample_bytree,\n    'min_child_weight': min_child_weight,\n    'base_score': base_score,\n    'learning_rate': learning_rate,\n    'n_estimators': n_estimators\n}\n\nsearch = RandomizedSearchCV(estimator = brute_xgb,\n                                   param_distributions = params, \n                                   cv = 5,\n                                   n_iter = 300,\n                                   scoring = 'neg_mean_squared_error',\n                                   n_jobs = -1,\n                                   verbose = 5,\n                                   random_state = 23 )\n\nsearch.fit(X_train,y)\n'''\n#done, result copied and re-initialised. no need to do again.","1e2635ca":"#search.best_estimator_","4f1f4707":"best_xgb = xgb.XGBRegressor(base_score=0.45, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.62, gamma=0.0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.01, max_delta_step=0, max_depth=4,\n             min_child_weight=3, monotone_constraints=None,\n             n_estimators=3000, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=0.61, tree_method=None,\n             validate_parameters=False, verbosity=None).fit(X_train,y)","0cc29058":"from sklearn.ensemble import VotingRegressor\n\nvoting = VotingRegressor([('Lasso',lassocv), ('Ridge',ridgecv), ('ElasticNet', elasticcv), ('XGBoost', best_xgb)])\nvoting = voting.fit(X_train,y)\n\nrmse_cv(voting).mean()","f26f4184":"model = voting\ny_pred = model.predict(X_test)\ny_pred = np.exp(y_pred)\noutput = pd.DataFrame({'Id': testID, 'SalePrice': y_pred})\noutput.to_csv('submission.csv',index=False)","8be85e97":"# Data cleaning and feature engineering:\n1. Fixing missing values\n2. Search for correlations between features and target feature\n3. Label encoding ordinal categorical features\n4. Creating new features\n5. Transformation of skewed features\n6. One-hot encoding of nominal categorical features","3512a63c":"### One-hot encoding of nominal categorical variables \nNominal variables are categorical variables that dont have order. One-hot encoding is the appropriate encoding for this type, since no value is given a higher encoding value than any other.\n\nThis increases the number of columns by a lot.","476b7972":"### XGBoost","9849b087":"### Fixing missing values","59253473":"### Voting Regressor\nAn ensemble method - it fits multiple models and takes the average of their predictions. It works by playing to the strengths of it's models - where one model is weak, the results from the other models cancel it out. This produced the best result out of all things tried here.","3166c0e2":"### Miscellaneous\nMost values of the following column are the same, so the models don't learn much from it, so it is dropped.","0b38811a":"### ElasticNetCV (same procedure as LassoCV)","ccf1625b":"### Interesting!\n* Being located in the Crawford neighborhood has a high positive impact on sale price.\n* OverallQual, GrLivArea, TotalSF, SaleType_New are the most important features (makes sense!)\n* YearsSinceRemodel (a feature we created) and SaleCondition_Abnormal have a large negative impact on price (makes sense again!)","a48f35a4":"### Search for correlations between features and SalePrice\nUse Spearman coefficient to hopefully capture non-linear dependencies","b12ee26b":"### Define cross validation root mean square error\nRMSE is the evaluation metric used in the competition.","264d7215":"### Outliers\nRemoving a few outliers from GrLivArea and OverallQual columns","b72411a6":"### Delete ID row","9c98ed15":"#### In the following cell, a large Randomized Search is run to find the best hyperparameters for the XGBoost model.\n* The performance of XGBoost is highly dependent on the choice of hyperparameters.\n* It is not so straightforward to tune.\n* RandomizedSearchCV tries out random combinations of hyperparameters and computes K-Fold cross validation accuracy.\n* It is often better than GridSearchCV(which searches all possible combinations), since it achieves similar accuracy in less time.\n* The process takes an hour to run - uncomment it if you wish.\n\n#### The process is run twice - first with a coarse hyperparameter grid, and then with a dense grid centered around the results of the first search.","d4067693":"### Label encoding of ordinal categorical variables\nOrdinal variables have natural, ordered categories and the distances between the categories is not known.\n\nMost of the variables have been manually encoded by consulting the **dataset documentation**. The rest have been done with sklearn's LabelEncoder. ","bf443378":"### Log transformation of skewed features","a0ad3bff":"# Conclusion\n\nThese steps achieved a score of 0.11697, which will get you to the top 500 on the leaderboard.\n\n### Things tried that didn't work:\n* Creating binary features indicating the presence of certain other features - possibly led to overfitting.\n* Stacking - another popular ensembling technique - perhaps I didn't implement it correctly.\n* Neural networks - gave poor results.\n* Using CountEncoding instead of LabelEncoding\n\n### Thanks for reading :)\n\n### Upvote and leave your feedback","0f20eefb":"### Finding the best hyperparameter value for Ridge regressor using K-Fold cross validation\nDefine a large range of regularization values to search through. Evaluate RMSE using K-Fold cross validation (our helper function does this) and pick the regularization value that gives lowest value of RMSE.","f367f4ce":"### Use sklearn's Robust Scaler to scale values.\nRobust Scaler worked better than Standard Scaler","cc9574cc":"### Create second degree cross-polynomial features\n\nThe most important features were selected from the \"Search for correlations between features and SalePrice\" section. Second degree cross-polynomial features are obtained by taking pairwise product of these selected features.\n\nDoing this resulted in a good improvement in performance.","eff82a75":"Recover train and test data.","d93eb3e2":"### Check the coefficients of the Lasso model to see which features it deems important","f46ccea2":"### But sometimes it doesn't work as intended.\nLassoCV automatically finds the best alpha for a Lasso regression model. ","5d87c41c":"Copy the best model parameters returned by the search and re-initialize it.","98f1502c":"alpha = 10 gives lowest RMSE","a4a50af1":"### Create some new features","6d99fddf":"### Transforming the target variable\nSkewness of target variable distribution can be reduced by log transformation. Linear regression models work better with normal distributions","7401bbc0":"### RidgeCV - find hyperparameter automatically\nRidgeCV does the same thing as the previous section. In one line, it returns the best model.","0c71c93e":"#### Process followed here:\nRead dataset documentation. For some features, a missing value indicates the absense of that feature, so the missing values are replaced with 'None' or 0 (zero) depending on the type of feature.\n\nFor other features with small number of missing values, replace with mean or median value.","3a745887":"# Modelling","6ffc032d":"### This is a quick guide for anyone looking to get started with this problem. It is a great starting point to learn the basics of solving a machine learning problem and I'm glad I went through it! \n\n### This is \u00a0my first Kaggle kernel. If you enjoyed it, please **upvote** and leave your feedback :)","3167a21b":"#### The smallest value tried is the one selected, so the best value may be smaller. Therefore, it is important to check this and specify a range manually.","bc9fd594":"### Code to generate submission"}}