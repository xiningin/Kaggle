{"cell_type":{"14319dac":"code","74763000":"code","40945fe7":"code","d7b8f3c7":"code","759d5eeb":"code","a89f39bf":"code","bfb43e33":"code","60ddc761":"code","197d1259":"code","870c213d":"code","ddee459d":"code","ecf1085d":"code","6be43be0":"code","08e65dd8":"code","be9815d7":"code","02c7dfdb":"code","93c9ffdf":"code","fcdcd900":"code","8165e519":"code","0ccb4d26":"code","202b5010":"code","10a3f238":"markdown","e4ee215e":"markdown","f222534c":"markdown"},"source":{"14319dac":"# insall\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport re\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm_notebook\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy import stats\nfrom sklearn.model_selection import KFold\nimport os,urllib,glob,tarfile\nfrom transformers import BertJapaneseTokenizer, BertModel, BertConfig","74763000":"!pip install mecab-python3==0.996.2 fugashi ipadic","40945fe7":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntrain_df = pd.read_csv(\"..\/input\/japanesereviewratingprediction\/yahoo_review_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/japanesereviewratingprediction\/yahoo_review_test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/japanesereviewratingprediction\/sample_submission.csv\")","d7b8f3c7":"train_df.head(5)","759d5eeb":"# Neutral\u306f\u4eca\u56de\u4f7f\u7528\u3057\u306a\u3044\ntrain_df = train_df[train_df[\"Label\"]!=-1]","a89f39bf":"# tokenizer\u306e\u5b9a\u7fa9\u3068vocab\u3092\u4f5c\u6210\ntokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku\/bert-base-japanese-whole-word-masking\")\nfor i,item in enumerate(tokenizer.vocab.items()):\n    print(item)\n    if i > 20:\n        break","bfb43e33":"pad = tokenizer.vocab[\"[PAD]\"]\nmax_lengths=512","60ddc761":"# tokenizer\u306e\u5206\u5272\u4f8b\ntext = \"\u4eca\u65e5\u306f\u6771\u4eac\u30c7\u30a3\u30ba\u30cb\u30fc\u30e9\u30f3\u30c9\u306b\u884c\u3063\u305f\"\ntokenizer.tokenize(text)","197d1259":"token = [\"[CLS]\"]+tokenizer.tokenize(text)[:max_lengths-2]+[\"[SEP]\"]\ninput_id = tokenizer.convert_tokens_to_ids(token)\nsegment_id = [0]*max_lengths\nattention_mask = [1]*len(input_id)+[0]*(max_lengths - len(input_id))\ninput_id = input_id+[pad]*(max_lengths-len(input_id))","870c213d":"print(\"text:\",text)\nprint(\"token:\",token)\nprint(\"input_id:\",input_id[:15],\"len:\",len(input_id))\nprint(\"segment_id:\",segment_id[:15],\"len:\",len(segment_id))\nprint(\"attention_mask:\",attention_mask[:15],\"len:\",len(attention_mask))","ddee459d":"# \u4e0a\u8a18\u306einput_data\u306e\u4f5c\u6210\u65b9\u6cd5\u3092\u3082\u3068\u306bdataset\u5316\nclass ReviewDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels=[]):\n        self.input_ids, self.segment_ids, self.attention_masks = [],[],[]\n        for text in tqdm(texts):\n            token = [\"[CLS]\"]+tokenizer.tokenize(text)[:max_lengths-2]+[\"[SEP]\"]\n            input_id = tokenizer.convert_tokens_to_ids(token)\n            segment_id = [0]*max_lengths\n            attention_mask = [1]*len(input_id)+[0]*(max_lengths - len(input_id))\n            input_id = input_id+[pad]*(max_lengths-len(input_id))\n            self.input_ids.append(input_id)\n            self.segment_ids.append(segment_id)\n            self.attention_masks.append(attention_mask)\n        self.input_ids = np.array(self.input_ids)\n        self.segment_ids = np.array(self.segment_ids)\n        self.attention_masks = np.array(self.attention_masks)\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        if len(self.labels):\n            return self.input_ids[idx], self.segment_ids[idx], self.attention_masks[idx], self.labels[idx]\n        else:\n            return self.input_ids[idx], self.segment_ids[idx], self.attention_masks[idx]","ecf1085d":"# dataset\u3068dataloader\u306e\u4f5c\u6210\nbatch_size=8\nX,y = train_df[\"Body\"].values, train_df[\"Label\"].values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=100,stratify=y)\ntrain_ds = ReviewDataset(texts=X_train, labels=y_train)\ntrain_dl = torch.utils.data.DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True)  \nval_ds = ReviewDataset(texts=X_val, labels=y_val)\nval_dl = torch.utils.data.DataLoader(\n    val_ds, batch_size=batch_size, shuffle=False)","6be43be0":"# Bert\u306e\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3092\u4f7f\u7528\u3057\u30662\u5024\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u4f5c\u6210\n\nclass BertClassification(nn.Module):\n    def __init__(self, bert):\n        super(BertClassification, self).__init__()\n\n        # BERT\u30e2\u30b8\u30e5\u30fc\u30eb\n        self.bert = bert  # BERT\u30e2\u30c7\u30eb\n\n        self.cls = nn.Linear(in_features=768, out_features=2)\n\n        # \u91cd\u307f\u521d\u671f\u5316\u51e6\u7406\n        nn.init.normal_(self.cls.weight, std=0.02)\n        nn.init.normal_(self.cls.bias, 0)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n\n          # BERT\u306e\u57fa\u672c\u30e2\u30c7\u30eb\u90e8\u5206\u306e\u9806\u4f1d\u642c\n          # \u9806\u4f1d\u642c\u3055\u305b\u308b\n        pooled_output = self.bert(\n                  input_ids, token_type_ids, attention_mask)[1]\n\n        # \u5165\u529b\u6587\u7ae0\u306e1\u5358\u8a9e\u76ee[CLS]\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u7528\u3057\u3066\u3001\u30dd\u30b8\u30fb\u30cd\u30ac\u3092\u5206\u985e\u3057\u307e\u3059\n        pooled_output = pooled_output.view(-1, 768)  # size\u3092[batch_size, hidden_size\u306b\u5909\u63db\n        out = self.cls(pooled_output)\n\n        return out","08e65dd8":"cuda = torch.cuda.is_available()\n# \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\nbert = BertModel.from_pretrained(\"cl-tohoku\/bert-base-japanese-whole-word-masking\")\nmodel =  BertClassification(bert)\n# \u9ad8\u901f\u5316\ntorch.backends.cudnn.benchmark = True\nif cuda:\n    model.cuda()\n# optimizer\u306e\u8a2d\u5b9a\noptimizer = optim.Adam(model.parameters(),lr = 4e-4 ,betas=(0.9, 0.999))\n\n# \u640d\u5931\u95a2\u6570\u306e\u8a2d\u5b9a\ncriterion = nn.CrossEntropyLoss()","be9815d7":"# \u6700\u5f8c\u4ee5\u5916\u306eBertLayer\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u56fa\u5b9a(\u4eca\u56de\u306f\u901f\u3055\u306e\u305f\u3081\u306b\u3057\u3066\u3044\u308b\u3060\u3051\u3002\u7cbe\u5ea6\u3092\u4e0a\u3052\u308b\u306a\u3089\u56fa\u5b9a\u305b\u305a\u306b\u5b9f\u884c\u3059\u308b\u3079\u304d)\nfor param in model.bert.encoder.layer[:-1].parameters():\n    param.requires_grad = False","02c7dfdb":"## \u5b66\u7fd2\u3057\u307e\u3059\u3002\nepochs = 2\nfor epoch in range(epochs):\n    total_loss = 0\n    accuracy = 0\n    model.train()\n  \n\n    print(\"epoch {} start!\".format(epoch+1))\n    # train\n    for iter_num, (input_ids, segment_ids, attention_masks, labels) in tqdm(enumerate(train_dl),total = len(train_dl)):\n        optimizer.zero_grad()\n        if cuda:\n            input_ids, segment_ids, attention_masks, labels =\\\n            input_ids.cuda(), segment_ids.cuda(), attention_masks.cuda(), labels.cuda()\n        # forward(\u9806\u4f1d\u642c)\n        outputs = model(input_ids = input_ids,\n                        token_type_ids = segment_ids,\n                        attention_mask = attention_masks)\n        pred_proba = outputs.softmax(dim=1)[:,1]\n        pred = (pred_proba>=0.5).type(torch.int)\n        loss = criterion(outputs,labels)\n\n        # backward(\u9006\u4f1d\u642c)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        accuracy += (pred==labels).sum().item()\n        #50 iter\u3054\u3068\u306bloss\u3068accuracy\u3092\u8868\u793a\n        if (iter_num+1) % 50 == 0:\n            size = batch_size*(iter_num+1)\n            print(\"{} iter loss:{:.4f} accuracy:{:.4f}\".format(\n            iter_num+1,total_loss\/(iter_num+1),accuracy\/size))\n\n    total_loss \/= len(train_dl)\n    accuracy \/= len(train_ds)\n\n    # validation\n    val_total_loss = 0\n    val_accuracy = 0\n    model.eval()\n    for input_ids, segment_ids, attention_masks, labels in tqdm(val_dl):\n        if cuda:\n            input_ids, segment_ids, attention_masks, labels =\\\n            input_ids.cuda(), segment_ids.cuda(), attention_masks.cuda(), labels.cuda()\n        with torch.no_grad():\n            outputs = model(input_ids = input_ids,\n                      token_type_ids = segment_ids,\n                      attention_mask = attention_masks)\n        pred_proba = outputs.softmax(dim=1)[:,1]\n        pred = (pred_proba>=0.5).type(torch.int)\n        loss = criterion(outputs,labels)\n        val_total_loss += loss.item()\n        val_accuracy += (pred==labels).sum().item()\n\n    val_total_loss \/= len(val_dl)\n    val_accuracy \/= len(val_ds)\n    print(\"epoch{} total loss:{:.4f}, accuracy:{:.4f}, val_total loss:{:.4f}, val_accuracy:{:.4f}\"\\\n        .format(epoch+1,total_loss,accuracy,val_total_loss,val_accuracy))\n#torch.save(model.state_dict(), '.\/model.hdf5')","93c9ffdf":"#model.load_state_dict(torch.load('.\/model.hdf5'))","fcdcd900":"# test datset dataloader\u3092\u4f5c\u6210\u3002\ntest_ds = ReviewDataset(texts=test_df[\"Body\"].values)\ntest_dl = torch.utils.data.DataLoader(\n    test_ds, batch_size=8, shuffle=False)","8165e519":"# \u4e88\u6e2c\u3092\u4ed8\u4e0e\u3002\nlists = []\nfor input_ids, segment_ids, attention_masks in tqdm(test_dl):\n    if cuda:\n        input_ids, segment_ids, attention_masks =\\\n        input_ids.cuda(), segment_ids.cuda(), attention_masks.cuda()\n    outputs = model(input_ids = input_ids,\n                  token_type_ids = segment_ids,\n                  attention_mask = attention_masks)\n    pred_proba = outputs.softmax(dim=1)[:,1]\n    lists += list(pred_proba.cpu().detach().numpy())","0ccb4d26":"test_df[\"Label\"]=lists\nsubmission_df=test_df[[\"Id\",\"Label\"]]\nsubmission_df.to_csv(\"sample_submission.csv\",index=False)","202b5010":"test_df.head(5)","10a3f238":"## Training","e4ee215e":"## Prediction","f222534c":"## Make Input Data"}}