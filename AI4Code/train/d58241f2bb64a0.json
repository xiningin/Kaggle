{"cell_type":{"a6e9d011":"code","a440499b":"code","aaf9f1d8":"code","fdaca085":"code","4b32cbda":"code","55b05074":"code","322cc0ae":"code","3f1983c5":"code","a1ec08dd":"code","5081215e":"code","2cd169ff":"code","7b52b216":"code","75dcd6d9":"code","38767540":"code","e75cc83d":"code","93b35340":"code","eba517f0":"code","5fa95a4b":"code","a9279e62":"code","04b54e68":"markdown","e22508b9":"markdown","a73f2582":"markdown","f98ea305":"markdown","3fadb73d":"markdown","1ae3779b":"markdown","cecfc57e":"markdown","793052a8":"markdown","d643013d":"markdown","77140040":"markdown","edb98464":"markdown","eb023219":"markdown","80cd6ad1":"markdown","61404c79":"markdown","cb1ab768":"markdown","ccdd16d4":"markdown","43b5e5ee":"markdown","a3f39811":"markdown"},"source":{"a6e9d011":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns    \nimport math\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import  LabelEncoder\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, BayesianRidge, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain.columns","a440499b":"train['SalePrice'].describe()","aaf9f1d8":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","fdaca085":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","4b32cbda":"train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index, inplace = True)","55b05074":"fig, ax = plt.subplots()\nax.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","322cc0ae":"sns.distplot(train['SalePrice'], fit = norm);","3f1983c5":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice'], fit = norm);","a1ec08dd":"y = train['SalePrice'].reset_index(drop = True)\ntrain.drop(['SalePrice'], axis = 1,  inplace = True)\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\ndata = pd.concat([train, test], sort = True).reset_index(drop = True)","5081215e":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'MissingRatio'])\nmissing_data = missing_data[missing_data['MissingRatio'] > 0.0]\nmissing_data.head(10)","2cd169ff":"data.drop(['PoolQC',  'MiscFeature', 'Alley',  'Fence', 'FireplaceQu'], axis=1, inplace = True)","7b52b216":"data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nattributes = ['Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities']\nfor attribute in attributes:\n    data[attribute] = data[attribute].fillna(data[attribute].mode()[0])\n# For categorical \nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType']:\n    data[col] = data[col].fillna('None')\n# For numerical \nfor col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n            'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']:\n    data[col] = data[col].fillna(0)","75dcd6d9":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'MissingRatio'])\nmissing_data = missing_data[missing_data['MissingRatio'] > 0.0]\n\nmissing_data","38767540":"numerics = data.dtypes[data.dtypes != \"object\"].index\n\ndata.loc[data['TotalBsmtSF'] > 0, 'TotalBsmtSF'] = boxcox1p(data['TotalBsmtSF'], 0.15)\n\nskewness = data[numerics].apply(lambda x: skew(x))\nskew_index = skewness[abs(skewness) > 0.5].index\nskewness = skewness[skew_index].sort_values(ascending = False)\nfor idx in skew_index:\n    data[idx] = boxcox1p(data[idx], 0.15)","e75cc83d":"data['hasGarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasBsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasPool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasFireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)","93b35340":"data = pd.get_dummies(data).reset_index(drop=True)\n\nfeatures = data.keys()\ndata = data.drop(data.loc[:, (data == 0).sum() >= (data.shape[0] * 0.99)], axis = 1)\ndata = data.drop(data.loc[:, (data == 1).sum() >= (data.shape[0] * 0.99)], axis = 1)\nremove = [feat for feat in features if feat not in data.keys()]\n\nprint(\"Del %2d features\"%(len(remove)))","eba517f0":"data = pd.DataFrame(RobustScaler().fit_transform(data))","5fa95a4b":"data_train = np.array(data[:len(train)])\ndata_test = np.array(data[len(train):])\n\nkfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, x = data_train):\n    rmse = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv = kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter = 1e7, alphas = alphas2, random_state = 42, cv = kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter = 1e7, alphas = e_alphas, cv = kfolds, l1_ratio = e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C = 20, epsilon = 0.008, gamma = 0.0003))\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, \n        min_samples_split=10, loss='huber', random_state =42)\nxgb = XGBRegressor(learning_rate=0.01,n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, \n        colsample_bytree=0.7, objective ='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nlgb = LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=5000, max_bin=200, \n        bagging_fraction=0.75, bagging_freq=5, bagging_seed=7, feature_fraction=0.2, feature_fraction_seed=7, verbose=-1)\n                \nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgb, lgb), meta_regressor = xgb, use_features_in_secondary = True)\n\nstack_gen_model = stack_gen.fit(data_train, y)\nelastic_model = elasticnet.fit(data_train, y)\nlasso_model = lasso.fit(data_train, y)\nridge_model = ridge.fit(data_train, y)\nsvr_model = svr.fit(data_train, y)\ngbr_model = gbr.fit(data_train, y)\nxgb_model = xgb.fit(data_train, y)\nlgb_model = lgb.fit(data_train, y)","a9279e62":"def models_predict(x):\n    return ((0.1 * ridge_model.predict(x)) + \\\n            (0.05 * lasso_model.predict(x)) + \\\n            (0.1 * elastic_model.predict(x)) + \\\n            (0.1 * svr_model.predict(x)) + \\\n            (0.1 * gbr_model.predict(x)) + \\\n            (0.15 * xgb_model.predict(x)) + \\\n            (0.1 * lgb_model.predict(x)) + \\\n            (0.3 * stack_gen_model.predict(x)))\n\npre =  np.floor(np.expm1(models_predict(data_test)))\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = pre\nsubmission.to_csv(\"submission.csv\", index=False)","04b54e68":"The ' SalePrice ' corrected more normally distributed now.\n\n\n### 2.  Features Engineering","e22508b9":"As we can see in the image, ' SalePrice ' and ' GrLivArea ' have a good linear relationship, but they include some outliers values about large area with low price at bottom right. Therefor, we delete them.\n","a73f2582":"According to heat map, here are some variables more correlated with ' SalePrice ' :\n\n- OverallQual\n- GrLivArea\n- GarageCars\n- GarageArea\n- TotalBsmtSF\n\n\n' GrLivArea ' and ' TotalBsmtSF ' are numerical variables which we can check relationship first.\n\n#### GrLivArea","f98ea305":"## Training Model","3fadb73d":"#### 1.1.  Relationship\nHeatmap is a good way to get a clear overview of our variable's relationships.","1ae3779b":"#### TotalBsmtSF","cecfc57e":"When a property has large missing ratio, it can also considered as having lower or no impact on final forecast. So a property should be delete if it has missing ratio more than 20%.","793052a8":"Other missing values :","d643013d":"#### 2.1. Missing Data\n","77140040":"#### 2.2. Skewness","edb98464":"#### 1.2.  Distibution","eb023219":"#### 2.4. Normalize ","80cd6ad1":"## Make Submission","61404c79":"## Data Processing\n### 1. Target\n' SalePrice ' is our target which we want to predict, so the first thing is to check this property in our dataset. ","cb1ab768":"Finally:","ccdd16d4":"In many classical analytical methods, data is required to follow or approximate a normal distribution. Apparently,'SalePrice' is not. so for our right skewed taget vaiable, it need to transorm its distribution to make it more normally.","43b5e5ee":"# House Prices - A brief introduction\n\n\n## Setup","a3f39811":"#### 2.3. Add and Del"}}