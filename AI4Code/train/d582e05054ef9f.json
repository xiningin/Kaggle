{"cell_type":{"57e8c20e":"code","6459feba":"code","9a2146b5":"code","8e6ff2a5":"code","bb01c84f":"code","18f40a0c":"code","75afeaa7":"code","0f4ac30e":"code","c37f2382":"code","2e4aec71":"code","e95e194f":"code","0652ac49":"code","8b5dfd60":"code","0eaa96ad":"code","9a255cc6":"code","3fcf38cb":"code","4053295b":"code","29c2e464":"code","1f4b7bf7":"code","fa6da14f":"code","c18ba328":"code","437eaba8":"code","24f210c9":"code","2dc13498":"code","6bf8029b":"code","c0a4fde2":"code","f85f8848":"code","7ff763c4":"markdown","81622e5f":"markdown","dc6c9efa":"markdown","2e658380":"markdown","d34ec5e4":"markdown","a099da6d":"markdown","b3cad520":"markdown","6ee37ef6":"markdown","49f1456f":"markdown","a7ffb0f0":"markdown","5542d8ef":"markdown","fcccac42":"markdown","b36216c1":"markdown","a40a6904":"markdown","251046de":"markdown","c11d0381":"markdown","475250ac":"markdown","cdcdfc82":"markdown"},"source":{"57e8c20e":"import pandas as pd             # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np              # linear algebra\nimport matplotlib.pyplot as plt # data visualisation\n%matplotlib inline              \nimport seaborn as sns           # data visualisation","6459feba":"# Path of the file to read\ntrain_file_path = '..\/input\/train.csv'\ntest_file_path  = '..\/input\/test.csv' \n\n# Fill in the line below to read the file into a variable home_data\ntrain = pd.read_csv(train_file_path, index_col='Id')\ntest  = pd.read_csv(test_file_path, index_col='Id')","9a2146b5":"train.head()","8e6ff2a5":"test.head()","bb01c84f":"mystring = \"The train data contains {0} rows (observations) and {1} columns (variables).\"\nprint(mystring.format(train.shape[0], train.shape[1]))\n\nmystring = \"The test data contains {0} rows (observations) and {1} columns (variables).\"\nprint(mystring.format(test.shape[0], test.shape[1]))","18f40a0c":"print(train['SalePrice'].describe())\nprint(\"median  \", train['SalePrice'].median())\nprint(\"Number of missings:\", train['SalePrice'].isna().sum())","75afeaa7":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","0f4ac30e":"# Set the width and height of the two plots combined\nf, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n\n# make distribution plot\nsns.distplot(a=train['SalePrice'], ax=axes[0])\nsns.distplot(a = np.log(train['SalePrice']), ax=axes[1])","c37f2382":"#skewness and kurtosis\nprint(\"Skewness before log transformation: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis before log transformation: %f\" % train['SalePrice'].kurt())\n\nprint(\"Skewness after log transformation: %f\" % np.log(train['SalePrice']).skew())\nprint(\"Kurtosis after log transformation: %f\" % np.log(train['SalePrice']).kurt())","2e4aec71":"# get a list of column names\nvar_names = list(train.drop(columns = ['SalePrice']).columns) \n\nprint(\"There are {0} featues that can be used to predict the Saleprice.\".format(len(var_names)))","e95e194f":"train.drop(columns = ['SalePrice']).info()","0652ac49":"# Here I assume that columns containing text values (dtypes == 'object') are categorical features\ns = (train.dtypes == 'object')\ncat_vars = list(s[s].index)\n# I manually checked for other categorical features containing numeric values\nother_cat_vars = ['MSSubClass', 'OverallQual', 'OverallCond', ]\ncategorical_vars = cat_vars + other_cat_vars","8b5dfd60":"# make empty dictionary\ndata = {}\n\n# get number of unique values per variable\nfor i in categorical_vars:\n    variables = i\n    n_unique_values = len(train[i].unique().tolist())\n    data[i] = (variables, n_unique_values)\n    \n# go from dictionary to pandas dataframe    \ndf_cat_vars = pd.DataFrame.from_dict(data, orient='index', columns=['variables','n_unique_values'])\n\n# sort values by n_unique values\nordered_df = df_cat_vars.sort_values(by = ['n_unique_values'], ascending=True)\n\n# make horizontal barchart to visualize results\nplt.figure(figsize=(10,10))                                       #set size of figure to 10x10 inches\nplt.barh(ordered_df.variables, ordered_df.n_unique_values)        #plot barchart\nplt.xlabel('number of unique values')                             #name x-label\nplt.title('The number of unique values per categorical variable') #give plot a title\nplt.show()\n","0eaa96ad":"# get number of missings per column in train and test set\nmistrain = train.isnull().sum().to_frame()\nmistest = test.isnull().sum().to_frame()\n\n# give column the name nMissings\nmistrain.columns = ['nMissings']\nmistest.columns = ['nMissings']\n\n# make new columns that gives information about missing percentage\nmistrain['percMissing'] = mistrain['nMissings']\/1460\nmistest['percMissing'] = mistest['nMissings']\/1459\n\n# select only rows with nMissings >= 1\nmistrain = mistrain[mistrain.nMissings >= 1]\nmistest = mistest[mistest.nMissings >= 1]\n\n# sort values by nMissings values\nordered_df_train = mistrain.sort_values(by = ['nMissings'], ascending=False)\nordered_df_test = mistest.sort_values(by = ['nMissings'], ascending=False)","9a255cc6":"# set figure size\nplt.figure(figsize=(4,8))          \n\n# add title\nplt.title('Missings values in the train set')  \n\n# add lable for horizontal axis\nplt.xlabel('Percentage of missing values')                            \n\n# make barplot\nsns.barplot(y = ordered_df_train.index, x = ordered_df_train['percMissing'])   ","3fcf38cb":"# set figure size\nplt.figure(figsize=(4,8))          \n\n# add title\nplt.title('Missings values in the test set')  \n\n# add lable for horizontal axis\nplt.xlabel('Percentage of missing values')\n\n# make barplot\nsns.barplot(y=ordered_df_test.index, x=ordered_df_test['percMissing'])      ","4053295b":"# select target variable SalePrice and features related to size\nmy_vars = ['SalePrice', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\n\n# get the correlation coefficients between these features.\ncorr = train[my_vars].corr()","29c2e464":"# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Correlation between each feature related to size and SalePrice\")\n\n# Heatmap showing average arrival delay for each airline by month\nsns.heatmap(data=corr, annot=True)","1f4b7bf7":"# set zeros to Nan, and get correlation coefficients.\nmydata = train[my_vars].replace(0, np.nan)","fa6da14f":"# check how many missings we have per feature\nmissings = mydata.isnull().sum().to_frame()\n\n# give column the name nMissings\nmissings.columns = ['nMissings']\n\n# make new columns that gives information about missing percentage\nmissings['percMissing'] = missings['nMissings']\/1460\n\n# sort values by nMissings values\nordered_missings = missings.sort_values(by = ['nMissings'], ascending=False)\nordered_missings","c18ba328":"corr = mydata.corr()\n\n# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Correlation between each feature related to size and SalePrice\")\n\n# Heatmap showing correlation coefficients\nsns.heatmap(data=corr, annot=True)","437eaba8":"my_vars = ['1stFlrSF', 'LotArea', 'GrLivArea', 'SalePrice']\nmydata = train[my_vars].replace(0, np.nan)\nfor i in my_vars:\n    print(i)\n    print(\"Skewness before log transformation: %f\" % mydata[i].skew())\n    print(\"Kurtosis before log transformation: %f\" % mydata[i].kurt())","24f210c9":"for i in my_vars:\n    print(i)\n    print(\"Skewness after log transformation: %f\" % np.log(mydata[i]).skew())\n    print(\"Kurtosis after log transformation: %f\" % np.log(mydata[i]).kurt())","2dc13498":"corr =np.log(mydata[my_vars]).corr()\n\n# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Correlation between each feature related to size and SalePrice\")\n\n# Heatmap showing correlation coefficients\nsns.heatmap(data=corr, annot=True)","6bf8029b":"train.head()","c0a4fde2":"train.MSZoning.unique()\n\n","f85f8848":"# break data into different parts\nrl = train[train.MSZoning == 'RL']\nrm = train[train.MSZoning == 'RM']\nc = train[train.MSZoning == 'C (all)']\nfv = train[train.MSZoning == 'FV']\nrh = train[train.MSZoning == 'RH']\n\n# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Histograms for each species\nsns.distplot(a = np.log(rl['SalePrice']), label=\"Residential Low Density\", kde=False)\nsns.distplot(a = np.log(rm['SalePrice']), label=\"Residential Medium Density\", kde=False)\nsns.distplot(a = np.log(c['SalePrice']), label=\"Commercial\", kde=False)\nsns.distplot(a = np.log(fv['SalePrice']), label=\"Floating Village Residential\", kde=False)\nsns.distplot(a = np.log(rh['SalePrice']), label=\"Residential High Density\", kde=False)\n\n# Add title\nplt.title(\"Histogram of SalePrice, by MSzoning\")\n\n# Force legend to appear\nplt.legend()","7ff763c4":"How about categorical variables","81622e5f":"Here we have the first rows of the train and the test dataset. The variable we have to predict is called *SalePrice* and it is located at the last column in the train dataset. The test dataset does not have a column SalePrice. \n\nThe remaining columns can be used to predict this SalePrice and are present in both the train and test dataset. These features contain numbers or text values, there are also features with missing data (see NaN, which means not a number).","dc6c9efa":"**1 Introduction**\n\nIt is good practice to examine the data first before making predictions on the data. Here I perform an extensive data analysis (EDA) on the data provided for the House Prices Competition. This document contains the following sections:\n1. Importing Libraries\n2. Importing the data\n3. Have a quick look at the data\n4. Investigate the target variable\n5. Investigate the predictors\n6. Check for missings\n7. How are numeric features related to saleprice","2e658380":"**6: Check for missings**\n\nWe already observed that the data contains missing values. This ranges some questions:\n1. How many missings do we have?\n2. Why are they missing?\n3. Are these values missing at random (MAR)? \n\nLets start with the first question, how many missings do we have. It is always a good idea to visualize the number of missings.","d34ec5e4":"**3: Have a peak at the data**","a099da6d":"**1: Importing libraries**","b3cad520":"**2: Importing the data**","6ee37ef6":"The plot above shows that the SalePrice has a right skewed distribution and a high kurtosis. When the log is taken we get a normal distribution. Since a lot of machine learning models assume that they are fed normally distributed data, I think we should give them normally distributed data.","49f1456f":"Some correlations coefficients are changed and other are not. Especially the features with a lot of zero values have changed, since these zero's no longer contribute to the correlation coefficient. We already knew that SalePrice has a non-normal distribution. Lets also check the distribution of the other numeric features that have non-missing values ","a7ffb0f0":"**7: How are numeric features related to SalePrice**\n\nLets check how numeric features are related to SalePrice. We start with the features that represent size. First we calculate the correlation coefficients between all these size related features and SalePrice, next these values are visualized in a heatmap.","5542d8ef":"The number of categories varies from two to 25 different categories. Keep in mind that it might be good idea to reduce the number of categories, especially when a feature has more than 10 categories or if a categorie contains a few obervations. Perhaps categories with only a few observations can combined into a category called remaining.","fcccac42":"**5: Examine the predictors**","b36216c1":"**4: Investigate the target variable SalePrice**\n\nThe target variable SalePrice is the one we have to predict. ","a40a6904":"The avarage saleprice for which a house was sold was 180.921 dollars, with a minimum of 34.900 and a maximum of 755.000 dollars. The median saleprice was 163.000 dollars.","251046de":"From the charts we can observe that there are 19 features with missing values in the train data set and 33 features with missing values in the test set.\n\nThere are four features with 50 percent or more missings in both the train and test set. These features include: PoolQC, MiscFeature, Alley, Fence and FireplaceQC. Lets check these features using information from datadescription. I did this manually. For the features PoolQC, MiscFeature, Alley, Fence, FireplaceQu, the information from the datadescription.txt file tells us that a missing value indicates that the feature is not present in the house. \n\nThe feature LotFrontage has about 20 percent missing values in both the train and test set. Information from the datadescription file tells us that this feature gives information of the length of street connected to the front of the proporty and it is a continous feature.\n\nThe following features have less that 10 percent missing values: GarageType, GarageQual, GarageCond, BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual. The data from the datadescription.txt file tells us that a missing value indicates that the feature is not present in the house.\n\nFeature that need to be checked: LotFrontage, Electrical, MasVnrArea, MasVnrType\n\nThe feature LotFrontage describes how much feet of street is connected to the property. In about 300 observations this value is missing. For the other three features, Electrical, MasVnrArea and MasVnrType only a few observations are missing.\n","c11d0381":"Also here a lot of non-normal distributions. Lets get the skewness and kurtosis after log transformations.","475250ac":"*SalePrice* shows strong correlations with the features *GrLivArea* (0.71), *GarageArea* (0.62), *1stFlSF* (0.61), *TotalBsmtSF* (0.61). MasVnrArea (0.48) shows a moderate correlation with Saleprice . The remaining features show weak () or very weak (PoolArea, ScreenPorch, 3SsnPorch, EnclosedPorch, BsmtFinSF2) correlations with SalePrice.\n\nNote that if a feature is not present a zero is given for the area. Lets set all zero to missing!","cdcdfc82":"There are three two types of features: \n* numerical features (float64, int64), n = 36\n* features containing text values (object), n= 43\nIt is likely that numerical features represent continuous features and features containing text are categorical.\nWe can also observe that the data set contains features with missing values. Features with missing data need to be dropped or imputed since XGBoost cannot handle data with missing values."}}