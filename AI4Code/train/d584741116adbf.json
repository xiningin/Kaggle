{"cell_type":{"a19b9954":"code","cbe57ecd":"code","445bd967":"code","185ea1f0":"code","b4fe5733":"code","e6569f58":"code","6c3766df":"code","c580886c":"code","9fc0afd7":"code","c2f8ece6":"code","64538b7a":"code","1175ca39":"code","d4177117":"code","6fbb4cab":"code","3124c034":"code","0fb82af3":"markdown","8d57a34d":"markdown","5f68ffdb":"markdown","e603950b":"markdown","e04f4469":"markdown","3e058f47":"markdown","192bb3c0":"markdown","8729381f":"markdown","6fc72565":"markdown","4df343cb":"markdown"},"source":{"a19b9954":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nimport plotly.express as px","cbe57ecd":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","445bd967":"model_name = 'jplu\/tf-xlm-roberta-large'\nn_epochs = 10\nmax_len = 80\n\n# Our batch size will depend on number of replicas\nbatch_size = 16 * strategy.num_replicas_in_sync","185ea1f0":"train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')","b4fe5733":"# First load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","e6569f58":"# Convert the text so that we can feed it to `batch_encode_plus`\ntrain_text = train[['premise', 'hypothesis']].values.tolist()\ntest_text = test[['premise', 'hypothesis']].values.tolist()\n\n# Now, we use the tokenizer we loaded to encode the text\ntrain_encoded = tokenizer.batch_encode_plus(\n    train_text,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded = tokenizer.batch_encode_plus(\n    test_text,\n    pad_to_max_length=True,\n    max_length=max_len\n)","6c3766df":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded['input_ids']","c580886c":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","9fc0afd7":"with strategy.scope():\n    # First load the transformer layer\n    transformer_encoder = TFAutoModel.from_pretrained(model_name)\n\n    # This will be the input tokens \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Now, we encode the text using the transformers we just loaded\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Only extract the token used for classification, which is <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # Finally, pass it through a 3-way softmax, since there's 3 possible laels\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","c2f8ece6":"n_steps = len(x_train) \/\/ batch_size\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","64538b7a":"test_preds = model.predict(test_dataset, verbose=1)\nsubmission['prediction'] = test_preds.argmax(axis=1)","1175ca39":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","d4177117":"hist = train_history.history","6fbb4cab":"px.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","3124c034":"px.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['loss', 'val_loss'], \n    title='Model Loss', labels={'x': 'Epoch', 'value': 'Loss'}\n)","0fb82af3":"## Convert to tf.data.Dataset\n\n`tf.data.Dataset` is one of many different ways to define the input to our models. Here, it is a good choice since it is easily compatible with TPUs. Read more about it [in this article](https:\/\/towardsdatascience.com\/how-to-use-dataset-in-tensorflow-c758ef9e4428).","8d57a34d":"## Visualize Training History\n\nWith Plotly Express, this can be done in one function call:","5f68ffdb":"## Encode Training data\n\nNow, we need to encode the training and test data into `tokens`, which are numerical representation of our words. To learn more, [read this](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html).","e603950b":"Train and validation split happens here:","e04f4469":"## Train the model\n\nIt's time to teach our lovely XLM-Roberta how to infer natural language. Notice here we are using `strategy.scope()`. We need to load `transformer_encoder` inside this scope in order to tell Tensorflow that we want our model on the TPUs. Otherwise, it will try to load it in your CPU machine!\n\nXLM-Roberta is one of the best models out there for multilingual classification tasks. Essentially, it is a model that was trained on inherently multilingual text, and used methods that helped it become larger, train longer and on more data! Highly recommend you to read [this blog post by the authors](https:\/\/ai.facebook.com\/blog\/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision\/), as well as the [Huggingface docs](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html) on the subject.","3e058f47":"Unhide below to see the exactly training accuracy and loss after each epoch:","192bb3c0":"## Predict on test set and submit","8729381f":"## Load datasets\n\nJust regular CSV files. Nothing scary here!","6fc72565":"## Define variables\n\nMake sure to keep those variables in mind as you navigate this notebook! They are all placed below so you can easily change and rerun this notebook.\n\nDon't worry about the model right now. We will come back to it later.","4df343cb":"## Setting up the TPUs\n\nThis line is necessary in order to initialize the TPUs. \n\nHere, \"replicas\" simply means number of \"cores\". In the case of GPUs or CPUs, the number of replicas will be 1. [Read this](https:\/\/cloud.google.com\/tpu\/docs\/tpus#replicas) for more information."}}