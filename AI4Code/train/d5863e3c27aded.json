{"cell_type":{"938a3f2f":"code","1e4ba233":"code","7d4b8bfe":"code","26a2b8b0":"code","1c16bc7e":"code","dd772634":"code","fa8386f5":"code","9e66594e":"code","3e67b71c":"code","ad904ef7":"code","59bd037e":"markdown","269f1808":"markdown","0ad22a0b":"markdown","339f9912":"markdown","db6443e3":"markdown","c1fc3003":"markdown","2a33d203":"markdown","619509c4":"markdown","232c4742":"markdown","bf4f8e46":"markdown","e4163f61":"markdown"},"source":{"938a3f2f":"import pandas as pd\nimport numpy as np \nfrom sklearn.datasets import load_files\n\nimport time\n\n# Text cleaning and precprcessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","1e4ba233":"X, y = [], []\nemail = load_files(\"..\/input\/enron-spam\/enron1\")\nX = np.append(X, email.data)\ny = np.append(y, email.target)    ","7d4b8bfe":"df_all = pd.DataFrame(columns=['text', 'target'])\ndf_all['text'] = [x for x in X]\ndf_all['target'] = [t for t in y]","26a2b8b0":"df_all","1c16bc7e":"df_X = df_all.drop(['target'], axis=1)\ndf_y = df_all['target']","dd772634":"stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","fa8386f5":"start_time = time.time()\n\n# Create corpus\ncorpus = []\nfor i in range(0, len(df_X)):\n    # Remove special symbols\n    review = re.sub(r'\\\\r\\\\n', ' ', str(df_X['text'][i]))\n    # Remove all symbols except letters\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    # Replacing all gaps with spaces \n    review = re.sub(r'\\s+', ' ', review)                    \n    # Remove 'b' in the beginning of each text\n    review = re.sub(r'^b\\s+', '', review)       \n\n    review = review.lower()\n    review = review.split()\n    review = [stemmer.stem(word) for word in review if word not in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","9e66594e":"from sklearn.feature_extraction.text import TfidfVectorizer\n#tf = TfidfVectorizer()\n\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()","3e67b71c":"# Splitting data on train and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=9, test_size=0.2)","ad904ef7":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n\nmodel = MultinomialNB().fit(X_train, y_train)\npred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, pred)\nprecision = precision_score(y_test, pred)\nrecall = recall_score(y_test, pred)\nconf_m = confusion_matrix(y_test, pred)\n\nprint(f\"accuracy: %.3f\" %accuracy)\nprint(f\"precision: %.3f\" %precision)\nprint(f\"recall: %.3f\" %recall)\nprint(f\"confusion matrix: \")\nprint(conf_m)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","59bd037e":"With lemmatizer and using Tf-Idf:\n* accuracy: 0.907\n* precision: 0.995\n* recall: 0.686\n* It took 116.6 sec","269f1808":"This dataset have 7 emails, but reading, cleaning and preprocessing all of them take to much time. Using one of them is more than enough.","0ad22a0b":"With stemming and using Tf-Idf:\n* accuracy: 0.905\n* precision: 0.995\n* recall: 0.680\n* It took 116.6 sec","339f9912":"The likelihood of whether an email is spam or ham is a aposterior probability. So let's try bayes models. They usually shows high performance in spam detection","db6443e3":"As seen, Tf-Idf gives us very high preccision, but recall is bad. On the other hand Bag of words demonstrate high results in both cases. I think, that lemmatizer+Bag-of-words is the better solution. It took least of all time, what will become even more noticeable if you increase the size of dataset. It shows the highest accuracy compared to other solutions. Precision and recall are high too.","c1fc3003":"### Let's create Dataframe with text and target feature","2a33d203":"Now we have list of texts, that encoded binary. Using 'decode' is one of possible solutions, but some texts don't allow us to apply decoding. This can be solved by deleting these texts, but because of this, we can lose important information. Instead of this we can do following:\n\n1. We should remove all special symbols.\n1. Remove 'b' in beginning of each text\n1. Replace all gaps (\\t, \\n, \\r, \\f) between words with spaces\n1. Remove all non-letters characters","619509c4":"With stemming and using Bag of words I became this results:\n* accuracy: 0.978\n* precision: 0.961\n* recall: 0.964\n* It took 116.1 sec","232c4742":"I wrote this work during the training, and I will be glad if it turns out to be useful to someone :)    \n\nDataset was loaded from here:\nhttps:\/\/www.kaggle.com\/wanderfj\/enron-spam","bf4f8e46":"**Let's try different combinations of models (Bag of Words or Tf-Idf) with different preprocessing techniques (Stemming or Lemmatization)**","e4163f61":"With lemmatizer and using Bag of words:\n* accuracy: 0.979\n* precision: 0.970\n* recall: 0.957\n* It took 102.9 sec"}}