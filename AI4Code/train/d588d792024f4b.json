{"cell_type":{"f142819c":"code","44c35389":"code","91980d24":"code","9f962cf1":"code","a9aa49e7":"code","063e441a":"code","68e856c3":"code","8146c432":"code","c7c85e37":"code","c2f5df26":"code","f82b4020":"code","84b6260a":"code","129d326a":"code","d1c2d00a":"code","9bd778b6":"code","6322b03d":"code","03ccb4a0":"code","3584c6d2":"code","2d609abb":"code","0e50ba95":"code","1724b6fe":"code","df9c11ef":"code","9d072aa1":"code","fb3f980c":"code","a15c4c2b":"code","63daac9e":"code","c9019236":"code","487af1cd":"code","ae0b9f1a":"code","2644bef8":"code","f8c1ab6d":"code","db734d01":"code","809d91fa":"code","f707fc1c":"code","483d2b14":"markdown","f644c833":"markdown","b3b66f20":"markdown","1ae8f9cb":"markdown","2d18ae38":"markdown","b890d7ac":"markdown","d86bc6a4":"markdown","22300ba3":"markdown","fe243135":"markdown","af4c628e":"markdown","19e5333e":"markdown","95825bbe":"markdown","fa73e984":"markdown","b1c7de15":"markdown","ac99f64d":"markdown"},"source":{"f142819c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44c35389":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics","91980d24":"# reading the input csv file with pandas\ndata = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","9f962cf1":"data.head()","a9aa49e7":"corr = data.drop(['Attrition'], axis=1).corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap='YlGnBu')\nplt.show()","063e441a":"data['Attrition'] = data['Attrition'].apply(lambda row: 1 if row=='Yes' else 0)\ndata['Attrition'].value_counts()","68e856c3":"data.isna().sum()","8146c432":"data.info()","c7c85e37":"data.describe()","c2f5df26":"def groupDistanceFromHome(data):\n    if int(data) >=1 and int(data) <= 5:\n        return 'NearBy'\n    elif int(data) >=6 and int(data) <= 15:\n        return 'MidDistance'\n    else:\n        return 'Far'\n\ndef groupYearsInCurrentRole(data):\n    if int(data) >=0 and int(data) <= 3:\n        return 'short'\n    elif int(data) >3 and int(data) <= 8:\n        return 'medium'\n    else:\n        return 'long'\n    \ndef groupYearsWithCurrManager(data):\n    if int(data) >=0 and int(data) <= 3:\n        return 'short'\n    elif int(data) >3 and int(data) <= 8:\n        return 'medium'\n    else:\n        return 'long'\n\ndef groupYearsSinceLastPromotion(data):\n    if int(data) >=0 and int(data) <= 3:\n        return 'short'\n    elif int(data) >3 and int(data) <= 8:\n        return 'medium'\n    else:\n        return 'long'\n\ndef groupYearsAtCompany(data):\n    if int(data) >=0 and int(data) <= 3:\n        return 'short'\n    elif int(data) >3 and int(data) <= 8:\n        return 'medium'\n    else:\n        return 'long'\n    \ndef groupTotalWorkingYears(data):\n    if int(data) >=0 and int(data) <= 8:\n        return 'short'\n    elif int(data) >8 and int(data) <= 15:\n        return 'medium'\n    else:\n        return 'long'\n\ndef groupPercentSalaryHike_by_rating(data):\n    if int(data) == 3:\n        return 'good'\n    elif int(data) == 4:\n        return 'better'\n    else:\n        return 'best'\n\ndef groupAverageWorkingYearInEachComp(data):\n    if int(data) >= 0 and int(data) <= 3:\n        return 'short'\n    elif int(data) >3 and int(data) <= 8:\n        return 'medium'\n    else:\n        return 'long'\n\ndef getAvgWorkingYearInEachComp(TotalWorkingYears, NumCompaniesWorked):\n    if NumCompaniesWorked == 0:\n        return TotalWorkingYears\n    else:\n        return TotalWorkingYears \/ NumCompaniesWorked","f82b4020":"data['AverageWorkingYearInEachComp'] = data[['TotalWorkingYears', 'NumCompaniesWorked']].apply(lambda row: \n                                        getAvgWorkingYearInEachComp(row.TotalWorkingYears, row.NumCompaniesWorked), axis=1)\ndata['AverageWorkingYearInEachComp'] = data['AverageWorkingYearInEachComp'].astype(int)\ndata['AverageWorkingYearInEachComp'] = data['AverageWorkingYearInEachComp'].apply(lambda row:\n                                        groupAverageWorkingYearInEachComp(row))\ndata['DistanceFromHome'] = data['DistanceFromHome'].apply(lambda row: groupDistanceFromHome(row))\ndata['YearsInCurrentRole'] = data['YearsInCurrentRole'].apply(lambda row: groupYearsInCurrentRole(row))\ndata['YearsWithCurrManager'] = data['YearsWithCurrManager'].apply(lambda row: groupYearsWithCurrManager(row))\ndata['YearsSinceLastPromotion'] = data['YearsSinceLastPromotion'].apply(lambda row: groupYearsSinceLastPromotion(row))\ndata['YearsAtCompany'] = data['YearsAtCompany'].apply(lambda row: groupYearsAtCompany(row))\ndata['TotalWorkingYears'] = data['TotalWorkingYears'].apply(lambda row: groupTotalWorkingYears(row))","84b6260a":"data['MontlyIncomeByAge'] = data['MonthlyIncome'] \/ data['Age']\ndata['MontlyIncomeByAge'] = data['MontlyIncomeByAge'].astype(int)","129d326a":"data = data.drop(['EmployeeNumber', 'EmployeeCount','StandardHours', 'Over18'], axis=1)","d1c2d00a":"corr = data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap='YlGnBu')\nplt.show()","9bd778b6":"cat_columns =[]\nfor col, value in data.drop(['Attrition'], axis=1).iteritems():\n    if value.dtype == 'object':\n        cat_columns.append(col)\nnum_columns = data.drop(['Attrition'], axis=1).columns.difference(cat_columns)","6322b03d":"print(\"categorical columns - %s\" %(cat_columns))\nprint(\"\")\nprint(\"numerical columns - %s\" %(num_columns))","03ccb4a0":"attrition_data = data['Attrition']\ncat_data = data[cat_columns]\nnum_data = data[num_columns]","3584c6d2":"cat_data = pd.get_dummies(cat_data)","2d609abb":"final_data = pd.concat([cat_data, num_data, attrition_data], axis=1)\nfinal_data.head()","0e50ba95":"train_data, test_data = train_test_split(final_data, train_size=0.7, test_size=0.3)","1724b6fe":"y_train = train_data['Attrition']\nX_train = train_data.drop(['Attrition'], axis=1)\ny_test = test_data['Attrition']\nX_test = test_data.drop(['Attrition'], axis=1)","df9c11ef":"scaler = MinMaxScaler()\nX_train_transformed = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)","9d072aa1":"corr_matrix = X_train_transformed.corr()\ncorr_features = set()\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i):\n        if abs(corr_matrix.iloc[i,j]) > 0.8:\n            corr_features.add(corr_matrix.columns[i])\ncorr_features","fb3f980c":"# removing the columns for from the transformed data for which the value is greater than 0.8\nX_train_transformed = X_train_transformed.drop(columns=list(corr_features), axis=1)","a15c4c2b":"log_reg = LogisticRegression()\nrfe = RFECV(log_reg, cv=StratifiedKFold(5), scoring='neg_mean_squared_error', min_features_to_select=5)\nrfe.fit(X_train_transformed, y_train)","63daac9e":"X_train_transformed = X_train_transformed.drop(X_train_transformed.columns[np.where(rfe.support_ == False)], axis=1)","c9019236":"important_cols = pd.DataFrame()\nimportant_cols['Cols'] = X_train_transformed.columns\nimportant_cols['Percent'] = rfe.estimator_.coef_[0]","487af1cd":"important_cols = important_cols.sort_values(by='Percent', ascending=False)\nimportant_cols","ae0b9f1a":"### getting only the first 12 featues\nX_train_10_imp_feature = X_train_transformed[important_cols['Cols'].values[0:12]]","2644bef8":"log_reg.fit(X_train_10_imp_feature, y_train)","f8c1ab6d":"X_test_transform = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","db734d01":"X_test_transform_required_cols = X_test_transform[X_train_10_imp_feature.columns]","809d91fa":"y_test_pred = log_reg.predict(X_test_transform_required_cols)","f707fc1c":"metrics.accuracy_score(y_test_pred, y_test)","483d2b14":"now concatenating the dummyvariables columns with numerical data and depandent variables","f644c833":"<b>Checking if any column data type is not correct<\/b>","b3b66f20":"using the below code snippet i am getting the corr matrix and removing the one of columns for which the absolute value \nof corr is greater than 0.8 ","1ae8f9cb":"<b>Checking if any column contains null values<\/b>","2d18ae38":"<b>Creating dummy variables for the categorical variables<\/b>","b890d7ac":"There are some numerical columns from which we can create categorical variables <br>\n1) <b>DistanceFromHome<\/b><br>\n   distance from office can be convrted to nearby, middistance and far<br>\n2) <b>YearsInCurrentRole<\/b><br>\n   Years in current role can be converted to short, medium and long<br>\nsimilarly there are other columns <b>YearsWithCurrManager<\/b>, <b>YearsSinceLastPromotion<\/b>, <b>YearsAtCompany<\/b> etc<br>","d86bc6a4":"In the upcoming code we will get rid of the idependent variables with multi collinearity.<br>\nColumn Age is highly correlated with columns Job Level, MonthlyIncome, NumCompaniesWorked etc.","22300ba3":"selecting the first 12 features and creating a Logistic Regression model using the same","fe243135":"<b>Getting the descriptive stats of the dataframe<\/b>","af4c628e":"there is imbalanced data since the employees which are not retained are very few","19e5333e":"<b>Applying the LogisticRegression with rfecv to get the 10 top most features that lead to employee attrition<\/b><br>\nLogistic Regression is used for <b>Binary Classification<\/b> and classify the data points to one of the two categories.<br><br>\n\n<b><u>RFECV<\/u><\/b> - Recursive Feature Selection and Cross Validation Selection\n","95825bbe":"Normalizing the numerical values.<br>\n<b>Why normalizing of data is required?<\/b><br>\nIt may happen some of the numerical columns contains outliers. To make our model robut and doesn't get impacted from the outliers we need to normalize the data.<br>\n\nI am using the MinMaxScaler to normalize the numerical columns.","fa73e984":"We need to check if there are some indepandent variables which are highly correlated. For creating better model we need to \nreduce collinearity between indepandent variables.<br\/>\n<b>Why to remove collinearity between the indepandent variables?<\/b><br>\nwhen variables are highly correlated change in one variable would cause change in another variable so the model results \nfluctuate. Even a small change in the data can results a varied change in the model results.<br><br>\n<b>How to check if the there is collinearity between the indepandent variables?<\/b><br>\n1) By Correlation Matrix <br>\n2) By Variance inflation factor <br><br>\n\nI am using the correlation matrix to identify the collinearity<br>","b1c7de15":"<b>Getting the categorical columns and numerical columns<\/b>","ac99f64d":"<b>converting the data to train and test<\/b><br>\nusing the train_size as 0.7 and test_size as 0.3"}}