{"cell_type":{"2ab81a66":"code","ecaff5fb":"code","7404ca0e":"code","c8fbd277":"code","bcce2188":"code","825dd590":"code","a79df423":"code","bfcda582":"code","89a4b3fb":"code","495bef88":"code","cb4d7a06":"code","d87f2c77":"code","184b81cd":"code","10cb0ac7":"code","e328fb68":"code","ef62f0d2":"code","d7d3697d":"code","b1fc3e95":"code","43abe01b":"code","3ab6672a":"code","1bddab95":"code","712c7700":"code","bd461c20":"code","65b033a4":"code","5648c2ff":"code","a8da4cb0":"code","aa2de7ed":"code","61471cc6":"code","60ce38dc":"code","10caa7f3":"code","cdee1922":"markdown","151ec02f":"markdown","25dff0c8":"markdown","4b7be598":"markdown","540ea901":"markdown","8b820068":"markdown","e4ccf66b":"markdown","0ba4ff16":"markdown","f91ebd5b":"markdown","333f3545":"markdown","4a4c9804":"markdown","cfff46fb":"markdown","47a4264d":"markdown","4e33c94f":"markdown","8216d5c7":"markdown","b3eb94d1":"markdown","4c09e9b2":"markdown","c2db3081":"markdown","0c32f41b":"markdown","99148994":"markdown","e2315452":"markdown","37f58870":"markdown","2ab4da3b":"markdown","49409da2":"markdown","4513651d":"markdown","1d71d9cc":"markdown","212d8430":"markdown"},"source":{"2ab81a66":"!pip install matplotlib-venn","ecaff5fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotnine import *\nfrom matplotlib_venn import venn3, venn3_circles\n\nimport re\nimport logging\nimport itertools\nimport unicodedata\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n# import contractions\n\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom IPython.display import Markdown as md\n\nfrom wordsegment import load, segment\nload()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score, accuracy_score\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7404ca0e":"!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip","c8fbd277":"train_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\nsample_submission_df = pd.read_csv('sample_submission.csv')\ntest_labels_df = pd.read_csv('test_labels.csv')","bcce2188":"print(\"shape of train data : \", train_df.shape)\nprint(\"shape of test data : \", test_df.shape)\nprint(\"first 5 rows of train data :\")\ntrain_df.head()","825dd590":"for i in ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']:\n    print(\"{} :\".format(i.upper()))\n    print(train_df.loc[train_df[i]==1, 'comment_text'].sample().values[0][:500], \"\\n\")","a79df423":"## plots for EDA :\n\nplt.style.use(\"seaborn-pastel\")\ndef category_percentage(df):\n    df['clean'] = np.where((df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0), 1,0)\n    \n    categories = ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate', 'clean']\n    plot_data = df[categories].mean()*100\n\n    plt.figure(figsize=(10,5))\n    plt.title(\"percentage records by category\")\n    sns.barplot(x=plot_data.index, y = plot_data.values)\n    plt.show()\n    \n    return\n\ndef text_length_across_classes(df):\n    df['comment_length'] = df['comment_text'].apply(lambda x : len(x.split()))\n    \n    median_text_len = []\n    mean_text_len = []\n    min_text_len = []\n    max_text_len = []\n    max_distinct_tokens = []\n    \n    for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n        mean_text_len.append(df[df[i]==1]['comment_length'].mean())  \n        min_text_len.append(df[df[i]==1]['comment_length'].min())  \n        max_text_len.append(df[df[i]==1]['comment_length'].max())  \n        median_text_len.append(df[df[i]==1]['comment_length'].median())  \n        df['distinct_tokens'] = df['comment_text'].apply(lambda x : len(set(x.split())))\n        max_distinct_tokens.append(df[df[i]==1]['distinct_tokens'].max())\n        \n    mean_text_len.append(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0)]['comment_length'].mean())\n    min_text_len.append(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0)]['comment_length'].min())\n    max_text_len.append(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0)]['comment_length'].max())\n    median_text_len.append(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0)]['comment_length'].median())\n    max_distinct_tokens.append(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0)]['distinct_tokens'].max())\n    \n    fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n    sns.barplot(ax=axes[0,0], x=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','clean'], \n                y=median_text_len)\n    axes[0,0].set_title('median text length')\n    sns.barplot(ax=axes[0,1], x=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','clean'], \n                y=min_text_len)\n    axes[0,1].set_title('minimum text length')\n    sns.barplot(ax=axes[1,0], x=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','clean'], \n                y=max_text_len)\n    axes[1,0].set_title('max text length')\n    sns.barplot(ax=axes[1,1], x=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','clean'], \n                y=max_distinct_tokens)\n    axes[1,1].set_title('max distinct tokens')\n\n    fig.suptitle('text length statistics')\n    plt.show()\n    \n    return \n\ndef corr_between_labels(df):\n    plt.figure(figsize=(15,8))\n    plt.title(\"correlation between toxic categories\")\n    sns.heatmap(df.corr(),cmap='YlGnBu',annot=True)\n    plt.show()\n    return \n\n## Gram statistics\ndef gram_analysis(data,gram):\n    stop_words_set = set(stopwords.words('english'))\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stop_words_set]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\ndef gram_freq(df, gram, categ_col, text_col):\n    category_text = \" \".join(df[df[categ_col]==1][text_col].sample(200).values)\n    toks = gram_analysis(category_text, gram)\n    tok_freq = pd.DataFrame(data=[toks, np.ones(len(toks))]).T.groupby(0).sum().reset_index()\n    tok_freq.columns = ['token','frequency']\n    tok_freq = tok_freq.sort_values(by='frequency',ascending=False)\n    \n    plt.figure(figsize=(10,8))\n    plt.title(\"{} most common tokens\".format(categ_col))\n    sns.barplot(x='token', y='frequency', data=tok_freq.iloc[:30])\n    plt.xticks(rotation=90)\n    plt.show()\n    \n    return \n\ndef avg_word_len_plot(df):\n    # word distribution across categories\n    df['punct_count'] = df['comment_text'].apply(lambda x : len([a for a in x if a in string.punctuation]))\n    df['avg_word_length'] = df['comment_text'].apply(lambda x : np.round(np.mean([len(a) for a in x.split()])))\n    \n    clean = df[df['clean']==1].avg_word_length.value_counts().reset_index()\n    clean.columns = ['length', 'frequency']\n    print(\"clean comments max token length : {}\".format(max(clean.length)))\n    clean = clean.sort_values(by='length')\n    plt.figure(figsize=(20,7))\n    plt.title(\"Average word length - clean comments\")\n    sns.barplot(x=clean.length, y=clean.frequency)\n    plt.xticks(rotation = 90)\n    plt.show()\n\n    toxic = df[df['clean']==0].avg_word_length.value_counts().reset_index()\n    toxic.columns = ['length', 'frequency']\n    print(\"toxic comments max token length : {}\".format(max(toxic.length)))\n    toxic = toxic.sort_values(by='length')\n    plt.figure(figsize=(20,7))\n    plt.title(\"Average word length -toxic comments (all forms)\")\n    sns.barplot(x=toxic.length, y=toxic.frequency)\n    plt.xticks(rotation = 90)\n    plt.show()\n    \n    return \n\ndef generate_wordclouds(df, text_col, categ_col):\n    df['clean'] = np.where((df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']==0) & (df['insult']==0) & (df['identity_hate']==0), 1,0)\n    \n    if categ_col=='all_toxic':\n        category_text = df[df['clean']!=1][text_col].values\n    else:\n        category_text = df[df[categ_col]==1][text_col].values\n               \n    plt.figure(figsize=(15,8))\n    wc = WordCloud(background_color=\"black\", \n                   max_words=5000, \n                   stopwords=STOPWORDS, \n                   collocations=False,\n                   max_font_size= 40)\n    wc.generate(\" \".join(category_text))\n    plt.title(\"{} word cloud\".format(categ_col), fontsize=20)\n    # plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n    plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n    plt.axis('off')\n    \n    plt.show()\n    \n    return \n\ndef venn_(df):\n    figure, axes = plt.subplots(2,2,figsize=(20,20))\n    toxic = set(df[df['toxic']==1].index)\n    severe_toxic = set(df[df['severe_toxic']==1].index)\n    obscene = set(df[df['obscene']==1].index)\n    threat = set(df[df['threat']==1].index)\n    insult = set(df[df['insult']==1].index)\n    identity_hate = set(df[df['identity_hate']==1].index)\n    clean = set(df[df['clean']==1].index)\n\n    v1 = venn3([toxic, severe_toxic, obscene],\n          set_labels=('Toxic','Severe toxic','Obscene'), set_colors=('#a5e6ff', '#3c8492','#9D8189'), ax=axes[0][0])\n    for text in v1.set_labels:\n        text.set_fontsize(22)\n    v2 = venn3([threat, insult, identity_hate],\n          set_labels=('Threat','Insult','Identity hate'), set_colors=('#e196ce', '#F29CB7','#3c81a9'), ax=axes[0][1])\n    for text in v2.set_labels:\n        text.set_fontsize(22)\n    v3 = venn3([toxic, insult, obscene],\n          set_labels=('Toxic','Insult','Obscene'), set_colors=('#a5e6ff', '#F29CB7','#9D8189'), ax=axes[1][0])\n    for text in v3.set_labels:\n        text.set_fontsize(22)\n    v4 = venn3([threat, identity_hate, obscene],\n          set_labels=('Threat','Identity hate','Obscene'), set_colors=('#e196ce', '#3c81a9','#9D8189'), ax=axes[1][1])\n    for text in v4.set_labels:\n        text.set_fontsize(22)\n    plt.show()\n    \n    # deleting used variables\n    del toxic\n    del severe_toxic\n    del obscene\n    del threat\n    del insult\n    del identity_hate\n    del clean\n    \n    return\n\ndef meta_data_analysis(df, text_col):\n    meta_df = pd.DataFrame()\n    meta_df['punctuations'] = df[text_col].apply(lambda x : len([a for a in str(x) if a in string.punctuation]))\n    meta_df['hashtags'] = df[text_col].apply(lambda x : len([a for a in x.split() if a.startswith(\"#\")]))\n    meta_df['usernames'] = df[text_col].apply(lambda x : len([a for a in x.split() if a.startswith(\"@\")]))\n    meta_df['stop_words'] = df[text_col].apply(lambda x : len([a for a in x.lower().split() if a in STOPWORDS]))\n    meta_df['upper_case_words'] = df[text_col].apply(lambda x : len([a for a in x.split() if a.isupper()]))\n    meta_df['urls'] = df[text_col].apply(lambda x : len([a for a in x.split() if a.startswith(tuple(['http', 'www']))]))\n    meta_df['word_count'] = df[text_col].apply(lambda x : len(x.split()))\n    meta_df['distinct_word_count'] = df[text_col].apply(lambda x : len(set(x.split())))\n    meta_df['clean'] = df['clean'].copy()\n    \n    return meta_df","bfcda582":"category_percentage(train_df)\nprint(\"{}% of the comments are clean i.e., non-toxic\".format(np.round(100*train_df['clean'].sum()\/train_df.shape[0],2)))\nperc_clean_data = np.round(100*train_df['clean'].sum()\/train_df.shape[0],2)","89a4b3fb":"corr_between_labels(train_df)","495bef88":"venn_(train_df)","cb4d7a06":"text_length_across_classes(train_df)","d87f2c77":"avg_word_len_plot(train_df)","184b81cd":"# Word cloud using raw data\n\ngenerate_wordclouds(train_df, 'comment_text', 'all_toxic')\ngenerate_wordclouds(train_df, 'comment_text', 'clean')","10cb0ac7":"## Text cleaning\n\nclass TextCleaningUtils:\n    '''\n        This class contains implementations of various text cleaning operations (Static Methods)\n    '''\n    @staticmethod\n    def expand_abbreviations(text):\n        text = re.sub(r\"he's\", \"he is\", text)\n        text = re.sub(r\"there's\", \"there is\", text)\n        text = re.sub(r\"We're\", \"We are\", text)\n        text = re.sub(r\"That's\", \"That is\", text)\n        text = re.sub(r\"won't\", \"will not\", text)\n        text = re.sub(r\"they're\", \"they are\", text)\n        text = re.sub(r\"Can't\", \"Cannot\", text)\n        text = re.sub(r\"wasn't\", \"was not\", text)\n        text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n        text= re.sub(r\"aren't\", \"are not\", text)\n        text = re.sub(r\"isn't\", \"is not\", text)\n        text = re.sub(r\"What's\", \"What is\", text)\n        text = re.sub(r\"haven't\", \"have not\", text)\n        text = re.sub(r\"hasn't\", \"has not\", text)\n        text = re.sub(r\"There's\", \"There is\", text)\n        text = re.sub(r\"He's\", \"He is\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"You're\", \"You are\", text)\n        text = re.sub(r\"I'M\", \"I am\", text)\n        text = re.sub(r\"shouldn't\", \"should not\", text)\n        text = re.sub(r\"wouldn't\", \"would not\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"i'm\", \"I am\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", text)\n        text = re.sub(r\"I'm\", \"I am\", text)\n        text = re.sub(r\"Isn't\", \"is not\", text)\n        text = re.sub(r\"Here's\", \"Here is\", text)\n        text = re.sub(r\"you've\", \"you have\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n        text = re.sub(r\"we're\", \"we are\", text)\n        text = re.sub(r\"what's\", \"what is\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"we've\", \"we have\", text)\n        text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n        text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n        text = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", text)\n        text = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", text)\n        text = re.sub(r\"who's\", \"who is\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", text)\n        text = re.sub(r\"y'all\", \"you all\", text)\n        text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n        text = re.sub(r\"would've\", \"would have\", text)\n        text = re.sub(r\"it'll\", \"it will\", text)\n        text = re.sub(r\"we'll\", \"we will\", text)\n        text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n        text = re.sub(r\"We've\", \"We have\", text)\n        text = re.sub(r\"he'll\", \"he will\", text)\n        text = re.sub(r\"Y'all\", \"You all\", text)\n        text = re.sub(r\"Weren't\", \"Were not\", text)\n        text = re.sub(r\"Didn't\", \"Did not\", text)\n        text = re.sub(r\"they'll\", \"they will\", text)\n        text = re.sub(r\"DON'T\", \"DO NOT\", text)\n        text = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", text)\n        text = re.sub(r\"they've\", \"they have\", text)\n        text = re.sub(r\"they'd\", \"they would\", text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"should've\", \"should have\", text)\n        text = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", text)\n        text = re.sub(r\"where's\", \"where is\", text)\n        text = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"weren't\", \"were not\", text)\n        text = re.sub(r\"They're\", \"They are\", text)\n        text = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n        text = re.sub(r\"let's\", \"let us\", text)\n        text = re.sub(r\"it's\", \"it is\", text)\n        text = re.sub(r\"can't\", \"cannot\", text)\n        text = re.sub(r\"don't\", \"do not\", text)\n        text = re.sub(r\"you're\", \"you are\", text)\n        text = re.sub(r\"i've\", \"I have\", text)\n        text = re.sub(r\"that's\", \"that is\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"doesn't\", \"does not\",text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"didn't\", \"did not\", text)\n        text = re.sub(r\"ain't\", \"am not\", text)\n        text = re.sub(r\"you'll\", \"you will\", text)\n        text = re.sub(r\"I've\", \"I have\", text)\n        text = re.sub(r\"Don't\", \"do not\", text)\n        text = re.sub(r\"I'll\", \"I will\", text)\n        text = re.sub(r\"I'LL\", \"I will\", text)\n        text = re.sub(r\"I'd\", \"I would\", text)\n        text = re.sub(r\"Let's\", \"Let us\", text)\n        text = re.sub(r\"you'd\", \"You would\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"Ain't\", \"am not\", text)\n        text = re.sub(r\"Haven't\", \"Have not\", text)\n        text = re.sub(r\"Hadn't\", \"Had not\", text)\n        text = re.sub(r\"Could've\", \"Could have\", text)\n        text = re.sub(r\"youve\", \"you have\", text)  \n        text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)  \n\n        return text\n\n    cleaning_regex_map = {\n        'web_links': r'(?i)(?:(?:http(?:s)?:)|(?:www\\.))\\S+',\n        'email': r'[\\w.]+@\\w+\\.[a-z]{3}',\n        'twitter_handles': r'[#@]\\S+',\n        'redundant_newlines': r'[\\r|\\n|\\r\\n]+',\n        'redundant_spaces': r'\\s\\s+',\n        'punctuations': r'[\\.,!?;:]+',\n#         'special_chars': r'[^a-zA-Z0-9\\s\\.,!?;:]+',\n        'special_chars': r'[^a-zA-Z\\s\\.,!?;:]+'  ## removing nums\n        \n    }\n    \n    @staticmethod\n    def clean_text_from_regex(text, text_clean_regex):\n        '''\n            Follow a particular cleaning expression, provided\n            as an input by an user to clean the text.\n        '''\n\n        text = text_clean_regex.sub(' ', text).strip()\n        return text\n    \n    @staticmethod\n    def strip_html(text):\n        soup = BeautifulSoup(text, \"html.parser\")\n        return soup.get_text()\n    \n    @staticmethod\n    def remove_web_links(text):\n        '''\n            Removes any web link that follows a particular default expression,\n            present in the text.\n        '''\n\n        web_links_regex = re.compile(TextCleaningUtils.cleaning_regex_map['web_links'])\n        text = TextCleaningUtils.clean_text_from_regex(text, web_links_regex)\n        return text\n    \n    @staticmethod\n    def remove_email_addresses(text):\n        '''\n            Removes email addresses present in the text.\n        '''\n\n        email_regex = re.compile(TextCleaningUtils.cleaning_regex_map['email'])\n        text = TextCleaningUtils.clean_text_from_regex(text, email_regex)\n        return text\n    \n    @staticmethod\n    def remove_twitter_handles(text):\n        '''\n            Removes any twitter handle present in the text.\n        '''\n\n        twitter_handles_regex = re.compile(TextCleaningUtils.cleaning_regex_map['twitter_handles'])\n        text = TextCleaningUtils.clean_text_from_regex(text, twitter_handles_regex)\n        return text\n    \n    @staticmethod\n    def remove_emojis(text):\n        emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        text=emoji_clean.sub(r'',text)\n        url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n        text=url_clean.sub(r'',text)\n        return text\n    \n    @staticmethod\n    def remove_redundant_newlines(text):\n        '''\n            Removes any redundant new line present in the text.\n        '''\n\n        redundant_newlines_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_newlines'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_newlines_regex)\n        return text\n    \n    @staticmethod\n    def remove_redundant_spaces(text):\n        '''\n            Remove any redundant space provided as default,\n            that is present in the text.\n        '''\n\n        redundant_spaces_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_spaces'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_spaces_regex)\n        return text\n    \n    @staticmethod\n    def remove_punctuations(text):\n        '''\n            Removes any punctuation that follows the default expression, in the text.\n        '''\n\n        remove_punctuations_regex = re.compile(TextCleaningUtils.cleaning_regex_map['punctuations'])\n        text = TextCleaningUtils.clean_text_from_regex(text, remove_punctuations_regex)\n        return text\n\n    @staticmethod\n    def remove_special_chars(text):\n        '''\n            Replace any special character provided as default,\n            which is present in the text with space\n        '''\n\n        special_chars_regex = re.compile(TextCleaningUtils.cleaning_regex_map['special_chars'])\n        text = TextCleaningUtils.clean_text_from_regex(text, special_chars_regex)\n        return text\n\n    @staticmethod\n    def remove_exaggerated_words(text):\n        '''\n            Removes any exaggerated word present in the text.\n        '''\n\n        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n\n    @staticmethod\n    def replace_multiple_chars(text):\n        '''\n            Replaces multiple characters present in the text.\n        '''\n\n        char_list = ['.', '?', '!', '#', '$', '\/', '@', '*', '(', ')', '+']\n        final_text = ''\n        for i in char_list:\n            if i in text:\n                pattern = \"\\\\\" + i + '{2,}'\n                repl_str = i.replace(\"\\\\\", \"\")\n                text = re.sub(pattern, repl_str, text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def replace_sign(text):\n        '''\n            Replaces any sign with words like & with 'and', in the text.\n        '''\n        sign_list = {'&': ' and ', '\/': ' or ', '\\xa0': ' '}\n        final_text = ''\n        for i in sign_list:\n            if i in text:\n                text = re.sub(i, sign_list[i], text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def remove_accented_char(text):\n        text = unicodedata.normalize('NFD', text) \\\n            .encode('ascii', 'ignore') \\\n            .decode(\"utf-8\")\n        return str(text)\n\n    @staticmethod\n    def replace_characters(text, replace_map):\n        '''\n            Replaces any character custom provided by an user.\n        '''\n\n        for char, replace_val in replace_map.items():\n            text = text.replace(char, replace_val)\n        return text\n    \ndef clean_data(df,col_to_clean):\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_web_links)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_email_addresses)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_twitter_handles)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.expand_abbreviations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_emojis)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_special_chars)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_spaces)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_punctuations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_exaggerated_words)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_newlines)\n    df[col_to_clean] = df[col_to_clean].astype(str)\n    df[col_to_clean] = df[col_to_clean].str.lower()\n    \n    return df","e328fb68":"%%time\ntrain_df = clean_data(train_df, 'comment_text')","ef62f0d2":"avg_word_len_plot(train_df)","d7d3697d":"category_text = train_df[train_df['clean']!=1]['comment_text'].values\ncategory_text = \" \".join(category_text)\nlong_len_words = [word for word in category_text.split() if len(word)>20]\nprint(\"num of long length words in toxic(all forms) text : {}\".format(len(long_len_words)))\nlong_len_words[:5]","b1fc3e95":"train_df[(train_df['comment_length']>100) & (train_df['distinct_tokens']<30)].comment_text.sample().values[0][:500]","43abe01b":"class data_specific_preprocessing:\n    \n    @staticmethod\n    def long_word_cleaning(wrd):\n        if wrd.startswith('haha') | wrd.startswith('ahah'):\n            wrd = 'haha'\n        elif wrd.startswith('lol') | wrd.startswith('olo'):\n            wrd = 'lol'\n        elif wrd.startswith('fuckfuck') | wrd.startswith('uckfuc') | wrd.startswith('ckfuck') | wrd.startswith('kfuckf'):\n            wrd = 'fuck'\n        elif wrd.startswith('suck') | wrd.startswith('ucks') | wrd.startswith('cksu') | wrd.startswith('ksuc'):\n            wrd = 'suck'\n        elif wrd.startswith('mwahaha') | wrd.startswith('muahaha'):\n            wrd = 'muahahaha'\n        elif wrd.startswith('bwahaha'):\n            wrd = 'bwahaha'\n        elif wrd.startswith('cunt') | wrd.startswith('untc') | wrd.startswith('ntcu') | wrd.startswith('tcun'):\n            wrd = 'cunt'\n        elif wrd.startswith('blah'):\n            wrd = 'blah'\n        elif wrd.startswith('tytyty'):\n            wrd = 'ty'\n        return wrd\n    \n    @staticmethod\n    def long_word_fix(text):\n        x = text.split()\n        # fixes long words\n        x = [data_specific_preprocessing.long_word_cleaning(wrd) if len(wrd)>20 else wrd for wrd in x]\n        # returns words from spaceless phrases\n        text = \" \".join([\" \".join(segment(wrd)) if len(wrd)>20 else wrd for wrd in x])\n        return text\n    \n    @staticmethod\n    def repetitive_text_cleaning(text):\n        x = text.split()\n        if len(x)>100 and len(set(x))<=30:\n            text = \" \".join(x[:len(set(x))+10])\n        return text\n","3ab6672a":"%%time\ntrain_df['comment_text'] = train_df['comment_text'].apply(data_specific_preprocessing.long_word_fix)\ntrain_df['comment_text'] = train_df['comment_text'].apply(data_specific_preprocessing.repetitive_text_cleaning)","1bddab95":"avg_word_len_plot(train_df)","712c7700":"for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'clean']:\n    gram_freq(train_df,2, i, 'comment_text')","bd461c20":"%%time\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\ndef text_transformation(text):\n    lemm_words = \" \".join([lemm.lemmatize(word) for word in text.split()])    \n    return lemm_words\n                        \ntrain_df['comment_text'] = train_df['comment_text'].apply(text_transformation)","65b033a4":"# train validation split\ntr_df, val_df = train_test_split(train_df, test_size=0.25, random_state=0)","5648c2ff":"# count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,3), analyzer='word', \n#                                   strip_accents='unicode',token_pattern=r'\\w{1,}')\n# train_count = count_vectorizer.fit_transform(tr_df['comment_text'])\n# val_count =  count_vectorizer.transform(val_df['comment_text'])\n# for target in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n#    lr = LogisticRegression(C=1, class_weight='balanced', solver='saga',max_iter=1000)\n#    lr.fit(train_count, tr_df[target])\n#    lr_val_pred = lr.predict(val_count)\n#    print(\" model for {} category\".format(target))\n#    print(\"auc : \", roc_auc_score(val_df[target], lr_val_pred)) ","a8da4cb0":"%%time\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), analyzer='word', \n                                   strip_accents='unicode',token_pattern=r'\\w{1,}', use_idf=1,\n                                   smooth_idf=1,sublinear_tf=1)\n\n# tfidf_vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\ntrain_tfidf = tfidf_vectorizer.fit_transform(tr_df['comment_text'])\nval_tfidf =  tfidf_vectorizer.transform(val_df['comment_text'])\n\ntarget_category = []\naucs = []\nfor target in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    target_category.append(target)\n    lr = LogisticRegression(C=2, class_weight='balanced', max_iter=1000)\n    lr.fit(train_tfidf, tr_df[target])\n    lr_val_pred = lr.predict(val_tfidf)\n    print(\"model for {} category\".format(target))\n    auc_score = roc_auc_score(val_df[target], lr_val_pred)\n    aucs.append(auc_score)\n    print(\"validation auc : \", auc_score, \"\\n\")\n\nprint(\"avg auc : \", np.mean(aucs))","aa2de7ed":"from gensim.models import Word2Vec, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_reg_100d_file = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded = glove2word2vec(glove_input_file=glove_reg_100d_file, \n                              word2vec_output_file=word2vec_output_file)\nprint(glove_loaded)","61471cc6":"%%time\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)","60ce38dc":"def mean_word_embedding(text):\n    words = word_tokenize(text.lower())\n    words = [word for word in words if word in glove_model.index2word]\n    sum_emb = np.array(100)\n    if len(words)>=1:\n        for i in words:\n            sum_emb = sum_emb + glove_model[i]\n    return sum_emb\/len(words)","10caa7f3":"# mean embeddings of sample comments from categories\n\ntox = train_df[train_df['toxic']==1].comment_text.sample().values[0]\nobs = train_df[train_df['obscene']==1].comment_text.sample().values[0]\nthre = train_df[train_df['threat']==1].comment_text.sample().values[0]\niden = train_df[train_df['identity_hate']==1].comment_text.sample().values[0]\nsevtox = train_df[train_df['severe_toxic']==1].comment_text.sample().values[0]\nins = train_df[train_df['insult']==1].comment_text.sample().values[0]\nclean = train_df[train_df['clean']==1].comment_text.sample().values[0]\n\n\nplt.plot(mean_word_embedding(sevtox), label='severe toxic')\nplt.plot(mean_word_embedding(ins), label='insult')\nplt.plot(mean_word_embedding(clean), label='clean')\nplt.plot(mean_word_embedding(thre), label='threat')\nplt.legend()\nplt.show()","cdee1922":" This is clearly as case with high class imbalance","151ec02f":"From the above plot, it is evident that most words seem to be within 20 characters in length\n\nLet's take a closer look at long words (>20 characters)","25dff0c8":"Let's take a minute to see what is an acceptable word length in english\n\n\nDistribution of distinct word lengths - English","4b7be598":"# EDA","540ea901":"### Percentage of data vs Category","8b820068":"# Semantic embeddings WIP\n\n## Static embeddings\n\nThese are vector space representations of words in the corpus. \n\nCommonly used static embeddings are:\n- Word2vec\n- GloVe\n- Fasttext\n\nWe will be using Glove vectors in word2vec format here","e4ccf66b":"### Interrelation between categories\n\n- All severe toxic comments are toxic\n- Most severe toxic comments are also obscene in nature\n- toxic, insult and obscene comments share a good amount of overlap","0ba4ff16":"![Screenshot%20%2816%29.png](attachment:Screenshot%20%2816%29.png)","f91ebd5b":"## Bi-gram analysis\n- Most common bigrams in each category","333f3545":"### Sample comments corresponding to each category","4a4c9804":"### Apart from the long token problem seen above, we have another issue\nIt is the issue of repetitive phrases\/lines in comments\n- Comments with text repetition can cause computational overload. \n- It's best to trim such text to its basic, non-repetitve form to preserve the meaning and also to make computation efficient.\n\nsample text of the above nature in this dataset is as follows:","cfff46fb":"## Approach\n\n- [Basic EDA - Visualization, data problems and fixes](#EDA)\n- [Data Cleaning and Visualization](#Text-cleaning)\n- [Data Transformation](#Text-transformation)\n- [Non-semantic baseline](#Non-semantic-Approaches)\n- [WIP - Semantic embeddings, baseline ](#Semantic-embeddings-WIP) ","47a4264d":"## Average word length in clean and toxic comments\n- The average word length in both clean and toxic comments is unreasobaly high; \n- we need to clean the data and inspect further, if the issue persists","4e33c94f":"## TF-IDF baseline","8216d5c7":"# Text cleaning\n\nNext step is to remove unnecessary elements from text.\n\nIt is important to note that this \"unnecessary text\" elimination process could be more successful when followed in a particular sequence.\n\nFor instance, elimination of special characters before removal of hashtags, usernames defeats our purpose as the text following a '#' could be considered a token (ex: #kaggledays -> kaggledays).\n\nHere is the sequence we will implement:\n\n- HTML codes (if present)\n- URLs\/ email addresses\n- Hashtags\/Usernames\n- Emojis\n- Stopwords\n- Expanding Abbreviations\n- Punctuations\n- Special characters\/ Numbers","b3eb94d1":"# **Toxic comment classification**\n\nThis is a multi label classification challenge with each of the targets being binary in nature. We need to classify if a comment belongs to one or more categories.\n\nFollowing are the targets:\n- Toxic\n- Severe_toxic\n- Obscene\n- Threat\n- Insult\n- Identity_hate \n\nEvaluation: average of the individual AUCs of each predicted target","4c09e9b2":"### Data specific text preprocessing\n\nAddresses the following issues:\n- long words (formed by repetitive characters)\n- long texts (formed by repetitive words)\n- spaceless phrases (ex: idontlikethisline)","c2db3081":"## Wordclouds - unprocessed data\n\n- We observe a heavy use of upper-cased, racist text (offensive nouns) in toxic comments\n- Clean comments on the other had fewer nouns and no observable upper case text","0c32f41b":"- Clean comments don't appear to revolve around particular pattern given the frequency of common bigrams observed\n- Toxic (severe_toxic, obscene) comments on the other hand tend to be more specific given the frequently occurring bigrams\n\nwe could perform a similar exercise with trigrams to observe commonly occurring three word combinations","99148994":"### mean embeddings of comments from different categories","e2315452":"## Meta data analysis\n### Comment length statistics across categories\n\n- There is an observable difference between median length of clean and toxic data\n- The difference in max text length and max distinct tokens raises suggests possibility of heavy word repetition","37f58870":"### Average word lengths acrcoss toxic and clean comment categories (after cleaning)\n\n#### Word lengths still look unusual!","2ab4da3b":"# Non-semantic Approaches\n\nBag of words techniques do not take into consideration the order of words in a given text. These techniques are primarily concerned with number of occurences of words in the text. \n\nThere are three ways in which we could vectorize text:\n- Count Vectorizer\n- Tfidf Vectorizer\n- Hashing Vectorizer\n\nHere's a useful resource that discusses these techniques in detail\n[getting started with nlp feature vectors](https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-feature-vectors)\n\n## Count vectorizer baseline","49409da2":"## Correlation and Interrelation\n\n### Correlation between labels:\n- Insult and Obscene, Toxic and obscene exhibit a strong correlation","4513651d":"### Acknowledgements\n\nReferences : \n[NLP(End to end): CLL NLP workshop](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop)","1d71d9cc":"We have now achieved reasonable word lengths in both toxic and non-toxic groups of data","212d8430":"# Text transformation \n\n- Using lemmatization to convert words to their base form"}}