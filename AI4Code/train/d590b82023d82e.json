{"cell_type":{"e870a45d":"code","85967195":"code","e981c48c":"code","08808677":"code","5830c8a4":"code","cf6f5e00":"code","392701b4":"code","10374810":"code","09334293":"code","2d6d721b":"code","01c79621":"code","6884b2cc":"code","ac9ee187":"code","f0fdc9b4":"code","f9e35678":"code","ea772fe4":"code","dc54ec09":"code","0a8b05c2":"code","11e21530":"code","920d0c68":"code","a403940b":"code","e8c66801":"code","bfe2954f":"code","80e802fe":"code","3745b6f2":"code","de2ab362":"code","5299c6a1":"code","dbf7f6bd":"code","c93bd623":"code","587a0a39":"code","d4f38df6":"code","657fd3d1":"code","b4b9ad87":"code","4fbeec0b":"code","fe930cc7":"code","8886876e":"code","9bc77d4a":"code","394c4a59":"code","6ce61f4c":"code","40493a24":"code","5ea35740":"code","52410cc2":"code","af0523cf":"code","dd03608e":"code","ab81d078":"code","53f1ff6a":"code","ab5f8391":"code","86bd3f6b":"code","28d18395":"code","18322838":"markdown","40d0e1a2":"markdown","49146a16":"markdown","eea7c2a5":"markdown","ab8f08ed":"markdown","f8c61aec":"markdown","4a388083":"markdown","0c636643":"markdown","e589b7a2":"markdown","60dfa9de":"markdown","68735458":"markdown","73662cfa":"markdown","bc86182b":"markdown","20cbb376":"markdown","2e728b47":"markdown","db08aed5":"markdown","27062110":"markdown","9f41dd13":"markdown","888de0f6":"markdown","852b2076":"markdown","3a6bf04e":"markdown","f0015e0b":"markdown","921f5b07":"markdown","7fee05c6":"markdown","1a43e4b1":"markdown","f92eaf0b":"markdown","3e21b4c6":"markdown","f8fb97a8":"markdown"},"source":{"e870a45d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n\n# Any results you write to the current directory are saved as output.","85967195":"# Lets read the data into a dataframe\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","e981c48c":"df.tail()","08808677":"df.sample(10)","5830c8a4":"df.shape","cf6f5e00":"df.info()","392701b4":"df.columns","10374810":"df.describe()","09334293":"# defining the labels as \"y\"\ny = df['Class']\ny.value_counts()","2d6d721b":"# X is our features \nX = df.iloc[:,:-1]\nX.shape","01c79621":"# Now lets look at the correlationin the data. \n\n# find the correlation between the different variables.\ncorr_mtx = df.corr()\nf, ax = plt.subplots(figsize=(16, 14))\nax=sns.heatmap(corr_mtx,annot=False,cmap=\"YlGnBu\")\n\n    ","6884b2cc":"print(corr_mtx['Class'].sort_values(ascending = False)) ","ac9ee187":"# We will split out data into training and testing set.\nfrom sklearn.model_selection import train_test_split \ntrain_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,random_state=42)","f0fdc9b4":"# Here we will plot a hist of all features to see the spread of data.\n# doing a visual on data helps to further understand the data.\nimport matplotlib.pyplot as plt\nX.hist(figsize=(20,21))\nplt.show()","f9e35678":"# Separate out Fraud & Non-Fraud Data, and split to get train & test set.\nnon_fraud_data = df[df.Class == 0]\nfraud_data = df[df.Class == 1]","ea772fe4":"np.unique(non_fraud_data.Class)","dc54ec09":"non_fraud_label = non_fraud_data['Class']\nnon_fraud_data.head()","0a8b05c2":"non_fraud_data['Class'].value_counts()","11e21530":"non_fraud_X = non_fraud_data.iloc[:,:-1]\nnon_fraud_X.head()","920d0c68":"\n\n\nfraud_label = fraud_data['Class']\nfraud_X = fraud_data.iloc[:,:-1]\n\nnon_train_X, non_test_X, non_train_y, non_test_y = train_test_split(non_fraud_X,non_fraud_label,test_size=0.3,random_state=42)\n","a403940b":"# Lets look at Kmeans...\n# Lets see whether we can segregate data in Clusters.\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nks = range(1, 6)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n   # Fit model to samples\n    model.fit(non_train_X)\n   # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","e8c66801":"# lets use three clusters..\n\nfrom sklearn.cluster import KMeans\n\nmodel1 = KMeans(n_clusters=3, random_state=42)\nk_labels = model1.fit_predict(non_fraud_X,non_fraud_label)\n","bfe2954f":"model1.score (non_test_X,non_test_y)","80e802fe":"print('len of fraud', len(fraud_X))\nprint('len of non_fraud_X', len(non_fraud_X))\nprint('len of df', len(df))\n","3745b6f2":"fraud_predict_labels = model.predict(fraud_X)\nnp.unique(fraud_predict_labels)\n\nlen(fraud_predict_labels[fraud_predict_labels < 0])\n\n","de2ab362":"data=df.drop(columns=['Time', 'V1', 'V2', 'V3','V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18','V20','V22', 'V23','V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],axis=1)","5299c6a1":"data.sample(10)","dbf7f6bd":"data[\"Class\"].value_counts()","c93bd623":"y=data.iloc[:,-1]\n","587a0a39":"X=data.iloc[:,:-1]\n","d4f38df6":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","657fd3d1":"# Lets look at Kmeans...\n# Lets see whether we can segregate data in Clusters.\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nks = range(1, 6)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n   # Fit model to samples\n    model.fit(X_train)\n   # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","b4b9ad87":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=4)\nk_labels = model.fit(X_train,y_train)\n","4fbeec0b":"model.predict(X_test)","fe930cc7":"model.score(X_test,y_test)","8886876e":"from sklearn.model_selection import GridSearchCV\n#create new a knn model\nknn2 = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid = {\"n_neighbors\": np.arange(1, 25)}\n#use gridsearch to test all values for n_neighbors\nknn_gscv = GridSearchCV(knn2, param_grid, cv=5)\n#fit model to data\nknn_gscv.fit(X, y)","9bc77d4a":"knn_gscv.predict(X_test)","394c4a59":"#check mean score for the top performing value of n_neighbors\nknn_gscv.best_score_","6ce61f4c":"# compute outlier_fraction for the model here\noutlier_fraction = len(fraud_X)\/len(df)\nprint(outlier_fraction)\n\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest(n_estimators=10, max_samples= len(train_X),contamination = outlier_fraction,n_jobs=5, random_state=42, behaviour ='new')\nclf.fit(train_X)\nscore = clf.decision_function(train_X)\n\ny_pred_train = clf.predict(train_X)\ny_pred_test = clf.predict(test_X)\n","40493a24":"# lets try to build confusion matrix and get precision and accuracy scores.\ny_pred_test[y_pred_test == 1] = 0\ny_pred_test[y_pred_test == -1] = 1\nnp.unique(y_pred_test)\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\ncnf_mtrx = confusion_matrix(test_y,y_pred_test)\ncnf_mtrx","5ea35740":"accuracy_score(train_y,y_pred_train)\n\n# Calculate precison & recall. \n# Precision is actual positive prediction\/ total positive prediction\n# Recall is actual positive prediction\/ total actual positives\n\nprecision = cnf_mtrx[1,1]\/(cnf_mtrx[1,1]+cnf_mtrx[0,1])\nrecall = cnf_mtrx[1,1]\/(cnf_mtrx[1,1]+cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","52410cc2":"from sklearn.ensemble import AdaBoostClassifier\n\nclf_ada = AdaBoostClassifier(n_estimators=100, random_state=42)\nclf_ada.fit(train_X,train_y) ","af0523cf":"test_y_ada_predict = clf_ada.predict(test_X)\n\n# lets build confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nada_test_cnf_mtrx = confusion_matrix(test_y,test_y_ada_predict)\nada_test_cnf_mtrx","dd03608e":"##Find precision and recall for AdaBoost Model\nprecision = ada_test_cnf_mtrx[1,1]\/(ada_test_cnf_mtrx[1,1]+ada_test_cnf_mtrx[0,1])\nrecall = ada_test_cnf_mtrx[1,1]\/(ada_test_cnf_mtrx[1,1]+ada_test_cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","ab81d078":"from sklearn.linear_model import LogisticRegression\n\nlg_clf = LogisticRegression(penalty='l2',tol=0.0001,random_state=42)\nlg_clf.fit(train_X,train_y)","53f1ff6a":"test_y_lg_predict = lg_clf.predict(test_X)\n\n# lets build confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nlg_test_cnf_mtrx = confusion_matrix(test_y,test_y_lg_predict)\nlg_test_cnf_mtrx","ab5f8391":"##Find precision and recall for Logistic Regression Model\nprecision = lg_test_cnf_mtrx[1,1]\/(lg_test_cnf_mtrx[1,1]+lg_test_cnf_mtrx[0,1])\nrecall = lg_test_cnf_mtrx[1,1]\/(lg_test_cnf_mtrx[1,1]+lg_test_cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","86bd3f6b":"from keras import  backend as K\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense,Activation, Flatten, Dropout\nfrom keras.optimizers import Adam,RMSprop, SGD\n\nfrom keras.utils import np_utils\nimport numpy as np\n\nn_cols = train_X.shape[1]\ny_train = np_utils.to_categorical(train_y,2)\ny_test = np_utils.to_categorical(test_y,2)\nprint('shape is',n_cols)\n\nmodel = Sequential()\nmodel.add(Dense(128,activation='relu',input_shape=(n_cols,)))\nmodel.add(Dense(2,activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\nhistory = model.fit(train_X,y_train,batch_size=1000,epochs=200,verbose='VERBOSE',validation_split = 0.2)\n\n","28d18395":"score1 = model.evaluate(train_X,y_train)\nprint('train score',score1[0])\nprint('train accuracy',score1[1])\n\nscore = model.evaluate(test_X,y_test)\nprint('test score',score[0])\nprint('test accuracy',score[1])","18322838":"V11, V4 and V2 are the features which have the most correlation impact on Class, and there are attributes which are negatively correlated as well","40d0e1a2":"Lets try our first model, and see where it gets us..\n\n**KMEANS**\n\nWe will try with KMeans Clustering algorithm. We will seggregate the data in different clusters, and see whether the fraud gets segregated differently...Also, I will train the model using the non-fraud data (meaning, I will remove the fraud data out, and train the model with non fraud data only). Then we will feed in fraud data, and see how the prediction is..\n \nIn the below step, I am separating fraud from non-fraud, and creating train and set set for non fraud data.","49146a16":"Precision and recall both are around 32%. Although, this may seem low, but the results are fantastic! \nModel is now able to detect 32% of fraud cases.","eea7c2a5":"Only 44 fraud's have been detected by our model, and 93 have been missed whereas 92 have been incorrectly classified as fraud by our model.","ab8f08ed":"Its good to always do df.info() & df.describe()\nThese methods help you to find out whether there are any invalid\/empty data, and how the data is spread as well.","f8c61aec":"From the above plot of inertia with number of clusters, we can see that the inertial change is smaller post the 3 clusters. We will now build our model for three clusters, and use it for predictions.","4a388083":"Lets look at correlation of features. This will tell us how the features are correlated with each other. \nCorrelation gives us a intution on which variables are important, and have impact on the predicted class.","0c636643":"So we can see that the data has no nan or null values.","e589b7a2":"KMeans was not really helpful here. \nOfcourse, you can try including fraud data when you are training the model, and see how it works.\nThe predictions has classified them in one of the existing clusters. Not much helpful.\nSo lets move on to other predictor models.","60dfa9de":"Here I am plotting feature data, and seeing their spread. Histogram will help us to do that. :)","68735458":"Now using the Grid Search CV algorithm","73662cfa":"Lets try to find accuracy and precision of our model for the test data.\n\nFirst we have to align both the prediction and the observation. To match with Observation categorical values, we change predictions values to '1', when it is fraud, and '0' when it is not a fraud.\nWe will build a confusion matrix, and then calculate our precision and accuracy values.","bc86182b":"![](http:\/\/)We can see an accuracy of 99.90% ","20cbb376":"Lets change our gear to using Supervised Learning. We will use ADABOOST & Logistic Regression, and see where it gets us...\n\n**ADABOOST**","2e728b47":"Lets read data from the credit card csv file. \nYou can read more about this data, but essentially the dimension of the data has been reduced by applying PCA (Principal Component Analysis). ","db08aed5":"Training and spliting the data set and the applying KNN model to the dataset","27062110":"Lets use a simple Logistic Regression Model and check our precision.\n\n**Logistic Regression**","9f41dd13":"In this Kernel, I will take up a use case for identifying anamoly in the data, and thereby  will take a use case of credit card fraud data.\n\nFirstly, we will have a look at our provided data, and glean insights by using data exploratory and visualization tools.\n\nThen we will explore different ways to look for anamolies, and compare & contrast between them. We will start with Unsupervised learning, in that we will develop KMeans Cluster & IsolationForest Model, and use them to predict anamolies. After that, we will use Supervised learning methods, in that we will use gradient boost & Logistic regression to predict anamolies.\n\nFinally, we will dive into deep learning methods. We will build a deep sequential model, and predict anamolies.\n","888de0f6":"A simple Log Regression model has given us a precision of 69% and recall of 60%.\nNow lets dive into Deep Learning Model, and see how further it can take us......","852b2076":"#### Importing Libraries and Dataset ","3a6bf04e":"**WOW!!! Blown away....**\ntrain accuracy is 99.82% whereas test accuracy is 99.84%.\n\nI am satisfied with this outcome. Question is, **Are You?**","f0015e0b":"#### Visualizing the dataset","921f5b07":"Now, here we are trying on KNN Classification after removing the not required features from the dataset, according to the correlation matrix","7fee05c6":"#### Using Isolation Forest","1a43e4b1":"Here we can see using Grid Search CV ,we found a score of 99.89% ","f92eaf0b":"Plotting an elbow plot to determing the number of neighbors ","3e21b4c6":"**WOW!!** If you were excited about 32% accuracy with IsolationForest, AdaBoost has been able to give us 87% accuracy. We will able to predict 87 out of 100 Fraud cases, and can potentially stop them before happening :)","f8fb97a8":"Now..Lets do DeepLearning using Keras and TensorFlow as our backend.\nI have built a very simple model with no hidden layers.\nI have 128 neuron for the input layer, and output layer has 2 neurons (0 or 1) with softmax activation..\nI have used to_categorical method to change to binary output for train_y & test_y label array.\nI have used sgd optimizer, and then I run the model..."}}