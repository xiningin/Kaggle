{"cell_type":{"b88a579c":"code","d2a4a335":"code","3557cb84":"code","74b760f0":"code","938ae7d1":"code","57107366":"code","94e8237a":"code","bf51ca40":"code","507b5c8d":"code","09980c14":"code","de20c007":"code","e1318287":"code","9cfde4d7":"code","de6ee22c":"code","898f4460":"code","b09d5b2f":"code","495c606f":"code","e029cc45":"code","50d53649":"code","0015fe9e":"code","a0c624a3":"code","627db204":"code","7d69ad93":"code","53281bc9":"code","d7b752de":"code","dac8823b":"code","c48d028e":"code","3c1298b5":"code","a158d41c":"code","e8b41ccd":"code","aece5633":"code","be0a447c":"code","0795dc86":"code","7034b433":"code","f26584e0":"code","f5dab5b2":"code","c49dbb0a":"code","d9241b64":"code","a332120c":"code","989de190":"code","b50f7c6d":"code","38fb37d2":"code","4667fb85":"code","09aff302":"code","df2d38c4":"code","7e4f08f3":"code","8be081fe":"code","3ea1a246":"code","16293f02":"code","58c52ae2":"code","cab00004":"code","2efca7ce":"code","a3204eef":"code","fc5ad8a9":"code","ed9daa82":"code","80814031":"code","825e0138":"code","24e5185f":"code","5d88fe0e":"code","4395ccbc":"code","d44d2ebd":"code","25e713da":"code","b41c4b53":"markdown","d6c13a60":"markdown","11f7266c":"markdown","1e0e0373":"markdown","8506ead5":"markdown","1fa735db":"markdown","26b824af":"markdown","cce84854":"markdown","71d606e3":"markdown","8b2a5850":"markdown","8aff0cb3":"markdown","a6b8c57f":"markdown","7325d41b":"markdown","fa5648fc":"markdown","976c2197":"markdown","3fc8b235":"markdown","ad471150":"markdown","a9f21546":"markdown","4e318564":"markdown","1970e2cc":"markdown","eb948e87":"markdown","bade975d":"markdown","8202b2fa":"markdown","69165eeb":"markdown","82617f98":"markdown","8787a556":"markdown","019a9e1b":"markdown","09f196be":"markdown","13d99091":"markdown","e8dafa89":"markdown","0c7ceeab":"markdown","2f01015c":"markdown","3d3bd855":"markdown","191c71cb":"markdown","94dcc0e2":"markdown","2141ed28":"markdown","2c627c4c":"markdown","83f10733":"markdown","1cc1c572":"markdown","99a60a87":"markdown","9cd7c333":"markdown","4f8e5d3b":"markdown","f9871f27":"markdown","9e9bf0cc":"markdown","1cf435dc":"markdown","e60dddc0":"markdown","9e2baa30":"markdown","f50c3196":"markdown","dfba68a4":"markdown","3cdc08f6":"markdown","a67932f9":"markdown","9612f546":"markdown","8b0c5209":"markdown","9fbc1270":"markdown","0b55220d":"markdown","5ae4d1f8":"markdown","29fcbedc":"markdown","da76fa25":"markdown","a96b4205":"markdown","f45f513e":"markdown","5d81ae33":"markdown","4afb55dd":"markdown","d124cd7f":"markdown","27460325":"markdown","22ea2d35":"markdown","23f25b1e":"markdown","109103c4":"markdown","2920b73d":"markdown","bbd095bf":"markdown","de5697df":"markdown","b0b80d4f":"markdown","0c0dda39":"markdown","c71f49b1":"markdown"},"source":{"b88a579c":"# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport keras.layers","d2a4a335":"# local imports.\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/pynd-lib\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/pytools-lib\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/neuron\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/')\nimport voxelmorph as vxm\nimport neuron","3557cb84":"# import\n# You should most often have this import together with all other imports at the top, \n# but we include here here explicitly to show where data comes from\nfrom keras.datasets import mnist","74b760f0":"# load the data. \n# `mnist.load_data()` already splits our data into train and test.  \n# (x_train_load, y_train_load), (x_test_load, y_test_load) = mnist.load_data()\n\n# unfortunately the above seems to fail on the keras kernel\n# so we will load it from a pre-downloaded mnist numpy file\nmnist_file = '\/kaggle\/input\/learn2reg-mnist\/mnist.npz'\nx_train_load = np.load(mnist_file)['x_train']\ny_train_load = np.load(mnist_file)['y_train']\nx_test_load = np.load(mnist_file)['x_test']\ny_test_load = np.load(mnist_file)['y_test']\n\n# extract only instances of the digit 5\nx_train = x_train_load[y_train_load==5, ...]\ny_train = y_train_load[y_train_load==5]\nx_test = x_test_load[y_test_load==5, ...]\ny_test = y_test_load[y_test_load==5]\n\n# let's get some shapes to understand what we loaded.\nprint('shape of x_train: ', x_train.shape)\nprint('shape of y_train: ', y_train.shape)","938ae7d1":"nb_val = 1000 # keep 10,000 subjects for validation\nx_val = x_train[-nb_val:, ...]  # this indexing means \"the last nb_val entries\" of the zeroth axis\ny_val = y_train[-nb_val:]\nx_train = x_train[:-nb_val, ...]\ny_train = y_train[:-nb_val]","57107366":"nb_vis = 5\n\n# choose nb_vis sample indexes\nidx = np.random.choice(x_train.shape[0], nb_vis, replace=False)\nexample_digits = [f for f in x_train[idx, ...]]\n\n# plot\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","94e8237a":"# fix data\nx_train = x_train.astype('float')\/255\nx_val = x_val.astype('float')\/255\nx_test = x_test.astype('float')\/255\n\n# verify\nprint('training maximum value', x_train.max())","bf51ca40":"# re-visualize\nexample_digits = [f for f in x_train[idx, ...]]\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","507b5c8d":"pad_amount = ((0, 0), (2,2), (2,2))\n\n# fix data\nx_train = np.pad(x_test, pad_amount, 'constant')\nx_val = np.pad(x_test, pad_amount, 'constant')\nx_test = np.pad(x_test, pad_amount, 'constant')\n\n# verify\nprint('shape of training data', x_train.shape)","09980c14":"ndims = 2\nvol_shape = x_train.shape[1:]\nnb_enc_features = [32, 32, 32, 32]\nnb_dec_features = [32, 32, 32, 32, 32, 16]","de20c007":"# first, let's get a unet (before the final layer)\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);","e1318287":"# inputs\nprint('numer of inputs', len(unet.inputs))\nmoving_input_tensor = unet.inputs[0]\nfixed_input_tensor = unet.inputs[1]\n    \n# output\nprint('output:', unet.output)","9cfde4d7":"# transform the results into a flow field.\ndisp_tensor = keras.layers.Conv2D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# check\nprint('displacement tensor:', disp_tensor)","de6ee22c":"# a cool aspect of keras is that we can easily form new models via tensor pointers:\ndef_model = keras.models.Model(unet.inputs, disp_tensor)\n# def_model will now *share layers* with the UNet -- if we change layer weights \n# in the UNet, they change in the def_model ","898f4460":"spatial_transformer = neuron.layers.SpatialTransformer(name='spatial_transformer')\n\n# warp the image\nmoved_image_tensor = spatial_transformer([moving_input_tensor, disp_tensor])","b09d5b2f":"inputs = [moving_input_tensor, fixed_input_tensor]\noutputs = [moved_image_tensor, disp_tensor]\nvxm_model = keras.models.Model(inputs, outputs)","495c606f":"# losses. Keras recognizes the string 'mse' as mean squared error, so we don't have to code it\nlosses = ['mse', vxm.losses.Grad('l2').loss]\n\n# usually, we have to balance the two losses by a hyper-parameter.\nlambda_param = 0.05\nloss_weights = [1, lambda_param]","e029cc45":"vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)","50d53649":"def vxm_data_generator(x_data, batch_size=32):\n    \"\"\"\n    generator that takes in data of size [N, H, W], and yields data for our vxm model\n    \n    Note that we need to provide numpy data for each input, and each output\n    \n    inputs:  moving_image [bs, H, W, 1], fixed_image [bs, H, W, 1]\n    outputs: moved_image  [bs, H, W, 1], zeros [bs, H, W, 2]\n    \"\"\"\n    # preliminary sizing\n    vol_shape = x_data.shape[1:] # extract data shape\n    ndims = len(vol_shape)\n    \n    # prepare a zero array the size of the deformation. We'll explain this below.\n    zero_phi = np.zeros([batch_size, *vol_shape, ndims])\n    \n    while True:\n        # prepare inputs\n        # inputs need to be of the size [batch_size, H, W, number_features]\n        #   number_features at input is 1 for us\n        idx1 = np.random.randint(0, x_data.shape[0], size=batch_size)\n        moving_images = x_data[idx1, ..., np.newaxis]\n        idx2 = np.random.randint(0, x_data.shape[0], size=batch_size)\n        fixed_images = x_data[idx2, ..., np.newaxis]\n        inputs = [moving_images, fixed_images]\n        \n        # outputs\n        # we need to prepare the \"true\" moved image.  \n        # Of course, we don't have this, but we know we want to compare \n        # the resulting moved image with the fixed image. \n        # we also wish to penalize the deformation field. \n        outputs = [fixed_images, zero_phi]\n        \n        yield inputs, outputs        ","0015fe9e":"# let's test it\ntrain_generator = vxm_data_generator(x_train)\ninput_sample, output_sample = next(train_generator)\n\n# visualize\nslices_2d = [f[0,...,0] for f in input_sample + output_sample]\ntitles = ['input_moving', 'input_fixed', 'output_moved_ground_truth', 'zero']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","a0c624a3":"nb_epochs = 10\nsteps_per_epoch = 100\nhist = vxm_model.fit_generator(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);","627db204":"# as with other imports, this import should be at the top, or use notebook matplotlib magic\n# we keep it here to be explicit why we need it\nimport matplotlib.pyplot as plt\n\ndef plot_history(hist, loss_name='loss'):\n    \"\"\"\n    Quick function to plot the history \n    \"\"\"\n    plt.figure()\n    plt.plot(hist.epoch, hist.history[loss_name], '.-')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.show()\n\nplot_history(hist)","7d69ad93":"# let's get some data\nval_generator = vxm_data_generator(x_val, batch_size = 1)\nval_input, _ = next(val_generator)","53281bc9":"val_pred = vxm_model.predict(val_input)","d7b752de":"# %timeit is a 'jupyter magic' that times the given line over several runs\n%timeit vxm_model.predict(val_input)","dac8823b":"# visualize\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","c48d028e":"neuron.plot.flow([val_pred[1].squeeze()], width=5);","3c1298b5":"# extract only instances of the digit 5\nsys.exit(\"Predict before you run!\")\nx_sevens = x_train_load[y_train_load==7, ...].astype('float')\/255\nx_sevens = np.pad(x_sevens, pad_amount, 'constant')\n\nseven_generator = vxm_data_generator(x_sevens, batch_size=1)\nseven_sample, _ = next(seven_generator)\nseven_pred = vxm_model.predict(seven_sample)","a158d41c":"# visualize\nslices_2d = [f[0,...,0] for f in seven_sample + seven_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","e8b41ccd":"sys.exit(\"Predict before you run!\")\nfactor = 5\nval_pred = vxm_model.predict([f*factor for f in val_input])\n\n# visualizeb\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","aece5633":"# we've prepared the data in the following files\n# prepared as N x H x W\ncore_path = '\/kaggle\/input\/mri-2d\/'\nx_train = np.load(os.path.join(core_path, 'train_vols.npy'))\nx_val = np.load(os.path.join(core_path, 'validate_vols.npy'))\n# x_test = np.load(os.path.join(core_path, 'test_vols.npy'))\n\nvol_shape = x_train.shape[1:]\nprint('train shape:', x_train.shape)","be0a447c":"# extract some brains\nnb_vis = 5\nidx = np.random.randint(0, x_train.shape[0], [5,])\nexample_digits = [f for f in x_train[idx, ...]]\n\n# visualize\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","0795dc86":"# unet\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);\ndisp_tensor = keras.layers.Conv2D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# spatial transfomer\nspatial_transformer = neuron.layers.SpatialTransformer(name='image_warping')\nmoved_image_tensor = spatial_transformer([unet.inputs[0], disp_tensor])\n\n# final model\nvxm_model = keras.models.Model(unet.inputs, [moved_image_tensor, disp_tensor])","7034b433":"# losses. Keras recognizes the string 'mse' as mean squared error, so we don't have to code it\nlosses = ['mse', vxm.losses.Grad('l2').loss]\n\n# usually, we have to balance the two losses by a hyper-parameter.\nlambda_param = 0.01\nloss_weights = [1, lambda_param]","f26584e0":"vxm_model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=losses, loss_weights=loss_weights)","f5dab5b2":"# let's test it\ntrain_generator = vxm_data_generator(x_train, batch_size=8)\ninput_sample, output_sample = next(train_generator)\n\n# visualize\nslices_2d = [f[0,...,0] for f in input_sample + output_sample]\ntitles = ['input_moving', 'input_fixed', 'output_sample_true', 'zero']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","c49dbb0a":"nb_epochs = 10\nsteps_per_epoch = 10","d9241b64":"hist = vxm_model.fit_generator(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);","a332120c":"# for the purpose of the tutorial we ran very few epochs.  \n# Here we load a model that was run for 10 epochs and 100 steps per epochs\nvxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_shortrun.h5')","989de190":"# as before, let's visualize what happened\nplot_history(hist)","b50f7c6d":"val_generator = vxm_data_generator(x_val, batch_size = 1)","38fb37d2":"val_input, _ = next(val_generator)","4667fb85":"val_pred = vxm_model.predict(val_input)","09aff302":"# visualize\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","df2d38c4":"flow = val_pred[1].squeeze()[::3,::3]\nneuron.plot.flow([flow], width=5);","7e4f08f3":"vxm_model.save_weights('brain_2d_shortrun.h5')","8be081fe":"vxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_shortrun.h5')\nour_val_pred = vxm_model.predict(val_input)\n\nvxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_mseonly.h5')\nmse_val_pred = vxm_model.predict(val_input)","3ea1a246":"# visualize both models\nslices_2d = [f[0,...,0] for f in [val_input[1]] + our_val_pred ]\ntitles = ['input_fixed', 'our_pred_moved', 'our_disp_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);\n\n# visualize both models\nslices_2d = [f[0,...,0] for f in [val_input[1]] + mse_val_pred]\ntitles = ['input_fixed', 'mse_pred_moved', 'mse_pred_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","16293f02":"neuron.plot.flow([f[1].squeeze()[::3,::3] for f in [our_val_pred, mse_val_pred]], width=10);","58c52ae2":"# our data will be of shape 160 x 192 x 224\nvol_shape = [160, 192, 224]\nndims = 3","cab00004":"nb_enc_features = [16, 32, 32, 32]\nnb_dec_features = [32, 32, 32, 32, 32, 16, 16]","2efca7ce":"# unet\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);\ndisp_tensor = keras.layers.Conv3D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# spatial transfomer\nspatial_transformer = neuron.layers.SpatialTransformer(name='image_warping')\nmoved_image_tensor = spatial_transformer([unet.inputs[0], disp_tensor])\n\n# final model\nvxm_model = keras.models.Model(unet.inputs, [moved_image_tensor, disp_tensor])","a3204eef":"val_volume_1 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/subject_1_vol.npz')['vol_data']\nseg_volume_1 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/subject_1_seg.npz')['vol_data']\nval_volume_2 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/atlas_norm_3d.npz')['vol']\nseg_volume_2 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/atlas_norm_3d.npz')['seg']\n\n\nval_input = [val_volume_1[np.newaxis, ..., np.newaxis], val_volume_2[np.newaxis, ..., np.newaxis]]","fc5ad8a9":"vxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/\/cvpr2018_vm2_cc.h5')","ed9daa82":"val_pred = vxm_model.predict(val_input);","80814031":"moved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]","825e0138":"mid_slices_fixed = [np.take(val_volume_2, vol_shape[d]\/\/2, axis=d) for d in range(ndims)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]\/\/2, axis=d) for d in range(ndims)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nneuron.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);","24e5185f":"warp_model = vxm.networks.nn_trf(vol_shape)","5d88fe0e":"warped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])","4395ccbc":"from pytools import plotting as pytools_plot\nimport matplotlib\n\n[ccmap, scrambled_cmap] = pytools_plot.jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)","d44d2ebd":"mid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]\/\/1.8, axis=d) for d in range(ndims)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]\/\/1.8, axis=d) for d in range(ndims)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nneuron.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);","25e713da":"%timeit vxm_model.predict(val_input)","b41c4b53":"Let's visualize the segmentations, and essentially make sure they are not crazy","d6c13a60":"### References\nA good chunk of this tutorial build on work from [voxelmorph](https:\/\/github.com\/voxelmorph\/voxelmorph) ([TMI](https:\/\/arxiv.org\/abs\/1809.05231) and [MedIA](https:\/\/arxiv.org\/abs\/1903.03545)) and code from [neuron](https:\/\/github.com\/adalca\/neuron) ([CVPR](http:\/\/arxiv.org\/abs\/1903.03148))","11f7266c":"Note that separating your data in *only* train\/test **often leads to problems**   \nYou wouldn't want to iteratively (A) build a model, (B) train on training data, and (C) test on test data  \nDoing so will **overfit to you test set** (because you will have adapted your algorithm to your test data  \n\nWe will split the 'training' into 'train\/validate' data, and keep the test set for later  \nAnd will only look at the test data at the very end (once we're ready to submit the paper!)  ","1e0e0373":"We will now register slightly more realistic data: MRIs of the brain.  \nTo be able to train and easily register during this tutorial,  \nwe will first extract the middle slice of brain scans. \n\nNothat because this task does not capture deformations in the third dimensions,  \ncertain  correspondances are not exactly possible.  Nonetheless, this exercise  \nwill illustrate registration with more realistic complex images.   \n\nThe brains have been intensity-normalized affinely aligned, and skull stripped  \nwith FreeSurfer, to enable focusing on deformable registration  ","8506ead5":"---","1fa735db":"---","26b824af":"It's always a good idea to visualize the loss, not just read off the numbers  \nThis will give us a better idea of whether it's converged, etc.  \nTensorflow offers a powerful interactive system for visualizing called tensorboard  \nFor this short tutorial, we will simply plot the loss","cce84854":"### Loss","71d606e3":"from experimentation, we have found the Adam optimizer learning rate of `1e-4`  \nperforms better than `1e-3` for this problem. ","8b2a5850":"and that's it! \n\nEven though this is on MNIST only, let's see how long this takes","8aff0cb3":"# Preamble\n## Setup of environment","a6b8c57f":"At first look, the mse-only model matches the fixed image better  \nBut we can see that it obtains a deformation field that is very noisy, unlikely to be anatomically meaningful  ","7325d41b":"Finally, let's define the actual loss. The way keras works, we need to define a loss for each output.    \nThe first loss is easy, it's simply MSE between the warped image $m \\circ \\phi$.\nFor the second, we will use a spatial gradient of the displacement.  \nWe won't code this from scratch here, but we'll use the `voxelmorph` implementation.","fa5648fc":"Clearly, this is not converged, and you should run it to convergence.  \nFor the purposes of this tutorial, we'll move on.","976c2197":"Given two images, out goal is to find the deformation between them  \n\nIn learning-based methods, we use a network that takes in two images $m$ and $f$ (e.g. MNIST digits of size 32x32)    \nand outputs a dense deformation $\\phi$ (e.g. size 32x32x2, because at each pixel we want a vector telling us where to go)  \n\n**Note**: registration also includes (or refers to) affine transforms, but we ignore that here  ","3fc8b235":"*Registration*: `predict()` essentially executes the network given an input.","ad471150":"# Registration of Brain MRI","a9f21546":"Finally, we get to 3D models, which are of particular interest in medical image analysis. \n\nHowever, due to the size of the models and data, we won't be able to train a model  \nwithin a short tutorial time. Instead, here we assume one has been trained, and demonstrate its use.\nYou can train one very similar to how you trained the 2D models above.","4e318564":"---","1970e2cc":"We're going to start by registering 2D MNIST digits, and then move on to medical data later.  \nIf the data is small (like 2D MNIST), you can often load it in memory, which enables for faster training and testing.  \nIf the data is large (large 3D scans), we will need to load the scans on demand. More on this later...\n\nFirst, we're going to **load the data**  \nLuckily, MNIST comes with the keras framework, so we can just load it here as follows\n\n","eb948e87":"Looks good, time to **train the model**","bade975d":"We'll start with some common imports  ","8202b2fa":"### Validation data","69165eeb":"Load a trained 3D model","82617f98":"Finally, we can compile the model. \nThis sets up the model for training, by associating the model with a loss and an optimizer","8787a556":"Let's register","019a9e1b":"# Generalization \nHow do learning-based methods generalize beyond training distribution ?","09f196be":"In a **supervised setting** we would have ground truth deformations $\\phi_{gt}$,  \nand we could use a supervised loss like MSE $= \\| \\phi - \\phi_{gt} \\|$","13d99091":"Throughout this tutorial we assume that the images have been rigidly aligned in a (roughly) similar space.  \nRigid alignment is also possible with unsupervised learning-based registration, but it's not our focus here.  ","e8dafa89":"Let's look at the segmentations! To do this, we'll need to warp segmentations. ","0c7ceeab":"Next, we import two packages that will help us   \n- [neuron](https:\/\/github.com\/adalca\/neuron) is a library for medical image analysis with tensorflow  \n- [voxelmorph](http:\/\/voxelmorph.mit.edu) is a deep-learning based registration library  ","2f01015c":"---","3d3bd855":"With pair-wise optimization methods (like most classical methods),  \nto register a new pair you would need to optimize a deformation field.  \n\nWith learning based registration, we simply evaluate the network for a new input pair  ","191c71cb":"An important caveat to learning-based registration is that  \nThey will, in general, only register samples fromt he distribution they've been trained from  \n\nSo, what happens if we register two 7s? Predict the behaviour before running the code bellow!","94dcc0e2":"let's explore the model bit","2141ed28":"As with MNIST, we'll start with losses of 'mse' and spatial smoothing","2c627c4c":"### Model","83f10733":"~3ms per registration is quite fast, even for MNIST.  \nLet's visualize the results","1cc1c572":"# Introduction to Unsupervised Deep Learning Registration\n[Adrian Dalca](http:\/\/adalca.mit.edu) for the [learn2reg 2019 tutorial](http:\/\/learn2reg.github.io)\n\nUnsupervised is also often referred to end-to-end\/self-supervised\n\n### Outline\n- **Core concepts with MNIST**   \nWe will first learn to deal with data, building a model, training, registration and generalization\n- **More realistic complexity: Brain MRI (2D slices)**  \nWe will then show how these models work for 2d slices of brain scans, presenting a more complex scenario    \n- **Realistic 3D Brain MRI**  \nWe will illustrate full 3D registration\n- **Advances topics**  \nFinally, we close with more advanced topics, including diffeomorphisms and fine-tuning deformations","99a60a87":"# 3D MRI brain scan registration","9cd7c333":"What we often do isntead of use **external anotations** for evaluation  \nOne way is using anatomical segmentations.  \n\nIn the next section, we demonstrate the use of a 3D model, and show how to evaluate it with segmentations ","4f8e5d3b":"### Model","f9871f27":"In our tests, a run is 10s, for an entire 3D volume.  \nClassically, this would take hours.","9e9bf0cc":"## runtime","1cf435dc":"okay, we need to make sure the final output has 2 features,  \nrepresenting the deformation at each voxel","e60dddc0":"Was your prediction correct? Describe the behaviour.","9e2baa30":"Believeing we are done loading, it's always great to visualize the data  \nHere, we use some tools from a package called `neuron`, which uses matplotlib  \nYou could use matplotlib as well directly, but it would just be a bit messier  \nand here we want to illustrate the main concepts.  ","f50c3196":"### Visualize Data","dfba68a4":"An important advantage of learning-based methods is the dramatically lowered runtime","3cdc08f6":"# Evaluation","a67932f9":"Luckily, we can use the same generator as before, since we're using the same format.  \nLet's test it first","9612f546":"# Registration","8b0c5209":"Did it work? If so, why? Did it match your prediction?\n\nLet's try a different variation.  \nWhat if we just modify the (original) set, but multiplied the intensities by a factor? Write down your prediction!","9fbc1270":"The main idea in **unsupervised registration** is to use loss inspired by classical registration  \n\nWithout supervision, how do we know this deformation is good?  \n(1) make sure that $m \\circ \\phi$ ($m$ warped by $\\phi$) is close to $f$  \n(2) regularize $\\phi$ (often meaning make sure it's smooth)  ","0b55220d":"Evaluating registration results is tricky.  \nThe first tendancy is to look at the images (as above),  \nand conclude that if they match, The registration has succeeded. \n\nHowever, this can be achieved by an optimization that only penalizes the image matching term  \nFor example, in the next cell we compare our model with one that wastrained on maximizing MSE only (no smoothness loss)","5ae4d1f8":"As before, we create a model based on a unet, plus a spatial transformer","29fcbedc":"---","da76fa25":"To make sure the moved_image is close to the fixed image, and  \nto achieve smoothness loss of $\\phi$ in (2), we will want these two as outputs from the full model ","a96b4205":"Looks good!  \n\nHowever, luckily we included a **colorbar**, which shows us that the data is in [0, 255].  \nIn neural networks it's often great to work in the ranges of [0, 1] or [-1, 1] or around there.  \nLet's fix this. \n\nIn general, you should always plot your data with colorbars, which helps you catch issues before training  ","f45f513e":"# Data","5d81ae33":"To train, we need to make sure the data is in the right format and fed to the model the way we want it  \nkeras models can be trained with `model.fit`, which requires all the data to be in a big array, or `model.fit_generator`, which requires a python generator that gives you batches of data\n\nLet's code a simple data generator based on the MNIST data","4afb55dd":"# Train Model","d124cd7f":"To achieve (1), we need to *warp* input image $m$.  \nTo do this, we use a spatial transformation network layer, which essentially does linear interpolation  \nThis layer is defined in [neuron](https:\/\/github.com\/adalca\/neuron)","27460325":"One last change. Later on, we'll see that some of the most popular models  \nlike to have inputs that are sized as multiples of 2^N for N being the number of layers  \nHere, we force our images to be size 32 (2x 2^4)","22ea2d35":"We're going to abstract the UNet for this tutorial and use a function from [voxelmorph](http:\/\/voxelmorph.mit.edu)  \n`vxm.networks.unet_core()` enables this","23f25b1e":"Let's visualize the flow a bit better","109103c4":"let's see some results","2920b73d":"Given that the displacement $\\phi$ is output from the network,  \nwe need to figure out a loss to tell if it makes sense","bbd095bf":"# CNN Model","de5697df":"Understanding when the network generalizes and when it does not is very important, and still a part of active research  ","b0b80d4f":"let's save out model weights, we'll use them in a bit","0c0dda39":"We're first going to prepare a colormap","c71f49b1":"The 208 volumes are of size `160x192`.  \nLet's look at some"}}