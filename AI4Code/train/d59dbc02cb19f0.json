{"cell_type":{"87ebd6d4":"code","ef90672c":"code","47507264":"code","69827d1d":"code","3a835200":"code","5409cd84":"code","f9dc63c9":"code","cf4ad088":"code","20c45631":"code","1871178c":"code","04f74955":"code","c1928341":"code","8bc4a550":"code","a08a0e99":"code","ff91e548":"markdown","a84a5b91":"markdown","4e1f275d":"markdown","826f14c8":"markdown","5444335f":"markdown","99727643":"markdown","e4609bca":"markdown","2ded4e77":"markdown","8745ca6d":"markdown","43e9b2ca":"markdown","133da29c":"markdown","ee88860e":"markdown","c0fe5d73":"markdown"},"source":{"87ebd6d4":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n#plt.style.use('fivethirtyeight')\nplt.rcParams.update({'font.size': 12})\n#plt.rcParams[\"figure.figsize\"] = (12, 4)\nfrom datetime import date","ef90672c":"!pip install -q gluonts","47507264":"train = pd.read_csv(\"..\/input\/g-research-crypto-forecasting\/train.csv\")\n# extract the data corresponding to Bitcoin (Asset_ID = 1)\nbitcoin = train.query(\"Asset_ID == 1\").reset_index(drop = True)\nbitcoin['timestamp'] = pd.to_datetime(bitcoin['timestamp'], unit='s')\nbitcoin = bitcoin.set_index('timestamp')\n# extract the \"High\" value at 9:00 a.m. daily\nindexer_9am = bitcoin.index.indexer_at_time('9:00:00')\nvalues_at_9am = bitcoin.iloc[indexer_9am]\nhigh_values_at_9am = values_at_9am[[\"High\"]]\n# take a look\nhigh_values_at_9am","69827d1d":"freq = \"1D\"             # the frequency of our data, here daily\ncontext_length    = 180 # train on this number of days\nprediction_length =  90 # predict these many days, these are removed from the end of the training data","3a835200":"from gluonts.dataset.common import ListDataset\n\ndata_list = [{\"start\": \"2018-01-01 09:00:00\", \"target\": high_values_at_9am[c].values} for c in high_values_at_9am.columns]\ntrain_ds  = ListDataset(data_iter=data_list,freq=freq)","5409cd84":"from gluonts.model.simple_feedforward  import SimpleFeedForwardEstimator\n# to use the DeepAREstimator \n# from gluonts.model.deepar import DeepAREstimator\nfrom gluonts.mx.distribution.student_t import StudentTOutput\nfrom gluonts.mx import Trainer\n\nestimator = SimpleFeedForwardEstimator(num_hidden_dimensions=[50],\n                                       freq=freq,\n                                       context_length=context_length,\n                                       prediction_length=prediction_length,\n                                       distr_output=StudentTOutput(),\n                                       trainer=Trainer(epochs=50,\n                                                       learning_rate=1e-3,\n                                                       num_batches_per_epoch=100,\n                                                       patience=10))\n\npredictor = estimator.train(train_ds)\nprint(\"Done\")","f9dc63c9":"from gluonts.evaluation import make_evaluation_predictions\n\nforecast_it, ts_it = make_evaluation_predictions(\n    dataset=train_ds,  # dataset\n    predictor=predictor,  # predictor\n    num_samples=2000,  # number of sample paths we want for evaluation\n)\n\nforecasts = list(forecast_it)\ntss = list(ts_it)","cf4ad088":"def plot_prob_forecasts(ts_entry, forecast_entry):\n    plot_length = context_length + prediction_length\n    prediction_intervals = (50.0, 90.0)\n    legend = [\"ground truth\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n\n    fig, ax = plt.subplots(1, 1, figsize=(18, 7))\n    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n    forecast_entry.plot(prediction_intervals=prediction_intervals, color='cadetblue')\n    plt.axvline(forecast_entry.start_date, color='g', lw=1) # end of train dataset\n    plt.grid(which=\"major\")\n    plt.legend(legend, loc=\"upper left\")\n    plt.show();\n    \nplot_prob_forecasts(tss[0], forecasts[0])","20c45631":"# first entry of the forecast list\nforecast_entry = forecasts[0]\nprint(f\"Mean of the future window:\\n {forecast_entry.mean}\")\nprint(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")","1871178c":"!pip install -q hfda\nimport hfda","04f74955":"k_max = 15\nD = hfda.measure(np.concatenate(high_values_at_9am.values), k_max)\nprint(\"Higuchi fractal dimension = %.2f\" % D)","c1928341":"bitcoin_minutes = bitcoin[[\"High\"]].tail(1358)\n\nD = hfda.measure(np.concatenate(bitcoin_minutes.values), k_max)\nprint(\"Higuchi fractal dimension = %.2f\" %D)","8bc4a550":"freq = \"1min\"           # the frequency of our data, now in minutes\ncontext_length    = 180 # train on this number of days\nprediction_length =  90 # predict these many days, these are removed from the end of the training data\n\ndata_list = [{\"start\": \"2021-09-20 01:23:00\", \"target\": bitcoin_minutes[c].values} for c in bitcoin_minutes.columns]\ntrain_ds  = ListDataset(data_iter=data_list,freq=freq)\n\nestimator = SimpleFeedForwardEstimator(num_hidden_dimensions=[50],\n                                       freq=freq,\n                                       context_length=context_length,\n                                       prediction_length=prediction_length,\n                                       distr_output=StudentTOutput(),\n                                       trainer=Trainer(epochs=50,\n                                                       learning_rate=1e-3,\n                                                       num_batches_per_epoch=100,\n                                                       patience=10))\n\npredictor = estimator.train(train_ds)","a08a0e99":"forecast_it, ts_it = make_evaluation_predictions(\n    dataset=train_ds,  # dataset\n    predictor=predictor,  # predictor\n    num_samples=2000,  # number of sample paths we want for evaluation\n)\n\nforecasts = list(forecast_it)\ntss = list(ts_it)\n\nplot_prob_forecasts(tss[0], forecasts[0])","ff91e548":"Indeed it does seem that forecasting on the minute scale is just as challenging as forecasting on the daily scale.\n\n### Related packages\n* [NGBoost: Natural Gradient Boosting for Probabilistic Prediction](https:\/\/stanfordmlgroup.github.io\/projects\/ngboost\/)\n\n### Related kaggle notebooks\n* [\"M5 - Sales Uncertainty Prediction\"](https:\/\/www.kaggle.com\/allunia\/m5-sales-uncertainty-prediction) written by [Laura Fink](https:\/\/www.kaggle.com\/allunia)\n* [\"M5 Forecasting Competition GluonTS Template\"](https:\/\/www.kaggle.com\/steverab\/m5-forecasting-competition-gluonts-template) written by [Stephan Rabanser](https:\/\/www.kaggle.com\/steverab)\n\n### Related reading\n* [Rob J. Hyndman and George Athanasopoulos \"*Forecasting: Principles and Practice*\", (3rd Edition)](https:\/\/otexts.com\/fpp3\/)\n* [Fotios Petropoulos, *et al. \"Forecasting: Theory and practice*\", arXiv:2012.03854 (2020)](https:\/\/arxiv.org\/pdf\/2012.03854.pdf)\n* [DeepAR Forecasting Algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html)\n* [Benoit B. Mandelbrot \"*The Variation of Certain Speculative Prices*\", The Journal of Business **Vol. 36** pp. 394-419 (1963)](https:\/\/www.jstor.org\/stable\/2350970)\n* [Benoit B. Mandelbrot \"*The Variation of Some Other Speculative Prices*\", The Journal of Business **Vol. 40** pp. 393-413 (1967)](https:\/\/www.jstor.org\/stable\/2351623)\n* [Benoit B. Mandelbrot, Adlai J. Fisher and Laurent E. Calvet \"*A Multifractal Model of Asset Returns*\", Cowles Foundation Discussion Paper No. 1164 (1997)](https:\/\/ssrn.com\/abstract=78588)\n* [T. Higuchi \"*Approach to an irregular time series on the basis of the fractal theory*\", Physica D: Nonlinear Phenomena **Vol. 31** pp. 277-283 (1988)](https:\/\/doi.org\/10.1016\/0167-2789(88)90081-4)","a84a5b91":"### Visualization\nAnd finally we shall now plot our probabilistic forecast (here with the 50% and 90% prediction intervals shown) along with the ground truth values","4e1f275d":"We can extract the mean and median values of our forecast as follows:","826f14c8":"For our data we shall be using the dataset provided by the kaggle [G-Research Crypto Forecasting](https:\/\/www.kaggle.com\/c\/g-research-crypto-forecasting) competition. We shall extract the data corresponding to the Bitcoin cryptocurrency. From this Bitcoin data we shall use only one datapoint each day, arbitrarily chosen to be the **High** value at 9:00 a.m.","5444335f":"We shall forecast three months (90 days) worth of data, based on the 180 days prior to the start of the forecasting period","99727643":"### Prediction\nWe shall now make (here 2000) predictions using our model","e4609bca":"It is worth mentioning that if desired [point forecasts](https:\/\/ts.gluon.ai\/tutorials\/forecasting\/extended_tutorial.html#Point-forecasts-with-a-simple-feedforward-network) can also be made using GluonTS.\n\n### What about the minutes?\nIn the above example we have taken one sample per day, however, we have much more fine grained data at our disposal. Would we do better if we looked at the data on the scale of minutes? It was [Benoit Mandelbrot](https:\/\/en.wikipedia.org\/wiki\/Benoit_Mandelbrot) that was the first to suggest that stock data may be [fractal](https:\/\/en.wikipedia.org\/wiki\/Fractal) in nature; the data looking similar no matter what the scale. To quantify this somewhat here we shall calculate the [Higuchi fractal dimension](https:\/\/en.wikipedia.org\/wiki\/Higuchi_dimension) for the daily sample above, and for a similar sample size on the minute scale. To do this we shall use the [HFDA](https:\/\/github.com\/hiroki-kojima\/HFDA) package written by Hiroki Kojima. ","2ded4e77":"Now we shall create the minutes dataset","8745ca6d":"Indeed the values seem similar, with only a slightly lower value for the minute data . Let us repeat a probabilistic forecast, now for the minutes data","43e9b2ca":"Install GluonTS","133da29c":"### Training\nHere we use the GulonTS\u2019s pre-built [feedforward neural network estimator](https:\/\/ts.gluon.ai\/api\/gluonts\/gluonts.model.simple_feedforward.html) `SimpleFeedForwardEstimator` in conjunction with a [trainer](https:\/\/ts.gluon.ai\/api\/gluonts\/gluonts.mx.trainer.html). This feedforward network can be [substituted for a recurrent neural network (RNN)](https:\/\/ts.gluon.ai\/tutorials\/forecasting\/extended_tutorial.html#From-feedforward-to-RNN), such as the  [DeepAREstimator](https:\/\/ts.gluon.ai\/api\/gluonts\/gluonts.model.deepar.html).","ee88860e":"Convert our dataframe into a GluonTS dataset","c0fe5d73":"### Probabilistic forecasting using [GluonTS](https:\/\/ts.gluon.ai\/index.html): Bitcoin example\n[Probabilistic forecasting](https:\/\/en.wikipedia.org\/wiki\/Probabilistic_forecasting), rather than providing a single point prediction, provides a probability distribution as the outcome.\nTo do this we shall be using the [GluonTS - Probabilistic Time Series Modeling](https:\/\/ts.gluon.ai\/index.html) package. This notebook is heavily based on the [Quick Start Tutorial](https:\/\/ts.gluon.ai\/tutorials\/forecasting\/quick_start_tutorial.html) and the [Extended Forecasting Tutorial](https:\/\/ts.gluon.ai\/tutorials\/forecasting\/extended_tutorial.html) that are both provided with the package."}}