{"cell_type":{"84d5caa1":"code","2229c137":"code","eaaa5a85":"code","ea80bb2e":"code","9f0cfdfd":"code","4dbe4de0":"code","9f8ea777":"code","7c5bb456":"code","1a84def3":"code","1597609d":"code","9d974e6b":"code","1255d4b3":"markdown","daea1d34":"markdown","a733f7e9":"markdown","399cb686":"markdown"},"source":{"84d5caa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2229c137":"# Importing all the required modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model, svm, dummy, metrics, neural_network\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_val_score, validation_curve, GridSearchCV\nfrom sklearn.decomposition import PCA\n\n# Importing the data frame downloaded from Kaggle\ndf = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","eaaa5a85":"# Separating features and Target variable from data frame to matrix\/array\nX = np.matrix(df)\nX = X[:, 0:12]\ny = np.array(df['DEATH_EVENT'])\nX.shape, y.shape","ea80bb2e":"# Sample Check of the variable data types and their entries\nprint(X,'\\n',y,'\\n',df.columns)\ndf.head()","9f0cfdfd":"# Feature Preprocessing and Train-Test-Split\nscaler = StandardScaler()\npoly = PolynomialFeatures(degree = 2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_train_poly = poly.fit_transform(X_train_scaled)\nX_test_poly = poly.fit_transform(X_test_scaled)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","4dbe4de0":"# Principal Component Analysis of scaled features\npca = PCA(n_components = 2).fit(X_train_scaled)\nX_train_pca = pca.transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\nX_train_pca.shape","9f8ea777":"# Visualising for any clustering\/separation in the labelled training examples after pca\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nplt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)\nplt.xlabel('First principal component')\nplt.ylabel('Second principal component')\nplt.show()","7c5bb456":"# Fitting Logistic Regression and Support Vector Machines with RBF and Linear kernels\nlr1 = linear_model.LogisticRegression(C = 1).fit(X_train_scaled, y_train)\nlr2 = linear_model.LogisticRegression(C = 0.1).fit(X_train_poly, y_train)\nsv1 = svm.SVC(gamma = 0.1, C = 1).fit(X_train_scaled, y_train)\nsv2 = svm.SVC(kernel='linear', C = 1).fit(X_train_scaled, y_train)\n\n# Dummy Classifier for baseline\ndm1 = dummy.DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n\n# Comparing the accuracies of these models\nprint('Logistic Regression\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(lr1.score(X_train_scaled, y_train), lr1.score(X_test_scaled, y_test)))\nprint('Logistic Regression (poly)\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(lr2.score(X_train_poly, y_train), lr2.score(X_test_poly, y_test)))\nprint('SVM RBF\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(sv1.score(X_train_scaled, y_train), sv1.score(X_test_scaled, y_test)))\nprint('SVM Linear\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(sv2.score(X_train_scaled, y_train), sv2.score(X_test_scaled, y_test)))\nprint('Dummy Classifier\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(dm1.score(X_train_scaled, y_train), dm1.score(X_test_scaled, y_test)))\n\n# Obtaining the Confusion Matrices\ny_lr = lr1.predict(X_test_scaled)\ny_sv = sv1.predict(X_test_scaled)\ny_lsv = sv2.predict(X_test_scaled)\nconfusion_lr = metrics.confusion_matrix(y_test, y_lr)\nconfusion_sv = metrics.confusion_matrix(y_test, y_sv)\nconfusion_lsv = metrics.confusion_matrix(y_test, y_lsv)\nprint('Logistic Regression\\n{}\\n'.format(confusion_lr))\nprint('SVM RBF\\n{}\\n'.format(confusion_sv))\nprint('SVM Linear\\n{}\\n'.format(confusion_lsv))\n\n# Checking the F1 Score and Recall of SVM with RBF Kernal\nprint('Logistic Regression\\n F1 score = {}\\t recall = {}\\n'.format(metrics.f1_score(y_test, y_lr), metrics.recall_score(y_test, y_lr)))\nprint('SVM RBF\\n F1 score = {}\\t recall = {}\\n'.format(metrics.f1_score(y_test, y_sv), metrics.recall_score(y_test, y_sv)))\nprint('SVM Linear\\n F1 score = {}\\t recall = {}\\n'.format(metrics.f1_score(y_test, y_lsv), metrics.recall_score(y_test, y_lsv)))","1a84def3":"# Validation curve for SVM Radial Basis Function\nmy_C = np.logspace(-2, 2)\ntrain_scores, test_scores = validation_curve(svm.SVC(kernel='rbf', gamma = 0.1), X_train_scaled, y_train, cv = 5, param_name = 'C', param_range = my_C)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\nplt.figure()\nplt.semilogx(my_C, train_scores_mean, label='Training score',color='darkorange', lw=2)\nplt.semilogx(my_C, test_scores_mean, label='Training score',color='navy', lw=2)\nplt.xlabel('C value', fontsize=16)\nplt.ylabel('Accuracy', fontsize=16)\nplt.legend(loc='best')\nplt.show()","1597609d":"# Precision-Recall Curve for C = 2\nsv3 = svm.SVC(kernel='rbf', C = 2, gamma = 0.1).fit(X_train_scaled, y_train)\ny_sv3 = sv3.predict(X_test_scaled)\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_sv3)\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()","9d974e6b":"# Model Predictions and Confusion Matrix for C = 2 RBF SVM\nconfusion_sv3 = metrics.confusion_matrix(y_test, y_sv3)\nprint('Model Predictions\\n{}\\n'.format(y_sv3))\nprint('Confusion Matrix\\n{}\\n'.format(confusion_sv3))\nprint('SVM RBF C = 2\\nTraining accuracy = {}\\tTest Accuracy = {}\\n'.format(sv3.score(X_train_scaled, y_train), sv3.score(X_test_scaled, y_test)))\nprint('SVM RBF C = 2\\nF1 score = {}\\t recall = {}\\t precision = {}\\n'.format(metrics.f1_score(y_test, y_sv3), metrics.recall_score(y_test, y_sv3), metrics.precision_score(y_test, y_sv3)))","1255d4b3":"___From the Valdation Curve, it appears that the best possible accuracies are obtained when C is near about 2.___","daea1d34":"___Out of these models, SVM Radial Basis Function shows the best results. So, let's focus on it.___","a733f7e9":"### Project on Heart failure clinical records","399cb686":"### Hence, at the default train-test-split of (75%-25%), by using the Support Vector Machine Algorithm with Radial Basis Function Kernel, we can predict whether a person will die by Heart failure or not with an accuracy of 81%, precision of 81% and recall of 63%\n\n#### The limit to the accuracy might be because of the small size of the data available"}}