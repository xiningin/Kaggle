{"cell_type":{"ace49670":"code","5ddfed7c":"code","afe86959":"code","64537a19":"code","866e3242":"code","51e12d65":"code","010b77ff":"code","80ef7bbd":"code","8241e432":"code","e4145136":"code","d22f9e4f":"code","37f89ae6":"code","a510124f":"code","39b56b86":"code","06e3cbb4":"code","630a215b":"code","033cf0b2":"code","0407c5f9":"code","091de885":"code","45321ff2":"code","d6260e35":"code","43e97c53":"markdown","a05ed736":"markdown","45ab960f":"markdown","4ef592a1":"markdown","3b998acd":"markdown","af17cce2":"markdown","b5e81ec6":"markdown","4329b057":"markdown","f7f29ec3":"markdown"},"source":{"ace49670":"import numpy as np\nimport pandas as pd\n\n\n#Visulization Library\nimport plotly.express as px\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings('ignore')","5ddfed7c":"data = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndata.head()","afe86959":"X=data.iloc[:,3:5]","64537a19":"X","866e3242":"plt.figure(figsize=(12,6))\n\nx = X['Annual Income (k$)']\n#y = X['Age']\nz = X['Spending Score (1-100)']\n\n#plt.scatter(x,y)\nplt.title('Mall Analysis')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\n#sns.lineplot(x, y, color = 'blue')\nsns.lineplot(x, z, color = 'green')\n\nplt.show()","51e12d65":"from sklearn.cluster import Birch\nms = Birch(branching_factor=50, n_clusters=5\n           , threshold=1.5)\nlabels=ms.fit_predict(X)\n","010b77ff":"fig = plt.figure(figsize=(12,6))\nplt.scatter(X['Annual Income (k$)'],X['Spending Score (1-100)'],c=labels)\nplt.title('Mall Analysis')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.show()","80ef7bbd":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n","8241e432":"wcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(X_std)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(1, 11), wcss)\nplt.title(' Elbow Method')\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()","e4145136":"\nkm = KMeans(n_clusters=5, max_iter=100)\nres=km.fit_predict(X_std)","d22f9e4f":"res","37f89ae6":"\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.scatter(X_std[res == 0, 0], X_std[res == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[res == 1, 0], X_std[res == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(X_std[res == 2, 0], X_std[res == 2, 1],\n            c='yellow', label='cluster 3')\nplt.scatter(X_std[res == 3, 0], X_std[res == 3, 1],\n            c='violet', label='cluster 4')\nplt.scatter(X_std[res == 4, 0], X_std[res == 4, 1],\n            c='red', label='cluster 5')\nplt.legend()\nplt.title('Mall Analysis')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nax.set_aspect('equal');","a510124f":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(X,method='ward')) # Within cluster variance is reduced with ward method\nplt.title('Dendogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()","39b56b86":"from sklearn.cluster import AgglomerativeClustering\nhc=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')\ny_hc=hc.fit_predict(X_std)","06e3cbb4":"y_hc","630a215b":"\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.scatter(X_std[y_hc == 0, 0], X_std[y_hc == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[y_hc == 1, 0], X_std[y_hc == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(X_std[y_hc == 2, 0], X_std[y_hc == 2, 1],\n            c='yellow', label='cluster 3')\nplt.scatter(X_std[y_hc == 3, 0], X_std[y_hc == 3, 1],\n            c='violet', label='cluster 4')\nplt.scatter(X_std[y_hc == 4, 0], X_std[y_hc == 4, 1],\n            c='red', label='cluster 5')\nplt.legend()\nplt.title('Mall Analysis')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nax.set_aspect('equal');","033cf0b2":"from sklearn.cluster import MeanShift\nms = MeanShift()","0407c5f9":"ms.fit_predict(X)","091de885":"labels = ms.labels_\nlabels","45321ff2":"cluster_center=ms.cluster_centers_\ncluster_center","d6260e35":"fig = plt.figure(figsize=(12,6))\nplt.scatter(X['Annual Income (k$)'],X['Spending Score (1-100)'])\nplt.scatter(cluster_center[:,0],cluster_center[:,1],marker='x',color='red')\nplt.title('Mall Analysis')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.show()","43e97c53":"### BIRCH Clustering\nBIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation\u2013maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.","a05ed736":"### K-Means Clustering\n\nK-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2, there will be two clusters, and for K=3, there will be three clusters, and so on.\n\nThe k-means clustering algorithm mainly performs two tasks:\n\nDetermines the best value for K center points or centroids by an iterative process.\nAssigns each data point to its closest k-center. Those data points which are near to the particular k-center, create a cluster.","45ab960f":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. Birch](#3)\n    \n* [4. KMeans](#4)   \n    \n* [5. Hierarchical](#5) \n      \n* [6. Mean Shift](#6)\n    \n* [7. END](#7)\n","4ef592a1":"####  clusters based on the annual income and the spending score","3b998acd":"### Mean Shift Clustering","af17cce2":"# <p style=\"background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Mall Customer Analysis<\/p>\n\nIn this notebook I have basically tried to compare different clustering algorithm. We have different types of clustering algorithm availabe such as kmeans, hierarchical, DBScan, MinShift, Birch etc.\n\n","b5e81ec6":"#### End of notebook","4329b057":"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.","f7f29ec3":"### Hirarchical Clustering\n\nIn data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n\n* **Agglomerative**: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n* **Divisive**: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy."}}