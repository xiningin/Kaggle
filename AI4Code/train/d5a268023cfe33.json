{"cell_type":{"11f520cd":"code","3f6f60bd":"code","c3d14cc9":"code","6a305e5a":"code","083c7de2":"code","0c36695d":"code","4da80fb5":"code","ba40e48e":"code","12383678":"code","ed6a0639":"code","93eacff4":"code","4e257e55":"code","f89e769e":"code","6f4bb1d7":"code","45f61327":"code","5b1ee1ff":"code","06d91929":"code","5737ae73":"code","1ff9df29":"code","24864f39":"code","0b73aa5e":"code","9227e964":"code","d0d5642b":"code","e73bf070":"code","3b166d63":"code","654ea6d4":"code","1b866851":"code","a4c908cb":"code","2b0595ff":"code","72ea2d7b":"code","c4fadaa8":"code","24953066":"markdown","5c006c41":"markdown","8ae1c23e":"markdown","029ac1e7":"markdown","cccb5d2e":"markdown","69d8b8ad":"markdown","3c5f2a15":"markdown","2b143368":"markdown","85d91dda":"markdown","b538be9b":"markdown","500944bf":"markdown","0dc6a14f":"markdown","ff6a8ca2":"markdown","e35d565c":"markdown","59cc0639":"markdown"},"source":{"11f520cd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3f6f60bd":"# Read data\n# Convert text to NaN\ndata = pd.read_excel('..\/input\/lel-6370-data\/LEL Modelling 220221.xlsx', na_values = '')\ndata.head(), data.tail()","c3d14cc9":"#Trim the edges\ndata = data.iloc[1:,2:5]\ndata.head()","6a305e5a":"# Select top 3 rows as column names for reference later\ncol_names = data.iloc[:2,:]\ncol_names","083c7de2":"# Define df to include header and data only\ndf = data.iloc[1:,:]\ndf.head()","0c36695d":"#Set row index as header\ndf.columns = df.iloc[0]\ndf.head()","4da80fb5":"# Select top row as header\n# Remove duplicated row\n# Reset row index\n# Print shape of df\n\ndf = df.drop(2)\ndf = df.reset_index(drop=True)\ndf.head()","ba40e48e":"# Apply isna for every row\n# Print number of rows with NaN\ndf.isna().sum()","12383678":"# Drop row if entire rows contain Na\n# df = df.dropna(how='all')\n# df","ed6a0639":"# Check rows containing Na\ndf.info()","93eacff4":"# Drop rows containing at least 1 Na\n#df = df.dropna()\n#df\n#df.isna().sum()\n#df.info()\n","4e257e55":"# Convert all datatype from object to float64\ndf = df.astype('float64')\ndf.info()","f89e769e":"# Define y as inlet LEL\nY = df.iloc[:,0]\nY","6f4bb1d7":"# Define X\nX = df.iloc[:,1:]\nX","45f61327":"# Split data into training and testing datasets\nfrom sklearn.model_selection import train_test_split\n\n#will be done in pipeline","5b1ee1ff":"# Check if data needs scaling or standardizing\n# will be done in pipeline","06d91929":"# Standardize data to have centered mean and unit standard deviation\n#from sklearn.preprocessing import StandardScaler\n#scaler1 = StandardScaler().fit(X1_train)\n#scaler2 = StandardScaler().fit(X2_train)\n#X1_train_s = scaler1.transform(X1_train)\n#X2_train_s = scaler2.transform(X2_train)\n#X1_test_s = scaler1.fit_transform(X1_test)\n#X2_test_s = scaler2.fit_transform(X2_test)\n#print(\"Mean of train1\", round(X1_train_s.mean()),\"Stdev of train1\", X1_train_s.std(), \"\\nMean of test1\", round(X1_test_s.mean()), \"Stdev of test1\", X1_test_s.std())\n#print(\"Mean of train2\", round(X2_train_s.mean()),\"Stdev of train2\", X2_train_s.std(), \"\\nMean of test2\", round(X2_test_s.mean()), \"Stdev of test2\", X2_test_s.std())","5737ae73":"#define function to run a chosen model to find the optimum polynomial transformation\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import mean_squared_error\n\nmodels = [LinearRegression(),Ridge(), Lasso()]\ndef poly_func(models, max):\n    for k in models:\n        print(\"\\n\", k,\":\")\n        df = pd.DataFrame(columns = ['Degree','Score'])\n        for i in range(1,max +1):\n            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n            pipe = make_pipeline(PolynomialFeatures(degree=i), k)\n            pipe.fit(x_train, y_train)\n            y_pred = pipe.predict(x_test)\n            score = pipe.score(x_test, y_test)\n            rmse = mean_squared_error(y_test, y_pred, squared = False)\n            coef = k.coef_\n            intercept = k.intercept_\n            df2 = pd.DataFrame([[i,score]],columns = ['Degree','Score'])\n            df = df.append(df2)\n            print(\"Polynomial degree:\",i, \" R2 =\", round(score,2), \" RMSE =\", round(rmse,2))\n    \n    \npoly_func(models,10)\n   ","1ff9df29":"# We use simple Linear Regression model to fit the training set. There will be two sets of modelling, one for y1 and y2\n# Once the model is trained, it is used to predict the outcome of the test data set, which it has not seen\n# The l\n","24864f39":"# The plot shows how well our model predict the outcome which is the LEL% from the test data set\n# The model has not been trained on the test set so the prediction is unbiased \n\nx_ax = range(len(X_test))\nplt.scatter(x_ax, Y_test, s=5, color=\"black\", label=\"Observation\")\nplt.scatter(x_ax, Y_pred, s=5, color=\"red\", label=\"Prediction\")\nplt.legend()\nplt.show()","0b73aa5e":"# Define X_names only to include X variable columns and transpose it from horizontal to vertical\n# Define coef to be a vertical dataframe of regression model coefficients\n#X_names = col_names.iloc[:,2:]\n#X_names.columns = range(X_names.shape[1])\n#linreg_importance = X_names.T\n#linreg_importance.columns = ['desc','unit','tag']\n\n#coef = pd.DataFrame(linreg2.coef_)\n#linreg_importance['coefficient'] = coef\n#linreg_importance['rank'] = linreg_importance['coefficient'].abs()\n#linreg_importance = linreg_importance.sort_values(by=['rank'], ascending = False)\n#linreg_importance['rank'] = range(1,len(linreg_importance.unit)+1)\n#linreg_importance","9227e964":"# Transform features to polynomial of degree 2\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nDegree = 4\npoly = PolynomialFeatures(degree=Degree)\nX_p = poly.fit_transform(X)\nX_p.shape\n#feature = np.array([[2,3]])\n#feature_new = poly.fit_transform(feature)\n#feature_new.shape\n#feature_new","d0d5642b":"# Split into train and test data sets\n\nX_p_train, X_p_test, Y_train, Y_test = train_test_split(X_p, Y, test_size=0.3, random_state=42)\nprint( \"X_p_train:\", X_p_train.shape, \"Y_train:\", Y_train.shape)\nprint(\"X_p_test:\", X_p_test.shape, \"Y_test:\", Y_test.shape)\n","e73bf070":"#Run linear regression\nlinregp = LinearRegression().fit(X_p_train, Y_train)\nY_pred = linregp.predict(X_p_test)\nscore = linregp.score(X_p_test, Y_test)\nrmse = mean_squared_error(Y_test, Y_pred, squared = False)\ncoefp = linregp.coef_\ninterp = linregp.intercept_\nprint(\"LinearRegression with polynomial degree =\",Degree, \": R2 =\", round(score,2), \"RMSE =\", round(rmse,2))\nprint(\"\\nLEL Model: Y =\", coefp[0],\"+\", coefp[1],\"Level +\", coefp[2], \"Flow +\", coefp[3], \"Level^2 +\", coefp[4], \"LevelFLow +\"\n      , coefp[5], \"Flow^2 +\", coefp[6], \"Level^3 +\", coefp[7], \"Level^2Flow +\", coefp[8], \"LevelFlow^2 +\", coefp[9], \"Flow^3 +\", \n     coefp[10], \"Level^4 +\", coefp[11], \"Level^3Flow +\", coefp[12], \"Level^2Flow^2 +\", coefp[13], \"LevelFlow^3 +\", coefp[14],\n     \"Flow^4 +\", interp)\n","3b166d63":"# The plot shows how well our model predict the outcome which is the LEL% from the test data set\n# The model has not been trained on the test set so the prediction is unbiased \n\nx_ax = range(len(X_p_test))\nplt.scatter(x_ax, Y_test, s=5, color=\"black\", label=\"Observation\")\nplt.scatter(x_ax, Y_pred, s=5, color=\"red\", label=\"Prediction\")\nplt.legend()\nplt.show()","654ea6d4":"print( \"X:\", X_p_train.shape, \"Y:\", Y_train.shape)","1b866851":"from sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.linear_model import Ridge\n\nalphas = [0.01, 0.1, 0.3, 0.5, 0.7, 1]\nelasticcv=ElasticNetCV(alphas=alphas, cv=5)\nmodel = elasticcv.fit(X_p_train, Y_train)\n\nprint(\"Best alpha =\", model.alpha_, \"Intercept =\", model.intercept_)\n\nelastic=ElasticNet(alpha=0.5, random_state = 20).fit(X_p_train, Y_train)\nY_pred = elastic.predict(X_p_test)\nscore = elastic.score(X_p_test, Y_test)\nrmse = mean_squared_error(Y_test, Y_pred, squared = False)\ncoefp = elastic.coef_\ninterp = elastic.intercept_\nprint(\"ElasticNet with polynomial degree =\",Degree, \": R2 =\", round(score,2), \"RMSE =\", round(rmse,2))\n\n","a4c908cb":"from sklearn.linear_model import Ridge\n\nalphas = [0.01, 0.1, 0.3, 0.5, 0.7, 1]\nridge = Ridge(alpha = 1)\nridge.fit(X_p_train, Y_train)\nY_pred = ridge.predict(X_p_test)\nscore = ridge.score(X_p_test, Y_test)\nrmse = mean_squared_error(Y_test, Y_pred, squared = False)\ncoefp = ridge.coef_\ninterp = ridge.intercept_\nprint(\"Ridge with polynomial degree =\",Degree, \": R2 =\", round(score,2), \"RMSE =\", round(rmse,2))","2b0595ff":"#from sklearn.tree import DecisionTreeRegressor\n#from sklearn.metrics import mean_squared_error\n\n#DTR1 = DecisionTreeRegressor()\n#DTR1.fit(X1_train, y1_train)\n#score1 = DTR1.score(X1_test, y1_test)\n#feature1 = DTR1.feature_importances_\n\n#DTR2 = DecisionTreeRegressor()\n#DTR2.fit(X2_train, y2_train)\n#score2 = DTR.score(X2_test, y2_test)\n#feature2 = DTR2.feature_importances_\n\n#print(\"R2 for y1:\", score1, \"\\nR2 for y2:\", score2)\n#feature1, feature2","72ea2d7b":"#DTR_importance = X_names.T\n#DTR_importance.columns = ['desc','unit','tag']\n\n#coef = pd.DataFrame(feature1)\n#DTR_importance['coefficient'] = coef\n#DTR_importance['rank'] = DTR_importance['coefficient'].abs()\n#DTR_importance = DTR_importance.sort_values(by=['rank'], ascending = False)\n#DTR_importance['rank'] = range(1,len(DTR_importance.unit)+1)\n#DTR_importance.head(10)\n\n#DTR_importance","c4fadaa8":"#coef2 = pd.DataFrame(feature2)\n#DTR_importance['coefficient'] = coef2\n#DTR_importance['rank'] = DTR_importance['coefficient'].abs()\n#DTR_importance = DTR_importance.sort_values(by=['rank'], ascending = False)\n#DTR_importance['rank'] = range(1,len(DTR_importance.unit)+1)\n#DTR_importance.head(10)\n\n#DTR_importance","24953066":"# 4. Decision Tree \n* Decision Tree Regressor will be used to predict the outcome and list the feature importance\n* We will use the same train-test split but without the standardization as decision tree does not require it","5c006c41":" ## 2.3 Trimming empty columns and rows","8ae1c23e":"## 3.1 Linear Regression with 2 Features\n* The first model fitting with linear regression shows very poor result with y1 and slightly better score with y2\n* This indicates that relationship between parameters and outcome is non-linear\n* By listing the coefficients from largest to smallest we can see that Wao Separator, EQ Tank A\/B and COC Receiving Pit have some contributions towards the outcome\n","029ac1e7":"## 2.5 Numerics to Float64","cccb5d2e":"# 1. Background\n* Unit 6370 is designed to extract Volatile Organic Compound (VOC) gasses from various storage tanks and pits in Unit 6300, 6330 and 6340 and treat primarily through absorption by three activated carbon vessels before discharging to atmosphere. Lower Explosive Limit (LEL) is being measured at inlet and discharge of the carbon beds. A high LEL reading indicates a potential fire in presence of ignition sources. Since initial operation in 2018, this unit has been registering high LEL % compared to alarm limit of 20% and Safe Operating Limit of 80%\n\n* The objective of this study is to utilize Machine Learning to identify which parameters contribute to high LEL %. If relevant parameters are identified, a more detailed investigation and perhaps mitigations can be prescribed to lower LEL % to safe levels. Secondly an accurate LEL model can be used to predict LEL and potentially provide advance warning to plant operators \n","69d8b8ad":"## 2.7 Standardizing the train and test set","3c5f2a15":"# 2. Preprocessing\n* Preprocessing constitues the bulk of work in the modelling. Typically data is imported and then shaped so the codes know which are headers and values. Empty rows and columns at the edges of data tables are typically trimmed. Missing values, depending on severity, can be deleted on row basis or can also be imputed for smaller datasets. If there are large variance between parameters, data can be standardized especially for regression modelling\n","2b143368":"## 3.3 ElasticNet with Polynomial Features","85d91dda":"## 2.1 Importing Data ","b538be9b":"# 3. Modelling\n* This is where various models are trained and tested. The result is not the end objective here which is to determine features or process parameters which are highly correlated to the outcome. Since the outcome LEL% is a continuous number, this will be setup as a regression problem. Traditional linear regression and tree-based regressor will be used. Feature importance will be the main output","500944bf":"## 2.4 Remove Nulls","0dc6a14f":"## 2.2 Capturing Text Info into Dataframe\n* The top 3 rows contain information that will be useful in later stages, so it will be saved into a separate dataframe (col_names)","ff6a8ca2":"## 2.6 Splitting Train and Test Data","e35d565c":"## 3.4 Ridge Regression with Polynomial Features","59cc0639":"## 3.2 Linear Regression with Polynomial Features"}}