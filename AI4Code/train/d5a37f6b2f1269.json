{"cell_type":{"836922b3":"code","85aca22b":"code","c302f5d0":"code","c4985314":"code","77477399":"code","9661d4e8":"code","c801f2db":"code","49a1f4e0":"code","64b49f6f":"code","fe1883a9":"code","7c9575c7":"code","4db0b1a5":"code","a5a153b2":"code","065f4485":"code","46a6c142":"code","e5dbfb4d":"code","1a2a2956":"code","d7e24634":"code","bdf1b92b":"code","8110a881":"code","bae41476":"code","8d27b776":"code","52561ca3":"code","56903d21":"code","f5872b72":"code","3313cb3d":"code","71b5357f":"code","f0f27b27":"code","33ba4c25":"code","ef7b1d55":"code","16dfff9a":"code","d52cb326":"code","0a0851b6":"code","65f82756":"code","ded301bf":"code","87682c24":"code","aba081d1":"code","87c6b45c":"code","abc047a4":"code","04471bda":"code","58b7a16c":"code","75827650":"code","c66c06e6":"code","20fc1d4c":"code","5c0ac5bf":"code","b1ef4166":"code","3d0d6e3b":"code","7d014490":"code","3a7427fe":"code","e867c8ac":"code","13e566de":"code","cc395619":"code","4860f720":"code","2a19c2d0":"code","965a9135":"code","987a9a2c":"code","389b3eaf":"code","9686c533":"code","85944d7f":"code","9c4161e2":"markdown","26842c59":"markdown","01d3e17d":"markdown","23043317":"markdown","75d58891":"markdown","680e507b":"markdown","54df6c5d":"markdown","e2f48aa4":"markdown","475622b3":"markdown","9f5e12aa":"markdown","f5ee0ffd":"markdown","1b295036":"markdown","e768f3f0":"markdown","a98393d5":"markdown","0e9cc6b0":"markdown","b80c0c40":"markdown","c23e88ad":"markdown","6a71cedc":"markdown","cacadb6f":"markdown","f2744c2b":"markdown","63d32668":"markdown","68e8f6ba":"markdown","eecd4e43":"markdown","217a4dc0":"markdown","e4c84cb3":"markdown","b106602e":"markdown","e72ff3be":"markdown","1959c6db":"markdown","1bbfe0df":"markdown","97d2c797":"markdown","e9a5d2d2":"markdown","403d5d6f":"markdown","f676dc4d":"markdown","a7a9085d":"markdown","6ca6fc97":"markdown","5ee44b5e":"markdown","ab972cb6":"markdown","327643fb":"markdown","c36a5b9c":"markdown","74adbb2d":"markdown","b41162c4":"markdown","905fbd5e":"markdown","0d227fcd":"markdown","a515af4f":"markdown","da29c1bb":"markdown","5847e5a0":"markdown"},"source":{"836922b3":"%%bash\npip install transformers","85aca22b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch # This is where the models are built, and what we will use to read them.\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import * #pretrained fastai tokenizer will cooperate with our transformer tokenizer.\nfrom fastai.callbacks import *\nfrom fastai.tabular import * #This allows for rapid generation of visualizations.\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig #This is the one we'll be using, although it's configured to take any of them.","c302f5d0":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)\nprint('torch version :', torch.__version__)","c4985314":"for dirname, _, filenames in os.walk('\/kaggle\/input'): \n    for filename in filenames:\n        print(os.path.join(dirname, filename))","77477399":"DATA_ROOT = Path(\"..\") \/ \"\/kaggle\/input\/sampledata\"\ntrain = pd.read_csv(DATA_ROOT \/ 'train.tsv' \/ 'train.tsv', sep=\"\\t\")\ntest = pd.read_csv(DATA_ROOT \/ 'test.tsv' \/ 'test.tsv', sep=\"\\t\")\n#test = pd.read_csv(DATA_ROOT \/ 'test.csv', index_col=0) #These are our actual datasets\n#train = pd.read_csv(DATA_ROOT \/ 'train.csv', index_col=0)\n#train.dropna()\n#test.dropna()\nprint(train.shape,test.shape)\ntrain.head()","9661d4e8":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","c801f2db":"# Parameters\nseed = 42\nuse_fp16 = False\nbs = 16\n\n# model_type = 'roberta'\n# pretrained_model_name = 'roberta-base'\n\n# model_type = 'bert'\n# pretrained_model_name='bert-base-uncased'\n\nmodel_type = 'distilbert'\npretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n#model_type = 'xlnet'\n#pretrained_model_name = 'xlnet-base-cased'","49a1f4e0":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","64b49f6f":"model_class.pretrained_model_archive_map.keys()","fe1883a9":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","7c9575c7":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n        return [CLS] + tokens + [SEP]","4db0b1a5":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","a5a153b2":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","065f4485":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","46a6c142":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","e5dbfb4d":"tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)","1a2a2956":"databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'Sentiment')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","d7e24634":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","bdf1b92b":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","8110a881":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n        \n        logits = self.transformer(input_ids,\n                                attention_mask = attention_mask)[0]   \n        return logits","bae41476":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 5\nconfig.use_bfloat16 = use_fp16\nprint(config)","8d27b776":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","52561ca3":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","56903d21":"print(learner.model)","f5872b72":"#For DistilBERT\nlist_layers = [learner.model.transformer.distilbert.embeddings,\n                learner.model.transformer.distilbert.transformer.layer[0],\n                learner.model.transformer.distilbert.transformer.layer[1],\n                learner.model.transformer.distilbert.transformer.layer[2],\n                learner.model.transformer.distilbert.transformer.layer[3],\n                learner.model.transformer.distilbert.transformer.layer[4],\n                learner.model.transformer.distilbert.transformer.layer[5],\n                learner.model.transformer.pre_classifier]\n\n# For roberta-base\n#list_layers = [learner.model.transformer.roberta.embeddings,\n#              learner.model.transformer.roberta.encoder.layer[0],\n#              learner.model.transformer.roberta.encoder.layer[1],\n#              learner.model.transformer.roberta.encoder.layer[2],\n#              learner.model.transformer.roberta.encoder.layer[3],\n#              learner.model.transformer.roberta.encoder.layer[4],\n#              learner.model.transformer.roberta.encoder.layer[5],\n#              learner.model.transformer.roberta.encoder.layer[6],\n#              learner.model.transformer.roberta.encoder.layer[7],\n#              learner.model.transformer.roberta.encoder.layer[8],\n#              learner.model.transformer.roberta.encoder.layer[9],\n#              learner.model.transformer.roberta.encoder.layer[10],\n#              learner.model.transformer.roberta.encoder.layer[11],\n#              learner.model.transformer.roberta.pooler]","3313cb3d":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)\n","71b5357f":"learner.save('untrain')","f0f27b27":"seed_all(seed)\nlearner.load('untrain');","33ba4c25":"learner.freeze_to(-1)","ef7b1d55":"learner.summary()","16dfff9a":"learner.lr_find()","d52cb326":"learner.recorder.plot(skip_end=10,suggestion=True)","0a0851b6":"learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))","65f82756":"learner.save('first_cycle')","ded301bf":"from fastai.tabular import *\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()","87682c24":"seed_all(seed)\nlearner.load('first_cycle');\n#learner.load('third_cycle');","aba081d1":"learner.freeze_to(-2)","87c6b45c":"lr = 1e-5","abc047a4":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","04471bda":"learner.save('second_cycle')","58b7a16c":"from fastai.tabular import *\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()","75827650":"seed_all(seed)\nlearner.load('second_cycle');","c66c06e6":"learner.freeze_to(-3)","20fc1d4c":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","5c0ac5bf":"learner.save('third_cycle')","b1ef4166":"seed_all(seed)\nlearner.load('third_cycle');","3d0d6e3b":"learner.unfreeze()","7d014490":"learner.fit_one_cycle(3, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","3a7427fe":"from fastai.tabular import *\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()","e867c8ac":"learner.predict('This is the best movie of 2020')","13e566de":"learner.predict('This is the worst movie of 2020')","cc395619":"learner.export(file = 'transformer.pkl');","4860f720":"path = '\/kaggle\/working'\nexport_learner = load_learner(path, file = 'transformer.pkl')","2a19c2d0":"export_learner.predict('This is the worst movie of 2020')","965a9135":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\ntest_preds = get_preds_as_nparray(DatasetType.Test)","987a9a2c":"sample_submission = pd.read_csv(DATA_ROOT \/ 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","389b3eaf":"test.head()","9686c533":"sample_submission.head()","85944d7f":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='predictions.csv')","9c4161e2":"Check batch and numericalizer :","26842c59":"## Libraries Installation\nBefore starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https:\/\/github.com\/fastai\/fastai\/blob\/master\/README.md#installation) and [here](https:\/\/github.com\/huggingface\/transformers#installation).\n\nIn Kaggle, the ``fastai`` and ``torch`` libraries are already installed. So you just have to install ``transformers`` with :","01d3e17d":"Check groups : ","23043317":"### Custom processor\nNow that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer.","75d58891":"We check the order.","680e507b":"## Setting up the Databunch\nFor DataBunch creation, you need to set the processor argument as our new custom processor ``transformer_processor`` and manage the padding correctly.\n\nAs mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left.","54df6c5d":"Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used.","e2f48aa4":"Check batch and tokenizer :","475622b3":"## Data pre-processing\n\nTo match pre-training, we have to format the model input sequence in a specific format.\nTo do so, we have to first **tokenize** and then **numericalize** the texts correctly.\nThe difficulty here is that each pre-trained model requires exactly the same specific pre-process\u200a-\u200a**tokenization** & **numericalization**\u200a-\u200athat the pre-process used during the pre-train part.\nFortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n\nIn the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \nAs you will see in the ``DataBunch`` API, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n\n``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n\nLet's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n\n### Custom Tokenizer\nThis part can be a little bit confusing because a lot of classes are wrapped around each other with similar names.\nTo sum it up, if we look carefully at the ``fastai`` implementation, we notice that :\n1. The [``TokenizeProcessor`` object](https:\/\/docs.fast.ai\/text.data.html#TokenizeProcessor) takes the ``tokenizer`` argument as a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#Tokenizer) takes the ``tok_func`` argument as a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#BaseTokenizer) implements the function ``tokenizer(t:str) \u2192 List[str]`` that takes a text ``t`` and returns a list of its tokens.\n\nTherefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.","9f5e12aa":"# Fastai with HuggingFace \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)","f5ee0ffd":"We will pick a value a bit before the minimum, where the loss still improves. Here 2e^-3 seems to be a good value.\n\nNext we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. ","1b295036":"The current versions of the fastai, torch, and transformers libraries are respectively 1.0.58 (1.0.60 does not produce any errors), 2.4.1, and 1.3.0 (1.4.0 does not produce any errors)","e768f3f0":"## Util function","a98393d5":"## Creating prediction\nNow that the model is trained, we want to generate predictions from the test dataset.\n\nAs specified in Keita Kurita's [article](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order.","0e9cc6b0":"It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label. \n\nThis differs from our library dataset, where the dataset consists of entire conversations loaded at once. Here's a quick run-down:\n\nOne sentence from the Movie Reviews dataset -\n* 156061\tAn intermittently pleasing but mostly routine effort .\n* 156062\tAn intermittently pleasing but mostly routine effort\n* 156063\tAn\n* 156064\tintermittently pleasing but mostly routine effort\n* 156065\tintermittently pleasing but mostly routine\n* 156066\tintermittently pleasing but\n* 156067\tintermittently pleasing\n* 156068\tintermittently\n* 156069\tpleasing\n* 156070\tbut\n* 156071\tmostly routine\n* 156072\tmostly\n* 156073\troutine\n* 156074\teffort\n* 156075\t.\nAnd one from our dataset -\n* 3133 PATRON: I think I am searching wrong. I can't seem to find articles on anything I type in.  STUDENT: Hi! Be with you in just a second PATRON: I am under subject guides, nutrition, dietetics and food science and find articles. PATRON: thank you! STUDENT: Hi! Sorry for the wait. There was someone at the desk I had to help quickly. Let me take a look STUDENT: Ok, I have the Guide page pulled up. Where and what were you trying to search? PATRON: I'm trying to find articles for my nutrition research paper PATRON: under the guide page i clicked on the the nutrition tab PATRON: and then i'm on find articles PATRON: i tried searching in the search bar my topic, but I know its not right because only one or two articles came up PATRON: So i think i have to click somewhere else to search more articles but i'm not sure where... STUDENT: Yes, you're on the right track! The subject guide page that you navigated to with the picture of of Greg Nelson is a list of good resources. Each link will take you to a database with thousands of articles. http:\/\/guides.lib.byu.edu\/ndfs PATRON: gotcha PATRON: just one click away PATRON: thankyou STUDENT: You're welcome! STUDENT: Anything else I can help with? PATRON: I'm good thanks! STUDENT: Good luck! Just shoot us another chat or an email at science_reference@byu.edu if you have more questions!\n\nThis makes our dataset significantly harder to analyze, since each phrase isn't broken up individually.","b80c0c40":"We check which layer are trainable.","c23e88ad":"# \ud83d\udee0 Integrating transformers with fastai for multiclass classification\nBefore beginning the implementation, note that integrating ``transformers`` and ``fastai`` can be done in multiple different ways. For that reason, I decided to create simple solutions, that are generic and flexible. More precisely, I try to make the minimum amount of modifications in both libraries while making them compatible with the maximum amount of transformer architectures.\n\nNote that in addition to this NoteBook and the [Medium article](https:\/\/medium.com\/p\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376), I made another version available on my [GitHub](https:\/\/github.com\/IMJONEZZ\/).","6a71cedc":"For **Slanted Triangular Learning Rates** we have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https:\/\/docs.fast.ai\/callbacks.one_cycle.html). \n\nTo use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``.","cacadb6f":"## Train\nNow we can finally use all the fastai built-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**.","f2744c2b":"As mentioned [here](https:\/\/docs.fast.ai\/basic_train.html#load_learner), you have to be careful that each custom classes - like ``TransformersVocab`` - are first defined before executing ``load_learner``.","63d32668":"## Export Learner\nIn order to export and load the learner you can do these operations:","68e8f6ba":"# References\n* Hugging Face, Transformers GitHub (Nov 2019), [https:\/\/github.com\/huggingface\/transformers](https:\/\/github.com\/huggingface\/transformers)\n* Fast.ai, Fastai documentation (Nov 2019), [https:\/\/docs.fast.ai\/text.html](https:\/\/docs.fast.ai\/text.html)\n* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https:\/\/arxiv.org\/abs\/1801.06146](https:\/\/arxiv.org\/abs\/1801.06146)\n* Keita Kurita's article : [A Tutorial to Fine-Tuning BERT with Fast AI](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) (May 2019)\n* Dev Sharma's article : [Using RoBERTa with Fastai for NLP](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)","eecd4e43":"# Introduction : Story of transfer learning in NLP\nIn early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the  [Universal Language Model Fine-tuning for Text Classification](https:\/\/medium.com\/r\/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf) (ULMFiT) method. ULMFiT was the first **Transfer Learning** method applied to NLP. As a result, besides significantly outperforming many state-of-the-art tasks, it allowed, with only 100 labeled examples, to match performances equivalent to models trained on 100\u00d7  more data.\n\nThe first time I heard about ULMFiT was listening and following along to a [fast.ai course](https:\/\/course.fast.ai\/videos\/?lesson=4) given by Jeremy Howard in a class at BYU. He demonstrated how the ``fastai`` library makes the ULMFit method significantly easier to implement for someone without much code experience. In his demo, he used an AWD-LSTM neural network pre-trained on Wikitext-103 and got state-of-the-art results. He also explained key techniques to fine-tune the models like **Discriminate Learning Rate**, **Gradual Unfreezing** and **Slanted Triangular Learning Rates**.\n\nSince the introduction of ULMFiT, **Transfer Learning** has become very popular in NLP and even Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) and OpenAI (GPT, GPT-2) have begun to pre-train their own models on very large corpora. This time, instead of using LSTMs, they all use a more powerful architecture based on the Transformer (cf. [Attention is all you need](https:\/\/arxiv.org\/abs\/1706.03762)).\n\nAlthough these models are powerful, ``fastai`` does not include them. Fortunately, [HuggingFace](https:\/\/huggingface.co\/) \ud83e\udd17 created the well-known [transformers library](https:\/\/github.com\/huggingface\/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRL\u2026) implemented in PyTorch as opposed to Tensorflow, Julia, etc.. The implementation gives interesting additional utilities like pre-trained tokenizers, optimizers and configs.\n\nThe ``transformers`` library is standalone, but incorporating it with the ``fastai`` library provides a simpler implementation compatible with powerful fastai tools like  **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**. The point here is to allow anyone \u2014 expert or non-expert \u2014 to easily achieve state-of-the-art results and to \u201cmake NLP cool again\u201d.\n\nIt worth noting that the integration of HuggingFace ``transformers`` with ``fastai`` has already been demonstrated in:\n* Keita Kurita's article [A Tutorial to Fine-Tuning BERT with Fast AI](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) which makes ``pytorch_pretrained_bert`` library compatible with ``fastai``.\n* Dev Sharma's article [Using RoBERTa with Fastai for NLP](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) which makes ``pytorch_transformers`` library compatible with ``fastai``.\n* N.B. Medium article [\"Fastai with Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\"](https:\/\/medium.com\/p\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376).\n\nAlthough these articles are of high quality, not all of them are currently compatible with the last version of ``transformers``, and even the last one, which this implementation is based off of in a large part is deprecated.","217a4dc0":"In this implementation, be careful about 3 things :\n1. As we are not using an RNN, we have to limit the sequence length to the model input size.\n2. Most of the models require special tokens placed at the beginning and end of the sequences.\n3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n\nBelow, you can find the dossier of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https:\/\/huggingface.co\/transformers\/) in each model section.\n\n    bert:       [CLS] + tokens + [SEP] + padding\n\n    roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n    \n    distilbert: [CLS] + tokens + [SEP] + padding\n\n    xlm:        [CLS] + tokens + [SEP] + padding\n\n    xlnet:      padding + [CLS] + tokens + [SEP]\n    \nIt is worth noting that we don't add padding in this part of the implementation. \nAs we will see later, ``fastai`` manages it automatically during the creation of the ``DataBunch``.","e4c84cb3":"We can decide to divide the model in 14 blocks :\n* 1 Embedding\n* 6 transformer\n* 1 classifier\n\nIn this case, we can split our model in this way :","b106602e":"Note here that we use slice to create separate learning rate for each group.","e72ff3be":"Function to set the seed for generating random numbers.","1959c6db":"Note that I haven't found any documentation about studying the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you find any interesting documentation, please let me know.","1bbfe0df":"## Main transformers classes\nIn ``transformers``, each model architecture is associated with 3 main classes:\n* A **model class** to load\/store a particular pre-trained model.\n* A **tokenizer class** to pre-process the data and make it compatible with the given model.\n* A **configuration class** to load\/store the configuration of the given model.\n\nFor example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertconfig) for the **configuration class**. \n\nIn order to switch easily between classes \u200a-\u200a each related to a specific model type \u200a-\u200a HuggingFace provides a dictionary that allows loading the correct classes by just specifying the correct model type name.","97d2c797":"# Conclusion\n\nIn this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and even **Gradual Unfreezing**. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.\n\nThis year, the transformers became an essential tool to NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.\n\nI hope you enjoyed this first article and found it useful. \nThanks for reading and don't hesitate in leaving questions or suggestions.","e9a5d2d2":"seed_all(seed)","403d5d6f":"Now, you can predict examples with:","f676dc4d":"It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types are :\n* BERT (from Google)\n* XLNet (from Google\/CMU)\n* XLM (from Facebook)\n* RoBERTa (from Facebook)\n* DistilBERT (from HuggingFace)","a7a9085d":"## Learner : Custom Optimizer \/ Custom Metric\nIn ``pytorch-transformers``, HuggingFace implemented two specific optimizers \u200a-\u200a BertAdam and OpenAIAdam \u200a-\u200a that have been replaced by a single AdamW optimizer.\nThis optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\nIt is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.","6ca6fc97":"### Custom model\nAs mentioned [here](https:\/\/github.com\/huggingface\/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are only interested in accessing the logits. \nOne way to access them is to create a custom model.","5ee44b5e":"Here, we finally unfreeze all the layers.","ab972cb6":"To adapt our transformers to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) (Section: *Initializing the Learner*) the ``num_labels`` argument.","327643fb":"Therefore, we first freeze all the groups but the classifier with :","c36a5b9c":"NB: The functions ``__getstate__`` and ``__setstate__`` allow us to correctly use ``export()`` and ``load_learner()`` functions.","74adbb2d":"We can now submit our predictions to Kaggle !  In our example, without playing too much with the parameters, we get a score of 0.70059, which leads us to the 5th position on the leaderboard! ","b41162c4":"There are multible ways to create a DataBunch, in our implementation, we use [the data block API](https:\/\/docs.fast.ai\/data_block.html#The-data-block-API) for flexibility.","905fbd5e":"We then unfreeze the second group of layers and repeat the operation. This allows the classifier to train more than the rest of the layers while still allowing us to take advantage of the pretrained model.","0d227fcd":"## Discriminative Fine-tuning and Gradual unfreezing (Optional)\nTo use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https:\/\/docs.fast.ai\/basic_train.html#Discriminative-layer-training).\n\nUnfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n\nFor example, if we use the DilBERT model, we can observe the architecture by calling ``print(learner.model)``.","a515af4f":"You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name, ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model\/tokenizer\/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html#pretrained-models).","da29c1bb":"### Custom Numericalizer\n\nIn ``fastai``, [``NumericalizeProcessor``  object](https:\/\/docs.fast.ai\/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https:\/\/docs.fast.ai\/text.transform.html#Vocab). \nFrom this analyse, we suggest two ways to adapt the fastai numericalizer:\n1. You can, like decribed in the [Dev Sharma's article](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n\nEven if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \nTherefore, I implemented the second solution, which runs for each model type.\nIt consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``.","5847e5a0":"## The example task\nThe given task is a multi-class text classification on [Movie Reviews](https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews\/overview).\n\nFor each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:\n* 0 \u2192 Negative\n* 1 \u2192 Somewhat negative\n* 2 \u2192 Neutral\n* 3 \u2192 Somewhat positive\n* 4 \u2192 Positive\n\nThis will provide a perfect context for our Library Chat analysis where the labels are:\n* 0 \u2192 Dissatisfied\/Frustrated\n* 1 \u2192 Dissatisfied\n* 2 \u2192 Neither\n* 3 \u2192 Satisfied\n* 4 \u2192 Above and Beyond\n\nThe data is loaded into a ``DataFrame`` using ``pandas``."}}