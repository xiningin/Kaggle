{"cell_type":{"78e85f07":"code","861f9850":"code","33a9214a":"code","5671357f":"code","bcaddeaa":"code","ace0867c":"code","71d9d5ac":"code","bcd217a4":"code","6394c5e0":"code","1f6740af":"code","ece45f08":"code","779371b9":"code","460e042e":"code","26374a26":"code","5f012867":"code","152f6e13":"code","48435434":"code","b75ae07c":"code","5c170d5e":"code","27137e73":"code","53bb00d1":"code","05919cf4":"code","e9aabb68":"code","5f305a8e":"code","abc46a7e":"code","2010fcdb":"code","fa8e29ec":"code","8924a5cf":"code","2b75437d":"code","4fee03df":"code","cc110412":"code","e824a72f":"code","ef99d110":"code","f1124c0a":"code","927ec587":"code","f7135378":"code","a4ee1008":"code","8127337d":"code","c5fec28c":"code","65afd042":"code","08a6bfa4":"code","9b794057":"code","3b573c02":"code","7b6e4e44":"code","21debc43":"code","aa5501b5":"code","01658e86":"code","d7b60353":"code","987194c7":"code","b1a4d086":"code","05c62315":"code","397de5dd":"code","1ba76b82":"code","6e30b5da":"code","903ce5ca":"code","6d99f368":"markdown","a0843168":"markdown","1c837420":"markdown","a38a43e4":"markdown","342a4c8f":"markdown","c9d0ae16":"markdown","400e94cf":"markdown","f5aba983":"markdown","d90e0068":"markdown","f7c97f5f":"markdown","bd02258e":"markdown","6b44d390":"markdown","918c2515":"markdown","dc81e8bc":"markdown","1dcc4839":"markdown","1d2ccf16":"markdown","96b795b2":"markdown"},"source":{"78e85f07":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # plot library of python\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two partsas\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics # to check the error and accuracy of the model\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nimport plotly.figure_factory as ff\nimport plotly.offline as py","861f9850":"data_df= pd.read_csv(\"..\/input\/data.csv\")","33a9214a":"data_df.head()","5671357f":"data_df.shape","bcaddeaa":"data_df.isna().sum()","ace0867c":"data_df.drop(['Unnamed: 32'],axis = 1,inplace = True)","71d9d5ac":"data_df.describe()","bcd217a4":"data_df[['diagnosis','radius_worst','radius_mean','radius_se']].groupby('diagnosis').mean()","6394c5e0":"M = data_df[(data_df['diagnosis'] == 'M')]\nB = data_df[(data_df['diagnosis'] == 'B')]","1f6740af":"def myplot(data_new,bin_size):\n    tmp1 = M[data_new]\n    tmp2 = B[data_new]\n    hist_data = [tmp1, tmp2]\n    group_labels = [\"Malignant\",\"Benign\"]\n    colors = ['#F24027', '#2CD166']\n    fig  = ff.create_distplot(hist_data,group_labels,colors = colors, show_hist = True, bin_size = bin_size,curve_type = 'kde')\n    fig['layout'].update(title = data_new)\n    py.iplot(fig, filename = 'Density Plot')","ece45f08":"myplot('radius_mean',.5)\nmyplot('texture_mean',.5)\nmyplot('compactness_mean' , 0.005)","779371b9":"features_mean = data_df.columns[2:11]\nfeatures_mean","460e042e":"features_se = data_df.columns[12:22]\nfeatures_se","26374a26":"features_worst = data_df.columns[23:]\nfeatures_worst","5f012867":"data_df['diagnosis'] = data_df['diagnosis'].map({'M':1,'B':0})","152f6e13":"sns.catplot(x=\"diagnosis\", kind=\"count\", palette=\"ch:.30\", data=data_df)","48435434":"corr_mean = data_df[features_mean].corr()","b75ae07c":"plt.figure(figsize= (10,10))\nsns.heatmap(corr_mean,annot = True)","5c170d5e":"corr_mean.abs()","27137e73":"select_pred_mean = ['radius_mean','texture_mean','smoothness_mean','compactness_mean','symmetry_mean']","53bb00d1":"train , test = train_test_split(data_df,test_size = 0.2)\nprint(train.shape)\nprint(test.shape)","05919cf4":"train_X = train[select_pred_mean]# taking the training data input \ntrain_y=train.diagnosis\ntest_X= test[select_pred_mean] # taking test data inputs\ntest_y =test.diagnosis   #output value of test data\nmodel_rf=RandomForestClassifier(n_estimators=100)\nmodel_rf.fit(train_X,train_y)","e9aabb68":"predict_value = model_rf.predict(test_X)","5f305a8e":"predict_value","abc46a7e":"metrics.accuracy_score(predict_value,test_y)","2010fcdb":"model_logreg = LogisticRegression()\nmodel_logreg.fit(train_X,train_y)","fa8e29ec":"predict_logreg = model_logreg.predict(test_X)\nmetrics.accuracy_score(predict_logreg,test_y)","8924a5cf":"metrics.f1_score(predict_logreg,test_y)","2b75437d":"metrics.confusion_matrix(predict_logreg,test_y)","4fee03df":"metrics.confusion_matrix(predict_value,test_y)","cc110412":"metrics.f1_score(predict_value,test_y)","e824a72f":"def roc_curve(model_num,name_model):\n    probs = model_num.predict_proba(test_X)\n    preds = probs[:,1] # tpr\n    fpr, tpr, threshold = metrics.roc_curve(test_y, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic for ' + name_model)\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","ef99d110":"roc_curve(model_logreg,\"LogisticRegression\")\nroc_curve(model_rf,\"Random Forest\")","f1124c0a":"#Checking with taking all features\nselect_pred_mean_full = features_mean\ntrain_X= train[select_pred_mean_full]\ntrain_y= train.diagnosis\ntest_X = test[select_pred_mean_full]\ntest_y = test.diagnosis","927ec587":"#Default Random Forest Classifier Algorithm without any tuning\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)","f7135378":"#Calculating Feature Importance\nfeatimp = pd.Series(model.feature_importances_, index=select_pred_mean_full).sort_values(ascending=False)\nprint(featimp)","a4ee1008":"#Using Xgboost\nfrom xgboost import XGBRegressor\nmodel_xgb = XGBRegressor()\n# We can Add silent=True to avoid printing out updates with each cycle\nmodel_xgb.fit(train_X, train_y, verbose=False)","8127337d":"y_pred = model_xgb.predict(test_X)","c5fec28c":"predictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = metrics.accuracy_score(predictions,test_y)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","65afd042":"kfold = StratifiedKFold(n_splits=10)","08a6bfa4":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, train_X, y = train_y, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set2\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","9b794057":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(train_X,train_y)\npp = gsadaDTC.predict(test_X)\n\ngsadaDTC_Acc = metrics.accuracy_score(pp,test_y)\n\nprint(gsadaDTC_Acc)\n\nada_best = gsadaDTC.best_estimator_","3b573c02":"gsadaDTC.best_score_","7b6e4e44":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 9],\n              \"min_samples_leaf\": [1, 3, 9],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(train_X,train_y)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","21debc43":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 9],\n              \"min_samples_leaf\": [1, 3, 9],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(train_X,train_y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","aa5501b5":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(train_X,train_y)\n\nGBC_best = gsGBC.best_estimator_\n# Best score\ngsGBC.best_score_","01658e86":"#Checking HyperParameter Values\nRFC_best","d7b60353":"rows = cols = 2\nfig, axes = plt.subplots(rows , cols ,figsize=(14,14))\n\nbest_classifiers = [(\"AdaBoosting\", ada_best),(\"GradientBoosting\",GBC_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best)]\n\nnclassifier = 0\nfor row in range(rows):\n    for col in range(cols):\n        name = best_classifiers[nclassifier][0]\n        classifier = best_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:9]\n        g = sns.barplot(y=train_X.columns[indices][:9],x = classifier.feature_importances_[indices][:9] ,\n                        orient='h',ax=axes[row][col],palette=\"rocket\")\n        g.set_xlabel(\"Relative importance\",fontsize=10)\n        g.set_ylabel(\"Features\",fontsize=10)\n        g.tick_params(labelsize=10)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","987194c7":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(train_X,train_y)","b1a4d086":"pred_voting = votingC.predict(test_X)\nmetrics.accuracy_score(pred_voting,test_y)","05c62315":"metrics.f1_score(pred_voting,test_y)","397de5dd":"pd.crosstab(pred_voting,test_y)","1ba76b82":"def roc_curve(model_num,name_model):\n    probs = model_num.predict_proba(test_X)\n    preds = probs[:,1] # True Positive Rate\n    fpr, tpr, threshold = metrics.roc_curve(test_y, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic for ' + name_model)\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","6e30b5da":"roc_curve(votingC,\"Vf\")","903ce5ca":"final_sub = pd.DataFrame({'Id': test.id, 'Ensembled_Diagnosis_Prediction': pred_voting})\nfinal_sub.head()","6d99f368":"https:\/\/stackoverflow.com\/questions\/36063014\/what-does-kfold-in-python-exactly-do","a0843168":"** Ensembled Voting**","1c837420":"https:\/\/medium.com\/@srnghn\/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3","a38a43e4":"Hard voting is where a model is selected from an ensemble to make the final prediction by a simple majority vote for accuracy.\n\nSoft Voting can only be done when all your classifiers can calculate probabilities for the outcomes. Soft voting arrives at the best result by averaging out the probabilities calculated by individual algorithms.","342a4c8f":"Segregating the features","c9d0ae16":"     It's good idea to drop Unnamed: 32 Column","400e94cf":"https:\/\/towardsdatascience.com\/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00****","f5aba983":"https:\/\/datascience.stackexchange.com\/questions\/21877\/how-to-use-the-output-of-gridsearch","d90e0068":"We can observe that on an average of radius in all scenarios is less in Benign Tumor as compared to Malignant one.","f7c97f5f":"Calculating Accuracy ","bd02258e":"Accuracy Increases a Bit by including all mean features but not by a significant margin. So we keep the model complexity low.","6b44d390":"ROC Curves\ntells how much model is capable of distinguishing between classes.  ","918c2515":"https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae","dc81e8bc":"Decision Tree\nRandom Forest\nET\nGB","1dcc4839":"> Modelling","1d2ccf16":"> ****Highly Correalted Pairs:\n>     (radius_mean <-> area_mean)\n>     (perimeter_mean <-> area_mean)\n>     (concavity_mean <-> concave points_mean)","96b795b2":"Thanks for your time ! This is my first Kernel. Please let me know any suggestion for improvement.\nDo Upvote if you liked the kernel it will motivate me. \n\n**Happy Learning ! **"}}