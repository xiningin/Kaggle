{"cell_type":{"110b2ebf":"code","71430e17":"code","f0b658a7":"code","f0f66ceb":"code","eb3ce157":"code","7148a406":"code","55ba6f4d":"code","1b03ac64":"code","58236bcd":"code","0db2af88":"code","6d5deecc":"markdown","a82d9f9b":"markdown"},"source":{"110b2ebf":"import matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom tensorflow.python.framework import ops\nimport tensorflow as tf\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport os\nimport numpy as np  # linear algebra\nimport pandas as pd  # CSV file\nimport scipy.io.wavfile as sci_wav  # Open wav files\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random","71430e17":"ROOT_DIR = '..\/input\/cats_dogs\/'\nCSV_PATH = '..\/input\/train_test_split.csv'\n\n\ndef read_wav_files(wav_files):\n    '''Returns a list of audio waves\n    Params:\n        wav_files: List of .wav paths\n\n    Returns:\n        List of audio signals\n    '''\n    if not isinstance(wav_files, list):\n        wav_files = [wav_files]\n    return [sci_wav.read(ROOT_DIR + f)[1] for f in wav_files]\n\n\ndef get_trunk(_X, idx, sample_len, rand_offset=False):\n    '''Returns a trunk of the 1D array <_X>\n\n    Params:\n        _X: the concatenated audio samples\n        idx: _X will be split in <sample_len> items. _X[idx]\n        rand_offset: boolean to say whether or not we use an offset\n    '''\n    randint = np.random.randint(10000) if rand_offset is True else 0\n    start_idx = (idx * sample_len + randint) % len(_X)\n    end_idx = ((idx + 1) * sample_len + randint) % len(_X)\n    if end_idx > start_idx:  # normal case\n        return _X[start_idx: end_idx]\n    else:\n        return np.concatenate((_X[start_idx:], _X[:end_idx]))\n\n\ndef get_augmented_trunk(_X, idx, sample_len, added_samples=0):\n    X = get_trunk(_X, idx, sample_len)\n\n    # Add other audio of the same class to this sample\n    for _ in range(added_samples):\n        ridx = np.random.randint(len(_X))  # random index\n        X = X + get_trunk(_X, ridx, sample_len)\n\n    # One might add more processing (like adding noise)\n\n    return X\n\n\ndef dataset_gen(is_train=True, batch_shape=(20, 16000), sample_augmentation=0):\n    '''This generator is going to return training batchs of size <batch_shape>\n\n    Params:\n        is_train: True if you want the training generator\n        batch_shape: a tupple (or list) consisting of 2 arguments, the number\n            of samples per batchs and the number datapoints per samples\n            (16000=1s)\n        sample_augmentation: augment each audio sample by n other audio sample.\n            Only works when <is_train> is True\n    '''\n    s_per_batch = batch_shape[0]\n    s_len = batch_shape[1]\n\n    X_cat = dataset['train_cat'] if is_train else dataset['test_cat']\n    X_dog = dataset['train_dog'] if is_train else dataset['test_dog']\n    \n    # Go through all the permutations\n    y_batch = np.zeros(s_per_batch)\n    X_batch = np.zeros(batch_shape)\n    # Random permutations (for X indexes)\n    nbatch = int(max(len(X_cat), len(X_cat)) \/ s_len)\n    perms = [list(enumerate([i] * nbatch)) for i in range(2)]\n    perms = sum(perms, [])\n    random.shuffle(perms)\n\n    while len(perms) > s_per_batch:\n\n        # Generate a batch\n        for bidx in range(s_per_batch):\n            perm, _y = perms.pop()  # Load the permutation\n            y_batch[bidx] = _y  \n\n            # Select wether the sample is a cat or a dog\n            _X = X_cat if _y == 0 else X_dog\n\n            # Apply the permutation to the good set\n            if is_train:\n                X_batch[bidx] = get_augmented_trunk(\n                    _X,\n                    idx=perm,\n                    sample_len=s_len,\n                    added_samples=sample_augmentation)\n            else:\n                X_batch[bidx] = get_trunk(_X, perm, s_len)\n\n        yield (X_batch.reshape(s_per_batch, s_len, 1),\n               y_batch.reshape(-1, 1))\n\n\ndef load_dataset(dataframe):\n    '''Load the dataset in a dictionary.\n    From the dataframe, it reads the [train_cat, train_dog, test_cat, test_dog]\n    columns and loads their corresponding arrays into the <dataset> dictionary\n\n    Params:\n        dataframe: a pandas dataframe with 4 columns [train_cat, train_dog, \n        test_cat, test_dog]. In each columns, many WAV names (eg. ['cat_1.wav',\n        'cat_2.wav']) which are going to be read and append into a list\n\n    Return:\n        dataset = {\n            'train_cat': [[0,2,3,6,1,4,8,...],[2,5,4,6,8,7,4,5,...],...]\n            'train_dog': [[sound 1],[sound 2],...]\n            'test_cat': [[sound 1],[sound 2],...]\n            'test_dog': [[sound 1],[sound 2],...]\n        }\n    '''\n    df = dataframe\n\n    dataset = {}\n    for k in ['train_cat', 'train_dog', 'test_cat', 'test_dog']:\n        v = list(df[k].dropna())\n        v = read_wav_files(v)\n        v = np.concatenate(v).astype('float32')\n\n        # Compute mean and variance\n        if k == 'train_cat':\n            dog_std = dog_mean = 0\n            cat_std, cat_mean = v.std(), v.mean()\n        elif k == 'train_dog':\n            dog_std, dog_mean = v.std(), v.mean()\n\n        # Mean and variance suppression\n        std, mean = (cat_std, cat_mean) if 'cat' in k else (dog_std, dog_mean)\n        v = (v - mean) \/ std\n        dataset[k] = v\n\n        print('loaded {} with {} sec of audio'.format(k, len(v) \/ 16000))\n\n    return dataset\n\n\ndf = pd.read_csv(CSV_PATH)\ndataset = load_dataset(df)","f0b658a7":"print('This is how the generator works : ')\nprint(help(dataset_gen))","f0f66ceb":"batch_size=512\nnum_data_points = 16000\nn_augment = 10\n\n# train_gen = dataset_gen(is_train=True, batch_shape=(batch_size, num_data_points), sample_augmentation=n_augment)\n# val_gen = dataset_gen(is_train=False, batch_shape=(batch_size, num_data_points))","eb3ce157":"from keras import backend as K\nK.clear_session()","7148a406":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding, BatchNormalization\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Conv1D(20, 4, strides=2, activation='relu', input_shape=(num_data_points, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(20, 4, strides=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(2))\n\nmodel.add(Conv1D(40, 4, strides=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(40, 4, strides=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(2))\n\nmodel.add(Conv1D(80, 4, strides=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(80, 4, strides=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(GlobalAveragePooling1D())\n\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","55ba6f4d":"NUM_EPOCHS = 50\nadam_optimizer = Adam(decay=1e-3)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=adam_optimizer,\n              metrics=['accuracy'])","1b03ac64":"NUM_EPOCHS = 50\n\ntrain_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []\n\n# Loop through epoch samples (batchs)\nfor epochs in range(NUM_EPOCHS):\n    train_gen = dataset_gen(is_train=True, batch_shape=(batch_size, num_data_points), sample_augmentation=n_augment)\n    \n    for batch_x, batch_y in train_gen:\n        history = model.fit(batch_x, batch_y, epochs=1, validation_split=0.2)\n        train_loss.extend(history.history['loss'])\n        val_loss.extend(history.history['val_loss'])\n        train_acc.extend(history.history['acc'])\n        val_acc.extend(history.history['val_acc'])","58236bcd":"fig = plt.figure(figsize=(15,8))\nax = fig.add_subplot(111)\nax.plot(train_loss, label=\"train loss\")\nax.plot(val_loss, label=\"val loss\", color='green')\nplt.legend()\nplt.title(\"Log Loss\")\nplt.show()","0db2af88":"fig = plt.figure(figsize=(15,8))\nax = fig.add_subplot(111)\nax.plot(train_acc, label=\"training accuracy\")\nax.plot(val_acc, label=\"val accuracy\", color='green')\nplt.legend()\nplt.title(\"Accuracy\")\nplt.show()","6d5deecc":"# Build a Keras Network","a82d9f9b":"# Load modules and utils\n\nWe load the utils file provided in the dataset and show how the generator work"}}