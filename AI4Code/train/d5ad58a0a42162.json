{"cell_type":{"8cc0f8cd":"code","cecfcca7":"code","dfe623ed":"code","9d31b777":"code","1ee5ffd3":"code","6943b4f7":"code","53655a94":"code","206af139":"code","ba9f02f1":"code","85c217b9":"code","fec113fc":"code","8a1fb804":"code","467ac068":"code","77e208d4":"code","7f4f5ea3":"code","a637417b":"code","842ccd78":"code","c33bd4da":"code","c012adf6":"code","2f92853b":"code","0e77dc66":"code","e511de4e":"code","5f0d5032":"code","a0ae4efc":"code","36f5df57":"code","d4bbb855":"code","95816eaf":"code","10568b16":"code","607d408f":"code","e5571a3c":"code","5bdffbbf":"code","092f2144":"code","9035b70e":"code","2b6ae3a7":"code","ba440574":"code","06fdda31":"code","fc80d420":"code","b8adbbab":"code","374a687a":"code","fb6e57dd":"code","6ac79f80":"code","0aa665ef":"code","48ee40ac":"code","e11cca3d":"code","3dd12d93":"code","0b5ef061":"code","e8cb3003":"code","99d41f7c":"code","16914df1":"code","b4489f06":"code","d3a3e989":"code","95bba9d7":"code","e2574f03":"code","751e8a24":"code","081f2ef9":"code","d98ea78b":"code","3f3f1fa4":"code","ace13475":"code","ac454bf7":"code","c614ead3":"code","d7dfff0b":"code","9d5c6a0d":"code","0d3be7b9":"code","579cdbdc":"code","2dc84d8a":"code","26b76e4f":"code","e8498ba4":"code","629ec7b5":"code","76dd42ee":"code","f5319b67":"code","a49546c4":"code","ce91fc7a":"code","dc71aade":"code","b7bd5707":"code","851d2d95":"code","2e02cb7d":"code","015e8cc7":"code","a3fecda7":"code","d648576e":"code","107d8058":"code","c0cb2725":"code","e9718acb":"code","31148384":"code","5aadfcb9":"code","8bd038f1":"code","541ae1ec":"code","9f64f6cd":"code","de014335":"code","5334217d":"code","29b430da":"code","127f19e7":"code","8c5fa769":"markdown","52e0b12d":"markdown","5e4e3bcc":"markdown","b07313f6":"markdown","0dd65b98":"markdown","612f08c5":"markdown","daddd9ad":"markdown","542c5386":"markdown","5bd61793":"markdown","a7c99956":"markdown","e21d3394":"markdown","0a69e399":"markdown","27f832ab":"markdown","17ba2227":"markdown","7271d352":"markdown","0697597b":"markdown","20602d2d":"markdown","7b1431e7":"markdown","4b2b0160":"markdown","4a8ac9cb":"markdown","7ffeaaa9":"markdown","5d2491ac":"markdown","3b4ef78e":"markdown","7e40ee83":"markdown","d34e8f11":"markdown","3a628ed1":"markdown","73bc1a26":"markdown","6a696258":"markdown","de2a6117":"markdown","cc285753":"markdown","c184f6b1":"markdown","d6df326a":"markdown","62e628a2":"markdown","5ff7d1d0":"markdown"},"source":{"8cc0f8cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cecfcca7":"import seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport plotly.tools\n\n#Libraries for handling imbalance data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\n\nfrom hyperopt import hp,fmin,tpe,STATUS_OK,Trials\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score","dfe623ed":"df = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","9d31b777":"df.shape","1ee5ffd3":"df.info()","6943b4f7":"df.describe()","53655a94":"df.isna().sum()","206af139":"df.nunique()","ba9f02f1":"# Droping columns\ndf.drop(['EmployeeNumber','EmployeeCount','Over18','StandardHours'],1, inplace=True)","85c217b9":"fig = px.bar(x=df['Attrition'].value_counts().index,\n             y=df['Attrition'].value_counts(), \n             title='Attrition Distribution', \n             text=(df['Attrition'].value_counts()\/len(df['Attrition'])*100))\n\nfig.update_traces(textposition='outside', \n                  texttemplate='%{text:.4s}%',\n                  marker = dict(color = ['silver','gainsboro'],line = dict(color = \"black\", width = 3)))\n\nfig['layout'].update(height=500, width=600)\nfig.show()","fec113fc":"fig = px.histogram(df['Age'],nbins=100, height=500, width=700, template='ggplot2')\nfig.show()","8a1fb804":"fig = px.histogram(df, x=df['Age'], color=df['Attrition'],nbins=70, height=500, width=700, template='ggplot2')\nfig.show()","467ac068":"fig = px.bar(x=df['BusinessTravel'].value_counts().index, \n             y=df['BusinessTravel'].value_counts(), \n             title='Business Travel',\n             text=(df['BusinessTravel'].value_counts()\/len(df['BusinessTravel'])*100), \n             height=500,\n             width=600)\n\nfig.update_traces(textposition='outside', \n                  texttemplate='%{text:.4s}%',\n                  marker = dict(color = ['peachpuff','moccasin','papayawhip'],line = dict(color = \"black\", width = 3)))\nfig.show()\n\n","77e208d4":"\nfig = px.histogram(x=df['BusinessTravel'], \n                   color=df['Attrition'],\n                   barmode='group',\n                   color_discrete_sequence=['lemonchiffon','darkkhaki'], \n                   height=500, \n                   width=600,\n                  title='Business Travel VS Attrition')\nfig.show()","7f4f5ea3":"fig = px.histogram(x=df['DailyRate'],\n                   nbins=100, \n                   height=500,\n                   width=700, \n                   title='Daily Rate Distribution',\n                   color_discrete_sequence=['darkgray'])\nfig.show()\n\n\nfig = px.histogram(x=df['HourlyRate'],\n                   nbins=100,\n                   height=500,\n                   width=700,\n                   title='Hourly Rate Distribution',\n                   color_discrete_sequence=['darkslategray'])\nfig.show()\n\n","a637417b":"fig = px.histogram(x=df['DailyRate'],\n                   color=df['Attrition'],\n                   nbins=100, \n                   height=500,\n                   width=700, \n                   title='Daily Rates VS Attrition', \n                   color_discrete_sequence=['black','silver'])\nfig.show()\n\n\nfig = px.histogram(x=df['HourlyRate'],\n                   color=df['Attrition'],\n                   nbins=100, \n                   height=500,\n                   width=700, \n                   title='Hourly Rates VS Attrition', \n                   color_discrete_sequence=['ghostwhite','darkslategray'])\nfig.show()","842ccd78":"fig = px.pie(values=df['Department'].value_counts(),\n             names=df['Department'].value_counts().index, \n             title='Department')\nfig.update_traces(marker = dict(colors = ['violet','plum','thistle'],line = dict(color = \"mediumpurple\", width = 2)))\nfig.show()\n","c33bd4da":"fig = px.histogram(x=df['Department'], \n                   color=df['Attrition'],\n                   barmode='group',\n                   color_discrete_sequence=['plum','purple'], \n                   height=500, \n                   width=600,\n                  title='Department VS Attrition')\nfig.show()","c012adf6":"fig = px.histogram(x=df['DistanceFromHome'], nbins=100, height=500, width=700, color_discrete_sequence=['deeppink'])\nfig.show()","2f92853b":"fig = px.histogram(x=df['DistanceFromHome'],\n                  marginal='box',\n                  color=df['Attrition'],\n                  barmode='group',\n                  nbins=25,\n                  title='Distance From Home VS Attrition',\n                  height=500,\n                  width=800,\n                  color_discrete_sequence=['deeppink','lightpink'])\nfig.show()","0e77dc66":"fig = px.pie(values=df['Education'].value_counts(), \n             names=df['Education'].value_counts().index, \n             title='Education Level')\nfig.update_traces(marker=dict(colors=['darkorange','orange','gold','goldenrod','khaki'],line=dict(color='chocolate',width=2)))\nfig.show()","e511de4e":"fig = px.histogram(x=df['Education'], color=df['Attrition'], barmode='group', height=500, width=600,color_discrete_sequence=['gold','orange'])\n\nfig.show()","5f0d5032":"fig = px.bar(x=df['EducationField'].value_counts().index,\n             y=df['EducationField'].value_counts(),\n             height=500, width=700,\n           text=(df['EducationField'].value_counts()\/len(df['EducationField'])*100), \n             title='Education Fields Count')\n\nfig.update_traces(textposition='outside', \n                  texttemplate='%{text:.4s}%',\n                  marker=dict(color=['dodgerblue','deepskyblue','skyblue','lightskyblue','lightblue','powderblue'],\n                                                                                  line=dict(color='navy', width=2)))\nfig.show()","a0ae4efc":"fig = px.histogram(x=df['EducationField'],\n                   color=df['Attrition'],\n                  barmode='group',\n                  height=500,\n                  width=700,\n                  color_discrete_sequence=['cornflowerblue','steelblue'])\nfig.show()","36f5df57":"fig = px.pie(values = df['EnvironmentSatisfaction'].value_counts(),\n            names=df['EnvironmentSatisfaction'].value_counts().index,\n            title='Environment Satisfaction Distribution ')\n\nfig.update_traces(marker=dict(colors=['lightcoral','darksalmon','salmon','lightsalmon'], line=dict(color='darkred', width=2)))\nfig.show()","d4bbb855":"fig = px.histogram(x=df['EnvironmentSatisfaction'],\n                  color=df['Attrition'],\n                  barmode='group',\n                  height=500,\n                  width=700,\n                  title='Environment Satisfaction VS Attrition',\n                  color_discrete_sequence=['palevioletred','orangered'])\n\nfig.show()","95816eaf":"\nfig = px.bar(x=df['Gender'].value_counts().index, y=df['Gender'].value_counts(), \n                text=(df['Gender'].value_counts()\/len(df['Gender'])*100),\n            height=500,\n            width=500,\n            title='Gender Distribution ')\n\nfig.update_traces(textposition='outside',\n                 texttemplate='%{text:.4s}%',\n                 marker=dict(color=['powderblue','pink'],line=dict(color=['darkblue','mediumvioletred'], width=2)))\n\nfig.show()","10568b16":"fig = px.histogram(x=df['Gender'],\n                  color=df['Attrition'],\n                  barmode='group',\n                  color_discrete_sequence=['mediumvioletred','darkblue'],\n                  height=500,\n                  width=700,\n                  title='Gender VS Attrition')\nfig.show()","607d408f":"fig = px.pie(values=df['JobInvolvement'].value_counts(),\n            names=df['JobInvolvement'].value_counts().index,\n            title='Job Involvement')\n\nfig.update_traces(marker=dict(colors = ['darkcyan','turquoise','mediumturquoise','paleturquoise'], line=dict(color='white', width=2)))\n\nfig.show()","e5571a3c":"fig = px.histogram(x=df['JobInvolvement'],\n                  color=df['Attrition'],\n                  barmode='group',\n                  height=500,\n                  width=700,\n                  color_discrete_sequence=['turquoise','darkcyan'],\n                  title='Job Involvement VS Attriition')\nfig.show()","5bdffbbf":"fig = px.bar(x=df['JobRole'].value_counts().index, \n             y=df['JobRole'].value_counts(),\n            text = (df['JobRole'].value_counts()\/len(df['JobRole'])*100),\n            title='Job Role',\n            height=500,\n            width=700)\nfig.update_traces(textposition='outside', texttemplate = '%{text:.4s}%', marker=dict(color='snow', line=dict(color='black', width=3)))","092f2144":"fig = px.histogram(x=df['JobRole'],\n                  color=df['Attrition'],\n                  barmode='group',\n                  color_discrete_sequence=['chocolate','burlywood'],\n                  height=500,\n                  width=900,\n                  title='Job Role VS Attrition')\n\nfig.show()","9035b70e":"fig = px.pie(values=df['JobSatisfaction'].value_counts(), \n             names=df['JobSatisfaction'].value_counts().index,\n            title='Job Satisfaction')\nfig.update_traces(marker = dict(colors = ['dimgray','gray','darkgray','silver'], line = dict(color=['black'], width=2)))\nfig.show()","2b6ae3a7":"fig = px.histogram(x=df['JobSatisfaction'],\n                  color=df['Attrition'],\n                  color_discrete_sequence=['black','silver'],\n                  barmode='group',\n                  height=500,\n                  width=700,\n                  title='Job Satisfaction VS Attrition')\nfig.show()","ba440574":"fig = px.histogram(x = df['MonthlyIncome'], \n                  nbins = 100,\n                  title='Monthly Income Distribution',\n                  height=500,\n                  width=600,\n                  color_discrete_sequence=['lightgreen'])\nfig.show()","06fdda31":"fig = px.histogram(x=df['MonthlyIncome'],\n                  color=df['Attrition'],\n                  height=500,\n                  width=700,\n                  color_discrete_sequence=['yellowgreen','olive'],\n                  barmode='group',\n                  title='Monthly Income VS Attrition')\nfig.show()","fc80d420":"gen_income = df.groupby('Gender')['MonthlyIncome'].mean().reset_index()\n\nfig = px.bar(x=gen_income['Gender'], \n            y=gen_income['MonthlyIncome'],\n            title='Average Monthly Income Of Gender',\n            height=500,\n            width=600,\n            )\nfig.update_traces(marker = dict(color = 'whitesmoke', line=dict(color='olivedrab', width=3)))\nfig.show()","b8adbbab":"df['Attrition'] = pd.factorize(df['Attrition'])[0]\ndf1 = df.drop('Attrition', 1)","374a687a":"df1 = pd.get_dummies(df1)\ndf1['Attrition'] = df['Attrition']\ndf1.head(3)","fb6e57dd":"x = df1.drop('Attrition',1)\ny = df['Attrition']\n\nprint(x.shape)\nprint(y.shape)","6ac79f80":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state=101, test_size=0.2)","0aa665ef":"accuracies =dict()\n\nrecall = dict()","48ee40ac":"rf = RandomForestClassifier()\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)","e11cca3d":"accuracies['RandomForest Classifier'] = accuracy_score(y_pred, y_test)\nrecall['RandomForest Classifier'] = metrics.recall_score(y_test,y_pred)\n\nprint('Accuracy of RandomForest Classifier is: ', accuracy_score(y_test,y_pred))\nprint('Recall Score of RndomForest Classifier is: ', metrics.recall_score(y_test, y_pred))","3dd12d93":"xgb = XGBClassifier(use_label_encoder=False)\nxgb.fit(x_train, y_train)\n\n\n#predicting x_test\ny_pred = xgb.predict(x_test)\n\n#appending accuracy score to accuracies dict\naccuracies['XGB Classifier'] = accuracy_score(y_pred, y_test)\nrecall['XGB Classifier'] = metrics.recall_score(y_test,y_pred)\n\nprint('Accuracy Score of XGB Classifier is: ', accuracy_score(y_test,y_pred))\nprint('Recall Score of XGB Classifier is: ', metrics.recall_score(y_test, y_pred))","0b5ef061":"lgr = LogisticRegression(max_iter=100000)\nlgr.fit(x_train, y_train)\n\ny_pred = lgr.predict(x_test)\n\naccuracies['Logistic Regression'] = accuracy_score(y_test, y_pred)\nrecall['Logistic Regression'] = metrics.recall_score(y_test,y_pred)\n\nprint('Accuracy Score of Logistic Regression is: ', accuracy_score(y_test, y_pred))\nprint('Recall Score of Logistic Regression Model is: ', metrics.recall_score(y_test, y_pred))","e8cb3003":"lgbm = LGBMClassifier()\n\nlgbm.fit(x_train, y_train)\ny_pred = lgbm.predict(x_test)\n\n\naccuracies['LGBM Classifier'] = accuracy_score( y_test, y_pred)\nrecall['LGBM Classifier'] = metrics.recall_score(y_test,y_pred)\n\n\nprint('Accuracy Score of LGBM Classifier is: ', accuracy_score(y_test, y_pred))\nprint('Recall Score of LGBM Classifier Model is: ', metrics.recall_score(y_test, y_pred))","99d41f7c":"recall = pd.DataFrame(list(recall.items()), columns=['Model', 'Recall Score'])\naccuracies = pd.DataFrame(list(accuracies.items()),columns=['Model','Accuracy Score'])","16914df1":"accuracies_df = pd.merge(accuracies, recall, on='Model')\naccuracies_df","b4489f06":"x_train1, x_test1, y_train1, y_test1 = train_test_split(x,y, random_state=22, test_size=0.2, shuffle=True)","d3a3e989":"# before applying smote\n\none_count=0\nzero_count =0\nfor i in y_train1:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one count is:', one_count)\nprint('Number of zero count is: ', zero_count)\n    ","95bba9d7":"\noversample = SMOTE(random_state=101)\n\nx_train1, y_train1 = oversample.fit_resample(x_train1, y_train1)","e2574f03":"# after applying SMOTE\n\none_count = 0\nzero_count =0\n\nfor i in y_train1:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n\nprint('Number of one count after applying SMOTE is: ', one_count)\nprint('Number of zero count after applying SMOTE is: ', zero_count)","751e8a24":"accuracies_smote = dict()\nrecall_smote = dict()","081f2ef9":"#LGBM Classifier After SMOTE\n\nlgbm.fit(x_train1, y_train1)\ny_smote_pred = lgbm.predict(x_test1)\n\n\nprint('Accuracy Score of LGBM Classifier after applying SMOTE is: ', accuracy_score(y_test1,y_smote_pred ))\nprint('Recall:',metrics.recall_score(y_test1,y_smote_pred))\n\n\naccuracies_smote['LGBM Classifier'] = accuracy_score(y_test1,y_smote_pred)\nrecall_smote['LGBM Classifier'] = metrics.recall_score(y_test1,y_smote_pred)","d98ea78b":"#RandomForest After SMOTE\n\nrf.fit(x_train1, y_train1)\ny_smote_pred =rf.predict(x_test1)\n\n\naccuracies_smote['RandomForest Classifier'] = accuracy_score(y_test1,y_smote_pred )\nrecall_smote['RandomForest Classifier'] = metrics.recall_score(y_test1,y_smote_pred)\n\n\nprint('Accuracy Score of RandomForest Classifier after applying SMOTE is: ', accuracy_score(y_test1,y_smote_pred ))\nprint('Recall:',metrics.recall_score(y_test,y_smote_pred))","3f3f1fa4":"#Logistic Regression After SMOTE\n\nlgr.fit(x_train1, y_train1)\ny_smote_pred =lgr.predict(x_test1)\n\n\naccuracies_smote['Logistic Regression'] = accuracy_score(y_test1,y_smote_pred )\nrecall_smote['Logistic Regression'] = metrics.recall_score(y_test1,y_smote_pred)\n\n\nprint('Accuracy Score of Logistic Regression after applying SMOTE is: ', accuracy_score(y_test1,y_smote_pred ))\nprint('Recall:',metrics.recall_score(y_test,y_smote_pred))","ace13475":"# XGB Classifier After SMOTE\n\nxgb.fit(x_train1, y_train1)\ny_smote_pred =xgb.predict(x_test1)\n\n\naccuracies_smote['XGB Classifier'] = accuracy_score(y_test1,y_smote_pred )\nrecall_smote['XGB Classifier'] = metrics.recall_score(y_test1,y_smote_pred)\n\n\nprint('Accuracy Score of XGB Classifier after applying SMOTE is: ', accuracy_score(y_test1,y_smote_pred ))\nprint('Recall:',metrics.recall_score(y_test,y_smote_pred))","ac454bf7":"recall_smote = pd.DataFrame(list(recall_smote.items()),columns=['Model','Recall Score'])\naccuracies_smote = pd.DataFrame(list(accuracies_smote.items()), columns=['Model', 'Accuracy Score'])","c614ead3":"smote_df = pd.merge(accuracies_smote, recall_smote, on='Model')\nsmote_df","d7dfff0b":"accuracies_under = dict()\nrecall_under = dict()","9d5c6a0d":"x_train2, x_test2, y_train2, y_test2 = train_test_split(x,y, random_state=101, test_size= 0.2)","0d3be7b9":"# Before Applying RandomUNDER Sampling\n\none_count = 0\nzero_count = 0\n\nfor i in y_train2:\n    if i == 1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one count after applying RandomUnder Sampler is: ', one_count)\nprint('Number of zero count after applying RandomUnder Sampler is: ', zero_count)","579cdbdc":"under = RandomUnderSampler(sampling_strategy= 0.6)\n\nx_train2, y_train2 = under.fit_resample(x_train2, y_train2)","2dc84d8a":"# After Applying RandomUnder Sampling\none_count = 0\nzero_count = 0\n\nfor i in y_train2:\n    if i == 1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one count after applying RandomUnder Sampler is: ', one_count)\nprint('Number of zero count after applying RandomUnder Sampler is: ', zero_count)","26b76e4f":"#logistic Regression After Applying RandomUnder Sampling\n\nlgr.fit(x_train2, y_train2)\ny_under_pred = lgr.predict(x_test2)\n\n\nprint('Accuracy Score of Logistic Regression is: ',accuracy_score(y_test2, y_under_pred))\nprint('Recall: ',metrics.recall_score(y_test2, y_under_pred))\n\n\naccuracies_under['Logistic Regression'] = accuracy_score(y_test2, y_under_pred)\nrecall_under['Logistic Regression'] = metrics.recall_score(y_test2, y_under_pred)","e8498ba4":"#LGBM Classifier After Applying RandomUnder Sampling\n\nlgbm.fit(x_train2, y_train2)\ny_under_pred = lgbm.predict(x_test2)\n\n\nprint('Accuracy Score of LGBM Classifier is: ',accuracy_score(y_test2, y_under_pred))\nprint('Recall: ',metrics.recall_score(y_test2, y_under_pred))\n\n\naccuracies_under['LGBM Classifier'] = accuracy_score(y_test2, y_under_pred)\nrecall_under['LGBM Classifier'] = metrics.recall_score(y_test2, y_under_pred)","629ec7b5":"#RandomForest Classifier After Applying RandomUnder Sampling\n\nrf.fit(x_train2, y_train2)\ny_under_pred = rf.predict(x_test2)\n\n\nprint('Accuracy Score of RandomForest Classifier is: ',accuracy_score(y_test2, y_under_pred))\nprint('Recall: ',metrics.recall_score(y_test2, y_under_pred))\n\n\naccuracies_under['RandomForest Classifier'] = accuracy_score(y_test2, y_under_pred)\nrecall_under['RandomForest Classifier'] = metrics.recall_score(y_test2, y_under_pred)","76dd42ee":"#XGB Classifier After Applying RandomUnder Sampling\n\nxgb.fit(x_train2, y_train2)\ny_under_pred = xgb.predict(x_test2)\n\n\nprint('Accuracy Score of xgb Classifier is: ',accuracy_score(y_test2, y_under_pred))\nprint('Recall: ',metrics.recall_score(y_test2, y_under_pred))\n\n\naccuracies_under['XGB Classifier'] = accuracy_score(y_test2, y_under_pred)\nrecall_under['XGB Classifier'] = metrics.recall_score(y_test2, y_under_pred)","f5319b67":"accuracies_under = pd.DataFrame(list(accuracies_under.items()), columns=['Model','Accuracy Score'])\nrecall_under = pd.DataFrame(list(recall_under.items()), columns=['Model','Recall Score'])","a49546c4":"under_df = pd.merge(accuracies_under, recall_under, on='Model')\nunder_df","ce91fc7a":"accuracies_tomek = dict()\nrecall_tomek = dict()","dc71aade":"x_train3, x_test3, y_train3, y_test3 = train_test_split(x,y, random_state=22, test_size=0.2, shuffle=True)\n","b7bd5707":"# Before applying SMOTE Tomek\n\none_count = 0\nzero_count = 0\n\nfor i in y_train3:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one after applying SMOTE Tomek is: ', one_count)\nprint('Number of zero after applying SMOTE Tomek is: ', zero_count)","851d2d95":"\ncombine = SMOTETomek()\n\nx_train3, y_train3 = combine.fit_resample(x_train3, y_train3)","2e02cb7d":"# After applying SMOTE Tomek\n\none_count = 0\nzero_count = 0\n\nfor i in y_train3:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one after applying SMOTE Tomek is: ', one_count)\nprint('Number of zero after applying SMOTE Tomek is: ', zero_count)","015e8cc7":"#Logistic Regression\n\nlgr.fit(x_train3, y_train3)\ny_tomek_pred = lgr.predict(x_test3)\n\n\nprint('Accuracy Score of Logistic Regression is: ',accuracy_score(y_test3, y_tomek_pred))\nprint('Recall: ',metrics.recall_score(y_test3,y_tomek_pred))\n\n\naccuracies_tomek['Logistic Regression'] = accuracy_score(y_test3, y_tomek_pred)\nrecall_tomek['Logistic Regression'] = metrics.recall_score(y_test3, y_tomek_pred)","a3fecda7":"#LGBM Classifier\n\nlgbm.fit(x_train3, y_train3)\n\ny_tomek_pred = lgbm.predict(x_test3)\n\n\nprint('Accuracy Score of LGBM Classifier is: ',accuracy_score(y_test3, y_tomek_pred))\nprint('Recall: ',metrics.recall_score(y_test3,y_tomek_pred))\n\n\naccuracies_tomek['LGBM Classifier'] = accuracy_score(y_test3,y_tomek_pred )\nrecall_tomek['LGBM Classifier'] = metrics.recall_score(y_test3,y_tomek_pred)","d648576e":"#RandomForest Classifier\n\nrf.fit(x_train3, y_train3)\ny_tomek_pred =rf.predict(x_test3)\n\n\nprint('Accuracy Score of RandomForest Classifier is: ',accuracy_score(y_test3, y_tomek_pred))\nprint('Recall: ',metrics.recall_score(y_test3,y_tomek_pred))\n\n\naccuracies_tomek['RandomForest Classifier'] = accuracy_score(y_test3,y_tomek_pred )\nrecall_tomek['RandomForest Classifier'] = metrics.recall_score(y_test3,y_tomek_pred)","107d8058":"# XGBOOST Classifier\n\nxgb.fit(x_train3, y_train3)\ny_tomek_pred =xgb.predict(x_test3)\n\n\nprint('Accuracy Score of XGB Classifier is: ',accuracy_score(y_test3, y_tomek_pred))\nprint('Recall: ',metrics.recall_score(y_test3,y_tomek_pred))\n\n\naccuracies_tomek['XGB Classifier'] = accuracy_score(y_test3,y_tomek_pred )\nrecall_tomek['XGB Classifier'] = metrics.recall_score(y_test3,y_tomek_pred)","c0cb2725":"recall_tomek = pd.DataFrame(list(recall_tomek.items()), columns=['Model', 'Recall Score'])\naccuracies_tomek = pd.DataFrame(list(accuracies_tomek.items()), columns=['Model','Accuracy Score'])","e9718acb":"tomek_df = pd.merge(accuracies_tomek, recall_tomek, on='Model')\ntomek_df","31148384":"f, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y='Model', x='Accuracy Score', data=accuracies, color='pink', edgecolor='black')\nplt.title('Accuracy Score Before Using Imbalance Handling Data Technique', fontsize=18)\nplt.show()","5aadfcb9":"f, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y='Model', x='Accuracy Score', data=accuracies_smote, color='lightblue', edgecolor='black')\nplt.title('Accuracy Score After Using SMOTE', fontsize=18)\nplt.show()","8bd038f1":"f, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y='Model', x='Accuracy Score', data=accuracies_under, color='lightsalmon', edgecolor='black')\nplt.title('Accuracy Score After Using RandomUnder Sampling', fontsize=18)\nplt.show()","541ae1ec":"f, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y='Model', x='Accuracy Score', data=accuracies_tomek, color='plum', edgecolor='black')\nplt.title('Accuracy Score After Using SMOTE Tomek', fontsize=18)\nplt.show()","9f64f6cd":"space = {'criterion' : hp.choice('criterion',['entropy','gini']), \n        'max_depth': hp.quniform('max_depth', 10,1200,10),\n        'max_features': hp.choice('max_features',['auto','sqrt','log2']),\n        'min_samples_leaf': hp.uniform('min_samples_leaf',0, 0.5),\n        'min_samples_split': hp.uniform('min_samples_split',0,1),\n        'n_estimators': hp.choice('n_estimators',[10,50,300,750,1200,1300])}","de014335":"def objective(space):\n    model = RandomForestClassifier(criterion= space['criterion'],\n                                  max_depth=space['max_depth'],\n                                  max_features=space['max_features'],\n                                  min_samples_leaf=space['min_samples_leaf'],\n                                  min_samples_split=space['min_samples_split'],\n                                  n_estimators=space['n_estimators'])\n    \n    # we aim to maximize the accuracy, therefore we return as negative value\n    accuracy = cross_val_score(model, x_train3, y_train3, cv=5).mean()\n    return{'loss': -accuracy, 'status':STATUS_OK}\n    \n    ","5334217d":"trials = Trials()\n\nbest = fmin(fn = objective,\n            space=space,\n           algo = tpe.suggest,\n           max_evals=80,\n           trials=trials)\n\nbest","29b430da":"crit = {0: 'entropy', 1:'gini'}\nfeat = {0:'auto', 1:'sqrt', 2:'log2'}\nn_est = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5: 1300}","127f19e7":"rf_clf = RandomForestClassifier(criterion= crit[best['criterion']],\n                           max_depth= best['max_depth'],\n                           max_features= feat[best['max_features']],\n                           min_samples_leaf= best['min_samples_leaf'],\n                           min_samples_split= best['min_samples_split'],\n                           n_estimators= n_est[best['n_estimators']])\nrf = rf_clf.fit(x_train3,y_train3)\npred = rf.predict(x_test3)\nprint(confusion_matrix(y_test3, pred))\nprint('Recall score after using  Bayesian optimisation is: ',metrics.recall_score(y_test3,pred))\nprint('Accuracy score after using  Bayesian optimisation is: ',accuracy_score(y_test3, pred))","8c5fa769":"# SMOTE Tomek (OverSampling and UnderSampling Combined)","52e0b12d":"# **Step 3** \n# Handling Imbalance Data","5e4e3bcc":"# Step 2  \n# Data PreProcessing","b07313f6":"Almost **71%** of employess are rarely required to travel for work.\n\n**18.8%** employees travel frequently for their Jobs.\n\n**10.2%** employees job dosen't require travel at all.","0dd65b98":"**Monthly Income range between 1000 to upto 20k.**","612f08c5":"**Education**\n\n1 'Below College'\n\n2 'College'\n\n3 'Bachelor'\n\n4 'Master'\n\n5 'Doctor'\n\n**38.9%** of employees have Bachelor's Qualification.\n\n**27%** employees have a Master's Degree\n\nOnly 3% employees are Doctor","daddd9ad":"# LGBM Classifier","542c5386":"**Majority of employees that took attritions are the ones whose job require them to rarely travel.**","5bd61793":"Data is highly imbalanced as only 16% of the employees took attrition and 83.3% continued their jobs.\n\nWill use handling imbalance data techniques later.\n","a7c99956":"**XGB & LGBM Classifier has the highest Accuracy Score, whereas the RandomForest Classifier has the highest Recall score.**","e21d3394":"**Enviroment Satisfaction**\n\n1 'Low'\n\n2 'Medium'\n\n3 'High'\n\n4 'Very High'\n\n**Almost 80% Employees are highly satisfied with work Environment.**\n\n****And 20% employees are not satisfied ****","0a69e399":"**RandomForest Classifier has the highest Accuracy and Recall Score after using SMOTE(OverSampling)**","27f832ab":"*****Average Monthly Income Among Male and Female*****","17ba2227":"**Although Accuracy Scores have droped a little for this method, i will select RandomForest Classifier, as recall score is highest for this model.**","7271d352":"# Logistic Regression Classifier","0697597b":"# Bayesian optimisation","20602d2d":"Majority of employees that got attrition range between **28** to **35**.","7b1431e7":"# XGBOOST","4b2b0160":"**Most of the employees stay nearby to Office.**","4a8ac9cb":"**71%** of the employees are associated with **Science** field\n\n**10**% are associated with **Marketing**\n\nOnly **1.8%** work in **Human resources**","7ffeaaa9":"**Slight Difference Between average salary of Male and Female.**\n\n**Female earning slightly more than Males.**","5d2491ac":"**Job Involvement**\n\n1 'Low'\n\n2 'Medium'\n\n3 'High'\n\n4 'Very High'\n\n**70% Employees are highly involved with their job.**\n","3b4ef78e":"# **Model Selection**\n\n# RandomForest","7e40ee83":"Highest Attrition rate is seen among **Lab technicians**.","d34e8f11":"Since data is highly imbalance accuracy scores of trivial prediction model is completely useless as it has absolutely no discriminatory power.\nData needs to be balance for more accurate prediction.\n\nSMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling.\n","3a628ed1":"Number of one in y before applying smote is **1233**\nNumber of zero in y before applying smote is **237**\n\nLets apply SMOTE and see how the values changes.","73bc1a26":"**In this method, again RandomForest Classifier has the highest Accuracy & Recall score.**","6a696258":"# **Step 1**\n\n\n# Analysing Data","de2a6117":"SMOTE may be the most popular oversampling technique and can be combined with many different undersampling techniques.\n\nSMOTE+TOMEK is such a hybrid technique that aims to clean overlapping data points for each of the classes distributed in sample space.\n","cc285753":"# Smote (OverSampling)","c184f6b1":"**65.4%** of employees belong to **Research and Development Department**\n\n**30.3%** belong to **Sales** Department\n\nOnly **4.2%** belong to **Human Resources**","d6df326a":"Job Satisfaction\n\n1 'Low'\n\n2 'Medium'\n\n3 'High'\n\n4 'Very High'\n\n**60% Employees are Satisfied with their Job**","62e628a2":"Both one's and zero's has same values now.\n\nLets apply Classifier model to Oversampled data and see if the prediction has improved or not.\n\n**Oversampling and Undersampling Techniques should only be applied to train data not test data.**","5ff7d1d0":"# RandomUnder Sampling"}}