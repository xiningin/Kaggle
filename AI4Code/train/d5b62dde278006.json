{"cell_type":{"ffd0c140":"code","492b9eca":"code","b40475f0":"code","cd9e39b0":"code","dcf05085":"code","e3bda8fe":"code","1992f551":"code","a3c03e46":"code","5756a59d":"code","3ac093e1":"code","173e35ea":"code","19313ef1":"code","674d5bc9":"code","16ce890a":"code","14251cf9":"code","730ab7bb":"code","d94e3362":"code","5f140079":"code","2ca78341":"code","eb41f969":"code","1669dd6f":"code","2368be51":"code","f955aa94":"code","0ff59142":"code","0d5abb4b":"code","02ea236f":"code","c3aa40d2":"code","22b58a36":"code","962f93d2":"code","2b819012":"code","88c8f1f3":"code","39a340f8":"code","d3b0253b":"code","85040a81":"code","ee40b285":"code","4137b89e":"code","e5876bd4":"code","01f15c0f":"code","4b433e0a":"code","d94a10e7":"code","eea394b8":"code","9c56aca4":"code","3813c3fb":"code","b851cc84":"code","5e9eb45d":"code","944e18d9":"code","bcaed45a":"code","6999bef3":"code","2a7d4e3b":"code","e82f36aa":"markdown","10f433eb":"markdown","58c5014e":"markdown","ac44c06f":"markdown","cbb130e0":"markdown","d85d2bb6":"markdown","0774d6c1":"markdown","09e438aa":"markdown","2e229b55":"markdown","9cab9484":"markdown","a3ecef08":"markdown","3ee53fdf":"markdown","d61d110d":"markdown","6a739a8f":"markdown","fd486962":"markdown","a52b0023":"markdown","737e77c2":"markdown","fb868824":"markdown","446cade5":"markdown"},"source":{"ffd0c140":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re","492b9eca":"os.chdir('..\/input')\nos.getcwd()","b40475f0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \ndata = pd.read_csv('\/kaggle\/input\/haiwaiian-hotel-reviews\/Hilton_Hawaiian_Village_Waikiki_Beach_Resort-Honolulu_Oahu_Hawaii__en.csv',engine='python',index_col=False, nrows = 100)\ndata.head()","cd9e39b0":"import spacy\nfrom tqdm import tqdm\nnlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)","dcf05085":"data.head(3)","e3bda8fe":"data.shape","1992f551":"txt = 'This is a huge resort, capacity-wise (not necessarily in terms of the territory. It sits next to a commercial \"village\" of sorts (the usual array of overpriced restaurants and various shops catering to tourist clientele).There\"s nothing special to it, I\"d even call it \"faceless\" despite a stream of celebrities and dignitaries of all kinds that graced the hotel with their presence and even movies exposure - but, at the same time, there\"s nothing to complain about. The hotel is being well-run and well-maintained. The rooms are rather standard for resorts of this kind. The beach is right there (not the best, though - large-grain sand and quite a bit of crushed shells). In the room, we were pleasantly surprised to find a PS-3 gaming console that can run both games and DVDs you may rent from automated kiosks (free if you have a status of certain level with Hilton).The hotel has a large, well-equipped gym (I counted 33 cardio machines plus HIIT\/pulley rack, all typical main-muscle groups strength machines, a full rack of dumbbells running through 100 lbs, balls, batons, etc. There\"s a nice spa, too. They also offer group classes in various sports.Breakfasts were ok, although I\"d expect a bit higher variety of morning foods. In any case, there are options for dining within a walking distance from the hotel.All in all, a positive experience. By Hawaii standards, an excellent hotel'\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","a3c03e46":"txt = 'We stayed at HHV every summer in the 1980\"s. This is our first trip back since then. The Village is as amazing and as beautiful as almost 40 years ago. It is self-contained-ABC stores, jewelers, clothing stores, luggage stores, eateries of all stripes, etc. With 4000 rooms, the check-in desk is bustling 24-7. The beaches (public) are pristine and the water beautiful. There is even a Church on the Beach which has operated there for 49 years and is open to all faiths. We actually found the food competitive in taste and price. The only complaint I have is the soundproofing between rooms. Our neighbor played loud music night and day, beginning at 7.20 a.m.(perhaps they are having a staycation in their room). After security visited, they would lower the volume, but we could still hear the music and their voices. As soon as security left, they started up again. We finally asked to be moved. I feel bad for whoever ended with the room next. The next room was quiet, because we had nice neighbors. But because of the location and convenience, and customer service, I still rate them a 5.'\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","5756a59d":"txt =  \"I love hilton, but for this hotel, I really cannot recommend for anyone. We booked the most expansive room of the hotel, but except bigger room, the whole room was old and feel like in a 3 stars hotel room. Not worthy for the price at all. Won't recommend for this hotel.\"\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","3ac093e1":"txt =  \"Me and my father really enjoyed everything about this hotel. Would recommend it to anyone and hopefully we can soon return for another vacation.Beds were big and really comfy, also nice and big rooms.\"\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","173e35ea":"competitors = ['Chevy','chevy','Ford','ford','Nissan','nissan','Honda','honda','Chevrolet','chevrolet','Volkswagen','volkswagen','benz','Benz','Mercedes','mercedes','subaru','Subaru','VW']","19313ef1":"data.head(2)","674d5bc9":"aspect_terms = []\ncomp_terms = []\neaspect_terms = []\necomp_terms = []\nenemy = []\n\ndef get_aspect_adj(data):\n    for x in tqdm(range(len(data['review_body']))):\n        amod_pairs = []\n        advmod_pairs = []\n        compound_pairs = []\n        xcomp_pairs = []\n        neg_pairs = []\n        eamod_pairs = []\n        eadvmod_pairs = []\n        ecompound_pairs = []\n        eneg_pairs = []\n        excomp_pairs = []\n        enemlist = []\n        if len(str(data['review_body'][x])) != 0:\n            lines = str(data['review_body'][x]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n            for line in lines:\n                enem_list = []\n                for eny in competitors:\n                    enem = re.search(eny,line)\n                    if enem is not None:\n                        enem_list.append(enem.group())\n                if len(enem_list)==0:\n                    doc = nlp(line)\n                    str1=''\n                    str2=''\n                    for token in doc:\n                        if token.pos_ is 'NOUN':\n                            for j in token.lefts:\n                                if j.dep_ == 'compound':\n                                    compound_pairs.append((j.text+' '+token.text,token.text))\n                                if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n                                    str1 = j.text+' '+token.text\n                                    amod_pairs.append(j.text+' '+token.text)\n                                    for k in j.lefts:\n                                        if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n                                            str2 = k.text+' '+j.text+' '+token.text\n                                            amod_pairs.append(k.text+' '+j.text+' '+token.text)\n                                    mtch = re.search(re.escape(str1),re.escape(str2))\n                                    if mtch is not None:\n                                        amod_pairs.remove(str1)\n                        if token.pos_ is 'VERB':\n                            for j in token.lefts:\n                                if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                                    advmod_pairs.append(j.text+' '+token.text)\n                                if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                                    neg_pairs.append(j.text+' '+token.text)\n                            for j in token.rights:\n                                if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                                    advmod_pairs.append(token.text+' '+j.text)\n                        if token.pos_ is 'ADJ':\n                            for j,h in zip(token.rights,token.lefts):\n                                if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n                                    for k in j.lefts:\n                                        if k.dep_ is 'aux':\n                                            xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n                                elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n                                    if k.dep_ is 'aux':\n                                            neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n\n                else:\n                    enemlist.append(enem_list)\n                    doc = nlp(line)\n                    str1=''\n                    str2=''\n                    for token in doc:\n                        if token.pos_ is 'NOUN':\n                            for j in token.lefts:\n                                if j.dep_ == 'compound':\n                                    ecompound_pairs.append((j.text+' '+token.text,token.text))\n                                if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n                                    str1 = j.text+' '+token.text\n                                    eamod_pairs.append(j.text+' '+token.text)\n                                    for k in j.lefts:\n                                        if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n                                            str2 = k.text+' '+j.text+' '+token.text\n                                            eamod_pairs.append(k.text+' '+j.text+' '+token.text)\n                                    mtch = re.search(re.escape(str1),re.escape(str2))\n                                    if mtch is not None:\n                                        eamod_pairs.remove(str1)\n                        if token.pos_ is 'VERB':\n                            for j in token.lefts:\n                                if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                                    eadvmod_pairs.append(j.text+' '+token.text)\n                                if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                                    eneg_pairs.append(j.text+' '+token.text)\n                            for j in token.rights:\n                                if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                                    eadvmod_pairs.append(token.text+' '+j.text)\n                        if token.pos_ is 'ADJ':\n                            for j in token.rights:\n                                if j.dep_ is 'xcomp':\n                                    for k in j.lefts:\n                                        if k.dep_ is 'aux':\n                                            excomp_pairs.append(token.text+' '+k.text+' '+j.text)\n            pairs = list(set(amod_pairs+advmod_pairs+neg_pairs+xcomp_pairs))\n            epairs = list(set(eamod_pairs+eadvmod_pairs+eneg_pairs+excomp_pairs))\n            for i in range(len(pairs)):\n                if len(compound_pairs)!=0:\n                    for comp in compound_pairs:\n                        mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n                        if mtch is not None:\n                            pairs[i] = pairs[i].replace(mtch.group(),comp[0])\n            for i in range(len(epairs)):\n                if len(ecompound_pairs)!=0:\n                    for comp in ecompound_pairs:\n                        mtch = re.search(re.escape(comp[1]),re.escape(epairs[i]))\n                        if mtch is not None:\n                            epairs[i] = epairs[i].replace(mtch.group(),comp[0])\n\n        aspect_terms.append(pairs)\n        comp_terms.append(compound_pairs)\n        easpect_terms.append(epairs)\n        ecomp_terms.append(ecompound_pairs)\n        enemy.append(enemlist)\n    data['compound_nouns'] = comp_terms\n    data['aspect_keywords'] = aspect_terms\n    data['competition'] = enemy\n    data['competition_comp_nouns'] = ecomp_terms\n    data['competition_aspects'] = easpect_terms\n    return data","16ce890a":"data.shape","14251cf9":"data1 = get_aspect_adj(data)","730ab7bb":"!pip install vaderSentiment","d94e3362":"my_df = pd.DataFrame(columns = ['row_index','aspect_adj_string','pos_score','neg_score','compound_score','neutral_score'])\nmy_df","5f140079":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\nmy_df = pd.DataFrame(columns = ['row_index','aspect_adj_string','pos_score','neg_score','compound_score','neutral_score'])\nglobal_sentiment = {}\nfor aspect_adj_tuple in data1.aspect_keywords.iteritems():\n    row_sentiment = {}\n    for text in aspect_adj_tuple[1]:\n        sentiment_array = []\n        polarity = analyser.polarity_scores(text)\n#         print(polarity)\n#         sentiment_array.append(polarity['pos'])\n#         sentiment_array.append(polarity['neg'])\n#         row_sentiment[text] = sentiment_array\n        my_df = my_df.append({'row_index':aspect_adj_tuple[0],'aspect_adj_string':text,'pos_score':polarity['pos'],'neg_score':polarity['neg'],'compound_score':polarity['compound'],'neutral_score':polarity['neu']},ignore_index=True)\n#     global_sentiment[aspect_adj_tuple[0]] = row_sentiment\n    \n# print(global_sentiment)\n","2ca78341":"my_df[(my_df['compound_score']<0) & (my_df['pos_score']>0) & (my_df['neg_score']>0)]","eb41f969":"my_df.shape","1669dd6f":"final_df = my_df[((my_df['compound_score']>0.1) | (my_df['compound_score']< -0.1))]\n","2368be51":"final_df[(final_df['compound_score']<0) & (final_df['pos_score']> final_df['neg_score'])]","f955aa94":"final_df['final_sentiment'] = ['pos' if i>0 else 'neg' for i in final_df['compound_score']]","0ff59142":"final_df.head(30)","0d5abb4b":"# import spacy\n\n# nlp = spacy.load(\"en_core_web_sm\")\n# # adj =[]\n# # aspect = []\n# temp['adj']='nan'\n# temp['aspect']='nan'\n\n# for row_txt in final_df.aspect_adj_string.iteritems():\n#     print(row_txt)\n#     doc = nlp(row_txt[1])\n#     adj =[]\n#     aspect = []\n#     print(doc)\n#     for token in doc:\n#         if (token.tag_ == 'ADV') or (token.tag_ == 'ADJ') or (token.tag_ == 'JJ'):\n#             print(token.text)\n#             adj.append(token.text)\n\n        \n#         elif (token.tag_ == 'NN') or (token.tag_ == 'NNP') or (token.tag_ == 'VBG') or (token.tag_ == 'VB'):\n#             print(token.text)\n#             aspect.append(token.text)\n    \n#     temp=temp.append({'aspect':aspect,'adj':adj},ignore_index=True)\n            \n","02ea236f":"doc = 'well done'\ndoc = nlp(doc)\nfor token in doc:\n    print(token.tag_)\n    print(token.text)","c3aa40d2":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nfor row_txt in final_df.aspect_adj_string.iteritems():\n    doc = nlp(row_txt[1])\n    adj = []\n    aspect = []\n    for token in doc:\n\n        if (token.tag_ == 'ADV') or (token.tag_ == 'ADJ') or (token.tag_ == 'UH') or (token.tag_ == 'JJS') or (token.tag_ == 'RB') or (token.tag_ == 'JJ') :\n            adj.append(token.text)\n            \n        elif (token.tag_ == 'NN') or (token.tag_ == 'NNP') or (token.tag_ == 'VBG') or (token.tag_ == 'VB')  or (token.tag_ == 'VBN') or (token.tag_ == 'NNS'):\n            aspect.append(token.text)\n\n    if (len(adj) == len(aspect)) & (len(adj)>0):\n        final_df.at[row_txt[0],'adj']= adj\n        final_df.at[row_txt[0],'aspect']= aspect\n    elif len(adj) > len(aspect):\n        final_df.at[row_txt[0],'adj']= adj[:len(aspect)]\n        final_df.at[row_txt[0],'aspect']= aspect\n    elif  len(adj) < len(aspect):\n        final_df.at[row_txt[0],'adj']= adj\n        final_df.at[row_txt[0],'aspect']= aspect[:len(adj)]\n    else:\n        final_df.at[row_txt[0],'adj']= adj\n        final_df.at[row_txt[0],'aspect']= aspect\n            ","22b58a36":"final_df","962f93d2":"import csv\nword_vec = pd.read_csv('\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt',sep=' ',index_col=0,header=None,quoting=csv.QUOTE_NONE)","2b819012":"word_vec.head()","88c8f1f3":"final_df.reset_index(inplace=True,drop=True)","39a340f8":"final_df.head()","d3b0253b":"import numpy as np\naspect_vec = np.zeros((final_df.shape[0],50))\nfor aspect in final_df.aspect.iteritems():\n    if len(aspect[1]):\n        embedding_vector = word_vec[word_vec.index == aspect[1][0]].iloc[:,:].values\n        if len(embedding_vector):\n            aspect_vec[aspect[0]] = embedding_vector\n            ","85040a81":"# pd.DataFrame(aspect_vec)","ee40b285":"from sklearn.cluster import KMeans\nkmeans= KMeans(n_clusters=60, random_state=0)\nkmeans.fit(aspect_vec)","4137b89e":"# print(kmeans.cluster_centers_)","e5876bd4":"print(kmeans.labels_)","01f15c0f":"final_df['aspect_cluster_labels'] = kmeans.labels_","4b433e0a":"final_df.sort_values(by=['aspect_cluster_labels'],ascending=True)[:30]","d94a10e7":" # clustering dataset\n# determine k using elbow method\n\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\n\n# create new plot and data\nplt.plot()\nX = aspect_vec\ncolors = ['b', 'g', 'r']\nmarkers = ['o', 'v', 's']\n\n# k means determine k\ndistortions = []\nK = range(1,100)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","eea394b8":"pd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\npd.options.display.max_colwidth = 1000","9c56aca4":"data[['review_body', 'aspect_keywords']].head(10)","3813c3fb":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","b851cc84":"import operator\nsentiment = []\nfor i in range(len(data)):\n    score_dict={'pos':0,'neg':0,'neu':0}\n    if len(data['aspect_keywords'][i])!=0: \n        for aspects in data['aspect_keywords'][i]:\n            sent = analyser.polarity_scores(aspects)\n            score_dict['neg'] += sent['neg']\n            score_dict['pos'] += sent['pos']\n        #score_dict['neu'] += sent['neu']\n        sentiment.append(max(score_dict.items(), key=operator.itemgetter(1))[0])\n    else:\n        sentiment.append('NaN')\ndata['sentiment'] = sentiment\ndata.head()","5e9eb45d":"int_sent = []\nfor sent in data['sentiment']:\n    if sent is 'NaN':\n        int_sent.append('NaN')\n    elif sent is 'pos':\n        int_sent.append('1')\n    else:\n        int_sent.append('0')\ndata['int_sent'] = int_sent\ndata.head()","944e18d9":"d = {'sent':toy_rev['Positive Review'],'sent_pred':toy_rev['int_sent']}\nmetric_df = pd.DataFrame(data=d)\nmetric_df.head()","bcaed45a":"len(metric_df.sent)","6999bef3":"metric_df = metric_df[metric_df.sent_pred != 'NaN']\nlen(metric_df.sent)","2a7d4e3b":"from sklearn.metrics import accuracy_score,auc,f1_score,recall_score,precision_score\nprint('accuracy')\nprint(accuracy_score(metric_df.sent, metric_df.sent_pred))\nprint('f1 score')\nprint(f1_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\nprint('recall')\nprint(recall_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\nprint('precision')\nprint(precision_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))","e82f36aa":"### ADVMOD - adverb modifier\n#### An adverb modifier of a word is a (non-clausal) adverb or adverb-headed phrase that serves to modify\n#### the meaning of the word\n### ex - 'Drives --advmod--> well'","10f433eb":"## **Using spaCy for dependency parsing which forms the crux of aspect extraction**","58c5014e":"### Here we have arbitarily taken ratings greater than 3 as positive and everything else as negative","ac44c06f":"## Possible improvements that can be made\n*  Tricky situation of removing stopwords to reduce unwanted extractions of non-aspects but this can also affect spaCy's dependency parsing. Same goes with noun chunk merging as well. If someone can think of a better way to remove stopwords and still retain spaCy's dependency goodness it can greatly improve the accuracy\n\n* This is not a ML task per se since we do more of parsing than ML. Although Bi-Directional LSTM have been very good at ABSA tasks in the past, unlike semeval tasks we do not have a fixed topic for our aspects to fall into. If someone can use the parsing aspect of the code to implement BLSTM in this case, that would be great\n\n* better alternatives to vaderSentiment if available (unsupervised\/ semi-supervised methods might be better here I think)\n\n* The very definition of aspects can be a bit vague at times hence we do not have a valid metric to measure the aspect extraction's accuracy\n","cbb130e0":"# Aspect Based Sentiment Analysis on Car Reviews\n## Taking Toyota Cars as an example","d85d2bb6":"## Initial Verification","0774d6c1":"**Reason for using competitor name list is to remove potential misleading aspects-sentiments, since we are interested to acquire aspect info about Toyota and not any other brand. This is because a reviewer might be comparing a Benz saying it has superior handling when compared to the car the person is reviewing and this can lead to misclassifications******","09e438aa":"## =============================================================","2e229b55":"**from https:\/\/nlp.stanford.edu\/software\/dependencies_manual.pdf**\n### AMOD - adjectival modifier\n#### An adjectival modifier of a Noun is any adjectival phrase that serves to modify the meaning of the Noun\n### ex - 'Great <--amod-- Car', 'Long <--amod-- range'","9cab9484":"### NEG - self explanatory\n### ex - not <--neg-- wonderful","a3ecef08":"## P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis.","3ee53fdf":"## Removing NaN values in the sentiment predictions","d61d110d":"### XCOMP -  open clausal complement\n#### An open clausal complement (xcomp) of a verb or an adjective is a predicative or clausal complement without its own subject\n### ex - 'wonderful --xcomp--> drive'\n","6a739a8f":"## **Combining the review title and review body for the text corpus****","fd486962":"## Filtering the data to remove the non-sentimental\/garbage\/gray sentiments from entire dataframe","a52b0023":" ## **Using spaCy's awesome displacy module to show the dependency relations**","737e77c2":"### COMPOUND WORDS\n#### Generally from a review standpoint, compound words often do not offer us sentiments per se, hence my code looks for possible compound word pairs and then checks with the aspect words extracted if it can add more detail to the extracted aspects - ex Outstanding passenger van gives *more context* than Outstanding van (which is what my code would have extracted without the compound word search) while the compound word search will identify passenger van as a compound word","fb868824":"## We use vaderSentiment for sentiment analysis because of it's speed and simplicity. It offers 3 types of polarity -  positive, negative and neutral. As a result we can filter all aspects which have high neutral scores hence minimizing errors caused due to wrong extraction of aspects and stopwords","446cade5":"## Verification on filtered data"}}