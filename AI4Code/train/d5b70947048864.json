{"cell_type":{"c0e5fba3":"code","c4349b51":"code","10952ab8":"code","c3005bd5":"code","10a4a41a":"code","426404a1":"code","1553244a":"code","008bdf56":"code","ed355ef9":"code","c2e6440b":"code","4682bc72":"code","5dad1eb1":"code","196497b7":"code","c43b57ae":"code","a720dd63":"code","466578ed":"code","92687cef":"code","951887fb":"code","f93bfc37":"code","e1b216a7":"code","2ee441c2":"code","1ceffa50":"markdown","d6055e19":"markdown","91ccd5c3":"markdown","6808e3af":"markdown","d50c0171":"markdown","b9d7443c":"markdown","67ef65b3":"markdown","c79cf5ff":"markdown","9bbcb9ef":"markdown","ae0a9d76":"markdown","74911285":"markdown","6fedee7f":"markdown","25abddd5":"markdown","1bc432a1":"markdown","dd9cfcc0":"markdown","3fee194d":"markdown"},"source":{"c0e5fba3":"pip install -U tensorflow-addons","c4349b51":"import math\nimport numpy as np\nimport pandas as pd\nimport wandb\nimport warnings\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#ignore warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","10952ab8":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')\n    \nCONFIG = dict(competition = 'TabTransformer',_wandb_kernel = 'tensorgirl')","c3005bd5":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")\n\nall_features = [\n    \"A0T1G3C6\",\n    \"A0T0G6C4\",\n    \"A0T0G5C5\",\n    \"A0T0G4C6\",\n    \"A0T1G2C7\",\n    \"A0T0G7C3\",\n    \"A0T1G0C9\",\n    \"A0T0G3C7\",\n    \"A0T0G8C2\",\n    \"A0T0G10C0\",\n    \"A0T10G0C0\",\n    \"target\"\n    ]\n\nNUMERIC_FEATURE_NAMES = [\n    \"A0T1G3C6\",\n    \"A0T0G6C4\",\n    \"A0T0G5C5\",\n    \"A0T0G4C6\",\n    \"A0T1G2C7\",\n    \"A0T0G7C3\",\n    \"A0T1G0C9\",\n    \"A0T0G3C7\",\n    \"A0T0G8C2\",\n    ]\n\nTARGET_FEATURE_NAME  = \"target\"\nTARGET_LABELS = [\"Bacteroides_fragilis\",\"Streptococcus_pyogenes\",\"Streptococcus_pneumoniae\",\"Campylobacter_jejuni\",        \n\"Salmonella_enterica\",  \n\"Escherichia_coli\",           \n\"Enterococcus_hirae\",          \n\"Escherichia_fergusonii\",      \n\"Staphylococcus_aureus\",       \n\"Klebsiella_pneumoniae\"]\n\n# A dictionary of the categorical features and their vocabulary.\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {\n    \"A0T0G10C0\": sorted(list(train[\"A0T0G10C0\"].unique())),\n    \"A0T10G0C0\": sorted(list(train[\"A0T10G0C0\"].unique()))}\n\n# A list of the categorical feature names.\nCATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n# A list of all the input features.\nFEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n# A list of column default values for each feature.","10a4a41a":"# basic stats of features\ntrain.describe().style.background_gradient(cmap=\"Pastel1\")","426404a1":"plt.figure(figsize = (25,9))\nsns.heatmap(train.isna().values, cmap = [\"#2a9d8f\",\"#ff355d\"], xticklabels=train.columns)\nplt.title(\"Missing values in training Data\", size=20);","1553244a":"fig, ax = plt.subplots(3,3, figsize=(18, 18))\nfor i, feature in enumerate(NUMERIC_FEATURE_NAMES):\n    sns.distplot(train[feature], color = \"#ff355d\", ax=ax[math.floor(i\/3),i%3]).set_title(f'{feature} Distribution')\nfig.show()","008bdf56":"def countplot_features(df_train, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''    \n           \n    plt.figure(figsize = (25, 9))\n    \n    sns.countplot(df_train[feature], color = '#ff355d')\n        \n    plt.title(title, fontsize=15)\n    plt.xticks(rotation=45)\n    plt.show();\n\n# plot distributions of categorical features\nfor feature in CATEGORICAL_FEATURE_NAMES:\n    fig = countplot_features(train, feature=feature, title = \"Frequency of \"+ feature)","ed355ef9":"#code copied from https:\/\/www.kaggle.com\/hamzaghanmi\/welcome-tps-feb-2022\n\npie, ax = plt.subplots(figsize=[18,8])\ntrain.groupby('target').size().plot(kind='pie',autopct='%.2f',ax=ax,title='Target distibution' , cmap = \"Pastel1\")","c2e6440b":"plt.figure(figsize=(25, 9))\nsns.heatmap(train[[f'{feature}' for feature in all_features]].corr(),annot=True ,cmap = \"Pastel1\");","4682bc72":"train, val = np.split(train.sample(frac=1), [int(0.8*len(train))])\ntrain = train[all_features]\nval = val[all_features]\ntrain = train.dropna()\ntest = test.dropna()","5dad1eb1":"train_data_file = \"train_data.csv\"\ntest_data_file = \"test_data.csv\"\n\ntrain.to_csv(train_data_file, index=False, header=False)\nval.to_csv(test_data_file, index=False, header=False)","196497b7":"# Save train data to W&B Artifacts\ntrain.to_csv(\"train_wandb.csv\", index = False)\nrun = wandb.init(project='TabTransformer', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\".\/train_wandb.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","c43b57ae":"LEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0001\nDROPOUT_RATE = 0.2\nBATCH_SIZE = 265\nNUM_EPOCHS = 15\n\nNUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\nNUM_HEADS = 4  # Number of attention heads.\nEMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\nMLP_HIDDEN_UNITS_FACTORS = [\n    2,\n    1,\n]  # MLP hidden layer units, as factors of the number of inputs.\nNUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.","a720dd63":"def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path,\n        batch_size=batch_size,\n        column_names=all_features,\n        label_name=TARGET_FEATURE_NAME,\n        num_epochs=1,\n        header=False,\n        shuffle=shuffle,\n    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n    return dataset.cache()","466578ed":"target_label_lookup = layers.StringLookup(\n    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n)\n\n\ndef prepare_example(features, target):\n    target_index = target_label_lookup(target)\n    \n    return features, target_index\n","92687cef":"def run_experiment(\n    model,\n    train_data_file,\n    test_data_file,\n    num_epochs,\n    learning_rate,\n    weight_decay,\n    batch_size,\n):\n\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.BinaryCrossentropy(),\n        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n    )\n\n    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n\n    print(\"Start training the model...\")\n    history = model.fit(\n        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n    )\n    print(\"Model training finished\")\n\n    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n\n    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n\n    return history\n","951887fb":"def create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURE_NAMES:\n        if feature_name in NUMERIC_FEATURE_NAMES:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.float32\n            )\n        else:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.string\n            )\n    return inputs","f93bfc37":"def encode_inputs(inputs, embedding_dims):\n\n    encoded_categorical_feature_list = []\n    numerical_feature_list = []\n\n    for feature_name in inputs:\n        if feature_name in CATEGORICAL_FEATURE_NAMES:\n\n            # Get the vocabulary of the categorical feature.\n            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n\n            # Create a lookup to convert string values to an integer indices.\n            # Since we are not using a mask token nor expecting any out of vocabulary\n            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n            lookup = layers.StringLookup(\n                vocabulary=vocabulary,\n                mask_token=None,\n                num_oov_indices=0,\n                output_mode=\"int\",\n            )\n\n            # Convert the string input values into integer indices.\n            encoded_feature = lookup(inputs[feature_name])\n\n            # Create an embedding layer with the specified dimensions.\n            embedding = layers.Embedding(\n                input_dim=len(vocabulary), output_dim=embedding_dims\n            )\n\n            # Convert the index values to embedding representations.\n            encoded_categorical_feature = embedding(encoded_feature)\n            encoded_categorical_feature_list.append(encoded_categorical_feature)\n\n        else:\n\n            # Use the numerical features as-is.\n            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n            numerical_feature_list.append(numerical_feature)\n\n    return encoded_categorical_feature_list, numerical_feature_list\n\n","e1b216a7":"def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n\n    mlp_layers = []\n    for units in hidden_units:\n        mlp_layers.append(normalization_layer),\n        mlp_layers.append(layers.Dense(units, activation=activation))\n        mlp_layers.append(layers.Dropout(dropout_rate))\n\n    return keras.Sequential(mlp_layers, name=name)","2ee441c2":"def create_tabtransformer_classifier(\n    num_transformer_blocks,\n    num_heads,\n    embedding_dims,\n    mlp_hidden_units_factors,\n    dropout_rate,\n    use_column_embedding=False,\n):\n\n    # Create model inputs.\n    inputs = create_model_inputs()\n    # encode features.\n    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n        inputs, embedding_dims\n    )\n    # Stack categorical feature embeddings for the Tansformer.\n    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n    # Concatenate numerical features.\n    numerical_features = layers.concatenate(numerical_feature_list)\n\n    # Add column embedding to categorical feature embeddings.\n    if use_column_embedding:\n        num_columns = encoded_categorical_features.shape[1]\n        column_embedding = layers.Embedding(\n            input_dim=num_columns, output_dim=embedding_dims\n        )\n        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n        encoded_categorical_features = encoded_categorical_features + column_embedding(\n            column_indices\n        )\n\n    # Create multiple layers of the Transformer block.\n    for block_idx in range(num_transformer_blocks):\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embedding_dims,\n            dropout=dropout_rate,\n            name=f\"multihead_attention_{block_idx}\",\n        )(encoded_categorical_features, encoded_categorical_features)\n        # Skip connection 1.\n        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n            [attention_output, encoded_categorical_features]\n        )\n        # Layer normalization 1.\n        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n        # Feedforward.\n        feedforward_output = create_mlp(\n            hidden_units=[embedding_dims],\n            dropout_rate=dropout_rate,\n            activation=keras.activations.gelu,\n            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n            name=f\"feedforward_{block_idx}\",\n        )(x)\n        # Skip connection 2.\n        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n        # Layer normalization 2.\n        encoded_categorical_features = layers.LayerNormalization(\n            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n        )(x)\n\n    # Flatten the \"contextualized\" embeddings of the categorical features.\n    categorical_features = layers.Flatten()(encoded_categorical_features)\n    # Apply layer normalization to the numerical features.\n    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n    # Prepare the input for the final MLP block.\n    features = layers.concatenate([categorical_features, numerical_features])\n\n    # Compute MLP hidden_units.\n    mlp_hidden_units = [\n        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n    ]\n    # Create final MLP.\n    features = create_mlp(\n        hidden_units=mlp_hidden_units,\n        dropout_rate=dropout_rate,\n        activation=keras.activations.selu,\n        normalization_layer=layers.BatchNormalization(),\n        name=\"MLP\",\n    )(features)\n\n    # Add a sigmoid as a binary classifer.\n    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","1ceffa50":"## **<span style=\"color:#e76f51;\">Numerical Feature Distribution<\/span>**","d6055e19":"## **<span style=\"color:#e76f51;\">Tab Transformer<\/span>**\n\n\n[Source](https:\/\/colab.research.google.com\/github\/keras-team\/keras-io\/blob\/master\/examples\/structured_data\/ipynb\/tabtransformer.ipynb)\n\nThe TabTransformer architecture works as follows:\n\n\ud83d\udccc All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n\n\ud83d\udccc A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n\n\ud83d\udccc The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n\n\ud83d\udccc The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n\n\ud83d\udccc A softmax classifer is applied at the end of the model.\n\nThe [paper](https:\/\/arxiv.org\/pdf\/2012.06678.pdf) discusses both addition and concatenation of the column embedding in the Appendix: Experiment and Model Details section. The architecture of TabTransformer is shown below, as presented in the paper.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/keras-team\/keras-io\/master\/examples\/structured_data\/img\/tabtransformer\/tabtransformer.png\" width=\"500\"\/>","91ccd5c3":"# **<span style=\"color:#e76f51;\">W & B Artifacts<\/span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https:\/\/docs.wandb.ai\/guides\/artifacts)\n\n![](https:\/\/drive.google.com\/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","6808e3af":"# **<span style=\"color:#e76f51;\">Preprocessing<\/span>**","d50c0171":"## **<span style=\"color:#e76f51;\">Categorical Feature Distribution<\/span>**","b9d7443c":"### Acknowledgements :\nGoogle supported this work by providing Google Cloud credit\n\n### References :\n\nhttps:\/\/www.kaggle.com\/odins0n\/tps-feb-22-eda-modelling#Feature-Engineering\n\nhttps:\/\/www.kaggle.com\/hamzaghanmi\/welcome-tps-feb-2022\n\nhttps:\/\/colab.research.google.com\/github\/keras-team\/keras-io\/blob\/master\/examples\/structured_data\/ipynb\/tabtransformer.ipynb\n\nhttps:\/\/arxiv.org\/pdf\/2012.06678.pdf\n\n### Work in progress \ud83d\udea7","67ef65b3":"## **<span style=\"color:#e76f51;\">\ud83c\udfafCategorical Feature Transformation<\/span>**\n","c79cf5ff":"\n## **<span style=\"color:#e76f51;\">\ud83c\udfafModel Inputs<\/span>**","9bbcb9ef":"## **<span style=\"color:#e76f51;\">Feature representation using Keras Preprocessing Layers<\/span>**\n\nFeature representations can be one of the crucial aspect in model developement workflows . It is a experimental process and there is no perfect solution . Keras preprocessing Layers helps us create more flexible preprocessing pipeline where new data transformations can be applied while changing the model architecture .\n\n![](https:\/\/drive.google.com\/uc?id=1248y8JYTwjnxZnIEaTQHr1xV5jUZotLm)\n\n[ImageSource](https:\/\/blog.tensorflow.org\/2021\/11\/an-introduction-to-keras-preprocessing.html)\n\n## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Numerical Features<\/span>**\n\nThe Keras preprocessing layers available for numerical features are below \n\n`tf.keras.layers.Normalization`: performs feature-wise normalization of input features.\n  \n`tf.keras.layers.Discretization`: turns continuous numerical features into integer categorical features.\n\n`adapt():`\n\nAdapt is an optional utility function which helps in setting the internal state of layers from input data . adapt() is available on all stateful processing layerrs and it computes mean and variance for the layerrs and stores them as layers weights . adapt() is called before fit() , evaluate or predict()\n\n\n## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Categorical Features<\/span>**\n\nThe various keras preprocessing layers available for categorical variables are below .\n\n`tf.keras.layers.CategoryEncoding:` turns integer categorical features into one-hot, multi-hot, or count dense representations.\n\n`tf.keras.layers.Hashing:` performs categorical feature hashing, also known as the \"hashing trick\".\n\n`tf.keras.layers.StringLookup:` turns string categorical values an encoded representation that can be read by an Embedding layer or Dense layer.\n\n`tf.keras.layers.IntegerLookup:` turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.","ae0a9d76":"Based on this [notebook](https:\/\/www.kaggle.com\/odins0n\/tps-feb-22-eda-modelling) from Sanskar Hasija , I have used only the top 9 features of LGBM feature importance attribute","74911285":"\n\n## **<span style=\"color:#e76f51;\">\ud83c\udfaftf.data<\/span>**\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n\n## **<span style=\"color:#e76f51;\">\ud83c\udfaftf.data.Dataset<\/span>**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label\n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation","6fedee7f":"## **<span style=\"color:#e76f51;\">Target Distribution<\/span>**","25abddd5":"# **<span style=\"color:#e76f51;\">Exploratory Data Analysis<\/span>**\n\n","1bc432a1":"## **<span style=\"color:#e76f51;\">Null Values<\/span>**\n\n\ud83d\udccc There are no null values in the dataset","dd9cfcc0":"![](https:\/\/drive.google.com\/uc?id=16xvFPMC5llfX_AAM4cwzdS13g5hgZCBJ)\n\n[Image Source](https:\/\/en.wikipedia.org\/wiki\/Pan-genome)\n\nFor Tabular Playground Series - Feb 2022 , the problem deals with classifying 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss.  In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment `ATATGGCCTT` becomes `A2T4G2C2`\n\n# **<span style=\"color:#e76f51;\">Goal<\/span>**\n \nThe goal is to predict bacteria species based on repeated lossy measurements of DNA snippets.\n\n# **<span style=\"color:#e76f51;\">Data<\/span>**\n\n**Training Data**\n\nEach row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g.,  to ), which then has a bias spectrum (of totally random ATGC) subtracted from the results.\n\nThe data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging.\n\n> - ```train.csv``` -  the training set, which contains the spectrum of 10-mer histograms for each sample\n> - ```test.csv``` -  the test set; your task is to predict the bacteria species (target) for each row_id\n> - ```sample_submission.csv``` - a sample submission file in the correct format\n\n# **<span style=\"color:#e76f51;\">Metric<\/span>**\n\nClassification accuracy is a metric that summarizes the performance of a classification model as the number of correct predictions divided by the total number of predictions. [Source](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/accuracy)\n\n\n<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [TPS Feb 2022 Project on W&B Dashboard]\n(https:\/\/wandb.ai\/usharengaraju\/TabTransformer)\n> \n> - To get the API key, create an account in the [website](https:\/\/wandb.ai\/site) .\n> - Use secrets to use API Keys more securely ","3fee194d":"## **<span style=\"color:#e76f51;\">Correlation of Features<\/span>**"}}