{"cell_type":{"cef12c57":"code","db7a7f28":"code","0cd8e6c5":"code","57eda2f2":"code","80217395":"code","2f35873a":"code","c8d2c9b9":"code","90e73bfc":"code","e4ff220f":"code","8e6b1baa":"code","87025fc0":"code","d4f6a9ff":"code","cde53ca8":"code","a43d7cc7":"code","e3becc75":"code","f789c3a3":"code","1e4f474c":"code","b9a08722":"code","b20e208b":"code","5a8109b9":"code","ec8e87a8":"code","bac4ed39":"code","27c4f0dc":"code","255e5425":"code","963b1802":"code","f5b20c00":"code","da22d546":"markdown","612f72c2":"markdown","24b6aae2":"markdown","f32cd958":"markdown","4a007af1":"markdown","86326eec":"markdown","ba522731":"markdown","eb9ee431":"markdown","5e55f231":"markdown","3b0f3855":"markdown","679a2738":"markdown","bd7bf743":"markdown","e0504966":"markdown","85d20ff8":"markdown","6abd8885":"markdown","54993f3b":"markdown","4b84ad2e":"markdown","69894c4d":"markdown","4550e5ec":"markdown","53844233":"markdown","5da03668":"markdown","462faea4":"markdown","08abcc65":"markdown","3c7cc388":"markdown","2eae18e7":"markdown","cbed7314":"markdown","a5baf7f5":"markdown","1044cff2":"markdown","820b5431":"markdown","2c5dbffc":"markdown"},"source":{"cef12c57":"import pandas as pd\nimport numpy as np\n\nimport IPython.display\nimport matplotlib.pyplot as plt\n\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import subplots\nimport plotly as py\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv')\nquestions = df.iloc[0,:]\ndf = df.drop(df.index[0])","db7a7f28":" df[\"Q3\"].unique()","0cd8e6c5":"COUNTRY = \"Iran, Islamic Republic of...\"","57eda2f2":"non_profs = (df['Q15'] == \"Under 1 year\") | (df['Q15'] == \"1-2 years\") | (df['Q15'] == \"I do not use machine learning methods\") #professionals\nprofs = ~non_profs #non-professionals\n\nSouthEastAsia = (df['Q3'] == 'Japan')  | (df['Q3'] == 'China') | (df['Q3'] == 'South Korea') | (df['Q3'] == 'Taiwan') \nEurope = (df['Q3'] == 'United Kingdom of Great Britain and Nothern Irland')  | (df['Q3'] == 'Germany') | (df['Q3'] == 'Spain') | (df['Q3'] == 'France') | (df['Q3'] == 'Italy') \nIndia = (df['Q3'] == 'India')\nUSA = (df['Q3'] == 'United States of America')\nalt = (df['Q3'] == COUNTRY) #alternative\nall_resp = df['Q1'] == df['Q1'] #an array of True\nRest =  ~(India | SouthEastAsia | Europe | USA | alt) #Rest of the world\n\ncountries = {'India': India,'SE_Asia':SouthEastAsia, 'USA':USA,'Europe': Europe, COUNTRY: alt, 'All': all_resp }\ncolors = {'India': '#58ABAE','SE_Asia':'#A5CBC6', 'USA':'#FAF4F0','Europe': '#E5E5E5', COUNTRY: '#FFD6D4', 'All': '#FEB8C5', 'Rest of the World': '#AAB8C5'}","80217395":"num_rest = len(df) - len(df[India]) - len(df[SouthEastAsia]) - len(df[USA]) - len(df[Europe]) - len(df[alt])\nnum_respondants = [len(df[India]), len(df[SouthEastAsia]), len(df[USA]), len(df[Europe]), len(df[alt]), num_rest]\nrespondant_groups = np.append(list(countries.keys())[:-1],['Rest of the World'])","2f35873a":"fig = subplots.make_subplots(rows = 1, cols = 3,  shared_yaxes  = True, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}]],\n                             subplot_titles = ['All Participants','All Participants',\n                                               'Professional Participants'])\n\nfig.update_layout(title = go.layout.Title(text =\"<b> Regions and Countries Participation<\/b>\", x = 0.4))\n\ntrace1 = go.Pie(labels = respondant_groups, values = num_respondants, \n                name = '', domain = dict(x = [0, 0.3]))\n\ntrace1.marker = dict(colors = list(colors.values()))\n\ntrace2 = go.Pie(labels = ['profs', 'non_profs'], values = [len(df[profs]), len(df[non_profs])], name = '')\ntrace2.marker = dict(colors = ['#FFB347', '#78A2CC'])\n\nnum_others_profs = len(df[profs]) - len(df[India & profs]) - len(df[SouthEastAsia & profs]) - len(df[USA & profs]) - len(df[Europe & profs]) - len(df[alt & profs])\nnum_respondants_profs = [len(df[India & profs]), len(df[SouthEastAsia & profs]), len(df[USA & profs]), len(df[Europe & profs]), len(df[alt & profs]), num_others_profs]\ntrace3 = go.Pie(labels = respondant_groups, values = num_respondants_profs, name = '') \n\nfig.append_trace(trace1,1,1)\nfig.append_trace(trace2,1,2)\nfig.append_trace(trace3,1,3)\n\nfig.show()\n","c8d2c9b9":"fields = df[\"Q5\"].unique()\nfields = fields[1:]\nfields = np.append(fields,['Other'])","90e73bfc":"title_format = \"<span style='font-size:20px; font-family:Times New Roman'>%s<\/span>\"\nfig = subplots.make_subplots(rows = 1, cols = 1, \n                             subplot_titles = [title_format % \"Age Among Different Regions and Countries \"],\n                             vertical_spacing = 0.07)\n\nfor country in countries.keys():\n    count = df[countries[country]]['Q1'].value_counts()\n    trace = go.Bar(\n                    y = np.round(df[countries[country]]['Q1'].value_counts()\/len(df[countries[country]])*100,1),\n                    x = df[countries[country]]['Q1'].value_counts().index,\n                    name = country,\n                    hovertext = count,\n                    marker = dict(color = colors[country]),\n                    legendgroup = country\n                    )\n\n    fig.append_trace(trace,1,1)\n    fig.update_traces(hovertemplate = '<b>Percent<\/b>: %{y}%<br>'+\n                                      '<b>Count<\/b>: %{hovertext}')\n\n\nfig['layout'].update(height = 400, width = 1000)\n\niplot(fig)","e4ff220f":"title_format = \"<span style='font-size:20px; font-family:Times New Roman'>%s<\/span>\"\nfig = subplots.make_subplots(rows = 1, cols = 1, \n                             subplot_titles = [title_format % \"Gender Among Different Regions and Coutnries\"],\n                             vertical_spacing = 0.07)\n\nfor country in countries.keys():\n    trace2 = go.Bar(\n                    y = np.round(df[countries[country]]['Q2'].value_counts()\/len(df[countries[country]])*100,1),\n                    x = df[countries[country]]['Q2'].value_counts().index,\n                    name = country,\n                    hovertext = df[countries[country]]['Q2'].value_counts(),\n                    marker = dict(color = colors[country]),\n                    legendgroup = country,\n                    showlegend = True\n                    )\n    \n\n    fig.append_trace(trace2,1,1)\n    fig.update_traces(hovertemplate = '<b>Percent<\/b>: %{y}%<br>'+\n                                      '<b>Count<\/b>: %{hovertext}')\n\n\nfig['layout'].update(height = 400, width = 1000)\n\niplot(fig)","8e6b1baa":"title_format = \"<span style='font-size:20px; font-family:Times New Roman'>%s<\/span>\"\nfig = subplots.make_subplots(rows = 1, cols = 1, \n                             subplot_titles = [title_format % \"Education Among Different Regions and Countries\"],\n                             vertical_spacing = 0.07)\n\nfor country in countries.keys():\n    \n    trace3 = go.Bar(\n                    y = np.round(df[countries[country]]['Q4'].value_counts()\/len(df[countries[country]])*100,1),\n                    x = df[countries[country]]['Q4'].value_counts().index,\n                    name = country,\n                    hovertext = df[countries[country]]['Q4'].value_counts(),\n                    marker = dict(color = colors[country]),\n                    legendgroup = country,\n                    showlegend = True\n                    )\n\n    fig.append_trace(trace3,1,1)\n    fig.update_traces(hovertemplate = '<b>Percent<\/b>: %{y}%<br>'+\n                                      '<b>Count<\/b>: %{hovertext}')\n\nfig['layout'].update(height = 600, width = 1000)\n\niplot(fig)","87025fc0":"experience_labels = ['I have never written code', '< 1 years', '1-3 years', '3-5 years','5-10 years', '10-20 years', '20+ years']\n\nexperience_country = pd.DataFrame()\nfield_country = pd.DataFrame()\n\nfor country in countries.keys():\n    experience_country[country] = df[countries[country]][\"Q6\"].value_counts().reindex(experience_labels)\n    experience_country[country] = np.round(experience_country[country]\/(countries[country].sum()) * 100,1)\n\nfor country in countries.keys():\n    field_country[country] = df[countries[country]][\"Q5\"].value_counts().reindex(fields)\n    field_country[country] = np.round(field_country[country]\/(countries[country].sum()) * 100,1)\n\nfig = subplots.make_subplots(rows = 1, cols = 2, horizontal_spacing = 0.3, subplot_titles = ['Years of Programming Experience','Current Role at Work'] )\nfig.update_layout(title = go.layout.Title(text =\"<b> Programming Experience and Current Role for All participats<\/b>\", x = 0.5))\nfig.add_trace(go.Heatmap(x = list(countries.keys()), y = experience_labels, z = experience_country.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,1)\nfig.add_trace(go.Heatmap(x = list(countries.keys()), y = fields, z = field_country.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,2)\nfig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                    '<b>Experience<\/b>: %{y}<br>' +\n                                    '<b>Percent<\/b>: %{z}%')\nfig.update_layout(autosize = False, width = 1000, height = 500)\nfig.update_traces(showscale = False)\nfig.show()","d4f6a9ff":"fig = subplots.make_subplots(rows = 5, cols = 3,  shared_yaxes  = True,\n                             subplot_titles = fields)\nfig.update_layout(title = go.layout.Title(text =\"<b> Programming Experience and Current Role for All participats<\/b>\", x = 0.5))\n\ni = 0\n\nfor field in fields:\n    for country in countries.keys():\n        experience_country[country] = df[(countries[country]) & (df[\"Q5\"] == field)][\"Q6\"].value_counts()\n        experience_country[country] = np.round(experience_country[country]\/(countries[country].sum()) * 100,1)\n    trace = go.Heatmap(x = list(countries.keys()), y = experience_labels, z = experience_country.values, type = 'heatmap', colorscale = 'tealrose', name = '')\n    fig.append_trace(trace,i\/\/3+1, i%3+1)\n    i += 1\nfig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                    '<b>Experience<\/b>: %{y}<br>' +\n                                    '<b>Percent<\/b>: %{z}%')\nfig.update_layout(autosize = False, width = 1000, height = 1500)\nfig.update_traces(showscale = False)\nfig.show()","cde53ca8":"program = pd.DataFrame()\nfor col in df.columns[7:20]:\n    prgrm = questions[col][102:]\n    for country in countries.keys():\n        program.loc[prgrm,country]= (countries[country] & df.loc[:,col].notnull() & non_profs).sum()\n        program.loc[prgrm,country] = np.round(program.loc[prgrm,country]\/((countries[country]& non_profs).sum()) * 100,1)\n\nprogram_profs = pd.DataFrame()\nfor col in df.columns[7:20]:\n    prgrm = questions[col][102:]\n    for country in countries.keys():\n        program_profs.loc[prgrm,country]= (countries[country] & df.loc[:,col].notnull() & profs).sum()\n        program_profs.loc[prgrm,country] = np.round(program_profs.loc[prgrm,country]\/((countries[country]& profs).sum()) * 100,1)\n\ndev_env = pd.DataFrame()\nfor col in df.columns[21:34]:\n    environment = questions[col][142:]\n    for country in countries.keys():\n        dev_env.loc[environment,country]= (countries[country] & df.loc[:,col].notnull() & non_profs).sum()\n        dev_env.loc[environment,country] = np.round(dev_env.loc[environment,country]\/((countries[country] & non_profs).sum()) * 100,1)\n        \ndev_env_profs = pd.DataFrame()\nfor col in df.columns[21:34]:\n    environment = questions[col][142:]\n    for country in countries.keys():\n        dev_env_profs.loc[environment,country]= (countries[country] & df.loc[:,col].notnull() & profs).sum()\n        dev_env_profs.loc[environment,country] = np.round(dev_env_profs.loc[environment,country]\/((countries[country]& profs).sum()) * 100,1)\n        \nhost_not = pd.DataFrame()\nfor col in df.columns[34:51]:\n    Notebook = questions[col][124:]\n    for country in countries.keys():\n        host_not.loc[Notebook,country]= (countries[country] & df.loc[:,col].notnull() & non_profs).sum()\n        host_not.loc[Notebook,country] = np.round(host_not.loc[Notebook,country]\/((countries[country]& non_profs).sum())  * 100,1)\n        \nhost_not_profs = pd.DataFrame()\nfor col in df.columns[34:51]:\n    Notebook = questions[col][124:]\n    for country in countries.keys():\n        host_not_profs.loc[Notebook,country]= (countries[country] & df.loc[:,col].notnull() & profs).sum()\n        host_not_profs.loc[Notebook,country] = np.round(host_not_profs.loc[Notebook,country]\/((countries[country] & profs).sum()) * 100,1)\n        \nhost_not","a43d7cc7":"fig = subplots.make_subplots(rows = 1, cols = 3, horizontal_spacing = 0.04, shared_yaxes  = True, subplot_titles = ['Non-professionals','Professionals', 'Difference'] )\nfig.update_layout(title = go.layout.Title(text =\"<b> Frequently Used Programming Language <\/b>\", x = 0.5))\n\nfig.add_trace(go.Heatmap(x = program.columns, y = program.index, z = program.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,1)\nfig.add_trace(go.Heatmap(x = program_profs.columns, y = program_profs.index, z = program_profs.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,2)\nfig.add_trace(go.Heatmap(x = program_profs.columns, y = program_profs.index, z = program_profs.values - program.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,3)\n\nfig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                    '<b>Experience<\/b>: %{y}<br>' +\n                                    '<b>Percent<\/b>: %{z}%')\n\nfig.update_layout(width = 900, height = 600)\nfig.update_traces(showscale = False)\nfig.show()","e3becc75":"fig = subplots.make_subplots(rows = 1, cols = 3, horizontal_spacing = 0.05, shared_yaxes  = True, subplot_titles = ['Non-professionals','Professionals', 'Difference'] )\nfig.update_layout(title = go.layout.Title(text =\"<b> Frequently Used Development Environment <\/b>\", x = 0.6))\n\nfig.add_trace(go.Heatmap(x = dev_env.columns, y = dev_env.index, z = dev_env.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,1)\nfig.add_trace(go.Heatmap(x = dev_env_profs.columns, y = dev_env_profs.index, z = dev_env_profs.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,2)\nfig.add_trace(go.Heatmap(x = dev_env.columns, y = dev_env.index, z = dev_env_profs.values - dev_env.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,3)\n\nfig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                  '<b>Experience<\/b>: %{y}<br>'+\n                                  '<b>Percent<\/b>: %{z}%')\n\nfig.update_layout(width = 900, height = 600)\nfig.update_traces(showscale = False)\nfig.show()","f789c3a3":"fig = subplots.make_subplots(rows = 1, cols = 3, horizontal_spacing = 0.05, shared_yaxes  = True, subplot_titles = ['Non-professionals','Professionals', 'Difference'] )\nfig.update_layout(title = go.layout.Title(text =\"<b> Frequently Used Hoste Notebook <\/b>\", x = 0.6))\n\nfig.add_trace(go.Heatmap(x = host_not.columns, y = host_not.index, z = host_not.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,1)\nfig.add_trace(go.Heatmap(x = program.columns, y = host_not_profs.index, z = host_not_profs.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,2)\nfig.add_trace(go.Heatmap(x = dev_env.columns, y = host_not.index, z = host_not_profs.values - host_not.values, type = 'heatmap', colorscale = 'tealrose', name = ''),1,3)\n\nfig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                  '<b>Experience<\/b>: %{y}<br>'+\n                                  '<b>Percent<\/b>: %{z}%')\n\nfig.update_layout(width = 900, height = 600)\nfig.update_traces(showscale = False)\nfig.show()","1e4f474c":"computer_vision = ~all_resp #an array of False\nNLP = ~all_resp\n\nfor col in range(102, 107):\n    computer_vision = computer_vision | (~pd.isna(df.iloc[:,col]))\n    \nfor col in range(109, 113):\n    NLP = NLP| (~pd.isna(df.iloc[:,col]))\n    \nmodes = {'Computer Vision': '#58ABAE','NLP':'#FAF4F0', 'Both':'#FEB8C5'}","b9a08722":"modes = {'Computer Vision': '#58ABAE','NLP':'#FAF4F0', 'Both':'#FEB8C5'}\ntitle_format = \"<span style='font-size:20px; font-family:Times New Roman'>%s<\/span>\"\nfig = subplots.make_subplots(rows = 1, cols = 1, \n                             subplot_titles = [title_format % \"Computer Vision or NLP\"],\n                             vertical_spacing = 0.07)\nfor mode in modes.keys():\n    y_value = []\n    for country in countries.keys():\n        if mode == 'Computer Vision':\n            y_value.append((computer_vision & countries[country]).sum() \/len(df[countries[country]])*100)\n        elif mode == 'NLP':                                                         \n            y_value.append((NLP & countries[country]).sum()\/len(df[countries[country]])*100)\n        else:\n            y_value.append( (computer_vision & NLP & countries[country]).sum()\/len(df[countries[country]])*100)\n    y_value = np.round(y_value, 1)\n    trace = go.Bar(\n                    y = y_value,\n                    x = list(countries.keys()),\n                    name = mode,\n                    marker = dict(color = modes[mode])\n                    )\n    fig.update_traces(hovertemplate = '<b>Region<\/b>: %{x}<br>'+\n                                      '<b>Percent<\/b>: %{y}%<br>')\n    fig.append_trace(trace,1,1)\n\nfig['layout'].update(height = 400, width = 900)\niplot(fig)","b20e208b":"team_size = pd.DataFrame()\nfor country in countries.keys():\n    team_size[country] = df[countries[country]]['Q22'].value_counts()#\/len(df[countries[country]])*100\nteam_size = team_size.reindex(['0', '1-2', '3-4', '5-9','10-14', '15-19','20+'])\n#team_size.loc['nan',:] = len(df) - team_size.sum(axis = 0)\nteam_size","5a8109b9":"fig = subplots.make_subplots(\n    rows = 1, \n    cols = 1, \n    shared_yaxes = True, \n    shared_xaxes = False,\n    subplot_titles = [title_format % \"Data Science Team Size for Different Countries and Regions\"],\n    horizontal_spacing = 0.02, \n    vertical_spacing = 0.01\n)\n\nfor country in list(countries.keys())[:-1]:\n    team_size_prct_country = np.round(team_size[country]\/len(df[countries[country]])*100,1)\n    txt = list(zip(team_size[country], team_size_prct_country))\n    txt2 = team_size_prct_country\n    trace = go.Bar(\n        y = team_size.index,\n        x = team_size[country],\n        name = country,\n        marker = dict(color= colors[country]),\n        orientation = \"h\",\n        hovertext = txt2\n        #hoverinfo = 'text',\n    )\n    fig.update_traces(hovertemplate = '<b>Count<\/b>: %{x}<br>'+\n                                      '<b>Percent<\/b>: %{hovertext}%<br>')\n    fig.append_trace(trace,1,1);\n\n\nlayout = dict(barmode = 'stack')\nfig.update_layout(layout)\nfig['layout'].update(height = 400, width = 1000)\niplot(fig)","ec8e87a8":"y_values = []\nfor col in range(119, 127):\n    temp = df[~pd.isna(df.iloc[:,col])].iloc[:,col]\n    y_values.append(temp.unique()[0])\nbreak_line = (lambda x: (x[:49]+'<br>'+x[49:100]+'<br>'+x[100:]) if (len(x) > 50) else (x[:50]+'<br>'+x[50:]))\ny_values = [break_line(item) for item in y_values]","bac4ed39":"fig = subplots.make_subplots(\n    rows = 1, \n    cols = 1, \n    subplot_titles = [title_format % \"Major Working Activity Among Professionals\"],\n    horizontal_spacing = 0.02, \n    vertical_spacing = 0.01\n)\nfor country in countries.keys():\n    x_values = []\n    counts = []\n    for col in range(119, 127):\n        count = df[countries[country] & profs].iloc[:,col].notnull().sum()\n        x = count\/len(df[countries[country] & profs])*100\n        x = np.round(x,1)\n        x_values.append(x)\n        counts.append(count)\n        \n    trace = go.Bar(\n                    y = y_values,\n                    x = x_values,\n                    name = country,\n                    marker = dict(color= colors[country]),#\"#a2885e\"\n                    orientation = \"h\",\n                    hovertext = counts\n                    )\n    fig.update_traces(hovertemplate = '<b>Count<\/b>: %{hovertext}<br>'+\n                                      '<b>Percent<\/b>: %{x}%<br>')\n    fig.append_trace(trace,1,1);\n\nfig['layout'].update(height = 1000, width = 900)\niplot(fig)","27c4f0dc":"ML_Usage_Company_size = pd.crosstab(df.Q21, df.Q23, margins = True, margins_name = \"Total\")\nML_Usage_Company_size = ML_Usage_Company_size.reindex(index = ['0-49 employees','50-249 employees', '250-999 employees', '1000-9,999 employees', '10,000 or more employees', 'Total'])\nML_Usage_Company_size","255e5425":"fig = go.Figure()\nfig.update_layout(title = go.layout.Title(\n                                    text = '<b>The Extent of Machine Learning Usage by Current Employer<\/b>', xref = \"paper\", x = 0.5))\ntxt = []\nfor country in countries.keys():\n    \n    ML_Usage_Company_size = pd.crosstab(df[countries[country]].Q21, df[countries[country]].Q23, margins = False, margins_name = \"Total\")\n    ML_Usage_Company_size = ML_Usage_Company_size.reindex(index = ['0-49 employees','50-249 employees', '250-999 employees', '1000-9,999 employees', '10,000 or more employees'])\n    marker_size_abs = (ML_Usage_Company_size.values).flatten() #absolut values\n    marker_size_prc = np.round((ML_Usage_Company_size.values).flatten()\/len(df[countries[country]])*100,1) #percentage values for each country\n    for count, perc in zip(marker_size_abs,marker_size_prc):\n        txt.append('<b>Count:<\/b>{}<br> <b>Percent:<\/b> {}%'.format(count,perc))\n    \n    x,y = np.meshgrid(ML_Usage_Company_size.columns, ML_Usage_Company_size.index)\n    trace = go.Scatter(\n                        x = x.flatten(), \n                        y = y.flatten(),\n                        marker_size = (ML_Usage_Company_size.values).flatten()\/len(df[countries[country]])*50000,\n                        text = txt,\n                        hoverinfo = 'text',\n                        name = country,\n                        )\n    txt = []\n    fig.add_trace(trace)\n    # Tune marker appearance and layout\n    fig.update_traces(mode = 'markers', marker = dict(sizemode='area', line_width = 2))\n\nfig['layout'].update(height = 800, width = 1000)\nfig.show()\n","963b1802":"cloud = dict()\nstorage = dict()\nfor col in range(147,155):\n    storage[questions.iloc[col][118:]] = ~pd.isna(df.iloc[:,col])\n\nfor col in range(129,141):\n    cloud[questions.iloc[col][124:]] = ~pd.isna(df.iloc[:,col])\n    \n\nCloud_Storage = pd.DataFrame()\nfor cloud_name, cloud_cond in list(cloud.items())[:6]:\n    for storage_name, storage_cond in storage.items():\n        Cloud_Storage.loc[cloud_name, storage_name] = len(df[cloud_cond & storage_cond])\n        \nCloud_Storage    ","f5b20c00":"fig = go.Figure()\nfig.update_layout(title = go.layout.Title(text = '<b>Regularly Used Cloud Computing Platforms and Storage Products<\/b>', xref = \"paper\", x = 0.5))\nCloud_Storage = pd.DataFrame()\ntxt = []\nfor country in countries.keys():\n    \n    for cloud_name, cloud_cond in list(cloud.items())[:4]:\n        for storage_name, storage_cond in storage.items():\n            Cloud_Storage.loc[cloud_name, storage_name] = len(df[cloud_cond & storage_cond & countries[country] ])\n    \n    x,y = np.meshgrid(Cloud_Storage.columns, Cloud_Storage.index)\n    marker_size_abs = (Cloud_Storage.values).flatten()\n    marker_size_prc = np.round((Cloud_Storage.values).flatten()\/len(df[countries[country]])*100,1)\n    for count, perc in zip(marker_size_abs,marker_size_prc):\n        txt.append('<b>Count: <\/b>{}<br> <b>Percent: <\/b>{}%'.format(count,perc))\n    \n    trace = go.Scatter(\n                        x = x.flatten(), \n                        y = y.flatten(),\n                        marker_size = (Cloud_Storage.values).flatten()\/len(df[countries[country]])*50000,\n                        text = txt,\n                        hoverinfo = 'text',\n                        name = country,\n                        )\n    txt = []\n    fig.add_trace(trace)\n    # Tune marker appearance and layout\n    fig.update_traces(mode = 'markers', marker = dict(sizemode='area', line_width = 2))\n\nfig['layout'].update(height = 600, width = 1000)\nfig.show()","da22d546":"Hope you find this analysis interesting! For me it's been very exciting to get to konw the community I learn the most from and to which I try to contribute!\nContinously trying to improve, I truly appreaciate your feedbacks about my analysis. :-)","612f72c2":"## 12. Machine Learning Usage at Work and Size of Company","24b6aae2":"Jupyter notebook is the absolut winner among development environments with 58-71% of professionals and non-professional from different regions using it <br> \nVisual Studio Code is the second most popluar with 32-43%. It is specially popular among SE Asian and European professionals. <br> \nPyCharm is the third most popular environment with the usage rate of 22-42% among different groups. It is appricated most among Iranian professionals. <br>\nRStudio is relatively used more among professionals from USA and Europe. <br>\nUS professionals use Vim\/Emacs more than non-professionals. <br>\nMATLAB is more popular among Iranian professionals than non-professionals. <br>\nProfessionals in Inida use Spyder more than non-professionals. however, the trend is reversed for Jupyter notebook. <br>\nJust like python as programming language, Jupyter Notebook is more used by non-professionals than professionals in Iran, as in India. <br>\n","f32cd958":"Below is the list of names of the countries in the kaggle survey. You can choose any country from this list to run this comparetive analysis. I should admit that I'd be happier, if I saw the name of Iran it as simple as Iran, rather than Islamic Republic of... ;)","4a007af1":"**How to read the plot** <br>\nThis plot shows the comparision of machine learning usage in companies with different sizes. The size of the bubble corresponds to the relative percentage of the respondents in that group. For example in case of the India, the bubble on the left most bottom part of the plot shows that 230 or(3.1%) participants from India work for 0-49 employee company and do not know if ML is used by their employer<br>\nTo get the most out of this plot, it's better to select two counties\/regions that you would like to compare and deselct the other ones. I summerize some of the patterns I find interesting in this comparison. <br> <br>\n\n**Insights** <br>\nMost of the respondents work for companies with less than 50 people. The respondents working for big corporations with more than 10,000 employees are the second group. <br>\nMost of the respondents in 0-49 employee sized companies either do not use ML methods or only use it for exploring without having models in productions. Big corporation employees however report that they have well established ML methods(i.e. models in production for more than 2 years). <br>\nIn US, the respondents relatively work more for big corporations with 10k+ employees compared to rest of the world. The plot indicates that more companies in US regardless of the number of their employees have well established ML models compared to the rest of the world. <br> \nIn SE Asia, respondents relatively work more for companies with 1k-10k employees. More relative respondents also state that they use ML methods for generating insights, but do not put working models in production. <br>\nIn Europe also, companies relatively have more well established ML methods with models in production compared to the rest of the world. <br>\nIn India, the respondents have less deviation from the rest of the wrold. They slightly tend to work more in bigger corporations and less in smaller componies with below 50 epmplyees. Lower percentage of the poeple from Inida has replied to this part of survey compared to rest of the world. <br>\nMost of the respondents from Iran work for companies with 0-49 employees. <br>\n\n","86326eec":"## Introduction\nThis report is an attempt to show and compare the emerging trends of machine learning and data science in different regions in the world including Indida, United States of America, Europe, South East Asia and any country of your interest. In this report I picked Iran as the country of interest, since I grew up there and I'm curious about it. You can change it to any other arbitrary country and see how it deviates from rest of the world or from a specific other region. <br>\n- In this report I will initially show the differences and similarities of kaggle respondents in age, gender, education, years of experience and their current role at work. <br>\n- Then we'll see how their choices for programming language, development environments, hosted notebooks and computer vision and NLP techiques differ. <br>\n- We would also asses how the team and company size differs in different regions and countries. Also how different companies approach machine learning and use cloud computing platforms together with data storage products. <br>\n<br>\nHope you find this analysis interesting. And if you have any comments or feedbacks, I would truely appreciate it! :)","ba522731":"The numbers on each bar shows the percentage of the respondents in a specific region that has has picked that activity as important part of their work at work. <br>\nMost of the respondents form all regions and coutries state that analyzing and understanding data to influence product or business decisoins is an important part of their role. <br>\nEurope and US follow similar patterns for their major activity at work. SE Asia and India have more similar patterns. <br>\nHigher percentage of respondents from US and Europe engage in machine learning related activities than SE Asia and India. For example, 58% and 51% of participants from US and EU as compared to 36 and 34% from SE Asia and India report that analyzing and understanding data to influence product or business decisions is an important activity at work for them. <br>\nBuilding prototyes an applying machine learning to new areas is the second most important activity in all countries and regions except for Iran. <br>\nFor Iranian respondants, the activies are more research oriented rather than practical in industries. <br>","eb9ee431":"As you hover the mouse on the bars, the first number shows the total count of respondents in a specific country or region with the specific team-size. The second number shows the percentage of those repsondents for that country or region. <br>\nMost of respondents work in data science teams with more than 20 people. This is specially true for respondents from US with highest relative percentage. <br>\nFor teams with 0-20 members, as the size of teams increases, the number of people working in those teams decreases. This pattern is followed in all countries and regions in this report. <br>\nIt appears that the data science teams are either more than 20 people or 1-2 people. Team size of 1-2 people is more common among European respondents. <br>","5e55f231":"## 9. Computer Vision and NLP","3b0f3855":"Most of the respondents from India are 18-21, from Iran 22-29, from SE Asia and Europe are 25-29 and from US 25-34 years old. <br>\n36% of respondants from India are 18-21 years old. This group is the largest among other age groups from different regions. <br>\nMost of the respondants in age groups of 40-44, 45-49, 50-54, 55-59, 60-69 and 70+ are from US. <br>","679a2738":"## 10. Data Science Team Size","bd7bf743":"## 4. Educaiton","e0504966":"## 8. Hosted Notebooks","85d20ff8":"Due to high numbers of respondents in India and USA, I keep the survey analysis on country level for these two countries. However, to make the comparision relative I combine the survey results of the top 5 European countries in numbers of participants together. This includes UK and Irland, Germany, Spain, France and Italy. I know we have gone through painful process of Brexit, but let's keep it in Europe for this report :D For the South East Asia, also I combined the results from Japan, China, South Korea and Taiwan.   ","6abd8885":"79% and 19% of total respondants are male and female correspondingly. <br> \nThe rate of female respondents are the highest among Iranians with 26%. (woohoo!) <br>\nFemale participation is the lowest among SE Asians and Europeans with 13% and 14%. <br>","54993f3b":"## 11. Important Working Activity","4b84ad2e":"The first plot shows the frequently used programming languages for non professionals. Non-professionals here refers to people with less than two years of experience using machine learning techniques. The second plot shows the same for professional population and the third plot highlights the differences between the two group. <br>\nAs you can see, python is quite popular among both groups of professionals and non professionals in all countries and regions with 77-95% of respondants reporting frequently using it in all groups. Python is least used among Inidan professionals and American non-professionals. <br>\nSQL is the second most frequently used programming language with 24-49% of usage for different groups. It is mostly used among US professionals and least among SE_Asia non-professionasls. <br>\nR popularity ranges between 12-36% and this rate is between 13-30% for C++. <br>\nR is most popular among US professionals and C++ is most popular among SE Asians and Iranians. <br>\nThe diff plot shows that R is better embraced among professionals for all regions. <br>\nPython is more popular among non-professionals compared to professionals in India, Iran and all countries combined. <br>\nBash is appreciated by professionals better, specially for professionals in US and Europe. <br>\nMATLAB is relatively popular among Iranian professionals.","69894c4d":"This plot shows how many percent of participants for a sepcific country or region regurlarly work with computer vision, NLP or both techniques. <br>\nAbout 23% of the total respondents on kaggle survey work with computer vision, 14% with NLP and 10% with both of these techniques. <br>\nBoth computer vision and NLP is used more relatively among SE Asian and Europeans compared to US, India and overall community. <br>\nAlso it seems that more than half of the people that work with NLP, work with computer vision as well. This is interesting, since some of the teqniques such as transformers that were initially developed for NLP are used in computer vision nowadays. <br>","4550e5ec":"## 5. Work and Programming Experience","53844233":"## 13. Cloud Computing Platforms and Storage Products","5da03668":"Most of the resondents have either a Bachlor's degree or a Master's degree. They comprise 38% and 39% of total repondents correspondingly. <br> \nMost of the poeple from Inida have a Bachlor's dgree, while most of the poeple from SE Asia, Europe, US and Iran have a Master degree. This is probably due to age difference of participants from India compared to other regions. <br>\n\nRelatively more respondants from Europe have a PhD compared to other groups. 20% of participants from Europe have a PhD. This number is 4% for respondents from Inida and is 10% for total respondents. <br> \nEducation is not limited to university degrees. Among the respondents with some university studies and without earning a degree, SE Asia holds the relative majority. 10.5% of respondents from SE Asia belong to this group. <br>","462faea4":"**How to read the plot** <br>\nThis plot shows the frequently usded cloud computing platforms and storage products and how the usage differ for different countries and regiolns. For simplicity of plots, I only kept the most popular products and removed the other ones from the matrix. This means that Oracle, SAP, Salesforce, VMware, Alibaba and Tencent Cloud were excluded. <br>\nFor getting to know how to read the plot, please read the notes for part 12. <br><br>\n\n**Insights** <br>\nMost of the participants from the overall places use Amazon Web Services(AWS) together with Amazon Simple Storage Services(S3) (8.4%). <br>\nThe second most populer products are Google Cloud Platform(GCP) together with Google Cloud Storage (GCS) (6.5%). <br>\nThe combination of (AWS + GCS) and (GCP + S3) are more popular than Microsoft products (3.6% vs 3% respectively). IBM Cloud is the least popular among other clouds. (~2%) <br>\nUS participants use cloud services and storage products above average of participants. They specially use (AWS + S3) combo more than other cominiation products compared to other countries and regions. <br>\nInidan participants use cloud services and storage products a bit below average of participants. However, their choices of products follows the common pattern for total participants from all places. <br> \nSE Asian repondents use (GCP) more than average. They use AWS and Microsoft Azure less than average. The combination of (GCS and GCP) is more popular than other ones in SE Asia.<br>\nIn Europe, Microsoft Azure is used more than othe places. However the most popular combination of product is still (AWS + S3) with 11.8% of respondents using that. <br>\nRespondents from Iran use Google products significantly more than Amazon or Microsoft products. <br>\n\n## Conclusion\n\nIn this notebook we learned about how kagglers from different regions and countreis differ from each other from different aspects. These aspects include their demographics (age and sex), eduction, programming experience, job roles and their machine learning experience. <br>\nAfter recognizing these groups of people, we tried to examine different practices among these groups. This analysis is useful for people to get a perspective about what programming languages, development environments and hosted notebooks are popular globaly, in a specific region or in a specific country of their interest. A comparision for the popularity of NLP versus computer vision is presented as well. <br>\nWe later tried to understand the current ecosystems at data science jobs in industries. This analysis gives insights about the data science teams size, company size and their approach to machine learning techniques as well as mostly used cloud computing platforms and data storage products. <br>\n","08abcc65":"Colab Notebooks and kaggle notebooks are equally popular among all respondants. The usage rate is 23-50% for different groups. <br>\nBoth professionals and non-professionals in US use these notebooks less than other groups. <br>\nIn all regions except India professionals use Colab notebooks more than non-professionals. <br>\nIn all regions except SE Asia non-professionals use kaggle notebooks more than professionals. <br>\nColab notebooks are specially popular among Inidans and Iranians both professionals and non-professinals compared to other groups. <br>\nGoogle Cloud notebooks are the 3rd most popular after colab notebooks and kaggle notebooks with a low rate of usage of 7% for all respondants. <br>\nOther notebooks doesn't show significat results. <br>","3c7cc388":"## 2.Age","2eae18e7":"## 3. Gender","cbed7314":"## 1. Respondents Distributions\nHere you can see the distribution of respondents in different regions. <br> 28.6% of respondants are from India, 10.2% from USA, 9.35% from South East Asia, 6.3% from Europe, 0.75% from Iran and 42.6% from rest of the world. This is a quite low percentage of total participants for Iran, with 195 respondents in total. Though, I'd like to keep it as is. <br>\nFor some parts of the analysis I prefered to look at the responses from somehow more experienced people. The selection criteria for this group has been people with more than 2 years of experience of using machine learning techniques. Here you can see that only 31.7% of total respondants fall into this category. <br> \nThe pie chart on the right shows the distribution of this group of respondants(professionals) for different regions. You can see that the relative count of responses for USA and Europe increases to 14.1% and 8.91%. For India it decreases to 21.6% and it remains almost the same for the rest of regions. ","a5baf7f5":"Now let's have a closer look at how the programming experience changes for different working roles.<br>\nIn the plots below the data is normalized for each country. In other words, each number in each square shows the percentage of respondents among the whole respondents in a specific region. For example, 17.56% of Indian repspondents are students with 1-3 years of programming experience, Or 3.17% of respondents from SE Asia are data analysts with 1-3 years of programming experience.  <br>\nThe colors in the plots are adjusted per plot. Dark red in one plot corresponds to a different percentage number than dark red in another plot. It only shows the maximum number for a specific plot. <br>\nThere are relatively more software engineers, research scientists, data engineers and database engineers with +10 years of programming experience from Europe and US. In SE_Asia there are more data analyst, business analyst, data engineer and product manager with 1-3 years of programming experience. <br>\nMost of the Indian respondents have 1-3 years of programming experience and work as software engineers, data scientists, data analysts and business analysts. The biggest sub-group among the respondants are Inidan students with 1-3 years or less than a year of programming experience. <br>\nIranian data is sparse. This is probably due to low number of participants (195 people). They are more data scientists and machine learning engineers with 3-5 years of programming experience or software engineers or between jobs (currently not employed) with 1-3 years of programming experience. <br>\nMost of the respondents from all places are also students with 1-3 years of programming experience, as well as data scientists and data analysts with 1-3 or 3-5 years of programming experience.","1044cff2":"## 6. Programming Language","820b5431":"## 7. Development Environment","2c5dbffc":"For all regions most of the respondents have 1-3 years of programming experience. So it seems that kaggle is popular among people who are new in the field, know the basics and foundemntals and have practiced programming for at least a year. <br>\nThere is a very low percentage of respondents with absolute no experience in coding for all regions. <br>\nFor USA and Europe the respondents are quite evenly distributed between different years of experience, however, in India, South East Asia and Iran most of the respondents have less than 5 years of experience. This is in accordance with the general trend among all respondents. <br>\nMost of the respondents from all regions are students. Data scientists are the second biggest group and software engineers come as third. This is the trend more or less for all the regions. There are relatively more data scientists from US and Europe compared to more students from Inida and South East Asia. Surprisingly, there are more data analysts, research scientists or even people with no work than machine learning engineers for all regions in this survey! "}}