{"cell_type":{"bd0cdb6c":"code","768d6cd6":"code","aa0bca5f":"code","e01ac6d9":"code","55e867b8":"code","aa61fe66":"code","f6cc689f":"code","accce5f5":"code","b4f15800":"code","b5ce6103":"code","2c84de0a":"code","f1a03f28":"code","d688b0aa":"code","7aebf05d":"code","73ee5c56":"code","4ce98af9":"code","9159d91b":"code","b5449eee":"code","cee744c9":"code","e5887b15":"code","c4d6a40a":"code","f84ef9d4":"code","8cd64a3e":"code","97ee21a0":"code","7d3de679":"code","fd330d81":"code","314ff137":"code","089f0e21":"code","218dfaf9":"code","30f6843c":"code","b06db528":"code","2a81d9ec":"code","7d9045e9":"code","6b4f6095":"code","4d1a0d07":"code","e7914c70":"code","d531e961":"code","3c94d59f":"code","711435c4":"code","dda0ada4":"code","f3507c9e":"code","4e520291":"code","aaecaafc":"code","501e2fd7":"code","8aa1d0c7":"code","a718cbca":"code","bcd64aa1":"code","40344e72":"code","874c8f2f":"code","6b4c7035":"code","2049701f":"code","def81d6e":"code","72d5d088":"code","24468920":"code","9c2bd328":"markdown","8a26d869":"markdown","27b2e87a":"markdown","b952d5d8":"markdown","978662ed":"markdown","5b9250f9":"markdown","dbce749b":"markdown","8de40af6":"markdown","ea6a4cbb":"markdown","7191c272":"markdown","3d107f53":"markdown","316cf083":"markdown","9f22125e":"markdown","1fb1846b":"markdown","159d2bed":"markdown","64d7ab3a":"markdown","97031559":"markdown","b9e05ad5":"markdown","d4b40aaa":"markdown","0335fa41":"markdown","c1c9a668":"markdown"},"source":{"bd0cdb6c":"import pandas as pd\nfrom matplotlib import pyplot as plt\npd.set_option('display.max_columns', 500)\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\nimport xgboost as xgb\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nimport time\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nimport shap","768d6cd6":"data = pd.read_csv('..\/input\/appliances-energy-prediction\/KAG_energydata_complete.csv')","aa0bca5f":"data.head()","e01ac6d9":"data.info()","55e867b8":"data.shape","aa61fe66":"data.isnull().sum()","f6cc689f":"data.describe()","accce5f5":"data.plot(x='date', y='Appliances', figsize=(15,7), rot=30);","b4f15800":"data['date'] = pd.to_datetime(data['date'])","b5ce6103":"data['month'] = data.date.dt.month\ndata['weekday'] = data.date.dt.weekday\ndata['hour'] = data.date.dt.hour\ndata['week'] = data.date.dt.week","2c84de0a":"data.drop(columns=['date'], inplace=True)","f1a03f28":"# Seggregate the columns based on its category.\n\ntemp_cols = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\"]\nhumidity_cols = [\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_5\",\"RH_6\",\"RH_7\",\"RH_8\",\"RH_9\"]\nweather_cols = [\"T_out\", \"Tdewpoint\",\"RH_out\",\"Press_mm_hg\",\"Windspeed\",\"Visibility\"] \nlight_cols = [\"lights\"]\nrandom_cols = [\"rv1\", \"rv2\"]\ndate_time_cols = ['month', 'weekday', 'hour', 'week']\ntarget = [\"Appliances\"]","d688b0aa":"data.describe()","7aebf05d":"#sns.pairplot(data[temp_cols+target]);","73ee5c56":"#sns.pairplot(data[humidity_cols+target]);","4ce98af9":"#sns.pairplot(data[weather_cols+target]);","9159d91b":"data[temp_cols+ humidity_cols+ weather_cols+ light_cols+ random_cols+ target].hist(bins=50, figsize=(15,20));","b5449eee":"for col in data[temp_cols+ humidity_cols+ weather_cols+ light_cols+ random_cols+ target].columns:\n    stats.probplot(data[col], dist='norm', plot=plt, fit=True)\n    plt.title(col)\n    plt.show()","cee744c9":"target_var_original = data[['Appliances']].copy()\n# normality check\ndef normality(data,feature):\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    sns.kdeplot(data[feature])\n    plt.subplot(1,2,2)\n    stats.probplot(data[feature],plot=plt)\n    plt.show()","e5887b15":"normality(target_var_original,'Appliances')","c4d6a40a":"target_var_original['log_transform']=np.log(target_var_original['Appliances'])\nnormality(target_var_original,'log_transform')","f84ef9d4":"target_var_original['reciprocal_transform']=1\/target_var_original.Appliances\nnormality(target_var_original,'reciprocal_transform')","8cd64a3e":"target_var_original['sqroot_transform']= np.sqrt(target_var_original.Appliances)\nnormality(target_var_original,'sqroot_transform')","97ee21a0":"target_var_original['boxcox_transform'], parameters= stats.boxcox(target_var_original['Appliances'])\nnormality(target_var_original,'boxcox_transform')","7d3de679":"data_log_transformed = data.copy()\ndata_log_transformed['Appliances'] = np.log(data_log_transformed['Appliances'])","fd330d81":"# Heatmap without log transformation on target variable\ncorr_matrix = data.corr()\nfig = plt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, annot=True, square=True);","314ff137":"# Heatmap with log transformation on target variable\n\ncorr_matrix_transformed = data_log_transformed.corr()\nfig = plt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix_transformed, annot=True, square=True);","089f0e21":"sns.relplot(y='Appliances', x='hour', data=data_log_transformed, kind='line', hue='weekday', height=7, aspect=1.7)\nplt.show()","218dfaf9":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# Function to get top correlations \n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(data_log_transformed, 40))","30f6843c":"corr_matrix_transformed = data_log_transformed.corr()\ncorr_matrix_transformed","b06db528":"corr_matrix_transformed = corr_matrix_transformed[['Appliances']].copy()","2a81d9ec":"corr_matrix_transformed.drop(index='Appliances', inplace=True)","7d9045e9":"corr_matrix_transformed.sort_values('Appliances', inplace=True)","6b4f6095":"# remove those features who have less than 10% (0.1) relationship with the targe variable\n\nfeatures_to_keep = corr_matrix_transformed.loc[~corr_matrix_transformed['Appliances'].between(-0.099, 0.0999)]","4d1a0d07":"features_to_keep = features_to_keep.index.tolist()","e7914c70":"features_to_keep","d531e961":"data_log_transformed = data_log_transformed[features_to_keep+['Appliances']].copy()","3c94d59f":"data_log_transformed.shape","711435c4":"X_data = data_log_transformed.drop(columns=['Appliances']).copy()\ny_data = data_log_transformed[['Appliances']].copy()","dda0ada4":"X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 0)","f3507c9e":"# Scale the data\nscaler = StandardScaler() \nX_train_sc=scaler.fit_transform(X_train)\nX_test_sc=scaler.transform(X_test)\ny_train_sc=scaler.fit_transform(y_train)\ny_test_sc=scaler.transform(y_test)","4e520291":"# Applying PCA\n#pca = PCA(n_components = 0.99)\n#X_train_sc = pca.fit_transform(X_train_sc)\n#X_test_sc = pca.transform(X_test_sc)","aaecaafc":"# Note: I used PCA also but it was not giving better results, so I skipped it.","501e2fd7":"models = [\n           ['Lasso: ', Lasso()],\n           ['Ridge: ', Ridge()],\n           ['ElasticNet: ', ElasticNet(random_state=0)],\n           ['DecisionTreeRegresson: ', DecisionTreeRegressor()],\n           ['KNeighborsRegressor: ',  neighbors.KNeighborsRegressor()],\n           ['SVR:' , SVR(kernel='rbf')],\n           ['RandomForest ',RandomForestRegressor()],\n           ['ExtraTreeRegressor :',ExtraTreesRegressor()],\n           ['GradientBoostingClassifier: ', GradientBoostingRegressor()] ,\n           ['XGBRegressor: ', xgb.XGBRegressor()],\n           ['AdaBoostRegressor: ',AdaBoostRegressor()]\n         ]","8aa1d0c7":"model_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model.random_state = 78\n    curr_model_data[\"Name\"] = name\n    start = time.time()\n    curr_model.fit(X_train_sc,y_train_sc)\n    end = time.time()\n    curr_model_data[\"Train_Time\"] = end - start\n    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(y_train_sc,curr_model.predict(X_train_sc))\n    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(y_test_sc,curr_model.predict(X_test_sc))\n    curr_model_data[\"Test_RMSE_Score\"] = sqrt(mean_squared_error(y_test_sc,curr_model.predict(X_test_sc)))\n    model_data.append(curr_model_data)","a718cbca":"model_data","bcd64aa1":"result_df = pd.DataFrame(model_data)","40344e72":"result_df.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;","874c8f2f":"param_grid = [{\n              'max_depth': [80, 150, 200,250],\n              'n_estimators' : [100,150,200,250],\n              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n            }]\nreg = ExtraTreesRegressor(random_state=40)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\ngrid_search.fit(X_train_sc, y_train_sc)","6b4c7035":"# Tuned parameter set\ngrid_search.best_params_","2049701f":"# Best possible parameters for ExtraTreesRegressor\ngrid_search.best_estimator_","def81d6e":"# R2 score on training set with tuned parameters\n\ngrid_search.best_estimator_.score(X_train_sc,y_train_sc)","72d5d088":"# R2 score on test set with tuned parameters\ngrid_search.best_estimator_.score(X_test_sc,y_test_sc)","24468920":"# RMSE score on test set with tuned parameters\n\nnp.sqrt(mean_squared_error(y_test_sc, grid_search.best_estimator_.predict(X_test_sc)))","9c2bd328":"# Processing the data before feeding it to ML models.","8a26d869":"### Splitting into train and validation","27b2e87a":"So, out of 34 features, we have removed the redundent ones and using only useful 15 features.","b952d5d8":"#### Observation:\nWe observe that none of the transformations are making our target variable perfectly normal, but still log trasnformation is giving better results than others. So we will be applying log transformation on the target variable.","978662ed":"### based on all the above EDA, we can remove below features.\n\nrv1, rv2, rh1, rh2, rh3, ","5b9250f9":"### Improvemente points:\n\nDefinitely, we have a scope of improvement here, specially in the feature engineering. Many of the features have outliers and are skewed. We can do some more EDA and FE to extract better features which can have better relationships with the target variable. Below are some more transformation which we can do to data.\n\n* To handle skeweness data:\n    * We can apply some transformation like log, exp, boxcox, reciprocal on skewed features.\n    * We can do discretisation on on some highly skewed data and check how it is performing.\n* To handle outliers:\n    * Trimg the outliers. (chances of data loss)\n    * Treat outliers as missing data and do imputation on them\n    * Discretization\n* Model interpretation:\n    * We can also use shap value to interprete the predictions.\n    ","dbce749b":"# Hyper parameter tuning","8de40af6":"We see that everyday, the usage is maximum around 15 hours. But on 5th and 6th day, it is even higher during the same hours.","ea6a4cbb":"# EDA","7191c272":"### Observation\n\nThe data contains date and time as well. This may or may not be a case of time-series forecasting problem. We need to check if this can be solved by using time-series techniques . As of now, I am not proceeding with time-series approach. I will be solving the problem using general ML techniques. But if given more time, i would give a chance to time-series also and would check how our data behaves on time-series models.\n\nEven if we are not proceeding with time-series appraoch, we can extract useful information from the datetime feature.","3d107f53":"#### Observation:   \nFrom the above two heatmaps, we observe that after log trasnformation, our target variable is having better relationship with other variables. That's a good thing for us.","316cf083":"#### Q-Q Plots","9f22125e":"# Making ML models","1fb1846b":"#### Observation:\nThe model which has minimum rmse and maximum r2 is performing the best. Hence, ExtraTreeRegressor is performing the best.   \ntest r2: 0.71   \ntest RMSE: 0.54","159d2bed":"#### Observation\n\nFeatures near to normal: pressure, r7, r8, r9 , t out, r2,r3,r4,r5, r1, t6, t7, t8, t1, t2\n\nsqewed features: Appliances, visibility, windspeed, rh out, t dewpoint, t9, t3, t4, t5\n\nrandomly distributes features : rv1, rv2, r6\n\nNote: Our target variable is sqewed. We will apply some transofrmation on it to bring it closer to normal distribution.   \nsome transformations that can be done to make the feature normal are:\n1. Log \n2. Exponential \n3. square root\n4. box-cox\n5. reciprocal","64d7ab3a":"#### Hitograms","97031559":"## Checking distribution of features","b9e05ad5":"## Checking relationship between variables","d4b40aaa":"#### Observation:\n\nWith hyperparameter tuning the r2 has increased to 0.73 and RMSE is decreased to 0.52 which is good","0335fa41":"## Multi collinearity check","c1c9a668":"### Observation\nThere are no categorical feature in the data.  There are two 'int' features which might be categorical. We will check it later in the notebook.   \n\nThere are no information about the features rv1, rv2 and what it denotes. We will keep it or discard it based on its relationship with the target variable.\n\nThere are in total 29 variables out of which 28 are independent variables and 1 is dependent which is our target variable."}}