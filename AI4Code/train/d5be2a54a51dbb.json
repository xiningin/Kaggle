{"cell_type":{"5fb1024c":"code","d0b07570":"code","96c40ea4":"code","0c765fc6":"code","b95ab321":"code","0babc607":"code","598994a5":"code","b59c175f":"code","f42a1bf0":"code","5042064b":"code","a29ad55a":"code","1858e65c":"code","3358c767":"code","90e13efe":"code","a7c1c164":"code","622c22d3":"markdown","6ebc5870":"markdown","34e4620b":"markdown","9d91c115":"markdown","8dc02d37":"markdown","6f631a8f":"markdown","0c73c079":"markdown","794637e3":"markdown","5629a281":"markdown","7f886c84":"markdown","3be01010":"markdown","ec3bcad6":"markdown"},"source":{"5fb1024c":"import pandas as pd\ndf = pd.read_csv('..\/input\/entity-annotated-corpus\/ner_dataset.csv', encoding=\"latin1\")\ndf = df.fillna(method=\"ffill\")\ndf.head()","d0b07570":"df = df.drop(['POS'], axis=1)\ndf = df.groupby('Sentence #').agg(list)\ndf = df.reset_index(drop=True)\ndf.head()","96c40ea4":"tag_list = []\nfor element in df.Tag:\n    for i in element:\n        if i not in tag_list:\n            tag_list.append(i)\nnum_classes = len(tag_list)\nprint('Number of classes : {} \\nItems : {}'.format(num_classes,tag_list))","0c765fc6":"vocab = [str(item).lower() for element in df.Word for item in element]\nvocab = list(set(vocab))\nnb_mots = len(vocab)\nprint('Number of different words : ',nb_mots)\nvocab[:10]","b95ab321":"import tensorflow as tf\n\ntokenizer_txt = tf.keras.preprocessing.text.Tokenizer(num_words=nb_mots, filters=None)\ntokenizer_txt.fit_on_texts(df.Word)\n\nword2idx = tokenizer_txt.word_index\nidx2word = tokenizer_txt.index_word\nvocab_size = tokenizer_txt.num_words\n\nprint(vocab_size)","0babc607":"#show the 20 first words\nfor i in range(1,20):\n    print(idx2word[i])","598994a5":"tokenizer_tag = tf.keras.preprocessing.text.Tokenizer(num_words=num_classes)\ntokenizer_tag.fit_on_texts(df.Tag)\n\nword2idx_tag = tokenizer_tag.word_index\nidx2word_tag = tokenizer_tag.index_word\nvocab_size_tag = tokenizer_tag.num_words\n\nword2idx_tag","b59c175f":"X = tokenizer_txt.texts_to_sequences(df.Word)\ny = tokenizer_tag.texts_to_sequences(df.Tag)\nprint(len(X[0]), X[0])\nprint(len(y[0]), y[0])","f42a1bf0":"longueur = df.Tag.apply(lambda x: len(x))\nlong_max = max(longueur)\nprint(\"Tag max Tength : \",long_max)\n\nX = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=long_max, padding='post', truncating='post')\ny = tf.keras.preprocessing.sequence.pad_sequences(y, maxlen=long_max, padding='post', truncating='post')","5042064b":"print(len(X[0]), X[0])\nprint(len(y[0]), y[0])","a29ad55a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","1858e65c":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, RNN, GRUCell, Dropout, Dense\n\nout_dim = 64\n\nmodel = Sequential(layers= [\n    Embedding(input_dim=nb_mots+1, output_dim=out_dim, input_length=long_max),\n    RNN(cell=GRUCell(out_dim), return_sequences=True),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(num_classes, activation='softmax')\n])\nmodel.summary()","3358c767":"Sparse_loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\ndef Sparse_loss_function(real, pred):\n    # Mask\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    # Avoid type error\n    mask = tf.cast(mask, dtype=pred.dtype)\n    # Loss function\n    loss_ = Sparse_loss_object(real, pred)\n    # Apply mask on loss function\n    loss_ *= mask\n    return tf.reduce_mean(loss_)","90e13efe":"model.compile(optimizer='adam', loss=Sparse_loss_function, metrics=['accuracy'])\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=128,workers=-1)","a7c1c164":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\nprob = model.predict(X_test)\npred = prob.argmax(axis=-1)\n\nprint(confusion_matrix(y_test.argmax(axis=1), pred.argmax(axis=1)))","622c22d3":"# Change the Text and Tag to integer lists","6ebc5870":"Good results with only 5 epochs, and accuracy of 99% on the validation data !","34e4620b":"# Pad sequences to have the same lenth","9d91c115":"# Check that X and y have been completed by 0 to have a length of 104","8dc02d37":"# Define a custom loss function to mask the zeros added at the end of each sequence","6f631a8f":"**OK so that's all for the moment, a simple RNN get an impressive 99% accuracy on the validation data.\nIf you have any idea on how to improve this notebook, please comment and don't forget to upvote if you like this work !**","0c73c079":"# Compile and train the model on a few epochs","794637e3":"# Get number of words","5629a281":"# Tokenize the Tags","7f886c84":"# Get Classes numbers and list","3be01010":"# Simple RNN model, could be more complex to achieve better results","ec3bcad6":"# Tokenize the text"}}