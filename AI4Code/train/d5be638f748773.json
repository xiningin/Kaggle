{"cell_type":{"877f360f":"code","4c7f73df":"code","d1316028":"code","7250eda7":"code","be544504":"code","9ea79dc6":"code","5a71e429":"code","f1dd9566":"code","d0fe1026":"code","7e01bfd5":"code","f6ac6184":"code","eb5c69f6":"code","79782e64":"code","8f7024a9":"code","9cbff179":"code","1dc21bf8":"code","c2325442":"code","21967840":"code","b639e49f":"markdown","786329ae":"markdown","53822cb9":"markdown"},"source":{"877f360f":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nfrom xgboost import XGBClassifier\nfrom scipy.stats import rankdata\nfrom pathlib import Path\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import BayesianRidge, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pickle","4c7f73df":"n_folds = 10\nseed_list = [i for i in range(2000, 2022)]","d1316028":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(seed_list[0])","7250eda7":"INPUT_PATH = Path(\"..\/input\/tabular-playground-series-mar-2021\")\n\nTRAIN_PATH = Path(\"..\/input\/tabularplaygroundseriesmar2021\/preprocessed-data\/train\")\nTEST_PATH = Path(\"..\/input\/tabularplaygroundseriesmar2021\/preprocessed-data\/test\")","be544504":"train_df = pd.read_csv(INPUT_PATH \/ \"train.csv\")\ntest_df = pd.read_csv(INPUT_PATH \/ \"test.csv\")\nsub_df = pd.read_csv(INPUT_PATH \/ 'sample_submission.csv')","9ea79dc6":"train_oof_dict = {\n    'trans_1': 'train_rgr_epoch2000_probas8_params0_batch512.npy',\n    'trans_2': 'train_rgr_epoch2000_probas8_params1_batch512.npy',\n    'trans_3': 'train_rgr_epoch2000_probas8_params2_batch512.npy',\n    'trans_4': 'train_rgr_epoch2000_probas8_params3_batch512.npy',\n    'trans_5': 'train_rgr_epoch2000_probas8_params4_batch512.npy',\n    'trans_6': 'train_rgr_epoch2000_probas8_params5_batch512.npy',\n    'trans_7': 'train_rgr_epoch2000_probas8_params6_batch512.npy',\n    'lightgbm1': 'train_lgb.npy',\n    'lightgbm2': 'train_oof_lgbm_0.npy',\n    'lightgbm3': 'train_oof_lgbm_1.npy',\n    'xgboost': 'train_xgb.npy',\n    'catboost': 'train_cbt.npy',\n    'logistic_regression1': 'train_lr.npy',\n    'logistic_regression2': 'train_oof_lr_0.npy',\n    'random_forest': 'train_rf.npy',\n    'tabnet1': 'train_tabnet_0.npy',\n    'tabnet2': 'train_tabnet_1.npy',\n    'histgradient1': 'train_oof_hgb_0.npy',\n    'histgradient2': 'train_oof_hgb_1.npy',\n    'keras1': 'train_keras_0.npy',\n    'keras2': 'train_keras_1.npy'\n}\n\ntest_pred_dict = {\n    'trans_1': 'test_rgr_epoch2000_probas8_params0_batch512.npy',\n    'trans_2': 'test_rgr_epoch2000_probas8_params1_batch512.npy',\n    'trans_3': 'test_rgr_epoch2000_probas8_params2_batch512.npy',\n    'trans_4': 'test_rgr_epoch2000_probas8_params3_batch512.npy',\n    'trans_5': 'test_rgr_epoch2000_probas8_params4_batch512.npy',\n    'trans_6': 'test_rgr_epoch2000_probas8_params5_batch512.npy',\n    'trans_7': 'test_rgr_epoch2000_probas8_params6_batch512.npy',\n    'lightgbm1': 'test_lgb.npy',\n    'lightgbm2': 'test_preds_lgbm_0.npy',\n    'lightgbm3': 'test_preds_lgbm_1.npy',\n    'xgboost': 'test_xgb.npy',\n    'catboost': 'test_cbt.npy',\n    'logistic_regression1': 'test_lr.npy',\n    'logistic_regression2': 'test_preds_lr_0.npy',\n    'random_forest': 'test_rf.npy',\n    'tabnet1': 'test_tabnet_0.npy',\n    'tabnet2': 'test_tabnet_1.npy',\n    'histgradient1': 'test_preds_hgb_0.npy',\n    'histgradient2': 'test_preds_hgb_1.npy',\n    'keras1': 'test_keras_0.npy',\n    'keras2': 'test_keras_1.npy'\n}\n","5a71e429":"oof_df = pd.DataFrame()\npreds_df = pd.DataFrame()\n\nfor name, train_oof in train_oof_dict.items():\n    oof_df = pd.concat([oof_df, pd.Series(np.load(TRAIN_PATH \/ train_oof), name=name)], axis=1)\n    \nfor name, test_pred in test_pred_dict.items():\n    preds_df = pd.concat([preds_df, pd.Series(np.load(TEST_PATH \/ test_pred), name=name)], axis=1)","f1dd9566":"oof_df.shape","d0fe1026":"def normalize(dataset):\n    dataNorm=((dataset-dataset.min())\/(dataset.max()-dataset.min()))\n    return dataNorm\n","7e01bfd5":"oof_df = normalize(oof_df)\npreds_df = normalize(preds_df)","f6ac6184":"data = oof_df\ntarget = train_df['target']\n\ndef objective(trial , data = data , target = target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n                test_size = 0.0356789 , random_state = 42)\n    params = {\n        'eval_metric' : 'auc',\n        'booster' : 'gbtree',\n        'tree_method' : 'gpu_hist' , \n        'use_label_encoder' : False , \n        'lambda' : trial.suggest_loguniform('lambda' , 1e-5 , 1.0),\n        'alpha' : trial.suggest_loguniform('alpha' , 1e-5 , 1.0),\n        'colsample_bytree' : trial.suggest_uniform('colsample_bytree' , 0 , 1.0),\n        'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.02),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'max_depth' : trial.suggest_int('max_depth' , 1 , 20),\n        'random_state' : trial.suggest_categorical('random_state' , [0,42,2021]),\n        'min_child_weight' : trial.suggest_int('min_child_weight' , 1 , 300),\n        'gamma' : trial.suggest_loguniform('gamma' , 1e-5 , 1.0)\n    }\n    model = XGBClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , early_stopping_rounds = 222 , \\\n              verbose = False)\n    preds = model.predict_proba(test_x)[: , 1]\n    auc = roc_auc_score(test_y , preds )\n    return auc\n","eb5c69f6":"study = optuna.create_study(direction = 'maximize' , study_name = 'xgbclassifier')\nstudy.optimize(objective , n_trials = 60)\nprint('number of the finished trials:' , len(study.trials))\nprint('the parametors of best trial:' , study.best_trial.params)\nprint('best value:' , study.best_value)","79782e64":"params = {'lambda': 0.00021936500359658444, 'alpha': 0.022578559219294244, 'colsample_bytree': 0.3922738071998231, \n          'subsample': 0.12739514555784553, 'learning_rate': 0.01986398851633632, 'n_estimators': 9489, 'max_depth': 15, 'random_state': 2021, 'min_child_weight': 63, 'gamma': 0.008018001812029647}","8f7024a9":"params['eval_metric'] = 'auc'\nparams['booster'] = 'gbtree'\nparams['tree_method'] = 'gpu_hist'\nparams['use_label_encoder'] = False","9cbff179":"test = preds_df\nall_features =oof_df.columns.values","1dc21bf8":"preds = np.zeros(test.shape[0])\noof_predictions = np.zeros(len(data))\nskf = StratifiedKFold(n_splits = 20 , random_state = 42 , shuffle = True)\nroc = []\nn = 0\nfor trn_idx , val_idx in skf.split(data , target):\n    train_x = data.iloc[trn_idx]\n    train_y = target.iloc[trn_idx]\n    val_x = data.iloc[val_idx]\n    val_y = target.iloc[val_idx]\n    \n    model = XGBClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] , early_stopping_rounds = 100 , \\\n             verbose = False)\n    preds += model.predict_proba(test[all_features])[:,1]\/skf.n_splits\n    oof_predictions += model.predict_proba(data[all_features])[:,1]\/skf.n_splits\n    roc.append(roc_auc_score( val_y , model.predict_proba(val_x)[:,1]))\n    print(n+1 , roc[n])\n    n+=1","c2325442":"df = pd.DataFrame(preds, columns = ['target'])\ndf.insert(loc=0, column='id', value=test_df['id'])","21967840":"df.to_csv('sub.csv', index=False)","b639e49f":"# **optuna**","786329ae":"## Stacking and creating a new DataFrame","53822cb9":"# XGBoost for final prediction."}}