{"cell_type":{"7e48172f":"code","a569ce4c":"code","355de862":"code","aaf9deda":"code","3ed3168c":"code","bb01feac":"code","24fdd4ef":"code","2441af71":"code","9ae02634":"code","05c89ec3":"code","dc5cdb20":"code","cd7f2d13":"code","672daf24":"code","c1313788":"code","18030999":"code","87f530b4":"code","ecbbb4fa":"code","39a77f6c":"code","66e3f25e":"code","8f772a65":"code","e285d3d8":"code","26f36da3":"code","bab11fd7":"code","116dec1a":"markdown","d51e7c3e":"markdown","ea694aa6":"markdown","d97d664c":"markdown","8f2f7a4e":"markdown","04752afd":"markdown","3a4f4192":"markdown","410b7264":"markdown","a19701ec":"markdown","f68b383a":"markdown","3ce8732a":"markdown","9e76f6ed":"markdown","bda1c469":"markdown","47ffc2ef":"markdown","46d1772d":"markdown"},"source":{"7e48172f":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n#allow you to plot directly without having to call .show()\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") #ignore warnings","a569ce4c":"data = pd.read_csv(\"..\/input\/creditcard.csv\") #loading our data","355de862":"data.head()","aaf9deda":"data['Class'].value_counts() #492 frauds in our dataset.","3ed3168c":"data['Amount'].value_counts().sum() #just checking the sum of all the transactions in our dataset.","bb01feac":"corr = data.corr()\ncorr['Class'].sort_values(ascending=False).head(12) #only the first 12 positively correlated values.","24fdd4ef":"corr = data.corr()\ncorr['Class'].sort_values(ascending=True).head(12) #only the first 12 negatively correlated values.","2441af71":"from sklearn.model_selection import train_test_split #importing the necessary module\n\ntrain_set, test_set = train_test_split(data, test_size=0.2, random_state=42)","9ae02634":"print(train_set.shape, test_set.shape) #printing the shape of the training and testing set","05c89ec3":"train_labels = train_set[\"Class\"].copy()\ntrain = train_set.drop(\"Class\", axis=1).copy()\ntest_labels = test_set[\"Class\"].copy()\ntest = test_set.drop(\"Class\", axis=1).copy()","dc5cdb20":"print(train.shape, test.shape) #checking the shape again","cd7f2d13":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nimport time #implementing in this function the time spent on training the model\nfrom sklearn.model_selection import cross_val_score\n\n#Generic function for making a classification model and accessing performance:\ndef classification_model(model, X_train, y_train):\n    #Fit the model:\n    time_start = time.perf_counter() #start counting the time\n    model.fit(X_train,y_train)\n    n_cache = []\n    \n    train_predictions = model.predict(X_train)\n    precision = precision_score(y_train, train_predictions)\n    recall = recall_score(y_train, train_predictions)\n    f1 = f1_score(y_train, train_predictions)\n    \n    print(\"Precision \", precision)\n    print(\"Recall \", recall)\n    print(\"F1 score \", f1)\n    \n    cr_val = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n    \n    time_end = time.perf_counter()\n    \n    total_time = time_end-time_start\n    print(\"Cross Validation Score: %f\" %np.mean(cr_val))\n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))","672daf24":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver=\"liblinear\")\nclassification_model(log_reg, train,train_labels)","c1313788":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest = RandomForestClassifier(n_estimators=10)\nclassification_model(rand_forest,train, train_labels)","18030999":"# Plot feature importance\nfeature_importance = rand_forest.feature_importances_\n# make importances relative to max importance\nplt.figure(figsize=(20, 10)) #figure size\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max()) #making it a percentage relative to the max value\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos,feature_importance[sorted_idx], align='center')\nplt.yticks(pos,train.iloc[sorted_idx],fontsize=15)\nplt.xlabel('Relative Importance', fontsize=20)\nplt.ylabel('Features', fontsize=20)\nplt.title('Variable Importance', fontsize=30)","87f530b4":"from sklearn.model_selection import cross_val_predict #it is gonna be used to use the cross validation prediction\n#fitting the testing dataset\ntest_predictions = rand_forest.predict(test)\nprecision = precision_score(test_labels, test_predictions)\nrecall = recall_score(test_labels, test_predictions)\nf1 = f1_score(test_labels, test_predictions)\ny_scores = rand_forest.predict_proba\n\ncross_value = cross_val_score(rand_forest,test,test_labels ,cv=5, scoring='recall')\n\ny_probas_forest = cross_val_predict(rand_forest, test, test_labels, cv=3,\n                                    method=\"predict_proba\")\n\ny_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n","ecbbb4fa":"from sklearn.metrics import roc_auc_score #gonna use the roc_auc score, importing the module necessary to use it.\n\n\n#printing the results\nprint(\"Precision \", precision)\nprint(\"Recall \", recall)\nprint(\"F1 score \", f1)\nprint(\"Cross Validation Score: %f\" %np.mean(cross_value))\nprint(\"ROC AUC score: \", roc_auc_score(test_labels, y_scores_forest))","39a77f6c":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\nclassification_model(knn_clf, train, train_labels)","66e3f25e":"##SHOULD'VE IMPLEMENTED A FUNCTION IN ORDER TO AVOID REPEATING CODE HERE\n\ntest_predictions_knn = knn_clf.predict(test)\nprecision_knn = precision_score(test_labels, test_predictions_knn)\nrecall_knn = recall_score(test_labels, test_predictions_knn)\nf1_knn = f1_score(test_labels, test_predictions_knn)\ny_scores_knn = knn_clf.predict_proba\n\ncross_value = cross_val_score(knn_clf,test,test_labels ,cv=5, scoring='recall')\n\ny_probas_knn = cross_val_predict(knn_clf, test, test_labels, cv=3,\n                                    method=\"predict_proba\")\n\ny_scores_knn = y_probas_knn[:,1] # score = proba of positive class","8f772a65":"#printing the results\nprint(\"Precision \", precision_knn)\nprint(\"Recall \", recall_knn)\nprint(\"F1 score \", f1_knn)\nprint(\"Cross Validation Score: %f\" %np.mean(cross_value))\nprint(\"ROC AUC score: \", roc_auc_score(test_labels, y_scores_knn))","e285d3d8":"import lightgbm as lgb\n\nlgb_model = lgb.LGBMClassifier(num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nclassification_model(lgb_model, train, train_labels)","26f36da3":"##SHOULD'VE IMPLEMENTED A FUNCTION IN ORDER TO AVOID REPEATING CODE HERE\n\ntest_predictions_lgb = lgb_model.predict(test)\nprecision_lgb = precision_score(test_labels, test_predictions_lgb)\nrecall_lgb = recall_score(test_labels, test_predictions_lgb)\nf1_lgb = f1_score(test_labels, test_predictions_lgb)\ny_scores_lgb = lgb_model.predict_proba\n\ncross_value_lgb = cross_val_score(lgb_model,test,test_labels ,cv=5, scoring='recall')\n\ny_probas_lgb = cross_val_predict(lgb_model, test, test_labels, cv=3,\n                                    method=\"predict_proba\")\n\ny_scores_lgb = y_probas_lgb[:,1] # score = proba of positive class","bab11fd7":"#printing the results\nprint(\"Precision \", precision_lgb)\nprint(\"Recall \", recall_lgb)\nprint(\"F1 score \", f1_lgb)\nprint(\"Cross Validation Score: %f\" %np.mean(cross_value_lgb))\nprint(\"ROC AUC score: \", roc_auc_score(test_labels, y_scores_lgb))","116dec1a":"## Implementing a function to help with this task, gonna check the precision of each model, recall and the F1 score. Finally, the cross validation score is also given","d51e7c3e":"## Using the light gradient boosting, we could raise recall and make the model more robust","ea694aa6":"#### Now let's start fitting each of our models and check the results","d97d664c":"## What happened above is a good example of overfitting the training set, not generalizing well and performing poorly on the testing set","8f2f7a4e":"## Quite good results, However, in cases like credit card fraud, we would like to have a higher value of recall instead of precision, as false positives are not as bad as false negatives...\n### That said, It would be nice to tune the model a bit in order to raise recall a bit.\n\nbut first, why not try some different models.","04752afd":"#### that's why I implement functions, again I am gonna need to repeat my code to see how the model performs in the test set. ","3a4f4192":"#### it's always a good strategy to plot a heatmap showing the pearson's correlation","410b7264":"##### Not gonna scale the data, as it could be seen, only numerical entries and apparently it doesn't need scale","a19701ec":"### Class - 0 = Normal \/ 1 = Tentative Fraud","f68b383a":"## Spliting the data","3ce8732a":"**Next model is gonna be a random forest, and I am gonna use the features_importance and plot it to see which features are more important in our prediction**","9e76f6ed":"#### Next, the Light Gradient Boost, an leaf-wise algorithm,unlike other decision tree models which use level-wise growth algorithm during model training. For larger datasets, it requires less memory and it often generalizes quite well.","bda1c469":"#### this data doesn't have columns description, it's been disclosed, we only have the numerical values, we must find correlations, check for null values, and see what we can get from it.","47ffc2ef":"**Impressive, let's see how it fits into the test**","46d1772d":"### random forest is fitting quite nicely the data and the cross validation score is telling us that it might not be overfitting\n#### gonna fit the test and see the results"}}