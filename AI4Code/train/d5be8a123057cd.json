{"cell_type":{"bef2e0a8":"code","e172801e":"code","abd62df3":"code","ec2b0a51":"code","31dfca78":"code","b1dd1ddc":"code","d5bd1de0":"code","9020aebe":"code","50571f5d":"code","9c4dd972":"code","29cc3556":"code","a1e39ceb":"code","949f7b67":"code","d862d756":"code","fb1eb3df":"code","ab3d1a5b":"code","ed90a2ee":"code","fd3c2171":"code","ac6a947e":"code","09caf9d6":"code","94fc26fd":"code","29f3abbb":"code","060cb880":"code","66dd46dc":"code","edbcffe1":"code","c8265680":"code","8d7c608d":"code","265a4211":"code","95c91fa8":"code","550f8f33":"code","1dd6300a":"code","2e159eac":"code","b3b4af24":"code","5ab9b5e4":"code","c8c33da9":"code","65eb4635":"code","a5b45bc6":"code","93e48720":"code","1ade04aa":"code","cb117f9d":"code","e54763b5":"code","158ade4d":"code","bdcc76e5":"code","fbcdef5f":"code","243c6001":"code","f642ac3e":"code","5f5ab125":"code","d133baee":"code","623fb66d":"code","fb50e4e2":"markdown","745414bc":"markdown","66984cf3":"markdown","5f15d926":"markdown","e98199b1":"markdown","59a6296f":"markdown","59fb0c9b":"markdown","f36ef618":"markdown","05839c10":"markdown","cbf9ba06":"markdown","1c766ca7":"markdown","ed8ac73c":"markdown","006ad6d2":"markdown","2d8d06d6":"markdown","b6c02c6d":"markdown","46729291":"markdown","15effb76":"markdown","fac7287c":"markdown","29c70b9d":"markdown","f3692d4f":"markdown","70b1536d":"markdown","151471ee":"markdown","c3ecb29d":"markdown","b9671700":"markdown","ba94b315":"markdown","3a594991":"markdown","5027b463":"markdown","27bc067f":"markdown","fd1c66d9":"markdown","a2cc2a67":"markdown"},"source":{"bef2e0a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e172801e":"path = r\"\/kaggle\/input\/szeged-weather\/weatherHistory.csv\"\ndf = pd.read_csv(path)\ndf.head()","abd62df3":"df.shape","ec2b0a51":"df.columns","31dfca78":"df.describe()","b1dd1ddc":"# Removing duplicates\ndf.drop_duplicates(inplace=True)\ndf.reset_index(inplace=True, drop=True)","d5bd1de0":"# Percentage of null values\n(df.isnull().sum()\/df.shape[0])*100","9020aebe":"# Looking the column data\ndf[\"Precip Type\"]","50571f5d":"(df[\"Precip Type\"].value_counts()\/df.shape[0])*100","9c4dd972":"df[\"Precip Type\"] = df[\"Precip Type\"].fillna(\"None\")","29cc3556":"# Double checking the values\ndf[\"Precip Type\"].value_counts()","a1e39ceb":"categorical_columns = [\"Summary\", \"Precip Type\", \"Daily Summary\"]\n\nfor column in categorical_columns:\n    print()\n    print(column, len(df[column].unique()))\n    print((df[column].value_counts()\/df.shape[0])*100)","949f7b67":"# Dropping unnecessary columns at once\ndf.drop(columns = [\"Loud Cover\", \"Daily Summary\" , \"Formatted Date\"], inplace=True)\n\n# Updating categorical column list\ncategorical_columns = [\"Summary\", \"Precip Type\"]","d862d756":"df.corr()","fb1eb3df":"# List of numeric columns\nnumerical_columns = [\"Apparent Temperature (C)\", \"Humidity\", \"Wind Speed (km\/h)\", \"Visibility (km)\", \"Pressure (millibars)\"]\n\n# Dropping\ndf.drop(columns=[\"Temperature (C)\"], inplace=True)","ab3d1a5b":"df.info()","ed90a2ee":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy import stats","fd3c2171":"# Taking a sample dataset of 20% if the dataset for visulization for faster computation\nsample_df = df.sample(frac=0.2, replace=True, random_state=7)\n\n# Resetting Index\nsample_df.reset_index(drop=True, inplace=True)","ac6a947e":"sample_df.shape","09caf9d6":"# Using \"Precip Type\" to analysing patterns \nsns.pairplot(data=sample_df, hue=\"Precip Type\")","94fc26fd":"# Using \"Summary\" to analysing patterns \nsns.pairplot(data=sample_df, hue=\"Summary\")","29f3abbb":"# Distribution plot\nsns.displot(sample_df, x=\"Humidity\", kde=True)","060cb880":"stats.probplot(df[\"Humidity\"], plot = plt)","66dd46dc":"stats.probplot(np.power(df[\"Humidity\"], 2), plot = plt)\nplt.show()\n\nsns.distplot(np.power(df[\"Humidity\"], 2))","edbcffe1":"# Now lets look at the temperature and humidity relationship\nsns.jointplot(x=\"Apparent Temperature (C)\", y=\"Humidity\", data=df, kind=\"kde\")","c8265680":"sns.jointplot(x=\"Apparent Temperature (C)\", y=\"Humidity\", data=sample_df, kind=\"hist\")","8d7c608d":"# Distribution plot\nsns.displot(sample_df, x=\"Visibility (km)\", kde=True)","265a4211":"stats.probplot(df[\"Visibility (km)\"], plot = plt)","95c91fa8":"stats.probplot(np.power(df[\"Visibility (km)\"], 2), plot = plt)\nplt.show()\n\nsns.distplot(np.power(df[\"Visibility (km)\"], 2))","550f8f33":"sns.jointplot(x=\"Apparent Temperature (C)\", y=\"Visibility (km)\", data=sample_df, kind=\"kde\")","1dd6300a":"sns.jointplot(x=\"Apparent Temperature (C)\", y=\"Visibility (km)\", data=sample_df, kind=\"hist\")","2e159eac":"# Independent Variable\nsns.distplot(sample_df[\"Apparent Temperature (C)\"])","b3b4af24":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_squared_error as mse\nfrom xgboost import XGBRegressor","5ab9b5e4":" def evaluate(y_test, y_pred):\n    '''\n    Evaluates different score for predicted data!\n    '''\n    #print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_pred - y_test)))\n    #print(\"Residual sum of squares (MSE): %.2f\" % np((y_pred - y_test) ** 2))\n    print(\"Mean Absolute Error : %.4f\" % mae(y_test, y_pred))    \n    print(\"Mean Squared Error : %.4f\" % mse(y_test, y_pred))\n    print(\"R2-score: %.4f\" % r2_score(y_test , y_pred))","c8c33da9":"def coeffecients(model):\n    '''\n    Prints coeffecient of the model!\n    '''\n    print(\"Coefficients of the model:\", model.coef_)\n    print(\"Intercept of the model: \", model.intercept_)","65eb4635":"# Scalling dependent variable\ndependent_scaler = MinMaxScaler()\nX_scaled = dependent_scaler.fit_transform(df[[\"Humidity\", \"Visibility (km)\"]].values)\n\n# Scalling independent variable\nindependent_scaler = MinMaxScaler()\ny_scaled = independent_scaler.fit_transform(df[[\"Apparent Temperature (C)\"]].values)\n\ndel dependent_scaler, independent_scaler\n\n# Taking quick glance at the transformed data\ndisplay(pd.DataFrame(X_scaled, columns=[\"Humidity\", \"Visibility (km)\"]).describe())\ndisplay(pd.DataFrame(y_scaled, columns=[\"Apparent Temperature (C)\"]).describe())","a5b45bc6":"# Spliting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=23)\ndel X_scaled, y_scaled\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","93e48720":"# Linear Regresssion\nregression = LinearRegression()\n\n# Trainning model\nregression.fit(X_train, y_train)\n\n# Predicting results\ny_pred = regression.predict(X_test)\n\n# Scoring model\nevaluate(y_test, y_pred)\nprint(\"Linear Regression score: %.2f\" % regression.score(X_train, y_train))\ncoeffecients(regression)\n\ndel regression, y_pred","1ade04aa":"# Polynomial Regression\n# generating polynomial feature\npoly_reg = PolynomialFeatures(degree = 2)\nX_train_poly = poly_reg.fit_transform(X_train)\n\n# Trainining model\npolynomial_regression = LinearRegression()\npolynomial_regression.fit(X_train_poly, y_train)\n\n# Fitting model\nX_test_poly = poly_reg.fit_transform(X_test)\n\n# Prediction results\ny_pred = polynomial_regression.predict(X_test_poly)\n\n# Scoring model\nevaluate(y_test, y_pred)\nprint(\"Polynomial Regression score: %.2f\" % polynomial_regression.score(X_train_poly, y_train))\ncoeffecients(polynomial_regression)\n\ndel polynomial_regression, X_train_poly, X_test_poly, y_pred","cb117f9d":"# XGB Regressor\n# Initilizing model\nregressor = XGBRegressor()\n\n# Trainning model\nregressor.fit(X_train, y_train, verbose=False)\n\n# Prediction\ny_pred = regressor.predict(X_test)\n\n# Scoring model performance\nevaluate(y_test, y_pred)\nprint(\"XGB Regression score: %.2f\" % regressor.score(X_train, y_train))\n\ndel regressor, y_pred","e54763b5":"from sklearn.preprocessing import OneHotEncoder","158ade4d":"# One Hot encoding for categorical columns\nencoder = OneHotEncoder()\ncat_encoded = encoder.fit_transform(df[[\"Precip Type\", \"Summary\"]].values).toarray()\n\n# Removing the last column\ncat_encoded = np.delete(cat_encoded, -1, axis=1)\ndel encoder","bdcc76e5":"# Glance at the encoded values\ncat_encoded","fbcdef5f":"# Scalling dependent variable\ndependent_scaler = MinMaxScaler()\nX_scaled = dependent_scaler.fit_transform(df[[\"Humidity\", \"Visibility (km)\"]].values)\n\n# Scalling independent variable\nindependent_scaler = MinMaxScaler()\ny_scaled = independent_scaler.fit_transform(df[[\"Apparent Temperature (C)\"]].values)\n\ndel dependent_scaler, independent_scaler","243c6001":"# Merging categorical and numeric features\nX_scaled = np.concatenate((X_scaled, cat_encoded), axis=1)\ndel cat_encoded","f642ac3e":"# Spliting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=23)\ndel X_scaled, y_scaled\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","5f5ab125":"# Linear Regresssion\nregression = LinearRegression()\n\n# Trainning model\nregression.fit(X_train, y_train)\n\n# Predicting results\ny_pred = regression.predict(X_test)\n\n# Scoring model\nevaluate(y_test, y_pred)\nprint(\"Linear Regression score: %.2f\" % regression.score(X_train, y_train))\ncoeffecients(regression)\n\ndel regression, y_pred","d133baee":"# Polynomial Regression\n# generating polynomial feature\npoly_reg = PolynomialFeatures(degree = 2)\nX_train_poly = poly_reg.fit_transform(X_train)\n\n# Trainining model\npolynomial_regression = LinearRegression()\npolynomial_regression.fit(X_train_poly, y_train)\n\n# Fitting model\nX_test_poly = poly_reg.fit_transform(X_test)\n\n# Prediction results\ny_pred = polynomial_regression.predict(X_test_poly)\n\n# Scoring model\nevaluate(y_test, y_pred)\nprint(\"Polynomial Regression score: %.2f\" % polynomial_regression.score(X_train_poly, y_train))\n#coeffecients(polynomial_regression)\n\ndel polynomial_regression, X_train_poly, X_test_poly, y_pred","623fb66d":"# XGB Regressor\n# Initilizing model\nregressor = XGBRegressor()\n\n# Trainning model\nregressor.fit(X_train, y_train, verbose=False)\n\n# Prediction\ny_pred = regressor.predict(X_test)\n\n# Scoring model performance\nevaluate(y_test, y_pred)\nprint(\"XGB Regression score: %.2f\" % regressor.score(X_train, y_train))\n\ndel regressor, y_pred","fb50e4e2":"Defining some function for evaluating model performances!","745414bc":"**Visualization**\n\nWe will be using some graph to get a sense and distribution of data!","66984cf3":"Now, we are done with replacing the null values. So,first lets look at the categorical features.","5f15d926":"**Catgorical Features**\n\nFirst, identifying the categorical columns. Looking at the distribution of the data in respective categories and there percentage present!","e98199b1":"The value in the column is not evenly divided but for now we will be keeping this variable. Creating a seperate category for null values.","59a6296f":"This is a categorical variable. So will be looking at the diversity of the data and then will decide weather to include this in our model or not!","59fb0c9b":"**APPARENT TEMPERATURE vs Visibility (km)**, Corelation:0.392939","f36ef618":"\"Humidity\" follows a somewhat linear dependency with \"Apparent Temperature\"!","05839c10":"**Train, Test split**","cbf9ba06":"Data validates that:\n* Snow- low temperature\n* Rain- on higher side\n\nDensity:\n* Apparent Temperature - slighly left skewed\n* Humidity - left skewed\n* Wind Speed - right skewed\n* Wind Bearing - Balanced\n* Visibility - slighlty left skewed\n* Pressure - highly left skewed","1c766ca7":"**Cleaning**\n\nLoud Cover has all the values zero so will be removing this feature!","ed8ac73c":"# Analysing model performance with addition of different categorical features! \n**Linear regression**\n\nOnly \"Precip Type\"\n* Mean Absolute Error : 0.0859\n* Mean Squared Error : 0.0110\n* R2-score: 0.5627\n* Linear Regression score: 0.56\n\n\"Precip Type\" and Summary\n* Mean Absolute Error : 0.0841\n* Mean Squared Error : 0.0106\n* R2-score: 0.5791\n* Linear Regression score: 0.57\n\n**Polynomial Regression**\n\nOnly \"Precip Type\"\n* Mean Absolute Error : 0.0815\n* Mean Squared Error : 0.0102\n* R2-score: 0.5951\n* Polynomial Regression score: 0.59\n\n\"Precip Type\" and Summary\n* Mean Absolute Error : 0.0793\n* Mean Squared Error : 0.0097\n* R2-score: 0.6141\n* Polynomial Regression score: 0.61\n\n**XGB Regressor**\n\nOnly \"Precip Type\"\n* Mean Absolute Error : 0.0749\n* Mean Squared Error : 0.0091\n* R2-score: 0.6391\n* XGB Regression score: 0.66\n\n\"Precip Type\" and Summary\n* Mean Absolute Error : 0.0730\n* Mean Squared Error : 0.0087\n* R2-score: 0.6557\n* XGB Regression score: 0.67\n\n\nAddition of \"Precip Type\" feature has significantly impoved the model performance for all of the 3 models. And, Polynomial Regression beats Linear Regression here. But addition of Summary feature doesn't show a significant impact on model performance plus after one-hot endocing it add a lot of features which increase the time and computation required to train models.Thus,\n**XGB regressor >> Polynomial Regression >> Linear Regression**\n\nAlso, please observe that scalling feature didn't improve the r2_score. While, adding more features did!\n\nThank you for time.\nHope you enjoyed it!","006ad6d2":"Outlier is deviating, so will be taking the original data for Visibility as well!","2d8d06d6":"**APPARENT TEMPERATURE vs HUMIDITY**, Corelation:-0.632331","b6c02c6d":"For probplot,will be using the whole dataset as in a particular sample the total count of outliers will be reduced as compaird to whole dataset!","46729291":"**Numerical features**\n\nFirst step, is to look at the corelation of dependent variables with the independant variable!","15effb76":"Looking at the data, will be droping \"Daily Summary\" feature, \"Loud Cover\" with all values as 0 and \"Formatted Date\" feature as well.","fac7287c":"# Baseline model \nwith Linear Regression, Polynomial Regression and XGB Regressor!\n\nDependent Variables:\n* Humidity\n* Visibility (km)","29c70b9d":"**Linear Regression**","f3692d4f":"Outlier is deviating, so will be taking the original data for Humidity!","70b1536d":"**Scalling \/ Normalizing Features**","151471ee":"The data is left skewed, so will try to apply some transformations, lets look at the probplot first!","c3ecb29d":"# Model 2\nwill try to improve our baseline model by adding categorical values, \nwith Linear Regression, Polynomial Regression and XGB Regressor!\n\nDependent Variables:\n* Humidity\n* Visibility (km)\n* Precip Type\n* Summary","b9671700":"# Analysing model performance with and without scalling\/normalizing features! \n**Linear regression**\n\nWithout Scalling\n* Mean Absolute Error : 6.6859\n* Mean Squared Error : 68.5820\n* R2-score: 0.3951\n* Linear Regression score: 0.39\n* Coefficients of the model: [[-29.20160549   0.47429132]]\n* Intercept of the model:  [27.40557658]\n\nWith Scalling\n* Mean Absolute Error : 0.0997\n* Mean Squared Error : 0.0152\n* R2-score: 0.3951\n* Linear Regression score: 0.39\n* Coefficients of the model: [[-0.43544768  0.11386764]]\n* Intercept of the model:  [0.82197032]\n\n**Polynomial Regression**\n\nWithout Scalling\n* Mean Absolute Error : 6.6093\n* Mean Squared Error : 66.3651\n* R2-score: 0.4146\n* Polynomial Regression score: 0.41\n* Coefficients of the model: [[ 0.00000000e+00 -8.43603307e+01  1.33611626e+00  4.25335969e+01\n    1.08298653e-01 -4.12643671e-02]]\n* Intercept of the model:  [38.7791077]\n\nWith Scalling\n* Mean Absolute Error : 0.0986\n* Mean Squared Error : 0.0148\n* R2-score: 0.4146\n* Polynomial Regression score: 0.41\n* Coefficients of the model: [[ 0.         -1.25796202  0.32077416  0.6342513   0.02600029 -0.15949835]]\n* Intercept of the model:  [0.99156983]\n\n**XGB Regressor**\n\nWithout Scalling\n* Mean Absolute Error : 5.9262\n* Mean Squared Error : 56.6987\n* R2-score: 0.4999\n* XGB Regression score: 0.53\n\nWith Scalling\n* Mean Absolute Error : 0.0884\n* Mean Squared Error : 0.0126\n* R2-score: 0.4988\n* XGB Regression score: 0.53\n\n\nModels with scaled features are performing way better than models without scaled features. Polynomial Regression is not performing well as compare to Linear Regression. Thus,\n\n**XGB regressor >> Linear Regression >> Polynomial Regression**.\n\nScore of XGB regressor and Linear Regression are quite good. But, lets see if there is an impact of categorical variables on the models performance.","ba94b315":"We have one column with missing values and the percentage of null values is also very less.","3a594991":"\"Apparent temperature\" distribution looks okay!","5027b463":"**Linear Regresssion**\n\n* Is there a relationship between humidity and temperature? \n* What about between humidity and apparent temperature? \n* Can you predict the apparent temperature given the humidity?","27bc067f":"* Humidity and Visibility is closely related to Temperature\n* Apparent Temperature and Temperature are very strongly related\n\nWe will be predicting \"Apparent Temperature\".Thus, will be dropping Temperature feature ","fd1c66d9":"**Polynomial Regression**","a2cc2a67":"**XGB Regressor**"}}