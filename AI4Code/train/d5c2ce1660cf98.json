{"cell_type":{"424ad49c":"code","4ed26d1f":"code","8fd1bced":"code","708ac365":"code","6b52e563":"code","afe57a25":"code","dff83db2":"code","69a34160":"code","38baa539":"code","a859620e":"code","b3cee9bf":"code","66297c70":"code","7c7b8175":"code","a2861df7":"code","8db3415a":"code","44cf0343":"code","e191fd81":"code","2e4bedfb":"code","f57c6f20":"code","5a31f92b":"code","61271b90":"markdown"},"source":{"424ad49c":"import string\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport os\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4ed26d1f":"BASE_PATH = '..\/input\/tweet-sentiment-extraction\/'\ntrain_df = pd.read_csv(BASE_PATH+ 'train.csv')\n# train_df = train_df[:1000]\ntest_df = pd.read_csv( BASE_PATH+ 'test.csv')\n# test_df = test_df[:1000]","8fd1bced":"train_df.info()","708ac365":"test_df.info()","6b52e563":"# remove any rows containing nan values\ntrain_df= train_df.dropna()","afe57a25":"train_df.head()","dff83db2":"test_df.head()","69a34160":"train_df.describe()","38baa539":"#jaccard method \ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","a859620e":"train_df.head()","b3cee9bf":"# def train(train_data, output_dir, n_iter=20, model=None):\ndef train(train_data, nlp, n_iter=20 ):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add label to the model, which is always 'selected_text'\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    # we are interested only in 'ner' pipeline\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n#     nlp.begin_training()\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        optimizer = nlp.begin_training()\n\n        for itn in tqdm(range(n_iter)):\n            #shuffle the data\n            random.shuffle(train_data)\n            # making batches of train_data \n            # size of the batch is determined by compounding func, \n            # which yield an infinite series of compounding values. \n            # Each time the generator is called, a value is produced by\n            # multiplying the previous value by the compound rate.\n            # in this case min batch size is 5, max is 500, compound rate of 1.001 per iteration\n            batches = minibatch(train_data, size=compounding(start=4.0, stop=500.0, compound=1.001))    \n            batch_len = 0\n            # dict to store losses info during training\n            losses = {}\n            for batch in batches:\n                batch_len +=1\n                texts, annotations = zip(*batch)\n                # update the model\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,   # dropout rate, preventing overfitting, use default 0.5 rate\n                    losses=losses, \n                    sgd=optimizer #optimizer\n                )\n\n","66297c70":"# creating data in spacy data input format, like this:\n# {\n#     text_string,\n#     entities: [{start_index, end_index, ENTITY}]\n# }\n\ndef preprocess_data(train_df, sentiment):\n    train_data = []\n    for row in train_df.itertuples():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data\n","7c7b8175":"# Traing for positive sentiment\n\nsentiment = 'positive'\n\nprocessed_train_data = preprocess_data(train_df, sentiment)\n# model_path = get_model_out_path(sentiment)","a2861df7":"# create a blank english language model\nmodel_pos = spacy.blank(\"en\")\n# train the data for 4 iterations, more tends to overfit\ntrain(processed_train_data, model_pos, n_iter=4 )","8db3415a":"# Traing for negative sentiment\n\nsentiment = 'negative'\n\nprocessed_train_data = preprocess_data(train_df, sentiment)\n# model_path = get_model_out_path(sentiment)\n","44cf0343":"# create a blank english language model\nmodel_neg = spacy.blank(\"en\")\n# train the data for 4 iterations\ntrain(processed_train_data, model_neg, n_iter=4 )\n\n","e191fd81":"#Ignore neutral sentiment","2e4bedfb":"# Making prediction","f57c6f20":"# pass text into model and return selected_text\ndef predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text\n","5a31f92b":"selected_texts = []\n# loop through the test data and generate predictions\nfor row in test_df.itertuples():\n    text = row.text\n    output_str = \"\"\n    if row.sentiment == 'neutral':\n        selected_texts.append(text)\n    elif row.sentiment == 'positive':\n        selected_texts.append(predict_entities(text, model_pos))\n    else:\n        selected_texts.append(predict_entities(text, model_neg))\n\ndf_submission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')\ndf_submission['selected_text'] = selected_texts\ndf_submission.to_csv(\"submission.csv\", index=False)","61271b90":"# Begin training data"}}