{"cell_type":{"8e7574fb":"code","3e480203":"code","60a278f8":"code","20e59c9f":"code","fb41cbb1":"code","68286fcf":"code","78a2f633":"code","fe8244ad":"code","b7cd83a8":"markdown","8346ad90":"markdown","ea979131":"markdown","d1be4a05":"markdown"},"source":{"8e7574fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter \n\nfrom os import listdir\nfrom os.path import isfile, join\nimport json\nfrom collections import Counter\nimport geopandas \nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        break\n    \n    break \n# Any results you write to the current directory are saved as output.\n\n# Corona = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\ncovResearch = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\npaperName = covResearch[['title', 'cord_uid', 'pmcid','pubmed_id']]\n\n#https:\/\/www.geeksforgeeks.org\/python-pandas-series-str-find\/\n# dropping null value columns to avoid errors \npaperName.dropna(inplace = True) \n  \n# substring to be searched \nsub1 = 'Temperature'\nsub2 = 'temperature'\n\n\n# Alternatively: sub = 'emperature'\nstart = 2\n  \n# creating and passsing series to new column \npaperName[\"Indexes\"]= paperName[\"title\"].str.find(sub1, start) + paperName[\"title\"].str.find(sub2, start) \n# display\npaperName = paperName.drop(paperName[paperName.Indexes < 0].index)\n  \npaperName \n","3e480203":"# https:\/\/www.kaggle.com\/diamazov\/export-usa-names-into-csv\n    \n# import package with helper functions \nimport bq_helper\n\n# create a helper object for this dataset\npaper_names = bq_helper.BigQueryHelper(active_project=\"Temperature-Research-Papers-Covid\", dataset_name=\"paperNames\")\n\n# query and export data \nquery = \"\"\"SELECT title, pmcid, Indexes as number FROM `Temperature-Research-Papers-Covid` GROUP BY title, pmcid, Indexes\"\"\"\npaper_namesfile = paper_names.query_to_pandas_safe(query)\npaper_namesfile.to_csv(\"researchPapersonTemp.csv\")","60a278f8":"def extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results","20e59c9f":"def removeNestings(l): \n    for i in l: \n        if type(i) == list: \n            removeNestings(i) \n        else: \n            output.append(i)  ","fb41cbb1":"DATA_PATH = \"..\/input\/CORD-19-research-challenge\/\"\nfolder_paths = {\"biorxiv_medrxiv\/biorxiv_medrxiv\/\", \"comm_use_subset\/comm_use_subset\/\", \"custom_license\/custom_license\/\", \"noncomm_use_subset\/noncomm_use_subset\/\"} \npdf_pmc = {\"pdf_json\", \"pmc_json\"}\np = \"pmc_json\"\npmcID = paperName[['pmcid']]\npmc_list = pmcID.values.tolist()\n\nonlyfiles = []\ndata_list = []\n\nfor fp in folder_paths:\n    if (p in listdir(DATA_PATH+fp)):\n        onlyfiles = [f for f in listdir(DATA_PATH+fp+p) if isfile(join(DATA_PATH+fp+ p, f))]\n        for n in onlyfiles:\n            paper_id = re.sub('.xml.json$', '', n)\n            #for y in pmc_list:\n            if any(paper_id in s for s in pmc_list):\n                with open(f'{DATA_PATH}{fp}pmc_json\/{n}') as json_data:\n                    this_file = json.load(json_data)\n                data_list.append(extract_values(this_file, \"text\"))\n                paper_text = data_list\n        \n    \noutput = []\nremoveNestings(data_list)       \n#for l in this_file:\n                    #if (string pmcID in l):\n                        #data_list.append(l)\n                        \n","68286fcf":"#Wordloud: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n#Stopwords: https:\/\/programminghistorian.org\/en\/lessons\/counting-frequencies#removing-stop-words\nfrom wordcloud import WordCloud, STOPWORDS # Making wordcloud\nfrom PIL import Image    # Imports PIL module, to see and scan images  \n\nimport matplotlib.pyplot as plt \n\n#def list2string(l2s):\n#    textSTR = \" \"   #making the text into string\n#    return (textSTR.join(l2s))\n\npaper_textSTR = ' '.join(map(str, paper_text)) \n\n\nfilteredWords = set(STOPWORDS)\nfilteredWords.update([\"may\",\"Figure\", \"Fig\", \"et al\", \"showed\", \"one\", \"two\", \"three\", \"mean\", \"table\", \"using\", \"well\", \"result\", \"model\"])\n                      \nwordcloud = WordCloud(stopwords=filteredWords,\n                          background_color='white', \n                      max_words=300\n                         ).generate(paper_textSTR)\n\n\nplt.clf()\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\n","78a2f633":"#https:\/\/stackoverflow.com\/questions\/20723133\/remove-a-list-of-stopwords-from-a-counter-in-python\n#https:\/\/stackoverflow.com\/questions\/56558006\/counter-that-returns-the-10-most-common-words\n\n# This didnt work, Ignore this but an attempt was made and I might want to make it work in the future. this function is not in the paper \n\ndef removeStopwords(wordlist, stopwords):\n    return [w for w in wordlist if w not in stopwords]\n\nwordCounter = \"\".join(map(str, paper_textSTR))\n\nwordCounterSTR = removeStopwords(wordCounter, filteredWords)\nprint(wordCounterSTR)\n\n# split() returns list of all the words in the string \nsplit_it = wordCounterSTR.split() \n  \n# Pass the split_it list to instance of Counter class. \nCounter = Counter(split_it) \n\n# most_common() produces k frequently encountered \n# input values and their respective counts. \n = Counter.most_common(10) \n  \n#print(most_occur) ","fe8244ad":"DATA_PATH = \"..\/input\/CORD-19-research-challenge\/\"\nfolder_paths = {\"biorxiv_medrxiv\/biorxiv_medrxiv\/\", \"comm_use_subset\/comm_use_subset\/\", \"custom_license\/custom_license\/\", \"noncomm_use_subset\/noncomm_use_subset\/\"} \npdf_pmc = {\"pdf_json\", \"pmc_json\"}\n\nonlyfiles = []\ndata_list = []\n\nfor fp in folder_paths:\n    for p in pdf_pmc:\n        if (p in listdir(DATA_PATH+fp)):\n            onlyfiles = [f for f in listdir(DATA_PATH+fp+p) if isfile(join(DATA_PATH+fp+ p, f))]\n            for n in onlyfiles:\n                with open(f'{DATA_PATH}{fp}{p}\/{n}') as json_data:\n                    this_file = json.load(json_data)\n\n                this_file = extract_values(this_file, 'title')\n                for l in this_file:\n                    if (\"Effect of temperature and relative humidity on\" in l):\n                        data_list.append(l)\n                        print(data_list)\n\n\noutput = []\nremoveNestings(data_list)","b7cd83a8":"Top ","8346ad90":"Papers that has the word temperatures in their title. This might not be fully encompass an entire document but it does highlight papers that focus on the correlation between COVID-19 and temperature. ","ea979131":"Wordclouds","d1be4a05":"Counter"}}