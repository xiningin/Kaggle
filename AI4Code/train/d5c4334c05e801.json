{"cell_type":{"4be5c6ca":"code","b10531f0":"code","990d57ba":"code","69a2a943":"code","9bc77d13":"code","b235cba3":"code","d8fda964":"code","3102b3c6":"code","ca7507cb":"code","2e7b1361":"code","9c76f5c6":"code","c265c6f0":"code","3b6004d3":"code","fe58c403":"code","61273fda":"code","79e343b5":"code","7889935f":"code","693505e5":"code","f53e870c":"code","8171cb73":"code","12dd1684":"code","629a8b47":"code","41e78705":"code","65a26e74":"markdown","05b71576":"markdown","00adaad1":"markdown","fd3d178f":"markdown","78e62899":"markdown"},"source":{"4be5c6ca":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nfrom xgboost import XGBRegressor\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping","b10531f0":"X = pd.read_parquet(\"..\/input\/medallion-monthly-test\/train.parquet\")\nX_test = pd.read_parquet(\"..\/input\/medallion-monthly-test\/test.parquet\")\n\nY = X.pop(\"view_count\")","990d57ba":"X","69a2a943":"X_test","9bc77d13":"features = [\"categoryId\", \"trending_date\", \"comments_disabled\", \"ratings_disabled\", \"publishedAt\"]","b235cba3":"X[\"trending_date_year\"] = [i.year for i in X[\"trending_date\"]]\nX[\"trending_date_month\"] = [i.month for i in X[\"trending_date\"]]\nX[\"trending_date_day\"] = [i.day for i in X[\"trending_date\"]]\n\nX[\"publishedAt_year\"] = [i.year for i in X[\"publishedAt\"]]\nX[\"publishedAt_month\"] = [i.month for i in X[\"publishedAt\"]]\nX[\"publishedAt_day\"] = [i.day for i in X[\"publishedAt\"]]\n\nX_test[\"trending_date_year\"] = [i.year for i in X_test[\"trending_date\"]]\nX_test[\"trending_date_month\"] = [i.month for i in X_test[\"trending_date\"]]\nX_test[\"trending_date_day\"] = [i.day for i in X_test[\"trending_date\"]]\n\nX_test[\"publishedAt_year\"] = [i.year for i in X_test[\"publishedAt\"]]\nX_test[\"publishedAt_month\"] = [i.month for i in X_test[\"publishedAt\"]]\nX_test[\"publishedAt_day\"] = [i.day for i in X_test[\"publishedAt\"]]\n\n\ndel(features[1])\ndel(features[-1])\n\nfeatures.append(\"trending_date_year\")\nfeatures.append(\"trending_date_month\")\nfeatures.append(\"trending_date_day\")\n\nfeatures.append(\"publishedAt_year\")\nfeatures.append(\"publishedAt_month\")\nfeatures.append(\"publishedAt_day\")\n\nX = X[features]\nX_test = X_test[features+[\"id\"]]","d8fda964":"X_test","3102b3c6":"X","ca7507cb":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y)","2e7b1361":"X_train[:10]","9c76f5c6":"def score(X_train=X_train, X_valid=X_valid, Y_train=Y_train, Y_valid=Y_valid, n_estims=10, max_depth=5):\n    model = XGBRegressor(n_estimators=n_estims, max_depth=max_depth)\n\n    model.fit(X_train, Y_train)\n    \n    return mean_absolute_error(Y_valid, model.predict(X_valid))","c265c6f0":"n_estims = [1, 10, 50, 100, 150, 200, 300, 400, 500]\n\nerrors = []\n\nfor i in n_estims:\n    score_ = score(n_estims=i)\n    print(f\"Estim: {i}, Score: {score_}\")\n    errors.append(score_)\n    \nplt.figure(figsize=(16, 9))\n\nsns.lineplot(data=errors)","3b6004d3":"# max_depths = [1, 5, 10, 15, 20, 25, 30, 35, 40, 50]\n\n# errors = []\n\n# for i in max_depths:\n#     score_ = score(n_estims=500, max_depth=i)\n#     print(f\"Estim: {i}, Score: {score_}\")\n#     errors.append(score_)\n    \n# plt.figure(figsize=(16, 9))\n\n# sns.lineplot(data=errors)\n\n# I commented this code because it's takes a long time. So, the best max_depth is 10","fe58c403":"modelXG = XGBRegressor(n_estimators=500, max_depth=10, random_state=0, n_jobs=4, learning_rate=0.01)\n\nmodelXG.fit(X_train, Y_train, early_stopping_rounds=5, eval_set=[(X_valid, Y_valid)], verbose=10)","61273fda":"mean_squared_error(Y_valid, modelXG.predict(X_valid))","79e343b5":"modelXG.fit(X, Y)","7889935f":"from tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping","693505e5":"early_stop = EarlyStopping(\n    min_delta=0.001,\n    patience=5,\n    restore_best_weights=True\n)","f53e870c":"model = keras.Sequential([\n    BatchNormalization(input_shape=[len(X.columns)])\n    Dense(128, activation='relu', input_shape=[len(X.columns)]),\n    Dropout(0.2)\n    Dense(1, activation='linear')\n])\n\nmodel.compile(\n    loss='mae',\n    optimizer='adam'\n)","8171cb73":"hist = model.fit(X_train.astype(np.int32), Y_train, epochs=50, validation_data=(X_valid.astype(np.int32), Y_valid), callbacks=[early_stop])","12dd1684":"plt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])","629a8b47":"predictions1 = modelXG.predict(X_test.drop(\"id\", axis=1))\npredictions2 = model.predict(X_test.drop(\"id\", axis=1).astype(np.int32))\n\npreds = []\n\nfor i in range(len(predictions1)):\n    preds.append(np.array([predictions1[i], predictions2[i]]).mean()[0])\n   \nprint(preds[0])\n\ndf = pd.DataFrame({\n    \"id\": X_test[\"id\"],\n    \"view_count\": np.array(preds).astype(np.int32)\n}).to_csv(\"submission.csv\", index=False)","41e78705":"pd.read_csv(\".\/submission.csv\")","65a26e74":"# Data preparation","05b71576":"# Feature engineering","00adaad1":"# Create a model","fd3d178f":"# Creating an ensemble with XGBoost and Deep Learning (Fully-conected network)","78e62899":"# Now creating predictions for test set"}}