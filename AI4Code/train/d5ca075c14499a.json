{"cell_type":{"457b6c8b":"code","98c0b78b":"code","4a9ccbba":"code","f5383fbc":"code","564df274":"code","e630fdbf":"code","f4383a98":"code","f2c7c5e3":"code","b4aa15a3":"code","69e4d53e":"code","c0889aa1":"code","1ccb657e":"code","3879d2f0":"code","9e974721":"code","412570a7":"code","7073e6a8":"code","da6e0dba":"code","9102d111":"code","b8e58c74":"code","32b8c805":"code","6c53bbe3":"code","7e2a6883":"code","991d8ce6":"code","35c40fd9":"code","c83596f5":"code","8249fac9":"code","c55111a3":"code","bfa6c3cb":"code","5f6ac40f":"code","177ae9e8":"code","dec5e5e2":"code","ad2ba82a":"code","0bd787b8":"code","4538ab63":"code","ae6c79f3":"code","ced16c7a":"code","bea1e431":"code","3b9b37dd":"code","d3b561ce":"code","5e955e4a":"code","f18cd9eb":"code","258925f8":"code","5aad792b":"code","795a79f5":"code","8fda3431":"code","de9e4510":"code","d64a49bf":"code","a507a9e7":"code","7c1ff4ea":"code","e4063c94":"code","b228d82e":"code","1c1bfaf6":"code","e8241ba7":"code","475b5e1e":"code","402b115a":"code","5b134c2d":"code","17d3b176":"code","953b3739":"code","5145dea8":"code","bad72cb7":"code","5fdb2340":"code","ca94d444":"code","88c36f2f":"code","1460c59c":"code","935d867e":"code","31bc3c01":"code","48033c11":"code","8c7a344b":"code","9e5033f7":"code","f2f9aa00":"code","0cb3088f":"code","5888c4e4":"code","3090ba1c":"code","d9a28ad9":"code","0074792f":"code","063b7c37":"code","77bc545e":"code","6c5c4918":"code","c586d4d5":"code","43732fb8":"code","44b01e96":"code","45ba6cb2":"code","de388319":"code","44639026":"code","01321648":"code","7acb8a05":"code","ce7b9b45":"code","884c802d":"code","26374b55":"code","5306f724":"code","8aeced97":"code","e34961f3":"code","510e21f1":"code","58378a29":"code","46782090":"markdown","81bf7efc":"markdown","b7d31de7":"markdown","0b541511":"markdown","10eba479":"markdown","45209a94":"markdown","b77d7ac5":"markdown","f7ba8f58":"markdown","b45a685b":"markdown","b9bc44d0":"markdown","840758c5":"markdown","882cfd37":"markdown","d857cea3":"markdown","e7c2aaa3":"markdown","89d99a60":"markdown","d06d80b9":"markdown","e34d74d9":"markdown","d74224b8":"markdown","87a676cb":"markdown","c4ed9213":"markdown"},"source":{"457b6c8b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","98c0b78b":"import pandas as pd\nfrom math import log\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\nfrom sklearn.feature_selection import mutual_info_regression, SelectFromModel\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom math import ceil\nfrom sklearn.utils.validation import check_is_fitted, column_or_1d\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\nimport time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\n\nwarnings.filterwarnings('ignore')","4a9ccbba":"def evaluation_metric(predicted_value, actual_value) -> float:\n    '''\n        Evaluation of predicted result against actual result using RMSE \n        between the logarithm of the predicted value and logarithm of observed value\n\n        Parameters\n            ----------\n            Predicted value : array\n                the predicted value from the model\n\n            actual value: array\n                the actual observed value\n\n            Returns\n            -------\n            float\n                root mean square error between the logarithm of the predicted value and logarithm of actual value\n\n    '''\n    rmse = mean_squared_error(actual_value,predicted_value, squared=False)\n    return rmse\n\ndef verify_remodelling(remodelled_year:int, built_year:int):\n    '''\n        Compute if remodelling has been done on a house\n\n        Parameters\n            ----------\n            remodelled_year : int\n                the year remodel was done on the house\n\n            built_year: int\n                the year the house was built\n\n            Returns\n            -------\n            bool\n                returns True if remodel was done, False for None and -1 if discrepancy with the values\n    '''\n    if remodelled_year > built_year:\n        return True\n    elif remodelled_year == built_year:\n        return False\n    else:\n        return -1\n\n\ndef selling_period(month_sold: int) -> str:\n    '''\n        Compute selling season\n    '''\n    if month_sold >= 3 and month_sold <= 5:\n        return 'spring'\n    elif month_sold >= 6 and month_sold <= 8:\n        return 'summer'\n    elif month_sold >= 9 and month_sold <= 11:\n        return 'autumn'\n    else:\n        return 'winter'\n\n\nRANDOM_STATE = 0","f5383fbc":"train_set = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","564df274":"train_set.describe()","e630fdbf":"''' \n    Ordinal Ranking Meaning\n\n    Ex\tExcellent\n    Gd\tGood\n    TA\tTypical\/Average\n    Fa\tFair\n    Po\tPoor\n\n'''\nms_sub_class = {20:'1-STORY 1946 & NEWER ALL STYLES',\n                30:'1-STORY 1945 & OLDER',\n                40:'1-STORY W\/FINISHED ATTIC ALL AGES',\n                50:'1-1\/2 STORY FINISHED ALL AGES',\n                60: '2-STORY 1946 & NEWER',\n                70: '2-STORY 1945 & OLDER', \n                75:'2-1\/2 STORY ALL AGES', \n                80:'SPLIT OR MULTI-LEVEL', \n                85:'SPLIT FOYER',\n                90: 'DUPLEX - ALL STYLES AND AGES', \n                120: '1-STORY PUD (Planned Unit Development) - 1946 & NEWER', \n                150:'1-1\/2 STORY PUD - ALL AGES',\n                160:'2-STORY PUD - 1946 & NEWER', \n                180:'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER', \n                190:'2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\nordinal_ranking = {\n                    'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0, 'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, 'Reg':3, 'IR1':2, 'IR2':1,'IR3':0,'AllPub':4, 'NoSewr':3, \n                   'NoSeWa':2,'ELO':1, 'Gtl':3, 'Mod':2, 'Sev':1, 'Av':3, 'Mn':2, 'No':1, 'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':2, 'LwQ':1, 'Unf':-1, 'Typ':8, 'Min1':7, \n                   'Min2':6, 'Mod':5,'Maj1':4, 'Maj2':3, 'Sev':2, 'Sal':1, 'Fin':2, 'RFn':1\n                  }\n\ncontinious_features =['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea',\n                      'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea','MiscVal','LotFrontage', 'age_of_building','total_square_footage',\n                      'total_area_of_house']\nnorminal_features = ['MSSubClass', 'MSZoning', 'Street', 'LandContour', 'LotConfig','Neighborhood','Condition1','Condition2', 'BldgType','HouseStyle','RoofStyle', 'RoofMatl',\n                    'Exterior1st', 'Exterior2nd','MasVnrType','Foundation','Heating','CentralAir','GarageType','GarageYrBlt','PavedDrive', 'SaleType',\n                    'SaleCondition','Electrical', 'Alley', 'MiscFeature', 'remodelling_done', 'selling_season']\ndiscrete_features = ['YearBuilt','YearRemodAdd','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars',\n                     'MoSold', 'YrSold']\nordinal_cat_features = ['LotShape', 'Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond', 'BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC'\n                        ,'KitchenQual', 'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence']\nordinal_num_features = ['OverallQual','OverallCond' ]","f4383a98":"train_set['total_square_footage'] = train_set['TotalBsmtSF'] + train_set['GrLivArea']\ntest_set['total_square_footage'] = test_set['TotalBsmtSF'] + test_set['GrLivArea']\n\n\ntrain_set['remodelling_done'] = train_set.apply(lambda x: verify_remodelling(x['YearRemodAdd'], x['YearBuilt']), axis=1)\ntest_set['remodelling_done'] = test_set.apply(lambda x: verify_remodelling(x['YearRemodAdd'], x['YearBuilt']), axis=1)\n\n\ntrain_set['selling_season'] = train_set.apply(lambda x: selling_period(x['MoSold']), axis=1)\ntest_set['selling_season'] = test_set.apply(lambda x: selling_period(x['MoSold']), axis=1)\n\n\ntrain_set['total_area_of_house'] = train_set['TotalBsmtSF'] + train_set['1stFlrSF'] + train_set['2ndFlrSF']\ntest_set['total_area_of_house'] = test_set['TotalBsmtSF'] + test_set['1stFlrSF'] + test_set['2ndFlrSF']\n\n\ntrain_set['age_of_building'] = train_set['YearBuilt'].apply(lambda x: pd.datetime.now().year - x)\ntest_set['age_of_building'] = test_set['YearBuilt'].apply(lambda x: pd.datetime.now().year - x)\n","f2c7c5e3":"train_set[ordinal_cat_features].isnull().sum()","b4aa15a3":"n_cols = 3\nn_rows = ceil(len(ordinal_cat_features)\/n_cols)\ncounter = 1\n\nfig = plt.figure(figsize=(20,40))\nfor col in ordinal_cat_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.title('{}'.format(col))\n    plt.xlabel(col)\n    g = sns.countplot(train_set[col])\n    for p in g.patches:\n        x = p.get_bbox().get_points()[:,0]\n        y = p.get_bbox().get_points()[1,1]\n        g.annotate('{:.2g}%'.format(100.*y\/len(train_set[col])), (x.mean(), y), ha='center', va='bottom')\n    counter += 1\n\nplt.show();","69e4d53e":"fig = plt.figure(figsize=(20,40))\nfor col in ordinal_num_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.title('{}'.format(col))\n    plt.xlabel(col)\n    g = sns.countplot(train_set[col])\n    for p in g.patches:\n        x = p.get_bbox().get_points()[:,0]\n        y = p.get_bbox().get_points()[1,1]\n        g.annotate('{:.2g}%'.format(100.*y\/len(train_set[col])), (x.mean(), y), ha='center', va='bottom')\n    counter += 1\n\nplt.show();","c0889aa1":"for col in ordinal_cat_features:\n    train_set[col] = train_set[col].map(ordinal_ranking)\n    test_set[col] = test_set[col].map(ordinal_ranking)\n    train_set[col] = train_set[col].fillna(0)\n    test_set[col] = test_set[col].fillna(0)","1ccb657e":"train_set[continious_features].isnull().sum()","3879d2f0":"test_set[continious_features].isnull().sum()","9e974721":"n_cols = 3\nn_rows = ceil(len(continious_features)\/n_cols)\ncounter = 1\n\nfig = plt.figure(figsize=(20,40))\nfor col in continious_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.xlabel(col)\n    g = plt.hist(train_set[col])\n    \n    counter += 1\n\nplt.show();","412570a7":"imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nfor col in continious_features:\n    imp_mean.fit(train_set[col].values.reshape(-1,1))\n    train_set[col] = imp_mean.transform(train_set[col].values.reshape(-1,1))\n    test_set[col] = imp_mean.transform(test_set[col].values.reshape(-1,1))\n    train_set[col] =  np.log(train_set[col] + 1)\n    test_set[col] = np.log(test_set[col] + 1)","7073e6a8":"n_cols = 3\nn_rows = ceil(len(continious_features)\/n_cols)\ncounter = 1\n\nfig = plt.figure(figsize=(20,40))\nfor col in continious_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.xlabel(col)\n    g = plt.hist(train_set[col])\n    \n    counter += 1\n\nplt.show();","da6e0dba":"n_cols = 3\nn_rows = ceil(len(norminal_features)\/n_cols)\ncounter = 1\n\nfig = plt.figure(figsize=(30,40))\nfor col in norminal_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.xlabel(col)\n    g = sns.countplot(train_set[col])\n    for p in g.patches:\n        x = p.get_bbox().get_points()[:,0]\n        y = p.get_bbox().get_points()[1,1]\n        g.annotate('{:.2g}%'.format(100.*y\/len(train_set[col])), (x.mean(), y), ha='center', va='bottom')\n    counter += 1\n\nplt.show();","9102d111":"train_set[norminal_features].isnull().sum()","b8e58c74":"test_set[norminal_features].isnull().sum()","32b8c805":"for col in norminal_features:\n    train_set[col] = train_set[col].astype(str)\n    test_set[col] = test_set[col].astype(str)\n    train_set[col] = train_set[col].fillna('Unknown')\n    test_set[col] = test_set[col].fillna('Unknown')","6c53bbe3":"ohe = OneHotEncoder(handle_unknown = \"ignore\", sparse=False)\nohe_train_set = pd.DataFrame(ohe.fit_transform(train_set[norminal_features]))\nohe_test_set= pd.DataFrame(ohe.transform(test_set[norminal_features]))\nohe_train_set.index = train_set.index\nohe_test_set.index = test_set.index","7e2a6883":"train_set = train_set.drop(norminal_features, axis = 1)\ntest_set = test_set.drop(norminal_features, axis = 1)","991d8ce6":"train_base = train_set.copy()\ntest_base = test_set.copy()\ntrain_set = pd.concat([train_set, ohe_train_set], axis=1)\ntest_set = pd.concat([test_set, ohe_test_set], axis =1)","35c40fd9":"test_set[discrete_features].isnull().sum()","c83596f5":"n_cols = 3\nn_rows = ceil(len(discrete_features)\/n_cols)\ncounter = 1\n\nfig = plt.figure(figsize=(30,40))\nfor col in discrete_features:\n    plt.subplot(n_rows, n_cols, counter)\n    plt.xlabel(col)\n    g = sns.countplot(train_set[col])\n    for p in g.patches:\n        x = p.get_bbox().get_points()[:,0]\n        y = p.get_bbox().get_points()[1,1]\n        g.annotate('{:.2g}%'.format(100.*y\/len(train_set[col])), (x.mean(), y), ha='center', va='bottom')\n    counter += 1\n\nplt.show();","8249fac9":"train_set[discrete_features].isnull().sum()\ntest_set[discrete_features].isnull().sum()\nfor col in discrete_features:\n    test_set[col] = test_set[col].fillna(0)","c55111a3":"fig = plt.figure(figsize=(10,10))\nplt.xlabel('Sale Price Distribution')\nplt.hist(train_set['SalePrice']);","bfa6c3cb":"sale_price = np.log(train_set['SalePrice'] + 1)\nplt.xlabel('Sale Price (Log) Distribution')\nplt.hist(sale_price);","5f6ac40f":"corr_matrix = train_set.corr()\ncorr_cols = corr_matrix.nlargest(25, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_set[corr_cols].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    fig = plt.figure(figsize=(25,15))\n    sns.set(font_scale=1.25)\n    hm = sns.heatmap(cm, mask=mask, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corr_cols.values, xticklabels=corr_cols.values)\nplt.show()","177ae9e8":"corr_cols_small = corr_matrix.nsmallest(25, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_set[corr_cols_small].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    fig = plt.figure(figsize=(25,15))\n    sns.set(font_scale=1.25)\n    hm = sns.heatmap(cm, mask=mask, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corr_cols_small.values, \n                     xticklabels=corr_cols_small.values)\nplt.show()","dec5e5e2":"correlation_features =  corr_cols.append(corr_cols_small).drop('SalePrice')","ad2ba82a":"target = sale_price\ntest_set_Id = test_set['Id']\ntrain = train_set.drop(columns=['SalePrice', 'Id'])\ntest_set = test_set.drop(columns = 'Id')","0bd787b8":"model_rmse_val = []\nmodel_rmse_train = []\nmodel_type = []\nmodel_train_time = []\nmodel_pred_time = []","4538ab63":"train_x, val_x, train_y, val_y = train_test_split(train, target, train_size = 0.8, random_state = RANDOM_STATE)","ae6c79f3":"std_scaler = StandardScaler()\nstd_scaler.fit(train_x)\ntrain_x = std_scaler.transform(train_x)\nval_x = std_scaler.transform(val_x)\ntest_set = std_scaler.transform(test_set)\n","ced16c7a":"base_features = ['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea',\n                      'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea','MiscVal','LotFrontage', 'age_of_building','total_square_footage',\n                      'total_area_of_house','YearBuilt','YearRemodAdd','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars',\n                     'MoSold', 'YrSold','LotShape', 'Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond', 'BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC'\n                        ,'KitchenQual', 'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','OverallQual','OverallCond' ]","bea1e431":"train_x_base, val_x_base, train_y_base, val_y_base = train_test_split(train[base_features], target, train_size = 0.75, random_state = RANDOM_STATE)","3b9b37dd":"std_scaler_base = StandardScaler()\nstd_scaler_base.fit(train_x_base)\ntrain_x_base = std_scaler_base.transform(train_x_base)\nval_x_base = std_scaler_base.transform(val_x_base)","d3b561ce":"logreg = LinearRegression()\nstart_time = time.time()\nlogreg.fit(train_x_base, train_y_base)\ntime_lapse = time.time() - start_time\nmodel_type.append('LogReg Baseline')\nmodel_train_time.append(time_lapse)\ntime_lapse\n","5e955e4a":"logreg_pred_train = logreg.predict(train_x_base)\nlogreg_train_rmse = evaluation_metric(logreg_pred_train, train_y_base)\nmodel_rmse_train.append(logreg_train_rmse)\nlogreg_train_rmse","f18cd9eb":"start_time = time.time()\nlogreg_pred_val = logreg.predict(val_x_base)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","258925f8":"%time logreg_val_rmse = evaluation_metric(logreg_pred_val, val_y_base)","5aad792b":"model_rmse_val.append(logreg_val_rmse)\nlogreg_val_rmse","795a79f5":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","8fda3431":"train_x_corr, val_x_corr, train_y_corr, val_y_corr = train_test_split(train[correlation_features], target, train_size = 0.8, random_state = RANDOM_STATE)\nlogreg_corr = LinearRegression()\nstart_time = time.time()\nlogreg_corr.fit(train_x_corr, train_y_corr)\ntime_lapse = time.time() - start_time\nmodel_type.append('LogReg - Correlation Features')\nmodel_train_time.append(time_lapse)\ntime_lapse","de9e4510":"logreg_corr_pred_train = logreg_corr.predict(train_x_corr)\nlogreg_corr_train_rmse = evaluation_metric(logreg_corr_pred_train, train_y_corr)\nmodel_rmse_train.append(logreg_corr_train_rmse)\nlogreg_corr_train_rmse","d64a49bf":"start_time = time.time()\nlogreg_corr_pred_val = logreg_corr.predict(val_x_corr)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","a507a9e7":"logreg_corr_val_rmse = evaluation_metric(logreg_corr_pred_val, val_y_corr)\nmodel_rmse_val.append(logreg_corr_val_rmse)\nlogreg_corr_val_rmse","7c1ff4ea":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","e4063c94":"def make_mi_scores(train_data, target_data):\n    mi_scores = mutual_info_regression(train_data, target_data)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=train.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(train_x, train_y)\nmi_scores","b228d82e":"mi_features = mi_scores[mi_scores > 0.1].index.values","1c1bfaf6":"train_x_mi, val_x_mi, train_y_mi, val_y_mi = train_test_split(train[mi_features], target, train_size = 0.8, random_state = RANDOM_STATE)\nlogreg_mi = LinearRegression()\nstart_time = time.time()\nlogreg_mi.fit(train_x_mi, train_y_mi)\ntime_lapse = time.time() - start_time\nmodel_type.append('LogReg - Mutual Information Features')\nmodel_train_time.append(time_lapse)\ntime_lapse","e8241ba7":"logreg_mi_pred_train = logreg_mi.predict(train_x_mi)\nlogreg_mi_train_rmse = evaluation_metric(logreg_mi_pred_train, train_y_mi)\nmodel_rmse_train.append(logreg_mi_train_rmse)\nlogreg_mi_train_rmse","475b5e1e":"start_time = time.time()\nlogreg_mi_pred_val = logreg_mi.predict(val_x_mi)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","402b115a":"logreg_mi_val_rmse = evaluation_metric(logreg_mi_pred_val, val_y_mi)\nmodel_rmse_val.append(logreg_mi_val_rmse)\nlogreg_mi_val_rmse","5b134c2d":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","17d3b176":"lreg = Lasso(random_state = RANDOM_STATE)\nstart_time = time.time()\nlreg.fit(train_x, train_y)\ntime_lapse = time.time() - start_time\nmodel_type.append('Lasso')\nmodel_train_time.append(time_lapse)\ntime_lapse","953b3739":"lreg_pred_train = lreg.predict(train_x)\nlreg_pred_train_rmse = evaluation_metric(lreg_pred_train, train_y)\nmodel_rmse_train.append(lreg_pred_train_rmse)\nlreg_pred_train_rmse","5145dea8":"start_time = time.time()\nlreg_pred = lreg.predict(val_x)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","bad72cb7":"lreg_rmse = evaluation_metric(lreg_pred, val_y)\nmodel_rmse_val.append(lreg_rmse)\nlreg_rmse","5fdb2340":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","ca94d444":"rfr=RandomForestRegressor(max_features=0.4, n_estimators=1000, random_state=RANDOM_STATE)\nstart_time = time.time()\nrfr.fit(train_x, train_y)\ntime_lapse = time.time() - start_time\nmodel_type.append('Random Forest')\nmodel_train_time.append(time_lapse)\ntime_lapse","88c36f2f":"rfr_pred_train = rfr.predict(train_x)\nrfr_pred_train_rmse = evaluation_metric(rfr_pred_train, train_y)\nmodel_rmse_train.append(rfr_pred_train_rmse)\nrfr_pred_train_rmse","1460c59c":"start_time = time.time()\nrfr_pred_val = rfr.predict(val_x)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)\n","935d867e":"rfr_rmse = evaluation_metric(rfr_pred_val, val_y)\nmodel_rmse_val.append(rfr_rmse)\nrfr_rmse","31bc3c01":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","48033c11":"mnet = MLPRegressor(hidden_layer_sizes=(200,200,100, 100, 50, 25), activation = 'logistic', alpha = 0.001, max_iter = 1000, early_stopping = True,  random_state = RANDOM_STATE)\nstart_time = time.time()\nmnet.fit(train_x, train_y)\ntime_lapse = time.time() - start_time\nmodel_type.append('Neural Net')\nmodel_train_time.append(time_lapse)\ntime_lapse\n","8c7a344b":"mnet_pred_train = mnet.predict(train_x)\nmnet_pred_train_rmse = evaluation_metric(mnet_pred_train, train_y)\nmodel_rmse_train.append(mnet_pred_train_rmse)\nmnet_pred_train_rmse","9e5033f7":"start_time = time.time()\nmnet_pred_val = mnet.predict(val_x)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","f2f9aa00":"mnet_rmse = evaluation_metric(mnet_pred_val, val_y)\nmodel_rmse_val.append(mnet_rmse)\nmnet_rmse","0cb3088f":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","5888c4e4":"params = {'n_estimators': 1000,\n          'max_depth': 5,\n          'min_samples_split': 6,\n          'learning_rate': 0.01,\n          'loss': 'ls',\n          'max_features':0.25,\n          'random_state':RANDOM_STATE}\ngbm = GradientBoostingRegressor(**params)\nstart_time = time.time()\ngbm.fit(train_x, train_y)\ntime_lapse = time.time() - start_time\nmodel_type.append('Gradient Boosting')\nmodel_train_time.append(time_lapse)\ntime_lapse","3090ba1c":"gbm_pred_train = gbm.predict(train_x)\ngbm_pred_train_rmse = evaluation_metric(gbm_pred_train, train_y)\nmodel_rmse_train.append(gbm_pred_train_rmse)\ngbm_pred_train_rmse","d9a28ad9":"start_time = time.time()\ngbm_pred_val = gbm.predict(val_x)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","0074792f":"gbm_rmse = evaluation_metric(gbm_pred_val, val_y)\nmodel_rmse_val.append(gbm_rmse)\ngbm_rmse","063b7c37":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","77bc545e":"param = {'n_estimators': 1000,\n          'max_depth': 5,\n          'learning_rate': 0.01,\n          'random_state':RANDOM_STATE}\nxgb = XGBRegressor(**param)\nstart_time = time.time()\nxgb.fit(train_x, train_y,  eval_set=[(train_x, train_y), (val_x, val_y)], early_stopping_rounds = 5, eval_metric = 'rmse')\ntime_lapse = time.time() - start_time\nmodel_type.append('XGBoost')\nmodel_train_time.append(time_lapse)\ntime_lapse","6c5c4918":"xgb_pred_train = xgb.predict(train_x)\nxgb_pred_train_rmse = evaluation_metric(xgb_pred_train, train_y)\nmodel_rmse_train.append(xgb_pred_train_rmse)\nxgb_pred_train_rmse","c586d4d5":"start_time = time.time()\nxgb_pred_val = xgb.predict(val_x)\ntime_lapse = time.time() - start_time\nmodel_pred_time.append(time_lapse)","43732fb8":"xgb_rmse = evaluation_metric(xgb_pred_val, val_y)\nmodel_rmse_val.append(xgb_rmse)\nxgb_rmse","44b01e96":"result = pd.DataFrame(\n    {\n        'feature': model_type,\n        'train time': model_train_time,\n        'validation prediction time':model_pred_time,\n        'rmse train':model_rmse_train,\n        'rmse validation':model_rmse_val\n    }\n)\nresult.sort_values(by='rmse validation')","45ba6cb2":"test_pred = gbm.predict(test_set)\ntest_result = pd.DataFrame(\n    {\n        'Id': test_set_Id,\n        'SalePrice': np.exp(test_pred),\n    })","de388319":"test_result.to_csv('submission_gbm.csv', index=False)","44639026":"p_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\np_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","01321648":"def create_new_columns(dataset):\n    dataset['total_square_footage'] = dataset['TotalBsmtSF'] + dataset['GrLivArea']\n    dataset['remodelling_done'] = dataset.apply(lambda x: verify_remodelling(x['YearRemodAdd'], x['YearBuilt']), axis=1)\n    dataset['selling_season'] = dataset.apply(lambda x: selling_period(x['MoSold']), axis=1)\n    dataset['total_area_of_house'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']\n    dataset['age_of_building'] = dataset['YearBuilt'].apply(lambda x: pd.datetime.now().year - x)\n    return dataset\n\ndef map_ordinal_features(dataset):\n    for col in ordinal_cat_features:\n        dataset[col] = dataset[col].map(ordinal_ranking)\n        dataset[col] = dataset[col].fillna(0)\n    return dataset\n\ndef log_continious(dataset):\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    for col in continious_features:\n        imp_mean.fit(dataset[col].values.reshape(-1,1))\n        dataset[col] = imp_mean.transform(dataset[col].values.reshape(-1,1))\n        dataset[col] =  np.log(dataset[col] + 1)\n    return dataset\n    \ndef nominal_one_hot(dataset):\n    for col in norminal_features:\n        dataset[col] = dataset[col].astype(str)\n        dataset[col] = dataset[col].fillna('Unknown')\n\n    return dataset\n        \ndef discrete_fill(dataset):\n    for col in discrete_features:\n        dataset[col] = dataset[col].fillna(0)\n    return dataset\n        \ndef target_feature(train):\n    target = np.log(train['SalePrice'] + 1)\n    return target","7acb8a05":"target = target_feature(p_train)","ce7b9b45":"pipes = Pipeline(steps = [('new_columns', FunctionTransformer(create_new_columns)),\n                         \n                          ('continious_cleanup', FunctionTransformer(log_continious)),\n                          ('nominal_filling', FunctionTransformer(nominal_one_hot)),\n                          ('discrete_filling',FunctionTransformer(discrete_fill)),\n                          ('ordinal_cleanup', FunctionTransformer(map_ordinal_features)),\n                         ])\npipes.fit(p_train)","884c802d":"p_train = pipes.transform(p_train)","26374b55":"p_test = pipes.transform(p_test)","5306f724":"p_test_Id = p_test['Id']\np_train = p_train.drop(columns=['SalePrice', 'Id'])\np_test = p_test.drop(columns = 'Id')\np_train_x, p_val_x, p_train_y, p_val_y = train_test_split(p_train, target, train_size=0.8, test_size=0.2, random_state = RANDOM_STATE)","8aeced97":"p_test.shape","e34961f3":"categorical_transformer = OneHotEncoder(handle_unknown = \"ignore\", sparse=False)\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\npreprocessor = ColumnTransformer(remainder='passthrough',\n    transformers=[\n        ('num', numeric_transformer, selector(dtype_exclude=\"object\")),\n        ('cat', categorical_transformer,  selector(dtype_include=\"object\"))\n    ])","510e21f1":"params = {'n_estimators': 1000,\n          'max_depth': 5,\n          'min_samples_split': 6,\n          'learning_rate': 0.01,\n          'loss': 'ls',\n          'max_features':0.25,\n          'random_state':RANDOM_STATE}\nmodel_gbm = GradientBoostingRegressor(**params)\n\ngbm_pipeline = Pipeline(steps=[\n                            ('preprocess', preprocessor),\n                            ('model', model_gbm)\n                             ])\n\n\ngbm_pipeline.fit(p_train_x, p_train_y)\n\ngbm_preds = gbm_pipeline.predict(p_val_x)\n\n# Evaluate the model\nscore = evaluation_metric(gbm_preds, p_val_y)\nprint('RMSE:', score)","58378a29":"p_test_gbm = gbm_pipeline.predict(p_test)\ntest_result = pd.DataFrame(\n    {\n        'Id': p_test_Id,\n        'SalePrice': np.exp(p_test_gbm),\n    })\ntest_result.to_csv('pipeline_gbm.csv', index=False)\n","46782090":"# Baseline Model - Linear Regression\nThe baseline model for this project is Linear Regression with minimal alteration to the features. The features selected are numeric features","81bf7efc":"# OVERVIEW\n\n\nThe aim of the project is to develop a predictive model that predicts final prices of homes. This project is a submission for Kaggle House prices competition and Turing College to demostrate my knowledge of python and Machine Learning. \n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n\n\n\nhttp:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt has feature explanation\n\n# GOALS and METRICS\n\n\nThe goal is to predict the sales price for each house. For each Id in the test set, the value of SalePrice variable must be predicted. The predicted values for test is evaluated using Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. ","b7d31de7":"## Engineering New Columns","0b541511":"### Feature Importance - Mutual Information\nThe features are weighted using sklearn mutual_info_regression and the features with weight greater than 0.1 are selected","10eba479":"## Nominal Features\n\nMissing values in Nominal features are inputted with 'Unknown' and One Hot encoder is done for all nominal features","45209a94":"# Neural Network","b77d7ac5":"## Discrete Features\nDiscrete values have missing values only in few columns in the test dataset, the values are imputted with 0","f7ba8f58":"# EXPLORATION","b45a685b":"## Feature Scaling and Spliting\nThe train dataset is splitted into training and validation, then scalled","b9bc44d0":"## Continious Features\nSince there are few missing values in continious features, the missing value will be filled with simple imputter. The continious data are converted to log to address skewness\n","840758c5":"# XGBoost","882cfd37":"# Linear Regression with Selected Features\n\nFeature importance will be computed to select features to be used in Linear Regression\n### Correlation\nThe correlation selected features are used in Linear Regression","d857cea3":"## RANDOM FOREST MODEL\nUsing Random forest model","e7c2aaa3":"## Ordinal Features\nFor columns with ordinal categorical ranking, they will be mapped to numerical values that corresponds to the weight of the value. For missing values, are filled with 0 which is a class for missing information","89d99a60":"# Lasso","d06d80b9":"## Correlation\n\nCorrelation matrix  is computed for all features, the top 25 positive and negative columns are selected for modelling","e34d74d9":"# CONCLUSION\nBased on the evaluation table, Gradient Boosting model will be used for submission and pipeline will be created for duture iterations.","d74224b8":"## Target Feature\nSince sale price is left-skewed, the log of sale price is used for normal distribution","87a676cb":"# Gradient Boosting","c4ed9213":"# FEATURE ANALYSIS\n"}}