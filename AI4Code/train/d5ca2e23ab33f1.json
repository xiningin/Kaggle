{"cell_type":{"28fcf9d4":"code","7f28c603":"code","9616885e":"code","0115bb60":"code","7f124c9b":"code","bcacf28c":"code","9cad8888":"code","829fbcc1":"code","c850bf02":"code","9a8410d9":"code","3b0c19f1":"code","09530322":"code","2d060a47":"code","5e9ee099":"code","5b76532a":"code","0fe68157":"code","8dfcb722":"code","9b1c6a67":"code","3680e4f1":"code","dc2425ae":"code","261ae915":"code","6fa56996":"code","c92e9ed8":"code","5eeec37c":"code","61803cd5":"code","14df505d":"code","5ba7217c":"code","823fe773":"code","510b76c9":"code","96780e81":"code","5b458c99":"code","b72a1ec9":"code","16551a0d":"code","f7e31fba":"code","74ed6295":"code","1b1bbbbe":"code","23c5bd19":"code","2ba87167":"code","81a56556":"code","445f1403":"code","ad32dfe4":"code","86e4b149":"code","933d0815":"code","b58c4ca6":"code","f349f82b":"code","0fdba66b":"code","8df66f82":"code","8cc364c1":"code","cda1768c":"code","26b02e18":"code","fc1df056":"code","922a2635":"code","5852c1b4":"code","bdc9db6f":"code","9bc8e89c":"markdown","6d73626e":"markdown","b31cf3be":"markdown","ef0fc767":"markdown","37148bcd":"markdown","5c945e5a":"markdown","4c68de33":"markdown","1fcf1261":"markdown","65b5880a":"markdown","22aae39e":"markdown","0deed7b6":"markdown","02d00de9":"markdown","4e825840":"markdown","27f4214e":"markdown"},"source":{"28fcf9d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting graphs\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7f28c603":"import re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords, brown\nfrom sklearn.metrics import recall_score, f1_score, accuracy_score, confusion_matrix, precision_score","9616885e":"dataset = pd.read_csv('..\/input\/sentiment-analysis-data\/training_data.csv')\ndataset.drop(columns = 'Unnamed: 0', axis = 1, inplace = True)","0115bb60":"extra_data = pd.read_csv('..\/input\/data-for-train\/X_train2.csv')\nextra_data.drop(columns = 'Unnamed: 0', axis = 1, inplace = True)\ndataset = pd.concat([dataset, extra_data], axis = 0, ignore_index= True, verify_integrity=True)","7f124c9b":"dataset.head()","bcacf28c":"value_freq = dataset['label'].value_counts()\nprint(value_freq)","9cad8888":"aux = plt.bar(x = [1,2,0], height = value_freq, )\nplt.show()","829fbcc1":"# removing stopword from each row of the dataset\ndef preprocess(text):\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = text.lower()\n    text = text.split()\n    ps = PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    return text","c850bf02":"corpus = []\nfor i in range(dataset.shape[0]):\n    corpus.append(preprocess(dataset['text'][i]))","9a8410d9":"#file = open('..\/input\/bag_of_words.txt')\n#bag_of_words = file.read().split(' ')\n#file.close()","3b0c19f1":"#creating a bag of words of 1000 most frequently occuring word and then converting data to array on its basis.\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ntf = TfidfVectorizer(ngram_range = (1,2), stop_words= ['http', 'appl','aapl', 'co', 'tweet', 'rt','amp', 'iphon'], max_features = 1500)","09530322":"# fitting and creating the transformation of corpus to array\ntf.fit(set(corpus))\nX = tf.transform(corpus).toarray()\ny = dataset['label'].values","2d060a47":"bag_of_words = tf.get_feature_names()","5e9ee099":"pos_tweet = X[y == 2]\nneu_tweet = X[y == 1]\nneg_tweet = X[y == 0]","5b76532a":"#creating word cloud for positive, neutral and negtive tweets\nwords_count_positive = {}\nwords_count_neutral = {}\nwords_count_negative = {}\n\ni = 0\nwhile i <X.shape[1]:\n    words_count_positive[bag_of_words[i]] = np.sum(pos_tweet[:,i])\n    words_count_neutral[bag_of_words[i]] = np.sum(neu_tweet[:,i])\n    words_count_negative[bag_of_words[i]] = np.sum(neg_tweet[:,i])\n    \n    i = i+1","0fe68157":"from wordcloud import WordCloud\nwc_pos = WordCloud(background_color= 'white').generate_from_frequencies(words_count_positive)\nwc_neu = WordCloud(background_color= 'white').generate_from_frequencies(words_count_neutral)\nwc_neg = WordCloud(background_color= 'white').generate_from_frequencies(words_count_negative)\n\nplt.figure(figsize = (40,50))\n\nplt.subplot('131')\nplt.title('Positive',fontsize = 30)\nplt.imshow(wc_pos, interpolation=\"bicubic\")\nplt.axis(\"off\")\n\nplt.subplot('132')\nplt.title('Neutral',fontsize = 30)\nplt.imshow(wc_neu, interpolation=\"bicubic\")\nplt.axis(\"off\")\n\nplt.subplot('133')\nplt.title('Negative',fontsize = 30)\nplt.imshow(wc_neg, interpolation=\"bicubic\")\nplt.axis(\"off\")\n\nplt.show()","8dfcb722":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.15, random_state = 32)","9b1c6a67":"def model_info_train(classifier, data_X, data_Y, name):\n    # training set\n    print(\"****training_set****\")\n    print(\"Model is <<\"+name+\">>\")\n    data_Y_pred = classifier.predict(data_X)\n    print(\"accuracy:\",accuracy_score(data_Y, data_Y_pred))\n    print(\"f1 score:\", f1_score(data_Y,data_Y_pred, average = \"macro\")) # unweighted mean,i.e. does not take label imbalance in account\n    print(\"recall:\", recall_score(data_Y,data_Y_pred, average= \"macro\"))\n    print(\"precision:\", precision_score(data_Y, data_Y_pred, average = \"macro\"))\n    \n    # plotting\n    plt.figure(figsize = (10,4))\n    ax1 = plt.subplot('121', )\n    plt.title(\"X_train\")\n    train_freq = {}\n    train_freq['positive'] = np.sum((data_Y == 2)*1)\n    train_freq['neutral'] = np.sum((data_Y == 1)*1)\n    train_freq['negative'] = np.sum((data_Y == 0)*1)\n    \n    lists = sorted(train_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)\n    \n    plt.subplot('122', sharex = ax1, sharey = ax1)\n    plt.title(\"X_train predict\")\n    train_pred_freq = {}\n    train_pred_freq['positive'] = np.sum((data_Y_pred == 2)*1)\n    train_pred_freq['neutral'] = np.sum((data_Y_pred == 1)*1)\n    train_pred_freq['negative'] = np.sum((data_Y_pred == 0)*1)\n    \n    lists = sorted(train_pred_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)\n    \n    ","3680e4f1":"def model_info_validation(classifier, data_X, data_Y, name):\n    # validate set\n    print(\"****validate_set****\")\n    print(\"Model is <<\"+name+\">>\")\n    data_Y_pred = classifier.predict(data_X)\n    print(\"accuracy:\",accuracy_score(data_Y, data_Y_pred))\n    print(\"f1 score:\", f1_score(data_Y,data_Y_pred, average = \"macro\")) # unweighted mean,i.e. does not take label imbalance in account\n    print(\"recall:\", recall_score(data_Y,data_Y_pred, average= \"macro\"))\n    print(\"precision:\", precision_score(data_Y, data_Y_pred, average = \"macro\"))\n    \n    # plotting\n    plt.figure(figsize = (10,4))\n    ax1 = plt.subplot('121')\n    plt.title(\"X_validate\")\n    validate_freq = {}\n    validate_freq['positive'] = np.sum((data_Y == 2)*1)\n    validate_freq['neutral'] = np.sum((data_Y == 1)*1)\n    validate_freq['negative'] = np.sum((data_Y == 0)*1)\n   \n    lists = sorted(validate_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)\n    \n    plt.subplot('122', sharex = ax1, sharey = ax1)\n    plt.title(\"X_validate predict\")\n    validate_pred_freq = {}\n    validate_pred_freq['positive'] = np.sum((data_Y_pred == 2)*1)\n    validate_pred_freq['neutral'] = np.sum((data_Y_pred == 1)*1)\n    validate_pred_freq['negative'] = np.sum((data_Y_pred == 0)*1)\n    \n    lists = sorted(validate_pred_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)","dc2425ae":"def model_info_test(classifier, data_X, data_Y, name):\n    # test set\n    print(\"****test_set****\")\n    print(\"Model is <<\"+name+\">>\")\n    data_Y_pred = classifier.predict(data_X)\n    print(\"accuracy:\",accuracy_score(data_Y, data_Y_pred))\n    print(\"f1 score:\", f1_score(data_Y,data_Y_pred, average = \"macro\")) # unweighted mean,i.e. does not take label imbalance in account\n    print(\"recall:\", recall_score(data_Y,data_Y_pred, average= \"macro\"))\n    print(\"precision:\", precision_score(data_Y, data_Y_pred, average = \"macro\"))\n    \n    # plotting\n    plt.figure(figsize = (10,4))\n    ax1 = plt.subplot('121')\n    plt.title(\"X_test\")\n    test_freq = {}\n    test_freq['positive'] = np.sum((data_Y == 2)*1)\n    test_freq['neutral'] = np.sum((data_Y == 1)*1)\n    test_freq['negative'] = np.sum((data_Y == 0)*1)\n    lists = sorted(test_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)\n    \n    plt.subplot('122', sharex = ax1, sharey = ax1)\n    plt.title(\"X_test predict\")\n    test_pred_freq = {}\n    test_pred_freq['positive'] = np.sum((data_Y_pred == 2)*1)\n    test_pred_freq['neutral'] = np.sum((data_Y_pred == 1)*1)\n    test_pred_freq['negative'] = np.sum((data_Y_pred == 0)*1)\n    lists = sorted(test_pred_freq.items()) # sorted by key, return a list of tuples\n    x, y = zip(*lists)\n    plt.bar(x, y)","261ae915":"from sklearn.naive_bayes import MultinomialNB\nclassifier_naive_bayes = MultinomialNB(fit_prior = False)\nclassifier_naive_bayes.fit(X_train,y_train)","6fa56996":"model_info_train(classifier_naive_bayes, X_train, y_train, 'Naive Bayes Classifier')\n","c92e9ed8":"model_info_validation(classifier_naive_bayes, X_test, y_test, 'Naive Bayes Classifier')","5eeec37c":"from sklearn.linear_model import LogisticRegression\nlg_classifier = LogisticRegression(random_state = 0, max_iter = 10000, solver = 'saga', multi_class = 'multinomial', warm_start = True, n_jobs = -1)\nlg_classifier.fit(X_train, y_train)","61803cd5":"model_info_train(lg_classifier, X_train, y_train, 'Logistic Regression')","14df505d":"model_info_validation(lg_classifier, X_test, y_test, 'Logistic Regression')","5ba7217c":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'linear', random_state = 32, coef0 = 0, gamma = 'auto')\nsvc_classifier.fit(X_train,y_train)","823fe773":"model_info_train(svc_classifier, X_train, y_train, 'Support Vector Machine')","510b76c9":"model_info_validation(svc_classifier, X_test, y_test, 'Support Vector Machine')","96780e81":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state  = 32)\ndt_classifier.fit(X_train,y_train)","5b458c99":"model_info_train(dt_classifier, X_train, y_train, 'Decision Tree Classification')","b72a1ec9":"model_info_validation(dt_classifier, X_test, y_test, 'Decision Tree Classification')","16551a0d":"from sklearn.ensemble import RandomForestClassifier\nrdt_classifier = RandomForestClassifier(n_estimators=10, criterion='entropy',\n                                   n_jobs = -1, random_state = 32)\nrdt_classifier.fit(X_train,y_train)","f7e31fba":"model_info_train(rdt_classifier, X_train, y_train, 'Random Forest Classification')","74ed6295":"model_info_validation(rdt_classifier, X_test, y_test, 'Random Forest Classification')","1b1bbbbe":"from sklearn.linear_model import SGDClassifier\nsgd_classifier = SGDClassifier(max_iter = 10000, n_jobs = -1, random_state = 32, tol = 0.00001,early_stopping = True)\nsgd_classifier.fit(X_train,y_train)","23c5bd19":"model_info_train(sgd_classifier, X_train, y_train, 'Stochastic Gradient Descent Classification')","2ba87167":"model_info_validation(sgd_classifier, X_test, y_test, 'Stochastic Gradient Descent Classification')","81a56556":"dataset_test = pd.read_csv(\"..\/input\/sentiment-analysis-data\/commercial.csv\")\ndataset_test.head()","445f1403":"dataset_test['label'] = 0\nfor i in range(dataset_test.shape[0]):\n    if dataset_test['sentiment'][i] == 'neutral':\n        dataset_test['label'][i] = 1\n    elif dataset_test['sentiment'][i] == 'positive':\n        dataset_test['label'][i] = 2","ad32dfe4":"corpus_test = []\nfor i in range(dataset_test.shape[0]):\n    text = re.sub('[^a-zA-Z]', ' ', dataset_test['text'][i])\n    text = text.lower()\n    text = text.split()\n    ps = PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    corpus_test.append(text)\n\nX_comm_test = tf.transform(corpus_test).toarray()\ny_comm_test = dataset_test['label'].values","86e4b149":"# naive bayes classifier\nmodel_info_test(classifier_naive_bayes, X_comm_test, y_comm_test, 'Naive Bayes classifier')","933d0815":"# logistic Regression\nmodel_info_test(lg_classifier, X_comm_test, y_comm_test, 'logistic classifier')","b58c4ca6":"# SVM classifier\nmodel_info_test(svc_classifier, X_comm_test, y_comm_test, 'SVM classifier')","f349f82b":"# Decision Tree classifier\nmodel_info_test(dt_classifier, X_comm_test, y_comm_test, 'Decision Tree classifier')","0fdba66b":"# Random Forest Classifier\nmodel_info_test(rdt_classifier, X_comm_test, y_comm_test, 'Random Forest classifier')","8df66f82":"# Stochastic Gradient Classifier\nmodel_info_test(sgd_classifier, X_comm_test, y_comm_test, 'Stochastic Gradient classifier')","8cc364c1":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","cda1768c":"Y_train = pd.get_dummies(y_train).values","26b02e18":"# kernel_initializerialising the ANN\nclassifier = Sequential()\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 1024,kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n\n# Adding the hidden layer\nclassifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 256, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 256, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 256, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'softmax'))\n\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","fc1df056":"# Fitting the ANN to the Training set\nclassifier.fit(X_train, Y_train, batch_size = 16,epochs = 100, verbose = 5)","922a2635":"print(\"***Training Set***\")\ny_train_pred = classifier.predict_classes(X_train)\nprint(\"accuracy score :\",accuracy_score(y_train, y_train_pred))\nprint(\"precision score :\",precision_score(y_train, y_train_pred, average = 'macro'))\nprint(\"recall score :\",recall_score(y_train, y_train_pred, average = 'macro'))\nprint(\"f1 score :\",f1_score(y_train, y_train_pred, average = 'macro'))\n\nprint(\"***Test Set***\")\ny_test_pred = classifier.predict_classes(X_test)\nprint(\"accuracy score :\",accuracy_score(y_test, y_test_pred))\nprint(\"precision score :\",precision_score(y_test, y_test_pred, average = 'macro'))\nprint(\"recall score :\",recall_score(y_test, y_test_pred, average = 'macro'))\nprint(\"f1 score :\",f1_score(y_test, y_test_pred, average = 'macro'))","5852c1b4":"# plotting\nplt.figure(figsize = (10,4))\nax1 = plt.subplot('121')\nplt.title(\"X_train\")\nvalidate_freq = {}\nvalidate_freq['positive'] = np.sum((y_train == 2)*1)\nvalidate_freq['neutral'] = np.sum((y_train == 1)*1)\nvalidate_freq['negative'] = np.sum((y_train == 0)*1)\n\nlists = sorted(validate_freq.items()) # sorted by key, return a list of tuples\nx, y = zip(*lists)\nplt.bar(x, y)\n\nplt.subplot('122', sharex = ax1, sharey = ax1)\nplt.title(\"X_train predict\")\nvalidate_pred_freq = {}\nvalidate_pred_freq['positive'] = np.sum((y_train_pred == 2)*1)\nvalidate_pred_freq['neutral'] = np.sum((y_train_pred == 1)*1)\nvalidate_pred_freq['negative'] = np.sum((y_train_pred == 0)*1)\n\nlists = sorted(validate_pred_freq.items()) # sorted by key, return a list of tuples\nx, y = zip(*lists)\nplt.bar(x, y)\n\nplt.show()","bdc9db6f":"# On commercial dataset\nprint(\"***Commercial Test Set***\")\ny_comm_pred = classifier.predict_classes(X_comm_test)\nprint(\"accuracy score :\",accuracy_score(y_comm_test, y_comm_pred))\nprint(\"precision score :\",precision_score(y_comm_test, y_comm_pred, average = 'macro'))\nprint(\"recall score :\",recall_score(y_comm_test, y_comm_pred, average = 'macro'))\nprint(\"f1 score :\",f1_score(y_comm_test, y_comm_pred, average = 'macro'))","9bc8e89c":"# Naive Bayes Classifier","6d73626e":"# Simple perceptron Network","b31cf3be":"# Splitting the data X and Y in training and test set","ef0fc767":"# Logistic Regression","37148bcd":"# Testing on Commercial dataset ","5c945e5a":"# Decision Tree Classification","4c68de33":"### Creating bag of words","1fcf1261":"\nTill now the training data and testing data were from the same dataset. Now we downloaded random tweets and see how our model will perform.","65b5880a":"# Random Forest Classification","22aae39e":"# SVM","0deed7b6":"# Stochastic Gradient Descent Classifier","02d00de9":"The data set is having 3 classes with i.e. positive >2, negative>0 and neutral>1m","4e825840":"### Fuction to show the accuracy, f1 score, recall and precision , also plotting the graph of frequency of 3 classes of tweets for true and prediction","27f4214e":"# Creating word cloud"}}