{"cell_type":{"608195e5":"code","62843dc6":"code","83cd96b0":"code","a51ea63a":"code","aa7f3d81":"code","c6a953fd":"code","dbaad154":"code","098296f9":"code","2ed67d5a":"code","aa3ca7c3":"code","24065834":"code","9c0c4591":"code","8f3a7061":"code","97fa3864":"code","2423489b":"code","2dd2b28c":"code","b51fa2a8":"code","b39cb56a":"code","2ea16c20":"code","be436c39":"code","631c7b03":"code","db6e8c35":"code","542ebf67":"code","2059f18a":"code","8739971b":"code","b01921e2":"code","23155653":"code","ee3b0879":"code","cfbd6e79":"code","d71814f6":"code","cb54e50b":"code","b536b82b":"code","e8f04773":"code","63252fc4":"markdown","cfb99424":"markdown","5badb9dd":"markdown","1c7cbb29":"markdown","ca9c6e1c":"markdown","42e886c2":"markdown","094adb6c":"markdown","108656db":"markdown","91c1bb51":"markdown"},"source":{"608195e5":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nimport copy\nfrom collections import deque\nimport random\nimport time\nfrom tqdm.notebook import tqdm_notebook\nimport gc","62843dc6":"!node --version\n!pip install kaggle-environments -U\nfrom kaggle_environments import make","83cd96b0":"!cp -r ..\/input\/lux-ai-2021\/* .\n!cp -r ..\/input\/baselinemodelisaac150-1\/* .","a51ea63a":"game_state = None\ndef agent(observation, configuration):\n    global game_state\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    actions = []\n\n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    return actions","aa7f3d81":"env = make(\"lux_ai_2021\", configuration={\"seed\": random.randint(0, 100000000), \"loglevel\": 2}, debug=True)\nsteps = env.run([agent, \"simple_agent\"])\nenv.render(mode=\"ipython\", width=1200, height=800)","c6a953fd":"# maybe try to convert to tf function in the future\ndef get_map(game_state):\n    map_size = game_state.map.height\n    resource_map = np.zeros((3, map_size, map_size), np.float32)\n    road_map = np.zeros((1, map_size, map_size), np.float32)\n    players_map = np.zeros((4, map_size, map_size), np.float32)\n    player = game_state.players[0]\n    opponent = game_state.players[1]\n    fuels = np.array([0, 0], np.float32)\n    for y in range(map_size):\n        for x in range(map_size):\n            tile = game_state.map.get_cell(x, y)\n            if not tile.has_resource():\n                if tile.road != 0.0:\n                    road_map[0, y, x] = tile.road\n                continue\n            if(tile.resource.type == 'wood'):\n                resource_map[0, y, x] += tile.resource.amount\n            elif(tile.resource.type == 'coal'):\n                resource_map[1, y, x] += tile.resource.amount\n            else:\n                resource_map[2, y, x] += tile.resource.amount\n    \n    for city in player.cities.values():\n        fuels[0] += city.fuel\n        for city_tile in city.citytiles:\n            players_map[0, city_tile.pos.y, city_tile.pos.x] = 1\n    for city in opponent.cities.values():\n        fuels[1] += city.fuel\n        for city_tile in city.citytiles:\n            players_map[1, city_tile.pos.y, city_tile.pos.x] = 1\n    for unit in player.units:\n        players_map[2, unit.pos.y, unit.pos.x] = 1\n    for unit in opponent.units:\n        players_map[3, unit.pos.y, unit.pos.x] = 1\n    \n    research_points = np.array([player.research_points, opponent.research_points], np.float32)\n    tech_levels = np.array([player.researched_coal(), player.researched_uranium(), opponent.researched_coal(), opponent.researched_uranium()], np.float32)\n    \n    padding = (32-resource_map.shape[1])\/\/2\n    resource_map = np.pad(resource_map, [[0,0], [padding, padding], [padding, padding]])\n    road_map = np.pad(road_map, [[0,0], [padding, padding], [padding, padding]])\n    players_map = np.pad(players_map, [[0,0], [padding, padding], [padding, padding]])\n    \n    return resource_map, road_map, players_map, np.concatenate([fuels, research_points, tech_levels])","dbaad154":"resource_map, road_map, players_map, game_stats = get_map(game_state)","098296f9":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n\ng1 = sns.heatmap(resource_map[0], cmap=\"YlGnBu\", ax=ax1, cbar=False)\ng1.set_title(\"Wood\")\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')\ng2 = sns.heatmap(resource_map[1], cmap=\"YlGnBu\", ax=ax2, cbar=False)\ng2.set_title(\"Coal\")\ng2.set_ylabel('')\ng2.set_xlabel('')\ng2.set_yticks([])\ng2.set_xticks([])\ng2.set(adjustable='box', aspect='equal')\ng3 = sns.heatmap(resource_map[2], cmap=\"YlGnBu\", ax=ax3, cbar=False)\ng3.set_title(\"Uranium\")\ng3.set_ylabel('')\ng3.set_xlabel('')\ng3.set_yticks([])\ng3.set_xticks([])\ng3.set(adjustable='box', aspect='equal')\n\nplt.show()","2ed67d5a":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(road_map[0], cmap=\"YlGnBu\", ax=ax, cbar=False)\ng1.set_title(\"Road\")\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","aa3ca7c3":"fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2)\n\ng1 = sns.heatmap(players_map[0], cmap=\"YlGnBu\", ax=ax1, cbar=False)\ng1.set_title(\"Player's City\")\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(aspect='equal')\ng2 = sns.heatmap(players_map[1], cmap=\"YlGnBu\", ax=ax2, cbar=False)\ng2.set_title(\"Opponent's City\")\ng2.set_ylabel('')\ng2.set_xlabel('')\ng2.set_yticks([])\ng2.set_xticks([])\ng2.set(aspect='equal')\ng3 = sns.heatmap(players_map[2], cmap=\"YlGnBu\", ax=ax3, cbar=False)\ng3.set_title(\"Player's Unit\")\ng3.set_ylabel('')\ng3.set_xlabel('')\ng3.set_yticks([])\ng3.set_xticks([])\ng3.set(aspect='equal')\ng4 = sns.heatmap(players_map[3], cmap=\"YlGnBu\", ax=ax4, cbar=False)\ng4.set_title(\"Opponent's Unit\")\ng4.set_ylabel('')\ng4.set_xlabel('')\ng4.set_yticks([])\ng4.set_xticks([])\ng4.set(aspect='equal')\n\nplt.show()","24065834":"state_map = tf.random.uniform([8, 32, 32])\nstate_map2 = tf.random.uniform([8, 32, 32])\nmse = tf.keras.losses.MeanSquaredError()\nmse(state_map, state_map2)","9c0c4591":"enc = tf.keras.layers.Conv2D(1, (1, 1))\nstate_map = tf.transpose(state_map, perm=[2, 1, 0])\nstate_map = tf.expand_dims(state_map, axis=0)\nstate_map = enc(state_map)\nstate_map.shape\nstate_map = tf.squeeze(state_map, [0])\nstate_map = tf.transpose(state_map, perm=[2, 1, 0])\nstate_map = tf.squeeze(state_map, [0])\n\nstate_map = state_map - tf.math.reduce_min(state_map)\nstate_map = state_map \/ tf.math.reduce_max(state_map)","8f3a7061":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(state_map, cmap=\"YlGnBu\", ax=ax, cbar=True)\ng1.set_ylabel('')\ng1.set_xlabel('')\n# g1.set_yticks([])\n# g1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","97fa3864":"distance_mask = np.zeros(shape=(32, 32))\nx = 20\ny = 0\nfor i in range(len(distance_mask)):\n    for j in range(len(distance_mask)):\n        distance_mask[i, j] = 1 - ((i - y)**2 + (j - x)**2) \/ 2048","2423489b":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(distance_mask, cmap=\"YlGnBu\", ax=ax, cbar=True)\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","2dd2b28c":"distance_masked = distance_mask * state_map","b51fa2a8":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(distance_masked, cmap=\"YlGnBu\", ax=ax, cbar=True)\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","b39cb56a":"map_size = 24\nmap_mask = np.zeros((32, 32))\nmap_mask[(32-map_size)\/\/2:(32+map_size)\/\/2, (32-map_size)\/\/2:(32+map_size)\/\/2] = 1.0","2ea16c20":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(map_mask, cmap=\"YlGnBu\", ax=ax, cbar=True)\ng1.set_ylabel('')\ng1.set_xlabel('')\ng1.set_yticks([])\ng1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","be436c39":"map_masked = distance_masked * map_mask","631c7b03":"fig, ax = plt.subplots()\n\ng1 = sns.heatmap(map_masked, cmap=\"YlGnBu\", ax=ax, cbar=True)\ng1.set_ylabel('')\ng1.set_xlabel('')\n# g1.set_yticks([])\n# g1.set_xticks([])\ng1.set(adjustable='box', aspect='equal')","db6e8c35":"index = tf.argmax(tf.reshape(map_masked, 1024))\nx, y = index % 32, index \/\/ 32\nmap_masked.numpy()[y, x] == tf.reduce_max(map_masked)","542ebf67":"# class CartNet(tf.keras.Model):\n#     def __init__(self):\n#         super(CartNet, self).__init__()\n        \n#         self.dense1 = tf.keras.layers.Dense(128)\n#         self.bn1 = tf.keras.layers.BatchNormalization()\n        \n#         self.dense2 = tf.keras.layers.Dense(64)\n#         self.bn2 = tf.keras.layers.BatchNormalization()\n\n#         self.dense3 = tf.keras.layers.Dense(32)\n#         self.bn3 = tf.keras.layers.BatchNormalization()\n        \n#         self.dense4 = tf.keras.layers.Dense(5)\n    \n#     def call(self, input_tensor, state_map, training=False):\n#         x = tf.keras.layers.InputLayer((1, 270))(input_tensor)\n#         x = self.dense1(x)\n#         x = self.bn1(x, training)\n#         x = tf.nn.leaky_relu(x)\n        \n#         x = self.dense2(x)\n#         x = self.bn2(x, training)\n#         x = tf.nn.leaky_relu(x)\n        \n#         x = self.dense3(x)\n#         x = self.bn3(x, training)\n#         x = tf.nn.leaky_relu(x)\n        \n#         output_vector = self.dense4(x)\n        \n#         return output_vector\n\nclass WorkerNet(tf.keras.Model):\n    def __init__(self):\n        super(WorkerNet, self).__init__()\n        \n        self.dense1 = tf.keras.layers.Dense(16)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        \n        self.dense2 = tf.keras.layers.Dense(16)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        \n        self.dense3 = tf.keras.layers.Dense(8)\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        \n        self.dense4 = tf.keras.layers.Dense(3, activation='softmax')\n    # rework\n    def call(self, input_tensor, state_map, training=False):\n        x = tf.keras.layers.InputLayer((1, 29))(input_tensor)\n        x = self.dense1(x)\n        x = self.bn1(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        x = self.dense2(x)\n        x = self.bn2(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        x = self.dense3(x)\n        x = self.bn3(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        output_vector = self.dense4(x)\n        \n        return output_vector\n    \n    def move(self, unit_x, unit_y, state_map, training=False):\n        state_map = tf.transpose(state_map, perm=[2, 1, 0])\n        state_map = tf.expand_dims(state_map, axis=0)\n        state_map = self.Conv1(state_map)\n        state_map = tf.transpose(state_map, perm=[0, 3, 2, 1])\n        state_map = tf.squeeze(state_map, [0, 1])\n        state_map = state_map - tf.math.reduce_min(state_map)\n        state_map = state_map \/ tf.math.reduce_max(state_map)\n        \n        distance_mask = np.zeros(shape=(32, 32))\n        for i in range(len(distance_mask)):\n            for j in range(len(distance_mask)):\n                distance_mask[i, j] = 1 - ((i - unit_y)**2 + (j - unit_x)**2) \/ 2048\n        \n        map_mask = np.zeros((32, 32))\n        map_mask[(32-map_size)\/\/2:(32+map_size)\/\/2, (32-map_size)\/\/2:(32+map_size)\/\/2] = 1.0\n\nclass CityNet(tf.keras.Model):\n    def __init__(self):\n        super(CityNet, self).__init__()\n        \n        self.dense1 = tf.keras.layers.Dense(16)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        \n        self.dense2 = tf.keras.layers.Dense(16)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        \n        self.dense3 = tf.keras.layers.Dense(8)\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        \n        self.dense4 = tf.keras.layers.Dense(3, activation='softmax')\n    \n    def call(self, input_tensor, state_map, training=False):\n        x = tf.keras.layers.InputLayer((1, 26))(input_tensor)\n        x = self.dense1(x)\n        x = self.bn1(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        x = self.dense2(x)\n        x = self.bn2(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        x = self.dense3(x)\n        x = self.bn3(x, training)\n        x = tf.nn.leaky_relu(x)\n        \n        output_vector = self.dense4(x)\n        \n        return output_vector","2059f18a":"class ResnetIdentityBlock(tf.keras.Model):\n    def __init__(self, kernel_size, num_filters):\n        super(ResnetIdentityBlock, self).__init__()\n\n        self.conv2a = tf.keras.layers.Conv2D(num_filters, (1, 1))\n        self.bn2a = tf.keras.layers.BatchNormalization()\n\n        self.conv2b = tf.keras.layers.Conv2D(num_filters, kernel_size, padding='same')\n        self.bn2b = tf.keras.layers.BatchNormalization()\n\n        self.conv2c = tf.keras.layers.Conv2D(num_filters, (1, 1))\n        self.bn2c = tf.keras.layers.BatchNormalization()\n\n    def call(self, input_tensor, training=False):\n        x = self.conv2a(input_tensor)\n        x = self.bn2a(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv2b(x)\n        x = self.bn2b(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv2c(x)\n        x = self.bn2c(x, training=training)\n        x += input_tensor\n        return tf.nn.leaky_relu(x)","8739971b":"class DeLux(tf.keras.Model):\n    def __init__(self):\n        super(DeLux, self).__init__()\n\n        self.InputLayer = tf.keras.layers.InputLayer((32, 32, 8))\n        self.ConvPre = tf.keras.layers.Conv2D(2, (1, 1))\n        self.ResBlock1 = ResnetIdentityBlock(7, 2)\n        self.ResBlock2 = ResnetIdentityBlock(7, 2)\n        self.ResBlock3 = ResnetIdentityBlock(7, 2)\n        self.Conv1 = tf.keras.layers.Conv2D(4, (5, 5))\n        self.Conv2 = tf.keras.layers.Conv2D(8, (5, 5))\n        self.MaxPool = tf.keras.layers.MaxPooling2D()\n        self.Flat = tf.keras.layers.Flatten()\n        self.Dense1 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)\n        self.Dense2 = tf.keras.layers.Dense(16, activation=tf.nn.leaky_relu)\n        \n        self.ConvWorker = tf.keras.layers.Conv2D(1, (1, 1))\n        self.BNWorker = tf.keras.layers.BatchNormalization()\n        \n        self.game_stats = None\n        self.state_map = None \n        self.state_vector = None\n        self.history = None\n        self.map_size = 32\n        \n        self.CityNet = CityNet()\n#         self.CityNet.build((1, 18))\n        self.WorkerNet = WorkerNet()\n#         self.WorkerNet.build((1, 22))\n#         self.CartNet = CartNet()\n#         self.CartNet.build((1, 22))\n    \n    def call(self, inputs, training=False):\n        self.game_stats = tf.expand_dims(inputs[1], axis=0)\n        transposed_inputs = tf.transpose(inputs[0], perm=[2, 1, 0])\n        transposed_inputs = tf.expand_dims(transposed_inputs, axis=0)\n        \n#         x = self.InputLayer(transposed_inputs)\n        x = self.ConvPre(transposed_inputs)\n        x = self.ResBlock1(x, training)\n        x = self.ResBlock2(x, training)\n        x = self.ResBlock3(x, training)\n        self.state_map = tf.transpose(tf.squeeze(x, [0]), perm=[2, 1, 0])\n        \n        x = self.Conv1(x)       # (1, 28, 28, 4)\n        x = self.MaxPool(x)     # (1, 14, 14, 4)\n        x = self.Conv2(x)       # (1, 10, 10, 8)\n        x = self.MaxPool(x)     # (1, 5, 5, 8)\n        \n        # transposed_outputs = tf.squeeze(x, [0])\n        # transposed_outputs = tf.transpose(transposed_outputs, perm=[2, 1, 0])\n        \n        x = self.Flat(x)\n        x = self.Dense1(x)\n        self.state_vector = self.Dense2(x)\n        \n        return self.state_vector\n      \n    def reset(self):\n        self.game_stats = None\n        self.state_map = None \n        self.state_vector = None\n        self.history = None\n\n    \n    def city_call(self, input_tensor, training=False):\n        input_tensor = tf.expand_dims(input_tensor, axis=0)\n        input_tensor = tf.concat([self.state_vector, self.game_stats, input_tensor], axis=1)\n        output = self.CityNet(input_tensor,training)\n        return output\n    \n    def worker_call(self, input_tensor, training=False):\n        input_tensor = tf.expand_dims(input_tensor, axis=0)\n        input_tensor = tf.concat([self.state_vector, self.game_stats, input_tensor], axis=1)\n        output = self.WorkerNet(input_tensor, training)\n        return output\n    \n#     def cart_call(self, input_tensor, training=False):\n#         input_tensor = tf.expand_dims(input_tensor, axis=0)\n#         input_tensor = tf.concat([self.state_vector, self.game_stats, input_tensor], axis=1)\n#         output = self.CartNet(input_tensor, self.state_map, self.map_size, training)\n#         return output\n    \n    def worker_move(self, unit_x, unit_y, training=False):\n        unit_x, unit_y = unit_x + (32 - self.map_size) \/\/ 2, unit_y + (32 - self.map_size) \/\/ 2\n        \n        state_map = tf.transpose(self.state_map, perm=[2, 1, 0])\n        state_map = tf.expand_dims(state_map, axis=0)\n        state_map = self.ConvWorker(state_map)\n        state_map = self.BNWorker(state_map)\n        state_map = tf.transpose(state_map, perm=[0, 3, 2, 1])\n        state_map = tf.squeeze(state_map, [0, 1])\n#         state_map = state_map - tf.math.reduce_min(state_map)\n#         state_map = state_map \/ tf.math.reduce_max(state_map)\n        \n        distance_mask = np.zeros(shape=(32, 32))\n        for i in range(len(distance_mask)):\n            for j in range(len(distance_mask)):\n                distance_mask[i, j] = 1 - ((i - unit_y)**2 + (j - unit_x)**2) \/ 2048\n        \n        map_mask = np.zeros((32, 32))\n        map_mask[(32-self.map_size)\/\/2:(32+self.map_size)\/\/2, (32-self.map_size)\/\/2:(32+self.map_size)\/\/2] = 1.0\n        \n        state_map = state_map * distance_mask * map_mask\n        return state_map\n    \n#     def cart_move(self, unit_x, unit_y, training=False):\n#         unit_x, unit_y = unit_x + (32 - self.map_size) \/\/ 2, unit_y + (32 - self.map_size) \/\/ 2\n        \n#         state_map = tf.transpose(self.state_map, perm=[2, 1, 0])\n#         state_map = tf.expand_dims(state_map, axis=0)\n#         state_map = self.ConvWorker(state_map)\n#         state_map = tf.transpose(state_map, perm=[0, 3, 2, 1])\n#         state_map = tf.squeeze(state_map, [0, 1])\n#         state_map = state_map - tf.math.reduce_min(state_map)\n#         state_map = state_map \/ tf.math.reduce_max(state_map)\n        \n#         distance_mask = np.zeros(shape=(32, 32))\n#         for i in range(len(distance_mask)):\n#             for j in range(len(distance_mask)):\n#                 distance_mask[i, j] = 1 - ((i - unit_y)**2 + (j - unit_x)**2) \/ 2048\n        \n#         map_mask = np.zeros((32, 32))\n#         map_mask[(32-self.map_size)\/\/2:(32+self.map_size)\/\/2, (32-self.map_size)\/\/2:(32+self.map_size)\/\/2] = 1.0\n        \n#         state_map = state_map * distance_mask * map_mask\n#         index = tf.argmax(tf.reshape(map_masked, 1024))\n#         x, y = index % 32, index \/\/ 32\n        \n#         return x, y","b01921e2":"class DQNAgent:\n    def __init__(self, saved_weights_path=None):\n        self.memory = deque(maxlen=1800) # maximum of 5 games \n\n        self.gamma = 0.95 # discount factor\n\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.9999\n        self.epsilon_min = 0.001\n\n        self.learning_rate = 0.01\n        self.model = DeLux()\n        self.model.__init__()\n        \n        self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n        self.loss_fn2 = tf.keras.losses.MeanSquaredError()\n        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n        self.map_size = 32\n        if saved_weights_path is not None:\n            self.model((np.zeros((8, 32, 32)), np.zeros((8,))))\n            self.model.load_weights(saved_weights_path)\n    \n    def _get_state(self, game_state):\n        resource_map, road_map, players_map, game_stats = get_map(game_state)\n\n        tf_resource_map = tf.constant(resource_map)\n        tf_road_map = tf.constant(road_map)\n        tf_players_map = tf.constant(players_map)\n        tf_game_stats = tf.constant(game_stats)\n\n        tf_map_states = tf.concat([tf_resource_map, tf_road_map, tf_players_map], axis=0)\n        return (tf_map_states, tf_game_stats)\n  \n    def memorize(self, state, actions, reward, next_state, done):\n#         _state = copy.deepcopy(state)\n#         _next_state = copy.deepcopy(next_state)\n        self.memory.append((state, actions, reward, next_state, done))\n    \n    def save_weights(self, epoch):\n        self.model.save_weights(f'.\/checkpoints\/baseline-model\/checkpoint-{epoch}e')\n    \n    def reset(self):\n        self.model.reset()\n        \n    def act(self, game_state, training=False):\n        actions = []\n        translated_actions = []\n        player = game_state.players[0]\n        tf_state = self._get_state(game_state)\n        self.model.map_size = game_state.map_height\n        self.model(tf_state)\n        \n        for city in player.cities.values():\n            for city_tile in city.citytiles:\n                if not city_tile.can_act():\n                    continue\n                # should provide models the information of the city location and other to make prediction\n                if np.random.rand() <= self.epsilon and training:\n                    output = np.zeros(3)\n                    output[np.random.randint(0, 3)] = 1\n                    output = tf.constant(output)\n                    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n                else:\n                    input_tensor = tf.constant([city_tile.pos.x, city_tile.pos.y], dtype=tf.float32)\n                    output = self.model.city_call(input_tensor)\n                    output = tf.squeeze(output)\n                actions.append((output, \"city\"))\n                if tf.argmax(output) == 0:\n                    translated_actions.append(city_tile.research())\n                elif tf.argmax(output) == 1:\n                    translated_actions.append(city_tile.build_worker())\n                else:\n                    translated_actions.append(city_tile.build_cart())\n        for unit in player.units:\n            if not unit.can_act():\n                continue\n            if unit.is_worker():\n                # exclude transfer function\n                state_map = self.model.worker_move(unit.pos.x, unit.pos.y)\n                if np.random.rand() <= self.epsilon and training:\n                    target_x = np.random.randint(0, self.map_size)\n                    target_y = np.random.randint(0, self.map_size)\n                    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n                else:\n                    index = tf.argmax(tf.reshape(state_map, 1024))\n                    target_x, target_y = index % 32, index \/\/ 32\n                    target_x, target_y = target_x - (32 - self.map_size) \/\/ 2, target_y - (32 - self.map_size) \/\/ 2\n                if unit.pos.x == target_x and unit.pos.y == target_y:\n                    if np.random.rand() <= self.epsilon and training:\n                        output = np.zeros(3)\n                        output[np.random.randint(0, 3)] = 1\n                        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n                    else:\n                        curr_q_value = state_map[unit.pos.y, unit.pos.x].numpy()\n                        input_tensor = tf.constant([curr_q_value, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n                        output = self.model.worker_call(input_tensor)\n                        output = tf.squeeze(output)\n                    actions.append((output, \"worker_action\"))\n                    if tf.argmax(output) == 0:\n                        translated_actions.append(unit.build_city())\n                    elif tf.argmax(output) == 1:\n                        translated_actions.append(unit.pillage())\n                    else:\n                        pass\n                else:\n                    actions.append(((target_x, target_y), \"worker_move\"))\n                    translated_actions.append(annotate.x(target_x, target_y))\n                    translated_actions.append(annotate.line(unit.pos.x, unit.pos.y, target_x, target_y))\n                    translated_actions.append(unit.move(unit.pos.direction_to(Position(target_x, target_y))))\n#             else: \n#                 # exclude transfer function\n#                 if np.random.rand() <= self.epsilon and training:\n#                     target_x = np.random.randint(0, self.map_size)\n#                     target_y = np.random.randint(0, self.map_size)\n#                     q_value = np.random.random()\n#                     self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n#                 else:\n#                     state_map = self.model.cart_move(unit.pos.x, unit.pos.y)\n#                     index = tf.argmax(tf.reshape(state_map, 1024))\n#                     target_x, target_y = index % 32, index \/\/ 32\n#                     target_x, target_y = target_x - (32 - self.map_size) \/\/ 2, target_y - (32 - self.map_size) \/\/ 2\n#                     q_value = tf.reduce_max(state_map)\n#                 if unit.pos.x == target_x and unit.pos.y == target_y:\n#                     pass\n#                 else:\n#                     actions.append(((target_x, target_y), \"cart_move\"))\n#                     translated_actions.append(annotate.x(target_x, target_y))\n#                     translated_actions.append(annotate.line(unit.pos.x, unit.pos.y, target_x, target_y))\n#                     translated_actions.append(unit.move(unit.pos.direction_to(Position(target_x, target_y))))\n        \n        return actions, translated_actions\n  \n    def replay(self, batch_size):\n        \n        loss = 0\n        \n        minibatch = random.sample(self.memory, batch_size)\n\n        for state, actions, reward, next_state, done in tqdm_notebook(minibatch, desc = \"Sampling Progress\"):\n            city_tiles = []\n            workers = []\n#             carts = []\n            player = state.players[0]\n            num_actions = len(actions) if len(actions) != 0 else 1\n        \n            for city in player.cities.values():\n                for city_tile in city.citytiles:\n                    if not city_tile.can_act():\n                        continue\n                    city_tiles.append(city_tile)\n            for unit in player.units:\n                if not unit.can_act():\n                    continue\n                if unit.is_worker():\n                    workers.append(unit)\n#                 else:\n#                     carts.append(unit)\n        \n            if done:\n                target = reward # \/ num_actions\n                tf_state = self._get_state(state)\n                for i in range(len(actions)):\n                    prediction = None\n                    prediction_state_map = None\n                    with tf.GradientTape() as tape:\n                        self.model(tf_state, training=True)\n                        if actions[i][1] == 'city' and city_tiles:\n                            city_tile = city_tiles.pop(0)\n                            input_tensor = tf.constant([city_tile.pos.x, city_tile.pos.y], dtype=tf.float32)\n                            prediction = self.model.city_call(input_tensor, training=True)[0]\n                            target = target_city \n                        elif \"worker\" in actions[i][1]:\n                            unit = workers.pop(0)\n                            prediction_state_map = self.model.worker_move(unit.pos.x, unit.pos.y, training=True)\n                            if actions[i][1] == 'worker_action' and workers:          \n                                curr_q_value = prediction_state_map[unit.pos.y, unit.pos.x].numpy()\n                                input_tensor = tf.constant([curr_q_value, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n                                prediction = self.model.worker_call(input_tensor, training=True)[0]\n#                         elif actions[i][1] == 'carts' and carts:\n#                             unit = carts.pop(0)\n#                             input_tensor = tf.constant([unit.pos.x, unit.pos.y, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n#                             prediction = self.model.cart_call(input_tensor, training=True)[0]\n                        elif actions[i][1] == 'worker_move' and workers:\n                            unit = workers.pop(0)\n                            input_tensor = tf.constant([unit.pos.x, unit.pos.y, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n                            prediction = self.model.worker_call(input_tensor, training=True)[0]\n                            target = target_worker\n                            prediction_state_map = self.model.worker_move(unit.pos.x, unit.pos.y, training=True)\n                        \n                        if prediction is None:\n                            continue\n                        target_actions = prediction.numpy()\n                        target_actions[tf.argmax(actions[i][0]).numpy()] = target\n                        target_actions = tf.constant(target_actions)\n                        loss_value = self.loss_fn(target_actions, prediction)\n                        if prediction_state_map is not None:\n                            target_state_map = prediction_state_map.numpy()\n                            target_state_map[actions[i][0][1], actions[i][0][0]] = target\n                            target_state_map = tf.constant(target_state_map)\n                            loss_value += self.loss_fn2(target_state_map, prediction_state_map)\n                        loss += loss_value\n                    \n                    gradients = tape.gradient(loss_value, tape.watched_variables())\n                    self.optimizer.apply_gradients(\n                        (grad, var)\n                        for (grad, var) in zip(gradients, tape.watched_variables())\n                        if grad is not None)\n            else:\n                next_player = next_state.players[0]\n                target_city = 0\n                target_worker = 0\n#                 target_cart = 0\n                target_move = 0\n                \n                tf_next_state = self._get_state(next_state)\n                self.model(tf_next_state, training=True)\n                \n                for city in next_player.cities.values():\n                    for city_tile in city.citytiles:\n                        if not city_tile.can_act():\n                            continue\n                        input_tensor = tf.constant([city_tile.pos.x, city_tile.pos.y], dtype=tf.float32)\n                        target_city = max(target_city, np.amax(self.model.city_call(input_tensor, training=False).numpy()))\n                for unit in next_player.units:\n                    if not unit.can_act():\n                        continue\n                    if unit.is_worker():\n                        state_map = self.model.worker_move(unit.pos.x, unit.pos.y, training=False)\n                        target_move = tf.reduce_max(state_map)\n                        next_q_value = state_map[unit.pos.y, unit.pos.x].numpy()\n                        input_tensor = tf.constant([next_q_value, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n                        target_worker = max(target_worker, np.amax(self.model.worker_call(input_tensor, training=False).numpy()))\n#                     else:\n#                         input_tensor = tf.constant([unit.pos.x, unit.pos.y, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n#                         target_cart = max(target_cart, np.amax(self.model.cart_call(input_tensor, training=False).numpy()))\n                \n                target_city = (reward + self.gamma * target_city) # \/ num_actions\n                target_worker = (reward + self.gamma * target_worker) # \/ num_actions\n#                 target_cart = (reward + self.gamma * target_cart) \/ num_actions\n                target_move = (reward + self.gamma * target_move) # \/ num_actions\n\n                for i in range(len(actions)):\n                    prediction = None\n                    prediction_state_map = None\n                    with tf.GradientTape() as tape:\n                        tf_state = self._get_state(state)\n                        self.model(tf_state, training=True)\n                        if actions[i][1] == 'city' and city_tiles:\n                            city_tile = city_tiles.pop(0)\n                            input_tensor = tf.constant([city_tile.pos.x, city_tile.pos.y], dtype=tf.float32)\n                            prediction = self.model.city_call(input_tensor, training=True)[0]\n                            target = target_city \n                        elif \"worker\" in actions[i][1]:\n                            unit = workers.pop(0)\n                            prediction_state_map = self.model.worker_move(unit.pos.x, unit.pos.y, training=True)\n                            if actions[i][1] == 'worker_action' and workers:          \n                                curr_q_value = prediction_state_map[unit.pos.y, unit.pos.x].numpy()\n                                input_tensor = tf.constant([curr_q_value, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n                                prediction = self.model.worker_call(input_tensor, training=True)[0]\n                                target = target_worker\n#                         elif actions[i][1] == 'cart' and carts:\n#                             unit = carts.pop(0)\n#                             input_tensor = tf.constant([unit.pos.x, unit.pos.y, unit.get_cargo_space_left(), unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium], dtype=tf.float32)\n#                             prediction = self.model.cart_call(input_tensor, training=True)[0]\n#                             target = target_cart\n                        \n                        if prediction is None:\n                            continue\n                        target_actions = prediction.numpy()\n                        target_actions[tf.argmax(actions[i][0]).numpy()] = target\n                        target_actions = tf.constant(target_actions)\n                        loss_value = self.loss_fn(target_actions, prediction)\n                        if prediction_state_map is not None:\n                            target_state_map = prediction_state_map.numpy()\n                            target_state_map[actions[i][0][1], actions[i][0][0]] = target_move\n                            target_state_map = tf.constant(target_state_map)\n                            loss_value += self.loss_fn2(target_state_map, prediction_state_map)\n                        loss += loss_value\n                    \n                    gradients = tape.gradient(loss_value, tape.watched_variables())\n                    self.optimizer.apply_gradients(\n                        (grad, var)\n                        for (grad, var) in zip(gradients, tape.watched_variables())\n                        if grad is not None)\n        print(f\"Total loss: {loss}\")","23155653":"!ls","ee3b0879":"# define hand code agent\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nimport numpy as np\n\ndirections_dict = {\n    'c': (0,0),\n    's': (0,1),\n    'n': (0,-1),\n    'w': (-1,0),\n    'e': (1,0),\n}\n\ndef get_random_step():\n    return np.random.choice(['s','n','w','e'])\n\ndef find_resources(game_state):\n    resource_tiles: list[Cell] = []\n    width, height = game_state.map_width, game_state.map_height\n    for y in range(height):\n        for x in range(width):\n            cell = game_state.map.get_cell(x, y)\n            if cell.has_resource():\n                resource_tiles.append(cell)\n    return resource_tiles\n\ndef find_closest_resources(pos, player, resource_tiles):\n    closest_dist = math.inf\n    closest_resource_tile = None\n    for resource_tile in resource_tiles:\n        # we skip over resources that we can't mine due to not having researched them\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.COAL and not player.researched_coal(): continue\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.URANIUM and not player.researched_uranium(): continue\n        dist = resource_tile.pos.distance_to(pos)\n        if dist < closest_dist:\n            closest_dist = dist\n            closest_resource_tile = resource_tile\n    return closest_resource_tile, closest_dist\n\ndef find_closest_city_tile(pos, player):\n    closest_city_tile = None\n    closest_dist = math.inf\n    if len(player.cities) > 0:\n        # the cities are stored as a dictionary mapping city id to the city object, which has a citytiles field that\n        # contains the information of all citytiles in that city\n        for k, city in player.cities.items():\n            for city_tile in city.citytiles:\n                dist = city_tile.pos.distance_to(pos)\n                if dist < closest_dist:\n                    closest_dist = dist\n                    closest_city_tile = city_tile\n    return closest_city_tile, closest_dist\n\n# we declare this global game_state object so that state persists across turns so we do not need to reinitialize it all the time\ngame_state = None\ndef agent_teacher(observation, configuration):\n    global game_state\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    actions = []\n\n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    resource_tiles = find_resources(game_state)\n    \n#     # add debug statements like so!\n#     if game_state.turn == 0:\n#         print(f\"Agent is running!\", file=sys.stderr)\n    \n    \"\"\"\n    for unit in player.units:\n        # if the unit is a worker (can mine resources) and can perform an action this turn\n        if unit.is_worker() and unit.can_act():\n            # we want to mine only if there is space left in the worker's cargo\n            if unit.get_cargo_space_left() > 0:\n                # find the closest resource if it exists to this unit\n                closest_resource_tile = find_closest_resources(unit.pos, player, resource_tiles)\n                if closest_resource_tile is not None:\n                    # create a move action to move this unit in the direction of the closest resource tile and add to our actions list\n                    action = unit.move(unit.pos.direction_to(closest_resource_tile.pos))\n                    actions.append(action)\n            else:\n                # find the closest citytile and move the unit towards it to drop resources to a citytile to fuel the city\n                closest_city_tile = find_closest_city_tile(unit.pos, player)\n                if closest_city_tile is not None:\n                    # create a move action to move this unit in the direction of the closest resource tile and add to our actions list\n                    action = unit.move(unit.pos.direction_to(closest_city_tile.pos))\n                    actions.append(action)\n    \"\"\"\n    \n    # max number of units available\n    units_cap = sum([len(x.citytiles) for x in player.cities.values()])\n    # current number of units\n    units  = len(player.units)\n    \n    cities = list(player.cities.values())\n    if len(cities) > 0:\n        city = cities[0]\n        created_worker = (units >= units_cap)\n        for city_tile in city.citytiles[::-1]:\n            if city_tile.can_act():\n                if created_worker:\n                    # let's do research\n                    action = city_tile.research()\n                    actions.append(action)\n                else:\n                    # let's create one more unit in the last created city tile if we can\n                    action = city_tile.build_worker()\n                    actions.append(action)\n                    created_worker = True\n    \n    \n    # we want to build new tiles only if we have a lot of fuel in all cities\n    can_build = True\n    night_steps_left = ((359 - observation[\"step\"]) \/\/ 40 + 1) * 10\n    for city in player.cities.values():            \n        if city.fuel \/ (city.get_light_upkeep() + 70) < min(night_steps_left, 20):\n            can_build = False\n       \n    steps_until_night = 30 - observation[\"step\"] % 40\n    \n    \n    # we will keep all tiles where any unit wants to move in this set to avoid collisions\n    taken_tiles = set()\n    for unit in player.units:\n        # it is too strict but we don't allow to go to the the currently occupied tile\n        taken_tiles.add((unit.pos.x, unit.pos.y))\n    for unit in opponent.units:\n        taken_tiles.add((unit.pos.x, unit.pos.y))\n    for city in opponent.cities.values():\n        for tile in city.citytiles:\n            taken_tiles.add((tile.pos.x, tile.pos.y))\n    \n    # we can collide in cities so we will use this tiles as exceptions\n    city_tiles = {(tile.pos.x, tile.pos.y) for city in player.cities.values() for tile in city.citytiles}\n    \n    \n    for unit in player.units:\n        if unit.can_act():\n            closest_resource_tile, closest_resource_dist = find_closest_resources(unit.pos, player, resource_tiles)\n            closest_city_tile, closest_city_dist = find_closest_city_tile(unit.pos, player)\n            \n            # we will keep possible actions in a priority order here\n            directions = []\n            \n            # if we can build and we are near the city let's do it\n            if unit.is_worker() and unit.can_build(game_state.map) and ((closest_city_dist == 1 and can_build) or (closest_city_dist is None)):\n                # build a new cityTile\n                action = unit.build_city()\n                actions.append(action)  \n                can_build = False\n                continue\n            \n            # base cooldown for different units types\n            base_cd = 2 if unit.is_worker() else 3\n            \n            # how many steps the unit needs to get back to the city before night (without roads)\n            steps_to_city = unit.cooldown + base_cd * closest_city_dist\n            \n            # if we are far from the city in the evening or just full let's go home\n            if (steps_to_city + 3 > steps_until_night or unit.get_cargo_space_left() == 0) and closest_city_tile is not None:\n                directions = [unit.pos.direction_to(closest_city_tile.pos)]\n            else:\n                # if there is no risks and we are not mining resources right now let's move toward resources\n                if closest_resource_dist != 0 and closest_resource_tile is not None:\n                    directions = [unit.pos.direction_to(closest_resource_tile.pos)]\n                    # optionally we can add random steps\n                    for _ in range(1):\n                        directions.append(get_random_step())\n\n            moved = False\n            for next_step_direction in directions:\n                next_step_coordinates = (unit.pos.x + directions_dict[next_step_direction][0],\n                                         unit.pos.y + directions_dict[next_step_direction][1])\n                # make only moves without collision\n                if next_step_coordinates not in taken_tiles or next_step_coordinates in city_tiles:\n                    action = unit.move(next_step_direction)\n                    actions.append(action)\n                    taken_tiles.add(next_step_coordinates)\n                    moved = True\n                    break\n            \n            if not moved:\n                # if we are not moving the tile is occupied\n                taken_tiles.add((unit.pos.x,unit.pos.y))\n    \n    return actions","cfbd6e79":"initial_model_path = None # \"checkpoint-150e\"\n\ngame_state = None\ndef agent(observation, configuration):\n    global game_state\n    dqn_agent = DQNAgent(saved_weights_path=initial_model_path)\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    actions = []\n\n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    actions, translated_actions = dqn_agent.act(game_state, training=False)\n#     print(translated_actions)\n    \n    return translated_actions","d71814f6":"env = make(\"lux_ai_2021\", configuration={\"seed\": 4242, \"loglevel\": 2}, debug=True)\nsteps = env.run([agent, agent_teacher])\nenv.render(mode=\"ipython\", width=1200, height=800)","cb54e50b":"epochs = 150\ninitial_epoch = 0\nbatch_size = 1200\ndifficulty = 0.1\ndqn_agent = DQNAgent(saved_weights_path=initial_model_path)\ngame_state = Game()\navg_score = 0\navg_eps = 0\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 4242, \"loglevel\": 0}, debug=True)\nfor epoch in range(initial_epoch+1, initial_epoch+epochs+1):\n    \n    env.reset()\n    trainer = env.train([None, agent_teacher])\n    obs = trainer.reset()\n    game_state._initialize(env.state[0].observation[\"updates\"])\n    game_state._update(env.state[0].observation[\"updates\"][2:])\n    dqn_agent.reset()\n    \n    player = game_state.players[env.state[0].observation.player]\n    opponent = game_state.players[(env.state[1].observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    prev_reward = 0\n    \n    for episode in range(360):\n\n        actions, translated_actions = dqn_agent.act(game_state, training=True)\n        obs, _, done, _ = trainer.step(translated_actions) # obs, reward, done, info\n        reward = game_state.players[0].city_tile_count * 3 + len(game_state.players[0].units) + game_state.players[0].research_points \/ 100 + obs.step \/ 400\n        reward -= difficulty * (game_state.players[1].city_tile_count * 3 + len(game_state.players[1].units) + game_state.players[1].research_points \/ 100 )\n        \n        game_state_copy = copy.deepcopy(game_state)\n        game_state._update(obs[\"updates\"])\n        next_game_state_copy = copy.deepcopy(game_state)\n        \n        dqn_agent.memorize(game_state_copy, actions, reward-prev_reward, next_game_state_copy, done)\n        prev_reward = reward\n        \n        if done:\n            print(f\"Epoch: {epoch}, Episode: {episode}, Score: {reward:2.3f}, Epsilon: {dqn_agent.epsilon:.6f}\")\n            avg_score += reward\n            avg_eps += episode\n            break\n  \n    if len(dqn_agent.memory) >= batch_size and epoch % 10 == 0:\n        avg_score \/= 10\n        avg_eps \/= 10\n        print(f\"Average Episode: {avg_eps:3.1f}, Average score: {avg_score:2.3f} in the last 10 epochs.\")\n        avg_score, avg_eps = 0, 0\n        print(f\"Replaying from memory buffer...\")\n        dqn_agent.replay(batch_size)\n        gc.collect()\n        \n\n    if epoch % 30 == 0 and epoch > 0:\n        dqn_agent.save_weights(epoch)","b536b82b":"game_state = None\ndef agent(observation, configuration):\n    global game_state\n#     dqn_agent = DQNAgent(saved_weights_path=\"checkpoints\/baseline-model\/checkpoint-150e\")\n#     dqn_agent.epsilon = 0.01\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    actions = []\n    \n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    actions, translated_actions = dqn_agent.act(game_state, training=False)\n    print(translated_actions)\n    \n    return translated_actions","e8f04773":"env = make(\"lux_ai_2021\", configuration={\"seed\": 4242, \"loglevel\": 2}, debug=True)\nsteps = env.run([agent, agent_teacher])\nenv.render(mode=\"ipython\", width=1200, height=800)","63252fc4":"# Data Exploration\n\n- get resource map ( 3 layers \uff09\n- get player and  oppponent cities and units map\n    - player cities, and units, resources, research points","cfb99424":"# Model Building\ntry to build a Conv and dense network based on the given data\n\n## List of expected actions\n- CityTile actions\n1. Build Worker - Build Worker unit on top of this CityTile (cannot build a worker if current number of owned workers + carts equals the number of owned CityTiles)\n1. Build Cart - Build Carts unit on top of this CityTile (cannot build a cart if there are if current number of owned workers + carts equals the number of owned CityTiles)\n1. Research - Increase your team\u2019s Research Points by 1\n\n- Unit actions\n1. Worker\n    - Move(4) - North, East, South, West, Center.\n    - Pillage(1) - Reduce the Road level of the tile the unit is on by 0.5\n    - Transfer(1, we are not using this for now because it involves extra numeric outputs) - Send any amount of a single resource-type from own cargo to another (start-of-turn) adjacent Unit, up to the latter's cargo-capcity. Excess is returned to the original unit.\n    - Build CityTile(1) - Build a CityTile right under this worker\n1. Cart\n    - Move(4) - Move the unit in one of 5 directions, North, East, South, West, Center.\n    - Transfer(1, we are not using this for now because it involves extra numeric outputs) - Send any amount of a single resource-type from own cargo to another (start-of-turn) adjacent Unit","5badb9dd":"# DELUX\n## Backbone\nWe get the map status (of shape (8, 32, 32)) and fit into out backbone model to extract a 64-d state vector.\n## Submodels\nThere is 2 submodels(city tile and worker model, i omit the cart model to ease the training) attach to this backbone which will receive the state vector, game stats and the corresponding state of the unit to determine the action.\n\n## Reward function\nThe reward function is define as \n$ \\text{Reward, }R = \\text{Agent's score} - (\\text{difficulty constant}) * \\text{opponent's score} $ , where difficulty constant is currently set to 0.1 and\n\n$ \\text{Score, }S = 3(\\text{number of city tiles}) + \\text{number of units} + \\text{research points} \/ 100 + (\\text{turns survived}) \/ 360 - (\\text{opponent's score}) $\n\nI think the reward function can be modified or can replaced by a neural network trained based on others' gameplay.\n\n## Replay buffer \nWe save the tuple (state, action, reward, next_state) in a buffer for the model to relearn whether an action is good or bad. ","1c7cbb29":"# Import Libraries","ca9c6e1c":"# Planning Exploration","42e886c2":"# TODO \n- Build planning model for the units\n    - application of some kind of mask [X]\n    - sick annotations [X]\n- Save the current best model (target model), and train the model until it beats the target model (to avoid it learns backward)\n- Define penalty for invalid moves\n    - off map moves\n    - build city without enough resources\n    - collsion or move inside opponent city\n- Batch stuff to accelerate on GPU\n- Self play ? and record winning side's state in the replay buffer?","094adb6c":"# Evaluation Section\nUNDER CONSTRUCTION","108656db":"# Train Loop","91c1bb51":"# Before train test"}}