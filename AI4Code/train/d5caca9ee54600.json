{"cell_type":{"c2d731e9":"code","863bdeed":"code","976cac80":"code","f1735ff0":"code","f28279d5":"code","908d4cce":"code","add1cd87":"code","aa7ace2c":"code","208f713f":"code","93bb5f72":"code","a6f98e78":"code","cd5157b2":"code","fe31bdbc":"code","f1aed44c":"code","aa0b41ad":"code","7dab31d9":"code","55a71880":"code","07a95a77":"code","2bb7ce6b":"code","bfdcae91":"code","56266772":"code","0f0bf725":"code","a9489b4c":"code","bd323903":"code","7abc770b":"code","cabc62bc":"code","d710c0da":"code","19788a09":"code","4e3e1894":"code","0fa90f6a":"code","991b1ccc":"code","d8055904":"code","e59c0a7d":"code","1c86802d":"code","248efcbf":"code","9778d689":"code","70e8d0e7":"code","36c85fb4":"code","f0cdb0de":"code","75ce053a":"code","1020f74e":"code","ab92aabc":"code","03d31f0f":"code","06fd7433":"code","efc7f144":"code","3aaa34cd":"code","2785d6f3":"markdown","9f908a02":"markdown","0b8d08e9":"markdown","ba57aa38":"markdown","b54b8c7f":"markdown","ad688e5a":"markdown","46ccad5f":"markdown","f89d8bd7":"markdown","5ab1a296":"markdown","f3219a30":"markdown","b8cfcac9":"markdown","b43b4e9b":"markdown","3d986050":"markdown","72bf16ad":"markdown","a52f4a5e":"markdown","9896be8f":"markdown","d0417ad0":"markdown","c1d7e9b8":"markdown","1fa2bfa2":"markdown","3037de8d":"markdown","ba075152":"markdown","b8c9e841":"markdown","db633212":"markdown","f001a678":"markdown","0dea0b7c":"markdown","2d0d7b51":"markdown","f646535c":"markdown","36e6d78b":"markdown","edccac42":"markdown","7820e522":"markdown","68b77f3e":"markdown","8375b016":"markdown","ca348943":"markdown","ba2ebe1a":"markdown","483c14d6":"markdown"},"source":{"c2d731e9":"from numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix,roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve, classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb \nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_STATE = 2021","863bdeed":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')\n#train = train[~train.drop('target', axis = 1).duplicated()]\n\nX = pd.DataFrame(train.drop(\"target\", axis = 1))\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","976cac80":"sns.countplot(x = 'target', data= y)","f1735ff0":"top_submission = pd.read_csv(\"..\/input\/tps05-sample-top-submission\/tps-05-sample-submission-001.csv\", index_col = 'id')\n\ntop_submission.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","f28279d5":"# We define X_test for results comparision \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state= RANDOM_STATE)","908d4cce":"def training(model, X_train_oof, y_train_oof, weighted = False, b_type = True, ):\n    test_preds = None\n    test_oof_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 10\n    \n    skf = StratifiedKFold(n_splits = n_splits, shuffle = True,  random_state = 0)\n    for fold, (tr_index , val_index) in enumerate(skf.split(X_train_oof.values , y_train_oof.values)):\n\n        print(f\"\\nFold {fold + 1}\")\n\n        x_train_o, x_val_o = X_train_oof.iloc[tr_index] , X_train_oof.iloc[val_index]\n        y_train_o, y_val_o = y_train_oof.iloc[tr_index] , y_train_oof.iloc[val_index]\n        \n        if weighted:\n            weights_y = weights_df.iloc[tr_index]\n\n        eval_set = [(x_val_o, y_val_o)]\n        \n        if b_type:\n            if weighted:\n                model.fit(x_train_o, y_train_o, eval_set = eval_set, verbose = 500, sample_weight = weights_y)\n            else:\n                model.fit(x_train_o, y_train_o, eval_set = eval_set, verbose = 500)\n        \n        else:\n            model.fit(x_train_o, y_train_o)\n\n        train_preds = model.predict(x_train_o)\n        train_rmse += mean_squared_error(y_train_o ,train_preds , squared = False)\n        print(\"\\n- Training RMSE : \" , mean_squared_error(y_train_o ,train_preds , squared = False))\n\n        val_preds = model.predict(x_val_o)\n        val_rmse += mean_squared_error(y_val_o , val_preds , squared = False)\n        print(\"- Validation RMSE : \" , mean_squared_error(y_val_o , val_preds , squared = False))\n        print('---------------')\n\n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n            test_oof_preds = model.predict_proba(X_test.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n            test_oof_preds += model.predict_proba(X_test.values)\n\n    print(\"\\nAverage Training RMSE : \" , train_rmse \/ n_splits)\n    print(\"Average Validation RMSE : \" , val_rmse \/ n_splits)\n\n    test_preds \/= n_splits\n    test_oof_preds \/= n_splits\n    \n    return test_preds, test_oof_preds","add1cd87":"xgb_model = xgb.XGBClassifier(tree_method='gpu_hist',  eval_metric='mlogloss')\nxgb_preds_weighted, y_pred = training(xgb_model, X_train, y_train)","aa7ace2c":"fig, ax = plt.subplots(figsize=(10,10))\nsorted_idx = xgb_model.feature_importances_.argsort()\nplt.barh(X_train.columns[sorted_idx], xgb_model.feature_importances_[sorted_idx])\nplt.xlabel(\"Xgboost Feature Importance\")","208f713f":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")\n","93bb5f72":"sns.countplot(x = 'target', data= y_test)","a6f98e78":"sns.countplot(x = 'target', data= pd.DataFrame(y_preds, columns=['target']))","cd5157b2":"params = { \n        'objective': 'multiclass', \n        'num_class' : 4, \n        'metric': 'multi_logloss' \n    } ","fe31bdbc":"lgbm_model =  LGBMClassifier(**params)\nlgbm_preds_weighted, y_pred = training(lgbm_model, X_train, y_train)","f1aed44c":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","aa0b41ad":"# I do not use this one - this is only for testing\n\nlargest_class_weight_coef = max(y_train.target.value_counts().values)\/y_train.shape[0]\nprint(largest_class_weight_coef)\n\ndef BalancedSampleWeights(y_train, class_weight_coef):\n    classes = np.unique(y_train, axis = 0)\n    classes.sort()\n    class_samples = np.bincount(y_train)\n    total_samples = class_samples.sum()\n    n_classes = len(class_samples)\n    weights = total_samples \/ (n_classes * class_samples * 1.0)\n    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}\n    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * class_weight_coef\n    sample_weights = [class_weight_dict[i] for i in y_train]\n    return sample_weights\n\nweight = BalancedSampleWeights(y_train.target, largest_class_weight_coef)\nprint(len(weight))\nweights_df= pd.DataFrame(weight, columns=['weight'])","7dab31d9":"# I use only this one ...\n\nfrom sklearn.utils.class_weight import compute_sample_weight\nweights_df= pd.DataFrame(compute_sample_weight(\"balanced\", y_train.target), columns=['weight'])\n\n# Some models eg. RF, LR require weights as dictionary\ndict_weight = dict(enumerate(class_weight.compute_class_weight('balanced', np.unique(y_train.target), y_train.target).flatten(), 0))\nprint(dict_weight)","55a71880":"xgb_model_weighted = xgb.XGBClassifier(tree_method='gpu_hist',  eval_metric='mlogloss')\nxgb_preds_weighted, y_pred = training(xgb_model_weighted, X_train, y_train, weighted = True)","07a95a77":"fig, ax = plt.subplots(figsize=(10,10))\nsorted_idx = xgb_model_weighted.feature_importances_.argsort()\nplt.barh(X_train.columns[sorted_idx], xgb_model_weighted.feature_importances_[sorted_idx])\nplt.xlabel(\"Xgboost Feature Importance\")","2bb7ce6b":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","bfdcae91":"sns.countplot(x = 'target', data= pd.DataFrame(y_preds, columns=['target']))","56266772":"params_weighted = { \n        'objective': 'multiclass', \n        'num_class' : 4, \n        'metric': 'multi_logloss',\n        'class_weight': dict_weight  # or 'balanced'\n    } ","0f0bf725":"lgbm_model_weighted =  LGBMClassifier(**params_weighted)\nlgbm_preds_weighted, y_pred = training(lgbm_model_weighted, X_train, y_train)","a9489b4c":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","bd323903":"from sklearn.calibration import CalibratedClassifierCV\n\ncalibrated_clf = CalibratedClassifierCV(base_estimator = lgbm_model_weighted, cv=3, ensemble = False)\ncalibrated_clf.fit(X_train, y_train)\n\ny_pred = calibrated_clf.predict_proba(X_test)","7abc770b":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","cabc62bc":"from imblearn.under_sampling import RandomUnderSampler\nundersample = RandomUnderSampler(random_state=0)\n\nX_train_under, y_train_under = undersample.fit_resample(X_train, y_train)\nsns.countplot(x = 'target', data= y_train_under)\n\n# Other methods\n# - The Condensed Nearest Neighbor Rule \u2014 CondensedNearestNeighbour\n# - Two modifications of CNN \u2014 TomekLinks\n# - One-Sided Selection \u2014 OneSidedSelection\n# - Edited Nearest Neighbors \u2014 EditedNearestNeighbors\n# - Neighborhood Cleaning Rule \u2014 NeighborhoodCleaningRule","d710c0da":"xgb_model = xgb.XGBClassifier(tree_method='gpu_hist',  eval_metric='mlogloss')\nxgb_preds_weighted, y_pred = training(xgb_model, X_train_under, y_train_under)","19788a09":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","4e3e1894":"sns.countplot(x = 'target', data= pd.DataFrame(y_preds, columns=['target']))","0fa90f6a":"from imblearn.over_sampling import SMOTE\n\nX_train_over, y_train_over = SMOTE().fit_resample(X_train, y_train)\nsns.countplot(x = 'target', data= y_train_over)\n\n# Other methods\n# - Borderline-SMOTE\n# - Borderline-SMOTE SVM\n# - Adaptive Synthetic Sampling (ADASYN)","991b1ccc":"xgb_model = xgb.XGBClassifier(tree_method='gpu_hist',  eval_metric='mlogloss')\nxgb_preds_weighted, y_pred = training(xgb_model, X_train_over, y_train_over)","d8055904":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","e59c0a7d":"sns.countplot(x = 'target', data= pd.DataFrame(y_preds, columns=['target']))","1c86802d":"sns.countplot(x = 'target', data= y_test)","248efcbf":"rf_model = DecisionTreeClassifier()\nrf_preds_weighted, y_pred = training(rf_model, X_train, y_train, False, False)","9778d689":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","70e8d0e7":"rf_model_weighted_balanced = DecisionTreeClassifier(class_weight = 'balanced') # you can use class_weight{\u201cbalanced\u201d, \u201cbalanced_subsample\u201d}, dict or list of dicts, default=None\nrf_preds_weighted, y_pred = training(rf_model_weighted_balanced, X_train, y_train, False, False)","36c85fb4":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","f0cdb0de":"rf_model_weighted = DecisionTreeClassifier(class_weight = dict_weight)\nrf_preds_weighted, y_pred = training(rf_model_weighted, X_train, y_train, False, False)","75ce053a":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","1020f74e":"lr_model = LogisticRegression()\nlr_preds_weighted, y_pred = training(lr_model, X_train, y_train, False, False)","ab92aabc":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","03d31f0f":"lr_model_weighted = LogisticRegression(class_weight = \"balanced\")\nlr_preds_weighted, y_pred = training(lr_model_weighted, X_train, y_train, False, False)","06fd7433":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_test,y_preds)}\\n')\nprint(classification_report(y_test, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","efc7f144":"#sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\n#predictions_df = pd.DataFrame(test_preds, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\n#predictions_df['id'] = sub['id']","3aaa34cd":"#predictions_df.to_csv(\"xgboost_weighted_submission.csv\", index = False)","2785d6f3":"### A. DecisionTreeClassifier baseline","9f908a02":"## 4B. LogisticRegression","0b8d08e9":"### 1A. XGBoost - COST SENSITIVE LEARNING\n\n","ba57aa38":"# We can check this using LGBM as well","b54b8c7f":"# Why this is important?\n\n## **If you look at the world through pink glasses all the colors seem to be pink ...**\n\n# Hypothesis\n* Models on public leaderbord currently look thorough pink glasses","ad688e5a":"You can find the rest of my notebooks with TPS-05 here:\n\n- [Pytorch NN for tabular - step by step](https:\/\/www.kaggle.com\/remekkinas\/tps-5-pytorch-nn-for-tabular-step-by-step)\n- [CNN (2D Convolution) for solving TPS-05](https:\/\/www.kaggle.com\/remekkinas\/cnn-2d-convolution-for-solving-tps-05)\n- [SHAP + LGBM - looking for best features](https:\/\/www.kaggle.com\/remekkinas\/shap-lgbm-looking-for-best-features\/)\n- [HydraNet!! ... Keras Stacked Ensemble ..](https:\/\/www.kaggle.com\/remekkinas\/tps-5-hydranet-keras-stacked-ensemble)","46ccad5f":"### C. DecisionTreeClassifier weighted training (use custom weight params) ","f89d8bd7":"Conclutions:\n- The same here - we have problem with prediction - model can not predict class_1 and class_3 well\n- There is no bigger difference between XGBoost and Light Boost - when optimized (on oryginal dataset) it could improve better results but ... this is in my opiniion marginal gain (model does not see classes correctly) ","5ab1a296":"### 1B. LightGBM - COST SENSITIVE LEARNING","f3219a30":"Uppppssss ..... we are in starting point  ...... Not good ....","b8cfcac9":"# EXPERIMENT 3 - WEIGHTED TRAININGS\n\n# Explore class weight","b43b4e9b":"## But our model see .... through pink glasses ...","3d986050":"- As we can see in TOP Leaderboard submission there is no Class_1 .... almost no Class_4 .... It could be potential problem but we have to examine it more in next sections.\n- Distribution is almost the same comparing with training dataset.","72bf16ad":"# 3. Sampling Techniques - Over Sampling ","a52f4a5e":"### A. LogisticRegression baseline","9896be8f":"## In test dataset (dataset separated from training dataset) we have .... ","d0417ad0":"# Lets look on class distribution on TOP leaderboard submission \nI am not specifically mentioning which results. I took the first ones that are on the TOP list.","c1d7e9b8":"# 1. XGBoost training weighted training ","1fa2bfa2":"# Weighted training - XGBoost, RandomForest etc ....\nSince we have imbalanced dataset I am looking for way to build model using weighted training. I decided to build a notebook in which I compare different models using regular and weighted training.\n\n\n**This notebook is \"in progress\" but I encourage you to follow the progress of the work.** \n\n<div class=\"alert alert-block alert-success\">\n<b>Notebook scope:<\/b>\n    <ul>\n        <li>Curret top LB submission analysis.<\/li>\n        <li>XGBoost - baseline & weighted training comparition - I checked there is no difference between CatBoost and LightBoost (slight differences). You can find my notebok - Keras weighted training section below.<\/li>\n        <li>LightGBM - out of the box + weighted training + probability callibration.<\/li>\n        <li>RandomForest - out of the box & weighted.<\/li>\n        <li>Logistic Regression - out of the box & weighted.<\/li>\n        <li>Sampling Techniques - Under Sampling.<\/li>\n        <li>Sampling Techniques - Over Sampling - Synthetic Minority Oversampling Technique(SMOTE).<\/li>\n        <li>Result comparition and conlusion.<\/li>\n    <\/ul>\n<\/div>","3037de8d":"### B. DecisionTreeClassifier weighted training (use \"balanced\" param) ","ba075152":"# EXPERIMENT #1\n# Lets look on class distribution in TPS-05 TRAINING dataset","b8c9e841":"## Conclusion - distribution is ..... similar (\"pink color\" biased)???? Looks good on public leaderboard ... but what if class distribution in remaining 50% of test dataset )private leaderboard) is different? Still we should see pink color?","db633212":"### B. LogisticRegression weighted training (use \"balanced\" param)","f001a678":"### My personal opinion (summary and conclusions):\n- in this competition the main challange is dealing with data sparsity (not class imbalance) - creating new features, dimention reduction etc.\n- I tried many ways to deal with imbalance - weighted training \/ cost sensitive training (out of the box, custom weights), uder and over sampling, probability calibration. All methods did not  improve results (still looking for solution or your feedback - it is chance that I am wrong).\n- out of the box algorithms predicts class_2 and class_3 but do not predict correctly class_1 and class_4 (probably due to data sparsity)","0dea0b7c":"## Upppssss - something streange has happened ... distribution is ..... not biased but way from source ... Is it not? Am I making a mistake?","2d0d7b51":"# 2. Sampling Techniques - Under Sampling","f646535c":"LightGBM provides two ways - class_weight = 'balanced' || class_weight = custom_weights","36e6d78b":"## SMOTE looks good (class distribution) but .... metrics are .... not good :) !!!","edccac42":"## 4A. RandomForest","7820e522":"# EXPERIMENT #2\n# Lets make a prediction using XGBoost model and test them on train dataset\n# SCORE ON PUBLIC LEADERBOARD -> 1.08684 (without optimization)\n","68b77f3e":"## Upppssss - distribution is ..... pink color - class_2 biased ... Is it not? Am I making a mistake?\n\n","8375b016":"* I do not use any hyperparameter optimization - it will be done as a last step (after all methods development)\n* The weighted training for XGBoost seems to be not working. I am looking for a way to solve the problem.","ca348943":"We will use subset (20%) of training data to see accuracy of model. We want to see how model trains on dataset and then compare results with validation dataset distribution. ","ba2ebe1a":"As we can see .... still model does not performing well .... we can use probability calibration with isotonic regression or logistic regression.","483c14d6":"# 4. BONUS"}}