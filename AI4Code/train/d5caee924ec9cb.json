{"cell_type":{"3128ac10":"code","6ca25295":"code","8fe9f1a4":"code","6cf5c2a2":"code","fb320b15":"code","42f91fa0":"code","86102974":"code","d6a001d9":"code","af11f7ed":"code","c047a6d3":"markdown","cdeccb5f":"markdown","b9b133c9":"markdown","fbae1884":"markdown","9a81d887":"markdown","382fea24":"markdown","0f825c77":"markdown","e85df626":"markdown","b82e8129":"markdown","017d6266":"markdown","a8e70297":"markdown","1fe665ff":"markdown","1e220a38":"markdown"},"source":{"3128ac10":"import numpy as np\nfrom tqdm import tqdm\nfrom scipy.spatial.distance import cosine\nfrom sklearn.decomposition import PCA\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","6ca25295":"f = open('..\/input\/glove.6B.50d.txt')","8fe9f1a4":"embedding_values = {}\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:], dtype='float32')\n    embedding_values[word] = coef","6cf5c2a2":"ix_to_word = {}\nword_to_ix = {}\n\nfor word in tqdm(embedding_values):\n    ix_to_word[len(ix_to_word)] = word\n    word_to_ix[word] = len(ix_to_word)","fb320b15":"def most_similar(word, count):\n    cos = []\n    for i in tqdm(embedding_values):\n        cos.append(cosine(embedding_values[word], embedding_values[i]))\n    temp = cos.copy()\n    temp.sort()\n    for i in range(count):\n        id = cos.index(temp[i])\n        print(ix_to_word[id])","42f91fa0":"most_similar('king', 10)","86102974":"def analogy(word1, word2, word3):\n    embeds = embedding_values[word2]+embedding_values[word3]-embedding_values[word1]\n\n    cos = []\n    for i in tqdm(embedding_values):\n        cos.append(cosine(embeds, embedding_values[i]))\n\n    idx = np.array(cos).argsort()[1]\n    word4 = ix_to_word[idx]\n    \n    return word4\n","d6a001d9":"analogy('man', 'king', 'woman')","af11f7ed":"analogy('india', 'delhi', 'italy')","c047a6d3":"Since now we know the similarity between two words, it could also help us find answer some analogy based questions such as <br> *\u201cman is to king as woman is to ..?\u201d*<br>\nThis was explained in this paper. <a href = \"https:\/\/arxiv.org\/pdf\/1901.09813.pdf\">Link to the paper<\/a>","cdeccb5f":"As we can see, the results are very much as we expected them to be.","b9b133c9":"# Semantic Similarity Among Words","fbae1884":"Here, we will be using pre-trained glove embeddings stored in the file *glove.6B.50d.txt* . Here, 50d represents the dimensions of these embedding vectors. It is a text file which contains words followed by their word vectors.","9a81d887":"## Importing libraries","382fea24":"The above problem can be solved by finding the distance between first two words and based on that distance and the third word we will try to locate our answer in the vector space. Let's try to implement it.","0f825c77":"Let's check for the word *king*.<br>\n\nNote : The function takes some time as we are going through all the words in the dictionary and then matching the similarity for the given word.","e85df626":"This file represents the semantic similarity between words which are attained by finding word embeddings for these words. Word Embeddings are like features or attributes of a word. They store information about the word in a vector space and by semantic similarity we mean that two words having similar meaning(or words which are used mostly in the same context) will be closer in the vector space compared to the words which have dissimilar meaning.","b82e8129":"Now, let's create a function which gives us 'n' most similar words to a given word and see if the results are relevant.","017d6266":"Here, we will be creating two dictionaries which will help us locate words based on their indices.<br>\n>ix_to_word : It stores the index as key and the word as value;<br>\n>word_to_ix : It stores the word as key and corresponding index as its value;","a8e70297":"## Importing Pre-Trained Word Embeddings","1fe665ff":"### Finding analogies","1e220a38":"We will create a dictionary of words and their embeddings by parsing this file. The result is a python dictionary in which words are keys and their corresponding vectors are the values."}}