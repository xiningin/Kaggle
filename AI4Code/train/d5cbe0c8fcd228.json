{"cell_type":{"15bd032a":"code","48ffba35":"code","1378f638":"code","9bff20b2":"code","1ffae190":"code","23e48582":"code","733b55e1":"code","f73f777d":"code","c07a3817":"code","6915432b":"code","4afb26bf":"code","0f79b471":"code","f52cb2e9":"code","64f748e8":"code","51bfce8f":"code","f64409f1":"code","730f71de":"code","2297de82":"code","142faa8b":"code","2c0725cd":"code","1a6758ce":"code","a3f88da0":"code","54ab9b2f":"code","88de318d":"code","f0f798b3":"code","2085b222":"markdown","2d297316":"markdown","ceb3b6cd":"markdown","86fb13c6":"markdown","3b063216":"markdown","9a5e9344":"markdown","2c338918":"markdown","6564d6f2":"markdown","0a11916b":"markdown","0bcab083":"markdown","efd26b09":"markdown","a97e9290":"markdown","cdbf9927":"markdown","e947e5b9":"markdown","43cbcd94":"markdown","10fb0112":"markdown","df53dccf":"markdown","f598ab15":"markdown","cf703b69":"markdown","03db9d2b":"markdown","756a690d":"markdown","1e727fc1":"markdown","d5fd50e9":"markdown","3ba99430":"markdown","ff2e23e6":"markdown"},"source":{"15bd032a":"# Load libraries\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","48ffba35":"# Import data\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\ndf_all = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\ndf_s = [df_train, df_test]","1378f638":"display(df_train.head())\nprint(df_train.info())\ndisplay(df_train.describe()) # numerical values\ndisplay(df_train.describe(include=['O'])) # object columns","9bff20b2":"print(df_test.info())\ndisplay(df_test.head())","1ffae190":"display(df_train[['Age', 'Pclass', 'Fare', 'SibSp', 'Parch', 'Survived']].corrwith(df_train['Age']))\n\nage_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\nprint(age_by_pclass_sex)\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","23e48582":"df_all['Embarked'] = df_all['Embarked'].fillna('S')","733b55e1":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","f73f777d":"df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf_all.loc[df_all.query('Deck == \"T\"').index, 'Deck'] = 'A'\n\ndf_all['Deck'].value_counts()","c07a3817":"df_all.drop(['Cabin'], inplace=True, axis=1)\ndf_train, df_test = df_all.loc[:890], df_all.loc[891:].drop(['Survived'], axis=1)","6915432b":"fig, axs = plt.subplots(2, figsize=(15, 15))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)","4afb26bf":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(15, 15))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","0f79b471":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 15))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","f52cb2e9":"df_all['Fare'] = pd.qcut(df_all['Fare'], 10)","64f748e8":"df_all['Age'] = pd.qcut(df_all['Age'], 10)","51bfce8f":"# Plot to see each group. There is also an unusual group.\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","f64409f1":"df_all['FamilySize'] = df_all['SibSp'] + df_all['Parch'] + 1\n\n# Plot to see each group\nfig, axs = plt.subplots(figsize=(12, 10), ncols=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['FamilySize'].value_counts().index, y=df_all['FamilySize'].value_counts().values, ax=axs[0])\nsns.countplot(x='FamilySize', hue='Survived', data=df_all, ax=axs[1])\n\naxs[0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[1].set_title('Survival Counts in Family Size ', size=20, y=1.05)","730f71de":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\n# Group to lower cardinality\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nfig, axs = plt.subplots(figsize=(15, 10))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs)\naxs.set_title('Title Feature Value Counts After Grouping', size=15, y=1.05)\n\nplt.show()","2297de82":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","142faa8b":"df_all = df_all.drop(['Name', 'Ticket'], axis=1)\ndf_train, df_test = df_all.loc[:890], df_all.loc[891:]\ndfs =[df_train, df_test]","2c0725cd":"ordinal_features = ['Age', 'Fare', 'Sex']\n\nfor df in dfs:\n    for feature in ordinal_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","1a6758ce":"onehot_features = ['Embarked', 'Deck', 'Title']\n\ndf_train = (df_train.join(pd.concat([pd.get_dummies(df_train[feature], prefix=feature) for feature in onehot_features], axis=1))\n          .drop(onehot_features, axis=1))\n\ndf_test = (df_test.join(pd.concat([pd.get_dummies(df_test[feature], prefix=feature) for feature in onehot_features], axis=1))\n          .drop(onehot_features, axis=1))","a3f88da0":"roc, oob, feature_importance_df = {}, {}, pd.DataFrame()\n\nTRAIN, LABEL = df_train.drop(['Survived', 'PassengerId'], axis=1), df_train['Survived']\nTEST = df_test.drop(['PassengerId', 'Survived'], axis=1)\npreds, oof_preds = np.zeros(TRAIN.shape[0]), np.zeros(TEST.shape[0])\n\ncv = StratifiedKFold(n_splits=5, shuffle=True)\nfor i, (train_idx, val_idx) in enumerate(cv.split(TRAIN, LABEL)):\n    X_train, y_train = TRAIN.iloc[train_idx], LABEL.iloc[train_idx]\n    X_val, y_val = TRAIN.iloc[val_idx], LABEL.iloc[val_idx]\n\n    gbm = RandomForestClassifier(criterion='gini', n_estimators=1750, max_depth=7, min_samples_split=6, min_samples_leaf=6, max_features='auto',\n                                 oob_score=True,\n                                 random_state=42,\n                                 n_jobs=-1,\n                                 verbose=False).fit(X_train, y_train)\n\n    y_pred = gbm.predict(X_val)\n    y_pred_proba = gbm.predict_proba(X_val)[:, 1]\n        \n    preds[val_idx] = y_pred_proba\n    oof_preds += gbm.predict_proba(TEST)[:, 1] \/ cv.n_splits # average probability of output\n    \n    roc['FOLD_' + str(i+1)] = roc_auc_score(y_val, y_pred_proba)\n    oob['FOLD_' + str(i+1)] = gbm.oob_score_\n    \n    # For create feature importances\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = TRAIN.columns\n    fold_importance_df[\"importance\"] = gbm.feature_importances_\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print('Fold %2d AUC : %.6f' % (i + 1, roc_auc_score(y_val, y_pred_proba)))\n    print('Fold %2d OOB : %.6f' % (i + 1, gbm.oob_score_), '\\n')\n    \n# Resulting\nroc_auc = roc_auc_score(LABEL, preds)\nprint('Avg AUC score:', roc_auc)\nprint('Out-of-bag score:', np.mean(list(oob.values())))","54ab9b2f":"def display_importances(feature_importance_df_):\n\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:20].index\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(12,8))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n\ndisplay_importances(feature_importance_df_=feature_importance_df)","88de318d":"model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=False) .fit(TRAIN, LABEL)\n\nprint(model.oob_score_)\noutput = model.predict(TEST).astype(int)","f0f798b3":"submission = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': (oof_preds >= 0.5).astype(int)}) # or output as single model\ntime = pd.Timestamp.now().strftime(\"%Y%m%d_%H_%M_%S\")\nsubmission.to_csv(time + '.csv', index=False)","2085b222":"# EDA","2d297316":"### 2.3. Fare\nOnly one missing value in test set. This feature is related to family size (SibSp & Parch) & Pclass -> we could fill with median of this group.","ceb3b6cd":"## 1. k-FOLD approach","86fb13c6":"### 5.2 Categorical features\n* More than half of the passengers boarded from Embarked C has survived\n* Parch and SibSp features show that passengers with only one family member has a higher survival rate","3b063216":"## 1. Overview\nWe could get a first glance at our data including data type, number of unique values, missing properties of these columns, specifically:\n* Training set has 891 rows x 12 columns, test set has 418 rows x 11 columns (except Survived columns - our label)\n* PassengerId: index of an passenger and it does not affect our result\n* Survived: our label (0, 1) -> binary classification. Number of label 1 percentage: 38%\n* Pclass: socio-economic status of the passenger and it is categorical ordinal feature which has 3 unique values (1, 2, 3). More than 50% of Pclass is 3.\n* Name, Sex, Age are self-explanatory. Age feature contains missing values in both training and test set.\n* SibSp and Parch are the total number of the passengers' siblings and spouse and the total number of the passengers' parents and children, respectively.\n* Ticket is the ticket number of the passenger and is a high cardinality feature.\n* Fare is the passenger fare. This column contain only 1 missing value in test set.\n* Cabin is the cabin number of the passenger. This column has high missing ratio and high cardinality.\n* Embarked is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S). Most of the values of this column is S (644\/891 ~ 70%).","9a5e9344":"### 2.4. Cabin\nFirst letter of cabin values could affect to survival rate.\n* Only one person with 1st class has deck T -> replace by A (closet). \n* Missing cabin will be replaced by M\n\nNow we get a column with 8 unique values. We could stop here or dive deeper to find more interesting insights to reduce the number of cardinality. For example, when plotting Deck vs. passenger class, we could see 100% passenger on deck A, B, C are 1st class -> replace by ABC group. Plot Deck vs. survived columns -> deck D, E have the same class distribution and survival rate -> group to DE.","2c338918":"## 2. Single model\nThis approach using full dataset for training","6564d6f2":"### 2.1. Age\n* Fill by median of all passenger's age could be a starting choice. We could fill this field with median of Pclass group because of high correlation coefficient between them. To be more accurate, fillna with median of smaller group such as Sex as second index.","0a11916b":"## 6. Label\/one-hot encoding for categorical features","0bcab083":"## 3. Family size\n- FamilySize = sum of ('SibSp', 'Parch', 1)\n- Family Size is an ordinal feature and could be encode with LabelEncoder\n- Consider group to lower cardinality. ","efd26b09":"## 2. Age","a97e9290":"## 4. Title & IsMarried\nExploiting Name to extract Title and group to lower cardinality.","cdbf9927":"Now we could drop the Cabin feature and split back to train\/test set to go further.","e947e5b9":"# Submission\nMy model reached 0.79 score on leaderboard (top 10%). I have another notebook that do not rely heavily on domain knowledge but that result is slightly better (0.7966). I still don't know why :)","43cbcd94":"# Input","10fb0112":"## 4. Target distribution in features\n### 4.1. Continuous features:  Age and Fare\n* Good to split but potential problem is, the distribution has more spikes and bumps in training set and smoother in test set. Model may not be able to generalize to test set because of this reason.\n* We could see that Age > 15 has higher survival rates, Fare: the survival rate is higher on distribution tails -> think about create bin features.","df53dccf":"### 2.2. Embarked\nTwo missing rows have same ticket number could be fill by mode of this feature (S)","f598ab15":"## 3. Correlation\nFeatures are highly correlated with each other and dependent to each other. ","cf703b69":"We could choose between k-FOLD approach (for cross-validation) and single model (use full training set) to create the model.\nI have try RandomForest and LightGBM algorithms and seeing that RandomForest provide better results.","03db9d2b":"## 5. Ticket \nThis feature contains too many unique values, groupby Ticket and count the frequency. Do not consider using Ticket prefix because this information could be gotten from Pclass and Deck features.","756a690d":"Thank @Gunes Evitan for reference.","1e727fc1":"# Feature Engineering\n## 1. Fare\nHigh skew, higher survival at the end -> quantile","d5fd50e9":"# Modelling ","3ba99430":"## 2. Missing values\n* Missing columns in training set: *Age, Cabin, Embarked* and in test set: *Age, Cabin, Fare*\n* The count of missing values in Age, Embarked and Fare are smaller compared to total sample, but roughly 80% of the Cabin is missing. Missing values in Age, Embarked and Fare can be filled with descriptive statistical measures but that wouldn't work for Cabin which is needed to be preprocessed.\n\nNote that some boosting algorithms such as LightGBM, XGBoost could handle missing values automatically. But in my opinion, if the information about the data is available, fill missing values in a proper way will produce better results. \n","ff2e23e6":"### Conclusion\n* Most of the features are correlated with each other -> create new features with feature transformation and feature interaction.\n* Split points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\n* Categorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features."}}