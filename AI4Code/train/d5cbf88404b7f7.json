{"cell_type":{"918659d6":"code","b5596137":"code","fbeca6ff":"code","af83ac40":"code","287da78a":"code","c5a0fc6e":"code","584867b8":"code","3d14f65a":"code","90415eaf":"code","9ade8560":"code","cf60ea14":"code","e2133228":"code","1794034f":"code","445dfe37":"code","c770e747":"code","81ef6980":"code","a815b408":"code","2c32522b":"code","3c401bbf":"code","815f7782":"code","7486cf12":"code","d27faacb":"code","072eefa6":"code","35c19adf":"code","85f1ea5e":"code","2844b430":"code","03967679":"code","64db0722":"code","f9f1d361":"code","567745b5":"code","24a451d3":"code","8de0ca11":"code","d437705a":"code","5f75b22e":"code","54e411f2":"code","ff3c3ad6":"markdown","6c20b6fc":"markdown","74399e8f":"markdown","7d1b4548":"markdown","35964bbe":"markdown","42bc08e6":"markdown","7d6adc71":"markdown","018efbc0":"markdown","24860ac8":"markdown","6ebd1e71":"markdown","8dc84d7b":"markdown","bdb310df":"markdown","c7a18ece":"markdown","223a1bfd":"markdown"},"source":{"918659d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5596137":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")","fbeca6ff":"df = pd.read_csv('\/kaggle\/input\/imoveis-goiniago\/2021-08-05-all.csv')\ndf.info()","af83ac40":"def convert_money(txt):\n  money = txt.strip('R$ ').replace('.', '').replace(',', '.')\n  return float(money)\n\ndf = df[df['PRICE'] != 'Sob consulta']\ndf['PRICE'] = df['PRICE'].apply(lambda x : convert_money(x))\ndf['CONDOM\u00cdNIO'] = df['CONDOM\u00cdNIO'].apply(lambda x : convert_money(x) \n                                          if pd.isnull(x) != True else float(0))\ndf['IPTU'] = df['IPTU'].apply(lambda x : convert_money(x) \n                                          if pd.isnull(x) != True else np.nan)\nfor i in ['BEDROOMS','PARKING-SPACES', 'BATHROOMS']:\n    df[i] = df[i] = pd.to_numeric(df[i], errors='coerce').astype('Int64')\ndf.dropna(subset=['ADDRESS', 'AREAS'], inplace=True)\n\ndf['AREAS'] = df.AREAS.str.replace(' m\u00b2', '').str.split(' - ').apply(lambda x: [int(i) for i in x])\ndf['AREAS'] = df['AREAS'].apply(np.mean)","287da78a":"print('Resume Missing Values')\ndf.isnull().sum().sort_values(ascending=False)","c5a0fc6e":"df.IPTU.interpolate(limit_direction='both', inplace=True)\n\nm = (df['BEDROOMS'].isna()) & (df['TIPO'] == 'fazendas-sitios-chacaras')\ndf.loc[m,'BEDROOMS'] = df.loc[m,'BEDROOMS'].fillna(1)\n\nm1 = (df['BEDROOMS'].isna()) & (df['TIPO'] == 'apartamentos')\ndf.loc[m1,'BEDROOMS'] = df.loc[m1,'BEDROOMS'].fillna(3)\n\nm2 = (df['BEDROOMS'].isna()) & (df['TIPO'] == 'casas')\ndf.loc[m2,'BEDROOMS'] = df.loc[m2,'BEDROOMS'].fillna(3)\n\nm3 = (df['BEDROOMS'].isna()) & (df['TIPO'] == 'quitinetes')\ndf.loc[m3,'BEDROOMS'] = df.loc[m3,'BEDROOMS'].fillna(1)\n\nm4 = (df['BEDROOMS'].isna()) & (df['TIPO'] == 'terrenos-lotes-condominios')\ndf.loc[m4,'BEDROOMS'] = df.loc[m4,'BEDROOMS'].fillna(1)","584867b8":"df.BATHROOMS = df.BATHROOMS.replace({np.nan: np.nan})\ndf['PARKING-SPACES'] = df['PARKING-SPACES'].replace({np.nan: np.nan})\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2)\nfor c in ['BATHROOMS', 'PARKING-SPACES']:\n    x = imputer.fit_transform(df[c].values.reshape(-1, 1))\n    df[c] = x\n    \ndf['BATHROOMS'] = np.round(df['BATHROOMS'], 0)\ndf['PARKING-SPACES'] = np.round(df['PARKING-SPACES'], 0)","3d14f65a":"df['BEDROOMS'] = df['BEDROOMS'].astype(float)\nprint('Resume Missing Values')\ndf.isnull().sum().sort_values(ascending=False)","90415eaf":"df.info()","9ade8560":"df.drop(df[df.IPTU >= 75000].index, inplace=True)\n\nfor c in df[df.IPTU < 10].TIPO.unique():\n    mask = (df['IPTU'] < 10) & (df['TIPO'] == c)\n    df.loc[mask, 'IPTU'] = df[df.TIPO == c].IPTU.median()\n\nfor c in df[(df['PARKING-SPACES'] > 7)].TIPO.unique():\n    mask = (df['PARKING-SPACES'] > 7) & (df.TIPO == c)\n    df.loc[mask, 'PARKING-SPACES'] = df[df.TIPO == c]['PARKING-SPACES'].median()\n\nmask = (df['BEDROOMS'] >= 30)\ndf.loc[mask, 'BEDROOMS'] = df.BEDROOMS.median()\n\nmask = (df['BATHROOMS'] >= 30)\ndf.loc[mask, 'BATHROOMS'] = df.BATHROOMS.median()","cf60ea14":"df.describe()","e2133228":"def plot_relplot(df):\n    rc = {'axes.labelsize': 18,\n          'ytick.labelsize': 16, \n          'xtick.labelsize': 16,\n          'ytick.major.width': 4,\n          'xtick.major.width': 3,\n          'xtick.bottom': True,\n          'ytick.left': True,\n          'xtick.top': False,\n          'ytick.right': False,\n          'axes.spines.right': False,\n          'axes.spines.top': False,\n          'axes.edgecolor': '.15',\n          'axes.linewidth': 3,\n          'legend.fontsize': 18,\n          'legend.title_fontsize': 18,\n          'legend.markerscale': 2.0}\n    with plt.rc_context(rc):\n        ax = sns.relplot(data=df, \n                         x='AREAS', y='PRICE', col='TIPO', size='BEDROOMS',\n                         col_wrap=4, sizes=(20, 800), hue='BATHROOMS',\n                         hue_norm=(0, 10),\n                         facet_kws=dict(sharex=False, sharey=False))","1794034f":"plot_relplot(df)","445dfe37":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\n\noutliers_fraction = 0.1\nanomaly_algorithms = [\n    (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction,\n                                         random_state=1)),\n    (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction))]\noutliers = dict()\nfig, axs = plt.subplots(11, 2, figsize=(14, 44), dpi=110)\n\nnames = {\"Isolation Forest\": 0, \"Robust covariance\": 1}\nfor index, name in enumerate(df.TIPO.unique().tolist()):\n    X = df[(df['TIPO'] == name)][['AREAS', 'PRICE']]\n    outliers[name] = list()\n    for model, algorithm in anomaly_algorithms:\n        n = names[model]\n        axs[index, n].set_title(name, size=12)\n        colors = np.array(['#8736AA', '#F5BD1F'])\n        y_pred = algorithm.fit(X).predict(X)\n        outliers[name].append(y_pred)\n        axs[index, n].scatter(X['AREAS'], X['PRICE'], s=50, color=colors[(y_pred + 1) \/\/ 2], \n                              edgecolors='black')\n        axs[index, n].set_xlabel('AREAS')\n        axs[index, n].set_ylabel('PRICE')\n        axs[index, n].legend(['inliers', 'outliers'])\n        for g in ['left', 'bottom']:\n            axs[index, n].spines[g].set_linewidth(2)\n            axs[index, n].spines[g].set_color('#011627')\n        axs[index, n].yaxis.set_ticks_position('left')\n        axs[index, n].xaxis.set_ticks_position('bottom')\n        axs[index, n].tick_params(width=3)\nfig.text(0.22, 1,'Isolation Forest', fontsize=18)\nfig.text(0.70, 1,'Robust covariance', fontsize=18);\nfig.tight_layout()","c770e747":"df_new = df.copy()\nfor k in outliers.keys():\n    for out in outliers[k]:\n        mask = out != 1\n        rows = df[df.TIPO == k].loc[mask].index.tolist()\n        df_new.drop(rows, inplace=True, errors='ignore')","81ef6980":"plot_relplot(df_new)","a815b408":"colors = sns.cubehelix_palette(reverse=True)\ncolors","2c32522b":"features = df.columns[3:].tolist()","3c401bbf":"from sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        #print(colname)\n        X[colname], _ = X[colname].factorize()\n        \n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    params = {'xtick.labelsize': 7,\n              'ytick.labelsize': 7,}\n    with plt.rc_context(params):\n        plt.barh(width, scores, color=colors[0])\n        plt.axvline(linewidth=3.5, color=\"black\")\n        plt.yticks(width, ticks)\n        plt.title(\"Mutual Information Scores\")","815f7782":"X = df_new.drop(['DATE', 'ADDRESS'], axis=1).copy()\ny = X.pop('PRICE')\nmi_scores = make_mi_scores(X, y)","7486cf12":"print(mi_scores.head(20))\nplt.figure(dpi=100, figsize=(8, 4))\nplot_mi_scores(mi_scores.head(20))","d27faacb":"fig, ax1 = plt.subplots(1, 1, figsize=(10, 3), dpi=102)\ndf_new.TIPO.value_counts().plot(kind='bar', ax=ax1, color=colors[0])\nax1.set_title('TIPO: counts', fontsize=10)\nax1.axhline(linewidth=4, color=\"black\")\nax1.tick_params(labelsize=8)\nplt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8, \n         rotation_mode=\"anchor\");","072eefa6":"cmap = sns.cubehelix_palette(dark=0, light=1, as_cmap=True)\nfig, (ax1 ,ax2) = plt.subplots(1, 2, figsize=(16, 5), dpi=60)\nsns.heatmap(df.corr(), annot=True, cmap=cmap, ax=ax1)\nax1.set_title('Correlations', fontsize=16)\nax2.set_title('Distributions log(PRICE)', fontsize=16)\nsns.histplot(np.log1p(df.PRICE), kde=True, color=colors[0], ax=ax2);","35c19adf":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8), dpi=60)\ngroup = df.groupby(['TIPO']).mean()['PRICE'].to_frame()\ngroup.sort_values(by='PRICE', ascending=True).plot(kind='barh', ax=ax1, legend=None,\n                                                   color=colors[0])\nax1.xaxis.set_major_formatter('${x:1,.0f}')\nplt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", fontsize=16,\n         rotation_mode=\"anchor\")\ndf.groupby(['TIPO']).mean()['IPTU'].sort_values().plot(kind='barh', ax=ax2, color=colors[0])\nax2.xaxis.set_major_formatter('${x:1,.0f}')\nax1.set_ylabel(None)\nax2.set_ylabel(None)\nplt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", fontsize=14, \n         rotation_mode=\"anchor\")\ndf.groupby(['TIPO']).mean()['CONDOM\u00cdNIO'].sort_values().plot(kind='barh', ax=ax3, color=colors[0])\nax3.xaxis.set_major_formatter('${x:1,.0f}')\nplt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\", fontsize=14, \n         rotation_mode=\"anchor\")\nax3.set_ylabel(None)\nax1.set_title('PRICE (mean) $', fontsize=18)\nax2.set_title('IPTU (mean) $', fontsize=18)\nax3.set_title('CONDOM\u00cdNIO (mean) $', fontsize=18)\nax1.tick_params(labelsize=16)\nax2.tick_params(labelsize=16)\nax3.tick_params(labelsize=16)\nax1.axvline(linewidth=4, color=\"black\") \nax2.axvline(linewidth=4, color=\"black\")\nax3.axvline(linewidth=4, color=\"black\")\nfig.text(-0.02, 1, ' ')\nfig.text(1.10, -0.05, ' ')\nfig.tight_layout()","85f1ea5e":"one_hot = pd.get_dummies(df['TIPO'])\nX = df_new.drop(['TIPO', 'DATE', 'ADDRESS'], axis=1)\nX = X.join(one_hot)\ny = np.log1p(df_new.PRICE)","2844b430":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import QuantileTransformer\n\n# LassoCV\nlasso_model = LassoCV(n_alphas=150, max_iter=1e4, random_state=1)\n# SVR\nrbf_model = SVR(kernel='rbf', C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121)\n\n# Hist\nhist_model = HistGradientBoostingRegressor(min_samples_leaf=40, max_depth=5, \n                                           max_iter=1000, learning_rate=0.15,\n                                           loss='least_absolute_deviation', \n                                           random_state=1)\n# lightgbm\nlgbm_model = LGBMRegressor(objective='regression', n_estimators=2000, \n                           num_leaves=10, learning_rate=0.005,\n                           max_bin=163, bagging_fraction=0.85, \n                           n_jobs=-1, bagging_seed=42, \n                           feature_fraction_seed=42, bagging_freq=7, \n                           feature_fraction=0.1294, \n                           min_data_in_leaf=8, random_state=1)\n\n# xgboost\nxgboost_model = XGBRegressor(learning_rate=0.0139, n_estimators=2000, \n                             max_depth=4, min_child_weight=0,\n                             subsample=0.7968, colsample_bytree=0.4064, \n                             nthread=-1, scale_pos_weight=2,\n                             seed=42, random_state=1)","03967679":"# Transformer\ntransformer = QuantileTransformer(output_distribution='normal')\n\n# Models\nhist = make_pipeline(transformer, hist_model)\nxgboost = make_pipeline(transformer, xgboost_model)\nlgbm = make_pipeline(transformer, lgbm_model)\nlasso = make_pipeline(transformer, lasso_model)\nsvr = make_pipeline(transformer, rbf_model)\n\nmodels = [('HistGradientBoosting', hist),\n          ('XGBoost', xgboost), \n          ('LightGBM', lgbm),\n          ('LassoCV', lasso),\n          ('SVR', svr)]","64db0722":"def storm_model(x, y, models, cv, scoring):\n    df_evaluation = pd.DataFrame()\n    row_index = 0\n    for name, model in models:\n        # score\n        scores = cross_validate(model, np.array(x), np.array(y).ravel(), cv=cv, \n                                scoring=scoring, n_jobs=-1, verbose=0)\n        df_evaluation.loc[row_index, 'Model_Name'] = name\n        for i in scoring:\n            text = 'test_'+i\n            df_evaluation.loc[row_index, i] = -1*scores[text].mean()\n        row_index += 1\n    df_evaluation.rename(columns = {'neg_mean_absolute_error': 'MAE',\n                                    'neg_mean_squared_error': 'MSE', \n                                    'neg_root_mean_squared_error': 'RMSE'}, inplace = True)\n    df_evaluation.sort_values(by=['RMSE'], ascending=True, inplace=True)\n    df_evaluation.reset_index(drop=True, inplace=True)\n    return (df_evaluation)\n","f9f1d361":"%%time\nfrom sklearn.model_selection import cross_validate, cross_val_predict, KFold\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=1)\nscoring = ['neg_mean_absolute_error', \n           'neg_mean_squared_error', \n           'neg_root_mean_squared_error']\n\n# cross validate\ndf_score = storm_model(X, y, models, kfolds, scoring)","567745b5":"df_score.style.background_gradient(cmap=sns.cubehelix_palette(dark=0, light=1, \n                                                              as_cmap=True, reverse=True))","24a451d3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=1)\n\nprint('=' * 16+'TRAIN'+'=' * 16)\nprint('X Train set:', X_train.shape)\nprint('Y Train set:', Y_train.shape)\nprint('=' * 16+'VAL'+'=' * 18)\nprint('X test set:', X_test.shape)\nprint('Y test set:', Y_test.shape)\nprint('=' * 37)","8de0ca11":"%%time\nfrom sklearn.ensemble import VotingRegressor\n\n# VotingRegressor\nbest_models = [('HistGradientBoosting', hist),\n               ('XGBoost', xgboost)]\n\nvr = VotingRegressor(best_models)\nscores = cross_validate(vr, X, y, cv=kfolds, scoring=scoring, n_jobs=-1, verbose=0)\nprint(f\"MAE score: {-1*scores['test_neg_mean_absolute_error'].mean()}\")\nprint(f\"RMSE score: {-1*scores['test_neg_root_mean_squared_error'].mean()}\")","d437705a":"from sklearn.metrics import mean_squared_error\n\ndef fit_model(model, xtrain, ytrain, xval, yval):\n    model.fit(xtrain, ytrain)\n    pred = model.predict(xval)\n    rmse = mean_squared_error(yval, pred, squared=False)\n    return f'RMSE score: {rmse}'","5f75b22e":"%%time\nvr_fit = fit_model(vr, X_train, Y_train, X_test, Y_test)\nprint('VotingRegressor Evaluation =>', vr_fit)","54e411f2":"%%time\nxg_fit = fit_model(xgboost, X_train, Y_train, X_test, Y_test)\nprint('XGBoost Evaluation =>', xg_fit)","ff3c3ad6":"# **1. Preprocessing Data**","6c20b6fc":"## Missing Values","74399e8f":"# **3. Exploratory Data Analysis (EDA)**","7d1b4548":"### Filling missing values: *fillna*","35964bbe":"### Filling missing values: *sklearn Imputer*","42bc08e6":"### Detecting Outliers: *IsolationForest*","7d6adc71":"### Build Models","018efbc0":"# **5. Model selection and evaluation**\n","24860ac8":"# **5. Model validation**","6ebd1e71":"### Cross-Validation","8dc84d7b":"<p style = \"font-size:32px; font-family:Space Mono ; font-weight : normal; background-color: #210535; color :white   ; text-align: center; border-radius: 4px 4px; padding: 4px\"> Exploratory Data Analysis: Housing Prices in Goiania-Goias<\/p>","bdb310df":"# **4. Feature Engineering**","c7a18ece":"# **2. Outlier Detection**","223a1bfd":"### Detecting Outliers: *Imputer*"}}