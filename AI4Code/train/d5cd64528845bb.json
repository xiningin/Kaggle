{"cell_type":{"fbad18fa":"code","c2e813ed":"code","599fd702":"code","febd62b3":"code","53bc3562":"code","85584534":"code","61e08dcb":"code","2b63619d":"code","5746668d":"code","07966088":"code","a12ad3c3":"code","5d330f69":"code","493abc12":"code","a8ae5166":"code","c0002158":"code","6b297269":"code","6103ecab":"markdown","0b6bb505":"markdown","54a6eb2b":"markdown","f8382912":"markdown","b8b7a81a":"markdown","d65ffff9":"markdown","cd1841d0":"markdown"},"source":{"fbad18fa":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","c2e813ed":"##printing the text\nshakespeare_text = open('..\/input\/sonnetdataset\/sonnets.txt').read()\nprint(len(shakespeare_text))","599fd702":"##create corpus by lowering the letters and splitting the text by \\n\ncorpus = shakespeare_text.lower().split(\"\\n\")\nprint(corpus[:5])","febd62b3":"\ntokenizer = tf.keras.preprocessing.text.Tokenizer()","53bc3562":"tokenizer.fit_on_texts(corpus)\n\n##calculate vocabulary size - be mindful of the <oov> token\nvocab_size = len(tokenizer.word_index) + 1\n\n# print(tokenizer.word_index)\nprint(vocab_size)","85584534":"##create sequences of \ninput_sequences = []\nfor line in corpus:\n    tokens = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(tokens)):\n        n_gram_sequence = tokens[:i+1]\n        input_sequences.append(n_gram_sequence)\n","61e08dcb":"##pad sequences\nmax_seq_len = max([len(i) for i in input_sequences])\ninput_seq_array = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n                                         maxlen=max_seq_len,\n                                         padding='pre')\n                        )\n","2b63619d":"##creating features(X) and label(y)\nX = input_seq_array[:, :-1]\nlabels = input_seq_array[:, -1]\n\n##one-hot encode the labels to get y - since it is actually just a classification problem\ny = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)","5746668d":"model = tf.keras.Sequential([\n                tf.keras.layers.Embedding(vocab_size, 120, input_length=max_seq_len-1),\n                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120)),\n                tf.keras.layers.Dense(vocab_size, activation='softmax')\n])\n\n##define the learning rate - step size for optimizer\nadam = tf.keras.optimizers.Adam(lr=0.01)","07966088":"model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","a12ad3c3":"epoch=20\nhistory = model.fit(X, y, epochs=epoch, verbose=1)","5d330f69":"def plot_metric(history, metric):\n    plt.plot(history.history[metric])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.show()","493abc12":"plot_metric(history, 'accuracy')","a8ae5166":"plot_metric(history, 'loss')","c0002158":"seed_text = \"It was a cold night.\"\nnext_words = 100\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list), axis=-1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","6b297269":"seed_text = \"Being your slave, what should I do but tend\"\nnext_words = 200\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list), axis=-1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","6103ecab":"## Define the LSTM model","0b6bb505":"## Step 1: Create a corpus","54a6eb2b":"## Set up the tokenizer","f8382912":"## Import the required libraries and APIs","b8b7a81a":"## Visualise the metrics","d65ffff9":"# Poetry generation with RNNs(LSTMs)\n\n>## Let's create poetry like Shakespeare by leveraging RNNs(LSTMs). We'll be using the Shakerpeare poetry as the training data and then use the trained network to predict the next words.","cd1841d0":"## Generate new text"}}