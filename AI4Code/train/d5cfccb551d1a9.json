{"cell_type":{"4192e7a4":"code","2e23929e":"code","16bc8645":"code","ee354295":"code","b3e85e3c":"code","8f9fc7a4":"code","b9e79cfc":"code","a376f63f":"code","c284a274":"code","c1d7d0dd":"code","fac12c9e":"code","e614ee19":"code","c3903802":"code","2133ac3f":"code","e0cbf4d1":"code","ce4283a8":"code","42349429":"code","8cd3efd9":"code","676b5597":"code","078c0dd2":"code","ff9ac6cc":"code","cbcd821d":"code","0053d133":"code","106b9e6a":"code","e6e38969":"code","a1bd4a9f":"code","2e5dee78":"markdown","df1b2232":"markdown","4e395f45":"markdown"},"source":{"4192e7a4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, cv2, random, time, shutil, csv\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\nnp.random.seed(42)\n%matplotlib inline \n\nimport tensorflow.keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img","2e23929e":"def get_num_files(path):\n    '''\n    Counts the number of files in a folder.\n    '''\n    if not os.path.exists(path):\n        return 0\n    return sum([len(files) for r, d, files in os.walk(path)])","16bc8645":"#Data Paths\ntrain_dir = '\/kaggle\/input\/dog-breed-identification\/train'\ntest_dir = '\/kaggle\/input\/dog-breed-identification\/test'\n#Count\/Print train and test samples.\ndata_size = get_num_files(train_dir)\ntest_size = get_num_files(test_dir)\nprint('Data samples size: ', data_size)\nprint('Test samples size: ', test_size)","ee354295":"#Read train labels.\nlabels_dataframe = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/labels.csv')\n#Read sample_submission file to be modified by pridected labels.\nsample_df = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/sample_submission.csv')\n#Incpect labels_dataframe.\nlabels_dataframe.head(1)","b3e85e3c":"#Create list of alphabetically sorted labels.\ndog_breeds = sorted(list(set(labels_dataframe['breed'])))\nn_classes = len(dog_breeds)","8f9fc7a4":"import seaborn as sns\n\n\nsns.color_palette(\"light:#5A9\", as_cmap=True)\nfig, ax = plt.subplots()\nfig.set_size_inches(16, 8)\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"breed\", data=labels_dataframe,palette=\"light:#5A9\",order=labels_dataframe.breed.value_counts().iloc[:5].index)","b9e79cfc":"#Map each label string to an integer label.\nclass_to_num = dict(zip(dog_breeds, range(n_classes)))","a376f63f":"def images_to_array(data_dir, labels_dataframe, img_size = (224,224,3)):\n    '''\n    1- Read image samples from certain directory.\n    2- Risize it, then stack them into one big numpy array.\n    3- Read sample's label form the labels dataframe.\n    4- One hot encode labels array.\n    5- Shuffle Data and label arrays.\n    '''\n    images_names = labels_dataframe['id']\n    images_labels = labels_dataframe['breed']\n    data_size = len(images_names)\n    #initailize output arrays.\n    X = np.zeros([data_size, img_size[0], img_size[1], img_size[2]], dtype=np.uint8)\n    y = np.zeros([data_size,1], dtype=np.uint8)\n    #read data and lables.\n    for i in tqdm(range(data_size)):\n        image_name = images_names[i]\n        img_dir = os.path.join(data_dir, image_name+'.jpg')\n        img_pixels = load_img(img_dir, target_size=img_size)\n        X[i] = img_pixels\n        \n        image_breed = images_labels[i]\n        y[i] = class_to_num[image_breed]\n    \n    #One hot encoder\n    y = to_categorical(y)\n    #shuffle    \n    ind = np.random.permutation(data_size)\n    X = X[ind]\n    y = y[ind]\n    print('Ouptut Data Size: ', X.shape)\n    print('Ouptut Label Size: ', y.shape)\n    return X, y","c284a274":"#img_size chosen to be 331 to suit the used architectures.\nimg_size = (331,331,3)\nX, y = images_to_array(train_dir, labels_dataframe, img_size)","c1d7d0dd":"def get_features(model_name, data_preprocessor, input_size, data):\n    '''\n    1- Create a feature extractor to extract features from the data.\n    2- Returns the extracted features and the feature extractor.\n    '''\n    #Prepare pipeline.\n    input_layer = Input(input_size)\n    preprocessor = Lambda(data_preprocessor)(input_layer)\n    base_model = model_name(weights='imagenet', include_top=False,\n                            input_shape=input_size)(preprocessor)\n    avg = GlobalAveragePooling2D()(base_model)\n    feature_extractor = Model(inputs = input_layer, outputs = avg)\n    #Extract feature.\n    feature_maps = feature_extractor.predict(data, batch_size=64, verbose=1)\n    print('Feature maps shape: ', feature_maps.shape)\n    return feature_maps","fac12c9e":"# Extract features using InceptionV3 as extractor.\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\ninception_preprocessor = preprocess_input\ninception_features = get_features(InceptionV3,\n                                  inception_preprocessor,\n                                  img_size, X)","e614ee19":"# Extract features using Xception as extractor.\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nxception_preprocessor = preprocess_input\nxception_features = get_features(Xception,\n                                 xception_preprocessor,\n                                 img_size, X)","c3903802":"# Extract features using ResNet152V2 as extractor.\nfrom tensorflow.keras.applications.resnet_v2 import ResNet152V2, preprocess_input\nresnet_preprocessor = preprocess_input\nresnet_features = get_features(ResNet152V2,\n                               resnet_preprocessor,\n                               img_size, X)","2133ac3f":"# Extract features using NASNetLarge as extractor.\nfrom tensorflow.keras.applications.nasnet import NASNetLarge, preprocess_input\nnasnet_preprocessor = preprocess_input\nnasnet_features = get_features(NASNetLarge,\n                               nasnet_preprocessor,\n                               img_size, X)","e0cbf4d1":"# Extract features using InceptionResNetV2 as extractor.\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\ninc_resnet_preprocessor = preprocess_input\ninc_resnet_features = get_features(InceptionResNetV2,\n                                   inc_resnet_preprocessor,\n                                   img_size, X)","ce4283a8":"#It's a good habit to free up some RAM memory.\n#X variable won't be needed anymore, so let's get rid of it.\ndel X","42349429":"final_features = np.concatenate([inception_features,\n                                 xception_features,\n                                 resnet_features,\n                                 nasnet_features,\n                                 inc_resnet_features,], axis=-1)\nprint('Final feature maps shape', final_features.shape)","8cd3efd9":"from tensorflow.keras.callbacks import EarlyStopping\n#Prepare call backs\nEarlyStop_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nmy_callback=[EarlyStop_callback]","676b5597":"#Prepare DNN model\ndnn = tensorflow.keras.models.Sequential([\n    InputLayer(final_features.shape[1:]),\n    Dropout(0.7),\n    Dense(n_classes, activation='softmax')\n])\n\ndnn.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n#Train simple DNN on extracted features.\nh = dnn.fit(final_features, y,\n            batch_size=128,\n            epochs=60,\n            validation_split=0.1,\n            callbacks=my_callback)","078c0dd2":"def images_to_array2(data_dir, labels_dataframe, img_size = (224,224,3)):\n    '''\n    Do same as images_to_array but omit some unnecessary steps for test data.\n    '''\n    images_names = labels_dataframe['id']\n    data_size = len(images_names)-5000\n    X = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)\n    \n    for i in tqdm(range(data_size)):\n        image_name = images_names[i]\n        img_dir = os.path.join(data_dir, image_name+'.jpg')\n        img_pixels = tf.keras.preprocessing.image.load_img(img_dir, target_size=img_size)\n        X[i] = img_pixels\n        \n    print('Ouptut Data Size: ', X.shape)\n    return X\n\ntest_data = images_to_array2(test_dir, sample_df, img_size)","ff9ac6cc":"# del sample_df","cbcd821d":"#Extract test data features.\ninception_features = get_features(InceptionV3, inception_preprocessor, img_size, test_data)\nxception_features = get_features(Xception, xception_preprocessor, img_size, test_data)\nresnet_features = get_features(ResNet152V2, resnet_preprocessor, img_size, test_data)\nnasnet_features = get_features(NASNetLarge, nasnet_preprocessor, img_size, test_data)\ninc_resnet_features = get_features(InceptionResNetV2, inc_resnet_preprocessor, img_size, test_data)\n\ntest_features = np.concatenate([inception_features,\n                                 xception_features,\n                                 resnet_features,\n                                 nasnet_features,\n                                 inc_resnet_features],axis=-1)\nprint('Final feature maps shape', test_features.shape)","0053d133":"#Free up some space.\ndel test_data","106b9e6a":"#Predict test labels given test data features.\ny_pred = dnn.predict(test_features, batch_size=128)","e6e38969":"#Create submission file\nsample_df = sample_df.head(5357)\nfor b in dog_breeds:\n    sample_df[b] = y_pred[:,class_to_num[b]]\nsample_df.to_csv('pred.csv', index=None)","a1bd4a9f":"%%html\n<marquee style='width: 90% ;height:70%; color: #45B39D ;'>\n    <b>Do UPVOTE if you like my work!  :) <\/b><\/marquee>","2e5dee78":"### What is transfer learning?\nTransfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. \n### An example \nFor example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.'\n\n\n![Transfer Learning](https:\/\/miro.medium.com\/max\/1400\/1*9GTEzcO8KxxrfutmtsPs3Q.png)\n\n\n### Explaining it in simple terms, or atleast trying to \n\n- With transfer learning, we basically try to exploit what has been learned in one task to improve generalization in another. We transfer the weights that a network has learned at \"task A\" to a new \"task B.\"\n\n\n\n- The general idea is to use the knowledge a model has learned from a task with a lot of available labeled training data in a new task that doesn't have much data. \n- Instead of starting the learning process from scratch, we start with patterns learned from solving a related task.\n\n- Transfer learning is mostly used in computer vision and natural language processing tasks like sentiment analysis due to the huge amount of computational power required.\n\n- Transfer learning isn't really a machine learning technique, but can be seen as a \"design methodology\" within the field, for example, active learning. \n- It is also not an exclusive part or study-area of machine learning. Nevertheless, it has become quite popular in combination with neural networks that require huge amounts of data and computational power.\n\n![](https:\/\/cdn.builtin.com\/sites\/www.builtin.com\/files\/styles\/ckeditor_optimize\/public\/inline-images\/national\/AGI-transfer-learning_0.png)\n\n\n\n##### I will be following the code done on the same dataset by [Ahmed Saied](https:\/\/www.kaggle.com\/phylake1337) explained in the link [The Code](https:\/\/www.kaggle.com\/phylake1337\/0-18-loss-simple-feature-extractors\/notebook)\n","df1b2232":"### Introduction\n\n* This kernel is a detailed guide for transfer learning on Dog Breeds problem, it's all about learning a new technique, evaluate it using only Kaggle training set without cheating.\n\n* The aim of this kernel is to show you how to use pre-trained CNN models as feature extractors, which one of the most effective transfer learning techniques.\n\n* A reasonable question comes to your mind, 'Wait, why do we have to use this technique, why don't we just use regular transfer learning ?', if you try to do so, you will figure out that the problem is pretty hard for a single model to handle (you would get higher loss and less accuracy).\n\n* It's even hard for humankind to distinguish between 120 dog breeds!, single poor CNN would struggle.\n\n### Explanation\n\n- Take look at general CNN architecture for image classification in two main parts, \u201cfeature extractor\u201d that based on conv-layers, and \u201cclassifier\u201d which usually based on fully connected layers:\n- Simply, feature extractor could be created as follow > (Feature Extractor = Pretrained Model - Late Fully Connected Layers)\n\n- For example, InceptionV3 feature extractor (without last FC layer) outputs 2048 vector for each image sample, each value represent a certain feature of dog image (Coded in numerical values of course), like Dog color?, How big is his head?, Shape of the eyes?, length of the tale?, Size? .. etc\n\n- Hence, more \"different\" feature extractors mean more features to be used to determine which breed does this dog belong.\n\n- So our strategy goes as the following,\n\n - Create 4 feature extractor using different pre-trained CNN models\n - Extract features from raw data and stacks the features together.\n - Use a simple DNN with one dense layer and a heavy dropout layer to figure out patterns in the feature extracted from the data.\n - The code is simple, concise and fully-commented. Feel free to ask for help \/ more info \/ more explanation in the comments.\n\n- Finally if this kernel helps you somehow, kindly don't forget to leave a little upvote.\n\n","4e395f45":"If there are any suggesion for the notebook please comment, that would be helpful. Also please upvote if you liked it! Thank you!!\n\nSome of my other works:\n\n* [TPS- APR](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model) \n* [HEART ATTACKS](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations) \n* [YOUTUBE DATA EXPLORATION](https:\/\/www.kaggle.com\/udbhavpangotra\/what-do-people-use-youtube-for-in-great-britain)\n* [TPS MAY](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-may-21-extensive-eda-catboost-shap)\n* [COVID-19 DIGITAL LEARNING](https:\/\/www.kaggle.com\/udbhavpangotra\/how-did-covid-19-impact-digital-learning-eda)\n* [TPS - SEPT](https:\/\/www.kaggle.com\/udbhavpangotra\/extensive-eda-baseline-shap)\n* [EDA - Wikimedia Image\/Caption Matching](https:\/\/www.kaggle.com\/udbhavpangotra\/eda-wikimedia-image-caption-matching)\n\n\n\n\nDataset \n* [also try this dataset ReliefWeb Crisis Figures Data](https:\/\/www.kaggle.com\/udbhavpangotra\/reliefweb-crisis-figures-data)"}}