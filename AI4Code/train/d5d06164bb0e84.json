{"cell_type":{"63328f7d":"code","f46b83de":"code","5b6b1929":"code","e459954b":"code","70d5a7c3":"code","2997b2a8":"code","7c240ff4":"code","cfb7adf2":"code","2d0b7b56":"code","dcfb0a86":"code","0bf8ba73":"code","e6e4c056":"code","e831c026":"code","8e2e7fce":"code","7bb71609":"code","921c2eef":"code","5d2ea530":"code","50f7cde6":"code","7a3a9473":"code","d770c9bb":"code","b4a3ff68":"code","c5cb3015":"code","ec14d212":"code","6dde7724":"markdown","5779ef06":"markdown","f6e603a4":"markdown","9447c54f":"markdown","8ed87b21":"markdown","5874304a":"markdown","ef513f46":"markdown","12c8bcf2":"markdown"},"source":{"63328f7d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f46b83de":"import random\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom lightgbm import LGBMRegressor\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn import preprocessing\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.pyplot as plt\n%matplotlib inline","5b6b1929":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest  = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsub = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","e459954b":"train.head()","70d5a7c3":"test.head()","2997b2a8":"train.info()","7c240ff4":"test.info()","cfb7adf2":"#Check if there'is null values\ntrain.isnull().sum()","2d0b7b56":"#Check if there'is null values\ntest.isnull().sum()","dcfb0a86":"train.describe()","0bf8ba73":"categorical_cols=['cat'+str(i) for i in range(10)]\ncontinous_cols=['cont'+str(i) for i in range(14)]","e6e4c056":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(7, 2,figsize=(20, 24))\nfor feature in continous_cols:\n    plt.subplot(7, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","e831c026":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(28, 44))\nfor feature in categorical_cols:\n    plt.subplot(5, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","8e2e7fce":"#Features correlation\ncorr = train[continous_cols+['target']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","7bb71609":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n\nf.suptitle('Target', fontsize=16)\ng = sns.kdeplot(train['target'], shade=True, label=\"%.2f\"%(train['target'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['target'], plot=axes[1])\nsns.boxplot(x='target', data=train, orient='h', ax=axes[2]);\n\nplt.tight_layout()\nplt.show()","921c2eef":"cols=categorical_cols+continous_cols\ntrain_objs_num = len(train)\ndataset = pd.concat(objs=[train[cols], test[cols]], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset,columns=categorical_cols)\ntrain_preprocessed = dataset_preprocessed[:train_objs_num]\ntest_preprocessed = dataset_preprocessed[train_objs_num:]","5d2ea530":"train_preprocessed","50f7cde6":"params = {'bagging_freq': 3, 'reg_alpha': 9.41856932526599, 'reg_lambda': 0.0010233933999487326,\n         'colsample_bytree': 0.11, 'subsample': 0.9, 'learning_rate': 0.01, 'max_depth': 100, 'num_leaves': 57,\n         'min_child_samples': 99, 'cat_smooth': 36,'metric': 'rmse', 'random_state': 48,'n_estimators': 40000}","7a3a9473":"preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=10,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train_preprocessed,train['target']):\n    X_tr,X_val=train_preprocessed.iloc[trn_idx],train_preprocessed.iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = LGBMRegressor(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=250,verbose=False)\n    preds+=model.predict(test_preprocessed)\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    #print(n+1,rmse[n])\n    print(f\"fold: {n+1}, rmse: {rmse[n]}\")\n    n+=1 ","d770c9bb":"np.mean(rmse)","b4a3ff68":"# most 20 important features for lgb model\nfrom optuna.integration import lightgbm as lgb\nlgb.plot_importance(model, max_num_features=20, figsize=(10,10))\nplt.show()","c5cb3015":"sub['target']=preds\nsub.to_csv('submission.csv', index=False)","ec14d212":"sub","6dde7724":"### Target distribution","5779ef06":"# Hi kagglers \ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f and Welcome to this competition!","f6e603a4":"## Let's import some libraries\n","9447c54f":"# Making a Submission","8ed87b21":"# Let's do some Exploratory Data Analysis (EDA)","5874304a":"# let's try LGBMRegressor","ef513f46":"# One Hot Encoding for Encoding Categorical Features ","12c8bcf2":"# I hope that you find this kernel usefull\ud83c\udfc4"}}