{"cell_type":{"14c9667e":"code","bfd33759":"code","665d62c4":"code","c00bdf11":"code","14d3e930":"code","d5d35151":"code","c6ef6ceb":"code","43dc1a76":"code","eb3c75d4":"code","b086ddb8":"code","1a317339":"markdown","f20791a7":"markdown","25aac3e7":"markdown","225f948e":"markdown","d6da625b":"markdown","62401bc7":"markdown"},"source":{"14c9667e":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import KFold\n\nx,y = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant= 0, n_clusters_per_class=1, random_state=60)\nX_train, X_test, y_train, y_test = train_test_split(x,y,stratify=y,random_state=42)","bfd33759":"%matplotlib inline\nimport matplotlib.pyplot as plt\ncolors = {0:'red', 1:'blue'}\nplt.scatter(X_test[:,0], X_test[:,1],c=y_test)\nplt.show()","665d62c4":"import numpy as np\nimport pandas as pd\ndef RandomSearchCV(x_train,y_train,classifier, param_range, folds):\n  params = np.random.uniform(param_range[0],param_range[1],10)\n  params = np.array([int(i) for i in params])\n  params = np.sort(params)\n  # Reference link : https:\/\/towardsdatascience.com\/why-and-how-to-cross-validate-a-model-d6424b45261f\n  kf = KFold(n_splits=folds)\n\n  x_train = pd.DataFrame(x_train)\n  y_train = pd.DataFrame(y_train)\n\n  TRAIN_SCORES = []\n  TEST_SCORES  = [] \n  for p in params:\n\n    training_scores = []\n    crossval_scores = []\n    classifier.n_neighbors = int(p)\n\n    for i in range(folds):\n      result = next(kf.split(x_train),None)\n      x_training = x_train.iloc[result[0]]\n      x_cv = x_train.iloc[result[1]]\n\n      y_training = y_train.iloc[result[0]]\n      y_cv = y_train.iloc[result[1]]\n      \n      model = classifier.fit(x_training,y_training)\n      training_scores.append(model.score(x_training,y_training))\n      crossval_scores.append(model.score(x_cv,y_cv))\n    TRAIN_SCORES.append(np.mean(training_scores))\n    TEST_SCORES.append(np.mean(crossval_scores))\n  return(TRAIN_SCORES , TEST_SCORES)\n     \n","c00bdf11":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nclassifier = KNeighborsClassifier()\nX_train = pd.DataFrame(X_train)\ny_train = pd.DataFrame(y_train)\ntrain_score , cv_scores = RandomSearchCV(X_train,y_train,classifier,(1,21),8)","14d3e930":"# 6. plot hyper-parameter vs accuracy plot as shown in reference notebook and choose the best hyperparameter\nimport matplotlib.pyplot as plt\nparams = np.random.uniform(1,21,10)\nparams = np.array([int(i) for i in params])\nparams = np.sort(params)\n#params = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nplt.plot(params,train_score, label='train cruve')\nplt.plot(params,cv_scores, label='test cruve')\nplt.xlabel(\"Hyperparameter k\")\nplt.ylabel(\"Accuracy\")\nplt.title('Hyper-parameter VS accuracy plot')\nplt.legend()\nplt.show()","d5d35151":"from matplotlib.colors import ListedColormap","c6ef6ceb":"neigh = KNeighborsClassifier(n_neighbors = 13)\nneigh.fit(X_train, y_train)","43dc1a76":"X1 = np.array(X_train[0])\nX2 = np.array(X_train[1])\ny = np.array(y_train)\ny = [j for sub in y for j in sub]","eb3c75d4":"# understanding this code line by line is not that importent \ndef plot_decision_boundary(X1, X2, y, clf):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n    x_min, x_max = X1.min() - 1, X1.max() + 1\n    y_min, y_max = X2.min() - 1, X2.max() + 1\n    \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n    # Plot also the training points\n    plt.scatter(X1, X2, c=y, cmap=cmap_bold)\n    \n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i)\" % (clf.n_neighbors))\n    plt.show()","b086ddb8":"plot_decision_boundary(X1,X2,y, neigh)","1a317339":"<h1>K-Fold cross validation to find best value of k for classification","f20791a7":"# Train KNN with best value of K","25aac3e7":"# Implement Random search to find best value of k","225f948e":"* As per different different experements 13 is best value of K","d6da625b":"# Implement accuracy vs hyperparameter graph to choose best value of k","62401bc7":"Implement function to plot decision boundry"}}