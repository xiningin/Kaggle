{"cell_type":{"72e23707":"code","b7fe1389":"code","4a5cd029":"code","2511787f":"code","e650f914":"code","4bd4756e":"code","452baa76":"code","b67a2d24":"code","873f1fa4":"code","63ea8b69":"code","97e76167":"code","963fcd3d":"code","94b9dab0":"code","c369d0e2":"code","20f351fd":"code","5ce8fc73":"code","185df61f":"code","05f94c7c":"code","78ef08f4":"code","e167528b":"code","6a04238a":"code","9df5fd49":"code","bc3f49cf":"code","a793ecdb":"code","c6e0a754":"code","a5d4918e":"code","8583d4ab":"code","9af208de":"code","8e0ce945":"code","fa767f0e":"code","cdcd434e":"code","ec77598e":"code","427caec5":"code","b36c003d":"markdown","3df449e8":"markdown","bf568fed":"markdown","61ff31a0":"markdown","eb46199d":"markdown","13d13ff7":"markdown","f5a27bdb":"markdown","ea9f89cc":"markdown","d0e01223":"markdown","5a905741":"markdown","7eaafb46":"markdown","271e2a18":"markdown","0e2d8953":"markdown","a7cf95ec":"markdown","63ef5bbb":"markdown","0cb74beb":"markdown","6cf53fbc":"markdown","31ce0711":"markdown","f4b35b9b":"markdown","e8d845bd":"markdown","72b6903d":"markdown","05b94dee":"markdown","e79c75bf":"markdown","75869430":"markdown"},"source":{"72e23707":"# to load, access, process and dump json files\nimport json\n# regular repression\nimport re\n# to parse HTML contents\nfrom bs4 import BeautifulSoup\n\n# for numerical analysis\nimport numpy as np \n# to store and process in a dataframe\nimport pandas as pd \n\n# for ploting graphs\nimport matplotlib.pyplot as plt\n# advancec ploting\nimport seaborn as sns\n# to create word clouds\nfrom wordcloud import WordCloud, STOPWORDS \n\n# To encode values\nfrom sklearn.preprocessing import LabelEncoder\n# Convert a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# for deep learning \nimport tensorflow as tf\n# to tokenize text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# to pad sequence \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","b7fe1389":"# import data\ndf = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\n# show first few rows\ndf.head()","4a5cd029":"# Stopwords list from https:\/\/github.com\/Yoast\/YoastSEO.js\/blob\/develop\/src\/config\/stopwords.js\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","2511787f":"# to plot n-gram\n# ==============\n\ndef plot_ngram(is_sarcastic, n):\n    \n    temp_df = df[df['is_sarcastic'] == is_sarcastic]\n    \n    word_vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(temp_df['headline'])\n    \n    frequencies = sum(sparse_matrix).toarray()[0]\n    \n    return pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\\\n            .sort_values(by='frequency', ascending=False) \\\n            .reset_index() \\\n            .head(10)","e650f914":"# to plot wordcloud\n# =================\n\ndef plot_wordcloud(headlines, cmap):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    wc = WordCloud(max_words = 1000, background_color ='white', stopwords = stopwords, \n                   min_font_size = 10, colormap=cmap)\n    wc = wc.generate(headlines)\n    plt.axis('off')\n    plt.imshow(wc)","4bd4756e":"# to plot model accuracy and loss\n# ===============================\n\ndef plot_history(history):\n    \n    plt.figure(figsize=(18, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy', c='dodgerblue', lw='2')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='orange', lw='2')\n    plt.title('Accuracy', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', c='dodgerblue', lw='2')\n    plt.plot(history.history['val_loss'], label='Validation Loss', c='orange', lw='2')\n    plt.title('Loss', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","452baa76":"# to plot confusion matrix\n# ========================\n\ndef plot_cm(pred):\n    \n    pred = pred.ravel()\n    pred = np.round(pred)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\n    cm = confusion_matrix(validation_labels, pred)\n    sns.heatmap(cm, annot=True, cbar=False, fmt='1d', cmap='Blues', ax=ax)\n\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_yticklabels(['Non-Sarcastic', 'Sarcastic', ])\n    ax.set_xticklabels(['Non-Sarcastic', 'Sarcastic'])\n\n    plt.show()","b67a2d24":"sns.set_style('darkgrid')\nplt.figure(figsize=(4, 5))\nsns.countplot(df['is_sarcastic'], palette='Dark2')\nplt.title('No. of non sarcastic and sarcastic headlines')\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()","873f1fa4":"sarc = df[df['is_sarcastic'] == 1]\nnon_sarc = df[df['is_sarcastic'] != 1]\n\nprint('No. of saracastic headlines :', len(sarc))\nprint('No. of non-saracastic headlines :', len(non_sarc))","63ea8b69":"non_sarc = non_sarc.sample(len(sarc))\ndf = pd.concat([sarc, non_sarc])\ndf = df.sample(frac=1)\n\nprint('No. of saracastic headlines :', len(sarc))\nprint('No. of non-saracastic headlines :', len(non_sarc))\n\ndf.head()","97e76167":"# removing non alphanumeric character\ndef alpha_num(text):\n    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n\n# removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwords:\n            final_text.append(i.strip())\n    return \" \".join(final_text)","963fcd3d":"# apply preprocessing steps\n\ndf['headline'] = df['headline'].str.lower()\ndf['headline'] = df['headline'].apply(alpha_num)\ndf['headline'] = df['headline'].apply(remove_stopwords)\n\ndf.head()","94b9dab0":"# word cloud of saracastic headlines\nsarcastic = ' '.join(df[df['is_sarcastic']==1]['headline'].to_list())\nplot_wordcloud(sarcastic, 'Reds')","c369d0e2":"# word cloud of non-saracastic headlines\nnon_sarcastic = ' '.join(df[df['is_sarcastic']==0]['headline'].to_list())\nplot_wordcloud(non_sarcastic, 'Blues')","20f351fd":"# n-grams of non-saracastic headlines\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle('Non-Sarcastic Headlines', ha='left', fontsize=16)\nplt.subplots_adjust(wspace=0.7)\naxes = axes.flatten()\n\ntitles = ['Unigram', 'Bigram', 'Trigram']\n\nfor i in range(3):\n    sns.barplot(data=plot_ngram(0, i+1), y='index', x='frequency', ax=axes[i])\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].set_title(titles[i], loc='left')","5ce8fc73":"# n-grams of non-saracastic headlines\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle('Sarcastic Headlines', ha='left', fontsize=16)\nplt.subplots_adjust(wspace=0.7)\naxes = axes.flatten()\n\ntitles = ['Unigram', 'Bigram', 'Trigram']\n\nfor i in range(3):\n    sns.barplot(data=plot_ngram(1, i+1), y='index', x='frequency', ax=axes[i])\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].set_title(titles[i], loc='left')","185df61f":"# container for sentences\nheadlines = np.array([headline for headline in df['headline']])\n\n# container for labels\nlabels = np.array([label for label in df['is_sarcastic']])","05f94c7c":"# train-test split\ntrain_sentences, validation_sentences, train_labels, validation_labels = train_test_split(headlines, labels, \n                                                                                          test_size=0.33, \n                                                                                          stratify=labels)","78ef08f4":"# model parameters\nvocab_size = 1200\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","e167528b":"# tokenize sentences\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\n# convert train dataset to sequence and pad sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\n# convert validation dataset to sequence and pad sequences\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","6a04238a":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nprint(model.summary())","9df5fd49":"# fit model\nnum_epochs = 20\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=1,\n                    validation_split=0.3)\n\n# predict values\npred = model.predict(validation_padded)","bc3f49cf":"# plot history\nplot_history(history)","a793ecdb":"# plot confusion matrix\nplot_cm(pred)","c6e0a754":"# reviews on which we need to predict\nsentence = [\"Breathing oxygen related to staying alive\", \n            \"Safety meeting ends in accident\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","a5d4918e":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","8583d4ab":"# fit model\nnum_epochs = 20\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=1,\n                    validation_split=0.3)\n\n# predict values\npred = model.predict(validation_padded)","9af208de":"# plot history\nplot_history(history)","8e0ce945":"# plot confusion matrix\nplot_cm(pred)","fa767f0e":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","cdcd434e":"# fit model\nnum_epochs = 20\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=1,\n                    validation_split=0.3)\n\n# predict values\npred = model.predict(validation_padded)","ec77598e":"# plot history\nplot_history(history)","427caec5":"# plot confusion matrix\nplot_cm(pred)","b36c003d":"## About the Data","3df449e8":"## With Convolution","bf568fed":"This News Headlines dataset for Sarcasm Detection is collected from two news website. \n    \n1. **TheOnion**   \n> TheOnions aims at producing sarcastic versions of current events and the data are headlines from News in Brief and News in Photos categories *(which are sarcastic)*.   \n      \n       \n2. **HUffPost**  \n> Real *(and non-sarcastic)* news headlines are from **HuffPost**","61ff31a0":"## Import Libraries","eb46199d":"### Equalize no. of headlines","13d13ff7":"> 1. Can you identify sarcastic sentences? \n> 2. Can you distinguish between fake news and legitimate news?","f5a27bdb":"From : https:\/\/www.kaggle.com\/madz2000\/sarcasm-detection-with-glove-word2vec-82-accuracy\n","ea9f89cc":"## Utility Function","d0e01223":"## Glove","5a905741":"## Stopwords","7eaafb46":"## Data Preprocessing","271e2a18":"## With Word Embedding","0e2d8953":"> Disparity in the no. headlines in each category would lead to a biased model.  \n> We need to make sure that there are equal no. of sarcastic and non sarcastic headlines.  \n> We will have drop some non sarcastic headlines.  ","a7cf95ec":"## With LSTM","63ef5bbb":"## Train-test split","0cb74beb":"## BERT","6cf53fbc":"## Model parameters","31ce0711":"## Data","f4b35b9b":"## Why Classifying ?","e8d845bd":"## is_sarcastic column\n> ***0***  - indicates real \/ Non saracastic headline   \n> ***1***  - indicates sarcastic headline","72b6903d":"## Plot","05b94dee":"### No. of headlines in each category","e79c75bf":"## Get headlines and labels","75869430":"## Tokenize and Sequence text"}}