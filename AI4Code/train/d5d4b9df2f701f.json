{"cell_type":{"bbde9661":"code","5cc05939":"code","226f2364":"code","364c1d98":"code","412bda27":"code","ed8983eb":"code","c5a17b45":"code","64fad498":"code","e6218ea6":"code","f00cfd3f":"code","b07ee8d1":"code","500f33c8":"code","9e606ecc":"code","d4dbdbff":"code","bed8cf55":"code","139a79e6":"code","635976a0":"code","cd7db7ad":"code","f17a22a7":"code","d14d63a2":"code","a266882b":"code","1c091830":"code","5fb22700":"code","869f429d":"code","205cee54":"code","5cdbf4c2":"code","45f6f19b":"code","ee02ad4e":"code","5e99ffbc":"markdown","c97f0078":"markdown","caf59ef1":"markdown","fa78197a":"markdown","6f8bbf3e":"markdown","1a900a7f":"markdown","1c98eb2e":"markdown","3a9bcdf2":"markdown","e61eff32":"markdown","0a9a87fa":"markdown","1422f819":"markdown","87dbb768":"markdown","d36bb099":"markdown","9b31fb99":"markdown","27f0ecf9":"markdown","9e257969":"markdown","b7640333":"markdown","d1f48b59":"markdown","bf417052":"markdown","856d3863":"markdown","51606f74":"markdown","98dc7c8c":"markdown","fea38873":"markdown"},"source":{"bbde9661":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5cc05939":"import matplotlib.pyplot as plt\nimport seaborn as sns","226f2364":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain.head()","364c1d98":"#Train Data - Check for Null Values \nfor column in train.columns:\n    if(train[column].isnull().sum() != 0):\n        print('Feature : ', column , ' ------ No of nulls : ',train[column].isnull().sum(), ' ----- Per : ',train[column].isnull().sum()*100\/1460)","412bda27":"train = train.drop(columns=['LotFrontage','Alley','FireplaceQu','PoolQC','Fence','MiscFeature'], axis=1)\n\ntest = test.drop(columns=['LotFrontage','Alley','FireplaceQu','PoolQC','Fence','MiscFeature'], axis=1)","ed8983eb":"for column in train.columns:\n    if(train[column].isnull().sum() != 0):\n        print('Feature : ', column , ' # No of nulls : ', train[column].isnull().sum(), ' # Per : ', train[column].isnull().sum()*100\/1460)","c5a17b45":"train = train.drop(columns=['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', \n                            'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MasVnrArea'], \n                            axis=1)\n\ntest = test.drop(columns=['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', \n                            'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MasVnrArea'], \n                            axis=1)","64fad498":"train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n\n#Test dataset doesnot contain any Null value for 'Eletrical' column\n#print(test['Electrical'].isnull().sum()) \n#give 0 as result","e6218ea6":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 20))\n\nsns.set(font_scale=1)\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","f00cfd3f":"#SalePrice Correlation with following columns is less than 10%. Let's drop these columns.\ntrain = train.drop(columns=['MSSubClass', 'OverallCond', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2',\n                              'LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'], axis=1)\n\ntest = test.drop(columns=['MSSubClass', 'OverallCond', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2',\n                              'LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'], axis=1)","b07ee8d1":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 20))\n\nsns.set(font_scale=1)\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","500f33c8":"#bivariate analysis saleprice\/overallqual\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","9e606ecc":"#deleting points\ntrain.loc[(train['OverallQual'] == 10) & (train['SalePrice'] < 200000)]\n\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)","d4dbdbff":"#bivariate analysis saleprice\/totalbsmtsf\nvar = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","bed8cf55":"#bivariate analysis saleprice\/1stflrsf\nvar = '1stFlrSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","139a79e6":"#deleting points\ntrain.sort_values(by = '1stFlrSF', ascending = False)[:2]\n\ntrain = train.drop(train[train['Id'] == 497].index)\ntrain = train.drop(train[train['Id'] == 1025].index)","635976a0":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","cd7db7ad":"#bivariate analysis saleprice\/overallqual\nvar = 'GarageCars'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f17a22a7":"#bivariate analysis saleprice\/garagearea\nvar = 'GarageArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","d14d63a2":"#deleting points\ntrain.sort_values(by = 'GarageArea', ascending = False)[:3]\n\ntrain = train.drop(train[train['Id'] == 582].index)\ntrain = train.drop(train[train['Id'] == 1062].index)\ntrain = train.drop(train[train['Id'] == 1191].index)","a266882b":"#Checking Null values in Test Dataset\ntest.isnull().sum()","1c091830":"#Filling null values with Mode\nfor column in test.columns:\n    if(test[column].isnull().sum() != 0):\n        test[column].fillna(test[column].mode()[0], inplace=True)","5fb22700":"train['train']  = 1\ntest['train']  = 0\ndf = pd.concat([train, test], axis=0, sort=False)\nprint(df.shape)","869f429d":"#Convert categorical variable into dummy\ndf = pd.get_dummies(df)\ndf.head()","205cee54":"df_final = df.drop(['Id'], axis=1)\n\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train'], axis=1)\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['train'], axis=1)\ndf_test = df_test.drop(['SalePrice'], axis=1)","5cdbf4c2":"target = df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df_train, target, test_size=0.2, random_state=0)","45f6f19b":"from lightgbm import LGBMRegressor\nimport sklearn.metrics as metrics\nimport math\n\nlgbm2 = LGBMRegressor(objective='regression', \n                                       num_leaves=8,\n                                       learning_rate=0.005, \n                                       n_estimators=15000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       )\n\nlgbm2.fit(x_train, y_train, eval_metric='rmse')\n\nlgbm2_pred = lgbm2.predict(x_test)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, lgbm2_pred))))","ee02ad4e":"lgbm2.fit(df_train, target, eval_metric='rmse')\nlgbm2_pred_allTest = lgbm2.predict(df_test)\n\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": lgbm2_pred_allTest * 2})\nsubmission.to_csv('Final_submission_best.csv', index=False)","5e99ffbc":"Let's drop records with 1stFlrSF value greater than 2750 as their SalePrice is not following trend here...","c97f0078":"So. with this Our Train Dataset is ready for Model fitting...","caf59ef1":"## **Separating Features and Label**","fa78197a":"## **GarageArea**","6f8bbf3e":"# **Fitting With all the dataset**","1a900a7f":"## **Model**","1c98eb2e":"Let's keep this column values as it is...","3a9bcdf2":"Two records of OverallQual with Quality = 10 and SalePrice < 200000 seem to be Outliers and not fitting the treding price.\nSo, We will remove these two records.","e61eff32":"## **GarageCars**","0a9a87fa":"Here, We will remove Top Three values of GarageArea as it is not following the trend...","1422f819":"## **GrLivArea**","87dbb768":"We will drop columns that are having more than 15% Null values","d36bb099":"In this notebook, I would like to perform Outlier removal for Training Data to improve the results.\nWe will not merge Train and Test datasets here, because after that It becomes difficult to apply Bivariate Analysis on Label column 'SalePrice'. (Because SalePrice colun is not available in Test Dataset.)\n\nWe will perform operations regarding column removals on Train data and Test data separately.\n\nLet see what we get....!! ","9b31fb99":"We can see that 'GarageX' variables have the same number of missing data. Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'GarageX' variables. The same logic applies to 'BsmtX' variables.\n\nRegarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' which is already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation from Training Data and keep the variable.","27f0ecf9":"## **TotalBsmtSF**","9e257969":"## **OverallQual**","b7640333":"With Respect To \"SalePrice\", Following columns seem to be important as they share more than 60% correlation factor : <br>\n\n* OverallQual\n* TotalBsmtSF\n* 1stFlrSF\n* GrLivArea\n* GarageCars\n* GarageArea","d1f48b59":"Leaving this feature as it is...","bf417052":"## **1stFlrSF**","856d3863":"This Feature looks pretty good... We will keep it as it is...","51606f74":"## **Convert categorical variable into dummy**\nFor this, We will mix train and test dataset and apply OneHotEncoding. <br>\nReason is : <br>\nSometimes Test Dataset doesnot contain all the possible categories of a column that is available in Train Dataset.\nAnd, In such cases, when we apply pd.get_dummies on Train and Test Dataset separately, We get our columns mismatched.","98dc7c8c":"# **Cheking Correlation of Training Data**","fea38873":"# **Let's Perform Bivariate Analysis and Outlier Removals for Columns that are Highly Correlated with \"SalePrice\" column**"}}