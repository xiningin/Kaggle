{"cell_type":{"a4ed011d":"code","3bd08008":"code","2307c693":"code","8a4a6272":"code","20e57ed0":"code","14e8dc5e":"code","a99a9600":"code","33ba57cc":"code","e2ea366f":"code","8b8ff0e5":"code","649c7387":"code","8064ff01":"code","84fd1575":"markdown","3fe48d21":"markdown","d2e3f2ae":"markdown","bca1fd36":"markdown","340d6fe8":"markdown","abefb5b9":"markdown","c9b17aa6":"markdown","29086277":"markdown","eac68483":"markdown","fe9edd86":"markdown","79ce6b4c":"markdown","dc85836d":"markdown","31eb4276":"markdown","d78bef36":"markdown","417976bc":"markdown","98fae569":"markdown","fc357678":"markdown","1dea2ec4":"markdown","8d159cc6":"markdown","78933817":"markdown","3c9da5c7":"markdown","7b5774bd":"markdown","58846cce":"markdown","3ee4a94b":"markdown"},"source":{"a4ed011d":"import sys #Used exclusively to get ja_core_news_lg, which does not save between sessions\n!{sys.executable} -m spacy download ja_core_news_lg #Downloads ja_core_news_lg, which is used for spacy Japanese processing","3bd08008":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport html # HTML content, like &amp;\n\nimport spacy #For general NLP\nfrom spacy.lang.ja.stop_words import STOP_WORDS #Get the Japanese stopwords\n\nimport ja_core_news_lg #Japanese language handling\nnlp =  ja_core_news_lg.load() #Initializing Spacy for Japanese\nimport operator #For dictionary sorting\n\nimport matplotlib.pylab as plt #For plot testing\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2307c693":"tweetData = pd.read_csv(\"..\/input\/ml-tweet\/Twitter ML Data.csv\") #Load the dataset into pandas\ntweetData.head() #Take a peek at the dataset","8a4a6272":"tweetData = tweetData.rename(columns = {\"Name\" : \"Tweeter\", \"2\" : \"OriginalTweet\", \"3\" : \"TweetLink\", \"4\" : \"TweetTime\"}) #Changes the column names to something more helpful\ntweetData.head() #Take a peek at the dataset","20e57ed0":"print(tweetData.count()) #Get the counts of values in the dataset\n\n#Check each column if there are any null values\nprint(tweetData[\"Tweeter\"].isnull().any())\nprint(tweetData[\"OriginalTweet\"].isnull().any())\nprint(tweetData[\"TweetLink\"].isnull().any())\nprint(tweetData[\"TweetTime\"].isnull().any())\nprint(tweetData[\"5\"].isnull().any())\nprint(tweetData[\"6\"].isnull().any())","14e8dc5e":"tweetData = tweetData.drop(columns = [\"5\", \"6\"]) #Drop the null columns\ntweetData.head() #Take a peek at the dataset","a99a9600":"tweetData[\"OriginalTweet\"] = tweetData[\"OriginalTweet\"].apply(lambda x: np.nan if not x else x) #Change blank tweets to null as well\ntweetData.dropna(subset = [\"OriginalTweet\"], inplace = True) #Drop the nulls based on the tweet data\ntweetData.reset_index(drop=True, inplace=True) #Reset the index for later looping\ntweetData.head() #Take a peek at the dataset","33ba57cc":"#Check each column if there are any null values\nprint(tweetData[\"Tweeter\"].isnull().any())\nprint(tweetData[\"OriginalTweet\"].isnull().any())\nprint(tweetData[\"TweetLink\"].isnull().any())\nprint(tweetData[\"TweetTime\"].isnull().any())","e2ea366f":"#Modified from my previous nlp project: https:\/\/www.kaggle.com\/lunamcbride24\/coronavirus-tweet-processing\n\npunctuations = \"\"\"!()\uff08\uff09\u300c\u300d\u3001-!\uff01[]{};:+'\"\\,<>.\/?@#$%^&*_~\u00c2\u3002\u2026\u30fb\uff0c\u3010\u3011\uff1f\"\"\" #List of punctuations to remove, including a weird A that will not process out any other way\n\nstopwords = spacy.lang.ja.stop_words.STOP_WORDS\n\n#CleanTweets: parces the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    for i in range(0,len(tweetParse)):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"RT\", ' ', tweet)\n        tweet = re.sub(r\"\\n\", ' ', tweet)\n        tweet = re.sub(r\"@\\w+\", ' ', tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r'https\\S+', ' ', tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        tweet = re.sub(r\"\\d+\", ' ', tweet) #Removes numbers, as well as cases like the \"th\" in \"14th\"\n        tweet = ''.join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = nlp.tokenizer(tweet) #Splits the tweet into individual words\n        tweetParse[i] = ''.join([word.orth_ + \" \" for word in tweetWord if word.is_stop == False]) #Checks if the words are stop words\n       \n        \n    return tweetParse #Returns the parsed tweets\n\ntweets = tweetData[\"OriginalTweet\"].copy() #Gets a copy of the tweets to send to the function call\ntweetData[\"CleanTweet\"] = cleanTweets(tweets) #Adds a CleanTweet column and fills it with processed tweets\nprint(tweetData[\"OriginalTweet\"][3], \"\\n \\n\", tweetData[\"CleanTweet\"][3]) #Prints an example sentence\ntweetData.head() #Takes a peek at the dataframe","8b8ff0e5":"print(list(stopwords)) #Print the stopwords for reference","649c7387":"tweets = tweetData[\"CleanTweet\"].copy() #Copy the clean tweets for processing\ncount = dict() #Creates a dictionary \nfor i in range(0,len(tweets)):\n    words = tweets[i].split()\n    for word in words:\n        if word in count:\n            count[word] += 1\n        else:\n            count[word] = 1\n\nsortCount = {word : summ for word, summ in sorted(count.items(), key=operator.itemgetter(1),reverse=True)}\nprint(dict(list(sortCount.items())[0: 20]))","8064ff01":"top = dict(list(sortCount.items())[0: 20])\nfig, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(16,8))\nplt.bar(top.keys(), top.values(), color = \"g\")","84fd1575":"# Get word counts","3fe48d21":"Quick source list: https:\/\/www.geeksforgeeks.org\/python-get-first-n-keyvalue-pairs-in-given-dictionary\/ , https:\/\/www.w3resource.com\/python-exercises\/dictionary\/python-data-type-dictionary-exercise-1.php ","d2e3f2ae":"---","bca1fd36":"It appears there are no values at all in 5 and 6, so those should be removed entirely. There are also null rows, as shown in the head functions above, so I will just drop those with null tweets and see where to go from there","340d6fe8":"---","abefb5b9":"# Natural Language Processing Project: Japanese ML Tweets","c9b17aa6":"---","29086277":"# Japanese Stopwords","eac68483":"# Load the dataset","fe9edd86":"---","79ce6b4c":"# Tweet cleaning","dc85836d":"Coded by Luna McBride\n\nThe point of this notebook is to test Japanese language processing.","31eb4276":"# Matplotlib Japanese Test","d78bef36":"Interestingly, there are several top options here that I would consider stopwords, but are not in the stopwords list. For example, \u3051\u3069, which means but, but is also used in sentences to imply another sentence (which is not something a machine can likely pick up on). The other words I would consider stopwords in the top 20 would be \u7684, \u3066\u308b, \u3063\u3066, \u7b2c, and \u6b21.","417976bc":"As I sort of expected, Matplotlib does not recognize Japanese characters. It is the whole reason I wanted to test this.","98fae569":"# Check for and remove null values","fc357678":"---","1dea2ec4":"---","8d159cc6":"# Conclusion","78933817":"That took out all the null values. ","3c9da5c7":"---","7b5774bd":"---","58846cce":"# Add column names","3ee4a94b":"The spacy Japanese is pretty strong. Of course, I tried NLTK Japanese and could not get it to work at all, so at the very least, getting spacy up and working is pretty big. In terms of processing, Japanese is considered a big, single block by functions like split(). This thus requires tokenizing followed by getting the individual word out in order to actually process the text. And even when tokenized, words like \u6a5f\u68b0\u5b66\u7fd2 (Machine Learning) that are made of two words are split into the two words, which is something to look out for. English may have concepts like this that are two words like machine learning, but Japanese words like this this often hold more meaning together than as the sum of their parts. \n\nThis is followed by a check with the stopwords, which does not include some words that I would consider stopwords, so that is another thing to look out for when processing Japanese. One more key part of this is matplotlib, as it does not recognize Japanese characters at all. It just leaves boxes in the place of a character. This means numeric data like model accuracy hold a lot more value when working with Japanese than categorical data that would typically be powerful during exploration. \n\nAs for the dataset itself, it is pretty barebones. It is machine learning, so the most common words (besides those I would consider stopwords), are expectedly machine, learning, AI, programming, and python. There are also words like ability (\u53ef\u80fd) and world (\u4e16\u754c), so likely some optimistic discussion on changing the world and learning ability, similar to how it is discussed in English. "}}