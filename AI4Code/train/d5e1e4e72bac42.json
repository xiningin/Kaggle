{"cell_type":{"f884bc22":"code","237a055f":"code","6eeb40fb":"code","e70a38be":"code","340b1f54":"code","9898dcfe":"code","4b926045":"code","cd562ec3":"code","36091771":"code","11156a5a":"code","e2bf0fae":"code","83f31837":"code","48033e9e":"code","bdb39c5a":"code","63cb6fd6":"code","dca2b635":"code","02ebf711":"code","3577fdfa":"code","ae91e79e":"code","81ac1224":"code","2c51fa37":"code","c01680bc":"code","e0c1e97d":"markdown","1e8d5c4b":"markdown","24dd64b1":"markdown","2902a60a":"markdown","ad23216e":"markdown","123b9fb7":"markdown","38156bd3":"markdown","748eae4a":"markdown","544b4e0f":"markdown","056f6fd9":"markdown","4c53e70c":"markdown","07ec90ad":"markdown","cdb0447a":"markdown"},"source":{"f884bc22":"#Importing the required libraries to read,visualize and model the givn dataset files\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport missingno as msno \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\n\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom platform import python_version\nprint (python_version())\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","237a055f":"df_train = pd.read_csv('..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip', delimiter=\"\\t\")\ndf_train.head()","6eeb40fb":"df_train = df_train.drop(['id'], axis=1)\ndf_train.head()","e70a38be":"df_test =pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()","340b1f54":"print(\"-\"*50)\nprint('Shape of the dataframe:',df_train.shape)\nprint(\"Number of records in train data set:\",df_train.shape[0])\nprint(\"Information of the dataset:\")\ndf_train.info()\nprint(\"-\"*50)\nprint(\"First 5 records of the dataset:\")\ndf_train.head()\nprint(\"-\"*50)","9898dcfe":"print('Sentiment of 0 is {} % of total'.format(round(df_train['sentiment'].value_counts()[0]\/len(df_train['sentiment'])*100)))\nprint('Sentiment of 1 is {} % of total'.format(round(df_train['sentiment'].value_counts()[1]\/len(df_train['sentiment'])*100)))\nx=df_train.sentiment.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","4b926045":"import re\nimport string\n# Create a function to clean the text\n\ndef clean_text(text):\n\n    '''\n    Input- 'text' to be cleaned\n       \n       Output- Convert input 'text' to lowercase,remove square brackets,links,punctuation\n       and words containing numbers. Return clean text.\n    \n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","cd562ec3":"# Lets apply the clean_text function to both test and training datasets copies\ndf_train1=df_train.copy()\ndf_test1=df_test.copy()\ndf_train1['review'] = df_train1['review'].apply(lambda x: clean_text(x))\ndf_test1['review'] = df_test1['review'].apply(lambda x: clean_text(x))","36091771":"#Lets look cleaned text data\ndef text_after_preprocess(before_text,after_text):\n    \n    '''\n    Input- before_text=text column before cleanup\n              after_text= text column after cleanup\n       Output- print before and after text to compare how it looks after cleanup\n       \n    '''\n    print('-'*60)\n    print('Text before cleanup')\n    print('-'*60)\n    print(before_text.head(5))\n    print('-'*60)\n    print('Text after cleanup')\n    print('-'*60)\n    print(after_text.head(5))","11156a5a":"text_after_preprocess(df_train.review,df_train1.review)\n","e2bf0fae":"import nltk\n\n# Example how tokenization of text works\ntext = \"Heard about #earthquake is different cities, stay safe everyone.\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\nprint(\"-\"*100)\nprint(\"Example Text: \",text)\nprint(\"-\"*100)\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","83f31837":"# Lets Tokenize the training and the test dataset copies with RegEx tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ndf_train1['review'] = df_train1['review'].apply(lambda x: tokenizer.tokenize(x))\n\ndf_test1['review'] = df_test1['review'].apply(lambda x: tokenizer.tokenize(x))","48033e9e":"#Lets checy tokenized text\ndf_train1['review'].head()","bdb39c5a":"df_train1['review'].head()","63cb6fd6":"# Stemming and Lemmatization examples\ntext =  \"ran deduced dogs talking studies\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","dca2b635":"# Lets combine text after processing it\ndef combine_text(text):\n    \n    '''\n    Input-text= list cleand and tokenized text\n    Output- Takes a list of text and returns combined one large chunk of text.\n    \n    '''\n    all_text = ' '.join(text)\n    return all_text","02ebf711":"df_train1['review'] = df_train1['review'].apply(lambda x : combine_text(x))\ndf_test1['review'] = df_test1['review'].apply(lambda x : combine_text(x))","3577fdfa":"!wget -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\n!gunzip GoogleNews-vectors-negative300.bin.gz","ae91e79e":"\"\"\" Load and index the GoogleNews word2vec file \"\"\"\nfrom gensim.models import KeyedVectors\n\n\nprint('Indexing word vectors')\nword2vec = KeyedVectors.load_word2vec_format(\".\/GoogleNews-vectors-negative300.bin\", binary=True)\nprint('Found %s word vectors of word2vec' % len(word2vec.key_to_index))\n","81ac1224":"\"\"\" Compute feature representation for each text\/tweet: mean Word2vec \"\"\"\nmV = []\nfor sentence in df_train1['review'].values:\n    #print(sentence)\n    vectors = []\n    for x in sentence.split(' '):\n        try:\n            v = word2vec[x]\n            vectors.append(v)\n        except:\n            i = False # meaningless\n            #print(\"%s not found\"%x)\n    vectors = np.array(vectors)\n    # We use the mean of the words' vectors, \n    # but inner-product might be a more widely accepted solution\n    meanw2v = np.mean(vectors,axis=0)\n    mV.append(meanw2v)\nmV = np.array(mV)\nprint(mV.shape)","2c51fa37":"\"\"\" The data is initially imbalanced. We want to train our models with equal positive and negative classes. The imblearn pacakge\nhas a number of options to deal with imbalance. We use the simple undersampling.\"\"\"\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n\nY = [[int(sent==\"1\")] for sent in np.array(df_train1['sentiment'])]\nX = np.copy(mV)\n\nrus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(X, Y)\nprint(\"Positive and negative samples (after random undersampling):\")\nprint(X_resampled.shape,y_resampled.shape)\nprint(sorted(Counter(y_resampled.T[0]).items()))","c01680bc":"\"\"\" Train\/test split and classification\"\"\"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled.ravel(), test_size = 0.3, \n                                                    random_state = 42, stratify=y_resampled, shuffle=True)\nprint(X_train.shape,y_train.shape)\nprint(\"Negative and positive examples in training:\",sorted(Counter(y_train.tolist()).items()))\nprint(X_test.shape,y_test.shape)\nprint(\"Negative and positive examples in testing:\",sorted(Counter(y_test.tolist()).items()))\n\n\"\"\" The typical knn \"\"\"\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"k-NN Score:\",knn.score(X_test, y_test))\n\n\"\"\" RandomForestClassifier \"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nimport math\nMAX_FEAT = int(math.sqrt(X_train.shape[1])) # this is a rule of thumb\nclf = RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators = 40, max_features = MAX_FEAT)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprint(\"RF Score:\",clf.score(X_test, y_test))\n\n\"\"\" Gradient Boosting \"\"\"\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbrt =  GradientBoostingClassifier(random_state = 0)\ngbrt.fit(X_train,y_train)\ny_pred = gbrt.predict(X_test)\nprint(\"GBRT Score:\", gbrt.score(X_test, y_test))\n\n\"\"\" Vanilla MLP \"\"\"\nfrom sklearn.neural_network import MLPClassifier\nfor a in [0.0001,0.01,0.1,1]:\n    mlp = MLPClassifier(solver = \"adam\",activation=\"relu\", random_state = 0, \n                        hidden_layer_sizes = [100,100], alpha = a)\n    mlp.fit(X_train,y_train)\n    y_pred = mlp.predict(X_test)\n    print(\"MLP Score:\",mlp.score(X_test, y_test), \" alpha:\",a)","e0c1e97d":"# Class distribution\n## lets see the class distibution for 0 and 1\n","1e8d5c4b":"we will be removing the id column bc it is useless","24dd64b1":"# Project Overview\n\nThis tutorial will help you get started with Word2Vec for natural language processing. It has two goals: \n\n* Basic Natural Language Processing: Part 1 of this tutorial is intended for beginners and covers basic natural language processing techniques, which are needed for later parts of the tutorial.\n\n* Deep Learning for Text Understanding: In Parts 2 and 3, we delve into how to train a model using Word2Vec and how to use the resulting word vectors for sentiment analysis.\n\nSince deep learning is a rapidly evolving field, large amounts of the work has not yet been published, or exists only as academic papers. Part 3 of the tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output.\n\nTo achieve these goals, we rely on an IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative. ","2902a60a":"Reading the data","ad23216e":"### Tokenization\nTokenization is a process which splits an input text into tokens and the tokens can be a word, sentence, paragraph etc.\n\nFollowing code will show how tokenization of text works:","123b9fb7":"** # Download pretrained word2vec model from Google **\n\nThis tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing","38156bd3":"## Vectorization of text\nPre-process text needs to be transformed into a vetor matrix of numbers before a machine learning model can undertsand it and learn from it.This can be done by a number of tecniques:\n\n### Bag of Words\nThe bag of words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\nA vocabulary of known words.\nA measure of the presence of known words.\nWhy is it is called a \u201cbag\u201d of words? Its called bag of words because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.\n\nBag of Words - Countvectorizer Features\u00b6\nCountvectorizer converts a collection of text documents to a matrix of token counts. It is important to note that CountVectorizer comes with a lot of options to automatically do preprocessing, tokenization, and stop word removal. However, all the pre-processing of the text has already been performed by creating a function.Only vanilla version of Countvectorizer will be used.","748eae4a":"# The Problem\nThe problem to be solved by this project is how to identify which Reviews as a positive sentiment and which is not .\nThis project will build a machine learning model to predict which Reviews has a positive sentiment and which one\u2019s aren\u2019t.","544b4e0f":"### Stemming and Lemmatization\n**Stemming**: Removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\n\n\n![![image.png](attachment:ab776c63-e75d-480e-9d71-184e666c3a57.png)]\n\n\n**Lemmatization** : Returns the base or dictionary form of a word, which is known as the lemma\n\n![![image.png](attachment:fb0b579f-98b2-404a-9a15-82edcba1ef7d.png)]\n\nIt is important to note that stemming and lemmatization sometimes doesnt necessarily improve results as sometimes we dont want to trim words rather preserve their original form.\nIts usage from problem to problem and for this problem it wouldnt be good idea to use it.","056f6fd9":"# **Bag of Words Meets Bags of Popcorn**\nPredict which Reviews is positive and which is negative.","4c53e70c":"## Data cleaning \nBefore starting any NLP project, text data needs to be pre-processed to convert it into in a consistent format.Text will be cleaned, tokneized and converted into a matrix.\n\nSome of the basic text pre-processing techniques includes:\n\nMake text all lower or uppercase\nAlgorithms does not treat the same word different in different cases.\n\n### Removing Noise\nEverything in the text that isn\u2019t a standard number or letter i.e. Punctuation, Numerical values,etc.\n\n### Tokenization\nTokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e. words.Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n\n### Stopword Removal\nSometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n\n### Stemming\nStemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form\u200a\u2014\u200agenerally a written word form. Example if we were to stem the following words: \u201cStems\u201d, \u201cStemming\u201d, \u201cStemmed\u201d, \u201cand Stemtization\u201d, the result would be a single word \u201cstem\u201d.\n\n### Lemmatization\nA slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that \u201crun\u201d is a base form for words like \u201crunning\u201d or \u201cran\u201d or that the word \u201cbetter\u201d and \u201cgood\u201d are in the same lemma so they are considered the same.","07ec90ad":"# Data Preprocessing","cdb0447a":"### Stopwords Removal\nNow, let's get rid of the stopwords i.e words which occur very frequently and have possible value like a, an, the, are etc."}}