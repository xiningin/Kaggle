{"cell_type":{"f389566c":"code","ecdd86fb":"code","0701949a":"code","6c432abd":"code","3e738ba7":"code","fb63bfba":"code","385e31b7":"code","ebf3a528":"code","7e654acb":"code","1b43ef82":"code","30de7aae":"code","9fe05739":"code","68ab38e6":"code","55f096ac":"code","ea0db9bf":"code","6cac427d":"code","80e86a40":"code","64b69a83":"code","d71d40b8":"code","a712eb5b":"code","1e102be8":"code","720b7f3c":"code","dd0ef872":"markdown","a0c211f6":"markdown","5c8c7809":"markdown","7b244c0b":"markdown","07ef7279":"markdown","3c1db7a8":"markdown","cf9bb922":"markdown","584a4d90":"markdown","fa6a497a":"markdown","7cfe2ff8":"markdown","d36c10b4":"markdown","448288e0":"markdown","fa2df65e":"markdown","186f8ddf":"markdown","be3c2f41":"markdown"},"source":{"f389566c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ecdd86fb":"from sklearn.datasets import load_breast_cancer","0701949a":"df = load_breast_cancer()","6c432abd":"df.keys()","3e738ba7":"print(df['DESCR'])","fb63bfba":"df['feature_names']","385e31b7":"df_feat = pd.DataFrame(df['data'],columns=df['feature_names'])\ndf_feat.info()","ebf3a528":"df_feat.head()","7e654acb":"df['target']","1b43ef82":"df['target_names']","30de7aae":"df_target = pd.DataFrame(df['target'],columns=['Cancer'])","9fe05739":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.30, random_state=101)\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\n\npredictions = model.predict(X_test)","68ab38e6":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","55f096ac":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} ","ea0db9bf":"from sklearn.model_selection import GridSearchCV","6cac427d":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n","80e86a40":"# May take awhile!\ngrid.fit(X_train,y_train)","64b69a83":"grid.best_params_","d71d40b8":"grid.best_estimator_","a712eb5b":"grid_predictions = grid.predict(X_test)","1e102be8":"print(confusion_matrix(y_test,grid_predictions))","720b7f3c":"print(classification_report(y_test,grid_predictions))","dd0ef872":"What fit does is a bit more involved then usual. First, it runs the same loop with cross-validation, to find the best parameter combination. Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to built a single new model using the best parameter setting.","a0c211f6":"## Get the Data\n\nWe'll use the built in breast cancer dataset from Scikit Learn. We can get with the load function:","5c8c7809":"Then you can re-run predictions on this grid object just like you would with a normal model.","7b244c0b":"The data set is presented in a dictionary form:","07ef7279":"## EVALUATIONS\n\nNow let's predict using the trained model.","3c1db7a8":"# Gridsearch\n\nIt helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters. This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV! The CV stands for cross-validation which is the\n\nGridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested. ","cf9bb922":"## Set up DataFrame","584a4d90":"Dictionary where the keys are the parameters that go into the model.\nC controls the cost of misclassification on the training data.\n\nA large C value gives you low bias and high variance. low bias because you penalize the cost of misclassification a lot\nsmaller C value = higher bias, lower variance\n\nGamma parameter-Gamma is large then the variance is small implying that the support vector does not have a widespread influence.\nThese are paramaters that can be adjusted with the GridSearch function\n\n","fa6a497a":"# WHAT IS A SUPPORT VECTOR MACHINE?\n\nThey are algorithims that analyze data and recognize patterns.\n\nTheir use is for classification and regression analysis.\n\nPoints that are mapped in space, in seperate catagories that are clearly seperated by a gap. New points are then mapped in this space and predicted to belong to either category.\n\nWhen we add a new point does it belong to the red or the green class?\n\n![image.png](attachment:163a8f97-8eb3-4430-923d-5f7284afbd71.png)\n\nThe vector points that the margin line touches are the support vector points.  They are the training points.\n\nNon-linear seperable data can not be seperated with a straight ine.\nWe use the kernel trick to use the Z-label to view in a 3rd dimension with another hyperplane.\n![Untitled 3.png](attachment:1a1d3c8a-5288-4082-8c5c-74ba478d0fd8.png)\n","7cfe2ff8":"## TRAIN TEST SPLIT AND PREDICTIONS","d36c10b4":"Model doing a little better after the GridSearch","448288e0":"Shows the target column of the df. 0 and 1's","fa2df65e":"We can grab information and arrays out of this dictionary to set up our data frame and understanding of the features:","186f8ddf":"You can inspect the best parameters found by GridSearchCV in the best_params_ attribute, and the best estimator in the best\\_estimator_ attribute:","be3c2f41":"One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC, and creates a new estimator, that behaves exactly the same - in this case, like a classifier. You should add refit=True and choose verbose to whatever number you want, higher the number, the more verbose (verbose just means the text output describing the process)."}}