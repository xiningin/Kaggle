{"cell_type":{"65bcca90":"code","a85ae46f":"code","d4c366a3":"code","2b279a8e":"code","84481574":"code","543eac6b":"code","24966a75":"code","040a4fcb":"code","69660515":"code","6fc0ac3b":"code","6e18bc6d":"code","98a4e049":"code","2325518f":"code","d94a4cab":"code","00f1271a":"code","b7445161":"code","b39c51c1":"code","0b6471b8":"code","f3392c7d":"code","d4cd1924":"code","470b1227":"code","2093e8ad":"code","dea2a62b":"code","972ab79e":"code","fda7db26":"code","0caf22dc":"code","46afb6c8":"code","056f2825":"code","460fd77a":"code","a674cfe9":"code","fd530d99":"code","b648b976":"code","6804d2fb":"code","b53229ad":"code","ac210b84":"code","a0aa0066":"code","ccf4da52":"markdown","476288a7":"markdown","c31d4ac9":"markdown","391d1324":"markdown","b97bd78f":"markdown","d9699b3e":"markdown","8acc50e4":"markdown","4a0c7fec":"markdown","b3ac61bc":"markdown","a444778c":"markdown","9d061338":"markdown","79edbfb1":"markdown","95112040":"markdown","46e17cc5":"markdown","f44d199a":"markdown","954478f5":"markdown","70309d53":"markdown","03563c0e":"markdown"},"source":{"65bcca90":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport os\nimport glob\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nimport gc\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","a85ae46f":"class Config:\n    data_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n    seed = 42","d4c366a3":"train = pd.read_csv(Config.data_dir + 'train.csv')\ntrain.head()","2b279a8e":"train.stock_id.unique()","84481574":"test = pd.read_csv(Config.data_dir + 'test.csv')\ntest.head()","543eac6b":"display(train.groupby('stock_id').size())\n\nprint(\"\\nUnique size values\")\ndisplay(train.groupby('stock_id').size().unique())","24966a75":"def get_trade_and_book_by_stock_and_time_id(stock_id, time_id=None, dataType = 'train'):\n    book_example = pd.read_parquet(f'{Config.data_dir}book_{dataType}.parquet\/stock_id={stock_id}')\n    trade_example =  pd.read_parquet(f'{Config.data_dir}trade_{dataType}.parquet\/stock_id={stock_id}')\n    if time_id:\n        book_example = book_example[book_example['time_id']==time_id]\n        trade_example = trade_example[trade_example['time_id']==time_id]\n    book_example.loc[:,'stock_id'] = stock_id\n    trade_example.loc[:,'stock_id'] = stock_id\n    return book_example, trade_example","040a4fcb":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1\/b1 + a2\/b2)\/ 2\n    \n    return x\n\n\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)\/ b\n    return x\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n\n    stock_id = file_path.split('=')[1]\n\n    df_book = pd.read_parquet(file_path)\n    df_book['wap1'] = calculate_wap1(df_book)\n    df_book['wap2'] = calculate_wap2(df_book)\n\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n\n    df_rvps =  pd.DataFrame(df_book.groupby(['time_id'])[['log_return1', 'log_return2']].agg(realized_volatility)).reset_index()\n    df_rvps[prediction_column_name] = 0.6 * df_rvps['log_return1'] + 0.4 * df_rvps['log_return2']\n\n    df_rvps['row_id'] = df_rvps['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_rvps[['row_id',prediction_column_name]]","69660515":"def get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     #min_size = ('size', 'min'),\n                                                     #min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","6fc0ac3b":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_subset, trade_subset = get_trade_and_book_by_stock_and_time_id(stock_id, dataType=dataType)\n    book_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    ## book data processing\n    \n    book_subset['bas'] = (book_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n\n    \n    book_subset['wap1'] = calculate_wap1(book_subset)\n    book_subset['wap2'] = calculate_wap2(book_subset)\n    \n    book_subset['log_return_bid_price1'] = np.log(book_subset['bid_price1'].pct_change() + 1)\n    book_subset['log_return_ask_price1'] = np.log(book_subset['ask_price1'].pct_change() + 1)\n    # book_subset['log_return_bid_price2'] = np.log(book_subset['bid_price2'].pct_change() + 1)\n    # book_subset['log_return_ask_price2'] = np.log(book_subset['ask_price2'].pct_change() + 1)\n    book_subset['log_return_bid_size1'] = np.log(book_subset['bid_size1'].pct_change() + 1)\n    book_subset['log_return_ask_size1'] = np.log(book_subset['ask_size1'].pct_change() + 1)\n    # book_subset['log_return_bid_size2'] = np.log(book_subset['bid_size2'].pct_change() + 1)\n    # book_subset['log_return_ask_size2'] = np.log(book_subset['ask_size2'].pct_change() + 1)\n    book_subset['log_ask_1_div_bid_1'] = np.log(book_subset['ask_price1'] \/ book_subset['bid_price1'])\n    book_subset['log_ask_1_div_bid_1_size'] = np.log(book_subset['ask_size1'] \/ book_subset['bid_size1'])\n    \n\n    book_subset['log_return1'] = (book_subset.groupby(by = ['time_id'])['wap1'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    book_subset['log_return2'] = (book_subset.groupby(by = ['time_id'])['wap2'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    \n    stock_stat = pd.merge(\n        book_subset.groupby(by = ['time_id'])['log_return1'].agg(realized_volatility).reset_index(),\n        book_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return2'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1_size'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    \n    stock_stat['stock_id'] = stock_id\n    \n    # Additional features that can be added. Referenced from https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n    \n    # trade_subset_agg = get_agg_info(trade_subset)\n    \n    #     stock_stat = pd.merge(\n    #         stock_stat,\n    #         trade_subset_agg,\n    #         on = ['stock_id', 'time_id'],\n    #         how = 'left'\n    #     )\n    \n    ## trade data processing \n    \n    return stock_stat\n\ndef get_data_set(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","6e18bc6d":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","98a4e049":"def plot_feature_importance(df, model):\n    feature_importances_df = pd.DataFrame({\n        'feature': df.columns,\n        'importance_score': model.feature_importances_\n    })\n    plt.rcParams[\"figure.figsize\"] = [10, 5]\n    ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n    ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n    plt.xticks(rotation=45)\n    plt.show()\n    return feature_importances_df","2325518f":"book_stock_1, trade_stock_1 = get_trade_and_book_by_stock_and_time_id(1, 5)\ndisplay(book_stock_1.shape)\ndisplay(trade_stock_1.shape)","d94a4cab":"book_stock_1.head()","00f1271a":"trade_stock_1.head()","b7445161":"%%time\ntrain_stock_stat_df = get_data_set(train.stock_id.unique(), dataType = 'train')\ntrain_stock_stat_df.head()","b39c51c1":"train_data_set = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntrain_data_set.head()","0b6471b8":"train_data_set.info()","f3392c7d":"%%time\ntest_stock_stat_df = get_data_set(test['stock_id'].unique(), dataType = 'test')\ntest_stock_stat_df","d4cd1924":"test_data_set = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_data_set.fillna(-999, inplace=True)\ntest_data_set","470b1227":"train_data_set.to_pickle('train_features_df.pickle')\ntest_data_set.to_pickle('test_features_df.pickle')","2093e8ad":"x = gc.collect()","dea2a62b":"X_display = train_data_set.drop(['stock_id', 'time_id', 'target'], axis = 1)\nX = X_display.values\ny = train_data_set['target'].values\n\nX.shape, y.shape","972ab79e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=Config.seed, shuffle=False)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","fda7db26":"xgb = XGBRegressor(tree_method='gpu_hist', random_state = Config.seed, n_jobs= - 1)","0caf22dc":"%%time\nxgb.fit(X_train, y_train)","46afb6c8":"xgb_preds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = xgb_preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = xgb_preds), 6)\nprint(f'Performance of the naive XGBOOST prediction: R2 score: {R2}, RMSPE: {RMSPE}')","056f2825":"plot_feature_importance(X_display, xgb)","460fd77a":"lgbm = LGBMRegressor(device='gpu', random_state=Config.seed)","a674cfe9":"%%time\nlgbm.fit(X_train, y_train)","fd530d99":"lgbm_preds = lgbm.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = lgbm_preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = lgbm_preds),6)\nprint(f'Performance of the naive LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","b648b976":"plot_feature_importance(X_display, lgbm)","6804d2fb":"np.shape(X_train)","b53229ad":"test_data_set_final = test_data_set.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_data_set_final[['row_id']]\nX_test = test_data_set_final.drop(['row_id'], axis = 1)","ac210b84":"X_test","a0aa0066":"y_pred = y_pred.assign(target = lgbm.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","ccf4da52":"#### Basic LGBMRegressor model","476288a7":"#### Metric","c31d4ac9":"#### Most of the feature engineering code","391d1324":"### Preparing Train and Test set for training and prediction with the desired features\nThe following cell takes around 25 mins for execution. You can also use the pickled data from the notebook output and build on that","b97bd78f":"#### Feature engineering","d9699b3e":"#### Plotting","8acc50e4":"#### Storing for later usages. Processing time for features took 25 mins\nYou can directly use this from the notebook output and build on that","4a0c7fec":"### Trying out basic Regression models without any tuning\n\n#### Basic XGB model","b3ac61bc":"### Example of book and trade data","a444778c":"#### File reading","9d061338":"# Submission","79edbfb1":"### Config","95112040":"#### Feature importance score for xgb model","46e17cc5":"## Feature Engineering\n\nWe have got the orde book and trade data. Here, i have tried to capture various relationships between teh bid, ask prices and the volumes.\nI have also included features from the following notebook https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data that capture features from the data. \n\nWill try to explore more features and further relationships between book and trade data. \n\nBasic XGBoost and LBGMRegressor models are used to test out the features and demonstrate their importance.\n\nReferences - \n1. https:\/\/www.kaggle.com\/munumbutt\/naive-optuna-tuned-stacked-ensemble\n2. https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n3. https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper","f44d199a":"### Helper Functions","954478f5":"Above i have tried some experimental features. Will be trying out some other features as well and look to improve the score. Please leave down any suggestions in the comments","70309d53":"#### Feature importance score for xgb model","03563c0e":"### Imports"}}