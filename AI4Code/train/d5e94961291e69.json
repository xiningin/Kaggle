{"cell_type":{"15792184":"code","dd4623b0":"code","7616454b":"code","6fa1564f":"code","180c07a7":"code","468ad7dc":"code","4e5c94c6":"code","5ac9c21e":"code","8ce41768":"code","f8dc3f4e":"code","e8c2a144":"code","04d9b478":"code","6061ccec":"code","cb38d794":"code","8d08ab96":"code","5e794edc":"code","07de6809":"code","59e91e15":"code","f9f45f27":"code","f2728a0b":"code","a1a7486f":"code","650ac249":"code","886a76fd":"code","f46d94bd":"code","beba67c9":"markdown","80356b01":"markdown","5293a7ef":"markdown","10e5ce37":"markdown","42b7055a":"markdown","1541faa4":"markdown","52dd3ba7":"markdown","fa2ff52f":"markdown","171726eb":"markdown","fc905a15":"markdown","2de5db45":"markdown","35aec200":"markdown"},"source":{"15792184":"import pandas as pd\n\nexecute = {\n    'model1': True,\n    'model2': True,\n    'model3': True,\n    'model4': True,\n    'model5': True,\n    'model6': True,\n    'model7': True,\n    'model8': True\n}\n\n\n# Weights of each model\nweights = {\n    'model1': 0.25, # 0.816\n    'model2': 0.005, # 0.825\n    'model3': 0.04, # 0.807\n    'model4': 0.04, # 0.806\n    'model5': 0.005, # 0.782\n    'model6': 0.005, # 0.768\n    'model7': 0.02,  # 0.812,\n    'model8': 0.65  # 0.85,\n}\n\n\ndef get_dummy_preds():\n    df_sample = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\n    return df_sample['score'].values","dd4623b0":"%%time\nif execute['model1']:\n    import os\n    import gc\n    import cv2\n    import copy\n    import time\n    import random\n\n    # For data manipulation\n    import numpy as np\n    import pandas as pd\n\n    # Pytorch Imports\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n\n    # For Transformer Models\n    from transformers import AutoTokenizer, AutoModel\n\n    # Utils\n    from tqdm import tqdm\n\n    # For descriptive error messages\n    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n    CONFIG = dict(\n        seed = 42,\n        model_name = '..\/input\/roberta-base',\n        test_batch_size = 64,\n        max_length = 128,\n        num_classes = 1,\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    )\n\n    CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\n    MODEL_PATHS = [\n        '..\/input\/pytorch-w-b-jigsaw-starter\/Loss-Fold-0.bin',\n        '..\/input\/pytorch-w-b-jigsaw-starter\/Loss-Fold-1.bin',\n        '..\/input\/pytorch-w-b-jigsaw-starter\/Loss-Fold-2.bin',\n        '..\/input\/pytorch-w-b-jigsaw-starter\/Loss-Fold-3.bin',\n        '..\/input\/pytorch-w-b-jigsaw-starter\/Loss-Fold-4.bin'\n    ]\n\n    def set_seed(seed = 42):\n        '''Sets the seed of the entire notebook so results are the same every time we run.\n        This is for REPRODUCIBILITY.'''\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Set a fixed value for the hash seed\n        os.environ['PYTHONHASHSEED'] = str(seed)\n\n\n    class JigsawDataset(Dataset):\n        def __init__(self, df, tokenizer, max_length):\n            self.df = df\n            self.max_len = max_length\n            self.tokenizer = tokenizer\n            self.text = df['text'].values\n\n        def __len__(self):\n            return len(self.df)\n\n        def __getitem__(self, index):\n            text = self.text[index]\n            inputs = self.tokenizer.encode_plus(\n                            text,\n                            truncation=True,\n                            add_special_tokens=True,\n                            max_length=self.max_len,\n                            padding='max_length'\n                        )\n\n            ids = inputs['input_ids']\n            mask = inputs['attention_mask']        \n\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long)\n            }    \n\n\n    class JigsawModel(nn.Module):\n        def __init__(self, model_name):\n            super(JigsawModel, self).__init__()\n            self.model = AutoModel.from_pretrained(model_name)\n            self.drop = nn.Dropout(p=0.2)\n            self.fc = nn.Linear(768, CONFIG['num_classes'])\n\n        def forward(self, ids, mask):        \n            out = self.model(input_ids=ids,attention_mask=mask,\n                             output_hidden_states=False)\n            out = self.drop(out[1])\n            outputs = self.fc(out)\n            return outputs\n\n    @torch.no_grad()\n    def valid_fn(model, dataloader, device):\n        model.eval()\n\n        dataset_size = 0\n        running_loss = 0.0\n\n        PREDS = []\n\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n\n            outputs = model(ids, mask)\n            PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n\n        PREDS = np.concatenate(PREDS)\n        gc.collect()\n\n        return PREDS\n\n\n    def inference(model_paths, dataloader, device):\n        final_preds = []\n        for i, path in enumerate(model_paths):\n            model = JigsawModel(CONFIG['model_name'])\n            model.to(CONFIG['device'])\n            model.load_state_dict(torch.load(path))\n\n            print(f\"Getting predictions for model {i+1}\")\n            preds = valid_fn(model, dataloader, device)\n            final_preds.append(preds)\n\n        final_preds = np.array(final_preds)\n        final_preds = np.mean(final_preds, axis=0)\n        return final_preds\n\n\n    set_seed(CONFIG['seed'])\n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    df.head()\n\n    test_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                             num_workers=2, shuffle=False, pin_memory=True)\n\n    preds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])\nelse:\n    preds1 = get_dummy_preds()","7616454b":"preds1[:5]","6fa1564f":"%%time\nif execute['model2']:\n    import joblib\n    import pandas as pd\n    import numpy as np\n\n    from sklearn.linear_model import Ridge\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.metrics import mean_squared_error\n    from scipy.stats import rankdata\n\n    def ridge_pred(model_name, vec, X_test, folds):\n        preds = []\n        for fold in range(folds):\n            model = joblib.load(f'..\/input\/0825-ridge-model-2\/{model_name}-fold_{fold}.pkl')\n            pred = model.predict(X_test)\n            preds.append(pred)\n        preds = np.mean( np.vstack(preds), axis=0 )\n        return preds\n\n    FOLDS = 7\n    df_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n\n    print(\"Running 1st model\")\n    vec = joblib.load(\"..\/input\/0825-ridge-model-2\/vectorizer1.pkl\")\n    X_test = vec.transform(df_test['text'])\n    jc_preds = ridge_pred(\"model_1\", vec, X_test, FOLDS)\n\n    print(\"Running 2rd model\")\n    \n    FOLDS = 7\n    vec = joblib.load(\"..\/input\/0825-ridge-model-2\/vectorizer2.pkl\")\n    X_test = vec.transform(df_test['text'])\n\n    juc_preds = ridge_pred(\"model_2\", vec, X_test, FOLDS)\n\n    print(\"Running 3rd model\")\n    \n    vec = joblib.load(\"..\/input\/0825-ridge-model-2\/vectorizer3.pkl\")\n    X_test = vec.transform(df_test['text'])\n\n    rud_preds =  ridge_pred (\"model_3\", vec, X_test, FOLDS)\n\n    jc_max = 9.89323978366957\n    juc_max = 6.857029899317271\n    rud_max = 2.1424524202196573\n    \n    preds2 = jc_preds\/jc_max + juc_preds\/juc_max + rud_preds\/rud_max\nelse:\n    preds2 = get_dummy_preds()","180c07a7":"preds2[:5]","468ad7dc":"%%time\nif execute['model3']:\n    # Asthetics\n    import warnings\n    import sklearn.exceptions\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    # General\n    from tqdm.auto import tqdm\n    from bs4 import BeautifulSoup\n    from collections import defaultdict\n    import pandas as pd\n    import numpy as np\n    import os\n    import re\n    import random\n    import gc\n    import glob\n    pd.set_option('display.max_columns', None)\n    np.seterr(divide='ignore', invalid='ignore')\n    gc.enable()\n\n    # Deep Learning\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from torch.optim.lr_scheduler import OneCycleLR\n    # NLP\n    from transformers import AutoTokenizer, AutoModel\n\n    # Random Seed Initialize\n    RANDOM_SEED = 42\n\n    def seed_everything(seed=RANDOM_SEED):\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n    seed_everything()\n\n    # Device Optimization\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n\n    print(f'Using device: {device}')\n\n    data_dir = '..\/input\/jigsaw-toxic-severity-rating'\n    models_dir = '..\/input\/jrstc-models\/roberta_base'\n    test_file_path = os.path.join(data_dir, 'comments_to_score.csv')\n    print(f'Train file: {test_file_path}')\n\n    test_df = pd.read_csv(test_file_path)\n\n    # Text Cleaning\n\n    def text_cleaning(text):\n        '''\n        Cleans text into a basic form for NLP. Operations include the following:-\n        1. Remove special charecters like &, #, etc\n        2. Removes extra spaces\n        3. Removes embedded URL links\n        4. Removes HTML tags\n        5. Removes emojis\n\n        text - Text piece to be cleaned.\n        '''\n        template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n        text = template.sub(r'', text)\n\n        soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n        only_text = soup.get_text()\n        text = only_text\n\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n\n        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n        text = re.sub(' +', ' ', text) #Remove Extra Spaces\n        text = text.strip() # remove spaces at the beginning and at the end of string\n\n        return text\n\n    tqdm.pandas()\n    test_df['text'] = test_df['text'].progress_apply(text_cleaning)\n\n    test_df.sample(10)\n\n    # CFG\n\n    params = {\n        'device': device,\n        'debug': False,\n        'checkpoint': '..\/input\/roberta-base',\n        'output_logits': 768,\n        'max_len': 256,\n        'batch_size': 32,\n        'dropout': 0.2,\n        'num_workers': 2\n    }\n\n    if params['debug']:\n        train_df = train_df.sample(frac=0.01)\n        print('Reduced training Data Size for Debugging purposes')\n\n    # Dataset\n\n    class BERTDataset:\n        def __init__(self, text, max_len=params['max_len'], checkpoint=params['checkpoint']):\n            self.text = text\n            self.max_len = max_len\n            self.checkpoint = checkpoint\n            self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n            self.num_examples = len(self.text)\n\n        def __len__(self):\n            return self.num_examples\n\n        def __getitem__(self, idx):\n            text = str(self.text[idx])\n\n            tokenized_text = self.tokenizer(\n                text,\n                add_special_tokens=True,\n                truncation=True,\n                padding='max_length',\n                max_length=self.max_len,\n                return_attention_mask=True,\n                return_token_type_ids=True,\n            )\n\n            ids = tokenized_text['input_ids']\n            mask = tokenized_text['attention_mask']\n            token_type_ids = tokenized_text['token_type_ids']\n\n            return {'ids': torch.tensor(ids, dtype=torch.long),\n                    'mask': torch.tensor(mask, dtype=torch.long),\n                    'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)}\n\n    # NLP Model\n\n    class ToxicityModel(nn.Module):\n        def __init__(self, checkpoint=params['checkpoint'], params=params):\n            super(ToxicityModel, self).__init__()\n            self.checkpoint = checkpoint\n            self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n            self.layer_norm = nn.LayerNorm(params['output_logits'])\n            self.dropout = nn.Dropout(params['dropout'])\n            self.dense = nn.Sequential(\n                nn.Linear(params['output_logits'], 256),\n                nn.LeakyReLU(negative_slope=0.01),\n                nn.Dropout(params['dropout']),\n                nn.Linear(256, 1)\n            )\n\n        def forward(self, input_ids, token_type_ids, attention_mask):\n            _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n            pooled_output = self.layer_norm(pooled_output)\n            pooled_output = self.dropout(pooled_output)\n            preds = self.dense(pooled_output)\n            return preds\n\n    # Prediction\n\n    predictions_nn = None\n    for model_name in glob.glob(models_dir + '\/*.pth'):\n        model = ToxicityModel()\n        model.load_state_dict(torch.load(model_name))\n        model = model.to(params['device'])\n        model.eval()\n\n        test_dataset = BERTDataset(\n            text = test_df['text'].values\n        )\n        test_loader = DataLoader(\n            test_dataset, batch_size=params['batch_size'],\n            shuffle=False, num_workers=params['num_workers'],\n            pin_memory=True\n        )\n\n        temp_preds = None\n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=f'Predicting. '):\n                ids= batch['ids'].to(device)\n                mask = batch['mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                predictions = model(ids, token_type_ids, mask).to('cpu').numpy()\n\n                if temp_preds is None:\n                    temp_preds = predictions\n                else:\n                    temp_preds = np.vstack((temp_preds, predictions))\n\n        if predictions_nn is None:\n            predictions_nn = temp_preds\n        else:\n            predictions_nn += temp_preds\n\n    predictions_nn \/= (len(glob.glob(models_dir + '\/*.pth')))\n\n    preds3 = predictions_nn\n    preds3 = preds3.squeeze(-1)\nelse:\n    preds3 = get_dummy_preds()","4e5c94c6":"preds3[:5]","5ac9c21e":"%%time\nif execute['model4']:\n    import os\n    import gc\n    import copy\n    import time\n    import random\n    import string\n\n    import nltk\n    from nltk.stem import SnowballStemmer, WordNetLemmatizer\n    import re\n    from nltk.corpus import stopwords\n\n    from tqdm import tqdm\n    from collections import defaultdict\n\n    import torch\n    import torch.nn as nn\n\n    import numpy as np\n    import pandas as pd\n\n    from sklearn.model_selection import train_test_split\n    from transformers import AdamW, get_linear_schedule_with_warmup\n    from transformers import AutoTokenizer, AutoModel\n    from torch.utils.data import Dataset, DataLoader\n    from torch.optim import lr_scheduler\n\n    class Config:\n        num_classes=1\n        epochs=10\n        margin=0.5\n        model_name = '..\/input\/robertalarge'\n        batch_size = 8\n        lr = 1e-4\n        weight_decay=0.01\n        scheduler = 'CosineAnnealingLR'\n        max_length = 196\n        accumulation_step = 1\n        patience = 1\n\n    class ToxicDataset(Dataset):\n        def __init__(self, comments, tokenizer, max_length):\n            self.comment = comments\n            self.tokenizer = tokenizer\n            self.max_len = max_length\n\n        def __len__(self):\n            return len(self.comment)\n\n        def __getitem__(self, idx):\n\n            inputs_more_toxic = self.tokenizer.encode_plus(\n                                    self.comment[idx],\n                                    truncation=True,\n                                    add_special_tokens=True,\n                                    max_length=self.max_len,\n                                    padding='max_length'\n                                )\n\n\n            input_ids = inputs_more_toxic['input_ids']\n            attention_mask = inputs_more_toxic['attention_mask']\n\n\n            return {\n                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            }\n\n\n    class ToxicModel(nn.Module):\n        def __init__(self, model_name, args):\n            super(ToxicModel, self).__init__()\n            self.args = args\n            self.model = AutoModel.from_pretrained(model_name)\n            self.dropout = nn.Dropout(p=0.2)\n            self.output = nn.Linear(1024, self.args.num_classes)\n\n\n        def forward(self, toxic_ids, toxic_mask):\n\n            out = self.model(\n                input_ids=toxic_ids,\n                attention_mask=toxic_mask,\n                output_hidden_states=False\n            )\n\n            out = self.dropout(out[1])\n            outputs = self.output(out)\n\n            return outputs\n\n\n    def get_predictions(model, dataloader):\n        model.eval()\n\n        PREDS=[]\n        with torch.no_grad():\n            bar = tqdm(enumerate(dataloader), total=len(dataloader))\n            for step, data in bar:        \n                input_ids = data['input_ids'].cuda()\n                attention_mask = data['attention_mask'].cuda()\n\n                outputs = model(input_ids, attention_mask)\n\n                PREDS.append(outputs.view(-1).cpu().detach().numpy())\n\n                bar.set_postfix(Stage='Inference')  \n\n            PREDS = np.hstack(PREDS)\n            gc.collect()\n\n            return PREDS\n\n    df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\n\n    args = Config()\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n\n    def washing_machine(comments):\n        corpus=[]\n        for i in tqdm(range(len(comments))):\n            comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n            comment = comment.lower()\n            comment = comment.split()\n            stemmer = SnowballStemmer('english')\n            lemmatizer = WordNetLemmatizer()\n            all_stopwords = stopwords.words('english')\n            comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n            comment = [lemmatizer.lemmatize(word) for word in comment]\n            comment = ' '.join(comment)\n            corpus.append(comment)\n\n        return corpus\n\n\n\n\n    def inference(dataloader):\n        final_preds = []\n        args = Config()\n        base_path='..\/input\/large-jigsaw-kishal-v1\/'\n        for fold in range(1):\n            model = ToxicModel(args.model_name, args)\n            model = model.cuda()\n            path = base_path + f'model_fold_0.bin'\n            model.load_state_dict(torch.load(path))\n\n            print(f\"Getting predictions for model {fold+1}\")\n            preds = get_predictions(model, dataloader)\n            final_preds.append(preds)\n\n        final_preds = np.array(final_preds)\n        final_preds = np.mean(final_preds, axis=0)\n        return final_preds\n\n    # Prediction and submission\n\n    sub = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\n\n    sub.head(1)\n\n    sub_dataset = ToxicDataset(washing_machine(sub['text'].values), tokenizer, max_length=args.max_length)\n    sub_loader = DataLoader(sub_dataset, batch_size=2*args.batch_size,\n                            num_workers=2, shuffle=False, pin_memory=True)\n\n    preds4 = inference(sub_loader)\nelse:\n    preds4 = get_dummy_preds()","8ce41768":"preds4[:5]","f8dc3f4e":"%%time\nif execute['model5']:\n    import os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n    import torch\n    import pandas as pd\n    import numpy as np\n    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n    from sklearn.preprocessing import MinMaxScaler\n\n\n    class Dataset:\n        \"\"\"\n        For comments_to_score.csv (the submission), get only one comment per row\n        \"\"\"\n        def __init__(self, text, tokenizer, max_len):\n            self.text = text\n            self.tokenizer = tokenizer\n            self.max_len = max_len\n\n        def __len__(self):\n            return len(self.text)\n\n        def __getitem__(self, item):\n            text = str(self.text[item])\n            inputs = self.tokenizer(\n                text, \n                max_length=self.max_len, \n                padding=\"max_length\", \n                truncation=True\n            )\n\n            ids = inputs[\"input_ids\"]\n            mask = inputs[\"attention_mask\"]\n\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n            }\n\n    def generate_predictions(model_path, max_len, is_multioutput):\n        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n        model.to(\"cuda\")\n        model.eval()\n\n        df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n\n        dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n        )\n\n        final_output = []\n\n        for data in data_loader:\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output = model(**data)\n\n                if is_multioutput:\n                    # Sum the logits for all the toxic labels\n                    # One strategy out of various possible\n                    output = output.logits.sum(dim=1)\n                else:\n                    # Classifier. Get logits for \"toxic\"\n                    output = output.logits[:, 1]\n\n                output = output.detach().cpu().numpy().tolist()\n                final_output.extend(output)\n\n        torch.cuda.empty_cache()\n        return np.array(final_output)\n\n    preds_bert = generate_predictions(\"..\/input\/toxic-bert\", max_len=192, is_multioutput=True)\n    preds_rob1 = generate_predictions(\"..\/input\/roberta-base-toxicity\", max_len=192, is_multioutput=False)\n    preds_rob2 = generate_predictions(\"..\/input\/roberta-toxicity-classifier\", max_len=192, is_multioutput=False)\n\n\n\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    df_sub[\"score_bert\"] = preds_bert\n    df_sub[\"score_rob1\"] = preds_rob1\n    df_sub[\"score_rob2\"] = preds_rob2\n    df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]] = MinMaxScaler().fit_transform(df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]])\n\n    preds5 = df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]].sum(axis=1)\nelse:\n    preds5 = get_dummy_preds()","e8c2a144":"preds5[:5]","04d9b478":"%%time\nif execute['model6']:\n    import pandas as pd\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n    df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\n    df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n    min_len = (df['y'] == 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\n    df = pd.concat([df[df['y'] == 1], df_y0_undersample])\n    vec = TfidfVectorizer()\n    X = vec.fit_transform(df['text'])\n    model = MultinomialNB()\n    model.fit(X, df['y'])\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    X_test = vec.transform(df_sub['text'])\n    preds6 = model.predict_proba(X_test)[:, 1]\nelse:\n    preds6 = get_dummy_preds()","6061ccec":"preds6[:5]","cb38d794":"%%time\nif execute['model7']:\n    import pandas as pd\n    import numpy as np\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.linear_model import Ridge\n    from sklearn.pipeline import Pipeline\n    import scipy\n    pd.options.display.max_colwidth=300\n\n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n\n    # Give more weight to severe toxic \n    df['severe_toxic'] = df.severe_toxic * 2\n    df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\n    df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n\n\n    df = pd.concat([df[df.y>0] , \n                    df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\n    pipeline = Pipeline(\n        [\n            (\"vect\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n            (\"clf\", Ridge()),\n        ]\n    )\n    pipeline.fit(df['text'], df['y'])\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    preds7 = pipeline.predict(df_sub['text'])\nelse:\n    preds7 = get_dummy_preds()","8d08ab96":"preds7[:5]","5e794edc":"%%time\nif execute['model8']:\n    import joblib\n    import pandas as pd\n    import numpy as np\n\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n    from sklearn.linear_model import Ridge, LinearRegression\n    from sklearn.pipeline import Pipeline, FeatureUnion\n    from sklearn.base import TransformerMixin, BaseEstimator\n\n    import re \n    import scipy\n    from scipy import sparse\n    import gc \n\n    from IPython.display import display\n    from pprint import pprint\n    from matplotlib import pyplot as plt \n\n    import time\n    import scipy.optimize as optimize\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    pd.options.display.max_colwidth=300\n    pd.options.display.max_columns=100\n\n    base_path = \"..\/input\/085-ridge-model-8-samarth\/\"\n\n    def clean(data, col):\n        data[col] = data[col].str.replace('\\n', ' \\n ')\n        data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n        data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n        data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n        data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n        data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n        data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n        data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n\n        return data\n\n    def predict(model_name, clean_prm, df_sub):\n        test_preds_arr_tmp = np.zeros((df_sub.shape[0], 2))\n        for fld in range(2):\n            pipeline = joblib.load(f'{base_path}{model_name}-fold_{fld}.pkl')\n            X = clean(df_sub,'text')['text'] if clean_prm else df_sub['text']\n            test_preds_arr_tmp[:,fld] = pipeline.predict(X)\n        return test_preds_arr_tmp.mean(axis=1)\n\n    df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n\n\n\n    prds1 = predict(\"toxic_1\", False, df_sub)\n    prds2 = predict(\"toxic_clean_1\", True, df_sub)\n    prds3 = predict(\"ruddit_1\", False, df_sub)\n    prds4 = predict(\"multilingual_1\", True, df_sub)\n\n    \"\"\"\n    test_preds_arr = toxic_1 prds1 w1\n    test_preds_arrc = toxic_clean_1 prds2 w3\n    test_preds_arr_ = ruddit_1 preds3 w2\n    test_preds_arrm = multiliggual_1 prds4 we\n    df_sub['score'] = w1*test_preds_arr.mean(axis=1) + \\\n                  w2*test_preds_arr_.mean(axis=1) + \\\n                  w3*test_preds_arrc.mean(axis=1) + \\\n                  w4*test_preds_arrm.mean(axis=1)\n    \"\"\"\n\n    w1 = 0.2\n    w2 = 0.37999999999999995 \n    w3 = 0.05\n    w4 = 0.49999999999999994\n\n    preds8 = w1*prds1 + w3*prds2 + w2*prds3 + w4*prds4\nelse:\n    preds8 = get_dummy_preds()","07de6809":"preds8[:5]","59e91e15":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf['model1'] = preds1\ndf['model2'] = preds2\ndf['model3'] = preds3\ndf['model4'] = preds4\ndf['model5'] = preds5\ndf['model6'] = preds6\ndf['model7'] = preds7\ndf['model8'] = preds8\n\ncols = [c for c in df.columns if c.startswith('model')]\n\n# Put all predictions in the same scale. \n# Make all the distances between predictions uniform\ndf[cols] = df[cols].rank(method='first').astype(int)\n\n\n# A weighted sum determines the final position\n# It is the same as an average in the end\ndf['score'] = pd.DataFrame([df[c] * weights[c] for c in cols]).T.sum(axis=1).rank(method='first').astype(int)\ndf.head()","f9f45f27":"df.sort_values(\"score\", ascending=True).head(3)","f2728a0b":"df.sort_values(\"score\", ascending=True).tail(3)","a1a7486f":"df.sort_values(\"score\", ascending=True).sample(3)","650ac249":"import seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\nsns.heatmap(df.drop(\"comment_id\", axis=1).corr(), annot=True, fmt='.2f', cmap='Greens')","886a76fd":"df.head()","f46d94bd":"df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","beba67c9":"# \u2623\ufe0f Jigsaw - Early Ensemble\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n\n# If it is interesting, you like it, or you fork...\n# \ud83d\ude4c\ud83d\ude4f Please, _DO_ upvote !! \ud83d\ude4f\ud83d\ude4c\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n---\n\n\n## Simple weighted sum of the following 8 public models:\n\nIt got a much higher LB than I was expecting. It might be overfitting badly so... be aware of that...\n\n|Number |Model| Author\/s| Type | Runtime| LB |\n|--|--|--|--|--:| --:|\n|1 |[[0.816] Jigsaw Inference](https:\/\/www.kaggle.com\/debarshichanda\/0-816-jigsaw-inference)| [Debarshi Chanda](https:\/\/www.kaggle.com\/debarshichanda)| Roberta base on validation |`3:31`|`0.816`|\n|2 |[JRSoTC - RidgeRegression (ensemble of 3)](https:\/\/www.kaggle.com\/adityasharma01\/jrsotc-ridgeregression-ensemble-of-3)|[steubk](https:\/\/www.kaggle.com\/steubk)\/[Aditya Sharma](https:\/\/www.kaggle.com\/adityasharma01\/) |Ridge | ~22:00~ --> `0:33` |`0.825`|\n|3 | [Pytorch RoBERTa Ranking Baseline JRSTC [Infer]](https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer) | [Manav](https:\/\/www.kaggle.com\/manabendrarout)|Roberta on training |`6:42`|`0.807` |\n|4 | [JRSTC \\| INFER \\| LB : 0.806 \ud83c\udf83](https:\/\/www.kaggle.com\/kishalmandal\/jrstc-infer-lb-0-806)|[Kishal](https:\/\/www.kaggle.com\/kishalmandal)| Transformer |`3:42`|`0.806`|\n|5 |[\u2623\ufe0f Jigsaw - \ud83e\udd17 HF hub out-of-the-box models](https:\/\/www.kaggle.com\/julian3833\/jigsaw-hf-hub-out-of-the-box-models)|[dataista0 (Juli\u00e1n Peller)](https:\/\/www.kaggle.com\/julian3833\/) | 3 transformers |`2:53`|`0.782`|\n|6 |[\u2623\ufe0f Jigsaw - Incredibly Simple Naive Bayes [0.768]](https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768)|[dataista0 (Juli\u00e1n Peller)](https:\/\/www.kaggle.com\/julian3833\/)|Bayes |`0:05`|`0.768`|\n|7 |[*#&@ the Benchmark [0.81+] - TFIDF - Ridge](https:\/\/www.kaggle.com\/samarthagarwal23\/the-benchmark-0-81-tfidf-ridge)|[Samarth Agarwal](https:\/\/www.kaggle.com\/samarthagarwal23) | Simple Ridge |`0:44`|`0.812`|\n|8 |[Mega (B)Ridge to the top [LB: 0.85]](https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-85)|[Samarth Agarwal](https:\/\/www.kaggle.com\/samarthagarwal23)| Various ridges | `0:48`|`0.85`|\n\n---\n\n## Changelog\n\n|Best|Version | Description | LB |\n|--|-- | -- | --: |\n||[**V1**](https:\/\/www.kaggle.com\/julian3833\/jigsaw-new-ensemble-lb-0-843?scriptVersionId=80520136) | Comes from early ensemble. Add model 8 being [Mega (B)Ridge to the top [LB: 0.85]](https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-85) when it was `0.843` with `weights=[0.1, 0.1, 0.02, 0.02, 0.02, 0.02, 0.02, 0.70]` | `0.843` |\n||[**V2**](https:\/\/www.kaggle.com\/julian3833\/jigsaw-new-ensemble-lb-0-843?scriptVersionId=80524310) | V1 + `weights=[0.24, 0.005, 0.05, 0.05, 0.005, 0.005, 0.005, 0.64]` | `0.852` |\n|_Best_|[**V3**](https:\/\/www.kaggle.com\/julian3833\/jigsaw-new-ensemble-lb-0-843?scriptVersionId=80535134) | V1 + `weights=[0.27, 0.005, 0.02, 0.02, 0.005, 0.005, 0.005, 0.67]` | `0.853` |\n|_Best_|[**V4**](https:\/\/www.kaggle.com\/julian3833\/jigsaw-new-ensemble-lb-0-843?scriptVersionId=80629381) | V1 + `weights=[0.25, 0.005, 0.04, 0.04, 0.005, 0.005, 0.005, 0.02, 0.65]` | `0.853` |\n||[**V9**](https:\/\/www.kaggle.com\/julian3833\/jigsaw-new-ensemble-lb-0-843) | Replace model 8 with its `0.85` version. Move all heavy ridge trainings to other notebooks and import as datasets. Add option to disable execution of models. Model1*0.3 + Model8*0.7 | `??` |\n","80356b01":"# Model 5: [\u2623\ufe0f Jigsaw - \ud83e\udd17 HF hub out-of-the-box models](https:\/\/www.kaggle.com\/julian3833\/jigsaw-hf-hub-out-of-the-box-models)\n`LB=0.782`","5293a7ef":"# Submission","10e5ce37":"# Model 1: [[0.816] Jigsaw Inference](https:\/\/www.kaggle.com\/debarshichanda\/0-816-jigsaw-inference)\n`LB=0.816`","42b7055a":"# Model 7: [*#&@ the Benchmark [0.81+] - TFIDF - Ridge](https:\/\/www.kaggle.com\/samarthagarwal23\/the-benchmark-0-81-tfidf-ridge)\n`LB=0.812`","1541faa4":"# Model 6: [\u2623\ufe0f Jigsaw - Incredibly Simple Naive Bayes [0.768]](https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768)\n`LB=0.768`","52dd3ba7":"# Model 2: [JRSoTC - RidgeRegression (ensemble of 3)](https:\/\/www.kaggle.com\/adityasharma01\/jrsotc-ridgeregression-ensemble-of-3)\n`LB=0.825`","fa2ff52f":"# Model 3: [Pytorch RoBERTa Ranking Baseline JRSTC [Infer]](https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer)\n`LB=0.807`","171726eb":"# Model Correlation","fc905a15":"# Model 8: [Mega (B)Ridge to the top [LB: 0.85]](https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-85)\n\nTrained in another notebook and dumped as `.pkl` in a dataset: [0.85 Ridge - Model 8 - Samarth\n](https:\/\/www.kaggle.com\/julian3833\/085-ridge-model-8-samarth)","2de5db45":"# Model 4: [JRSTC | INFER | LB : 0.806 \ud83c\udf83](https:\/\/www.kaggle.com\/kishalmandal\/jrstc-infer-lb-0-806)\n`LB=0.806`","35aec200":"# Ensemble"}}