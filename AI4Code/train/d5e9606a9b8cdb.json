{"cell_type":{"d84053f2":"code","96454fe7":"code","0363df2a":"code","b461fbaf":"code","6a36c20e":"code","51e05dd6":"code","059a3ad3":"code","f7b512cc":"code","555e53ae":"code","4457c911":"code","f7a0dbfb":"code","8ebbc4ab":"code","b7d3a681":"code","25a2885b":"code","763e72ce":"code","be9d5521":"code","9a571bab":"code","a68f9a1f":"code","712cb480":"code","833efcd7":"code","558ffe42":"code","9c9e7444":"code","71c492cb":"code","53ac5a9b":"markdown","69e66c54":"markdown","c93c4af7":"markdown","632725a7":"markdown","bacd315a":"markdown","c681fc64":"markdown","b7ee4002":"markdown","a03422bd":"markdown"},"source":{"d84053f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96454fe7":"# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('..\/input\/life-expectation\/Gapminer.csv')\n\n# Create arrays for features and target variable\ny = df['life']\nX = df['fertility']\n\n# Print the dimensions of y and X before reshaping\nprint(\"Dimensions of y before reshaping: \", y.shape)\nprint(\"Dimensions of X before reshaping: \", X.shape)\n\n# Reshape X and y\ny_reshaped = y.values.reshape(-1,1)\nX_reshaped = X.values.reshape(-1,1)\nprint(\"Dimension of y after reshaping:\", y_reshaped.shape)\nprint(\"Dimension of X after reshaping:\", X_reshaped.shape)","0363df2a":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), square=True, cmap='RdYlGn')\nplt.show()","b461fbaf":"df.shape","6a36c20e":"df.info()","51e05dd6":"df.describe()","059a3ad3":"# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create the regressor: reg\nreg = LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_reshaped), max(X_reshaped)).reshape(-1,1)\n","f7b512cc":"# Fit the model to the data\nreg.fit(X_reshaped, y_reshaped)","555e53ae":"# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)","4457c911":"# Print R^2 \nprint(reg.score(X_reshaped, y))","f7a0dbfb":"# Plot regression line\nplt.plot(prediction_space, y_pred, color='blue', linewidth=3)\nplt.xlabel('Fertility')\nplt.ylabel('Life Expectation')\nplt.show()","8ebbc4ab":"# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","b7d3a681":"# Create train test split\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_reshaped, test_size = 0.3, random_state=42)","25a2885b":"# Create the regressor: \nregr = LinearRegression()\n# Fit the regressor to the training data\nregr.fit(X_train, y_train)","763e72ce":"# Compute and print R^2 \nprint(regr.score(X_test, y_test))","be9d5521":"# Predict on the test data: y_pred\ny_pred = regr.predict(X_test)\n","9a571bab":"plt.scatter(X_test, y_test, color ='b')\nplt.plot(X_test, y_pred, color ='k')\n  \nplt.show()","a68f9a1f":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg,X_reshaped,y_reshaped,cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n","712cb480":"# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg, X_reshaped,y_reshaped,cv=3)\nprint(' R score when K-fold is 3:',np.mean(cvscores_3))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg, X_reshaped,y_reshaped,cv=10)\nprint(' R score when K-fold is 10:',np.mean(cvscores_10))\n\n#the time each 3-fold CV takes to split the data\n%timeit cross_val_score(reg, X_reshaped,y_reshaped,cv=3)\n# the time each 10-fold CV takes to split the data\n%timeit cross_val_score(reg, X_reshaped,y_reshaped,cv=10)\n","833efcd7":"# Import Lasso\nfrom sklearn.linear_model import Lasso\n# Train, test split\nX_train, X_test,y_train, y_test = train_test_split( X_reshaped, y_reshaped, test_size= 0.3, random_state= 42)\n\n# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha= 0.4, normalize= True)\n\n# Fit the regressor to the data\nlasso.fit(X_train,y_train)\n\n# Predict lasso:\nlasso_pred = lasso.predict(X_test)\n# Lasso accuracy score\nlasso_score= lasso.score(X_test, y_test)\n\nprint(lasso_score)\n","558ffe42":"# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X_reshaped, y_reshaped, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n","9c9e7444":"def display_plot(ridge_scores, ridge_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, ridge_scores)\n\n    std_error = ridge_scores_std \/ np.sqrt(10)\n\n    ax.fill_between(alpha_space, ridge_scores + std_error, ridge_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +\/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(ridge_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()","71c492cb":"display_plot(ridge_scores, ridge_scores_std)","53ac5a9b":"# 2. Linear Regression: Train_test_split Model","69e66c54":"Reference: \n* Datacamp\n* Analytics Vidhya","c93c4af7":"# 5. Regularization I: Lasso Regression\n\n","632725a7":"## 4. K-fold CV comparision\nCross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes.","bacd315a":"### Regularization works by adding a penalty or complexity term or shrinkage term with Residual Sum of Squares (RSS) to the complex model.\n\n### Mainly, there are two types of regularization techniques, which are given below:\n\n* Ridge Regression\n* Lasso Regression\n\n### Key Differences between Ridge and Lasso Regression\n\n* Ridge regression helps us to reduce only the overfitting in the model while keeping all the features present in the model. It reduces the complexity of the model by shrinking the coefficients whereas Lasso regression helps in reducing the problem of overfitting in the model as well as automatic feature selection.\n\n* Lasso Regression tends to make coefficients to absolute zero whereas Ridge regression never sets the value of coefficient to absolute zero.","c681fc64":"# 1. Linear Regression ","b7ee4002":"# 6. Regularization II: Ridge Regression\n\nFitting ridge regression models over a range of different alphas, and plot cross-validated \nR square scores ","a03422bd":"## 3. Cross Validation"}}