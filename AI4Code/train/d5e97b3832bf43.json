{"cell_type":{"34bade5b":"code","f42e99f9":"code","98aafe89":"code","b3e20c4b":"code","3dfe4156":"code","802de3cb":"code","622dee27":"code","0fd1e6d1":"code","650f8480":"code","b77b8202":"code","c11057b9":"code","7eb783dd":"code","24af451a":"code","50000b6c":"code","91112750":"code","5a0343d8":"code","b0cbd96b":"code","edcaf03d":"code","b4871646":"code","e45d6772":"code","18931aaf":"code","006a464d":"code","665b7a17":"markdown","da2ee667":"markdown","b6abdc6e":"markdown","34be163c":"markdown","57696e6d":"markdown","1783668d":"markdown"},"source":{"34bade5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f42e99f9":"fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")","98aafe89":"fake.head()","b3e20c4b":"fake['prediction'] = 0","3dfe4156":"true['prediction'] = 1","802de3cb":"fake.drop(labels=['subject','date'],axis=1,inplace=True)\ntrue.drop(labels=['subject','date'],axis=1,inplace=True)","622dee27":"data = pd.concat([fake,true])","0fd1e6d1":"data = data.sample(len(data))","650f8480":"data.head()","b77b8202":"data['text_'] = data['title']+\" \"+data['text']","c11057b9":"data.head()","7eb783dd":"data = data[['text_','prediction']]","24af451a":"data.head()","50000b6c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.text_, data.prediction, test_size=0.1, random_state=37)","91112750":"from tensorflow.keras.preprocessing.text import Tokenizer\ntk = Tokenizer(num_words=1000,\nfilters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\ntk.fit_on_texts(X_train)\nX_train_seq = tk.texts_to_sequences(X_train)\nX_test_seq = tk.texts_to_sequences(X_test)","5a0343d8":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nX_train_seq_trunc = pad_sequences(X_train_seq, maxlen=100)\nX_test_seq_trunc = pad_sequences(X_test_seq, maxlen=100)","b0cbd96b":"from tensorflow.keras import models,layers\nemb_model = models.Sequential()\nemb_model.add(layers.Embedding(tk.num_words, 8, input_length=100))\nemb_model.add(layers.Convolution1D(16,4,activation='relu'))\nemb_model.add(layers.AveragePooling1D())\nemb_model.add(layers.Convolution1D(32,4,activation='relu'))\nemb_model.add(layers.AveragePooling1D())\nemb_model.add(layers.Flatten())\nemb_model.add(layers.Dense(1, activation='sigmoid'))","edcaf03d":"import tensorflow as tf\nemb_model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(),metrics=[tf.keras.metrics.BinaryAccuracy()])","b4871646":"history = emb_model.fit(x=X_train_seq_trunc,y=y_train,batch_size=256,epochs=100,validation_data=(X_test_seq_trunc,y_test))","e45d6772":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])","18931aaf":"plt.plot(history.history['val_loss'])","006a464d":"history = emb_model.fit(x=X_train_seq_trunc,y=y_train,batch_size=256,epochs=12,validation_data=(X_test_seq_trunc,y_test))","665b7a17":" ### Retraining the model for 12 epochs","da2ee667":"# Tokenizing and Padding Text","b6abdc6e":"# Splitting the Data","34be163c":"## You can see that the validation loss is increasing after 12 epochs after 12 epochs the model is getting overfitted","57696e6d":"# Model (CNN)","1783668d":"# Data Preprocessing"}}