{"cell_type":{"549905d6":"code","0e72c968":"code","59197985":"code","d03ee78f":"code","ee1a4984":"code","8ce61823":"code","06aa2c9d":"code","1dc5785a":"code","1b82ef2f":"code","4ed54cc3":"markdown","a82d158c":"markdown","5c849532":"markdown","b556fd52":"markdown","9fc9225d":"markdown","8367d9c9":"markdown","ef2b4c0a":"markdown"},"source":{"549905d6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.autograd import Variable","0e72c968":"train_df = pd.read_csv(r\"..\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\ntest_df = pd.read_csv(r\"..\/input\/digit-recognizer\/test.csv\",dtype = np.float32)","59197985":"train_feats=train_df.loc[:,train_df.columns!='label'].values\/255\ntrain_label = train_df['label']\n\nXtrain,Xtest,Ytrain,Ytest=train_test_split(train_feats,train_label,test_size=0.2,random_state=42)","d03ee78f":"X_train=torch.from_numpy(np.asarray(Xtrain))\nY_train=torch.from_numpy(np.asarray(Ytrain.astype(np.float32))).type(torch.LongTensor)\nX_test=torch.from_numpy(np.asarray(Xtest))\nY_test=torch.from_numpy(np.asarray(Ytest.astype(np.float32))).type(torch.LongTensor)\n\nn_iters=10000\nbatch_size=100\nn_epochs=n_iters\/(len(X_train)\/batch_size)\nn_epochs=int(n_epochs)\n\ntrain = torch.utils.data.TensorDataset(X_train,Y_train)\ntest = torch.utils.data.TensorDataset(X_test,Y_test)\n\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)","ee1a4984":"class LogisReg(nn.Module):\n    def __init__(self,input_dim,output_dim):\n        super(LogisReg,self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n    def forward(self,X):\n        Y_pred=self.linear(X)\n        return Y_pred\nmodel=LogisReg(784,10)\n\ncriterion=nn.CrossEntropyLoss()\n\noptimizer=torch.optim.SGD(model.parameters(),lr=0.001)","8ce61823":"count=0\nfor epoch in range(n_epochs):\n    for i,(images,labels) in enumerate(train_loader):\n        train=Variable(images.view(-1,784))\n        labels=Variable(labels)\n        optimizer.zero_grad()\n        output=model(train)\n        loss = criterion(output,labels)\n        loss.backward()\n        optimizer.step()\n        count+=1\n        if count%1000==0:\n            #calculate accuracy\n            total=0\n            correct=0\n            for images,labels in test_loader:\n                test=Variable(images.view(-1,784))\n                outputs=model(test)\n                predicted = torch.max(outputs.data, 1)[1]\n                total+=len(labels)\n                correct+= (predicted==labels).sum()\n            accuracy=(torch.div(correct,float(total)))*100\n            #Lets print the results\n            print('Iteration : {}, Loss : {}, Accuracy : {}'.format(count,loss,accuracy))","06aa2c9d":"class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        #First Layer\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        self.mp1 = nn.MaxPool2d(kernel_size=2)\n        #Second Layer\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        self.mp2 = nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(32*4*4, 10) \n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.mp1(out)\n        out = self.conv2(out)\n        out = self.relu2(out)\n        out = self.mp2(out)\n        out = out.view(out.size(0),-1)\n        out = self.fc1(out)\n        return out\n#CNN Model\nmodelCNN = CNNModel()\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizerCNN = torch.optim.SGD(modelCNN.parameters(), lr=0.01)\nn_itersCNN=10000\nbatch_size=100\nn_epochsCNN = n_itersCNN \/ (len(X_train) \/ batch_size)\nn_epochsCNN = int(n_epochsCNN)","1dc5785a":"count=0\nfor epoch in range(n_epochsCNN):\n    for i, (images,labels) in enumerate(train_loader):\n        train=Variable(images.view(100,1,28,28))\n        labels=Variable(labels)\n        optimizerCNN.zero_grad()\n        outputCNN=modelCNN(train)\n        loss = criterion(outputCNN,labels)\n        loss.backward()\n        optimizerCNN.step()\n        count+=1\n        if count%1000==0:\n            #calculate accuracy\n            total=0\n            correct=0\n            for images,labels in test_loader:\n                test=Variable(images.view(100,1,28,28))\n                outputsCNN=modelCNN(test)\n                predicted = torch.max(outputsCNN.data, 1)[1]\n                total+=len(labels)\n                correct+= (predicted==labels).sum()\n            accuracy=(torch.div(correct,float(total)))*100\n            #Lets print results\n            print('Iteration : {}, Loss : {}, Accuracy : {}'.format(count,loss,accuracy))","1b82ef2f":"test_df_numpy=test_df.to_numpy()\ntest_df_numpy=test_df_numpy\/255\ntest_df_numpy = test_df_numpy.reshape(-1,1,28,28)\npreds = modelCNN(torch.from_numpy(test_df_numpy))\npreds.relu()\npreds = np.argmax(preds.detach(),axis = 1)\nresults = pd.DataFrame()\nresults['ImageId'] = np.arange(len(preds)) + 1\nresults['Label'] = pd.Series(preds.detach())\nresults.to_csv('final_submission.csv', index = False)","4ed54cc3":"Importing necessary libraries","a82d158c":"We normalized our dataset by dividing with 255 because, normalisation makes CNN faster","5c849532":"Data input","b556fd52":"CNN model","9fc9225d":"Logistic Regression model","8367d9c9":"Dividing train data into train & test sets using train test split","ef2b4c0a":"Training CNN model"}}