{"cell_type":{"232d4ba4":"code","5b0d168f":"code","8e8f4fb2":"code","c7fafdd9":"code","919e3f9c":"code","878d2221":"code","e79ca99b":"code","ec533101":"code","edd308d9":"code","9f198a3f":"code","cb389b40":"code","25500165":"code","0bf15690":"code","36e04081":"code","7601c8e6":"code","a0cf512d":"code","c86968a8":"code","28cda1e9":"code","aec59880":"code","29941498":"code","a4548bf0":"markdown","55fd4fc6":"markdown","9f0ef5a7":"markdown","02ebaca5":"markdown","fd045639":"markdown","8b91199d":"markdown","98d96361":"markdown","79681458":"markdown","d7bf6f11":"markdown","40544ac9":"markdown","c76d2b84":"markdown","a41eea41":"markdown","654715df":"markdown","cf6a314d":"markdown","985d12e6":"markdown","a087be47":"markdown","8ca583d9":"markdown","37eb9065":"markdown","fa522c6d":"markdown"},"source":{"232d4ba4":"# Data Handling toolbox\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport imageio\nimport os\nimport time\n\n# Machine Learning Toolbox\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, Dropout, Flatten\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\n# Visualization Purpose\nfrom IPython import display","5b0d168f":"# Load the data\nfrom tensorflow.keras.datasets import fashion_mnist as dataset\n(train_images, train_labels), (_, _) = dataset.load_data()","8e8f4fb2":"# reshaping the train images and converting data type to \"float32\"\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')","c7fafdd9":"# Normalizing the train images\ntrain_images = (train_images - 127.5) \/ 127.5","919e3f9c":"# define the constants\nBUFFER_SIZE = 60000\nBATCH_SIZE = 256","878d2221":"# batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","e79ca99b":"# We use the basic generator model defined by the TensorFlow Documentation [1].\ngenerator = Sequential([\n                        # input dense layer, with batch normalization and Leaky ReLU\n                        Dense(7*7*256, use_bias=False, input_shape=(100,)),\n                        BatchNormalization(),\n                        LeakyReLU(alpha=0.35),\n\n                        # Reshaping\n                        Reshape((7,7,256)),\n\n                        # Add Conv2D Transpose Layer with Batch Normalization and LeakyReLU\n                        Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n                        BatchNormalization(),\n                        LeakyReLU(alpha=0.3), # default value of 0.3 is used\n\n                        # Add Another Layer with the same composition\n                        Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n                        BatchNormalization(),\n                        LeakyReLU(alpha=0.3),\n\n                        Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n])","ec533101":"# We define the discriminator model\ndiscriminator = Sequential([\n                        #First Convolution Layer\n                        Conv2D(filters=64,kernel_size=(3,3),padding='same',\n                               kernel_initializer='he_normal',\n                               activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n                        \n                        # Add drop out\n                        Dropout(0.4),\n\n                        #Third Convolution Layer\n                        Conv2D(filters=128,kernel_size=(3,3),padding='same',\n                               kernel_initializer='he_normal',\n                               activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n                        \n                        #Flattening the output from last conv layer\n                        Flatten(),\n\n                        #Using Feed Forward NN as final layers for Classification\n                        #Feed Forward Layer 1\n                        Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)),\n\n                        #Feed Forward Layer 2\n                        Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)),\n\n                        #Feed Forward Layer 3\n                        Dense(32, activation='relu'),\n\n                        #Output Layer\n                        Dense(1)\n])","edd308d9":"cross_entropy = BinaryCrossentropy(from_logits=True)","9f198a3f":"# define the Generator Loss\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","cb389b40":"# define the discriminator loss\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","25500165":"# Optimizer for Generator\ngenerator_optimizer = Adam(1e-4)","0bf15690":"# Optimizer for Discriminator\ndiscriminator_optimizer = Adam(1e-4)","36e04081":"# Set up the checkpoint directory\ncheckpoint_dir = '\/content\/chkpt\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","7601c8e6":"# define a helper function\ndef generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","a0cf512d":"# define the parameters and set the seed\nEPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","c86968a8":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    # Since we are simultaneously training two models,\n    # we use tf.GradientTape() to look out for respective variables of two models\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","28cda1e9":"# Let's define the train loop\ndef train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as you go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)","aec59880":"# Let's train\ntrain(train_dataset, EPOCHS)","29941498":"# The code is based on TensorFlow Official Documentation [1]\n# define the GIF file\nanim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  # get the files starting with \"image\"\n  filenames = glob.glob('image*.png')\n  # sort the files to visualize the evolution\n  filenames = sorted(filenames)\n  for filename in filenames:\n    # read the image file\n    image = imageio.imread(filename)\n    # append image file\n    writer.append_data(image)\n  # get the \"image\" form\n  image = imageio.imread(filename)\n  # append the image\n  writer.append_data(image)","a4548bf0":"In this notebook, we will implement Deep Convolutional Generative Adversarial Network (DC GAN) using TensorFlow framework on the benchmark Fashion MNIST dataset. To follow along, a basic understanding of Python and Deep Learning is necessary. Besides, it is important to be aware of the theory of the Generative Adversarial Network.","55fd4fc6":"As we know, the Generative Adversarial Network consists of two models - Generator and Discriminator. Generator generates images whereas Discriminator tries to estimate if the generated image is really \"generated\" or is just a part of the dataset used. Therefore, Discriminator is a binary classifier and Generator is an unsupervised Deep Learning Model. In this example, we demonstrate DC GAN and, hence, we Deep Convolutional Networks for both the models.  ","9f0ef5a7":"[Source](https:\/\/www.researchgate.net\/figure\/Deep-convolutional-generative-adversarial-networks-DCGAN-for-generative-model-of-BF-NSP_fig3_331282441)","02ebaca5":"## Introduction","fd045639":"## Define the Loss Functions","8b91199d":"## Define the Models","98d96361":"## Data Preprocessing","79681458":"Let us generate a GIF to understand the evolution of Fake Images as the Generator gets better in its performance","d7bf6f11":"Like Loss Functions, we need to define two optimizers","40544ac9":"# Deep Convolutional Generative Adversarial Network using TensorFlow 2","c76d2b84":"## Define the Optimizers","a41eea41":"## Checkpoint","654715df":"## Training the Deep Convolutional GAN","cf6a314d":"## Installation and imports","985d12e6":"* [TensorFlow Documentation](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)\n\n* [Difference between Upsampling and Conv2DTranspose used in U-Net and GANs](https:\/\/youtu.be\/fMwti6zFcYY)\n\n* [How to use the UpSampling2D and Conv2DTranspose Layers in Keras](https:\/\/machinelearningmastery.com\/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks\/)\n\n* [Understand the loss functions and training of a GAN](https:\/\/machinelearningmastery.com\/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions\/)\n\n* [Understand the Gradient Tape](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/GradientTape)","a087be47":"Since we have two models, we need to define two loss functions - one for generator and one for discriminator. \n\nThe losses defined below are based on the official TensorFlow Documentation.","8ca583d9":"## Generate the GIF ","37eb9065":"## References","fa522c6d":"![Image](https:\/\/www.researchgate.net\/publication\/331282441\/figure\/fig3\/AS:729118295478273@1550846756282\/Deep-convolutional-generative-adversarial-networks-DCGAN-for-generative-model-of-BF-NSP.png)"}}